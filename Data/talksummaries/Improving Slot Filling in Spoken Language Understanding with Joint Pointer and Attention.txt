0	83	Slot filling is a key component in spoken language understanding (SLU), which is usually treated as a sequence labeling problem and solved using methods such as conditional random fields (CRFs) (Raymond and Riccardi, 2007) or recurrent neural networks (RNNs) (Yao et al., 2013; Mesnil et al., 2015).
1	8	Although these models have achieved good results, they are learned on the datasets with wordlevel annotations, e.g., with the BIO tagging schema as in ATIS (Hemphill et al., 1990).
2	21	Manual annotations at word level require big effort and some corpora has only sentence-level annotations available, e.g., the utterance “... moderately priced restaurant” has a slot-value pair annotation of “pricerange=moderate”.
3	38	As such datasets lack explicit alignment between the annotations and the input words, some systems rely on handcrafted rules to find the alignments in order to automatically create word-level labels to learn the sequence model (Zhou and He, 2011; Henderson, 2015), but finding such alignments is non-trivial.
4	19	For example, it was shown in (Henderson, 2015) that when applying the manually created word aliases to the speech recognition hypotheses, only around 73% of alignments can be found due to the noise, and a CRF model trained on such noisy data performs particularly worse than some other methods.
6	43	Some other work avoids this issue by regarding slot filling as a classification task (Henderson et al., 2012; Williams, 2014; Barahona et al., 2016), where an utterance is classified into one or more slot-value pairs.
8	25	One is that some types of slots may have a large or even unlimited number of possible values, so the classifiers may suffer from the data sparsity problem when the training data is limited.
9	18	Another is the OOV problem caused by unknown slot values (e.g., restaurant name, street name), which is impossible to predefine and is very common in real-world spoken dialogue applications.
10	31	To address these challenges, we present a neural generative model for slot filling on unaligned dialog data, specifically for slot value prediction as it has more challenges caused by OOV.
11	19	The model uses Seq2Seq learning to predict a sequence of slot values from an utterance.
15	8	To summarize, our main contributions are: • We use a neural generative model for slot filling on the data without word-level annotations which has received less attention.
24	23	The two components (Seq2Seq and Ptr-Net) share the same encoder-decoder architecture and attention scores.
27	32	The slot vocabulary is set to contain only the values of enumerable slots, but not those of non-enumerable slots (e.g., values of “restaurant name”) as we assume these are not known in advance in the real scenarios.
45	20	For better evaluation and comparison, we integrated our model of slot value prediction into a complete SLU system, which uses a CNN classifier to obtain dialog-acts and slot types respectively after slot value prediction.
46	20	For dialog act prediction, the input to the CNN model is the utterance and the output is one or more dialog acts (some utterances can have more than one dialog acts).
47	9	For slot type prediction, the input is each predicted slot value together with the utterance, and the output is one of the predefined slot types.
55	20	For slot value prediction, since it is a sub-task of SLU and not reported in the prior work, we implemented another two models for it.
65	10	Since our assumption is that the proposed model can better handle the OOV problem, we analyze the OOV rate in the corpus to obtain more insight.
67	8	This could be a reason why our model does not obtain larger gain on the complete dataset.
72	8	In this way, we can create training data with less non-enumerable slot values thus resulting in a higher OOV ratio.
84	11	We also conduct the similar OOV experiments as in Section 4.5 for SLU (Table 4).
85	23	Similar trend is observed as discussed before.
86	22	The performance of the proposed model with 20% training data already reaches that of the best system reported in the literature with 100% training data.
88	17	We can see that for the less frequent slots, our model can still predict the values correctly, while without the Ptr-Net, the basic Seq2Seq model tends to generate words not appearing in the input, and CNN outputs nothing in many cases, which aligns with our assumption.
89	29	We analyze the cases where PtrNet does not perform well and find several major types of errors: 1) partial prediction (e.g., detect only “oriental” for “asian oriental food”; 2) the prediction contains repetition of correct values; 3) speech recognition error although the prediction is proper if we look at the hypothesis itself (the third example).
90	12	There are also cases where all the models fail to give a completely correct prediction, yet with different behaviors (the last example).
91	16	We adopt an attentional Seq2Seq model with Ptr-Net to predict slot values on dialogue data when only sentence-level semantic annotations are available.
92	25	By switching between copying and generating words, this solution can bypass the need of word-level annotations and overcome the OOV issue which is very common in real-world spoken dialogue applications.
93	40	It does not require any domain specific rules or dictionaries, and therefore can be easily adapted to new domains.
94	89	Our model has achieved the state-of-the-art performance for both slot value prediction and SLU on the benchmark even with less training data.
