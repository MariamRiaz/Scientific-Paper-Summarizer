0	27	Entities can often be described by very fine grained types.
1	43	Consider the sentences “Bill robbed John.
2	39	He was arrested.” The noun phrases “John,” “Bill,” and “he” have very specific types that can be inferred from the text.
5	100	To address this challenge, we present a new task: given a sentence with a target entity mention, predict free-form noun phrases that describe appropriate types for the role the target entity plays in the sentence.
6	45	Table 1 shows three examples that exhibit a rich variety of types at different granularities.
7	20	Our task effectively subsumes existing finegrained named entity typing formulations due to the use of a very large type vocabulary and the fact that we predict types for all noun phrases, including named entities, nominals, and pronouns.
12	21	For instance, annotators of the OntoNotes dataset (Gillick et al., 2014) marked about half of the mentions as “other,” because they could not find a suitable type in their ontology (see Figure 1 for a visualization and Section 2.2 for details).
14	29	To better understand entity types in an unrestricted setting, we crowdsource a new dataset of 6,000 examples.
15	58	Compared to previous fine-grained entity typing datasets, the label distribution in our data is substantially more diverse and fine-grained.
28	33	We provide the sentence and the target entity mention to five crowd workers on Mechanical Turk, and ask them to annotate the entity’s type.
46	17	To cover 80% of the examples, FIGER requires only the top 7 types, while OntoNotes needs only 4; our dataset requires 429 different types.
53	29	Training data for fine-grained NER systems is typically obtained by linking entity mentions and drawing their types from knowledge bases (KBs).
54	19	This approach has two limitations: recall can suffer due to KB incompleteness (West et al., 2014), and precision can suffer when the selected types do not fit the context (Ritter et al., 2011).
57	26	Using head words as a form of distant supervision provides fine-grained information about named entities and nominal mentions.
62	62	For KB supervision, we leveraged training data from prior work (Ling and Weld, 2012; Gillick et al., 2014) by manually mapping their ontology to our 10,000 noun type vocabulary, which covers 130 of our labels (general and fine-grained).2 Section 6 defines this mapping in more detail.
75	19	The architecture resembles the recent neural AttentiveNER model (Shimaoka et al., 2017), while improving the sentence and mention representations, and introducing a new multitask objective to handle multiple sources of supervision.
83	42	The final representation is the concatenation of the context and mention representations: r = [c;m].
84	23	Label Prediction We learn a type label embedding matrix Wt ∈ Rn×d where n is the number of labels in the prediction space and d is the dimension of r. This matrix can be seen as a combination of three sub matrices, Wgeneral,Wfine,Wultra, each of which contains the representations of the general, fine, and ultra-fine types respectively.
87	22	Multitask Objective The distant supervision sources provide partial supervision for ultra-fine types; KBs often provide more general types, while head words usually provide only ultra-fine types, without their generalizations.
89	39	Prior work used a customized hinge loss (Abhishek et al., 2017) or max margin loss (Ren et al., 2016a) to improve robustness to noisy or incomplete supervision.
94	27	We use this relatively small manuallyannotated training set (Crowd in Table 4) alongside the two distant supervision sources: entity linking (KB and Wikipedia definitions) and head words.
95	83	To combine supervision sources of different magnitudes (2K crowdsourced data, 4.7M entity linking data, and 20M head words), we sample a batch of equal size from each source at each iteration.
96	17	We reimplement the recent AttentiveNER model (Shimaoka et al., 2017) for reference.5 We report macro-averaged precision, recall, and F1, and the average mean reciprocal rank (MRR).
97	85	Results Table 3 shows the performance of our model and our reimplementation of AttentiveNER.
98	23	Our model, which uses a multitask objective to learn finer types without punishing more general types, shows recall gains at the cost of drop in precision.
101	49	Overall, as seen in previous work on finegrained NER literature (Gillick et al., 2014; Ren et al., 2016a), finer labels were more challenging to predict than coarse grained labels, and this issue is exacerbated when dealing with ultra-fine types.
103	28	Head word supervision is particularly helpful for predicting ultra-fine labels, while entity linking improves fine label prediction.
147	75	Using virtually unrestricted types allows us to expand the standard KB-based training methodology with typing information from Wikipedia definitions and naturally-occurring head-word supervision.
148	321	These new forms of distant supervision boost performance on our new dataset as well as on an existing fine-grained entity typing benchmark.
149	141	These results set the first performance levels for our evaluation dataset, and suggest that the data will support significant future work.
