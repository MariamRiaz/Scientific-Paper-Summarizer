{"sections": [{"heading": "1. Introduction", "text": "Language evolves over time and words change their meaning due to cultural shifts, technological inventions, or political events. We consider the problem of detecting shifts in the meaning and usage of words over a given time span based on text data. Capturing these semantic shifts requires a dynamic language model.\nWord embeddings are a powerful tool for modeling semantic relations between individual words (Bengio et al., 2003; Mikolov et al., 2013a; Pennington et al., 2014; Mnih & Kavukcuoglu, 2013; Levy & Goldberg, 2014; Vilnis & McCallum, 2014; Rudolph et al., 2016). Word embed-\n1Disney Research, 4720 Forbes Avenue, Pittsburgh, PA 15213, USA. Correspondence to: Robert Bamler <Robert.Bamler@disneyresearch.com>, Stephan Mandt <Stephan.Mandt@disneyresearch.com>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\ndings model the distribution of words based on their surrounding words in a training corpus, and summarize these statistics in terms of low-dimensional vector representations. Geometric distances between word vectors reflect semantic similarity (Mikolov et al., 2013a) and difference vectors encode semantic and syntactic relations (Mikolov et al., 2013c), which shows that they are sensible representations of language. Pre-trained word embeddings are useful for various supervised tasks, including sentiment analysis (Socher et al., 2013b), semantic parsing (Socher et al., 2013a), and computer vision (Fu & Sigal, 2016). As unsupervised models, they have also been used for the exploration of word analogies and linguistics (Mikolov et al., 2013c).\nWord embeddings are currently formulated as static models, which assumes that the meaning of any given word is the same across the entire text corpus. In this paper, we propose a generalization of word embeddings to sequential data, such as corpora of historic texts or streams of text in social media.\nCurrent approaches to learning word embeddings in a dynamic context rely on grouping the data into time bins and training the embeddings separately on these bins (Kim et al., 2014; Kulkarni et al., 2015; Hamilton et al., 2016). This approach, however, raises three fundamental problems. First, since word embedding models are non-convex, training them twice on the same data will lead to different results. Thus, embedding vectors at successive times can only be approximately related to each other, and only if the embedding dimension is large (Hamilton et al., 2016). Second, dividing a corpus into separate time bins may lead to training sets that are too small to train a word embedding model. Hence, one runs the risk of overfitting to few data whenever the required temporal resolution is fine-grained, as we show in the experimental section. Third, due to the finite corpus size the learned word embedding vectors are subject to random noise. It is difficult to disambiguate this noise from systematic semantic drifts between subsequent times, in particular over short time spans, where we expect only minor semantic drift.\nIn this paper, we circumvent these problems by introducing a dynamic word embedding model. Our contributions are as follows:\nar X\niv :1\n70 2.\n08 35\n9v 2\n[ st\nat .M\nL ]\n1 7\nJu l 2\n01 7\n\u2022 We derive a probabilistic state space model where word and context embeddings evolve in time according to a diffusion process. It generalizes the skip-gram model (Mikolov et al., 2013b; Barkan, 2017) to a dynamic setup, which allows end-to-end training. This leads to continuous embedding trajectories, smoothes out noise in the word-context statistics, and allows us to share information across all times.\n\u2022 We propose two scalable black-box variational inference algorithms (Ranganath et al., 2014; Rezende et al., 2014) for filtering and smoothing. These algorithms find word embeddings that generalize better to held-out data. Our smoothing algorithm carries out efficient black-box variational inference for structured Gaussian variational distributions with tridiagonal precision matrices, and applies more broadly.\n\u2022 We analyze three massive text corpora that span over long periods of time. Our approach allows us to automatically find the words whose meaning changes the most. It results in smooth word embedding trajectories and therefore allows us to measure and visualize the continuous dynamics of the entire embedding cloud as it deforms over time.\nFigure 1 exemplifies our method. The plot shows a fit of our dynamic skip-gram model to Google books (we give details in section 5). We show the ten words whose meaning changed most drastically in terms of cosine distance over the last 150 years. We thereby automatically discover words such as \u201ccomputer\u201d or \u201cradio\u201d whose meaning changed due to technological advances, but also words like\n\u201cpeer\u201d and \u201cnotably\u201d whose semantic shift is less obvious.\nOur paper is structured as follows. In section 2 we discuss related work, and we introduce our model in section 3. In section 4 we present two efficient variational inference algorithms for our dynamic model. We show experimental results in section 5. Section 6 summarizes our findings."}, {"heading": "2. Related Work", "text": "Probabilistic models that have been extended to latent time series models are ubiquitous (Blei & Lafferty, 2006; Wang et al., 2008; Sahoo et al., 2012; Gultekin & Paisley, 2014; Charlin et al., 2015; Ranganath et al., 2015; Jerfel et al., 2017), but none of them relate to word embeddings. The closest of these models is the dynamic topic model (Blei & Lafferty, 2006; Wang et al., 2008), which learns the evolution of latent topics over time. Topic models are based on bag-of-word representations and thus treat words as symbols without modelling their semantic relations. They therefore serve a different purpose.\nMikolov et al. (2013a;b) proposed the skip-gram model with negative sampling (word2vec) as a scalable word embedding approach that relies on stochastic gradient descent. This approach has been formulated in a Bayesian setup (Barkan, 2017), which we discuss separately in section 3.1. These models, however, do not allow the word embedding vectors to change over time.\nSeveral authors have analyzed different statistics of text data to analyze semantic changes of words over time (Mihalcea & Nastase, 2012; Sagi et al., 2011; Kim et al., 2014;\nKulkarni et al., 2015; Hamilton et al., 2016). None of them explicitly model a dynamical process; instead, they slice the data into different time bins, fit the model separately on each bin, and further analyze the embedding vectors in post-processing. By construction, these static models can therefore not share statistical strength across time. This limits the applicability of static models to very large corpora.\nMost related to our approach are methods based on word embeddings. Kim et al. (2014) fit word2vec separately on different time bins, where the word vectors obtained for the previous bin are used to initialize the algorithm for the next time bin. The bins have to be sufficiently large and the found trajectories are not as smooth as ours, as we demonstrate in this paper. Hamilton et al. (2016) also trained word2vec separately on several large corpora from different decades. If the embedding dimension is large enough (and hence the optimization problem less non-convex), the authors argue that word embeddings at nearby times approximately differ by a global rotation in addition to a small semantic drift, and they approximately compute this rotation. As the latter does not exist in a strict sense, it is difficult to distinguish artifacts of the approximate rotation from a true semantic drift. As discussed in this paper, both variants result in trajectories which are noisier.1"}, {"heading": "3. Model", "text": "We propose the dynamic skip-gram model, a generalization of the skip-gram model (word2vec) (Mikolov et al., 2013b) to sequential text data. The model finds word embedding vectors that continuously drift over time, allowing to track changes in language and word usage over short and long periods of time. Dynamic skip-gram is a probabilistic model which combines a Bayesian version of the skip-gram model (Barkan, 2017) with a latent time series. It is jointly\n1 Rudolph & Blei (2017) independently developed a similar model, using a different likelihood model. Their approach uses a non-Bayesian treatment of the latent embedding trajectories, which makes the approach less robust to noise when the data per time step is small.\ntrained end-to-end and scales to massive data by means of approximate Bayesian inference.\nThe observed data consist of sequences of words from a finite vocabulary of size L. In section 3.1, all sequences (sentences from books, articles, or tweets) are considered time-independent; in section 3.2 they will be associated with different time stamps. The goal is to maximize the probability of every word that occurs in the data given its surrounding words within a so-called context window. As detailed below, the model learns two vectors ui, vi \u2208 Rd for each word i in the vocabulary, where d is the embedding dimension. We refer to ui as the word embedding vector and to vi as the context embedding vector."}, {"heading": "3.1. Bayesian Skip-Gram Model", "text": "The Bayesian skip-gram model (Barkan, 2017) is a probabilistic version of word2vec (Mikolov et al., 2013b) and forms the basis of our approach. The graphical model is shown in Figure 2a). For each pair of words i, j in the vocabulary, the model assigns probabilities that word i appears in the context of word j. This probability is \u03c3(u>i vj) with the sigmoid function \u03c3(x) = 1/(1 + e\u2212x). Let zij \u2208 {0, 1} be an indicator variable that denotes a draw from that probability distribution, hence p(zij = 1) = \u03c3(u>i vj). The generative model assumes that many word-word pairs (i, j) are uniformly drawn from the vocabulary and tested for being a word-context pair; hence a separate random indicator zij is associated with each drawn pair.\nFocusing on words and their neighbors in a context window, we collect evidence of word-word pairs for which zij = 1. These are called the positive examples. Denote n+ij the number of times that a word-context pair (i, j) is observed in the corpus. This is a sufficient statistic of the model, and its contribution to the likelihood is p(n+ij |ui, vj) = \u03c3(u>i vj)n + ij . However, the generative process also assumes the possibility to reject word-word pairs if zij = 0. Thus, one needs to construct a fictitious second training set of rejected word-word pairs, called negative examples. Let the corresponding counts be n\u2212ij . The total likelihood of both positive and negative examples is then\np(n+, n\u2212|U, V ) = L\u220f\ni,j=1\n\u03c3(u>i vj) n+ij\u03c3(\u2212u>i vj)n \u2212 ij . (1)\nAbove we used the antisymmetry \u03c3(\u2212x) = 1 \u2212 \u03c3(x). In our notation, dropping the subscript indices for n+ and n\u2212 denotes the entire L \u00d7 L matrices, U = (u1, \u00b7 \u00b7 \u00b7 , uL) \u2208 Rd\u00d7L is the matrix of all word embedding vectors, and V is defined analogously for the context vectors. To construct negative examples, one typically chooses n\u2212ij \u221d P (i)P (j)3/4 (Mikolov et al., 2013b), where P (i) is the\nfrequency of word i in the training corpus. Thus, n\u2212 is well-defined up to a constant factor which has to be tuned.\nDefining n\u00b1 = (n+, n\u2212) the combination of both positive and negative examples, the resulting log likelihood is\nlog p(n\u00b1|U, V ) = L\u2211\ni,j=1\n( n+ij log \u03c3(u > i vj) + n \u2212 ij log \u03c3(\u2212u>i vj) ) . (2)\nThis is exactly the objective of the (non-Bayesian) skipgram model, see (Mikolov et al., 2013b). The count matrices n+ and n\u2212 are either pre-computed for the entire corpus, or estimated based on stochastic subsamples from the data in a sequential way, as done by word2vec. Barkan (2017) gives an approximate Bayesian treatment of the model with Gaussian priors on the embeddings."}, {"heading": "3.2. Dynamic Skip-Gram Model", "text": "The key extension of our approach is to use a Kalman filter as a prior for the time-evolution of the latent embeddings (Welch & Bishop, 1995). This allows us to share information across all times while still allowing the embeddings to drift.\nNotation. We consider a corpus of T documents which were written at time stamps \u03c41 < . . . < \u03c4T . For each time step t \u2208 {1, . . . , T} the sufficient statistics of word-context pairs are encoded in the L\u00d7L matrices n+t , n\u2212t of positive and negative counts with matrix elements n+ij,t and n \u2212 ij,t, respectively. Denote Ut = (u1,t, \u00b7 \u00b7 \u00b7 , uL,t) \u2208 Rd\u00d7L the matrix of word embeddings at time t, and define Vt correspondingly for the context vectors. Let U, V \u2208 RT\u00d7d\u00d7L denote the tensors of word and context embeddings across all times, respectively.\nModel. The graphical model is shown in Figure 2b). We consider a diffusion process of the embedding vectors over time. The variance \u03c32t of the transition kernel is\n\u03c32t = D(\u03c4t+1 \u2212 \u03c4t), (3) whereD is a global diffusion constant and (\u03c4t+1\u2212\u03c4t) is the time between subsequent observations (Welch & Bishop, 1995). At every time step t, we add an additional Gaussian prior with zero mean and variance \u03c320 which prevents the embedding vectors from growing very large, thus\np(Ut+1|Ut) \u221d N (Ut, \u03c32t )N (0, \u03c320). (4) Computing the normalization, this results in\nUt+1|Ut \u223c N (\nUt 1 + \u03c32t /\u03c3 2 0 , 1 \u03c3\u22122t + \u03c3 \u22122 0 I\n) , (5)\nVt+1|Vt \u223c N (\nVt 1 + \u03c32t /\u03c3 2 0 , 1 \u03c3\u22122t + \u03c3 \u22122 0 I\n) . (6)\nIn practice, \u03c30 \u03c3t, so the damping to the origin is very weak. This is also called Ornstein-Uhlenbeck process (Uhlenbeck & Ornstein, 1930). We recover the Wiener process for \u03c30 \u2192 \u221e, but \u03c30 < \u221e prevents the latent time series from diverging to infinity. At time index t = 1, we define p(U1|U0) \u2261 N (0, \u03c320I) and do the same for V1. Our joint distribution factorizes as follows:\np(n\u00b1, U, V ) = T\u22121\u220f\nt=0\np(Ut+1|Ut) p(Vt+1|Vt)\n\u00d7 T\u220f\nt=1\nL\u220f\ni,j=1\np(n\u00b1ij,t|ui,t, vj,t) (7)\nThe prior model enforces that the model learns embedding vectors which vary smoothly across time. This allows to associate words unambiguously with each other and to detect semantic changes. The model efficiently shares information across the time domain, which allows to fit the model even in setups where the data at every given point in time are small, as long as the data in total are large."}, {"heading": "4. Inference", "text": "We discuss two scalable approximate inference algorithms. Filtering uses only information from the past; it is required in streaming applications where the data are revealed to us sequentially. Smoothing is the other inference method, which learns better embeddings but requires the full sequence of documents ahead of time.\nIn Bayesian inference, we start by formulating a joint distribution (Eq. 7) over observations n\u00b1 and parameters U and V , and we are interested in the posterior distribution over parameters conditioned on observations,\np(U, V |n\u00b1) = p(n \u00b1, U, V )\u222b\np(n\u00b1, U, V ) dUdV (8)\nThe problem is that the normalization is intractable. In variational inference (VI) (Jordan et al., 1999; Blei et al., 2016) one sidesteps this problem and approximates the posterior with a simpler variational distribution q\u03bb(U, V ) by minimizing the Kullback-Leibler (KL) divergence to the posterior. Here, \u03bb summarizes all parameters of the variational distribution, such as the means and variances of a Gaussian, see below. Minimizing the KL divergence is equivalent to optimizing the evidence lower bound (ELBO) (Blei et al., 2016),\nL(\u03bb) = Eq\u03bb [log p(n\u00b1, U, V )]\u2212Eq\u03bb [log q\u03bb(U, V )]. (9)\nFor a restricted class of models, the ELBO can be computed in closed-form (Hoffman et al., 2013). Our model is\nnon-conjugate and requires instead black-box VI using the reparameterization trick (Rezende et al., 2014; Kingma & Welling, 2014)."}, {"heading": "4.1. Skip-Gram Filtering", "text": "In many applications such as streaming, the data arrive sequentially. Thus, we can only condition our model on past and not on future observations. We will first describe inference in such a (Kalman) filtering setup (Kalman et al., 1960; Welch & Bishop, 1995).\nIn the filtering scenario, the inference algorithm iteratively updates the variational distribution q as evidence from each time step t becomes available. We thereby use a variational distribution that factorizes across all times, q(U, V ) =\u220fT t=1 q(Ut, Vt) and we update the variational factor at a given time t based on the evidence at time t and the approximate posterior of the previous time step. Furthermore, at every time t we use a fully-factorized distribution:\nq(Ut, Vt) =\nL\u220f\ni=1\nN (ui,t;\u00b5ui,t,\u03a3ui,t)N (vi,t;\u00b5vi,t.\u03a3vi,t),\nThe variational parameters are the means \u00b5ui,t, \u00b5vi,t \u2208 Rd and the covariance matrices \u03a3ui,t and \u03a3vi,t, which we restrict to be diagonal (mean-field approximation).\nWe now describe how we sequentially compute q(Ut, Vt) and use the result to proceed to the next time step. As other Markovian dynamical systems, our model assumes the following recursion,\np(Ut, Vt|n\u00b11:t) \u221d p(n\u00b1t |Ut, Vt) p(Ut, Vt|n\u00b11:t\u22121). (10)\nWithin our variational approximation, the ELBO (Eq. 9) therefore separates into a sum of T terms, L = \u2211t Lt with\nLt = E[log p(n\u00b1t |Ut, Vt)] + E[log p(Ut, Vt|n\u00b11:t\u22121)] \u2212 E[log q(Ut, Vt)], (11)\nwhere all expectations are taken under q(Ut, Vt). We compute the entropy term \u2212E[log q] in Eq. 11 analytically and estimate the gradient of the log likelihood by sampling from the variational distribution and using the reparameterization trick (Kingma & Welling, 2014; Salimans & Kingma, 2016). However, the second term of Eq. 11, containing the prior at time t, is still intractable. We approximate the prior as\np(Ut, Vt|n\u00b11:t\u22121) \u2261 Ep(Ut\u22121,Vt\u22121|n\u00b11:t\u22121) [ p(Ut, Vt|Ut\u22121, Vt\u22121) ]\n\u2248 Eq(Ut\u22121,Vt\u22121) [ p(Ut, Vt|Ut\u22121, Vt\u22121) ] . (12)\nThe remaining expectation involves only Gaussians and can be carried-out analytically. The resulting approximate\nprior is a fully factorized distribution p(Ut, Vt|n\u00b11:t\u22121) \u2248\u220fL i=1N (ui,t; \u00b5\u0303ui,t, \u03a3\u0303ui,t)N (vi,t; \u00b5\u0303vi,t, \u03a3\u0303vit) with\n\u00b5\u0303ui,t = \u03a3\u0303ui,t ( \u03a3ui,t\u22121 + \u03c3 2 t I )\u22121 \u00b5ui,t\u22121; \u03a3\u0303ui,t = [( \u03a3ui,t\u22121 + \u03c3 2 t I )\u22121 + (1/\u03c320)I ]\u22121 . (13)\nAnalogous update equations hold for \u00b5\u0303vi,t and \u03a3\u0303vi,t. Thus, the second contribution in Eq. 11 (the prior) yields a closedform expression. We can therefore compute its gradient."}, {"heading": "4.2. Skip-Gram Smoothing", "text": "In contrast to filtering, where inference is conditioned on past observations until a given time t, (Kalman) smoothing performs inference based on the entire sequence of observations n\u00b11:T . This approach results in smoother trajectories and typically higher likelihoods than with filtering, because evidence is used from both future and past observations.\nBesides the new inference scheme, we also use a different variational distribution. As the model is fitted jointly to all time steps, we are no longer restricted to a variational distribution that factorizes in time. For simplicity we focus here on the variational distribution for the word embeddings U ; the context embeddings V are treated identically. We use a factorized distribution over both embedding space and vocabulary space,\nq(U1:T ) =\nL\u220f\ni=1\nd\u220f\nk=1\nq(uik,1:T ). (14)\nIn the time domain, our variational approximation is structured. To simplify the notation we now drop the indices for words i and embedding dimension k, hence we write q(u1:T ) for q(uik,1:T ) where we focus on a single factor. This factor is a multivariate Gaussian distribution in the time domain with tridiagonal precision matrix \u039b,\nq(u1:T ) = N (\u00b5,\u039b\u22121) (15) Both the means \u00b5 = \u00b51:T and the entries of the tridiagonal precision matrix \u039b \u2208 RT\u00d7T are variational parameters. This gives our variational distribution the interpretation of a posterior of a Kalman filter (Blei & Lafferty, 2006), which captures correlations in time.\nWe fit the variational parameters by training the model jointly on all time steps, using black-box VI and the reparameterization trick. As the computational complexity of an update step scales as \u0398(L2), we first pretrain the model by drawing minibatches of L\u2032 < L random words and L\u2032 random contexts from the vocabulary (Hoffman et al., 2013). We then switch to the full batch to reduce the sampling noise. Since the variational distribution does not factorize in the time domain we always include all time steps {1, . . . , T} in the minibatch.\nWe also derive an efficient algorithm that allows us to estimate the reparametrization gradient using \u0398(T ) time and memory, while a naive implementation of black-box variational inference with our structured variational distribution would require \u0398(T 2) of both resources. The main idea is to parametrize \u039b = B>B in terms of its Cholesky decomposition B, which is bidiagonal (K\u0131l\u0131c\u0327 & Stanica, 2013), and to express gradients of B\u22121 in terms of gradients of B. We use mirror ascent (Ben-Tal et al., 2001; Beck & Teboulle, 2003) to enforce positive definiteness of B. The algorithm is detailed in our supplementary material."}, {"heading": "5. Experiments", "text": "We evaluate our method on three time-stamped text corpora. We demonstrate that our algorithms find smoother embedding trajectories than methods based on a static model. This allows us to track semantic changes of individual words by following nearest-neighbor relations over time. In our quantitative analysis, we find higher predictive likelihoods on held-out data compared to our baselines.\nAlgorithms and Baselines. We report results from our proposed algorithms from section 4 and compare against baselines from section 2:\n\u2022 SGI denotes the non-Bayesian skip-gram model with independent random initializations of word vectors (Mikolov et al., 2013b). We used our own implementation of the model by dropping the Kalman filtering prior and point-estimating embedding vectors. Word vectors at nearby times are made comparable by approximate orthogonal transformations, which corresponds to Hamilton et al. (2016).\n\u2022 SGP denotes the same approach as above, but with word and context vectors being pre-initialized with the values from the previous year, as in Kim et al. (2014).\n\u2022 DSG-F: dynamic skip-gram filtering (proposed). \u2022 DSG-S: dynamic skip-gram smoothing (proposed).\nData and preprocessing. Our three corpora exemplify opposite limits both in the covered time span and in the amount of text per time step.\n1. We used data from the Google books corpus2 (Michel et al., 2011) from the last two centuries (T = 209). This amounts to 5 million digitized books and approximately 1010 observed words. The corpus consists of n-gram tables with n \u2208 {1, . . . , 5}, annotated by year of publication. We considered the years from 1800 to\n2http://storage.googleapis.com/books/ ngrams/books/datasetsv2.html\n2008 (the latest available). In 1800, the size of the data is approximately\u223c 7 \u00b7107 words. We used the 5-gram counts, resulting in a context window size of 4.\n2. We used the \u201cState of the Union\u201d (SoU) addresses of U.S. presidents, which spans more than two centuries, resulting in T = 230 different time steps and approximately 106 observed words.3 Some presidents gave both a written and an oral address; if these were less than a week apart we concatenated them and used the average date. We converted all words to lower case and constructed the positive sample counts n+ij using a context window size of 4.\n3. We used a Twitter corpus of news tweets for 21 randomly drawn dates from 2010 to 2016. The median number of tweets per day is 722. We converted all tweets to lower case and used a context window size of 4, which we restricted to stay within single tweets.\nHyperparameters. The vocabulary for each corpus was constructed from the 10,000 most frequent words throughout the given time period. In the Google books corpus, the number of words per year grows by a factor of 200 from the year 1800 to 2008. To avoid that the vocabulary is dominated by modern words we normalized the word frequencies separately for each year before adding them up.\nFor the Google books corpus, we chose the embedding dimension d = 200, which was also used in Kim et al. (2014). We set d = 100 for SoU and Twitter, as d = 200 resulted in overfitting on these much smaller corpora. The ratio \u03b7 = \u2211 ij n \u2212 ij,t/ \u2211 ij n + ij,t of negative to positive wordcontext pairs was \u03b7 = 1. The precise construction of the matrices n\u00b1t is explained in the supplementary material. We used the global prior variance \u03c320 = 1 for all corpora and all algorithms, including the baselines. The diffusion constant D controls the time scale on which information is shared between time steps. The optimal value for D depends on the application. A single corpus may exhibit semantic shifts of words on different time scales, and the optimal choice for D depends on the time scale in which one is interested. We used D = 10\u22123 per year for Google books and SoU, and D = 1 per year for the Twitter corpus, which spans a much shorter time range. In the supplementary material, we provide details of the optimization procedure.\nQualitative results. We show that our approach results in smooth word embedding trajectories on all three corpora. We can automatically detect words that undergo significant semantic changes over time.\nFigure 1 in the introduction shows a fit of the dynamic skip-gram filtering algorithm to the Google books corpus.\n3http://www.presidency.ucsb.edu/sou.php\nHere, we show the ten words whose word vectors change most drastically over the last 150 years in terms of cosine distance. Figure 3 visualizes word embedding clouds over four subsequent years of Google books, where we compare DSG-F against SGI. We mapped the normalized embedding vectors to two dimensions using dynamic t-SNE (Rauber et al., 2016) (see supplement for details). Lines indicate shifts of word vectors relative to the preceding year. In our model only few words change their position in the embedding space rapidly, while embeddings using SGI show strong fluctuations, making the cloud\u2019s motion hard to track.\nFigure 4 visualizes the smoothness of the trajectories directly in the embedding space (without the projection to two dimensions). We consider differences between word vectors in the year 1998 and the subsequent 10 years. In more detail, we compute histograms of the Euclidean distances ||uit \u2212 ui,t+\u03b4|| over the word indexes i, where \u03b4 = 1, . . . , 10 (as discussed previously, SGI uses a global rotation to optimally align embeddings first). In our model, embedding vectors gradually move away from their original position as time progresses, indicating a directed motion. In contrast, both baseline models show only little directed motion after the first time step, suggesting that most temporal changes are due to finite-size fluctuations of n\u00b1ij,t. Initialization schemes alone, thus, seem to have a minor effect on smoothness.\nOur approach allows us to detect semantic shifts in the usage of specific words. Figures 5 and 1 both show the cosine distance between a given word and its neighboring words (colored lines) as a function of time. Figure 5 shows results on all three corpora and focuses on a comparison across methods. We see that DSG-S and DSG-F (both proposed)\nresult in trajectories which display less noise than the baselines SGP and SGI. The fact that the baselines predict zero cosine distance (no correlation) between the chosen word pairs on the SoU and Twitter corpora suggests that these corpora are too small to successfully fit these models, in contrast to our approach which shares information in the time domain. Note that as in dynamic topic models, skipgram smoothing (DSG-S) may diffuse information into the past (see \u201dpresidential\u201d to \u201dclinton-trump\u201d in Fig. 5).\nQuantitative results. We show that our approach generalizes better to unseen data. We thereby analyze held-out predictive likelihoods on word-context pairs at a given time t, where t is excluded from the training set,\n1 |n\u00b1t | log p(n\u00b1t |U\u0303t, V\u0303t). (16)\nAbove, |n\u00b1t | = \u2211 i,j ( n+ij,t + n \u2212 ij,t ) denotes the total number of word-context pairs at time \u03c4t. Since inference is different in all approaches, the definitions of word and context embedding matrices U\u0303t and V\u0303t in Eq. 16 have to be adjusted:\n\u2022 For SGI and SGP, we did a chronological pass through the time sequence and used the embeddings U\u0303t = Ut\u22121 and V\u0303t = Vt\u22121 from the previous time step to predict the statistics n\u00b1ij,t at time step t. \u2022 For DSG-F, we did the same pass to test n\u00b1ij,t. We thereby set U\u0303t and V\u0303t to be the modes Ut\u22121, Vt\u22121 of the approximate posterior at the previous time step.\n\u2022 For DSG-S, we held out 10%, 10% and 20% of the documents from the Google books, SoU, and Twitter corpora for testing, respectively. After training, we estimated the word (context) embeddings U\u0303t (V\u0303t) in\nEq. 16 by linear interpolation between the values of Ut\u22121 (Vt\u22121) and Ut+1 (Vt+1) in the mode of the variational distribution, taking into account that the time stamps \u03c4t are in general not equally spaced.\nThe predictive likelihoods as a function of time \u03c4t are shown in Figure 6. For the Google Books corpus (left panel in figure 6), the predictive log-likelihood grows over time with all four methods. This must be an artifact of the corpus since SGI does not carry any information of the past. A possible explanation is the growing number of words per year from 1800 to 2008 in the Google Books corpus. On all three corpora, differences between the two implementations of the static model (SGI and SGP) are small, which suggests that pre-initializing the embeddings with the previous result may improve their continuity but seems to have little impact on the predictive power. Log-likelihoods for the skip-gram filter (DSG-F) grow over the first few time steps as the filter sees more data, and then saturate. The improvement of our approach over the static model is particularly pronounced in the SoU and Twitter corpora, which are much smaller than the massive Google books corpus. There, sharing information between across time is crucial because there is little data at every time slice. Skip-gram smoothing outperforms skip-gram filtering as it shares in-\nformation in both time directions and uses a more flexible variational distribution."}, {"heading": "6. Conclusions", "text": "We presented the dynamic skip-gram model: a Bayesian probabilistic model that combines word2vec with a latent continuous time series. We showed experimentally that both dynamic skip-gram filtering (which conditions only on past observations) and dynamic skip-gram smoothing (which uses all data) lead to smoothly changing embedding vectors that are better at predicting word-context statistics at held-out time steps. The benefits are most drastic when the data at individual time steps is small, such that fitting a static word embedding model is hard. Our approach may be used as a data mining and anomaly detection tool when streaming text on social media, as well as a tool for historians and social scientists interested in the evolution of language."}, {"heading": "Acknowledgements", "text": "We would like to thank Marius Kloft, Cheng Zhang, Andreas Lehrmann, Brian McWilliams, Romann Weber, Michael Clements, and Ari Pakman for valuable feedback."}, {"heading": "1. Dimensionality Reduction in Figure 1", "text": "To create the word-clouds in Figure 1 of the main text we mapped the fitted word embeddings from Rd to the twodimensional plane using dynamic t-SNE (Rauber et al., 2016). Dynamic t-SNE is a non-parametric dimensionality reduction algorithm for sequential data. The algorithm finds a projection to a lower dimension by solving a non-convex optimization problem that aims at preserving nearest-neighbor relations at each individual time step. In\n1Disney Research, 4720 Forbes Avenue, Pittsburgh, PA 15213, USA. Correspondence to: Robert Bamler <Robert.Bamler@disneyresearch.com>, Stephan Mandt <Stephan.Mandt@disneyresearch.com>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\naddition, projections at neighboring time steps are aligned with each other by a quadratic penalty with prefactor \u03bb \u2265 0 for sudden movements.\nThere is a trade-off between finding good local projections for each individual time step (\u03bb \u2192 0), and finding smooth projections (large \u03bb). Since we want to analyze the smoothness of word embedding trajectories, we want to avoid bias towards smooth projections. Unfortunately, setting \u03bb = 0 is not an option since, in this limit, the optimization problem is invariant under independent rotations at each time, rendering trajectories in the two-dimensional projection plane meaningless. To still avoid bias towards smooth projections, we anneal \u03bb exponentially towards zero over the course of the optimization. We start the optimizer with \u03bb = 0.01, and we reduce \u03bb by 5% with each training step. We run 100 optimization steps in total, so that \u03bb \u2248 6\u00d710\u22126 at the end of the training procedure. We used the opensource implementation,1 set the target perplexities to 200, and used default values for all other parameters."}, {"heading": "2. Hyperparemeters and Construction of n\u00b11:T", "text": "Table S1 lists the hyperparameters used in our experiments. For the Google books corpus, we used the same context window size cmax and embedding dimension d as in (Kim et al., 2014). We reduced d for the SoU and Twitter corpora to avoid overfitting to these much smaller data sets.\nIn constrast to word2vec, we construct our positive and negative count matrices n\u00b1ij,t deterministically in a preprocessing step. As detailed below, this is done such that it resembles as closely as possible the stochastic approach in word2vec (Mikolov et al., 2013). In every update step, word2vec stochastically samples a context window size uniformly in an interval [1, \u00b7 \u00b7 \u00b7 , cmax], thus the context size fluctuates and nearby words appear more often in the same context than words that are far apart from each other in the sentence. We follow a deterministic scheme that results in similar statistics. For each pair of words (w1, w2) in a given sentence, we increase the counts n+iw1 jw2 by max (0, 1\u2212 k/cmax), where 0 \u2264 k \u2264 cmax is the number of words that appear between w1 and w2, and iw1 and jw2 are the words\u2019 unique indices in the vocabulary.\n1https://github.com/paulorauber/thesne\nar X\niv :1\n70 2.\n08 35\n9v 2\n[ st\nat .M\nL ]\n1 7\nJu l 2\n01 7\nAlgorithm 1 Skip-gram filtering; see section 4 of the main text.\nRemark: All updates are analogous for word and context vectors; we drop their indices for simplicity. Input: number of time steps T , time stamps \u03c41:T , positive and negative examples n\u00b11:T , hyperparameters. Init. prior means \u00b5\u0303ik,1 \u2190 0 and variances \u03a3\u0303i,1 = Id\u00d7d Init. variational means \u00b5ik,1 \u2190 0 and var. \u03a3i,1 = Id\u00d7d for t = 1 to T do\nif t 6= 1 then Update approximate Gaussian prior with params. \u00b5\u0303ik,t and \u03a3\u0303i,t using \u00b5ik,t\u22121 and \u03a3i,t\u22121, see Eq. 13. end if Compute entropy Eq[log q(\u00b7)] analytically. Compute expected log Gaussian prior with parameters \u00b5\u0303ik,t and \u03a3\u0303k,t analytically. Maximize Lt in Eq. 11, using black-box VI with the reparametrization trick. Obtain \u00b5ik,t and \u03a3i,t as outcome of the optimization.\nend for\nWe also used a deterministic variant of word2vec to construct the negative count matrices n\u2212t . In word2vec, \u03b7 negative samples (i, j) are drawn for each positive sample (i, j\u2032) by drawing \u03b7 independent values for j from a distribution P \u2032t (j) defined below. We define n \u2212 ij,t such that it matches the expectation value of the number of times that word2vec would sample the negative word-context pair (i, j). Specifically, we define\nPt(i) =\n\u2211L j=1 n\n+ ij,t\u2211L\ni\u2032,j=1 n + i\u2032j,t\n, (S1)\nP \u2032t (j) =\n( Pt(j) )\u03b3 \u2211L j\u2032=1 ( Pt(j\u2032) )\u03b3 , (S2)\nn\u2212ij,t =\n( L\u2211\ni\u2032,j\u2032=1\nn+i\u2032j\u2032,t ) \u03b7Pt(i)P \u2032 t (j). (S3)\nWe chose \u03b3 = 0.75 as proposed in (Mikolov et al., 2013), and we set \u03b7 = 1. In practice, it is not necessary to explicitly construct the full matrices n\u2212t , and it is more efficient to keep only the distributions Pt(i) and P \u2032t (j) in memory."}, {"heading": "3. Skip-gram Filtering Algorithm", "text": "The skip-gram filtering algorithm is described in section 4 of the main text. We provide a formulation in pseudocode in Algorithm 1."}, {"heading": "4. Skip-gram Smoothing Algorithm", "text": "In this section, we give details for the skip-gram smoothing algorithm, see section 4 of the main text. A summary is\nAlgorithm 2 Skip-gram smoothing; see section 4. We drop indices i, j, and k for word, context, end embedding dimension, respectively, when they are clear from context.\nInput: number of time steps T , time stamps \u03c41:T , wordcontext counts n+1:T , hyperparameters in Table S1 Obtain n\u2212t \u2200t using Eqs. S1\u2013S3 Initialize \u00b5u,1:T , \u00b5v,1:T \u2190 0 Initialize \u03bdu,1:T , \u03bdv,1:T , \u03c9u,1:T\u22121, and \u03c9v,1:T\u22121 such\nthat B>u Bu = B > v Bv = \u03a0 (see Eqs. S5 and S11) for step = 1 to N \u2032tr do Draw I \u2282 {1, . . . , L\u2032} with |I| = L\u2032 uniformly Draw J \u2282 {1, . . . , L\u2032} with |J | = L\u2032 uniformly for all i \u2208 I do\nDraw [s]ui,1:T \u223c N (0, I) Solve Bu,ixui,1:T = ui,1:T for xui,1:T\nend for Obtain xvj,1:T by repeating last loop \u2200j \u2208 J Calculate gradient estimates of L for minibatch\n(I,J ) using Eqs. S10, S14, and S15 Obtain update steps d[\u00b7] for all variational parameters\nusing Adam optimizer with parameters in Table S1 Update \u00b5u,1:T \u2190 \u00b5u,1:T +d[\u00b5u,1:T ], and analogously\nfor \u00b5v,1:T , \u03c9u,1:T\u22121, and \u03c9v,1:T\u22121 Update \u03bdu,1:T and \u03bdv,1:T according to Eq. S18\nend for Repeat above loop for Ntr more steps, this time without\nminibatch sampling (i.e., setting L\u2032 = L)\nprovided in Algorithm 2.\nVariational distribution. For now, we focus on the word embeddings, and we simplify the notation by dropping the indices for the vocabulary and embedding dimensions. The variational distribution for a single embedding dimension of a single word embedding trajectory is\nq(u1:T ) = N (\u00b5u,1:T , (B>u Bu)\u22121). (S4)\nHere, \u00b5u,1:T is the vector of mean values, and Bu is the Cholesky decomposition of the precision matrix. We restrict the latter to be bidiagonal,\nBu =   \u03bdu,1 \u03c9u,1 \u03bdu,2 \u03c9u,2 . . . . . . \u03bdu,T\u22121 \u03c9u,T\u22121\n\u03bdT\n  (S5)\nwith \u03bdu,t > 0 for all t \u2208 {1, . . . , T}. The variational parameters are \u00b5u,1:T , \u03bdu,1:T , and \u03c91:T\u22121. The variational distribution of the context embedding trajectories v1:T has the same structure.\nWith the above form of Bu, the variational distribution is a Gaussian with an arbitrary tridiagonal symmetric precision matrix B>u Bu. We chose this variational distribution because it is the exact posterior of a hidden time-series model with a Kalman filtering prior and Gaussian noise (Blei & Lafferty, 2006). Note that our variational distribution is a generalization of a fully factorized (mean-field) distribution, which is obtained for \u03c9u,t = 0 \u2200t. In the general case, \u03c9u,t 6= 0, the variational distribution can capture correlations between all time steps, with a dense covariance matrix (B>u Bu) \u22121.\nGradient estimation. The skip-gram smoothing algorithm uses stochastic gradient ascent to find the variational parameters that maximize the ELBO,\nL = Eq [ log p(U1:T , V1:T , n \u00b1 1:T ) ] \u2212 Eq [ log q(U1:T , V1:T ) ] .\n(S6)\nHere, the second term is the entropy, which can be evaluated analytically. We obtain for each component in vocabulary and embedding space,\n\u2212Eq[log q(u1:T )] = \u2212 \u2211\nt\nlog(\u03bdu,t) + const. (S7)\nand analogously for \u2212Eq[log q(v1:T )]. The first term on the right-hand side of Eq. S6 cannot be evaluated analytically. We approximate its gradient by sampling from q using the reparameterization trick (Kingma & Welling, 2014; Rezende et al., 2014). A naive calculation would require \u2126(T 2) computing time since the derivatives of L with respect to \u03bdu,t and \u03c9u,t for each t depend on the count matrices n\u00b1t\u2032 of all t\n\u2032. However, as we show next, there is a more efficient way to obtain all gradient estimates in \u0398(T ) time.\nWe focus again on a single dimension of a single word embedding trajectory u1:T , and we drop the indices i and k. We draw S independent samples u[s]1:T with s \u2208 {1, . . . , S} from q(u1:T ) by parameterizing\nu [s] 1:T = \u00b5u,1:T + x [s] u,1:T (S8)\nwith\nx [s] u,1:T = B \u22121 u [s] u,1:T where [s] u,1:T \u223c N (0, I). (S9)\nWe obtain x[s]u,1:T in \u0398(T ) time by solving the bidiagonal linear system Bux [s] u,1:T = [s] u,1:T . Samples v [s] 1:T for the context embedding trajectories are obtained analogously. Our implementation uses S = 1, i.e., we draw only a single sample per training step. Averaging over several samples is done implicitly since we calculate the update steps\nusing the Adam optimizer (Kingma & Ba, 2014), which effectively averages over several gradient estimates in its first moment estimate.\nThe derivatives of L with respect to \u00b5u,1:T can be obtained using Eq. S8 and the chain rule. We find\n\u2202L \u2202\u00b5u,1:T \u2248 1 S\nS\u2211\ns=1\n[ \u0393 [s] u,1:T \u2212\u03a0u [s] 1:T ] . (S10)\nHere, \u03a0 \u2208 RT\u00d7T is the precision matrix of the prior u1:T \u223c N (0,\u03a0\u22121). It is tridiagonal and therefore the matrix-multiplication \u03a0u[s]1:T can be carried out efficiently. The non-zero matrix elements of \u03a0 are\n\u03a011 = \u03c3 \u22122 0 + \u03c3 \u22122 1\n\u03a0TT = \u03c3 \u22122 0 + \u03c3 \u22122 T\u22121\n\u03a0tt = \u03c3 \u22122 0 + \u03c3 \u22122 t\u22121 + \u03c3 \u22122 t \u2200t \u2208 {2, . . . , T \u2212 1}\n\u03a01,t+1 = \u03a0t+1,1 = \u2212\u03c3\u22122t . (S11)\nThe term \u0393[s]u,1:T on the right-hand side of Eq. S10 comes from the expectation value of the log-likelihood under q. It is given by\n\u0393 [s] ui,t =\nL\u2211\nj=1\n[( n+ij,t + n \u2212 ij,t ) \u03c3 ( \u2212u[s]>i,t v [s] j,t ) \u2212 n\u2212ij,t ] v [s] j,t\n(S12)\nwhere we temporarily restored the indices i and j for words and contexts, respectively. In deriving Eq. S12, we used the relations \u2202 log \u03c3(x)/\u2202x = \u03c3(\u2212x) and \u03c3(\u2212x) = 1\u2212 \u03c3(x). The derivatives of L with respect to \u03bdu,t and \u03c9u,t are more intricate. Using the parameterization in Eqs. S8\u2013S9, the derivatives are functions of \u2202B\u22121u /\u2202\u03bdt and \u2202B \u22121 u /\u2202\u03c9t, respectively, where B\u22121u is a dense (upper triangular) T \u00d7 T matrix. An efficient way to obtain these derivatives is to use the relation\n\u2202B\u22121u \u2202\u03bdt = \u2212B\u22121u \u2202Bu \u2202\u03bdt B\u22121u (S13)\nand similarly for \u2202B\u22121u /\u2202\u03c9t. Using this relation and Eqs. S8\u2013S9, we obtain the gradient estimates\n\u2202L \u2202\u03bdu,t \u2248 \u2212 1 S\nS\u2211\ns=1\ny [s] u,tx [s] u,t \u2212\n1\n\u03bdu,t , (S14)\n\u2202L \u2202\u03c9u,t \u2248 \u2212 1 S\nS\u2211\ns=1\ny [s] u,tx [s] u,t+1. (S15)\nThe second term on the right-hand side of Eq. S14 is the derivative of the entropy, Eq. S7, and\ny [s] u,1:T = (B > u ) \u22121 [ \u0393 [s] u,1:T \u2212\u03a0u [s] 1:T ] . (S16)\nThe values y[s]u,1:T can again be obtained in \u0398(T ) time by bringing B>u to the left-hand side and solving the corresponding bidiagonal linear system of equations.\nSampling in vocabulary space. In the above paragraph, we described an efficient strategy to obtain gradient estimates in only \u0398(T ) time. However, the gradient estimation scales quadratic in the vocabulary size L because all L2 elements of the positive count matrices n+t contribute to the gradients. In order speed up the optimization, we pretrain the model using a minibatch of size L\u2032 < L in vocabulary space as explained below. The computational complexity of a single training step in this setup scales as (L\u2032)2 rather than L2. After N \u2032tr = 5000 training steps with minibatch size L\u2032, we switch to the full batch size of L and train the model for another Ntr = 1000 steps.\nThe subsampling in vocabulary space works as follows. In each training step, we independently draw a set I of L\u2032 random distinct words and a set J of L\u2032 random distinct contexts from a uniform probability over the vocabulary. We then estimate the gradients of L with respect to only the variational parameters that correspond to words i \u2208 I and contexts j \u2208 J . This is possible because both the prior of our dynamic skip-gram model and the variational distribution factorize in the vocabulary space. The likelihood of the model, however, does not factorize. This affects only the definition of \u0393[s]uik,t in Eq. S12. We replace \u0393 [s] uik,t by an estimate \u0393[s]\u2032uik,t based on only the contexts j \u2208 J in the current minibatch,\n\u0393 [s] ui,t =\nL L\u2032 \u2211\nj\u2208J\n[ ( n+ij,t + n \u2212 ij,t ) \u03c3 ( \u2212u[s]>i,t v [s] j,t )\n\u2212 n\u2212ij,t ] v [s] j,t. (S17)\nHere, the prefactor L/L\u2032 restores the correct ratio between evidence and prior knowledge (Hoffman et al., 2013).\nEnforcing positive definiteness. We update the variational parameters using stochastic gradient ascent with the Adam optimizer (Kingma & Ba, 2014). The parameters \u03bdu,1:T are the eigenvalues of the matrix Bu, which is the Cholesky decomposition of the precision matrix of q. Therefore, \u03bdu,t has to be positive for all t \u2208 {1, . . . , T}. We use mirror ascent (Ben-Tal et al., 2001; Beck & Teboulle, 2003) to enforce \u03bdu,t > 0. Specifically, we update \u03bdt to a new value \u03bd\u2032t defined by\n\u03bd\u2032u,t = 1\n2 \u03bdu,td[\u03bdu,t] +\n\u221a( 1\n2 \u03bdu,td[\u03bdu,t]\n)2 + \u03bd2u,t (S18)\nwhere d[\u03bdu,t] is the step size obtained from the Adam optimizer. Eq. S18 can be derived from the general mirror ascent update rule \u03a6\u2032(\u03bd\u2032u,t) = \u03a6 \u2032(\u03bdu,t) + d[\u03bdu,t] with the\nmirror map \u03a6 : x 7\u2192 \u2212c1 log(x)+c2x2/2, where we set the parameters to c1 = \u03bdu,t and c2 = 1/\u03bdu,t for dimensional reasons. The update step in Eq. S18 increases (decreases) \u03bdu,t for positive (negative) d[\u03bdu,t], while always keeping its value positive.\nNatural basis. As a final remark, let us discuss an optional extension to the skip-gram smoothing algorithm that converges in less training steps. This extension only increases the efficiency of the algorithm. It does not change the underlying model or the choice of variational distribution. Observe that the prior of the dynamic skip-gram model connects only neighboring time-steps with each other. Therefore, the gradient of L with respect to \u00b5u,t depends only on the values of \u00b5u,t\u22121 and \u00b5u,t+1. A naive implementation of gradient ascent would thus require T\u22121 update steps until a change of \u00b5u,1 affects updates of \u00b5u,T .\nThis problem can be avoided with a change of basis from \u00b5u,1:T to new parameters \u03c1u,1:T ,\n\u00b5u,1:T = A\u03c1u,1:T (S19)\nwith an appropriately chosen invertible matrix A \u2208 RT\u00d7T . Derivatives of L with respect to \u03c1u,1:T are given by the chain rule, \u2202L/\u2202\u03c1u,1:T = (\u2202L/\u2202\u00b5u,1:T )A. A natural (but inefficient) choice for A is to stack the eigenvectors of the prior precision matrix \u03a0, see Eq. S11, into a matrix. The eigenvectors of \u03a0 are the Fourier modes of the Kalman filtering prior (with a regularization due to \u03c30). Therefore, there is a component \u03c1u,t that corresponds to the zero-mode of \u03a0, and this component learns an average word embedding over all times. Higher modes correspond to changes of the embedding vector over time. A single update to the zero immediately affects all elements of \u00b5u,1:T , and therefore changes the word embeddings at all time steps. Thus, information propagates quickly along the time dimension. The downside of this choice for A is that the transformation in Eq. S19 has complexity \u2126(T 2), which makes update steps slow.\nAs a compromise between efficiency and a natural basis, we propose to set A in Eq. S19 to the Cholesky decomposition of the prior covariance matrix \u03a0\u22121 \u2261 AA>. Thus, A is still a dense (upper triangular) matrix, and, in our experiments, updates to the last component \u03c1u,T affect all components of \u00b5u,1:T in an approximately equal amount. Since \u03a0 is tridiagonal, the inverse of A is bidiagonal, and Eq. S19 can be evaluated in \u0398(T ) time by solving A\u00b5u,1:T = \u03c1u,1:T for \u00b5u,1:T . This is the parameterization we used in our implementation of the skip-gram smoothing algorithm."}], "year": 2017, "references": [{"title": "Bayesian Neural Word Embedding", "authors": ["References Barkan", "Oren"], "venue": "In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence,", "year": 2017}, {"title": "Mirror Descent and Nonlinear Projected Subgradient Methods for Convex Optimization", "authors": ["Beck", "Amir", "Teboulle", "Marc"], "venue": "Operations Research Letters,", "year": 2003}, {"title": "The Ordered Subsets Mirror Descent Optimization Method with Applications to Tomography", "authors": ["Ben-Tal", "Aharon", "Margalit", "Tamar", "Nemirovski", "Arkadi"], "venue": "SIAM Journal on Optimization,", "year": 2001}, {"title": "A Neural Probabilistic Language Model", "authors": ["Bengio", "Yoshua", "Ducharme", "R\u00e9jean", "Vincent", "Pascal", "Jauvin", "Christian"], "venue": "Journal of Machine Learning Research,", "year": 2003}, {"title": "Dynamic Topic Models", "authors": ["Blei", "David M", "Lafferty", "John D"], "venue": "In Proceedings of the 23rd International Conference on Machine Learning,", "year": 2006}, {"title": "Variational Inference: A Review for Statisticians", "authors": ["Blei", "David M", "Kucukelbir", "Alp", "McAuliffe", "Jon D"], "venue": "arXiv preprint arXiv:1601.00670,", "year": 2016}, {"title": "Dynamic Poisson Factorization", "authors": ["Charlin", "Laurent", "Ranganath", "Rajesh", "McInerney", "James", "Blei", "David M"], "venue": "In Proceedings of the 9th ACM Conference on Recommender Systems,", "year": 2015}, {"title": "Semi-Supervised Vocabulary-Informed Learning", "authors": ["Fu", "Yanwei", "Sigal", "Leonid"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "year": 2016}, {"title": "A Collaborative Kalman Filter for Time-Evolving Dyadic Processes", "authors": ["Gultekin", "San", "Paisley", "John"], "venue": "In Proceedings of the 2nd International Conference on Data Mining, pp", "year": 2014}, {"title": "Diachronic word embeddings reveal statistical laws of semantic change", "authors": ["Hamilton", "William L", "Leskovec", "Jure", "Jurafsky", "Dan"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics,", "year": 2016}, {"title": "Stochastic Variational Inference", "authors": ["Hoffman", "Matthew D", "Blei", "David M", "Wang", "Chong", "Paisley", "John William"], "venue": "Journal of Machine Learning Research,", "year": 2013}, {"title": "Dynamic Compound Poisson Factorization", "authors": ["Jerfel", "Ghassen", "Basbug", "Mehmet E", "Engelhardt", "Barbara E"], "venue": "In Artificial Intelligence and Statistics,", "year": 2017}, {"title": "An Introduction to Variational Methods for Graphical Models", "authors": ["Jordan", "Michael I", "Ghahramani", "Zoubin", "Jaakkola", "Tommi S", "Saul", "Lawrence K"], "venue": "Machine learning,", "year": 1999}, {"title": "A New Approach to Linear Filtering and Prediction Problems", "authors": ["Kalman", "Rudolph Emil"], "venue": "Journal of Basic Engineering,", "year": 1960}, {"title": "The Inverse of Banded Matrices", "authors": ["K\u0131l\u0131\u00e7", "Emrah", "Stanica", "Pantelimon"], "venue": "Journal of Computational and Applied Mathematics,", "year": 2013}, {"title": "Temporal Analysis of Language Through Neural Language Models", "authors": ["Kim", "Yoon", "Chiu", "Yi-I", "Hanaki", "Kentaro", "Hegde", "Darshan", "Petrov", "Slav"], "venue": "In Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science,", "year": 2014}, {"title": "Auto-Encoding Variational Bayes", "authors": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "In Proceedings of the 2nd International Conference on Learning Representations (ICLR),", "year": 2014}, {"title": "Statistically Significant Detection of Linguistic Change", "authors": ["Kulkarni", "Vivek", "Al-Rfou", "Rami", "Perozzi", "Bryan", "Skiena", "Steven"], "venue": "In Proceedings of the 24th International Conference on World Wide Web,", "year": 2015}, {"title": "Neural Word Embedding as Implicit Matrix Factorization", "authors": ["Levy", "Omer", "Goldberg", "Yoav"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2014}, {"title": "Word Epoch Disambiguation: Finding how Words Change Over Time", "authors": ["Mihalcea", "Rada", "Nastase", "Vivi"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short PapersVolume", "year": 2012}, {"title": "Efficient Estimation of Word Representations in Vector Space", "authors": ["Mikolov", "Tomas", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey"], "venue": "arXiv preprint arXiv:1301.3781,", "year": 2013}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "authors": ["Mikolov", "Tomas", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg S", "Dean", "Jeff"], "venue": "In Advances in Neural Information Processing Systems", "year": 2013}, {"title": "Linguistic Regularities in Continuous Space Word Representations", "authors": ["Mikolov", "Tomas", "Yih", "Wen-tau", "Zweig", "Geoffrey"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies", "year": 2013}, {"title": "Learning Word Embeddings Efficiently with Noise-Contrastive Estimation", "authors": ["Mnih", "Andriy", "Kavukcuoglu", "Koray"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2013}, {"title": "Glove: Global Vectors for Word Representation", "authors": ["Pennington", "Jeffrey", "Socher", "Richard", "Manning", "Christopher D"], "venue": "In EMNLP,", "year": 2014}, {"title": "The Survival Filter: Joint Survival Analysis with a Latent Time Series", "authors": ["Ranganath", "Rajesh", "Perotte", "Adler J", "Elhadad", "No\u00e9mie", "Blei", "David M"], "venue": "In UAI, pp", "year": 2015}, {"title": "Visualizing Time-Dependent Data Using Dynamic t-SNE", "authors": ["Rauber", "Paulo E", "Falc\u00e3o", "Alexandre X", "Telea", "Alexandru C"], "venue": "In EuroVis 2016 - Short Papers,", "year": 2016}, {"title": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models", "authors": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir", "Wierstra", "Daan"], "venue": "In The 31st International Conference on Machine Learning (ICML),", "year": 2014}, {"title": "Dynamic Bernoulli Embeddings for Language Evolution", "authors": ["Rudolph", "Maja", "Blei", "David"], "venue": "arXiv preprint arXiv:1703.08052,", "year": 2017}, {"title": "Exponential Family Embeddings", "authors": ["Rudolph", "Maja", "Ruiz", "Francisco", "Mandt", "Stephan", "Blei", "David"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2016}, {"title": "Tracing Semantic Change with Latent Semantic Analysis", "authors": ["Sagi", "Eyal", "Kaufmann", "Stefan", "Clark", "Brady"], "venue": "Current Methods in Historical Semantics,", "year": 2011}, {"title": "A Hidden Markov Model for Collaborative Filtering", "authors": ["Sahoo", "Nachiketa", "Singh", "Param Vir", "Mukhopadhyay", "Tridas"], "venue": "MIS Quarterly,", "year": 2012}, {"title": "Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks", "authors": ["Salimans", "Tim", "Kingma", "Diederik P"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2016}, {"title": "Recursive Deep Models for Semantic Compositionality over a Sentiment Treebank", "authors": ["Socher", "Richard", "Perelygin", "Alex", "Wu", "Jean Y", "Chuang", "Jason", "Manning", "Christopher D", "Ng", "Andrew Y", "Potts", "Christopher"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Nat-", "year": 2013}, {"title": "On the Theory of the Brownian Motion", "authors": ["Uhlenbeck", "George E", "Ornstein", "Leonard S"], "venue": "Physical Review,", "year": 1930}, {"title": "Word Representations via Gaussian Embedding", "authors": ["Vilnis", "Luke", "McCallum", "Andrew"], "venue": "In Proceedings of the 2nd International Conference on Learning Representations (ICLR),", "year": 2014}, {"title": "Continuous time dynamic topic models", "authors": ["Wang", "Chong", "Blei", "David", "Heckerman"], "venue": "In Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence,", "year": 2008}, {"title": "An Introduction to the Kalman Filter", "authors": ["Welch", "Greg", "Bishop", "Gary"], "year": 1995}, {"title": "Mirror Descent and Nonlinear Projected Subgradient Methods for Convex Optimization", "authors": ["References Beck", "Amir", "Teboulle", "Marc"], "venue": "Operations Research Letters,", "year": 2003}, {"title": "The Ordered Subsets Mirror Descent Optimization Method with Applications to Tomography", "authors": ["Ben-Tal", "Aharon", "Margalit", "Tamar", "Nemirovski", "Arkadi"], "venue": "SIAM Journal on Optimization,", "year": 2001}, {"title": "Dynamic Topic Models", "authors": ["Blei", "David M", "Lafferty", "John D"], "venue": "In Proceedings of the 23rd International Conference on Machine Learning,", "year": 2006}, {"title": "Stochastic Variational Inference", "authors": ["Hoffman", "Matthew D", "Blei", "David M", "Wang", "Chong", "Paisley", "John William"], "venue": "Journal of Machine Learning Research,", "year": 2013}, {"title": "Temporal Analysis of Language Through Neural Language Models", "authors": ["Kim", "Yoon", "Chiu", "Yi-I", "Hanaki", "Kentaro", "Hegde", "Darshan", "Petrov", "Slav"], "venue": "In Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science,", "year": 2014}, {"title": "Adam: A Method for Stochastic Optimization", "authors": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "year": 2014}, {"title": "Auto-Encoding Variational Bayes", "authors": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "In Proceedings of the 2nd International Conference on Learning Representations (ICLR),", "year": 2014}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "authors": ["Mikolov", "Tomas", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg S", "Dean", "Jeff"], "venue": "In Advances in Neural Information Processing Systems", "year": 2013}, {"title": "Visualizing Time-Dependent Data Using Dynamic t-SNE", "authors": ["Rauber", "Paulo E", "Falc\u00e3o", "Alexandre X", "Telea", "Alexandru C"], "venue": "In EuroVis 2016 - Short Papers,", "year": 2016}, {"title": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models", "authors": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir", "Wierstra", "Daan"], "venue": "In The 31st International Conference on Machine Learning (ICML),", "year": 2014}], "id": "SP:b2535d2e7629f37571046f9abcb91feeced3b3c2", "authors": [{"name": "Robert Bamler", "affiliations": []}, {"name": "Stephan Mandt", "affiliations": []}], "abstractText": "We present a probabilistic language model for time-stamped text data which tracks the semantic evolution of individual words over time. The model represents words and contexts by latent trajectories in an embedding space. At each moment in time, the embedding vectors are inferred from a probabilistic version of word2vec (Mikolov et al., 2013b). These embedding vectors are connected in time through a latent diffusion process. We describe two scalable variational inference algorithms\u2014skipgram smoothing and skip-gram filtering\u2014that allow us to train the model jointly over all times; thus learning on all data while simultaneously allowing word and context vectors to drift. Experimental results on three different corpora demonstrate that our dynamic model infers word embedding trajectories that are more interpretable and lead to higher predictive likelihoods than competing methods that are based on static models trained separately on time slices.", "title": "Dynamic Word Embeddings"}