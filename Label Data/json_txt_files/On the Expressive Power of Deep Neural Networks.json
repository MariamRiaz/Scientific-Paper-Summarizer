{"sections": [{"text": "(1) The complexity of the computed function grows exponentially with depth. We design measures of expressivity that capture the non-linearity of the computed function. Due to how the network transforms its input, these measures grow exponentially with depth.\n(2) All weights are not equal (initial layers matter more). We find that trained networks are far more sensitive to their lower (initial) layer weights: they are much less robust to noise in these layer weights, and also perform better when these weights are optimized well.\n(3) Trajectory Regularization works like Batch Normalization. We find that batch norm stabilizes the learnt representation, and based on this propose a new regularization scheme, trajectory regularization."}, {"heading": "1. Introduction", "text": "Deep neural networks have proved astoundingly effective at a wide range of empirical tasks, from image classification (Krizhevsky et al., 2012) to playing Go (Silver et al., 2016), and even modeling human learning (Piech et al., 2015).\n1Cornell University 2Google Brain 3Stanford University. Correspondence to: Maithra Raghu <maithrar@gmail.com>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nDespite these successes, understanding of how and why neural network architectures achieve their empirical successes is still lacking. This includes even the fundamental question of neural network expressivity, how the architectural properties of a neural network (depth, width, layer type) affect the resulting functions it can compute, and its ensuing performance.\nThis is a foundational question, and there is a rich history of prior work addressing expressivity in neural networks. However, it has been challenging to derive conclusions that provide both theoretical generality with respect to choices of architecture as well as meaningful insights into practical performance.\nIndeed, the very first results on this question take a highly theoretical approach, from using functional analysis to show universal approximation results (Hornik et al., 1989; Cybenko, 1989), to analysing expressivity via comparisons to boolean circuits (Maass et al., 1994) and studying network VC dimension (Bartlett et al., 1998). While these results provided theoretically general conclusions, the shallow networks they studied are very different from the deep models that have proven so successful in recent years.\nIn response, several recent papers have focused on understanding the benefits of depth for neural networks (Pascanu et al., 2013; Montufar et al., 2014; Eldan and Shamir, 2015; Telgarsky, 2015; Martens et al., 2013; Bianchini and Scarselli, 2014). These results are compelling and take modern architectural changes into account, but they only show that a specific choice of weights for a deeper network results in inapproximability by a shallow (typically one or two hidden layers) network.\nIn particular, the goal of this new line of work has been to establish lower bounds \u2014 showing separations between shallow and deep networks \u2014 and as such they are based on hand-coded constructions of specific network weights. Even if the weight values used in these constructions are robust to small perturbations (as in (Pascanu et al., 2013; Montufar et al., 2014)), the functions that arise from these constructions tend toward extremal properties by design, and there is no evidence that a network trained on data ever resembles such a function.\nThis has meant that a set of fundamental questions about\nar X\niv :1\n60 6.\n05 33\n6v 6\n[ st\nat .M\nL ]\n1 8\nJu n\n20 17\nneural network expressivity has remained largely unanswered. First, we lack a good understanding of the \u201ctypical\u201d case rather than the worst case in these bounds for deep networks, and consequently have no way to evaluate whether the hand-coded extremal constructions provide a reflection of the complexity encountered in more standard settings. Second, we lack an understanding of upper bounds to match the lower bounds produced by this prior work; do the constructions used to date place us near the limit of the expressive power of neural networks, or are there still large gaps? Finally, if we had an understanding of these two issues, we might begin to draw connections between network expressivity and observed performance.\nOur contributions: Measures of Expressivity and their Applications In this paper, we address this set of challenges by defining and analyzing an interrelated set of measures of expressivity for neural networks; our framework applies to a wide range of standard architectures, independent of specific weight choices. We begin our analysis at the start of training, after random initialization, and later derive insights connecting network expressivity and performance.\nOur first measure of expressivity is based on the notion of an activation pattern: in a network where the units compute functions based on discrete thresholds, we can ask which units are above or below their thresholds (i.e. which units are \u201cactive\u201d and which are not). For the range of standard architectures that we consider, the network is essentially computing a linear function once we fix the activation pattern; thus, counting the number of possible activation patterns provides a concrete way of measuring the complexity beyond linearity that the network provides. We give an upper bound on the number of possible activation patterns, over any setting of the weights. This bound is tight as it matches the hand-constructed lower bounds of earlier work (Pascanu et al., 2013; Montufar et al., 2014).\nKey to our analysis is the notion of a transition, in which changing an input x to a nearby input x + \u03b4 changes the activation pattern. We study the behavior of transitions as we pass the input along a one-dimensional parametrized trajectory x(t). Our central finding is that the trajectory length grows exponentially in the depth of the network.\nTrajectory length serves as a unifying notion in our measures of expressivity, and it leads to insights into the behavior of trained networks. Specifically, we find that the exponential growth in trajectory length as a function of depth implies that small adjustments in parameters lower in the network induce larger changes than comparable adjustments higher in the network. We demonstrate this phenomenon through experiments on MNIST and CIFAR-10, where the network displays much less robustness to noise\nin the lower layers, and better performance when they are trained well. We also explore the effects of regularization methods on trajectory length as the network trains and propose a less computationally intensive method of regularization, trajectory regularization, that offers the same performance as batch normalization.\nThe contributions of this paper are thus:\n(1) Measures of expressivity: We propose easily computable measures of neural network expressivity that capture the expressive power inherent in different neural network architectures, independent of specific weight settings.\n(2) Exponential trajectories: We find an exponential depth dependence displayed by these measures, through a unifying analysis in which we study how the network transforms its input by measuring trajectory length\n(3) All weights are not equal (the lower layers matter more): We show how these results on trajectory length suggest that optimizing weights in lower layers of the network is particularly important.\n(4) Trajectory Regularization Based on understanding the effect of batch norm on trajectory length, we propose a new method of regularization, trajectory regularization, that offers the same advantages as batch norm, and is computationally more efficient.\nIn prior work (Poole et al., 2016), we studied the propagation of Riemannian curvature through random networks by developing a mean field theory approach. Here, we take an approach grounded in computational geometry, presenting measures with a combinatorial flavor and explore the consequences during and after training."}, {"heading": "2. Measures of Expressivity", "text": "Given a neural network of a certain architecture A (some depth, width, layer types), we have an associated function, FA(x;W ), where x is an input and W represents all the parameters of the network. Our goal is to understand how the behavior of FA(x;W ) changes asA changes, for values of W that we might encounter during training, and across inputs x.\nThe first major difficulty comes from the high dimensionality of the input. Precisely quantifying the properties of FA(x;W ) over the entire input space is intractable. As a tractable alternative, we study simple one dimensional trajectories through input space. More formally:\nDefinition: Given two points, x0, x1 \u2208 Rm, we say x(t) is a trajectory (between x0 and x1) if x(t) is a curve\nparametrized by a scalar t \u2208 [0, 1], with x(0) = x0 and x(1) = x1.\nSimple examples of a trajectory would be a line (x(t) = tx1 + (1 \u2212 t)x0) or a circular arc (x(t) = cos(\u03c0t/2)x0 + sin(\u03c0t/2)x1), but in general x(t) may be more complicated, and potentially not expressible in closed form.\nArmed with this notion of trajectories, we can begin to define measures of expressivity of a network FA(x;W ) over trajectories x(t)."}, {"heading": "2.1. Neuron Transitions and Activation Patterns", "text": "In (Montufar et al., 2014) the notion of a \u201clinear region\u201d is introduced. Given a neural network with piecewise linear activations (such as ReLU or hard tanh), the function it computes is also piecewise linear, a consequence of the fact that composing piecewise linear functions results in a piecewise linear function. So one way to measure the \u201cexpressive power\u201d of different architectures A is to count the number of linear pieces (regions), which determines how nonlinear the function is.\nIn fact, a change in linear region is caused by a neuron transition in the output layer. More precisely:\nDefinition For fixed W , we say a neuron with piecewise linear region transitions between inputs x, x+ \u03b4 if its activation function switches linear region between x and x+\u03b4.\nSo a ReLU transition would be given by a neuron switching from off to on (or vice versa) and for hard tanh by switching between saturation at \u22121 to its linear middle region to saturation at 1. For any generic trajectory x(t), we can thus define T (FA(x(t);W )) to be the number of transitions undergone by output neurons (i.e. the number of linear regions) as we sweep the input x(t). Instead of just concentrating on the output neurons however, we can look at this pattern over the entire network. We call this an activation patten:\nDefinition We can defineAP(FA(x;W )) to be the activation pattern \u2013 a string of form {0, 1}num neurons (for ReLUs) and {\u22121, 0, 1}num neurons (for hard tanh) of the network encoding the linear region of the activation function of every neuron, for an input x and weights W .\nOverloading notation slightly, we can also define (similarly to transitions) A(FA(x(t);W )) as the number of distinct activation patterns as we sweep x along x(t). As each distinct activation pattern corresponds to a different linear function of the input, this combinatorial measure captures how much more expressive A is over a simple linear mapping.\nReturning to Montufar et al, they provide a construction i.e. a specific set of weights W0, that results in an exponen-\ntial increase of linear regions with the depth of the architectures. They also appeal to Zaslavsky\u2019s theorem (Stanley, 2011) from the theory of hyperplane arrangements to show that a shallow network, i.e. one hidden layer, with the same number of parameters as a deep network, has a much smaller number of linear regions than the number achieved by their choice of weights W0 for the deep network.\nMore formally, letting A1 be a fully connected network with one hidden layer, and Al a fully connected network with the same number of parameters, but l hidden layers, they show\n\u2200WT (FA1([0, 1];W )) < T (FA1([0, 1];W0) (*)\nWe derive a much more general result by considering the \u2018global\u2019 activation patterns over the entire input space, and prove that for any fully connected network, with any number of hidden layers, we can upper bound the number of linear regions it can achieve, over all possible weight settings W . This upper bound is asymptotically tight, matched by the construction given in (Montufar et al., 2014). Our result can be written formally as:\nTheorem 1. (Tight) Upper Bound for Number of Activation Patterns Let A(n,k) denote a fully connected network with n hidden layers of width k, and inputs in Rm. Then the number of activation patterns A(FAn,k(Rm;W ) is upper bounded byO(kmn) for ReLU activations, andO((2k)mn) for hard tanh.\nFrom this we can derive a chain of inequalities. Firstly, from the theorem above we find an upper bound of A(FAn,k(Rm;W )) over all W , i.e.\n\u2200W A(FA(n,k))(R m;W ) \u2264 U(n, k,m).\nNext, suppose we haveN neurons in total. Then we want to compare (for wlog ReLUs), quantities like U(n\u2032, N/n\u2032,m) for different n\u2032.\nBut U(n\u2032, N/n\u2032,m) = O((N/n\u2032)mn \u2032 ), and so, noting that the maxima of ( a x )mx (for a > e) is x = a/e, we get, (for n, k > e), in comparison to (*),\nU(1, N,m) < U(2, N\n2 ,m) < \u00b7 \u00b7 \u00b7\n\u00b7 \u00b7 \u00b7 < U(n\u2212 1, N n\u2212 1 ,m) < U(n, k,m)\nWe prove this via an inductive proof on regions in a hyperplane arrangement. The proof can be found in the Appendix. As noted in the introduction, this result differs from earlier lower-bound constructions in that it is an upper bound that applies to all possible sets of weights. Via our analysis, we also prove\n-1 0 1 x0\n-1\n0 1 x 1 Layer 0\n-1 0 1 x0\n-1\n0\n1 Layer 1\n-1 0 1 x0\n-1\n0\n1 Layer 2\nFigure 1. Deep networks with piecewise linear activations subdivide input space into convex polytopes. We take a three hidden layer ReLU network, with input x \u2208 R2, and four units in each layer. The left pane shows activations for the first layer only. As the input is in R2, neurons in the first hidden layer have an associated line in R2, depicting their activation boundary. The left pane thus has four such lines. For the second hidden layer each neuron again has a line in input space corresponding to on/off, but this line is different for each region described by the first layer activation pattern. So in the centre pane, which shows activation boundary lines corresponding to second hidden layer neurons in green (and first hidden layer in black), we can see the green lines \u2018bend\u2019 at the boundaries. (The reason for this bending becomes apparent through the proof of Theorem 2.) Finally, the right pane adds the on/off boundaries for neurons in the third hidden layer, in purple. These lines can bend at both black and green boundaries, as the image shows. This final set of convex polytopes corresponds to all activation patterns for this network (with its current set of weights) over the unit square, with each polytope representing a different linear function.\nTheorem 2. Regions in Input Space Given the corresponding function of a neural network FA(Rm;W ) with ReLU or hard tanh activations, the input space is partitioned into convex polytopes, with FA(Rm;W ) corresponding to a different linear function on each region.\nThis result is of independent interest for optimization \u2013 a linear function over a convex polytope results in a well behaved loss function and an easy optimization problem. Understanding the density of these regions during the training process would likely shed light on properties of the loss surface, and improved optimization methods. A picture of a network\u2019s regions is shown in Figure 1."}, {"heading": "2.1.1. EMPIRICALLY COUNTING TRANSITIONS", "text": "We empirically tested the growth of the number of activations and transitions as we varied x along x(t) to understand their behavior. We found that for bounded non linearities, especially tanh and hard-tanh, not only do we observe exponential growth with depth (as hinted at by the upper bound) but that the scale of parameter initialization also affects the observations (Figure 2). We also experimented with sweeping the weights W of a layer through a trajectory W (t), and counting the different labellings output by the network. This \u2018dichotomies\u2019 measure is discussed further in the Appendix, and also exhibits the same growth properties, Figure 14."}, {"heading": "2.2. Trajectory Length", "text": "In fact, there turns out to be a reason for the exponential growth with depth, and the sensitivity to initialization scale. Returning to our definition of trajectory, we can define an immediately related quantity, trajectory length\nDefinition: Given a trajectory, x(t), we define its length, l(x(t)), to be the standard arc length:\nl(x(t)) = \u222b t \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223dx(t)dt \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 dt\nIntuitively, the arc length breaks x(t) up into infinitesimal intervals and sums together the Euclidean length of these intervals.\nIf we letA(n,k) denote, as before, fully connected networks with n hidden layers each of width k, and initializing with weights \u223c N (0, \u03c32w/k) (accounting for input scaling as typical), and biases \u223c N (0, \u03c32b ), we find that: Theorem 3. Bound on Growth of Trajectory Length Let FA(x\n\u2032,W ) be a ReLU or hard tanh random neural network and x(t) a one dimensional trajectory with x(t+ \u03b4) having a non trival perpendicular component to x(t) for all t, \u03b4\n(i.e, not a line). Then defining z(d)(x(t)) = z(d)(t) to be the image of the trajectory in layer d of the network, we have\n(a)\nE [ l(z(d)(t)) ] \u2265 O ( \u03c3w \u221a k\u221a\nk + 1\n)d l(x(t))\nfor ReLUs\n(b)\nE [ l(z(d)(t)) ] \u2265 O  \u03c3w\u221ak\u221a \u03c32w + \u03c3 2 b + k \u221a \u03c32w + \u03c3 2 b d l(x(t)) for hard tanh\nThat is, l(x(t) grows exponentially with the depth of the network, but the width only appears as a base (of the exponent). This bound is in fact tight in the limits of large \u03c3w and k.\nA schematic image depicting this can be seen in Figure 3 and the proof can be found in the Appendix. A rough outline is as follows: we look at the expected growth of the difference between a point z(d)(t) on the curve and a small perturbation z(d)(t+dt), from layer d to layer d+1. Denoting this quantity\n\u2223\u2223\u2223\u2223\u03b4z(d)(t)\u2223\u2223\u2223\u2223, we derive a recurrence relating \u2223\u2223\u2223\u2223\u03b4z(d+1)(t)\u2223\u2223\u2223\u2223 and \u2223\u2223\u2223\u2223\u03b4z(d)(t)\u2223\u2223\u2223\u2223 which can be composed to give the desired growth rate.\nThe analysis is complicated by the statistical dependence on the image of the input z(d+1)(t). So we instead form a recursion by looking at the component of the difference perpendicular to the image of the input in that layer, i.e.\n\u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d+1)\u22a5 (t)\u2223\u2223\u2223\u2223\u2223\u2223, which results in the condition on x(t) in the statement.\nIn Figures 4, 12, we see the growth of an input trajectory for ReLU networks on CIFAR-10 and MNIST. The CIFAR10 network is convolutional but we observe that these layers also result in similar rates of trajectory length increases to the fully connected layers. We also see, as would be expected, that pooling layers act to reduce the trajectory length. We discuss upper bounds in the Appendix.\nFor the hard tanh case (and more generally any bounded non-linearity), we can formally prove the relation of trajectory length and transitions under an assumption: assume that while we sweep x(t) all neurons are saturated unless transitioning saturation endpoints, which happens very rapidly. (This is the case for e.g. large initialization scales). Then we have:\nTheorem 4. Transitions proportional to trajectory length Let FAn,k be a hard tanh network with n hidden layers each of width k. And let\ng(k, \u03c3w, \u03c3b, n) = O  \u221ak\u221a 1 +\n\u03c32b \u03c32w\nn\nThen T (FAn,k(x(t);W ) = O(g(k, \u03c3w, \u03c3b, n)) for W initialized with weight and bias scales \u03c3w, \u03c3b.\nNote that the expression for g(k, \u03c3w, \u03c3b, n) is exactly the expression given by Theorem 3 when \u03c3w is very large and dominates \u03c3b. We can also verify this experimentally in settings where the simpilfying assumption does not hold, as in Figure 5."}, {"heading": "3. Insights from Network Expressivity", "text": "Here we explore the insights gained from applying our measurements of expressivity, particularly trajectory length, to understand network performance. We examine the connection of expressivity and stability, and inspired by this, propose a new method of regularization, trajectory\nregularization that offers the same advantages as the more computationally intensive batch normalization."}, {"heading": "3.1. Expressivity and Network Stability", "text": "The analysis of network expressivity offers interesting takeaways related to the parameter and functional stability of a network. From the proof of Theorem 3, we saw that a perturbation to the input would grow exponentially in the depth of the network. It is easy to see that this analysis is not limited to the input layer, but can be applied to any layer. In this form, it would say\nA perturbation at a layer grows exponentially in the remaining depth after that layer.\nThis means that perturbations to weights in lower layers should be more costly than perturbations in the upper layers, due to exponentially increasing magnitude of noise, and result in a much larger drop of accuracy. Figure 6, in which we train a conv network on CIFAR-10 and add noise of varying magnitudes to exactly one layer, shows exactly this.\nWe also find that the converse (in some sense) holds: after initializing a network, we trained a single layer at different depths in the network and found monotonically increasing performance as layers lower in the network were trained. This is shown in Figure 7 and Figure 17 in the Appendix."}, {"heading": "3.2. Trajectory Length and Regularization: The Effect of Batch Normalization", "text": "Expressivity measures, especially trajectory length, can also be used to better understand the effect of regulariza-\ntion. One regularization technique that has been extremely successful for training neural networks is Batch Normalization (Ioffe and Szegedy, 2015).\nBy taking measures of trajectories during training we find that without batch norm, trajectory length tends to increase during training, as shown in Figures 8 and Figure 18 in the Appendix. In these experiments, two networks were initialized with \u03c32w = 2 and trained to high test accuracy on CIFAR10 and MNIST. We see that in both cases, trajectory length increases as training progresses.\nA surprising observation is \u03c32w = 2 is not in the exponential growth increase regime at initialization for the CIFAR10\narchitecture (Figure 8 at Step 0.). But note that even with a smaller weight initialization, weight norms increase during training, shown in Figure 9, pushing typically initialized networks into the exponential growth regime.\nWhile the initial growth of trajectory length enables greater functional expressivity, large trajectory growth in the learnt representation results in an unstable representation, witnessed in Figure 6. In Figure 10 we train another conv net on CIFAR10, but this time with batch normalization. We see that the batch norm layers reduce trajectory length, helping stability."}, {"heading": "3.3. Trajectory Regularization", "text": "Motivated by the fact that batch normalization decreases trajectory length and hence helps stability and generalization, we consider directly regularizing on trajectory length: we replace every batch norm layer used in the conv net in Figure 10 with a trajectory regularization layer. This layer adds to the loss \u03bb(current length/orig length), and\nthen scales the outgoing activations by \u03bb, where \u03bb is a parameter to be learnt. In implementation, we typically scale the additional loss above with a constant (0.01) to reduce magnitude in comparison to classification loss. Our results, Figure 11 show that both trajectory regularization and batch norm perform comparably, and considerably better than not using batch norm. One advantage of using Trajectory Regularization is that we don\u2019t require different computations to be performed for train and test, enabling more efficient implementation."}, {"heading": "4. Discussion", "text": "Characterizing the expressiveness of neural networks, and understanding how expressiveness varies with parameters of the architecture, has been a challenging problem due to the difficulty in identifying meaningful notions of expressivity and in linking their analysis to implications for these networks in practice. In this paper we have presented an interrelated set of expressivity measures; we have shown tight exponential bounds on the growth of these measures in the depth of the networks, and we have offered a unifying view of the analysis through the notion of trajectory length. Our analysis of trajectories provides insights for the performance of trained networks as well, suggesting that networks in practice may be more sensitive to small perturbations in weights at lower layers. We also used this to explore the empirical success of batch norm, and developed a new regularization method \u2013 trajectory regularization.\nThis work raises many interesting directions for future work. At a general level, continuing the theme of \u2018principled deep understanding\u2019, it would be interesting to link\nmeasures of expressivity to other properties of neural network performance. There is also a natural connection between adversarial examples, (Goodfellow et al., 2014), and trajectory length: adversarial perturbations are only a small distance away in input space, but result in a large change in classification (the output layer). Understanding how trajectories between the original input and an adversarial perturbation grow might provide insights into this phenomenon. Another direction, partially explored in this paper, is regularizing based on trajectory length. A very simple version of this was presented, but further performance gains might be achieved through more sophisticated use of this method."}, {"heading": "Acknowledgements", "text": "We thank Samy Bengio, Ian Goodfellow, Laurent Dinh, and Quoc Le for extremely helpful discussion."}, {"heading": "Appendix", "text": "Here we include the full proofs from sections in the paper."}, {"heading": "A. Proofs and additional results from Section 2.1", "text": ""}, {"heading": "Proof of Theorem 2", "text": "Proof. We show inductively that FA(x;W ) partitions the input space into convex polytopes via hyperplanes. Consider the image of the input space under the first hidden layer. Each neuron v(1)i defines hyperplane(s) on the input space: letting W (0)i be the ith row of W (0), b(0)i the bias, we have the hyperplane W (0) i x + bi = 0 for a ReLU and hyperplanes W (0) i x + bi = \u00b11 for a hard-tanh. Considering all such hyperplanes over neurons in the first layer, we get a hyperplane arrangement in the input space, each polytope corresponding to a specific activation pattern in the first hidden layer.\nNow, assume we have partitioned our input space into convex polytopes with hyperplanes from layers \u2264 d \u2212 1. Consider v (d) i and a specific polytope Ri. Then the activation pattern on layers \u2264 d \u2212 1 is constant on Ri, and so the input to v (d) i\non Ri is a linear function of the inputs \u2211 j \u03bbjxj + b and some constant term, comprising of the bias and the output of saturated units. Setting this expression to zero (for ReLUs) or to \u00b11 (for hard-tanh) again gives a hyperplane equation, but this time, the equation is only valid in Ri (as we get a different linear function of the inputs in a different region.) So the defined hyperplane(s) either partition Ri (if they intersect Ri) or the output pattern of v (d) i is also constant on Ri. The theorem then follows.\nThis implies that any one dimensional trajectory x(t), that does not \u2018double back\u2019 on itself (i.e. reenter a polytope it has previously passed through), will not repeat activation patterns. In particular, after seeing a transition (crossing a hyperplane to a different region in input space) we will never return to the region we left. A simple example of such a trajectory is a straight line:\nCorollary 1. Transitions and Output Patterns in an Affine Trajectory For any affine one dimensional trajectory x(t) = x0 + t(x1 \u2212 x0) input into a neural network FW , we partition R 3 t into intervals every time a neuron transitions. Every interval has a unique network activation pattern on FW .\nGeneralizing from a one dimensional trajectory, we can ask how many regions are achieved over the entire input \u2013 i.e. how many distinct activation patterns are seen? We first prove a bound on the number of regions formed by k hyperplanes in Rm (in a purely elementary fashion, unlike the proof presented in (Stanley, 2011))\nTheorem 5. Upper Bound on Regions in a Hyperplane Arrangement Suppose we have k hyperplanes in Rm - i.e. k equations of form \u03b1ix = \u03b2i. for \u03b1i \u2208 Rm, \u03b2i \u2208 R. Let the number of regions (connected open sets bounded on some sides by the hyperplanes) be r(k,m). Then\nr(k,m) \u2264 m\u2211 i=0 ( k i )"}, {"heading": "Proof of Theorem 5", "text": "Proof. Let the hyperplane arrangement be denoted H, and let H \u2208 H be one specific hyperplane. Then the number of regions in H is precisely the number of regions in H \u2212 H plus the number of regions in H \u2229 H . (This follows from the fact that H subdivides into two regions exactly all of the regions inH \u2229H , and does not affect any of the other regions.)\nIn particular, we have the recursive formula\nr(k,m) = r(k \u2212 1,m) + r(k \u2212 1,m\u2212 1)\nWe now induct on k + m to assert the claim. The base cases of r(1, 0) = r(0, 1) = 1 are trivial, and assuming the claim\nfor \u2264 k +m\u2212 1 as the induction hypothesis, we have\nr(k \u2212 1,m) + r(k \u2212 1,m\u2212 1) \u2264 m\u2211 i=0 ( k \u2212 1 i ) + m\u22121\u2211 i=0 ( k \u2212 1 i )\n\u2264 ( k \u2212 1\n0\n) + d\u22121\u2211 i=0 ( k \u2212 1 i ) + ( k \u2212 1 i+ 1 )\n\u2264 ( k\n0\n) + m\u22121\u2211 i=0 ( k i+ 1 ) where the last equality follows by the well known identity(\na\nb\n) + ( a\nb+ 1\n) = ( a+ 1\nb+ 1\n)\nThis concludes the proof.\nWith this result, we can easily prove Theorem 1 as follows:"}, {"heading": "Proof of Theorem 1", "text": "Proof. First consider the ReLU case. Each neuron has one hyperplane associated with it, and so by Theorem 5, the first hidden layer divides up the inputs space into r(k,m) regions, with r(k,m) \u2264 O(km).\nNow consider the second hidden layer. For every region in the first hidden layer, there is a different activation pattern in the first layer, and so (as described in the proof of Theorem 2) a different hyperplane arrangement of k hyperplanes in an m dimensional space, contributing at most r(k,m) regions.\nIn particular, the total number of regions in input space as a result of the first and second hidden layers is \u2264 r(k,m) \u2217 r(k,m) \u2264 O(k2m). Continuing in this way for each of the n hidden layers gives the O(kmn) bound.\nA very similar method works for hard tanh, but here each neuron produces two hyperplanes, resulting in a bound of O((2k)mn)."}, {"heading": "B. Proofs and additional results from Section 2.2", "text": ""}, {"heading": "Proof of Theorem 3", "text": ""}, {"heading": "B.1. Notation and Preliminary Results", "text": "Difference of points on trajectory Given x(t) = x, x(t+ dt) = x+ \u03b4x in the trajectory, let \u03b4z(d) = z(d)(x+ \u03b4x)\u2212 z(d)(x)\nParallel and Perpendicular Components: Given vectors x, y, we can write y = y\u22a5 + y\u2016 where y\u22a5 is the component of y perpendicular to x, and y\u2016 is the component parallel to x. (Strictly speaking, these components should also have a subscript x, but we suppress it as the direction with respect to which parallel and perpendicular components are being taken will be explicitly stated.)\nThis notation can also be used with a matrix W , see Lemma 1.\nBefore stating and proving the main theorem, we need a few preliminary results.\nLemma 1. Matrix Decomposition Let x, y \u2208 Rk be fixed non-zero vectors, and let W be a (full rank) matrix. Then, we can write\nW = \u2016W\u2016 + \u2016W\u22a5 + \u22a5W\u2016 + \u22a5W\u22a5\nsuch that\n\u2016W\u22a5x = 0 \u22a5W\u22a5x = 0 yT\u22a5W\u2016 = 0 y T\u22a5W\u22a5 = 0\ni.e. the row space of W is decomposed to perpendicular and parallel components with respect to x (subscript on right), and the column space is decomposed to perpendicular and parallel components of y (superscript on left).\nProof. Let V,U be rotations such that V x = (||x|| , 0..., 0)T and Uy = (||y|| , 0...0)T . Now let W\u0303 = UWV T , and let W\u0303 = \u2016W\u0303\u2016 + \u2016W\u0303\u22a5 + \u22a5W\u0303\u2016 +\n\u22a5W\u0303\u22a5, with \u2016W\u0303\u2016 having non-zero term exactly W\u030311, \u2016W\u0303\u22a5 having non-zero entries exactly W\u03031i for 2 \u2264 i \u2264 k. Finally, we let \u22a5W\u0303\u2016 have non-zero entries exactly W\u0303i1, with 2 \u2264 i \u2264 k and \u22a5W\u0303\u22a5 have the remaining entries non-zero.\nIf we define x\u0303 = V x and y\u0303 = Uy, then we see that\n\u2016W\u0303\u22a5x\u0303 = 0 \u22a5W\u0303\u22a5x\u0303 = 0 y\u0303T\u22a5W\u0303\u2016 = 0 y\u0303 T\u22a5W\u0303\u22a5 = 0\nas x\u0303, y\u0303 have only one non-zero term, which does not correspond to a non-zero term in the components of W\u0303 in the equations.\nThen, defining \u2016W\u2016 = UT \u2016W\u0303\u2016V , and the other components analogously, we get equations of the form\n\u2016W\u22a5x = U T \u2016W\u0303\u22a5V x = U T \u2016W\u0303\u22a5x\u0303 = 0\nObservation 1. Given W,x as before, and considering W\u2016, W\u22a5 with respect to x (wlog a unit vector) we can express them directly in terms of W as follows: Letting W (i) be the ith row of W , we have\nW\u2016 = ((W (0))T \u00b7 x)x\n... ((W (k))T \u00b7 x)x  i.e. the projection of each row in the direction of x. And of course\nW\u22a5 = W \u2212W\u2016\nThe motivation to consider such a decomposition of W is for the resulting independence between different components, as shown in the following lemma.\nLemma 2. Independence of Projections Let x be a given vector (wlog of unit norm.) If W is a random matrix with Wij \u223c N (0, \u03c32), then W\u2016 and W\u22a5 with respect to x are independent random variables.\nProof. There are two possible proof methods:\n(a) We use the rotational invariance of random Gaussian matrices, i.e. if W is a Gaussian matrix, iid entries N (0, \u03c32), and R is a rotation, then RW is also iid Gaussian, entries N (0, \u03c32). (This follows easily from affine transformation rules for multivariate Gaussians.)\nLet V be a rotation as in Lemma 1. Then W\u0303 = WV T is also iid Gaussian, and furthermore, W\u0303\u2016 and W\u0303\u22a5 partition the entries of W\u0303 , so are evidently independent. But then W\u2016 = W\u0303\u2016V T and W\u22a5 = W\u0303\u22a5V T are also independent.\n(b) From the observation note that W\u2016 and W\u22a5 have a centered multivariate joint Gaussian distribution (both consist of linear combinations of the entries Wij in W .) So it suffices to show that W\u2016 and W\u22a5 have covariance 0. Because both are centered Gaussians, this is equivalent to showing E(< W\u2016,W\u22a5 >) = 0. We have that\nE(< W\u2016,W\u22a5 >) = E(W\u2016WT\u22a5 ) = E(W\u2016WT )\u2212 E(W\u2016WT\u2016 )\nAs any two rows of W are independent, we see from the observation that E(W\u2016WT ) is a diagonal matrix, with the ith diagonal entry just ((W (0))T \u00b7 x)2. But similarly, E(W\u2016WT\u2016 ) is also a diagonal matrix, with the same diagonal entries - so the claim follows.\nIn the following two lemmas, we use the rotational invariance of Gaussians as well as the chi distribution to prove results about the expected norm of a random Gaussian vector.\nLemma 3. Norm of a Gaussian vector Let X \u2208 Rk be a random Gaussian vector, with Xi iid, \u223c N (0, \u03c32). Then\nE [||X||] = \u03c3 \u221a 2 \u0393((k + 1)/2)\n\u0393(k/2)\nProof. We use the fact that if Y is a random Gaussian, and Yi \u223c N (0, 1) then ||Y || follows a chi distribution. This means that E(||X/\u03c3||) = \u221a 2\u0393((k + 1)/2)/\u0393(k/2), the mean of a chi distribution with k degrees of freedom, and the result follows by noting that the expectation in the lemma is \u03c3 multiplied by the above expectation.\nWe will find it useful to bound ratios of the Gamma function (as appear in Lemma 3) and so introduce the following inequality, from (Kershaw, 1983) that provides an extension of Gautschi\u2019s Inequality.\nTheorem 6. An Extension of Gautschi\u2019s Inequality For 0 < s < 1, we have\n( x+ s\n2\n)1\u2212s \u2264 \u0393(x+ 1)\n\u0393(x+ s) \u2264\n( x\u2212 1\n2 +\n( s+ 1\n4\n) 1 2 )1\u2212s\nWe now show:\nLemma 4. Norm of Projections Let W be a k by k random Gaussian matrix with iid entries \u223c N (0, \u03c32), and x, y two given vectors. Partition W into components as in Lemma 1 and let x\u22a5 be a nonzero vector perpendicular to x. Then\n(a)\nE [\u2223\u2223\u2223\u2223\u22a5W\u22a5x\u22a5\u2223\u2223\u2223\u2223] = ||x\u22a5||\u03c3\u221a2 \u0393(k/2)\n\u0393((k \u2212 1)/2 \u2265 ||x\u22a5||\u03c3\n\u221a 2\n( k\n2 \u2212 3 4 )1/2 (b) If 1A is an identity matrix with non-zeros diagonal entry i iff i \u2208 A \u2282 [k], and |A| > 2, then\nE [\u2223\u2223\u2223\u22231A\u22a5W\u22a5x\u22a5\u2223\u2223\u2223\u2223] \u2265 ||x\u22a5||\u03c3\u221a2 \u0393(|A|/2)\n\u0393((|A| \u2212 1)/2) \u2265 ||x\u22a5||\u03c3\n\u221a 2 ( |A| 2 \u2212 3 4 )1/2 Proof. (a) Let U, V, W\u0303 be as in Lemma 1. As U, V are rotations, W\u0303 is also iid Gaussian. Furthermore for any fixed W ,\nwith a\u0303 = V a, by taking inner products, and square-rooting, we see that \u2223\u2223\u2223\u2223\u2223\u2223W\u0303 a\u0303\u2223\u2223\u2223\u2223\u2223\u2223 = ||Wa||. So in particular\nE [\u2223\u2223\u2223\u2223\u22a5W\u22a5x\u22a5\u2223\u2223\u2223\u2223] = E [\u2223\u2223\u2223\u2223\u2223\u2223\u22a5W\u0303\u22a5x\u0303\u22a5\u2223\u2223\u2223\u2223\u2223\u2223]\nBut from the definition of non-zero entries of \u22a5W\u0303\u22a5, and the form of x\u0303\u22a5 (a zero entry in the first coordinate), it follows that \u22a5W\u0303\u22a5x\u0303\u22a5 has exactly k\u22121 non zero entries, each a centered Gaussian with variance (k\u22121)\u03c32 ||x\u22a5||2. By Lemma 3, the expected norm is as in the statement. We then apply Theorem 6 to get the lower bound.\n(b) First note we can view 1A\u22a5W\u22a5 = \u22a51AW\u22a5. (Projecting down to a random (as W is random) subspace of fixed size |A| = m and then making perpendicular commutes with making perpendicular and then projecting everything down to the subspace.)\nSo we can viewW as a randomm by k matrix, and for x, y as in Lemma 1 (with y projected down ontom dimensions), we can again define U, V as k by k and m by m rotation matrices respectively, and W\u0303 = UWV T , with analogous\nproperties to Lemma 1. Now we can finish as in part (a), except that \u22a5W\u0303\u22a5x\u0303 may have only m\u2212 1 entries, (depending on whether y is annihilated by projecting down by1A) each of variance (k \u2212 1)\u03c32 ||x\u22a5||2.\nLemma 5. Norm and Translation Let X be a centered multivariate Gaussian, with diagonal covariance matrix, and \u00b5 a constant vector.\nE(||X \u2212 \u00b5||) \u2265 E(||X||)\nProof. The inequality can be seen intuitively geometrically: as X has diagonal covariance matrix, the contours of the pdf of ||X|| are circular centered at 0, decreasing radially. However, the contours of the pdf of ||X \u2212 \u00b5|| are shifted to be centered around ||\u00b5||, and so shifting back \u00b5 to 0 reduces the norm.\nA more formal proof can be seen as follows: let the pdf of X be fX(\u00b7). Then we wish to show\u222b x ||x\u2212 \u00b5|| fX(x)dx \u2265 \u222b x ||x|| fX(x)dx\nNow we can pair points x,\u2212x, using the fact that fX(x) = fX(\u2212x) and the triangle inequality on the integrand to get\u222b |x| (||x\u2212 \u00b5||+ ||\u2212x\u2212 \u00b5||) fX(x)dx \u2265 \u222b |x| ||2x|| fX(x)dx = \u222b |x| (||x||+ ||\u2212x||) fX(x)dx"}, {"heading": "B.2. Proof of Theorem", "text": "We use v(d)i to denote the i th neuron in hidden layer d. We also let x = z(0) be an input, h(d) be the hidden representation at layer d, and \u03c6 the non-linearity. The weights and bias are called W (d) and b(d) respectively. So we have the relations\nh(d) = W (d)z(d) + b(d), z(d+1) = \u03c6(h(d)). (1)\nProof. We first prove the zero bias case. To do so, it is sufficient to prove that\nE [\u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d+1)(t)\u2223\u2223\u2223\u2223\u2223\u2223] \u2265 O ( \u221a\u03c3k\u221a \u03c3 + k )d+1\u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(0)(t)\u2223\u2223\u2223\u2223\u2223\u2223 (**) as integrating over t gives us the statement of the theorem.\nFor ease of notation, we will suppress the t in z(d)(t).\nWe first write W (d) = W\n(d) \u22a5 +W (d) \u2016\nwhere the division is done with respect to z(d). Note that this means h(d+1) = W (d)\u2016 z (d) as the other component annihilates (maps to 0) z(d).\nWe can also define A W (d) \u2016 = {i : i \u2208 [k], |h(d+1)i | < 1} i.e. the set of indices for which the hidden representation is not saturated. Letting Wi denote the ith row of matrix W , we now claim that:\nEW (d) [\u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d+1)\u2223\u2223\u2223\u2223\u2223\u2223] = EW (d)\u2016 EW (d)\u22a5   \u2211 i\u2208A\nW (d) \u2016\n((W (d) \u22a5 )i\u03b4z (d) + (W (d) \u2016 )i\u03b4z (d))2  1/2  (*)\nIndeed, by Lemma 2 we first split the expectation over W (d) into a tower of expectations over the two independent parts of W to get EW (d) [\u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d+1)\u2223\u2223\u2223\u2223\u2223\u2223] = EW (d)\u2016 EW (d)\u22a5 [\u2223\u2223\u2223\u2223\u2223\u2223\u03c6(W (d)\u03b4z(d))\u2223\u2223\u2223\u2223\u2223\u2223]\nBut conditioning on W (d)\u2016 in the inner expectation gives us h (d+1) and A W (d)\n\u2016 , allowing us to replace the norm over\n\u03c6(W (d)\u03b4z(d)) with the sum in the term on the right hand side of the claim.\nTill now, we have mostly focused on partitioning the matrix W (d). But we can also set \u03b4z(d) = \u03b4z(d)\u2016 + \u03b4z (d) \u22a5 where the perpendicular and parallel are with respect to z(d). In fact, to get the expression in (**), we derive a recurrence as below:\nEW (d) [\u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d+1)\u22a5 \u2223\u2223\u2223\u2223\u2223\u2223] \u2265 O ( \u221a \u03c3k\u221a \u03c3 + k ) EW (d) [\u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d)\u22a5 \u2223\u2223\u2223\u2223\u2223\u2223]\nTo get this, we first need to define z\u0303(d+1) = 1A W\n(d) \u2016\nh(d+1) - the latent vector h(d+1) with all saturated units zeroed out.\nWe then split the column space of W (d) = \u22a5W (d) + \u2016W (d), where the split is with respect to z\u0303(d+1). Letting \u03b4z(d+1)\u22a5 be the part perpendicular to z(d+1), and A the set of units that are unsaturated, we have an important relation: Claim \u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d+1)\u22a5 \u2223\u2223\u2223\u2223\u2223\u2223 \u2265 \u2223\u2223\u2223\u2223\u2223\u2223\u22a5W (d)\u03b4z(d)1A\u2223\u2223\u2223\u2223\u2223\u2223 (where the indicator in the right hand side zeros out coordinates not in the active set.)\nTo see this, first note, by definition,\n\u03b4z (d+1) \u22a5 = W (d)\u03b4z(d) \u00b7 1A \u2212 \u3008W (d)\u03b4z(d) \u00b7 1A, z\u0302(d+1)\u3009z\u0302(d+1) (1)\nwhere the \u00b7\u0302 indicates a unit vector.\nSimilarly \u22a5W (d)\u03b4z(d) = W (d)\u03b4z(d) \u2212 \u3008W (d)\u03b4z(d), \u02c6\u0303z(d+1)\u3009\u02c6\u0303z(d+1) (2)\nNow note that for any index i \u2208 A, the right hand sides of (1) and (2) are identical, and so the vectors on the left hand side agree for all i \u2208 A. In particular,\n\u03b4z (d+1) \u22a5 \u00b7 1A = \u22a5W (d)\u03b4z(d) \u00b7 1A\nNow the claim follows easily by noting that \u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d+1)\u22a5 \u2223\u2223\u2223\u2223\u2223\u2223 \u2265 \u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d+1)\u22a5 \u00b7 1A\u2223\u2223\u2223\u2223\u2223\u2223.\nReturning to (*), we split \u03b4z(d) = \u03b4z(d)\u22a5 + \u03b4z (d) \u2016 , W (d) \u22a5 = \u2016W (d) \u22a5 + \u22a5W (d) \u22a5 (and W (d) \u2016 analogously), and after some cancellation, we have\nEW (d) [\u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d+1)\u2223\u2223\u2223\u2223\u2223\u2223] = EW (d)\u2016 EW (d)\u22a5   \u2211 i\u2208A\nW (d) \u2016\n( (\u22a5W\n(d) \u22a5 + \u2016W (d) \u22a5 )i\u03b4z (d) \u22a5 + ( \u22a5W (d) \u2016 + \u2016W (d) \u2016 )i\u03b4z (d) \u2016 )2 1/2 \nWe would like a recurrence in terms of only perpendicular components however, so we first drop the \u2016W (d)\u22a5 , \u2016W (d) \u2016 (which can be done without decreasing the norm as they are perpendicular to the remaining terms) and using the above claim, have\nEW (d) [\u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d+1)\u22a5 \u2223\u2223\u2223\u2223\u2223\u2223] \u2265 EW (d)\u2016 EW (d)\u22a5   \u2211 i\u2208A\nW (d) \u2016\n( (\u22a5W\n(d) \u22a5 )i\u03b4z (d) \u22a5 + ( \u22a5W (d) \u2016 )i\u03b4z (d) \u2016 )2 1/2 \nBut in the inner expectation, the term \u22a5W (d)\u2016 \u03b4z (d) \u2016 is just a constant, as we are conditioning on W (d) \u2016 . So using Lemma 5 we have\nE W\n(d) \u22a5   \u2211 i\u2208A\nW (d) \u2016\n( (\u22a5W\n(d) \u22a5 )i\u03b4z (d) \u22a5 + ( \u22a5W (d) \u2016 )i\u03b4z (d) \u2016 )2 1/2  \u2265 EW (d)\u22a5   \u2211 i\u2208A\nW (d) \u2016\n( (\u22a5W\n(d) \u22a5 )i\u03b4z (d) \u22a5 )2 1/2 \nWe can then apply Lemma 4 to get\nE W\n(d) \u22a5   \u2211 i\u2208A\nW (d) \u2016\n( (\u22a5W\n(d) \u22a5 )i\u03b4z (d) \u22a5 )2 1/2  \u2265 \u03c3\u221ak\u221a2 \u221a 2|A W (d) \u2016 | \u2212 3 2 E [\u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d)\u22a5 \u2223\u2223\u2223\u2223\u2223\u2223]\nThe outer expectation on the right hand side only affects the term in the expectation through the size of the active set of units. For ReLUs, p = P(h(d+1)i > 0) and for hard tanh, we have p = P(|h (d+1) i | < 1), and noting that we get a non-zero norm only if |A W (d) \u2016 | \u2265 2 (else we cannot project down a dimension), and for |A W (d) \u2016 | \u2265 2,\n\u221a 2\n\u221a 2|A\nW (d) \u2016 | \u2212 3\n2 \u2265 1\u221a\n2\n\u221a |A\nW (d) \u2016 |\nwe get\nEW (d) [\u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d+1)\u22a5 \u2223\u2223\u2223\u2223\u2223\u2223] \u2265 1\u221a2  k\u2211 j=2 ( k j ) pj(1\u2212 p)k\u2212j \u03c3\u221a k \u221a j E [\u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d)\u22a5 \u2223\u2223\u2223\u2223\u2223\u2223] We use the fact that we have the probability mass function for an (k, p) binomial random variable to bound the \u221a j term:\nk\u2211 j=2 ( k j ) pj(1\u2212 p)k\u2212j \u03c3\u221a k \u221a j = \u2212 ( k 1 ) p(1\u2212 p)k\u22121 \u03c3\u221a k + k\u2211 j=0 ( k j ) pj(1\u2212 p)k\u2212j \u03c3\u221a k \u221a j\n= \u2212\u03c3 \u221a kp(1\u2212 p)k\u22121 + kp \u00b7 \u03c3\u221a\nk k\u2211 j=1 1\u221a j ( k \u2212 1 j \u2212 1 ) pj\u22121(1\u2212 p)k\u2212j\nBut by using Jensen\u2019s inequality with 1/ \u221a x, we get\nk\u2211 j=1 1\u221a j ( k \u2212 1 j \u2212 1 ) pj\u22121(1\u2212 p)k\u2212j \u2265 1\u221a\u2211k j=1 j ( k\u22121 j\u22121 ) pj\u22121(1\u2212 p)k\u2212j = 1\u221a (k \u2212 1)p+ 1\nwhere the last equality follows by recognising the expectation of a binomial(k\u22121, p) random variable. So putting together, we get\nEW (d) [\u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d+1)\u22a5 \u2223\u2223\u2223\u2223\u2223\u2223] \u2265 1\u221a2 ( \u2212\u03c3 \u221a kp(1\u2212 p)k\u22121 + \u03c3 \u00b7 \u221a kp\u221a 1 + (k \u2212 1)p ) E [\u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d)\u22a5 \u2223\u2223\u2223\u2223\u2223\u2223] (a)\nFrom here, we must analyse the hard tanh and ReLU cases separately. First considering the hard tanh case:\nTo lower bound p, we first note that as h(d+1)i is a normal random variable with variance \u2264 \u03c32, if A \u223c N (0, \u03c32)\nP(|h(d+1)i | < 1) \u2265 P(|A| < 1) \u2265 1\n\u03c3 \u221a 2\u03c0 (b)\nwhere the last inequality holds for \u03c3 \u2265 1 and follows by Taylor expanding e\u2212x2/2 around 0. Similarly, we can also show that p \u2264 1\u03c3 .\nSo this becomes\nE [\u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d+1)\u2223\u2223\u2223\u2223\u2223\u2223] \u2265  1\u221a 2  1 (2\u03c0)1/4 \u221a \u03c3k\u221a\n\u03c3 \u221a 2\u03c0 + (k \u2212 1) \u2212 \u221a k\n( 1\u2212 1\n\u03c3\n)k\u22121E [\u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d)\u22a5 \u2223\u2223\u2223\u2223\u2223\u2223]\n= O ( \u221a \u03c3k\u221a \u03c3 + k ) E [\u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d)\u22a5 \u2223\u2223\u2223\u2223\u2223\u2223]\nFinally, we can compose this, to get\nE [\u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d+1)\u2223\u2223\u2223\u2223\u2223\u2223] \u2265  1\u221a 2  1 (2\u03c0)1/4 \u221a \u03c3k\u221a\n\u03c3 \u221a 2\u03c0 + (k \u2212 1) \u2212 \u221a k\n( 1\u2212 1\n\u03c3 )k\u22121d+1 c \u00b7 ||\u03b4x(t)|| (c) with the constant c being the ratio of ||\u03b4x(t)\u22a5|| to ||\u03b4x(t)||. So if our trajectory direction is almost orthogonal to x(t) (which will be the case for e.g. random circular arcs, c can be seen to be \u2248 1 by splitting into components as in Lemma 1, and using Lemmas 3, 4.)\nThe ReLU case (with no bias) is even easier. Noting that for random weights, p = 1/2, and plugging in to equation (a), we get\nEW (d) [\u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d+1)\u22a5 \u2223\u2223\u2223\u2223\u2223\u2223] \u2265 1\u221a2 ( \u2212\u03c3 \u221a k 2k + \u03c3 \u00b7 \u221a k\u221a 2(k + 1) ) E [\u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d)\u22a5 \u2223\u2223\u2223\u2223\u2223\u2223] (d)\nBut the expression on the right hand side has exactly the asymptotic form O(\u03c3 \u221a k/ \u221a k + 1), and we finish as in (c).\nResult for non-zero bias In fact, we can easily extend the above result to the case of non-zero bias. The insight is to note that because \u03b4z(d+1) involves taking a difference between z(d+1)(t + dt) and z(d+1)(t), the bias term does not enter at all into the expression for \u03b4z(d+1). So the computations above hold, and equation (a) becomes\nEW (d) [\u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d+1)\u22a5 \u2223\u2223\u2223\u2223\u2223\u2223] \u2265 1\u221a2 ( \u2212\u03c3w \u221a kp(1\u2212 p)k\u22121 + \u03c3w \u00b7 \u221a kp\u221a 1 + (k \u2212 1)p ) E [\u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d)\u22a5 \u2223\u2223\u2223\u2223\u2223\u2223]\nFor ReLUs, we require h(d+1)i = w (d+1) i z (d) i + b (d+1) i > 0 where the bias and weight are drawn from N (0, \u03c32b ) and N (0, \u03c32w) respectively. But with p \u2265 1/4, this holds as the signs for w, b are purely random. Substituting in and working through results in the same asymptotic behavior as without bias.\nFor hard tanh, not that as h(d+1)i is a normal random variable with variance \u2264 \u03c32w + \u03c32b (as equation (b) becomes\nP(|h(d+1)i | < 1) \u2265 1\u221a\n(\u03c32w + \u03c3 2 b ) \u221a 2\u03c0\nThis gives Theorem 3\nE [\u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d+1)\u2223\u2223\u2223\u2223\u2223\u2223] \u2265 O  \u03c3w (\u03c32w + \u03c3 2 b ) 1/4 \u00b7 \u221a k\u221a\u221a\n\u03c32w + \u03c3 2 b + k\nE [\u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d)\u22a5 \u2223\u2223\u2223\u2223\u2223\u2223]\nStatement and Proof of Upper Bound for Trajectory Growth for Hard Tanh Replace hard-tanh with a linear coordinate-wise identity map, h(d+1)i = (W\n(d)z(d))i + bi. This provides an upper bound on the norm. We also then recover a chi distribution with k terms, each with standard deviation \u03c3w\nk 1 2\n,\nE [\u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d+1)\u2223\u2223\u2223\u2223\u2223\u2223] \u2264 \u221a2\u0393 ((k + 1)/2)\n\u0393 (k/2)\n\u03c3w k 1 2 \u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d)\u2223\u2223\u2223\u2223\u2223\u2223 (2) \u2264 \u03c3w ( k + 1\nk\n) 1 2 \u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d)\u2223\u2223\u2223\u2223\u2223\u2223 , (3)\nwhere the second step follows from (Laforgia and Natalini, 2013), and holds for k > 1."}, {"heading": "Proof of Theorem 4", "text": "Proof. For \u03c3b = 0: For hidden layer d < n, consider neuron v(d)1 . This has as input \u2211k i=1W (d\u22121) i1 z (d\u22121) i . As we are in the large \u03c3 case, we assume that |z(d\u22121)i | = 1. Furthermore, as signs for z (d\u22121) i and W (d\u22121) i1 are both completely random, we can also assume wlog that z(d\u22121)i = 1. For a particular input, we can define v (d) 1 as sensitive to v (d\u22121) i if v (d\u22121) i transitioning (to\nwlog \u22121) will induce a transition in node v(d)1 . A sufficient condition for this to happen is if |Wi1| \u2265 | \u2211 j 6=iWj1|. But\nX = Wi1 \u223c N (0, \u03c32/k) and \u2211 j 6=iWj1 = Y\n\u2032 \u223c N (0, (k\u2212 1)\u03c32/k). So we want to compute P(|X| > |Y \u2032|). For ease of computation, we instead look at P(|X| > |Y |), where Y \u223c N (0, \u03c32).\nBut this is the same as computing P(|X|/|Y | > 1) = P(X/Y < \u22121) + P(X/Y > 1). But the ratio of two centered independent normals with variances \u03c321 , \u03c3 2 2 follows a Cauchy distribution, with parameter \u03c31/\u03c32, which in this case is\n1/ \u221a k. Substituting this in to the cdf of the Cauchy distribution, we get that\nP ( |X| |Y | > 1 ) = 1\u2212 2 \u03c0 arctan( \u221a k)\nFinally, using the identity arctan(x)+arctan(1/x) and the Laurent series for arctan(1/x), we can evaluate the right hand side to be O(1/ \u221a k). In particular P ( |X| |Y | > 1 ) \u2265 O ( 1\u221a k ) (c) This means that in expectation, any neuron in layer d will be sensitive to the transitions of \u221a k neurons in the layer below. Using this, and the fact the while v(d\u22121)i might flip very quickly from say \u22121 to 1, the gradation in the transition ensures that neurons in layer d sensitive to v(d\u22121)i will transition at distinct times, we get the desired growth rate in expectation as follows:\nLet T (d) be a random variable denoting the number of transitions in layer d. And let T (d)i be a random variable denoting the number of transitions of neuron i in layer d. Note that by linearity of expectation and symmetry, E [ T (d) ] = \u2211 i E [ T (d) i ] =\nkE [ T\n(d) 1 ] Now, E [ T\n(d+1) 1\n] \u2265 E [\u2211 i 1(1,i)T (d) i ] = kE [ 1(1,1)T (d) 1 ] where 1(1,i) is the indicator function of neuron 1 in layer d+ 1\nbeing sensitive to neuron i in layer d. But by the independence of these two events, E [ 1(1,1)T (d) 1 ] = E [ 1(1,1) ] \u00b7 E [ T (d) 1 ] . But the firt time on the right hand side is O(1/ \u221a k) by (c), so putting it all together, E [ T\n(d+1) 1\n] \u2265 \u221a kE [ T\n(d) 1\n] .\nWritten in terms of the entire layer, we have E [ T (d+1) ] \u2265 \u221a kE [ T (d) ] as desired.\nFor \u03c3b > 0: We replace \u221a k with \u221a k(1 + \u03c32b/\u03c3 2 w), by noting that Y \u223c N (0, \u03c32w + \u03c32b ). This results in a growth rate of form\nO( \u221a k/ \u221a\n1 + \u03c32b \u03c32w ).\nB.3. Dichotomies: a natural dual\nOur measures of expressivity have mostly concentrated on sweeping the input along a trajectory x(t) and taking measures of FA(x(t);W ). Instead, we can also sweep the weights W along a trajectory W (t), and look at the consequences (e.g. binary labels \u2013 i.e. dichotomies), say for a fixed set of inputs x1, ..., xs.\nIn fact, after random initialization, sweeping the first layer weights is statistically very similar to sweeping the input along a trajectory x(t). In particular, letting W \u2032 denote the first layer weights, for a particular input x0, x0W \u2032 is a vector, each coordinate is iid, \u223c N (0, ||x0||2\u03c32w). Extending this observation, we see that (providing norms are chosen appropriately), x0W \u2032 cos(t) +x1W \u2032 sin(t) (fixed x0, x1,W ) has the same distribution as x0W \u20320 cos(t) +x0W \u2032 1 sin(t) (fixed x0,W \u2032 0,W \u2032 1).\nSo we expect that there will be similarities between results for sweeping weights and for sweeping input trajectories, which we explore through some synthetic experiments, primarily for hard tanh, in Figures 15, 16. We find that the proportionality of transitions to trajectory length extends to dichotomies, as do results on the expressive power afforded by remaining depth.\nFor non-random inputs and non-random functions, this is a well known question upper bounded by the Sauer-Shelah lemma (Sauer, 1972). We discuss this further in Appendix ??. In the random setting, the statistical duality of weight sweeping and input sweeping suggests a direct proportion to transitions and trajectory length for a fixed input. Furthermore, if the xi \u2208 S are sufficiently uncorrelated (e.g. random) class label transitions should occur independently for each xi Indeed, we show this in Figure 14."}, {"heading": "C. Addtional Experiments from Section 3", "text": "Here we include additional experiments from Section 3"}], "year": 2017, "references": [{"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "authors": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "year": 2012}, {"title": "Deep knowledge tracing", "authors": ["Chris Piech", "Jonathan Bassen", "Jonathan Huang", "Surya Ganguli", "Mehran Sahami", "Leonidas J Guibas", "Jascha Sohl-Dickstein"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2015}, {"title": "Multilayer feedforward networks are universal approximators", "authors": ["Kurt Hornik", "Maxwell Stinchcombe", "Halbert White"], "venue": "Neural networks,", "year": 1989}, {"title": "Approximation by superpositions of a sigmoidal function", "authors": ["George Cybenko"], "venue": "Mathematics of control, signals and systems,", "year": 1989}, {"title": "A comparison of the computational power of sigmoid and Boolean threshold circuits", "authors": ["Wolfgang Maass", "Georg Schnitger", "Eduardo D Sontag"], "year": 1994}, {"title": "Almost linear vc-dimension bounds for piecewise polynomial networks", "authors": ["Peter L Bartlett", "Vitaly Maiorov", "Ron Meir"], "venue": "Neural computation,", "year": 1998}, {"title": "On the number of response regions of deep feed forward networks with piece-wise linear activations", "authors": ["Razvan Pascanu", "Guido Montufar", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1312.6098,", "year": 2013}, {"title": "The power of depth for feedforward neural networks", "authors": ["Ronen Eldan", "Ohad Shamir"], "venue": "arXiv preprint arXiv:1512.03965,", "year": 2015}, {"title": "Representation benefits of deep feedforward networks", "authors": ["Matus Telgarsky"], "venue": "arXiv preprint arXiv:1509.08101,", "year": 2015}, {"title": "On the representational efficiency of restricted boltzmann machines", "authors": ["James Martens", "Arkadev Chattopadhya", "Toni Pitassi", "Richard Zemel"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2013}, {"title": "On the complexity of neural network classifiers: A comparison between shallow and deep architectures", "authors": ["Monica Bianchini", "Franco Scarselli"], "venue": "Neural Networks and Learning Systems, IEEE Transactions on,", "year": 2014}, {"title": "Exponential expressivity in deep neural networks through transient chaos", "authors": ["Ben Poole", "Subhaneil Lahiri", "Maithra Raghu", "Jascha SohlDickstein", "Surya Ganguli"], "venue": "In Advances in neural information processing systems,", "year": 2016}, {"title": "Hyperplane arrangements", "authors": ["Richard Stanley"], "venue": "Enumerative Combinatorics,", "year": 2011}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "authors": ["Sergey Ioffe", "Christian Szegedy"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015,", "year": 2015}, {"title": "Explaining and harnessing adversarial examples", "authors": ["Ian J. Goodfellow", "Jonathon Shlens", "Christian Szegedy"], "venue": "CoRR, abs/1412.6572,", "year": 2014}, {"title": "Some extensions of w. gautschi\u2019s inequalities for the gamma function", "authors": ["D. Kershaw"], "venue": "Mathematics of Computation,", "year": 1983}, {"title": "On some inequalities for the gamma function", "authors": ["Andrea Laforgia", "Pierpaolo Natalini"], "venue": "Advances in Dynamical Systems and Applications,", "year": 2013}, {"title": "On the density of families of sets", "authors": ["Norbert Sauer"], "venue": "Journal of Combinatorial Theory, Series A,", "year": 1972}], "id": "SP:03b6eefcd0183a2387e98c99e5544163eb6ed1ea", "authors": [{"name": "Maithra Raghu", "affiliations": []}, {"name": "Ben Poole", "affiliations": []}, {"name": "Jon Kleinberg", "affiliations": []}, {"name": "Surya Ganguli", "affiliations": []}, {"name": "Jascha Sohl Dickstein", "affiliations": []}], "abstractText": "We propose a new approach to the problem of neural network expressivity, which seeks to characterize how structural properties of a neural network family affect the functions it is able to compute. Our approach is based on an interrelated set of measures of expressivity, unified by the novel notion of trajectory length, which measures how the output of a network changes as the input sweeps along a one-dimensional path. Our findings can be summarized as follows: (1) The complexity of the computed function grows exponentially with depth. We design measures of expressivity that capture the non-linearity of the computed function. Due to how the network transforms its input, these measures grow exponentially with depth. (2) All weights are not equal (initial layers matter more). We find that trained networks are far more sensitive to their lower (initial) layer weights: they are much less robust to noise in these layer weights, and also perform better when these weights are optimized well. (3) Trajectory Regularization works like Batch Normalization. We find that batch norm stabilizes the learnt representation, and based on this propose a new regularization scheme, trajectory regularization.", "title": "On the Expressive Power of Deep Neural Networks"}