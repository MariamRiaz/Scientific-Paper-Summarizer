0	37	Learning to rank aims to learn a model to rerank a list of objects, e.g., candidate documents in document retrieval.
1	26	Recent studies show that listwise learning delivers better performance in general than traditional pairwise learning (Liu, 2009), partly attributed to its capability of learning human-labelled scores as a full rank list.
4	24	This model has been utilized to tackle many ranking problems, e.g. modeling the hiring behavior in on- line labor markets (Kokkodis et al., 2015), ranking sentences in document summarization (Jin et al., 2010), improving detection of musical concepts (Yang et al., 2009) and ranking the results in video search (Yang and Hsu, 2008).
5	35	Basically, ListNet implements the rank function as a neural network (NN), with the objective function set to be the cross entropy between two probability distributions over the object permutations, one derived from the human-labelled scores and the other derived from the model prediction (network output).
14	19	Based on these two conjectures, we propose a stochastic ListNet method, which samples a subset 676 of the permutation classes (object lists) in model training and based on this subset to train the ListNet model.
19	20	Meanwhile, better performance was obtained with the stochastic ListNet approach, probably due to the learning of partial rank information.
20	32	The contributions of the paper are three-fold: (1) proposes a stochastic ListNet method that significantly reduces the training complexity and delivers better ranking performance; (2) investigates Top-k models based on the stochastic ListNet, and studies the impact of a large k; (3) provides an open source implementation based on RankLib.
23	63	Section 4 presents the experiments, and the paper is concluded by Section 6.
36	18	By this definition of permutation probability, Eq.
45	21	In fact, setting k=1 effectively marginalizes all the probabilities over the candidate objects of a permutation class except the top one.
54	21	As a comparison, the full set of permutation classes of the Top-k model is n!(nâˆ’k)!
70	83	These scores are normalized by softmax and are used as the probability distribution when sampling objects.
71	73	Because the probabilities of relevant objects are larger than those of irrelevant objects, more relevant objects would be selected by this sampling approach in model training.
75	19	Note that the network outputs are natural measures of object relevance based on the present ranking model.
95	51	It contains queries and corresponding candidate documents.
103	26	Specially, for all the three distribution sampling methods, the sampling process involves two steps: pre-selection and re-sampling.
104	63	The preselection step samples a list of documents following three distributions mentioned above, and in the re-sampling step, document lists including more relevant documents are retained with a higher probability.
106	23	The resampling approach is designed to encourage document lists containing more relevant documents, which is the most important for the uniform distribution sampling.
116	14	From these results, we first observe that stochastic ListNet with either fixed or adaptive distribution sampling tends to outperform the conventional ListNet approach, particularly with a large k. This confirms our argument that rank information can be learned from a subset of the permutation classes that are randomly selected, and the partial rank learning can lead to even better performance than the full rank learning, the case of conventional ListNet.
118	14	It is also seen that the adaptive distribution sampling performs slightly better than the fixed distribution sampling.
121	57	Another observation is that in all the four figures, the performance of the stochastic ListNet methods increases with more samples of the object lists.
126	30	Comparing the results with different k, it can be seen that a larger k leads to a better perfor- mance with stochastic ListNet.
128	34	However, this is not necessarily the case with the conventional ListNet.
129	114	For example, the Top-2 model does not offer better performance than the Top-1 model.
130	42	This is perhaps because high-order Top-k models consider a large number of document lists and most of them are not informative, which leads to ineffective learning.
131	45	Remind that the conventional ListNet is a special case of the stochastic ListNet with a very large sample set, and we have discussed that an over large sample set actually reduces performance.
132	162	The averaged training time and the performance in precession are presented in Table 1.
137	18	This is expected since the conventional ListNet considers the full set of permutations which is a huge number with a large k. With the stochastic ListNet, the training time is dramatically reduced.
138	115	Even with a large k, the computation cost is still manageable, because the computation is mostly determined by the number of object lists, rather than the value of k. When comparing the three sampling methods, it can be found the convergence speed of the uniform distribution approach is the slowest, probably due to the ineffective selection for relevant documents.
139	60	The adaptive distribution sampling is the fastest, probably attributed to the collaborative update of the model and the distribution.
