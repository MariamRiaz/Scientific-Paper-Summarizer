0	21	Every programmer has experienced the situation where they know what they want to do, but do not have the ability to turn it into a concrete implementation.
1	29	For example, a Python programmer may want to “sort my list in descending order,” but not be able to come up with the proper syntax sorted(my list, reverse=True) to realize his intention.
2	82	To resolve this impasse, it is common for programmers to search the web in natural language (NL), find an answer, and modify it into the desired form (Brandt et al., 2009, 2010).
3	42	However, this is time-consuming, and thus the software engineering literature is ripe with methods to directly generate code from NL descriptions, mostly with hand-engineered methods highly tailored to specific programming languages (Balzer, 1985; Little and Miller, 2009; Gvero and Kuncak, 2015).
8	17	This work treats code generation as a sequence-tosequence modeling problem, and introduce methods to generate words from character-level models, and copy variable names from input descriptions.
11	56	In order to capture the strong underlying syntax of the PL, we define a model that transduces an NL statement into an Abstract Syntax Tree (AST; Fig.
19	17	Our approach frees the model from recovering the underlying grammar from limited training data, and instead enables the system to focus on learning the compositionality among existing grammar rules.
30	18	We define a probabilistic grammar model of generating an AST y given x: p(y|x).
45	40	We fac- torize the generation process of an AST into sequential application of actions of two types: • APPLYRULE[r] applies a production rule r to the current derivation tree; • GENTOKEN[v] populates a variable terminal node by appending a terminal token v. Fig.
57	20	Unary Closure Sometimes, generating an AST requires applying a chain of unary productions.
62	20	Once we reach a frontier node nft that corresponds to a variable type (e.g., str), GENTOKEN actions are used to fill this node with values.
78	25	Each action step in the grammar model naturally grounds to a time step in the decoder RNN.
79	28	Therefore, the action sequence in Fig.
81	28	The RNN maintains an internal state to track the generation process (§ 4.2.1), which will then be used to compute action probabilities p(at|x, a<t) (§ 4.2.2).
82	20	Our implementation of the decoder resembles a vanilla LSTM, with additional neural connections (parent feeding, Fig.
93	17	Context Vector ct The decoder RNN uses soft attention to retrieve a context vector ct from the input encodings {hi} pertain to the prediction of the current action.
102	43	In this section we explain how action probabilities p(at|x, a<t) are computed based on st. APPLYRULE The probability of applying rule r as the current action at is given by a softmax6: p(at = APPLYRULE[r]|x, a<t) = softmax(WR · g(st))| · e(r) (4) where g(·) is a non-linearity tanh(W ·st+b), and e(r) the one-hot vector for rule r. GENTOKEN As in § 3.2, a token v can be generated from a predefined vocabulary or copied from the input, defined as the marginal probability: p(at = GENTOKEN[v]|x, a<t) = p(gen|x, a<t)p(v|gen, x, a<t) + p(copy|x, a<t)p(v|copy, x, a<t).
109	35	Given a dataset of pairs of NL descriptions xi and code snippets ci, we parse ci into its AST yi and decompose yi into a sequence of oracle actions, which explains the generation story of yi under the grammar model.
110	26	The model is then optimized by maximizing the log-likelihood of the oracle action sequence.
115	30	This dataset is relatively difficult: input descriptions are short, while the target code is in complex class structures, with each AST having 137 nodes on average.
116	49	DJANGO dataset (Oda et al., 2015) is a collection of lines of code from the Django web framework, each with a manually annotated NL description.
142	24	We compare primarily with two approaches: (1) Latent Predictor Network (LPN), a state-of-the-art sequenceto-sequence code generation model (Ling et al., 2016), and (2) SEQ2TREE, a neural semantic parsing model (Dong and Lapata, 2016).
187	42	The first one shows that the model learns the usage of common API calls (e.g., os.path.join), and how to populate the arguments by copying from inputs.
188	38	The second example illustrates the difficulty of generating code with complex nested structures like lambda functions, a scenario worth further investigation in future studies.
193	110	While the rarity of such examples suggests that our exact match metric is reasonable, more advanced evaluation metrics based on statistical code analysis are definitely intriguing future work.
194	23	For DJANGO, we found that 30% of failed cases were due to errors where the pointer network failed to appropriately copy a variable name into the correct position.
197	48	The remaining 30% of examples were errors stemming from multiple sources, or errors that could not be easily categorized into the above.
198	28	For HS, we found that all failed card examples were due to partial implementation errors, such as the one shown in Table 5.
212	17	This paper proposes a syntax-driven neural code generation approach that generates an abstract syntax tree by sequentially applying actions from a grammar model.
213	17	Experiments on both code generation and semantic parsing tasks demonstrate the effectiveness of our proposed approach.
