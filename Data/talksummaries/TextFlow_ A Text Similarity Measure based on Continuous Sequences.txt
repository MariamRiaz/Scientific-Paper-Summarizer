1	18	While a big part of this content consists of visual information such as pictures and videos, texts also continue growing at a very high pace.
2	10	A recent study shows that the average webpage weights 1,200 KB with plain text accounting for up to 16% of that size2.
9	11	In a more general study, Achananuparp et al. (2008) compared several text similarity measures for paraphrase recognition, textual entailment, and the TREC 9 question variants task.
14	20	More generally, existing standalone (or traditional) text similarity measures rely on the intersections between token sets and/or text sizes and frequency, including measures such as the Cosine similarity, Euclidean distance, Levenshtein (Sankoff and Kruskal, 1983), Jaccard (Jain and Dubes, 1988) and Jaro (Jaro, 1989).
15	14	The sequential nature of natural language is taken into account mostly through word n-grams and skipgrams which capture distinct slices of the analysed texts but do not preserve the order in which they appear.
16	30	In this paper, we use intuitions from a common representation in DNA sequence alignment to design a new standalone similarity measure called TextFlow (XF).
17	10	The proposed measure uses at the same time the full sequence of input texts in a natural sub-sequence matching approach together with individual token matches and mismatches.
22	8	• A neural network architecture to train TextFlow parameters for specific tasks.
25	19	2 The TextFlow Similarity XF is inspired from a dot matrix representation commonly used in pairwise DNA sequence alignment (cf.
26	34	We use a similar dot matrix representation for text pairs and draw a curve oscillating around the diagonal (cf.
31	11	The semantics are: the bigger the area under curve is, the lower the similarity between the compared texts.
38	21	If xi /∈ X ∩ Y , ∆P (xi, X, Y ) is set to the same reference value equal to m, (i.e., the cost of a missing word is set by default to the length of the target text), and: • Si is the length of the longest matching sequence between X and Y including the word xi, if xi ∈ X ∩ Y , or 1 otherwise.
51	10	We use identity as activation function in the dedicated XF layer in order to have a correct comparison with the other similarity measures, including canonical XF where the similarity value is provided in the input layer (cf.
52	13	This evaluation was performed on 8 datasets from 3 different classification tasks: Tex- tual Entailment Recognition, Paraphrase Detection, and ranking relevance.
62	8	• MSRP: the Microsoft Research Paraphrase corpus, consisting of 5,800 sentence pairs annotated with a binary label indicating whether the two sentences are paraphrases or not.
68	18	After a preprocessing step where we removed stopwords, we computed the similarity values using 7 different types of sequences constructed, respectively, with the following value from each token: • Word (plain text value) • Lemma • Part-Of-Speech (POS) tag • WordNet Synset6 OR Lemma • WordNet Synset OR Lemma for Nouns • WordNet Synset OR Lemma for Verbs • WordNet Synset OR Lemma for Nouns and Verbs.
74	20	For canonical XFc we use by default the best aggregation for the task as observed in table 3. liblinear/ Similarity Measures.
75	9	We considered nine traditional similarity measures included in the Simmetrics distribution8: Cosine, Euclidean distance, Word Overlap, Dice coefficient (Dice, 1945), Jaccard(Jain and Dubes, 1988), Damerau, Jaro-Winkler (JW) (Porter et al., 1997), Levenshtein (LEV) (Sankoff and Kruskal, 1983), and Longest Common Subsequence (LCS) (Friedman and Sideli, 1992).
82	5	We also study the relative rank in performance of each similarity measure across all datasets using the average rank, the rank variance and a new evaluation measure called Consistent peRformancE (CORE), computed as follows for a system m, a set of datasets D, a set of systems S, and an evaluation measure E ∈ {F1, P recision,Recall, Accuracy}: CORE D,S,E (m) = MIN p∈S ( AVG d∈D (RS(Ed(p)) + Vd∈D(RS(Ed(p))) ) AVG d∈D ( RS(Ed(m)) ) + Vd∈D ( RS(Ed(m)) ) (7) With RS(Ed(m)) the rank of m according to the evaluation measure E on dataset d w.r.t.
83	36	competing systems S. Vd∈D(RS(Ed(m))) is the rank variance of m over datasets.
85	24	Basically, CORE tells us how consistent a system/method is in having high performance, relatively to the set of competing systems S. The maximum value of CORE is 1 for the best performing system according to its rank.
86	9	It also allows quantifying how consistently a system achieves high performance for the remaining systems.
88	60	4.1 Canonical Text Flow TFc had the best average and micro-average accuracy on the 8 classification datasets, with a gap of +0.4 to +6.3 when compared to the state-of-the-art measures.
90	46	On the F1 score level XFc achieved the second best performance with a gap of -1.7, mainly caused by its underperformance in recall, where it had the third best performance with a gap of -6.3 (cf.
91	6	On a rank level, XFc had the best consistent rank for accuracy, F1 and precision, and the second best for recall.
92	9	When compared to state-of-the-art measures and to canonical XF, the trained version, XFt, obtained the best accuracy with a gap ranging from +1.4 to +7.8.
94	41	XFt obtained the best precision with a gap ranging from +0.8 to +7.1.
95	29	XFt did not perform well on recall with 64.5% micro-average compared to WordOverlap with 72%.
97	52	Canonical XF was more consistent than trained XF on all dimensions except accuracy, for which XFt was optimized.
101	18	The difference in positions is also not read literally, both because of the higher impact associated to missed words and to the α parameter which allows leveraging their impact in the trained version.
