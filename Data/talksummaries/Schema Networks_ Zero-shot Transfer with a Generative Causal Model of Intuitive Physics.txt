1	32	Such generalization is essential for an embodied agent working to accomplish a variety of goals in a changing world.
2	111	Despite remarkable progress on individual tasks like Atari 2600 games (Mnih et al., 2015; Van Hasselt et al., 2016; Mnih et al., 2016) and Go (Silver et al., 2016a), the ability of state-of-the-art models to transfer learning from one environment to the next remains lim- All authors affiliated with Vicarious AI, California, USA.
4	46	In these environments the positions of objects are perturbed, but the object movements and sources of reward remain the same.
5	74	While humans have no trouble generalizing experience from the basic Breakout to its variations, deep neural network-based models are easily fooled (Taylor & Stone, 2009; Rusu et al., 2016).
6	22	The model-free approach of deep reinforcement learning (Deep RL) such as the Deep-Q Network and its descendants is inherently hindered by the same feature that makes it desirable for single-scenario tasks: it makes no assumptions about the structure of the domain.
7	29	Recent work has suggested how to overcome this deficiency by utilizing object-based representations (Diuk et al., 2008; Usunier et al., 2016).
11	18	A causal model is essential for regression planning, in which an agent works backward from a desired future state to produce a plan (Anderson, 1990).
50	15	We first describe the architecture of the model informally.
85	49	Essentially any trackable image feature could be an entity, which most typically includes objects, their boundaries, and their surfaces.
87	29	Depending on the task, Schema Networks could learn to reason flexibly at different levels of representation.
88	17	For example, using entities from surfaces might be most relevant for predicting collisions, while using one entity per object might be most relevant for predicting whether it can be controlled by an action.
91	15	For simplicity we here restrict the entities to have fully observable attributes, but in general they could have latent attributes such as “bounciness” or “magnetism.”
105	15	Given a dataset of entity states over time, we preprocess the entity states into a representation that is more convenient for learning.
119	42	W ∈ {0, 1}D′×L is a binary matrix, with each column representing one (ungrounded) schema for at most L schemas.
123	19	A suitable objective function is min W∈{0,1}D′×L 1 D |y − fW (X)|1 + C|W |1, (1) where the first term computes the prediction error, the second term estimates the complexity and parameter C controls the trade-off between both.
125	41	We consider a greedy solution in which linear programming (LP) relaxations are used to find each new schema.
135	23	Finding the sequence of actions that will result in a given set of rewards becomes then a MAP inference problem.
140	26	Without loss of generality, we will consider the present time step to be t = 0.
156	64	Choosing a positive reward goal state We will choose the potentially feasible positive reward that happens soonest within our explored window, clamp its state to 1, and backtrack (see below) to find the set of actions that lead to it.
162	16	Unlike the HMM for which the Viterbi algorithm was designed, our model is loopy, so a standard backward pass is not enough to find a satisfying configuration (although can help to find good candidates).
164	73	We compared the performance of Schema Networks, A3C, and PNs (Progressive Networks) on several variations of the game Breakout.
166	34	A diverse set of concepts must be learned to correctly predict object movements and rewards.
167	30	For example, when predicting why rewards occur, the model must disentangle possible causes to discover that reward depends on the color of a brick but is independent of the ball’s velocity and position where it was hit.
168	200	While these causal relationships are straightforward for humans to recover, we have yet to see any existing approach for learning a generative model that can recover all of these dynamics without supervision and transfer them effectively.
169	51	Schema Networks rely on an input of entity states instead of raw images, and we provided the same information to A3C and PNs by augmenting the three color channels of the image with 34 additional channels.
177	32	This experiment examines how effectively Schema Networks and PNs are able to learn a new Breakout variation after pretraining, which examines how well the two models can transfer existing knowledge to a new task.
178	36	3a shows the learning rates during 100k frames of training on Mini Breakout.
179	42	In a second experiment, we pretrained on Large Breakout for 100k frames and continued training on the Middle Wall variation, shown in Fig.
184	35	1b-e shows some of these variations with the following modifications from the training environment: • Offset Paddle (Fig.
188	15	1f, enlarged from actual environment to see the balls): Without any bricks, three balls are launched in such a way that a perfect policy could juggle them without dropping any.
