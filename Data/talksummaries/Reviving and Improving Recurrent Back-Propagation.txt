9	41	The key observation exploited by RBP is that the gradient of the steady state w.r.t.
14	100	In this paper, we first revisit RBP in the context of modern deep learning.
21	17	For example, we show how RBP can be used to back propagate thorough the optimization of deep neural networks in order to tune hyperparameters.
51	56	During inference, the hidden state at time t is computed as follows, ht+1 = F (x,wF , ht), (1) where F is the update function parameterized by wF .
53	40	This RNN formulation differs from the one commonly used in language modeling, as the input is not time-dependent.
56	24	We compute the predicted output y based on the steady hidden state as follows, y = G(x,wG, h ∗), (3) where G is the output function parameterized by wG.
64	20	(6) In fact, Equations (4- 6) are an application of the Implicit Function Theorem (Rudin, 1964), which guarantees the existence and uniqueness of an implicit function φ such that h∗ = φ(wF ) if two conditions hold: I, Ψ is continuously differentiable and II, I − JF,h∗ is invertible.
79	42	Recall that in order to apply the implicit function theorem, Ψ(wF , h) has to satisfy two assumptions: I, Ψ is continuously differentiable.
80	27	Condition I requires the derivative of F to be continuous, a condition satisfied by many RNNs, like LSTM and GRU (Cho et al., 2014).
81	26	Condition II is equivalent to requiring the determinant of I − JF,h∗ to be nonzero, i.e., det(I − JF,h∗) 6= 0.
83	27	Recall that F is a contraction map on Banach space B, i.e., a complete normed vector space, iff, ∀h1, h2 ∈ B, ‖F (h1)− F (h2)‖ ≤ µ‖h1 − h2‖ where 0 ≤ µ < 1.
84	36	Banach fixed point theorem guarantees the uniqueness of the fix point of the contraction map F in B.
99	42	(9) in the derivation of original RBP, one would naturally think of the most common iterative solver, i.e., conjugate gradient method (Hestenes & Stiefel, 1952).
102	27	This increases the difficulty of solving the system.
106	34	Having a symmetric matrix multiplying z on the left hand side, we can now use the conjugate gradient method.
107	59	The detailed algorithm is easily obtained by instantiating the standard conjugate gradient (CG) template.
110	78	Since the condition number of the current system is the square of the original one, the system may be slower to converge in practice.
127	21	In practice, we can obtain further memory efficiency by performing updates within the for loops in-place (please refer to the example code in appendix), so that memory usage need not scale with the number of truncation steps.
131	54	, hT of an RNN, we have h∗ = hT = hT−1 = · · · = hT−K where h∗ is the fixed point.
133	32	Moreover, the following proposition bounds the error of K-step Neumann-RBP.
142	21	We consider a simplified continuous Hopfield network as described in (Haykin, 1993).
148	20	The set of neurons consists of three parts: observed, hidden and output neurons, of size 784, 1024 and 784 respectively.
153	27	(18) until convergence (which corresponds to the forward pass of RNNs), we are guaranteed to minimize the following (Lyapunov) energy function.
157	26	In training we feed clean data, and during testing we randomly corrupt 50% of the non-zero pixel values to zero.
162	47	Nevertheless, we can see that training curve of the original RBP blows up which validates its instability issue.
164	25	However, we notice that if we set the truncation step to 10, original RBP exhibits behaviors which fails to converge.
172	22	We adopt graph neural networks (GNNs) (Scarselli et al., 2009) model and employ the GRU as the update function similarly as (Li et al., 2016).
173	24	We refer to (Li et al., 2016; Liao et al., 2018) for more details.
178	29	The training, validation and difference norm curves of BPTT, TBPTT and all RBPs are shown in Fig.
210	39	One possible explanation is that the initial meta training loss of small training steps (e.g., (a)) is still very high as you can see from the log y-axis whereas the one with large training step, e.g., (d) is much lower.
