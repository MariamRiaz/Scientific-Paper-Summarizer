28	3	Owing to its gating mechanism, the minimalRNN on the other hand enjoys a robust multi-dimensional subspace of good initializations which all enable dynamical isometry.
45	1	We begin by developing a mean field theory for vanilla RNNs and discuss the notion of dynamical isometry.
49	1	R. For the purposes of this discussion we set = tanh.
65	2	deriving the recurrence relation of the covariance matrix qt from the recurrence on et in eq.
71	1	By contrast in recurrent networks, inputs perturb ct at each timestep.
96	1	In particular, dynamical isometry is achieved with orthogonal state-to-state transition matrices W , tanh non-linearities, and small values of q⇤.
107	1	The minimalRNN retains the most essential gate in LSTMs (Jozefowicz et al., 2015; Greff et al., 2017) and achieves competitive performance.
134	1	In this case, the minimalRNN experiences an order-to-chaos phase transition when 1 = 1 at which point the maximum timescale over which signal can propagate goes to infinity.
138	2	In other words, gating allows for arbitrarily long term signal propagation in recurrent neural networks independent of ⌃12.
139	1	We explore agreement between our theory and MC simulations of the minimalRNN in fig.
159	1	Following (Pennington et al., 2017; 2018), we use tools from free probability theory to compute the variance 2JJT of the limiting spectral density of JJT ; however, unlike previous work, in our case the relevant matrices are not symmetric and therefore we must invoke tools from nonHermitian free probability, see (Cakmak, 2012) for a review.
160	1	As in previous section, we make the simplifying assumption that the weights are untied, relying on the same motivations given in section 3.1.
161	1	Using these tools, an un-illuminating calculation reveals that, ✓ 1 + T 2(µ1 s1)µ2 + 21 + 2 2 21 ◆ , (12) where, 1 = µ1 + µ2 (13) µ1 = Z Dz 2( p q⇤z + µb) 21 = µ 2 1 + Z Dz 4( p q⇤z + µb) µ2 = 2 w(Q ⇤ + R) Z Dz [ 0( p q⇤z + µb)] 2 22 = µ 2 2 + 4 w((Q ⇤)2 + R2) Z Dz [ 0( p q⇤z + µb)] 4 and s1 is the first term in the Taylor expansion of the Stransform of the eigenvalue distribution of WWT (Pennington et al., 2018).
166	1	This situation is entirely analogous to the feed-forward analysis of (Pennington et al., 2017; 2018).
201	1	Note that we are more interested in the training speed of these networks under different initialization conditions than the test accuracy.
218	1	We show that the minimalRNN achieves competitive performance despite its simplicity.
228	1	There is a gap in perplexity between the performance of LSTMs and minimalRNNs.
230	7	The same strategy is employed in GRUs and may cause a conflict between keeping longer-range memory and updating new information as was originally pointed out by Hochreiter & Schmidhuber (1997).
231	28	We have developed a theory of signal propagation for random vanilla RNNs and a simple gated RNNs.
232	217	We demonstrate rigorously that the theory predicts trainability of these networks and gating mechanisms allow for a significantly larger trainable region.
233	218	We are planning to extend the theory to more complicated RNN cells as well as RNNs with multiple layers.
