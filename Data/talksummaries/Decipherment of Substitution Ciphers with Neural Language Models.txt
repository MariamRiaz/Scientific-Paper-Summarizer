0	13	Breaking substitution ciphers recovers the plaintext from a ciphertext that uses a 1:1 or homophonic cipher key.
1	21	Previous work using pretrained language models (LMs) for decipherment use n-gram LMs (Ravi and Knight, 2011; Nuhn et al., 2013).
8	15	The beginning tokens in the ciphertext (f0) and plaintext (e0) are set to “$” denoting the beginning of a sentence.
13	4	Decipherment is then the task of finding the φ for which the probability of the deciphered text is maximized.
15	16	Finding this argmax is solved using a beam search algorithm (Nuhn et al., 2013) which incrementally finds the most likely substitutions using the language model scores as the ranking.
18	7	Consider a sequence S = w1, w2, w3, ..., wN .
23	39	EXT LIMITS determines which extensions should be allowed and EXT ORDER picks the next cipher symbol for extension.
24	19	The search continues after pruning: Hs ← HISTOGRAM_PRUNE(Ht).
26	9	Algorithm 1 Beam Search for Decipherment 1: function (BEAM SEARCH (EXT ORDER, EXT LIM- ITS)) 2: initialize sets Hs, Ht 3: CARDINALITY = 0 4: Hs.ADD((∅,0)) 5: while CARDINALITY < |Vf | do 6: f = EXT ORDER[CARDINALITY] 7: for all φ ∈ Hs do 8: for all e ∈ Ve do 9: φ’ := φ ∪ {(e, f)} 10: if EXT LIMITS(φ’) then 11: Ht.ADD(φ’,SCORE(φ’)) 12: HISTOGRAM PRUNE(Ht) 13: CARDINALITY = CARDINALITY + 1 14: Hs = Ht 15: Ht.CLEAR() 16: return WINNER(Hs) 3 Score Estimation (SCORE) Score estimation evaluates the quality of the partial hypotheses φ.
27	27	Using the example from Nuhn et al. (2014), consider the vocabularies Ve = {a, b, c, d} and Vf = {A,B,C,D}, extension order (B,A,C,D), and ciphertext $ ABDDCABCDADCABDC $.
28	19	Let φ = {(a,A), (b, B))} be the partial hypothesis.
29	135	Then SCORE(φ) scores this hypothesized partial decipherment (only A and B are converted to plaintext) using a pre-trained language model in the hypothesized plaintext language.
30	24	The initial rest cost estimator introduced by Nuhn et al. nuhnbeam computes the score of hypotheses only based on partially deciphered text that builds a shard of n adjacent solved symbols.
36	19	We address this issue with a new improved version of the rest cost estimator by supplementing the partial decipherment φ(fN1 ) with predicted plaintext text symbols using our neural language model (NLM).
37	6	Applying φ = {(a,A), (b, B))} to the ciphertext above, we get the following partial hypothesis: φ(fN1 ) = $a1b2...a6b7..a10..a13b14..$ We introduce a scoring function that is able to score the entire plaintext including the missing plaintext symbols.
38	62	First, we sample1 the plaintext symbols from the NLM at all locations depending on the deciphered tokens from the partial hypothesis φ such that these tokens maintain their respective positions in the sequence, and at the same time are sampled from the neural LM to fit (probabilistically) in this context.
42	23	Since more terms participate in the rest cost estimation with global context, we use the plaintext LM to provide us with a better rest cost in the beam search.
43	17	Alignment by frequency similarity (Yarowsky and Wicentowski, 2000) assumes that two forms belong to the same lemma when their relative frequency fits the expected distribution.
45	36	The closer this value to 0, the more likely it is that f is mapped to e. Thus given a φ with the SCORE(φ), the extension φ′ (Algo.
47	18	Our experimental evaluations show that the global rest cost estimator and the frequency matching heuristic contribute positively towards the accuracy of different ciphertexts.
48	76	We carry out 2 sets of experiments: one on letter based 1:1, and another on homophonic substitution ciphers.
49	100	We report Symbol Error Rate (SER) which is the fraction of characters in the deciphered text that are incorrect.
54	17	In this experiment we use a synthetic 1:1 letter substitution cipher dataset following Ravi and Knight (2008), Nuhn et al. (2013) and Hauer et al. (2014).
55	5	The text is from English Wikipedia articles about history3, preprocessed by stripping the text of all images, tables, then lower-casing all characters, and removing all non-alphabetic and non-space characters.
57	57	Fig 1 plots the results of our method for cipher lengths of 16, 32, 64, 128 and 256 alongside Beam 6-gram (the best performing model) model (Nuhn et al., 2013)
58	23	Zodiac-408, a homophonic cipher, is commonly used to evaluate decipherment algorithms.
59	15	Our neural LM model with global rest cost estimation and frequency matching heuristic with a beam size of 1M has SER of 1.2% compared to the beam search algorithm (Nuhn et al., 2013) with beam size of 10M with a 6-gram LM which gives an SER of 2%.
60	11	The improved beam search (Nuhn et al., 2014) with an 8-gram LM, however, gets 52 out of 54 mappings correct on the Zodiac-408 cipher.
61	5	Part 2 of the Beale Cipher is a more challenging homophonic cipher because of a much larger search space of solutions.
62	21	Nunh et al. (2014) were the first to automatically decipher this Beale Cipher.
