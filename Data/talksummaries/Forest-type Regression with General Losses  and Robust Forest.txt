21	29	In Section 1.1 and 1.2 we review random forest and adaptive nearest neighbors.
22	19	Section 2 introduces the general framework of forest-type regression.
23	46	In Section 3 we plug in robust regression loss functions to get robust forest algorithms.
25	21	Finally, we test our robust forests in Section 5 and show that they are always superior to the traditional formulation in the presence of outliers in both synthetic and real data set.
26	19	Following the notation of Breiman (2001), let θ be the random parameter determining how a tree is grown, and data (X,Y ) ∈ X × Y .
28	31	Then for every x ∈ X , there is exactly one leaf l such that x ∈ Rl.
29	86	For each tree T (θ), the prediction of a new data point X = x is the average of data values in leaf l(x, θ), that is, Ŷ (x, θ) = ∑n j=1 w(Xi, x, θ)Yi, where w(Xi, x, θ) = 1I{Xi∈Rl(x,θ)} #{j : Xj ∈ Rl(x,θ)} .
46	39	Classical random forest can be understood as an estimator of conditional mean E[Y |X].
59	22	In more general form, we have the following local regression problem: Ŷ (x) = argmin s∈F n∑ i=1 w(Xi, x)φ(s(Xi), Yi) (6) where w(Xi, x) is a local weight, F is a family of functions, and φ(·) is a general loss.
62	22	Algorithm 1 Forest-type regression Step 1: Calculate local weights w(Xi, x) using ensemble or trees.
70	17	Meinshausen (2006) proposed the quantile random forest which can extract the information of different quantiles rather than just predicting the average.
73	15	It is well known that the τ -th quantile of an (empirical) distribution is the constant that minimizes the (empirical) risk using τ - th quantile loss function ρτ (z) = z(τ −1I{z<0}) (Koenker, 2005).
74	36	Now let the loss function in Algorithm 1 be the quantile loss ρτ (·), F be the family of constant functions, and w(Xi, x) be random forest weights (3).
75	15	Solving the optimization problem Ŷτ (x) = argmin λ∈R n∑ i=1 w(Xi, x)ρτ (Yi − λ), we get the corresponding first order condition n∑ i=1 w(Xi, x)(τ − 1I {Yi − Ŷτ (x) < 0}) = 0.
79	13	Therefore, we have rediscovered quantile random forest from a totally different point of view as a local regression estimator with quantile loss function and random forest weights.
82	24	In Section 3.1 we choose the famous robust loss – (pseudo) Huber loss, and in Section 3.2, we further investigate a non-convex loss – Tukey’s biweight.
84	12	The penalty acts like squared error loss when the error is within [−δ, δ] but becomes linear outside this range.
85	25	In this way, it will penalize the outliers more lightly but still preserves more efficiency than absolute deviation when data is concentrated in the center and has light tails (e.g. Normal).
86	52	By plugging Huber loss into the FTR framework 1, we get a robust counterpart of random forest.
87	17	The estimating equation is n∑ i=1 wi(x) sign(Ŷ (x)− Yi) min(Ŷ (x)− Yi, δ) = 0.
90	58	(9) is very similar to that of square error loss if we define a new weight wpHi (x) = wi(x)√ 1 + ( ŶpH(x)−Yi δ )2 .
99	30	Define Kδ(y) = ∑n i=1 wiYi√ 1+( y−Yiδ ) 2∑n i=1 wi√ 1+( y−Yiδ ) 2 , Algorithm 2 pseudo-Huber loss (δ) Input: Test points {xj}mj=1, initial guess {Ŷ (0)(xj)}, local weights wi,j , training responses {Yi}ni=1, and error tolerance 0. while > 0 do (a) Update the weights w (k) i,j = wi,j√ 1 + ( Ŷ (k−1)(xj)−Yi δ )2 (b) Update the estimator Ŷ (k)(xj) = ∑n i=1 w (k) i,j Yi∑n i=1 w (k) i,j (c) Calculate error = 1 m m∑ j=1 ( Ŷ k(xj)− Ŷ (k−1)(xj) )2 (d) k ← k + 1 end while Output the pseudo-Huber estimator: ŶpH(xj) = Ŷ (k)(xj) where ∑n i=1 wi = 1.
117	13	In classical random forest, all the data with positive weights (3) are included when calculating the final estimator Ŷ (x).
134	34	In this section, we plug in the quantile loss, Huber loss and Tukey’s biweight loss into the general forest framework and compare these algorithms with random forest.
142	12	We also repeat the experiments for 20 times, and report the average mean squared error (MSE), mean absolute deviation (MAD) and median absolute percentage error (MAPE) in Table 1.
148	64	On the clean data, random forest still play the best, however, Huber forest’s performance is also competitive and lose less efficiency than QRF and Tukey forest.
149	40	On the noisy data, all three robust methods outperform random forest.
152	21	We consider the same two models (1) and (2), and keep both training sample size and testing sample size to be 1000.
162	52	The experimental results show that Huber forest, Tukey forest and quantile random forest are all much more robust than random forest in the presence of outliers.
165	34	Besides random forest weights, other data dependent similarities could also be used in Algorithm 1.
