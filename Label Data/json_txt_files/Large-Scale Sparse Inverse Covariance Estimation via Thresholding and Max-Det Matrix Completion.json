{"sections": [{"heading": "1. Introduction", "text": "Consider the problem of estimating an n \u00d7 n covariance matrix \u03a3 (or its inverse \u03a3\u22121) of a n-variate probability distribution from N independently and identically distributed samples x1,x2, . . . ,xN drawn from the same probability distribution. In applications spanning from computer vision, natural language processing, to economics (Li, 1994; Manning & Sch\u00fctze, 1999; Durlauf, 1993), the matrix \u03a3\u22121 is often sparse, meaning that its matrix elements are mostly\nMATLAB source code: http://alum.mit.edu/www/ ryz. 1Department of Industrial Engineering and Operations Research, University of California, Berkeley, USA. 2Department of Electrical Engineering and Computer Science, University of California, Berkeley, USA.. Correspondence to: R.Y. Zhang <ryz@berkeley.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nzero. For Gaussian distributions, the statistical interpretation of sparsity in \u03a3\u22121 is that most of the variables are pairwise conditionally independent (Meinshausen & B\u00fchlmann, 2006; Yuan & Lin, 2007; Friedman et al., 2008; Banerjee et al., 2008).\nImposing sparsity upon \u03a3\u22121 can regularize the associated estimation problem and greatly reduce the number of samples required. This is particularly important in highdimensional settings where n is large, often significantly larger than the number of samples N n. One popular approach regularizes the associated maximum likelihood estimation (MLE) problem by a sparsity-promoting `1 term, as in\nminimize X 0 trCX \u2212 log detX + \u03bb n\u2211 i=1 n\u2211 j=1 |Xi,j |. (1)\nHere, C = 1N \u2211N i=1(xi \u2212 x\u0304)(xi \u2212 x\u0304)T is the sample\ncovariance matrix with sample mean x\u0304 = 1N \u2211N i=1 xi, and X is the resulting estimator for \u03a3\u22121. This approach, commonly known as the graphical lasso (Friedman et al., 2008), is known to enjoy a number of statistical guarantees (Rothman et al., 2008; Ravikumar et al., 2011), some of which are direct extensions of earlier work on the classical lasso (Obozinski et al., 2008; Negahban & Wainwright, 2008; Wainwright, 2009; Huang & Zhang, 2010). A variation on this theme is to only impose the `1 penalty on the off-diagonal elements of X , or to place different weights \u03bb on the elements of the matrix X , as in the classical weighted lasso.\nWhile the `1-regularized problem (1) is technically convex, it is commonly considered intractable for large-scale datasets. The decision variable is an n\u00d7nmatrix, so simply fitting all O(n2) variables into memory is already a significant issue. General-purpose algorithms have either prohibitively high complexity or slow convergence. In practice, (1) is solved using problem-specific algorithms. The state-of-the-art include GLASSO (Friedman et al., 2008), QUIC (Hsieh et al., 2014), and its \u201cbig-data\u201d extension BIG-QUIC (Hsieh et al., 2013). These algorithms use between O(n) and O(n3) time and between O(n2) and O(n) memory per iteration, but the number of iterations needed to converge to an accurate solution can be very large."}, {"heading": "1.1. Graphical lasso, soft-thresholding, and MDMC", "text": "The high practical cost of graphical lasso has inspired a number of heuristics, which enjoy less guarantees but are significantly cheaper to use. Indeed, heuristics are often the only viable option once n exceeds the order of a few tens of thousands.\nOne simple idea is to threshold the sample covariance matrix C: to examine all of its elements and keep only the ones whose magnitudes exceed some threshold. In a recent line of work (Mazumder & Hastie, 2012; Sojoudi, 2016; Fattahi & Sojoudi, 2017; Fattahi et al., 2018), this simple heuristic was shown to enjoy some surprising guarantees. In particular, (Sojoudi, 2016; Fattahi & Sojoudi, 2017) proved that when the lasso weight is imposed over only the off-diagonal elements of X that\u2014under some assumptions\u2014the sparsity pattern of the associated graphical lasso estimator can be recovered by performing a softthresholding operation on C, as in\n(C\u03bb)i,j =  Ci,j i = j,\nCi,j \u2212 \u03bb Ci,j > \u03bb, i 6= j, 0 |Ci,j | \u2264 \u03bb i 6= j, Ci,j + \u03bb \u2212\u03bb \u2264 Ci,j i 6= j,\n(2)\nand recovering the sparsity pattern\nG = {(i, j) \u2208 {1, . . . , n}2 : (C\u03bb)i,j 6= 0}. (3)\nThe associated graph (also denoted as G when there is no ambiguity) is obtained by viewing each nonzero element (i, j) in G as an edge between the i-th and j-th vertex in an undirected graph on n nodes. Moreover, they showed that the estimator X can be recovered by solving a version of (1) in which the sparsity pattern G is explicitly imposed, as in\nminimize X 0 trC\u03bbX \u2212 log detX (4)\nsubject to Xi,j = 0 \u2200(i, j) /\u2208 G.\nRecovering the exact value of X (and not just its sparsity pattern) is important because it provides a shrinkage MLE when the true MLE is ill-defined; for Gaussian fields, its nonzero values encode the partial correlations between variables. Problem (4) is named the maximum determinant matrix completion (MDMC) in the literature, for reasons explained below. The problem has a recursive closed-form solution whenever the graph of G is acyclic (i.e. a tree or forest) (Fattahi & Sojoudi, 2017), or more generally, if it is chordal (Fattahi et al., 2018). It is worth emphasizing that the closed-form solution is extremely fast to evaluate: a chordal example in (Fattahi et al., 2018) with 13,659 variables took just \u2248 5 seconds to solve on a laptop computer.\nThe assumptions needed for graphical lasso to be equivalent to thresolding are hard to check but relatively mild.\nIndeed, (Fattahi & Sojoudi, 2017) proves that they are automatically satisfied whenever \u03bb is sufficiently large relative to the sample covariance matrix. Their numerical study found \u201csufficiently large\u201d to be a fairly loose criterion in practice, particularly in view of the fact that large values of \u03bb are needed to induce a sufficiently sparse estimate of \u03a3\u22121, e.g. with \u2248 10n nonzero elements.\nHowever, the requirement for G to be chordal is very strong. Aside from trivial chordal graphs like trees and cliques, thresholding will produce a chordal graph with probability zero. When G is nonchordal, no closed-form solution exists, and one must resort to an iterative algorithm. The state-of-the-art for nonchordal MDMC is to embed the nonchordal graph within a chordal graph, and to solve the resulting problem as a semidefinite program using an interior-point method."}, {"heading": "1.2. Main results", "text": "The purpose of this paper is two-fold. First, we derive an extension of the guarantees derived by (Mazumder & Hastie, 2012; Sojoudi, 2016; Fattahi & Sojoudi, 2017; Fattahi et al., 2018) for a slightly more general version of the problem that we call restricted graphical lasso (RGL):\nX\u0302 = minimize X 0 trCX \u2212 log detX (5)\n+ n\u2211 i=1 n\u2211 j=i+1 \u03bbi,j |Xi,j |.\nsubject to Xi,j = 0 \u2200(i, j) /\u2208 H.\nIn other words, RGL is (1) penalized by a weighted lasso penalty \u03bbi,j on the off-diagonals, and with an a priori sparsity pattern H imposed as an additional constraint. We use the sparsity pattern H to incorporate prior information on the structure of the graphical model. For example, if the sample covariance C is collected over a graph, such as a communication system or a social network, then far-away variables can be assumed as pairwise conditionally independent (Park & Rilett, 1999; Honorio et al., 2009; Croft et al., 2010). Including these neighborhood relationships into H can regularize the statistical problem, as well as reduce the numerical cost for a solution.\nIn Section 2, we describe a procedure to transform RGL (5) into MDMC (4), in the same style as prior results by (Fattahi & Sojoudi, 2017; Fattahi et al., 2018) for graphical lasso. More specifically, we soft-threshold the sample covariance C and then project this matrix onto the sparsity pattern H . We give conditions for the resulting sparsity pattern to be equivalent to the one obtained by solving (5). Furthermore, we prove that the resulting estimator X can be recovered by solving the same MDMC problem (4) with C\u03bb appropriately modified.\nThe second purpose is to describe an efficient algorithm to solve MCDC when the graph G is nonchordal, based on the chordal embedding approach of (Dahl et al., 2008; Andersen et al., 2010; 2013b). We embed G within a chordal G\u0303 \u2283 G, to result in a convex optimization problem over Sn G\u0303\n, the space of real symmetric matrices with sparsity pattern G\u0303. This way, the constraint X \u2208 Sn\nG\u0303 is implicitly\nimposed, meaning that we simply ignore the nonzero elements not in G\u0303. Next, we solve an optimization problem on Sn\nG\u0303 using a custom Newton-CG method. The main idea is to use an inner conjugate gradients (CG) loop to solve the Newton subproblem of an outer Newton\u2019s method. The actual algorithm has a number of features designed to exploit problem structure, including the sparse chordal property of G\u0303, duality, and the ability for CG and Newton to converge superlinearly; these are outlined in Section 3.\nAssuming that the chordal embedding is sparse with |G\u0303| = O(n) nonzero elements, we prove in Section 3.4, that our algorithm converges to an -accurate solution of MDMC (4) in\nO(n \u00b7 log \u22121 \u00b7 log log \u22121) time and O(n) memory. (6)\nMost importantly, the algorithm is highly efficient in practice. In Section 4, we present computation results on a suite of test cases. Both synthetic and real-life graphs are considered. Using our approach, we solve sparse inverse covariance estimation problems containing as many as 200,000 variables, in less than an hour on a laptop computer."}, {"heading": "1.3. Related Work", "text": "Graphical lasso with prior information. A number of approaches are available in the literature to introduce prior information to graphical lasso. The weighted version of graphical lasso mentioned before is an example, though RGL will generally be more efficient to solve due to a reduction in the number of variables. (Egilmez et al., 2017) introduced a class of graphical lasso in which the true graphical model is assumed to have Laplacian structure. This structure commonly appears in signal and image processing (Milanfar, 2013). For the a priori graph-based correlation structure described above, (Grechkin et al., 2015) introduced a pathway graphical lasso method similar to RGL.\nAlgorithms for graphical lasso. Algorithms for graphical lasso are usually based on some mixture of Newton (Oztoprak et al., 2012), proximal Newton (Hsieh et al., 2013; 2014), iterative thresholding (Rolfs et al., 2012), and (block) coordinate descent (Friedman et al., 2008; Treister & Turek, 2014). All of these suffer fundamentally from the need to keep track and act on allO(n2) elements in the matrix X decision variable. Even if the final solution matrix were sparse with O(n) nonzeros, it is still possible for the\nalgorithm to traverse through a \u201cdense region\u201d in which the iterateX must be fully dense. Thresholding heuristics have been proposed to address issue, but these may adversely affect the outer algorithm and prevent convergence. It is generally impossible to guarantee a figure lower than O(n2) time per-iteration, even if the solution contains only O(n) nonzeros. Most of the algorithms mentioned above actually have worst-case per-iteration costs of O(n3).\nGraphical lasso via thresholding. The elementary estimator for graphical models (EE-GM) (Yang et al., 2014) is another thresholding-based low-complexity method that is able to recover the actual graphical lasso estimator. Both EE-GM and our algorithm have a similar level of performance in practice, because both algorithm are bottlenecked by the initial thresholding step, which is a quadratic O(n2) time operation.\nAlgorithms for MDMC. Our algorithm is inspired by a line of results (Dahl et al., 2008; Andersen et al., 2010; 2013b; Li et al., 2017) for minimizing the log-det penalty on chordal sparsity patterns, culminating in the CVXOPT package (Andersen et al., 2013a). These algorithms all solve the Newton subproblem by explicitly forming and factoring the fully-dense Newton matrix in O(nm2 +m3) time, where m = |G\u0303\\G| is the number of edges added during chordal embedding. By comparison, our algorithm solves the Newton subproblem iteratively using CG, in O(n+m) time to machine precision (see Section 3.4).\nNotations\nLet Rn and Sn be the set of n\u00d71 real vectors, and n\u00d7n real symmetric matrices. We endow Sn with the usual matrix inner product X \u2022 Y = trXY and Euclidean (i.e. Frobenius) norm \u2016X\u20162F = X \u2022X . Let Sn+ \u2282 Sn and Sn++ \u2282 Sn+ be the associated set of positive semidefinite and positive definite matrices. We will frequently write X 0 to mean X \u2208 Sn+ and write X 0 to mean X \u2208 Sn++. Given a sparsity pattern G, we define SnG \u2286 Sn as the set of n \u00d7 n real symmetric matrices with this sparsity pattern."}, {"heading": "2. Restricted graphical lasso, soft-thresholding, and MDMC", "text": "Let PH(X) denote the projection operator from Sn onto SnH , i.e. by setting all Xi,j = 0 if (i, j) /\u2208 H . Let C\u03bb be the sample covariance matrix C individually soft-thresholded by [\u03bbi,j ], as in\n(C\u03bb)i,j =  Ci,j i = j,\nCi,j \u2212 \u03bbi,j Ci,j > \u03bbi,j , i 6= j, 0 |Ci,j | \u2264 \u03bbi,j i 6= j, Ci,j + \u03bbi,j \u2212\u03bbi,j \u2264 Ci,j i 6= j, (7)\nIn this section, we state the conditions for PH(C\u03bb)\u2014the projection of the soft-thresholded matrix C\u03bb in (7) onto H\u2014to have the same sparsity pattern as the RGL estimator X\u0302 in (5). Furthermore, the estimator X\u0302 can be explicitly recovered by solving the MDMC problem (4) while replacing C\u03bb \u2190 PH(C\u03bb) and G \u2190 PH(G). For brevity, all proofs and remarks are omitted; these can be found in the supplementary materials.\nBefore we state the exact conditions, we begin by adopting the some definitions and notations from the literature.\nDefinition 1. (Fattahi & Sojoudi, 2017) Given a matrix M \u2208 Sn, define GM = {(i, j) : Mi,j 6= 0} as its sparsity pattern. Then M is called inverse-consistent if there exists a matrix N \u2208 Sn such that\nM +N 0 (8a) N = 0 \u2200(i, j) \u2208 GM (8b) (M +N)\u22121 \u2208 SnGM (8c)\nThe matrix N is called an inverse-consistent complement of M and is denoted by M (c). Furthermore, M is called sign-consistent if for every (i, j) \u2208 GM , the (i, j)-th elements of M and (M +M (c))\u22121 have opposite signs.\nMoreover, we take the usual matrix max-norm to exclude the diagonal, as in \u2016M\u2016max = maxi 6=j |Mij |, and adopt the \u03b2(G,\u03b1) function defined with respect to the sparsity pattern G and scalar \u03b1 > 0\n\u03b2(G,\u03b1) = max M 0 \u2016M (c)\u2016max\ns.t. M \u2208 SnG and \u2016M\u2016max \u2264 \u03b1 Mi,i = 1 \u2200i \u2208 {1, . . . , n} M is inverse-consistent.\nWe are now ready to state the conditions for softthresholding to be equivalent to RGL.\nTheorem 2. Define C\u03bb as in (7), define CH = PH(C\u03bb) and let GH = {(i, j) : (CH)i,j 6= 0} be its sparsity pattern. Then GH coincides with sparsity pattern of the optimal solution X\u0302 of RGL (5) if the normalized matrix C\u0303 = D\u22121/2CHD\n\u22121/2 where D = diag(CH) satisfies the following conditions:\n1. C\u0303 is positive definite,\n2. C\u0303 is sign-consistent, 3. Let \u03b2H = \u03b2 ( GH , \u2016C\u0303\u2016max ) . Then\n\u03b2H \u2264 min (k,l)/\u2208GH \u03bbk,l \u2212 |(CH)k,l|\u221a (CH)k,k \u00b7 (CH)l,l\n(9)\nProof. See supplementary materials.\nTheorem 2 leads to the following corollary, which asserts that the optimal solution of RGL can be obtained by maximum determinant matrix completion: computing the matrix Z 0 with the largest determinant that \u201cfills-in\u201d the zero elements of PH(C\u03bb).\nCorollary 3. Suppose that the conditions in Theorem 2 are satisfied. Define Z\u0302 as the solution to the following\nZ\u0302 = maximize Z 0 log detZ (10)\nsubject to Zi,j = PH(C\u03bb) for all (i, j)\nwhere [PH(C\u03bb)]i,j 6= 0\nThen Z\u0302 = X\u0302\u22121, where X\u0302 is the solution of (5).\nProof. See supplementary materials.\nStandard manipulations show that (10) is the Lagrangian dual of (4), thus explaining the etymology of (4) as MDMC."}, {"heading": "3. Proposed Algorithm", "text": "This section describes an efficient algorithm to solve MDMC (4) in which the sparsity pattern G is nonchordal. If we assume that the input matrix C\u03bb is sparse, and that sparse Cholesky factorization is able to solve C\u03bbx = b in O(n) time, then our algorithm is guaranteed to compute an -accurate solution in O(n log \u22121) time and O(n) memory.\nThe algorithm is fundamentally a Newton-CG method, i.e. Newton\u2019s method in which the Newton search directions are computed using conjugate gradients (CG). It is developed from four key insights:\n1. Chordal embedding is easy via sparse matrix heuristics. State-of-the-art algorithms for (4) begin by computing a chordal embedding G\u0303 forG. The optimal chordal embedding with the fewest number of nonzeros |G\u0303| is NP-hard to compute, but a good-enough embedding with O(n) nonzeros is sufficient for our purposes. Computing a good G\u0303 with |G\u0303| = O(n) is exactly the same problem as finding a sparse Cholesky factorization C\u03bb = LLT with O(n) fillin. Using heuristics developed for numerical linear algebra, we are able to find sparse chordal embeddings for graphs containing millions of edges and hundreds of thousands of nodes in seconds.\n2. Optimize directly on the sparse matrix cone. Using log-det barriers for sparse matrix cones (Dahl et al., 2008; Andersen et al., 2010; 2013b; Vandenberghe et al., 2015), we can optimize directly in the space Sn\nG\u0303 , while ignoring\nall matrix elements outside of G\u0303. If |G\u0303| = O(n), then only O(n) decision variables must be explicitly optimized.\nMoreover, each function evaluation, gradient evaluation, and matrix-vector product with the Hessian can be performed in O(n) time, using the numerical recipes in (Andersen et al., 2013b).\n3. The dual is easier to solve than the primal. The primal problem starts with a feasible point X \u2208 Sn\nG\u0303 and seeks\nto achieve first-order optimality. The dual problem starts with an infeasible optimal point X /\u2208 Sn\nG\u0303 satisfying first-\norder optimality, and seeks to make it feasible. Feasibility is easier to achieve than optimality, so the dual problem is easier to solve than the primal.\n4. Conjugate gradients (CG) converges in O(1) iterations. Our main result (Theorem 6) bounds the condition number of the Newton subproblem to be O(1), independent of the problem dimension n and the current accuracy . It is therefore cheaper to solve this subproblem using CG to machine precision \u03b4mach inO(n log \u03b4\u22121mach) time than it is to solve for it directly in O(nm2 + m3) time using Cholesky factorization (Dahl et al., 2008; Andersen et al., 2010; 2013b). Moreover, CG is an optimal Krylov subspace method, and as such, it is often able to exploit clustering in the eigenvalues to converge superlinearly. Finally, computing the Newton direction to high accuracy further allows the outer Newton method to also converge quadratically.\nThe remainder of this section describes each consideration in further detail. We state the algorithm explicitly in Section 3.5."}, {"heading": "3.1. Efficient chordal embedding", "text": "Following (Dahl et al., 2008), we begin by reformulating (4) into a sparse chordal matrix program\nX\u0302 = minimize trCX \u2212 log detX (11) subject to Xi,j = 0 \u2200(i, j) \u2208 G\u0303\\G.\nX \u2208 Sn G\u0303 .\nin which G\u0303 is a chordal embedding forG: a sparsity pattern G\u0303 \u2283 G whose graph contains no induced cycles greater than three. This can be implemented using standard algorithms for large-and-sparse linear equations, due to the following result. Proposition 4. Let C \u2208 SnG be a positive definite matrix with sparsity pattern G. Compute its unique lowertriangular Cholesky factor L satisfying C = LLT . Ignoring perfect numerical cancellation, the sparsity pattern of L+ LT is a chordal embedding G\u0303 \u2283 G.\nProof. The original proof is due to (Rose, 1970); see also (Vandenberghe et al., 2015).\nNote that G\u0303 can be determined directly from G using a\nsymbolic Cholesky algorithm, which simulates the steps of Gaussian elimination using Boolean logic. Moreover, we can substantially reduce the number of edges added toG by reordering the columns and rows of C using a fill-reducing ordering.\nCorollary 5. Let \u03a0 be a permutation matrix. For the same C \u2208 SnG in Proposition 4, compute the unique Cholesky factor satisfying \u03a0C\u03a0T = LLT . Ignoring perfect numerical cancellation, the sparsity pattern of \u03a0(L + LT )\u03a0T is a chordal embedding G\u0303 \u2283 G.\nThe problem of finding the best choice of \u03a0 is known as the fill-minimizing problem, and is NP-complete (Yannakakis, 1981). However, good orderings are easily found using heuristics developed for numerical linear algebra, like minimum degree ordering (George & Liu, 1989) and nested dissection (Gilbert, 1988; Agrawal et al., 1993). If G admits sparse chordal embeddings, then a good-enough |G\u0303| = O(n) will usually be found using a simple minimum degree ordering; see the MATLAB code snippet in Figure 1."}, {"heading": "3.2. Logarithmic barriers for sparse matrix cones", "text": "Define the cone of sparse positive semidefinite matrices K, and the cone of sparse matrices with positive semidefinite completions K\u2217, as the following\nK = Sn+ \u2229 SnG\u0303, K\u2217 = {S \u2022X \u2265 0 : S \u2208 SG\u0303} = PG\u0303(S n +).\nThen (11) can be posed as the primal-dual pair:\narg min X\u2208K {C \u2022X + f(X) : AT (X) = 0}, (12)\narg max S\u2208K\u2217,y\u2208Rm\n{\u2212f\u2217(S) : S = C \u2212A(y)}, (13)\nwhere the linear map A : Rm \u2192 Sn G\u0303\\G converts a list of m variables into the corresponding matrix in G\u0303\\G, and f and f\u2217 are the \u201clog-det\u201d barrier functions onK andK\u2217 as introduced by (Dahl et al., 2008; Andersen et al., 2010; 2013b)\nf(X) = \u2212 log detX, f\u2217(S) = \u2212 min X\u2208K {S \u2022X + f(X)}.\nAssuming that G\u0303 is sparse and chordal, the functions f and f\u2217, their gradient evaluations, and Hessian matrixvector products can all be efficiently evaluated in O(n) time and O(n) memory, using the numerical recipes described in (Andersen et al., 2013b)."}, {"heading": "3.3. Solving the dual problem", "text": "Our algorithm actually solves the dual problem (13), which can be rewritten as an unconstrained optimization problem\ny\u0302 \u2261 arg min y\u2208Rm g(y) \u2261 f\u2217(C\u03bb \u2212A(y)). (14)\nAfter the solution y\u0302 is found, we can recover the optimal estimator for the primal problem via X\u0302 = \u2212\u2207f\u2217(C\u03bb\u2212A(y)). The dual problem (13) is easier to solve than the primal (12) because the origin y = 0 often lies very close to the solution y\u0302. To see this, note that y = 0 produces a candidate estimator X\u0303 = \u2212\u2207f\u2217(C\u03bb) that solves the chordal matrix completion problem\nX\u0303 = arg min{trC\u03bbX \u2212 log detX : X \u2208 SnG\u0303},\nwhich is a relaxation of the nonchordal problem posed over SnG. As observed by previous authors (Dahl et al., 2008), this relaxation is a high quality guess, and X\u0303 is often \u201calmost feasible\u201d for the original nonchordal problem posed over SnG, as in X\u0303 \u2248 PG(X\u0303). Some simple algebra shows that \u2016\u2207g(0)\u2016 = \u2016X\u0303 \u2212PG(X\u0303)\u2016F , so if X\u0303 \u2248 PG(X\u0303) holds true, then the origin y = 0 is close to optimal. Starting from this point, we can expect Newton\u2019s method to rapidly converge at a quadratic rate.\n3.4. CG converges in O(1) iterations\nThe most computationally expensive part of Newton\u2019s method is the solution of the Newton direction \u2206y via the m\u00d7m system of equations\n\u22072g(y)\u2206y = \u2212\u2207g(y). (15)\nThe Hessian matrix \u22072g(y) is fully dense, but matrixvector products are linear O(n) time using the algorithms in Section 3.2. This insight motivates solving (15) using an iterative Krylov subspace method like conjugate gradients (CG), which is a matrix-free method that requires a single matrix-vector product with \u22072g(y) at each iteration (Barrett et al., 1994). Starting from the origin p = 0, the method converges to an -accurate search direction p satisfying\n(p\u2212\u2206y)T\u22072g(y)(p\u2212\u2206y) \u2264 |\u2206yT\u2207g(y)|\nin at most \u2308\u221a \u03bag log(2/ ) \u2309 CG iterations, (16)\nwhere \u03bag = \u2016\u22072g(y)\u2016\u2016\u22072g(y)\u22121\u2016 is the condition number of the Hessian matrix (Greenbaum, 1997; Saad, 2003).\nBelow, we state our main result, which says that the condition number \u03bag depends polynomially on the problem data and the quality of the initial point, but is independent of the problem dimension n and the accuracy of the current iterate .\nTheorem 6. At any y satisfying g(y) \u2264 g(y0) and \u2207g(y)T (y \u2212 y0) \u2264 \u03c6max, the condition number \u03bag of the Hessian matrix\u22072g(y) is bound\n\u03bag \u2264 4 ( 1 + \u03c62max\u03bbmax(X0)\n\u03bbmin(X\u0302)\n)2 . (17)\nwhere \u03c6max = g(y0)\u2212 g(y\u0302) is the initial infeasibility, A = [vecA1, . . . , vecAm] is the vectorized data matrix, X0 = \u2212\u2207f\u2217(C \u2212A(y0)), and X\u0302 = \u2212\u2207f\u2217(C \u2212A(y\u0302)).\nProof. See supplementary materials.\nRemark 7. Newton\u2019s method is a descent method, so its kth iterate yk trivially satisfies g(yk) \u2264 g(y0). Technically, the condition\u2207g(yk)T (yk\u2212y0) \u2264 \u03c6max can be guaranteed by enclosing Newton\u2019s method within an outer auxillary path-following loop; see Section 4.3.5 of (Nesterov, 2013). In practice, naive Newton\u2019s method will usually satisfy the condition on its own; see our numerical experiments in Section 4.\nApplying Theorem 6 to (16) shows that CG solves each Newton subproblem to -accuracy in O(log \u22121) iterations. Multiplying this figure by the O(log log \u22121) Newton steps to converge yields a global iteration bound of O(log \u22121 \u00b7log log \u22121) \u2248 O(1) CG iterations. Multiplying this figure by theO(n) cost of each CG iteration proves the claimed time complexity in (6). In practice, CG typically converges much faster than this worst-case bound, due to its ability to exploit the clustering of eigenvalues in \u22072g(y); see (Greenbaum, 1997; Saad, 2003). Moreover, accurate Newton directions are only needed to guarantee quadratic convergence close to the solution. During the initial Newton steps, we may loosen the error tolerance for CG for a significant speed-up. Inexact Newton steps can be used to obtain a speed-up of a factor of 2-3."}, {"heading": "3.5. The full algorithm", "text": "To summarize, we begin by computing a chordal embedding G\u0303 for the sparsity pattern G of C\u03bb, using the code snippet in Figure 1. We use the embedding to reformulate (4) as (11), and solve the unconstrained problem y\u0302 = miny g(y) defined in (14), using Newton\u2019s method\nyk+1 = yk + \u03b1k\u2206yk, \u2206yk \u2261 \u2212\u22072g(yk)\u22121\u2207g(yk)\nstarting at the origin y0 = 0. The function value g(y), gradient \u2207g(y) and Hessian matrix-vector products are all\nevaluated using the numerical recipes described by (Andersen et al., 2013b).\nAt each k-th Newton step, we compute the Newton search direction \u2206yk using conjugate gradients. A loose tolerance is used when the Newton decrement \u03b4k = |\u2206yTk\u2207g(yk)| is large, and a tight tolerance is used when the decrement is small, implying that the iterate is close to the true solution. Once a Newton direction \u2206yk is computed with a sufficiently large Newton decrement \u03b4k, we set the step-size \u03b1k to be the first instance of the sequence {1, \u03c1, \u03c12, \u03c13, . . . } that satisfies the Armijo\u2013Goldstein condition\ng(y + \u03b1\u2206y) \u2264 g(y) + \u03b3\u03b1\u2206yT\u2207g(y),\nin which \u03b3 \u2208 (0, 0.5) and \u03c1 \u2208 (0, 1) are line search parameters. Our implementation used \u03b3 = 0.01 and \u03c1 = 0.5. We complete the step and repeat the process, until convergence.\nWe terminate the outer Newton\u2019s method if the Newton decrement \u03b4k falls below a threshold. This implies either that the solution has been reached, or that CG is not converging to a good enough \u2206yk to make significant progress. The associated estimator for \u03a3\u22121 is recovered by evaluating X\u0302 = \u2212\u2207f\u2217(C\u03bb \u2212A(y\u0302))."}, {"heading": "4. Numerical Results", "text": "Finally, we benchmark our algorithm against QUIC (Hsieh et al., 2014), commonly considered the fastest solver for graphical lasso or RGL1. We consider two case studies. The first case study numerically verifies the claimed O(n) complexity of our MDMC algorithm on problems with a nearly-banded structure. The second case study performs the full threshold-MDMC procedure for graphical lasso and RGL, on graphs collected from real-life applications. All experiments are performed on a laptop computer with an Intel Core i7 quad-core 2.50 GHz CPU and 16GB RAM. The reported results are based on a serial implementation in MATLAB-R2017b. Both our Newton decrement threshold and QUIC\u2019s convergence threshold are 10\u22127."}, {"heading": "4.1. Case Study 1: Banded Patterns", "text": "The first case study aims to verify the claimed O(n) complexity of our algorithm for MDMC. Here, we avoid the proposed thresholding step, and focus solely on the MDMC (4) problem. Each sparsity pattern G is a corrupted banded matrices with bandwidth 101. The off-diagonal nonzero elements of C are selected from the uniform distribution in [\u22122, 0) and then corrupted to zero with probability 0.3. The diagonal elements are fixed to 5. Our numerical experiments fix the bandwidth and vary the number of variables n from 1,000 to 200,000. A time limit of 2 hours is set for both algorithms.\nFigure 2 compares the running time of both algorithms. A log-log regression results in an empirical time complexity ofO(n1.1) for our algorithm, andO(n2) for QUIC. The extra 0.1 in the exponent is most likely an artifact our MATLAB implementation. In either case, QUIC\u2019s quadratic complexity limits it to n = 1.5 \u00d7 104. By contrast, our algorithm solves an instance with n = 2\u00d7 105 in less than 33 minutes. The resulting solutions are extremely accurate, with optimality and feasibility gaps of less than 10\u221216 and 10\u22127, respectively."}, {"heading": "4.2. Case Study 2: Real-Life Graphs", "text": "The second case study aims to benchmark the full thresholding-MDMC procedure for sparse inverse covariance estimation on real-life graphs. The actual graphs (i.e. the sparsity patterns) for \u03a3\u22121 are chosen from SuiteSparse Matrix Collection (Davis & Hu, 2011)\u2014a publicly available dataset for large-and-sparse matrices collected from real-world applications. Our chosen graphs vary in size from n = 3918 to n = 201062, and are taken from ap-\n1Two other widely-used algorithms are GLASSO (Friedman et al., 2008) and BIGQUIC (Hsieh et al., 2013). On a serial machine and for the problem sizes that we consider, we found both to be slower than QUIC.\nplications in chemical processes, material science, graph problems, optimal control and model reduction, thermal processes and circuit simulations.\nFor each sparsity pattern G, we design a corresponding \u03a3\u22121 as follows. For each (i, j) \u2208 G, we select (\u03a3\u22121)i,j = (\u03a3\u22121)j,i from the uniform distribution in [\u22121, 1], and then corrupt it to zero with probability 0.3. Then, we set each diagonal to (\u03a3\u22121)i,i = 1 + \u2211 j |(\u03a3\u22121)i,j |. Using this \u03a3, we generate N = 5000 samples i.i.d. as x1, . . . ,xN \u223c N (0,\u03a3). This results in a sample covariance matrix C = 1 N \u2211N i=1 xix T i .\nWe solve graphical lasso and RGL with the C described above using our proposed soft-thresholding-MDMC algorithm and QUIC, in order to estimate \u03a3\u22121. In the case of RGL, we assume that the graph G is known a priori, while noting that 30% of the elements of \u03a3\u22121 have been corrupted to zero. Our goal here is to discover the location of these corrupted elements. In all of our simulations, the threshold \u03bb is set so that the number of nonzero elements in the the estimator is roughly the same as the ground truth. We limit both algorithms to 3 hours of CPU time.\nFigure 3 compares the CPU time of both two algorithms for this case study; the specific details are provided in Table 1. A log-log regression results in an empirical time complexity of O(n1.64) and O(n1.55) for graphical lasso and RGL using our algorithm, and O(n2.46) and O(n2.52) for the same using QUIC. The exponents of our algorithm are \u2265 1 due to the initial soft-thresholding step, which is quadratic-time on a serial computer, but \u2264 2 because procedure is dominated by the solution of the MDMC. Both algorithms solve graphs with n \u2264 1.5 \u00d7 104 within the allotted time limit, though our algorithm is 11 times faster on average. Only our algorithm is able to solve the estimation problem with n \u2248 2\u00d7 105 in a little more than an hour.\nTo check whether thresholding-MDMC really does solve graphical lasso and RGL, we substitute the two sets of estimators back into their original problems (1) and (5). The corresponding objective values have a relative difference \u2264 4 \u00d7 10\u22124, suggesting that both sets of estimators are about equally optimal. This observation verifies our claims in Theorem 2 and Corollary 3 that (1) and (5): thresholding-MDMC does indeed solve graphical lasso and RGL."}, {"heading": "5. Conclusions", "text": "Graphical lasso is a widely-used approach for estimating a covariance matrix with a sparse inverse from limited samples. In this paper, we consider a slightly more general formulation called restricted graphical lasso (RGL), which additionally enforces a prior sparsity pattern to the estimation. We describe an efficient approach that substantially reduces the cost of solving RGL: 1) soft-thresholding the sample covariance matrix and projecting onto the prior pattern, to recover the estimator\u2019s sparsity pattern; and 2) solving a maximum determinant matrix completion (MDMC) problem, to recover the estimator\u2019s numerical values. The first step is quadratic O(n2) time and memory but embarrassingly parallelizable. If the resulting sparsity pattern is sparse and chordal, then the second step can be performed using the Newton-CG algorithm described in this paper in linear O(n) time and memory. The algorithm is tested on both synthetic and real-life data, solving instances with as many as 200,000 variables to 7-9 digits of accuracy within an hour on a standard laptop computer.\nAcknowledgements. This work was supported by the ONR grants N00014-17-1-2933 and N00014-15-1-2835, DARPA grant D16AP00002, and AFOSR grant FA955017-1-0163."}], "year": 2018, "references": [{"title": "Cutting down on fill using nested dissection: Provably good elimination orderings", "authors": ["A. Agrawal", "P. Klein", "R. Ravi"], "venue": "In Graph Theory and Sparse Matrix Computation,", "year": 1993}, {"title": "Implementation of nonsymmetric interior-point methods for linear optimization over sparse matrix cones", "authors": ["M.S. Andersen", "J. Dahl", "L. Vandenberghe"], "venue": "Mathematical Programming Computation,", "year": 2010}, {"title": "CVXOPT: A Python package for convex optimization", "authors": ["M.S. Andersen", "J. Dahl", "L. Vandenberghe"], "venue": "Available at cvxopt. org,", "year": 2013}, {"title": "Logarithmic barriers for sparse matrix cones", "authors": ["M.S. Andersen", "J. Dahl", "L. Vandenberghe"], "venue": "Optimization Methods and Software,", "year": 2013}, {"title": "Model selection through sparse maximum likelihood estimation for multivariate Gaussian or binary data", "authors": ["O. Banerjee", "L.E. Ghaoui", "A. d\u2019Aspremont"], "venue": "Journal of Machine learning research,", "year": 2008}, {"title": "Templates for the solution of linear systems: building blocks for iterative methods, volume", "authors": ["R. Barrett", "M.W. Berry", "T.F. Chan", "J. Demmel", "J. Donato", "J. Dongarra", "V. Eijkhout", "R. Pozo", "C. Romine", "H. Van der Vorst"], "year": 1994}, {"title": "Reactome: a database of reactions, pathways and biological processes", "authors": ["D. Croft", "G. O\u2019Kelly", "G. Wu", "R. Haw", "M. Gillespie", "L. Matthews", "M. Caudy", "P. Garapati", "G. Gopinath", "B. Jassal", "S. Jupe"], "venue": "Nucleic acids research,", "year": 2010}, {"title": "Covariance selection for nonchordal graphs via chordal embedding", "authors": ["J. Dahl", "L. Vandenberghe", "V. Roychowdhury"], "venue": "Optimization Methods & Software,", "year": 2008}, {"title": "The university of florida sparse matrix collection", "authors": ["T.A. Davis", "Y. Hu"], "venue": "ACM Transactions on Mathematical Software (TOMS),", "year": 2011}, {"title": "Nonergodic economic growth", "authors": ["S.N. Durlauf"], "venue": "The Review of Economic Studies,", "year": 1993}, {"title": "Graph learning from data under laplacian and structural constraints", "authors": ["H.E. Egilmez", "E. Pavez", "A. Ortega"], "venue": "IEEE Journal of Selected Topics in Signal Processing,", "year": 2017}, {"title": "Graphical lasso and thresholding: Equivalence and closed-form solutions", "authors": ["S. Fattahi", "S. Sojoudi"], "venue": "https: //arxiv.org/abs/1708.09479,", "year": 2017}, {"title": "Sparse inverse covariance estimation for chordal structures", "authors": ["S. Fattahi", "R.Y. Zhang", "S. Sojoudi"], "venue": "https: //arxiv.org/abs/1711.09131,", "year": 2018}, {"title": "Sparse inverse covariance estimation with the graphical lasso", "authors": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "year": 2008}, {"title": "The evolution of the minimum degree ordering algorithm", "authors": ["A. George", "J.W. Liu"], "venue": "Siam review,", "year": 1989}, {"title": "Some nested dissection order is nearly optimal", "authors": ["J.R. Gilbert"], "venue": "Information Processing Letters,", "year": 1988}, {"title": "Pathway graphical lasso", "authors": ["M. Grechkin", "M. Fazel", "D.M. Witten", "Lee", "S.-I"], "year": 2015}, {"title": "Iterative methods for solving linear systems, volume 17", "authors": ["A. Greenbaum"], "year": 1997}, {"title": "Sparse and locally constant gaussian graphical models", "authors": ["J. Honorio", "D. Samaras", "N. Paragios", "R. Goldstein", "L.E. Ortiz"], "venue": "Advances in Neural Information Processing Systems,", "year": 2009}, {"title": "Big & quic: Sparse inverse covariance estimation for a million variables", "authors": ["Hsieh", "C.-J", "M.A. Sustik", "I.S. Dhillon", "P.K. Ravikumar", "R. Poldrack"], "venue": "In Advances in neural information processing systems,", "year": 2013}, {"title": "Quic: quadratic approximation for sparse inverse covariance estimation", "authors": ["C.J. Hsieh", "M.A. Sustik", "I.S. Dhillon", "P. Ravikumar"], "venue": "Journal of Machine Learning Research,", "year": 2014}, {"title": "The benefit of group sparsity", "authors": ["J. Huang", "T. Zhang"], "venue": "The Annals of Statistics,", "year": 2010}, {"title": "Inexact proximal newton methods for self-concordant functions", "authors": ["J. Li", "M.S. Andersen", "L. Vandenberghe"], "venue": "Mathematical Methods of Operations Research,", "year": 2017}, {"title": "Markov random field models in computer vision", "authors": ["S.Z. Li"], "venue": "European conference on computer vision,", "year": 1994}, {"title": "Foundations of statistical natural language processing", "authors": ["C.D. Manning", "H. Sch\u00fctze"], "venue": "MIT press,", "year": 1999}, {"title": "Exact covariance thresholding into connected components for large-scale graphical lasso", "authors": ["R. Mazumder", "T. Hastie"], "venue": "Journal of Machine Learning Research,", "year": 2012}, {"title": "High-dimensional graphs and variable selection with the lasso", "authors": ["N. Meinshausen", "P. B\u00fchlmann"], "venue": "The annals of statistics,", "year": 2006}, {"title": "A tour of modern image filtering: New insights and methods, both practical and theoretical", "authors": ["P. Milanfar"], "venue": "IEEE Signal Processing Magazine,", "year": 2013}, {"title": "Joint support recovery under high-dimensional scaling: Benefits and perils of l1,\u221e-regularization", "authors": ["S. Negahban", "M.J. Wainwright"], "venue": "Proceedings of the 21st International Conference on Neural Information Processing Systems,", "year": 2008}, {"title": "Introductory lectures on convex optimization: A basic course, volume 87", "authors": ["Y. Nesterov"], "venue": "Springer Science & Business Media,", "year": 2013}, {"title": "Union support recovery in high-dimensional multivariate regression", "authors": ["G. Obozinski", "M.J. Wainwright", "M.I. Jordan"], "venue": "Communication, Control, and Computing,", "year": 2008}, {"title": "Newton-like methods for sparse inverse covariance estimation", "authors": ["F. Oztoprak", "J. Nocedal", "S. Rennie", "P.A. Olsen"], "venue": "In Advances in neural information processing systems,", "year": 2012}, {"title": "Forecasting freeway link travel times with a multilayer feedforward neural network", "authors": ["D. Park", "L.R. Rilett"], "venue": "Computer Aided Civil and Infrastructure Engineering,", "year": 1999}, {"title": "High-dimensional covariance estimation by minimizing l1-penalized log-determinant divergence", "authors": ["P. Ravikumar", "M.J. Wainwright", "G. Raskutti", "B. Yu"], "venue": "Electronic Journal of Statistics,", "year": 2011}, {"title": "Iterative thresholding algorithm for sparse inverse covariance estimation", "authors": ["B. Rolfs", "B. Rajaratnam", "D. Guillot", "I. Wong", "A. Maleki"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2012}, {"title": "Triangulated graphs and the elimination process", "authors": ["D.J. Rose"], "venue": "Journal of Mathematical Analysis and Applications,", "year": 1970}, {"title": "Sparse permutation invariant covariance estimation", "authors": ["A.J. Rothman", "P.J. Bickel", "E. Levina", "J. Zhu"], "venue": "Electronic Journal of Statistics,", "year": 2008}, {"title": "Iterative methods for sparse linear systems, volume 82", "authors": ["Y. Saad"], "year": 2003}, {"title": "Equivalence of graphical lasso and thresholding for sparse graphs", "authors": ["S. Sojoudi"], "venue": "Journal of Machine Learning Research,", "year": 2016}, {"title": "A block-coordinate descent approach for large-scale sparse inverse covariance estimation", "authors": ["E. Treister", "J.S. Turek"], "venue": "In Advances in neural information processing systems,", "year": 2014}, {"title": "Chordal graphs and semidefinite optimization", "authors": ["L. Vandenberghe", "Andersen", "M. S"], "venue": "Foundations and Trends\u00ae in Optimization,", "year": 2015}, {"title": "Sharp thresholds for high-dimensional and noisy sparsity recovery using `1-constrained quadratic programming (lasso)", "authors": ["M.J. Wainwright"], "venue": "IEEE transactions on information theory,", "year": 2009}, {"title": "Elementary estimators for graphical models", "authors": ["E. Yang", "A.C. Lozano", "P.K. Ravikumar"], "venue": "Advances in neural information processing systems,", "year": 2014}, {"title": "Computing the minimum fill-in is NPcomplete", "authors": ["M. Yannakakis"], "venue": "SIAM Journal on Algebraic Discrete Methods,", "year": 1981}, {"title": "Model selection and estimation in the Gaussian graphical model", "authors": ["M. Yuan", "Y. Lin"], "venue": "Biometrika, pp", "year": 2007}], "id": "SP:7b9f0753a6324356736020c4c61120af758b2037", "authors": [{"name": "Richard Y. Zhang", "affiliations": []}, {"name": "Salar Fattahi", "affiliations": []}, {"name": "Somayeh Sojoudi", "affiliations": []}], "abstractText": "The sparse inverse covariance estimation problem is commonly solved using an `1-regularized Gaussian maximum likelihood estimator known as \u201cgraphical lasso\u201d, but its computational cost becomes prohibitive for large data sets. A recent line of results showed\u2013under mild assumptions\u2013that the graphical lasso estimator can be retrieved by soft-thresholding the sample covariance matrix and solving a maximum determinant matrix completion (MDMC) problem. This paper proves an extension of this result, and describes a Newton-CG algorithm to efficiently solve the MDMC problem. Assuming that the thresholded sample covariance matrix is sparse with a sparse Cholesky factorization, we prove that the algorithm converges to an -accurate solution in O(n log(1/ )) time and O(n) memory. The algorithm is highly efficient in practice: we solve the associated MDMC problems with as many as 200,000 variables to 7-9 digits of accuracy in less than an hour on a standard laptop computer running MATLAB.", "title": "Large-Scale Sparse Inverse Covariance Estimation via Thresholding and Max-Det Matrix Completion"}