21	4	We combine three ideas: (i) stochastic gradient variational Bayes (Kingma & Welling, 2014; Rezende et al., 2014; Titsias & Lázaro-Gredilla, 2014)2, (ii) persistent Markov chains (Younes, 1989), and (iii) a noncentered parameterization of scale-mixture priors, to inherit the benefits of hierarchical Bayesian sparsity in an efficient variational framework.
22	93	We make the following contributions: • We extend stochastic variational inference to undirected models with intractable normalizing constants by developing a learning algorithm based on persistent Markov chains, which we call Persistent Varia- tional Inference (PVI) (Section 2).
23	4	• We introduce a reparameterization approach for variational inference under sparsity-inducing scale-mixture priors (e.g. the Laplacian, ARD, and the Horseshoe) that significantly improves approximation quality by capturing scale uncertainty (Section 3).
24	17	When combined with Gaussian stochastic variational inference, we call this Fadeout.
25	154	• We demonstrate how a Bayesian approach for learning sparse undirected graphical models with PVI and Fadeout yields significantly improved inferences of both synthetic and real applications in physics and biology (Section 4).
30	17	Bayesian learning for undirected models is confounded by the partition function Z(θ).
31	31	Given the dataD, a prior p(θ), and the log potentials H[x|θ] = − ∑ i θifi(x) , the posterior distribution of the parameters is p(θ|D) = p(θ) ∏ i e −H[x(i)|θ]/Z(θ)∫ p(θ′) ∏ i e −H[x(i)|θ′]/Z(θ′)dθ′ , (5) which contains an intractable partition function Z(θ) within the already-intractable evidence term.
33	28	Here we consider how to approximate the intractable posterior in (5) without approximating the partition function Z(θ) or the likelihood p(x|θ) by using variational inference.
38	8	(7) Naively substituting the likelihood (3) in the score function estimator (7) nests the intractable log partition function logZ(θ) within the average over q(θ|φ), making this an untenable (and extremely high variance) approach to inference with undirected models.
39	12	We can avoid the need for a score-function estimator with the ‘reparameterization trick’ (Kingma & Welling, 2014; Rezende et al., 2014; Titsias & Lázaro-Gredilla, 2014) that has been incredibly useful for directed models.
40	22	Consider a variational approximation q(θ|φ) = ∏ i q(θi|µi, si) that is a fully factorized (mean field) Gaussian with means µ and log standard deviations s. The ELBO expectations under q(θ|φ) can be rewritten as expectations wrt an independent noise source ∼ N (0, I) where4 θ( ) = µ+exp {s} .
42	62	(9) Because these expectations require only the gradient of the likelihood ∇θ log p(D|θ), the gradient for the undirected model (4) can be substituted to form a nested expectation for ∇φL(φ).
43	18	This can then be used as a Monte Carlo gradient estimator by sampling ∼ N (0, I),x ∼ p(x|θ( )).
44	20	Persistent gradient estimation In Stochastic Maximum Likelihood estimation for undirected models, the intractable gradients of (4) are estimated by sampling p(x|θ).
45	18	Although sampling-based approaches are slow, they can be made considerably more efficient by running a set of Markov chains in parallel with state that persists between iterations (Younes, 1989).
46	17	Persistent state maintains the Markov chains near their equilibrium distributions, which means that they can quickly re-equilibrate after perturbations to the parameters θ during learning.
48	92	Following the notation of PCDn (Tieleman, 2008), PVI-n refers to using n sweeps of Gibbs sampling with persistent Markov chains between iterations.
49	5	This approach is generally compatible with any estimators of∇ELBO that are based on the gradient of the log likelihood, several examples of which are explained in (Kingma & Welling, 2014; Rezende et al., 2014; Titsias & Lázaro-Gredilla, 2014).
54	17	For scale-mixture priors that promote sparsity, these correlations come in the form of scale uncertainty.
55	26	Instead of assuming that the scale of a parameter in a model is known a priori, we posit that it is normally distributed with a randomly distributed variance p(σ2).
56	12	The joint prior p(θ|σ2)p(σ2) gives rise to a strongly curved ‘funnel’ shape (Figure 2) that illustrates a simple but profound principle about hierarchical models: Algorithm 1 Computing ∇ELBO for Fadeout Require: Global parameters {µτ , sτ} Require: Local parameters {µθ̃, µlogσ, sθ̃, slogσ} Require: Hyperprior gradient∇logσ,τ log p(logσ, τ ) Require: Likelihood gradient∇θp(x|θ) // Sample from variational distribution z1 ∼ N (0, I|τ |), z2 ∼ N (0, I|θ̃|), z3 ∼ N (0, I|σ|) τ ← µτ + exp{sτ} z1 θ̃ ← µθ̃ + exp{sθ̃} z2 σ ← exp {µlog σ + exp {slog σ} z3} θ ← θ̃ σ // Centered global parameters ∇µτL ← ∇τ log p(log σ, τ ) ∇sτL ← exp {sτ} z1 ∇µτL+ 1 // Noncentered local parameters ∇µθ̃L ← σ ∇θ log p(x|θ)− θ̃ ∇µlog σL ← θ ∇θ log p(x|θ) +∇log σ log p(log σ, τ ) ∇sθ̃L ← exp { sθ̃ } z2 ∇µθ̃L+ 1 ∇slog σL ← exp {slog σ} z3 ∇µlog σL+ 1 as the hyperparameter log σ decreases and the prior accepts a smaller range of values for θ, normalization increases the probability density at the origin, favoring sparsity.
58	6	While normalization-induced sharpening gives rise to sparsity, these extreme correlations are a disaster for meanfield variational inference.
59	11	Even if a tremendous amount of probability mass is concentrated at the base of the funnel, an uncorrelated mean-field approximation will yield estimates near the top.
60	40	The result is a potentially non-sparse estimate from a very-sparse prior.
62	28	Many models can be rewritten in a noncentered form where the parameters and hyperparmeters are a priori independen (Papaspiliopoulos et al., 2007; Betancourt & Girolami, 2013).
64	13	In noncentered form, the joint prior is independent and well approximated by a mean-field Gaussian, while the likelihood will be variably correlated depending on the strength of the data (Figure 2).
65	30	In this sense, centered parameterizations (CP) and noncentered parameterizations (NCP) are usually framed as favorable in strong and weak data regimes, respectively.5 We propose the use of non-centered parameterizations of scale-mixture priors for mean-field Gaussian variational inference.
66	137	For convenience, we like to call this Fadeout (see next section).
67	42	Fadeout can be easily implemented by either (i) using the chain rule to derive the gradient of the Evidence Lower BOund (ELBO) (Algorithm 1) or, for differentiable models, (ii) rewriting models in noncentered form and using automatic differentiation tools such as Stan (Kucukelbir et al., 2017) or autograd6 for ADVI.
69	5	Estimators for the centered posterior.
