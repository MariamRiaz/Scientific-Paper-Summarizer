7	66	One way to address this problem is to automatically rewrite a query so that it becomes more likely to retrieve relevant documents.
13	86	The environment is a search engine which produces a new state (i.e. retrieved documents).
15	111	The most important implication of this framework is that a search engine is treated as a black box that an agent learns to use in order to retrieve more relevant items.
17	91	To support this claim, we evaluate our agent on the task of question answering (Q&A), citation recommendation, and passage/snippet retrieval.
18	44	As for training data, we use two publicly available datasets (TREC-CAR and Jeopardy) and introduce a new one (MS Academic) with hundreds of thousands of query/relevant document pairs from the academic domain.
29	27	We convert the sequence {vj} to a fixed-size vector φa(v) by using either a Convolutional Neural Network (CNN) followed by a max pooling operation over the entire sequence (Kim, 2014) or by using the last hidden state of a Recurrent Neural Network (RNN).2 Similarly, we fed the candidate term vectors ei to a CNN or RNN to obtain a vector representation φb(ei) for each term ti.
36	24	We concatenate the terms in T to form a reformulated query q′, which will then be used to retrieve a new set of documents D′.
49	42	The perexample stochastic objective is defined as Ca = (R− R̄) ∑ t∈T − logP (t|q0), (4) where R is the reward and R̄ is the baseline, computed by the value network as: R̄ = σ(ST tanh(V (φa(v)‖ē) + b)), (5) where ē = 1N ∑N i=1 φb(ei), N = |q0 ∪ D0|, V ∈ Rd×2d and S ∈ Rd are weights and b ∈ R is a bias.
58	21	We address this issue by regularizing the negative entropy of the probability distribution.
59	25	We add the following regularization term to the original cost function in Eq.
85	29	In this work, the top-N TF-IDF terms from each of the top-K retrieved documents are added to the original query, where N and K are selected by a grid search on the validation data.
99	19	Reinforcement Learning (RL): We use multiple variants of the proposed RL method.
105	39	A query is the concatenation of an article title and one of its section titles.
106	17	The ground-truth documents are the paragraphs within that section.
115	22	Microsoft Academic (MSA) This dataset consists of academic papers crawled from Microsoft Academic API.5 The crawler started at the paper Silver et al. (2016) and traversed the graph of references until 500,000 papers were crawled.
149	29	For the PRF methods, the top-M terms according to a relevance metric (i.e., TF-IDF for PRF-TFIDF, cosine similarity for PRF-Emb, and conditional probability for PRF-RM) from each of the top-K retrieved documents are added to the original query.
153	20	Therefore, the numbers reported in the results section were all obtained from models running two rounds of search and reformulation.
156	26	We use a 2-layer convolutional network for candidate terms with window sizes of 9 and 3, respectively, and 256 filters in each layer.
168	25	RL-RNN-SEQ performs slightly worse than RL-RNN but produces queries that are three times shorter, on average (15 vs 47 words).
178	45	We show two examples of queries and the probabilities of each candidate term of being selected by the RL-CNN model in Fig.
179	150	Notice that terms that are more related to the query have higher probabilities, although common words such as ”the” are also selected.
180	16	This is a consequence of our choice of a reward that does not penalize the selection of neutral terms.
181	33	In Table 4 we show an original and reformulated query examples extracted from the MS Academic and TREC-CAR datasets, and their top-3 retrieved documents.
182	44	Notice that the reformulated query retrieves more relevant documents than the original one.
183	108	As we conjectured earlier, we see that a search engine tends to return a document simply with the largest overlap in the text, necessitating the reformulation of a query to retrieve semantically relevant documents.
184	92	Same query, different tasks We compare in Table 5 the reformulation of a sample query made by models trained on different datasets.
185	70	The model trained on TREC-CAR selects terms that are similar to the ones in the original query, such as “serves” and “accreditation”.
187	17	On the other hand, the model trained on Jeopardy prefers to select proper nouns, such as “Tunxis”, as these have a higher chance of being an answer to the question.
188	22	The model trained on MSA selects terms that cover different aspects of the entity being queried, such as “arts center” and “library”, since retrieving a diverse set of documents is necessary for the task the of citation recommendation.
196	48	In the future, more research is necessary in the directions of (1) iterative reformulation under the proposed framework, (2) using information from modalities other than text, and (3) better reinforcement learning algorithms for a partially-observable environment.
