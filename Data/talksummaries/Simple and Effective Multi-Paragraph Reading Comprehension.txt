3	25	The recent success of neural models at answering questions given a related paragraph (Wang et al., 2017c; Tan et al., 2017) suggests they have the potential to be a key part of a solution to this problem.
4	46	Most neural models are unable to scale beyond short paragraphs, so typically this requires adapting a paragraph-level model to process document-level input.
5	37	There are two basic approaches to this task.
6	86	Pipelined approaches select a single paragraph from the input documents, which is then passed to the paragraph model to extract an answer (Joshi et al., 2017; Wang et al., 2017a).
7	26	Confidence based methods apply the model to multiple paragraphs and return the answer with the highest confidence (Chen et al., 2017a).
33	22	On TriviaQA web, relative to truncating the document as done by prior work, this improves the chance of the selected text containing the correct answer from 83.1% to 85.1%.
34	33	In a distantly supervised setup we label all text spans that match the answer text as being correct.
35	24	This can lead to training the model to select unwanted answer spans.
39	30	This objective has the advantage of being agnostic to how the model distributes probability mass across the possible answer spans, allowing the model to focus on only the most relevant spans.
59	34	For the boundary-based models we use here, a span’s score is the sum of the start and end score given to its start and end token.
62	47	Our experiments in Section 5 show that these confidence scores can be very poor if the model is only trained on answer-containing paragraphs, as done by prior work.
66	40	As a result, nothing prevents models from producing scores that are arbitrarily all larger or all smaller for one paragraph than another.
67	17	Second, if the model only sees paragraphs that contain answers, it might become too confident in heuristics or patterns that are only effective when it is known a priori that an answer exists.
69	27	This kind of error has also been observed when distractor sentences are added to the context (Jia and Liang, 2017) We experiment with four approaches to training models to produce comparable confidence scores, shown in the following subsections.
77	17	We also experiment with allowing the model to select a special “no-answer” option for each paragraph.
85	16	As a final baseline, we consider training models with the sigmoid loss objective function.
88	30	The intuition is that, since the scores are being evaluated independently of one another, they are more likely to be comparable between different paragraphs.
89	28	We evaluate our approach on four datasets: TriviaQA unfiltered (Joshi et al., 2017), a dataset of questions from trivia databases paired with documents found by completing a web search of the questions; TriviaQA wiki, the same dataset but only including Wikipedia articles; TriviaQA web, a dataset derived from TriviaQA unfiltered by treating each question-document pair where the document contains the question answer as an individual training point; and SQuAD (Rajpurkar et al., 2016), a collection of Wikipedia articles and crowdsourced questions.
109	19	First, we do an ablation study on TriviaQA web to show the effects of our proposed methods for our pipeline model.
153	59	Given our estimate that 10% of the questions are ambiguous if the paragraph is unknown, our approach appears to have adapted to the document-level task very well.
161	18	Given a question, we retrieve up to 10 web documents using a Bing web search of the question, and all Wikipedia articles about entities the entity linker TAGME (Ferragina and Scaiella, 2010) identifies in the question.
162	49	We then use our linear paragraph ranker to select the 16 most relevant paragraphs from all these documents, which are passed to our model to locate the final answer span.
165	27	A demo of the system is publicly available8.
166	51	We find accuracy on the TriviaQA unfiltered questions remains almost unchanged (within half a percent exact match score) when using our document retrieval method instead of the given documents, showing our pipeline does a good job of producing evidence documents that are similar to the ones in the training data.
167	30	We test the system on questions from the TREC QA tasks (Voorhees et al., 1999), in particular a curated set of questions from Baudiš (2015), the same dataset used in Chen et al. (2017a).
168	27	We apply our system to the 694 test questions without retraining on the train questions.
178	21	The shared-norm approach consistently outperformed the other methods, especially on SQuAD and TriviaQA unfiltered, where many paragraphs were needed to reach peak performance.
181	23	The no-answer and merge approaches were moderately effective, we suspect because they at least expose the model to more irrelevant text.
186	75	We found 40.5% of the er- rors were caused because the document did not contain sufficient evidence to answer the question, and 17% were caused by the correct answer not being contained in the answer key.
190	19	Finally, some questions required background knowledge, or required the model to extract answers that were only stated indirectly (e.g., examining a list to extract the nth element).
