0,1,label2,summary_sentences
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1938–1947 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1938",text,[0],[0]
"The explicit modeling of morphology has been shown to improve a number of tasks (Seeker and Çetinoglu, 2015; Luong et al., 2013).",1 Introduction,[0],[0]
"In a large number of the world’s languages, many words are composed through morphological operations on subword units.",1 Introduction,[0],[0]
"Some languages are rich in inflectional morphology, characterized by syntactic transformations like pluralization.",1 Introduction,[0],[0]
"Similarly, languages like English are rich in derivational morphology, where the semantics of words are composed from
∗These authors contributed equally; listed alphabetically.
smaller parts.",1 Introduction,[0],[0]
"The AGENT derivational transformation, for example, answers the question, what is the word for ‘someone who runs’?",1 Introduction,[0],[0]
"with the answer, a runner.1 Here, AGENT is spelled out as suffixing -ner onto the root verb run.
",1 Introduction,[0],[0]
We tackle the task of derived word generation.,1 Introduction,[0],[0]
"In this task, a root word x and a derivational transformation t are given to the learner.",1 Introduction,[0],[0]
"The learner’s job is to produce the result of the transformation on the root word, called the derived word y. Table 1 gives examples of these transformations.
",1 Introduction,[0],[0]
"Previous approaches to derived word generation model the task as a character-level sequenceto-sequence (seq2seq) problem (Cotterell et al., 2017b).",1 Introduction,[0],[0]
"The letters from the root word and some encoding of the transformation are given as input to a neural encoder, and the decoder is trained to produce the derived word, one letter at a time.",1 Introduction,[0],[0]
"We identify the following problems with these approaches:
First, because these models are unconstrained, they can generate sequences of characters that do
1We use the verb run as a demonstrative example; the transformation can be applied to most verbs.
",1 Introduction,[0],[0]
not form actual words.,1 Introduction,[0],[0]
"We argue that requiring the model to generate a known word is a reasonable constraint in the special case of English derivational morphology, and doing so avoids a large number of common errors.
",1 Introduction,[0],[0]
"Second, sequence-based models can only generalize string manipulations (such as “add -ment”) if they appear frequently in the training data.",1 Introduction,[0],[0]
"Because of this, they are unable to generate derived words that do not follow typical patterns, such as generating truth as the nominative derivation of true.",1 Introduction,[0],[0]
We propose to learn a function for each transformation in a low dimensional vector space that corresponds to mapping from representations of the root word to the derived word.,1 Introduction,[0],[0]
"This eliminates the reliance on orthographic information, unlike related approaches to distributional semantics, which operate at the suffix level (Gupta et al., 2017).
",1 Introduction,[0],[0]
"We contribute an aggregation model of derived word generation that produces hypotheses independently from two separate learned models: one from a seq2seq model with only orthographic information, and one from a feed-forward network using only distributional semantic information in the form of pretrained word vectors.",1 Introduction,[0],[0]
The model learns to choose between the hypotheses according to the relative confidence of each.,1 Introduction,[0],[0]
This system can be interpreted as learning to decide between positing an orthographically regular form or a semantically salient word.,1 Introduction,[0],[0]
"See Figure 1 for a diagram of our model.
",1 Introduction,[0],[0]
"We show that this model helps with two open problems with current state-of-the-art seq2seq derived word generation systems, suffix ambiguity and orthographic irregularity (Section 2).",1 Introduction,[0],[0]
"We also
improve the accuracy of seq2seq-only derived word systems by adding external information through constrained decoding and hypothesis rescoring.",1 Introduction,[0],[0]
"These methods provide orthogonal gains to our main contribution.
",1 Introduction,[0],[0]
"We evaluate models in two categories: open vocabulary models that can generate novel words unattested in a preset vocabulary, and closedvocabulary models, which cannot.",1 Introduction,[0],[0]
Our best openvocabulary and closed-vocabulary models demonstrate 22% and 37% relative error reductions over the current state of the art.,1 Introduction,[0],[0]
Derivational transformations generate novel words that are semantically composed from the root word and the transformation.,2 Background: Derivational Morphology,[0],[0]
"We identify two unsolved problems in derived word transformation, each of which we address in Sections 3 and 4.
",2 Background: Derivational Morphology,[0],[0]
"First, many plausible choices of suffix for a single pair of root word and transformation.",2 Background: Derivational Morphology,[0],[0]
"For example, for the verb ground, the RESULT transformation could plausibly take as many forms as2
(ground, RESULT)→ grounding (ground, RESULT)→ *groundation (ground, RESULT)→ *groundment (ground, RESULT)→ *groundal
However, only one is correct, even though each suffix appears often in the RESULT transformation of other words.",2 Background: Derivational Morphology,[0],[0]
"We will refer to this problem as “suffix ambiguity.”
",2 Background: Derivational Morphology,[0],[0]
"Second, many derived words seem to lack a generalizable orthographic relationship to their root words.",2 Background: Derivational Morphology,[0],[0]
"For example, the RESULT of the verb speak is speech.",2 Background: Derivational Morphology,[0],[0]
"It is unlikely, given an orthographically similar verb creak, that the RESULT be creech instead of, say, creaking.",2 Background: Derivational Morphology,[0],[0]
Seq2seq models must grapple with the problem of derived words that are the result of unlikely or potentially unseen string transformations.,2 Background: Derivational Morphology,[0],[0]
We refer to this problem as “orthographic irregularity.”,2 Background: Derivational Morphology,[0],[0]
"In this section, we introduce the prior state-of-theart model, which serves as our baseline system.",3 Sequence Models and Corpus Knowledge,[0],[0]
"Then we build on top of this system by incorporating a dictionary constraint and rescoring the
2The * indicates a non-word.
model’s hypotheses with token frequency information to address the suffix ambiguity problem.",3 Sequence Models and Corpus Knowledge,[0],[0]
We begin by formalizing the problem and defining some notation.,3.1 Baseline Architecture,[0],[0]
"For source word x = x1, x2, . . .",3.1 Baseline Architecture,[0],[0]
"xm, a derivational transformation t, and target word y = y1, y2, . . .",3.1 Baseline Architecture,[0],[0]
"yn, our goal is to learn some function from the pair (x, t) to y. Here, xi and yj are the ith and jth characters of the input strings x and y.",3.1 Baseline Architecture,[0],[0]
"We will sometimes use x1:i to denote x1, x2, . . .",3.1 Baseline Architecture,[0],[0]
"xi, and similarly for y1:j .
",3.1 Baseline Architecture,[0],[0]
"The current state-of-the-art model for derivedform generation approaches this problem by learning a character-level encoder-decoder neural network with an attention mechanism (Cotterell et al., 2017b; Bahdanau et al., 2014).
",3.1 Baseline Architecture,[0],[0]
"The input to the bidirectional LSTM encoder (Hochreiter and Schmidhuber, 1997; Graves and Schmidhuber, 2005) is the sequence #, x1, x2, . . .",3.1 Baseline Architecture,[0],[0]
"xm, #, t, where # is a special symbol to denote the start and end of a word, and the encoding of the derivational transformation t is concatenated to the input characters.",3.1 Baseline Architecture,[0],[0]
The model is trained to minimize the cross entropy of the training data.,3.1 Baseline Architecture,[0],[0]
"We refer to our reimplementation of this model as SEQ.
",3.1 Baseline Architecture,[0],[0]
"For a more detailed treatment of neural sequenceto-sequence models with attention, we direct the reader to Luong et al. (2015).",3.1 Baseline Architecture,[0],[0]
The suffix ambiguity problem poses challenges for models which rely exclusively on input characters for information.,3.2 Dictionary Constraint,[0],[0]
"As previously demonstrated, words derived via the same transformation may take different suffixes, and it is hard to select among them based on character information alone.",3.2 Dictionary Constraint,[0],[0]
"Here, we describe a process for restricting our inference procedure to only generate known English words, which we call a dictionary constraint.",3.2 Dictionary Constraint,[0],[0]
"We believe that for English morphology, a large enough corpus will contain the vast majority of derived forms, so while this approach is somewhat restricting, it removes a significant amount of ambiguity from the problem.
",3.2 Dictionary Constraint,[0],[0]
"To describe how we implemented this dictionary constraint, it is useful first to discuss how decoding in a seq2seq model is equivalent to solving a shortest path problem.",3.2 Dictionary Constraint,[0],[0]
"The notation is specific to our model, but the argument is applicable to seq2seq models in general.
",3.2 Dictionary Constraint,[0],[0]
"The goal of decoding is to find the most probable structure ŷ conditioned on some observation x and transformation t. That is, the problem is to solve
ŷ =",3.2 Dictionary Constraint,[0],[0]
"argmax y∈Y
p(y | x, t) (1)
= argmin y∈Y − log p(y | x, t) (2)
where Y is the set of valid structures.",3.2 Dictionary Constraint,[0],[0]
"Sequential models have a natural ordering y = y1, y2, . . .",3.2 Dictionary Constraint,[0],[0]
"yn over which − log p(y | x, t) can be decomposed
− log p(y | x, t) = n∑
t=1
− log p(yt | y1:t−1,x, t)
(3)",3.2 Dictionary Constraint,[0],[0]
Solving Equation 2 can be viewed as solving a shortest path problem from a special starting state to a special ending state via some path which uniquely represents y.,3.2 Dictionary Constraint,[0],[0]
Each vertex in the graph represents some sequence y1,3.2 Dictionary Constraint,[0],[0]
":i, and the weight of the edge from y1:i to y1:i+1 is given by
− log p(yi+1 | y1:i−1,x, t) (4)
",3.2 Dictionary Constraint,[0],[0]
The weight of the path from the start state to the end state via the unique path that describes y is exactly equal to Equation 3.,3.2 Dictionary Constraint,[0],[0]
"When the vocabulary size is too large, the exact shortest path is intractable, and approximate search methods, such as beam search, are used instead.
",3.2 Dictionary Constraint,[0],[0]
"In derived word generation, Y is an infinite set of strings.",3.2 Dictionary Constraint,[0],[0]
"Since Y is unrestricted, almost all of the strings in Y are not valid words.",3.2 Dictionary Constraint,[0],[0]
"Given a dictionary YD, the search space is restricted to only those words in the dictionary by searching over the trie induced from YD, which is a subgraph of the unrestricted graph.",3.2 Dictionary Constraint,[0],[0]
"By limiting the search space to YD, the decoder is guaranteed to generate some known word.",3.2 Dictionary Constraint,[0],[0]
Models which use this dictionaryconstrained inference procedure will be labeled with +DICT.,3.2 Dictionary Constraint,[0],[0]
"Algorithm 1 has the pseudocode for our decoding procedure.
",3.2 Dictionary Constraint,[0],[0]
We discuss specific details of the search procedure and interesting observations of the search space in Section 6.,3.2 Dictionary Constraint,[0],[0]
Section 5.2 describes how we obtained the dictionary of valid words.,3.2 Dictionary Constraint,[0],[0]
"We also consider the inclusion of explicit word frequency information to help solve suffix ambiguity, using the intuition that “real” derived words
are likely to be frequently attested.",3.3 Word Frequency Knowledge through Rescoring,[0],[0]
"This permits a high-recall, potentially noisy dictionary.
",3.3 Word Frequency Knowledge through Rescoring,[0],[0]
"We are motivated by very high top-10 accuracy compared to top-1 accuracy, even among dictionary-constrained models.",3.3 Word Frequency Knowledge through Rescoring,[0],[0]
"By rescoring the hypotheses of a model using word frequency (a word-global signal) as a feature, attempt to recover a portion of this top-10 accuracy.
",3.3 Word Frequency Knowledge through Rescoring,[0],[0]
"When a model has been trained, we query it for its top-10 most likely hypotheses.",3.3 Word Frequency Knowledge through Rescoring,[0],[0]
The union of all hypotheses for a subset of the training observations forms the training set for a classifier that learns to predict whether a hypothesis generated by the model is correct.,3.3 Word Frequency Knowledge through Rescoring,[0],[0]
"Each hypothesis is labelled with its correctness, a value in {±1}.",3.3 Word Frequency Knowledge through Rescoring,[0],[0]
"We train a simple combination of two scores: the seq2seq model score for the hypothesis, and the log of the word frequency of the hypothesis.
",3.3 Word Frequency Knowledge through Rescoring,[0],[0]
"To permit a nonlinear combination of word frequency and model score, we train a small multilayer perceptron with the model score and the frequency of a derived word hypothesis as features.
",3.3 Word Frequency Knowledge through Rescoring,[0],[0]
"At testing time, the 10 hypotheses generated by a single seq2seq model for a single observation are rescored.",3.3 Word Frequency Knowledge through Rescoring,[0],[0]
"The new model top-1 hypothesis, then, is the argmax over the 10 hypotheses according to the rescorer.",3.3 Word Frequency Knowledge through Rescoring,[0],[0]
"In this way, we are able to incorporate word-global information, e.g. word frequency, that is ill-suited for incorporation at each character prediction step of the seq2seq model.",3.3 Word Frequency Knowledge through Rescoring,[0],[0]
We label models that are rescored in this way +FREQ.,3.3 Word Frequency Knowledge through Rescoring,[0],[0]
"So far, we have presented models that learn derivational transformations as orthographic operations.",4 Distributional Models,[0],[0]
"Such models struggle by construction with the orthographic irregularity problem, as they are trained to generalize orthographic information.",4 Distributional Models,[0],[0]
"However, the semantic relationships between root words and derived words are the same even when the orthography is dissimilar.",4 Distributional Models,[0],[0]
"It is salient, for example, that irregular word speech is related to its root speak in about the same way as how exploration is related to the word explore.
",4 Distributional Models,[0],[0]
"We model distributional transformations as functions in dense distributional word embedding spaces, crucially learning a function per derivational transformation, not per suffix pair.",4 Distributional Models,[0],[0]
"In this way, we aim to explicitly model the semantic transformation, not the othographic information.",4 Distributional Models,[0],[0]
"For all source words x and all target words y, we look up static distributional embeddings vx, vy ∈ Rd.",4.1 Feed-forward derivational transformations,[0],[0]
"For each derivational transformation t, we learn a function ft : Rd → Rd that maps vx to vy.",4.1 Feed-forward derivational transformations,[0],[0]
"ft is parametrized as two-layer perceptron, trained using a squared loss,
L = bTb (5) b =",4.1 Feed-forward derivational transformations,[0],[0]
"ft(vx)− vy (6)
We perform inference by nearest neighbor search in the embedding space.",4.1 Feed-forward derivational transformations,[0],[0]
"This inference strategy requires a subset of strings for our embedding dictionary, YV .
",4.1 Feed-forward derivational transformations,[0],[0]
"Upon receiving (x, t) at test time, we compute ft(vx) and find the most similar embeddings in YV .",4.1 Feed-forward derivational transformations,[0],[0]
"Specifically, we find the top-k most similar embeddings, and take the most similar derived word that starts with the same 4 letters as the root word, and is not identical to it.",4.1 Feed-forward derivational transformations,[0],[0]
"This heuristic filters out highly implausible hypotheses.
",4.1 Feed-forward derivational transformations,[0],[0]
"We use the single-word subset of the Google News vectors (Mikolov et al., 2013) as YV , so the size of the vocabulary is 929k words.",4.1 Feed-forward derivational transformations,[0],[0]
The seq2seq and distributional models we have presented learn with disjoint information to solve separate problems.,4.2 SEQ and DIST Aggregation,[0],[0]
"We leverage this intuition to build a model that chooses, for each observation, whether to generate according to orthographic information via the SEQ model, or produce a potentially irregular form via the DIST model.
",4.2 SEQ and DIST Aggregation,[0],[0]
"To train this model, we use a held-out portion of the training set, and filter it to only observations for which exactly one of the two models produces the correct derived form.",4.2 SEQ and DIST Aggregation,[0],[0]
"Finally, we make the strong assumption that the probability of a derived form being generated correctly according to 1 model as opposed to the other is dependent only on the unnormalized model score from each.",4.2 SEQ and DIST Aggregation,[0],[0]
"We model this as a logistic regression (t is omitted for clarity):
P (·|yD,yS,x) = softmax(We [DIST(yD|x); SEQ(yS|x)]",4.2 SEQ and DIST Aggregation,[0],[0]
"+ be)
where We and be are learned parameters, yD and yS are the hypotheses of the distributional and seq2seq models, and DIST(·) and SEQ(·) are the models’ likelihood functions.",4.2 SEQ and DIST Aggregation,[0],[0]
We denote this aggregate AGGR in our results.,4.2 SEQ and DIST Aggregation,[0],[0]
In this section we describe the derivational morphology dataset used in our experiments and how we collected the dictionary and token frequencies used in the dictionary constraint and rescorer.,5 Datasets,[0],[0]
"In our experiments, we use the derived word generation derivational morphology dataset released in Cotterell et al. (2017b).",5.1 Derivational Morphology,[0],[0]
"The dataset, derived from NomBank (Meyers et al., 2004) , consists of 4,222 training, 905 validation, and 905 test triples of the form (x, t,y).",5.1 Derivational Morphology,[0],[0]
"The transformations are from the following categories: ADVERB (ADJ→ ADV), RESULT (V→ N), AGENT (V→ N), and NOMINAL (ADJ→ N).",5.1 Derivational Morphology,[0],[0]
Examples from the dataset can be found in Table 1.,5.1 Derivational Morphology,[0],[0]
"The dictionary and token frequency statistics used in the dictionary constraint and frequency reranking come from the Google Books NGram corpus (Michel et al., 2011).",5.2 Dictionary and Token Frequency Statistics,[0],[0]
"The unigram frequency counts were aggregated across years, and any tokens which appear fewer than approximately 2,000 times, do not end in a known possible suffix, or contain a character outside of our vocabulary were removed.
",5.2 Dictionary and Token Frequency Statistics,[0],[0]
"The frequency threshold was determined using development data, optimizing for high recall.",5.2 Dictionary and Token Frequency Statistics,[0],[0]
We collect a set of known suffixes from the training data by removing the longest common prefix between the source and target words from the target word.,5.2 Dictionary and Token Frequency Statistics,[0],[0]
"The result is a dictionary with frequency information for around 360k words, which covers 98% of the target words in the training data.3",5.2 Dictionary and Token Frequency Statistics,[0],[0]
"In many sequence models where the vocabulary size is large, exact inference by finding the true shortest path in the graph discussed in Section 3.2 is intractable.",6 Inference Procedure Discussion,[0],[0]
"As a result, approximate inference techniques such as beam search are often used, or the size of the search space is reduced, for example, by using a Markov assumption.",6 Inference Procedure Discussion,[0],[0]
"We, however, observed that exact inference via a shortest path algorithm is not only tractable in our model, but
3 The remaining 2% is mostly words with hyphens or mistakes in the dataset.
only slightly more expensive than greedy search and significantly less expensive than beam search.
",6 Inference Procedure Discussion,[0],[0]
"To quantify this claim, we measured the accuracy and number of states explored by greedy search, beam search, and shortest path with and without a dictionary constraint on the development data.",6 Inference Procedure Discussion,[0],[0]
Table 2 shows the results averaged over 30 runs.,6 Inference Procedure Discussion,[0],[0]
"As expected, beam search and shortest path have higher accuracies than greedy search and explore more of the search space.",6 Inference Procedure Discussion,[0],[0]
"Surprisingly, beam search and shortest path have nearly identical accuracies, but shortest path explores significantly fewer hypotheses.
",6 Inference Procedure Discussion,[0],[0]
At least two factors contribute to the tractability of exact search in our model.,6 Inference Procedure Discussion,[0],[0]
"First, our characterlevel sequence model has a vocabulary size of 63, which is significantly smaller than token-level models, in which a vocabulary of 50k words is not uncommon.",6 Inference Procedure Discussion,[0],[0]
"The search space of sequence models is dependent upon the size of the vocabulary, so the model’s search space is dramatically smaller than for a token-level model.
",6 Inference Procedure Discussion,[0],[0]
"Second, the inherent structure of the task makes it easy to eliminate large subgraphs of the search space.",6 Inference Procedure Discussion,[0],[0]
"The first several characters of the input word and output word are almost always the same, so the model assigns very low probability to any sequence with different starting characters than the input.",6 Inference Procedure Discussion,[0],[0]
"Then, the rest of the search procedure is dedicated to deciding between suffixes.",6 Inference Procedure Discussion,[0],[0]
"Any suffix which does not appear frequently in the training data receives a low score, leaving the search to decide between a handful of possible options.",6 Inference Procedure Discussion,[0],[0]
The result is that the learned probability distribution is very spiked; it puts very high probability on just a few output sequences.,6 Inference Procedure Discussion,[0],[0]
"It is empirically true that the top few most probable sequences have significantly higher scores than the next most probable sequences, which supports this hypothesis.
",6 Inference Procedure Discussion,[0],[0]
"In our subsequent experiments, we decode using
Algorithm 1 The decoding procedure uses a shortest-path algorithm to find the most probable output sequence.",6 Inference Procedure Discussion,[0],[0]
"The dictionary constraint is (optionally) implemented on line 9 by only considering prefixes that are contained in some trie T .
1: procedure DECODE(x, t, V , T ) 2: H ← Heap() 3: H .insert(0, #) 4: while H is not empty",6 Inference Procedure Discussion,[0],[0]
do 5: y← H .remove() 6: if y is a complete word then return y 7: for y ∈ V do 8: y′,6 Inference Procedure Discussion,[0],[0]
"← y + y 9: if y′ ∈ T then
10: s← FORWARD(x, t,y′) 11: H .insert(s, y′)
exact inference by running a shortest path algorithm (see Algorithm 1).",6 Inference Procedure Discussion,[0],[0]
"For reranking models, instead of typically using a beam of size k, we use the top k most probable sequences.",6 Inference Procedure Discussion,[0],[0]
"In all of our experiments, we use the training, development, and testing splits provided by Cotterell et al. (2017b) and average over 30 random restarts.",7 Results,[0],[0]
"Table 3 displays the accuracies and average edit distances on the test set of each of the systems presented in this work and the state-of-the-art model from Cotterell et al. (2017b).
",7 Results,[0],[0]
"First, we observed that SEQ outperforms the results reported in Cotterell et al. (2017b) by a large margin, despite the fact that the model architectures are the same.",7 Results,[0],[0]
"We attribute this difference to better hyperparameter settings and improved learning rate annealing.
",7 Results,[0],[0]
"Then, it is clear that the accuracy of the distributional model, DIST, is significantly lower than any seq2seq model.",7 Results,[0],[0]
"We believe the orthographyinformed models perform better because most observations in the dataset are orthographically regular, providing low-hanging fruit.
",7 Results,[0],[0]
"Open-vocabulary models Our open-vocabulary aggregation model AGGR improves performance by 3.8 points accuracy over SEQ, indicating that the sequence models and the distributional model are contributing complementary signals.",7 Results,[0],[0]
"AGGR is an open-vocabulary model like Cotterell et al. (2017b) and improves upon it by 6.3 points, making it our best comparable model.",7 Results,[0],[0]
"We provide an in-
depth analysis of the strengths of SEQ and DIST in Section 7.1.
",7 Results,[0],[0]
Closed-vocabulary models We now consider closed-vocabulary models that improve upon the seq2seq model in AGGR.,7 Results,[0],[0]
"First, we see that restricting the decoder to only generate known words is extremely useful, with SEQ+DICT improving over SEQ by 6.2 points.",7 Results,[0],[0]
"Qualitatively, we note that this constraint helps solve the suffix ambiguity problem, since orthographically plausible incorrect hypotheses are pruned as non-words.",7 Results,[0],[0]
See Table 6 for examples of this phenomenon.,7 Results,[0],[0]
"Additionally, we observe that the dictionary-constrained model outperforms the unconstrained model according to top-10 accuracy (see Table 5).
",7 Results,[0],[0]
"Rescoring (+FREQ) provides further improvement of 0.8 points, showing that the decoding dictionary constraint provides a higher-quality beam that still has room for top-1 improvement.",7 Results,[0],[0]
"All together, AGGR+FREQ+DICT provides a 4.4 point improvement over the best open-vocabulary model, AGGR.",7 Results,[0],[0]
"This shows the disambiguating power of assuming a closed vocabulary.
",7 Results,[0],[0]
Edit Distance One interesting side effect of the dictionary constraint appears when comparing AGGR+FREQ with and without the dictionary constraint.,7 Results,[0],[0]
"Although the accuracy of the dictionaryconstrained model is better, the average edit distance is worse.",7 Results,[0],[0]
"The unconstrained model is free to put invalid words which are orthographically similar to the target word in its top-k, however the constrained model can only choose valid words.",7 Results,[0],[0]
"This means it is easier for the unconstrained model to generate words which have a low edit distance to the ground truth, whereas the constrained model
can only do that if such a word exists.",7 Results,[0],[0]
"The result is a more accurate, yet more orthographically diverse, set of hypotheses.
",7 Results,[0],[0]
"Results by Transformation Next, we compare our best open vocabulary and closed vocabulary models to previous work across each derivational transformation.",7 Results,[0],[0]
"These results are in Table 4.
",7 Results,[0],[0]
"The largest improvement over the baseline system is for NOMINAL transformations, in which the AGGR has a 49% reduction in error.",7 Results,[0],[0]
We attribute most of this gain to the difficulty of this particular transformation.,7 Results,[0],[0]
"NOMINAL is challenging because there are several plausible endings (e.g. -ity, -ness, -ence) which occur at roughly the same rate.",7 Results,[0],[0]
"Additionally, NOMINAL examples are the least frequent transformation in the dataset, so it is challenging for a sequential model to learn to generalize.",7 Results,[0],[0]
"The distributional model, which does not rely on suffix information, does not have this same weakness, so the aggregation AGGR model has better results.
",7 Results,[0],[0]
"The performance of AGGR+FREQ+DICT is worse than AGGR, however.",7 Results,[0],[0]
"This is surprising because, in all other transformations, adding dictionary information improves the accuracies.",7 Results,[0],[0]
"We believe this is due to the ambiguity of the ground truth: Many root words have seemingly multiple plausible nominal transformations, such as rigid → {rigidness, rigidity} and equivalent → {equivalence, equivalency}.",7 Results,[0],[0]
"The dictionary constraint produces a better set of hypotheses to rescore, as demonstrated in Table 5.",7 Results,[0],[0]
"Therefore, the dictionary-constrained model is likely to have more of these ambiguous cases, which makes the task more difficult.",7 Results,[0],[0]
In this subsection we explore why AGGR improves consistently over SEQ even though it maintains an open vocabulary.,7.1 Strengths of SEQ and DIST,[0],[0]
"We have argued that DIST is able to correctly produce derived words that are
orthographically irregular or infrequent in the training data.",7.1 Strengths of SEQ and DIST,[0],[0]
"Figure 2 quantifies this phenomenon, analyzing the difference in accuracy between the two models, and plotting this in relationship to the frequency of the suffix in the training data.",7.1 Strengths of SEQ and DIST,[0],[0]
"The plot shows that SEQ excels at generating derived words ending in -ly, -ion, and other suffixes that appeared frequently in the training data.",7.1 Strengths of SEQ and DIST,[0],[0]
"DIST’s improvements over SEQ are generally much less frequent in the training data, or as in the case of -ment, are less frequent than other suffixes for the same transformation (like -ion.)",7.1 Strengths of SEQ and DIST,[0],[0]
"By producing derived words whose suffixes show up rarely in the training data, DIST helps solve the orthographic irregularity problem.",7.1 Strengths of SEQ and DIST,[0],[0]
"There has been much work on the related task of inflected word generation (Durrett and DeNero,
2013; Rastogi et al., 2016; Hulden et al., 2014).",8 Prior Work,[0],[0]
"It is a structurally similar task to ours, but does not have the same difficulty of challenges (Cotterell et al., 2017a,b), which we have addressed in our work.",8 Prior Work,[0],[0]
The paradigm completion for derivational morphology dataset we use in this work was introduced in Cotterell et al. (2017b).,8 Prior Work,[0],[0]
"They apply the model that won the 2016 SIGMORPHON shared task on inflectional morphology to derivational morphology (Kann and Schütze, 2016; Cotterell et al., 2016).",8 Prior Work,[0],[0]
"We use this as our baseline.
",8 Prior Work,[0],[0]
Our implementation of the dictionary constraint is an example of a special constraint which can be directly incorporated into the inference algorithm at little additional cost.,8 Prior Work,[0],[0]
"Roth and Yih (2004, 2007) propose a general inference procedure that naturally incorporates constraints through recasting inference as solving an integer linear program.
Beam or hypothesis rescoring to incorporate an expensive or non-decomposable signal into search has a history in machine translation (Huang and Chiang, 2007).",8 Prior Work,[0],[0]
"In inflectional morphology, Nicolai et al. (2015) use this idea to rerank hypotheses using orthographic features and Faruqui et al. (2016) use a character-level language model.",8 Prior Work,[0],[0]
"Our approach is similar to Faruqui et al. (2016) in that we use statistics from a raw corpus, but at the token level.
",8 Prior Work,[0],[0]
There have been several attempts to use distributional information in morphological generation and analysis.,8 Prior Work,[0],[0]
"Soricut and Och (2015) collect pairs of words related by any morphological change in an unsupervised manner, then select a vector offset which best explains their observations.",8 Prior Work,[0],[0]
"There has been subsequent work exploring the vector offset method, finding it unsuccessful in captur-
ing derivational transformations (Gladkova et al., 2016).",8 Prior Work,[0],[0]
"However, we use more expressive, nonlinear functions to model derivational transformations and report positive results.",8 Prior Work,[0],[0]
Gupta et al. (2017) then learn a linear transformation per orthographic rule to solve a word analogy task.,8 Prior Work,[0],[0]
"Our distributional model learns a function per derivational transformation, not per orthographic rule, which allows it to generalize to unseen orthography.",8 Prior Work,[0],[0]
"Our models are implemented in Python using the DyNet deep learning library (Neubig et al., 2017).",9 Implementation Details,[0],[0]
"The code is freely available for download.4
Sequence Model The sequence-to-sequence model uses character embeddings of size 20, which are shared across the encoder and decoder, with a vocabulary size of 63.",9 Implementation Details,[0],[0]
"The hidden states of the LSTMs are of size 40.
",9 Implementation Details,[0],[0]
"For training, we use Adam with an initial learning rate of 0.005, a batch size of 5, and train for a maximum of 30 epochs.",9 Implementation Details,[0],[0]
"If after one epoch of the training data, the loss on the validation set does not decrease, we anneal the learning rate by half and revert to the previous best model.
",9 Implementation Details,[0],[0]
"During decoding, we find the top 1 most probable sequence as discussed in Section 6 unless rescoring is used, in which we use the top 10.
",9 Implementation Details,[0],[0]
Rescorer The rescorer is a 1-hidden-layer perceptron with a tanh nonlinearity and 4 hidden units.,9 Implementation Details,[0],[0]
"It is trained for a maximum of 5 epochs.
",9 Implementation Details,[0],[0]
Distributional Model,9 Implementation Details,[0],[0]
"The DIST model is a 1- hidden-layer perceptron with a tanh nonlinearity
4https://github.com/danieldeutsch/ acl2018
and 100 hidden units.",9 Implementation Details,[0],[0]
It is trained for a maximum of 25 epochs.,9 Implementation Details,[0],[0]
"In this work, we present a novel aggregation model for derived word generation.",10 Conclusion,[0],[0]
This model learns to choose between the predictions of orthographicallyand distributionally-informed models.,10 Conclusion,[0],[0]
"This ameliorates suffix ambiguity and orthographic irregularity, the salient problems of the generation task.",10 Conclusion,[0],[0]
"Concurrently, we show that derivational transformations can be usefully modeled as nonlinear functions on distributional word embeddings.",10 Conclusion,[0],[0]
"The distributional and orthographic models aggregated contribute orthogonal information to the aggregate, as shown by substantial improvements over state-of-the-art results, and qualitative analysis.",10 Conclusion,[0],[0]
Two ways of incorporating corpus knowledge – constrained decoding and rescoring – demonstrate further improvements to our main contribution.,10 Conclusion,[0],[0]
"We would like to thank Shyam Upadhyay, Jordan Kodner, and Ryan Cotterell for insightful discussions about derivational morphology.",Acknowledgements,[0],[0]
"We would also like to thank our anonymous reviewers for helpful feedback on clarity and presentation.
",Acknowledgements,[0],[0]
This work was supported by Contract HR001115-2-0025 with the US Defense Advanced Research Projects Agency (DARPA).,Acknowledgements,[0],[0]
"Approved for Public Release, Distribution Unlimited.",Acknowledgements,[0],[0]
The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government.,Acknowledgements,[0],[0]
"Modeling derivational morphology to generate words with particular semantics is useful in many text generation tasks, such as machine translation or abstractive question answering.",abstractText,[0],[0]
"In this work, we tackle the task of derived word generation.",abstractText,[0],[0]
"That is, given the word “run,” we attempt to generate the word “runner” for “someone who runs.”",abstractText,[0],[0]
We identify two key problems in generating derived words from root words and transformations: suffix ambiguity and orthographic irregularity.,abstractText,[0],[0]
We contribute a novel aggregation model of derived word generation that learns derivational transformations both as orthographic functions using sequence-to-sequence models and as functions in distributional word embedding space.,abstractText,[0],[0]
"Our best open-vocabulary model, which can generate novel words, and our best closed-vocabulary model, show 22% and 37% relative error reductions over current state-of-the-art systems on the same dataset.",abstractText,[0],[0]
A Distributional and Orthographic Aggregation Model for English Derivational Morphology,title,[0],[0]
"One of the simplest Markov chain Monte Carlo (MCMC) methods is Langevin dynamics (Grenander & Miller, 1994; Robert & Casella, 2004, Sec. 7.8.5) where one repeats gradient steps with injected Gaussian noise.",1. Introduction,[0],[0]
"Namely, one iterates
z z + ✏ 2 rz log p(z) + p ✏⌘, (1)
where ⌘ is sampled from a standard Gaussian distribution and ✏ is a step-size that may decay over time.",1. Introduction,[0],[0]
"To get exact samples, a Metropolis-Hastings rejection step should be used.",1. Introduction,[0],[0]
"However, as the step-size ✏ becomes smaller, the acceptance probability goes to one, and so this can be disregarded.",1. Introduction,[0],[0]
"In the Bayesian inference setting, z denotes unknown parameters and p(z) represents a posterior over
1College of Computing and Information Sciences, University of Massachusetts, Amherst, USA.",1. Introduction,[0],[0]
"Correspondence to: Justin Domke <domke@cs.umass.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
them.",1. Introduction,[0],[0]
Computing p(z) thus requires a full pass over the dataset.,1. Introduction,[0],[0]
"The idea of Stochastic Gradient Langevin Dynamics (Welling & Teh, 2011) is to get an unbiased estimate of the gradient of log p(z) by approximating the posterior using a minibatch.",1. Introduction,[0],[0]
"Since the gradient term is multiplied by ✏, the variance of its error has order ✏2, while the variance of p ✏⌘ has order ✏.",1. Introduction,[0],[0]
"Thus, with a small step ✏, the injected noise dominates, meaning one can might expect these to have a stationary distribution close to the target p(z).
",1. Introduction,[0],[0]
The idea of variational inference is to posit a simple family of distributions qw(z) and then optimize w to minimize the KL-divergence from qw(z) to the target p(z).,1. Introduction,[0],[0]
"(In a Bayesian setting, this is equivalent to maximizing the evidence lower bound.)",1. Introduction,[0],[0]
"If one were to use plain gradient ascent to perform this optimization, this leads to the iterate of
w w + ✏ 2 rwEqw(Z)[log",1. Introduction,[0],[0]
"p(Z) log qw(Z)], (2)
where the gradient step ✏ is divided by two for convenience, and may again decay over time.",1. Introduction,[0],[0]
"In practice, doing gradient ascent like this has two difficulties.",1. Introduction,[0],[0]
"First, in the Bayesian inference setting, computing log p(z) requires a full pass over the data.",1. Introduction,[0],[0]
This can be approximated without bias using a minibatch.,1. Introduction,[0],[0]
The second difficulty stems from the expectation of log p(z) with respect to qw(z) in Eq. 2.,1. Introduction,[0],[0]
The gradient of this expectation can be approximated in different ways using samples from qw(Z) (Section 3.2).,1. Introduction,[0],[0]
"Since this gives unbiased estimates of the gradient, stochastic optimization methods, such as stochastic gradient descent, will converge to a local optima when the step-size becomes small.
",1. Introduction,[0],[0]
"Intuitively, both MCMC and variational methods can be thought of as trying to find a high-probability region of log p(z), with different strategies to encourage entropy to get coverage of the distribution.",1. Introduction,[0],[0]
"In MCMC, entropy is created by randomness in the Markov chain, while in variational methods the KL-divergence directly measures the entropy of the variational distribution.",1. Introduction,[0],[0]
It is natural to think that hybrid methods might employ fractions of these two strategies.,1. Introduction,[0],[0]
This paper does so by defining a distribution over the parameters w of the approximating family q(z|w).,1. Introduction,[0],[0]
"This distribution q(w) is designed to minimize a bound on the KL-divergence of the resulting marginal distribution
q(z) to the target p(z).",1. Introduction,[0],[0]
"Intuitively, when defining q(w), there are two ways to encourage entropy, namely to give higher density to vectors w where q(Z|w) has high entropy or to encourage entropy over w itself.
",1. Introduction,[0],[0]
"Given a distribution q(w, z) and a target distribution p(z), this paper uses two bounds on the marginal divergence Dtrue = KL (q(Z)kp(Z)) .",1. Introduction,[0],[0]
"First, one can upper-bound by the conditional divergence D0 = KL (q(Z|W )kp(Z)).",1. Introduction,[0],[0]
"Second, one can “adjoin” some distribution p(w|z) to p(z), and bound Dtrue by the joint divergence D1 = KL (q(W,Z)kp(W,Z)).",1. Introduction,[0],[0]
"This paper takes a convex combination of these two bounds, i.e. uses the family of bounds D = (1 )D0 + D1 .",1. Introduction,[0],[0]
"For any between 0 and 1, the distribution q(w) is derived that minimizes this bound for fixed p and a fixed approximating family q(z|w) (Theorem 3).",1. Introduction,[0],[0]
"Then, one can sample from this distribution over w by (stochastic) Langevin dynamics.",1. Introduction,[0],[0]
"This turns out to lead to the iterate
w w +",1. Introduction,[0],[0]
"✏ 2
rw ⇣ Eqw [log p(Z) + ( 1) log qw(Z)]
+ log r(w) ⌘",1. Introduction,[0],[0]
"+ p ✏ ⌘, (3)
where r(w) is related to the adjoining distribution p(w|z) and may be thought of as a “base measure” 1 over the space of w. Of course, exactly computing this gradient is intractable in general, but the same strategies used for variational inference can be used to efficiently compute an un-
1To see why r is necessary, consider a reparameterization of w.",1. Introduction,[0],[0]
"In order for the marginal distribution q(z) to stay the same, the density of the base measure must be transformed corresponding to the reparameterization.",1. Introduction,[0],[0]
"Put another way, without r(w), the results of inference would depend on the parameterization of w, even with an arbitrarily small step-size.
biased estimate.
",1. Introduction,[0],[0]
"It can be shown that as ! 0, the solution concentrates in the optima of the variational optimization, and thus the sampling algorithm reduces to gradient ascent on the variational objective.",1. Introduction,[0],[0]
"Meanwhile, as will be discussed in Sections 2.3 and 5 it is best to make r a function of such that w with high entropy have less density when approaches 1.",1. Introduction,[0],[0]
"When this is true, then as ! 1, only w indexing highly concentrated distributions over z retain density, and so sampling from q(w) is essentially just a reparameterization of sampling from p(z), and the algorithm reduces to Langevin dynamics.
",1. Introduction,[0],[0]
"Thus, for the extreme cases of = 0 and = 1, this algorithm reduces to variational inference and Langevin dynamics, respectively.",1. Introduction,[0],[0]
"Informally, at these extremes, the algorithm is “fast but approximate” and “slow but accurate”.",1. Introduction,[0],[0]
"In the intermediate range, the algorithm exhibits new behavior with a fine-grained trade-off between speed and accuracy.",1. Introduction,[0],[0]
"Thus, this approach gives a smooth continuum of algorithms with different priorities of speed and accuracy.",1. Introduction,[0],[0]
This paper uses three notational conventions that are not universal.,1.1. Notation,[0],[0]
"First, the conditional probability q(z|w) will alternatively be written as qw(z) when convenient.",1.1. Notation,[0],[0]
"Second, A ' B indicates that A and B have the same expected value or (if A is a constant) that B is an unbiased estimator of A. Finally, upper and lower-cases indicate what terms are random variables in a KL-divergence.",1.1. Notation,[0],[0]
"So, KL (q(Z|w)kp(Z|w)) = Eq(Z|w) log (q(Z|w)/p(Z|w)) is the divergence between q(z|w) and p(z|w) for a fixed w, while KL (q(Z|W )kp(Z|W ))",1.1. Notation,[0],[0]
"=
Eq(W,Z) log (q(Z|W )/p(Z|W )) is a standard conditional divergence.",1.1. Notation,[0],[0]
"This section derives a few results from an information theoretic viewpoint, without any particular regard for form of the target distribution, or how one might sample from it.",2. Information Theoretic Results,[0],[0]
Proofs are given in the appendix.,2. Information Theoretic Results,[0],[0]
Let p(z) be the target distribution.,2.1. Preliminaries,[0],[0]
"For simplicity, z is treated here as a continuous variable, although the results in this section remain true if it is discrete.",2.1. Preliminaries,[0],[0]
"There is some fixed set of conditional distributions q(z|w), which may also be written as qw(z).",2.1. Preliminaries,[0],[0]
"In principle, one might like to know how to set q(w) such that the resulting marginal distribution over z, is close to p(z), as measured by the KLdivergence,
Dtrue = KL (q(Z)kp(Z))",2.1. Preliminaries,[0],[0]
"= Eq(Z) log q(Z)
p(Z) .",2.1. Preliminaries,[0],[0]
"(4)
This quantity is difficult to control directly, since the marginal q(z) typically cannot be evaluated in closed form.",2.1. Preliminaries,[0],[0]
"One bound comes from the conditional KL-divergence, as stated in the following Lemma.
",2.1. Preliminaries,[0],[0]
Lemma 1.,2.1. Preliminaries,[0],[0]
"The divergence from q(z) to p(z) is
KL (q(Z)kp(Z))",2.1. Preliminaries,[0],[0]
"= KL (q(Z|W )kp(Z))| {z } D0 Iq[W,Z],
(5) where D0 = Eq(W,Z) log (q(Z|W )/p(Z)) is the conditional divergence and Iq = KL (q(Z,W )kq(Z)q(W ))",2.1. Preliminaries,[0],[0]
"denotes mutual information under q.
A second bound comes from adjoining some distribution p(w|z) to p(z).",2.1. Preliminaries,[0],[0]
"Then, the following lemma is essentially just a re-statement of the chain rule for the KL-divergence (Cover & Thomas, 2006, Thm. 2.5.3).
",2.1. Preliminaries,[0],[0]
Lemma 2.,2.1. Preliminaries,[0],[0]
"The KL-divergence between q(z) and p(z) can be written as
KL(q(Z)kp(Z)))",2.1. Preliminaries,[0],[0]
"= KL (q(W,Z)kp(W,Z))| {z } D1
KL (q(W |Z)kp(W |Z)) .",2.1. Preliminaries,[0],[0]
"(6)
Since the mutual information in Eq. 5 and the conditional divergence on the right of Eq. 6 are both non-negative, both D0 and D1 are upper-bounds on Dtrue.",2.1. Preliminaries,[0],[0]
"Note that if p(w|z) = q(w), then D1 = D0, and so Lemma 1 follows from Lemma 2.",2.1. Preliminaries,[0],[0]
"This paper is based on convex combination of D0 and D1 , parameterized by some 2 [0, 1], namely
D = (1 )D0 + D1.",2.2. Divergence Bound and its Minimizer,[0],[0]
"(7)
Since D0 and D1 are upper-bounds, so is D .",2.2. Divergence Bound and its Minimizer,[0],[0]
"The following theorem gives the distribution q(w) to minimize D .
Theorem 3.",2.2. Divergence Bound and its Minimizer,[0],[0]
"For fixed values of and p(w|z), the distribution q(w) that minimizes D is
q⇤(w) = exp s(w) A) (8)
A = log
Z
w exp s(w)
s(w) = log p(w) KL (q(Z|w)kp(Z|w))
",2.2. Divergence Bound and its Minimizer,[0],[0]
"1 1 KL (q(Z|w)kp(Z)) .
",2.2. Divergence Bound and its Minimizer,[0],[0]
"Moreover, at q⇤, the objective value is D⇤ = A.
Since A is an upper-bound on the KLdivergence, A must be non-positive.",2.2. Divergence Bound and its Minimizer,[0],[0]
"This can also be seen directly, since it can be written as A = log R w p(w) exp( KL (q(Z|w)kp(Z|w))
1 1 KL (q(Z|w)kp(Z))), and the inner diver-
gences as well as 1 1 are non-negative.
",2.2. Divergence Bound and its Minimizer,[0],[0]
"To understand the connection of this bound to variational inference and MCMC, one can look at behavior where = 0 or where = 1.",2.2. Divergence Bound and its Minimizer,[0],[0]
"For the former case, one obtains a result closely related to the solution to the variational inference problem.",2.2. Divergence Bound and its Minimizer,[0],[0]
Remark 4.,2.2. Divergence Bound and its Minimizer,[0],[0]
In the limit where ! 0,2.2. Divergence Bound and its Minimizer,[0],[0]
"the divergence bound at the optimal q⇤ becomes
lim !0",2.2. Divergence Bound and its Minimizer,[0],[0]
D⇤ = infw KL (q(Z|w)kp(Z)) .,2.2. Divergence Bound and its Minimizer,[0],[0]
"(9)
Similarly, when ! 0, q⇤(w) concentrates at the minimizer(s) of this divergence.",2.2. Divergence Bound and its Minimizer,[0],[0]
"(Formally, with multiple global optima with the same divergence, q(w) will concentrate equally around all minimizers.)
",2.2. Divergence Bound and its Minimizer,[0],[0]
"For the case where = 1 , it is easy to see that
D⇤1 = log Z
w p(w) exp ( KL (q(Z|w)kp(Z|w))) ,
(10) which will be zero if p(w) is only supported on w where the divergence between q(z|w) and p(z|w) is zero.",2.2. Divergence Bound and its Minimizer,[0],[0]
"This may initially seem like a strange condition, given that p(w) results from both the target distribution p(z) and the adjoined distribution p(w|z).",2.2. Divergence Bound and its Minimizer,[0],[0]
"However, the next section gives more specifics.
2.3.",2.2. Divergence Bound and its Minimizer,[0],[0]
"Specific Form for p(w|z)
",2.2. Divergence Bound and its Minimizer,[0],[0]
The results in the previous section were written without without considering the specific form of p(w|z).,2.2. Divergence Bound and its Minimizer,[0],[0]
"This paper takes the approach of defining
p(w|z)",2.2. Divergence Bound and its Minimizer,[0],[0]
"= r(w)q(z|w) rz , (11)
where r(w) is a possibly improper distribution over w and rz = R w r(w)q(z|w) is a normalizer.",2.2. Divergence Bound and its Minimizer,[0],[0]
"Note that p(w|z) is still well-defined as long as the integral defining rz exists.
",2.2. Divergence Bound and its Minimizer,[0],[0]
"Here, we assume for convenience that rz is a constant, not depending on z. Enforcing rz to be constant essentially means choosing r(w) in such a way that it doesn’t “favor” any z over any other since if q(w) / r(w) then q(z) is uniform over z. Lemma 5.",2.2. Divergence Bound and its Minimizer,[0],[0]
If p(w|z),2.2. Divergence Bound and its Minimizer,[0],[0]
=,2.2. Divergence Bound and its Minimizer,[0],[0]
"r(w)q(z|w)/rz and rz is a constant, then the solution in Thm. 3 holds with
s(w) = log r(w)",2.2. Divergence Bound and its Minimizer,[0],[0]
log rz +,2.2. Divergence Bound and its Minimizer,[0],[0]
Eqw(Z)[ 1 log p(Z) + (1 1) log qw(Z)].,2.2. Divergence Bound and its Minimizer,[0],[0]
"(12)
This gives a distribution over q(w) that can be written in various equivalent ways, such as
q⇤(w) / r(w) exp 1E(w) + ( 1 1)H(w) (13)
=r(w) exp H(w) 1KL (qw(Z)kp(Z))
",2.2. Divergence Bound and its Minimizer,[0],[0]
=r(w) exp1/ E qw(Z),2.2. Divergence Bound and its Minimizer,[0],[0]
[log p(Z),2.2. Divergence Bound and its Minimizer,[0],[0]
"+ ( 1) log qw(Z)] ,
where H(w) = Eqw(Z)[log qw(Z)] is the entropy of qw and E(w) = Eqw(z)[log p(Z)].
",2.2. Divergence Bound and its Minimizer,[0],[0]
"Here, r(w) plays a role similar to a base density in an exponential family.",2.2. Divergence Bound and its Minimizer,[0],[0]
"If one simply used r(w) = 1, then p(w|z) may not be well-defined.",2.2. Divergence Bound and its Minimizer,[0],[0]
"Further, without a base density, the marginal q(z) would depend on the particular parameterization of w. To see this, take single parameter w, and imagine re-parameterizing so the negative reals are “squashed” by a factor of two.",2.2. Divergence Bound and its Minimizer,[0],[0]
The base density must increase by a factor of two for the negative reals to compensate in order to leave the marginal q(z) unchanged under the reparameterization.,2.2. Divergence Bound and its Minimizer,[0],[0]
Adjoining p(w|z) to p(z),2.4. Latent variables in variational inference,[0],[0]
"to define D1 above is strongly related to auxiliary random variables (Agakov & Barber, 2004) explored in variational inference to increase the representative power of q, e.g. by including latent stochastic transition operators (Salimans et al., 2015), random Gaussian process mappings (Tran et al., 2016), or hierarchical variables (Ranganath et al., 2016).",2.4. Latent variables in variational inference,[0],[0]
"Imagine doing variational inference with q✓(z, w), where now w is auxiliary
and the goal is to set ✓ so that q✓(z) ⇡ p(z).",2.4. Latent variables in variational inference,[0],[0]
"Since w must be integrated out, the entropy of z under q is typically intractable, but can be bounded by augmenting p with some distribution p(w|z) and then optimizing the joint KLdivergence over z and w, similarly to Lemma 2.",2.4. Latent variables in variational inference,[0],[0]
Note two differences.,2.4. Latent variables in variational inference,[0],[0]
"First, here the distribution over w is mathematically derived to minimize the bound, while previous work numerically optimizes ✓ at run-time.",2.4. Latent variables in variational inference,[0],[0]
"Second, previous work also optimizes parameters of the distribution p(w|z) to further improve the bound, while here it is left fixed (for each ) for simplicity (Sec. 5).",2.4. Latent variables in variational inference,[0],[0]
Future work might explore optimizing p(w|z) at run-time in the current paper’s framework.,2.4. Latent variables in variational inference,[0],[0]
This section considers algorithms to sample from the distribution defined by Eq. 13.,3. Bayesian Inference,[0],[0]
"Probabilistic inference can be used in various settings, but for concreteness the rest of this paper will focus on Bayesian inference.",3. Bayesian Inference,[0],[0]
"To fix notation, suppose a set of N inputs xi with corresponding outputs yi.",3. Bayesian Inference,[0],[0]
"(In a generative setting, xi would be empty.)",3. Bayesian Inference,[0],[0]
"Then, if z is a vector of parameters, the posterior distribution over z is, up to a constant
log p(z) = log p0(z)",3. Bayesian Inference,[0],[0]
"+ NX
i=1
log p(yi|xi, z) +",3. Bayesian Inference,[0],[0]
"C, (14)
where p0(z) is the prior, and p(yi|xi, z) is the likelihood for the i-th datum.",3. Bayesian Inference,[0],[0]
The goal of probabilistic inference is to be able to evaluate expectations with respect to p(z).,3. Bayesian Inference,[0],[0]
The goal of MCMC methods is to obtain samples from the target distribution p(z).,3.1. Langevin Dynamics,[0],[0]
Langevin dynamics sample by an extremely simple process of repeating gradient steps of log(z) with injected Gaussian noise.,3.1. Langevin Dynamics,[0],[0]
"Specifically, the iterate is
z z + ✏ 2 r log p(z) + p ✏⌘, (15)
where ⌘ is sampled from a standard Multivariate Gaussian distribution and ✏ is a step-size that may decay over time.",3.1. Langevin Dynamics,[0],[0]
"If Langevin dynamics are used as a proposal for a MetropolisHastings sampler, it can be shown that correct acceptance ratio is
exp s(z0) s(z) +",3.1. Langevin Dynamics,[0],[0]
"✏
8 krs(z)k2 ✏ 8 krs(z0)k2
+
1
2
(z z0) · (rs(z) +rs(z0)) , (16)
where s(z) = log p(z) up to a constant factor, and z0 is the proposed point from Eq. 15.",3.1. Langevin Dynamics,[0],[0]
"It is easy to see that as ✏ becomes small, this acceptance ratio will go to one, and so one can disregard the acceptance step with some bias in the results determined by the step size.
",3.1. Langevin Dynamics,[0],[0]
"Langevin dynamics explore the space of z through a random walk, and thus are likely to be slower than alternatives such as Hamiltonian Monte Carlo when used as an exact method (Neal, 2010, Section 5.2).",3.1. Langevin Dynamics,[0],[0]
The main practical advantage of Langevin dynamics comes from the case where the number of data N is large.,3.1. Langevin Dynamics,[0],[0]
"In that case, one can use a minibatch of M elements and approximate r log p(z) as
r log p(z) '",3.1. Langevin Dynamics,[0],[0]
"r log p0(z)+ N
M
X
i2minibatch r log p(yi|xi, z).
",3.1. Langevin Dynamics,[0],[0]
"(17)
Thus, Stochastic Gradient Langevin Dynamics, avoid a full pass through the dataset in each iteration, and so can scale to large datasets.",3.1. Langevin Dynamics,[0],[0]
Though Eq. 17 is an unbiased estimate of the gradient of log p(z) this still adds a bias to the stationary distribution of the the chain.,3.1. Langevin Dynamics,[0],[0]
"(See (Mandt et al., 2016) for an analysis.)",3.1. Langevin Dynamics,[0],[0]
"In the limit of a small step-size, the variance of the noise due to stochastic estimation of the gradient of log p(z) will be of order ✏2 while the variance of the injected noise is of order ✏, meaning the latter dominates (Welling & Teh, 2011; Teh et al., 2016).",3.1. Langevin Dynamics,[0],[0]
"The goal of variational inference is to maximize
L(w) =",3.2. Stochastic Gradient Variational Inference,[0],[0]
"Eqw(Z)[log p(Z) log qw(Z)], (18)
equivalent to minimizing the KL divergence between qw(z) and p(z).",3.2. Stochastic Gradient Variational Inference,[0],[0]
"If it were possible to exactly compute L, this could be maximized (to a local optima) by a simple gradient iteration like
w w + ✏ 2 rwL(w).",3.2. Stochastic Gradient Variational Inference,[0],[0]
"(19)
While in some cases with specific p and q, one can derive exact updates of L (Ghahramani & Beal, 2000) in general one cannot exactly evaluate the expectation over Z in L. One line of approach to this (Ranganath et al., 2014; Salimans & Knowles, 2014) is to write the gradient as rL = Eqw(Z)[(log qw(Z) log p(Z))r log qw(Z)], and estimate this by drawing samples from qw(z).",3.2. Stochastic Gradient Variational Inference,[0],[0]
"This experimentally seems to result in gradients with large variance, but this can be reduced by two strategies.",3.2. Stochastic Gradient Variational Inference,[0],[0]
"Firstly, one can use control variates, based on either Taylor expansion (Paisley et al., 2012) or the fact that the expected value of rw log qw(Z) is zero.",3.2. Stochastic Gradient Variational Inference,[0],[0]
"Secondly, since log p(z) is often a sum of terms defined on subsets of variables, one can RaoBlackwellize by integrating out other variables.",3.2. Stochastic Gradient Variational Inference,[0],[0]
"This approach only needs to be able to compute log p(z)– no other access, even to the gradient of p, is needed.",3.2. Stochastic Gradient Variational Inference,[0],[0]
"Another line of approach to variational inference is based on the “reparameterization trick” (Kingma & Welling,
2014; Rezende et al., 2014; Titsias & Lázaro-Gredilla, 2014), known in the stochastic approximation literature as a “pathwise” derivative (Kushner & Yin, 2003, Sec. 2.5.1).",3.2.1. REPARAMETERIZATION TRICK,[0],[0]
"Suppose that there is a deterministic mapping zr,w from parameters w and random numbers r (from some fixed standard distribution) such that zr,w ⇠ qw(z).",3.2.1. REPARAMETERIZATION TRICK,[0],[0]
"Then, we could equivalently write
L(w) = ER[log",3.2.1. REPARAMETERIZATION TRICK,[0],[0]
"p(zR,w)]",3.2.1. REPARAMETERIZATION TRICK,[0],[0]
+H(w).,3.2.1. REPARAMETERIZATION TRICK,[0],[0]
"(20)
where H(w) = Eqw(Z) log qw(Z) denotes the entropy.",3.2.1. REPARAMETERIZATION TRICK,[0],[0]
The advantage of Eq. 20,3.2.1. REPARAMETERIZATION TRICK,[0],[0]
"this is that the expectation is not a function of w, and so if computing the gradient of L, the gradient moves inside the expectation.",3.2.1. REPARAMETERIZATION TRICK,[0],[0]
"Then, provided zr,w is differentiable with respect to w, an unbiased estimate of the gradient of L is available as
rwL(w) '",3.2.1. REPARAMETERIZATION TRICK,[0],[0]
"rw log p(zr,w) +rwH(w).",3.2.1. REPARAMETERIZATION TRICK,[0],[0]
"(21)
If H(w) cannot be integrated in closed-form an alternative estimator based on
L(w) = ER[log",3.2.1. REPARAMETERIZATION TRICK,[0],[0]
"p(zR,w) log qw(zR,w)]",3.2.1. REPARAMETERIZATION TRICK,[0],[0]
"(22)
is possible, and in some circumstances this can even have lower variance (Roeder et al., 2016).
",3.2.1. REPARAMETERIZATION TRICK,[0],[0]
"Computing the gradient of log p(zr,w) with respect to w amounts to computing the derivative of log p(z) with respect to z multiplied by the Jacobian dzTr,w/dw.",3.2.1. REPARAMETERIZATION TRICK,[0],[0]
"For simple distributions like Gaussians zr,w this is easy to do analytically (Section 5).",3.2.1. REPARAMETERIZATION TRICK,[0],[0]
"Alternatively, this can all be done efficiently by automatic differentiation, (Kucukelbir et al., 2017)",3.2.1. REPARAMETERIZATION TRICK,[0],[0]
"the approach used here.
",3.2.1. REPARAMETERIZATION TRICK,[0],[0]
"As with Langevin dynamics, with a large dataset, log p(z) can be approximated by taking a sum over a minibatch.",3.2.1. REPARAMETERIZATION TRICK,[0],[0]
The most obious approach to this would be to choose a single vector r and use it throughout the minibatch.,3.2.1. REPARAMETERIZATION TRICK,[0],[0]
"However, variance can often be reduced by the “local reparameterization trick” (Kingma et al., 2015) in which a different random vector ri is drawn for each datum in the minibatch.",3.2.1. REPARAMETERIZATION TRICK,[0],[0]
"Intuitively, the motivation is that if i and j are two data in the minibatch then log p(yi|xi, zr,w) should be less correlated with log p(yj |xj , zr0,w) when a different random vector r0 is used for the second datum (consider the limiting case where all data are identical).",3.2.1. REPARAMETERIZATION TRICK,[0],[0]
"Then, one can write the approximation as
rwL(w) ' 1
M
X
i2minibatch
⇣ rw log p0(zri,w)
+Nrw log p(yi|xi, zri,w) rw log qw(zri,w) ⌘ .",3.2.1. REPARAMETERIZATION TRICK,[0],[0]
"(23)
Note that the approximation on the right-hand side depends both on the choice of the minibatch and on the random vectors ri, one chosen per element of the minibatch.",3.2.1. REPARAMETERIZATION TRICK,[0],[0]
An algorithm that interpolates between the methods in the previous section results from applying Langevin dynamics to the distribution over w defined by Thm. 3,4. Hybrid Dynamics,[0],[0]
"If the adjoining distribution p(w|z) is chosen as in in Sec. 2.3 with some r (w) that depends on , then define
L(w) = log",4. Hybrid Dynamics,[0],[0]
"r (w) + Eqw(Z)[log p(z) + ( 1) log q(z|w)], (24)
which is times the form of s(w) derived in 5, with the constant term of log rz dropped.",4. Hybrid Dynamics,[0],[0]
"Applying Langevin Dynamics2 to this results in the iteration
w w + ✏ 2
rL(w)",4. Hybrid Dynamics,[0],[0]
"+ p ✏ ⌘, (25)
where again ✏ is a step-size that may decrease over time and ⌘ is sampled from a standard Gaussian distribution.
",4. Hybrid Dynamics,[0],[0]
"This paper uses a closed-form for the entropy H(w) = Eqw(Z) log qw(Z), and so uses the gradient estimator
rL(w) ' r log r (w) + (1 )rH(w) + Eqw(Z)[log p(z) + ( 1) log q(z|w)].",4. Hybrid Dynamics,[0],[0]
(26),4. Hybrid Dynamics,[0],[0]
"The following experiments use the simplest common variational distribution for qw(Z), namely a fully-factorized Gaussian distribution.",5. Specifics For Gaussians,[0],[0]
"To make w unconstrained, let w = (µ, ⌫) where µ is a vector of mean components and the standard deviation of the i-th dimension is i = 10⌫i .",5. Specifics For Gaussians,[0],[0]
"To sample from this distribution given a vector r, one can simply take zr,w = µ + r where is element-wise product.",5. Specifics For Gaussians,[0],[0]
"The entropy of qw(Z) is, up to constant factors H(w)",5. Specifics For Gaussians,[0],[0]
= P i ⌫,5. Specifics For Gaussians,[0],[0]
"i ln 10.
",5. Specifics For Gaussians,[0],[0]
It remains to set the base density.,5. Specifics For Gaussians,[0],[0]
These experiments used an improper prior r (w) / Q,5. Specifics For Gaussians,[0],[0]
"i N (⌫i|u , 1) that is uniform over µ with a Gaussian prior on ⌫ with a fixed variance of 1 and a mean u .",5. Specifics For Gaussians,[0],[0]
"Since this is uniform over µ, rz is a constant.",5. Specifics For Gaussians,[0],[0]
The value u was calculated to numerically optimize the divergence bound D⇤ for a one-dimensional standard Gaussian p(z).,5. Specifics For Gaussians,[0],[0]
"Since D⇤ = A at the solution (Theorem 3), for any given and u , D⇤ can be evaluated by symbolically integrating out µ and then numerically integrating over ⌫.",5. Specifics For Gaussians,[0],[0]
"The values used in these experiments are below, with linear interpolation used for any other .
",5. Specifics For Gaussians,[0],[0]
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 u -.33 -.472 -.631 -.792,5. Specifics For Gaussians,[0],[0]
"-.953 -1.11 -1.29 -1.49 -1.74 -2.10 -10
2For some step-size ✏0, the Langevin dynamics would immediately be w w + (✏0/2)rs(w) + p ✏0⌘.",5. Specifics For Gaussians,[0],[0]
"If we define ✏ = ✏0, then this results in the given form since ✏0rs(w) =",5. Specifics For Gaussians,[0],[0]
"(✏/ )rs(w) = ✏rL(w).
",5. Specifics For Gaussians,[0],[0]
"Theoretically, it’s easy to show that as ! 1, D⇤ is minimized (with a minimum of zero) by u ! 1, meaning that after one iteration, ⌫ = 1, and henceforth, zr,w = µ.",5. Specifics For Gaussians,[0],[0]
"Thus, Eq. 25 is equivalent to Langevin dynamics as in Eq. 15.",5. Specifics For Gaussians,[0],[0]
"However, using u = 10, r(w) (a prior centered on a standard deviation of = 10 10) is practically equivalent and is more useful to guide linear interpolation.",5. Specifics For Gaussians,[0],[0]
"Meanwhile, when ! 0, the algorithm is independent of r(w), though the optimal value of u ⇡ .33 is correct for small .
",5. Specifics For Gaussians,[0],[0]
The above choice of r (w) is quite arbitrary.,5. Specifics For Gaussians,[0],[0]
"It is possible that other choices could be better, though performance was surprisingly insensitive in practice.",5. Specifics For Gaussians,[0],[0]
With large datasets log p(z) dominates log r(w) just as likelihoods dominate Bayesian priors.,5. Specifics For Gaussians,[0],[0]
"Calling on inspiration from recent work in variational inference, it might also be useful to optimize p(w|z) at runtime (Section 2.4).",5. Specifics For Gaussians,[0],[0]
There does not appear to be a single standard performance measure to evaluate approximate inference algorithms.,6. Experiments,[0],[0]
"For Bayesian inference, test-set accuracy or likelihood are sometimes used, but these measures have high variance due to the dataset, and conflate evaluation of the inference method with evaluation of the model it is running on.",6. Experiments,[0],[0]
"(An inference method with poor coverage of p(z) can have higher accuracy than a perfect sampler due to model mis-specification or dataset peculiarities.)
",6. Experiments,[0],[0]
"For a more fine-grained measure of inference performance, this paper uses the Maximum Mean Discrepancy (MMD) measure (Gretton et al., 2006; 2012).",6. Experiments,[0],[0]
MMD is essentially the empirical difference of the means of two samples in some feature space.,6. Experiments,[0],[0]
"We first draw a sample from p(z) by exhaustively running a traditional MCMC algorithm (Stan (Team, 2016), based on a variant of Hamil-
tonian Monte Carlo (Hoffman & Gelman, 2014)) for a large number of iterations, and compare this to the results of approximate inference.",6. Experiments,[0],[0]
"Since we have large samples, and want to compute the online performance at all iterations, a finite-dimensional feature space must be used to avoid the quadratic complexity of computing MMD using an arbitrary kernel.",6. Experiments,[0],[0]
"For two-dimensional problems, it was beneficial to use random Fourier features (Rahimi & Recht, 2007) to approximate the radial-basis function kernel k(x, y) = exp",6. Experiments,[0],[0]
( kx yk2).,6. Experiments,[0],[0]
"For higher dimensions, the original feature space was sufficiently discriminative.
",6. Experiments,[0],[0]
Approximate inference gives a sequence of vectors wt.,6. Experiments,[0],[0]
"For each time-step, a set of 100 samples At are drawn from q(z|wt).",6. Experiments,[0],[0]
"Then, the MMD between A1[A2[ · · ·At and the sample from exhaustive MCMC can be computed for all t in linear time.",6. Experiments,[0],[0]
"A disadvantage of this approach is the need for exhaustive MCMC for comparison, which by definition eludes the large-scale cases that motivate Langevin dynamics and stochastic variational inference.",6. Experiments,[0],[0]
However this gives a more fine-grained view of performance.,6. Experiments,[0],[0]
"For a first demonstration, the algorithm was applied to a set of toy 2-dimensional distributions, with various values of .",6.1. Toy Distributions,[0],[0]
"A few results are shown in Fig. 1, with more in
the appendix.",6.1. Toy Distributions,[0],[0]
"While the algorithm displays the expected trade-offs between speed and accuracy for different (Fig. 2), the results fairly uninteresting as all curves cross near the same time/MMD point where = 0 (variational inference) and = 1 (MCMC) meet.",6.1. Toy Distributions,[0],[0]
"So, for any given amount of time, either variational inference or MCMC are nearoptimal, and so intermediate do not much expand the “Pareto frontier” on these problems, although one might prefer an intermediate since the crossover horizon is not known in advance.",6.1. Toy Distributions,[0],[0]
"Next, the algorithm was applied to binary logistic regression with several standard datasets, using a minibatch size of 25, a different random vector r for each element in the minibatch, and a standard multivariate Laplace distribution as a prior.",6.2. Bayesian Logistic Regression,[0],[0]
"Simply picking a single value or schedule for the step ✏ is unsatisfactory, as the best schedule for a given problem, length of time, and value of varies.",6.2. Bayesian Logistic Regression,[0],[0]
"To give a fair comparison, inference was run for with a range of constant ✏ varying by factors of two from 23/N to 2 2/N .",6.2. Bayesian Logistic Regression,[0],[0]
This spans the range from large enough to often diverge to below optimal even after 106 iterations.,6.2. Bayesian Logistic Regression,[0],[0]
"Then, for each time, the best value of ✏ was retrospectively chosen (with performance averaged over repetitions before this choice).",6.2. Bayesian Logistic Regression,[0],[0]
"It is likely that decaying step-size schedules would perform
better, but this was not pursued since it would reduce transparency of the results.
",6.2. Bayesian Logistic Regression,[0],[0]
"The mean errors after 100 repetitions are shown in Fig. 4, while samples are compared to the ground truth for ionosphere in Fig. 3.",6.2. Bayesian Logistic Regression,[0],[0]
"This confirms the basic ideas of the algorithm, namely that it smoothly interpolates between the two previous algorithms without introducing any new behavior.",6.2. Bayesian Logistic Regression,[0],[0]
The main experimental finding is that there is usually a large range of time horizons for which an intermediate setting of performs better than either of the previous existing algorithms.,6.2. Bayesian Logistic Regression,[0],[0]
"This suggests that, say, someone who has a time budget slightly larger than that needed for variational inference could benefit from running the algorithm with a small value of for a slightly larger time.",6.2. Bayesian Logistic Regression,[0],[0]
This is in contrast to the toy two-dimensional examples where for almost all time horizons either = 0 or = 1 was optimal.,6.2. Bayesian Logistic Regression,[0],[0]
This paper has two contributions.,7. Discussion,[0],[0]
"First, a distribution over variational parameters is derived to minimize a bound on the marginal divergence to a target distribution.",7. Discussion,[0],[0]
"Secondly, this is used to derive an algorithm that interpolates between Langevin dynamics and stochastic variational inference.",7. Discussion,[0],[0]
"The Bayesian logistic regression experiments show that the intermediate values of this algorithm are useful in the sense that there are several orders of magnitude of time horizons where an intermediate algorithm performs better than either Langevin dynamics or variational inference.
",7. Discussion,[0],[0]
Many improvements to stochastic Langevin dynamics and variational inference have been proposed beyond the simple algorithms used here.,7. Discussion,[0],[0]
"For Langevin dynamics, Welling & Teh (2011) and Li et al. (2016) suggest adding a preconditioner, Patterson & Teh (2013) propose Riemannian Langevin Dynamics, and Dubey et al. (2016) propose to make use of the finite sum structure in a Bayesian posterior p(z) by incorporating techniques for incremental optimization.",7. Discussion,[0],[0]
"For stochastic variational inference, Hoffman et al.
(2013) propose to use the natural gradient, Mandt & Blei (2014) propose using smoothed gradients, and Sakaya & Klami (2016) propose a strategy of re-using previous gradient computations.",7. Discussion,[0],[0]
"It would be interesting to consider using these ideas with the hybrid algorithm, which could give insight into the relationship between the two different methods.
",7. Discussion,[0],[0]
There is other work based on combining the advantages of variational inference and MCMC.,7. Discussion,[0],[0]
"Stochastic Gradient Fisher Scoring (Ahn et al., 2012) interpolates between Langevin dynamics and a Gaussian approximation of the target.",7. Discussion,[0],[0]
"However, this approximation is based on the Bayesian central limit theorem and so will not be the same in general as the optimal variational distribution, even when a Gaussian is used as the variational family.",7. Discussion,[0],[0]
"Another line of research is that of interpreting the path of a MCMC algorithm as a variational distribution, and then fitting parameters to tighten a variational bound (Salimans et al., 2015; Rezende & Mohammed, 2015).",7. Discussion,[0],[0]
"While motivationally quite similar, the contribution of this paper is orthogonal, in that one could conceivably use these same ideas to define the variational family qw(z) used in this paper.",7. Discussion,[0],[0]
This would result in a somewhat unusual method that runs MCMC over the parameters of variational family of distributions that are themselves defined in terms of a (different) MCMC algorithm.,7. Discussion,[0],[0]
Two popular classes of methods for approximate inference are Markov chain Monte Carlo (MCMC) and variational inference.,abstractText,[0],[0]
"MCMC tends to be accurate if run for a long enough time, while variational inference tends to give better approximations at shorter time horizons.",abstractText,[0],[0]
"However, the amount of time needed for MCMC to exceed the performance of variational methods can be quite high, motivating more fine-grained tradeoffs.",abstractText,[0],[0]
"This paper derives a distribution over variational parameters, designed to minimize a bound on the divergence between the resulting marginal distribution and the target, and gives an example of how to sample from this distribution in a way that interpolates between the behavior of existing methods based on Langevin dynamics and stochastic gradient variational inference (SGVI).",abstractText,[0],[0]
A Divergence Bound for Hybrids of MCMC and Variational Inference and an Application to Langevin Dynamics and SGVI,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2388–2397, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
"Many complex speech and natural language processing (NLP) pipelines such as Automatic Speech Recognition (ASR) and Statistical Machine Translation (SMT) systems store alternative hypotheses produced at various stages of processing as weighted acyclic automata, also known as lattices.",1 Introduction,[0],[0]
Each lattice stores a large number of hypotheses along with the raw system scores assigned to them.,1 Introduction,[0],[0]
"While single-best hypothesis is
typically what is desired at the end of the processing, it is often beneficial to consider a large number of weighted hypotheses at earlier stages of the pipeline to hedge against errors introduced by various subcomponents.",1 Introduction,[0],[0]
"Standard ASR and SMT techniques like discriminative training, rescoring with complex models and Minimum Bayes-Risk (MBR) decoding rely on lattices to represent intermediate system hypotheses that will be further processed to improve models or system output.",1 Introduction,[0],[0]
"For instance, lattice based MBR decoding has been shown to give moderate yet consistent gains in performance over conventional MAP decoding in a number of speech and NLP applications including ASR (Goel and Byrne, 2000) and SMT (Tromble et al., 2008; Blackwood et al., 2010; de Gispert et al., 2013).
",1 Introduction,[0],[0]
Most lattice-based techniques employed by speech and NLP systems make use of posterior quantities computed from probabilistic lattices.,1 Introduction,[0],[0]
"In this paper, we are interested in two such posterior quantities: i) n",1 Introduction,[0],[0]
"-gram expected count, the expected number of occurrences of a particular n-gram in a lattice, and ii) n-gram posterior probability, the total probability of accepting paths that include a particular n-gram.",1 Introduction,[0],[0]
"Expected counts have applications in the estimation of language model statistics from probabilistic input such as ASR lattices (Allauzen et al., 2003) and the estimation term frequencies from spoken corpora while posterior probabilities come up in MBR decoding of SMT lattices (Tromble et al., 2008), relevance ranking of spoken utterances and the estimation of document frequencies from spoken corpora (Karakos et al., 2011; Can and Narayanan, 2013).
",1 Introduction,[0],[0]
"The expected count c(x|A) of n-gram x given lattice A is defined as
c(x|A) = ∑ y∈Σ∗ #y(x)p(y|A) (1)
where #y(x) is the number of occurrences of n-
2388
gram x in hypothesis y and p(y|A) is the posterior probability of hypothesis y given lattice A.",1 Introduction,[0],[0]
"Similarly, the posterior probability p(x|A) of n-gram x given lattice A is defined as
p(x|A) = ∑ y∈Σ∗ 1y(x)p(y|A) (2)
where 1y(x) is an indicator function taking the value 1 when hypothesis y includes n-gram x and 0 otherwise.",1 Introduction,[0],[0]
"While it is straightforward to compute these posterior quantities from weighted nbest lists by examining each hypothesis separately and keeping a separate accumulator for each observed n-gram type, it is infeasible to do the same with lattices due to the sheer number of hypotheses stored.",1 Introduction,[0],[0]
"There are efficient algorithms in literature (Allauzen et al., 2003; Allauzen et al., 2004) for computing n-gram expected counts from weighted automata that rely on weighted finite state transducer operations to reduce the computation to a sum over n-gram occurrences eliminating the need for an explicit sum over accepting paths.",1 Introduction,[0],[0]
"The rather innocent looking difference between Equations 1 and 2, #y(x) vs. 1y(x), makes it hard to develop similar algorithms for computing n-gram posteriors from weighted automata since the summation of probabilities has to be carried out over paths rather than n-gram occurrences (Blackwood et al., 2010; de Gispert et al., 2013).
",1 Introduction,[0],[0]
"The problem of computing n-gram posteriors from lattices has been addressed by a number of recent works (Tromble et al., 2008; Allauzen et al., 2010; Blackwood et al., 2010; de Gispert et al., 2013) in the context of lattice-based MBR for SMT.",1 Introduction,[0],[0]
"In these works, it has been reported that the time required for lattice MBR decoding is dominated by the time required for computing n-gram posteriors.",1 Introduction,[0],[0]
"Our interest in computing n-gram posteriors from lattices stems from its potential applications in spoken content retrieval (Chelba et al., 2008; Karakos et al., 2011; Can and Narayanan, 2013).",1 Introduction,[0],[0]
Computation of document frequency statistics from spoken corpora relies on estimating ngram posteriors from ASR lattices.,1 Introduction,[0],[0]
"In this context, a spoken document is simply a collection of ASR lattices.",1 Introduction,[0],[0]
"The n-grams of interest can be word, syllable, morph or phoneme sequences.",1 Introduction,[0],[0]
"Unlike in the case of lattice-based MBR for SMT where the n-grams of interest are relatively short – typically up to 4-grams –, the n-grams we are interested in
are in many instances relatively long sequences of subword units.
",1 Introduction,[0],[0]
"In this paper, we present an efficient algorithm for computing the posterior probabilities of all ngrams in a lattice and constructing a minimal deterministic weighted finite-state automaton associating each n-gram with its posterior for efficient storage and retrieval.",1 Introduction,[0],[0]
"Our n-gram posterior computation algorithm builds upon the custom forward procedure described in (de Gispert et al., 2013) and introduces a number of refinements to significantly improve the time and space requirements:
•",1 Introduction,[0],[0]
"The custom forward procedure described in (de Gispert et al., 2013) computes unigram posteriors from an input lattice.",1 Introduction,[0],[0]
Higher order n-gram posteriors are computed by first transducing the input lattice to an n-gram lattice using an order mapping transducer and then running the custom forward procedure on this higher order lattice.,1 Introduction,[0],[0]
We reformulate the custom forward procedure as a dynamic programming algorithm that computes posteriors for successively longer n-grams and reuses the forward scores computed for the previous order.,1 Introduction,[0],[0]
"This reformulation subsumes the transduction of input lattices to n-gram lattices and obviates the need for constructing and applying order mapping transducers.
",1 Introduction,[0],[0]
"• Comparing Eq. 1 with Eq. 2, we can observe that posterior probability and expected count are equivalent for an n-gram that do not repeat on any path of the input lattice.",1 Introduction,[0],[0]
The key idea behind our algorithm is to limit the costly posterior computation to only those ngrams that can potentially repeat on some path of the input lattice.,1 Introduction,[0],[0]
We keep track of repeating n-grams of order n and use a simple impossibility argument to significantly reduce the number of n-grams of order n + 1 for which posterior computation will be performed.,1 Introduction,[0],[0]
The posteriors for the remaining n-grams are replaced with expected counts.,1 Introduction,[0],[0]
"This filtering of n-grams introduces a slight bookkeeping overhead but in return dramatically reduces the runtime and memory requirements for long n-grams.
",1 Introduction,[0],[0]
• We store the posteriors for n-grams that can potentially repeat on some path of the input lattice in a weighted prefix tree that we construct on the fly.,1 Introduction,[0],[0]
"Once that is done, we com-
pute the expected counts for all n-grams in the input lattice and represent them as a minimal deterministic weighted finite-state automaton, known as a factor automaton (Allauzen et al., 2004; Mohri et al., 2007), using the approach described in (Allauzen et al., 2004).",1 Introduction,[0],[0]
Finally we use general weighted automata algorithms to merge the weighted factor automaton representing expected counts with the weighted prefix tree representing posteriors to obtain a weighted factor automaton representing posteriors that can be used for efficient storage and retrieval.,1 Introduction,[0],[0]
"This section introduces the definitions and notation related to weighted finite state automata and transducers (Mohri, 2009).",2 Preliminaries,[0],[0]
Definition 1,2.1 Semirings,[0],[0]
"A semiring is a 5-tuple (K,⊕,⊗, 0, 1) where (K,⊕, 0) is a commutative monoid, (K,⊗, 1) is a monoid, ⊗ distributes over ⊕ and 0 is an annihilator for ⊗.
Table 1 lists common semirings.",2.1 Semirings,[0],[0]
"In speech and language processing, two semirings are of particular importance.",2.1 Semirings,[0],[0]
The log semiring is isomorphic to the probability semiring via the negative-log morphism and can be used to combine probabilities in the log domain.,2.1 Semirings,[0],[0]
"The tropical semiring, provides the algebraic structure necessary for shortest-path algorithms and can be derived from the log semiring using the Viterbi approximation.",2.1 Semirings,[0],[0]
Definition 2 A weighted finite-state automaton (WFSA),2.2 Weighted Finite-State Automata,[0],[0]
"A over a semiring (K,⊕,⊗, 0, 1) is a 7- tuple",2.2 Weighted Finite-State Automata,[0],[0]
"A = (Σ, Q, I, F,E, λ, ρ) where: Σ is the finite input alphabet; Q is a finite set of states; I, F ⊆ Q are respectively the set of initial and
final states;",2.2 Weighted Finite-State Automata,[0],[0]
E ⊆ Q ×,2.2 Weighted Finite-State Automata,[0],[0]
"(Σ ∪ {ε}) × K × Q is a finite set of arcs; λ : I → K, ρ : F → K are respectively the initial and final weight functions.
",2.2 Weighted Finite-State Automata,[0],[0]
"Given an arc e ∈ E, we denote by i[e] its input label, w[e] its weight, s[e] its source or origin state and t[e] its target or destination state.",2.2 Weighted Finite-State Automata,[0],[0]
"A path π = e1 · · · ek is an element ofE∗ with consecutive arcs satisfying t[ei−1] = s[ei], i = 2, . . .",2.2 Weighted Finite-State Automata,[0],[0]
", k. We extend t and s to paths by setting t[π] = s[ek] and s[π] = t[e1].",2.2 Weighted Finite-State Automata,[0],[0]
The labeling and the weight functions can also be extended to paths by defining i[π] = i[e1] . . .,2.2 Weighted Finite-State Automata,[0],[0]
i[ek] and w[π] = w[e1] ⊗ . . .,2.2 Weighted Finite-State Automata,[0],[0]
⊗ w[ek].,2.2 Weighted Finite-State Automata,[0],[0]
"We denote by Π(q, q′) the set of paths from q to q′ and by Π(q, x, q′) the set of paths from q to q′ with input string x ∈",2.2 Weighted Finite-State Automata,[0],[0]
"Σ∗. These definitions can also be extended to subsets S, S′ ⊆ Q, e.g.
Π(S, x, S′) =",2.2 Weighted Finite-State Automata,[0],[0]
"⋃
q∈S,q′∈S′ Π(q, x, q′).
",2.2 Weighted Finite-State Automata,[0],[0]
"An accepting path in an automaton A is a path in Π(I, F ).",2.2 Weighted Finite-State Automata,[0],[0]
A string x is accepted byA if there exists an accepting path π labeled with,2.2 Weighted Finite-State Automata,[0],[0]
x. A is deterministic if it has at most one initial state and at any state no two outgoing transitions share the same input label.,2.2 Weighted Finite-State Automata,[0],[0]
"The weight associated by an automaton A to a string x ∈ Σ∗ is given by
JAK(x) = ⊕ π∈Π(I,x,F ) λ(s[π])⊗ w[π]⊗ ρ(t[π])
and JAK(x) , 0 when Π(I, x, F ) = ∅.",2.2 Weighted Finite-State Automata,[0],[0]
"A weighted automatonA defined over the probability semiring (R+,+,×, 0, 1) is said to be probabilistic if for any state q ∈ Q, the sum of the weights of all cycles at q, ⊕π∈Π(q,q)w[π], is well-defined and in R+ and ∑ x∈Σ∗JAK(x) = 1.",2.2 Weighted Finite-State Automata,[0],[0]
"We denote by Φn the n-gram mapping transducer (Blackwood et al., 2010; de Gispert et al., 2013)
of order n.",2.3 N-gram Mapping Transducer,[0],[0]
"This transducer maps label sequences to n-gram sequences of order n. Φn is similar in form to the weighted finite-state transducer representation of a backoff n-gram language model (Allauzen et al., 2003).",2.3 N-gram Mapping Transducer,[0],[0]
"We denote by An the ngram lattice of order n obtained by composing lattice A with Φn, projecting the resulting transducer onto its output labels, i.e. n-grams, to obtain an automaton, removing ε-transitions, determinizing and minimizing (Mohri, 2009).",2.3 N-gram Mapping Transducer,[0],[0]
An is a compact lattice of n-gram sequences of order n consistent with the labels and scores of lattice A.,2.3 N-gram Mapping Transducer,[0],[0]
An typically has more states than A due to the association of distinct n-gram histories with states.,2.3 N-gram Mapping Transducer,[0],[0]
Definition 3,2.4 Factor Automata,[0],[0]
"Given two strings x, y ∈ Σ∗, x is a factor (substring) of y",2.4 Factor Automata,[0],[0]
"if y = uxv for some u, v ∈ Σ∗. More generally, x is a factor of a language L ⊆ Σ∗",2.4 Factor Automata,[0],[0]
if x is a factor of some string y ∈,2.4 Factor Automata,[0],[0]
L.,2.4 Factor Automata,[0],[0]
The factor automaton S(y) of a string y is the minimal deterministic finite-state automaton recognizing exactly the set of factors of y.,2.4 Factor Automata,[0],[0]
"The factor automaton S(A) of an automaton A is the minimal deterministic finite-state automaton recognizing exactly the set of factors of A, that is the set of factors of the strings accepted by A.
Factor automaton (Mohri et al., 2007) is an efficient and compact data structure for representing a full index of a set of strings, i.e. an automaton.",2.4 Factor Automata,[0],[0]
It can be used to determine if a string x is a factor in time linear in its length O(|x|).,2.4 Factor Automata,[0],[0]
"By associating a weight with each factor, we can generalize the factor automaton structure to weighted automata and use it for efficient storage and retrieval of n-gram posteriors and expected counts.",2.4 Factor Automata,[0],[0]
"In this section we present an efficient algorithm based on the n-gram posterior computation algorithm described in (de Gispert et al., 2013) for computing the posterior probabilities of all ngrams in a lattice and constructing a weighted factor automaton for efficient storage and retrieval of these posteriors.",3 Computation of N-gram Posteriors,[0],[0]
We assume that the input lattice is an ε-free acyclic probabilistic automaton.,3 Computation of N-gram Posteriors,[0],[0]
"If that is not the case, we can use general weighted automata ε-removal and weight-pushing algorithms (Mohri, 2009) to preprocess the input automaton.
",3 Computation of N-gram Posteriors,[0],[0]
"Algorithm 1 reproduces the original algorithm of (de Gispert et al., 2013) in our no-
tation.",3 Computation of N-gram Posteriors,[0],[0]
"Each iteration of the outermost loop starting at line 1 computes posterior probabilities of all unigrams in the n-gram lattice An = (Σn, Qn, In, Fn, En, λn, ρn), or equivalently all n-grams of order n in the lattice A.",3 Computation of N-gram Posteriors,[0],[0]
"The inner loop starting at line 6 is essentially a custom forward procedure computing not only the standard forward probabilities α[q], the marginal probability of paths that lead to state q,
α[q] = ⊕
π ∈Π(I,q) λ(s[π])⊗ w[π] (3)
= ⊕ e∈E t[e] = q α[s[e]]⊗ w[e] (4)
but also the label specific forward probabilities α̃[q][x], the marginal probability of paths that lead to state q and include label x.
α̃[q][x] = ⊕
π ∈Π(I,q) ∃u,v ∈Σ∗: i[π] =uxv
λ(s[π])⊗ w[π] (5)
= ⊕ e∈E t[e] = q
i[e] =x
α[s[e]]⊗ w[e]
⊕ ⊕ e∈E t[e] = q
i[e] 6=x
α̃[s[e]][x]⊗ w[e] (6)
Just like in the case of the standard forward algorithm, visiting states in topological order ensures that forward probabilities associated with a state has already been computed when that state is visited.",3 Computation of N-gram Posteriors,[0],[0]
"At each state s, the algorithm examines each arc e = (s, x, w, q) and updates the forward probabilities for state q in accordance with the recursions in Equations 4 and 6 by propagating the forward probabilities computed for s (lines 8-12).",3 Computation of N-gram Posteriors,[0],[0]
"The conditional on line 11 ensures that the label specific forward probability α̃[s][y] is propagated to state q only if label y is different from label x, the label on the current arc.",3 Computation of N-gram Posteriors,[0],[0]
"In other words, if a label y repeats on some path π leading to state q, then π contributes to α̃[q][y] only once.",3 Computation of N-gram Posteriors,[0],[0]
This is exactly what is required by the indicator function in Equation 2 when computing unigram posteriors.,3 Computation of N-gram Posteriors,[0],[0]
"Whenever a final state is processed, the posterior probability accumulator for each label observed on paths reaching that state is updated by multiplying the label specific forward probability and the final weight associated with that state
Algorithm 1 Compute N-gram Posteriors 1 for n← 1, . . .",3 Computation of N-gram Posteriors,[0],[0]
", N do 2 An←Min(Det(RmEps(ProjOut(A ◦",3 Computation of N-gram Posteriors,[0],[0]
Φn)))),3 Computation of N-gram Posteriors,[0],[0]
"3 α[q]← λn(q), ∀ state q ∈",3 Computation of N-gram Posteriors,[0],[0]
"Qn 4 α̃[q][x]← 0, ∀ state q ∈ Qn, ∀ label x ∈",3 Computation of N-gram Posteriors,[0],[0]
"Σn 5 p(x|A)← 0, ∀ label x ∈",3 Computation of N-gram Posteriors,[0],[0]
Σn 6 for each state s ∈,3 Computation of N-gram Posteriors,[0],[0]
Qn do .,3 Computation of N-gram Posteriors,[0],[0]
"In topological order 7 for each arc (s, x, w, q) ∈ En do 8 α[q]← α[q]⊕ α[s]⊗ w 9 α̃[q][x]← α̃[q][x]⊕ α[s]⊗ w
10 for each label y ∈ α̃[s] do 11 if y 66= x then 12 α̃[q][y]← α̃[q][y]⊕ α̃[s][y]⊗",3 Computation of N-gram Posteriors,[0],[0]
"w 13 if s ∈ Fn then 14 for each label x ∈ α̃[s] do 15 p(x|A)← p(x|A)⊕ α̃[s][x]⊗ ρn(s) 16 P ←Min(ConstructPrefixTree(p))
and adding the resulting value to the accumulator (lines 13-15).",3 Computation of N-gram Posteriors,[0],[0]
"It should be noted that this algorithm is a form of marginalization (de Gispert et al., 2013), rather than a counting procedure, due to the conditional on line 11.",3 Computation of N-gram Posteriors,[0],[0]
"If that conditional were to be removed, this algorithm would compute n-gram expected counts instead of posterior probabilities.
",3 Computation of N-gram Posteriors,[0],[0]
The key idea behind our algorithm is to restrict the computation of posteriors to only those n-grams that may potentially repeat on some path of the input lattice and exploit the equivalence of expected counts and posterior probabilities for the remaining n-grams.,3 Computation of N-gram Posteriors,[0],[0]
It is possible to extend Algorithm 1 to implement this restriction by keeping track of repeating n-grams of order n and replacing the output labels of appropriate arcs in Φn+1 with ε labels.,3 Computation of N-gram Posteriors,[0],[0]
Alternatively we can reformulate Algorithm 1 as in Algorithm 2.,3 Computation of N-gram Posteriors,[0],[0]
In this formulation we compute n-gram posteriors directly on the input lattice A without constructing the n-gram lattice An.,3 Computation of N-gram Posteriors,[0],[0]
We explicitly associate states in the original lattice with distinct n-gram histories which is implicitly done in Algorithm 1 by constructing the n-gram lattice An.,3 Computation of N-gram Posteriors,[0],[0]
This explicit association lets us reuse forward probabilities computed at order n while computing the forward probabilities at order n + 1.,3 Computation of N-gram Posteriors,[0],[0]
"Further, we can directly restrict the n-grams for which posterior computation will be performed.
",3 Computation of N-gram Posteriors,[0],[0]
"In Algorithm 2, ά[n][q][h] represents the his-
tory specific forward probability of state q, the marginal probability of paths that lead to state q and include length n string h as a suffix.
ά[n][q][h] = ⊕
π ∈Π(I,q) ∃ z",3 Computation of N-gram Posteriors,[0],[0]
"∈Σ∗: i[π] = zh
λ(s[π])⊗ w[π] (7)
= ⊕ e∈E t[e] = q
g ∈ ά[n−1][s[e]] gi[e] =h
ά[n− 1][s[e]][g]⊗ w[e]
(8)
ά[n][q][h] is the analogue of α[q] in Algorithm 1.",3 Computation of N-gram Posteriors,[0],[0]
It splits the forward probability of state q,3 Computation of N-gram Posteriors,[0],[0]
"(Equation 3), among length n suffixes (or histories) of paths that lead to state q. We can interpret ά[n][q][h] as the forward probability of state (q, h) in the n-gram lattice An+1.",3 Computation of N-gram Posteriors,[0],[0]
"Here (q, h) ∈ Qn+1 denotes the unique state corresponding to state q in the original lattice A and state h in the mapping transducer Φn+1.",3 Computation of N-gram Posteriors,[0],[0]
"α̂[q][h][x] represents the history and n-gram specific forward probability of state q, the marginal probability of paths that lead to state q, include length n − 1 string h as a suffix and
Algorithm 2 Compute N-gram Posteriors (Reformulation) 1",3 Computation of N-gram Posteriors,[0],[0]
"R[0]← {ε} 2 ά[0][q][ε]← α[q], ∀ state q ∈ Q 3 for n← 1, . . .",3 Computation of N-gram Posteriors,[0],[0]
", N do 4 R[n]← ∅ 5 ά[n][q][x]← 0, ∀ state q ∈ Q, ∀ ngram x ∈",3 Computation of N-gram Posteriors,[0],[0]
"Σn 6 α̂[q][h][x]← 0, ∀ state q ∈ Q, ∀ history h ∈ Σn−1, ∀ ngram x ∈",3 Computation of N-gram Posteriors,[0],[0]
"Σn 7 p(x|A)← 0, ∀ ngram x ∈",3 Computation of N-gram Posteriors,[0],[0]
Σn 8 for each state s ∈ Q do .,3 Computation of N-gram Posteriors,[0],[0]
"In topological order 9 for each history g ∈ ά[n− 1][s] where g ∈ R[n− 1] do
10 for each arc (s, i, w, q) ∈ E",3 Computation of N-gram Posteriors,[0],[0]
do 11 x←,3 Computation of N-gram Posteriors,[0],[0]
gi .,3 Computation of N-gram Posteriors,[0],[0]
Concatenate history and label 12 h←,3 Computation of N-gram Posteriors,[0],[0]
x[1 : n] .,3 Computation of N-gram Posteriors,[0],[0]
Drop first label 13 if h ∈ R[n− 1] then 14 ά[n][q][x]← ά[n][q][x]⊕ ά[n− 1][s][g]⊗ w 15 α̂[q][h][x]← α̂[q][h][x]⊕ ά[n−,3 Computation of N-gram Posteriors,[0],[0]
1][s][g]⊗ w 16 for each ngram y ∈ α̂[s][g] do 17 if y 66= x then 18 α̂[q][h][y]← α̂[q][h][y]⊕ α̂[s][g][y]⊗,3 Computation of N-gram Posteriors,[0],[0]
w 19 else 20 R[n]← R[n] ∪ {y} 21 if s ∈ F then 22 for each history g ∈ α̂[s] do 23 for each ngram x ∈ α̂[s][g] do 24 p(x|A)← p(x|A)⊕ α̂[s][g][x]⊗ ρ(s),3 Computation of N-gram Posteriors,[0],[0]
"25 P ′← ConstructPrefixTree(p) 26 C ← ComputeExpectedCounts(A,N) 27 P ←Min(Det(RmEps((C",3 Computation of N-gram Posteriors,[0],[0]
"− RmWeight(P ′))⊕ P ′)))
",3 Computation of N-gram Posteriors,[0],[0]
"include n-gram x as a substring.
α̂[q][h][x] = ⊕
π ∈Π(I,q) ∃ z",3 Computation of N-gram Posteriors,[0],[0]
"∈Σ∗: i[π] = zh ∃u,v ∈Σ∗: i[π] =uxv
λ(s[π])⊗ w[π]
(9) = ⊕ e∈E t[e] = q
g ∈ ά[|h|][s[e]] gi[e] =x
ά[|h|][s[e]][g]⊗ w[e]
⊕ ⊕ e∈E t[e] = q
g ∈ α̂[s[e]] gi[e] 6=x
α̂[s[e]][g][x]⊗",3 Computation of N-gram Posteriors,[0],[0]
"w[e]
(10)
α̂[q][h][x] is the analogue of α̃[q][x] in Algorithm 1.",3 Computation of N-gram Posteriors,[0],[0]
"R[n] represents the set of n-grams of order n
that repeat on some path of A. We start by defining R[0] , {ε}, i.e. the only repeating n-gram of order 0 is the empty string ε, and computing ά[0][q][ε] ≡ α[q] using the standard forward algorithm.",3 Computation of N-gram Posteriors,[0],[0]
Each iteration of the outermost loop starting at line 3 computes posterior probabilities of all n-grams of order n directly on the lattice A.,3 Computation of N-gram Posteriors,[0],[0]
"At iteration n, we visit the states in topological order and examine each length n−1 history g associated with s, the state we are in.",3 Computation of N-gram Posteriors,[0],[0]
"For each history g, we go over the set of arcs leaving state s, construct the current n-gram x by concatenating g with the current arc label i (line 11), construct the length n−1 history h of the target state q (line 12), and update the forward probabilities for the target state history pair (q, h) in accordance with the recursions in Equations 8 and 10 by propagating the forward probabilities computed for the state history pair (s, g) (lines 14-18).",3 Computation of N-gram Posteriors,[0],[0]
"Whenever a final state is processed, the posterior probability accumulator for
each n-gram of order n observed on paths reaching that state is updated by multiplying the n-gram specific forward probability and the final weight associated with that state and adding the resulting value to the accumulator (lines 21-24).
",3 Computation of N-gram Posteriors,[0],[0]
We track repeating n-grams of order n to restrict the costly posterior computation operation to only those n-grams of order n+ 1 that can potentially repeat on some path of the input lattice.,3 Computation of N-gram Posteriors,[0],[0]
"The conditional on line 17 checks if any of the n-grams observed on paths reaching state history pair (s, g) is the same as the current n-gram x, and if so adds it to the set of repeating n-grams.",3 Computation of N-gram Posteriors,[0],[0]
"At each iteration n, we check if the current length n",3 Computation of N-gram Posteriors,[0],[0]
"− 1 history g of the state we are in is in R[n − 1], the set of repeating n-grams of order n−1 (line 9).",3 Computation of N-gram Posteriors,[0],[0]
"If it is not, then no n-gram x = gi can repeat on some path of A since that would require g to repeat as well.",3 Computation of N-gram Posteriors,[0],[0]
"If g is inR[n−1], then for each arc e = (s, i, w, q) we check if the length n− 1 history h",3 Computation of N-gram Posteriors,[0],[0]
=,3 Computation of N-gram Posteriors,[0],[0]
g[1 : n− 1]i of the next state q is in R[n − 1] (line 13).,3 Computation of N-gram Posteriors,[0],[0]
"If it is not, then the n-gram x = g[0]h can not repeat either.
",3 Computation of N-gram Posteriors,[0],[0]
We keep the posteriors p(x|A) for n-grams that can potentially repeat on some path of the input lattice in a deterministic WFSA P ′ that we construct on the fly.,3 Computation of N-gram Posteriors,[0],[0]
"P ′ is a prefix tree where each path π corresponds to an n-gram posterior, i.e. i[π] = x =⇒ w[π] = ρ(t[π])",3 Computation of N-gram Posteriors,[0],[0]
= p(x|A).,3 Computation of N-gram Posteriors,[0],[0]
"Once the computation of posteriors for possibly repeating n-grams is finished, we use the algorithm described in (Allauzen et al., 2004) to construct a weighted factor automaton C mapping all n-grams observed in A to their expected counts, i.e. ∀π in C, i[π] = x =⇒ w[π] = c(x|A).",3 Computation of N-gram Posteriors,[0],[0]
"We use P ′ and C to construct another weighted factor automaton P mapping all n-grams observed in A to their posterior probabilities, i.e. ∀π in P , i[π] = x =⇒ w[π] = p(x|A).",3 Computation of N-gram Posteriors,[0],[0]
First we remove the n-grams accepted by P ′,3 Computation of N-gram Posteriors,[0],[0]
"from C using the difference operation (Mohri, 2009),
C ′ = C − RmWeight(P ′) then take the union of the remaining automaton C ′ and P ′, and finally optimize the result by removing ε-transitions, determinizing and minimizing
P = Min(Det(RmEps(C ′ ⊕ P ′))).",3 Computation of N-gram Posteriors,[0],[0]
"In this section we provide experiments comparing the performance of Algorithm 2 with Algorithm 1
as well as a baseline algorithm based on the approach of (Tromble et al., 2008).",4 Experiments and Discussion,[0],[0]
"All algorithms were implemented in C++ using the OpenFst Library (Allauzen et al., 2007).",4 Experiments and Discussion,[0],[0]
Algorithm 1 implementation is a thin wrapper around the reference implementation.,4 Experiments and Discussion,[0],[0]
"All experiments were conducted on the 88K ASR lattices (total size: #states + #arcs = 33M, disk size: 481MB) generated from the training subset of the IARPA Babel Turkish language pack, which includes 80 hours of conversational telephone speech.",4 Experiments and Discussion,[0],[0]
"Lattices were generated with a speaker dependent DNN ASR system that was trained on the same data set using IBM’s Attila toolkit (Soltau et al., 2010).",4 Experiments and Discussion,[0],[0]
"All lattices were pruned to a logarithmic beam width of 5.
",4 Experiments and Discussion,[0],[0]
"Figure 1 gives a scatter plot of the posterior probability computation time vs. the number of lattice n-grams (up to 5-grams) where each point
represents one of the 88K lattices in our data set.",4 Experiments and Discussion,[0],[0]
"Similarly, Figure 2 gives a scatter plot of the maximum memory used by the program (maximum resident set size) during the computation of posteriors vs. the number of lattice n-grams (up to 5-grams).",4 Experiments and Discussion,[0],[0]
"Algorithm 2 requires significantly less resources, particularly in the case of larger lattices with a large number of unique n-grams.
",4 Experiments and Discussion,[0],[0]
"To better understand the runtime characteristics of Algorithms 1 and 2, we conducted a small experiment where we randomly selected 100 lattices (total size: #states + #arcs = 81K, disk size: 1.2MB) from our data set and analyzed the relation between the runtime and the maximum ngram length N .",4 Experiments and Discussion,[0],[0]
"Table 2 gives a runtime comparison between the baseline posterior computation algorithm described in (Tromble et al., 2008), Algorithm 1, Algorithm 2 and the expected count computation algorithm of (Allauzen et al., 2004).",4 Experiments and Discussion,[0],[0]
The baseline method computes posteriors separately for each n-gram by intersecting the lattice with an automaton accepting only the paths including that n-gram and computing the total weight of the resulting automaton in log semiring.,4 Experiments and Discussion,[0],[0]
Runtime complexities of the baseline method and Algorithm 1 are exponential in N due to the explicit enumeration of n-grams and we can clearly see this trend in the 3rd and 4th rows of Table 2.,4 Experiments and Discussion,[0],[0]
"Algorithm 2 (5th row) takes advantage of the WFSA based expected count computation algorithm (6th row) to do most of the work for long n-grams, hence does not suffer from the same exponential growth.",4 Experiments and Discussion,[0],[0]
Notice the drops in the runtimes of Algorithm 2 and the WFSA based expected count computation algorithm when all n-grams are included into the computation regardless of their length.,4 Experiments and Discussion,[0],[0]
These drops are due to the expected count computation algorithm that processes all n-grams simultaneously using WFSA operations.,4 Experiments and Discussion,[0],[0]
"Limiting the maximum n-gram length requires pruning long ngrams, which in general can increase the sizes of
intermediate WFSAs used in computation and result in longer runtimes as well as larger outputs.
",4 Experiments and Discussion,[0],[0]
"When there is no limit on the maximum n-gram length, the output of Algorithm 2 is a weighted factor automaton mapping each factor to its posterior.",4 Experiments and Discussion,[0],[0]
Table 3 compares the construction and storage requirements for posterior factor automata with similar factor automata structures.,4 Experiments and Discussion,[0],[0]
"We use the approach described in (Allauzen et al., 2004) for constructing both the unweighted and the expected count factor automata.",4 Experiments and Discussion,[0],[0]
We construct the unweighted factor automata by first removing the weights on the input lattices and then applying the determinization operation on the tropical semiring so that path weights are not added together.,4 Experiments and Discussion,[0],[0]
The storage requirements of the posterior factor automata produced by Algorithm 2 is similar to those of the expected count factor automata.,4 Experiments and Discussion,[0],[0]
"Unweighted factor automata, on the other hand, are significantly more compact than their weighted counterparts even though they accept the same set of strings.",4 Experiments and Discussion,[0],[0]
This difference in size is due to accommodating path weights which in general can significantly impact the effectiveness of automata determinization and minimization.,4 Experiments and Discussion,[0],[0]
"Efficient computation of n-gram expected counts from weighted automata was first addressed in (Allauzen et al., 2003) in the context of estimating n-gram language model statistics from ASR lattices.",5 Related Work,[0],[0]
"Expected counts for all n-grams of interest observed in the input automaton are computed by composing the input with a simple counting transducer, projecting on the output side, and removing ε-transitions.",5 Related Work,[0],[0]
The weight associated by the resulting WFSA to each n-gram it accepts is simply the expected count of that n-gram in the input automaton.,5 Related Work,[0],[0]
"Construction of such an automaton for all substrings (factors) of the input automaton was later explored in (Allauzen et al., 2004) in the con-
text of building an index for spoken utterance retrieval (SUR) (Saraclar and Sproat, 2004).",5 Related Work,[0],[0]
This is the approach used for constructing the weighted factor automaton C in Algorithm 2.,5 Related Work,[0],[0]
"While expected count works well in practice for ranking spoken utterances containing a query term, posterior probability is in theory a better metric for this task.",5 Related Work,[0],[0]
"The weighted factor automaton P produced by Algorithm 2 can be used to construct an SUR index weighted with posterior probabilities.
",5 Related Work,[0],[0]
"The problem of computing n-gram posteriors from lattices was first addressed in (Tromble et al., 2008) in the context of lattice-based MBR for SMT.",5 Related Work,[0],[0]
This is the baseline approach used in our experiments and it consists of building a separate FSA for each n-gram of interest and intersecting this automaton with the input lattice to discard those paths that do not include that n-gram and summing up the weights of remaining paths.,5 Related Work,[0],[0]
The fundamental shortcoming of this approach is that it requires separate intersection and shortest distance computations for each n-gram.,5 Related Work,[0],[0]
"This shortcoming was first tackled in (Allauzen et al., 2010) by introducing a counting transducer for simultaneous computation of posteriors for all n-grams of order n in a lattice.",5 Related Work,[0],[0]
This transducer works well for unigrams since there is a relatively small number of unique unigrams in a lattice.,5 Related Work,[0],[0]
"However, it is less efficient for n-grams of higher orders.",5 Related Work,[0],[0]
"This inefficiency was later addressed in (Blackwood et al., 2010) by employing n-gram mapping transducers to transduce the input lattices to n-gram lattices of order n and computing unigram posteriors on the higher order lattices.",5 Related Work,[0],[0]
"Algorithm 1 was described in (de Gispert et al., 2013) as a fast alternative to counting transducers.",5 Related Work,[0],[0]
"It is a lattice specialization of a more general algorithm for computing n-gram posteriors from a hypergraph in a single inside pass (DeNero et al., 2010).",5 Related Work,[0],[0]
"While this algorithm works really well for relatively short n-grams, its time and space requirements scale exponentially with the maximum n-gram length.",5 Related Work,[0],[0]
"Algorithm 2 builds upon this algorithm by exploiting the equiv-
alence of expected counts and posteriors for nonrepeating n-grams and eliminating the costly posterior computation operation for most n-grams in the input lattice.",5 Related Work,[0],[0]
We have described an efficient algorithm for computing n-gram posteriors from an input lattice and constructing an efficient and compact data structure for storing and retrieving them.,6 Conclusion,[0],[0]
The runtime and memory requirements of the proposed algorithm grow linearly with the length of the n-grams as opposed to the exponential growth observed with the original algorithm we are building upon.,6 Conclusion,[0],[0]
This is achieved by limiting the posterior computation to only those n-grams that may repeat on some path of the input lattice and using the relatively cheaper expected count computation algorithm for the rest.,6 Conclusion,[0],[0]
This filtering of n-grams introduces a slight bookkeeping overhead over the baseline algorithm but in return dramatically reduces the runtime and memory requirements for long n-grams.,6 Conclusion,[0],[0]
The authors would like to thank Cyril Allauzen and Graeme W. Blackwood for helpful discussions.,Acknowledgments,[0],[0]
This work uses IARPA-babel105b-v0.4 Turkish full language pack from the IARPA Babel Program language collection and is supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Defense U.S. Army Research Laboratory (DoD/ARL) contract number W911NF-12-C-0012.,Acknowledgments,[0],[0]
The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon.,Acknowledgments,[0],[0]
"Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoD/ARL, or the U.S. Government.",Acknowledgments,[0],[0]
Efficient computation of n-gram posterior probabilities from lattices has applications in lattice-based minimum Bayes-risk decoding in statistical machine translation and the estimation of expected document frequencies from spoken corpora.,abstractText,[0],[0]
"In this paper, we present an algorithm for computing the posterior probabilities of all ngrams in a lattice and constructing a minimal deterministic weighted finite-state automaton associating each n-gram with its posterior for efficient storage and retrieval.",abstractText,[0],[0]
"Our algorithm builds upon the best known algorithm in literature for computing ngram posteriors from lattices and leverages the following observations to significantly improve the time and space requirements: i) the n-grams for which the posteriors will be computed typically comprises all n-grams in the lattice up to a certain length, ii) posterior is equivalent to expected count for an n-gram that do not repeat on any path, iii) there are efficient algorithms for computing",abstractText,[0],[0]
n-gram expected counts from lattices.,abstractText,[0],[0]
We present experimental results comparing our algorithm with the best known algorithm in literature as well as a baseline algorithm based on weighted finite-state automata operations.,abstractText,[0],[0]
A Dynamic Programming Algorithm for Computing N-gram Posteriors from Lattices,title,[0],[0]
"We consider the problem of including additional knowledge in estimating sparse Gaussian graphical models (sGGMs) from aggregated samples, arising often in bioinformatics and neuroimaging applications. Previous joint sGGM estimators either fail to use existing knowledge or cannot scale-up to many tasks (large K) under a highdimensional (large p) situation. In this paper, we propose a novel Joint Elementary Estimator incorporating additional Knowledge (JEEK) to infer multiple related sparse Gaussian Graphical models from large-scale heterogeneous data. Using domain knowledge as weights, we design a novel hybrid norm as the minimization objective to enforce the superposition of two weighted sparsity constraints, one on the shared interactions and the other on the task-specific structural patterns. This enables JEEK to elegantly consider various forms of existing knowledge based on the domain at hand and avoid the need to design knowledgespecific optimization. JEEK is solved through a fast and entry-wise parallelizable solution that largely improves the computational efficiency of the state-of-the-art O(p5K4) to O(p2K4). We conduct a rigorous statistical analysis showing that JEEK achieves the same convergence rate O(log(Kp)/ntot) as the state-of-the-art estimators that are much harder to compute. Empirically, on multiple synthetic datasets and two real-world data, JEEK outperforms the speed of the state-ofarts significantly while achieving the same level of prediction accuracy.
1Department of Computer Science, University of Virginia, http://www.jointnets.org/ . Correspondence to: Beilun Wang <bw4mw@virginia.edu>, Yanjun Qi <yanjun@virginia.edu>.
Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).",text,[0],[0]
Technology revolutions in the past decade have collected large-scale heterogeneous samples from many scientific domains.,1 Introduction,[0],[0]
"For instance, genomic technologies have delivered petabytes of molecular measurements across more than hundreds of types of cells and tissues from national projects like ENCODE (Consortium et al., 2012) and TCGA (Network et al., 2011).",1 Introduction,[0],[0]
"Neuroimaging technologies have generated petabytes of functional magnetic resonance imaging (fMRI) datasets across thousands of human subjects (shared publicly through projects like openfMRI (Poldrack et al., 2013).",1 Introduction,[0],[0]
"Given such data, understanding and quantifying variable graphs from heterogeneous samples about multiple contexts is a fundamental analysis task.
",1 Introduction,[0],[0]
"Such variable graphs can significantly simplify networkdriven studies about diseases (Ideker & Krogan, 2012), can help understand the neural characteristics underlying clinical disorders (Uddin et al., 2013) and can allow for understanding genetic or neural pathways and systems.",1 Introduction,[0],[0]
"The number of contexts (denoted as K) that those applications need to consider grows extremely fast, ranging from tens (e.g., cancer types in TCGA (Network et al., 2011)) to thousands (e.g., number of subjects in openfMRI (Poldrack et al., 2013)).",1 Introduction,[0],[0]
"The number of variables (denoted as p) ranges from hundreds (e.g., number of brain regions) to tens of thousands (e.g., number of human genes).
",1 Introduction,[0],[0]
"The above data analysis problem can be formulated as jointly estimating K conditional dependency graphs G(1), G(2), . . .",1 Introduction,[0],[0]
", G(K) on a single set of p variables based on heterogeneous samples accumulated from K distinct contexts.",1 Introduction,[0],[0]
"For homogeneous data samples from a given i-th context, one typical approach is the sparse Gaussian Graphical Model (sGGM) (Lauritzen, 1996; Yuan & Lin, 2007).",1 Introduction,[0],[0]
"sGGM assumes samples are independently and identically drawn from Np(µ(i),Σ(i)), a multivariate Gaussian distribution with mean vector µ(i) and covariance matrix Σ(i).",1 Introduction,[0],[0]
"The graph structure G(i) is encoded by the sparsity pattern of the inverse covariance matrix, also named precision matrix, Ω(i).",1 Introduction,[0],[0]
Ω(i),1 Introduction,[0],[0]
:= (Σ(i))−1. Ω(i)jk = 0,1 Introduction,[0],[0]
"if and only if in G(i) an edge does not connect j-th node and k-th node (i.e., conditional independent).",1 Introduction,[0],[0]
"sGGM imposes an `1 penalty on the parameter Ω(i) to achieve a consistent estimation
under high-dimensional situations.",1 Introduction,[0],[0]
"When handling heterogeneous data samples, rather than estimating sGGM of each condition separately, a multi-task formulation that jointly estimatesK different but related sGGMs can lead to a better generalization(Caruana, 1997).
",1 Introduction,[0],[0]
"Previous studies for joint estimation of multiple sGGMs roughly fall into four categories: (Danaher et al., 2013; Mohan et al., 2013; Chiquet et al., 2011; Honorio & Samaras, 2010; Guo et al., 2011; Zhang & Wang, 2012; Zhang & Schneider, 2010; Zhu et al., 2014): (1) The first group seeks to optimize a sparsity regularized data likelihood function plus an extra penalty functionR′ to enforce structural similarity among multiple estimated networks.",1 Introduction,[0],[0]
"Joint graphical lasso (JGL) (Danaher et al., 2013) proposed an alternating direction method of multipliers (ADMM) based optimization algorithm to work with two regularization functions (`1 + R′).",1 Introduction,[0],[0]
(2) The second category tries to recover the support of Ω(i) using sparsity penalized regressions in a column by column fashion.,1 Introduction,[0],[0]
"Recently (Monti et al., 2015) proposed to learn population and subject-specific brain connectivity networks via a so-called “Mixed Neighborhood Selection” (MSN) method in this category.",1 Introduction,[0],[0]
(3) The third type of methods seeks to minimize the joint sparsity of the target precision matrices under matrix inversion constraints.,1 Introduction,[0],[0]
"One recent study, named SIMULE (Shared and Individual parts of MULtiple graphs Explicitly) (Wang et al., 2017b), automatically infers both specific edge patterns that are unique to each context and shared interactions preserved among all the contexts (i.e. by modeling each precision matrix as Ω(i) = Ω(i)I + ΩS) via the constrained `1 minimization.",1 Introduction,[0],[0]
"Following the CLIME estimator (Pang et al., 2014), the constrained `1 convex formulation can also be solved column by column via linear programming.",1 Introduction,[0],[0]
"However, all three categories of aforementioned estimators are difficult to scale up when the dimension p or the number of tasks K are large because they cannot avoid expensive steps like SVD (Danaher et al., 2013) for JGL, linear programming for SIMULE or running multiple iterations of p expensive penalized regressions in MNS.",1 Introduction,[0],[0]
"(4) The last category extends the so-called ”Elementary Estimator” graphical model (EE-GM) formulation (Yang et al., 2014b) to revise JGL’s penalized likelihood into a constrained convex program that minimizes (`1 + R′).",1 Introduction,[0],[0]
"One proposed estimator FASJEM (Wang et al., 2017a) is solved in an entry-wise manner and group-entry-wise manner that largely outperforms the speed of its JGL counterparts.",1 Introduction,[0],[0]
"More details of the related works are in Section (5).
",1 Introduction,[0],[0]
One significant caveat of state-of-the-art joint sGGM estimators is the fact that little attention has been paid to incorporating existing knowledge of the nodes or knowledge of the relationships among nodes in the models.,1 Introduction,[0],[0]
"In addition to the samples themselves, additional information is widely available in real-world applications.",1 Introduction,[0],[0]
"In fact, incorporating the
knowledge is of great scientific interest.",1 Introduction,[0],[0]
"A prime example is when estimating the functional brain connectivity networks among brain regions based on fMRI samples, the spatial position of the regions are readily available.",1 Introduction,[0],[0]
"Neuroscientists have gathered considerable knowledge regarding the spatial and anatomical evidence underlying brain connectivity (e.g., short edges and certain anatomical regions are more likely to be connected (Watts & Strogatz, 1998)).",1 Introduction,[0],[0]
Another important example is the problem of identifying gene-gene interactions from patients’ gene expression profiles across multiple cancer types.,1 Introduction,[0],[0]
Learning the statistical dependencies among genes from such heterogeneous datasets can help to understand how such dependencies vary from normal to abnormal and help to discover contributing markers that influence or cause the diseases.,1 Introduction,[0],[0]
"Besides the patient samples, state-ofthe-art bio-databases like HPRD (Prasad et al., 2009) have collected a significant amount of information about direct physical interactions among corresponding proteins, regulatory gene pairs or signaling relationships collected from high-qualify bio-experiments.
",1 Introduction,[0],[0]
"Although being strong evidence of structural patterns we aim to discover, this type of information has rarely been considered in the joint sGGM formulation of such samples.",1 Introduction,[0],[0]
"To the authors’ best knowledge, only one study named as WSIMULE tried to extend the constrained `1 minimization in SIMULE into weighted `1 for considering spatial information of brain regions in the joint discovery of heterogeneous neural connectivity graphs (Singh et al., 2017).",1 Introduction,[0],[0]
"This method was designed just for the neuroimaging samples and has O(p5K4) time cost, making it not scalable for large-scale settings (more details in Section 3).
",1 Introduction,[0],[0]
This paper aims to fill this gap by adding additional knowledge most effectively into scalable and fast joint sGGM estimations.,1 Introduction,[0],[0]
"We propose a novel model, namely Joint Elementary Estimator incorporating additional Knowledge (JEEK), that presents a principled and scalable strategy to include additional knowledge when estimating multiple related sGGMs jointly.",1 Introduction,[0],[0]
"Briefly speaking, this paper makes the following contributions:
• Novel approach: JEEK presents a new way of integrating additional knowledge in learning multi-task sGGMs in a scalable way.",1 Introduction,[0],[0]
(Section 3) •,1 Introduction,[0],[0]
Fast optimization: We optimize JEEK through an entrywise and group-entry-wise manner that can dramatically improve the time complexity to O(p2K4).,1 Introduction,[0],[0]
(Section 3.4) •,1 Introduction,[0],[0]
Convergence rate: We theoretically prove the convergence rate of JEEK asO(log(Kp)/ntot).,1 Introduction,[0],[0]
This rate shows the benefit of joint estimation and achieves the same convergence rate as the state-of-the-art that are much harder to compute.,1 Introduction,[0],[0]
"(Section 4) • Evaluation: We evaluate JEEK using several synthetic datasets and two real-world data, one from neuroscience and one from genomics.",1 Introduction,[0],[0]
"It outperforms state-of-the-art
baselines significantly regarding the speed.",1 Introduction,[0],[0]
"(Section 6)
JEEK provides the flexibility of using (K + 1) different weight matrices representing the extra knowledge.",1 Introduction,[0],[0]
"We try to showcase a few possible designs of the weight matrices in Section S:5, including (but not limited to):
• Spatial or anatomy knowledge about brain regions; • Knowledge of known co-hub nodes or perturbed nodes;",1 Introduction,[0],[0]
"• Known group information about nodes, such as genes
belonging to the same biological pathway or cellular location; • Using existing known edges as the knowledge, like the known protein interaction databases for discovering gene networks (a semi-supervised setting for such estimations).
",1 Introduction,[0],[0]
"We sincerely believe the scalability and flexibility provided by JEEK can make structure learning of joint sGGM feasible in many real-world tasks.
",1 Introduction,[0],[0]
"Att: Due to space limitations, we have put details of certain contents (e.g., proofs) in the appendix.",1 Introduction,[0],[0]
Notations with “S:” as the prefix in the numbering mean the corresponding contents are in the appendix.,1 Introduction,[0],[0]
"For example, full proofs are in Section (S:3).
",1 Introduction,[0],[0]
Notations: math notations we use are described in Section (S:1).,1 Introduction,[0],[0]
ntot = K∑ i=1,1 Introduction,[0],[0]
ni is the total number of data samples.,1 Introduction,[0],[0]
"Sparse Gaussian graphical model (sGGM):The classic formulation of estimating sparse Gaussian Graphical model (Yuan & Lin, 2007) from a single given condition (single sGGM) is the “graphical lasso” estimator (GLasso) (Yuan & Lin, 2007; Banerjee et al., 2008).",2 Background,[0],[0]
"It solves the following `1 penalized maximum likelihood estimation (MLE) problem:
argmin Ω>0
− log det(Ω)+ < Ω, Σ̂ > +λn||Ω||1 (2.1)
M-Estimator with Decomposable Regularizer in High-Dimensional Situations:",2 Background,[0],[0]
"Recently the seminal study (Negahban et al., 2009) proposed a unified framework for highdimensional analysis of the following general formulation: M-estimators with decomposable regularizers:
argmin θ L(θ) + λnR(θ) (2.2)
where R(·) represents a decomposable regularization function and L(·) represents a loss function (e.g., the negative log-likelihood function in sGGM L(Ω) =",2 Background,[0],[0]
− log det(Ω)+ <,2 Background,[0],[0]
"Ω, Σ̂ >).",2 Background,[0],[0]
"Here λn > 0 is the tuning parameter.
",2 Background,[0],[0]
"Elementary Estimators (EE): Using the analysis framework from (Negahban et al., 2009), recent studies (Yang
et al., 2014a;b;c) propose a new category of estimators named “Elementary estimator” (EE) with the following general formulation:
argmin θ R(θ) subject to:R∗(θ − θ̂n) ≤ λn (2.3)
",2 Background,[0],[0]
"WhereR∗(·) is the dual norm ofR(·),
R∗(v) := sup u6=0
< u, v >
R(u) = sup R(u)≤1",2 Background,[0],[0]
"< u, v > .",2 Background,[0],[0]
"(2.4)
",2 Background,[0],[0]
The solution of Eq.,2 Background,[0],[0]
(2.3) achieves the near optimal convergence rate as Eq.,2 Background,[0],[0]
(2.2) when satisfying certain conditions.,2 Background,[0],[0]
"R(·) represents a decomposable regularization function (e.g., `1-norm) andR∗(·) is the dual norm ofR(·) (e.g., `∞-norm is the dual norm of `1-norm).",2 Background,[0],[0]
"λn is a regularization parameter.
",2 Background,[0],[0]
The basic motivation of Eq.,2 Background,[0],[0]
"(2.3) is to build simpler and possibly fast estimators, that yet come with statistical guarantees that are nonetheless comparable to regularized MLE. θ̂n needs to be carefully constructed, well-defined and closedform for the purpose of simpler computations.",2 Background,[0],[0]
The formulation defined by Eq.,2 Background,[0],[0]
(2.3) is to ensure its solution having the desired structure defined by R(·).,2 Background,[0],[0]
"For cases of highdimensional estimation of linear regression models, θ̂n can be the classical ridge estimator that itself is closed-form and with strong statistical convergence guarantees in highdimensional situations.
",2 Background,[0],[0]
"EE-sGGM:(Yang et al., 2014b) proposed elementary estimators for graphical models (GM) of exponential families, in which θ̂n represents so-called proxy of backward mapping for the target GM (more details in Section S:4).",2 Background,[0],[0]
"The key idea (summarized in the upper row of Figure 1) is to investigate the vanilla MLE and where it breaks down for estimating a graphical model of exponential families in the case of high-dimensions (Yang et al., 2014b).",2 Background,[0],[0]
Essentially the vanilla graphical model MLE can be expressed as a backward mapping that computes the model parameters from some given moments in an exponential family distribution.,2 Background,[0],[0]
"For instance, in the case of learning Gaussian GM (GGM) with vanilla MLE, the backward mapping is Σ̂−1 that estimates Ω from the sample covariance matrix (moment) Σ̂.",2 Background,[0],[0]
"We introduce the details of backward mapping in Section S:4.
",2 Background,[0],[0]
"However, even though this backward mapping has a simple closed form for GGM, the backward mapping is normally not well-defined in high-dimensional settings.",2 Background,[0],[0]
"When given the sample covariance Σ̂, we cannot just compute the vanilla MLE solution as [Σ̂]−1 for GGM since Σ̂ is rankdeficient when p >",2 Background,[0],[0]
n.,2 Background,[0],[0]
"Therefore Yang et al. (Yang et al., 2014b) used carefully constructed proxy backward maps as θ̂n =",2 Background,[0],[0]
[Tv(Σ̂)] −1,2 Background,[0],[0]
"that is both available in closed-form, and
well-defined in high-dimensional settings for GGMs.",2 Background,[0],[0]
We introduce the details of [Tv(Σ̂)]−1 and its statistical property in Section S:4.,2 Background,[0],[0]
Now,2 Background,[0],[0]
Eq.,2 Background,[0],[0]
"(2.3) becomes the following closed-form estimator for learning sparse Gaussian graphical models (Yang et al., 2014b):
argmin Ω ||Ω||1,,off
",2 Background,[0],[0]
subject to:||Ω−,2 Background,[0],[0]
"[Tv(Σ̂)]−1||∞,off ≤ λn (2.5)
Eq. (2.5) is a special case of Eq.",2 Background,[0],[0]
"(2.3), in whichR(·) is the off-diagonal `1-norm and the precision matrix Ω is the θ we search for.",2 Background,[0],[0]
"When R(·) is the `1-norm, the solution of Eq.",2 Background,[0],[0]
(2.3) (and Eq. (2.5)) just needs to perform entry-wise thresholding operations on θ̂n to ensure the desired sparsity structure of its final solution.,2 Background,[0],[0]
"In applications of Gaussian graphical models, we typically have more information than just the data samples themselves.",3 Proposed Method: JEEK,[0],[0]
"This paper aims to propose a simple, scalable and theoretically-guaranteed joint estimator for estimating multiple sGGMs with additional knowledge in large-scale situations.",3 Proposed Method: JEEK,[0],[0]
"We first propose to jointly estimate multiple related sGGMs from K data blocks using the following formulation:
argmin Ω(1),Ω(2),...,Ω(K) K∑ i=1 L(Ω(i))",3.1 A Joint EE (JEE) Formulation,[0],[0]
"+ λnR(Ω(1),Ω(2), . .",3.1 A Joint EE (JEE) Formulation,[0],[0]
.,3.1 A Joint EE (JEE) Formulation,[0],[0]
",Ω(K))
(3.1)
where Ω(i) denotes the precision matrix for i-th task.",3.1 A Joint EE (JEE) Formulation,[0],[0]
L(Ω) =,3.1 A Joint EE (JEE) Formulation,[0],[0]
− log det(Ω)+ <,3.1 A Joint EE (JEE) Formulation,[0],[0]
"Ω, Σ̂ > describes the negative log-likelihood function in sGGM. Ω(i) 0 means that Ω(i) needs to be a positive definite matrix.",3.1 A Joint EE (JEE) Formulation,[0],[0]
"R(·) represents a decomposable regularization function enforcing sparsity and structure assumptions (details in Section (3.2)).
",3.1 A Joint EE (JEE) Formulation,[0],[0]
"For ease of notation, we denote that Ωtot = (Ω(1),Ω(2), . . .",3.1 A Joint EE (JEE) Formulation,[0],[0]
",Ω(K))",3.1 A Joint EE (JEE) Formulation,[0],[0]
"and Σtot = (Σ(1),Σ(2), . . .",3.1 A Joint EE (JEE) Formulation,[0],[0]
",Σ(K)).
",3.1 A Joint EE (JEE) Formulation,[0],[0]
Ωtot and Σtot are both p,3.1 A Joint EE (JEE) Formulation,[0],[0]
"× Kp matrices (i.e., Kp2 parameters to estimate).",3.1 A Joint EE (JEE) Formulation,[0],[0]
"Now define an inverse function as inv(Atot) := (A(1) −1 , A(2) −1 , . . .",3.1 A Joint EE (JEE) Formulation,[0],[0]
", A(K) −1 ), where Atot is a given p ×Kp matrix with the same structure as Σtot.",3.1 A Joint EE (JEE) Formulation,[0],[0]
Then we rewrite Eq.,3.1 A Joint EE (JEE) Formulation,[0],[0]
"(3.1) into the following form:
argmin Ωtot
L(Ωtot) + λnR(Ωtot) (3.2)
Now connecting Eq. (3.2) to Eq. (2.2) and Eq. (2.3), we propose the following joint elementary estimator (JEE) for learning multiple sGGMs:
argmin Ωtot
R(Ωtot)
subject to: R∗(Ωtot − Ω̂totntot) ≤ λn (3.3)
",3.1 A Joint EE (JEE) Formulation,[0],[0]
The fundamental component in Eq.,3.1 A Joint EE (JEE) Formulation,[0],[0]
(2.3) for the single context sGGM was to use a well-defined proxy function to approximate the vanilla MLE solution (named as the backward mapping for exponential family distributions),3.1 A Joint EE (JEE) Formulation,[0],[0]
"(Yang et al., 2014b).",3.1 A Joint EE (JEE) Formulation,[0],[0]
The proposed proxy θ̂n =,3.1 A Joint EE (JEE) Formulation,[0],[0]
[Tv(Σ̂)]−1 is both well-defined under high-dimensional situations and also has a simple closed-form.,3.1 A Joint EE (JEE) Formulation,[0],[0]
"Following a similar idea, when learning multiple sGGMs, we propose to use inv(Tv(Σ̂tot)) for Ω̂totntot and get the following joint elementary estimator:
argmin Ωtot
R(Ωtot)
",3.1 A Joint EE (JEE) Formulation,[0],[0]
Subject to: R∗(Ωtot − inv(Tv(Σ̂tot))),3.1 A Joint EE (JEE) Formulation,[0],[0]
≤ λn (3.4),3.1 A Joint EE (JEE) Formulation,[0],[0]
The main goal of this paper is to design a principled strategy to incorporate existing knowledge (other than samples or structured assumptions) into the multi-sGGM formulation.,3.2 Knowledge as Weight (KW-Norm),[0],[0]
"We consider two factors in such a design:
(1) When learning multiple sGGMs jointly from real-world applications, it is often of great scientific interests to model and learn context-specific graph variations explicitly, because such variations can “fingerprint” important markers in domains like cognition (Ideker & Krogan, 2012) or pathology (Kelly et al., 2012).",3.2 Knowledge as Weight (KW-Norm),[0],[0]
Therefore we design to share parameters between different contexts.,3.2 Knowledge as Weight (KW-Norm),[0],[0]
"Mathematically, we model Ω(i) as two parts:
Ω(i) =",3.2 Knowledge as Weight (KW-Norm),[0],[0]
"Ω (i) I + ΩS (3.5)
where Ω(i)I is the individual precision matrix for context i and ΩS is the shared precision matrix between contexts.",3.2 Knowledge as Weight (KW-Norm),[0],[0]
"Again, for ease of notation we denote ΩtotI = (Ω
(1) I ,Ω (2) I , . . .",3.2 Knowledge as Weight (KW-Norm),[0],[0]
",Ω (K) I ) and Ω tot S = (ΩS ,ΩS , . . .",3.2 Knowledge as Weight (KW-Norm),[0],[0]
",ΩS).
(2) We represent additional knowledge as positive weight matrices from Rp×p.",3.2 Knowledge as Weight (KW-Norm),[0],[0]
"More specifically, we represent
the knowledge of the task-specific graph as weight matrix {W (i)} and WS representing existing knowledge of the shared network.",3.2 Knowledge as Weight (KW-Norm),[0],[0]
The positive matrix-based representation is a powerful and flexible strategy that can describe many possible forms of existing knowledge.,3.2 Knowledge as Weight (KW-Norm),[0],[0]
"In Section (S:5), we provide a few different designs of {W (i)} and WS for real-world applications.",3.2 Knowledge as Weight (KW-Norm),[0],[0]
"In total, we have weight matrices {W (1)I ,W (2) I , . . .",3.2 Knowledge as Weight (KW-Norm),[0],[0]
",W (K) I ,WS} to represent additional knowledge.",3.2 Knowledge as Weight (KW-Norm),[0],[0]
"To simplify notations, we denote W totI = (W (1) I ,W (2), . . .",3.2 Knowledge as Weight (KW-Norm),[0],[0]
",W (K) I ) and W tot S = (WS ,WS , . . .",3.2 Knowledge as Weight (KW-Norm),[0],[0]
",WS).
",3.2 Knowledge as Weight (KW-Norm),[0],[0]
"Now we propose the following knowledge as weight norm (kw-norm) combining the above two:
R(Ωtot) = ||W totI ◦ ΩtotI ||1 + ||W",3.2 Knowledge as Weight (KW-Norm),[0],[0]
"totS ◦ ΩtotS ||1 (3.6)
",3.2 Knowledge as Weight (KW-Norm),[0],[0]
Here the Hadamard product ◦ is the element-wise product between two matrices i.e. [A ◦B]ij = AijBij .,3.2 Knowledge as Weight (KW-Norm),[0],[0]
"The kw-norm( Eq. (3.6)) has the following three properties:
• (i) kw-norm is a norm function if and only if any entries in W totI and W tot S do not equal to 0.",3.2 Knowledge as Weight (KW-Norm),[0],[0]
• (ii),3.2 Knowledge as Weight (KW-Norm),[0],[0]
"If the condition in (i) holds, kw-norm is a decomposable norm.",3.2 Knowledge as Weight (KW-Norm),[0],[0]
• (iii),3.2 Knowledge as Weight (KW-Norm),[0],[0]
"If the condition in (i) holds, the dual norm of kwnorm isR∗(u) = max(||W totI ◦ u||∞, ||W totS ◦ u||∞).
",3.2 Knowledge as Weight (KW-Norm),[0],[0]
Section S:3.1 provides proofs of the above claims.,3.2 Knowledge as Weight (KW-Norm),[0],[0]
"Plugging Eq. (3.6) to Eq. (3.4), we obtain the following formulation of JEEK for learning multiple related sGGMs from heterogereous samples:
argmin ΩtotI ,Ω tot S
||W totI ◦ ΩtotI ||1 + ||W",3.3 JEE with Knowledge (JEEK),[0],[0]
"totS ◦ ΩtotS ||
Subject to: ||W totI ◦ (Ωtot − inv(Tv(Σ̂tot)))||∞ ≤ λn ||W totS ◦",3.3 JEE with Knowledge (JEEK),[0],[0]
(Ωtot − inv(Tv(Σ̂tot)))||∞ ≤ λn Ωtot = ΩtotS + Ω,3.3 JEE with Knowledge (JEEK),[0],[0]
"tot I
(3.7)
",3.3 JEE with Knowledge (JEEK),[0],[0]
"In Section 4, we theoretically prove that the statistical convergence rate of JEEK achieves the same sharp convergence rate as the state-of-the-art estimators for multi-task sGGMs.",3.3 JEE with Knowledge (JEEK),[0],[0]
"Our proofs are inspired by the unified framework of the high-dimensional statistics (Negahban et al., 2009).",3.3 JEE with Knowledge (JEEK),[0],[0]
A huge computational advantage of JEEK (Eq. (3.7)) is that it can be decomposed into p × p independent small linear programming problems.,3.4 Solution of JEEK:,[0],[0]
"To simplify notations, we denote Ω(i)I j,k (the {j, k}-th entry of Ω (i))",3.4 Solution of JEEK:,[0],[0]
as ai.,3.4 Solution of JEEK:,[0],[0]
"Similarly
Algorithm 1.",3.4 Solution of JEEK:,[0],[0]
"Joint Elementary Estimator with additional knowledge (JEEK) for Multi-task sGGMs Input: Data sample matrix X(i) ( i = 1 toK), regularization hyperparameter λn, Knowledge weight matrices {W (i)I ,WS} and LP(.)",3.4 Solution of JEEK:,[0],[0]
(a linear programming solver),3.4 Solution of JEEK:,[0],[0]
"Output: {Ω(i)} ( i = 1 toK)
1: for i = 1 toK do 2: Initialize Σ̂(i) = 1ni−1 ∑ni",3.4 Solution of JEEK:,[0],[0]
"s=1(X (i) s, −µ̂ (i))(X(i)s, −µ̂ (i))T (the sample
covariance matrix of X(i))",3.4 Solution of JEEK:,[0],[0]
3: Initialize Ω(i) = 0p×p,3.4 Solution of JEEK:,[0],[0]
4: Calculate the proxy backward mapping [Tv(Σ̂(i))]−1 5: end for 6: for j = 1 to p do 7: for k = 1 to j do 8: ci =,3.4 Solution of JEEK:,[0],[0]
"[Tv(Σ̂(i))]−1j,k 9: wi = W (i)j,k 10: ws = WSj,k 11: ai, b = LP(wi, ws, ci, λn) where i = 1, . . .",3.4 Solution of JEEK:,[0],[0]
", K and LP(.) solves Eq.",3.4 Solution of JEEK:,[0],[0]
"(3.8) 12: for i = 1 toK do 13: Ω(i)j,k = Ω(i)k,j = ai + b 14: Ω(i)I j,k = ai 15: ΩSj,k = b 16: end for 17: end for 18: end for
we denote ΩSj,k as b and [Tv(Σ̂(i))",3.4 Solution of JEEK:,[0],[0]
] −1,3.4 Solution of JEEK:,[0],[0]
"j,k be ci.",3.4 Solution of JEEK:,[0],[0]
"Similarly we denote W (i)j,k = wi and W S j,k = ws. ”",3.4 Solution of JEEK:,[0],[0]
"A group of entries” means a set of parameters {a1, . . .",3.4 Solution of JEEK:,[0],[0]
", aK , b} for certain j, k.",3.4 Solution of JEEK:,[0],[0]
"In order to estimate {a1, . . .",3.4 Solution of JEEK:,[0],[0]
", aK , b}, JEEK (Eq. (3.7)) can be decomposed into the following formulation for a certain j, k :
argmin ai,b ∑",3.4 Solution of JEEK:,[0],[0]
"i |wiai|+K|wsb|
Subject to: |ai + b− ci| ≤",3.4 Solution of JEEK:,[0],[0]
"λn
min(wi, ws) ,
i = 1, . .",3.4 Solution of JEEK:,[0],[0]
.,3.4 Solution of JEEK:,[0],[0]
",K
(3.8)
Eq. (3.8) can be easily converted into a linear programming form of Eq.",3.4 Solution of JEEK:,[0],[0]
(S:1–1),3.4 Solution of JEEK:,[0],[0]
with only K + 1 variables.,3.4 Solution of JEEK:,[0],[0]
The time complexity of Eq.,3.4 Solution of JEEK:,[0],[0]
(3.8) is O(K4).,3.4 Solution of JEEK:,[0],[0]
Considering JEEK has a total p(p,3.4 Solution of JEEK:,[0],[0]
"− 1)/2 of such subproblems to solve, the computational complexity of JEEK (Eq. (3.7)) is therefore O(p2K4).",3.4 Solution of JEEK:,[0],[0]
We summarize the optimization algorithm of JEEK in Algorithm 1 (details in Section (S:1.2)).,3.4 Solution of JEEK:,[0],[0]
KW-Norm:We presented the three properties of kw-norm in Section 3.2.,4 Theoretical Analysis,[0],[0]
"The proofs of these three properties are included in Section (S:3.1).
",4 Theoretical Analysis,[0],[0]
"Theoretical error bounds of Proxy Backward Mapping: (Yang et al., 2014b) proved that when (p ≥ n), the proxy backward mapping [Tv(Σ̂)]−1 used by EE-sGGM achieves the sharp convergence rate to its truth (i.e., by proving
||Tv(Σ̂))−1",4 Theoretical Analysis,[0],[0]
− Σ∗−1||∞ = O( √ log p n )).,4 Theoretical Analysis,[0],[0]
"The proof was extended from the previous study (Rothman et al., 2009) that
devised Tv(Σ̂) for estimating covariance matrix consistently in high-dimensional situations.",4 Theoretical Analysis,[0],[0]
See detailed proofs in Section S:4.3.,4 Theoretical Analysis,[0],[0]
"To derive the statistical error bound of JEEK, we need to assume that inv(Tv(Σ̂tot)) are well-defined.",4 Theoretical Analysis,[0],[0]
"This is ensured by assuming that the true Ω(i) ∗ satisfy the conditions defined in Section (S:3.1).
",4 Theoretical Analysis,[0],[0]
"Theoretical error bounds of JEEK:We now use the highdimensional analysis framework from (Negahban et al., 2009), three properties of kw-norm, and error bounds of backward mapping from (Rothman et al., 2009; Yang et al., 2014b) to derive the statistical convergence rates of JEEK.",4 Theoretical Analysis,[0],[0]
"Detailed proofs of the following theorems are in Section 4 .
",4 Theoretical Analysis,[0],[0]
"Before providing the theorem, we need to define the structural assumption, the IS-Sparsity, we assume for the parameter truth.",4 Theoretical Analysis,[0],[0]
(IS-Sparsity): The ’true’ parameter of Ωtot∗ can be decomposed into two clear structures–{ΩtotI ∗ and ΩtotS ∗}.,4 Theoretical Analysis,[0],[0]
"ΩtotI ∗ is exactly sparse with ki non-zero entries indexed by a support set SI and ΩtotS ∗ is exactly sparse with ks
non-zero entries indexed by a support set SS .",4 Theoretical Analysis,[0],[0]
SI,4 Theoretical Analysis,[0],[0]
"⋂ SS = ∅.
All other elements equal to 0 (in (SI ⋃ SS) c).
",4 Theoretical Analysis,[0],[0]
Theorem 4.1.,4 Theoretical Analysis,[0],[0]
Consider Ωtot whose true parameter Ωtot∗ satisfies the (IS-Sparsity) assumption.,4 Theoretical Analysis,[0],[0]
Suppose we compute the solution of Eq. (3.7) with a bounded λn such that λn ≥ max(||W totI ◦,4 Theoretical Analysis,[0],[0]
"(Ωtot
∗− inv(Tv(Σ̂tot)))||∞, ||W totS ◦",4 Theoretical Analysis,[0],[0]
"(Ωtot
∗ − inv(Tv(Σ̂tot)))||∞), then the estimated solution Ω̂tot satisfies the following error bounds:
||Ω̂tot",4 Theoretical Analysis,[0],[0]
− Ωtot∗||F ≤ 4,4 Theoretical Analysis,[0],[0]
√ ki + ksλn max(||W,4 Theoretical Analysis,[0],[0]
totI ◦,4 Theoretical Analysis,[0],[0]
"(Ω̂tot − Ωtot ∗ )||∞, ||W totS ◦ (Ω̂tot − Ωtot
∗||∞) ≤ 2λn
||W totI ◦ (Ω̂totI − ΩtotI ∗ )||1 + ||W",4 Theoretical Analysis,[0],[0]
totS ◦,4 Theoretical Analysis,[0],[0]
"(Ω̂totS − ΩtotS ∗ )||1
≤ 8(ki + ks)λn",4 Theoretical Analysis,[0],[0]
"(4.1)
Proof.",4 Theoretical Analysis,[0],[0]
"See detailed proof in Section S:3.2
Theorem (4.1) provides a general bound for any selection of λn.",4 Theoretical Analysis,[0],[0]
"The bound of λn is controlled by the distance between Ωtot
∗ and inv(Tv(Σ̂tot)).",4 Theoretical Analysis,[0],[0]
We then extend Theorem (4.1) to derive the statistical convergence rate of JEEK.,4 Theoretical Analysis,[0],[0]
This gives us the following corollary: Corollary 4.2.,4 Theoretical Analysis,[0],[0]
"Suppose the high-dimensional setting, i.e., p > max(ni).",4 Theoretical Analysis,[0],[0]
Let v := a √ log(Kp) ntot .,4 Theoretical Analysis,[0],[0]
"Then for
λn := 8κ1a κ2 √ log(Kp) ntot
and ntot > c logKp, with a probability of at least 1− 2C1 exp(−C2Kp log(Kp)), the estimated optimal solution Ω̂tot has the following error bound:
||Ω̂tot−Ωtot∗||F
≤ 16κ1amax j,k (W totI j,k,W tot S j,k)
κ2
√ (ki + ks) log(Kp)
",4 Theoretical Analysis,[0],[0]
"ntot (4.2)
where a, c, κ1 and κ2 are constants.
",4 Theoretical Analysis,[0],[0]
Proof.,4 Theoretical Analysis,[0],[0]
See detailed proof in Section S:3.2.2 (especially from Eq.,4 Theoretical Analysis,[0],[0]
(S:3–11) to Eq.,4 Theoretical Analysis,[0],[0]
"(S:3–19)).
",4 Theoretical Analysis,[0],[0]
Bayesian View of JEEK:,4 Theoretical Analysis,[0],[0]
In Section (S:2) we provide a direct Bayesian interpretation of JEEK through the perspective of hierarchical Bayesian modeling.,4 Theoretical Analysis,[0],[0]
Our hierarchical Bayesian interpretation nicely explains the assumptions we make in JEEK.,4 Theoretical Analysis,[0],[0]
JEEK is closely related to a few state-of-the-art studies summarized in Table 1.,5 Connecting to Relevant Studies,[0],[0]
"We compare the time complexity and functional properties of JEEK versus these studies.
NAK: (Bu & Lederer, 2017)For the single task sGGM, one recent study (Bu & Lederer, 2017) (following ideas from (Shimamura et al., 2007)) proposed to integrating Additional Knowledge (NAK)into estimation of graphical models through a weighted Neighbourhood selection formulation (NAK) as: β̂j = argmin
β,βj=0
1 2 ||X j−Xβ||22 + ||rj ◦β||1.
NAK is designed for estimating brain connectivity networks from homogeneous samples and incorporate distance knowledge as weight vectors.",5 Connecting to Relevant Studies,[0],[0]
1,5 Connecting to Relevant Studies,[0],[0]
"In experiments, we compare JEEK to NAK (by running NAK R package K times) on multiple synthetic datasets of simulated samples about brain regions.",5 Connecting to Relevant Studies,[0],[0]
"The data simulation strategy was suggested by (Bu & Lederer, 2017).",5 Connecting to Relevant Studies,[0],[0]
"Same as the NAK (Bu & Lederer, 2017), we use the spatial distance among brain regions as additional knowledge in JEEK.
",5 Connecting to Relevant Studies,[0],[0]
"W-SIMULE: (Singh et al., 2017)Like JEEK, one recent study (Singh et al., 2017) of multi-sGGMs (following ideas from (Wang et al., 2017b)) also assumed that Ω(i) = Ω
(i)",5 Connecting to Relevant Studies,[0],[0]
"I + ΩS and incorporated spatial distance knowl-
edge in their convex formulation for joint discovery of heterogeneous neural connectivity graphs.",5 Connecting to Relevant Studies,[0],[0]
"This study, with name W-SIMULE (Weighted model for Shared and Individual parts of MULtiple graphs Explicitly) uses a weighted constrained `1 minimization:
argmin Ω (i) I ,ΩS
∑ i ||W ◦ Ω(i)I ||1 + K||W ◦ ΩS ||1 (5.1)
",5 Connecting to Relevant Studies,[0],[0]
"Subject to: ||Σ(i)(Ω(i)I + ΩS)− I||∞ ≤ λn, i = 1, . . .",5 Connecting to Relevant Studies,[0],[0]
", K
1Here β̂j indicates the sparsity of j-th column of a single Ω̂. Namely, β̂jk = 0 if and only if Ω̂k,j = 0.",5 Connecting to Relevant Studies,[0],[0]
"rj is a weight vector as the additional knowledge The NAK formulation can be solved by a classic Lasso solver like glmnet.
",5 Connecting to Relevant Studies,[0],[0]
W-SIMULE simply includes the additional knowledge as a weight matrix W .,5 Connecting to Relevant Studies,[0],[0]
"2
Different from W-SIMULE, JEEK separates the knowledge of individual context and the shared using different weight matrices.",5 Connecting to Relevant Studies,[0],[0]
"While W-SIMULE also minimizes a weighted `1 norm, its constraint optimization term is entirely different from JEEK.",5 Connecting to Relevant Studies,[0],[0]
The formulation difference makes the optimization of JEEK much faster and more scalable than WSIMULE (Section (6)).,5 Connecting to Relevant Studies,[0],[0]
"We have provided a complete theoretical analysis of error bounds of JEEK, while W-SIMULE provided no theoretical results.",5 Connecting to Relevant Studies,[0],[0]
"Empirically, we compare JEEK with W-SIMULE R package from (Singh et al., 2017) in the experiments.
",5 Connecting to Relevant Studies,[0],[0]
"JGL: (Danaher et al., 2013): Regularized MLE based multi-sGGMs Studies mostly follow the so called joint graphical lasso (JGL) formulation as Eq.",5 Connecting to Relevant Studies,[0],[0]
"(5.2):
argmin Ω(i) 0 K∑ i=1",5 Connecting to Relevant Studies,[0],[0]
(−L(Ω(i)),5 Connecting to Relevant Studies,[0],[0]
+ λn K∑ i=1,5 Connecting to Relevant Studies,[0],[0]
"||Ω(i)||1
",5 Connecting to Relevant Studies,[0],[0]
+ λ ′,5 Connecting to Relevant Studies,[0],[0]
nR ′,5 Connecting to Relevant Studies,[0],[0]
"(Ω (1) ,Ω (2) , . . .",5 Connecting to Relevant Studies,[0],[0]
",Ω (K) )
(5.2)
R′(·) is the second penalty function for enforcing some structural assumption of group property among the multiple graphs.",5 Connecting to Relevant Studies,[0],[0]
One caveat of JGL is that R′(·) cannot model explicit additional knowledge.,5 Connecting to Relevant Studies,[0],[0]
"For instance,it can not incorporate the information of a few known hub nodes shared by the contexts.",5 Connecting to Relevant Studies,[0],[0]
"In experiments, we compare JEEK to JGLco-hub and JGL-perturb-hub toolbox provided by (Mohan et al., 2013).
",5 Connecting to Relevant Studies,[0],[0]
"FASJEM: (Wang et al., 2017a)",5 Connecting to Relevant Studies,[0],[0]
One very recent study extended JGL using so-called Elementary superpositionstructured moment estimator formulation as Eq.,5 Connecting to Relevant Studies,[0],[0]
"(5.3):
argmin Ωtot
||Ωtot||1 + R′(Ωtot)
s.t.||Ωtot",5 Connecting to Relevant Studies,[0],[0]
− inv(Tv(Σ̂tot))||∞ ≤ λn R′∗(Ωtot − inv(Tv(Σ̂tot))),5 Connecting to Relevant Studies,[0],[0]
≤,5 Connecting to Relevant Studies,[0],[0]
"λn
(5.3)
",5 Connecting to Relevant Studies,[0],[0]
FASJEM is much faster and more scalable than the JGL estimators.,5 Connecting to Relevant Studies,[0],[0]
However like JGL estimators it can not model additional knowledge and its optimization needs to be carefully re-designed for differentR′(·).,5 Connecting to Relevant Studies,[0],[0]
"3
2It can be solved by any linear programming solver and can be column-wise paralleled.",5 Connecting to Relevant Studies,[0],[0]
"However, it is very slow when p > 200 due to the expensive computation cost O(K4p5).
",5 Connecting to Relevant Studies,[0],[0]
3FASJEM extends JGL into multiple independent group-entry wise optimization just like JEEK.,5 Connecting to Relevant Studies,[0],[0]
"HereR
′∗(·) is the dual norm of R′(·).",5 Connecting to Relevant Studies,[0],[0]
"Because (Wang et al., 2017a) only designs the optimization of two cases (group,2 and group,inf), we can not use it as a baseline.
",5 Connecting to Relevant Studies,[0],[0]
Both NAK and W-SIMULE only explored the formulation for estimating neural connectivity graphs using spatial information as additional knowledge.,5 Connecting to Relevant Studies,[0],[0]
"Differently our experiments (Section (6)) extend the weight-as-knowledge formulation on weights as distance, as shared hub knowledge, as perturbed hub knowledge, and as nodes’ grouping information (e.g., multiple genes are known to be in the same pathway).",5 Connecting to Relevant Studies,[0],[0]
This has largely extends the previous studies in showing the real-world adaptivity of the proposed formulation.,5 Connecting to Relevant Studies,[0],[0]
JEEK elegantly formulates existing knowledge based on the problem at hand and avoid the need to design knowledge-specific optimization.,5 Connecting to Relevant Studies,[0],[0]
"We empirically evaluate JEEK and baselines on four types of datasets, including two groups of synthetic data, one realworld fMRI dataset for brain connectivity estimation and one real-world genomics dataset for estimating interaction among regulatory genes (results in Section (6.2)).",6 Experiments,[0],[0]
"In order to incorporating various types of knowledge, we provide five different designs of the weight matrices in Section S:5.",6 Experiments,[0],[0]
"Details of experimental setup, metrics and hyper-parameter tuning are included in Section (S:6.1).",6 Experiments,[0],[0]
Baselines used in our experiments have been explained in details by Section (5).,6 Experiments,[0],[0]
"We also use JEEK with no additional knowledge (JEEKNK) as a baseline.
",6 Experiments,[0],[0]
JEEK is available as the R package ’jeek’ in CRAN.,6 Experiments,[0],[0]
"Inspired the JGL-co-hub and JGL-perturb-hub toolbox (JGL-node) provided by (Mohan et al., 2013), we empirically show JEEK’s ability to model known co-hub or perturbed-hub nodes as knowledge when estimating multiple sGGMs.",6.1 Experiment: Simulated Samples with Known Hubs as Knowledge,[0],[0]
"We generate multiple simulated Gaussian datasets through the random graph model (Rothman et al., 2008) to simulate both the co-hub and perturbed-hub graph structures (details in S:7.1).",6.1 Experiment: Simulated Samples with Known Hubs as Knowledge,[0],[0]
"We use JGL-node package, W-SIMULE and JEEK-NK as baselines for this set of experiments.",6.1 Experiment: Simulated Samples with Known Hubs as Knowledge,[0],[0]
"The weights in {W totI ,W totS } are designed using the strategy proposed in Section (S:5).
",6.1 Experiment: Simulated Samples with Known Hubs as Knowledge,[0],[0]
We use AUC score (to reflect the consistency and variance of a method’s performance when varying its important hyperparameter) and computational time cost to compare JEEK with baselines.,6.1 Experiment: Simulated Samples with Known Hubs as Knowledge,[0],[0]
"We compare all methods on many simulated cases by varying p from the set {100, 200, 300, 400, 500}",6.1 Experiment: Simulated Samples with Known Hubs as Knowledge,[0],[0]
"and the number of tasks K from the set {2, 3, 4}.",100 200 300 400 500,[0],[0]
"In Figure 2 and Figure S:1(a)(b), JEEK consistently achieves higher AUC-scores than the baselines JGL, JEEK-NK and W-SIMULE for all cases.",100 200 300 400 500,[0],[0]
JEEK is more than 10 times faster than the baselines on average.,100 200 300 400 500,[0],[0]
"In Figure 2, for each p > 300 case (with n = p/2), W-SIMULE takes more than one month and JGL takes more than one day.",100 200 300 400 500,[0],[0]
Therefore we can not show them with p > 300.,100 200 300 400 500,[0],[0]
"Next, we apply JEEK and the baselines on one real-world biomedical data about gene expression profiles across two different cell types.",6.2 Experiment: Gene Interaction Network from Real-World Genomics Data,[0],[0]
We explored two different types of knowledge: (1) Known edges and (2) Known group about genes.,6.2 Experiment: Gene Interaction Network from Real-World Genomics Data,[0],[0]
Figure S:1(c) shows that JEEK has lower time cost and recovers more interactions than baselines (higher number of matched edges to the existing bio-databases.).,6.2 Experiment: Gene Interaction Network from Real-World Genomics Data,[0],[0]
More results are in Appendix Section (S:7.2) and the design of weight matrices for this case is in Section (S:5).,6.2 Experiment: Gene Interaction Network from Real-World Genomics Data,[0],[0]
"Following (Bu & Lederer, 2017), we use one known Euclidean distance between human brain regions as additional knowledge W and use it to generate multiple simulated datasets (details in Section S:7.3).",6.3 Experiment: Simulated Data about Brain Connectivity with Distance as Knowledge,[0],[0]
"We compare JEEK with the baselines regarding (a) Scalability (computational time cost), and (b) effectiveness (F1-score, because NAK package does not allow AUC calculation).",6.3 Experiment: Simulated Data about Brain Connectivity with Distance as Knowledge,[0],[0]
"For each simulation case, the computation time for each estimator is the summation of a method’s execution time over all values of λn.",6.3 Experiment: Simulated Data about Brain Connectivity with Distance as Knowledge,[0],[0]
Figure S:2(a)(b) show clearly that JEEK outperforms its baselines.,6.3 Experiment: Simulated Data about Brain Connectivity with Distance as Knowledge,[0],[0]
JEEK has a consistently higher F1-Score and is almost 6 times faster than W-SIMULE in the high dimensional case.,6.3 Experiment: Simulated Data about Brain Connectivity with Distance as Knowledge,[0],[0]
"JEEK performs better than JEEK-NK, confirming the advantage of integrating additional distance knowledge.",6.3 Experiment: Simulated Data about Brain Connectivity with Distance as Knowledge,[0],[0]
"While NAK is fast, its F1-Score is nearly 0 and hence, not useful for multi-sGGM structure learning.",6.3 Experiment: Simulated Data about Brain Connectivity with Distance as Knowledge,[0],[0]
"We evaluate JEEK and relevant baselines for a classification task on one real-world publicly available resting-state fMRI dataset: ABIDE(Di Martino et al., 2014).",6.4 Experiment: Functional Connectivity Estimation from Real-World Brain fMRI Data,[0],[0]
"The ABIDE data aims to understand human brain connectivity and how it reflects neural disorders (Van Essen et al., 2013).",6.4 Experiment: Functional Connectivity Estimation from Real-World Brain fMRI Data,[0],[0]
"ABIDE includes two groups of human subjects: autism and control, and therefore we formulate it as K = 2 graph estimation.",6.4 Experiment: Functional Connectivity Estimation from Real-World Brain fMRI Data,[0],[0]
We utilize the spatial distance between human brain regions as additional knowledge for estimating functional connectivity edges among brain regions.,6.4 Experiment: Functional Connectivity Estimation from Real-World Brain fMRI Data,[0],[0]
We use Linear Discriminant Analysis (LDA) for a downstream classification task aiming to assess the ability of a graph estimator to learn the differential patterns of the connectome structures.,6.4 Experiment: Functional Connectivity Estimation from Real-World Brain fMRI Data,[0],[0]
"(Details of the ABIDE dataset, baselines, design of the additional knowledge W matrix, cross-validation and LDA classification method are in Section (S:7.4).)
",6.4 Experiment: Functional Connectivity Estimation from Real-World Brain fMRI Data,[0],[0]
"Figure S:2(c) compares JEEK and three baselines: JEEKNK, W-SIMULE and W-SIMULE with no additional knowledge (W-SIMULE-NK).",6.4 Experiment: Functional Connectivity Estimation from Real-World Brain fMRI Data,[0],[0]
"JEEK yields a classification accuracy of 58.62% for distinguishing the autism subjects versus the control subjects, clearly outperforming JEEK-NK and W-SIMULE-NK.",6.4 Experiment: Functional Connectivity Estimation from Real-World Brain fMRI Data,[0],[0]
"JEEK is roughly 7 times faster than the W-SIMULE estimators, locating at the top left region in Figure S:2(c) (higher classification accuracy and lower time cost).",6.4 Experiment: Functional Connectivity Estimation from Real-World Brain fMRI Data,[0],[0]
We also experimented with variations of theW matrix and found the classification results are fairly robust to the variations of W (Section (S:7.4)).,6.4 Experiment: Functional Connectivity Estimation from Real-World Brain fMRI Data,[0],[0]
"We propose a novel method, JEEK, to incorporate additional knowledge in estimating multi-sGGMs.",7 Conclusions,[0],[0]
JEEK achieves the same asymptotic convergence rate as the state-of-the-art.,7 Conclusions,[0],[0]
"Our experiments has showcased using weights for describing pairwise knowledge among brain regions, for shared hub knowledge, for perturbed hub knowledge, for describing group information among nodes (e.g., genes known to be in the same pathway), and for using known interaction edges as the knowledge.",7 Conclusions,[0],[0]
"We consider the problem of including additional knowledge in estimating sparse Gaussian graphical models (sGGMs) from aggregated samples, arising often in bioinformatics and neuroimaging applications.",abstractText,[0],[0]
Previous joint sGGM estimators either fail to use existing knowledge or cannot scale-up to many tasks (large K) under a highdimensional (large p) situation.,abstractText,[0],[0]
"In this paper, we propose a novel Joint Elementary Estimator incorporating additional Knowledge (JEEK) to infer multiple related sparse Gaussian Graphical models from large-scale heterogeneous data.",abstractText,[0],[0]
"Using domain knowledge as weights, we design a novel hybrid norm as the minimization objective to enforce the superposition of two weighted sparsity constraints, one on the shared interactions and the other on the task-specific structural patterns.",abstractText,[0],[0]
This enables JEEK to elegantly consider various forms of existing knowledge based on the domain at hand and avoid the need to design knowledgespecific optimization.,abstractText,[0],[0]
JEEK is solved through a fast and entry-wise parallelizable solution that largely improves the computational efficiency of the state-of-the-art O(pK) to O(pK).,abstractText,[0],[0]
We conduct a rigorous statistical analysis showing that JEEK achieves the same convergence rate O(log(Kp)/ntot) as the state-of-the-art estimators that are much harder to compute.,abstractText,[0],[0]
"Empirically, on multiple synthetic datasets and two real-world data, JEEK outperforms the speed of the state-ofarts significantly while achieving the same level of prediction accuracy.",abstractText,[0],[0]
"Department of Computer Science, University of Virginia, http://www.jointnets.org/ .",abstractText,[0],[0]
"Correspondence to: Beilun Wang <bw4mw@virginia.edu>, Yanjun Qi <yanjun@virginia.edu>.",abstractText,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",abstractText,[0],[0]
Copyright 2018 by the author(s).,abstractText,[0],[0]
A Fast and Scalable Joint Estimator for Integrating Additional Knowledge in Learning Multiple Related Sparse Gaussian Graphical Models,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1149–1159 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1149",text,[0],[0]
The process of language change should be thought of as a two-step cycle in which 1) individuals acquire their native languages from their predecessors then 2) pass them on to their successors.,1 Introduction,[0],[0]
Small changes accrue over time this way and create both small-scale interpersonal variation and large-scale typological differences.,1 Introduction,[0],[0]
It is easy to draw a strong analogy here between linguistic evolution and biological evolution.,1 Introduction,[0],[0]
"Both feature classic descent with modification, except while phenotypes are transmitted through genes and acted on by natural selection, language is both transmitted through and constrained by the individual
(Cavalli-Sforza and Feldman, 1981; Ritt, 2004, etc.).
",1 Introduction,[0],[0]
"But while evolution, linguistic or otherwise, is driven by forces acting on the individual, it unfolds on the level of populations (Cavalli-Sforza and Feldman, 1981).",1 Introduction,[0],[0]
"The influence of communitylevel social factors on the path of language change is a major focus of sociolinguistics (Labov, 2001; Milroy and Milroy, 1985; Rogers Everett, 1995).",1 Introduction,[0],[0]
"Ideally, one could observe population-level variation unfold in real time while testing out individual factors, but this is impossible because nobody can travel back in time or fit entire natural environments into a lab.",1 Introduction,[0],[0]
"Change that has already happened is out of reach, and change in progress is buried in a world of confounds.",1 Introduction,[0],[0]
"The classic sociolinguistic method instead approaches the problem by inferring causal factors from patterns discovered in field interviews and corpora (Labov, 1994; Labov et al., 2005, etc.).",1 Introduction,[0],[0]
"This is the primary source of empirical data in the field and the only way to look at language change in a naturalistic setting, but it is limited in that it cannot test cause and effect directly.",1 Introduction,[0],[0]
"More recently, controlled experimental studies have emerged as a complementary line of research which manipulate causal factors directly (Johnson et al., 1999; Campbell-Kibler, 2009, etc.), but are inherently removed natural time and scale.",1 Introduction,[0],[0]
"A third approach, the one we build upon here, relies on computational modeling to simulate how sociolinguistic factors might work together in larger populations (Klein, 1966; Blythe and Croft, 2012; Kauhanen, 2016, etc.).
",1 Introduction,[0],[0]
"It has long been argued that language acquisition is the primary cause of language change (Sweet, 1899; Lightfoot, 1979; Niyogi, 1998, etc.).",1 Introduction,[0],[0]
"In the last few decades, this connection has been modeled computationally (Gibson and Wexler, 1994; Kirby et al., 2000; Yang, 2000,
etc.), leading to the strong conclusion that change is the inevitable consequence of mixed linguistic input or finite learning periods (Niyogi and Berwick, 1996), even if children are “perfect” learners.",1 Introduction,[0],[0]
An important result connecting the learner and population emphasizes the need for this line of work: the space of paths of change available in populations is formally larger than the paths available to linear chains of iterated learners.,1 Introduction,[0],[0]
"Niyogi and Berwick (2009) prove formally that even perfectly-mixed (i.e., uniform and homogeneous social network) populations admit phase transitions in the path of change unavailable to chains of single learners commonly implemented in iterated learning (Kirby et al., 2000).",1 Introduction,[0],[0]
"This suggests that small-population experimental studies in sociolinguistics and in child language acquisition do not paint the full picture of language change.
",1 Introduction,[0],[0]
We introduce a new framework for modeling language change in populations.,1 Introduction,[0],[0]
"It has an outer loop to represent generational progression, but it replaces the inner loop which calculates randomized interactions between agents with a single formula that is defined generally enough to allow the simulation of a wide range of scenarios.",1 Introduction,[0],[0]
"It builds upon the principled formalism described by Niyogi and Berwick (1996, et seq.), privileging the acquisition model and separating it from the population model.",1 Introduction,[0],[0]
The resulting modular framework is described in the following sections.,1 Introduction,[0],[0]
"First, Section 1.1 presents a survey of previous simulation work followed by a description of the new population model in Section 2.",1 Introduction,[0],[0]
"Next, Section 3 addresses practical concerns relating population size to assumptions about language acquisition.",1 Introduction,[0],[0]
"Finally, Section 4 introduces a case study on phonological change which demonstrates the need for appropriate models both of acquisition and populations.",1 Introduction,[0],[0]
Computational models for the propagation of linguistic variation have been employed with a variety of research goals in mind.,1.1 Related Work,[0],[0]
"Every paper implements its own framework with few exceptions, so comparison across studies is difficult.",1.1 Related Work,[0],[0]
"Additionally, since each model is essentially ‘boutique,’ it is always possible that models are designed consciously or unconsciously to achieve a specific outcome rather driven by underlying principles.",1.1 Related Work,[0],[0]
"We group these frameworks into three classes according to their implementation, swarm, network,
and algebraic, and discusses their strengths and weaknesses.
",1.1 Related Work,[0],[0]
"The first class, called swarm here, models populations as collections of agents placed on a grid.",1.1 Related Work,[0],[0]
"They “swarm” around randomly according to some movement function, and “interact” when they occupy adjacent grid spaces (Satterfield, 2001; Harrison et al., 2002; Ke et al., 2008; Stanford and Kenny, 2013).",1.1 Related Work,[0],[0]
"This tends toward concrete interpretation, for example, more mobile populations are expressed directly by more mobile agents.",1.1 Related Work,[0],[0]
They capture Bloomfield (1933)’s “principle of density” which describes the observation that geographically or socially close individuals interact more frequently than those farther away.,1.1 Related Work,[0],[0]
"On the other hand, they provide little control over network structure, relying on series of explicit movement constraints in order to direct their agents, and since each one moves randomly at each iteration, these models have potentially thousands of degrees of freedom.",1.1 Related Work,[0],[0]
"Such simulations should be run many times if any sort of statistically expected results are to be computed.
",1.1 Related Work,[0],[0]
"The second class, network frameworks, model speakers as nodes and interaction probabilities as weighted edges on network graphs (Minett and Wang, 2008; Baxter et al., 2009; Fagyal et al., 2010; Blythe and Croft, 2012; Kauhanen, 2016).",1.1 Related Work,[0],[0]
These frameworks offer precise control over social network structure and can test specific community models from within sociolinguistics.,1.1 Related Work,[0],[0]
"However, implementations usually proceed by some kind of iterative probabilistic node-pair selection process, and in this way suffer from the same statistical pitfalls as swarm frameworks.",1.1 Related Work,[0],[0]
"In contrast to swarm models, interaction is rigidly restricted to immediately connected nodes, so to achieve gradient interaction probabilities, edges must be frequently updated or nearly fully-connected graphs with carefully assigned edge weights would need to be constructed and motivated.
",1.1 Related Work,[0],[0]
"The third class, algebraic frameworks, present analytic methods for determining the state of the network at the end of each iteration rather than relying on stochastic simulation of individual agents (Niyogi and Berwick, 1996, 1997; Yang, 2000; Baxter et al., 2006; Minett and Wang, 2008; Niyogi and Berwick, 2009).",1.1 Related Work,[0],[0]
Removing that inner loop is a more mathematically elegant approach and avoids dealing unnecessarily with statistics behind random trials.,1.1 Related Work,[0],[0]
"Removing that loop speeds
up calculation as well, making larger simulations more tractable than with network or swarm frameworks.",1.1 Related Work,[0],[0]
But this power is achieved by sacrificing the social network.,1.1 Related Work,[0],[0]
"Up to this point, such models have, to our knowledge, only been defined over perfectly-mixed (i.e., no network effects) populations.",1.1 Related Work,[0],[0]
"That assumption is useful for reasoning about the mathematical theory behind language change, but it hinders such models’ utility in empirical studies.",1.1 Related Work,[0],[0]
"For example, though Baxter et al. (2006) and Minett and Wang (2008) implement algebraic models for perfectly mixed populations, they fall back on network models to model network effects.",1.1 Related Work,[0],[0]
"Algebraic frameworks have their mathematical advantage, but network frameworks provide a richer model for representing real-world population structures and swarm models capture density effects by default.",2 Framework for Transmission in Social Networks,[0],[0]
An ideal framework would combine the benefits of all three of these.,2 Framework for Transmission in Social Networks,[0],[0]
Here we do just that.,2 Framework for Transmission in Social Networks,[0],[0]
We introduce a framework that instantiates Niyogi and Berwick (1996)’s acquisitiondriven formalism where change is handled explicitly as a two-step alternation between individual learners learning and populations interacting.,2 Framework for Transmission in Social Networks,[0],[0]
"It provides an analytic solution to the state of a network structure over which swarm-like behavior can be modeled.
",2 Framework for Transmission in Social Networks,[0],[0]
We begin by conceptualizing the framework in terms of agents traveling probabilistically over a network structure as in Algo.,2 Framework for Transmission in Social Networks,[0],[0]
1 before introducing the analytic solution.,2 Framework for Transmission in Social Networks,[0],[0]
"There is an individual standing at every node in the graph, and at every iteration, each individual begins at some location and travels along the network’s edges, at each step deciding to continue on or to stop and interact with the agent at that node.",2 Framework for Transmission in Social Networks,[0],[0]
"Any two agents with a nonzero weight path between them could potentially interact, so the overall probability of an interaction is a function of the shape of the network and the decay rate of the step probability.",2 Framework for Transmission in Social Networks,[0],[0]
"The shorter and higher weighted the path between two agents, the more likely they are to interact.",2 Framework for Transmission in Social Networks,[0],[0]
"This corresponds to the gradient interaction probabilities of swarm frameworks.
",2 Framework for Transmission in Social Networks,[0],[0]
"Algorithm 1: One iteration of the propagation model conceptualized on the level of an individual agent
for each individual node do Begin traveling; while traveling do
Randomly select an outgoing edge by weight and follow it OR stop travel;
increase chance of stopping next time; end Interact with the individual at the current node;
end",2 Framework for Transmission in Social Networks,[0],[0]
"Social networks are typically conceived of as graph structures with individuals as vertices and the social or geographical connections between individuals as edges, and this allows for a great deal of flexibility.",2.1 Representing the Network,[0],[0]
"If edges are undirected, then all interactions are equal and bidirectional, but if edges are directed, interactions may or may not be.",2.1 Representing the Network,[0],[0]
"Edges can be weighted to represent likelihood of interaction or some measure of social valuation, and this too can vary over time.",2.1 Representing the Network,[0],[0]
"Lastly, it is possible to add and remove nodes themselves to capture births, deaths, or migration.
",2.1 Representing the Network,[0],[0]
The network structure is represented computationally here as an adjacency matrix A.,2.1 Representing the Network,[0],[0]
"In a population of n individuals, this is n × n where each element aij is the weight of the connection from individual j to",2.1 Representing the Network,[0],[0]
individual i.,2.1 Representing the Network,[0],[0]
The matrix must be column stochastic (all columns sum to 1 and contain only positive elements) so that edge weights can be interpreted as probabilities.,2.1 Representing the Network,[0],[0]
"The special case where the matrix is symmetric (every aij = aji) models undirected edges, and more strongly, the model reduces to perfectly-mixed populations when each aij = 1n .
",2.1 Representing the Network,[0],[0]
We define a notion of communities over the nodes of the network in order to add the option to categorize groups of individuals.,2.1 Representing the Network,[0],[0]
"Membership among c communities is identified with an n × c indicator matrix C. Depending on the problem at hand, it is possible to calculate the average behavior of the learners within each community directly without having to calculate the behavior of each individual member.",2.1 Representing the Network,[0],[0]
"In a typical network model, the edge weights between nodes in A are interpreted directly as interaction probabilities, meaning that individuals only ever interact with their immediate graph neighbors.",2.2 Propagation in the Network,[0],[0]
We take a different approach by allowing the agents to “travel” and potentially interact with any other agent whose node is connected by a path of non-zero edges.,2.2 Propagation in the Network,[0],[0]
"If the number of traveling steps were fixed at k, the probability of each pair interacting would be defined as Ak.",2.2 Propagation in the Network,[0],[0]
It is more complicated for us since the number of steps traveled is a random variable.,2.2 Propagation in the Network,[0],[0]
The probability of j interacting with i (p(ij)) is the probability of them interacting after k steps times the probability of k for all values of k as in Eqn. 1.,2.2 Propagation in the Network,[0],[0]
"Combining this intuition with A yields the interaction probabilities for all i, j pairs.
p(ij) =",2.2 Propagation in the Network,[0],[0]
"∑ k p(ij|k steps) p(k steps) (1)
",2.2 Propagation in the Network,[0],[0]
"The pattern of linguistic variants or grammars (in the formal sense where grammar g is the intensional equivalent of languageLg) within a network unfolds as a dynamical system over the course of many iterations, and learners’ positions within the network mediate which ones they eventually acquire.",2.2 Propagation in the Network,[0],[0]
"In a system with g grammars and n individuals, a n × g row-stochastic matrix G specifies the probability with which each community expresses each grammar.",2.2 Propagation in the Network,[0],[0]
"Given this notion of interaction and the specification of grammars expressed within a network, it is possible to compute the distribution of grammars presented to each learner.",2.2 Propagation in the Network,[0],[0]
"This is the learners’ linguistic environment and is represented by a matrix E in the same form as G>.
",2.2 Propagation in the Network,[0],[0]
"An environment function En(Gt,A) =",2.2 Propagation in the Network,[0],[0]
Et+1 shown in Eqn. 2 calculates E by first calculating all the interaction probabilities in the network then multiplying those by the grammars which every agent expresses to get the environment E.,2.2 Propagation in the Network,[0],[0]
The α parameter from the geometric distribution1 defines the travel decay rate.,2.2 Propagation in the Network,[0],[0]
"A lower α defines conceptually more mobile agents.
",2.2 Propagation in the Network,[0],[0]
"More generally, En is a special case of E(Gt,Ct,At) = Et+1 where the number of communities equals the number of individuals (c = n).
",2.2 Propagation in the Network,[0],[0]
"1In this paper, jump probabilities decay according to a geometric distribution, but other distributions including the Poisson have been implemented as well.
",2.2 Propagation in the Network,[0],[0]
"C becomes the identity matrix without loss of generality, so the network’s initial condition does not have to be defined explicitly.",2.2 Propagation in the Network,[0],[0]
"For any other community definition, an initial condition has to be defined as in Eqn. 3 which specifies the starting point in the network that each agent conceptually begins traveling from.",2.2 Propagation in the Network,[0],[0]
"The output of E is a g × c matrix giving the environment of the average agent in each community.2
En(Gt,A) = G>t α (I− (1− α)A) −1",2.2 Propagation in the Network,[0],[0]
"(2) E(Gt,C,A) = En(Gt,A)C(C>C)−1 (3)
",2.2 Propagation in the Network,[0],[0]
"The output of E must be broadcast to g × n, which would result in the loss of some information unless the assumption can be made that each community is internally uniform.",2.2 Propagation in the Network,[0],[0]
"However, when that assumption can be made, the n×n adjacency matrix admits a c× c equitable partition",2.2 Propagation in the Network,[0],[0]
"Aπ (Eqn. 4) (Schaub et al., 2016) which permits an alternate environment function EEP (Gt,C,A) shown in Eqn. 5 that is equivalent to the lossless En if A. If n c, EEP is much faster to calculate because it only inverts a small c × c matrix rather than a large n ×",2.2 Propagation in the Network,[0],[0]
n.,2.2 Propagation in the Network,[0],[0]
"This makes it feasible to run much larger simulations than what has been done in the past.
",2.2 Propagation in the Network,[0],[0]
Aπ = (C>C)−1C>AC (4) EEP = αG>C (I− (1− α)Aπ)−1 (C>C)−1 (5),2.2 Propagation in the Network,[0],[0]
The environment function describes what inputs Et+1 are available to learners given the language expressed by the mature speakers of the previous age cohort with grammars Gt.,2.3 Learning in the Network,[0],[0]
The second component of the framework describes the learning algorithm A(Et+1),2.3 Learning in the Network,[0],[0]
"= Gt+1, how individuals respond to their input environment.",2.3 Learning in the Network,[0],[0]
The resulting Gt+1 describes which grammars those learners will eventually contribute to the subsequent generation’s environment Et+2.,2.3 Learning in the Network,[0],[0]
"This back-andforth between adults’ grammars G and childrens’ environment E is the two-step cycle of language change (Fig. 1).
",2.3 Learning in the Network,[0],[0]
"In neutral change, learners would acquire grammars at the rates that they are expressed in their environments, but there is good reason to believe
2(I− (1− α)A)−1 and C(C>C)−1 can be precomputed if network structure does not change over time.
. .",2.3 Learning in the Network,[0],[0]
.Gt,2.3 Learning in the Network,[0],[0]
→ Et+1 → Gt+1 . .,2.3 Learning in the Network,[0],[0]
.Gt+i,2.3 Learning in the Network,[0],[0]
"→ Et+i+1 . . .
",2.3 Learning in the Network,[0],[0]
"Figure 1: Language change as an alternation between G and E matrices
that most language change involves differential fitness between competing variants, and most nontrivial learning algorithms yield some kind of fitness (Kroch, 1989; Yang, 2000; Blythe and Croft, 2012, etc.), so A is rarely neutral.",2.3 Learning in the Network,[0],[0]
"A neutral and simple advantaged model are both considered in Section 3, and a more complex learning algorithm is described for Section 4.",2.3 Learning in the Network,[0],[0]
The general nature of the framework described here renders it suitable for reproducing the results of previous works and evaluating their assumptions.,3 Application: Testing Assumptions,[0],[0]
"To demonstrate this, we reproduce the major result from Kauhanen (2016), which tested the behavior of neutral change in networks of singlegrammar learners, in order to dissect two of its primary assumptions.",3 Application: Testing Assumptions,[0],[0]
"Implemented in a typical network framework, the original setup contains n = 200 individuals in probabilistically generated centralized networks in which individuals mature categorically to the single most frequent grammar in their input.",3 Application: Testing Assumptions,[0],[0]
The author found that categorical neutral change produced chaotic paths of change regardless of network shape and that periodically “rewiring” some of the network edges smoothed this out.,3 Application: Testing Assumptions,[0],[0]
"Without commenting on rewiring, we find that the combination of n and choice of categorical learners conspire to create the chaotic results.
",3 Application: Testing Assumptions,[0],[0]
"We create two communities, both centralized along the lines of the single cluster in Kauhanen (2016), initialize all members of cluster 1 with grammar g1 and all members of cluster 2 with grammar g2, and additional edges are added between members of clusters 1 and 2 to allow interaction.",3 Application: Testing Assumptions,[0],[0]
"G is converted to an indicator matrix at the end of each learning iteration by rounding values to 0 and 1 in order to model categorical learners who only internalize the most common grammar in their inputs as in the original model.
",3 Application: Testing Assumptions,[0],[0]
"In a pair of infinitely large clusters or two clusters where individuals are permitted to learn a probabilistic distribution of grammars, each cluster should homogenize to a 50/50 distribution of
g1 and g2 after some number of iterations depending on the specifics of the network shape and setting for α creating the red curves in Fig. 2.",3 Application: Testing Assumptions,[0],[0]
"At n = 20000, each of 10 trials roughly follows the path of the predicted curve, but when run at the original n = 200 for 10 trials, this produces the type of chaotic behavior which Kauhanen (2016) attempts to repair.",3 Application: Testing Assumptions,[0],[0]
"The outcome appears to be the result of an assumption made out of convenience (n = 200) rather than a principled decision.
",3 Application: Testing Assumptions,[0],[0]
"To further explore the impact of the population size assumption, we experiment on a model of advantaged change, which is typically contrasted with neutral change because of its tendency to produce “well-behaved” S-curve change (Blythe and Croft, 2012; Kauhanen, 2016).",3 Application: Testing Assumptions,[0],[0]
"This time, only a single cluster is created, and the advantaged grammar is initially assigned to 1% of the population.",3 Application: Testing Assumptions,[0],[0]
"As seen in Figure 3, results are chaotic for n = 200 once again and near predicted for n = 20000.",3 Application: Testing Assumptions,[0],[0]
"This is important because at n = 200, advantaged change is chaotic, and most simulations both rise and fall.",3 Application: Testing Assumptions,[0],[0]
An experimenter who only studied advantaged change in small population might concluded that it is as ill-behaved as neutral change.,3 Application: Testing Assumptions,[0],[0]
"While the conclusions that Kauhanen (2016) draws appear valid for n = 200, it is not clear to what extent they can be projected onto larger populations.",3 Application: Testing Assumptions,[0],[0]
This demonstrates the need for carefully choosing one’s modeling assumptions and testing them out when possible.,3 Application: Testing Assumptions,[0],[0]
The acquisition of phonological mergers in mixed input settings presents an interesting problem.,4 Application: Mergers in Progress,[0],[0]
"It appears that mergers have an inherent advantage because they tend to spread at the expense of distinctions, and once they begin, they are rarely reversed (Labov, 1994).",4 Application: Mergers in Progress,[0],[0]
"Yang (2009)’s acquisition model quantifies this advantage as the relatively
lower chance of misinterpretation if a listener assumes the merged grammar instead of the nonmerged grammar once a sufficient proportion of the environment is merged.",4 Application: Mergers in Progress,[0],[0]
"Applied to Johnson (2007)’s detailed population study of the frontier of the COT-CAUGHT merger in the small towns along the border between Rhode Island and Massachusetts, this accurately predicts the ratio of merged input for a child to acquire the merged grammar, however when applied to a perfectly mixed population of learners, it fails to model the spread of the merged grammar in the population.",4 Application: Mergers in Progress,[0],[0]
"Yang’s model is input-driven, so it is conducive to simulation with minimal assumptions past those drawn from the empirical data.",4 Application: Mergers in Progress,[0],[0]
We test the behavior of this learning model in a typical population network and demonstrate that it produces a reasonable path of change.,4 Application: Mergers in Progress,[0],[0]
"The COT-CAUGHT merger, also called the low back merger describes the phenomenon present in varieties of North American English spoken in eastern New England, western Pennsylvania, the American West, and Canada among others where the vowel in words like cot and the vowel in words like caught have come to be pronounced the same (Labov et al., 2005, pp. 58-65).",4.1 Background,[0],[0]
"The geographical extent of the merger is currently expanding, which might be expected if the merger has a cognitive or social advantage associated with it.",4.1 Background,[0],[0]
"Johnson (2007)’s study of the merger’s frontier on the border Rhode Island and Massachusetts uncovered an interesting social dynamic that illustrates the merger’s speed: there are families where the parents and older siblings non-merged, but the younger siblings are.",4.1 Background,[0],[0]
"The merger has swept through in only a few years and passed between the siblings.
",4.1 Background,[0],[0]
"Yang (2009) seeks to understand why mergers have an advantage from a cognitive perspective, and his model treats the acquisition of mergers as an evolutionary process.",4.1 Background,[0],[0]
Learners who receive both merged (M+) and non-merged (M−) input entertain both a merged (g+) and non-merged (g−) grammar and reward whichever grammar successfully parses the input.,4.1 Background,[0],[0]
"This kind of variational learner (Yang, 2000) is essentially an adaptation of the classic evolutionary Linear Reward Punishment model (Bush and Mosteller, 1953).",4.1 Background,[0],[0]
"The fitness of each grammar is the probability in the limit that it will fail to parse any given input, and since it is virtually always the case that this probability is different for both grammars, fitness is virtually always asymmetric.",4.1 Background,[0],[0]
"The variational learner is characterized as follows.
",4.1 Background,[0],[0]
"Given two grammars and an input token s, The learner parses s with g1 with probability p and with g2 with probability q = 1−",4.1 Background,[0],[0]
"p. p is rewarded according to whether the choice of g successfully parses s (g → s) or it fails to (g 9 s), where γ is some small constant.
p′ = { p+ γq, g → s (1− γ)p, g 9 s
Given a specific problem, one can calculate a penalty probability C for each g, the proportion of input that would cause g 9 s.",4.1 Background,[0],[0]
"The grammar with the lower C has the advantage, so the other one will be driven down in the long run.",4.1 Background,[0],[0]
"C can be estimated from type frequencies in a corpus, and the model is non-parametric because these values do not depend on γ.
lim t→∞
pt = C2
C1 + C2 lim t→∞
qt = C1
C1 + C2
To understand the COT-CAUGHT merger empirically, one must reason about what kind of input would trigger a penalty and then calculate the penalty probabilities of the merged grammar C+ and non-merged grammar C− from a corpus.",4.1 Background,[0],[0]
"This model considers parsing failure to be the rate of initial misinterpretation, and for a vowel merger, the only inputs that could create an initial misinterpretation are minimal pairs because they become homophones.",4.1 Background,[0],[0]
"Examples of COT-CAUGHT minimal pairs include cot-caught, Don-Dawn, stock-stalk, odd-awed, collar-caller, and so on.
",4.1 Background,[0],[0]
"The merged g+ grammar collapses would-be minimal pairs into homophones, so the penalty
rate C+ comes down to lexical access.",4.1 Background,[0],[0]
"Under the observation that more frequent homophones are retrieved first regardless of syntactic context (Caramazza et al., 2001), g+ listeners only suffer initial misinterpretation when the less frequent member of a pair is uttered regardless of the rate of M+.",4.1 Background,[0],[0]
"If H is the sum token frequency of all minimal pairs and hio, h",4.1 Background,[0],[0]
"i oh are the frequencies of the ith pair’s members, then C+ is calculated by Eqn. 6.",4.1 Background,[0],[0]
"In contrast, g− listeners are sensitive to the phonemic distinction, so they misinterpret M− input at the rate of mishearing one vowel for the other (Peterson and Barney, 1952)",4.1 Background,[0],[0]
(second half of Eqn. 7).,4.1 Background,[0],[0]
"And given M+ input, they misinterpret whenever they hear the phoneme which g− does not expect (e.g., a merged speaker pronouncing cot with the CAUGHT vowel) times the probability of not mishearing that vowel (1- ) plus times the probability of hearing the right vowel (i.e., the merged speaker pronounces cot with the COT vowel but it is misheard anyway)",4.1 Background,[0],[0]
(first half of Eqn. 7),4.1 Background,[0],[0]
.,4.1 Background,[0],[0]
"Since g− misinterpretation rates are a function of the rate of M+ (p) in the environment, there is a threshold of M+ speakers above which the merged grammar has a fitness advantage over the non-merged one.
",4.1 Background,[0],[0]
"C+ = 1
H ∑ i min(hio, h",4.1 Background,[0],[0]
"i oh) (6)
C− = 1
H ∑ i [ p0((1− oh)hio + ohhioh) (7)
+q0( ohh i o",4.1 Background,[0],[0]
+ ohh,4.1 Background,[0],[0]
"i oh) ]
Calculating this threshold for the frequent minimal pairs that Yang extracts from the Wortschatz project (Biemann et al., 2004) corpus3 and mishearing rates from Peterson and Barney (1952), the Yang model predicts that a learner exposed to at least ∼ 17% COT-CAUGHT-merged input will acquire the merger.",4.1 Background,[0],[0]
This threshold represents a strong advantage for M+ because it is well under the 50% threshold expected for neutral (non-advantaged) change and it is very close to what was found in Johnson (2007)’s sociolinguistic study.,4.1 Background,[0],[0]
"It predicts that younger children may have g+ while their parents and even older siblings
3Don (1052) – Dawn (736); collar (403) – caller (23); knotty (25) – naughty (195); odd (830) – awed (80); Otto (67) – auto (260); tot (9) – taught (1327); cot (39) – caught (2444); pond (258) – pawned (31); hock (25) – hawk (127); nod (180) – gnawed (53); sod (30) – sawed (37)
have g− if the 17% threshold was crossed in E after the acquisition period of the older sibling but before that of the younger sibling.",4.1 Background,[0],[0]
All the mechanics behind the learning model reduce to a simple statement: learners acquires g+ iff > 17% of their input is M+ and they acquire g− otherwise.,4.2 Model Setup,[0],[0]
"However, this kind of categorical learner in a perfectly-mixed population leads to immediate fixation at either g− or g+ in a single iteration, since the proportion of g+ speakers in the population is equivalent to the proportion of M+ input in every learner’s environment.",4.2 Model Setup,[0],[0]
This is not realistic change.,4.2 Model Setup,[0],[0]
"Clearly, social network structure is at least as important as the learning algorithm in modeling the spread of the merger.
",4.2 Model Setup,[0],[0]
We model the change in a non-uniform social network of 100 centralized clusters of 75 individuals each.,4.2 Model Setup,[0],[0]
"75 was chosen as half Dunbar’s number, the maximum number of reliable social connections that an adult can maintain (Dunbar, 2010).",4.2 Model Setup,[0],[0]
"There are two grammars, g+ and g−, and learners internalize one or the other according to the 17% threshold of M+ in their input.",4.2 Model Setup,[0],[0]
"One cluster represents the source of the merger and is initialized at 100% g+, while the rest begin 100%",4.2 Model Setup,[0],[0]
g−. Inter-cluster connections are chosen randomly so that some connections are between central members of the clusters and some are between peripheral members.,4.2 Model Setup,[0],[0]
"The one merged cluster is connected to half the other clusters representing those at the frontier of the change, and each other cluster is connected to five randomly chosen",4.2 Model Setup,[0],[0]
ones.4,4.2 Model Setup,[0],[0]
"This network structure echoes work in sociolinguistics, in particular, Milroy and Milroy (1985)’s notion of strong and weak connections in language change, where weak connections between social clusters are particularly important for propagation of a change.
",4.2 Model Setup,[0],[0]
"Propagation of the merged grammar is calculated by En because we are interested in the behavior of individuals without loss of precision and because it cannot be assumed that each cluster is internally uniform.5 Since the spread of the merger has been rapid enough to detect over a period of a few years, iterations are modeled as short age co-
4Originally, the clusters were set up as a “stepping-stone” chain with the merged community at one end, and that produced a similar S-curve.",4.2 Model Setup,[0],[0]
"The structure presented here is more geographically plausible but not crucial for the results.
",4.2 Model Setup,[0],[0]
"5α = 0.45.
horts rather than full generations in the first experiments by updating only a randomly chosen 10% of nodes at each iteration because only a fraction of the population is learning at any given time.",4.2 Model Setup,[0],[0]
A model where every node is updated is investigated as well.,4.2 Model Setup,[0],[0]
The behavior of this simulation is shown graphically in Figure 4.,4.3 Results,[0],[0]
"The fine/colored lines indicate the rate of M+ within each initially non-merged cluster, and the bold/black line shows the average rate across all initially non-merged.",4.3 Results,[0],[0]
"The merger spreads from cluster to cluster in succession over the “weak” inter-cluster connections and through each cluster over the ‘strong’ connections before moving on to the next ones.
",4.3 Results,[0],[0]
"Most individual clusters exhibit a period of time in which only a few early adopter (Rogers Everett, 1995) members have the merger, a period of rapid diffusion of the merger, then some time where a few laggards resist the merger.",4.3 Results,[0],[0]
"As a result, most clusters exhibit an S-like shape.",4.3 Results,[0],[0]
"A few clusters change rapidly because of their especially wellconnected positions in the network, and some lag behind the rest because they are poorly connected to the rest of the network.",4.3 Results,[0],[0]
"More interestingly, the population-wide average, the population-level data at the kind of granularity that is often studied, yields a smooth S-curve with a shallower slope than the individual clusters.",4.3 Results,[0],[0]
"The fact that it arises naturally here in a network that conforms with typical network shapes but was otherwise randomly generated is encouraging because the experiment was not set up so that it would produce such a curve, and the steep rate of change in individual
clusters is what is expected for a change that is rapid enough to affect siblings differently.
",4.3 Results,[0],[0]
"In the above simulation, only a fraction of nodes were updated at each iteration in order to model a rapid change.",4.3 Results,[0],[0]
"In order to confirm that this choice is not affecting the results and to test a purer implementation of the framework presented here, we remove that constraint and update every node at each iteration.",4.3 Results,[0],[0]
Figure 5 shows what happens over 20 iterations in a network that is otherwise identical but with 2/5 as many inter-cluster connections as the original.,4.3 Results,[0],[0]
"A qualitatively similar pattern arises, so the choice to update only a fraction of the population is not crucially affecting the results.
",4.3 Results,[0],[0]
"In all experiments so far, social connections were fixed at the first iteration even though connections in real populations tend to change over time.",4.3 Results,[0],[0]
"To investigate that modeling assumption, we perform another simulation in which connections are randomly updated both within and across clusters at each iteration akin to Kauhanen (2016)’s rewiring.",4.3 Results,[0],[0]
"The result as shown in Figure 6 is similar to before, with one major difference.",4.3 Results,[0],[0]
"The individual clusters transition more closely in time because no individual cluster remains poorly connected or especially well connected throughout the entire simulation.
",4.3 Results,[0],[0]
"Finally, we test our assumptions about population size by repeating the experiments on a smaller network of 40 clusters of 18 individuals.",4.3 Results,[0],[0]
"The results are qualitatively similar, but the S-curve appears to be more sensitive to probabilistic connections in the network.",4.3 Results,[0],[0]
"To explore this, we present the average network-wide rate of (M+) across 10 trials, revealing that an S-like curve is formed each time but that its slope varies.",4.3 Results,[0],[0]
"A few trials never
reach 100% because some of the clusters are not connected to the innovative one.",4.3 Results,[0],[0]
"The slope varies between trials, indicating that the rate of change is a function of both the population structure and the learning algorithm, but the network size does not substantially affect these results.",4.3 Results,[0],[0]
The algebraic-network framework for modeling population-level language change presented here has substantial practical and theoretical advantages over previous ones.,5 Discussion,[0],[0]
It is much simpler computationally than previous frameworks because it calculates the statistically expected behavior of each generation analytically and therefore removes the entire inner loop of calculating stochastic inter-agent interactions from the simulation.,5 Discussion,[0],[0]
"It follows the Niyogi and Berwick (1996) formalism for language change which presents a clean and modular way of reasoning about the problem and promotes the centrality of language acquisition.
",5 Discussion,[0],[0]
"In addition to the core algorithm, the framework offers enough flexibility to represent a wide variety of processes from the highly abstract (e.g., Kauhanen (2016)) to those grounded in soci-
olinguistic and acquisition research (e.g., Yang (2009)).",5 Discussion,[0],[0]
"In our investigation of Kauhanen’s basic assumptions, we discover how seemingly innocuous decisions about population size and learning conspire to drive simulation results.",5 Discussion,[0],[0]
"If learners are conceived as categorical learners, population size becomes a deciding factor in the path of change.",5 Discussion,[0],[0]
"So while the original results are interesting and meaningful, they may only valid for small (on the order of 102) populations.
",5 Discussion,[0],[0]
"In our simulation of the spread of the COTCAUGHT merger, we show how a cognitivelymotivated model of acquisition requires a network model in order to represent population-level language change.",5 Discussion,[0],[0]
"The population is represented as a collection of individual clusters based on sociological work, but the clusters themselves are connected randomly.",5 Discussion,[0],[0]
"The fact that S-curves arise naturally from these networks underscores their centrality to language change.
",5 Discussion,[0],[0]
"One problem that this line of simulation work has always faced has been the lack of viable comparison between models because every study implements its own learning, network, and interaction models.",5 Discussion,[0],[0]
The modular nature of our framework advances against this trend since it is now possible to hold the population model constant while slotting in various learning models to test them against one another and vice-versa.,5 Discussion,[0],[0]
"Finally, since this framework reduces to Niyogi & Berwick’s models in perfectly-mixed populations, it can be used to reason about the formal dynamics of language change as well.
",5 Discussion,[0],[0]
"Without simulation, it would be difficult or impossible to undercover the interplay between acquisition and social structure on the propagation of language change.",5 Discussion,[0],[0]
Neither factor alone can account for the theoretical or empirically observed patterns.,5 Discussion,[0],[0]
Simulations of this kind which explicitly model both simultaneously is well equipped to provide insights that fieldwork and laboratory work cannot.,5 Discussion,[0],[0]
"As such, it is an invaluable complement to those more traditional methodologies.",5 Discussion,[0],[0]
We thank Charles Yang for is input and audiences at FWAV 4 and DiGS 19 for comments on earlier versions of this work.,Acknowledgments,[0],[0]
This research was funded by an NDSEG fellowship awarded to the first author by the US Dept. of Defense.,Acknowledgments,[0],[0]
Language variation and change are driven both by individuals’ internal cognitive processes and by the social structures through which language propagates.,abstractText,[0],[0]
A wide range of computational frameworks have been proposed to connect these drivers.,abstractText,[0],[0]
We compare the strengths and weaknesses of existing approaches and propose a new analytic framework which combines previous network models’ ability to capture realistic social structure with practically and more elegant computational properties.,abstractText,[0],[0]
The framework privileges the process of language acquisition and embeds learners in a social network but is modular so that population structure can be combined with different acquisition models.,abstractText,[0],[0]
We demonstrate two applications for the framework: a test of practical concerns that arise when modeling acquisition in a population setting and an application of the framework to recent work on phonological mergers in progress.,abstractText,[0],[0]
A Framework for Representing Language Acquisition in a Population Setting,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2864–2870 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
2864",text,[0],[0]
"While word embedding has proven a good solution to reduce data sparsity in parsing (Koo et al., 2008), treating word forms as atomic units is at odds with the fact that words have a potentially complex internal structure.",1 Introduction,[0],[0]
"Furthermore, it makes parameters estimation difficult for morphologically rich languages (MRL) in which the number of possible forms a word can take can be very large1.
",1 Introduction,[0],[0]
"Recently, researchers have started to work on morphologically informed word embeddings (Cao and Rei, 2016; Botha and Blunsom, 2014), aiming at better capturing both lexical, syntactic and morphological information.",1 Introduction,[0],[0]
"But encoding lexicon and morphology in the same space makes it difficult to distinguish the role of each in syntactic tasks such
1A typical English noun has 2 forms while a Finnish one may have more than 30.",1 Introduction,[0],[0]
"This shows in data as English lemmas have 1.39 forms on average while Finnish ones have 2.19, as measured on UD data (Nivre et al., 2016).
",1 Introduction,[0],[0]
as dependency parsing.,1 Introduction,[0],[0]
"Furthermore, morphologically rich languages for which we hope to see a real impact from those morphologically aware representations, might not all rely to the same extent on morphology for syntax encoding.",1 Introduction,[0],[0]
"Some might benefit mostly from reducing data sparsity while others, for which paradigm richness correlate with freer word order (Comrie, 1981), will also benefit from morphological information encoding.
",1 Introduction,[0],[0]
This paper aims at characterizing the role of morphology as a syntax encoding device for various languages.,1 Introduction,[0],[0]
"Using simple word representations, we measure the impact of morphological information on dependency parsing and relate it to two measures of language morphological complexity: the basic form per lemma ratio and a new measure (HPE) defined in terms of head attachment preference encoded by its morphological attributes.",1 Introduction,[0],[0]
"We show that this new measure is predictive of parsing result differences observed when using different word representations and that it allows one to distinguish amongst morphologically rich languages, those that use morphology for syntactic purpose from those using morphology as a more semantic marker.",1 Introduction,[0],[0]
"To the best of our knowledge, this work is the first attempt at systematically measuring the syntactic content of morphology in a multi-lingual environment.
",1 Introduction,[0],[0]
Section 2 presents the representation learning method and the dependency parsing model.,1 Introduction,[0],[0]
It also defines two measures of morphological complexity.,1 Introduction,[0],[0]
Section 3 describes the experimental setting and analyses parsing results in terms of the previously defined morphological complexity measures.,1 Introduction,[0],[0]
Section 4 gives some conclusions and future work perspectives.,1 Introduction,[0],[0]
"This section details: (i) our method for learning lexical and morphological representations, (ii) how these can be used for graph-based dependency parsing, and (iii) how to measure morphological complexity.",2 Framework,[0],[0]
Our representation learning and parsing techniques are purposely very simple in order to let us separate lexical and morphological information and weight the role of morphology in dependency parsing of MRL.,2 Framework,[0],[0]
"We construct separate vectorial representations for lemmas, forms and morphological attributes, either learned via dimension reduction of their own cooccurrence count matrices or represented as raw one-hot vectors.
LetV be a vocabulary (it can be lemmas or forms or morphological attributes (incl. values for POS, number, case, tense, mood...)) for a given language.",2.1 Word Representation,[0],[0]
"Correspondingly, let C be the set of contexts defined over elements of V .",2.1 Word Representation,[0],[0]
"That is, lemmas appear in the context of other lemmas, forms in the context of forms, and attributes in the context of attributes.",2.1 Word Representation,[0],[0]
"Then, given a corpus annotated with lemmas and morphological information, we can gather the cooccurrence counts in the matrix M ∈ N|V|×|C|, such that M ij is the frequency of lemma (form or morphological attributes)",2.1 Word Representation,[0],[0]
Vi appearing in context Cj in the corpus.,2.1 Word Representation,[0],[0]
"Here, we consider plain sequential contexts (i.e. surrounding bag of “words”) of length 1, although we could extend them to more structured contexts (Bansal et al., 2014).",2.1 Word Representation,[0],[0]
Those cooccurrence matrices are then reweighted by unshifted Positive Point-wise Mutual Information (PPMI) and reduced via Singular Value Decomposition (SVD).,2.1 Word Representation,[0],[0]
"For more information on word embedding via matrix factorization, please refer to (Levy et al., 2015).
",2.1 Word Representation,[0],[0]
"Despite its apparent simplicity, this model is as expressive as more popular state of the art embedding techniques.",2.1 Word Representation,[0],[0]
"Indeed, Goldberg and Levy (2014) have shown that the SkipGram objective with negative sampling of Mikolov’s Word2vec (2013) can be framed as the factorization of a shifted PMI weighted cooccurrence matrix.
",2.1 Word Representation,[0],[0]
"This matrix reduction procedure gives us vectors for lemmas, forms and morphological attributes, noted R. Note that while a word has only one lemma and one form, it will often realize several morphological attributes.",2.1 Word Representation,[0],[0]
"We tackle this issue by
simply summing over all the attributes of a word (noted Morph(w)).",2.1 Word Representation,[0],[0]
"If we note rw the vectorial representation of word w we have:
rw = ∑
a∈Morph(w)
Ra.
",2.1 Word Representation,[0],[0]
"Simple additive models have been shown to be very efficient for compositionally derived embeddings (Arora et al., 2017).",2.1 Word Representation,[0],[0]
"We work with graph-based dependency parsing, which offers very competitive parsing models as recently re-emphasized by Dozat et al. (2017) in the CONLL 2017 shared-task on dependency parsing (Zeman et al., 2017).
",2.2 Dependency Parsing,[0],[0]
"Let x = (w1, w2, ..., wn) be a sentence, Tx be the set of all possible trees over it, ŷ",2.2 Dependency Parsing,[0],[0]
"the tree that we predict for x, and Score(•, •) a scoring function over sentence-tree pairs :
ŷ = argmax t∈Tx Score(x, t).
",2.2 Dependency Parsing,[0],[0]
We use edge factorization to make the inference problem tractable.,2.2 Dependency Parsing,[0],[0]
A tree score is thus the sum of its edges scores.,2.2 Dependency Parsing,[0],[0]
"We use a simple linear model:
Score(x, t) = ∑ e∈t θ> · φ(x, e),
where φ(x, e) is a feature vector representing edge e in sentence x, and θ ∈ Rm is a parameter vector to be learned.
",2.2 Dependency Parsing,[0],[0]
"The vector representation of an edge eij whose governor is the i-th word wi and dependent is the j-th word wj , is defined by the outer product of their respective representations in context.",2.2 Dependency Parsing,[0],[0]
"Let ⊕ note vector concatenation, ⊗ the outer product and wk±1 be the word just before/after wk, then: vi = wi−1⊕wi⊕wi+1, vj = wj−1⊕wj ⊕wj+1 and
φ(x, eij) = vec(vi ⊗ vj) ∈",2.2 Dependency Parsing,[0],[0]
"R9d 2 .
",2.2 Dependency Parsing,[0],[0]
"Recall that wi of length d V is a vector from R. We use the averaged Passive-Aggressive online algorithm for structured prediction (Crammer et al., 2006) for learning the model θ.",2.2 Dependency Parsing,[0],[0]
"Given a score for each edge, we use Eisner algorithm (Eisner, 1996) to retrieve the best projective spanning tree.",2.2 Dependency Parsing,[0],[0]
"Even though some languages display a fair amount of non-projective edges, on average Eisner algorithm scores higher than Chu-Liu-Edmonds algorithm (Chu and Liu, 1965) in our setting.",2.2 Dependency Parsing,[0],[0]
Some languages use morphological cues to encode syntactic information while other encode more semantic information with them.,2.3 Measuring Morpho-Syntactic Complexity,[0],[0]
"For example, the Case feature (especially core cases) is of prime syntactic importance, for it encodes the type of relation words have with each other.",2.3 Measuring Morpho-Syntactic Complexity,[0],[0]
"On the contrary, the Possessor feature (in Hungarian for example) is more semantic in nature and need not impact sentence structure.",2.3 Measuring Morpho-Syntactic Complexity,[0],[0]
This remark would support different treatment for each language.,2.3 Measuring Morpho-Syntactic Complexity,[0],[0]
"However, those languages tend to be treated equally in works dealing with MRL.
",2.3 Measuring Morpho-Syntactic Complexity,[0],[0]
Form to Lemma Ratio,2.3 Measuring Morpho-Syntactic Complexity,[0],[0]
"A basic measure of morphological complexity is the form per lemma ratio, we note it F/L.",2.3 Measuring Morpho-Syntactic Complexity,[0],[0]
It captures the tendency of words to inflect in a given language.,2.3 Measuring Morpho-Syntactic Complexity,[0],[0]
"Because some word classes tend not to inflect and not all forms are equally productive, we note F/iL the ratio of form per inflected lemma.",2.3 Measuring Morpho-Syntactic Complexity,[0],[0]
"Given a language l with a lemma vocabulary V l and a form counting function c : V l → N that returns the number of forms a lemma can take, we have:
F/L(l)",2.3 Measuring Morpho-Syntactic Complexity,[0],[0]
"= 1 |V l| ∑ w∈Vl c(w),
F/iL(l)",2.3 Measuring Morpho-Syntactic Complexity,[0],[0]
"= 1 |V li | ∑ w∈Vli c(w),V li = {w ∈ V l|c(w) >",2.3 Measuring Morpho-Syntactic Complexity,[0],[0]
"1}
F/L and F/iL do not measure the informative content of morphology, but simply its productivity.",2.3 Measuring Morpho-Syntactic Complexity,[0],[0]
"Bentz et al. (2016) compared five different measures of morphological complexity amongst which word entropy and the micro-averaged version of F/L (they call it TTR) and showed that they all have high positive correlation given enough data.
",2.3 Measuring Morpho-Syntactic Complexity,[0],[0]
Head POS Entropy,2.3 Measuring Morpho-Syntactic Complexity,[0],[0]
"In order to compare the morpho-syntactic complexity of different languages, we introduce a new measure called Head Part-of-speech Entropy or HPE.",2.3 Measuring Morpho-Syntactic Complexity,[0],[0]
The HPE of a token t represents the amount of information t has about the part-of-speech of its governor.,2.3 Measuring Morpho-Syntactic Complexity,[0],[0]
"More formally, let POS(Gov(t)) be the set of partsof-speech that t can depend on, and let πt(p) be the probability of t actually depending on part-ofspeech p, then the HPE is defined as:
HPE(t) = ∑
p∈POS(Gov(t))
−πt(p)log2(πt(p)).
",2.3 Measuring Morpho-Syntactic Complexity,[0],[0]
This is a measure of a token preferencial attachment to its head.,2.3 Measuring Morpho-Syntactic Complexity,[0],[0]
"A token with a low HPE tends to attach often to the same part-of-speech, while a token with a high HPE will attach to many different parts-of-speech.",2.3 Measuring Morpho-Syntactic Complexity,[0],[0]
"Thus a language with a low HPE will tend to encode a lot of syntactic information in the morphology, rather than in word order say.
",2.3 Measuring Morpho-Syntactic Complexity,[0],[0]
"For example, a noun can attach to another noun like a genitive, or to a verb as a subject or object, or even to an adjective in the case of transitive adjective.",2.3 Measuring Morpho-Syntactic Complexity,[0],[0]
"French nouns do not inflect for case, thus attachment to another noun or verb can only be infered from words relative positions.",2.3 Measuring Morpho-Syntactic Complexity,[0],[0]
"On the contrary, Gothic nouns do inflect for case, thus making verb or noun attachment clear directly from the morphological analysis.
",2.3 Measuring Morpho-Syntactic Complexity,[0],[0]
We compute the HPE of a language as the averaged HPE of its attributes sets over a given corpus.,2.3 Measuring Morpho-Syntactic Complexity,[0],[0]
"Likewise, we use the empirical counts as a surrogate for c in F/L and F/iL.",2.3 Measuring Morpho-Syntactic Complexity,[0],[0]
"In order to test the hypothesis that morphological representations contain syntactic information crucial for dependency parsing of morphologically rich languages, but that this information is not equally distributed across MRL, we run experiments on data from the Universal Dependencies (Nivre et al., 2016) project.
",3 Experiments,[0],[0]
"Data Description For conciseness, we focused on eleven languages that display varying degrees of morphological complexity and belong to four different language families.",3 Experiments,[0],[0]
Basque (eu) is an isolate and it is an ergative language.,3 Experiments,[0],[0]
"English (en), Gothic (got), Danish (da) and Swedish (sv) are Germanic languages, and French (fr) and Romanian (ro) are Romance languages (Indo-European).",3 Experiments,[0],[0]
"Finnish (fi), Estonian (et) and Hungarian (hu) are Finno-Ugric languages.",3 Experiments,[0],[0]
Hebrew (he) is a Semitic language.,3 Experiments,[0],[0]
"Basic statistics are provided in Table 1.
",3 Experiments,[0],[0]
Experimental Settings For the experiments we use the train/dev/test data provided by UD 2.0.,3 Experiments,[0],[0]
Basic statistics about the data are reported in the appendix.,3 Experiments,[0],[0]
"Lemmas and forms are embedded in 150 dimensions, while Morphological attributes are embedded in 50 dimensions, because they are much less numerous (less than 100).",3 Experiments,[0],[0]
"All embeddings are induced on their language respective train set only using a context window of size 1 (i.e. the
directly preceding and following words).",3 Experiments,[0],[0]
"Parsers are trained for 10 iterations using either lemma, form or morphological representations, and we pick the best iteration on the basis of UAS on the development set.
",3 Experiments,[0],[0]
"While we used gold lemmas as provided in the corpora, we ran two experiments for morphological attributes, one with gold attributes and one with predicted attributes.",3 Experiments,[0],[0]
"Morphological attributes are predicted with a simple multinomial logistic regression per attribute (POS, Tense, Case, Gender...), where we add a special undef value (except for POS) to represent the lack of an attribute (e.g., nouns have no Tense in English).",3 Experiments,[0],[0]
"The models predict attribute values for the center word of trigrams represented by feature vectors encoding word prefixes and suffixes of length 1, 2 and 3, word length and capitalization.",3 Experiments,[0],[0]
"We used the logistic regression implemented in the Scikit-Learn (Pedregosa et al., 2011) library with the default settings.",3 Experiments,[0],[0]
"It can output an argmaxed decision or a softmaxed decision, thus we tried both as input to the parser.",3 Experiments,[0],[0]
"The argmaxed decision gives a vector of zeros and ones, while the softmaxed decision gives a continuous vector with each each attributes summing to one (the probability assigned to each possible value for Gender like Masculine, Feminine,
Neuter and Undef must sum to one).",3 Experiments,[0],[0]
"Then those vectors are used unchanged for the one-hot representation or passed through an embedding matrix for the embedding representation.
",3 Experiments,[0],[0]
"Results For clarity, we focus on comparing results using form embeddings and gold morphological representations.",3 Experiments,[0],[0]
They are given in Table 2.,3 Experiments,[0],[0]
"Because the analysis carries to the labeled case, we stick to unlabeled scores (UAS) for the analysis.",3 Experiments,[0],[0]
A more complete table is provided in the appendix as well as a complete labeled accuracy score (LAS) table.,3 Experiments,[0],[0]
"Morphological complexity measures are also reported.
",3 Experiments,[0],[0]
One-hot gold morphological attributes consistently outerperform form embeddings.,3 Experiments,[0],[0]
This is expected since forms embedding were trained on much fewer data than usually considered necessary.,3 Experiments,[0],[0]
"However, improvements are not consistent across languages, ranging from 1.14 point for English to 15.20 points for Finnish.",3 Experiments,[0],[0]
"While those differences are not explained by morphological productivity alone (Figure 1a), a measure of preferential attachment gives a good account of them (Figure 1b).",3 Experiments,[0],[0]
"Those inconsistencies become even more striking, considering results using predicted attributes.",3 Experiments,[0],[0]
"We notice that despite a general drop of performance of 5-12 points, predicted attributes
3 3.5 4 4.5
−5
−2.5
0
2.5
5
7.5
10
12.5
15
da en
et
eu
fi
fr
got
he
hu
rosv
(a) F/iL
0.5 0.75 1
−5
−2.5
0
2.5
5
7.5
10
12.5
15
da en
eu
fi
fr
he
hu
ro sv
etgot
(b) HPE
Figure 1: Accuracy differences (y-axis) between parsers using form embeddings and parsers using onehot attributes, with respect to morphological complexity (x-axis).",3 Experiments,[0],[0]
"Red dots represent the gold attributes scores and blue squares the predicted attributes scores.
still perform significantly better than form embeddings for those morphologically rich languages that have an HPE lower than 0.65 as depicted on Figure 1b.
",3 Experiments,[0],[0]
Figures 1a and 1b plot the differences in parsing scores.,3 Experiments,[0],[0]
"For each language, the red dot corresponds to the score difference between using form embeddings and gold attributes one-hot representations, and the blue square corresponds to the score difference between using the same form embeddings and predicted attributes softmax representations (the complete scores are given in the appendix).",3 Experiments,[0],[0]
Figure 1a plots those differences with regard to the form per inflected lemma ratio (F/iL) and,3 Experiments,[0],[0]
"Figure 1b plots those differences with regard to the head POS entropy (HPE).
",3 Experiments,[0],[0]
Both Figures show trends.,3 Experiments,[0],[0]
Score differences seem to increase with F/iL and decrease with HPE.,3 Experiments,[0],[0]
"But while the F/iL plot suffers outliers (Hungarian, Estonian and Romanian), the HPE plot shows a clear boundary between languages benefiting fully from morphological information (even predicted) and those benefiting primarily from reducing data sparsity.",3 Experiments,[0],[0]
"While Hebrew seems to be an outlier, it might be due to its annotation style, where attached prepositions, articles and possessive markers are treated as independent words rather than morphological inflection as other languages do, thus artificially increasing the parsing accuracy with a lot of trivial dependencies.
",3 Experiments,[0],[0]
"This shows that indeed, HPE is a good measure
of the syntactic informativeness of a language morphology, and that it can help deciding between encoding morphological information or just reducing data sparsity.",3 Experiments,[0],[0]
"Furthermore, it seems to be link to the distinction that Kibort and Corbett (2010) do between morphosyntax and morphosemantic.",3 Experiments,[0],[0]
We have contributed a new measure of morphosyntactic complexity (HPE) that helps distinguishing languages that use morphology for syntactic purpose from languages that use morphology to encode more semantic information.,4 Conclusion,[0],[0]
We showed that this measure correlates much more with differences in parsing results using morphological representations than the simple form per lemma ratio.,4 Conclusion,[0],[0]
"It could thus be used to help designing language specific word representations.
",4 Conclusion,[0],[0]
It is worth mentioning that we focused here on dependent marked head selection.,4 Conclusion,[0],[0]
It would be interesting to have a similar measure for headmarking situations with dependencies marked on the governor.,4 Conclusion,[0],[0]
We leave it for future work.,4 Conclusion,[0],[0]
This work was supported by ANR Grant GRASP,5 Acknowledgement,[0],[0]
No.,5 Acknowledgement,[0],[0]
ANR-16-CE33-0011-01 and Grant from CPER Nord-Pas de Calais/FEDER DATA Advanced data science and technologies 2015-2020.,5 Acknowledgement,[0],[0]
We also thank the reviewers for their valuable feedback.,5 Acknowledgement,[0],[0]
Table 3 reports results for the predicted attributes experiment.,Appendix A: Supplementary Tables,[0],[0]
The POS and averaged attributes prediction accuracies are given.,Appendix A: Supplementary Tables,[0],[0]
"Are also reported, scores for the four representation regimes of predicted attributes.",Appendix A: Supplementary Tables,[0],[0]
"Predictions can be either probability distributions (Soft) or argmax (Hard) and either used as such (OH) or passed through an embedding (Emb).
",Appendix A: Supplementary Tables,[0],[0]
"Table 4 reports all the labeled accuracy scores
for parsers using either gold lemmas, forms or gold attributes, either as one-hot vectors or as dense embeddings.
",Appendix A: Supplementary Tables,[0],[0]
Table 5 reports results for the predicted attributes experiment.,Appendix A: Supplementary Tables,[0],[0]
"Are also reported, scores for the four representation regimes of predicted attributes as in table 4.",Appendix A: Supplementary Tables,[0],[0]
Predictions can be either probability distributions (Soft) or argmax (Hard) and either used as such (OH) or passed through an embedding (Emb).,Appendix A: Supplementary Tables,[0],[0]
This paper presents a simple framework for characterizing morphological complexity and how it encodes syntactic information.,abstractText,[0],[0]
"In particular, we propose a new measure of morphosyntactic complexity in terms of governordependent preferential attachment that explains parsing performance.",abstractText,[0],[0]
"Through experiments on dependency parsing with data from Universal Dependencies (UD), we show that representations derived from morphological attributes deliver important parsing performance improvements over standard word form embeddings when trained on the same datasets.",abstractText,[0],[0]
"We also show that the new morphosyntactic complexity measure is predictive of the gains provided by using morphological attributes over plain forms on parsing scores, making it a tool to distinguish languages using morphology as a syntactic marker from others.",abstractText,[0],[0]
A Framework for Understanding the Role of Morphology in Universal Dependency Parsing,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 288–298 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1027",text,[0],[0]
Greedy transition-based dependency parsers are widely used in different NLP tasks due to their speed and efficiency.,1 Introduction,[0],[0]
They parse a sentence from left to right by greedily choosing the highestscoring transition to go from the current parser configuration or state to the next.,1 Introduction,[0],[0]
The resulting sequence of transitions incrementally builds a parse for the input sentence.,1 Introduction,[0],[0]
"The scoring of the transitions is provided by a statistical model, previously trained to approximate an oracle, a function that selects the needed transitions to parse a gold tree.
",1 Introduction,[0],[0]
"Unfortunately, the greedy nature that grants these parsers their efficiency also represents their main limitation.",1 Introduction,[0],[0]
"McDonald and Nivre (2007) show that greedy transition-based parsers lose accuracy to error propagation: a transition erroneously chosen by the greedy parser can place it
in an incorrect and unknown configuration, causing more mistakes in the rest of the transition sequence.",1 Introduction,[0],[0]
"Training with a dynamic oracle (Goldberg and Nivre, 2012) improves robustness in these situations, but in a monotonic transition system, erroneous decisions made in the past are permanent, even when the availability of further information in later states might be useful to correct them.
",1 Introduction,[0],[0]
"Honnibal et al. (2013) show that allowing some degree of non-monotonicity, by using a limited set of non-monotonic actions that can repair past mistakes and replace previously-built arcs, can increase the accuracy of a transition-based parser.",1 Introduction,[0],[0]
"In particular, they present a modified arc-eager transition system where the Left-Arc and Reduce transitions are non-monotonic: the former is used to repair invalid attachments made in previous states by replacing them with a leftward arc, and the latter allows the parser to link two words with a rightward arc that were previously left unattached due to an erroneous decision.",1 Introduction,[0],[0]
"Since the Right-Arc transition is still monotonic and leftward arcs can never be repaired because their dependent is removed from the stack by the arc-eager parser and rendered inaccessible, this approach can only repair certain kinds of mistakes: namely, it can fix erroneous rightward arcs by replacing them with a leftward arc, and connect a limited set of unattached words with rightward arcs.",1 Introduction,[0],[0]
"In addition, they argue that non-monotonicity in the training oracle can be harmful for the final accuracy and, therefore, they suggest to apply it only as a fallback component for a monotonic oracle, which is given priority over the non-monotonic one.",1 Introduction,[0],[0]
"Thus, this strategy will follow the path dictated by the monotonic oracle the majority of the time.",1 Introduction,[0],[0]
Honnibal and Johnson (2015) present an extension of this transition system with an Unshift transition allowing it some extra flexibility to correct past errors.,1 Introduction,[0],[0]
"However, the restriction that only rightward
288
arcs can be deleted, and only by replacing them with leftward arcs, is still in place.",1 Introduction,[0],[0]
"Furthermore, both versions of the algorithm are limited to projective trees.
",1 Introduction,[0],[0]
"In this paper, we propose a non-monotonic transition system based on the non-projective Covington parser, together with a dynamic oracle to train it with erroneous examples that will need to be repaired.",1 Introduction,[0],[0]
"Unlike the system developed in (Honnibal et al., 2013; Honnibal and Johnson, 2015), we work with full non-monotonicity.",1 Introduction,[0],[0]
"This has a twofold meaning: (1) our approach can repair previous erroneous attachments regardless of their original direction, and it can replace them either with a rightward or leftward arc as both arc transitions are non-monotonic;1 and (2) we use exclusively a non-monotonic oracle, without the interferences of monotonic decisions.",1 Introduction,[0],[0]
"These modifications are feasible because the non-projective Covington transition system is less rigid than the arc-eager algorithm, as words are never deleted from the parser’s data structures and can always be revisited, making it a better option to exploit the full potencial of non-monotonicity.",1 Introduction,[0],[0]
"To our knowledge, the presented system is the first nonmonotonic parser that can produce non-projective dependency analyses.",1 Introduction,[0],[0]
"Another novel aspect is that our dynamic oracle is approximate, i.e., based on efficiently-computable approximations of the loss due to the complexity of calculating its actual value in a non-monotonic and non-projective scenario.",1 Introduction,[0],[0]
"However, this is not a problem in practice: experimental results show how our parser and oracle can use non-monotonic actions to repair erroneous attachments, outperforming the monotonic version developed by Gómez-Rodrı́guez and Fernández-González (2015) in a large majority of the datasets tested.",1 Introduction,[0],[0]
"The non-projective Covington parser was originally defined by Covington (2001), and then recast by Nivre (2008) under the transition-based parsing framework.
",2.1 Non-Projective Covington Transition System,[0],[0]
1The only restriction is that parsing must still proceed in left-to-right order.,2.1 Non-Projective Covington Transition System,[0],[0]
"For this reason, a leftward arc cannot be repaired with a rightward arc, because this would imply going back in the sentence.",2.1 Non-Projective Covington Transition System,[0],[0]
"The other three combinations (replacing leftward with leftward, rightward with leftward or rightward with rightward arcs) are possible.
",2.1 Non-Projective Covington Transition System,[0],[0]
"The transition system that defines this parser is as follows: each parser configuration is of the form c = 〈λ1, λ2, B,A〉, such that λ1 and λ2 are lists of partially processed words, B is another list (called the buffer) containing currently unprocessed words, and A is the set of dependencies that have been built so far.",2.1 Non-Projective Covington Transition System,[0],[0]
"Suppose that our input is a string w1 · · ·wn, whose word occurrences will be identified with their indices 1 · · ·n for simplicity.",2.1 Non-Projective Covington Transition System,[0],[0]
"Then, the parser will start at an initial configuration cs(w1 . . .",2.1 Non-Projective Covington Transition System,[0],[0]
wn) = 〈,2.1 Non-Projective Covington Transition System,[0],[0]
"[], [], [1 . . .",2.1 Non-Projective Covington Transition System,[0],[0]
n,2.1 Non-Projective Covington Transition System,[0],[0]
"], ∅〉, and execute transitions chosen from those in Figure 1 until a terminal configuration of the form {〈λ1, λ2,",2.1 Non-Projective Covington Transition System,[0],[0]
"[], A〉 ∈ C} is reached.",2.1 Non-Projective Covington Transition System,[0],[0]
"At that point, the sentence’s parse tree is obtained from A.2
These transitions implement the same logic as the double nested loop traversing word pairs in the original formulation by Covington (2001).",2.1 Non-Projective Covington Transition System,[0],[0]
"When the parser’s configuration is 〈λ1|i, λ2, j|B,A〉, we say that it is considering the focus words i and j, located at the end of the first list and at the beginning of the buffer.",2.1 Non-Projective Covington Transition System,[0],[0]
"At that point, the parser must decide whether these two words should be linked with a leftward arc",2.1 Non-Projective Covington Transition System,[0],[0]
i ← j,2.1 Non-Projective Covington Transition System,[0],[0]
"(Left-Arc transition), a rightward arc i → j (Right-Arc transition), or not linked at all (No-Arc transition).",2.1 Non-Projective Covington Transition System,[0],[0]
"However, the two transitions that create arcs will be disallowed in configurations where this would cause a violation of the single-head constraint (a node can have at most one incoming arc) or the acyclicity constraint (the dependency graph cannot have cycles).",2.1 Non-Projective Covington Transition System,[0],[0]
"After applying any of these three transitions, i is moved to the second list to make i − 1 and j the focus words for the next step.",2.1 Non-Projective Covington Transition System,[0],[0]
"As an alternative, we can instead choose to execute a Shift transition which lets the parser read a new input word, placing the focus on j and j + 1.
",2.1 Non-Projective Covington Transition System,[0],[0]
"The resulting parser can generate any possible dependency tree for the input, including arbitrary non-projective trees.",2.1 Non-Projective Covington Transition System,[0],[0]
"While it runs in quadratic worst-case time, in theory worse than lineartime transition-based parsers (e.g. (Nivre, 2003; Gómez-Rodrı́guez and Nivre, 2013)), it has been shown to outspeed linear algorithms in practice, thanks to feature extraction optimizations that cannot be implemented in other parsers (Volokh and Neumann, 2012).",2.1 Non-Projective Covington Transition System,[0],[0]
"In fact, one of the fastest dependency parsers ever reported uses this algorithm
2In general A is a forest, but it can be converted to a tree by linking headless nodes as dependents of an artificial root node at position 0.",2.1 Non-Projective Covington Transition System,[0],[0]
"When we refer to parser outputs as trees, we assume that this transformation is being implicitly made.
",2.1 Non-Projective Covington Transition System,[0],[0]
"(Volokh, 2013).",2.1 Non-Projective Covington Transition System,[0],[0]
A dynamic oracle is a function that maps a configuration c and a gold tree tG to the set of transitions that can be applied in c and lead to some parse tree t minimizing the Hamming loss with respect to tG (the amount of nodes whose head is different in t and tG).,2.2 Monotonic Dynamic Oracle,[0],[0]
"Following Goldberg and Nivre (2013), we say that an arc set A is reachable from configuration c, and we write c A, if there is some (possibly empty) path of transitions from c to some configuration c′ = 〈λ1, λ2, B,A′〉, with A ⊆ A′.",2.2 Monotonic Dynamic Oracle,[0],[0]
"Then, we can define the loss of configuration c as
`(c) = min t|c t
L(t, tG),
and therefore, a correct dynamic oracle will return the set of transitions
od(c, tG)",2.2 Monotonic Dynamic Oracle,[0],[0]
= {τ | `(c)− `(τ(c)),2.2 Monotonic Dynamic Oracle,[0],[0]
"= 0},
i.e., the set of transitions that do not increase configuration loss, and thus lead to the best parse (in terms of loss) reachable from c. Hence, implementing a dynamic oracle reduces to computing the loss `(c) for each configuration c.
Goldberg and Nivre (2013) show a straightforward method to calculate loss for parsers that are arc-decomposable, i.e., those where every arc set A that can be part of a well-formed parse verifies that if c (i → j) for every i → j ∈",2.2 Monotonic Dynamic Oracle,[0],[0]
"A (i.e., each of the individual arcs of A is reachable from a given configuration c), then c A (i.e., the set A as a whole is reachable from c).",2.2 Monotonic Dynamic Oracle,[0],[0]
"If this holds, then the loss of a configuration c equals the number of gold arcs that are not individually reachable from c, which is easy to compute in most parsers.
Gómez-Rodrı́guez and Fernández-González (2015) show that the non-projective Covington parser is not arc-decomposable because sets of individually reachable arcs may form cycles together with already-built arcs, preventing them
from being jointly reachable due to the acyclicity constraint.",2.2 Monotonic Dynamic Oracle,[0],[0]
"In spite of this, they prove that a dynamic oracle for the Covington parser can be efficiently built by counting individually unreachable arcs, and correcting for the presence of such cycles.",2.2 Monotonic Dynamic Oracle,[0],[0]
"Concretely, the loss is computed as:
`(c) = |U(c, tG)|+ nc(A ∪ I(c, tG))
",2.2 Monotonic Dynamic Oracle,[0],[0]
"where I(c, tG)",2.2 Monotonic Dynamic Oracle,[0],[0]
= {x → y ∈ tG,2.2 Monotonic Dynamic Oracle,[0],[0]
"| c (x → y)} is the set of individually reachable arcs of tG from configuration c; U(c, tG) is the set of individually unreachable arcs of tG from c, computed as tG\I(c, tG); and nc(G) denotes the number of cycles in a graph G.
Therefore, to calculate the loss of a configuration c, we only need to compute the two terms |U(c, tG)| and nc(A ∪ I(c, tG)).",2.2 Monotonic Dynamic Oracle,[0],[0]
"To calculate the first term, given a configuration cwith focus words i and j (i.e., c = 〈λ1|i, λ2, j|B,A〉), an arc x→ y will be in U(c, tG)",2.2 Monotonic Dynamic Oracle,[0],[0]
"if it is not in A, and at least one of the following holds:
• j > max(x, y), (i.e., we have read too far in the string and can no longer get max(x, y) as right focus word), • j = max(x, y)",2.2 Monotonic Dynamic Oracle,[0],[0]
∧,2.2 Monotonic Dynamic Oracle,[0],[0]
"i < min(x, y), (i.e., we
have max(x, y) as the right focus word but the left focus word has already moved left past min(x, y), and we cannot go back), • there is some z 6= 0, z",2.2 Monotonic Dynamic Oracle,[0],[0]
6= x such that z → y ∈,2.2 Monotonic Dynamic Oracle,[0],[0]
"A, (i.e., we cannot create x→ y because it would violate the single-head constraint), • x and y are on the same weakly connected
component of A (i.e., we cannot create x → y due to the acyclicity constraint).
",2.2 Monotonic Dynamic Oracle,[0],[0]
"The second term of the loss, nc(A ∪ I(c, tG)), can be computed by first obtaining I(c, tG) as tG \",2.2 Monotonic Dynamic Oracle,[0],[0]
"U(c, tG)",2.2 Monotonic Dynamic Oracle,[0],[0]
.,2.2 Monotonic Dynamic Oracle,[0],[0]
"Since the graph I(c, tG) has indegree 1, the algorithm by Tarjan (1972) can then be used to find and count the cycles in O(n) time.
",2.2 Monotonic Dynamic Oracle,[0],[0]
Algorithm 1 Computation of the loss of a configuration in the monotonic oracle.,2.2 Monotonic Dynamic Oracle,[0],[0]
"1: function LOSS(c = 〈λ1|i, λ2, j|B,A〉, tG) 2:",2.2 Monotonic Dynamic Oracle,[0],[0]
U ← ∅ .,2.2 Monotonic Dynamic Oracle,[0],[0]
"Variable U is for U(c, tG) 3: for each x→ y ∈ (tG \A) do 4: left ← min(x, y) 5: right ← max(x, y) 6: if j >",2.2 Monotonic Dynamic Oracle,[0],[0]
right ∨ 7: (j = right ∧ i <,2.2 Monotonic Dynamic Oracle,[0],[0]
"left)∨ 8: (∃z > 0, z",2.2 Monotonic Dynamic Oracle,[0],[0]
6= x,2.2 Monotonic Dynamic Oracle,[0],[0]
": z → y ∈ A)∨ 9: WEAKLYCONNECTED(A, x, y) then 10: U ← u ∪ {x→ y} 11: I ← tG",2.2 Monotonic Dynamic Oracle,[0],[0]
\U .,2.2 Monotonic Dynamic Oracle,[0],[0]
"Variable I is for I(c, tG) 12: return |U |+ COUNTCYCLES(A ∪ I )
",2.2 Monotonic Dynamic Oracle,[0],[0]
"Algorithm 1 shows the resulting loss calculation algorithm, where COUNTCYCLES is a function that counts the number of cycles in the given graph and WEAKLYCONNECTED returns whether two given nodes are weakly connected in A.",2.2 Monotonic Dynamic Oracle,[0],[0]
We now define a non-monotonic variant of the Covington non-projective parser.,3 Non-Monotonic Transition System for the Covington Non-Projective Parser,[0],[0]
"To do so, we allow the Right-Arc and Left-Arc transitions to create arcs between any pair of nodes without restriction.",3 Non-Monotonic Transition System for the Covington Non-Projective Parser,[0],[0]
"If the node attached as dependent already had a previous head, the existing attachment is discarded in favor of the new one.",3 Non-Monotonic Transition System for the Covington Non-Projective Parser,[0],[0]
"This allows the parser to correct erroneous attachments made in the past by assigning new heads, while still enforcing the single-head constraint, as only the most recent head assigned to each node is kept.
",3 Non-Monotonic Transition System for the Covington Non-Projective Parser,[0],[0]
"To enforce acyclicity, one possibility would be to keep the logic of the monotonic algorithm, forbidding the creation of arcs that would create cycles.",3 Non-Monotonic Transition System for the Covington Non-Projective Parser,[0],[0]
"However, this greatly complicates the definition of the set of individually unreachable arcs, which is needed to compute the loss bounds that will be used by the dynamic oracle.",3 Non-Monotonic Transition System for the Covington Non-Projective Parser,[0],[0]
This is because a gold arc x,3 Non-Monotonic Transition System for the Covington Non-Projective Parser,[0],[0]
"→ y may superficially seem unreachable due to forming a cycle together with arcs in A, but it might in fact be reachable if there is some transition sequence that first breaks the cycle using non-monotonic transitions to remove arcs from A, to then create x",3 Non-Monotonic Transition System for the Covington Non-Projective Parser,[0],[0]
"→ y. We do not know of a way to characterize the conditions under which such a transition sequence exists, and thus cannot estimate the loss efficiently.
",3 Non-Monotonic Transition System for the Covington Non-Projective Parser,[0],[0]
"Instead, we enforce the acyclicity constraint in a similar way to the single-head constraint:",3 Non-Monotonic Transition System for the Covington Non-Projective Parser,[0],[0]
"Right-Arc and Left-Arc transitions are always allowed, even if the prospective arc would create a
cycle in A.",3 Non-Monotonic Transition System for the Covington Non-Projective Parser,[0],[0]
"However, if the creation of a new arc x→ y generates a cycle in A, we immediately remove the arc of the form z → x from A (which trivially exists, and is unique due to the singlehead constraint).",3 Non-Monotonic Transition System for the Covington Non-Projective Parser,[0],[0]
"This not only enforces the acyclicity constraint while keeping the computation of U(c, tG) simple and efficient, but also produces a straightforward, coherent algorithm (arc transitions are always allowed, and both constraints are enforced by deleting a previous arc) and allows us to exploit non-monotonicity to the maximum (we can not only recover from assigning a node the wrong head, but also from situations where previous errors together with the acyclicity constraint prevent us from building a gold arc, keeping with the principle that later decisions override earlier ones).
",3 Non-Monotonic Transition System for the Covington Non-Projective Parser,[0],[0]
"In Figure 2, we can see the resulting nonmonotonic transition system for the non-projective Covington algorithm, where, unlike the monotonic version, all transitions are allowed at each configuration, and the single-head and acyclicity constraints are kept in A by removing offending arcs.",3 Non-Monotonic Transition System for the Covington Non-Projective Parser,[0],[0]
"To successfully train a non-monotonic system, we need a dynamic oracle with error exploration, so that the parser will be put in erroneous states and need to apply non-monotonic transitions in order to repair them.",4 Non-Monotonic Approximate Dynamic Oracle,[0],[0]
"To achieve that, we modify the dynamic oracle defined by Gómez-Rodrı́guez and Fernández-González (2015) so that it can deal with non-monotonicity.",4 Non-Monotonic Approximate Dynamic Oracle,[0],[0]
"Our modification is an approximate dynamic oracle: due to the extra flexibility added to the algorithm by non-monotonicity, we do not know of an efficient way of obtaining an exact calculation of the loss of a given configuration.",4 Non-Monotonic Approximate Dynamic Oracle,[0],[0]
"Instead, we use upper or lower bounds on the loss, which we empirically show to be very tight (less that 1% relative error with respect to the real loss) and are sufficient for the algorithm to provide better accuracy than the exact monotonic oracle.
",4 Non-Monotonic Approximate Dynamic Oracle,[0],[0]
"First of all, we adapt the computation of the set of individually unreachable arcs U(c, tG) to the new algorithm.",4 Non-Monotonic Approximate Dynamic Oracle,[0],[0]
"In particular, if c has focus words i and j (i.e., c = 〈λ1|i, λ2, j|B,A〉), then an arc x → y is in U(c, tG)",4 Non-Monotonic Approximate Dynamic Oracle,[0],[0]
"if it is not in A, and at least one of the following holds: • j > max(x, y), (i.e., we have read too far in
the string and can no longer get max(x, y) as
right focus word), •",4 Non-Monotonic Approximate Dynamic Oracle,[0],[0]
"j = max(x, y) ∧",4 Non-Monotonic Approximate Dynamic Oracle,[0],[0]
"i < min(x, y) (i.e., we
have max(x, y) as the right focus word but the left focus word has already moved left past min(x, y), and we cannot move it back).
",4 Non-Monotonic Approximate Dynamic Oracle,[0],[0]
"Note that, since the head of a node can change during the parsing process and arcs that produce cycles in A can be built, the two last conditions present in the monotonic scenario for computing U(c, tG) are not needed when we use nonmonotonicity and, as a consequence, the set of individually reachable arcs I(c, tG) is larger: due to the greater flexibility provided by nonmonotonicity, we can reach arcs that would be unreachable for the monotonic version.
",4 Non-Monotonic Approximate Dynamic Oracle,[0],[0]
"Since arcs that are in this new U(c, tG) are unreachable even by the non-monotonic parser, |U(c, tG)| is trivially a lower bound of the loss `(c).",4 Non-Monotonic Approximate Dynamic Oracle,[0],[0]
"It is worth noting that there always exists at least one transition sequence that builds every arc in I(c, tG) at some point (although not all of them necessarily appear in the final tree, due to non-monotonicity).",4 Non-Monotonic Approximate Dynamic Oracle,[0],[0]
This can be easily shown based on the fact that the non-monotonic parser does not forbid transitions at any configuration.,4 Non-Monotonic Approximate Dynamic Oracle,[0],[0]
"Thanks to this, we can can generate one such sequence by just applying the original Covington (2001) criteria (choose an arc transition whenever the focus words are linked in I(c, tG), and otherwise Shift or No-Arc depending on whether the left focus word is the first word in the sentence or not), although this sequence is not necessarily optimal in terms of loss.",4 Non-Monotonic Approximate Dynamic Oracle,[0],[0]
"In such a transition sequence, the gold arcs that are missed are (1) those in U(c, tG), and (2) those that are removed by the cycle-breaking in Left-Arc and Right-Arc transitions.",4 Non-Monotonic Approximate Dynamic Oracle,[0],[0]
"In practice configurations where (2) is needed are uncommon, so this lower bound is a very close approximation of the real loss, as will be seen empirically below.
",4 Non-Monotonic Approximate Dynamic Oracle,[0],[0]
"This reasoning also helps us calculate an up-
per bound of the loss: in a transition sequence as described, if we only build the arcs in I(c, tG) and none else, the amount of arcs removed by breaking cycles (2) cannot be larger than the number of cycles in A ∪ I(c, tG).",4 Non-Monotonic Approximate Dynamic Oracle,[0],[0]
"This means that |U(c, tG)|+nc(A∪I(c, tG)) is an upper bound of the loss `(c).",4 Non-Monotonic Approximate Dynamic Oracle,[0],[0]
"Note that, contrary to the monotonic case, this expression does not always give us the exact loss, for several reasons: firstly, A∪I(c, tG) can have non-disjoint cycles (a node may have different heads in A and I since attachments are not permanent, contrary to the monotonic version) and thus removing a single arc may break more than one cycle; secondly, the removed arc can be a non-gold arc of A and therefore not incur loss; and thirdly, there may exist alternative transition sequences where a cycle in A∪I(c, tG) is broken early by non-monotonic configurations that change the head of a wrongly-attached node in A to a different (and also wrong) head,3 removing the cycle before the cycle-breaking mechanism needs to come into play without incurring in extra errors.",4 Non-Monotonic Approximate Dynamic Oracle,[0],[0]
"Characterizing the situations where such an alternative exists is the main difficulty for an exact calculation of the loss.
",4 Non-Monotonic Approximate Dynamic Oracle,[0],[0]
"However, it is possible to obtain a closer upper bound to the real loss if we consider the following: for each cycle in A ∪ I(c, tG) that will be broken by the transition sequence described above, we can determine exactly which is the arc removed by cycle-breaking (if x → y is the arc that will close the cycle according to the Covington arc-building order, then the affected arc is the one of the form z → x).",4 Non-Monotonic Approximate Dynamic Oracle,[0],[0]
The cycle can only cause the loss of a gold arc if that arc z,4 Non-Monotonic Approximate Dynamic Oracle,[0],[0]
"→ x is gold, which can be trivially checked.",4 Non-Monotonic Approximate Dynamic Oracle,[0],[0]
"Hence, if we call cycles where that holds problematic cycles, then the expression
3Note that, in this scenario, the new head must also be wrong because otherwise the newly created arc would be an arc of I(c, tG) (and therefore, would not be breaking a cycle in A ∪ I(c, tG)).",4 Non-Monotonic Approximate Dynamic Oracle,[0],[0]
"However, replacing a wrong attachment with another wrong attachment need not increase loss.
",4 Non-Monotonic Approximate Dynamic Oracle,[0],[0]
"|U(c, tG)|+npc(A∪I(c, tG))",4 Non-Monotonic Approximate Dynamic Oracle,[0],[0]
", where “pc” stands for problematic cycles, is a closer upper bound to the loss `(c) and the following holds:
|U(c, tG)| ≤",4 Non-Monotonic Approximate Dynamic Oracle,[0],[0]
"`(c) ≤ |U(c, tG)|+npc(A∪I(c, tG))
",4 Non-Monotonic Approximate Dynamic Oracle,[0],[0]
"≤ |U(c, tG)|+ nc(A ∪ I(c, tG))
",4 Non-Monotonic Approximate Dynamic Oracle,[0],[0]
"As mentioned before, unlike the monotonic approach, a node can have a different head in A than in I(c, tG) and, as a consequence, the resulting graph A ∪ I(c, tG) has maximum in-degree 2 rather than 1, and there can be overlapping cycles.",4 Non-Monotonic Approximate Dynamic Oracle,[0],[0]
"Therefore, the computation of the non-monotonic terms nc(A ∪ I(c, tG)) and npc(A ∪ I(c, tG)) requires an algorithm such as the one by Johnson (1975) to find all elementary cycles in a directed graph.",4 Non-Monotonic Approximate Dynamic Oracle,[0],[0]
This runs in O((n + e)(c,4 Non-Monotonic Approximate Dynamic Oracle,[0],[0]
"+ 1)), where n is the number of vertices, e is the number of edges and c is the number of elementary cycles in the graph.",4 Non-Monotonic Approximate Dynamic Oracle,[0],[0]
This implies that the calculation of the two non-monotonic upper bounds is less efficient than the linear loss computation in the monotonic scenario.,4 Non-Monotonic Approximate Dynamic Oracle,[0],[0]
"However, a non-monotonic algorithm that uses the lower bound as loss expression is the fastest option (even faster than the monotonic approach) as the oracle does not need to compute cycles at all, speeding up the training process.
",4 Non-Monotonic Approximate Dynamic Oracle,[0],[0]
"Algorithm 2 shows the non-monotonic variant of Algorithm 1, where COUNTRELEVANTCYCLES is a function that counts the number of cycles or problematic cycles in the given graph,
Algorithm 2 Computation of the approximate loss of a non-monotonic configuration.",4 Non-Monotonic Approximate Dynamic Oracle,[0],[0]
"1: function LOSS(c = 〈λ1|i, λ2, j|B,A〉, tG) 2: U ← ∅ .",4 Non-Monotonic Approximate Dynamic Oracle,[0],[0]
"Variable U is for U(c, tG) 3: for each x→ y ∈ (tG \A) do 4: left ← min(x, y) 5: right ← max(x, y) 6: if j >",4 Non-Monotonic Approximate Dynamic Oracle,[0],[0]
right ∨ 7: (j = right ∧ i < left) then 8: U ← u ∪ {x→ y} 9: I ← tG,4 Non-Monotonic Approximate Dynamic Oracle,[0],[0]
\U .,4 Non-Monotonic Approximate Dynamic Oracle,[0],[0]
"Variable I is for I(c, tG)",4 Non-Monotonic Approximate Dynamic Oracle,[0],[0]
"10: return |U |+ COUNTRELEVANTCYCLES(A ∪ I )
depending on the upper bound implemented, and will return 0 in case we use the lower bound.",4 Non-Monotonic Approximate Dynamic Oracle,[0],[0]
"To determine how close the lower bound |U(c, tG)| and the upper bounds |U(c, tG)| + npc(A∪I(c, tG)) and |U(c, tG)|+nc(A∪I(c, tG)) are to the actual loss in practical scenarios, we use exhaustive search to calculate the real loss of a given configuration, to then compare it with the bounds.",5 Evaluation of the Loss Bounds,[0],[0]
"This is feasible because the lower and upper bounds allow us to prune the search space: if an upper and a lower bound coincide for a configuration we already know the loss and need not keep searching, and if we can branch to two configurations such that the lower bound of one is greater or equal than an upper bound of the other, we can discard the former as it will never lead to smaller loss than the latter.",5 Evaluation of the Loss Bounds,[0],[0]
"Therefore, this ex-
haustive search with pruning guarantees to find the exact loss.
",5 Evaluation of the Loss Bounds,[0],[0]
"Due to the time complexity of this process, we undertake the analysis of only the first 100,000 transitions on each dataset of the nineteen languages available from CoNLL-X and CoNLL-XI shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007).",5 Evaluation of the Loss Bounds,[0],[0]
"In Table 1, we present the average values for the lower bound, both upper bounds and the loss, as well as the relative differences from each bound to the real loss.",5 Evaluation of the Loss Bounds,[0],[0]
"After those experiments, we conclude that the lower and the closer upper bounds are a tight approximation of the loss, with both bounds incurring relative errors below 0.8% in all datasets.",5 Evaluation of the Loss Bounds,[0],[0]
"If we compare them, the real loss is closer to the upper bound |U(c, tG)| + npc(A ∪ I(c, tG))",5 Evaluation of the Loss Bounds,[0],[0]
"in the majority of datasets (12 out of 18 languages, excluding Japanese where both bounds were exactly equal to the real loss in the whole sample of configurations).",5 Evaluation of the Loss Bounds,[0],[0]
"This means that the term npc(A∪I(c, tG)) provides a close approximation of the gold arcs missed by the presence of cycles in A. Regarding the upper bound |U(c, tG)|+nc(A∪I(c, tG)),
it presents a more variable relative error, ranging from 0.1% to 4.0%.
",5 Evaluation of the Loss Bounds,[0],[0]
"Thus, although we do not know an algorithm to obtain the exact loss which is fast enough to be practical, any of the three studied loss bounds can be used to obtain a feasible approximate dynamic oracle with full non-monotonicity.",5 Evaluation of the Loss Bounds,[0],[0]
"To prove the usefulness of our approach, we implement the static, dynamic monotonic and nonmonotonic oracles for the non-projective Covington algorithm and compare their accuracies on nine datasets4 from the CoNLL-X shared task (Buchholz and Marsi, 2006) and all datasets from the CoNLL-XI shared task (Nivre et al., 2007).",6 Experiments,[0],[0]
"For the non-monotonic algorithm, we test the three different loss expressions defined in the previous section.",6 Experiments,[0],[0]
We train an averaged perceptron model for 15 iterations and use the same feature templates for all languages5 which are listed in detail in Table 2.,6 Experiments,[0],[0]
"The accuracies obtained by the non-projective Covington parser with the three available oracles are presented in Table 3, in terms of Unlabeled (UAS) and Labeled Attachment Score (LAS).",6.1 Results,[0],[0]
"For the non-monotonic dynamic oracle, three variants are shown, one for each loss expression implemented.",6.1 Results,[0],[0]
"As we can see, the novel non-monotonic oracle improves over the accuracy of the monotonic version on 14 out of 19 languages (0.32 in UAS on average) with the best loss calculation being |U(c, tG)| + nc(A ∪ I(c, tG)), where 6 of these improvements are statistically significant at the .05 level (Yeh, 2000).",6.1 Results,[0],[0]
"The other two loss calculation methods also achieve good results, outperforming the monotonic algorithm on 12 out of 19 datasets tested.
",6.1 Results,[0],[0]
"The loss expression |U(c, tG)| + nc(A ∪ I(c, tG)) obtains greater accuracy on average than the other two loss expressions, including the more adjusted upper bound that is provably closer to the real loss.",6.1 Results,[0],[0]
"This could be explained by the fact that
4We excluded the languages from CoNLL-X that also appeared in CoNLL-XI, i.e., if a language was present in both shared tasks, we used the latest version.
",6.1 Results,[0],[0]
"5No feature optimization is performed since our priority in this paper is not to compete with state-of-the-art systems, but to prove, under uniform experimental settings, that our approach outperforms the baseline system.
identifying problematic cycles is a difficult task to learn for the parser, and for this reason a more straightforward approach, which tries to avoid all kinds of cycles (regardless of whether they will cost gold arcs or not), can perform better.",6.1 Results,[0],[0]
"This also leads us to hypothesize that, even if it were feasible to build an oracle with the exact loss, it would not provide practical improvements over these approximate oracles; as it appears difficult for a statistical model to learn the situations where replacing a wrong arc with another indirectly helps due to breaking prospective cycles.
",6.1 Results,[0],[0]
"It is also worth mentioning that the nonmonotonic dynamic oracle with the best loss expression accomplishes an average improvement over the static version (1.26 UAS) greater than that obtained by the monotonic oracle (0.98 UAS), resulting in 13 statistically significant improvements achieved by the non-monotonic variant over the static oracle in comparison to the 12 obtained by the monotonic system.",6.1 Results,[0],[0]
"Finally, note that, despite this remarkable performance, the non-monotonic version (regardless of the loss expression implemented) has an inexplicable drop in accuracy in Basque in comparison to the other two oracles.",6.1 Results,[0],[0]
"In order to provide a broader contextualization of our approach, Table 4 presents a comparison of the average accuracy and parsing speed obtained by some well-known transition-based systems with dynamic oracles.",6.2 Comparison,[0],[0]
"Concretely, we include in this comparison both monotonic (Goldberg and Nivre, 2012) and non-monotonic (Honnibal et al., 2013) versions of the arc-eager parser, as well as the original monotonic Covington system (Gómez-Rodrı́guez and Fernández-González, 2015).",6.2 Comparison,[0],[0]
The three of them were ran with our own implementation so the comparison is homogeneous.,6.2 Comparison,[0],[0]
"We also report the published accuracy of the non-projective Attardi algorithm (GómezRodrı́guez et al., 2014) on the nineteen datasets used in our experiments.",6.2 Comparison,[0],[0]
"From Table 4 we can see that our approach achieves the best average UAS score, but is slightly slower at parsing time than the monotonic Covington algorithm.",6.2 Comparison,[0],[0]
"This can be explained by the fact that the non-monotonic parser has to take into consideration the whole set of transitions at each configuration (since all are allowed), while the monotonic parser only needs to evaluate a limited set of transitions in some con-
figurations, speeding up the parsing process.",6.2 Comparison,[0],[0]
We also carry out some error analysis to provide some insights about how non-monotonicity is improving accuracy with respect to the original Covington parser.,6.3 Error Analysis,[0],[0]
"In particular, we notice that nonmonotonicity tends to be more beneficial on projective than on non-projective arcs.",6.3 Error Analysis,[0],[0]
"In addition, the non-monotonic algorithm presents a notable performance on long arcs (which are more prone to error propagation): average precision on arcs with length greater than 7 goes from 58.41% in the monotonic version to 63.19% in the non-monotonic parser, which may mean that non-monotonicity is alleviating the effect of error propagation.",6.3 Error Analysis,[0],[0]
"Finally, we study the effectiveness of non-monotonic arcs (i.e., those that break a previously-created arc), obtaining that, on average across all datasets tested, 36.86% of the arc transitions taken were non-monotonic, replacing an existing arc with a new one.",6.3 Error Analysis,[0],[0]
"Out of these transitions, 60.31% created a gold arc, and only 5.99% were harmful (i.e., they replaced a previously-built gold arc with an incorrect arc), with the remaining cases creating non-gold arcs without introducing extra errors (replacing a non-gold arc with another).",6.3 Error Analysis,[0],[0]
These results back up the usefulness of non-monotonicity in transition-based parsing.,6.3 Error Analysis,[0],[0]
"We presented a novel, fully non-monotonic variant of the well-known non-projective Covington parser, trained with a dynamic oracle.",7 Conclusion,[0],[0]
"Due to the unpredictability of a non-monotonic scenario, the real loss of each configuration cannot be computed.",7 Conclusion,[0],[0]
"To overcome this, we proposed three different loss expressions that closely bound the loss and enable us to implement a practical non-monotonic dynamic oracle.
",7 Conclusion,[0],[0]
"On average, our non-monotonic algorithm obtains better performance than the monotonic version, regardless of which of the variants of the loss calculation is used.",7 Conclusion,[0],[0]
"In particular, one of the loss expressions developed proved very promising by providing the best average accuracy, in spite of being the farthest approximation from the actual loss.",7 Conclusion,[0],[0]
"On the other hand, the proposed lower bound makes the non-monotonic oracle the fastest one among all dynamic oracles developed for the non-projective Covington algorithm.
",7 Conclusion,[0],[0]
"To our knowledge, this is the first implementation of non-monotonicity for a nonprojective parsing algorithm, and the first approximate dynamic oracle that uses close, efficientlycomputable approximations of the loss, showing this to be a feasible alternative when it is not practical to compute the actual loss.
",7 Conclusion,[0],[0]
"While we used a perceptron classifier for our experiments, our oracle could also be used in neuralnetwork implementations of greedy transitionbased parsing (Chen and Manning, 2014; Dyer et al., 2015), providing an interesting avenue for future work.",7 Conclusion,[0],[0]
"We believe that gains from both techniques should be complementary, as they apply to orthogonal components of the parsing system (the scoring model vs. the transition system), although we might see a ”diminishing returns”effect.",7 Conclusion,[0],[0]
This research has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No 714150 - FASTPARSE).,Acknowledgments,[0],[0]
The second author has received funding from the TELEPARES-UDC project (FFI2014-51978-C2-2-R) from MINECO.,Acknowledgments,[0],[0]
"Restricted non-monotonicity has been shown beneficial for the projective arceager dependency parser in previous research, as posterior decisions can repair mistakes made in previous states due to the lack of information.",abstractText,[0],[0]
"In this paper, we propose a novel, fully non-monotonic transition system based on the non-projective Covington algorithm.",abstractText,[0],[0]
"As a non-monotonic system requires exploration of erroneous actions during the training process, we develop several non-monotonic variants of the recently defined dynamic oracle for the Covington parser, based on tight approximations of the loss.",abstractText,[0],[0]
Experiments on datasets from the CoNLL-X and CoNLL-XI shared tasks show that a non-monotonic dynamic oracle outperforms the monotonic version in the majority of languages.,abstractText,[0],[0]
A Full Non-Monotonic Transition System for Unrestricted Non-Projective Parsing,title,[0],[0]
"People have systematic intuitions about which sequences of sounds would constitute likely or unlikely words in their language: Although blick is not an English word, it sounds like it could be, while bnick does not (Chomsky and Halle, 1965).",1 Introduction,[0],[0]
"Such in-
tuitions reveal that speakers are aware of the restrictions on sound sequences which can make up possible morphemes in their language—the phonotactics of the language.",1 Introduction,[0],[0]
"Phonotactic restrictions mean that each language uses only a subset of the logically, or even articulatorily, possible strings of phonemes.",1 Introduction,[0],[0]
"Admissible phoneme combinations, on the other hand, typically recur in multiple morphemes, leading to redundancy.
",1 Introduction,[0],[0]
"It is widely accepted that phonotactic judgments may be gradient: the nonsense word blick is better as a hypothetical English word than bwick, which is better than bnick (Hayes and Wilson, 2008; Albright, 2009; Daland et al., 2011).",1 Introduction,[0],[0]
"To account for such graded judgements, there have been a variety of probabilistic (or, more generally, weighted) models proposed to handle phonotactic learning and generalization over the last two decades (see Daland et al. (2011) and below for review).",1 Introduction,[0],[0]
"However, inspired by optimality-theoretic approaches to phonology, the most linguistically informed and successful such models have been constraint-based— formulating the problem of phonotactic generalization in terms of restrictions that penalize illicit combinations of sounds (e.g., ruling out ∗bn-).
",1 Introduction,[0],[0]
"In this paper, by contrast, we adopt a generative approach to modeling phonotactic structure.",1 Introduction,[0],[0]
"Our approach harkens back to early work on the sound structure of lexical items which made use of morpheme structure rules or conditions (Halle, 1959; Stanley, 1967; Booij, 2011; Rasin and Katzir, 2014).",1 Introduction,[0],[0]
"Such approaches explicitly attempted to model the
73
Transactions of the Association for Computational Linguistics, vol. 5, pp.",1 Introduction,[0],[0]
"73–86, 2017.",1 Introduction,[0],[0]
Action Editor: Eric Fosler-Lussier.,1 Introduction,[0],[0]
"Submission batch: 8/2016; Revision batch: 11/2016; Published 2/2017.
",1 Introduction,[0],[0]
c©2017 Association for Computational Linguistics.,1 Introduction,[0],[0]
"Distributed under a CC-BY 4.0 license.
redudancy within the set of allowable lexical forms in a language.",1 Introduction,[0],[0]
"We adopt a probabilistic version of this idea, conceiving of the phonotactic system as the component of the linguistic system which generates the phonological form of lexical items such as words and morphemes.1 Our system learns inventories of reusable phonotactically licit structures from existing lexical items, and assembles new lexical items by combining these learned phonotactic patterns using phonologically plausible structurebuilding operations.",1 Introduction,[0],[0]
"Thus, instead of modeling phonotactic generalizations in terms of constraints, we treat the problem as a problem of learning language specific inventories of phonological units and language specific biases on how these phones are likely to be combined.
",1 Introduction,[0],[0]
Although there have been a number of earlier generative models of phonotactic structure (see Section 4) these models have mostly used relatively simplistic or phonologically implausible representations of phones and phonological structure-building.,1 Introduction,[0],[0]
"By contrast, our model is built around three representational assumptions inspired by the generative phonology literature.",1 Introduction,[0],[0]
"First, we capture sparsity in the space of feature-specifications of phonemes by using feature dependency graphs—an idea inspired by work on feature geometries and the contrastive hierarchy (Clements, 1985; Dresher, 2009).",1 Introduction,[0],[0]
"Second, our system can represent phonotactic generalizations not only at the level of fully specified segments, but also allows the storage and reuse of subsegments, inspired by the autosegments and class nodes of autosegmental phonology.",1 Introduction,[0],[0]
"Finally, also inspired by autosegmental phonology, we make use of a structure-building operation which is senstitive to tier-based contextual structure.
",1 Introduction,[0],[0]
"To model phonotactic learning, we make use of tools from Bayesian nonparametric statistics.",1 Introduction,[0],[0]
"In particular, we make use of the notion of lexical memoization (?; Goodman et al., 2008; Wood et al., 2009; O’Donnell, 2015)—the idea that languagespecific generalizations can be captured by the storage and reuse of frequent patterns from a linguisti-
1Ultimately, we conceive of phonotactics as the module of phonology which generates the underlying forms of lexical items, which are then subject to phonological transformations (i.e., transductions).",1 Introduction,[0],[0]
"In this work, however, we do not attempt to model transformations from underlying to surface forms.
cally universal inventory.",1 Introduction,[0],[0]
"In our case, this amounts to the idea that an inventory of segments and subsegments can be acquired by a learner that stores and reuses commonly occuring segments in particular, phonologically relevant contexts.",1 Introduction,[0],[0]
"In short, we view the problem of learning the phoneme inventory as one of concentrating probability mass on the segments which have been observed before, and the problem of phonotactic generalization as learning which (sub-)segments are likely in particular tierbased phonological contexts.",1 Introduction,[0],[0]
"In this section, we give an overview of how our model works and discuss the phenomena and theoretical ideas that motivate it.",2 Model Motivations,[0],[0]
"Most formal models of phonology posit that segments are grouped into sets, known as natural classes, that are characterized by shared articulatory and acoustic properties, or phonological features (Trubetzkoy, 1939; Jakobson et al., 1952; Chomsky and Halle, 1968).",2.1 Feature Dependency Graphs,[0],[0]
"For example, the segments /n/ and /m/ are classified with a positive value of a nasality feature (i.e., NASALITY:+).",2.1 Feature Dependency Graphs,[0],[0]
"Similarly, /m/ and /p/ can be classified using the labial value of a PLACE feature, PLACE:labial.",2.1 Feature Dependency Graphs,[0],[0]
"These features allow compact description of many phonotactic generalizations.2
From a probabilistic structure-building perspective, we need to specify a generative procedure which assembles segments out of parts defined in terms of these features.",2.1 Feature Dependency Graphs,[0],[0]
"In this section, we will build up such a procedure starting from the simplest possible procedure and progressing towards one which is more phonologically informed.",2.1 Feature Dependency Graphs,[0],[0]
"We will clarify the
2For compatibility with the data sources used in evaluation (Section 5.2), the feature system we use here departs in several ways from standard feature sets: (1) We use multivalent rather than binary-valued features.",2.1 Feature Dependency Graphs,[0],[0]
"(2) We represent manner with a single feature, which has values such as vocalic, stop, and fricative.",2.1 Feature Dependency Graphs,[0],[0]
"This approach allows us to refer to manners more compactly than in systems that employ combinations of features such as sonorant, continuant, and consonantal.",2.1 Feature Dependency Graphs,[0],[0]
"For example, rather than referring to vowels as ‘non-syllabic’, we refer to them using feature value vocalic for the feature MANNER.
generative process here using an analogy to PCFGs, but this analogy will break down in later sections.
",2.1 Feature Dependency Graphs,[0],[0]
The simplest procedure for generating a segment from features is to specify each feature independently.,2.1 Feature Dependency Graphs,[0],[0]
"For example, consider the set of feature-value pairs for /t/: {NASALITY:-, PLACE:alveolar, ...}.",2.1 Feature Dependency Graphs,[0],[0]
"In a naive generative procedure, one could generate an instance of /t/ by independently choosing values for each feature in the set {NASALITY, PLACE, ...}.",2.1 Feature Dependency Graphs,[0],[0]
We express this process using the and-or graph notation below.,2.1 Feature Dependency Graphs,[0],[0]
"Box-shaped nodes—called or-nodes—represent features such as NASALITY, while circular nodes represent groups of features whose values are chosen independently and are called and-nodes.
NASALITY ...",2.1 Feature Dependency Graphs,[0],[0]
"PLACE
",2.1 Feature Dependency Graphs,[0],[0]
"This generative procedure is equivalent (ignoring order) to a PCFG with rules:
SEGMENT→ NASALITY ...",2.1 Feature Dependency Graphs,[0],[0]
"PLACE NASALITY→ + NASALITY→ - PLACE→ bilabial PLACE→ alveolar ...
",2.1 Feature Dependency Graphs,[0],[0]
Not all combinations of feature-value pairs correspond to possible phonemes.,2.1 Feature Dependency Graphs,[0],[0]
"For example, while /l/ is distinguished from other consonants by the feature LATERAL, it is incoherent to specify vowels as LATERAL.",2.1 Feature Dependency Graphs,[0],[0]
"In order to concentrate probability mass on real segments, our process should optimally assign zero probability mass to these incoherent phonemes.",2.1 Feature Dependency Graphs,[0],[0]
"We can avoid specifying a LATERAL feature for vowels by structuring the generative process as below, so that the LATERAL or-node is only reached for consonants:
VOCALIC
A
LATERAL ...
B
HEIGHT ...
consonant vowel
Beyond generating well-formed phonemes, a basic requirement of a model of phonotactics is that it concentrates mass only on the segments in a particular language’s segment inventory.",2.1 Feature Dependency Graphs,[0],[0]
"For example, the model of English phonotactics should put
zero or nominal mass on any sequence containing the segment /x/, although this is a logically possible phoneme.",2.1 Feature Dependency Graphs,[0],[0]
"So our generative procedure for a phoneme must be able to learn to generate only the licit segments of a language, given some probability distributions at the and- and or-nodes.",2.1 Feature Dependency Graphs,[0],[0]
"For this task, independently sampling values at and-nodes does not give us a way to rule out particular combinations of features such as those forming /x/.
Our approach to this problem uses the idea of stochastic memoization (or adaptation), in which the results of certain computations are stored and may be probabilistically reused “as wholes,” rather than recomputed from scratch (Michie, 1968; Goodman et al., 2008).",2.1 Feature Dependency Graphs,[0],[0]
"This technique has been applied to the problem of learning lexical items at various levels of linguistic structure (de Marcken, 1996; Johnson et al., 2007; Goldwater, 2006; O’Donnell, 2015).",2.1 Feature Dependency Graphs,[0],[0]
"Given our model so far, applying stochastic memoization is equivalent to specifying an adaptor grammar over the PCFGs described so far.
",2.1 Feature Dependency Graphs,[0],[0]
Let f be a stochastic function which samples feature values using the and-or graph representation described above.,2.1 Feature Dependency Graphs,[0],[0]
We apply stochastic memoization to each node.,2.1 Feature Dependency Graphs,[0],[0]
"Following Johnson et al. (2007) and Goodman et al. (2008), we use a distribution for probabilistic memoization known as the Dirichlet Process (DP) (Ferguson, 1973; Sethuraman, 1994).",2.1 Feature Dependency Graphs,[0],[0]
Let mem{f} be a DP-memoized version of f .,2.1 Feature Dependency Graphs,[0],[0]
The behavior of a DP-memoized function can be described as follows.,2.1 Feature Dependency Graphs,[0],[0]
"The first time we invoke mem{f}, the feature specification of a new segment will be sampled using f .",2.1 Feature Dependency Graphs,[0],[0]
"On subsequent invocations, we either choose a value from among the set of previous sampled values (a memo draw), or we draw a new value from f (a base draw).",2.1 Feature Dependency Graphs,[0],[0]
"The probability of sampling the ith old value in a memo draw is niN+θ , where N is the number of tokens sampled so far, ni is the number of times that value i has been used in the past, and θ > 0 is a parameter of the model.",2.1 Feature Dependency Graphs,[0],[0]
A base draw happens with probability θN+θ .,2.1 Feature Dependency Graphs,[0],[0]
"This process induces a bias to reuse items from f which have been frequently generated in the past.
",2.1 Feature Dependency Graphs,[0],[0]
We apply mem recursively to the sampling procedure for each node in the feature dependency graph.,2.1 Feature Dependency Graphs,[0],[0]
"The more times that we use some particular set of features under a node to generate words in a language, the more likely we are to reuse that set of
features in the future in a memo draw.",2.1 Feature Dependency Graphs,[0],[0]
This dynamic leads our model to rapidly concentrate probability mass on the subset of segments which occur in the inventory of a language.,2.1 Feature Dependency Graphs,[0],[0]
"Our use of and-or graphs and lexical memoization to model inter-feature dependencies is inspired by work in phonology on distinctiveness and markedness hierarchies (Kean, 1975; Berwick, 1985; Dresher, 2009).",2.2 Class Node Structure,[0],[0]
"In addition to using feature hierarchies to delineate possible segments, the literature has used these structures to designate bundles of features that have privileged status in phonological description, i.e. feature geometries (Clements, 1985; Halle, 1995; McCarthy, 1988).",2.2 Class Node Structure,[0],[0]
"For example, many analyses group features concerning laryngeal states (e.g., VOICE, ASPIRATION, etc.)",2.2 Class Node Structure,[0],[0]
"under a laryngeal node, which is distinct from the node containing oral place-of-articulation features (Clements and Hume, 1995).",2.2 Class Node Structure,[0],[0]
These nodes are known as class nodes.,2.2 Class Node Structure,[0],[0]
"In these analyses, features grouped together under the laryngeal class node may covary while being independent of features grouped under the oral class node.
",2.2 Class Node Structure,[0],[0]
"The lexical memoization technique discussed above captures this notion of class node directly, because the model learns an inventory of subsegments under each node.
",2.2 Class Node Structure,[0],[0]
Consider the feature dependency graph below.,2.2 Class Node Structure,[0],[0]
"A
B
NASALITY VOICE ...
VOCALIC
C
BACKNESS HEIGHT ...
...",2.2 Class Node Structure,[0],[0]
"consonantvowel
In this graph, the and-node A generates fully specified segments.",2.2 Class Node Structure,[0],[0]
"And-node B can be thought of as generating the non-oral properties of a segment, including voicing and nasality.",2.2 Class Node Structure,[0],[0]
"And-node C is a class node bundling together the oral features of vowel segments.
",2.2 Class Node Structure,[0],[0]
"The features under B are outside of the VOCALIC node, so these features are specified for both consonant and vowel segments.",2.2 Class Node Structure,[0],[0]
"This allows combinations such as voiced nasal consonants, and also rarer combinations such as unvoiced nasal vow-
els.",2.2 Class Node Structure,[0],[0]
"Because all and-nodes are recursively memoized, our model is able to bind together particular non-oral choices (node B), learning for instance that the combination {NASALITY:+, VOICED:+} commonly recurs for both vowels and consonants in a language.",2.2 Class Node Structure,[0],[0]
"That is, {NASALITY:+, VOICED:+} becomes a high-probability memo draw.
",2.2 Class Node Structure,[0],[0]
"Since the model learns an inventory of fully specified segments at node A, the model could learn oneoff exceptions to this generalization as well.",2.2 Class Node Structure,[0],[0]
"For example, it could store at a high level a segment with {NASALITY:+, VOICED:-} along with some other features, while maintaining the generalization that {NASALITY:+, VOICED:+} is highly frequent in base draws.",2.2 Class Node Structure,[0],[0]
Language-specific phoneme inventories abound with such combinations of class-nodebased generalizations and idiosyncrasies.,2.2 Class Node Structure,[0],[0]
"By using lexical memoization at multiple different levels, our model can capture both the broader generalizations described in class node terminology and the exceptions to those generalizations.",2.2 Class Node Structure,[0],[0]
"In Section 2.2, we focused on the role that features play in defining a language’s segment inventory.",2.3 Sequential Structure as Memoization in Context,[0],[0]
"We gave a phonologically-motivated generative process, equivalent to an adaptor grammar, for phonemes in isolation.",2.3 Sequential Structure as Memoization in Context,[0],[0]
"However, features also play an important role in characterizing licit sequences.",2.3 Sequential Structure as Memoization in Context,[0],[0]
We model sequential restrictions as context-dependent segment inventories.,2.3 Sequential Structure as Memoization in Context,[0],[0]
"Our model learns a distribution over segments and subsegments conditional on each preceding sequence of (sub)segments, using lexical memoization.",2.3 Sequential Structure as Memoization in Context,[0],[0]
Introducing context-dependence means that the model can no longer be formulated as an adaptor grammar.,2.3 Sequential Structure as Memoization in Context,[0],[0]
One salient property of sequential restrictions in phonotactics is that segments are often required to bear the same feature values as nearby segments.,2.4 Tier-based Interaction,[0],[0]
"For example, a sequence of a nasal and a following stop must agree in place features at the end of a morpheme in English.",2.4 Tier-based Interaction,[0],[0]
Such restrictions may even be non-local.,2.4 Tier-based Interaction,[0],[0]
"For example, many languages prefer combinations of vowels that agree in features such as HEIGHT, BACKNESS, or ROUNDING, even
across arbitrary numbers of intervening consonants (i.e., vowel harmony).
",2.4 Tier-based Interaction,[0],[0]
One way to describe these sequential feature interactions is to assume that feature values of one segment in a word depend on values for the same or closely related features in other segments.,2.4 Tier-based Interaction,[0],[0]
"This is accomplished by dividing segments into subsets (such as consonants and vowels), called tiers, and then making a segment’s feature values preferentially dependent on the values of other segments on the same tier.
",2.4 Tier-based Interaction,[0],[0]
Such phonological tiers are often identified with class nodes in a feature dependency graph.,2.4 Tier-based Interaction,[0],[0]
"For example, a requirement that one vowel identically match the vowel in the preceding syllable would be stated as a requirement that the vowel’s HEIGHT, BACKNESS, and ROUNDING features match the values of the preceding vowel’s features.",2.4 Tier-based Interaction,[0],[0]
"In this case, the vowels themselves need not be adjacent—by assuming that vowel quality features are not present in consonants, it is possible to say that two vowels are adjacent on a tier defined by the nodes HEIGHT, BACKNESS, and ROUNDING.
",2.4 Tier-based Interaction,[0],[0]
Our full generative process for a segment following other segments is the following.,2.4 Tier-based Interaction,[0],[0]
"We follow the example of the generation of a phoneme conditional on a preceding context of /ak/, shown with simplified featural specifications and tiers in Figure 1.
",2.4 Tier-based Interaction,[0],[0]
"At each node in the feature dependency graph, we can either generate a fully-specified subsegment for that node (memo draw), or assemble a novel subsegment for that node out of parts defined by the feature dependency graph (base draw).",2.4 Tier-based Interaction,[0],[0]
"Starting at the root node of the feature dependency graph, we decide whether to do a memo draw or base draw conditional on the previous n subsegments at that node.
",2.4 Tier-based Interaction,[0],[0]
So in order to generate the next segment following /ak/,2.4 Tier-based Interaction,[0],[0]
"in the example, we start at node A in the next draw from the feature geometry, with some probability we do a memo draw conditioned on /ak/, defined by the red tier.",2.4 Tier-based Interaction,[0],[0]
"If we decide to do a base draw instead, we then repeat the procedure conditional on the previous n − 1 segments, recursively until we are conditioning on the empty context.",2.4 Tier-based Interaction,[0],[0]
"That is, we do a memo draw conditional on /k/, or conditional on the empty context.",2.4 Tier-based Interaction,[0],[0]
"This process of conditioning on successively smaller contexts is a standard technique in Bayesian nonparametric language modeling (Teh, 2006; Goldwater et al., 2006).
",2.4 Tier-based Interaction,[0],[0]
"At the empty context, if we decide to do a base draw, then we generate a novel segment by repeating the whole process at each child node, to generate several subsegments.",2.4 Tier-based Interaction,[0],[0]
"In the example, we would assemble a phoneme by independently sampling subsegments at the nasal/laryngeal node B and the MANNER node, and then combining them.",2.4 Tier-based Interaction,[0],[0]
"Crucially, the conditioning context consists only of the values at the current node in the previous phonemes.",2.4 Tier-based Interaction,[0],[0]
"So when we sample a subsegment from node B, it is conditional on the previous two values at node B, { VOICE:+, NASAL:-} and { VOICE:-, NASAL:-}, defined by the blue tier in the figure.",2.4 Tier-based Interaction,[0],[0]
The process continues down the feature dependency graph recursively.,2.4 Tier-based Interaction,[0],[0]
"At the point where the model decides on vowel place features such as height and backness, these will be conditioned only on the vowel places features of the preceding /a/, with /k/ skipped entirely as it does not have values at vowel place nodes.
",2.4 Tier-based Interaction,[0],[0]
This section has provided motivations and a walkthrough of our proposed generative procedure for sequences of segments.,2.4 Tier-based Interaction,[0],[0]
"In the next section, we give the formalization of the model.",2.4 Tier-based Interaction,[0],[0]
Here we give a full formal description of our proposed model in three steps.,3 Formalization of the Models,[0],[0]
"First, in Section 3.1, we formalize the generative process for a segment in isolation.",3 Formalization of the Models,[0],[0]
"Second, in Section 3.2, we give formulation of Bayesian nonparametric N-gram models with backoff.",3 Formalization of the Models,[0],[0]
"Third, in Section 3.3, we show how to drop the generative process for a phoneme into the N-gram model such that tier-based interactions emerge naturally.",3 Formalization of the Models,[0],[0]
"A feature dependency graph G is a fully connected, singly rooted, directed, acyclic graph given by the triple 〈V,A,t, r〉 where V is a set of vertices or nodes,A is a set of directed arcs, t is a total function t(n) : V 7→ {and,or}, and r is a distinguished root node in V .",3.1 Generative Process for a Segment,[0],[0]
"A directed arc is a pair 〈p, c〉 where the parent p and child c are both elements in V .",3.1 Generative Process for a Segment,[0],[0]
The function t(n) identifies whether n is an and- or ornode.,3.1 Generative Process for a Segment,[0],[0]
"Define ch(n) to be the function that returns all children of node n, that is, all n′ ∈ N such that 〈n, n′〉 ∈ A.
A subgraph Gs of feature dependency graph G is the graph obtained by starting from node s by retaining only nodes and arcs reachable by traversing arcs starting from",3.1 Generative Process for a Segment,[0],[0]
s. A subsegment ps is a subgraph rooted in node s for which each or-node contains exactly one outgoing arc.,3.1 Generative Process for a Segment,[0],[0]
Subsegments represent sampled phone constituents.,3.1 Generative Process for a Segment,[0],[0]
"A segment is a subsegment rooted in r—that is, a fully specified phoneme.
",3.1 Generative Process for a Segment,[0],[0]
The distribution associated with a subgraph Gs is given by Gs below.,3.1 Generative Process for a Segment,[0],[0]
Gs is a distribution over subsegments; the distribution for the full graph Gr is a distribution over fully specified segments.,3.1 Generative Process for a Segment,[0],[0]
"We occasionally overload the notation such that Gs(ps) will refer to the probability mass function associated with distribution Gs evaluated at the subsegment ps.
",3.1 Generative Process for a Segment,[0],[0]
"Hs ∼ DP(θs, Gs) (1)
Gs(ps) =    ∏ s′∈ch(s) H s′ (p s′ ) t(s) = AND ∑
s′∈ch(s) ψ s s′H
s′ (p s′ ) t(s) =",3.1 Generative Process for a Segment,[0],[0]
"OR
The first case of the definition covers and-nodes.",3.1 Generative Process for a Segment,[0],[0]
"We assume that the leaves of our feature dependency graph—which represent atomic feature values such as the laryngeal value of a PLACE feature—are childless and-nodes.
",3.1 Generative Process for a Segment,[0],[0]
"The second case of the definition covers or-nodes in the graph, where ψss′ is the probability associated with choosing outgoing arc 〈s, s′〉 from parent ornode s to child node s′.",3.1 Generative Process for a Segment,[0],[0]
"Thus, or-nodes define mixture distributions over outgoing arcs.",3.1 Generative Process for a Segment,[0],[0]
The mixture weights are drawn from a Dirichlet process.,3.1 Generative Process for a Segment,[0],[0]
"In particular, for or-node n in the underlying graph G, the vector of probabilities over outgoing edges is distributed as follows.
",3.1 Generative Process for a Segment,[0],[0]
"~ψs ∼ DP(θs, UNIFORM(|ch(s)|))
Note that in both cases the distribution over child subgraphs is drawn from a Dirichlet process, as below, capturing the notion of subsegmental storage discussed above.",3.1 Generative Process for a Segment,[0],[0]
"Let T be a set of discrete objects (e.g., atomic symbols or structured segments as defined in the preceding sections).",3.2 N-Gram Models with DP-Backoff,[0],[0]
"Let T ∗ be the set of all finite-length strings which can be generated by combining elements of T , under concatenation, ·, including the empty string .",3.2 N-Gram Models with DP-Backoff,[0],[0]
"A context, u is any finite string beginning with a special distinguished start symbol and ending with some sequence in T ∗, that is, u ∈ {start · T ∗}.
",3.2 N-Gram Models with DP-Backoff,[0],[0]
"For any string α, define hd(α) to be the function that returns the first symbol in the string, tl(α) to be the function that returns suffix of αminus the first symbol, and |α| to be the length of α, with hd( ) = tl( ) =",3.2 N-Gram Models with DP-Backoff,[0],[0]
and | | = 0.,3.2 N-Gram Models with DP-Backoff,[0],[0]
"Write the concatenation of two strings α and α′ as α · α′.
Let Hu be a distribution on next symbols—that is, objects in T ∪ {stop}—conditioned on a given context u.",3.2 N-Gram Models with DP-Backoff,[0],[0]
"For an N-gram model of order N , the probability of a string β in T ∗ is given byKNstart(β · stop), where KNu (α) is defined as:
KNu (α) = { 1 α =
HfN (u) (hd(α))×KNu·hd(α)(tl(α))",3.2 N-Gram Models with DP-Backoff,[0],[0]
"otherwise ,
(2) where fn(·) is a context-management function which determines which parts of the left-context should be used to determine the probability of the current symbol.",3.2 N-Gram Models with DP-Backoff,[0],[0]
"In the case of the N-gram models used in this paper, fn(·) takes a sequence u and returns only the rightmost n − 1 elements from the sequence, or the entire sequence if it has length less than n.
Note two aspects of this formulation of N-gram models.",3.2 N-Gram Models with DP-Backoff,[0],[0]
"First, Hu is a family of distributions over next symbols or more general objects.",3.2 N-Gram Models with DP-Backoff,[0],[0]
"Later, we will drop in phonological-feature-based generative processes for these distributions.",3.2 N-Gram Models with DP-Backoff,[0],[0]
"Second, the function fn is a parameter of the above definitions.",3.2 N-Gram Models with DP-Backoff,[0],[0]
"In what follows, we will use a variant of this function which is sensitive to tier-based structure, returning the previous n− 1 only on the appropriate tier.
MacKay and Peto (1994) introduced a hierarchical Dirichlet process-based backoff scheme for N-
gram models, with generalizations in Teh (2006) and Goldwater et al. (2006).",3.2 N-Gram Models with DP-Backoff,[0],[0]
"In this setup, the distribution over next symbols given a context u is drawn hierarchically from a Dirichlet process whose base measure is another Dirichlet process associated with context tl(u), and so on, with all draws ultimately backing off into some unconditioned distribution over all possible next symbols.",3.2 N-Gram Models with DP-Backoff,[0],[0]
"That is, in a hierarchical Dirichlet process N-gram model, Hfn(u) is given as follows.
Hfn(u)",3.2 N-Gram Models with DP-Backoff,[0],[0]
"∼ { DP(θfn(u), Hfn−1(u))",3.2 N-Gram Models with DP-Backoff,[0],[0]
n,3.2 N-Gram Models with DP-Backoff,[0],[0]
"≥ 1 DP(θfn(u), UNIFORM(T ∪ {stop}))",3.2 N-Gram Models with DP-Backoff,[0],[0]
n,3.2 N-Gram Models with DP-Backoff,[0],[0]
= 0,3.2 N-Gram Models with DP-Backoff,[0],[0]
"To make the N-gram model defined in the last section capture tier-based interactions, we make two changes.",3.3 Tier-Based Interactions,[0],[0]
"First, we generalize the generative process Hs from Equation 1 to Hsu, which generates subsegments conditional on a sequence u. Second, we define a context-truncating function fsn(u) which takes a context of segments u and returns the rightmost n− 1 non-empty subsegments whose root node is s.",3.3 Tier-Based Interactions,[0],[0]
Then we substitute the generative processHsfsn(u) (which applies the context-management function fsn(·) to the context u) for Hfn(u) in Equation 2.,3.3 Tier-Based Interactions,[0],[0]
"The resulting probability distribution is:
KNu (α) = { 1 α =
Hr fr N (u) (hd(α))×KNu·hd(α)(tl(α)) otherwise .
",3.3 Tier-Based Interactions,[0],[0]
KNu (α) is the distribution over continuations given a context of segments.,3.3 Tier-Based Interactions,[0],[0]
"Its definition depends on Hsfsn(u), which is the generalization of the generative process for segments Hs to be conditional on some tier-based N-gram context fsn(u).",3.3 Tier-Based Interactions,[0],[0]
"H
s fsn(u) is:
Hsfsn(u) ∼ { DP(θsfsn(u) , Hs fs n−1(u) )",3.3 Tier-Based Interactions,[0],[0]
n,3.3 Tier-Based Interactions,[0],[0]
"≥ 1 DP(θsfsn(u) , Gs fs N (u) ) n",3.3 Tier-Based Interactions,[0],[0]
"= 0
Gsfsn(u)(p s) =
{ ∏",3.3 Tier-Based Interactions,[0],[0]
"s′∈ch(s)H s′ fs′n (u) (ps ′ ) t(s) = AND
∑ s′∈ch(s) ψ s s′H s′",3.3 Tier-Based Interactions,[0],[0]
"fs′n (u) (ps ′ ) t(s) = OR.
Hsfsn(u) and Gsfsn(u) above are mutually recursive functions.",3.3 Tier-Based Interactions,[0],[0]
"Hsfsn(u) implements backoff in the tierbased context of previous subsegments; Gsfsn(u) implements backoff by going down into the probability distributions defined by the feature dependency graph.
",3.3 Tier-Based Interactions,[0],[0]
"Note that the function Hsfsn(u) recursively backs off to the empty context, but its ultimate base distribution is indexed by fsN (u), using the global maximum N-gram order N .",3.3 Tier-Based Interactions,[0],[0]
"So when samples are drawn from the feature dependency graph, they are conditioned on non-empty tier-based contexts.",3.3 Tier-Based Interactions,[0],[0]
"In this way, subsegments are generated based on tier-based context and based on featural backoff in an interleaved fashion.",3.3 Tier-Based Interactions,[0],[0]
We use the Chinese Restaurant Process representation for sampling.,3.4 Inference,[0],[0]
Inference in the model is over seating arrangements for observations of subsegments and over the hyperparameters θ for each restaurant.,3.4 Inference,[0],[0]
We perform Gibbs sampling on seating arrangements in the Dirichlet N-gram models by removing and re-adding observations in each restaurant.,3.4 Inference,[0],[0]
These Gibbs sweeps had negligible impact on model behavior.,3.4 Inference,[0],[0]
"For the concentration parameter θ, we set a prior Gamma(10, .1).",3.4 Inference,[0],[0]
We draw posterior samples using the slice sampler described in Johnson and Goldwater (2009).,3.4 Inference,[0],[0]
We draw one posterior sample of the hyperparameters for each Gibbs sweep.,3.4 Inference,[0],[0]
"In contrast to the Gibbs sweeps, we found resampling hyperparameters to be crucial for achieving the performance described below (Section 5.3).",3.4 Inference,[0],[0]
Phonotactics has proven a fruitful problem domain for computational models.,4 Related Work,[0],[0]
"Most such work has adopted a constraint-based approach, attempting to design a scoring function based on phonological features to separate acceptable forms from unacceptable ones, typically by formulating restrictions or constraints to rule out less-good structures.
",4 Related Work,[0],[0]
"This concept has led naturally to the use of undirected (maximum-entropy, log-linear) models.",4 Related Work,[0],[0]
"In this class of models, a form is scored by evaluation against a number of predicates, called factors3—for example, whether two adjacent segments have the phonological features VOICE:+ VOICE:-.",4 Related Work,[0],[0]
"Each factor is associated with a weight, and the score for a form is the sum of the weights of the factors which are true for the form.",4 Related Work,[0],[0]
"The well-known model of
3Factors are also commonly called “features”—a term we avoid to prevent confusion with phonological features.
",4 Related Work,[0],[0]
"Hayes and Wilson (2008) adopts this framework, pairing it with a heuristic procedure for finding explanatory factors while preventing overfitting.",4 Related Work,[0],[0]
"Similarly, Albright (2009) assigns a score to forms based on factors defined over natural classes of adjacent segments.",4 Related Work,[0],[0]
Constraint-based models have the advantage of flexibility: it is possible to score forms using arbitrarily complex and overlapping sets of factors.,4 Related Work,[0],[0]
"For example, one can state a constraint against adjacent phonemes having features VOICE:+ and LATERAL:+, or any combination of feature values.
",4 Related Work,[0],[0]
"In contrast, we have presented a model where forms are built out of parts by structure-building operations.",4 Related Work,[0],[0]
"From this perspective, the goal of a model is not to rule out bad forms, but rather to discover repeating structures in good forms, such that new forms with those structures can be generated.
",4 Related Work,[0],[0]
In this setting there is less flexibility in how phonological features can affect well-formedness.,4 Related Work,[0],[0]
"For a structure-building model to assign “scores” to arbitrary pairs of co-occurring features, there must be a point in the generative process where those features are considered in isolation.",4 Related Work,[0],[0]
Coming up with such a process has been challenging.,4 Related Work,[0],[0]
"As a result of this limitation, structure-building models of phonotactics have not generally included rich featural interactions.",4 Related Work,[0],[0]
"For example, Coleman and Pierrehumbert (1997) give a probabilistic model for phonotactics where words are generated using grammar over units such as syllables, onsets, and rhymes.",4 Related Work,[0],[0]
"This model does not incorporate fine-grained phonological features such as voicing and place.
",4 Related Work,[0],[0]
"In fact, it has been argued that a constraintbased approach is required in order to capture rich feature-based interactions.",4 Related Work,[0],[0]
"For example, Goldsmith and Riggle (2012) develop a tier-based structurebuilding model of Finnish phonotactics which captures nonlocal vowel harmony interactions, but argue that this model is inadequate because it does not assign higher probabilities to forms than an Ngram model, a common baseline model for phonotactics (Daland et al., 2011).",4 Related Work,[0],[0]
They argue that this deficiency is because the model cannot simultaneously model nonlocal vowel-vowel interactions and local consonant-vowel interactions.,4 Related Work,[0],[0]
"Because of our tier-based conditioning mechanism (Sections 2.4 and 3.3), our model can simultaneously produce local and nonlocal interactions between features us-
ing structure-building operations, and does assign higher probabilities to held-out forms than an Ngram model (Section 5.3).",4 Related Work,[0],[0]
"From this perspective, our model can be seen as a proof of concept that it is possible to have rich feature-based conditioning without adopting a constraint-based approach.
",4 Related Work,[0],[0]
"While our model can capture featural interactions, it is less flexible than a constraint-based model in that the allowable interactions are specified by the feature dependency graph.",4 Related Work,[0],[0]
"For example, there is no way to encode a direct constraint against adjacent phonemes having features VOICE:+ and LATERAL:+.",4 Related Work,[0],[0]
"We consider this a strength of the approach: A particular feature dependency graph is a parameter of our model, and a specific scientific hypothesis about the space of likely featural interactions between phonemes, similar to feature geometries from classical generative phonology (Clements, 1985; McCarthy, 1988; Halle, 1995).4
While probabilistic approaches have mostly taken a constraint-based approach, recent formal language theoretic approaches to phonology have investigated what basic parts and structure building operations are needed to capture realistic feature-based interactions (Heinz et al., 2011; Jardine and Heinz, 2015).",4 Related Work,[0],[0]
"We see probabilistic structure-building approaches such as this work as a way to unify the recent formal language theoretic advances in computational phonology with computational phonotactic modeling.
",4 Related Work,[0],[0]
"Our model joins other NLP work attempting to do sequence generation where each symbol is generated based on a rich featural representation of previous symbols (Bilmes and Kirchhoff, 2003; Duh and Kirchhoff, 2004), though we focus more on phonology-specific representations.",4 Related Work,[0],[0]
"Our and-or graphs are similar to those used in computer vision to represent possible objects (Jin and Geman, 2006).",4 Related Work,[0],[0]
"Here we evaluate some of the design decisions of our model and compare it to a baseline N-gram model and to a widely-used constraint-based model, BLICK.",5 Model Evaluation and Experiments,[0],[0]
"In order to probe model behavior, we also
4We do however note that it may be possible to learn feature hierarchies on a language-by-language basis from universal articulatory and acoustic biases, as suggested by Dresher (2009).
present evaluations on artificial data, and a sampling of “representative forms” preferred by one model as compared to another.
",5 Model Evaluation and Experiments,[0],[0]
Our model consists of structure-building operations over a learned inventory of subsegments.,5 Model Evaluation and Experiments,[0],[0]
"If our model can exploit more repeated structure in phonological forms than the N-gram model or constraintbased models, then it should assign higher probabilities to forms.",5 Model Evaluation and Experiments,[0],[0]
"The log probability of a form under a model corresponds to the description length of that form under the model; if a model assigns a higher log probability to a form, that means the model is capable of compressing the form more than other models.",5 Model Evaluation and Experiments,[0],[0]
"Therefore, we compare models on their ability to assign high probabilities to phonological forms, as in Goldsmith and Riggle (2012).",5 Model Evaluation and Experiments,[0],[0]
"We are interested in discovering the extent to which each model component described above— feature dependency graphs (Section 2.1), class node structure (Section 2.2), and tier-based conditioning (Section 2.4)— contributes to the ability of the model to explain wordforms.
",5.1 Evaluation of Model Components,[0],[0]
"To evaluate the contribution of feature dependency graphs, we compare our models with a baseline N-gram model, which represents phonemes as atomic units.",5.1 Evaluation of Model Components,[0],[0]
"For this N-gram model, we use a Hierarchical Dirichlet Process with n = 3.
To evaluate feature dependency graphs with and without articulated class node structure, we compare models using the graph shown in Figure 3 (the minimal structure required to produce wellformed phonemes) to models with the graph shown in Figure 2, which includes phonologically motivated “class nodes”.5
To evaluate tier-based conditioning, we compare models with the conditioning described in Sections 2.4 and 3.3 to models where all decisions are conditioned on the full featural specification of the previous n − 1 phonemes.",5.1 Evaluation of Model Components,[0],[0]
"This allows us to isolate improvements due to tier-based conditioning beyond improvements from the feature hierarchy.
",5.1 Evaluation of Model Components,[0],[0]
5These feature dependency graphs differ from those in the exposition in Section 2 in that they do not include a MANNER feature; but rather treat vowel as a possible value of MANNER.,5.1 Evaluation of Model Components,[0],[0]
"The WOLEX corpus provides transcriptions for words in dictionaries of 60 diverse languages, represented in terms of phonological features (Graff, 2012).",5.2 Lexicon Data,[0],[0]
"In addition to words, the dictionaries include some short set phrases, such as of course.",5.2 Lexicon Data,[0],[0]
"We use the featural representation of WOLEX, and design our feature dependency graphs to generate only well-formed phonemes according to this feature system.",5.2 Lexicon Data,[0],[0]
"For space reasons, we present the evaluation of our model on 14 of these languages, chosen based on the quality of their transcribed lexicons, and the authors’ knowledge of their phonological systems.",5.2 Lexicon Data,[0],[0]
Here we test whether the different model configurations described above assign high probability to held-out forms.,5.3 Held-Out Evaluation,[0],[0]
This tests the models’ ability to generalize beyond their training data.,5.3 Held-Out Evaluation,[0],[0]
"We train each model on 2500 randomly selected wordforms from a WOLEX dictionary, and compute posterior predictive probabilities for the remaining wordforms from the final state of the model.
",5.3 Held-Out Evaluation,[0],[0]
"Table 1 shows the average probability of a heldout word under our models and under the N-gram model for one model run.6 For all languages, we get a statistically significant increase in probabilities by adopting the autosegmental model with class nodes and tier-based conditioning.",5.3 Held-Out Evaluation,[0],[0]
Model variants without either component do not significantly outperform the N-gram model except in Chinese.,5.3 Held-Out Evaluation,[0],[0]
The combination of class nodes and tier-based conditioning results in model improvements beyond the contributions of the individual features.,5.3 Held-Out Evaluation,[0],[0]
"Our model outperforms the N-gram model in predicting held-out forms, but it remains to be shown that this performance is due to capturing the kinds of linguistic intuitions discussed in Section 2.",5.4 Evaluation on Artificial Data,[0],[0]
"An alternative possibility is that the Autosegmental Ngram model, which has many more parameters than a plain N-gram model, can simply learn a more accurate model of any sequence, even if that sequence has none of the structure discussed above.",5.4 Evaluation on Artificial Data,[0],[0]
"To evaluate this possibility, we compare the performance of our model in predicting held-out linguistic forms to its performance in predicting held-out forms from artificial lexicons which expressly do not have the
6The mean standard deviation per form of log probabilities over 50 runs of the full model ranged from .09 for Amharic to .23 for Dutch.
linguistic structure we are interested in.",5.4 Evaluation on Artificial Data,[0],[0]
"If the autosegmental model outperforms the Ngram model even on artificial data with no phonological structure, then its performance on the real linguistic data in Section 5.3 might be overfitting.",5.4 Evaluation on Artificial Data,[0],[0]
"On the other hand, if the autosegmental model does better on real data but not artificial data, then we can conclude that it is picking up on some real distinctive structure of that data.
",5.4 Evaluation on Artificial Data,[0],[0]
"For each real lexicon Lr, we generate an artificial lexicon La by training a DP 3-gram model on Lr and forward-sampling |Lr| forms.",5.4 Evaluation on Artificial Data,[0],[0]
"Additionally, the forms in La are constrained to have the same distribution over lengths as the forms in Lr.",5.4 Evaluation on Artificial Data,[0],[0]
The resulting lexicons have no tier-based or featural interactions except as they appear by chance from the N-gram model trained on these lexica.,5.4 Evaluation on Artificial Data,[0],[0]
"For each La we then train our models on the first 2500 forms and score the probabilities of the held-out forms, the same procedure as in Section 5.3.
",5.4 Evaluation on Artificial Data,[0],[0]
We ran this procedure for all the lexicons shown in Table 1.,5.4 Evaluation on Artificial Data,[0],[0]
"For all but one lexicon, we find that the autosegmental models do not significantly outperform the N-gram models on artificial data.",5.4 Evaluation on Artificial Data,[0],[0]
"The exception is Mandarin Chinese, where the average log probability of an artificial form is −13.81 under the N-gram model and −13.71 under the full autosegmental model.",5.4 Evaluation on Artificial Data,[0],[0]
"The result suggests that the anomalous behavior of Mandarin Chinese in Section 5.3 may be due to overfitting.
",5.4 Evaluation on Artificial Data,[0],[0]
"When exposed to data that explicitly does not have autosegmental structure, the model is not more accurate than a plain sequence model for almost all languages.",5.4 Evaluation on Artificial Data,[0],[0]
"But when exposed to real linguistic data, the model is more accurate.",5.4 Evaluation on Artificial Data,[0],[0]
"This result provides evidence that the generative model developed in Section 2 captures true distributional properties of lexicons that are absent in N-gram distributions, such as featural and tier-based interactions.",5.4 Evaluation on Artificial Data,[0],[0]
"Here we provide a comparison with Hayes and Wilson (2008)’s Phonotactic Learner, which outputs a phonotactic grammar in the form of a set of weighted constraints on feature co-occurrences.",5.5 Comparison with a Constraint-Based Model,[0],[0]
"This grammar is optimized to match the constraint violation profile in a training lexicon, and so can be
seen as a probabilistic model of that lexicon.",5.5 Comparison with a Constraint-Based Model,[0],[0]
"The authors have distributed one such grammar, BLICK, as a “reference point for phonotactic probability in experimentation” (Hayes, 2012).",5.5 Comparison with a Constraint-Based Model,[0],[0]
"Here we compare our model against BLICK on its ability to assign probabilities to forms, as in Section 5.3.
",5.5 Comparison with a Constraint-Based Model,[0],[0]
"Ideally, we would simply compute the probability of forms like we did in our earlier model comparisons.",5.5 Comparison with a Constraint-Based Model,[0],[0]
BLICK returns scores for each form.,5.5 Comparison with a Constraint-Based Model,[0],[0]
"However, since the probabilistic model underlying BLICK is undirected, these scores are in fact unnormalized log probabilities, so they cannot be compared directly to the normalized probabilities assigned by the other models.",5.5 Comparison with a Constraint-Based Model,[0],[0]
"Furthermore, because the probabilistic model underlying BLICK does not penalize forms for length, the normalizing constant over all forms is in fact infinite, making straightforward comparison of predictive probabilities impossible.",5.5 Comparison with a Constraint-Based Model,[0],[0]
"Nevertheless, we can turn BLICK scores into probabilities by conditioning on further constraints, such as the length k of the form.",5.5 Comparison with a Constraint-Based Model,[0],[0]
We enumerate all possible forms of length k to compute the normalizing constant for the distribution over forms of that length.,5.5 Comparison with a Constraint-Based Model,[0],[0]
"The same procedure can also be used to compute the probabilities of each form, conditioned on the length of the form k, under the N-gram and Autosegmental models.
",5.5 Comparison with a Constraint-Based Model,[0],[0]
"To compare our models against BLICK, we calculate conditional probabilities for forms of length 2 through 5 from the English lexicon.7",5.5 Comparison with a Constraint-Based Model,[0],[0]
The forms are those in the WOLEX corpus; we include them for this evaluation if they are k symbols long in the WOLEX representation.,5.5 Comparison with a Constraint-Based Model,[0],[0]
"For our N-gram and Autosegmental models, we use the same models as in Section 5.3.",5.5 Comparison with a Constraint-Based Model,[0],[0]
The average probabilities of forms under the three models are shown in Table 2.,5.5 Comparison with a Constraint-Based Model,[0],[0]
"For length 3-5, the autosegmental model assigns the highest probabilities, followed by the N-gram model and BLICK.",5.5 Comparison with a Constraint-Based Model,[0],[0]
"For length 2, BLICK outperforms the DP N-gram model but not the autosegmental model.
",5.5 Comparison with a Constraint-Based Model,[0],[0]
Our model assigns higher probabilities to short forms than BLICK.,5.5 Comparison with a Constraint-Based Model,[0],[0]
"That is, our models have identified more redundant structure in the forms than BLICK, allowing them to compress the data more.",5.5 Comparison with a Constraint-Based Model,[0],[0]
"However, the comparison is imperfect in several
7Enumerating and scoring the 22,164,361,129 possible forms of length 6 was computationally impractical.
ways.",5.5 Comparison with a Constraint-Based Model,[0],[0]
"First, BLICK and our models were trained on different data; it is possible that our training data are more representative of our test data than BLICK’s training data were.",5.5 Comparison with a Constraint-Based Model,[0],[0]
"Second, BLICK uses a different underlying featural decomposition than our models; it is possible that our feature system is more accurate.",5.5 Comparison with a Constraint-Based Model,[0],[0]
"Nevertheless, these results show that our model concentrates more probability mass on (short) forms attested in a language, whereas BLICK likely spreads its probability mass more evenly over the space of all possible (short) strings.",5.5 Comparison with a Constraint-Based Model,[0],[0]
"In order to get a sense of the differences between models, we investigate what phonological forms are preferred by different kinds of models.",5.6 Representative Forms,[0],[0]
These forms might be informative about the phonotactic patterns that our model is capturing which are not wellrepresented in simpler models.,5.6 Representative Forms,[0],[0]
"We calculate the representativeness of a form f with respect to model m1 as opposed to m2 as p(f |m1)/p(f |m2) (Good, 1965; Tenenbaum and Griffiths, 2001).",5.6 Representative Forms,[0],[0]
"The forms that are most “representative” of model m1 are not the forms thatm1 assigns the highest probability, but rather the forms thatm1 ranks highest relative tom2.
",5.6 Representative Forms,[0],[0]
Tables 3 and 4 show forms from the lexicon that are most representative of our full model and of the N-gram model for English and Turkish.,5.6 Representative Forms,[0],[0]
"The most
uniquely representative forms for our full model are morphologically complex forms consisting of many productive, frequently reused morphemes such as ness.",5.6 Representative Forms,[0],[0]
"On the other hand, the representative forms for the N-gram model include foreign forms such as a posteriori (for English) and ekskavatör (for Turkish), which are not built out of parts that frequently repeat in those languages.",5.6 Representative Forms,[0],[0]
"The representative forms suggest that the full model places more probability mass on words which are built out of highly productive, phonotactically well-formed parts.",5.6 Representative Forms,[0],[0]
"We find that our models succeed in assigning high probabilities to unseen forms, that they do so specifically for linguistic forms and not random sequences, that they tend to favor forms with many productive parts, and that they perform comparably to a stateof-the-art constraint-based model in assigning probabilities to short forms.
",6 Discussion,[0],[0]
The improvement for our models over the N-gram baseline is consistent but not large.,6 Discussion,[0],[0]
"We attribute this to the way in which phonological generalizations are used in the present model: in particular, phonological generalizations function primarily as a form of backoff for a sequence model.",6 Discussion,[0],[0]
"Our models have lexical memoization at each node in a feature dependency graph; as such, the top node in the graph ends up representing transition probabilities for whole phonemes conditioned on previous phonemes, and the rest of the feature dependency graph functions as a backoff distribution.",6 Discussion,[0],[0]
"When a model has been exposed to many training forms, its behavior will be largely dominated by the N-gramlike behavior of the top node.",6 Discussion,[0],[0]
"In future work it might be effective to learn an optimal backoff procedure which gives more influence to the base distribution (Duh and Kirchhoff, 2004; Wood and Teh, 2009).
",6 Discussion,[0],[0]
"While the tier-based conditioning in our model would seem to be capable of modeling nonlocal interactions such as vowel harmony, we have not found that the models do well at reproducing these nonlocal interactions.",6 Discussion,[0],[0]
We believe this is because the model’s behavior is dominated by nodes high in the feature dependency graph.,6 Discussion,[0],[0]
"In any case, a simple Markov model defined over tiers, as we have presented here, might not be enough to fully model vowel harmony.",6 Discussion,[0],[0]
"Rather, a model of phonological processes, transducing underlying forms to surface forms, seems like a more natural way to capture these phenomena.
",6 Discussion,[0],[0]
We stress that this model is not tied to a particular feature dependency graph.,6 Discussion,[0],[0]
"In fact, we believe our model provides a novel way of testing different hypotheses about feature structures, and could form the basis for learning the optimal feature hierarchy for a given data set.",6 Discussion,[0],[0]
The choice of feature dependency graph has a large effect on what featural interactions the model can represent directly.,6 Discussion,[0],[0]
"For example, neither feature dependency graph has shared place features for consonants and vowels, so the model has limited ability to represent place-based restrictions on consonant-vowel sequences such as requirements for labialized or palatalized consonants in the context of /u/ or /i/.",6 Discussion,[0],[0]
"These interactions can be treated in our framework if vowels and consonants share place features, as in Padgett (2011).",6 Discussion,[0],[0]
"We have presented a probabilistic generative model for sequences of phonemes defined in terms of phonological features, based on representational ideas from generative phonology and tools from Bayesian nonparametric modeling.",7 Conclusion,[0],[0]
We consider our model as a proof of concept that probabilistic structure-building models can include rich featural interactions.,7 Conclusion,[0],[0]
"Our model robustly outperforms an Ngram model on simple metrics, and learns to generate forms consisting of highly productive parts.",7 Conclusion,[0],[0]
"We also view this work as a test of the scientific hypotheses that phonological features can be organized in a hierarchy and that they interact along tiers: in our model evaluation, we found that both concepts were necessary to get an improvement over a baseline N-gram model.",7 Conclusion,[0],[0]
"We would like to thank Tal Linzen, Leon Bergen, Edward Flemming, Edward Gibson, Bob Berwick, Jim Glass, and the audiences at MIT’s Phonology Circle, SIGMORPHON, and the LSA 2016 Annual Meeting for helpful comments.",Acknowledgments,[0],[0]
This work was supported in part by NSF DDRIG Grant #1551543 to R.F.,Acknowledgments,[0],[0]
"We present a probabilistic model of phonotactics, the set of well-formed phoneme sequences in a language.",abstractText,[0],[0]
"Unlike most computational models of phonotactics (Hayes and Wilson, 2008; Goldsmith and Riggle, 2012), we take a fully generative approach, modeling a process where forms are built up out of subparts by phonologically-informed structure building operations.",abstractText,[0],[0]
"We learn an inventory of subparts by applying stochastic memoization (Johnson et al., 2007; Goodman et al., 2008) to a generative process for phonemes structured as an and-or graph, based on concepts of feature hierarchy from generative phonology (Clements, 1985; Dresher, 2009).",abstractText,[0],[0]
Subparts are combined in a way that allows tier-based feature interactions.,abstractText,[0],[0]
"We evaluate our models’ ability to capture phonotactic distributions in the lexicons of 14 languages drawn from the WOLEX corpus (Graff, 2012).",abstractText,[0],[0]
Our full model robustly assigns higher probabilities to held-out forms than a sophisticated N-gram model for all languages.,abstractText,[0],[0]
We also present novel analyses that probe model behavior in more detail.,abstractText,[0],[0]
A Generative Model of Phonotactics,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 118–124 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-2019",text,[0],[0]
"Generative models defining joint distributions over parse trees and sentences are good theoretical models for interpreting natural language data, and appealing tools for tasks such as parsing, grammar induction and language modeling (Collins, 1999; Henderson, 2003; Titov and Henderson, 2007; Petrov and Klein, 2007; Dyer et al., 2016).",1 Introduction,[0],[0]
"However, they often impose strong independence assumptions which restrict the use of arbitrary features for effective disambiguation.",1 Introduction,[0],[0]
"Moreover, generative parsers are typically trained by maximizing the joint probability of the parse tree and the sentence—an objective that only indirectly relates to the goal of parsing.",1 Introduction,[0],[0]
"At test time, these models require a relatively expensive recognition algo-
1Our code is available at https://github.com/ cheng6076/virnng.git.
rithm (Collins, 1999; Titov and Henderson, 2007) to recover the parse tree, but the parsing performance consistently lags behind their discriminative competitors (Nivre et al., 2007; Huang, 2008; Goldberg and Elhadad, 2010), which are directly trained to maximize the conditional probability of the parse tree given the sentence, where linear-time decoding algorithms exist (e.g., for transition-based parsers).
",1 Introduction,[0],[0]
"In this work, we propose a parsing and language modeling framework that marries a generative model with a discriminative recognition algorithm in order to have the best of both worlds.",1 Introduction,[0],[0]
The idea of combining these two types of models is not new.,1 Introduction,[0],[0]
"For example, Collins and Koo (2005) propose to use a generative model to generate candidate constituency trees and a discriminative model to rank them.",1 Introduction,[0],[0]
Sangati et al. (2009) follow the opposite direction and employ a generative model to re-rank the dependency trees produced by a discriminative parser.,1 Introduction,[0],[0]
"However, previous work combines the two types of models in a goal-oriented, pipeline fashion, which lacks model interpretations and focuses solely on parsing.
",1 Introduction,[0],[0]
"In comparison, our framework unifies generative and discriminative parsers with a single objective, which connects to expectation maximization and variational inference in grammar induction settings.",1 Introduction,[0],[0]
"In a nutshell, we treat parse trees as latent factors generating natural language sentences and parsing as a posterior inference task.",1 Introduction,[0],[0]
"We showcase the framework using Recurrent Neural Network Grammars (RNNGs; Dyer et al. 2016), a recently proposed probabilistic model of phrase-structure trees based on neural transition systems.",1 Introduction,[0],[0]
"Different from this work which introduces separately trained discriminative and generative models, we integrate the two in an auto-encoder which fits our training objective.",1 Introduction,[0],[0]
"We show how the framework enables grammar induction, parsing and language
118
modeling within a single implementation.",1 Introduction,[0],[0]
"On the English Penn Treebank, we achieve competitive performance on constituency parsing and state-ofthe-art single-model language modeling score.",1 Introduction,[0],[0]
"In this section we briefly describe Recurrent Neural Network Grammars (RNNGs; Dyer et al. 2016), a top-down transition-based algorithm for parsing and generation.",2 Preliminaries,[0],[0]
"There are two versions of RNNG, one discriminative, the other generative.",2 Preliminaries,[0],[0]
"We follow the original paper in presenting the discriminative variant first.
",2 Preliminaries,[0],[0]
The discriminative RNNG follows a shiftreduce parser that converts a sequence of words into a parse tree.,2 Preliminaries,[0],[0]
"As in standard shift-reduce parsers, the RNNG uses a buffer to store unprocessed terminal symbols and a stack to store partially completed syntactic constituents.",2 Preliminaries,[0],[0]
"At each timestep, one of the following three operations2 is performed:
• NT(X) introduces an open non-terminal X onto the top of the stack, represented as an open parenthesis followed by X, e.g., (NP.
• SHIFT fetches the terminal in the front of the buffer and pushes it onto the top of the stack.
",2 Preliminaries,[0],[0]
• REDUCE completes a subtree by repeatedly popping the stack until an open non-terminal is encountered.,2 Preliminaries,[0],[0]
"The non-terminal is popped as well, after which a composite term representing the entire subtree is pushed back onto the top of the stack, e.g., (NP the cat).
",2 Preliminaries,[0],[0]
The above transition system can be adapted with minor modifications to an algorithm that generates trees and sentences.,2 Preliminaries,[0],[0]
"In generator transitions, there is no input buffer of unprocessed words but there is an output buffer for storing words that have been generated.",2 Preliminaries,[0],[0]
"To reflect the change, the previous SHIFT operation is modified into a GEN operation defined as follows:
• GEN generates a terminal symbol and add it to the stack and the output buffer.
",2 Preliminaries,[0],[0]
"2To be precise, the total number of operations under our description is |X|+2 since the NT operation varies with the non-terminal choice X.",2 Preliminaries,[0],[0]
Our framework unifies generative and discriminative parsers within a single training objective.,3 Methodology,[0],[0]
"For illustration, we adopt the two RNNG variants introduced above with our customized features.",3 Methodology,[0],[0]
"Our starting point is the generative model (§ 3.1), which allows us to make explicit claims about the generative process of natural language sentences.",3 Methodology,[0],[0]
"Since this model alone lacks a bottom-up recognition mechanism, we introduce a discriminative recognition model (§ 3.2) and connect it with the generative model in an encoder-decoder setting.",3 Methodology,[0],[0]
"To offer a clear interpretation of the training objective (§ 3.3), we first consider the parse tree as latent and the sentence as observed.",3 Methodology,[0],[0]
We then discuss extensions that account for labeled parse trees.,3 Methodology,[0],[0]
"Finally, we present various inference techniques for parsing and language modeling within the framework (§ 3.4).",3 Methodology,[0],[0]
"The decoder is a generative RNNG that models the joint probability p(x, y) of a latent parse tree y and an observed sentence x.",3.1 Decoder (Generative Model),[0],[0]
"Since the parse tree is defined by a sequence of transition actions a, we write p(x, y) as p(x, a).3 The joint distribution p(x, a) is factorized into a sequence of transition probabilities and terminal probabilities (when actions are GEN), which are parametrized by a transitional state embedding u:
p(x, a) = p(a)p(x|a)
",3.1 Decoder (Generative Model),[0],[0]
"=
|a|∏
t=1
p(at|ut)p(xt|ut)I(at=GEN) (1)
where I is an indicator function and ut represents the state embedding at time step t. Specifically, the conditional probability of the next action is:
p(at|ut) = |a|∏
t=1
exp(atu T t + ba)∑
a′∈A exp(a ′uTt + ba′)
(2)
where at represents the action embedding at time step t, A the action space and ba the bias.",3.1 Decoder (Generative Model),[0],[0]
"Similarly, the next word probability (when GEN is invoked) is computed as:
p(wt|ut) = |a|∏
t=1
exp(wtu T t +",3.1 Decoder (Generative Model),[0],[0]
"bw)∑
w′∈W exp(w ′uTt + bw′)
(3)
3We assume that the action probability does not take the actual terminal choice into account.
",3.1 Decoder (Generative Model),[0],[0]
whereW denotes all words in the vocabulary.,3.1 Decoder (Generative Model),[0],[0]
"To satisfy the independence assumptions imposed by the generative model, ut uses only a restricted set of features defined over the output buffer and the stack — we consider p(a) as a context insensitive prior distribution.",3.1 Decoder (Generative Model),[0],[0]
"Specifically, we use the following features: 1) the stack embedding dt which encodes the stack of the decoder and is obtained with a stack-LSTM (Dyer et al., 2015, 2016); 2) the output buffer embedding ot; we use a standard LSTM to compose the output buffer and ot is represented as the most recent state of the LSTM; and 3) the parent non-terminal embedding nt which is accessible in the generative model because the RNNG employs a depth-first generation order.",3.1 Decoder (Generative Model),[0],[0]
"Finally, ut is computed as:
ut = W2 tanh(W1[dt,ot,nt] + bd) (4)
where Ws are weight parameters and bd the bias.",3.1 Decoder (Generative Model),[0],[0]
The encoder is a discriminative RNNG that computes the conditional probability q(a|x) of the transition action sequence a given an observed sentence x.,3.2 Encoder (Recognition Model),[0],[0]
"This conditional probability is factorized over time steps as:
q(a|x) = |a|∏
t=1
q(at|vt) (5)
where vt is the transitional state embedding of the encoder at time step t.
The next action is predicted similarly to Equation (2), but conditioned on vt.",3.2 Encoder (Recognition Model),[0],[0]
"Thanks to the discriminative property, vt has access to any contextual features defined over the entire sentence and the stack — q(a|x) acts as a context sensitive posterior approximation.",3.2 Encoder (Recognition Model),[0],[0]
"Our features4 are: 1) the stack embedding et obtained with a stack-LSTM that encodes the stack of the encoder; 2) the input buffer embedding it; we use a bidirectional LSTM to compose the input buffer and represent each word as a concatenation of forward and backward LSTM states; it is the representation of the word on top of the buffer; 3) to incorporate more global features and a more sophisticated look-ahead mechanism for the buffer, we also use an adaptive buffer embedding īt; the latter is computed by having the stack embedding et attend to
4Compared to Dyer et al. (2016), the new features we introduce are 3) and 4), which we found empirically useful.
",3.2 Encoder (Recognition Model),[0],[0]
all remaining embeddings on the buffer with the attention function in Vinyals et al. (2015); and 4) the parent non-terminal embedding nt.,3.2 Encoder (Recognition Model),[0],[0]
"Finally, vt is computed as follows:
vt = W4 tanh(W3[et, it, īt,nt] + be) (6)
where Ws are weight parameters and be the bias.",3.2 Encoder (Recognition Model),[0],[0]
"Consider an auto-encoder whose encoder infers the latent parse tree and the decoder generates the observed sentence from the parse tree.5 The maximum likelihood estimate of the decoder parameters is determined by the log marginal likelihood of the sentence:
log p(x) = log ∑
a
p(x, a) (7)
We follow expectation-maximization and variational inference techniques to construct an evidence lower bound of the above quantity (by Jensen’s Inequality), denoted as follows:
log p(x) ≥ Eq(a|x) log p(x, a)
q(a|x)",3.3 Training,[0],[0]
"= Lx (8)
where p(x, a) = p(x|a)p(a) comes from the decoder or the generative model, and q(a|x) comes from the encoder or the recognition model.",3.3 Training,[0],[0]
"The objective function6 in Equation (8), denoted by Lx, is unsupervised and suited to a grammar induction task.",3.3 Training,[0],[0]
"This objective can be optimized with the methods shown in Miao and Blunsom (2016).
",3.3 Training,[0],[0]
"Next, consider the case when the parse tree is observed.",3.3 Training,[0],[0]
"We can directly maximize the log likelihood of the parse tree for the encoder output log q(a|x) and the decoder output log p(a):
La = log q(a|x) + log p(a) (9)
",3.3 Training,[0],[0]
"This supervised objective leverages extra information of labeled parse trees to regularize the distribution q(a|x) and p(a), and the final objective is:
L = Lx + La (10) where Lx and La can be balanced with the task focus (e.g, language modeling or parsing).
5Here, GEN and SHIFT refer to the same action with different definitions for encoding and decoding.
6See § 4 and Appendix A for comparison between this objective and the importance sampler of Dyer et al. (2016).",3.3 Training,[0],[0]
"We consider two inference tasks, namely parsing and language modeling.
",3.4 Inference,[0],[0]
"Parsing In parsing, we are interested in the parse tree that maximizes the posterior p(a|x) (or the joint p(a, x)).",3.4 Inference,[0],[0]
"However, the decoder alone does not have a bottom-up recognition mechanism for computing the posterior.",3.4 Inference,[0],[0]
"Thanks to the encoder, we can compute an approximated posterior q(a|x) in linear time and select the parse tree that maximizes this approximation.",3.4 Inference,[0],[0]
"An alternative is to generate candidate trees by sampling from q(a|x), re-rank them with respect to the joint p(x, a) (which is proportional to the true posterior), and select the sample that maximizes the true posterior.
",3.4 Inference,[0],[0]
"Language Modeling In language modeling, our goal is to compute the marginal probability p(x) = ∑ a p(x, a), which is typically intractable.",3.4 Inference,[0],[0]
"To approximate this quantity, we can use Equation (8) to compute a lower bound of the log likelihood log p(x) and then exponentiate it to get a pessimistic approximation of p(x).7
Another way of computing p(x) (without lower bounding) would be to use the variational approximation q(a|x) as the proposal distribution as in the importance sampler of Dyer et al. (2016).",3.4 Inference,[0],[0]
"However, this is beyond the scope of this work and we leave detailed discussions to Appendix A.",3.4 Inference,[0],[0]
"Our framework is related to a class of variational autoencoders (Kingma and Welling, 2014), which use neural networks for posterior approximation in variational inference.",4 Related Work,[0],[0]
"This technique has been previously used for topic modeling (Miao et al.,
7As a reminder, the language modeling objective is exp(NLL/T ), where NLL denotes the total negative log likelihood of the test data and T the token counts.
2016) and sentence compression (Miao and Blunsom, 2016).",4 Related Work,[0],[0]
"Another interpretation of the proposed framework is from the perspective of guided policy search in reinforcement learning (Bachman and Precup, 2015), where a generative parser is trained to imitate the trace of a discriminative parser.",4 Related Work,[0],[0]
Further connections can be drawn with the importance-sampling based inference of Dyer et al. (2016).,4 Related Work,[0],[0]
"There, a generative RNNG and a discriminative RNNG are trained separately; during language modeling, the output of the discriminative model serves as the proposal distribution of an importance sampler p(x) = Eq(a|x) p(x,a) q(a|x) .",4 Related Work,[0],[0]
"Compared to their work, we unify the generative and discriminative RNNGs in a single framework, and adopt a joint training objective.",4 Related Work,[0],[0]
"We performed experiments on the English Penn Treebank dataset; we used sections 2–21 for training, 24 for validation, and 23 for testing.",5 Experiments,[0],[0]
"Following Dyer et al. (2015), we represent each word in three ways: as a learned vector, a pretrained vector, and a POS tag vector.",5 Experiments,[0],[0]
The encoder word embedding is the concatenation of all three vectors while the decoder uses only the first two since we do not consider POS tags in generation.,5 Experiments,[0],[0]
Table 1 presents details on the hyper-parameters we used.,5 Experiments,[0],[0]
To find the MAP parse tree argmaxa,5 Experiments,[0],[0]
"p(a, x) (where p(a, x) is used rank the output of q(a|x)) and to compute the language modeling perplexity with the evidence lower bound (where a ∼ q(a|x)), we collect 100 samples from q(a|x), same as Dyer et al. (2016).
",5 Experiments,[0],[0]
"Experimental results for constituency parsing and language modeling are shown in Tables 2 and 3, respectively.",5 Experiments,[0],[0]
"As can be seen, the single framework we propose obtains competitive pars-
ing performance.",5 Experiments,[0],[0]
"Comparing the two inference methods for parsing, ranking approximated MAP trees from q(a|x) with respect to p(a, x) yields a small improvement, as in Dyer et al. (2016).",5 Experiments,[0],[0]
It is worth noting that our parsing performance lags behind Dyer et al. (2016).,5 Experiments,[0],[0]
"We believe this is due to implementation disparities, such as the modeling of the reduce operation.",5 Experiments,[0],[0]
"While Dyer et al. (2016) use an LSTM as the syntactic composition function of each subtree, we adopt a rather simple composition function based on embedding averaging for memory concern.
",5 Experiments,[0],[0]
"On language modeling, our framework achieves lower perplexity compared to Dyer et al. (2016) and baseline models.",5 Experiments,[0],[0]
This gain possibly comes from the joint optimization of both the generative and discriminative components towards a language modeling objective.,5 Experiments,[0],[0]
"However, we acknowledge a subtle difference between Dyer et al. (2016) and our approach compared to baseline language models: while the latter incrementally estimate the next word probability, our approach (and Dyer et al. 2016) directly assigns probability to the entire sentence.",5 Experiments,[0],[0]
"Overall, the advantage of our framework compared to Dyer et al. (2016) is that it opens an avenue to unsupervised training.",5 Experiments,[0],[0]
We proposed a framework that integrates a generative parser with a discriminative recognition model and showed how it can be instantiated with RNNGs.,6 Conclusions,[0],[0]
"We demonstrated that a unified framework, which relates to expectation maximization and variational inference, enables effective parsing and language modeling algorithms.",6 Conclusions,[0],[0]
"Evaluation on the English Penn Treebank, revealed that our framework obtains competitive performance on constituency parsing and state-of-the-art results on single-model language modeling.",6 Conclusions,[0],[0]
"In the future, we would like to perform grammar induction based on Equation (8), with gradient descent and posterior regularization techniques (Ganchev et al., 2010).
",6 Conclusions,[0],[0]
"Acknowledgments We thank three anonymous reviewers and members of the ILCC for valuable feedback, and Muhua Zhu and James Cross for help with data preparation.",6 Conclusions,[0],[0]
The support of the European Research Council under award number 681760 “Translating Multiple Modalities into Text” is gratefully acknowledged.,6 Conclusions,[0],[0]
"(Dyer et al., 2016)
",A Comparison to Importance Sampling,[0],[0]
"In this appendix we highlight the connections between importance sampling and variational inference, thereby comparing our method with Dyer et al. (2016).
",A Comparison to Importance Sampling,[0],[0]
"Consider a simple directed graphical model with discrete latent variables a (e.g., a is the transition action sequence) and observed variables x (e.g., x is the sentence).",A Comparison to Importance Sampling,[0],[0]
"The model evidence, or the marginal likelihood p(x) = ∑ a p(x, a) is often intractable to compute.",A Comparison to Importance Sampling,[0],[0]
"Importance sampling transforms the above quantity into an expectation over a distribution q(a), which is known and easy to sample from:
p(x) = ∑
a
p(x, a) q(a)
q(a) =",A Comparison to Importance Sampling,[0],[0]
"Eq(a)w(x, a) (11)
where q(a) is the proposal distribution and w(x, a) = p(x,a)q(a)",A Comparison to Importance Sampling,[0],[0]
"the importance weight.
",A Comparison to Importance Sampling,[0],[0]
"The proposal distribution can potentially depend on the observations x, i.e., q(a) , q(a|x).
",A Comparison to Importance Sampling,[0],[0]
A challenge with importance sampling lies in choosing a proposal distribution which leads to low variance.,A Comparison to Importance Sampling,[0],[0]
"As shown in Rubinstein and Kroese (2008), the optimal choice of the proposal distribution is in fact the true posterior p(a|x), in which case the importance weight p(a,x)p(a|x) = p(x) is constant with respect to a.",A Comparison to Importance Sampling,[0],[0]
"In Dyer et al. (2016), the proposal distribution depends on x, i.e., q(a) , q(a|x), and is computed with a separately-trained, discriminative model.",A Comparison to Importance Sampling,[0],[0]
"This proposal choice is close to optimal, since in a fully supervised setting a is also observed and the discriminative model can be trained to approximate the true posterior well.",A Comparison to Importance Sampling,[0],[0]
We hypothesize that the performance of their importance sampler is dependent on this specific proposal distribution.,A Comparison to Importance Sampling,[0],[0]
"Besides, their training strategy does not generalize to an unsupervised setting.
",A Comparison to Importance Sampling,[0],[0]
"In comparison, variational inference approach approximates the log marginal likelihood log p(x) with the evidence lower bound.",A Comparison to Importance Sampling,[0],[0]
"It is a natural choice when one aims to optimize Equation (11) directly:
log p(x) = log ∑
a
p(x, a) q(a)
q(a)
≥ Eq(a) log p(x, a)
q(a)
(12)
where q(a) is the variational approximation of the true posterior.",A Comparison to Importance Sampling,[0],[0]
"Again, the variational approximation can potentially depend on the observation x (i.e., q(a) , q(a|x)) and can be computed with a discriminative model.",A Comparison to Importance Sampling,[0],[0]
"Equation (12) is a well-defined, unsupervised training objective which allows us to jointly optimize generative (i.e., p(x, a)) and discriminative (i.e., q(a|x)) models.",A Comparison to Importance Sampling,[0],[0]
"To further support the observed variable a, we augment this objective with supervised terms shown in Equation (10), following Kingma et al. (2014) and Miao and Blunsom (2016).
",A Comparison to Importance Sampling,[0],[0]
"Equation (12) can be also used to approximate the marginal likelihood p(x) (e.g., in language modeling) with its lower bound.",A Comparison to Importance Sampling,[0],[0]
An alternative choice without lower bounding is to use the variational approximation q(a|x) as the proposal distribution in importance sampling (Equation (11)).,A Comparison to Importance Sampling,[0],[0]
Ghahramani and Beal (2000) show that this proposal distribution leads to improved results of importance samplers.,A Comparison to Importance Sampling,[0],[0]
"However, a potential drawback of importance sampling-based approach is that it
is prone to numerical underflow.",A Comparison to Importance Sampling,[0],[0]
"In practice, we observed similar language modeling performance for both methods.",A Comparison to Importance Sampling,[0],[0]
"Generative models defining joint distributions over parse trees and sentences are useful for parsing and language modeling, but impose restrictions on the scope of features and are often outperformed by discriminative models.",abstractText,[0],[0]
We propose a framework for parsing and language modeling which marries a generative model with a discriminative recognition model in an encoder-decoder setting.,abstractText,[0],[0]
"We provide interpretations of the framework based on expectation maximization and variational inference, and show that it enables parsing and language modeling within a single implementation.",abstractText,[0],[0]
"On the English Penn Treenbank, our framework obtains competitive performance on constituency parsing while matching the state-of-the-art singlemodel language modeling score.1",abstractText,[0],[0]
A Generative Parser with a Discriminative Recognition Algorithm,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 1792–1801 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,[0],[0]
Conversation modeling has been a long interest of natural language research.,1 Introduction,[0],[0]
"Recent approaches for data-driven conversation modeling mostly build upon recurrent neural networks (RNNs) (Vinyals and Le, 2015; Sordoni et al., 2015b; Shang et al., 2015; Li et al., 2017; Serban et al., 2016).",1 Introduction,[0],[0]
Serban et al. (2016) use a hierarchical RNN structure to model the context of conversation.,1 Introduction,[0],[0]
"Serban et al. (2017) further exploit an utterance latent
variable in the hierarchical RNNs by incorporating the variational autoencoder (VAE) framework (Kingma and Welling, 2014; Rezende et al., 2014).
VAEs enable us to train a latent variable model for natural language modeling, which grants us several advantages.",1 Introduction,[0],[0]
"First, latent variables can learn an interpretable holistic representation, such as topics, tones, or high-level syntactic properties.",1 Introduction,[0],[0]
"Second, latent variables can model inherently abundant variability of natural language by encoding its global and long-term structure, which is hard to be captured by shallow generative processes (e.g. vanilla RNNs) where the only source of stochasticity comes from the sampling of output words.
",1 Introduction,[0],[0]
"In spite of such appealing properties of latent variable models for natural language modeling, VAEs suffer from the notorious degeneration problem (Bowman et al., 2016; Chen et al., 2017) that occurs when a VAE is combined with a powerful decoder such as autoregressive RNNs.",1 Introduction,[0],[0]
"This issue makes VAEs ignore latent variables, and eventually behave as vanilla RNNs.",1 Introduction,[0],[0]
"Chen et al. (2017) also note this degeneration issue by showing that a VAE with a RNN decoder prefers to model the data using its decoding distribution rather than using latent variables, from bits-back coding perspective.",1 Introduction,[0],[0]
"To resolve this issue, several heuristics have been proposed to weaken the decoder, enforcing the models to use latent variables.",1 Introduction,[0],[0]
"For example, Bowman et al. (2016) propose some heuristics, including KL annealing and word drop regularization.",1 Introduction,[0],[0]
"However, these heuristics cannot be a complete solution; for example, we observe that they fail to prevent the degeneracy in VHRED (Serban et al., 2017), a conditional VAE model equipped with hierarchical RNNs for conversation modeling.
",1 Introduction,[0],[0]
"The objective of this work is to propose a novel VAE model that significantly alleviates the degen-
1792
eration problem.",1 Introduction,[0],[0]
Our analysis reveals that the causes of the degeneracy are two-fold.,1 Introduction,[0],[0]
"First, the hierarchical structure of autoregressive RNNs is powerful enough to predict a sequence of utterances without the need of latent variables, even with the word drop regularization.",1 Introduction,[0],[0]
"Second, we newly discover that the conditional VAE structure where an utterance is generated conditioned on context, i.e. a previous sequence of utterances, induces severe data sparsity.",1 Introduction,[0],[0]
"Even with a large-scale training corpus, there only exist very few target utterances when conditioned on the context.",1 Introduction,[0],[0]
"Hence, the hierarchical RNNs can easily memorize the context-to-utterance relations without relying on latent variables.
",1 Introduction,[0],[0]
"We propose a novel model named Variational Hierarchical Conversation RNN (VHCR), which involves two novel features to alleviate this problem.",1 Introduction,[0],[0]
"First, we introduce a global conversational latent variable along with local utterance latent variables to build a hierarchical latent structure.",1 Introduction,[0],[0]
"Second, we propose a new regularization technique called utterance drop.",1 Introduction,[0],[0]
"We show that our hierarchical latent structure is not only crucial for facilitating the use of latent variables in conversation modeling, but also delivers several additional advantages, including gaining control over the global context in which the conversation takes place.
",1 Introduction,[0],[0]
"Our major contributions are as follows: (1) We reveal that the existing conditional VAE model with hierarchical RNNs for conversation modeling (e.g. (Serban et al., 2017)) still suffers from the degeneration problem, and this problem is caused by data sparsity per context that arises from the conditional VAE structure, as well as the use of powerful hierarchical RNN decoders.
",1 Introduction,[0],[0]
"(2) We propose a novel variational hierarchical conversation RNN (VHCR), which has two distinctive features: a hierarchical latent structure and a new regularization of utterance drop.",1 Introduction,[0],[0]
"To the best of our knowledge, our VHCR is the first VAE conversation model that exploits the hierarchical latent structure.
",1 Introduction,[1.0000000155776951],"['To the best of our knowledge, our VHCR is the first VAE conversation model that exploits the hierarchical latent structure.']"
"(3) With evaluations on two benchmark datasets of Cornell Movie Dialog (Danescu-NiculescuMizil and Lee, 2011) and Ubuntu Dialog Corpus (Lowe et al., 2015), we show that our model improves the conversation performance in multiple metrics over state-of-the-art methods, including HRED (Serban et al., 2016), and VHRED (Serban et al., 2017) with existing degeneracy solu-
tions such as the word drop (Bowman et al., 2016), and the bag-of-words loss (Zhao et al., 2017).",1 Introduction,[0],[0]
Conversation Modeling.,2 Related Work,[0],[0]
"One popular approach for conversation modeling is to use RNN-based encoders and decoders, such as (Vinyals and Le, 2015; Sordoni et al., 2015b; Shang et al., 2015).",2 Related Work,[0],[0]
"Hierarchical recurrent encoder-decoder (HRED) models (Sordoni et al., 2015a; Serban et al., 2016, 2017) consist of utterance encoder and decoder, and a context RNN which runs over utterance representations to model long-term temporal structure of conversation.
",2 Related Work,[0],[0]
"Recently, latent variable models such as VAEs have been adopted in language modeling (Bowman et al., 2016; Zhang et al., 2016; Serban et al., 2017).",2 Related Work,[0],[0]
"The VHRED model (Serban et al., 2017) integrates the VAE with the HRED to model Twitter and Ubuntu IRC conversations by introducing an utterance latent variable.",2 Related Work,[0],[0]
This makes a conditional VAE where the generation process is conditioned on the context of conversation.,2 Related Work,[0],[0]
"Zhao et al. (2017) further make use of discourse act labels to capture the diversity of conversations.
",2 Related Work,[0],[0]
Degeneracy of Variational Autoencoders.,2 Related Work,[0],[0]
"For sequence modeling, VAEs are often merged with the RNN encoder-decoder structure (Bowman et al., 2016; Serban et al., 2017; Zhao et al., 2017) where the encoder predicts the posterior distribution of a latent variable z, and the decoder models the output distributions conditioned on z.",2 Related Work,[0],[0]
"However, Bowman et al. (2016) report that a VAE with a RNN decoder easily degenerates; that is, it learns to ignore the latent variable z and falls back to a vanilla RNN.",2 Related Work,[0],[0]
They propose two techniques to alleviate this issue: KL annealing and word drop.,2 Related Work,[0],[0]
Chen et al. (2017) interpret this degeneracy in the context of bits-back coding and show that a VAE equipped with autoregressive models such as RNNs often ignores the latent variable to minimize the code length needed for describing data.,2 Related Work,[0],[0]
They propose to constrain the decoder to selectively encode the information of interest in the latent variable.,2 Related Work,[0],[0]
"However, their empirical results are limited to an image domain.",2 Related Work,[0],[0]
"Zhao et al. (2017) use an auxiliary bag-of-words loss on the latent variable to force the model to use z. That is, they train an auxiliary network that predicts bag-of-words representation of the target utterance based on z.",2 Related Work,[0],[0]
"Yet this loss works in an opposite di-
rection to the original objective of VAEs that minimizes the minimum description length.",2 Related Work,[0],[0]
"Thus, it may be in danger of forcibly moving the information that is better modeled in the decoder to the latent variable.",2 Related Work,[0],[0]
"We assume that the training set consists of N i.i.d samples of conversations {c1, c2, ..., cN} where each ci is a sequence of utterances (i.e. sentences) {xi1,xi2, ...,xini}.",3 Approach,[0],[0]
"Our objective is to learn the parameters of a generative network θ using Maximum Likelihood Estimation (MLE):
argmax θ
∑
i
log pθ(ci) (1)
We first briefly review the VAE, and explain the degeneracy issue before presenting our model.",3 Approach,[0],[0]
We follow the notion of Kingma and Welling (2014).,3.1 Preliminary: Variational Autoencoder,[0],[0]
"A datapoint x is generated from a latent variable z, which is sampled from some prior distribution p(z), typically a standard Gaussian distribution N (z|0, I).",3.1 Preliminary: Variational Autoencoder,[1.0],"['A datapoint x is generated from a latent variable z, which is sampled from some prior distribution p(z), typically a standard Gaussian distribution N (z|0, I).']"
We assume parametric families for conditional distribution pθ(x|z).,3.1 Preliminary: Variational Autoencoder,[1.0],['We assume parametric families for conditional distribution pθ(x|z).']
"Since it is intractable to compute the log-marginal likelihood log pθ(x), we approximate the intractable true posterior pθ(z|x) with a recognition model qφ(z|x) to maximize the variational lower-bound:
log pθ(x) ≥",3.1 Preliminary: Variational Autoencoder,[0],[0]
"L(θ,φ;x) (2) = Eqφ(z|x)[− log qφ(z|x) + log pθ(x, z)]",3.1 Preliminary: Variational Autoencoder,[0],[0]
"= −DKL(qφ(z|x)‖p(z))+Eqφ(z|x)[log pθ(x|z)]
Eq. 2 is decomposed into two terms: KL divergence term and reconstruction term.",3.1 Preliminary: Variational Autoencoder,[0],[0]
"Here, KL divergence measures the amount of information encoded in the latent variable z.",3.1 Preliminary: Variational Autoencoder,[1.0],"['Here, KL divergence measures the amount of information encoded in the latent variable z.']"
"In the extreme where KL divergence is zero, the model completely ignores z, i.e. it degenerates.",3.1 Preliminary: Variational Autoencoder,[0],[0]
The expectation term can be stochastically approximated by sampling z from the variational posterior qφ(z|x).,3.1 Preliminary: Variational Autoencoder,[0],[0]
"The gradients to the recognition model can be efficiently estimated using the reparameterization trick (Kingma and Welling, 2014).",3.1 Preliminary: Variational Autoencoder,[0],[0]
"Serban et al. (2017) propose Variational Hierarchical Recurrent Encoder Decoder (VHRED) model
for conversation modeling.",3.2 VHRED,[1.0000000049703526],['Serban et al. (2017) propose Variational Hierarchical Recurrent Encoder Decoder (VHRED) model for conversation modeling.']
"It integrates an utterance latent variable zuttt into the HRED structure (Sordoni et al., 2015a) which consists of three RNN components: encoder RNN, context RNN, and decoder RNN.",3.2 VHRED,[1.0],"['It integrates an utterance latent variable zuttt into the HRED structure (Sordoni et al., 2015a) which consists of three RNN components: encoder RNN, context RNN, and decoder RNN.']"
"Given a previous sequence of utterances x1, ...xt−1 in a conversation, the VHRED generates the next utterance xt as:
henct−1 = f enc θ (xt−1) (3)
hcxtt = f cxt θ (h cxt t−1,h enc t−1) (4)
pθ(z utt t |x<t) = N",3.2 VHRED,[0],[0]
"(z|µt,σtI) (5)
where µt = MLPθ(h cxt t )",3.2 VHRED,[0],[0]
"(6)
σt = Softplus(MLPθ(h cxt t ))",3.2 VHRED,[0],[0]
"(7)
pθ(xt|x<t) = fdecθ (x|hcxtt , zuttt ) (8)
At time step t, the encoder RNN f encθ takes the previous utterance xt−1 and produces an encoder vector henct−1 (Eq. 3).",3.2 VHRED,[0],[0]
The context RNN f cxt θ models the context of the conversation by updating its hidden states using the encoder vector (Eq. 4).,3.2 VHRED,[0],[0]
"The context hcxtt defines the conditional prior pθ(z utt t |x<t), which is a factorized Gaussian distribution whose mean µt and diagonal variance σt are given by feed-forward neural networks (Eq. 5-7).",3.2 VHRED,[0],[0]
"Finally the decoder RNN fdecθ generates the utterance xt, conditioned on the context vector hcxtt and the latent variable zuttt (Eq. 8).",3.2 VHRED,[0.9925631213645331],"['Finally the decoder RNN fdecθ generates the utterance xt, conditioned on the context vector hcxtt and the latent variable zuttt (Eq.']"
"We make two important notes: (1) the context RNN can be viewed as a high-level decoder, and together with the decoder RNN, they comprise a hierarchical RNN decoder.",3.2 VHRED,[0],[0]
"(2) VHRED follows a conditional VAE structure where each utterance xt is generated conditioned on the context hcxtt (Eq. 5-8).
",3.2 VHRED,[0],[0]
"The variational posterior is a factorized Gaussian distribution where the mean and the diagonal variance are predicted from the target utterance and the context as follows:
qφ(z utt t |x≤t) = N (z|µ′t,σ′tI) (9)
where µ′t = MLPφ(xt,h cxt t ) (10)
σ′t = Softplus(MLPφ(xt,h cxt t ))",3.2 VHRED,[0],[0]
(11),3.2 VHRED,[0],[0]
A known problem of a VAE that incorporates an autoregressive RNN decoder is the degeneracy that ignores the latent variable z.,3.3 The Degeneration Problem,[1.0],['A known problem of a VAE that incorporates an autoregressive RNN decoder is the degeneracy that ignores the latent variable z.']
"In other words, the KL divergence term in Eq. 2 goes to zero and the decoder fails to learn any dependency between the latent variable and the data.",3.3 The Degeneration Problem,[0],[0]
"Eventually, the model behaves as a vanilla RNN.",3.3 The Degeneration Problem,[1.0],"['Eventually, the model behaves as a vanilla RNN.']"
"This problem is
first reported in the sentence VAE (Bowman et al., 2016), in which following two heuristics are proposed to alleviate the problem by weakening the decoder.
",3.3 The Degeneration Problem,[0],[0]
"First, the KL annealing scales the KL divergence term of Eq.",3.3 The Degeneration Problem,[0],[0]
"2 using a KL multiplier λ, which gradually increases from 0 to 1 during training:
L̃(θ,φ;x) = −λDKL(qφ(z|x)‖p(z))",3.3 The Degeneration Problem,[0],[0]
"(12) +Eqφ(z|x)[log pθ(x|z)]
This helps the optimization process to avoid local optima of zero KL divergence in early training.",3.3 The Degeneration Problem,[0],[0]
"Second, the word drop regularization randomly replaces some conditioned-on word tokens in the RNN decoder with the generic unknown word token (UNK) during training.",3.3 The Degeneration Problem,[0],[0]
"Normally, the RNN decoder predicts each next word in an autoregressive manner, conditioned on the previous sequence of ground truth (GT) words.",3.3 The Degeneration Problem,[0],[0]
"By randomly replacing a GT word with an UNK token, the word drop regularization weakens the autoregressive power of the decoder and forces it to rely on the latent variable to predict the next word.",3.3 The Degeneration Problem,[0],[0]
"The word drop probability is normally set to 0.25, since using a higher probability may degrade the model performance (Bowman et al., 2016).
",3.3 The Degeneration Problem,[1.0000000771710016],"['The word drop probability is normally set to 0.25, since using a higher probability may degrade the model performance (Bowman et al., 2016).']"
"However, we observe that these tricks do not solve the degeneracy for the VHRED in conversation modeling.",3.3 The Degeneration Problem,[0],[0]
An example in Fig. 1 shows that the VHRED learns to ignore the utterance latent variable as the KL divergence term falls to zero.,3.3 The Degeneration Problem,[0],[0]
"The decoder RNN of the VHRED in Eq. 8 conditions on two information sources: deterministic hcxtt and stochastic z
utt.",3.4 Empirical Observation on Degeneracy,[0],[0]
"In order to check whether the presence of deterministic source hcxtt causes the degeneration, we drop the deterministic hcxtt and condition the decoder only on the stochastic utterance latent variable zutt:
pθ(xt|x<t) = fdecθ (x|zuttt )",3.4 Empirical Observation on Degeneracy,[0],[0]
"(13)
While this model achieves higher values of KL divergence than original VHRED, as training proceeds it again degenerates with the KL divergence term reaching zero (Fig. 2).
",3.4 Empirical Observation on Degeneracy,[0],[0]
"To gain an insight of the degeneracy, we examine how the conditional prior pθ(zuttt |x<t) (Eq. 5) of the utterance latent variable changes during training, using the model above (Eq. 13).",3.4 Empirical Observation on Degeneracy,[0],[0]
"Fig. 2 plots the ratios of E[σ2t ]/Var(µt), where E[σ2t ] indicates the within variance of the priors, and Var(µt) is the between variance of the priors.",3.4 Empirical Observation on Degeneracy,[0.9976246901129063],"['2 plots the ratios of E[σ2t ]/Var(µt), where E[σ2t ] indicates the within variance of the priors, and Var(µt) is the between variance of the priors.']"
"Note that traditionally this ratio is closely related to Analysis of Variance (ANOVA) (Lomax and Hahs-Vaughn, 2013).",3.4 Empirical Observation on Degeneracy,[0],[0]
"The ratio gradually falls to zero, implying that the priors degenerate to separate point masses as training proceeds.",3.4 Empirical Observation on Degeneracy,[1.0],"['The ratio gradually falls to zero, implying that the priors degenerate to separate point masses as training proceeds.']"
"Moreover, we find that the degeneracy of priors coincide with the degeneracy of KL divergence, as shown in (Fig. 2).",3.4 Empirical Observation on Degeneracy,[0],[0]
"This is intuitively natural: if the prior is already narrow enough to specify the target utterance, there is little pressure to encode any more information in the variational posterior for reconstruction of the target utterance.
",3.4 Empirical Observation on Degeneracy,[0],[0]
This empirical observation implies that the fundamental reason behind the degeneration may originate from combination of two factors: (1) strong expressive power of the hierarchical RNN decoder and (2) training data sparsity caused by the conditional VAE structure.,3.4 Empirical Observation on Degeneracy,[0],[0]
"The VHRED is trained to predict a next target utterance xt conditioned on the context hcxtt which encodes information about previous utterances {x1, . . .",3.4 Empirical Observation on Degeneracy,[0],[0]
",xt−1}.",3.4 Empirical Observation on Degeneracy,[0],[0]
"However, conditioning on the context makes the range of training target xt very sparse; even in a large-scale conversation corpus such as Ubuntu Dialog (Lowe et al., 2015), there exist one or very few target utterances per context.",3.4 Empirical Observation on Degeneracy,[1.0],"['However, conditioning on the context makes the range of training target xt very sparse; even in a large-scale conversation corpus such as Ubuntu Dialog (Lowe et al., 2015), there exist one or very few target utterances per context.']"
"Therefore, hierarchical RNNs, given their autoregressive power, can easily overfit to training data without using the latent variable.",3.4 Empirical Observation on Degeneracy,[0],[0]
"Consequently, the VHRED will not encode any information in the latent variable, i.e. it degenerates.",3.4 Empirical Observation on Degeneracy,[1.0],"['Consequently, the VHRED will not encode any information in the latent variable, i.e. it degenerates.']"
It explains why the word drop fails to prevent the degeneracy in the VHRED.,3.4 Empirical Observation on Degeneracy,[1.0],['It explains why the word drop fails to prevent the degeneracy in the VHRED.']
"The word drop only regularizes the decoder RNN; however, the context RNN is also powerful enough to predict a next utterance in a given context even with the weakened decoder RNN.",3.4 Empirical Observation on Degeneracy,[1.0],"['The word drop only regularizes the decoder RNN; however, the context RNN is also powerful enough to predict a next utterance in a given context even with the weakened decoder RNN.']"
"Indeed we observe that using a larger word drop probability such as 0.5 or 0.75 only slows down, but fails to stop the KL divergence from vanishing.",3.4 Empirical Observation on Degeneracy,[0],[0]
"As discussed, we argue that the two main causes of degeneration are i) the expressiveness of the hierarchical RNN decoders, and ii) the conditional VAE structure that induces data sparsity.",3.5 Variational Hierarchical Conversation RNN (VHCR),[0],[0]
"This finding hints us that in order to train a nondegenerate latent variable model, we need to design a model that provides an appropriate way to
regularize the hierarchical RNN decoders and alleviate data sparsity per context.",3.5 Variational Hierarchical Conversation RNN (VHCR),[1.0000000740270778],"['This finding hints us that in order to train a nondegenerate latent variable model, we need to design a model that provides an appropriate way to regularize the hierarchical RNN decoders and alleviate data sparsity per context.']"
"At the same time, the model should be capable of modeling complex structure of conversation.",3.5 Variational Hierarchical Conversation RNN (VHCR),[0],[0]
"Based on these insights, we propose a novel VAE structure named Variational Hierarchical Conversation RNN (VHCR), whose graphical model is illustrated in Fig. 3.",3.5 Variational Hierarchical Conversation RNN (VHCR),[0],[0]
"Below we first describe the model, and discuss its unique features.
",3.5 Variational Hierarchical Conversation RNN (VHCR),[0],[0]
"We introduce a global conversation latent variable zconv which is responsible for generating a sequence of utterances of a conversation c = {x1, . . .",3.5 Variational Hierarchical Conversation RNN (VHCR),[0.9937936365917862],"['We introduce a global conversation latent variable zconv which is responsible for generating a sequence of utterances of a conversation c = {x1, .']"
",xn}:
pθ(c|zconv) = pθ(x1, . . .",3.5 Variational Hierarchical Conversation RNN (VHCR),[0],[0]
",xn|zconv)",3.5 Variational Hierarchical Conversation RNN (VHCR),[0],[0]
"(14)
Overall, the VHCR builds upon the hierarchical RNNs, following the VHRED (Serban et al., 2017).",3.5 Variational Hierarchical Conversation RNN (VHCR),[0],[0]
"One key update is to form a hierarchical latent structure, by using the global latent variable zconv per conversation, along with local the latent variable zuttt injected at each utterance (Fig. 3):
henct = f enc θ (xt) (15)
hcxtt = { MLPθ(zconv), if t = 0 f cxtθ (h cxt t−1,h enc t−1, z conv), otherwise pθ(xt|x<t, zuttt , zconv) =",3.5 Variational Hierarchical Conversation RNN (VHCR),[0],[0]
"fdecθ (x|hcxtt , zuttt , zconv) pθ(z
conv) = N",3.5 Variational Hierarchical Conversation RNN (VHCR),[0],[0]
"(z|0, I) (16) pθ(z utt t |x<t, zconv)",3.5 Variational Hierarchical Conversation RNN (VHCR),[0],[0]
"= N (z|µt,σtI) (17)
where µt = MLPθ(h cxt t , z conv) (18)
σt = Softplus(MLPθ(h cxt t , z conv)).",3.5 Variational Hierarchical Conversation RNN (VHCR),[0],[0]
"(19)
",3.5 Variational Hierarchical Conversation RNN (VHCR),[0],[0]
"For inference of zconv, we use a bi-directional RNN denoted by f conv, which runs over the utterance vectors generated by the encoder RNN:
qφ(z conv|x1, ...,xn) = N (z|µconv,σconvI) (20)
where hconv = f conv(henc1 , ...,h enc n )",3.5 Variational Hierarchical Conversation RNN (VHCR),[0],[0]
"(21)
µconv = MLPφ(hconv) (22) σconv = Softplus(MLPφ(hconv)).",3.5 Variational Hierarchical Conversation RNN (VHCR),[0],[0]
"(23)
",3.5 Variational Hierarchical Conversation RNN (VHCR),[0],[0]
"The posteriors for local variables zuttt are then conditioned on zconv:
qφ(z utt t |x1, ...,xn, zconv)",3.5 Variational Hierarchical Conversation RNN (VHCR),[0],[0]
"= N (z|µ′t,σ′tI) (24)
where µ′t = MLPφ(xt,h cxt t , z conv) (25)
σ′t = Softplus(MLPφ(xt,h cxt t , z conv)).
",3.5 Variational Hierarchical Conversation RNN (VHCR),[0],[0]
Our solution of VHCR to the degeneration problem is based on two ideas.,3.5 Variational Hierarchical Conversation RNN (VHCR),[0],[0]
"The first idea is to build a hierarchical latent structure of zconv for
a conversation and zuttt for each utterance.",3.5 Variational Hierarchical Conversation RNN (VHCR),[0],[0]
"As z conv is independent of the conditional structure, it does not suffer from the data sparsity problem.",3.5 Variational Hierarchical Conversation RNN (VHCR),[0],[0]
"However, the expressive power of hierarchical RNN decoders makes the model still prone to ignore latent variables zconv and zuttt .",3.5 Variational Hierarchical Conversation RNN (VHCR),[0],[0]
"Therefore, our second idea is to apply an utterance drop regularization to effectively regularize the hierarchical RNNs, in order to facilitate the use of latent variables.",3.5 Variational Hierarchical Conversation RNN (VHCR),[0],[0]
"That is, at each time step, the utterance encoder vector henct is randomly replaced with a generic unknown vector hunk with a probability p.",3.5 Variational Hierarchical Conversation RNN (VHCR),[0],[0]
"This regularization weakens the autoregressive power of hierarchical RNNs and as well alleviates the data sparsity problem, since it induces noise into the context vector hcxtt which conditions the decoder RNN.",3.5 Variational Hierarchical Conversation RNN (VHCR),[0],[0]
"The difference with the word drop (Bowman et al., 2016) is that our utterance drop depresses the hierarchical RNN decoders as a whole, while the word drop only weakens the lower-level decoder RNNs.",3.5 Variational Hierarchical Conversation RNN (VHCR),[0],[0]
"Fig. 4 confirms that with the utterance drop with a probability of 0.25, the VHCR effectively learns to use latent variables, achieving a significant degree of KL divergence.",3.5 Variational Hierarchical Conversation RNN (VHCR),[0],[0]
Is the hierarchical latent structure of the VHCR crucial for effective utilization of latent variables?,3.6 Effectiveness of Hierarchical Latent Structure,[0],[0]
We investigate this question by applying the utterance drop on the VHRED which lacks any hierarchical latent structure.,3.6 Effectiveness of Hierarchical Latent Structure,[0],[0]
"We observe that the KL divergence still vanishes (Fig. 4), even though
the utterance drop injects considerable noise in the context hcxtt .",3.6 Effectiveness of Hierarchical Latent Structure,[0],[0]
"We argue that the utterance drop weakens the context RNN, thus it consequently fail to predict a reasonable prior distribution for zutt (Eq. 5-7).",3.6 Effectiveness of Hierarchical Latent Structure,[0],[0]
"If the prior is far away from the region of zutt that can generate a correct target utterance, encoding information about the target in the variational posterior will incur a large KL divergence penalty.",3.6 Effectiveness of Hierarchical Latent Structure,[0],[0]
"If the penalty outweighs the gain of the reconstruction term in Eq. 2, then the model would learn to ignore zutt, in order to maximize the variational lower-bound in Eq. 2.
",3.6 Effectiveness of Hierarchical Latent Structure,[0],[0]
"On the other hand, the global variable zconv allows the VHCR to predict a reasonable prior for local variable zuttt even in the presence of the utterance drop regularization.",3.6 Effectiveness of Hierarchical Latent Structure,[0],[0]
"That is, zconv can act as a guide for zutt by encoding the information for local variables.",3.6 Effectiveness of Hierarchical Latent Structure,[0],[0]
This reduces the KL divergence penalty induced by encoding information in zutt to an affordable degree at the cost of KL divergence caused by using zconv.,3.6 Effectiveness of Hierarchical Latent Structure,[0],[0]
"This trade-off is indeed a fundamental strength of hierarchical models that provide parsimonious representation; if there exists any shared information among the local variables, it is coded in the global latent variable reducing the code length by effectively reusing the information.",3.6 Effectiveness of Hierarchical Latent Structure,[0],[0]
"The remaining local variability is handled properly by the decoding distribution and local latent variables.
",3.6 Effectiveness of Hierarchical Latent Structure,[0],[0]
"The global variable zconv provides other benefits by representing a latent global structure of a conversation, such as a topic, a length, and a tone of the conversation.",3.6 Effectiveness of Hierarchical Latent Structure,[0],[0]
"Moreover, it allows us to control such global properties, which is impossible for models without hierarchical latent structure.",3.6 Effectiveness of Hierarchical Latent Structure,[1.0],"['Moreover, it allows us to control such global properties, which is impossible for models without hierarchical latent structure.']"
"We first describe our experimental setting, such as datasets and baselines (section 4.1).",4 Results,[0],[0]
We then report quantitative comparisons using three different metrics (section 4.2–4.4).,4 Results,[0],[0]
"Finally, we present qualitative analyses, including several utterance control tasks that are enabled by the hierarchal latent structure of our VHCR (section 4.5).",4 Results,[0],[0]
We defer implementation details and additional experiment results to the supplementary file.,4 Results,[0],[0]
Datasets.,4.1 Experimental Setting,[0],[0]
"We evaluate the performance of conversation generation using two benchmark datasets: 1) Cornell Movie Dialog Corpus (Danescu-
Niculescu-Mizil and Lee, 2011), containing 220,579 conversations from 617 movies.",4.1 Experimental Setting,[0],[0]
"2) Ubuntu Dialog Corpus (Lowe et al., 2015), containing about 1 million multi-turn conversations from Ubuntu IRC channels.",4.1 Experimental Setting,[1.0],"['2) Ubuntu Dialog Corpus (Lowe et al., 2015), containing about 1 million multi-turn conversations from Ubuntu IRC channels.']"
"In both datasets, we truncate utterances longer than 30 words.
Baselines.",4.1 Experimental Setting,[0],[0]
We compare our approach with four baselines.,4.1 Experimental Setting,[0],[0]
They are combinations of two state-ofthe-art models of conversation generation with different solutions to the degeneracy.,4.1 Experimental Setting,[0],[0]
(i),4.1 Experimental Setting,[0],[0]
Hierarchical recurrent encoder-decoder (HRED),4.1 Experimental Setting,[0],[0]
"(Serban et al., 2016), (ii) Variational HRED (VHRED) (Serban et al., 2017), (iii) VHRED with the word drop (Bowman et al., 2016), and (iv) VHRED with the bag-of-words (bow) loss (Zhao et al., 2017).
",4.1 Experimental Setting,[0],[0]
Performance Measures.,4.1 Experimental Setting,[0],[0]
"Automatic evaluation of conversational systems is still a challenging problem (Liu et al., 2016).",4.1 Experimental Setting,[0],[0]
"Based on literature, we report three quantitative metrics: i) the negative log-likelihood (the variational bound for variational models), ii) embedding-based metrics (Serban et al., 2017), and iii) human evaluation via Amazon Mechanical Turk (AMT).",4.1 Experimental Setting,[0],[0]
Table 1 summarizes the per-word negative loglikelihood (NLL) evaluated on the test sets of two datasets.,4.2 Results of Negative Log-likelihood,[0],[0]
"For variational models, we instead present the variational bound of the negative loglikelihood in Eq. 2, which consists of the reconstruction error term and the KL divergence term.",4.2 Results of Negative Log-likelihood,[0],[0]
"The KL divergence term can measure how much each model utilizes the latent variables.
",4.2 Results of Negative Log-likelihood,[0],[0]
We observe that the NLL is the lowest by the HRED.,4.2 Results of Negative Log-likelihood,[0],[0]
"Variational models show higher NLLs, because they are regularized methods that are forced to rely more on latent variables.",4.2 Results of Negative Log-likelihood,[0],[0]
"Independent of NLL values, we later show that the latent variable models often show better generalization performance in terms of embedding-based metrics and human evaluation.",4.2 Results of Negative Log-likelihood,[0],[0]
"In the VHRED, the KL divergence term gradually vanishes even with the word drop regularization; thus, early stopping is necessary to obtain a meaningful KL divergence.",4.2 Results of Negative Log-likelihood,[0],[0]
"The VHRED with the bag-of-words loss (bow) achieves the highest KL divergence, however, at the cost of high NLL values.",4.2 Results of Negative Log-likelihood,[0],[0]
"That is, the variational lower-bound minimizes the minimum description length, to which the bow loss works in an opposite direction by forcing latent variables to encode bag-of-words representation of utterances.",4.2 Results of Negative Log-likelihood,[0],[0]
"Our VHCR achieves stable KL divergence without any auxiliary objective, and the NLL is lower than the VHRED + bow model.
",4.2 Results of Negative Log-likelihood,[0],[0]
Table 2 summarizes how global and latent variable are used in the VHCR.,4.2 Results of Negative Log-likelihood,[0],[0]
"We observe that VHCR encodes a significant amount of information in the global variable zconv as well as in the local variable zutt, indicating that the VHCR successfully exploits its hierarchical latent structure.",4.2 Results of Negative Log-likelihood,[0],[0]
"The embedding-based metrics (Serban et al., 2017; Rus and Lintean, 2012) measure the textual similarity between the words in the model response and the ground truth.",4.3 Results of Embedding-Based Metrics,[0],[0]
We represent words using Word2Vec embeddings trained on the Google News Corpus1.,4.3 Results of Embedding-Based Metrics,[0],[0]
"The average metric projects each utterance to a vector by taking the mean over word embeddings in the utterance, and computes the cosine similarity between the model response vector and the ground truth vector.",4.3 Results of Embedding-Based Metrics,[1.0],"['The average metric projects each utterance to a vector by taking the mean over word embeddings in the utterance, and computes the cosine similarity between the model response vector and the ground truth vector.']"
"The extrema metric is similar to the average metric, only except that it takes the extremum of each di-
1https://code.google.com/archive/p/word2vec/.
mension, instead of the mean.",4.3 Results of Embedding-Based Metrics,[0],[0]
"The greedy metric first finds the best non-exclusive word alignment between the model response and the ground truth, and then computes the mean over the cosine similarity between the aligned words.
",4.3 Results of Embedding-Based Metrics,[0],[0]
Table 3 compares the different methods with three embedding-based metrics.,4.3 Results of Embedding-Based Metrics,[0],[0]
Each model generates a single response (1-turn) or consecutive three responses (3-turn) for a given context.,4.3 Results of Embedding-Based Metrics,[0],[0]
"For 3-turn cases, we report the average of metrics measured for three turns.",4.3 Results of Embedding-Based Metrics,[0],[0]
"We use the greedy decoding for all the models.
",4.3 Results of Embedding-Based Metrics,[0],[0]
Our VHCR achieves the best results in most metrics.,4.3 Results of Embedding-Based Metrics,[0],[0]
"The HRED is the worst on the Cornell Movie dataset, but outperforms the VHRED and VHRED + bow on the Ubuntu Dialog dataset.",4.3 Results of Embedding-Based Metrics,[0],[0]
"Although the VHRED + bow shows the highest KL divergence, its performance is similar to that of VHRED, and worse than that of the VHCR model.",4.3 Results of Embedding-Based Metrics,[0],[0]
It suggests that a higher KL divergence does not necessarily lead to better performance; it is more important for the models to balance the modeling powers of the decoder and the latent variables.,4.3 Results of Embedding-Based Metrics,[0],[0]
"The VHCR uses a more sophisticated hierarchical latent structure, which better reflects the structure of
natural language conversations.",4.3 Results of Embedding-Based Metrics,[0],[0]
Table 4 reports human evaluation results via Amazon Mechanical Turk (AMT).,4.4 Results of Human Evaluation,[0],[0]
The VHCR outperforms the baselines in both datasets; yet the performance improvement in Cornell Movie Dialog are less significant compared to that of Ubuntu.,4.4 Results of Human Evaluation,[0],[0]
"We empirically find that Cornell Movie dataset is small in size, but very diverse and complex in content and style, and the models often fail to generate sensible responses for the context.",4.4 Results of Human Evaluation,[0],[0]
"The performance gap with the HRED is the smallest, suggesting that the VAE models without hierarchical latent structure have overfitted to Cornell Movie dataset.",4.4 Results of Human Evaluation,[0],[0]
Comparison of Predicted Responses.,4.5 Qualitative Analyses,[0],[0]
Table 5 compares the generated responses of algorithms.,4.5 Qualitative Analyses,[0],[0]
"Overall, the VHCR creates more consistent responses within the context of a given conversation.",4.5 Qualitative Analyses,[0],[0]
This is supposedly due to the global latent variable zconv that provides a more direct and effective way to handle the global context of a conversation.,4.5 Qualitative Analyses,[0],[0]
"The context RNN of the baseline models can handle long-term context to some extent, but not as much as the VHCR.
Interpolation on zconv.",4.5 Qualitative Analyses,[0],[0]
"We present examples of one advantage by the hierarchical latent structure of the VHCR, which cannot be done by the other existing models.",4.5 Qualitative Analyses,[0],[0]
Table 6 shows how the generated responses vary according to the interpolation on zconv.,4.5 Qualitative Analyses,[0],[0]
"We randomly sample two zconv from a standard Gaussian prior as references (i.e. the top and the bottom row of Table 6), and interpolate points between them.",4.5 Qualitative Analyses,[0],[0]
We generate 3-turn conversations conditioned on given zconv.,4.5 Qualitative Analyses,[0],[0]
"We see that zconv controls the overall tone and content of conversations; for example, the tone of the response is friendly in the first sample, but gradually becomes hostile as zconv changes.
",4.5 Qualitative Analyses,[1.0000000134294431],"['We see that zconv controls the overall tone and content of conversations; for example, the tone of the response is friendly in the first sample, but gradually becomes hostile as zconv changes.']"
Generation on a Fixed zconv.,4.5 Qualitative Analyses,[0],[0]
We also study how fixing a global conversation latent variable zconv affects the conversation generation.,4.5 Qualitative Analyses,[0],[0]
"Table 7 shows an example, where we randomly fix a reference zconv from the prior, and generate multiple examples of 3-turn conversation using randomly sampled local variables zutt.",4.5 Qualitative Analyses,[0],[0]
"We observe that zconv heavily affects the form of the first utterance; in the examples, the first utterances all start with a “where” phrase.",4.5 Qualitative Analyses,[0],[0]
"At the same time, responses show
variations according to different local variables zutt.",4.5 Qualitative Analyses,[0],[0]
These examples show that the hierarchical latent structure of VHCR allows both global and fine-grained control over generated conversations.,4.5 Qualitative Analyses,[0],[0]
We introduced the variational hierarchical conversation RNN (VHCR) for conversation modeling.,5 Discussion,[0],[0]
"We noted that the degeneration problem in existing VAE models such as the VHRED is persistent, and proposed a hierarchical latent variable model with the utterance drop regularization.",5 Discussion,[1.0],"['We noted that the degeneration problem in existing VAE models such as the VHRED is persistent, and proposed a hierarchical latent variable model with the utterance drop regularization.']"
Our VHCR obtained higher and more stable KL divergences than various versions of VHRED models without using any auxiliary objective.,5 Discussion,[1.0],['Our VHCR obtained higher and more stable KL divergences than various versions of VHRED models without using any auxiliary objective.']
"The empir-
ical results showed that the VHCR better reflected the structure of natural conversations, and outperformed previous models.",5 Discussion,[1.000000046584961],"['The empir- ical results showed that the VHCR better reflected the structure of natural conversations, and outperformed previous models.']"
"Moreover, the hierarchical latent structure allowed both global and finegrained control over the conversation generation.",5 Discussion,[0],[0]
"This work was supported by Kakao and Kakao Brain corporations, and Creative-Pioneering Researchers Program through Seoul National University.",Acknowledgments,[0],[0]
Gunhee Kim is the corresponding author.,Acknowledgments,[0],[0]
Variational autoencoders (VAE) combined with hierarchical RNNs have emerged as a powerful framework for conversation modeling.,abstractText,[0],[0]
"However, they suffer from the notorious degeneration problem, where the decoders learn to ignore latent variables and reduce to vanilla RNNs.",abstractText,[0],[0]
We empirically show that this degeneracy occurs mostly due to two reasons.,abstractText,[0],[0]
"First, the expressive power of hierarchical RNN decoders is often high enough to model the data using only its decoding distributions without relying on the latent variables.",abstractText,[0],[0]
"Second, the conditional VAE structure whose generation process is conditioned on a context, makes the range of training targets very sparse; that is, the RNN decoders can easily overfit to the training data ignoring the latent variables.",abstractText,[0],[0]
"To solve the degeneration problem, we propose a novel model named Variational Hierarchical Conversation RNNs (VHCR), involving two key ideas of (1) using a hierarchical structure of latent variables, and (2) exploiting an utterance drop regularization.",abstractText,[0],[0]
"With evaluations on two datasets of Cornell Movie Dialog and Ubuntu Dialog Corpus, we show that our VHCR successfully utilizes latent variables and outperforms state-of-the-art models for conversation generation.",abstractText,[0],[0]
"Moreover, it can perform several new utterance control tasks, thanks to its hierarchical latent structure.",abstractText,[0],[0]
A Hierarchical Latent Structure for Variational Conversation Modeling,title,[0],[0]
