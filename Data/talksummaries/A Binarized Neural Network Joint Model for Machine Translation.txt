0	23	Neural network translation models, which learn mappings over real-valued vector representations in high-dimensional space, have recently achieved large gains in translation accuracy (Hu et al., 2014; Devlin et al., 2014; Sundermeyer et al., 2014; Auli et al., 2013; Schwenk, 2012; Sutskever et al., 2014; Bahdanau et al., 2015).
1	39	Notably, Devlin et al. (2014) proposed a neural network joint model (NNJM), which augments the n-gram neural network language model (NNLM) with an m-word source context window, as shown in Figure 1a.
2	6	While this model is effective, the computation cost of using it in a large-vocabulary SMT task is quite expensive, as probabilities need to be normalized over the entire vocabulary.
3	4	To solve this problem, Devlin et al. (2014) presented a technique to train the NNJM to be selfnormalized and avoided the expensive normalization cost during decoding.
5	5	To remedy the problem of long training times in the context of NNLMs, Vaswani et al. (2013) used a method called noise contrastive estimation (NCE).
6	11	Compared with MLE, NCE does not require repeated summations over the whole vocabulary and performs nonlinear logistic regression to discriminate between the observed data and artificially generated noise.
7	46	This paper proposes an alternative framework of binarized NNJMs (BNNJM), which are similar to the NNJM, but use the current target word not as the output, but as the input of the neural network, estimating whether the target word under examination is correct or not, as shown in Figure 1b.
9	7	The BNNJM learns a simple binary classifier, given the context and target words, therefore it can be trained by MLE very efficiently.
23	12	Because the BNNJM uses the current target word as input, the information about the current target word can be combined with the context word information and processed in the hidden layers.
24	7	Thus, the hidden layers can be used to learn the difference between correct target words and noise in the BNNJM, while in the NNJM the hidden layers just contain information about context words and only the output layer can be used to discriminate between the training data and noise, giving the BNNJM more power to learn this classification problem.
28	22	Positive examples can be extracted directly from the word-aligned parallel corpus as〈 s ai+(m−1)/2 ai−(m−1)/2, t i−1 i−n+1, ti 〉 ; Negative examples can be generated for each positive example in the same way that NCE generates noise data as〈 s ai+(m−1)/2 ai−(m−1)/2, t i−1 i−n+1, ti ′ 〉 , where ti′ ∈ V \ {ti}.
29	54	Vaswani et al. (2013) adopted the unigram probability distribution (UPD) to sample noise for train- ing NNLMs with NCE, q (ti′) = occur(ti ′)∑ ti ′′∈V occur(ti′′) where occur (ti′) stands for how many times ti′ occurs in the training corpus.
30	25	In this paper, we propose a noise distribution specialized for translation models, such as the NNJM or BNNJM.
32	6	Focusing on sai=“安排”, this is translated into ti =“arrange”.
33	28	For this positive example, UPD is allowed to sample any arbitrary noise, such as ti′ = “banana”.
34	6	However, in this case, noise ti′ = “banana” is not useful for model training, as constraints on possible translations given by the phrase table ensure that “安排” will never be translated into “banana”.
36	20	Based on this intuition, we propose the use of another noise distribution that only uses ti′ that are possible translations of sai , i.e., ti ′ ∈ U (sai) \ {ti}, where U (sai) contains all target words aligned to sai in the parallel corpus.
37	4	Because U (sai) may be quite large and contain many wrong translations caused by wrong alignments, “banana” may actually be included in U (“安排”).
46	5	Word segmentation was done by BaseSeg (Zhao et al., 2006) for Chinese and Mecab4 for Japanese.
47	13	For the FE language pair, we used standard data for the WMT 2014 translation task.
51	20	Feature weights were tuned by MERT (Och, 2003).
52	34	The word-aligned training set was used to learn the NNJM and the BNNJM.6 For both NNJM and BNNJM, we set m = 7 and n = 5.
55	93	The number of noise samples for NCE was set to be 100.
56	72	For the BNNJM, we used only one negative example for each positive example in each training epoch, as the BNNJM needs to calculate the whole neural network (not just the output layer like the NNJM) for each noise sample and thus noise computation is more expensive.
57	15	However, for different epochs, we resampled the negative example for each positive example, so the BNNJM can make use of different negative examples.
58	80	Table 1 shows how many epochs these two models needed and the training time for each epoch on a 10-core 3.47GHz Xeon X5690 machine.7 Translation results are shown in Table 2.
59	103	We can see that using TPD instead of UPD as a noise distribution for the NNJM trained by NCE can speed up the training process significantly, with a small improvement in performance.
62	7	From Table 2, the NNJM does not improve translation performance significantly on the FE task.
63	10	Note that the baseline BLEU for the FE task is lower than CE and JE tasks, indicating that learning is harder for the FE task than CE and JE tasks.
64	12	The validation perplexities of the NNJM with UPD for CE, JE and FE tasks are 4.03, 3.49 and 8.37.
