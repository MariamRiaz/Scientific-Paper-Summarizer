1	13	Linguistic theories have argued for a very tight integration of syntactic and semantic processing (Steedman, 2000; Copestake and Flickinger, 2000), and many systems have used syntactic dependency or phrase-based parsers as preprocessing for semantic analysis (Gildea and Palmer, 2002; Punyakanok et al., 2008; Das et al., 2014).
3	18	Because annotated training datasets for semantics will always be limited, we expect that syntax—which offers an incomplete but potentially useful view of semantic structure—will continue to offer useful inductive bias, encouraging semantic models toward better generalization.
4	11	We address the central question: is there a way for semantic analyzers to benefit from syntax without the computational cost of syntactic parsing?
5	33	We propose a multitask learning approach to incorporating syntactic information into learned representations of neural semantics models (§2).
6	47	Our approach, the syntactic scaffold, minimizes an auxiliary supervised loss function, derived from a syntactic treebank.
7	47	The goal is to steer the distributed, contextualized representations of words and spans toward accurate semantic and syntactic labeling.
8	60	We avoid the cost of training or executing a full syntactic parser, and at test time (i.e., runtime in applications) the semantic analyzer has no additional cost over a syntax-free baseline.
9	20	Further, the method does not assume that the syntactic treebank overlaps the dataset for the primary task.
12	27	PropBank; Palmer et al., 2005), making phrase-based syntax a natural choice for a scaffold.
13	34	See Figure 1 for an example sentence with syntactic and semantic annotations.
14	100	Since the scaffold task is not an end in itself, we relax the syntactic parsing problem to a collection of independent span-level predictions, with no constraint that they form a valid parse tree.
22	12	We use the term “scaffold” to refer to a second task, T2, that can be combined with T1 during multitask learning.
58	18	We assume two sources of supervision: a corpusD1 with instances x annotated for the primary task’s outputs y (semantic role labeling or coreference resolution), and a treebankD2 with sentences x, each with a phrase-structure tree z.
67	14	In our model, every span xi: j is represented by an embedding vector vi: j (see details in §5.3).
121	17	We construct an embedding for the span using • hi and h j: contextualized embeddings for the words at the span boundary (§5.3.1), • ui: j: a span summary that pools over the con- tents of the span (§5.3.2), and • ai: j: and a hand-engineered feature vector for the span (§5.3.3).
131	13	We use the following three features for each span: • width of the span in tokens (Das et al., 2014) • distance (in tokens) of the span from the tar- get (Täckström et al., 2015) • position of the span with respect to the tar- get (before, after, overlap) (Täckström et al., 2015) Each of these features is encoded as a one-hotembedding and then linearly transformed to yield a feature vector, ai: j.
142	16	For each span s and a potential antecedent a ∈ Y(s), pairwise coreference scores Ψ(vs, va, φ(s, a)) are computed via feedforward networks with the span embeddings as input.
151	18	We evaluate our models on the test set of FrameNet 1.5 for frame SRL and on the test set of OntoNotes for both PropBank SRL and coreference.
152	19	For the syntactic scaffold in each case, we use syntactic annotations from OntoNotes 5.0 (Weischedel et al., 2013; Pradhan et al., 2013).4 Further details on experimental settings and datasets have been elaborated in the supplemental material.
176	20	We obtain further improvement of 0.8 absolute F1 with the best syntactic scaffold from the frame SRL task.
180	58	Since the datasets for learning PropBank semantics and syntactic scaffolds completely overlap, the performance improvement cannot be attributed to a larger training corpus (or, by extension, a larger vocabulary), though that might be a factor for frame SRL.
187	45	We experiment with the best syntactic scaffold from the frame SRL task.
188	34	We used NP, OTHER, and null as the labels for the common nonterminals scaffold here, since coreferring mentions are rarely prepositional phrases.
190	31	Contemporaneously, Lee et al. (2018) proposed a model which takes in account higher order inference and more aggressive pruning, as well as initialization with ELMo embeddings, resulting in 73.0 average F1.
192	16	To investigate the performance of the syntactic scaffold, we focus on the frame SRL results, where we observed the greatest improvement with respect to a non-syntactic baseline.
198	12	Similarly, for frequent frame elements, scaffolding improves performance across the board, as shown in Fig.
199	29	The largest improvements come for Theme and Goal, which are predominantly realized as noun phrases and prepositional phrases.
203	37	While our focus was on span-based tasks, syntactic scaffolds could be applied in other settings (e.g., dependency and graph representations).
205	18	It remains an open empirical question to determine the relative merits of different kinds of scaffolds and multitask learners, and how they can be most produc- tively combined.
206	18	Our code is publicly available at https://github.com/swabhs/scaffolding.
