0	60	Semantic parsing tackles the task of mapping natural language (NL) utterances into structured formal meaning representations (MRs).
1	116	This includes parsing to general-purpose logical forms such as λ-calculus (Zettlemoyer and Collins, 2005, 2007) and the abstract meaning representation (AMR, Banarescu et al. (2013); Misra and Artzi (2016)), as well as parsing to computerexecutable programs to solve problems such as question answering (Berant et al., 2013; Yih et al., 2015; Liang et al., 2017), or generation of domainspecific (e.g., SQL) or general purpose programming languages (e.g., Python) (Quirk et al., 2015; Yin and Neubig, 2017; Rabinovich et al., 2017).
3	56	However, these models are also extremely data hungry: optimization of such models requires large amounts of training data of parallel NL utterances and manually annotated MRs, the creation of which can be expensive, cumbersome, and time-consuming.
5	56	These data requirements can be alleviated with weakly-supervised learning, where the denotations (e.g., answers in question answering) of MRs (e.g., logical form queries) are used as indirect supervision (Clarke et al. (2010); Liang et al. (2011); Berant et al. (2013), inter alia), or dataaugmentation techniques that automatically generate pseudo-parallel corpora using hand-crafted or induced grammars (Jia and Liang, 2016; Wang et al., 2015).
6	33	In this work, we focus on semi-supervised learning, aiming to learn from both limited amounts of parallel NL-MR corpora, and unlabeled but readily-available NL utterances.
9	58	This formulation enables our model to perform both standard supervised learning by optimizing the inference model (i.e., the parser) using parallel corpora, and unsupervised learning by maximizing the variational lower bound of the likelihood of the unlabeled utterances (§ 3.3).
18	39	As noted above, there are many varieties of MRs that can be represented as either graph structures (e.g., AMR) or tree structures (e.g., λ-calculus and ASTs for programming languages).
48	62	STRUCTVAE models the semantic parser pφ(z|x) as the inference model qφ(z|x) in VAE (§ 2.2), which maps NL utterances x into tree-structured meaning representations z. qφ(z|x) can be any trainable semantic parser, with the corresponding MRs forming the structured latent semantic space.
65	22	Fields with composite types are instantiated by constructors of the same type, while fields with primitive types store values (e.g., identifier names or string literals).
85	97	Supervised Learning For the supervised learning objective, we modify Js, and use the labeled data to optimize both the inference model (the se- mantic parser) and the reconstruction model: Js , ∑ (x,z)∈L ( log qφ(z|x) + log pθ(x|z) ) (2) Unsupervised Learning To optimize the unsupervised learning objective Ju in Eq.
86	24	(1), we maximize the variational lower-bound of log p(x): log p(x) ≥ Ez∼qφ(z|x) ( log pθ(x|z) ) − λ ·KL[qφ(z|x)||p(z)] = L (3) where KL[qφ||p] is the Kullback-Leibler (KL) divergence.
97	22	Discussion Perhaps the most intriguing question here is why semi-supervised learning could improve semantic parsing performance.
98	34	While the underlying theoretical exposition still remains an active research problem (Singh et al., 2008), in this paper we try to empirically test some likely hypotheses.
99	158	(4), the gradient received by the inference model from each latent sample z is weighed by the learning signal l(x, z).
101	24	It can also be viewed as weights associated with pseudo-training examples {〈x, z〉 : z ∈ S(x)} sampled from the inference model.
102	176	Intuitively, a sample z with higher rewards should: (1) have z adequately encode the input, leading to high reconstruction score log pθ(x|z); and (2) have z be succinct and natural, yielding high prior probability.
103	19	Let z∗ denote the gold-standard MR of x.
111	43	The target MRs are defined using λ-calculus logical forms (e.g., “lambda $0 e (and (flight $0) (from $ci0) (to $ci1))”).
141	26	First, comparing our proposed STRUCTVAE with the supervised parser when there are extra unlabeled data (i.e., |L| < 4, 434 for ATIS and |L| < 16, 000 for DJANGO), semi-supervised learning with STRUCTVAE consistently achieves better performance.
142	88	Notably, on DJANGO, our model registers results as competitive as previous state-of-the-art method (YN17) using only half the training data (71.5 when |L| = 8000 v.s.
153	27	Study of Learning Signals As discussed in § 3.3, in semi-supervised learning, the gradient received by the inference model from each sampled latent MR is weighted by the learning signal.
156	21	Figures 4a and 4b depict the histograms of learning signals on DJANGO and ATIS, resp.
159	20	On average, we have l(x, z∗) being positive and l(x, z) negative.
165	67	Finally, to study the relative contribution of the reconstruction score log p(x|z) and the prior log p(z) to the learning signal, we present examples of inferred latent MRs during training (Tab.
167	44	This is in line with our assumption that a good latent MR should adequately encode the semantics of the utterance.
169	35	These results also suggest that the prior and the reconstruction model perform well with linearization of MRs.
170	103	Finally, note that in Examples 2&3 the learning signals for the correct samples zs1 are positive even if their inference scores q(z|x) are lower than those of zs2.
172	41	Generalizing to Other Latent MRs Our main results are obtained using a strong AST-based semantic parser as the inference model, with copyaugmented reconstruction model and an LSTM language model as the prior.
176	37	The inference model is trained to construct a tree-structured logical form using the transition actions defined in Cheng et al. (2017).
178	22	4 lists the results for this STRUCTVAE-SEQ model.
