21	13	In this work we focus on the former.
23	8	The control component is presented in §4 and the learning algorithm is presented in §5.
50	14	In this section, we give a brief introduction to predictive state representations, which constitute the state tracking (filtering) component of our model.1 We provide more technical details in the appendix.
54	12	In a recurrent neural network (Figure 1 (a,b)), q is latent and the function g that connects states to the output is unknown and has to be learned.
56	10	Predictive state models use a predictive representation of the state.
57	25	That means the qt is explicitly defined as the conditional distribution of future observations ot:t+k−1 conditioned on future actions at:t+k−1.2 (e.g., in the discrete case, qt could be a vectorized conditional probability table).
58	12	Predictive states are thus defined entirely in terms of observable features with no latent variables involved.
59	63	That means the mapping between the predictive state qt and the prediction of ot given at can be fully known or simple to learn consistently (Hefny et al., 2015b; Sun et al., 2016).
63	18	Wext is a parameter to be learned.
64	16	pt = Wextqt (1) 1 We follow the predictive state controlled model formulation in Hefny et al. (2018).
69	8	A system is k-observable if maintaining the predictive state is equivalent to maintaining the distribution of the system’s latent state.
74	15	In continuous systems we can use Hilbert space embedding of distributions (Boots et al., 2013), where fcond uses kernel Bayes rule (Fukumizu et al., 2013).
76	11	Observation and action features are based on random Fourier features (RFFs) of RBF kernel (Rahimi & Recht, 2008) projected into a lower dimensional subspace using randomized PCA (Halko et al., 2011).
82	7	Here ht ≡ h(a1:t−1,o1:t−1) denotes a set of features extracted from previous observations and actions (typically from a fixed length window ending at t− 1).
91	18	can benefit from local optimization.
92	12	Downey et al. (2017) and Hefny et al. (2018) note that a PSR defines a recursive computation graph similar to that of an RNN where we have qt+1 = fcond(Wext(qt),at,ot)) E[ot | qt,at] = Wpred(qt ⊗ φ(at)), (3) With a differentiable fcond, the PSR can be trained using backpropagation through time to minimize prediction error.
107	13	Similar to Schulman et al. (2015) we assume a Gaussian distribution N (µt,Σ), where µ = ϕ(qt;θµ); Σ = diag(exp(r)) 2 (4) for a non-linear map ϕ parametrized by θµ (e.g. a feedforward network) , and a learnable vector r. An RPSP is thus a stochastic recurrent policy with the recurrent part corresponding to a PSR.
117	12	The hyper-parameters α1, α2 ∈ R determine the importance of the expected return and prediction error respectively.
127	11	With an estimate of both gradients, we can compute (5) and update the parameters trough gradient descent.
130	6	This constraint results in smoother changes of policy parameters.
132	15	θn+1 = arg min θ Eτ∼p(τ |πn) T∑ t=0 [ πθ(at|qt) πn(at|qt) (Rt(τ)− bt) ] s.t.
148	6	We then experiment with both joint VRPG optimization (RPSP-VRPG) described in §5.1 and alternating optimization (RPSP-Alt) in §5.2.
154	7	Competing Models: We compare our models to a finite memory model (FM) and gated recurrent units (GRU).
155	6	The finite memory models are analogous to RPSP, but replace the predictive state with a window of past observations.
164	4	The reactive policy contains one hidden layer of 16 nodes with ReLU activation.
183	24	We propose RPSP-networks, combining ideas from predictive state representations and recurrent networks for reinforcement learning.
187	8	There has been a body of work for online learning of predictive state representations (Venkatraman et al., 2016; Boots & Gordon, 2011; Azizzadenesheli et al., 2016; Hamilton et al., 2014).
188	9	To our knowledge, none of them is able to deal with continuous actions and make use of local optimization.
189	13	We are also interested in applying off-policy methods and more elaborate exploration strategies.
190	11	12 We report results for partially observable setting which is different from RL experiments in (Venkatraman et al., 2017).
