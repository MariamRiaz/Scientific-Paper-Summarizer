0	10	Bayesian optimization (BO) (Shahriari et al., 2016) is a powerful and versatile tool for black-box function optimization, with applications including parameter tuning, robotics, molecular design, sensor networks, and more.
1	22	The idea is to model the unknown function as a Gaussian process with a given kernel function dictating the smoothness properties.
2	7	This model is updated using (typically noisy) samples, which are selected to steer towards the function maximum.
3	7	One of the most attractive properties of BO is its efficiency in terms of the number of function samples used.
5	4	Perhaps the most prominent work in the literature giving such guarantees is that of (Srinivas et al., 2010), who consider the cumulative regret: RT = T∑ t=1 ( max x f(x)− f(xt) ) , (1) where f is the function being optimized, and xt is the point chosen at time t. Under a Gaussian process (GP) prior and Gaussian noise, it is shown in (Srinivas et al., 2010) that an algorithm called Gaussian Process Upper Confidence Bound (GP-UCB) achieves a cumulative regret of the form RT = O ∗( √ TγT ), (2) where γT = maxx1,...,xT I(f ;y) (with function values f = (f(x1), .
6	21	, f(xT )) and noisy samples y = (y1, .
7	20	, yT )) is known as the maximum information gain.
8	2	Here I(f ;y) denotes the mutual information (Cover & Thomas, 2001) between the function values and noisy samples, and O∗(·) denotes asymptotic notation up to logarithmic factors.
9	45	The guarantee (2) ensures sub-linear cumulative regret for many kernels of interest.
10	98	However, the literature is severely lacking in algorithm-independent lower bounds, and without these, it is impossible to know to what extent the upper bounds, including (2), can be improved.
12	8	We show that the best possible cumulative regret behaves as Θ∗( √ T ) under mild assumptions on the kernel, thus identifying both cases where (2) is nearoptimal, and cases where it is strictly suboptimal.
28	6	The main results of this paper are informally summarized as follows.
29	40	Under mild technical assumptions on the kernel, satisfied (for example) by the SE kernel and Matérn-ν kernel with ν > 2, the best possible cumulative regret of noisy BO in one dimension behaves as Ω( √ T ) and O( √ T log T ).
30	24	Our results have several important implications: • To our knowledge, our lower bound is the first of any kind in the noisy Bayesian setting, and is tight up to a√ log T factor under our technical assumptions.
32	35	• On the other hand, our upper bound establishes that the upper bound of (Srinivas et al., 2010) for the Matérn-ν kernel, namely O∗(T ν+2 2ν+2 ), is strictly suboptimal for ν > 2.
33	16	For example, if ν = 3, then this is O∗(T 0.625), as opposed to our upper bound of O∗(T 0.5).
34	21	(See also (Shekhar & Javidi, 2017) for recent improvements over (Srinivas et al., 2010) under the Matérn kernel in higher dimensions and/or with smaller ν).
35	5	• Another important implication for the Matérn kernel with ν > 2 is that the Bayesian setting is provably harder than the non-Bayesian RKHS counterpart; the latter has cumulative regret Ω(T ν+1 2ν+1 ) (Scarlett et al., 2017), which is strictly worse than O( √ T log T ).
37	28	We build on the ideas of (de Freitas et al., 2012) for the noiseless setting, while addressing highly non-trivial challenges arising in the presence of noise.
38	4	Our lower bound is stated formally in Section 4, and its technical assumptions are given in Section 2.1.
42	3	At time t, we query a single point xt ∈ D and observe a noisy sample yt = f(xt) + zt, where zt ∼ N(0, σ2) for some noise variance σ2 > 0, with independence across different times.
45	24	The posterior distribution of f given the points xt = [x1, .
48	13	The kernel k is stationary, depending on its inputs (x, x′) only through τ = x− x′; 2.
49	3	The kernel k satisfies k(x, x′) ≤ 1 for all (x, x′), and k(x, x) = 1 for all x ∈ D; Given the stationarity assumption, the assumptions k(x, x′) ≤ 1 and k(x, x) = 1 are without loss of generality, as one can always re-scale the function and adjust the noise variance σ2 accordingly.
50	12	Next, we give some high-probability assumptions on the random function f itself.
51	6	There exists a constant δ1 ∈ (0, 1) such that, with probability at least 1− δ1, we have the following: 1.
54	74	The function f and its first two derivatives are bounded: |f(x)| ≤ c0, |f ′(x)| ≤ c1, |f ′′(x)| ≤ c2 (8) for all x ∈ D and some constants (c0, c1, c2).
55	3	This implies that f is c1-Lipschitz continuous, and f ′ is c2-Lipschitz continuous.
56	22	The assumption of a unique maximizer holds with probability one in most non-trivial cases (de Freitas et al., 2012), and (7) simply formally defines the gap to the second-highest peak.
