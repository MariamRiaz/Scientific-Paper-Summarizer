18	31	In this paper we provide a positive answer by developing a novel proof technique.
19	15	Prior analyses require ‘local’ restricted strong convexity conditions (RSC): one condition for the sparse component and one for the group sparse component.
25	24	As an additional key contribution of this paper, we consider the extension of sparse+group sparse dirty models to non-convex regularizers, and show their `∞ consistency.
41	18	We aim at recovering parameter θ∗ which is the unique minimizer of the population risk: θ∗ := argminθ∈Ω EZ [L(θ;Z)] in cases where θ∗ = α∗ + β∗, (1) where α∗ is a sparse component and β∗ is a group-sparse component obeying the group structure G. For that purpose, we focus on regularized M -estimators under a dirty learning setting that combines sparsity and group-sparsity.
48	93	Consider the standard linear model y = Xθ∗ +w where y ∈ Rn is the observation vector, θ∗ is the true regression parameter which is the sum of sparse α∗ and group sparse β∗, X ∈ Rn×p is the design matrix, and w ∈ Rn is the observation noise.
50	49	The formulation can be seamlessly extended to cover the dirty multitask learning setting of Jalali et al. (2010): minimize α,β∈Rp×m m∑ k=1 1 2n ∥∥y(k) −X(k)([α+ β](·,k))∥∥22 (4) +λ1‖α‖1 + λ2‖β‖1,∞ where we have m related tasks in columns: α,β ∈ Rp×m, and the groups can be defined across tasks in rows.
51	25	Here, [α + β](·,k) indicates k-th column of matrix input α+ β. Graphical Model Estimation.
54	21	To recover Θ∗ we solve minimize S+B 0 trace ( (S +B) Σ̂ ) − log det(S +B) (5) +λ1‖S‖1 + λ2‖B‖1,a where Σ̂ is the sample covariance matrix and regularizers are applied to off-diagonal entries of S and B.
56	23	This estimator is discussed in Hara & Washio (2013).
87	33	This naturally leads to the following question: Can we provide tight error bounds for the problem (2) not requiring the joint RSC across individual structures and hence bypassing the incoherence condition?
88	18	In order to address the above question, our key proof technique is to establish the decomposability between two components of error vectors, by making the target components dependent of our estimation.
96	11	Sparse + Group Sparse Dirty Models 3 1 1 1 2 1 0 0 1 0 1 0 0 0 0 Theta^* (a) θ∗ 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 a^* (b) α∗ 3 1 1 1 2 1 0 0 0 0 0 0 0 0 0 b^* (c) β∗ 1.9 0 0.2 0 1.1 0 0.1 0 1 0 0.9 0.2 0.1 0 0 a_hat (d) α̃ 0.8 0.8 0.8 1.1 1.1 1.1 0.2 -0.2 0.2 0 0 0 0 0 0 b_hat (e) β̃ 1.9 0 0.2 0 1.1 0 0.1 0 0 0 -0.1 0.2 0.1 0 0 Del^* (f) α̃−α∗ -2.2 -0.2 -0.2 0.1 -0.9 0.1 0.2 -0.2 0.2 0 0 0 0 0 0 Gam^* (g) β̃ − β∗ 1.9 0 0.2 0 1.1 0 0 0 0.8 0 1 0 0 0 0 a_bar (h) ᾱ
99	14	Error vectors based on surrogates are decomposable (see text for details).
101	9	It turns out that the error vectors computed based on the surrogate ᾱ and β̄ are always decomposable as described in the following proposition, and the consequence of this decomposability plays a key role for showing i) `2-error bounds without incoherence condition and ii) support set recovery guarantee for non-convex `1 + `1,a dirty regularizers (with faster estimation rates than for convex dirty regularizers).
102	12	Consider any local optimum θ̃ of convex or non-convex dirty models, and corresponding θ̄ := T (α∗,β∗; α̃, β̃).
107	10	However, it is always guaranteed that ∆s∗ = ∆s̄ = ∆U and Γb∗ = Γb̄ = ΓU (11) where ∆s∗ represents the projection of ∆ onto the s∗coordinate space; that is, [∆s∗ ]j is ∆j if j ∈ s∗, and 0 otherwise.
109	18	Suppose (i) the target parameter is given by (a), (ii) we define (b) and (c) as the sparse and group sparse components of θ∗, and (iii) the minimizer of program (6) are computed as in (d) and (e), respectively for α̃ and β̃.
113	15	(13) RSC of the loss is also used to guarantee `2-consistency (Negahban et al., 2012; Loh & Wainwright, 2015) or `∞- consistency (Loh & Wainwright, 2014) of “clean” structurally constrained problems (i.e. problems with a single structure).
154	14	Letting Q̂ := ∫ 1 0 ∇2L ( θ∗ + t(θ̂ − θ∗) ) dt, it holds that ‖θ̂ − θ∗‖∞ ≤ ∥∥(Q̂UU)−1∇L(θ∗)U∥∥∞ + min{λ1, λ2} ∣∣∣∣∣∣(Q̂UU)−1∣∣∣∣∣∣∞ where ||| · |||∞ denotes a matrix induced norm (maximum absolute row sum).
156	13	Then, the error bound in the statement 2 is reduced to tighter bound as ‖θ̂ − θ∗‖∞ ≤∥∥(Q̂UU)−1∇L(θ∗)U∥∥∞.
157	21	Multi-task high-dimensional linear regression.
161	46	Now, we derive a corollary for this particular nonconvex dirty model.
168	23	Not only the result in (Jalali et al., 2010) requires the incoherence on X (specifically, maxj∈Uc ∑m k=1 ∥∥Σ(k)j,Uk(Σ(k)j,Uk)−1∥∥1 < 1), but it also has an additional sλ1 Cmin √ n term in |||Θ̃ − Θ∗|||max bound.
170	11	To illustrate the practical consequences of the superior statistical guarantees of models with non-convex penalties, we perform experiments on both simulated and real-world data and compare convex and non-convex dirty models for sparse + group-sparse structures.
171	32	We consider multitask regression problems with m = 10 tasks and p = 260 variables for settings of parameters (s, sG) ∈ {(2p/10, p/20), (p/10, p/10)} with respectively less / more support overlap across tasks (recall s and sG are the number of nonzero elements in α∗ and the number of nonzero groups in β∗, respectively).
177	25	For varying sample size n we measure the `∞ error of parameters estimated by (i) convex dirty model (Jalali et al., 2010), (ii) non-convex dirty model with SCAD + Group-SCAD penalty, and (iii) nonconvex dirty model with MCP + Group-MCP penalty.
186	10	We analyze three biological activity datasets from the “molecular activity challenge” (http://www.kaggle.com/ c/MerckActivity).
191	40	Non-convex dirty models achieved the best R2, which illustrate their capability as a valuable tool for high-dimensional data analysis.
192	25	This paper finally resolved the outstanding case of sparse + group-sparse dirty models with convex penalties: we provided the first satisfactory consistency results that do not require implausible assumptions, thereby fully justifying their practical success.
193	10	In addition we proposed and studied dirty models with non-convex penalties and showed that they enjoy superior theoretical guarantees that translate into significant practical impact.
