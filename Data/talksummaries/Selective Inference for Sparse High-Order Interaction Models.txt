39	10	The main contribution in this paper is to extend the selective inference framework into sparse high-order interaction models by introducing novel computational algorithms.
54	63	The high-order interaction model Eq.
55	35	Let us write the mapping from the original covariates z := [z1, .
56	73	, zd]> ∈ Rd to the highorder interaction features x := [x1, .
57	14	, xD]> ∈ RD as φ : [0, 1]d → [0, 1]D, z 7→ x,, i.e., x := φ(z) = [z1, .
58	16	, zd−r+1 ···zd]> Then, the high-order interaction model Eq.
60	16	Since a high-order interaction feature is a product of original covariates defined in [0, 1], the range of each feature xj , j ∈ [D] is also [0, 1].
61	27	The original training set is denoted as {(zi, yi) ∈ [0, 1]d × R}i∈[n], while the expanded training set is written as {(xi, yi) ∈ [0, 1]D ×R}i∈[n].
65	16	Our goal is to identify statistically significant high-order interaction terms that have large impacts on the response Y by identifying regression coefficients αs which are significantly deviated from zero.
70	14	A key finding by Lee et al. (2016) is that, if the first selection stage is described as a linear selection event, then exact statistical inference of the fitted coefficients conditional on the selection event can be done.
73	18	The selective inference framework in Lee et al. (2016) can be applied to feature selection algorithms whose selection process can be characterized by a set of linear inequalities in the form of Ay ≤ b with a certain matrix A and a certain vector b that do not depend on y.
81	22	(3) For two-sided test at level α, if the critical values `α/2 and uα/2 are chosen to be the lower and the upper α/2 points of the sampling distribution in Eq.
84	11	If the critical values are computed as ` (S,j) α/2 := (F [L(S,j),U(S,j)] 0,σ2S,j )−1(α/2), (6a) u (S,j) α/2 := (F [L(S,j),U(S,j)] 0,σ2S,j )−1(1− α/2), (6b) then the selective type I error is controlled as in Eq.
89	37	(7) indicates that the truncation points are obtained by considering the interval where the test statistic β̂S,j can move within the polyhedron Pol(S).
90	57	Figure 2 schematically illustrates that, when we make inferences conditional on a linear selection event S, the sampling distribution is defined within the polytope Pol(S), and it follows a truncated normal distribution when y is normally distributed.
91	10	Unfortunately, we cannot directly apply this selective inference framework to high-order interaction models because the polytope Pol(S) is characterized by extremely large number of linear inequalities, and the optimization problems in Eq.
92	13	In this section, we present two feature selection algorithms for high-order interaction models.
95	38	Consider selecting the top k interaction features from all the D interaction features that have marginal strong correlations with the response.
96	29	Noting that each feature is defined in [0, 1] and the value indicates (the degree of) the existence of a certain property, we consider a score x>·jy, j ∈ [D] for each of the D features, and select the top k features according to their absolute scores |x>·jy|.
105	17	(8) is smaller than the current k-th largest score at a certain node j, then we can quit searching over its descendant nodes j̃ ∈ Des(j).
106	10	As pointed out in Lee & Taylor (2014), feature selection processes of marginal screening is a linear selection event, i.e., characterized by a set of linear constraints.
114	9	Consider again selecting k interaction features by OMP.
136	16	When we search over these k trees, we introduce a novel pruning strategy by deriving a condition such that, if the j′-th node in the j-th tree satisfies certain conditions, then all the (j, j̃′)-th inequalities for all j̃′ ∈ Desj(j′) are guaranteed to be irrelevant to the selective inference results because they do not affect the optimal solutions in Eq.
172	17	(2) with the significance level α/k where α = 0.05, and we refer this error as family-wise false positive rates (FW-FPRs).
182	16	The FW-FPRs of the other two approaches select and split were successfully controlled.
187	38	The risk of failing to select truly correlated features in split would be higher than select because only half of the data would be used in the selection stage.
189	34	Table 1 shows the computation times in seconds for the selective inference approach with and without the computational tricks described in §4 for various values of the number of transactions n ∈ {100, .
190	16	, 10, 000}, the number of original covariates d ∈ {100, .
195	51	From the results, we conclude that computational trick described in §4 is indispensable for selective inferences for sparse high-order interaction models.
202	17	These results indicate that the selective inference approach could successfully identify statistically significant high-order interactions of multiple mutations.
