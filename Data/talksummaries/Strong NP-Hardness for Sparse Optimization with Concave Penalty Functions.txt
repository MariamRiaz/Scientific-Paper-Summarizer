0	26	We study the sparse minimization problem, where the objective is the sum of empirical losses over input data and a sparse penalty function.
1	37	Such problems commonly arise from empirical risk minimization and variable selection.
2	52	The role of the penalty function is to induce sparsity in the optimal solution, i.e., to minimize the empirical loss using as few nonzero coefficients as possible.
4	82	We are interested in the computational complexity of Problem 1 under general conditions of the loss function ` and the sparse penalty p. In particular, we focus on the case where ` is a convex loss function and p is a concave penalty with a unique minimizer at 0.
11	23	More precisely, we show that there exists a lower bound on the suboptimality error of any polynomial-time deterministic algorithm.
14	32	The strong NP-hardness of approximation is one of the strongest forms of complexity result for continuous optimization.
15	45	To our best knowledge, this paper gives the first and strongest set of hardness results for Problem 1 under very general assumptions regarding the loss and penalty functions.
20	14	The condition is a slight weaker version of strict concavity.
22	84	To the best of our knowledge, this is the most gen- eral condition on the penalty function in the literature.
28	13	Section 3 presents the key assumptions and illustrates examples of loss and penalty functions that satisfy the assumptions.
63	24	In this section, we state the two critical assumptions that lead to the strong NP-hardness results: one for the penalty function p, the other one for the loss function `.
74	27	The corresponding minimization problem is called the bridge regression problem (Frank & Friedman, 1993).
82	14	We state our assumption about the loss function `.
87	12	Assumption 2 is a critical, but very general, assumption regarding the loss function `(y, b).
95	14	We will show that Assumptions 2 is satisfied by a variety of loss functions.
104	21	The loss function takes the form Lδ(y, b) = { 1 2 |y − b| 2 for |y − b| ≤ δ, δ(|y − b| − 12δ) otherwise.
108	18	In Poisson regression (Cameron & Trivedi, 2013), the negative log-likelihood minimization is min x∈Rd − logL(x;A, b) = min x∈Rd n∑ i=1 (exp(aTi x)−bi·aTi x).
117	10	We claim that the loss function `(y, b) = log(1 + exp(y))−b ·y satisfies Assumption 2.
130	15	To sum up, the combination of any loss function given in Section 3.1 and any penalty function given in Section 3.2 results in a strongly NP-hard optimization problem.
134	12	Let Assumption 1 hold, and let p(·) be twice continuously differentiable in (0,∞).
159	19	(Fan et al., 2015) further showed that we only need to find a √ n log d-optimal solution to (2) to achieve such a small estimation error.
162	25	As a result, we see that d,n = Ω(n 1/2d1/3) √ n log d, for high values of the dimension d. According to Theorem 2, it is strongly NP-hard to approximately solve problem (2) within the required statistical precision √ n log d. This result illustrates a sharp contrast between statistical properties of sparse estimation and the worst-case computational complexity.
169	16	Our results do not exclude the possibility that, under more stringent modeling and distributional assump- tions, the problem would be tractable with high probability or on average.
174	79	We construct a polynomial-time reduction from the 3-partition problem (Garey & Johnson, 1978) to the sparse optimization problem.
175	155	Given a set S of 3m integers s1, ...s3m, the three partition problem is to determine whether S can be partitioned into m triplets such that the sum of the numbers in each subset is equal.
176	15	This problem is known to be strongly NPhard (Garey & Johnson, 1978).
179	32	We first illustrate several properties of the penalty function if it satisfies the conditions in Theorem 1.
180	19	If p(t) satisfies the conditions in Theorem 1, then for any l ≥ 2, and any t1, t2, .
181	16	If p(t) satisfies the conditions in Theorem 1, then there exists τ0 ∈ (0, τ) such that p(·) is concave but not linear on [0, τ0] and is twice continuously differentiable on [τ0, τ ].
182	14	, tl such that t1 + · · ·+ tl = t̃, we have p(|t1|) + · · ·+ p(|tl|) < p(t̃) + C1δ only if |ti − t̃| < δ for some i while |tj | < δ for all j 6= i, where C1 = p(τ0/3)+p(2τ0/3)−p(τ0) τ0/3 > 0.
