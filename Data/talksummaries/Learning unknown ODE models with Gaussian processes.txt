6	94	We consider the dynamics of a system governed by multivariate ordinary differential functions: ẋ(t) = dx(t) dt = f(x(t)) (1) where x(t) ∈ X = RD is the state vector of a Ddimensional dynamical system at time t, and the ẋ(t) ∈ Ẋ = RD is the first order time derivative of x(t) that drives the state x(t) forward, and where f : RD → RD is the vector-valued derivative function.
7	35	The ODE solution is determined by x(t) = x0 + ∫ t 0 f(x(τ))dτ, (2) where we integrate the system state from an initial state x(0) = x0 for time t forward.
8	57	We assume that f(·) is completely unknown and we only observe one or several multivariate time series Y = (y1, .
14	20	To overcome the computationally intensive forward solution, a family of methods denoted as gradient matching (Varah, 1982; Ellner et al., 2002; Ramsay et al., 2007) have proposed to replace the forward solution by matching f(yi) ≈ ẏi to empirical gradients ẏi of the data instead, which do not require the costly integration step.
18	22	Recently, initial work to handle unknown or non-parametric ODE models have been proposed, although with various limiting approximations.
22	20	To our knowledge, there exists no model that can learn non-linear ODE functions ẋ(t) = f(x(t)) over the state x against the true forward solutions x(ti).
24	14	We do not use gradient matching or other approximative models, but instead propose to directly optimise the exact ODE system with the fully forward simulated responses against data.
25	43	We parameterise our model as an augmented Gaussian process vector field with inducing points, while we propose sensitivity equations to efficiently compute the gradients of the system.
28	33	We model the vector field as a vector-valued Gaussian process (Rasmussen and Williams, 2006) f(x) ∼ GP(0,K(x,x′)), (4) which defines a priori distribution over function values f(x) whose mean and covariances are E[f(x)] = 0 (5) cov[f(x), f(x′)] = K(x,x′), (6) and where the kernel K(x,x′) ∈ RD×D is matrixvalued.
35	12	Instead, we resort to augmenting the Gaussian process with a set of M inducing points z ∈ X and u ∈ Ẋ , such that f(z) = u (Quiñonero-Candela and Rasmussen, 2005).
41	15	The vector-valued kernel function (8) uses operator-valued kernels, which result in matrix-valued kernels Kθ(z, z′) ∈ RD×D for real valued states x, z, while the kernel matrix over data points becomes Kθ = (K(zi, zj))Mi,j=1 ∈ RMD×MD (See Alvarez et al. (2012) for a review).
47	20	Similarly, curl-free kernels induce curl-free vector fields that can contain sources or sinks, that is, trajectories can accelerate or decelerate.
53	65	(11) The model posterior is then p(U,x0,θ,ω|Y ) ∝ p(Y |x0, U,ω)p(U |θ) = L, (12) where we have for brevity omitted the dependency on the locations of the inducing points Z and also the parameter hyperpriors p(θ) and p(ω) since we assume them to be uniform, unless there is specific domain knowledge of the priors.
59	13	The gradients of the whitened posterior can be retrieved analytically as (Heinonen et al., 2016) ∇Ũ logL = L T θ∇U logL.
60	32	(15) Finally, we find a maximum a posteriori (MAP) estimate for the initial state x0, latent vector field Ũ , kernel parameters θ and noise variances ω by gradient ascent, x0,MAP, ŨMAP,θMAP,ωMAP = arg max x0,Ũ ,θ,ω logL, (16) while keeping the inducing locations Z fixed on a sufficiently dense grid (See Figure 1).
64	20	Given Equation (15) we only need to compute the gradients with respect to the inducing vectors u = vec(U) ∈ RMD, d log p(Y |x0,u,ω) du = N∑ s=1 d logN (ys|x(ts,u),Ω) dx dx(ts,u) du .
72	50	(22) The sensitivity equation provides us with an additional ODE system which describes the time evolution of the derivatives with respect to the inducing vectors S(t).
73	14	The sensitivities are coupled with the actual ODE system and, thus both systems x(t) and S(t) are concatenated as the new augmented state that is solved jointly by Equation (2) driven by the differentials ẋ(t) and Ṡ(t) (Leis and Kramer, 1988).
77	14	The sensitivity equation based approach is superior to the finite differences approximation because we have exact formulation for the gradients of state over inducing points, which can be solved up to the numerical accuracy of the ODE solver.
78	84	As first illustration of the proposed nonparametric ODE method we consider three simulated differential systems: the Van der Pol (VDP), FitzHugh-Nagumo (FHN) and Lotka-Volterra (LV) oscillators of form VDP : ẋ1 = x2 ẋ2 = (1− x21)x2 − x1 FHN : ẋ1 = 3(x1 − x31 3 + x2) ẋ2 = 0.2− 3x1 − 0.2x2 3 LV : ẋ1 = 1.5x1 − x1x2 ẋ2 = −3x2 + x1x2.
79	45	In the conventional ODE case the coefficients of these equations can be inferred using standard statistical techniques if sufficient amount of time series data is available (Girolami, 2008; Raue et al., 2013).
82	23	We employ 25 data points from one cycle of noisy observation data from VDP and FHN models, and 25 data points from 1.7 cycles from the LV model with a noise variance of σ2n = 0.1 2.
83	32	We learn the npODE model with five training sequences using M = 62 inducing locations on a fixed grid, and forecast between 4 and 8 future cycles starting from true initial state x0 at time 0.
86	23	The model accurately learns the dynamics from less than two cycles of data and can reproduce them reliably into future.
95	24	We use a benchmark dataset of human motion capture data from the Carnegie Mellon University motion capture (CMU mocap) database.
99	31	In order to tackle the problem of dimensionality, we project the original dataset with PCA to a three dimensional latent space where the system is specified, following Damianou et al. (2011) and Wang et al. (2006).
109	13	In a state-space or dynamical model a transition function x(tk+1) = g(x(tk)) moves the system forward in discrete steps.
116	12	In the forecasting task we train all models with the first half of the trajectory, while forecasting the second half starting from the first frame.
146	16	Furthermore, an interesting future avenue is the study of various vector field kernels, such as divergence-free, curl-free or spectral kernels (Remes et al., 2017).
150	21	Conventional ODE models have also been considered from the stochastic perspective with stochastic differential equation (SDE) models that commonly model the deterministic system drift and diffusion processes separately leading to a distribution of trajectories p(x(t)) (Archambeau et al., 2007; Garcı́a et al., 2017).
