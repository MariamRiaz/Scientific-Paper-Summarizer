55	2	In Section 3 we present a unifying framework for local convergence analysis of second order methods.
59	4	In Section 6, we derive the local convergence properties of inexact Newton methods from our framework.
96	3	Moreover, we will make the following two assumptions.
105	4	A sequence of vectors {x(t)} is said to converge linearly to a limit point x∗, if for some 0 < ρ < 1, lim sup t→∞ ‖∇F (x(t+1))‖M∗ ‖∇F (x(t))‖M∗ = ρ.
110	5	Second, they also enjoy the same algorithm procedure summarized as follows.
111	4	In each iteration, they first construct an approximate Hessian matrix H(t) such that (1− 0)H(t) ∇2F (x(t)) (1 + 0)H(t), (2) where 0 ≤ 0 < 1.
113	6	Finally, their update equation is given as x(t+1) = x(t)− p(t).
114	7	With this procedure, we regard these stochastic second order methods as approximate Newton methods.
118	3	H(t) is a positive definite matrix that satisfies Eqn.
133	3	until termination do 3: Construct an 0-subspace embedding matrix S for B(x(t)) and where ∇2F (x) is of the form ∇2F (x) = (B(x(t)))TB(x(t)), and calculate H(t) = [B(x(t))]TSTSB(x(t)); 4: Calculate p(t) ≈ argminp 12p TH(t)p− pT∇F (x(t)); 5: Update x(t+1) = x(t) − p(t); 6: end for Third, the unifying framework of Theorem 3 contains not only stochastic second order methods, but also the deterministic versions.
140	4	Theorem 4 Let F (x) satisfy the conditions described in Theorem 3.
167	3	The main difference is the construction manner of H(t) which should satisfy Eqn.
168	13	Algorithm 2 relies on the assumption that each ‖∇2fi(x)‖ is upper bounded (i.e., Eqn.
170	2	In ill-conditioned cases (i.e., Kσ is large), the subsampled Newton method in Algorithm 2 should take a lot of samples because the sample size |S| depends on Kσ quadratically.
173	5	Erdogdu & Montanari (2015) proposed NewSamp which is another regularized subsampled Newton method depicted in Algorithm 4.
174	2	In the following analysis, we prove that adding a regularizer is an effective way to reduce the sample size while keeping converging in theory.
187	3	The first term describes the relationship between the regularizer αI and sample size.
191	6	Conversely, if we want to sample a small part of fi’s, then we should choose a large α.
199	3	Therefore, a proper regularizer which balances the cost of each iteration and convergence rate is the key in the regularized subsampled Newton algorithm.
206	7	1: Input: x(0), 0 < δ < 1, r, sample size |S|; 2: for t = 0, 1, .
209	7	Set the subsampled size |S| such that |S| ≥ 16K 2 log(2d/δ) β2 , and define 0 = max ( β λ (t) r+1 − β , 2β + λ (t) r+1 σ + 2β + λ (t) r+1 ) , (12) which implies 0 < 0 < 1.
216	3	Without loss of generality, we can set β = λ(t)r+1/4.
226	2	(11), then 0 = 2β+λ (t) r+1 σ+2β+λ (t) r+1 which equals to the second term on the right-hand side in Eqn.
238	5	Now we validate that our theoretical result that sketched size is independent of the condition number of the Hessian in Sketch Newton.
242	3	Then the condition number of A is κ(A) = 1.254 = 1.8741 × 104.
265	14	In this paper, we have proposed a framework to analyze the local convergence properties of second order methods including stochastic and deterministic versions.
266	30	This framework reveals some important convergence properties of the subsampled Newton method and sketch Newton method, which are unknown before.
267	27	The most important thing is that our analysis lays the theoretical foundation of several important stochastic second order methods.
268	185	We believe that this framework might also provide some useful insights for developing new subsampled Newtontype algorithms.
269	186	We would like to address this issue in future.
