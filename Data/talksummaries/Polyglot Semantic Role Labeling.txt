0	46	The standard approach to multilingual NLP is to design a single architecture, but tune and train a separate model for each language.
2	28	The reality is that most languages have very little annotated data for most NLP tasks.
3	25	Ammar et al. (2016a) found that using training data from multiple languages annotated with Universal Dependencies (Nivre et al., 2016), and represented using multilingual word vectors, outperformed monolingual training.
11	18	To our knowledge, this is the first multilingual SRL approach to combine supervision from several languages.
12	33	The CoNLL 2009 dataset includes seven different languages, allowing study of trends across the same.
13	14	Unlike the Universal Dependencies dataset, however, the semantic label spaces are entirely language-specific, making our task more challenging.
14	28	Nonetheless, the success of polyglot training in this setting demonstrates that sharing of statistical strength across languages does not depend on explicit alignment in annotation conventions, and can be done simply through parameter sharing.
17	48	We include a breakdown into label categories of the differences between the monolingual and polyglot models.
19	14	We evaluate our system on the semantic role labeling portion of the CoNLL-2009 shared task (Hajič et al., 2009), on all seven languages, namely Catalan, Chinese, Czech, English, German, Japanese and Spanish.
21	17	Each predicate takes as arguments other words in the same sentence, their relationship marked by labeled dependency arcs.
24	9	Catalan, Chinese, English, German, and Spanish include (but are not limited to) labels such as “arg0-agt” (for “agent”) or “A0” that may correspond to some degree to each other and to the English roles.
36	12	Each token embedding is also concatenated with a vector indicating whether the word is a predicate or not.
50	24	In the first polyglot variant, we consider multilingual sharing between each language and English by using pretrained multilingual embeddings.
53	24	Pretrained multilingual embeddings.
54	10	The basis of our polyglot training is the use of pretrained multilingual word vectors, which allow representing entirely distinct vocabularies (such as the tokens of different languages) in a shared representation space, allowing crosslingual learning (Klementiev et al., 2012).
60	25	In the second variant, we concatenate a language ID vector to each multilingual word embedding and predicate indicator feature in the input representation.
64	13	In addition to processing every example with a shared biLSTM as in previous models, we add language-specific biLSTMs that are trained only on the examples belonging to one language.
65	10	Each of these languagespecific biLSTMs is two layers deep, and is combined with the shared biSLTM in the input to the third layer.
66	10	This adds a greater degree of languagespecific processing while still sharing representations across languages.
77	25	Nevertheless, our baseline is on par with the best published scores for Chinese, and it shows strong performance on most languages.
86	54	Future work on language pairs that do not include English could provide further insights.
87	17	Catalan and Spanish, for example, are closely related and use the same argument label set (both being drawn from the AnCora corpus) which would allow for sharing output representations as well as input tokens and parameters.
88	43	For each language pair, we also evaluated the simple polyglot model on the English test set from the CoNLL 2009 shared task (Table 4).
89	13	English SRL consistently benefits from polyglot training, with an increase of 0.25–0.7 absolute F1 points, depending on the language.
90	19	Surprisingly, Czech provides the smallest improvement, despite the large amount of data added; the absence of crosslingual transfer in both directions for the English-Czech case, breaking the pattern seen in other languages, could therefore be due to differences in annotation rather than questions of dataset size.
91	54	Table 5 provides unlabeled F1 scores for each language pair.
93	27	The pattern of seeing the largest improvements on the languages with the smallest datasets generally holds here: the largest F1 gains are in German and Catalan, followed by Japanese, with minimal or no improvement elsewhere.
106	17	In this work, we have explored a straightforward method for polyglot training in SRL: use multilingual word vectors and combine training data across languages.
107	11	This allows sharing without crosslingual alignments, shared annotation, or parallel data.
108	15	We demonstrate that a polyglot model can outperform a monolingual one for semantic analysis, particularly for languages with less data.
