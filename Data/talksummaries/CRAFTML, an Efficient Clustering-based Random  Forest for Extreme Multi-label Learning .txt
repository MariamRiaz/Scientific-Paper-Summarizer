0	20	Multi-label classification has received a tremendous attention in the last decade and recently, stimulated by real-life applications involving large datasets (e.g. image (Partalas et al., 2015) and text (Deng et al., 2009) annotation and product recommendation (McAuley et al., 2015)), it has been extended to problems where the number of labels can exceed one million (Agrawal et al., 2013).
1	3	In this new context, called eXtreme Multi-label Learning (XML), most of the classical algorithms face scalability issues (Weston et al., 2013) or performance degradation.
2	38	In an attempt to overcome these challenges, researchers have recently explored three directions: (i) using optimization tricks such as pri- mal/dual conversion or sparsification (Yen et al., 2016) and parallelization on supercomputers (Babbar & Schölkopf, 2017; Yen et al., 2017), (ii) reducing the data dimensionality for solving a smaller size problem with a classical approach (Yu et al., 2014; Bhatia et al., 2015) or (iii) hierarchically partitioning the initial problem into small scale sub-problems (Jasinska et al., 2016; Jain et al., 2016).
5	16	Moreover, by decomposing learning into subtasks, it reduces the learning/prediction complexity and opens the way to parallelization.
6	4	Finally, its sequence of successive decisions allows a great expressivity.
7	26	Motivated by those properties, we here present a novel fast and accurate tree-based approach called CRAFTML (Clustering-based RAndom Forest of predictive Trees for extreme Multi-label Learning).
8	63	Similarly to PFastReXML (Jain et al., 2016) which is among the best tree-based approaches for XML, CRAFTML is a forest of decision trees trained with the supervision of the labels where the splitting conditions are based on all the features.
9	14	But CRAFTML has two major differences with PFastReXML: (i) it exploits a random forest strategy which not only randomly reduces both the feature and the label spaces to obtain diversity but also replaces random selections with random projections to preserve more information; (ii) it uses a novel lowcomplexity splitting strategy which avoids the resolution of a multi-objective optimization problem at each node.
10	3	Numerical experiments on nine datasets from the XML literature show that CRAFTML outperforms the existing XML tree-based approaches with a lower training time and a smaller memory size.
67	3	CRAFTML computes a forest F of mF k-ary instance trees whose construction follows the common scheme of the instance tree-based methods (see Algorithm 1) recalled in Section 2.
72	6	Therefore, the node training Algorithm 1 trainTree Input: Training set with a feature matrix X and a label matrix Y .
76	3	and (Y ′ s )j,.. stage in CRAFTML is decomposed into three consecutive steps (see Algorithm 2): 1. a random projection into lower dimensional spaces of the label and feature vectors of the node’s instances.
77	37	2. a k-means based partitioning of the instances into k temporary subsets from their projected labels.
78	2	The training of a simple multi-class classifier to assign each instance to its relevant temporary subset (i.e. cluster index computed at step 2) from its feature vector.
79	36	The instances are partitioned into k final subsets (child nodes) by the classifier (”split” in Algorithm 1).
80	28	In the prediction phase, for each tree, the input instance follows a root-to-leaf path determined by the successive decisions of the classifier and the provided prediction is the average label vector stored in the leaf reached.
81	5	The forest aggregates the tree predictions with the average operator.
82	48	Let us specify that contrary to classical random forests which use bootstraps, each tree of CRAFTML is trained on the full initial dataset.
85	20	We here detail the three steps of the instance partitioning process in each node v of a tree T of a forest F .
86	52	Step 1: Random Projection of the Instances of v The feature and label vectors x and y of each instance of v are projected into a space with a lower dimensionality: x′ = xPx and y′ = yPy where Px (resp Py) is a random projection matrix of Rdx×d′x (resp.
87	4	d′y) is the dimension of the reduced feature (resp.
92	35	It led to slightly better performances.
95	51	We have considered four combinations: SxSy, SxDy, DxSy and DxDy where Sx (resp.
96	18	Sy) is the case where the feature (resp.
97	3	label) projections are the Same in each node of T and Dx (resp.
102	29	The cosine metric of the spherical k-means is fast to compute and is welladapted to sparse data.
103	20	The cluster centroids are initialized with the k-means++ strategy (Arthur & Vassilvitskii, 2007) which improves cluster stability and algorithm performances against a random initialization.
108	63	In this section, we provide bounds for the time and memory complexities of CRAFTML.
109	10	Then, we analyze the impact of the hyperparameters which govern its performances, and we recommend a parameter setting adapted to XML data.
