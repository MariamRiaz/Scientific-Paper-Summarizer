0	20	Deep neural networks (NNs) have achieved impressive performance in a wide variety of tasks in recent years, however, success is generally in terms of aggregated accuracy metrics.
1	21	For many real-world applications, it is not enough that on average a model performs well, rather the uncertainty of each prediction must also be quantified.
2	5	This can be particularly important where there is a large downside to an incorrect prediction: Examples can be found in prognostics, manufacturing, finance, weather, traffic and energy networks.
3	27	There is therefore interest in how NNs can be modified to meet this requirement (Krzywinski & Altman, 2013; Gal, 2016).
4	32	In this work the output of prediction intervals (PIs) in regression tasks is considered.
5	46	Whilst NNs by default output point estimates, PIs directly communicate uncertainty, offering a lower and upper bound for a prediction and assurance that, with some high probability (e.g. 95% or 99%), the realised data point will fall between these bounds.
7	57	As an example, a point estimate stating that a machine will fail in 60 days may not be sufficient to schedule a repair, however given a PI of 45-65 days with 99% probability, timing of a repair is easily scheduled.
10	32	In this work we formulate PI output as a constrained optimisation problem.
11	117	It is self evident that high-quality PIs should be as narrow as possible, whilst capturing some specified proportion of data points (hereafter referred to as the HQ principle).
12	20	Indeed it is through these metrics that PI quality is often assessed (Papadopoulos et al., 2000; Khosravi et al., 2011b; Galván et al., 2017).
13	79	We show how a loss function can be derived directly from this HQ principle, and used in an ensemble to produce PIs accounting for both model uncertainty and data noise variance.
19	4	Implementations therefore require nongradient based methods for training, such as Simulated Annealing (SA) and Particle Swarm Optimisation (PSO).
21	6	• Loss Form - Its current form suffers from several problems.
23	16	It was also designed through qualitative assessment of the desired behaviour rather than on a statistical basis.
24	30	• Model Uncertainty - LUBE accounts only for datanoise variance and not model uncertainty (section 2.1).
25	14	This is an oversimplification (Heskes, 1996), implicitly assuming that training data fully populates the input space, which is seldom the case.
27	9	We link early literature on PIs for NNs (Tibshirani, 1996; Heskes, 1996; Papadopoulos et al., 2000; Khosravi et al., 2011a), with recent work on uncertainty in deep learning (Hernández-Lobato & Adams, 2015; Gal & Ghahramani, 2015; Lakshminarayanan et al., 2017) - areas which have remained surprisingly distinct.
54	19	We attempt to reconcile them here.
60	10	However, when estimating the uncertainty of y, additional terms must be estimated.
61	7	(1) have associated sources of uncertainty, and assuming they are independent, the total variance of observations is given by, σ2y = σ 2 model + σ 2 noise, (2) with σ2model termed model uncertainty or epistemic uncertainty - uncertainty in f̂(x) - and σ2noise irreducible variance, data noise variance, or aleatoric uncertainty.
65	11	Model uncertainty can be attributed to several factors.
69	5	• Parameter uncertainty - Uncertainty exists around the optimum parameters of the model, increasing in regions sparsely represented in the training data.
70	10	Different model types have different weightings for each of these factors (bias-variance trade-off ).
72	19	Work on uncertainty in NNs therefore generally ignores model misspecification, and only estimates training data uncertainty and parameter uncertainty (Heskes, 1996).
73	12	To construct PIs, σ2y must be estimated at each prediction point.
75	25	In regions of the input space with little data, σ2model grows.
76	12	Lakshminarayanan et al. (2017) recognise this in more intuitive terms - that two sources of uncertainty exist.
80	24	Let the set of input covariates and target observations be X and y, for n data points, and with xi ∈ RD denoting the ith D dimensional input corresponding to yi, for 1 ≤ i ≤ n. The predicted lower and upper PI bounds are ŷL, ŷU.
82	28	(3) A vector, k, of length n represents whether each data point has been captured by the estimated PIs, with each element ki ∈ {0, 1} given by, ki = { 1, if yLi ≤ yi ≤ yUi 0, else.
85	12	(7) According to the HQ principle, PIs should minimise MPIW subject to PICP ≥ (1 − α).
