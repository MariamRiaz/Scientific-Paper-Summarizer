16	95	[G301, U101 ] Who are the professors working on unsupervised machine learning?
22	8	tional queries (e.g., “all CS undergrads who took my class last semester”), while at the same time (ii) personal knowledge generally evolves through time and has an open and growing set of relations, making natural language the only practical interface for creating and maintaining that knowledge by non-expert users.
23	34	In short, the task that we address in this work is: multi-relational question answering from dynamic knowledge expressed via narrative.
24	204	Although we hypothesize that questionanswering over personal knowledge of this sort is ubiquitous (e.g., between a professor and their administrative assistant, or even if just in the user’s head), such interactions are rarely recorded, presenting a significant practical challenge to collecting a sufficiently large real-world dataset of this type.
56	40	In this work, we synthesize narratives in five diverse worlds, each containing a thousand narratives and where each narrative describes the evolution of a simulated user’s world from a firstperson perspective.
57	23	In each narrative, the simu- lated user may introduce new knowledge, update existing knowledge or express a state change (e.g., “Homework 3 is now due on Friday” or “Samantha passed her thesis defense”).
58	20	Each narrative is interleaved with questions about the current state of the world, and questions range in complexity depending on the amount of knowledge that needs to be integrated to answer them.
59	35	This allows us to benchmark a range of QA models at their ability to answer questions that require different extents of relational reasoning to be answered.
66	13	Each world is represented by a set of entities E and a set of unary, binary or ternary relations R. Formally, a single step in one simulation of a world involves a combination of instantiating new entities and defining new (or mutating existing) relations between entities.
69	9	Each simulation step is then expressed as a natural language statement, which is added to the narrative.
73	46	Furthermore, all generated stories also provide additional annotation that maps all entities to underlying gold-standard KB ids, allowing to perform experiments that provide models with different degrees of access to the “simulation oracle”.
80	14	In the narrative, the questions are expressed in natural language, employing the same anaphora mechanism used in generating the narrative (e.g., “who is attending the last meeting I added?”).
82	22	We categorize each question in our dataset into one of the following four categories: Single Entity/Single Relation Answers to these questions are a single entity, e.g. “what is John’s email address?”, or expressed in lambda-calculus notation: λx.EmailAddress(John, x) The answers to these questions are found in a single sentence in the narrative, although it is possible that the answer may change through the course of the narrative (e.g., “John’s new office is GHC122”).
83	71	Multi-Entity/Single Relation Answers to these questions can be multiple entities but involve a single relation, e.g., “Who is enrolled in the Math class?”, or expressed in lambda calculus notation: λx.TakingClass(x, Math) Unlike the previous category, answers to these questions can be sets of entities.
84	16	Multi-Entity/Two Relations Answers to these questions can be multiple entities and involve two relations, e.g., “Who is enrolled in courses that I am teaching?”, or expressed in lambda calculus: λx.∃y.EnrolledInClass(x, y) ∧ CourseTaughtByMe(y) Multi-Entity/Three Relations Answers to these questions can be multiple entities and involve three relations, e.g., “Which undergraduates are enrolled in courses that I am teaching?”, or expressed in lambda calculus notation: λx.∃y.EnrolledInClass(x, y) ∧ CourseTaughtByMe(y) ∧ Undergrad(x) In the data that we generate, answers to questions are always sets of spans in the narrative (the reason for this constraint is for easier evaluation of several existing machine-reading models; this assumption can easily be relaxed in the simulation).
86	7	We develop several baselines for our QA task, including a logistic regression model and four different neural network models: Seq2Seq (Bahdanau et al., 2015), MemN2N (Sukhbaatar et al., 2015), BiDAF (Seo et al., 2017), and DrQA (Chen et al., 2017).
100	7	During test, we search for an optimal threshold that maximizes the F1 performance on the validation data.
143	42	In the acrossworld evaluation setting, the model is trained on four out of the five worlds, and tested on the remaining world.
144	12	The across-world regime is obviously more challenging, as it requires the model to be able to learn to generalize to unseen relations and vocabulary.
145	32	We consider the across-world evaluation setting to be the main evaluation criteria for any future models used on this dataset, as it mimics the practical requirement of any QA system used in personal assistants: it has to be able to answer questions on any new domain the user introduces to the system.
147	141	First, we observe that more compositional questions (i.e., those that integrate multiple relations) are more challenging for most models - as all models (except Seq2seq) decrease in performance with the number of relations composed in a question (Figure 5.1).
149	17	One surprising observation from our results is that the performance on questions that ask about a single relation and have only a single answer is lower than questions that ask about a single relation but that can have multiple answers (see detailed results in the Appendix).
150	115	This is in part because questions that can have multiple answers typically have canonical entities as answers (e.g., person’s name), and these entities generally repeat in the text, making it easier for the model to find the correct answer.
166	10	Our hypothesis is that this task will become increasingly important as users begin to teach personal knowledge about their world to the personal assistants embedded in their devices.
167	191	This task naturally synthesizes two main branches of question answering research: QA over KBs and QA over free text.
168	146	One of our main contributions is a collection of diverse datasets that feature rich compositional questions over a dynamic knowledge graph expressed through simulated narrative.
169	53	Another contribution of our work is a thorough set of experiments and analysis of different types of endto-end architectures for QA at their ability to answer multi-relational questions of varying degrees of compositionality.
170	209	Our long-term goal is that both the data and the simulation code we release will inspire and motivate the community to look towards the vision of letting end-users teach our personal assistants about the world around us.
171	112	The TEXTWORDSQA dataset and the code can be downloaded at https://igorlabutov.
172	54	github.io/textworldsqa.github.io/
