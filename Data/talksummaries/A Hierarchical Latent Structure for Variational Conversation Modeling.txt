25	78	To the best of our knowledge, our VHCR is the first VAE conversation model that exploits the hierarchical latent structure.
47	34	A datapoint x is generated from a latent variable z, which is sampled from some prior distribution p(z), typically a standard Gaussian distribution N (z|0, I).
48	20	We assume parametric families for conditional distribution pθ(x|z).
49	24	Since it is intractable to compute the log-marginal likelihood log pθ(x), we approximate the intractable true posterior pθ(z|x) with a recognition model qφ(z|x) to maximize the variational lower-bound: log pθ(x) ≥ L(θ,φ;x) (2) = Eqφ(z|x)[− log qφ(z|x) + log pθ(x, z)] = −DKL(qφ(z|x)‖p(z))+Eqφ(z|x)[log pθ(x|z)] Eq.
50	14	2 is decomposed into two terms: KL divergence term and reconstruction term.
51	18	Here, KL divergence measures the amount of information encoded in the latent variable z.
55	31	Serban et al. (2017) propose Variational Hierarchical Recurrent Encoder Decoder (VHRED) model for conversation modeling.
56	14	It integrates an utterance latent variable zuttt into the HRED structure (Sordoni et al., 2015a) which consists of three RNN components: encoder RNN, context RNN, and decoder RNN.
60	27	Finally the decoder RNN fdecθ generates the utterance xt, conditioned on the context vector hcxtt and the latent variable zuttt (Eq.
64	16	A known problem of a VAE that incorporates an autoregressive RNN decoder is the degeneracy that ignores the latent variable z.
66	34	2 goes to zero and the decoder fails to learn any dependency between the latent variable and the data.
67	16	Eventually, the model behaves as a vanilla RNN.
70	17	2 using a KL multiplier λ, which gradually increases from 0 to 1 during training: L̃(θ,φ;x) = −λDKL(qφ(z|x)‖p(z)) (12) +Eqφ(z|x)[log pθ(x|z)] This helps the optimization process to avoid local optima of zero KL divergence in early training.
74	16	The word drop probability is normally set to 0.25, since using a higher probability may degrade the model performance (Bowman et al., 2016).
82	80	2 plots the ratios of E[σ2t ]/Var(µt), where E[σ2t ] indicates the within variance of the priors, and Var(µt) is the between variance of the priors.
84	18	The ratio gradually falls to zero, implying that the priors degenerate to separate point masses as training proceeds.
85	15	Moreover, we find that the degeneracy of priors coincide with the degeneracy of KL divergence, as shown in (Fig.
89	28	However, conditioning on the context makes the range of training target xt very sparse; even in a large-scale conversation corpus such as Ubuntu Dialog (Lowe et al., 2015), there exist one or very few target utterances per context.
91	12	Consequently, the VHRED will not encode any information in the latent variable, i.e. it degenerates.
92	20	It explains why the word drop fails to prevent the degeneracy in the VHRED.
93	21	The word drop only regularizes the decoder RNN; however, the context RNN is also powerful enough to predict a next utterance in a given context even with the weakened decoder RNN.
96	30	This finding hints us that in order to train a nondegenerate latent variable model, we need to design a model that provides an appropriate way to regularize the hierarchical RNN decoders and alleviate data sparsity per context.
100	104	We introduce a global conversation latent variable zconv which is responsible for generating a sequence of utterances of a conversation c = {x1, .
128	22	Moreover, it allows us to control such global properties, which is impossible for models without hierarchical latent structure.
134	11	2) Ubuntu Dialog Corpus (Lowe et al., 2015), containing about 1 million multi-turn conversations from Ubuntu IRC channels.
156	12	The average metric projects each utterance to a vector by taking the mean over word embeddings in the utterance, and computes the cosine similarity between the model response vector and the ground truth vector.
181	24	We see that zconv controls the overall tone and content of conversations; for example, the tone of the response is friendly in the first sample, but gradually becomes hostile as zconv changes.
188	17	We noted that the degeneration problem in existing VAE models such as the VHRED is persistent, and proposed a hierarchical latent variable model with the utterance drop regularization.
189	16	Our VHCR obtained higher and more stable KL divergences than various versions of VHRED models without using any auxiliary objective.
190	15	The empir- ical results showed that the VHCR better reflected the structure of natural conversations, and outperformed previous models.
