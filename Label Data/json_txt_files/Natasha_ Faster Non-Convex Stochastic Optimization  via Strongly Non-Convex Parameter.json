{"sections": [{"heading": "1 Introduction", "text": "We study the problem of composite non-convex minimization:\nmin x\u2208Rd\n{ F (x) := \u03c8(x) + f(x) := \u03c8(x) + 1\nn n\u2211 i=1 fi(x) }\n(1.1) where each fi(x) is nonconvex but smooth, and \u03c8(\u00b7) is proper convex, possibly nonsmooth, but relatively simple. We are interested in finding a point x that is an approximate local minimum of F (x).\n\u2022 The finite-sum structure f(x) = 1n \u2211n i=1 fi(x) arises\nprominently in large-scale machine learning tasks. In particular, when minimizing loss over a training set, each example i corresponds to one loss function fi(\u00b7) in the summation. This finite-sum structure allows one to perform stochastic gradient descent with respect to a\nFuture version of this paper shall be found at http:// arxiv.org/abs/1702.00763. 1Microsoft Research. Correspondence to: Zeyuan Allen-Zhu <zeyuan@csail.mit.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nrandom\u2207fi(x). \u2022 The so-called proximal term \u03c8(x) adds more general-\nity to the model. For instance, if \u03c8(x) is the indicator function of a convex set, then problem (1.1) becomes constraint minimization; if \u03c8(x) = \u2016x\u20161, then we can allow problem (1.1) to perform feature selection. In general, \u03c8(x) has to be a simple function where the projection operation arg minx{\u03c8(x) + 12\u03b7\u2016x \u2212 x0\u2016\n2} is efficiently computable. At a first reading of this paper, one can assume \u03c8(x) \u2261 0 for simplicity.\nMany non-convex machine learning problems fall into problem (1.1). Most notably, training deep neural networks and classifications with sigmoid loss correspond to (1.1) where neither fi(x) or f(x) is convex. However, our understanding to this challenging non-convex problem is very limited."}, {"heading": "1.1 Strongly Non-Convex Optimization", "text": "LetL be the smoothness parameter for each fi(x), meaning all the eigenvalues of\u22072fi(x) lie in [\u2212L,L].1\nWe denote by \u03c3 \u2208 [0, L] the strong-nonconvexity parameter of f(x) = 1n \u2211n i=1 fi(x), meaning that\nall the eigenvalues of\u22072f(x) lie in [\u2212\u03c3, L].\nWe emphasize that parameter \u03c3 is analogous to the strongconvexity parameter \u00b5 for convex optimization, where all the eigenvalues of \u22072f(x) lie in [\u00b5,L] for some \u00b5 > 0. We wish to find an \u03b5-approximate stationary point (a.k.a. critical point) of F (x), that is\na point x satisfying \u2016G(x)\u2016 \u2264 \u03b5\nwhere G(x) is the so-called gradient mapping of F (x) (see Section 2 for a formal definition). In the special case of \u03c8(\u00b7) \u2261 0, gradient mapping G(x) is the same as gradient \u2207f(x), so x satisfies \u2016\u2207f(x)\u2016 \u2264 \u03b5. Since f(\u00b7) is \u03c3-strongly nonconvex, any \u03b5-approximate stationary point is automatically also an (\u03b5, \u03c3)-approximate local minimum \u2014 meaning that the Hessian of the output point \u22072f(x) \u2212\u03c3I is approximately positive semidefinite (PSD).\n1This definition also applies to functions f(x) that are not twice differentiable, see Section 2 for details."}, {"heading": "1.2 Motivations and Remarks", "text": "\u2022 We focus on strongly non-convex optimization because introducing this parameter \u03c3 allows us to perform a more refined study of non-convex optimization. If \u03c3 equals L then L-strongly nonconvex optimization is equivalent to the general non-convex optimization.\n\u2022 We focus only on finding stationary points as opposed to local minima, because in a recent study \u2014 see Appendix A\u2014 researchers have shown that finding (\u03b5, \u03b4)-approximate local minima reduces to finding \u03b5approximate stationary points in an O(\u03b4)-strongly nonconvex function.\n\u2022 Parameter \u03c3 is often not constant and can be much smaller than L. For instance, second-order methods often find (\u03b5, \u221a \u03b5)-approximate local minima (Nesterov,\n2008) and this corresponds to \u03c3 = \u221a \u03b5."}, {"heading": "1.3 Known Results", "text": "Despite the widespread use of nonconvex models in machine learning and related fields, our understanding to nonconvex optimization is still very limited. Until recently, nearly all research papers have been mostly focusing on either \u03c3 = 0 or \u03c3 = L:\n\u2022 If \u03c3 = 0, the accelerated SVRG method (ShalevShwartz, 2016; Allen-Zhu & Yuan, 2016) finds x satisfying F (x) \u2212 F (x\u2217) \u2264 \u03b5, in gradient complexity O\u0303 ( n + n3/4 \u221a L/\u03b5 ) .2 This result is irrelevant to this\npaper because f(x) is simply convex.\n\u2022 If \u03c3 = L, the SVRG method (Allen-Zhu & Hazan, 2016) finds an \u03b5-approximate stationary point of F (x) in gradient complexity O(n+ n2/3L/\u03b52).\n\u2022 If \u03c3 = L, gradient descent finds an \u03b5-approximate stationary point in gradient complexity O(nL/\u03b52).\n\u2022 If \u03c3 = L, stochastic gradient descent finds an \u03b5-approx. stationary point in gradient complexity O(L2/\u03b54).\nThroughout this paper, we refer to gradient complexity as the total number of stochastic gradient computations \u2207fi(x) and proximal computations y \u2190 Prox\u03c8,\u03b7(x) := arg miny{\u03c8(y) + 12\u03b7\u2016y \u2212 x\u2016 2}.3 Very recently, it was observed by two independent groups (Agarwal et al., 2017; Carmon et al., 2016) \u2014 although implicitly, see Section 2.1\u2014 that for solving the \u03c3-strongly nonconvex problem, one can repeatedly regularize F (x) to make it \u03c3-strongly convex, and then apply the accelerated SVRG method to minimize this regularized\n2We use O\u0303 to hide poly-logarithmic factors in n,L, 1/\u03b5. 3Some authors also refer to them as incremental first-order oracle (IFO) and proximal oracle (PO) calls. In most machine learning applications, each IFO and PO call can be implemented to run in time O(d) where d is the dimension of the model, or even in time O(s) if s is the average sparsity of the data vectors.\nfunction. Under mild assumption \u03c3 \u2265 \u03b52, this approach \u2022 finds an \u03b5-approximate stationary point in gradient\ncomplexity O\u0303 ( n\u03c3+n3/4 \u221a L\u03c3\n\u03b52\n) .\nWe call this method repeatSVRG in this paper. Unfortunately, repeatSVRG is even slower than the vanilla SVRG for \u03c3 = L by a factor n1/3, see Figure 1.\nRemark on SGD. Stochastic gradient descent (SGD) has a slower convergence rate (i.e., in terms of 1/\u03b54) than other cited first-order methods (i.e., in terms of 1/\u03b52), see for instance (Ghadimi & Lan, 2015). However, the complexity of SGD does not depend on n and thus is incomparable to gradient descent, SVRG, or repeatSVRG.4 This is one of the main motivations to study how to reduce the complexity of non-SGD methods, especially in terms of n."}, {"heading": "1.4 Our New Results", "text": "In this paper, we identify an interesting dichotomy with respect to the spectrum of the nonconvexity parameter \u03c3 \u2208 [0, L]. In particular, we showed that if \u03c3 \u2265 L/ \u221a n, then our new method Natasha finds an \u03b5-approximate stationary point of F (x) in gradient complexity\nO ( n log 1\n\u03b5 + n2/3(L2\u03c3)1/3 \u03b52\n) .\nIn other words, together with repeatSVRG, we have improved the gradient complexity for \u03c3-stringly nonconvex optimization to5\nO\u0303 ( min {n3/4\u221aL\u03c3\n\u03b52 , n2/3(L2\u03c3)1/3 \u03b52 }) and the first term in the min is smaller if \u03c3 < L/ \u221a n and the second term is smaller if \u03c3 > L/ \u221a n. We illustrate our\n4In practice, there are examples in non-convex empirical risk minimization (Allen-Zhu & Hazan, 2016) and in training neural networks (Allen-Zhu & Hazan, 2016; Reddi et al., 2016) where SVRG can outperform SGD. Of course, for deep learning tasks, SGD remains to be the best practical method of choice.\n5We remark here that this is under mild assumptions for \u03b5 being sufficiently small. For instance, the result of (Agarwal et al., 2017; Carmon et al., 2016) requires \u03b52 \u2264 \u03c3. In our result, the term n log 1\n\u03b5 disappears when \u03b56 \u2264 L2\u03c3/n.\nperformance improvement in Figure 1. Our result matches that of SVRG for \u03c3 = L, and has a much simpler analysis.\nAdditional Results. One can take a step further and ask what if each function fi(x) is (`1, `2)-smooth for parameters `1, `2 \u2265 \u03c3, meaning that all the eigenvalues of\u22072fi(x) lie in [\u2212`2, `1]. We show that a variant of our method, which we call Natashafull, solves this more refined problem of (1.1) with total gradient complexity O ( n log 1\u03b5 + n2/3(`1`2\u03c3) 1/3 \u03b52 ) as long as `1`2\u03c32 \u2264 n 2. Remark 1.1. In applications, `1 and `2 can be of very different magnitudes. The most influential example is finding the leading eigenvector of a symmetric matrix. Using the so-called shift-and-invert reduction (Garber et al., 2016), computing the leading eigenvector reduces to the convex version of problem (1.1), where each fi(x) is (\u03bb, 1)- smooth for \u03bb 1. Other examples include all the applications that are built on shift-and-invert, including high rank SVD/PCA (Allen-Zhu & Li, 2016), canonical component analysis (Allen-Zhu & Li, 2017a), online matrix learning (Allen-Zhu & Li, 2017b), and approximate local minima algorithms (Agarwal et al., 2017; Carmon et al., 2016).\nMini-Batch. Our result generalizes trivially to the minibatch stochastic setting, where in each iteration one computes\u2207fi(x) for b random choices of index i \u2208 [n] and average them. The stated gradient complexities of Natasha and Natashafull can be adjusted so that the factor n2/3 is replaced with n2/3b1/3."}, {"heading": "1.5 Our Techniques", "text": "Let us first recall the main idea behind stochastic variancereduced methods, such as SVRG (Johnson & Zhang, 2013).\nThe SVRG method divides iterations into epochs, each of length n. It maintains a snapshot point x\u0303 for each epoch, and computes the full gradient \u2207f(x\u0303) only for snapshots. Then, in each iteration t at point xt, SVRG defines gradient estimator \u2207\u0303 = \u2207fi(xt)\u2212\u2207fi(x\u0303)+\u2207f(x\u0303) which satisfies Ei[\u2207\u0303] = \u2207f(xt), and performs proximal update xt+1 \u2190 Prox\u03c8,\u03b1 ( xt \u2212 \u03b1\u2207\u0303 ) for some learning rate \u03b1. (Recall that if \u03c8(\u00b7) \u2261 0 then we would have xt+1 \u2190 xt \u2212 \u03b1\u2207\u0303.) In nearly all the aforementioned results for nonconvex optimization, researchers have either directly applied SVRG (Allen-Zhu & Hazan, 2016) (for the case \u03c3 = L), or repeatedly applied SVRG (Agarwal et al., 2017; Carmon et al., 2016) (for general \u03c3 \u2208 [0, L]). This puts some limitation in the algorithmic design, because SVRG requires each epoch to be of length exactly n.6\n6The epoch length of SVRG is always n (or a constant multiple of n in practice), because this ensures the computation of \u2207\u0303 is of amortized gradient complexity O(1). The per-iteration complexity of SVRG is thus the same as the traditional stochastic\nOur New Idea. In this paper, we propose Natasha and Natashafull, two methods that are no longer black-box reductions to SVRG. Both of them still divide iterations into epochs of length n, and compute gradient estimators \u2207\u0303 the same way as SVRG. However, we do not apply compute xt \u2212 \u03b1\u2207\u0303 directly. \u2022 In our base algorithm Natasha, we divide each epoch\ninto p sub-epochs, each with a starting vector x\u0302. Our theory suggests the choice p \u2248 ( \u03c3 2\nL2n) 1/3. Then, we\nreplace the use of \u2207\u0303 with \u2207\u0303 + 2\u03c3(xt \u2212 x\u0302). This is equivalent to replacing f(x) with its regularized version f(x)+\u03c3\u2016x\u2212 x\u0302\u20162, where the center x\u0302 varies across subepochs. We provide pseudocode in Algorithm 1 and illustrate it in Figure 2.\nWe view this additional term 2\u03c3(xt \u2212 x\u0302) as a type of retraction, which stabilizes the algorithm by moving the vector a bit in the backward direction towards x\u0302.\n\u2022 In our full algorithm Natashafull, we add one more ingredient on top of Natasha. That is, we perform updates zt+1 \u2190 Prox\u03c8,\u03b1(zt \u2212 \u03b1\u2207\u0303) with respect to a different sequence {zt}, and then define xt = 12zt + 1 2 x\u0302\nand compute gradient estimators \u2207\u0303 at points xt. We provide pseudocode in Algorithm 2 in the appendix.\nWe view this averaging xt = 12zt + 1 2 x\u0302 as another type of retraction, which stabilizes the algorithm by moving towards x\u0302. The technique of computing gradients at points xt but moving a different sequence of points zt is related to the Katyusha momentum recently developed for convex optimization (Allen-Zhu, 2017)."}, {"heading": "1.6 Other Related Work", "text": "Methods based on variance-reduced stochastic gradients were first introduced for convex optimization. The first such method is SAG by Schmidt et al (Schmidt et al., 2013). The two most popular choices for gradient estimators are the SVRG-like one we adopted in this paper (independently introduced by (Johnson & Zhang, 2013; Zhang et al., 2013), and the SAGA-like one introduced by (Defazio et al., 2014). In nearly all applications, the results proven for SVRG-like estimators and SAGA-like estimators are simply exchangeable (therefore, the results of this paper naturally generalize to SAGA-like estimators).\nThe first \u201cnon-convex use\u201d of variance reduction is by Shalev-Shwartz (Shalev-Shwartz, 2016) who assumes that each fi(x) is non-convex but their average f(x) is still convex. This result has been slightly improved to several more refined settings (Allen-Zhu & Yuan, 2016). The first truly non-convex use of variance reduction (i.e., for f(x) being also non-convex) is independently by (Allen-Zhu & Hazan, 2016) and (Reddi et al., 2016). First-order methods only\ngradient descent (SGD).\nfind stationary points (unless there is extra assumption on the randomness of the data), and converge no faster than 1/\u03b52.\nWhen the second-order Hessian information is used, one can (1) find local minima instead of stationary points, and (2) improve the 1/\u03b52 rate to 1/\u03b51.5. The first such result is by cubic-regularized Newton\u2019s method (Nesterov, 2008); however, its per-iteration complexity is very high. Very recently, two independent groups of authors tackled this problem from a somewhat similar viewpoint (Carmon et al., 2016; Agarwal et al., 2017): if the computation of Hessian-vector multiplications (i.e., ( \u22072fi(x) ) v) is on the same order of the computation of gradients \u2207fi(x),7 then one can obtain a (\u03b5, \u221a \u03b5)-approximate local minimum in\ngradient complexity O\u0303 ( n \u03b51.5 + n3/4 \u03b51.75 ) , if we use big-O to also hide dependencies on the smoothness parameters.\nOther related papers include Ge et al. (Ge et al., 2015) where the authors showed that a noise-injected version of SGD converges to local minima instead of critical points, as long as the underlying function is \u201cstrict-saddle.\u201d Their theoretical running time is a large polynomial in the dimension. Lee et al. (Lee et al., 2016) showed that gradient descent, starting from a random point, almost surely converges to a local minimum if the function is \u201cstrict-saddle\u201d. The rate of convergence required is somewhat unknown."}, {"heading": "2 Preliminaries", "text": "Throughout this paper, we denote by \u2016 \u00b7 \u2016 the Euclidean norm. We use i \u2208R [n] to denote that i is generated from [n] = {1, 2, . . . , n} uniformly at random. We denote by \u2207f(x) the full gradient of function f if it is differentiable, and \u2202f(x) any subgradient if f is only Lipschitz continuous at point x. We let x\u2217 be any minimizer of F (x).\nRecall some definitions on strong convexity (SC), strongly nonconvexity, and smoothness.\nDefinition 2.1. For a function f : Rd \u2192 R, 7A lot of interesting problems satisfy this property, including training neural nets.\n\u2022 f is \u03c3-strongly convex if \u2200x, y \u2208 Rd, it satisfies\nf(y) \u2265 f(x) + \u3008\u2202f(x), y \u2212 x\u3009+ \u03c3 2 \u2016x\u2212 y\u20162 .\n\u2022 f is \u03c3-strongly nonconvex if \u2200x, y \u2208 Rd, it satisfies\nf(y) \u2265 f(x) + \u3008\u2202f(x), y \u2212 x\u3009 \u2212 \u03c3 2 \u2016x\u2212 y\u20162 .\n\u2022 f is (`1, `2)-smooth if \u2200x, y \u2208 Rd, it satisfies\nf(x) + \u3008\u2207f(x), y \u2212 x\u3009+ `12 \u2016x\u2212 y\u2016 2 \u2265 f(y)\n\u2265 f(x) + \u3008\u2207f(x), y \u2212 x\u3009 \u2212 `2 2 \u2016x\u2212 y\u20162 .\n\u2022 f is L-smooth if it is (L,L)-smooth.\nThe (`1, `2)-smoothness parameters were introduced in (Allen-Zhu & Yuan, 2016) to tackle the convex setting of problem (1.1). The notion of strong nonconvexity is also known as \u201calmost convexity (Carmon et al., 2016)\u201d or \u201clower smoothness (Allen-Zhu & Yuan, 2016).\u201d We refrain from using the name \u201calmost convexity\u201d because it coincides with several other non-equivalent definitions in optimization literatures.\nDefinition 2.2. Given a parameter \u03b7 > 0, the gradient mapping of F (\u00b7) in (1.1) at point x is\nG\u03b7(x) := 1\n\u03b7\n( x\u2212 x\u2032 ) where x\u2032 = arg miny { \u03c8(y) + \u3008\u2207f(x), y\u3009+ 12\u03b7\u2016y\u2212x\u2016 2 }\n."}, {"heading": "In particular, if \u03c8(\u00b7) \u2261 0, then G\u03b7(x) \u2261 \u2207f(x).", "text": "The following theorem for the SVRG method can be found for instance in (Allen-Zhu & Yuan, 2016), which is built on top of the results (Shalev-Shwartz, 2016; Lin et al., 2015; Frostig et al., 2015):\nTheorem 2.3 (SVRG). Let G(y) := \u03c8(y) + 1n \u2211n i=1 gi(y) be \u03c3-strongly convex, then the SVRG method finds a point y satisfying G(y)\u2212G(y\u2217) \u2264 \u03b5\n\u2022 with gradient complexity O ( (n + L 2\n\u03c32 ) log 1 \u03b5\n) , if each\ngi(\u00b7) is L-smooth (for L \u2265 \u03c3); or\n\u2022 with gradient complexity O ( (n + `1`2\u03c32 ) log 1 \u03b5 ) , if each\ngi(\u00b7) is (`1, `2)-smooth (for `1, `2 \u2265 \u03c3)."}, {"heading": "If one performs acceleration, the running times become", "text": "O\u0303 ( n+ n3/4 \u221a L/\u03c3 ) and O\u0303 ( n+ n3/4(`1`2\u03c3 2)1/4 ) ."}, {"heading": "2.1 RepeatSVRG", "text": "We recall the idea behind a simple algorithm \u2014that we call repeatSVRG\u2014 which finds the \u03b5-approximate stationary points for problem (1.1) when f(x) is \u03c3-strongly nonconvex. The algorithm is divided into stages. In each stage t, consider a modified function Ft(x) := F (x)+\u03c3\u2016x\u2212xt\u20162. It is easy to see that Ft(x) is \u03c3-strongly convex, so one can apply the accelerated SVRG method to minimize Ft(x). Let xt+1 be any sufficiently accurate approximate minimizer of Ft(x).8\nNow, one can prove (c.f. Section 4) that xt+1 is an O(\u03c3\u2016xt \u2212 xt+1\u2016)-approximate stationary point for F (x). Therefore, if \u03c3\u2016xt \u2212 xt+1\u2016 \u2264 \u03b5 we can stop the algorithm because we have already found an O(\u03b5)-approximate stationary point. If \u03c3\u2016xt \u2212 xt+1\u2016 > \u03b5 , then it must satisfy that F (xt) \u2212 F (xt+1) \u2265 \u03c3\u2016xt \u2212 xt+1\u20162 \u2265 \u2126(\u03b52/\u03c3), but this cannot happen for more than T = O ( \u03c3 \u03b52 (F (x0) \u2212 F \u2217) stages. Therefore, the total gradient complexity is T multiplied with the complexity of accelerated SVRG in each stage (which is O\u0303(n + n3/4 \u221a L/\u03c3) according to Theorem 2.3). Remark 2.4. The complexity of repeatSVRG can be inferred from (Agarwal et al., 2017; Carmon et al., 2016), but is not explicitly stated. For instance, the paper (Carmon et al., 2016) does not allow F (x) to have a non-smooth proximal term \u03c8(x), and applies accelerated gradient descent instead of accelerated SVRG."}, {"heading": "3 Our Algorithms", "text": "We introduce two variants of our algorithms: (1) the base method Natasha targets on the simple regime when f(x) and each fi(x) are both L-smooth, and (2) the full method Natashafull targets on the more refined regime when f(x) is L-smooth but each fi(x) is (`1, `2)-smooth.\nBoth methods follow the general idea of variance-reduced stochastic gradient descent: in each inner-most iteration, they compute a gradient estimator \u2207\u0303 that is of the form \u2207\u0303 = \u2207f(x\u0303)\u2212\u2207fi(x\u0303)+\u2207fi(x) and satisfies Ei\u2208R[n][\u2207\u0303] = \u2207f(x). Here, x\u0303 is a snapshot point that is changed once every n iterations (i.e., for each different k = 1, 2, . . . , T \u2032 in the pseudocode), and we call it a full epoch for every distinct k. Notice that the amortized gradient complexity for computing \u2207\u0303 is O(1) per-iteration.\nBase Method. In Natasha (see Algorithm 1), as illustrated by Figure 2, we divide each full epoch into p subepochs s = 0, 1, . . . , p \u2212 1, each of length m = n/p. In\n8Since the accelerated SVRG method has a linear convergence rate for strongly convex functions, the complexity to find such xt+1 only depends logarithmically on this accuracy.\neach sub-epoch s, we start with a point x0 = x\u0302, and replace f(x) with its regularized version fs(x) := f(x) + \u03c3\u2016x \u2212 x\u0302\u20162. Then, in each iteration t of the sub-epoch s, we \u2022 compute gradient estimator \u2207\u0303 with respect to fs(xt), \u2022 perform update xt+1 = arg miny { \u03c8(y) + \u3008\u2207\u0303, y\u3009 +\n1 2\u03b1\u2016y \u2212 xt\u2016\n2 }\nwith learning rate \u03b1.\nEffectively, the introduction of the regularizer \u03c3\u2016x \u2212 x\u0302\u20162 makes sure that when performing update xt \u2190 xt+1, we also move a bit towards point x\u0302 (i.e., retraction by regularization). Finally, when the sub-epoch is done, we define x\u0302 to be a random one from {x0, . . . , xm\u22121}.\nFull Method. In Natashafull (see full version), we also divide each full epoch into p sub-epochs. In each sub-epoch s, we start with a point x0 = z0 = x\u0302 and define fs(x) := f(x) + \u03c3\u2016x \u2212 x\u0302\u20162. However, this time in each iteration t, we\n\u2022 compute gradient estimator \u2207\u0303 with respect to fs(xt), \u2022 perform update zt+1 = arg miny { \u03c8(y) + \u3008\u2207\u0303, y\u3009 +\n1 2\u03b1\u2016y \u2212 zt\u2016\n2 }\nwith learning rate \u03b1, and\n\u2022 choose xt+1 = 12zt+1 + 1 2 x\u0302.\nEffectively, the regularizer \u03c3\u2016x\u2212 x\u0302\u20162 makes sure that when performing updates, we move a bit towards point x\u0302 (i.e., retraction by regularization); at the same time, the choice xt+1 = 1 2zt+1 + 1 2 x\u0302 also helps us move towards point x\u0302 (i.e., retraction by the so-called \u201cKatyusha momentum\u201d9). Finally, when the sub-epoch is over, we define x\u0302 to be a random one from the set {x0, . . . , xm\u22121}, and move to the next sub-epoch."}, {"heading": "4 A Sufficient Stopping Criterion", "text": "In this section, we present a sufficient condition for finding approximate stationary points in a \u03c3-strongly nonconvex function. Lemma 4.1 below states that, if we regularize the original function and define G(x) := F (x) + \u03c3\u2016x \u2212 x\u0302\u20162 for an arbitrary point x\u0302, then the minimizer of G(x) is an approximate saddle-point for F (x).\nLemma 4.1. Suppose G(y) = F (y) +\u03c3\u2016y\u2212 x\u0302\u20162 for some given point x\u0302, and let x\u2217 be the minimizer of G(y). If we minimize G(y) and obtain a point x satisfying\nG(x)\u2212G(x\u2217) \u2264 \u03b42\u03c3 , then for every \u03b7 \u2208 ( 0, 1max{L,4\u03c3} ] we have the gradient mapping\n\u2016G\u03b7(x)\u20162 \u2264 12\u03c32\u2016x\u2217 \u2212 x\u0302\u20162 +O ( \u03b42 ) .\nNotice that when \u03c8(x) \u2261 0 this lemma is trivial, and can be found for instance in (Carmon et al., 2016). The main\n9The idea for this second kind of retraction, and the idea of having the updates on a sequence zt but computing gradients at points xt, is largely motivated by our recent work on the Katyusha momentum and the Katyusha acceleration (Allen-Zhu, 2017).\nAlgorithm 1 Natasha(x\u2205, p, T \u2032, \u03b1)\nInput: starting vector x\u2205, sub-epoch count p \u2208 [n], epoch count T \u2032, learning rate \u03b1 > 0. Output: vector xout.\n1: x\u0302\u2190 x\u2205; m\u2190 n/p; X \u2190 []; 2: for k \u2190 1 to T \u2032 do T \u2032 full epochs 3: x\u0303\u2190 x\u0302; \u00b5\u2190 \u2207f(x\u0303); 4: for s\u2190 0 to p\u2212 1 do p sub-epochs in each epoch 5: x0 \u2190 x\u0302; X \u2190 [X, x\u0302]; 6: for t\u2190 0 to m\u2212 1 do m iterations in each sub-epoch 7: i\u2190 a random choice from {1, \u00b7 \u00b7 \u00b7 , n}. 8: \u2207\u0303 \u2190 \u2207fi(xt)\u2212\u2207fi(x\u0303) + \u00b5+ 2\u03c3(xt \u2212 x\u0302) Ei[\u2207\u0303] = \u2207 ( f(x) + \u03c3\u2016x\u2212 x\u0302\u20162 )\u2223\u2223 xt\n9: xt+1 = arg miny\u2208Rd { \u03c8(y) + 12\u03b1\u2016y \u2212 xt\u2016 2 + \u3008\u2207\u0303, y\u3009 }\n10: end for 11: x\u0302\u2190 a random choice from {x0, x1, . . . , xm\u22121}; for practitioners, choose the average 12: end for 13: end for 14: x\u0302\u2190 a random vector in X; for practitioners, choose the last 15: xout \u2190 an approximate minimizer of G(y) := F (y) + \u03c3\u2016y \u2212 x\u0302\u20162 using SVRG. 16: return xout. it suffices to run SVRG for O(n log 1\n\u03b5 ) iterations.\ntechnical difficulty arises in order to deal with \u03c8(x) 6= 0. The proof is included in the full version."}, {"heading": "5 Base Method: Analysis for One Full Epoch", "text": "In this section, we consider problem (1.1) where each fi(x) is L-smooth and F (x) is \u03c3-strongly nonconvex. We use our base method Natasha to minimize F (x), and analyze its behavior for one full epoch in this section. We assume \u03c3 \u2264 L without loss of generality, because any L-smooth function is also L-strongly nonconvex.\nNotations. We introduce the following notations for analysis purpose only.\n\u2022 Let x\u0302s be the vector x\u0302 at the beginning of sub-epoch s. \u2022 Let xst be the vector xt in sub-epoch s. \u2022 Let ist be the index i \u2208 [n] in sub-epoch s at iteration t. \u2022 Let fs(x) := f(x) + \u03c3\u2016x \u2212 x\u0302s\u20162, F s(x) := F (x) + \u03c3\u2016x\u2212 x\u0302s\u20162, and xs\u2217 := arg minx{F s(x)}.\n\u2022 Let \u2207\u0303fs(xst ) := \u2207fi(xst )\u2212\u2207fi(x\u0303)+\u2207f(x\u0303)+2\u03c3(xt\u2212 x\u0302) where i = ist .\n\u2022 Let \u2207\u0303f(xst ) := \u2207fi(xst ) \u2212 \u2207fi(x\u0303) + \u2207f(x\u0303) where i = ist . We obviously have that fs(x) and F s(x) are \u03c3-strongly convex, and fs(x) is (L+ 2\u03c3)-smooth."}, {"heading": "5.1 Variance Upper Bound", "text": "The following lemma gives an upper bound on the variance of the gradient estimator \u2207\u0303fs(xst ):\nLemma 5.1. We have Eist [ \u2016\u2207\u0303fs(xst ) \u2212 \u2207fs(xst )\u20162 ] \u2264\npL2\u2016xst \u2212 x\u0302s\u20162 + pL2 \u2211s\u22121 k=0 \u2016x\u0302k \u2212 x\u0302k+1\u20162 .\nProof. We have Eist [ \u2016\u2207\u0303fs(xst )\u2212\u2207fs(xst )\u20162 ] = Eist [ \u2016\u2207\u0303f(xst )\u2212\u2207f(xst )\u20162 ] = Ei\u2208R[n]\n[\u2225\u2225(\u2207fi(xst )\u2212\u2207fi(x\u0303))\u2212 (\u2207f(xst )\u2212\u2207f(x\u0303)))\u2225\u22252] \u00ac\n\u2264 Ei\u2208R[n] [\u2225\u2225\u2207fi(xst )\u2212\u2207fi(x\u0303)\u2225\u22252]\n \u2264 pEi\u2208R[n] [\u2225\u2225\u2207fi(xst )\u2212\u2207fi(x\u0302s)\u2225\u22252]\n+ p \u2211s\u22121 k=0 Ei\u2208R[n] [\u2225\u2225\u2207fi(x\u0302k)\u2212\u2207fi(x\u0302k+1)\u2225\u22252] \u00ae\n\u2264 pL2\u2016xst \u2212 x\u0302s\u20162 + pL2 \u2211s\u22121 k=0 \u2016x\u0302 k \u2212 x\u0302k+1\u20162 .\nAbove, inequality \u00ac is because for any random vector \u03b6 \u2208 Rd, it holds that E\u2016\u03b6\u2212E\u03b6\u20162 = E\u2016\u03b6\u20162\u2212\u2016E\u03b6\u20162; inequality  is because x\u03020 = x\u0303 and for any p vectors a1, a2, . . . , ap \u2208 Rd, it holds that \u2016a1+\u00b7 \u00b7 \u00b7+ap\u20162 \u2264 p\u2016a1\u20162+\u00b7 \u00b7 \u00b7+p\u2016ap\u20162; and inequality \u00ae is because each fi(\u00b7) is L-smooth."}, {"heading": "5.2 Analysis for One Sub-Epoch", "text": "The following inequality is classically known as the \u201cregret inequality\u201d for mirror descent (Allen-Zhu & Orecchia, 2017), and its proof is classical (see full version):\nFact 5.2. \u3008\u2207\u0303fs(xst ), xst+1 \u2212 u\u3009 + \u03c8(xst+1) \u2212 \u03c8(u) \u2264 \u2016xst\u2212u\u2016 2\n2\u03b1 \u2212 \u2016xst+1\u2212u\u2016 2 2\u03b1 \u2212 \u2016xst+1\u2212x s t\u2016 2 2\u03b1 for every u \u2208 R d.\nThe following lemma is our main contribution for the base method Natasha.\nLemma 5.3. As long as \u03b1 \u2264 12L+4\u03c3 , we have E [( F s(x\u0302s+1)\u2212 F s(xs\u2217) )] \u2264 E [F s(x\u0302s)\u2212 F s(xs\u2217) \u03c3\u03b1m/2 +\u03b1pL2 ( s\u2211 k=0 \u2016x\u0302k\u2212x\u0302k+1\u20162 )] .\nProof. We first compute that F s(xst+1)\u2212 F s(u) = fs(xst+1)\u2212 fs(u) + \u03c8(xst+1)\u2212 \u03c8(u)\n\u00ac \u2264 fs(xst ) + \u3008\u2207fs(xst ), xst+1 \u2212 xst \u3009+ L+ 2\u03c3\n2 \u2016xst \u2212 xst+1\u20162\n\u2212 fs(u) + \u03c8(xst+1)\u2212 \u03c8(u)  \u2264 \u3008\u2207fs(xst ), xst+1 \u2212 xst \u3009+ L+ 2\u03c3\n2 \u2016xst \u2212 xst+1\u20162\n+ \u3008\u2207fs(xst ), xst \u2212 u\u3009+ \u03c8(xst+1)\u2212 \u03c8(u) . (5.1)\nAbove, inequality \u00ac uses the fact that fs(\u00b7) is (L + 2\u03c3)smooth; and inequality  uses the convexity of fs(\u00b7). Now, we take expectation with respect to ist on both sides of (5.1), and derive that:\nEist [ F s(xst+1) ] \u2212 F s(u)\n\u00ac \u2264 Eist [ \u3008\u2207\u0303fs(xst )\u2212\u2207fs(xst ), xst \u2212 xst+1\u3009+ \u3008\u2207\u0303fs(xst ), xst+1 \u2212 u\u3009\n+ L+ 2\u03c3\n2 \u2016xst \u2212 xst+1\u20162 + \u03c8(xst+1)\u2212 \u03c8(u) ] \n\u2264 Eist [ \u3008\u2207\u0303fs(xst )\u2212\u2207fs(xst ), xst \u2212 xst+1\u3009+ \u2016xst \u2212 u\u20162\n2\u03b1\n\u2212 \u2016x s t+1 \u2212 u\u20162 2\u03b1 \u2212 ( 1 2\u03b1 \u2212 L+ 2\u03c3 2 ) \u2016xst+1 \u2212 xst\u20162 ] \u00ae\n\u2264 Eist [ \u03b1 \u2225\u2225\u2207\u0303fs(xst )\u2212\u2207fs(xst )\u2225\u22252 + \u2016xst \u2212 u\u20162 2\u03b1 \u2212 \u2016x s t+1 \u2212 u\u20162 2\u03b1 ] \u00af\n\u2264 Eist [ \u03b1pL2\u2016xst \u2212 x\u0302s\u20162 + \u03b1pL2 s\u22121\u2211 k=0 \u2016x\u0302k \u2212 x\u0302k+1\u20162\n+ \u2016xst \u2212 u\u20162 2\u03b1 \u2212 \u2016x s t+1 \u2212 u\u20162 2\u03b1\n] . (5.2)\nAbove, inequality \u00ac is follows from (5.1) together with the fact that Eist [\u2207\u0303f s(xst )] = \u2207fs(xst ) implies\nEist [ \u3008\u2207fs(xst ), xst+1 \u2212 xst \u3009+ \u3008\u2207fs(xst ), xst \u2212 u\u3009 ] = Eist [ \u3008\u2207\u0303fs(xst )\u2212\u2207fs(xst ), xst\u2212xst+1\u3009+\u3008\u2207\u0303fs(xst ), xst+1\u2212u\u3009 ] ;\ninequality  uses Fact 5.2; inequality \u00ae uses \u03b1 \u2264 12L+4\u03c3 together with Young\u2019s inequality \u3008a, b\u3009 \u2264 12\u2016a\u2016 2 + 12\u2016b\u2016 2; and inequality \u00af uses Lemma 5.1. Finally, choosing u = xs\u2217 to be the (unique) minimizer of F s(\u00b7) = fs(\u00b7) + \u03c8(\u00b7), and telescoping inequality (5.2) for t = 0, 1, . . . ,m\u2212 1, we have\nE [m\u22121\u2211 t=1 ( F s(xst )\u2212 F s(xs\u2217) )] \u2264 E [\u2016xs0 \u2212 xs\u2217\u20162 2\u03b1 + m\u22121\u2211 t=0 ( \u03b1pL2\u2016xst \u2212 x\u0302s\u20162\n+ \u03b1pL2 s\u22121\u2211 k=0 \u2016x\u0302k \u2212 x\u0302k+1\u20162 )]\n\u2264 E [F s(x\u0302s)\u2212 F s(xs\u2217)\n\u03c3\u03b1 + \u03b1pmL2 ( s\u2211 k=0 \u2016x\u0302k \u2212 x\u0302k+1\u20162 )] .\nAbove, the second inequality uses the fact that x\u0302s+1 is chosen from {xs0, . . . , xsm\u22121} uniformly at random, as well as the \u03c3-strong convexity of F s(\u00b7).\nDividing both sides by m and rearranging the terms (using 1 2\u03c3\u03b1 \u2265 1), we have\nE [( F s(x\u0302s+1)\u2212 F s(xs\u2217) )] \u2264 E [F s(x\u0302s)\u2212 F s(xs\u2217) \u03c3\u03b1m/2 + \u03b1pL2 ( s\u2211 k=0 \u2016x\u0302k \u2212 x\u0302k+1\u20162 )] ."}, {"heading": "5.3 Analysis for One Full Epoch", "text": "One can telescope Lemma 5.3 for an entire epoch and arrive at the following lemma (see full version):\nLemma 5.4. If \u03b1 \u2264 12L+4\u03c3 , \u03b1 \u2265 4 \u03c3m and \u03b1 \u2264 \u03c3 p2L2 , we have p\u22121\u2211 s=0 E [( F s(x\u0302s)\u2212 F s(xs\u2217) )] \u2264 2E [ F (x\u03020)\u2212 F (x\u0302p) ] ."}, {"heading": "6 Base Method: Final Theorem", "text": "We are now ready to state and prove our main convergence theorem for Natasha: Theorem 1. Suppose in (1.1), each fi(x) is L-smooth and F (x) is \u03c3-strongly nonconvex for \u03c3 \u2264 L. Then, if L2\n\u03c32 \u2264 n, p = \u0398 ( ( \u03c3 2 L2n) 1/3 )\nand \u03b1 = \u0398( \u03c3p2L2 ), our base method Natasha outputs a point xout satisfying\nE[\u2016G\u03b7(xout)\u20162] \u2264 O ( (L2\u03c3)1/3n2/3\nT \u2032n\n) \u00b7 (F (x\u2205)\u2212 F \u2217) .\nfor every \u03b7 \u2208 ( 0, 1max{L,4\u03c3} ] . In other words, to obtain E[\u2016G\u03b7(xout)\u20162] \u2264 \u03b52, we need gradient complexity\nO ( n log 1\n\u03b5 +\n(L2\u03c3)1/3n2/3\n\u03b52 \u00b7 (F (x\u2205)\u2212 F \u2217)\n) .\nIn the above theorem, we have assumed \u03c3 \u2264 L without loss of generality because any L-smooth function is also L-strongly nonconvex. Also, we have assumed L 2\n\u03c32 \u2264 n and if this inequality does not hold, then one should apply repeatSVRG for a faster running time (see Figure 1). Proof of Theorem 1. We choose p = ( \u03c32 24L2n )1/3\n, m = n/p, and \u03b1 = 4\u03c3m = \u03c3 6p2L2 \u2264 1 2L+4\u03c3 , so we can apply Lemma 5.4. If we telescope Lemma 5.4 for the entire algorithm (which has T \u2032 full epochs), and use the fact that x\u0302p of the previous epoch equals x\u03020 of the next epoch, we conclude that if we choose a random epoch and a random subepoch s, we will have\nE[F s(x\u0302s)\u2212 F s(xs\u2217)] \u2264 2 pT \u2032 (F (x\u2205)\u2212 F \u2217) .\nBy the \u03c3-strong convexity of F s(\u00b7), we have E[\u03c3\u2016x\u0302s \u2212 xs\u2217\u20162] \u2264 4pT \u2032 (F (x \u2205)\u2212 F \u2217).\nNow, F s(x) = F (x)+\u03c3\u2016x\u2212 x\u0302s\u20162 satisfies the assumption of G(x) in Lemma 4.1. If we use the SVRG method (see Theorem 2.3) to minimize the convex function F s(x), we\nget an output xout satisfying F s(xout) \u2212 F s(xs\u2217) \u2264 \u03b52\u03c3 in gradient complexity O ( (n+ L 2\n\u03c32 ) log 1 \u03b5 ) \u2264 O(n log 1\u03b5 ).\nWe can therefore apply Lemma 4.1 and conclude that this output xout satisfies\nE[\u2016G\u03b7(xout)\u20162] \u2264 O ( \u03c3 pT \u2032 ) \u00b7 (F (x\u2205)\u2212 F \u2217)\n= O ( (L2\u03c3)1/3n2/3\nT \u2032n\n) \u00b7 (F (x\u2205)\u2212 F \u2217) .\nIn other words, we obtain E[\u2016G\u03b7(xout)\u20162] \u2264 \u03b52 using T \u2032n = O ( n+ (L 2\u03c3)1/3n2/3\n\u03b52 \u00b7 (F (x \u2205)\u2212 F \u2217) ) computations of the stochastic gradients. Here, the additive term n is because T \u2032 \u2265 1. Finally, adding this withO(n log 1\u03b5 ), the gradient complexity for the application of SVRG in the last line of Natasha, we finish the proof of the total gradient complexity."}, {"heading": "7 Full Method: Final Theorem", "text": "We analyze and state the main theorems for our full method Natashafull in the full version of this paper."}, {"heading": "8 Conclusion", "text": "Stochastic gradient descent and gradient descent (including alternating minimization) have become the canonical methods for solving non-convex machine learning tasks. However, can we design new non-convex methods to run even faster than SGD or GD?\nThis present paper tries to tackle this general question, by providing a new Natashamethod which is intrinsically different from GD or SGD. It runs faster than GD and SVRGbased methods at least in theory. We hope that this could be a non-negligible step towards our better understanding of non-convex optimization.\nFinally, our results give rise to an interesting dichotomy in the best-known complexity of first-order non-convex optimization: the complexity scales with n3/4 for \u03c3 < L/ \u221a n and with n2/3 for \u03c3 > L/ \u221a n. It remains open to investigate whether this dichotomy is intrinsic, or we can design a more efficient algorithm that outperforms both."}], "year": 2017, "references": [{"title": "Finding Approximate Local Minima for Nonconvex Optimization in Linear Time", "authors": ["Agarwal", "Naman", "Allen-Zhu", "Zeyuan", "Bullins", "Brian", "Hazan", "Elad", "Ma", "Tengyu"], "year": 2017}, {"title": "Katyusha: The First Direct Acceleration of Stochastic Gradient Methods", "authors": ["Allen-Zhu", "Zeyuan"], "venue": "In STOC,", "year": 2017}, {"title": "Variance Reduction for Faster Non-Convex Optimization", "authors": ["Allen-Zhu", "Zeyuan", "Hazan", "Elad"], "venue": "In NIPS,", "year": 2016}, {"title": "LazySVD: Even Faster SVD Decomposition Yet Without Agonizing Pain", "authors": ["Allen-Zhu", "Zeyuan", "Li", "Yuanzhi"], "venue": "In NIPS,", "year": 2016}, {"title": "Doubly Accelerated Methods for Faster CCA and Generalized Eigendecomposition", "authors": ["Allen-Zhu", "Zeyuan", "Li", "Yuanzhi"], "venue": "In Proceedings of the 34th International Conference on Machine Learning,", "year": 2017}, {"title": "Follow the Compressed Leader: Faster Online Learning of Eigenvectors and Faster MMWU", "authors": ["Allen-Zhu", "Zeyuan", "Li", "Yuanzhi"], "venue": "In Proceedings of the 34th International Conference on Machine Learning,", "year": 2017}, {"title": "Linear Coupling: An Ultimate Unification of Gradient and Mirror Descent", "authors": ["Allen-Zhu", "Zeyuan", "Orecchia", "Lorenzo"], "venue": "In Proceedings of the 8th Innovations in Theoretical Computer Science,", "year": 2017}, {"title": "Improved SVRG for Non-Strongly-Convex or Sum-of-Non-Convex Objectives", "authors": ["Allen-Zhu", "Zeyuan", "Yuan", "Yang"], "venue": "In ICML,", "year": 2016}, {"title": "Accelerated Methods for Non-Convex Optimization", "authors": ["Carmon", "Yair", "Duchi", "John C", "Hinder", "Oliver", "Sidford", "Aaron"], "venue": "ArXiv e-prints,", "year": 2016}, {"title": "SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives", "authors": ["Defazio", "Aaron", "Bach", "Francis", "Lacoste-Julien", "Simon"], "venue": "In NIPS,", "year": 2014}, {"title": "Un-regularizing: approximate proximal point and faster stochastic algorithms for empirical risk minimization", "authors": ["Frostig", "Roy", "Ge", "Rong", "Kakade", "Sham M", "Sidford", "Aaron"], "venue": "In ICML,", "year": 2015}, {"title": "Robust shift-and-invert preconditioning: Faster and more sample efficient algorithms for eigenvector computation", "authors": ["Garber", "Dan", "Hazan", "Elad", "Jin", "Chi", "Kakade", "Sham M", "Musco", "Cameron", "Netrapalli", "Praneeth", "Sidford", "Aaron"], "year": 2016}, {"title": "Escaping from saddle points\u2014online stochastic gradient for tensor decomposition", "authors": ["Ge", "Rong", "Huang", "Furong", "Jin", "Chi", "Yuan", "Yang"], "venue": "In Proceedings of the 28th Annual Conference on Learning", "year": 2015}, {"title": "Accelerated gradient methods for nonconvex nonlinear and stochastic programming", "authors": ["Ghadimi", "Saeed", "Lan", "Guanghui"], "venue": "Mathematical Programming,", "year": 2015}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "authors": ["Johnson", "Rie", "Zhang", "Tong"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2013}, {"title": "Gradient descent only converges to minimizers", "authors": ["Lee", "Jason D", "Simchowitz", "Max", "Jordan", "Michael I", "Recht", "Benjamin"], "venue": "In Proceedings of the 29th Conference on Learning Theory, COLT 2016,", "year": 2016}, {"title": "A Universal Catalyst for First-Order Optimization", "authors": ["Lin", "Hongzhou", "Mairal", "Julien", "Harchaoui", "Zaid"], "venue": "In NIPS,", "year": 2015}, {"title": "Accelerating the cubic regularization of newton\u2019s method on convex problems", "authors": ["Nesterov", "Yurii"], "venue": "Mathematical Programming,", "year": 2008}, {"title": "Stochastic variance reduction for nonconvex optimization", "authors": ["Reddi", "Sashank J", "Hefny", "Ahmed", "Sra", "Suvrit", "Poczos", "Barnabas", "Smola", "Alex"], "venue": "ArXiv e-prints,", "year": 2016}, {"title": "Minimizing finite sums with the stochastic average gradient", "authors": ["Schmidt", "Mark", "Le Roux", "Nicolas", "Bach", "Francis"], "venue": "arXiv preprint arXiv:1309.2388,", "year": 2013}, {"title": "SDCA without Duality, Regularization, and Individual Convexity", "authors": ["Shalev-Shwartz", "Shai"], "venue": "In ICML,", "year": 2016}, {"title": "Linear convergence with condition number independent access of full gradients", "authors": ["Zhang", "Lijun", "Mahdavi", "Mehrdad", "Jin", "Rong"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2013}], "id": "SP:298138e121aa60c15615b565ebf8a785591da3e0", "authors": [{"name": "Zeyuan Allen-Zhu", "affiliations": []}], "abstractText": "Given a non-convex function f(x) that is an average of n smooth functions, we design stochastic first-order methods to find its approximate stationary points. The performance of our new methods depend on the smallest (negative) eigenvalue \u2212\u03c3 of the Hessian. This parameter \u03c3 captures how strongly non-convex f(x) is, and is analogous to the strong convexity parameter for convex optimization. At least in theory, our methods outperform known results for a range of parameter \u03c3, and can also be used to find approximate local minima. Our result implies an interesting dichotomy: there exists a threshold \u03c30 so that the (currently) fastest methods for \u03c3 > \u03c30 and for \u03c3 < \u03c30 have different behaviors: the former scales with n and the latter scales with n.", "title": "Natasha: Faster Non-Convex Stochastic Optimization  via Strongly Non-Convex Parameter"}