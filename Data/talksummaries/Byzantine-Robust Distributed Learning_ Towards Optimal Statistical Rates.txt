1	35	As the scale of the datasets in these learning tasks continues to grow, it is crucial to utilize the power of distributed computing and storage.
4	50	Security issues are only exacerbated in the so-called Federated Learning setting, a modern distributed learning paradigm that is more decentralized, and that uses the data owners’ devices (such as mobile phones and personal computers) as worker machines (McMahan & Ramage, 2017; Konečnỳ et al., 2016).
21	69	Intuitively, the above error rate is the optimal rate that one should target for, as 1√ n is the effective standard deviation for each machine with n data points, α is the bias effect of Byzantine machines, and 1√ m is the averaging effect of m normal machines.
30	17	To summarize, we aim to develop distributed learning algorithms that simultaneously achieve two objectives: • Statistical optimality: attain an Õ( α√ n + 1√ nm ) rate.
33	24	In particular, previous robust algorithms either have unclear or sub-optimal statistical guarantees, or incur a high communication cost and hence not applicable in a distributed setting—we discuss related work in more detail in Section 2.
34	24	Our Contributions We propose two robust distributed gradient descent (GD) algorithms, one based on coordinatewise median, and the other on coordinate-wise trimmed mean.
35	35	We establish their statistical error rates for strongly convex, non-strongly convex, and non-convex population loss functions.
38	19	The statistical error rates of these three algorithms are summarized as follows.
44	23	Furthermore, for the analysis of median-based algorithms, we cannot simply adapt standard techniques (such as those in Minsker et al. (2015)), which can only show that the output of the master machine is as accurate as that of one normal machine, leading to a sub-optimal O( 1√ n ) rate even without Byzantine failures.
66	21	Let f(w; z) be a loss function of a parameter vector w ∈ W ⊆ Rd associated with the data point z, whereW is the parameter space, and F (w) := Ez∼D[f(w; z)] is the corresponding population loss function.
67	18	Our goal is to learn a model defined by the parameter that minimizes the population loss: w∗ = arg min w∈W F (w).
69	64	We consider a distributed computation model with one master machine and m worker machines.
70	31	Each worker machine stores n data points, each of which is sampled independently from D. Denote by zi,j the j-th data on the i-th worker machine, and Fi(w) := 1n ∑n j=1 f(w; z i,j) the empirical risk function for the i-th worker.
71	38	We assume that an α fraction of the m worker machines are Byzantine, and the remaining 1− α fraction are normal.
73	19	The master machine communicates with the worker machines using some predefined protocol.
75	21	We introduce the coordinate-wise median and trimmed mean operations, which serve as building blocks for our algorithm.
77	17	For vectors xi ∈ Rd, i ∈ [m], the coordinate-wise median g := med{xi : i ∈ [m]} is a vector with its k-th coordinate being gk = med{xik : i ∈ [m]} for each k ∈ [d], where med is the usual (one-dimensional) median.
85	17	For a one-dimensional random variable X , define its absolute skewness2 as Var(X)3/2 .
103	22	We note that L̂ appears because we have coordinate-wise operations and the L̂ quantity combines the smoothness parameter of all the d partial derivatives.
115	18	Assumption 4 (Bounded skewness of gradient).
123	43	We now state our main technical results on the median-based algorithm, namely statistical error guarantees for strongly convex, non-strongly convex, and smooth non-convex population loss functions F .
135	26	The third term 1n is due to the dependence of median on the skewness of the gradients.
187	37	On the other hand, the median-based algorithm requires milder tail/moment assumptions on the loss derivatives (bounded skewness) than its trimmed-mean counterpart (sub-exponentiality).
189	18	Using an overly large β may lead to a looser bound and sub-optimal performance.
227	29	Consider the distributed mean estimation problem in (6) with Byzantine failure probability α, and suppose that Z is Gaussian distribution with mean µ and covariance matrix σ2I (σ = O(1)).
229	66	We prove Observation 1 in Appendix G. According to this observation, we see that the α√ n + 1√ nm dependence cannot be avoided, which in turn implies the order-optimality of the results in Theorem 1 (when n & m) and Theorem 4.
230	63	We conduct experiments to show the effectiveness of the median and trimmed mean operations.
241	37	As we can see, in the adversarial settings, the vanilla distributed gradient descent algorithm suffers from severe performance loss, and using the median and trimmed mean operations, we observe significant improvement in test accuracy.
245	23	, 9}, and these machines train models using the faulty data.
247	26	As we can see, for the one-round algorithm, although the theoretical guarantee is only proved for quadratic loss, in practice, the median-based one-round algorithm still improves the test accuracy in problems with other loss functions, such as the logistic loss here.
