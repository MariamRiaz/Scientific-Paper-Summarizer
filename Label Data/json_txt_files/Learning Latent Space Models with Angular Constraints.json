{"sections": [{"heading": "1. Introduction", "text": "Latent space models (LSMs), such as sparse coding (Olshausen & Field, 1997), topic models (Blei et al., 2003) and neural networks, are widely used in machine learning to extract hidden patterns and learn latent representations of data. An LSM consists of a set of components. Each component aims at capturing one latent pattern and is pa-\n1Machine Learning Department, Carnegie Mellon University 2Petuum Inc. 3School of Engineering and Applied Sciences, Harvard University 4College of Engineering and Computer Science, Syracuse University 5Groupon Inc. 6School of Computer Science, University of Waterloo 7Department of Biomedical Data Science, Stanford University. Correspondence to: Pengtao Xie <pengtaox@cs.cmu.edu>, Eric P. Xing <eric.xing@petuum.com>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nrameterized by a weight vector. For instance, in a topic model (Blei et al., 2003), the components are referred to as topics, aiming at discovering the semantics underlying documents. Each topic is associated with a weight vector. The modeling power of LSMs can be very large when the number of components is large and the dimension of weight vectors is high. For instance, in the LightLDA (Yuan et al., 2015) topic model, the number of topics is 1 million and the dimension of topic vector is 50000, resulting in a topic matrix with 50 billion parameters. The vast model capacity of LSMs enables them to flexibly adapt to the complex patterns underlying data and achieve great predictive performance therefrom.\nWhile highly expressive, LSMs are prone to overfitting, because of their large amount of model parameters. A key ingredient to successfully train LSMs is regularization, which imposes certain control over the model parameters to reduce model complexity and improve the generalization performance on unseen data. Many regularizers have been proposed, including `2 regularization, `1 regularization (Tibshirani, 1996), nuclear norm (Recht et al., 2010), Dropout (Srivastava et al., 2014) and so on.\nRecently, a new type of regularization approaches (Yu et al., 2011; Zou & Adams, 2012a; Xie et al., 2015; 2016a; Rodr\u0131\u0301guez et al., 2016), which aim at encouraging the weight vectors of components in LSMs to be \u201cdiverse\u201d, are emerging. Zou & Adams (2012b) apply Determinantal Point Process (Kulesza & Taskar, 2012) to encourage the topic vectors in LDA to be \u201cdiverse\u201d. Bao et al. (2013) develop a softmax regularizer to promote incoherence among hidden units in neural network. Xie et al. (2015) propose an angle-based regularizer to \u201cdiversify\u201d the weight vectors in Restricted Boltzmann Machine (RBM). While these approaches have demonstrated promising effectiveness on a wide range of empirical studies, in theory how they reduce overfitting is still unclear. One intuitive explanation could be: promoting diversity imposes a structural constraint on model parameters, which reduces the model capacity of LSMs and therefore alleviates overfitting. However, how to make this formal is challenging. In this paper, we aim to bridge this gap, by proposing a diversitypromoting approach that is both empirically effective and theoretically analyzable. We use near-orthogonality to represent \u201cdiversity\u201d and propose to learn LSMs with angular\nconstraints (ACs) where the angle between components is constrained to be close to \u03c02 , which hence encourages the components to be close to orthogonal (therefore \u201cdiverse\u201d). Using sparse coding and neural network as study cases, we analyze how ACs affect the generalization performance of these two LSMs. The analysis shows that the more close to \u03c02 the angles are, the smaller the estimation error is and the larger the approximation error is. The best tradeoffs of these two errors can be explored by properly tuning the angles. We develop an alternating direction method of multipliers (ADMM) (Boyd et al., 2011) algorithm to solve the angle-constrained LSM (AC-LSM) problems. In various experiments, we demonstrate that ACs improve the generalization performance of LSMs and outperform other diversity-promoting regularization approaches.\nThe major contributions of this work are:\n\u2022 We propose a new approach to promote diversity in LSMs, by imposing angular constraints (ACs) on components, for the sake of alleviating overfitting.\n\u2022 We perform theoretical analysis on how ACs affect the generalization error of two exemplar LSMs: sparse coding and neural networks.\n\u2022 We develop an efficient ADMM algorithm to solve the AC-LSM problems.\n\u2022 Empirical evaluation demonstrates that ACs are very effective in reducing overfitting and outperforms other diversity-promoting approaches.\nThe rest of the paper is organized as follows. Section 2 reviews related works. Second 3 introduces the angleconstrained LSMs and Section 4 gives the theoretical analysis. Section 5 presents experimental results and Section 6 concludes the paper."}, {"heading": "2. Related Works", "text": "Diversity-promoting learning of latent space models has been widely studied recently. Ramirez et al. (2010) define a regularizer based on squared Frobenius norm to encourage the dictionary in sparse coding to be incoherent. Zou & Adams (2012a) use the determinantal point process (DPP) (Kulesza & Taskar, 2012) to encourage the location vectors in Gaussian mixture model (GMM) and topics in latent Dirichlet allocation (Blei et al., 2003) to be \u201cdiverse\u201d. Given m vectors {wj}mj=1, DPP is defined as log det(G). G is a kernel matrix whereGij = k(wi,wj) and k(\u00b7, \u00b7) is a kernel function. det(G) is the volume of the parallelepiped formed by {\u03c6(wj)}mj=1, where \u03c6(\u00b7) denotes the reproducing kernel feature map associated with kernel k. Vectors with larger volume are considered to be more diverse since they are more spread out. Xie (2015) develop an anglebased regularizer to encourage the weight vectors of hidden units in restricted Boltzmann machine to be close to orthogonal. The non-obtuse angle between each pair of weight\nvectors is measured and the regularizer is defined as the mean of these angles minus their variance. A larger mean encourages the vectors to have larger angles overall and a smaller variance encourages the vectors to be evenly different from each other. Xie (2015) apply this regularizer to encourage the projection vectors in distance metric learning to be diverse and show that promoting diversity can reduce model size without sacrificing modeling power. Besides frequentist-style regularization, diversity-promoting learning is also investigated in Bayesian learning where the components are random variables. Affandi et al. (2013) apply DPP as a repulsive prior to encourage the location vectors in GMM to be far apart. Xie et al. (2016a) propose a mutual angular prior that has an inductive bias towards vectors having larger angles.\nIn the literature of neural networks, many works have studied the \u201cdiversification\u201d of hidden units. Le et al. (2010) apply a strict-orthogonality constraint over the weight parameters to make the hidden units uncorrelated (therefore \u201cdiverse\u201d). In practice, this hard constraint might be too restrictive and hurts performance, as we will confirm in experiments. Bao et al. (2013) propose a softmax regularizer to encourage the weight vectors of hidden units to have small cosine similarity. Cogswell et al. (2015) propose to decorrelate hidden activations by minimizing their covariance. In convolutional neural networks (CNNs) where the number of activations is much larger than that of weight parameters, this regularizer is computationally prohibitive since it is defined over activations rather than weights. Henaff et al. (2016) perform a study to show that random orthogonal initialization of the weight matrices in recurrent neural networks improves its ability to perform longmemory tasks. Xiong et al. (2016) propose a structured decorrelation constraint which groups hidden units and encourages units within the same group to have strong connections during the training procedure and forces units in different groups to learn nonredundant representations by minimizing the cross-covariance between them. Rodr\u0131\u0301guez et al. (2016) show that regularizing negatively correlated features inhibits effective diversity and propose a solution which locally enforces feature orthogonality. Chen et al. (2017) propose a group orthogonal CNN which leverages side information to learn diverse feature representations. Mao et al. (2017) impose a stochastic decorrelation constraint based on covariance to reduce the co-adaptation of hidden units. Xie et al. (2017) define a kernel-based regularizer to promote diversity and analyze how it affects the generalize performance of neural networks (NNs). It is unclear how to generalize the analysis to other LSMs.\nDiversity-promoting learning has been investigated in nonLSM models as well. In multi-class classification, Malkin & Bilmes (2008) encourage the coefficient vectors of different classes to be diverse by maximizing the determi-\nnant of the covariance matrix of the coefficient vectors. In classifiers ensemble, Yu et al. (2011) develop a regularizer to encourage the coefficient vectors of support vector machines (SVMs) to have small cosine similarity and analyze how this regularizer affects the generalization performance. The analysis is specific to SVM ensemble. It is unclear how to generalize it to latent space models."}, {"heading": "3. Methods", "text": "In this section, we propose Angle-Constrained Latent Space Models (AC-LSMs) and present an ADMM algorithm to solve them."}, {"heading": "3.1. Latent Space Models with Angular Constraints", "text": "An LSM consists of m components and these components are parameterized by vectors W = {wj}mj=1. Let L(W) denote the objective function of this LSM. Similar to (Bao et al., 2013; Xie et al., 2015; Rodr\u0131\u0301guez et al., 2016), we use angle to characterize diversity: the components are considered to be more diverse if they are close to being orthogonal, i.e., their angles are close to \u03c02 . To encourage this, we require the absolute value of cosine similarity between each pair of components to be less than a small value \u03c4 , which leads to the following angle-constrained LSM (AC-LSM) problem\nmin W L(W) s.t. 1 \u2264 i < j \u2264 m, |wi\u00b7wj |\u2016wi\u20162\u2016wj\u20162 \u2264 \u03c4 (1)\nThe parameter \u03c4 controls the level of near-orthogonality (or diversity). A smaller \u03c4 indicates that the vectors are more close to being orthogonality, and hence are more diverse. As will be shown later, representing diversity using the angular constraints facilitates theoretical analysis and is empirically effective as well."}, {"heading": "3.2. Case Studies", "text": "In this section, we apply the ACs to two LSMs.\nSparse Coding Given a set of data samples {xi}ni=1, where x \u2208 Rd, sparse coding (SC) (Olshausen & Field, 1997) aims to use a set of \u201cbasis\u201d vectors (referred to as dictionary) W = {wj}mj=1 to reconstruct the data samples. Each data sample x is reconstructed by taking a sparse linear combination of the basis vectors x \u2248 \u2211m j=1 \u03b1jwj where {\u03b1j}mj=1 are the linear coefficients (referred to as sparse codes) and most of them are zero. The reconstruction error is measured using the squared `2 norm \u2016x\u2212 \u2211m j=1 \u03b1jwj\u201622. To achieve sparsity among the coeffi-\ncients, `1-regularization is utilized: \u2211m j=1 |\u03b1j |1. To avoid the degenerated case where most coefficients are zero and the basis vectors are of large magnitude, `2-regularization is applied to the basis vectors: \u2016wj\u201622. Putting these pieces together, we learn the basis vectors and sparse codes\n(denoted by A) by minimizing the following objective function: L(W,A) = 12 \u2211n i=1(\u2016xi \u2212 \u2211m j=1 \u03b1ijwj\u201622 +\n\u03bb1 \u2211m j=1 |\u03b1ij |1) + \u03bb2 \u2211m j=1 \u2016wj\u201622. Applying ACs to the basis vectors, we obtain the following AC-SC problem:\nminW,A L(W,A) s.t. 1 \u2264 i < j \u2264 m, |wi\u00b7wj |\u2016wi\u20162\u2016wj\u20162 \u2264 \u03c4\n(2)\nNeural Networks In a neural network (NN) with L hidden layers, each hidden layer l is equipped with m(l) units and each unit i is connected with all units in layer l \u2212 1. Hidden unit i at layer l is parameterized by a weight vector w\n(l) i . These hidden units aim at capturing latent features underlying data. Applying ACs to the weight vectors of hidden units, we obtain the following AC-NN problem\nmin W L(W) s.t. \u2200 1 \u2264 l \u2264 L, 1 \u2264 i < j \u2264 m(l), |w (l) i \u00b7w (l) j |\n\u2016w(l)i \u20162\u2016w (l) j \u20162\n\u2264 \u03c4\nwhereW denotes weight vectors in all layers and L(W) is the objective function of this NN."}, {"heading": "3.3. Algorithm", "text": "In this section, we develop an ADMM-based algorithm to solve the AC-LSM problem. To make it amenable for optimization, we first factorize each weight vector w into its `2 norm g = \u2016w\u20162 and direction w\u0303 = w\u2016w\u20162 . Under such a factorization, w can be reparameterized as w = gw\u0303, where g > 0 and \u2016w\u0303\u20162 = 1. Then the problem defined in Eq.(1) can be transformed into\nmin W\u0303,G L(W\u0303,G) s.t. \u2200j, gj \u2265 0, \u2016w\u0303j\u20162 = 1 \u2200i 6= j, |w\u0303i \u00b7 w\u0303j | \u2264 \u03c4\n(3)\nwhere W\u0303 = {w\u0303j}mj=1 and G = {gj}mj=1. We solve this new problem by alternating between W\u0303 and G. Fixing W\u0303 , the problem defined over G is: minG L(G) s.t. \u2200j, gj \u2265 0, which can be solved using projected gradient descent. Fixing G, the sub-problem defined over W\u0303 is\nmin W\u0303 L(W\u0303) s.t. \u2200j, \u2016w\u0303j\u20162 = 1 \u2200i 6= j, |w\u0303i \u00b7 w\u0303j | \u2264 \u03c4\n(4)\nwhich we solve using an ADMM algorithm. There are R = m(m \u2212 1) pairwise constraints |w\u0303i \u00b7 w\u0303j | \u2264 \u03c4 . For the r-th constraint, let p(r) and q(r) be the index of the first and second vector respectively, i.e., the r-th constraint is |w\u0303p(r) \u00b7 w\u0303q(r)| \u2264 \u03c4 . First, we introduce auxiliary variables {v(r)1 }Rr=1 and {v (r) 2 }Rr=1, to rewrite the problem in\nEq.(4) into an equivalent form. For each pairwise constraint: |w\u0303p(r) \u00b7 w\u0303q(r)| \u2264 \u03c4 , we introduce two auxiliary vectors v(r)1 and v (r) 2 , and let w\u0303p(r) = v (r) 1 , w\u0303q(r) = v (r) 2 , \u2016v(r)1 \u20162 = 1, \u2016v (r) 2 \u20162 = 1, |v (r) 1 \u00b7 v (r) 2 | \u2264 \u03c4 . To this end, we obtain the following problem\nmin W\u0303,V L(W\u0303) s.t. \u2200j, \u2016w\u0303j\u20162 = 1 \u2200r, w\u0303p(r) = v (r) 1 , w\u0303q(r) = v (r) 2\n\u2200r, \u2016v(r)1 \u20162 = 1, \u2016v (r) 2 \u20162 = 1, |v (r) 1 \u00b7 v (r) 2 | \u2264 \u03c4\nwhere V = {(v(r)1 ,v (r) 2 )}Rr=1. Then we define the augmented Lagrangian, with Lagrange multipliers Y = {(y(r)1 ,y (r) 2 )}Rr=1 and parameter \u03c1\nmin W\u0303,V,Y L(W\u0303) + R\u2211 r=1 (y (r) 1 \u00b7 (w\u0303p(r) \u2212 v (r) 1 )\n+y (r) 2 \u00b7 (w\u0303q(r) \u2212 v (r) 2 ) + \u03c1 2\u2016w\u0303p(r) \u2212 v (r) 1 \u201622 +\u03c12\u2016w\u0303q(r) \u2212 v (r) 2 \u201622)\ns.t. \u2200j, \u2016w\u0303j\u20162 = 1 \u2200r, \u2016v(r)1 \u20162 = 1, \u2016v (r) 2 \u20162 = 1, |v (r) 1 \u00b7 v (r) 2 | \u2264 \u03c4\nwhich can be solved by alternating between W\u0303 , V , Y .\nSolve W\u0303 The sub-problem defined over W\u0303 is\nmin W\u0303 L(W\u0303) + R\u2211 r=1 (y (r) 1 \u00b7 w\u0303p(r) + y (r) 2 \u00b7 w\u0303q(r)\n+\u03c12\u2016w\u0303p(r) \u2212 v (r) 1 \u201622 + \u03c1 2\u2016w\u0303q(r) \u2212 v (r) 2 \u201622)\ns.t. \u2200j, \u2016w\u0303j\u20162 = 1\n(5)\nFor sparse coding, we solve this sub-problem using coordinate descent. At each iteration, we update w\u0303j by fixing the other variables. Please refer to the supplements for details. For neural network, this sub-problem can be solved using projected gradient descent which iteratively performs the following three steps: (1) compute the gradient of w\u0303j using backpropagation; (2) perform a gradient descent update of w\u0303j ; (3) project each vector onto the unit sphere: w\u0303j \u2190 w\u0303j/\u2016w\u0303j\u20162.\nSolve v(r)1 ,v (r) 2 The corresponding sub-problem is\nmin v (r) 1 ,v (r) 2\n\u2212y(r)1 \u00b7 v (r) 1 \u2212 y (r) 2 \u00b7 v (r) 2\n+\u03c12\u2016w\u0303p(r) \u2212 v (r) 1 \u201622 + \u03c1 2\u2016w\u0303q(r) \u2212 v (r) 2 \u201622\ns.t. \u2016v(r)1 \u20162 = 1, \u2016v (r) 2 \u20162 = 1,\nv (r) 1 \u00b7 v (r) 2 \u2264 \u03c4,\u2212v (r) 1 \u00b7 v (r) 2 \u2264 \u03c4\nLet \u03b31, \u03b32, \u03bb1 \u2265 0, \u03bb2 \u2265 0 be the KKT multipliers associated with the four constraints in this sub-problem. According to the KKT conditions, we have\n\u2212y(r)1 +\u03c1(v (r) 1 \u2212w\u0303p(r))+2\u03b31v (r) 1 +(\u03bb1\u2212\u03bb2)v (r) 2 = 0 (6)\n\u2212y(r)2 +\u03c1(v (r) 2 \u2212w\u0303q(r))+2\u03b32v (r) 2 +(\u03bb1\u2212\u03bb2)v (r) 1 = 0 (7)\nWe solve these two equations by examining four cases.\nCase 1 First, we assume \u03bb1 = 0, \u03bb2 = 0, then (\u03c1 + 2\u03b31)v (r) 1 = y (r) 1 + \u03c1w\u0303p(r) and (\u03c1 + 2\u03b32)v (r) 2 = y (r) 2 + \u03c1w\u0303q(r). According to the primal feasibility \u2016v (r) 1 \u20162 = 1 and \u2016v(r)2 \u20162 = 1, we know\nv (r) 1 =\ny (r) 1 + \u03c1w\u0303p(r)\n\u2016y(r)1 + \u03c1w\u0303p(r)\u20162 , v\n(r) 2 =\ny (r) 2 + \u03c1w\u0303q(r)\n\u2016y(r)2 + \u03c1w\u0303q(r)\u20162\nThen we check whether the constraint |v(r)1 \u00b7 v (r) 2 | \u2264 \u03c4 is satisfied. If so, then v(r)1 and v (r) 2 are the optimal solution.\nCase 2 We assume \u03bb1 > 0 and \u03bb2 = 0, then\n(\u03c1+ 2\u03b31)v (r) 1 + \u03bb1v (r) 2 = y (r) 1 + \u03c1w\u0303p(r) (8)\n(\u03c1+ 2\u03b32)v (r) 2 + \u03bb1v (r) 1 = y (r) 2 + \u03c1w\u0303q(r) (9)\nAccording to the complementary slackness condition, we know v(r)1 \u00b7 v (r) 2 = \u03c4 . For the vectors on both sides of Eq.(8), taking the square of their `2 norm, we get\n(\u03c1+2\u03b31) 2+\u03bb21+2(\u03c1+2\u03b31)\u03bb1\u03c4 = \u2016y (r) 1 +\u03c1w\u0303p(r)\u201622 (10)\nSimilarly, from Eq.(9), we get\n(\u03c1+2\u03b32) 2+\u03bb21+2(\u03c1+2\u03b32)\u03bb1\u03c4 = \u2016y (r) 2 +\u03c1w\u0303q(r)\u201622 (11)\nTaking the inner product of the two vectors on the left hand sides of Eq.(8,9), and that on the right hand sides, we get\n(2\u03c1+ 2\u03b31 + 2\u03b32)\u03bb1 + ((\u03c1+ 2\u03b31)(\u03c1+ 2\u03b32) + \u03bb 2 1)\u03c4 = (y (r) 1 + \u03c1w\u0303p(r)) >(y (r) 2 + \u03c1w\u0303q(r))\n(12) Solving the system of equations consisting of Eq.(10-12), we obtain the optimal values of \u03b31, \u03b32 and \u03bb1. Plugging them into Eq.(8) and Eq.(9), we obtain a solution of v(r)1 and v(r)2 . Then we check whether this solution satisfies \u2212v(r)1 \u00b7 v (r) 2 \u2264 \u03c4 . If so, this is an optimal solution.\nIn Case 3, we discuss \u03bb1 = 0, \u03bb2 > 0. In Case 4, we discuss \u03bb1 > 0, \u03bb2 > 0. The corresponding problems can be solved in a similar way as Case 2. Please refer to the supplements for details.\nSolve y(r)1 ,y (r) 2 We simply perform the following:\ny (r) 1 = y (r) 1 + \u03c1(w\u0303p(r) \u2212 v (r) 1 ) (13)\ny (r) 2 = y (r) 2 + \u03c1(w\u0303q(r) \u2212 v (r) 2 ) (14)\nCompared with a vanilla backpropagation algorithm, the major extra cost in this ADMM algorithm comes from solving the R = m(m \u2212 1) pairs of vectors {v(r)1 ,v (r) 2 }Rr=1. Solving each pair incurs O(m) cost. The R pairs bring in a total cost of O(m3). Such a cubic cost is also incurred in other decorrelation methods such as (Le et al., 2010; Bao et al., 2013). In practice,m is typically less than 1000. This O(m3) cost does not substantially bottleneck computation, as we will validate in experiments."}, {"heading": "4. Analysis", "text": "In this section, we discuss how the parameter \u03c4 which controls the level of diversity affects the generalization performance of sparse coding and neural network."}, {"heading": "4.1. Sparse Coding", "text": "Following (Vainsencher et al., 2011), we assume the data example x \u2208 Rd and basis vector w \u2208 Rd are both of unit length, and the linear coefficient vector a \u2208 Rm is at most k sparse, i.e., \u2016a\u20160 \u2264 k. The estimation error of dictionary W is defined as\nL(W) = Ex\u223cp\u2217 [min\u2016a\u20160\u2264k \u2016x\u2212 \u2211m\nj=1 ajwj\u20162]. (15)\nLet L\u0303(W) = 1n \u2211n i=1 min\u2016a\u20160\u2264k \u2016xi \u2212 \u2211m j=1 ajwj\u20162 be the empirical reconstruction error on n samples. We have the following theorem.\nTheorem 1 Assume \u03c4 < 1k , with probability at least 1\u2212\u03b4:\nL(W) \u2264 L\u0303(W)+\n\u221a dm ln 4 \u221a nk\n1\u2212k\u03c4 2n +\n\u221a ln 1/\u03b4\n2n +\n\u221a 4\nn . (16)\nNote that the right hand side is an increasing function w.r.t \u03c4 . As expected, a smaller \u03c4 (implying more diversity) would induce a lower estimation error bound."}, {"heading": "4.2. Neural Network", "text": "The generalization error of a hypothesis f represented with a neural network is defined as L(f) = E(x,y)\u223cp\u2217 [`(f(x), y)], where p\u2217 is the distribution of inputoutput pair (x, y) and `(\u00b7, \u00b7) is the loss function. The training error is L\u0302(f) = 1n \u2211n i=1 `(f(x\n(i)), y(i)), where n is the number of training samples. Let f\u2217 \u2208 argminf\u2208FL(f) be the true risk minimizer and f\u0302 \u2208 argminf\u2208F L\u0302(f) be the\nempirical risk minimizer. We aim to analyze the generalization error L(f\u0302) of the empirical risk minimizer f\u0302 . L(f\u0302) can be decomposed into L(f\u0302) = L(f\u0302) \u2212 L(f\u2217) + L(f\u2217), where L(f\u0302) \u2212 L(f\u2217) is the estimation error and L(f\u2217) is the approximation error.\nFor simplicity, we start with a \u201csimple\u201d fully connected network with one hidden layer of m units, used for univariate regression (one output unit) with squared loss. Analysis for more complicated fully connected NNs with multiple hidden layers can be achieved in a straightforward way by cascading our analysis for this \u201csimple\u201d fully connected NN. Let x \u2208 Rd be the input vector and y be the response value. For simplicity, we assume max{\u2016x\u20162, |y|} \u2264 1. Let wj \u2208 Rd be the weights connecting the j-th hidden unit with input units, with \u2016wj\u20162 \u2264 C.\nLet \u03b1 be a vector where \u03b1j is the weight connecting hidden unit j to the output unit, with \u2016\u03b1\u20162 \u2264 B. We assume the activation function h(t) applied on the hidden units is Lipschitz continuous with constant L. Commonly used activation functions such as rectified linear h(t) = max(0, t), tanh h(t) = (et \u2212 e\u2212t)/(et + e\u2212t), and sigmoid h(t) = 1/(1+ e\u2212t) are all Lipschitz continuous with L = 1, 1, 0.25, respectively. Consider the hypothesis set\nF = {x 7\u2192 m\u2211 j=1 \u03b1jh(w > j x) | \u2016\u03b1\u20162 \u2264 B, \u2016wj\u20162 \u2264 C,\n\u2200i 6= j, |wi \u00b7wj | \u2264 \u03c4\u2016wi\u20162\u2016wj\u20162}.\nThe estimation error given in Theorem 2 below indicates how well the algorithm is able to learn from the samples.\nTheorem 2 Let the activation function h be L-Lipschitz continuous and the loss `(y\u0302, y) = 12 (y\u0302 \u2212 y)\n2. Then, with probability at least 1\u2212 \u03b4:\nL(f\u0302)\u2212L(f\u2217) \u2264 \u03b32\n\u221a 2 ln(4/\u03b4) + 4\u03b3B(2CL+ |h(0)|)\n\u221a m\u221a\nn (17) where \u03b3 = 1 +BCL \u221a (m\u2212 1)\u03c4 + 1 + \u221a mB|h(0)|.\nNote that \u03b3, hence the above bound on estimation error, decreases as \u03c4 becomes smaller. The bound goes to zero as n (sample size) goes to infinite. The inverse square root dependence on nmatches existing results (Bartlett & Mendelson, 2003). We note that it is straightforward to extend our bound to any bounded Lipschitz continuous loss `.\nThe approximation error indicates how capable the hypothesis set F is to approximate a target function g = E[y|x], where the error is measured by minf\u2208F\u2016f \u2212 g\u2016L2 and \u2016f \u2212 g\u20162L2 = \u222b (f(x) \u2212 g(x))2P (dx). Following (Barron, 1993), we assume the target function g satisfies certain smoothness condition that is expressed in the first moment of its Fourier representation: \u222b \u2016\u03c9\u20162|g\u0303(\u03c9)|d\u03c9 \u2264 B/2\nwhere g\u0303(\u03c9) is the Fourier representation of g(x). For such function the following theorem states its approximation error. (In order to derive explicit constants we restrict h to be the sigmoid function, but other Lipschitz continuous activation function can be similarly handled.)\nTheorem 3 Let C > 1, m \u2264 2(b \u03c0 2\u2212\u03b8 \u03b8 c + 1), where \u03b8 = arccos(\u03c4), and h(t) = 1/(1 + e\u2212t). Then, there is a function f \u2208 F such that\n\u2016f \u2212 g\u2016L2 \u2264 B( 1\u221am + 1+2 lnC C )+\n+ 2 \u221a mBC sin(min(3m\u03b8,\u03c0)2 ). (18)\nThis theorem implies that whether to use the angular constraint (AC) or not has a significant influence on the approximate error bound: without using AC (\u03c4 = 1), the bound is a decreasing function of m (the number of hidden units); using AC (\u03c4 < 1), the bound increases with m. This striking phrase-change indicates the impact of AC. Given a fixed m, the bound decreases with \u03c4 , implying that a stronger regularization (smaller \u03c4 ) incurs larger approximation error. When \u03c4 = 1, the second term in the bound vanishes and the bound is reduced to the one in (Barron, 1993), which is a decreasing function of m (and C, the upper bound on the weights). When \u03c4 < 1, the second term increases withm up to the upper bound 2(b\n\u03c0 2\u2212\u03b8 \u03b8 c+1). This\nis because a larger number of hidden units bear a larger difficulty in satisfying the pairwise ACs, which causes the function space F to shrink rapidly; accordingly, the approximation power of F decreases quickly.\nThe analysis in the two theorems shows that \u03c4 incurs a tradeoff between the estimation error and the approximation error: decreasing \u03c4 reduces the estimation error and enlarges the approximation error. Since the generalization error is the sum of the estimation error and the approximation error, \u03c4 has an optimal value to yield the minimal generalization error."}, {"heading": "5. Experiments", "text": "In this section, we present experimental results. Due to space limit, we put some results into supplements."}, {"heading": "5.1. Sparse Coding", "text": "Following (Yang et al., 2009), we applied sparse coding for image feature learning. We used three datasets in the experiments: Scenes-15 (Lazebnik et al., 2006), Caltech256 (Griffin et al., 2007) and UIUC-Sport (Li & Fei-Fei, 2007). For each dataset, five random train/test splits are performed and the results are averaged over the five runs. We extract pixel-level dense SIFT (Lowe, 2004) features where the step size and patch size are 8 and 16 respectively. On top of the SIFT features, we use sparse coding methods to learn a dictionary and represent each SIFT\nfeature into a sparse code. To obtain image-level features, we apply max-pooling (Yang et al., 2009) and spatial pyramid matching (Lazebnik et al., 2006; Yang et al., 2009) over the pixel-level sparse codes. Then a linear SVM is applied to classify the images. We compare with other diversity-promoting regularizers including determinant of covariance matrix (DCM) (Malkin & Bilmes, 2008), cosine similarity (CS) (Yu et al., 2011), determinantal point process (DPP) (Kulesza & Taskar, 2012; Zou & Adams, 2012b), InCoherence (IC) (Bao et al., 2013) and mutual angles (MA) (Xie et al., 2015). We use 5-fold cross validation to tune \u03c4 in {0.3, 0.4, \u00b7 \u00b7 \u00b7 , 1} and the number of basis vectors in {50, 100, 200, \u00b7 \u00b7 \u00b7 , 500}. The parameter \u03c1 in ADMM is set to 1.\nTable 1 shows the classification accuracy on three datasets, from which we can see that compared with unregularized SC, AC-SC greatly improves performance. For example, on the Sports dataset, AC improves the accuracy from 87.4% to 90.9%. This suggests that AC is effective in reducing overfitting and improving generalization performance. Compared with other diversity-promoting regularizers, AC achieves better performance, demonstrating its better efficacy in promoting diversity."}, {"heading": "5.2. Neural Networks", "text": "We evaluate AC on three types of neural networks: fullyconnected NN (FNN) for phone recognition (Hinton et al., 2012), CNN for image classification (Krizhevsky et al., 2012), and RNN for question answering (Seo et al., 2017). In the main paper, we report results on four datasets: TIMIT1, CIFAR-102, CNN (Hermann et al., 2015), Daily Mail (Hermann et al., 2015). Please refer to the supplements for results on other datasets.\nFNN for Phone Recognition The TIMIT dataset contains a total of 6300 sentences (5.4 hours), divided into a training set (462 speakers), a validation set (50 speakers) and a core test set (24 speakers). We used the Kaldi (Povey et al., 2011) toolkit to train the monophone system which was utilized to do forced alignment and to get labels for speech frames. The toolkit was also utilized to preprocess the data into log-filter banks. Among methods based on FNN, Karel\u2019s recipe in Kaldi achieves state\n1https://catalog.ldc.upenn.edu/LDC93S1 2https://www.cs.toronto.edu/ kriz/cifar.html\nof the art performance. We apply AC to the FNN in this recipe. The inputs of the FNN are the FMLLR (Gales, 1998) features of the neighboring 21 frames, which are mean centered and normalized to have unit variance. The number of hidden layers is 4. Each layer has 1024 hidden units. Stochastic gradient descent (SGD) is used to train the network. The learning rate is set to 0.008. We compare with four diversity-promoting regularizers: CS, IC, MA and DeCorrelation (DC) (Cogswell et al., 2015). The regularization parameter in these methods are tuned in {10\u22126, 10\u22125, \u00b7 \u00b7 \u00b7 , 105}. The \u03b2 parameter in IC is set to 1.\nTable 2 shows state of the art phone error rate (PER) on the TIMIT core test set. Methods in the first panel are mostly based on FNN, which perform less well than Kaldi. Methods in the third panel are all based on RNNs which in general perform better than FNN since they are able to capture the temporal structure in speech data. In the second panel, we compare AC with other diversity-promoting regularizers. Without regularization, the error is 18.53%. With AC, the error is reduced to 18.41%, which is very close to a strong RNN-based baseline Connectionist Temporal Classification (CTC) (Graves et al., 2013). Besides, AC outperforms other regularizers.\nCNN for Image Classification The CIFAR-10 dataset contains 32x32 color images from 10 categories, with 50,000 images for training and 10,000 for testing. We used 5000 training images as the validation set to tune hyperparameters. The data is augmented by first zero-padding the images with 4 pixels on each side, then randomly cropping\nthe padded images to reproduce 32x32 images. We apply AC to wide residual network (WideResNet) (Zagoruyko & Komodakis, 2016) where the depth is set to 28 and the width is set to 10. SGD is used for training, with epoch number 200, initial learning rate 0.1, minibatch size 128, Nesterov momentum 0.9, dropout probability 0.3 and weight decay 0.0005. The learning rate is dropped by 0.2 at 60, 120 and 160 epochs. The performance is the median of 5 runs. We compare with CS, IC, MA, DC and an Orthogonality-Promoting (OP) regularizer (Rodr\u0131\u0301guez et al., 2016).\nTable 3 shows state of the art classification error on the test set. Compared with the unregularized WideResNet which achieves an error of 3.89%, applying AC reduces the error to 3.63%. AC achieves lower error than other regularizers.\nLSTM for Question Answering We apply AC to long short-term memory (LSTM) (Hochreiter & Schmidhuber, 1997) network, which is a type of RNN. Given the input xt at timestamp t, LSTM produces a hidden state ht based on the following transition equations:\nit = \u03c3(W (i)xt +U (i)ht\u22121 + b (i)) ft = \u03c3(W (f)xt +U (f)ht\u22121 + b (f)) ot = \u03c3(W (o)xt +U (o)ht\u22121 + b (o)) ct = it tanh(W(c)xt +U(c)ht\u22121 + b(c)) + ft ct\u22121 ht = ot tanh(ct)\nwhere Ws are Us are gate-specific weight matrices. On the row vectors of each weight matrix, the AC is applied. The LSTM is used for a question answering (QA) task on two datasets: CNN and DailyMail (Hermann et al., 2015),\neach containing a training, development and test set with 300k/4k/3k and 879k/65k/53k examples respectively. Each example consists of a passage, a question and an answer. The question is a cloze-style task where an entity is replaced by a placeholder and the goal is to infer this missing entity (answer) from all the possible entities appearing in the passage. The LSTM architecture and experimental settings follow the Bidirectional Attention Flow (BIDAF) (Seo et al., 2017) model, which consists of the following layers: character embedding, word embedding, contextual embedding, attention flow, modeling and output. LSTM is applied in the contextual embedding and modeling layer. Character embedding is based on onedimensional convolutional neural network, where the number of filters is set to 100 and the width of receptive field is set to 5. In LSTM, the size of hidden state is set to 100. Optimization is based on AdaDelta (Zeiler, 2012), where the minibatch size and initial learning rate are set to 48 and 0.5. The model is trained for 8 epochs. Dropout (Srivastava et al., 2014) with probability 0.2 is applied. We compare with four diversity promoting regularizers: CS, IC, MA and DC.\nTable 4 shows state of the art accuracy on the two datasets. As can be seen, after applying AC to BIDAF, the accuracy is improved from 76.94% to 77.23% on the CNN test set and from 79.63% to 79.88% on the DailyMail test set. Among the diversity-promoting regularizers, AC achieves the highest accuracy."}, {"heading": "5.3. Sensitivity to Parameter \u03c4", "text": "In the theoretical analysis presented in Section 4, we have shown that the parameter \u03c4 which controls the level of near-\northogonality (or diversity) incurs a tradeoff between estimation error and approximation error. In this section, we provide an empirical verification, using FNN on TIMIT as a study case. Figure 1 shows how the phone error rates vary on the TIMIT core test set. As can be seen, the lowest test error is achieved under a moderate \u03c4 (= 0.75). Either a smaller or a larger \u03c4 degrades the performance. This empirical observation is aligned with the theoretical analysis that the best generalization performance is achieved under a properly chosen \u03c4 . When \u03c4 is close to 0, the hidden units are close to orthogonality, which yields much poorer performance. This confirms that the strict-orthogonality constraint proposed by (Le et al., 2010) is too restrictive and is less favorable than a \u201csoft\u201d regularization approach."}, {"heading": "5.4. Computational Time", "text": "We compare the computational time of neural networks under different regularizers. Table 5 shows the total runtime time of FNNs on TIMIT and CNNs on CIFAR-10 with a single GTX TITAN X GPU, and the runtime of LSTM networks on the CNN dataset with 2 TITAN X GPUs. Compared with no regularization, AC incurs a 18.2% extra time on TIMIT, 12.7% on CIFAR-10 and 14.8% on CNN. The runtime of AC is comparable to that under other diversitypromoting regularizers."}, {"heading": "6. Conclusions", "text": "In this paper, we propose Angled-Constrained Latent Space Models (AC-LSMs) that aim at promoting diversity among components in LSMs for the sake of alleviating overfitting. Compared with previous diversity-promoting methods, AC has two benefits. First, it is theoretically analyzable: the generalization error analysis shows that larger diversity leads to smaller estimation error and larger approximation error. Second, it is empirically effective, as validated in various experiments."}, {"heading": "Acknowledgements", "text": "We would like to thank the anonymous reviewers for the suggestions and comments that help to improve this work a lot, and thank Yajie Miao for helping with some of the experiments. P.X and E.X are supported by National Institutes of Health P30DA035778, R01GM114311, National Science Foundation IIS1617583, DARPA FA872105C0003 and Pennsylvania Department of Health BD4BH4100070287."}], "year": 2017, "references": [{"title": "Deep segmental neural networks for speech recognition", "authors": ["Abdel-Hamid", "Ossama", "Deng", "Li", "Yu", "Dong", "Jiang", "Hui"], "year": 2013}, {"title": "Approximate inference in continuous determinantal processes", "authors": ["Affandi", "Raja Hafiz", "Fox", "Emily", "Taskar", "Ben"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2013}, {"title": "Do deep nets really need to be deep? In Advances in neural information processing", "authors": ["Ba", "Jimmy", "Caruana", "Rich"], "year": 2014}, {"title": "Incoherent training of deep neural networks to de-correlate bottleneck features for speech recognition", "authors": ["Bao", "Yebo", "Jiang", "Hui", "Dai", "Lirong", "Liu", "Cong"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "year": 2013}, {"title": "Universal approximation bounds for superpositions of a sigmoidal function", "authors": ["Barron", "Andrew R"], "venue": "Information Theory, IEEE Transactions on,", "year": 1993}, {"title": "Rademacher and gaussian complexities: Risk bounds and structural results", "authors": ["Bartlett", "Peter L", "Mendelson", "Shahar"], "venue": "Journal of Machine Learning Research,", "year": 2003}, {"title": "A thorough examination of the cnn/daily mail reading comprehension", "authors": ["Chen", "Danqi", "Bolton", "Jason", "Manning", "Christopher D"], "venue": "task. arXiv preprint arXiv:1606.02858,", "year": 2016}, {"title": "Training group orthogonal neural networks with privileged information", "authors": ["Chen", "Yunpeng", "Jin", "Xiaojie", "Feng", "Jiashi", "Yan", "Shuicheng"], "venue": "arXiv preprint arXiv:1701.06772,", "year": 2017}, {"title": "Attention-based models for speech recognition", "authors": ["Chorowski", "Jan K", "Bahdanau", "Dzmitry", "Serdyuk", "Dmitriy", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2015}, {"title": "Fast and accurate deep network learning by exponential linear units (elus)", "authors": ["Clevert", "Djork-Arn\u00e9", "Unterthiner", "Thomas", "Hochreiter", "Sepp"], "venue": "arXiv preprint arXiv:1511.07289,", "year": 2015}, {"title": "Reducing overfitting in deep networks by decorrelating representations", "authors": ["M. Cogswell", "F. Ahmed", "R. Girshick", "L. Zitnick", "D. Batra"], "year": 2015}, {"title": "Attention-over-attention neural networks for reading comprehension", "authors": ["Cui", "Yiming", "Chen", "Zhipeng", "Wei", "Si", "Wang", "Shijin", "Liu", "Ting", "Hu", "Guoping"], "venue": "arXiv preprint arXiv:1607.04423,", "year": 2016}, {"title": "Phone recognition with the meancovariance restricted boltzmann machine", "authors": ["Dahl", "George", "Mohamed", "Abdel-rahman", "Hinton", "Geoffrey E"], "venue": "In Advances in neural information processing systems,", "year": 2010}, {"title": "Gated-attention readers for text comprehension", "authors": ["Dhingra", "Bhuwan", "Liu", "Hanxiao", "Cohen", "William W", "Salakhutdinov", "Ruslan"], "venue": "arXiv preprint arXiv:1606.01549,", "year": 2016}, {"title": "Linguistic knowledge as memory for recurrent neural networks", "authors": ["Dhingra", "Bhuwan", "Yang", "Zhilin", "Cohen", "William W", "Salakhutdinov", "Ruslan"], "venue": "arXiv preprint arXiv:1703.02620,", "year": 2017}, {"title": "Maximum likelihood linear transformations for hmm-based speech recognition", "authors": ["Gales", "Mark JF"], "venue": "Computer speech & language,", "year": 1998}, {"title": "Fractional max-pooling", "authors": ["Graham", "Benjamin"], "venue": "arXiv preprint arXiv:1412.6071,", "year": 2014}, {"title": "Speech recognition with deep recurrent neural networks. In Acoustics, speech and signal processing", "authors": ["Graves", "Alex", "Mohamed", "Abdel-rahman", "Hinton", "Geoffrey"], "venue": "(icassp), 2013 ieee international conference on,", "year": 2013}, {"title": "Caltech256 object category dataset", "authors": ["Griffin", "Gregory", "Holub", "Alex", "Perona", "Pietro"], "year": 2007}, {"title": "Deep residual learning for image recognition", "authors": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "year": 2016}, {"title": "Orthogonal rnns and long-memory tasks", "authors": ["Henaff", "Mikael", "Szlam", "Arthur", "LeCun", "Yann"], "venue": "arXiv preprint arXiv:1602.06662,", "year": 2016}, {"title": "Teaching machines to read and comprehend", "authors": ["Hermann", "Karl Moritz", "Kocisky", "Tomas", "Grefenstette", "Edward", "Espeholt", "Lasse", "Kay", "Will", "Suleyman", "Mustafa", "Blunsom", "Phil"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2015}, {"title": "The goldilocks principle: Reading children\u2019s books with explicit memory", "authors": ["Hill", "Felix", "Bordes", "Antoine", "Chopra", "Sumit", "Weston", "Jason"], "venue": "representations. ICLR,", "year": 2015}, {"title": "Long shortterm memory", "authors": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "year": 1997}, {"title": "Densely connected convolutional networks", "authors": ["Huang", "Gao", "Liu", "Zhuang", "Weinberger", "Kilian Q", "van der Maaten", "Laurens"], "venue": "arXiv preprint arXiv:1608.06993,", "year": 2016}, {"title": "Text understanding with the attention sum reader network", "authors": ["Kadlec", "Rudolf", "Schmid", "Martin", "Bajgar", "Ondrej", "Kleindienst", "Jan"], "year": 2016}, {"title": "Dynamic entity representation with maxpooling improves machine reading", "authors": ["Kobayashi", "Sosuke", "Tian", "Ran", "Okazaki", "Naoaki", "Inui", "Kentaro"], "venue": "In Proceedings of NAACL-HLT,", "year": 2016}, {"title": "Imagenet classification with deep convolutional neural networks", "authors": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In Advances in neural information processing systems,", "year": 2012}, {"title": "Determinantal point processes for machine learning", "authors": ["Kulesza", "Alex", "Taskar", "Ben"], "venue": "Foundations and Trends in Machine Learning,", "year": 2012}, {"title": "Beyond bags of features: Spatial pyramid matching for recognizing scene categories", "authors": ["Lazebnik", "Svetlana", "Schmid", "Cordelia", "Ponce", "Jean"], "venue": "In CVPR,", "year": 2006}, {"title": "Tiled convolutional neural networks", "authors": ["Le", "Quoc V", "Ngiam", "Jiquan", "Chen", "Zhenghao", "Chia", "Daniel", "Koh", "Pang Wei", "Ng", "Andrew Y"], "venue": "In Proceedings of the 23rd International Conference on Neural Information Processing Systems,", "year": 2010}, {"title": "What, where and who? classifying events by scene recognition", "authors": ["Li", "Li-Jia", "Fei-Fei"], "venue": "In ICCV,", "year": 2007}, {"title": "Distinctive image features from scaleinvariant keypoints", "authors": ["Lowe", "David G"], "year": 2004}, {"title": "Segmental recurrent neural networks for end-to-end speech recognition", "authors": ["Lu", "Liang", "Kong", "Lingpeng", "Dyer", "Chris", "Smith", "Noah A", "Renals", "Steve"], "venue": "arXiv preprint arXiv:1603.00223,", "year": 2016}, {"title": "Multi-task learning with ctc and segmental crf for speech recognition", "authors": ["Lu", "Liang", "Kong", "Lingpeng", "Dyer", "Chris", "Smith", "Noah A"], "venue": "arXiv preprint arXiv:1702.06378,", "year": 2017}, {"title": "Ratio semi-definite classifiers", "authors": ["Malkin", "Jonathan", "Bilmes", "Jeff"], "venue": "In Acoustics, Speech and Signal Processing,", "year": 2008}, {"title": "Stochastic decorrelation constraint regularized autoencoder for visual recognition", "authors": ["Mao", "Fengling", "Xiong", "Wei", "Du", "Bo", "Zhang", "Lefei"], "venue": "In International Conference on Multimedia Modeling,", "year": 2017}, {"title": "All you need is a good init", "authors": ["Mishkin", "Dmytro", "Matas", "Jiri"], "venue": "arXiv preprint arXiv:1511.06422,", "year": 2015}, {"title": "Rnndrop: A novel dropout for rnns in asr", "authors": ["Moon", "Taesup", "Choi", "Heeyoul", "Lee", "Hoshik", "Song", "Inchul"], "venue": "In Automatic Speech Recognition and Understanding (ASRU),", "year": 2015}, {"title": "Sparse coding with an overcomplete basis set: A strategy employed by v1", "authors": ["Olshausen", "Bruno A", "Field", "David J"], "venue": "Vision research,", "year": 1997}, {"title": "The kaldi speech recognition toolkit", "authors": ["Povey", "Daniel", "Ghoshal", "Arnab", "Boulianne", "Gilles", "Burget", "Lukas", "Glembek", "Ondrej", "Goel", "Nagendra", "Hannemann", "Mirko", "Motlicek", "Petr", "Qian", "Yanmin", "Schwarz"], "venue": "In IEEE 2011 workshop on automatic speech recognition and un-", "year": 2011}, {"title": "Classification and clustering via dictionary learning with structured incoherence and shared features", "authors": ["Ramirez", "Ignacio", "Sprechmann", "Pablo", "Sapiro", "Guillermo"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "year": 2010}, {"title": "Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization", "authors": ["Recht", "Benjamin", "Fazel", "Maryam", "Parrilo", "Pablo A"], "venue": "SIAM review,", "year": 2010}, {"title": "Regularizing cnns with locally constrained decorrelations", "authors": ["Rodr\u0131\u0301guez", "Pau", "Gonz\u00e0lez", "Jordi", "Cucurull", "Guillem", "Gonfaus", "Josep M", "Roca", "Xavier"], "year": 1967}, {"title": "Bidirectional attention flow for machine comprehension", "authors": ["Seo", "Minjoon", "Kembhavi", "Aniruddha", "Farhadi", "Ali", "Hajishirzi", "Hannaneh"], "year": 2017}, {"title": "Reasonet: Learning to stop reading in machine comprehension", "authors": ["Shen", "Yelong", "Huang", "Po-Sen", "Gao", "Jianfeng", "Chen", "Weizhu"], "venue": "arXiv preprint arXiv:1609.05284,", "year": 2016}, {"title": "Iterative alternating neural attention for machine reading", "authors": ["Sordoni", "Alessandro", "Bachman", "Philip", "Trischler", "Adam", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1606.02245,", "year": 2016}, {"title": "Striving for simplicity: The all convolutional net", "authors": ["Springenberg", "Jost Tobias", "Dosovitskiy", "Alexey", "Brox", "Thomas", "Riedmiller", "Martin"], "venue": "arXiv preprint arXiv:1412.6806,", "year": 2014}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "authors": ["Srivastava", "Nitish", "Hinton", "Geoffrey E", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "Journal of Machine Learning Research,", "year": 1929}, {"title": "Discriminative segmental cascades for featurerich phone recognition", "authors": ["Tang", "Hao", "Wang", "Weiran", "Gimpel", "Kevin", "Livescu", "Karen"], "venue": "In Automatic Speech Recognition and Understanding (ASRU),", "year": 2015}, {"title": "Regression shrinkage and selection via the lasso", "authors": ["Tibshirani", "Robert"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), pp", "year": 1996}, {"title": "Phone recognition with deep sparse rectifier neural networks", "authors": ["T\u00f3th", "L\u00e1szl\u00f3"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "year": 2013}, {"title": "Combining time-and frequency-domain convolution in convolutional neural network-based phone recognition", "authors": ["T\u00f3th", "L\u00e1szl\u00f3"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "year": 2014}, {"title": "Natural language comprehension with the epireader", "authors": ["Trischler", "Adam", "Ye", "Zheng", "Yuan", "Xingdi", "Suleman", "Kaheer"], "venue": "arXiv preprint arXiv:1606.02270,", "year": 2016}, {"title": "The sample complexity of dictionary learning", "authors": ["Vainsencher", "Daniel", "Mannor", "Shie", "Bruckstein", "Alfred M"], "venue": "Journal of Machine Learning Research,", "year": 2011}, {"title": "Diversity leads to generalization in neural networks", "authors": ["Xie", "Bo", "Liang", "Yingyu", "Song", "Le"], "year": 2017}, {"title": "Learning compact and effective distance metrics with diversity regularization", "authors": ["Xie", "Pengtao"], "venue": "In ECML,", "year": 2015}, {"title": "Diversifying restricted boltzmann machine for document modeling", "authors": ["Xie", "Pengtao", "Deng", "Yuntian", "Xing", "Eric P"], "venue": "In ACM SIGKDD Conference on Knowledge Discovery and Data Mining,", "year": 2015}, {"title": "Diversitypromoting bayesian learning of latent variable models", "authors": ["Xie", "Pengtao", "Zhu", "Jun", "Xing", "Eric"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "year": 2016}, {"title": "Aggregated residual transformations for deep neural networks", "authors": ["Xie", "Saining", "Girshick", "Ross", "Doll\u00e1r", "Piotr", "Tu", "Zhuowen", "He", "Kaiming"], "venue": "arXiv preprint arXiv:1611.05431,", "year": 2016}, {"title": "Regularizing deep convolutional neural networks with a structured decorrelation constraint", "authors": ["Xiong", "Wei", "Du", "Bo", "Zhang", "Lefei", "Hu", "Ruimin", "Tao", "Dacheng"], "venue": "In Data Mining (ICDM),", "year": 2016}, {"title": "Deep pyramidal residual networks with separated stochastic depth", "authors": ["Yamada", "Yoshihiro", "Iwamura", "Masakazu", "Kise", "Koichi"], "venue": "arXiv preprint arXiv:1612.01230,", "year": 2016}, {"title": "Log-linear system combination using structured support vector machines", "authors": ["J Yang", "Ragni", "Anton", "Gales", "Mark JF", "Knill", "Kate M"], "venue": "In Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH,", "year": 2016}, {"title": "Linear spatial pyramid matching using sparse coding for image classification", "authors": ["Yang", "Jianchao", "Yu", "Kai", "Gong", "Yihong", "Huang", "Thomas"], "venue": "In CVPR,", "year": 2009}, {"title": "Diversity regularized machine", "authors": ["Yu", "Yang", "Li", "Yu-Feng", "Zhou", "Zhi-Hua"], "venue": "In IJCAI,", "year": 2011}, {"title": "Wide residual networks", "authors": ["Zagoruyko", "Sergey", "Komodakis", "Nikos"], "venue": "arXiv preprint arXiv:1605.07146,", "year": 2016}, {"title": "Adadelta: an adaptive learning rate method", "authors": ["Zeiler", "Matthew D"], "venue": "arXiv preprint arXiv:1212.5701,", "year": 2012}, {"title": "Priors for diversity in generative latent variable models", "authors": ["Zou", "James Y", "Adams", "Ryan P"], "venue": "In NIPS,", "year": 2012}, {"title": "Priors for diversity in generative latent variable models", "authors": ["Zou", "James Y", "Adams", "Ryan P"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2012}], "id": "SP:129d8e5cbf7c8c60631a1afa9779d9dad78940f6", "authors": [{"name": "Pengtao Xie", "affiliations": []}, {"name": "Yuntian Deng", "affiliations": []}, {"name": "Yi Zhou", "affiliations": []}, {"name": "Abhimanu Kumar", "affiliations": []}, {"name": "Yaoliang Yu", "affiliations": []}, {"name": "James Zou", "affiliations": []}, {"name": "Eric P. Xing", "affiliations": []}], "abstractText": "The large model capacity of latent space models (LSMs) enables them to achieve great performance on various applications, but meanwhile renders LSMs to be prone to overfitting. Several recent studies investigate a new type of regularization approach, which encourages components in LSMs to be diverse, for the sake of alleviating overfitting. While they have shown promising empirical effectiveness, in theory why larger \u201cdiversity\u201d results in less overfitting is still unclear. To bridge this gap, we propose a new diversitypromoting approach that is both theoretically analyzable and empirically effective. Specifically, we use near-orthogonality to characterize \u201cdiversity\u201d and impose angular constraints (ACs) on the components of LSMs to promote diversity. A generalization error analysis shows that larger diversity results in smaller estimation error and larger approximation error. An efficient ADMM algorithm is developed to solve the constrained LSM problems. Experiments demonstrate that ACs improve generalization performance of LSMs and outperform other diversitypromoting approaches.", "title": "Learning Latent Space Models with Angular Constraints"}