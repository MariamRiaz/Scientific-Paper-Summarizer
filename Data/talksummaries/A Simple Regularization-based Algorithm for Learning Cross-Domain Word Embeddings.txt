7	19	In this paper, we propose a simple and easyto-implement approach for learning cross-domain word embeddings.
23	34	Let us first state the objective of the skip-gram model (Mikolov et al., 2013a) as follows: LD = ∑ (w,c)∈D #(w, c) ( log σ(w · c) + k∑ i=1 Ec′i∼P (w)[log σ(−w · c′i)] ) (1) where D refers to the complete text corpus from which we learn the word embeddings.
29	72	Our approach is inspired by the recent regularization-based domain adaptation framework (Lu et al., 2016).
31	21	We define αw as follows: αw = σ(λ · φ(w)) (3) where λ is a hyper-parameter to decide the scaling factor of the significance function φ(·), which allows the user to control the degree of “knowledge transfer” from source domain to target domain.
32	19	How do we define the significance function φ(w) that controls the amount of transfer for the word w?
34	6	We define the function φ(·) based on the following metric that is motivated by the well-known Sørensen-Dice coefficient (Sørensen, 1948; Dice, 1945) commonly used for measuring similarities: φ(w) = 2 · FDs(w) · FDt(w) FDs(w) + FDt(w) (5) Why does such a definition make sense?
35	9	We note that the value of φ(w) would be high only if both both FDs(w) and FDt(w) are high – in this case the word w is a frequent word across different domains.
37	8	On the other hand, domain-specific words tend to be more frequent in one domain than the other.
41	17	We present extensive evaluations to assess the effectiveness of our approach.
42	8	Following recent advice by Nayak et al. (2016) and Faruqui et al. (2016), to assess the quality of the learned word embeddings, we considered employing the learned word embeddings as continuous features in several down-stream NLP tasks, including entity recognition, sentiment classification, and targeted sentiment analysis.
46	12	• DISCRETE: only discrete features (such as bag of words, POS tags, word n-grams and POS tag n-grams, depending on the actual down-stream task) were considered.
47	7	All following systems include both these base features and the respective additional features.
49	22	• TARGET: we train word embeddings from the target domain as additional features.
50	17	• ALL: we combined the data from two domains to form a single dataset for learning word embeddings as additional features.
53	17	Our first experiment was conducted on entity recognition (Tjong Kim Sang and De Meulder, 2003; Florian et al., 2004), where the task is to extract semantically meaning entities and their mentions from the text.
58	16	For the GENIA dataset which consists of 10,946 sentences, we used Enwik9 as the source domain and PubMed as the target domain for learning word embeddings.
68	40	We note that in such a task, many entities consist of domain-specific terms, therefore learning good representations for such words can be crucial.
80	12	We conducted two sets of experiments: we first used the Yelp dataset as the source domain and IMDB as the target domain, and then we switched these two datasets in our second set of experiments.
81	9	Figure 1 shows the F1 measures for different word embeddings when different amounts of training data were used.
86	17	However, as the training set size increases, there is no significant improvement for such an approach.
91	14	We used the state-of-the-art system for targeted sentiment analysis by Li and Lu (2017) whose code is publicly available 5, and used the data from (Mitchell et al., 2013) which consists of 7,105 Spanish tweets and 2,350 English tweets, with named entities and their sentiment information annotated.
96	41	For the English task, we used Enwik9 as the source domain for learning word embeddings, and our crawled English tweets as the target domain.
97	7	For the Spanish task, we used Eswiki as the source domain, and we also crawled Spanish tweets as the target domain.
101	10	Results are reported in Table 3, which show our approach is able to achieve the best results across two datasets in such a task, and outperforms DARep (p < 0.05).
104	21	In contrast, our approach learns embeddings for the target domain by capturing useful cross-domain information and therefore can lead to improved modeling of embeddings that are shown more helpful for this specific down-stream task.
107	20	Our work can be easily extended to multi-domain scenarios.
108	14	The method is also flexible, allowing different user-defined metrics to be incorporated for defining the function controlling the amount of domain transfer.
109	37	Future work includes performing further investigations to better understand and to visualize what types of information has been transferred across domains and how such information influence different types of down-stream NLP tasks.
110	96	It is also important to understand how such an approach will work on other types of models such as neural networks based NLP models.
111	27	Our code is available at http://statnlp.org/research/lr/.
