0	23	Contemporary natural language processing systems typically rely on annotated data to learn how to perform a task (e.g., classification, sequence tagging, natural language inference).
4	60	A scalable way to build multilingual systems is through cross-lingual language understanding (XLU), in which a system is trained primarily on data in one language and evaluated on data in others.
5	19	While XLU shows promising results for tasks such as cross-lingual document classification (Klementiev et al., 2012; Schwenk and Li, 2018), there are very few, if any, XLU benchmarks for more difficult language understanding tasks like natural language inference.
6	28	Large-scale natural language inference (NLI), also known as recognizing textual entailment (RTE), has emerged as a practical test bed for work on sentence understanding.
7	11	In NLI, a system is tasked with reading two sentences and determining whether one entails the other, contradicts it, or neither (neutral).
8	121	Recent crowdsourced annotation efforts have yielded datasets for NLI in English (Bowman et al., 2015; Williams et al., 2017) with nearly a million examples, and these have been widely used to evaluate neural network architectures and training strategies (Rocktäschel et al., 2016; Gong et al., 2018; Peters et al., 2018; Wang et al., 2018), as well as to train effective, reusable sentence representations (Conneau et al., 2017; Subramanian et al., 2018; Cer et al., 2018).
9	50	In this work, we introduce a benchmark that we call the Cross-lingual Natural Language Inference corpus, or XNLI, by extending these NLI corpora to 15 languages.
10	71	XNLI consists of 7500 human-annotated development and test examples in NLI three-way classification format in English, French, Spanish, German, Greek, Bulgarian, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese, Hindi, Swahili and Urdu, making a total of 112,500 annotated pairs.
11	66	These languages span several language families, and with the inclusion of Swahili and Urdu, include two lower-resource languages as well.
64	11	The tenth, Fiction, is drawn from the novel Captain Blood (Sabatini, 1922).
73	17	We translate the premises and hypotheses separately, to ensure that no context is added to the hypothesis that was not there originally, and simply copy the labels from the English source text.
91	15	There are two natural ways to use a translation system: TRANSLATE TRAIN, where the training data is translated into each target language to provide data to train each classifier, and TRANSLATE TEST, where a translation system is used at test time to translate input sentences to the training language.
94	16	Both approaches are limited by the quality of the translation system, which itself varies with the quantity of available training data and the similarity of the language pair involved.
96	136	If an encoder produces an embedding of an English sentence close to the embedding of its translation in another language, then a classifier learned on top of English sentence embeddings will be able to classify sentences from different languages without needing a translation system at inference time.
97	35	We evaluate two types of cross-lingual sentence encoders: (i) pretrained universal multilingual sentence embeddings based on the average of word embeddings (X-CBOW), (ii) bidirectionalLSTM (BiLSTM) (Hochreiter and Schmidhuber, 1997) sentence encoders trained on the MultiNLI training data (X-BILSTM).
99	13	Both approaches use the same alignment loss for aligning sentence embedding spaces from multiple languages which is present below.
103	19	The second approach consists in learning an English sentence encoder on the MultiNLI training data along with an encoder on the target language, with the objective that the representations of two translations are nearby in the embedding space.
104	14	In both approaches, an English encoder is fixed, and we train target language encoders to match the output of this encoder.
105	13	This allows us to build sentence representations that belong to the same space.
106	12	Joint training of encoders and parameter sharing are also promising directions to improve and simplify the alignment of sentence embedding spaces.
112	59	Cross-lingual embeddings also provide an efficient mechanism to bootstrap neural machine translation (NMT) systems for low-resource language pairs, which is critical in the case of unsupervised machine translation (Lample et al., 2018; Artetxe et al., 2018).
113	37	In that case, the use cross-lingual embeddings directly helps the alignment of sentence-level encoders.
114	36	Cross-lingual embeddings can be generated efficiently using a very small amount of supervision.
117	26	Xing et al. (2015) show that enforcing the orthogonality constraint on the linear mapping leads to better results on the word translation task.
118	19	In this paper, we use common-crawl word embeddings (Grave et al., 2018) aligned with the MUSE library of Conneau et al. (2018).
128	12	We propose a simple alignment loss function to align the embedding spaces of two different languages.
129	34	Specifically, we train an English encoder on NLI, and train a target encoder by minimizing the loss: Lalign(x, y) = sim(x, y)− λ(sim(xc, y) + sim(x, yc)) where (x, y) corresponds to the source and target sentence embeddings, (xc, yc) is a contrastive term (i.e. negative sampling), λ controls the weight of the negative examples in the loss.
133	9	As opposed to Lalign, Lrank does not encourage the embeddings of sentence pairs to be close enough so that the shared classifier can understand that these sentences have the same meaning.
135	32	For X-CBOW, the encoder is pretrained (transfer-learning), while the English XBiLSTMs are trained on NLI (in-domain).
138	74	These encoders are trained to "copy" the English encoder using the Lalign loss and the parallel data described in section 5.2.
