1	43	A common representation of knowledge graph beliefs is in the form of a discrete relational triple such as LocatedIn(NewOrleans,Louisiana).
4	21	The main idea is to represent the entities and relations in a vector space, and one can use machine learning technique to learn the continuous representation of the knowledge graph in the latent space.
5	200	However, even steady progress has been made in developing novel algorithms for knowledge graph embedding, there is still a common challenge in this line of research.
6	23	For space efficiency, common knowledge graphs such as Freebase (Bollacker et al., 2008), Yago (Suchanek et al., 2007), and NELL (Mitchell et al., 2015) by default only stores beliefs, rather than disbeliefs.
7	106	Therefore, when training the embedding models, there is only the natural presence of the positive examples.
8	105	To use negative examples, a common method is to remove the correct tail entity, and randomly sample from a uniform distribution (Bordes et al., 2013).
9	101	Unfortunately, this approach is not ideal, because the sampled entity could be completely unrelated to the head and the target relation, and thus the quality of randomly generated negative examples is often poor (e.g, LocatedIn(NewOrleans,BarackObama)).
13	114	More specifically, we consider probabilitybased, log-loss embedding models as the generator to supply better quality negative examples, and use distance-based, margin-loss embedding models as the discriminator to generate the final knowledge graph embeddings.
63	19	The loss function is the negative log-likelihood of this probabilistic model: Ll = ∑ (h,r,t)∈T − log exp f(h, r, t)∑ exp f(h′, r, t′) (h′, r, t′) ∈ {(h, r, t)} ∪Neg(h, r, t) (2) where Neg(h, r, t) ⊂ {(h′, r, t)|h′ ∈ E} ∪ {(h, r, t′)|t′ ∈ E} is a set of sampled corrupted triples.
77	24	For instance, (Trouillon et al., 2016) found that a 100:1 negative-to-positive ratio results in the best performance for COMPLEX.
83	141	We use softmax probabilistic models as the generator because they can adequately model the “sampling from a probability distribu- Algorithm 1: The KBGAN algorithm Data: training set of positive fact triples T = {(h, r, t)} Input: Pre-trained generator G with parameters θG and score function fG(h, r, t), and pre-trained discriminator D with parameters θD and score function fD(h, r, t) Output: Adversarially trained discriminator 1 b←− 0; // baseline for policy gradient 2 repeat 3 Sample a mini-batch of data Tbatch from T ; 4 GG ←− 0, GD ←− 0; // gradients of parameters of G and D 5 rsum ←− 0; // for calculating the baseline 6 for (h, r, t) ∈ Tbatch do 7 Uniformly randomly sample Ns negative triples Neg(h, r, t) = {(h′i, r, t′i)}i=1...Ns ; 8 Obtain their probability of being generated: pi = exp fG(h ′ i,r,t ′ i)∑Ns j=1 exp fG(h ′ j ,r,t ′ j) ; 9 Sample one negative triple (h′s, r, t′s) from Neg(h, r, t) according to {pi}i=1...Ns .
92	23	Let fD(h, r, t) be the score function of the discriminator.
105	21	To reduce the variance of REINFORCE algorithm, it is common to subtract a baseline from the reward, which is an arbitrary number that only depends on the state, with- out affecting the expectation of gradients.2 In our case, we replace −fD(h′, r, t′) with −fD(h′, r, t′) − b(h, r, t) in the equation above to introduce the baseline.
107	74	In practice, b is approximated by the mean of rewards of recently generated negative triples.
120	32	We use three common knowledge base completion datasets for our experiment: FB15k-237, WN18 and WN18RR.
121	32	FB15k-237 is a subset of FB15k introduced by (Toutanova and Chen, 2015), which removed redundant relations in FB15k and greatly reduced the number of relations.
125	77	Following previous works like (Yang et al., 2015) and (Trouillon et al., 2016), for each run, we report two common metrics, mean reciprocal ranking (MRR) and hits at 10 (H@10).
127	55	3 In the pre-training stage, we train every model to convergence for 1000 epochs, and divide every epoch into 100 mini-batches.
138	20	All settings of adversarial training bring a pronounced improvement to the model, which indicates that our method is consistently effective in various cases.
146	53	As all these graphs show, our performances are always in increasing trends, converging to its max- imum as training proceeds, which indicates that KBGAN is a robust GAN that can converge to good results in various settings, although GANs are wellknown for difficulty in convergence.
148	38	Note that in some cases the curve still tends to rise after 5000 epochs.
150	31	To demonstrate that our approach does generate better negative samples, we list some examples of them in Table 5, using the KBGAN (TRANSE + DISTMULT) model and the WN18 dataset.
152	80	Compared to uniform random negatives which are almost always totally unrelated, the generator generates more semantically related negative samples, which is different from type relatedness we used as example in Section 3.2, but also helps training.
153	19	In the first example, two of the five terms are physically related to the process of distilling liquids.
156	128	Because we deliberately limited the strength of generated negatives by using a small Ns as described in Section 3.3, the semantic relation is pretty weak, and there are still many unrelated entities.
157	134	However, empirical results (when selecting the optimal Ns) shows that such situation is more beneficial for training the discriminator than generating even stronger negatives.
158	31	We propose a novel adversarial learning method for improving a wide range of knowledge graph embedding models—We designed a generatordiscriminator framework with dual KGE components.
159	212	Unlike random uniform sampling, the generator model generates higher quality negative examples, which allow the discriminator model to learn better.
160	22	To enable backpropagation of error, we introduced a one-step REINFORCE method to seamlessly integrate the two modules.
161	61	Experimentally, we tested the proposed ideas with four commonly used KGE models on three datasets, and the results showed that the adversarial learning framework brought consistent improvements to various KGE models under different settings.
