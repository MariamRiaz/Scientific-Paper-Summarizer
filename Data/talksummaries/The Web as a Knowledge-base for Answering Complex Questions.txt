2	19	Recently, interest in question answering (QA) has surged in the context of reading comprehension (RC), where an answer is sought for a question given one or more documents (Hermann et al., 2015; Joshi et al., 2017; Rajpurkar et al., 2016).
5	22	Moreover, RC assumes documents with the information relevant for the answer are available – but when questions are complex, even retrieving the documents can be difficult.
6	56	Conversely, work on QA through semantic parsing has focused primarily on compositionality: questions are translated to compositional programs that encode a sequence of actions for finding the answer in a knowledge-base (KB) (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Artzi and Zettlemoyer, 2013; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Liang et al., 2011).
7	54	However, this reliance on a manually-curated KB has limited the coverage and applicability of semantic parsers.
9	38	Our thesis is that answering simple questions can be achieved 641 by combining a search engine with a RC model.
10	95	Thus, answering complex questions can be addressed by decomposing the question into a sequence of simple questions, and computing the answer from the corresponding answers.
12	16	Our model decomposes the question in the figure into a sequence of simple questions, each is submitted to a search engine, and then an answer is extracted from the search result.
42	13	First, the question is submitted to a search engine that retrieves a list of web snippets.
49	38	COMP(·, ·): This function takes a string containing one unique variable VAR, and a set of answers.
52	16	Formally, COMP(q,A) = ∪a∈ASIMPQA(q/a), where q/a denotes the string produced when replacing VAR in q with a.
79	15	Figure 3: Overview of data collection process.
81	47	To generate complex questions we use the dataset WEBQUESTIONSSP (Yih et al., 2016), which contains 4,737 questions paired with SPARQL queries for Freebase (Bollacker et al., 2008).
93	16	Therefore, we automatically generate a question they can paraphrase.
98	26	Question Rephrasing We used AMT workers to paraphrase MG questions into natural language (NL).
110	28	COMPLEXWEBQUESTIONS contains all these semantic phenomena, but we add four compositionality types by generating composition questions (45% of the times), conjunctions (45%), superlatives (5%) and comparatives (5%).
111	31	Paraphrasing To generate rich paraphrases, we gave a bonus to workers that substantially modified MG questions.
129	18	However, this requires training from denotations using methods such as maximum marginal likelihood or reinforcement learning (Guu et al., 2017) that are difficult to optimize.
132	24	We consider a subset of all possible computation trees that allows us to automatically generate noisy full supervision.
150	28	We generate noisy programs from SPARQL queries in the following manner: First, we automatically identify composition and conjunction questions.
151	26	Because we generated the MG question, we can exactly identify the split points (i, j in composition questions and i in conjunction questions) in the MG question.
152	65	Then, we use a rulebased algorithm that takes the alignment matrix A (Section 4), and approximates the split points in the NL question and the index j to copy in conjunction questions.
172	31	Simple QA model As our SIMPQA function, we download the web-based QA model of Talmor et al. (2017).
173	44	This model sends the question to Google’s search engine and extracts a distribution over answers from the top-100 web snippets using manually-engineered features.
177	21	We evaluate the following models and oracles: 1.
179	64	SPLITQA: Our main model that answers complex questions by decomposition.
183	17	GOOGLEBOX: We sample 100 random development set questions and check whether Google returns a box that contains one of the correct answers.
184	107	HUMAN: We sample 100 random development set questions and manually answer the questions with Google’s search engine, including all available information.
187	21	SIMPQA, which does not decompose questions obtained 20.8 p@1, while by performing question decomposition we substantially improve performance to 27.5 p@1.
190	30	More importantly SPLITRCQA outperforms RCQA by 3.4 points, illustrating that this RC model also benefits from question decomposition, despite the fact that it was not created with question decomposition in mind.
195	31	We find that answering complex questions takes roughly 1.3 minutes on average.
