2	11	Another route is to convert natural language into a formal lan- Cubes: initial – select left 6 – select front 8 – black 10x10x10 frame – black 10x10x10 frame – move front 10 – red cube size 6 – move bot 2 – blue cube size 6 – green cube size 4 – (some steps are omitted) Monsters, Inc: initial – move forward – add green monster – go down 8 – go right and front – add brown floor – add girl – go back and down – add door – add black column 30 – go up 9 – finish door – (some steps for moving are omitted) Deer: initial – bird’s eye view – deer head; up; left 2; back 2; { left antler }; right 2; {right antler} – down 4; front 2; left 3; deer body; down 6; {deer leg front}; back 7; {deer leg back}; left 4; {deer leg back}; front 7; {deer leg front} – (some steps omitted) Figure 1: Some examples of users building structures using a naturalized language in Voxelurn: http://www.voxelurn.com guage, which has been the subject of work in semantic parsing (Zettlemoyer and Collins, 2005; Artzi and Zettlemoyer, 2011, 2013; Pasupat and Liang, 2015).
3	13	However, the capability of semantic parsers is still quite primitive compared to the power one wields with a programming language.
5	13	In this paper, we propose bridging this gap with an interactive language learning process which we call naturalization.
10	75	This process accommodates both users’ preferences and the computer action space, where the final language is both interpretable by the computer and easier to produce by human users.
11	55	Compared to interactive language learning with weak denotational supervision (Wang et al., 2016), definitions are critical for learning complex actions (Figure 1).
12	14	Definitions equate a novel utterance to a sequence of utterances that the system already understands.
18	75	We implemented a system called Voxelurn, which is a command language interface for a voxel world initially equipped with a programming language supporting conditionals, loops, and variable scoping etc.
37	67	Suppose one is operating on one of the palm trees in Figure 2.
56	39	In Figure 2, ‘add yellow palm tree’ is defined as a sequence of steps for building the palm tree.
57	22	Once the system understands an utterance, it can be used in the body of other definitions.
67	66	Rather than propagating this ambiguity to the head, we force the user to commit to one interpretation by selecting a particular candidate.
76	13	The user u does not appear in previous work on semantic parsing, but we use it to personalize the semantic parser trained on the community.
79	22	The resulting derivations are sorted by model score and only the top K are kept.
83	25	Derivations are scored using a weighted combination of features.
87	23	Type features track whether a rule is part of the core language or induced, whether it has been used again after it was defined, if it was used by someone other than its author, and if the user and the author are the same (5 + #rules features).
90	17	Friends features are cross products of author ID and user ID, which captures whether rules from a particular author are systematically preferred or not by the current user, due to stylistic similarities or differences (#users+#users×#users features).
149	26	Our ultimate goal is to create a community of users who can build interesting structures in Voxelurn while naturalizing the core language.
152	16	Next, we allowed the workers who qualified to enter the second freebuilding task, in which they were asked to build any structure they wanted in 30 minutes.
155	21	70 workers passed the qualifier task, and 42 workers participated in the final freebuilding experiment.
158	29	There were 2,495 definitions combining over 15,000 body utterances with 6.5 body utterances per head on average (96 max).
159	22	From these definitions, 2,817 grammar rules were induced, compared to less than 100 core rules.
164	11	At the conclusion of the experiment, 72.9% of all accepted utterances are induced—this becomes 85.9% if we only consider the final 10,000 accepted utterances.
166	28	For very common operations, like moving the selection, people found ‘select left’ too verbose and shorterned this to l, left, >, sel l. One user preferred ‘go down and right’ instead of ‘select bot; select right’ in core and defined it as ‘go down; go right’.
177	141	To incentivize this behavior, we created a leaderboard which ranked structures based on recency and upvotes (like Hacker News).
179	10	The prize categories for each day were bridge, house, animal; tower, monster, flower; ship, dancer, and castle.
180	18	To incentivize more definitions, we also track citations.
181	47	When a rule is used in an accepted utterance by another user, the rule (and its author) receives a citation.
182	36	We pay bonuses to top users according to their h-index.
183	25	Most cited definitions are also displayed on the leaderboard.
184	12	Our qualitative results should be robust to the incentives scheme, because the users do not overfit to the incentives—e.g., around 20% of the structures are not in the prize categories and users define complex concepts that are rarely cited.
