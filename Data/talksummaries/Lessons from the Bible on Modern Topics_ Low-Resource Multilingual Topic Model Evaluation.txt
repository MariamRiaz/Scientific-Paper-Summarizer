6	6	We explore the behaviors of CNPMI at both the model and topic levels with six language pairs and varying model specifications.
7	6	This metric correlates well with human judgments and crosslingual classification results (Sections 5 and 6).
13	13	After defining the multilingual topic model, we describe topic model evaluation extending standard monolingual approaches to multilingual settings.
18	9	In these models, each document d indexes a tuple of parallel/comparable language-specific documents, 1090 d(`), and the language-specific “views” of a document share the document-topic distribution θd.
20	5	, d(L) ) do 4 Draw a distribution over topics θd ∼ Dirichlet(α); 5 for each language ` = 1, .
24	14	The most successful (Lau et al., 2014) is normalized pointwise mutual information (Bouma, 2009, NPMI).
29	13	While automatic evaluation has been well-studied for monolingual topic models, there are no robust evaluations for multilingual topic models.
32	5	A simple adaptation of NPMI is to calculate the monolingual NPMI score for each language independently and take the average.
34	11	However, this metric does not consider whether the topic is coherent across languages—that is, whether a language-specific word distribution φ`1k is related to the corresponding distribution in another language, φ`2k.
36	7	This metric can measure whether a topic is well-aligned across languages literally, but cannot capture non-literal more holistic similarities across languages.
43	22	When one or both words in a bilingual pair do not appear in the reference corpus, the cooccurrence score is zero.
44	57	Similar to monolingual settings, CNPMI for a bilingual topic k is the average of the NPMI scores of all C2 bilingual word pairs, CNPMI(`1, `2, k) = ∑C i,j NPMI (wi,`1 ,wj,`2) C2 .
47	7	Wikipedia, which has good coverage of topics and vocabularies is a common choice (Lau and Baldwin, 2016).
50	24	Since CNPMI requires comparable documents, the usable reference corpus is defined by paired documents.
51	38	Another option for a parallel reference corpus is the Bible (Resnik et al., 1999), which is available in most world languages;2 however, it is small and archaic.
52	17	It is good at evaluating topics such as family and religion, but not “modern” topics like biology and Internet.
53	6	Without reference co-occurrence statistics relevant to these topics, CNPMI will fail to judge topic coherence—it must give the ambiguous answer of zero.
54	22	Such a score could mean a totally incoherent topic where each word pair never appears together (Topics 6 in Figure 1), or an unjudgeable topic (Topic 5).
56	14	We propose a model that can correct the drawbacks of a Bible-derived CNPMI.
58	18	We take Wikipedia’s CNPMI from high-resource languages as accurate estimations.
62	12	The key to the estimator is to find features that capture whether we should trust the Bible.
64	23	This section describes the features, which we split into four groups.
65	18	Base Features (BASE) Our base features include information we can collect from the Bible and the topic model: cardinality C, CNPMI and INPMI, MTA, and topic word coverage (TWC), which counts the percentage of topic words in a topic that appear in a reference corpus.
66	13	Crosslingual Gap (GAP) A low CNPMI score could indicate a topic pair where each language has a monolingually coherent topic but that are not about the same theme (Topic 6 in Figure 1).
67	12	Thus, we add two features to capture this information using the Bible: mismatch coefficients (MC) and internal comparison coefficients (ICC): MC(`1; `2, k) = CNPMI(`1, `2, k) INPMI(`1, k) + α , (5) ICC(`1, `2, k) = INPMI(`1, k) + α INPMI(`2, k) + α , (6) where α is a smoothing factor (α = 0.001 in our experiments).
68	20	MC recognizes the gap between crosslingual and monolingual coherence, so a higher MC score indicates a gap between coherence within and across languages.
69	37	Similarly, ICC compares monolingual coherence to tell if both languages are coherent: the closer to 1 the ICC is, the more comparable internal coherence both languages have.
70	25	Word Era (ERA) Because the Bible’s vocabulary is unable to evaluate modern topics, we must tell the model what the modern words are.
71	27	The word era features are the earliest usage year 3 for each word in a topic.
72	8	We use both the mean and standard deviation as features.
