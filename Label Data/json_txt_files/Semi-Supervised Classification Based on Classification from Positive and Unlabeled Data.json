{"sections": [{"heading": "1. Introduction", "text": "Collecting a large amount of labeled data is a critical bottleneck in real-world machine learning applications due to the laborious manual annotation. In contrast, unlabeled data can often be collected automatically and abundantly, e.g., by a web crawler. This has led to the development of various semi-supervised classification algorithms over the past decades.\nTo leverage unlabeled data in training, most of the existing semi-supervised classification methods rely on particular assumptions on the data distribution (Chapelle et al., 2006). For example, the manifold assumption supposes that samples are distributed on a low-dimensional manifold in the data space (Belkin et al., 2006). In the existing framework, such a distributional assumption is encoded as a reg-\n1The University of Tokyo, Japan 2RIKEN, Japan. Correspondence to: Tomoya Sakai <sakai@ms.k.u-tokyo.ac.jp>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nularizer for training a classifier and biases the classifier toward a better one under the assumption. However, if such a distributional assumption contradicts the data distribution, the bias behaves adversely, and the performance of the obtained classifier becomes worse than the one obtained with supervised classification (Cozman et al., 2003; Sokolovska et al., 2008; Li & Zhou, 2015; Krijthe & Loog, 2017).\nRecently, classification from positive and unlabeled data (PU classification) has been gathering growing attention (Elkan & Noto, 2008; du Plessis et al., 2014; 2015; Jain et al., 2016), which trains a classifier only from positive and unlabeled data without negative data. In PU classification, the unbiased risk estimators proposed in du Plessis et al. (2014; 2015) utilize unlabeled data for risk evaluation, implying that label information is directly extracted from unlabeled data without restrictive distributional assumptions, unlike existing semi-supervised classification methods that utilize unlabeled data for regularization. Furthermore, theoretical analysis (Niu et al., 2016) showed that PU classification (or its counterpart, NU classification, classification from negative and unlabeled data) is likely to outperform classification from positive and negative data (PN classification, i.e., ordinary supervised classification) depending on the number of positive, negative, and unlabeled samples. It is thus naturally expected that combining PN, PU, and NU classification can be a promising approach to semisupervised classification without restrictive distributional assumptions.\nIn this paper, we propose a novel semi-supervised classification approach by considering convex combinations of the risk functions of PN, PU, and NU classification. Without any distributional assumption, we theoretically show that the confidence term of the generalization error bounds decreases at the optimal parametric rate with respect to the number of positive, negative, and unlabeled samples, and the variance of the proposed risk estimator is almost always smaller than the plain PN risk function given an infinite number of unlabeled samples. Through experiments, we analyze the behavior of the proposed approach and demonstrate the usefulness of the proposed semi-supervised classification methods.\nar X\niv :1\n60 5.\n06 95\n5v 4\n[ cs\n.L G\n] 1\n6 Ju\nn 20\n17"}, {"heading": "2. Background", "text": "In this section, we first introduce the notation commonly used in this paper and review the formulations of PN, PU, and NU classification."}, {"heading": "2.1. Notation", "text": "Let random variables x \u2208 Rd and y \u2208 {+1,\u22121} be equipped with probability density p(x, y), where d is a positive integer. Let us consider a binary classification problem from x to y, given three sets of samples called the positive (P), negative (N), and unlabeled (U) data:\nXP := {xPi } nP i=1 i.i.d.\u223c pP(x) := p(x | y = +1),\nXN := {xNi } nN i=1 i.i.d.\u223c pN(x) := p(x | y = \u22121),\nXU := {xUi } nU i=1 i.i.d.\u223c p(x) := \u03b8PpP(x) + \u03b8NpN(x),\nwhere\n\u03b8P := p(y = +1), \u03b8N := p(y = \u22121)\nare the class-prior probabilities for the positive and negative classes such that \u03b8P + \u03b8N = 1.\nLet g : Rd \u2192 R be an arbitrary real-valued decision function for binary classification, and classification is performed based on its sign. Let ` : R \u2192 R be a loss function such that `(m) generally takes a small value for large margin m = yg(x). Let RP(g), RN(g), RU,P(g), and RU,N(g) be the risks of classifier g under loss `:\nRP(g) := EP[`(g(x))], RN(g) := EN[`(\u2212g(x))], RU,P(g) := EU[`(g(x))], RU,N(g) := EU[`(\u2212g(x))],\nwhere EP, EN, and EU denote the expectations over pP(x), pN(x), and p(x), respectively. Since we do not have any samples from p(x, y), the true risk R(g) = Ep(x,y)[`(yg(x))], which we want to minimize, should be recovered without using p(x, y) as shown below."}, {"heading": "2.2. PN Classification", "text": "In standard supervised classification (PN classification), we have both positive and negative data, i.e., fully labeled data. The goal of PN classification is to train a classifier using labeled data.\nThe risk in PN classification (the PN risk) is defined as\nRPN(g) := \u03b8P EP[`(g(x))] + \u03b8N EN[`(\u2212g(x))] = \u03b8PRP(g) + \u03b8NRN(g), (1)\nwhich is equal to R(g), but p(x, y) is not included. If we use the hinge loss function `H(m) := max(0, 1 \u2212m), the PN risk coincides with the risk of the support vector machine (Vapnik, 1995)."}, {"heading": "2.3. PU Classification", "text": "In PU classification, we do not have labeled data for the negative class, but we can use unlabeled data drawn from marginal density p(x). The goal of PU classification is to train a classifier using only positive and unlabeled data. The basic approach to PU classification is to discriminate P and U data (Elkan & Noto, 2008). However, naively classifying P and U data causes a bias.\nTo address this problem, du Plessis et al. (2014; 2015) proposed a risk equivalent to the PN risk but where pN(x) is not included. The key idea is to utilize unlabeled data to evaluate the risk for negative samples in the PN risk. Replacing the second term in Eq. (1) with1\n\u03b8N EN[`(\u2212g(x))] = EU[`(\u2212g(x))]\u2212 \u03b8P EP[`(\u2212g(x))],\nwe obtain the risk in PU classification (the PU risk) as\nRPU(g) := \u03b8P EP[\u02dc\u0300(g(x))] + EU[`(\u2212g(x))] = \u03b8PR C P(g) +RU,N(g), (2)\nwhere RCP(g) := EP[\u02dc\u0300(g(x))] and \u02dc\u0300(m) = `(m)\u2212 `(\u2212m) is a composite loss function.\nNon-Convex Approach: If the loss function satisfies\n`(m) + `(\u2212m) = 1, (3)\nthe composite loss function becomes \u02dc\u0300(m) = 2`(m) \u2212 1. We thus obtain the non-convex PU risk as\nRN-PU(g) := 2\u03b8PRP(g) +RU,N(g)\u2212 \u03b8P. (4)\nThis formulation can be seen as cost-sensitive classification of P and U data with weight 2\u03b8P (du Plessis et al., 2014).\nThe ramp loss used in the robust support vector machine (Collobert et al., 2006),\n`R(m) := 1\n2 max(0,min(2, 1\u2212m)), (5)\nsatisfies the condition (3). However, the use of the ramp loss (and any other losses that satisfy the condition (3)) yields a non-convex optimization problem, which may be solved locally by the concave-convex procedure (CCCP) (Yuille & Rangarajan, 2002; Collobert et al., 2006; du Plessis et al., 2014).\nConvex Approach: If a convex surrogate loss function satisfies\n`(m)\u2212 `(\u2212m) = \u2212m, (6) 1 The equation comes from the definition of the marginal den-\nsity p(x) = \u03b8PpP(x) + \u03b8NpN(x).\nthe composite loss function becomes a linear function\u02dc\u0300(m) = \u2212m (see Table 1 in du Plessis et al., 2015). We thus obtain the convex PU risk as\nRC-PU(g) := \u03b8PR L P(g) +RU,N(g),\nwhere RLP(g) := EP[\u2212g(x)] is the risk with the linear loss `Lin(m) := \u2212m. This formulation yields the convex optimization problem that can be solved efficiently."}, {"heading": "2.4. NU Classification", "text": "As a mirror of PU classification, we can consider NU classification. The risk in NU classification (the NU risk) is given by\nRNU(g) := \u03b8N EN[\u02dc\u0300(\u2212g(x))] + EU[`(g(x))] = \u03b8NR C N(g) +RU,P(g),\nwhere RCN(g) := EN[\u02dc\u0300(\u2212g(x))] is the risk function with the composite loss. Similarly to PU classification, the nonconvex and convex NU risks are expressed as\nRN-NU(g) := 2\u03b8NRN(g) +RU,P(g)\u2212 \u03b8N, (7) RC-NU(g) := \u03b8NR L N(g) +RU,P(g), (8)\nwhere RLN(g) := EN[g(x)] is the risk with the linear loss."}, {"heading": "3. Semi-Supervised Classification Based on PN, PU, and NU Classification", "text": "In this section, we propose semi-supervised classification methods based on PN, PU, and NU classification."}, {"heading": "3.1. PUNU Classification", "text": "A naive idea to build a semi-supervised classifier is to combine the PU and NU risks. For \u03b3 \u2208 [0, 1], let us consider a linear combination of the PU and NU risks:\nR\u03b3PUNU(g) := (1\u2212 \u03b3)RPU(g) + \u03b3RNU(g).\nWe refer to this combined method as PUNU classification.\nIf we use a loss function satisfying the condition (3), the non-convex PUNU risk R\u03b3N-PUNU(g) can be expressed as\nR\u03b3N-PUNU(g) = 2(1\u2212 \u03b3)\u03b8PRP(g) + 2\u03b3\u03b8NRN(g) + EU[(1\u2212 \u03b3)`(\u2212g(x)) + \u03b3`(g(x))] \u2212 (1\u2212 \u03b3)\u03b8P \u2212 \u03b3\u03b8N.\nHere, R1/2N-PUNU(g) agrees with RPN(g) due to the condition (3). Thus, when \u03b3 = 1/2, PUNU classification is reduced to ordinary PN classification.\nOn the other hand, \u03b3 = 1/2 is still effective when the condition (6) is satisfied. Its riskR\u03b3C-PUNU(g) can be expressed as\nR\u03b3C-PUNU(g) = (1\u2212 \u03b3)\u03b8PR L P(g) + \u03b3\u03b8NR L N(g)\n+ EU[(1\u2212 \u03b3)`(g(x)) + \u03b3`(\u2212g(x))].\nHere, (1 \u2212 \u03b3)`(g(x)) + \u03b3`(\u2212g(x)) can be regarded as a loss function for unlabeled samples with weight \u03b3.\nWhen \u03b3 = 1/2, unlabeled samples incur the same loss for the positive and negative classes. On the other hand, when 0 < \u03b3 < 1/2, a smaller loss is incurred for the negative class than the positive class. Thus, unlabeled samples tend to be classified into the negative class. The opposite is true when 1/2 < \u03b3 < 1."}, {"heading": "3.2. PNU Classification", "text": "Another possibility of using PU and NU classification in semi-supervised classification is to combine the PN and PU/NU risks. For \u03b3 \u2208 [0, 1], let us consider linear combinations of the PN and PU/NU risks:\nR\u03b3PNPU(g) := (1\u2212 \u03b3)RPN(g) + \u03b3RPU(g), R\u03b3PNNU(g) := (1\u2212 \u03b3)RPN(g) + \u03b3RNU(g).\nIn practice, we combine PNPU and PNNU classification and adaptively choose one of them with a new trade-off parameter \u03b7 \u2208 [\u22121, 1] as\nR\u03b7PNU(g) := { R\u03b7PNPU(g) (\u03b7 \u2265 0), R\u2212\u03b7PNNU(g) (\u03b7 < 0).\nWe refer to the combined method as PNU classification. Clearly, PNU classification with \u03b7 = \u22121, 0,+1 corresponds to NU, PN, and PU classification. As \u03b7 gets large/small, the effect of the positive/negative classes is more emphasized.\nIn the theoretical analyses in Section 4, we denote the combinations of the PN risk with the non-convex PU/NU risks by R\u03b3N-PNPU and R \u03b3 N-PNNU, and that with the convex PU/NU risks by R\u03b3C-PNPU and R \u03b3 C-PNNU."}, {"heading": "3.3. Practical Implementation", "text": "We have so far only considered the true risks R (with respect to the expectations over true data distributions). When a classifier is trained from samples in practice, we use the empirical risks R\u0302 where the expectations are replaced with corresponding sample averages.\nMore specifically, in the theoretical analysis in Section 4 and experiments in Section 5, we use a linear-in-parameter model given by\ng(x) = b\u2211 j=1 wj\u03c6j(x) = w >\u03c6(x),\nwhere > denotes the transpose, b is the number of basis functions, w = (w1, . . . , wb)> is a parameter vector, and \u03c6(x) = (\u03c61(x), . . . , \u03c6b(x))\n> is a basis function vector. The parameter vectorw is learned in order to minimize the `2-regularized empirical risk:\nmin w\nR\u0302(g) + \u03bbw>w,\nwhere \u03bb \u2265 0 is the regularization parameter."}, {"heading": "4. Theoretical Analyses", "text": "In this section, we theoretically analyze the behavior of the empirical versions of the proposed semi-supervised classification methods. We first derive generalization error bounds and then discuss variance reduction. Finally, we discuss which approach, PUNU or PNU classification, is more promising. All proofs can be found in Appendix A."}, {"heading": "4.1. Generalization Error Bounds", "text": "Let G be a function class of bounded hyperplanes:\nG = {g(x) = \u3008w,\u03c6(x)\u3009 | \u2016w\u2016 \u2264 Cw, \u2016\u03c6(x)\u2016 \u2264 C\u03c6},\nwhere Cw and C\u03c6 are certain positive constants. Since `2- regularization is always included, we can naturally assume that the empirical risk minimizer g belongs to a certain G. Denote by `0-1(m) = (1 \u2212 sign(m))/2 the zero-one loss and I(g) = Ep(x,y)[`0-1(yg(x))] the risk of g for binary classification, i.e., the generalization error of g. In the following, we study upper bounds of I(g) holding uniformly for all g \u2208 G. We respectively focus on the (scaled) ramp and squared losses for the non-convex and convex methods due to limited space. Similar results can be obtained with a little more effort if other eligible losses are used. For convenience, we define a function as\n\u03c7(cP, cN, cU) = cP\u03b8P/ \u221a nP + cN\u03b8N/ \u221a nN + cU/ \u221a nU.\nNon-Convex Methods: A key observation is that `0-1(m) \u2264 2`R(m), and consequently I(g) \u2264 2R(g). Note that by definition we have\nR\u03b3N-PUNU(g) = R \u03b3 N-PNPU(g) = R \u03b3 N-PNNU(g) = R(g).\nThe theorem below can be proven using the Rademacher analysis (see, for example, Mohri et al., 2012; Ledoux & Talagrand, 1991).\nTheorem 1 Let `R(m) be the loss for defining the empirical risks. For any \u03b4 > 0, the following inequalities hold separately with probability at least 1\u2212 \u03b4 for all g \u2208 G:\nI(g) \u2264 2R\u0302\u03b3N-PUNU(g) + Cw,\u03c6,\u03b4 \u00b7 \u03c7(2\u2212 2\u03b3, 2\u03b3, |2\u03b3 \u2212 1|), I(g) \u2264 2R\u0302\u03b3N-PNPU(g) + Cw,\u03c6,\u03b4 \u00b7 \u03c7(1 + \u03b3, 1\u2212 \u03b3, \u03b3), I(g) \u2264 2R\u0302\u03b3N-PNNU(g) + Cw,\u03c6,\u03b4 \u00b7 \u03c7(1\u2212 \u03b3, 1 + \u03b3, \u03b3),\nwhere Cw,\u03c6,\u03b4 = 2CwC\u03c6 + \u221a 2 ln(3/\u03b4).\nTheorem 1 guarantees that when `R(m) is used, I(g) can be bounded from above by two times the empirical risks, i.e., 2R\u0302\u03b3N-PUNU(g), 2R\u0302 \u03b3 N-PNPU(g), and 2R\u0302 \u03b3 N-PNNU(g), plus the corresponding confidence terms of order\nOp(1/ \u221a nP + 1/ \u221a nN + 1/ \u221a nU).\nSince nP, nN, and nU can increase independently, this is already the optimal convergence rate without any additional assumption (Vapnik, 1998; Mendelson, 2008).\nConvex Methods: Analogously, we have `0-1(m) \u2264 4`S(m) for the squared loss. However, it is too loose when |m| 0. Fortunately, we do not have to use `S(m) if we work on the generalization error rather than the estimation error. To this end, we define the truncated (scaled) squared loss `TS(m) as\n`TS(m) = { `S(m) 0 < m \u2264 1, `0-1(m)/4 otherwise,\nso that `0-1(m) \u2264 4`TS(m) is much tighter. For `TS(m), RC-PU(g) and RC-NU(g) need to be redefined as follows (see du Plessis et al., 2015):\nRC-PU(g) := \u03b8PR \u2032 P(g) +RU,N(g), RC-NU(g) := \u03b8NR \u2032 N(g) +RU,P(g),\nwhere R\u2032P(g) and R \u2032 N(g) are simply RP(g) and RN(g) w.r.t. the composite loss \u02dc\u0300TS(m) = `TS(m) \u2212 `TS(\u2212m). The condition \u02dc\u0300TS(m) 6= \u2212m means the loss of convexity, but the equivalence is not lost; indeed, we still have\nR\u03b3C-PUNU(g) = R \u03b3 C-PNPU(g) = R \u03b3 C-PNNU(g) = R(g).\nTheorem 2 Let `TS(m) be the loss for defining the empirical risks (where RC-PU(g) and RC-NU(g) are redefined). For any \u03b4 > 0, the following inequalities hold separately with probability at least 1\u2212 \u03b4 for all g \u2208 G:\nI(g) \u2264 4R\u0302\u03b3C-PUNU(g) + C \u2032 w,\u03c6,\u03b4 \u00b7 \u03c7(1\u2212 \u03b3, \u03b3, 1), I(g) \u2264 4R\u0302\u03b3C-PNPU(g) + C \u2032 w,\u03c6,\u03b4 \u00b7 \u03c7(1, 1\u2212 \u03b3, \u03b3), I(g) \u2264 4R\u0302\u03b3C-PNNU(g) + C \u2032 w,\u03c6,\u03b4 \u00b7 \u03c7(1\u2212 \u03b3, 1, \u03b3),\nwhere C \u2032w,\u03c6,\u03b4 = 4CwC\u03c6 + \u221a 2 ln(4/\u03b4).\nTheorem 2 ensures that when `TS(m) is used (for evaluating the empirical risks rather than learning the empirical risk minimizers), I(g) can be bounded from above by four times the empirical risks plus confidence terms in the optimal parametric rate. As `TS(m) \u2264 `S(m), Theorem 2 is valid (but weaker) if all empirical risks are w.r.t. `S(m)."}, {"heading": "4.2. Variance Reduction", "text": "Our empirical risk estimators proposed in Section 3 are all unbiased. The next question is whether their variance can be smaller than that of R\u0302PN(g), i.e., whether XU can help reduce the variance in estimating R(g). To answer this question, pick any g of interest. For simplicity, we assume that nU \u2192 \u221e, to illustrate the maximum variance reduction that could be achieved. Due to limited space, we only focus on the non-convex methods.\nSimilarly to RP(g) and RN(g), let \u03c32P(g) and \u03c3 2 N(g) be the corresponding variance:\n\u03c32P(g) := VarP[`(g(x))], \u03c3 2 N(g) := VarN[`(\u2212g(x))],\nwhere VarP and VarN denote the variance over pP(x) and pN(x). Moreover, denote by \u03c8P = \u03b82P\u03c3 2 P(g)/nP and \u03c8N = \u03b82N\u03c3 2 N(g)/nN for short, and let Var be the variance over pP(xP1 ) \u00b7 \u00b7 \u00b7 pP(xPnP) \u00b7 pN(x N 1 ) \u00b7 \u00b7 \u00b7 pN(xNnN) \u00b7 p(xU1 ) \u00b7 \u00b7 \u00b7 p(xUnU).\nTheorem 3 Assume nU \u2192\u221e. For any fixed g, let\n\u03b3N-PUNU = argmin \u03b3\nVar[R\u0302\u03b3N-PUNU(g)] = \u03c8P\n\u03c8P + \u03c8N . (9)\nThen, we have \u03b3N-PUNU \u2208 [0, 1]. Further, Var[R\u0302\u03b3N-PUNU(g)] < Var[R\u0302PN(g)] for all \u03b3 \u2208 (2\u03b3N-PUNU \u2212 1/2, 1/2) if \u03c8P < \u03c8N, or for all \u03b3 \u2208 (1/2, 2\u03b3N-PUNU \u2212 1/2) if \u03c8P > \u03c8N.2\nTheorem 3 guarantees that the variance is always reduced by R\u0302\u03b3N-PUNU(g) if \u03b3 is close to \u03b3N-PUNU, which is optimal for variance reduction. The interval of such good \u03b3 values has the length min{|\u03c8P \u2212 \u03c8N|/(\u03c8P + \u03c8N), 1/2}. In particular, if 3\u03c8P \u2264 \u03c8N or \u03c8P \u2265 3\u03c8N, the length is 1/2.\nTheorem 4 Assume nU \u2192\u221e. For any fixed g, let\n\u03b3N-PNPU= argmin \u03b3 Var[R\u0302\u03b3N-PNPU(g)]= \u03c8N \u2212 \u03c8P \u03c8P + \u03c8N , (10)\n\u03b3N-PNNU= argmin \u03b3 Var[R\u0302\u03b3N-PNNU(g)]= \u03c8P \u2212 \u03c8N \u03c8P + \u03c8N . (11)\nThen, we have \u03b3N-PNPU \u2208 [0, 1] if \u03c8P \u2264 \u03c8N or \u03b3N-PNNU \u2208 [0, 1] if \u03c8P \u2265 \u03c8N. Additionally, Var[R\u0302\u03b3N-PNPU(g)] < Var[R\u0302PN(g)] for all \u03b3 \u2208 (0, 2\u03b3N-PNPU) if \u03c8P < \u03c8N, or Var[R\u0302 \u03b3 N-PNNU(g)] < Var[R\u0302PN(g)] for all \u03b3 \u2208 (0, 2\u03b3N-PNNU) if \u03c8P > \u03c8N.\nTheorem 4 implies that the variance of R\u0302PN(g) is reduced by either R\u0302\u03b3N-PNPU(g) if \u03c8P \u2264 \u03c8N or R\u0302 \u03b3 N-PNNU(g)\n2Being fixed means g is determined before seeing the data for evaluating the empirical risk. For example, if g is trained by some learning method, and the empirical risk is subsequently evaluated on the validation/test data, g is regarded as fixed in the evaluation.\nif \u03c8P \u2265 \u03c8N, where \u03b3 should be close to \u03b3N-PNPU or \u03b3N-PNNU. The range of such good \u03b3 values is of length min{2|\u03c8P \u2212 \u03c8N|/(\u03c8P + \u03c8N), 1}. In particular, if 3\u03c8P \u2264 \u03c8N, R\u0302 \u03b3 N-PNPU(g) given any \u03b3 \u2208 (0, 1) can reduce the variance, and if \u03c8P \u2265 3\u03c8N, R\u0302\u03b3N-PNNU(g) given any \u03b3 \u2208 (0, 1) can reduce the variance.\nAs a corollary of Theorems 3 and 4, the minimum variance achievable by R\u0302\u03b3N-PUNU(g), R\u0302 \u03b3 N-PNPU(g), and R\u0302\u03b3N-PNNU(g) at their optimal \u03b3N-PUNU, \u03b3N-PNPU, and \u03b3N-PNNU is exactly the same, namely, 4\u03c8P\u03c8N/(\u03c8P +\u03c8N). Nevertheless, R\u0302\u03b3N-PNPU(g) and R\u0302 \u03b3 N-PNNU(g) have a much wider range of nice \u03b3 values than R\u0302\u03b3N-PUNU(g).\nIf we further assume that \u03c3P(g) = \u03c3N(g), the condition in Theorems 3 and 4 as to whether \u03c8P \u2264 \u03c8N or \u03c8P \u2265 \u03c8N will be independent of g. Also, it will coincide with the condition in Theorem 7 in Niu et al. (2016) where the minimizers of R\u0302PN(g), R\u0302PU(g) and R\u0302NU(g) are compared.\nA final remark is that learning is uninvolved in Theorems 3 and 4, such that `(m) can be any loss that satisfies `(m) + `(\u2212m) = 1, and g can be any fixed decision function. For instance, we may adopt `0-1(m) and pick some g resulted from some other learning methods. As a consequence, the variance of I\u0302PN(g) over the validation data can be reduced, and then the cross-validation should be more stable, given that nU is sufficiently large. Therefore, even without being minimized, our proposed risk estimators are themselves of practical importance."}, {"heading": "4.3. PUNU vs. PNU Classification", "text": "We discuss here which approach, PUNU or PNU classification, is more promising according to state-of-the-art theoretical comparisons (Niu et al., 2016), which are based on estimation error bounds.\nLet g\u0302PN, g\u0302PU, and g\u0302NU be the minimizers of R\u0302PN(g), R\u0302PU(g), and R\u0302NU(g), respectively. Let \u03b1PU,PN := (\u03b8P/ \u221a nP + 1/ \u221a nU)/(\u03b8N/ \u221a nN) and \u03b1NU,PN := (\u03b8N/ \u221a nN + 1/ \u221a nU)/(\u03b8P/ \u221a nP). The finite-sample comparisons state that if \u03b1PU,PN > 1 (\u03b1NU,PN > 1), PN classification is more promising than PU (NU) classification, i.e., R(g\u0302PN) < R(g\u0302PU) (R(g\u0302PN) < R(g\u0302NU)); otherwise PU (NU) classification is more promising than PN classification (cf. Section 3.2 in Niu et al., 2016).\nSuppose that nU is not sufficiently large against nP and nN. According to the finite-sample comparisons, PN classification is most promising, and either PU or NU classification is the second best, i.e., R(g\u0302PN) < R(g\u0302PU) < R(g\u0302NU) or R(g\u0302PN) < R(g\u0302NU) < R(g\u0302PU). On the other hand, if nU is sufficiently large (nU \u2192 \u221e, which is faster than nP, nN \u2192 \u221e), we have the asymptotic comparisons: \u03b1\u2217PU,PN = limnP,nN,nU\u2192\u221e \u03b1PU,PN, \u03b1 \u2217 NU,PN =\nlimnP,nN,nU\u2192\u221e \u03b1NU,PN, and \u03b1 \u2217 PU,PN \u00b7\u03b1\u2217NU,PN = 1. From the last equation, if \u03b1\u2217PU,PN < 1, then \u03b1 \u2217 NU,PN > 1, implying that PU (PN) classification is more promising than PN (NU) classification, i.e., R(g\u0302PU) < R(g\u0302PN) < R(g\u0302NU). Similarly, when \u03b1\u2217PU,PN > 1 and \u03b1 \u2217 NU,PN < 1, R(g\u0302NU) < R(g\u0302PN) < R(g\u0302PU) (cf. Section 3.3 in Niu et al., 2016).\nIn real-world applications, since we do not know whether the number of unlabeled samples is sufficiently large or not, a practical approach is to combine the best methods in both the finite-sample and asymptotic cases. PNU classification is the combination of the best methods in both cases, but PUNU classification is not. In addition, PUNU classification includes the worst one in its combination in both cases. From this viewpoint, PNU classification would be more promising than PUNU classification, as demonstrated in the experiments shown in the next section."}, {"heading": "5. Experiments", "text": "In this section, we first numerically analyze the proposed approach and then compare the proposed semi-supervised classification methods against existing methods. All experiments were carried out using a PC equipped with two 2.60GHz Intel\u00ae Xeon\u00ae E5-2640 v3 CPUs."}, {"heading": "5.1. Experimental Analyses", "text": "Here, we numerically analyze the behavior of our proposed approach. Due to limited space, we show results on two out of six data sets and move the rest to Appendix C.\nCommon Setup: As a classifier, we use the Gaussian kernel model: g(x) = \u2211n i=1 wi exp(\u2212\u2016x\u2212 xi\u20162/(2\u03c32)), where n = nP + nN, {wi}ni=1 are the parameters, {xi}ni=1 = XP\u222aXN, and \u03c3 > 0 is the Gaussian bandwidth. The bandwidth candidates are {1/8, 1/4, 1/2, 1, 3/2, 2} \u00d7 median(\u2016xi \u2212 xj\u2016ni,j=1). The classifier trained by minimizing the empirical PN risk is denoted by g\u0302PN. The number of labeled samples for training is 20, where the classprior was 0.5. In all experiments, we used the squared loss for training. We note that the class-prior of test data was the same as that of unlabeled data.\nVariance Reduction in Practice: Here, we numerically investigate how many unlabeled samples are sufficient in practice such that the variance of the empirical PNU risk is smaller than that of the PN risk: Var[R\u0302\u03b7PNU(g)] < Var[R\u0302PN(g)] given a fixed classifier g.\nAs the fixed classifier, we used the classifier g\u0302PN, where the hyperparameters were determined by five-fold crossvalidation. To compute the variance of the empirical PN and PNU risks, Var[R\u0302PN(g\u0302PN)] and Var[R\u0302 \u03b7 PNU(g\u0302PN)], we repeatedly drew additional nVP = 10 positive, n V N = 10\nnegative, and nVU unlabeled samples from the rest of the data set. The additional samples were also used for approximating \u03c3\u0302P(g\u0302PN) and \u03c3\u0302N(g\u0302PN) to compute \u03b7, i.e., \u03b3 in Eqs.(10) and (11).\nFigure 1 shows the ratio between the variance of the empirical PNU risk and that of the PN risk, Var[R\u0302\u03b7PNU(g\u0302PN)]/Var[R\u0302PN(g\u0302PN)]. The number of unlabeled samples for validation nVU increases from 10 to 300. We see that with a rather small number of unlabeled samples, the ratio becomes less than 1. That is, the variance of the empirical PNU risk becomes smaller than that of the PN risk. This implies that although the variance reduction is proved for an infinite number of unlabeled samples, it can be observed under a finite number of samples in practice.\nCompared to when \u03b8P = 0.3 and 0.7, the effect of variance reduction is small when \u03b8P = 0.5. This is because if we assume \u03c3P(g) \u2248 \u03c3N(g), when nP \u2248 nN and \u03b8P = 0.5, we have \u03b3N-PNPU \u2248 \u03b3N-PNNU \u2248 0 (because \u03c8P \u2248 \u03c8N. See Theorem 4). That is, the PNU risk is dominated by the PN risk, implying that Var[R\u0302\u03b7PNU(g)] \u2248 Var[R\u0302PN(g)]. Note that the class-prior is not the only factor for variance reduction; for example, if \u03b8P = 0.5, nP nN, and \u03c3P(g) \u2248 \u03c3N(g), then \u03b3N-PNPU 6\u2248 0 (because \u03c8P \u03c8N) and the variance reduction will be large.\nPNU Risk in Validation: As discussed in Section 4, the empirical PNU risk will be a reliable validation score due to its having smaller variance than the empirical PN risk. We show here that the empirical PNU risk is a promising alternative to a validation score.\nTo focus on the effect of validation scores only, we trained two classifiers by using the same risk, e.g, the empirical PN risk. We then tune the classifiers with the empirical PN and PNU risks denoted by g\u0302PNPN and g\u0302 PNU PN , respectively. The number of validation samples was the same as in the previous experiment.\nFigure 2 shows the ratio between the misclassification rate of g\u0302PNUPN and that of g\u0302 PN PN . The number of unlabeled samples for validation increases from 10 to 300. With a rather small number of unlabeled samples, the ratio becomes less than 1, i.e., g\u0302PNUPN achieves better performance than g\u0302 PN PN . In particular, when \u03b8P = 0.3 and 0.7, g\u0302PNUPN improved substantially; the large improvement tends to give the large variance reduction (cf. Figure 1). This result shows that the use of the empirical PNU risk for validation improved the classification performance given a relatively large size of unlabeled data."}, {"heading": "5.2. Comparison with Existing Methods", "text": "Next, we numerically compare the proposed methods against existing semi-supervised classification methods.\nCommon Setup: We compare our methods against five conventional semi-supervised classification methods: entropy regularization (ER) (Grandvalet & Bengio, 2004), the Laplacian support vector machine (LapSVM) (Belkin et al., 2006; Melacci & Belkin, 2011), squared-loss mutual information regularization (SMIR) (Niu et al., 2013), the weakly labeled support vector machine (WellSVM) (Li et al., 2013), and the safe semi-supervised support vector machine (S4VM) (Li & Zhou, 2015).\nAmong the proposed methods, PNU classification and\nPUNU classification with the squared loss were tested.3\nData Sets: We used sixteen benchmark data sets taken from the UCI Machine Learning Repository (Lichman, 2013), the Semi-Supervised Learning book (Chapelle et al., 2006), the LIBSVM (Chang & Lin, 2011), the ELENA Project,4 and a paper by Chapelle & Zien (2005).5 Each feature was scaled to [0, 1]. Similarly to the setting in Section 5.1, we used the Gaussian kernel model for all methods. The training data is {xi}ni=1 = XP \u222aXN \u222aXU, where n = nP +nN +nU. We selected all hyper-parameters with validation samples of size 20 (nVP = n V N = 10). For training, we drew nL labeled and nU = 300 unlabeled samples. The class-prior of labeled data was set at 0.7 and that of unlabeled samples was set at \u03b8P = 0.5 that were assumed to be known. In practice, the class-prior, \u03b8P, can be estimated\n3 In preliminary experiments, we tested other loss functions such as the ramp and logistic losses and concluded that the difference in loss functions did not provide noticeable difference.\n4 https://www.elen.ucl.ac.be/neural-nets/ Research/Projects/ELENA/elena.htm\n5 http://olivier.chapelle.cc/lds/\nby methods proposed, e.g., by Blanchard et al. (2010), Ramaswamy et al. (2016), or Kawakubo et al. (2016).\nTable 1 lists the average and standard error of the misclassification rates over 50 trials and the number of best/comparable performances of each method in the bottom row. The superior performance of PNU classification over PUNU classification agrees well with the discussion in Section 4.3. With the g50c data set, which well satisfies the low-density separation principle, the WellSVM achieved the best performance. However, in the Banana data set, where the two classes are highly overlapped, the performance of WellSVM was worse than the other methods. In contrast, PNU classification achieved consistently better/comparable performance and its performance did not degenerate considerably across data sets. These results show that the idea of using PU classification in semisupervised classification is promising.\nFigure 3 plots the computation time, which shows that the fastest computation was achieved using the proposed methods with the square loss.\nImage Classification: Finally, we used the Places 205 data set (Zhou et al., 2014), which contains 2.5 million images in 205 scene classes. We used a 4096-dimensional feature vector extracted from each image by AlexNet under the framework of Caffe,6 which is available on the project website7. We chose two similar scenes to construct binary classification tasks (see the description of data sets in Appendix B.3). We drew 100 labeled and nU unlabeled samples from each task; the class-prior of labeled and unlabeled data were respectively set at 0.5 and \u03b8P = mP/(mP + mN), where mP and mN respectively denote the number of total samples in positive and negative scenes. We used a linear\n6 http://caffe.berkeleyvision.org/ 7http://places.csail.mit.edu/\nclassifier g(x) = w>x+w0, wherew is the weight vector and w0 is the offset (in the SMIR, the linear kernel model is used; see Niu et al. (2013) for details).\nWe selected hyper-parameters in PNU classification by applying five-fold cross-validation with respect to R\u03b7\u0304PNU(g) with the zero-one loss, where \u03b7\u0304 was set at Eq.(10) or Eq.(11) with \u03c3P(g) = \u03c3N(g). The class-prior p(y = +1) = \u03b8P was estimated using the method based on energy distance minimization (Kawakubo et al., 2016).\nTable 2 lists the average and standard error of the misclassification rates over 30 trials, where methods taking more than 2 hours were omitted and indicated as N/A. The results show that PNU classification was most effective. The average computation times are shown in Figure 4, revealing again that PNU classification was the fastest method."}, {"heading": "6. Conclusions", "text": "In this paper, we proposed a novel semi-supervised classification approach based on classification from positive and unlabeled data. Unlike most of the conventional methods, our approach does not require strong assumptions on the data distribution such as the cluster assumption. We theoretically analyzed the variance of risk estimators and showed that unlabeled data help reduce the variance without the conventional distributional assumptions. We also established generalization error bounds and showed that the confidence term decreases with respect to the number of positive, negative, and unlabeled samples without the conventional distributional assumptions in the optimal parametric order. We experimentally analyzed the behavior of the proposed methods and demonstrated that one of the proposed methods, termed PNU classification, was most effective in terms of both classification accuracy and computational efficiency. It was recently pointed out that PU classification can behave undesirably for very flexible models and a modified PU risk has been proposed (Kiryo et al., 2017). Our future work is to develop a semi-supervised classification method based on the modified PU classification."}, {"heading": "Acknowledgements", "text": "TS was supported by JSPS KAKENHI 15J09111. GN was supported by the JST CREST program and Microsoft Research Asia. MCdP and MS were supported by the JST CREST program."}, {"heading": "A. Proofs of Theorems", "text": "In this section, we give the proofs of Theorems in Section 4.\nA.1. Proof of Theorem 1\nRecall that\nR\u03b3N-PUNU(g) = (1\u2212 \u03b3)RN-PU(g) + \u03b3RN-NU(g) = (2\u2212 2\u03b3)\u03b8PRP(g) + 2\u03b3\u03b8NRN(g) + (1\u2212 \u03b3)RU,N(g) + \u03b3RU,P(g) + Const, R\u03b3N-PNPU(g) = (1\u2212 \u03b3)RPN(g) + \u03b3RN-PU(g) = (1 + \u03b3)\u03b8PRP(g) + (1\u2212 \u03b3)\u03b8NRN(g) + \u03b3RU,N(g) + Const, R\u03b3N-PNNU(g) = (1\u2212 \u03b3)RPN(g) + \u03b3RN-NU(g) = (1\u2212 \u03b3)\u03b8PRP(g) + (1 + \u03b3)\u03b8NRN(g) + \u03b3RU,P(g) + Const.\nLet R\u0302P(g), R\u0302N(g), R\u0302U,P(g) and R\u0302U,N(g) be the empirical risks. In order to prove Theorem 1, the following concentration lemma is needed:\nLemma 5 For any \u03b4 > 0, we have these uniform deviation bounds with probability at least 1\u2212 \u03b4/3:\nsupg\u2208G(RP(g)\u2212 R\u0302P(g)) \u2264 CwC\u03c6\u221a nP +\n\u221a ln(3/\u03b4)\n2nP ,\nsupg\u2208G(RN(g)\u2212 R\u0302N(g)) \u2264 CwC\u03c6\u221a nN +\n\u221a ln(3/\u03b4)\n2nN ,\nsupg\u2208G(RU,P(g)\u2212 R\u0302U,P(g)) \u2264 CwC\u03c6\u221a nU +\n\u221a ln(3/\u03b4)\n2nU ,\nsupg\u2208G(RU,N(g)\u2212 R\u0302U,N(g)) \u2264 CwC\u03c6\u221a nU +\n\u221a ln(3/\u03b4)\n2nU .\nAll inequalities in Lemma 5 are from the basic uniform deviation bound using the Rademacher complexity (Mohri et al., 2012), Talagrand\u2019s contraction lemma (Ledoux & Talagrand, 1991), as well as the fact that the Lipschitz constant of `R is 1/2. For these reasons, the detailed proof of Lemma 5 is omitted.\nConsider R\u03b3N-PNPU(g). It is clear that\nsupg\u2208G(R \u03b3 N-PNPU(g)\u2212 R\u0302 \u03b3 N-PNPU(g))\n\u2264 (1 + \u03b3)\u03b8P supg\u2208G(RP(g)\u2212 R\u0302P(g)) + (1\u2212 \u03b3)\u03b8N supg\u2208G(RN(g)\u2212 R\u0302N(g)) + \u03b3 supg\u2208G(RU,N(g)\u2212 R\u0302U,N(g)).\nTherefore, by applying Lemma 5, for any \u03b4 > 0, it holds with probability at least 1\u2212 \u03b4 that\nsupg\u2208G(R \u03b3 N-PNPU(g)\u2212 R\u0302 \u03b3 N-PNPU(g)) \u2264\n1 2 Cw,\u03c6,\u03b4 \u00b7 \u03c7(1 + \u03b3, 1\u2212 \u03b3, \u03b3).\nSince I(g) \u2264 2R\u03b3N-PNPU, with the same probability,\nsupg\u2208G(I(g)\u22122R\u0302 \u03b3 N-PNPU(g)) \u2264 Cw,\u03c6,\u03b4 \u00b7\u03c7(1 + \u03b3, 1\u2212 \u03b3, \u03b3).\nSimilarly, supg\u2208G(I(g)\u2212 2R\u0302 \u03b3 N-PNNU(g)) \u2264 Cw,\u03c6,\u03b4 \u00b7 \u03c7(1\u2212 \u03b3, 1 + \u03b3, \u03b3) with probability at least 1\u2212 \u03b4.\nFinally,R\u03b3N-PUNU(g) is slightly more involved, for that there are bothRU,P(g) andRU,N(g). From `R(m)+`R(\u2212m) = 1, we can know RU,P(g) +RU,N(g) = 1 and then\n(1\u2212 \u03b3)RU,N(g) + \u03b3RU,P(g) = { (2\u03b3 \u2212 1)RU,P(g) + Const \u03b3 \u2265 1/2, (1\u2212 2\u03b3)RU,N(g) + Const \u03b3 < 1/2.\nAs a result, supg\u2208G(I(g)\u2212 2R\u0302 \u03b3 N-PUNU(g)) \u2264 Cw,\u03c6,\u03b4 \u00b7 \u03c7(2\u2212 2\u03b3, 2\u03b3, |2\u03b3 \u2212 1|) with probability at least 1\u2212 \u03b4.\nA.2. Proof of Theorem 2\nIn fact,\n`TS(m) =  1/4 m \u2264 0, (m\u2212 1)2/4 0 < m \u2264 1, 0 m > 1,\nand after plugging this `TS(m) into \u02dc\u0300TS(m), \u02dc\u0300 TS(m) = `TS(m)\u2212 `TS(\u2212m)\n=  1/4 m \u2264 \u22121, 1/4\u2212 (m+ 1)2/4 \u22121 < m \u2264 0, (m\u2212 1)2/4\u2212 1/4 0 < m \u2264 1, \u22121/4 m > 1.\nIt is easy to see that `TS(m) and \u02dc\u0300TS(m) are Lipschitz continuous with the same Lipschitz constant 1/2. Next, recall that\nR\u03b3C-PUNU(g) = (1\u2212 \u03b3)RC-PU(g) + \u03b3RC-NU(g) = (1\u2212 \u03b3)\u03b8PR\u2032P(g) + \u03b3\u03b8NR\u2032N(g) + (1\u2212 \u03b3)RU,N(g) + \u03b3RU,P(g), R\u03b3C-PNPU(g) = (1\u2212 \u03b3)RPN(g) + \u03b3RC-PU(g) = (1\u2212 \u03b3)\u03b8PRP(g) + (1\u2212 \u03b3)\u03b8NRN(g) + \u03b3\u03b8PR\u2032P(g) + \u03b3RU,N(g), R\u03b3C-PNNU(g) = (1\u2212 \u03b3)RPN(g) + \u03b3RC-NU(g) = (1\u2212 \u03b3)\u03b8PRP(g) + (1\u2212 \u03b3)\u03b8NRN(g) + \u03b3\u03b8NR\u2032N(g) + \u03b3RU,P(g).\nLet R\u0302P(g), R\u0302N(g), R\u0302U,P(g), R\u0302U,N(g), R\u0302\u2032P(g) and R\u0302 \u2032 N(g) be the empirical risks. Again, the following concentration lemma is needed:\nLemma 6 For any \u03b4 > 0, we have these uniform deviation bounds with probability at least 1\u2212 \u03b4/4:\nsupg\u2208G(RP(g)\u2212 R\u0302P(g)) \u2264 CwC\u03c6\u221a nP +\n\u221a ln(4/\u03b4)\n32nP ,\nsupg\u2208G(RN(g)\u2212 R\u0302N(g)) \u2264 CwC\u03c6\u221a nN +\n\u221a ln(4/\u03b4)\n32nN ,\nsupg\u2208G(RU,P(g)\u2212 R\u0302U,P(g)) \u2264 CwC\u03c6\u221a nU +\n\u221a ln(4/\u03b4)\n32nU ,\nsupg\u2208G(RU,N(g)\u2212 R\u0302U,N(g)) \u2264 CwC\u03c6\u221a nU +\n\u221a ln(4/\u03b4)\n32nU ,\nsupg\u2208G(R \u2032 P(g)\u2212 R\u0302\u2032P(g)) \u2264 CwC\u03c6\u221a nP +\n\u221a ln(4/\u03b4)\n8nP ,\nsupg\u2208G(R \u2032 N(g)\u2212 R\u0302\u2032N(g)) \u2264 CwC\u03c6\u221a nN +\n\u221a ln(4/\u03b4)\n8nN .\nThe detailed proof of Lemma 6 is omitted for the same reason as Lemma 5. The difference is due to that 0 \u2264 `TS(m) \u2264 1/4 and \u22121/4 \u2264 \u02dc\u0300TS(m) \u2264 1/4 whereas 0 \u2264 `R(m) \u2264 1 just like 0 \u2264 `0-1(m) \u2264 1. For convenience, we will relax 1/32 to 1/8 in the square root for RP(g), RN(g), RU,P(g), RU,N(g).\nConsider R\u03b3C-PUNU(g). By applying Lemma 6, for any \u03b4 > 0, it holds with probability at least 1\u2212 \u03b4 that\nsupg\u2208G(R \u03b3 C-PUNU(g)\u2212 R\u0302 \u03b3 C-PUNU(g)) \u2264\n1 4 C \u2032w,\u03c6,\u03b4 \u00b7 \u03c7(1\u2212 \u03b3, \u03b3, 1).\nSince I(g) \u2264 4R\u03b3C-PUNU, with the same probability,\nsupg\u2208G(I(g)\u2212 4R\u0302 \u03b3 C-PUNU(g)) \u2264 C \u2032 w,\u03c6,\u03b4 \u00b7 \u03c7(1\u2212 \u03b3, \u03b3, 1).\nThe other two generalization error bounds can be proven similarly.\nA.3. Proofs of Theorems 3 and 4\nNote that g is independent of the data for evaluating R\u0302\u03b3N-PUNU(g), since it is fixed in the evaluation. Thus, VarP[R\u0302P(g)] = \u03c32P(g)/nP and VarN[R\u0302N(g)] = \u03c3 2 N(g)/nN. When nU \u2192\u221e,\nVar[R\u0302\u03b3N-PUNU(g)] = 4(1\u2212 \u03b3) 2\u03b82P VarP[R\u0302P(g)] + 4\u03b3 2\u03b82N VarN[R\u0302N(g)]\n= 4(1\u2212 \u03b3)2\u03c8P + 4\u03b32\u03c8N = 4(\u03c8P + \u03c8N)\u03b3 2 \u2212 8\u03c8P\u03b3 + 4\u03c8P,\nand it is obvious that \u03b3N-PUNU \u2208 [0, 1]. All other claims in Theorem 3 follow from that Var[R\u0302\u03b3N-PUNU(g)] is quadratic in \u03b3, that Var[R\u0302\u03b3N-PUNU(g)] = Var[R\u0302PN(g)] at \u03b3 = 1/2, and that \u03b3N-PUNU < 1/2 if \u03c8P < \u03c8N or \u03b3N-PUNU > 1/2 if \u03c8P > \u03c8N.\nLikewise, when nU \u2192\u221e,\nVar[R\u0302\u03b3N-PNPU(g)] = (1 + \u03b3) 2\u03c8P + (1\u2212 \u03b3)2\u03c8N, Var[R\u0302\u03b3N-PNNU(g)] = (1\u2212 \u03b3) 2\u03c8P + (1 + \u03b3) 2\u03c8N,\nand \u03b3N-PNPU \u2265 0 if \u03c8P \u2264 \u03c8N or \u03b3N-PNNU \u2265 0 if \u03c8P \u2265 \u03c8N. The rest of proof of Theorem 4 is analogous to that of Theorem 3."}, {"heading": "B. Experimental Setting", "text": "Here, we summarized the experimental settings.\nB.1. Implementation in Our Experiments\nWe implemented the ER by ourselves, and for the other methods, we used the codes available at the authors\u2019 websites:\n\u2022 LapSVM: http://www.dii.unisi.it/~melacci/lapsvmp/\n\u2022 SMIR: http://www.ms.k.u-tokyo.ac.jp/software/SMIR.zip\n\u2022 WellSVM: http://lamda.nju.edu.cn/code_WellSVM.ashx\n\u2022 S4VM: http://lamda.nju.edu.cn/files/s4vm.rar.\nNote that we modified the original code of the S4VM for transductive learning to inductive learning according to Li & Zhou (2015).\nB.2. Parameter Candidates in Our Experiments\nThe regularization parameters for all the methods were chosen from {10\u22125, 10\u22124, . . . , 102}, except the regularization parameter of the SMIR for the squared loss mutual information (SMI) and that of the S4VM for labeled data. The number of nearest-neighbors to construct Laplacian matrix for the LapSVM was chosen from the candidates {5, 6, . . . , 10}. The combination parameter \u03b7 of PNU classification was chosen from {\u22121,\u22120.9, . . . , 1}, and \u03b3 of PUNU classification was chosen from {0, 0.05, . . . , 1}. We chose these hyper-parameters by five-fold cross-validation. The parameter for the `2- regularizer of the SMIR is set at \u03b3S/(n \u00b7mink\u2208{\u00b11} p(y = k)) + 0.001, where \u03b3S is the regularization parameter for the SMI. The regularization parameter of the S4VM for the labeled data is set at 1. The other parameters were set at the default values.\nB.3. Data Set Description of Image Classification Data Set\nTable 3 is the description of the data sets used in the image classification experiment."}, {"heading": "C. Supplementary Results for Experimental Analyses", "text": "Figure 5 and Figure 6 respectively show the results of variance reduction and comparison of validation scores. The details of experimental setting and the interpretation of results can be found in Section 5.1."}, {"heading": "D. Magnified Versions of Experimental Results", "text": "Here, we show magnified versions of the experimental results in Section 5."}], "year": 2017, "references": [{"title": "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples", "authors": ["M. Belkin", "P. Niyogi", "V. Sindhwani"], "venue": "Journal of Machine Learning Research,", "year": 2006}, {"title": "Semi-supervised novelty detection", "authors": ["G. Blanchard", "G. Lee", "C. Scott"], "venue": "Journal of Machine Learning Research,", "year": 2010}, {"title": "LIBSVM: A library for support vector machines", "authors": ["Chang", "C.-C", "Lin", "C.-J"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "year": 2011}, {"title": "Semi-supervised classification by low density separation", "authors": ["O. Chapelle", "A. Zien"], "venue": "In AISTATS, pp", "year": 2005}, {"title": "Trading convexity for scalability", "authors": ["R. Collobert", "F. Sinz", "J. Weston", "L. Bottou"], "venue": "In ICML, pp", "year": 2006}, {"title": "Semisupervised learning of mixture models", "authors": ["F.G. Cozman", "I. Cohen", "M.C. Cirelo"], "venue": "In ICML, pp", "year": 2003}, {"title": "Analysis of learning from positive and unlabeled data", "authors": ["M.C. du Plessis", "G. Niu", "M. Sugiyama"], "venue": "In NIPS,", "year": 2014}, {"title": "Convex formulation for learning from positive and unlabeled data", "authors": ["M.C. du Plessis", "G. Niu", "M. Sugiyama"], "venue": "In ICML,", "year": 2015}, {"title": "Learning classifiers from only positive and unlabeled data", "authors": ["C. Elkan", "K. Noto"], "venue": "In SIGKDD, pp", "year": 2008}, {"title": "Semi-supervised learning by entropy minimization", "authors": ["Y. Grandvalet", "Y. Bengio"], "venue": "In NIPS, pp", "year": 2004}, {"title": "Estimating the class prior and posterior from noisy positives and unlabeled data", "authors": ["S. Jain", "M. White", "P. Radivojac"], "year": 2016}, {"title": "Computationally efficient class-prior estimation under class balance change using energy distance", "authors": ["H. Kawakubo", "M.C. du Plessis", "M. Sugiyama"], "venue": "IEICE Transactions on Information and Systems,", "year": 2016}, {"title": "Positive-unlabeled learning with non-negative risk estimator", "authors": ["R. Kiryo", "G. Niu", "M.C. du Plessis", "M. Sugiyama"], "venue": "arXiv preprint arXiv:1703.00593,", "year": 2017}, {"title": "Robust semi-supervised least squares classification by implicit constraints", "authors": ["J.H. Krijthe", "M. Loog"], "venue": "Pattern Recognition,", "year": 2017}, {"title": "Probability in Banach Spaces: Isoperimetry and Processes", "authors": ["M. Ledoux", "M. Talagrand"], "year": 1991}, {"title": "Towards making unlabeled data never hurt", "authors": ["Li", "Y.-F", "Zhou", "Z.-H"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "year": 2015}, {"title": "Convex and scalable weakly labeled SVMs", "authors": ["Li", "Y.-F", "I.W. Tsang", "J.T. Kwok", "Zhou", "Z.-H"], "venue": "Journal of Machine Learning Research,", "year": 2013}, {"title": "Laplacian support vector machines trained in the primal", "authors": ["S. Melacci", "M. Belkin"], "venue": "Journal of Machine Learning Research,", "year": 2011}, {"title": "Lower bounds for the empirical minimization algorithm", "authors": ["S. Mendelson"], "venue": "IEEE Transactions on Information Theory,", "year": 2008}, {"title": "Foundations of Machine Learning", "authors": ["M. Mohri", "A. Rostamizadeh", "A. Talwalkar"], "year": 2012}, {"title": "Squared-loss mutual information regularization: A novel information-theoretic approach to semisupervised learning", "authors": ["G. Niu", "W. Jitkrittum", "B. Dai", "H. Hachiya", "M. Sugiyama"], "venue": "In ICML,", "year": 2013}, {"title": "Theoretical comparisons of positive-unlabeled learning against positive-negative learning", "authors": ["G. Niu", "M.C. du Plessis", "T. Sakai", "Y. Ma", "M. Sugiyama"], "year": 2016}, {"title": "Mixture proportion estimation via kernel embedding of distributions", "authors": ["H.G. Ramaswamy", "C. Scott", "A. Tewari"], "venue": "In ICML,", "year": 2016}, {"title": "The asymptotics of semi-supervised learning in discriminative probabilistic models", "authors": ["N. Sokolovska", "O. Capp\u00e9", "F. Yvon"], "venue": "In ICML, pp", "year": 2008}, {"title": "Statistical Learning Theory", "authors": ["V.N. Vapnik"], "year": 1998}, {"title": "The Nature of Statistical Learning Theory", "authors": ["V.N. Vapnik"], "year": 1995}, {"title": "The concave-convex procedure (CCCP)", "authors": ["A.L. Yuille", "A. Rangarajan"], "venue": "In NIPS, pp. 1033\u20131040,", "year": 2002}, {"title": "Learning deep features for scene recognition using places database", "authors": ["B. Zhou", "A. Lapedriza", "J. Xiao", "A. Torralba", "A. Oliva"], "venue": "In NIPS, pp", "year": 2014}], "id": "SP:d4e7587b37206f7bc8ff45b7347067d7e3973e92", "authors": [{"name": "Tomoya Sakai", "affiliations": []}, {"name": "Marthinus Christoffel du Plessis", "affiliations": []}, {"name": "Gang Niu", "affiliations": []}, {"name": "Masashi Sugiyama", "affiliations": []}], "abstractText": "Most of the semi-supervised classification methods developed so far use unlabeled data for regularization purposes under particular distributional assumptions such as the cluster assumption. In contrast, recently developed methods of classification from positive and unlabeled data (PU classification) use unlabeled data for risk evaluation, i.e., label information is directly extracted from unlabeled data. In this paper, we extend PU classification to also incorporate negative data and propose a novel semi-supervised classification approach. We establish generalization error bounds for our novel methods and show that the bounds decrease with respect to the number of unlabeled data without the distributional assumptions that are required in existing semi-supervised classification methods. Through experiments, we demonstrate the usefulness of the proposed methods.", "title": "Semi-Supervised Classification  Based on Classification from Positive and Unlabeled Data"}