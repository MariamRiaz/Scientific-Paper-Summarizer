{"sections": [{"heading": "1 Introduction", "text": "Local search algorithms like stochastic gradient descent [4] or variants have gained huge success in training deep neural networks (see, [5]; [6]; [7], for example). Despite the spurious saddle points and local minima on the loss surface [3], it has been widely conjectured that all local minima of the empirical loss lead to similar training performance [1, 2]. For example, [8] empirically showed that neural networks with identical architectures but different initialization points can converge to local minima with similar classification performance. However, it still remains a challenge to characterize the theoretical properties of the loss surface for neural networks.\nIn the setting of regression problems, theoretical justifications has been established to support the conjecture that all local minima lead to similar training performance. For shallow models, [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20] provide conditions under which the local search algorithms are guaranteed to converge to the globally optimal solution for the regression problem. For deep linear networks, it has been shown that every local minimum of the empirical loss is a global minimum [21, 22, 23, 24, 25]. In order to characterize the loss surface of more general deep networks for regression tasks, [2] have proposed an interesting approach. Based on certain constructions on network models and additional assumptions, they relate the loss function to a spin glass model and show that the almost all local minima have similar empirical loss and the number of bad local minima decreases quickly with the distance to the global optimum. Despite the interesting results, it remains a concern to properly justify their assumptions. More recently, it has been shown [26, 27] that, when the dataset satisfies certain conditions, if one layer in the multilayer network has more neurons than the number of training samples, then a subset of local minima are global minima.\n\u2217University of Illinois at Urbana-Champaign \u2020Facebook Research\nar X\niv :1\n80 3.\n00 90\n9v 2\n[ cs\n.L G\n] 5\nM ar\n2 01\nAlthough the loss surfaces in regression tasks have been well studied, the theoretical understanding of loss surfaces in classification tasks is still limited. [27, 28, 29] treat the classification problem as the regression problem by using quadratic loss, and show that (almost) all local minima are global minima. However, the global minimum of the quadratic loss does not necessarily have zero misclassification error even in the simplest cases (e.g., every global minimum of quadratic loss can have non-zero misclassification error even when the dataset is linearly separable and the network is a linear network). This issue was mentioned in [26] and a different loss function was used, but their result only studied the linearly separable case and a subset of the critical points.\nIn view of the prior work, the context and contributions of our paper are as follows:\n\u2022 Prior work on quadratic and related loss functions suggest that one can achieve zero misclassification error at all local minima by overparameterizing the neural network. The reason for over-parameterization is that the quadratic loss function tries to match the output of the neural network to the label of each training sample.\n\u2022 On the other hand, hinge loss-type functions only try to match the sign of the outputs with the labels. So it may be possible to achieve zero misclassification error without over-parametrization. We provide conditions under which the misclassification error of neural networks is zero at all local minima for hinge-loss functions.\n\u2022 Our conditions are roughly in the following form: the neurons have to be increasing and strictly convex, the neural network should either be single-layered or is multi-layered with a shortcut-like connection and the surrogate loss function should be a smooth version of the hinge loss function.\n\u2022 We also provide counterexamples to show that when these conditions are relaxed, the result may not hold.\n\u2022 We establish our results under the assumption that either the dataset is linearly separable or the positively and negatively labeled samples are located on different subspaces. Whether this assumption is necessary is an open problem, except in the case of certain special neurons.\nThe outline of this paper is as follows. In Section 2, we present the necessary definitions. In Section 3, we present the main results and we discuss each condition in Section 4. Conclusions are presented in Section 5. All proofs are provided in Appendix."}, {"heading": "2 Preliminaries", "text": "Network models. Given an input vector x of dimension d, we consider a neural network with L layers for binary classification. We denote by Ml the number of neurons on the l-th layer (note that M0 = d and ML = 1). We denote the neuron activation function by \u03c3. Let Wl \u2208 RMl\u22121\u00d7Ml denote the weight matrix connecting the (l \u2212 1)-th layer and the l-th layer and bl \u2208 RMl denote the bias vector for the neurons in the l-th layer. Therefore, the output of the network f : Rd \u2192 R can be expressed by\nf(x;\u03b8) = W>L \u03c3 ( ...\u03c3(W>1 x+ b1) + bL\u22121 ) + bL,\nwhere \u03b8 denotes all parameters in the neural network. Data distribution. In this paper, we consider binary classification tasks where each sample (X, Y ) \u2208 Rd \u00d7 {\u22121, 1} is drawn from an underlying data distribution PX\u00d7Y defined on Rd \u00d7 {\u22121, 1}. The sample (X, Y ) is considered positive if Y = 1, and negative otherwise. Let E = {e1, ..., ed} denote a set of orthonormal basis on the space Rd. Let U+ and U\u2212 denote two subsets of E such that all\npositive and negative samples are located on the linear span of the set U+ and U\u2212, respectively, i.e., PX|Y (X \u2208 Span(U+)|Y = 1) = 1 and PX|Y (X \u2208 Span(U\u2212)|Y = \u22121) = 1. Let r denote the size of the set U+ \u222a U\u2212, r+ denote the size of the set U+ and r\u2212 denote the size of the set U\u2212, respectively. Loss and error. Let D = {(xi, yi)}ni=1 denote a dataset with n samples, each independently drawn from the distribution PX\u00d7Y . Given a neural network f(x;\u03b8) parameterized by \u03b8 and a loss function ` : R \u2192 R, in binary classification tasks1, we define the empirical loss L\u0302n(\u03b8) as the average loss of the network f on a sample in the dataset D, i.e.,\nL\u0302n(\u03b8) = 1\nn n\u2211 i=1 `(\u2212yif(xi;\u03b8)).\nFurthermore, for a neural network f , we define a binary classifier gf : Rd \u2192 {\u22121, 1} of the form gf = sgn(f), where the sign function sgn(z) = 1, if z \u2265 0, and sgn(z) = 0 otherwise. We define the training error (also called the misclassification error) R\u0302n(\u03b8) as the misclassification rate of the neural network f(x;\u03b8) on the dataset D, i.e.,\nR\u0302n(\u03b8) = 1\nn n\u2211 i=1 I{yi 6= sgn(f(xi;\u03b8))},\nwhere I{\u00b7} is the indicator function. The training error R\u0302n measures the classification performance of the network f on the finite samples in the dataset D."}, {"heading": "3 Main Results", "text": "In this section, we present the main results. We first introduce several important conditions in order to derive the main results, and we will provide further discussions on these conditions in the next section."}, {"heading": "3.1 Conditions", "text": "To fully specify the problem, we need to specify our assumptions on several components of the model, including: (1) the loss function, (2) the data distribution, (3) the network architecture and (4) the neuron activation function.\nAssumption 1 (Loss function) Let `p : R \u2192 R denote a loss function satisfying the following conditions: (1) `p is a surrogate loss function, i.e., `p(z) \u2265 I{z \u2265 0} for all z \u2208 R, where I(\u00b7) denotes the indicator function; (2) `p has continuous derivatives up to order p on R; (3) `p is non-decreasing (i.e., `\u2032p(z) \u2265 0 for all z \u2208 R) and there exists a positive constant z0 such that `\u2032p(z) = 0 iff z \u2264 \u2212z0.\nThe first condition in Assumption 1 ensures that the training error R\u0302n is always upper bounded by the empirical loss L\u0302n, i.e., R\u0302n \u2264 L\u0302n. This guarantees that the neural network can correctly classify all samples in the dataset (i.e., R\u0302n = 0), when the neural network achieves zero empirical loss (i.e., L\u0302n = 0). The second condition ensures that the empirical loss L\u0302n has continuous derivatives with respect to the parameters up to a sufficiently high order. The third condition ensures that the loss function is non-decreasing and `\u2032p(z) = 0 is achievable if and only if z \u2264 \u2212z0. Here, we provide a simple example of the loss function satisfying all conditions in Assumption 1: the polynomial hinge loss, i.e., `p(z) = [max{z + 1, 0}]p+1. We note that, in this paper, we use L\u0302n(\u03b8; p) to denote the empirical loss\n1We note that, in regression tasks, the empirical loss is usually defined as L\u0302n(\u03b8) = 1 n \u2211n i=1 `(yi \u2212 f(xi;\u03b8)).\nwhen the loss function is `p and the network is parametrized by a set of parameters \u03b8. Further results on the impact of loss functions are presented in Section 4.\nAssumption 2 (Data distribution) Assume that for random vectors X1, ...,Xr+ independently drawn from the distribution PX|Y=1 and Z1, ...,Zr\u2212 independently drawn from the distribution PX|Y=\u22121, matrices ( X1, ...,Xr+ ) \u2208 Rr+\u00d7d and ( Z1, ...,Zr\u2212 ) \u2208 Rr\u2212\u00d7d are full rank matrices with probability one.\nAssumption 2 states that support of the conditional distribution PX|Y=1 is sufficiently rich so that r+ samples drawn from it will be linearly independent. In other words, by stating this assumption, we are avoiding trivial cases where all the positively labeled points are located in a very small subset of the linear span of U+. Similarly for the negatively labeled samples.\nAssumption 3 (Data distribution) Assume |U+ \u222a U\u2212| > max{|U+|, |U\u2212|}, i.e., r > max{r+, r\u2212}.\nAssumption 3 assumes that the positive and negative samples are not located on the same linear subspace. Previous works [30, 31, 32, 30] have observed that some classes of natural images (e.g., images of faces, handwritten digits, etc) can be reconstructed from lower-dimensional representations. For example, using dimensionality reduction methods such as PCA, one can approximately reconstruct the original image from only a small number of principal components [30, 31]. Here, Assumption 3 states that both the positively and negatively labeled samples have lower-dimensional representations, and they do not exist in the same lower-dimensional subspace. We provide additional analysis in Section 4, showing how our main results generalize to other data distributions.\nAssumption 4 (Network architecture) Assume that the neural network f is a single-layered neural network, or more generally, has shortcut-like connections shown in Fig 1 (b), where fS is a single layer network and fD is a feedforward network.\nShortcut connections are widely used in the modern network architectures (e.g., Highway Networks [34], ResNet [33], DenseNet [35], etc.), where the skip connections allow the deep layers to have direct access to the outputs of shallow layers. For instance, in the residual network, each residual block has a identity shortcut connection, shown in Fig 1 (a), where the output of each residual block is the vector sum of its input and the output of a network H.\nInstead of using the identity shortcut connection, in this paper, we first pass the input through a single layer network fS(x;\u03b8S) = a0 + a >\u03c3 ( W>x ) , where vector a denotes the weight vector, matrix W denotes the weight matrix and vector \u03b8S denotes the vector containing all parameters in fS . We next add the output of this network to a network fD and use the addition as the output of the whole network, i.e., f(x;\u03b8) = fS(x;\u03b8S) + fD(x;\u03b8D), where vector \u03b8D and \u03b8 denote the vector containing all parameters in the\nnetwork fD and the whole network f , respectively. We note here that, in this paper, we do not restrict the number of layers and neurons in the network fD and this means that the network fD can be a feedforward network introduced in Section 2 or a single layer network or even a constant. In fact, when the network fD is a single layer network or a constant, the whole network f becomes a single\nlayer network. Furthermore, we note that, in Section 4, we will show that if we remove this connection or replace this shortcut-like connection with the identity shortcut connection, the main result does not hold.\nAssumption 5 (Neuron activation) Assume that neurons \u03c3(z) in the network fS are real analytic and satisfy \u03c3\u2032\u2032(z) > 0 for all z \u2208 R. Assume that neurons in the network fD are real functions on R.\nIn Assumption 5, we assume that neurons in the network fS are infinitely differentiable and have positive second order derivatives on R, while neurons in the network fD are real functions. We make the above assumptions to ensure that the loss function L\u0302n(\u03b8S ,\u03b8D; p) is partially differentiable w.r.t. the parameters \u03b8S in the network fS up to a sufficiently high order and allow us to use Taylor expansion in the analysis. Here, we list a few neurons which can be used in the network fS : softplus neuron, i.e., \u03c3(z) = log2(1+e\nz), quadratic neuron, i.e, \u03c3(z) = z2, etc. We note that neurons in the network fS and fD do not need to be of the same type and this means that a more general class of neurons can be used in the network fD, e.g., threshold neuron, i.e., \u03c3(z) = I{z \u2265 0}, rectified linear unit \u03c3(z) = max{z, 0}, sigmoid neuron \u03c3(z) = 1\n1+e\u2212z , etc. Further discussion on the effects of neurons on the main results are provided in Section 4."}, {"heading": "3.2 Main Results", "text": "Now we present the following theorem to show that when assumptions 1-5 are satisfied, every local minimum of the empirical loss function has zero training error if the number of neurons in the network fS are chosen appropriately.\nTheorem 1 (Linear subspace data) Suppose that assumptions 1-5 are satisfied. Assume that samples in the dataset D = {(xi, yi)}ni=1, n \u2265 1 are independently drawn from the distribution PX\u00d7Y . Assume that the number of neurons M in the network fS satisfies M \u2265 2 max{ n\u2206r , r+, r\u2212}, where \u2206r = r \u2212 max{r+, r\u2212}. If \u03b8\u2217 = (\u03b8\u2217S ,\u03b8\u2217D) is a local minimum of the loss function L\u0302n(\u03b8S ,\u03b8D; p) and p \u2265 6, then R\u0302n(\u03b8\u2217S ,\u03b8\u2217D) = 0 holds with probability one.\nRemark: (i) By setting the network fD to a constant, it directly follows from Theorem 1 that if a single layer network fS(x;\u03b8S) consisting of neurons satisfying Assumption 5 and all other conditions in Theorem 1 are satisfied, then every local minimum of the empirical loss L\u0302n(\u03b8S ; p) has zero training error. (ii) The positiveness of \u2206r is guaranteed by Assumption 3. In the worst case (e.g., \u2206r = 1 and \u2206r = 2), the number of neurons needs to be at least greater than the number of samples, i.e., M \u2265 n. However, when the two orthonormal basis sets U+ and U\u2212 differ significantly (i.e., \u2206r 1), the number of neurons required by Theorem 1 can be significantly smaller than the number of samples (i.e., n 2n/\u2206r). In fact, we can show that, when the neuron has quadratic activation function \u03c3(z) = z2, the assumption M \u2265 2n/\u2206r can be further relaxed such that the number of neurons is independent of the number of samples. We discuss this in the following proposition.\nProposition 1 Assume that assumptions 1-5 are satisfied. Assume that samples in the dataset D = {(xi, yi)}ni=1, n \u2265 1 are independently drawn from the distribution PX\u00d7Y . Assume that neurons in the network fS satisfy \u03c3(z) = z\n2 and the number of neurons in the network fS satisfies M > r. If \u03b8\u2217 = (\u03b8\u2217S ,\u03b8 \u2217 D) is a local minimum of the loss function L\u0302n(\u03b8S ,\u03b8D; p) and p \u2265 6, then R\u0302n(\u03b8\u2217S ,\u03b8D) = 0 holds with probability one.\nRemark: Proposition 1 shows that if the number of neuron M is greater than the dimension of the subspace, i.e., M > r, then every local minimum of the empirical loss function has zero training error. We note here that although the result is stronger with quadratic neurons, it does not imply that the\nquadratic neuron has advantages over the other types of neurons (e.g., softplus neuron, etc). This is due to the fact that when the neuron has positive derivatives on R, the result in Theorem 1 holds for the dataset where positive and negative samples are linearly separable. We provide the formal statement of this result in Theorem 2. However, when the neuron has quadratic activation function, the result in Theorem 1 may not hold for linearly separable dataset and we will illustrate this by providing a counterexample in the next section.\nAs shown in Theorem 1, when the data distribution satisfies Assumption 2 and 3, every local minimum of the empirical loss has zero training error. However, we can easily see that distributions satisfying these two assumptions may not be linearly separable. Therefore, to provide a complementary result to Theorem 1, we consider the case where the data distribution is linearly separable. Before presenting the result, we first present the following assumption on the data distribution.\nAssumption 6 (Linear separability) Assume that there exists a vector w \u2208 Rd such that the data distribution satisfies PX\u00d7Y (Yw>X > 0) = 1.\nIn Theorem 2, we will show that when the samples drawn from the data distribution are linearly separable, and the network has a shortcut-like connection shown in Figure 1, all local minima of the empirical loss function have zero training errors if the type of the neuron in the network fS are chosen appropriately.\nTheorem 2 (Linearly separable data) Suppose that the loss function `p satisfies Assumption 1 and the network architecture satisfies Assumption 4. Assume that samples in the dataset D = {(xi, yi)}ni=1, n \u2265 1 are independently drawn from a distribution satisfying Assumption 6. Assume that the single layer network fS has M \u2265 1 neurons and neurons \u03c3 in the network fS are twice differentiable and satisfy \u03c3\u2032(z) > 0 for all z \u2208 R. If \u03b8\u2217 = (\u03b8\u2217S ,\u03b8\u2217D) is a local minimum of the loss function L\u0302n(\u03b8S ,\u03b8D; p), p \u2265 3, then R\u0302n(\u03b8\u2217S ,\u03b8\u2217D) = 0 holds with probability one.\nRemark: Similar to Proposition 1, Theorem 2 does not require the number of neurons to be in scale with the number of samples. In fact, we make a weaker assumption here: the single layer network fS only needs to have at least one neuron, in contrast to at least r neurons required by Proposition 1. Furthermore, we note here that, in Theorem 2, we assume that neurons in the network fS have positive derivatives on R. This implies that Theorem 2 may not hold for a subset of neurons considered in Theorem 1 (e.g., quadratic neuron, etc). We will provide further discussions on the effects of neurons in the next section.\nSo far, we have provided results showing that under certain constraints on the (1) neuron activation function, (2) network architecture, (3) loss function and (4) data distribution, every local minimum of the empirical loss function has zero training error. In the next section, we will discuss the implications of these conditions on our main results."}, {"heading": "4 Discussions", "text": "In this section, we discuss the effects of the (1) neuron activation, (2) shortcut-like connections, (3) loss function and (4) data distribution on the main results, respectively. We show that the result may not hold if these assumptions are relaxed."}, {"heading": "4.1 Neuron Activations", "text": "To begin with, we discuss whether the results in Theorem 1 and 2 still hold if we vary the neuron activation function in the single layer network fS . Specifically, we consider the following five classes of\nneurons: (1) softplus class, (2) rectified linear unit (ReLU) class, (3) leaky rectified linear unit (Leaky ReLU) class, (4) quadratic class and (5) sigmoid class. In the following, for each class of neurons, we show whether the main results hold and provide counterexamples if certain conditions in the main results are violated. We summarize our findings in Table 4.1. We visualize some neurons activation functions from these five classes in Fig. 2(a). Softplus class contains neurons with real analytic activation functions \u03c3, where \u03c3\u2032(z) > 0, \u03c3\u2032\u2032(z) > 0 for all z \u2208 R. A widely used neuron in this class is the softplus neuron, i.e., \u03c3(z) = log2(1 + ez), which is a smooth approximation of ReLU. We can see that neurons in this class satisfy assumptions in both Theorem 1 and 2 and this indicates that both theorems hold for the neurons in this class. ReLU class contains neurons with \u03c3(z) = 0 for all z \u2264 0 and \u03c3(z) is piece-wise continuous on R. Some commonly adopted neurons in this class include: threshold units, i.e., I{z \u2265 0}, rectified linear units (ReLU), i.e., max{z, 0} and rectified quadratic units (ReQU), i.e., [max{z, 0}]2. We can see that neurons in this class do not satisfy neither assumptions in Theorem 1 nor 2. In proposition 2, we show that when the single layer network fS consists of neurons in the ReLU class, even if all other conditions in Theorem 1 or 2 are satisfied, the empirical loss function can have a local minimum with non-zero training error.\nProposition 2 Suppose that assumptions 1 and 4 are satisfed. Assume that neurons in the network fS satisfy that \u03c3(z) = 0 for all z \u2264 0 and \u03c3(z) is piece-wise continuous on R. Then there exists a network architecture fD and a distribution satisfying assumptions in Theorem 1 or 2 such that with probability one, the empirical loss L\u0302n(\u03b8; p), p \u2265 2 has a local minima \u03b8\u2217 = (\u03b8\u2217S ,\u03b8\u2217D) satisfying R\u0302n(\u03b8 \u2217) \u2265 min{n+,n\u2212}n , where n+ and n\u2212 are the number of positive and negative samples, respectively.\nRemark: (i) We note here that the above result holds in the over-parametrized case, where the number of neurons in the network fS is larger than the number of samples in the dataset. In addition, all counterexamples shown in Section 4.1 hold in the over-parametrized case. (ii) We note here that applying the same analysis, we can generalize the above result to a larger class of neurons satisfying the following condition: there exists a scalar z1 such that \u03c3(z) = constant for all z \u2264 z1 and \u03c3(z) is piece-wise continuous on R. (iii) We note that the training error is strictly non-zero when the dataset has both positive and negative samples and this can happen with probability at least 1\u2212 e\u2212\u2126(n). Leaky-ReLU class contains neurons with \u03c3(z) = z for all z \u2265 0 and \u03c3(z) is piece-wise continuous on R. Some commonly used neurons in this class include ReLU, i.e., max{z, 0}, leaky rectified linear unit (Leaky-ReLU), i.e., \u03c3(z) = z for z \u2265 0, \u03c3 = \u03b1z for z \u2264 0 and some constant \u03b1 \u2208 (0, 1), exponential linear unit (ELU), i.e., \u03c3(z) = z for z \u2265 0, \u03c3(z) = \u03b1(exp(z)\u2212 1) for z \u2264 0 and some constant \u03b1 < 0."}, {"heading": "Theorem Softplus ReLU Leaky-ReLU Sigmoid Quadratic", "text": "We can see that all neurons in this class do not satisfy assumptions in Theorem 1, while some neurons in this class satisfy the condition in Theorem 2 (e.g., linear neuron, \u03c3(z) = z) and some neurons do not (e.g., ReLU). In Proposition 2, we have provided a counterexample showing that Theorem 2 does not hold for some neurons in this class (e.g., ReLU). Next, we will present the following proposition to show that when the network fS consists of neurons in the Leaky-ReLU class, even if all other conditions in Theorem 1 are satisfied, the empirical loss function is likely to have a local minimum with non-zero training error with high probability.\nProposition 3 Suppose that Assumption 1 and 4 are satisfied. Assume that neurons in the network fS satisfy that \u03c3(z) = z for all z \u2265 0 and \u03c3(z) is piece-wise continuous on R. Then there exists a network architecture fD and a distribution satisfying assumptions in Theorem 1 such that, with probability at least 1\u2212 e\u2212\u2126(n), the empirical loss L\u0302n(\u03b8; p), p \u2265 2 has a local minima \u03b8\u2217 = (\u03b8\u2217S ,\u03b8\u2217D) with non-zero training error.\nRemark: We note that applying the same proof, we can generalize the above result to a larger class of neurons, i.e., neurons satisfying the condition that there exists two scalars z1 and \u03b1 such that \u03c3(z) = \u03b1(z \u2212 z1) for all z \u2265 0 and \u03c3 is piece-wise continuous on R. In addition, we note that the ReLU neuron (but not all neurons in the ReLU class) satisfies the definition of both ReLU class and Leaky-ReLU class, and therefore both Proposition 2 and 3 hold for the ReLU neuron. Sigmoid class contains neurons with \u03c3(z) + \u03c3(\u2212z) \u2261 constant on R. We list a few commonly adopted neurons in this family: sigmoid neuron, i.e., \u03c3(z) = 1\n1+e\u2212z , hyperbolic tangent neuron, i.e.,\n\u03c3(z) = e z\u22121 ez+1 , arctangent neuron, i.e., \u03c3(z) = tan \u22121(z) and softsign neuron, i.e., \u03c3(z) = z1+|z| . We note that all real odd functions2 satisfy the conditions of the sigmoid class. We can see that none of the above neurons satisfy assumptions in Theorem 1, since neurons in this class satisfy either \u03c3\u2032\u2032(z) + \u03c3\u2032\u2032(\u2212z) \u2261 0 for all z \u2208 R or \u03c3(z) is not twice differentiable. For Theorem 2, we can see that some neurons in this class satisfy the condition in Theorem 2 (e.g., sigmoid neuron) and some neurons do not (e.g., constant neuron \u03c3(z) \u2261 0 for all z \u2208 R). In Proposition 2, we provided a counterexample showing that Theorem 2 does not hold for some neurons in this class (e.g., constant neuron). Next, we present the following proposition showing that when the network fS consists of neurons in the sigmoid class, then there always exists a data distribution satisfying the assumptions in Theorem 1 such that, with a positive probability, the empirical loss has a local minima with non-zero training error.\nProposition 4 Suppose that assumptions 1 and 4 are satisfed. Assume that there exists a constant c \u2208 R such that neurons in the network fS satisfy \u03c3(z)+\u03c3(\u2212z) \u2261 c for all z \u2208 R. Assume that the dataset D has 2n samples. There exists a network architecture fD and a distribution satisfying assumptions in Theorem 1 such that, with a positive probability, the empirical loss function L\u03022n(\u03b8; p), p \u2265 2 has a local minimum \u03b8\u2217 = (\u03b8\u2217S ,\u03b8 \u2217 D) satisfying R\u03022n(\u03b8\n\u2217) \u2265 min{n\u2212,n+}2n , where n+ and n\u2212 denote the number of positive and negative samples in the dataset, respectively.\nRemark: Proposition 4 shows that when the network fS consists of neurons in the sigmoid class, even if all other conditions are satisfied, the results in Theorem 1 does not hold with a positive probability.\n2A real function f : R\u2192 R is an odd function, if f(x) + f(\u2212x) \u2261 0 for all x \u2208 R.\nQuadratic family contains neurons where \u03c3(z) is real analytic and strongly convex on R and has a global minimum at the point z = 0. A simple example of neuron in this family is the quadratic neuron, i.e., \u03c3(z) = z2. It is easy to check that all neurons in this class satisfy the conditions in Theorem 1 but not in Theorem 2. For Theorem 2, we present a counterexample and show that, when the network fS consists of neurons in the quadratic class, even if positive and negative samples are linearly separable, the empirical loss can have a local minimum with non-zero training error.\nProposition 5 Suppose that Assumption 1 and 4 are satisfied. Assume that neurons in fS satisfy that \u03c3 is strongly convex and twice differentiable on R and has a global minimum at z = 0. There exists a network architecture fD and a distribution satisfying assumptions in Theorem 2 such that with probability one, the empirical loss L\u0302n(\u03b8; p), p \u2265 2 has a local minima \u03b8\u2217 = (\u03b8\u2217S ,\u03b8\u2217D) satisfying R\u0302n(\u03b8\n\u2217) \u2265 min{n+,n\u2212}n , where n+ and n\u2212 denote the number of positive and negative samples in the dataset, respectively."}, {"heading": "4.2 Shortcut-like Connections", "text": "In this subsection, we discuss whether the main results still hold if we remove the shortcut-like connections or replace them with the identity shortcut connections used in the residual network [33]. Specifically, we provide two counterexamples and show that the main results do not hold if the shortcut-like connections are removed or replaced with the identity shortcut connections.\nFeed-forward networks. When the shortcut-like connections (i.e., the network fS in Figure 1(b)) are removed, the network architecture can be viewed as a standard feedforward neural network. We provide a counterexample to show that, for a feedforward network with ReLU neurons, even if the other conditions in Theorem 1 or 2 are satisfied, the empirical loss functions is likely to have a local minimum with non-zero training error. In other words, neither Theorem 1 nor 2 holds when the shortcut-like connections are removed.\nProposition 6 Suppose that assumption 1 is satisfied. Assume that the feedforward network f(x;\u03b8) has at least one hidden layer and at least one neuron in each hidden layer. If neurons in the network f satisfy that \u03c3(z) = 0 for all z \u2264 0 and \u03c3(z) is continuous on R, then for any dataset D with n samples, the empirical loss L\u0302n(\u03b8; p), p \u2265 2 has a local minima \u03b8\u2217 with R\u0302n(\u03b8\u2217) \u2265 min{n+,n\u2212}n , where n+ and n\u2212 are the number of positive and negative samples in the dataset, respectively.\nRemark: The result holds for ReLUs, since it is easy to check that the ReLU neuron satisfies the above assumptions.\nIdentity shortcut connections. As we stated earlier, adding shortcut-like connections to a network can improve the loss surface. However, the shortcut-like connections shown in Fig 1(b) are different from some popular shortcut connections used in the real-world applications, e.g., the identity shortcut connections in the residual network. Thus, a natural question arises: do the main results still hold if we use the identity shortcut connections? To address the question, we provide the following counterexample to show that, when we replace the shortcut-like connections with the identity shortcut connections, even if the other conditions in Theorem 1 are satisfied, the empirical loss function is likely to have a local minimum with non-zero training error. In other words, Theorem 1 does not hold for the identity shortcut connections.\nProposition 7 Assume that H : Rd \u2192 Rd is a feedforward neural network parameterized by \u03b8 and all neurons in H are ReLUs. Define a network f : Rd \u2192 R with identity shortcut connections as f(x;a,\u03b8, b) = a>(x + H(x;\u03b8)) + b, a \u2208 Rd, b \u2208 R. Then there exists a distribution PX\u00d7Y satisfying the assumptions in Theorem 1 such that with probability at least 1 \u2212 e\u2212\u2126(n), the empirical loss L\u0302n(a,\u03b8, b; p) = 1 n \u2211n i=1 `(\u2212yif(xi;\u03b8); p), p \u2265 2 has a local minimum with non-zero training error."}, {"heading": "4.3 Loss Functions", "text": "In this subsection, we discuss whether the main results still hold if we change the loss function. We mainly focus on the following two types of surrogate loss functions: quadratic loss and logistic loss. We will show that if the loss function is replaced with the quadratic loss or logistic loss, then neither Theorem 1 nor 2 holds. In addition, we show that when the loss function is the logistic loss and the network is a feedforward neural network, there are no local minima with zero training error in the real parameter space. In Fig. 2(b), we visualize some surrogate loss functions discussed in this subsection. Quadratic loss. The quadratic loss `(z) = (1 + z)2 has been well-studied in prior works. It has been shown that when the loss function is quadratic, under certain assumptions, all local minima of the empirical loss are global minima. However, the global minimum of the quadratic loss does not necessarily have zero misclassification error, even in the realizable case (i.e., the case where there exists a set of parameters such that the network achieves zero misclassification error on the dataset or the data distriubtion). To illustrate this, we provide a simple example where the network is a simplified linear network and the data distribution is linearly separable.\nExample 1 Let the distribution PX\u00d7Y satisfy that P(Y = 1) = P(Y = \u22121) = 0.5, P(X = 5/4|Y = 1) = 1 and PX|Y=\u22121 is a uniform distribution on the interval [0, 1]. For a linear model f(x; a, b) = ax+b, a, b \u2208 R, every global minimum (a\u2217, b\u2217) of the population loss L(a, b) = EX\u00d7Y [(1\u2212Y f(X; a, b))2] satisfies PX\u00d7Y [Y 6= sgn(f(X; a\u2217, b\u2217))] \u2265 1/16.\nRemark: The proof of the above result in Appendix B.7 is very straightforward. We have only provided it there since we are unable to find a reference which explicitly states such a result, but we will not be surprised if this result has been known to others. This example shows that every global minimum of the quadratic loss has non-zero misclassification error, although the linear model is able to achieve zero misclassification error on this data distribution. Similarly, one can easily find datasets under which all global minima of the quadratic loss have non-zero training error.\nIn addition, we provide two examples in Appendix B.8 and show that, when the loss function is replaced with the quadratic loss, even if the other conditions in Theorem 1 or 2 are satisfied, every global minimum of the empirical loss has a training error larger than 1/8 with a positive probability. In other words, our main results do hold for the quadratic loss.\nThe following observation may be of independent interest. Different from the quadratic loss, the loss functions conditioned in Assumption 1 have the following two properties: (i) the minimum empirical loss is zero if and only if there exists a set of parameters achieving zero training error; (ii) every global minimum of the empirical loss has zero training error in the realizable case.\nProposition 8 Let f : Rd \u2192 R denote a feedforward network parameterized by \u03b8 and let the dataset have n samples. When the loss function `p satisfies Assumption 1 and p \u2265 1, we have min\u03b8 L\u0302n(\u03b8; p) = 0 if and only if min\u03b8 R\u0302n(\u03b8) = 0. Furthermore, if min\u03b8 R\u0302n(\u03b8) = 0, every global minimum \u03b8\n\u2217 of the empirical loss L\u0302n(\u03b8; p) has zero training error, i.e., R\u0302n(\u03b8 \u2217) = 0.\nRemark: We note that the network does not need to be a feedforward network. In fact, the same results hold for a large class of network architectures, including both architectures shown in Fig 1. We provide additional analysis in Appendix B.9.\nLogistic loss. The logistic loss `(z) = log2 (1 + e z) is different from the loss functions conditioned in Assumption 1, since the logistic loss does not have a global minimum on R. Here, for the logistic loss function, we show that even if the remaining assumptions in Theorem 1 hold, every critical point is a saddle point. In other words, Theorem 1 does not hold for logistic loss. Additional analysis on Theorem 2 are provided in Appendix B.11.\nProposition 9 Assume that the loss function is the logistic loss, i.e., `(z) = log2(1 + e z). Assume that assumptions 2-5 are satisfied. Assume that samples in the dataset D = {(xi, yi)}ni=1, n \u2265 1 are independently drawn from the distribution PX\u00d7Y . Assume that the number of neurons M in the network fS satisfies M \u2265 2 max{ n\u2206r , r+, r\u2212}, where \u2206r = r \u2212 max{r+, r\u2212}. If \u03b8\u2217 denotes a critical point of the empirical loss L\u0302n(\u03b8), then \u03b8 \u2217 is a saddle point. In particular, there are no local minima.\nRemark: We note here that the result can be generalized to every loss function ` which is real analytic and has a positive derivative on R. Furthermore, we provide the following result to show that when the dataset contains both positive and negative samples, if the loss is the logistic loss, then every critical point of the empirical loss function has non-zero training error.\nProposition 10 Assume the dataset D = {(xi, yi)}ni=1 consists of both positive and negative samples. Assume that f(x;\u03b8) is a feedforward network parameterized by \u03b8. Assume that the loss function is logistic, i.e., `(z) = log2 (1 + e\nz). If the real parameters \u03b8\u2217 denote a critical point of the empirical loss L\u0302n(\u03b8 \u2217), then R\u0302n(\u03b8 \u2217) > 0.\nRemark: We provide the proof in Appendix B.12. The above proposition implies every critical point is either a local minimum with non-zero training error or is a saddle point (also with non-zero training error). We note here that, similar to Proposition 9, the result can be generalized to every loss function ` that is differentiable and has a positive derivative on R."}, {"heading": "4.4 Open Problem: Datasets", "text": "In this paper, we have mainly considered a class of non-linearly separable distribution where positive and negative samples are located on different subspaces. We show that if the samples are drawn from such a distribution, under certain additional conditions, all local minima of the empirical loss have zero training errors. However, one may ask: how well does the result generalize to other non-linearly separable distributions or datasets? Here, we partially answer this question by presenting the following necessary condition on the dataset so that Theorem 1 can hold.\nProposition 11 Suppose that assumptions 1, 4 and 5 are satisfied. For any feedforward architecture fD(x;\u03b8D), every local minimum \u03b8 \u2217 = (\u03b8\u2217S ,\u03b8 \u2217 D) of the empirical loss function L\u0302n(\u03b8S ,\u03b8D; p), p \u2265 6 satisfies R\u0302n(\u03b8 \u2217) = 0 only if the matrix \u2211n i=1 \u03bbiyixix > i is neither positive nor negative definite for all\nsequences {\u03bbi \u2265 0}ni=1 satisfying \u2211 i:yi=1 \u03bbi = \u2211 i:yi=\u22121 \u03bbi > 0 and \u2016 \u2211n i=1 \u03bbiyixi\u20162 = 0.\nRemark: The proposition implies that when the dataset does not meet this necessary condition, there exists a feedforward architecture fD such that the empirical loss function has a local minimum with a non-zero training error. We use this implication to prove the counterexamples provided in Appendix B.14 when Assumption 2 or 3 on the dataset is not satisfied. Therefore, Theorem 1 no longer holds when Assumption 2 or 3 is removed. We note that the necessary condition shown here is not equivalent to Assumption 2 and 3. Now we present the following result to show the sufficient and necessary condition that the dataset should satisfy so that Proposition 1 can hold.\nProposition 12 Suppose that the loss function `p satisfies Assumption 1 and neurons in the network satisfy Assumption 5. Assume that the single layer network fS(x;\u03b8S) has M > d neurons and assume that neurons in fS are quadratic neurons, i.e., \u03c3(z) = z\n2. For any network architecture fD(x;\u03b8D), every local minimum \u03b8 \u2217 = (\u03b8\u2217S ,\u03b8 \u2217 D) of the empirical loss function L\u0302n(\u03b8S ,\u03b8D; p), p \u2265 6 satisfies R\u0302n(\u03b8 \u2217) = 0 if and only if the matrix \u2211n i=1 \u03bbiyixix > i is indefinite for all sequences {\u03bbi \u2265 0}ni=1\nsatisfying \u2211\ni:yi=1 \u03bbi = \u2211 i:yi=\u22121 \u03bbi > 0.\nRemark: (i) This sufficient and necessary condition implies that for any network architecture fD, there exists a set of parameters \u03b8 = (\u03b8S ,\u03b8D) such that the network f(x;\u03b8) = fS(x;\u03b8S) + fD(x;\u03b8D) can correctly classify all samples in the dataset. This also indicates the existence of a set of parameters achieving zero training error, regardless of the network architecture of fD. We provide the proof in Appendix B.15. (ii) We note that Proposition 12 only holds for the quadratic neuron. The problem of finding the sufficient and necessary conditions for the other types of neurons is open."}, {"heading": "5 Conclusions", "text": "In this paper, we studied the surface of a smooth version of the hinge loss function in binary classification problems. We provided conditions under which the neural network has zero misclassification error at all local minima and also provide counterexamples to show that when some of these assumptions are relaxed, the result may not hold. Further work involves exploiting our results to design efficient training algorithms classification tasks using neural networks."}, {"heading": "A Additional Results in Section 3", "text": ""}, {"heading": "A.1 Proof of Lemma 1", "text": "Lemma 1 (Necessary condition.) Assume that neurons \u03c3 in the network fS are twice differentiable and the loss function ` : R\u2192 R has a continuous derivative on R up to the third order. If n \u2265 1 and parameters \u03b8\u2217 = (\u03b8\u2217S ,\u03b8 \u2217 D) denote a local minimum of the loss function L\u0302n(\u03b8), then for any j = 1, ...,M ,\nn\u2211 i=1 `\u2032(\u2212yif(xi;\u03b8\u2217))yi\u03c3\u2032(w\u2217j>xi)xi = 0d.\nProof: We first recall some notations defined in the paper. The output of the neural network is\nf(x;\u03b8) = fS(x;\u03b8S) + fD(x;\u03b8D),\nwhere fS(x;\u03b8S) is the single layer neural network parameterized by \u03b8S , i.e.,\nfS(x;\u03b8S) = a0 + M\u2211 j=1 aj\u03c3 ( w>j x ) ,\nand fD(x;\u03b8D) is a deep neural network parameterized by \u03b8D. The empirical loss function is given by\nL\u0302n(\u03b8) = L\u0302n(\u03b8S ,\u03b8D) = 1\nn n\u2211 i=1 `(\u2212yif(xi;\u03b8)).\nSince the loss function ` has a continuous derivative on R up to the third order, neurons \u03c3 in the network fS are twice differentiable, then the gradient vector \u2207\u03b8S L\u0302n(\u03b8\u2217S ,\u03b8\u2217D) and the Hessian matrix \u22072\u03b8S L\u0302n(\u03b8 \u2217 S ,\u03b8 \u2217 D) exists. Furthermore, by the assumption that \u03b8 \u2217 = (\u03b8\u2217S ,\u03b8 \u2217 D) is a local minima of the loss function L\u0302n(\u03b8), then we should have for j = 1, ...,M ,\n0d = \u2207wjLn(\u03b8\u2217) = n\u2211 i=1 `\u2032(\u2212yif(xi;\u03b8\u2217))(\u2212yi\u2207wjf(xi;\u03b8\u2217))\n= n\u2211 i=1 `\u2032(\u2212yif(xi;\u03b8\u2217))(\u2212yia\u2217j\u03c3\u2032(w\u2217j>xi)xi)\n= \u2212a\u2217j n\u2211 i=1 `\u2032(\u2212yif(xi;\u03b8\u2217))yi\u03c3\u2032(w\u2217j>xi)xi. (1)\nNow we need to prove that if \u03b8\u2217 is a local minima, then\n\u2200j \u2208 [M ], \u2225\u2225\u2225\u2225\u2225 n\u2211 i=1 `\u2032(\u2212yif(xi;\u03b8\u2217))yi\u03c3\u2032(w\u2217j>xi)xi \u2225\u2225\u2225\u2225\u2225\n2\n= 0.\nWe prove it by contradiction. Assume that there exists j \u2208 [M ] such that\u2225\u2225\u2225\u2225\u2225 n\u2211 i=1 `\u2032(\u2212yif(xi;\u03b8\u2217))yi\u03c3\u2032(w\u2217j>xi)xi \u2225\u2225\u2225\u2225\u2225\n2\n6= 0.\nThen by equation (1), we have a\u2217j = 0. Now, we consider the following Hessian matrix H(aj ,wj). Since \u03b8\u2217 is a local minima of the loss function L\u0302n(\u03b8), then the matrix H(aj ,wj) should be positive semidefinite at (a\u2217j ,w \u2217 j ). By a \u2217 j = 0, we have\n\u22072wjLn(\u03b8\u2217) = \u2212a\u2217j\u2207wj [ n\u2211 i=1 `\u2032(\u2212yif(xi;\u03b8\u2217))yi\u03c3\u2032(w\u2217j>xi)xi ] = 0d\u00d7d,\n\u2202 [ \u2207wjLn(\u03b8\u2217) ] \u2202aj = \u2212 n\u2211 i=1 `\u2032(\u2212yif(xi;\u03b8\u2217))yi\u03c3\u2032(w\u2217j>xi)xi\n\u2212 a\u2217j \u2202\n\u2202aj [ n\u2211 i=1 `\u2032(\u2212yif(xi;\u03b8\u2217))yi\u03c3\u2032(w\u2217j>xi)xi ]\n= \u2212 n\u2211 i=1 `\u2032(\u2212yif(xi;\u03b8\u2217))yi\u03c3\u2032(w\u2217j>xi)xi.\nIn addition, we have\n\u22022Ln(\u03b8 \u2217)\n\u2202a2j =\n\u2202\n\u2202aj [ n\u2211 i=1 `\u2032(\u2212yif(xi;\u03b8\u2217))(\u2212yi\u03c3(w\u2217j>xi)) ]\n= n\u2211 i=1 `\u2032\u2032(\u2212yif(xi;\u03b8\u2217))\u03c32(w\u2217j>xi).\nSince the matrix H(a\u2217j ,w \u2217 j ) is positive semidefinite, then for any \u03b1 \u2208 R and \u03c9 \u2208 Rd,\n( \u03b1 \u03c9> ) H(a\u2217j ,w \u2217 j ) ( \u03b1 \u03c9 ) \u2265 0.\nSince\n( \u03b1 \u03c9> ) H(a\u2217j ,w \u2217 j ) ( \u03b1 \u03c9 ) = \u03b12 n\u2211 i=1 `\u2032\u2032(\u2212yif(xi;\u03b8\u2217))\u03c32(w\u2217j>xi)\n\u2212 \u03b1\u03c9> n\u2211 i=1 `\u2032(\u2212yif(xi;\u03b8\u2217))yi\u03c3\u2032(w\u2217j>xi)xi,\nand by setting\n\u03c9 = n\u2211 i=1 `\u2032(\u2212yif(xi;\u03b8\u2217))yi\u03c3\u2032(w\u2217j>xi)xi,\nthen\n( \u03b1 \u03c9> ) H(a\u2217j ,w \u2217 j ) ( \u03b1 \u03c9 ) = \u03b12 n\u2211 i=1 `\u2032\u2032(\u2212yif(xi;\u03b8\u2217))\u03c32(w\u2217j>xi)\n\u2212 \u03b1 \u2225\u2225\u2225\u2225\u2225 n\u2211 i=1 `\u2032(\u2212yif(xi;\u03b8\u2217))yi\u03c3\u2032(w\u2217j>xi)xi \u2225\u2225\u2225\u2225\u2225 2\n2\n.\nFurthermore, since we assume that\u2225\u2225\u2225\u2225\u2225 n\u2211 i=1 `\u2032(\u2212yif(xi;\u03b8\u2217))yi\u03c3\u2032(w\u2217j>xi)xi \u2225\u2225\u2225\u2225\u2225 2\n2\n> 0,\nthen clearly, there exists \u03b1 such that\n( \u03b1 \u03c9> ) H(a\u2217j ,w \u2217 j ) ( \u03b1 \u03c9 ) < 0.\nand this leads to the contradiction. Thus, we proved the lemma."}, {"heading": "A.2 Proof of Theorem 1", "text": "Theorem 3 Assume that the loss function `p satisfies assumption 1, the distribution PX\u00d7Y satisfies assumption 2 and 3, the network architecture satisfies assumption 4 and neurons in the network satisfy assumption 5. Assume that samples in the dataset D = {(xi, yi)}ni=1, n \u2265 1 are independently drawn from the distribution PX\u00d7Y . Assume that the number of neurons M in the network fS satisfies M \u2265 2 max{ n\u2206r , r+, r\u2212}, where \u2206r = r \u2212max{r+, r\u2212}. If the real parameters \u03b8\u2217 = (\u03b8\u2217S ,\u03b8\u2217D) denote a local minimum of the loss function L\u0302n(\u03b8S ,\u03b8D; p) and p \u2265 6, then R\u0302n(\u03b8\u2217S ,\u03b8\u2217D) = 0 holds with probability one.\nProof: We first present some notations used in this proof. The output of the neural network is\nf(x;\u03b8) = fS(x;\u03b8S) + fD(x;\u03b8D),\nwhere fS(x;\u03b8S) is the single layer neural network parameterized by \u03b8S , i.e.,\nfS(x;\u03b8S) = a0 + M\u2211 j=1 aj\u03c3 ( w>j x ) ,\nand fD(x;\u03b8D) is a deep neural network parameterized by \u03b8D. The empirical loss function is given by\nL\u0302n(\u03b8; p) = L\u0302n(\u03b8S ,\u03b8D; p) = 1\nn n\u2211 i=1 `p(\u2212yif(xi;\u03b8))\nWe first assume that the real parameters \u03b8\u2217 = (\u03b8\u2217S ,\u03b8 \u2217 D) denote a local minima of the loss function L\u0302n(\u03b8; p). Next, we prove the following two claims: Claim 1: If \u03b8\u2217 = (\u03b8\u2217S ,\u03b8 \u2217 D) is a local minima and there exists j \u2208 [M ] such that a\u2217j = 0, then R\u0302n(\u03b8 \u2217) = 0. Claim 2: If \u03b8\u2217 = (\u03b8\u2217S ,\u03b8 \u2217 D) is a local minima and a \u2217 j 6= 0 for all j \u2208 [M ], then R\u0302n(\u03b8\u2217) = 0. (a) Proof of claim 1. We prove that if \u03b8\u2217 = (\u03b8\u2217S ,\u03b8 \u2217 D) is a local minima of the loss function L\u0302n(\u03b8; p) and there exists j \u2208 [M ] such that a\u2217j = 0, then R\u0302n(\u03b8\u2217) = 0. Without loss of generality, we assume that a\u22171 = 0. Since \u03b8 \u2217 = (\u03b8\u2217S ,\u03b8 \u2217 D) is a local minima, then there exists \u03b50 > 0 such that for all small perturbations \u2206a1, \u2206w1 on the parameters a \u2217 1 and w \u2217 1, i.e., |\u2206a1|2 + \u2016\u2206w1\u201622 \u2264 \u03b520, we have\nL\u0302n(\u03b8\u0303S ,\u03b8 \u2217 D; p) \u2265 L\u0302n(\u03b8\u2217S ,\u03b8\u2217D; p),\nwhere \u03b8\u0303S = (a\u03030, a\u03031, ..., a\u0303M , w\u03031, ..., w\u0303M ), a\u03031 = a \u2217 1 + \u2206a1, w\u03031 = w \u2217 1 + \u2206w1 and a\u0303j = a \u2217 j , w\u0303j = w \u2217 j for j 6= 1. Now we consider the Taylor expansion of L\u0302n(\u03b8\u0303S ,\u03b8\u2217D; p) at the point \u03b8\u2217 = (\u03b8\u2217S ,\u03b8\u2217D). We note here that the Taylor expansion of L\u0302(\u03b8S ,\u03b8 \u2217 D; p) on \u03b8S always exists, since the empirical loss function L\u0302n has continuous derivatives with respect to fS up to the p-th order and the output of the neural network f(x;\u03b8S) is infinitely differentiable with respect to \u03b8S due to the fact that neuron activation function \u03c3 is real analytic. We first calculate the first order derivatives at the point \u03b8\u2217,\ndL\u0302n(\u03b8 \u2217; p)\nda1 =\n1\nn n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))(\u2212yi)\u03c3 ( w\u22171 >xi ) = 0, \u03b8\u2217 is a critical point,\n\u2207w1L\u0302n(\u03b8\u2217; p) = a\u22171 n n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))(\u2212yi)\u03c3\u2032 ( w\u22171 >xi ) xi = 0d, \u03b8 \u2217 is a critical point.\nNext, we calculate the second order derivatives at the point \u03b8\u2217,\nd2L\u0302n(\u03b8 \u2217; p)\nda21 =\n1\nn N\u2211 i=1 `\u2032\u2032p(\u2212yif(xi;\u03b8\u2217))\u03c32 ( w\u22171 >xi ) \u2265 0,\nd\nda1 (\u2207w1L\u0302n(\u03b8\u2217; p)) =\n1\nn n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))(\u2212yi)\u03c3\u2032 ( w\u22171 >xi ) xi\n+ a\u22171 n n\u2211 i=1 `\u2032\u2032p(\u2212yif(xi;\u03b8\u2217))\u03c3 ( w\u22171 >xi ) \u03c3\u2032 ( w\u22171 >xi ) xi\n= 0d,\nwhere the first term equals to the zero vector by the necessary condition for a local minima presented in Lemma 1 and the second term equals to the zero vector by the assumption that a\u22171 = 0. Furthermore, by the assumption that a\u22171 = 0, we have\n\u22072w1L\u0302n(\u03b8\u2217; p) = a\u22171 n \u2207w1 [ n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))(\u2212yi)\u03c3\u2032 ( w\u22171 >xi ) xi ] = 0d\u00d7d.\nNow, we further calculate the third order derivatives\nd\nda1\n[ \u22072w1L\u0302n(\u03b8\u2217; p) ] = 1\nn\nd\nda1\n[ a\u22171\u2207w1 [ n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))(\u2212yi)\u03c3\u2032 ( w\u22171 >xi ) xi ]]\n= \u2207w1\n[ 1\nn n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))(\u2212yi)\u03c3\u2032 ( w\u22171 >xi ) xi ] + 0d\u00d7d by a \u2217 1 = 0\n= 1\nn n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))(\u2212yi)\u03c3\u2032\u2032 ( w\u22171 >xi ) xix > i\n+ a\u22171 n n\u2211 i=1 `\u2032\u2032p(\u2212yif(xi;\u03b8\u2217)) [ \u03c3\u2032 ( w\u22171 >xi )]2 xix > i\n= 1\nn n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))(\u2212yi)\u03c3\u2032\u2032 ( w\u22171 >xi ) xix > i by a \u2217 1 = 0\nand\n\u22073w1L\u0302n(\u03b8\u2217; p) = a\u22171 n \u22072w1 [ n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))(\u2212yi)\u03c3\u2032 ( w\u22171 >xi ) xi ] = 0d\u00d7d\u00d7d.\nIn fact, it is easy to show that for any 2 \u2264 k \u2264 p,\n\u2207kw1L\u0302n(\u03b8\u2217; p) = a\u22171 n \u2207k\u22121w1 [ n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))(\u2212yi)\u03c3\u2032 ( w\u22171 >xi ) xi ] = 0d\u00d7 d\u00d7 ...\u00d7 d\ufe38 \ufe37\ufe37 \ufe38\nk times\n.\nLet \u03b5 > 0, |\u2206a1| = \u03b59/4 and \u2206w1 = \u03b5u1 for u1 : \u2016u1\u20162 = 1. Clearly, when \u03b5\u2192 0, \u2206a1 = o(\u2016\u2206w1\u20162), \u2206a1 = o(1) and \u2016\u2206w1\u2016 = o(1). Then we expand L\u0302n(\u03b8\u0303; p) at the point \u03b8\u2217 up to the sixth order and\nthus as \u03b5\u2192 0,\nL\u0302n(\u03b8\u0303; p) = L\u0302n(\u03b8 \u2217; p) +\n1\n2!\nd2L\u0302n(\u03b8 \u2217; p)\nd2a1 (\u2206a1)\n2\n+ 1\n2 \u2206a1\u2206w\n> 1\nd\nda1\n[ \u22072w1L\u0302n(\u03b8\u2217; p) ] \u2206w1 + o(|\u2206a1|2) + o(|\u2206a1|\u2016\u2206w1\u201622) + o(\u2016\u2206w1\u201652)\n= L\u0302n(\u03b8 \u2217) +\n1\n2!\nd2L\u0302n(\u03b8 \u2217; p)\nd2a1 \u03b59/2\n+ 1\n2n sgn(\u2206a1)\u03b5 9/4+2 n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))(\u2212yi)\u03c3\u2032\u2032 ( w\u22171 >xi ) (u>1 xi) 2\n+ o(\u03b59/2) + o(\u03b59/4+2) + o(\u03b55)\n= L\u0302n(\u03b8 \u2217) +\n1\n2n sgn(\u2206a1)\u03b5 17/4 n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))(\u2212yi)\u03c3\u2032\u2032 ( w\u22171 >xi ) (u>1 xi) 2 + o(\u03b517/4).\nSince \u03b5 > 0 and L\u0302n(\u03b8\u0303; p) \u2265 L\u0302n(\u03b8\u2217; p) holds for any u1 : \u2016u1\u20162 = 1 and any sgn(\u2206a1) \u2208 {\u22121, 1}, then n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))(\u2212yi)\u03c3\u2032\u2032 ( w\u22171 >xi ) (u>xi) 2 = 0, for any u \u2208 Rd. (2) Therefore, n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))(\u2212yi)\u03c3\u2032\u2032 ( w\u22171 >xi ) xix > i = 0d\u00d7d. By assumption that there exists a set of orthogonal basis E = {e1, ..., ed} in Rd and a subset U+ \u2286 E such that PX|Y (X \u2208 Span(U1)|Y = 1) = 1 and by assumption that r = |U+ \u222a U\u2212| > max{r+, r\u2212} = max{|U+|, |U\u2212|}, then the set U+\\U\u2212 is not an empty set. It is easy to show that for any vector v \u2208 U+\\U\u2212, PX\u00d7Y (v>X = 0|Y = 1) = 0. We prove it by contradiction. If we assume p = PX\u00d7Y (v>X = 0|Y = 1) > 0, then for random vectors X1, ...,X|U+| independently drawn from the conditional distribution PX|Y=1,\nPX|Y=1 |U+|\u22c3 i=1 { v>Xi = 0 } \u2223\u2223\u2223\u2223\u2223Y = 1  = |U+|\u220f i=1 PX|Y=1 ( v>Xi = 0|Y = 1 ) = p|U+| > 0.\nFurthermore, since X1, ...,X|U+| \u2208 Span(U+), v>Xi = 0, i = 1, ..., |U+| and v \u2208 U+, then the rank of the matrix ( X1, ...,X|U+| ) is at most |U+| \u2212 1 and this indicates that the matrix is not a full rank matrix with probability p|U+| > 0. This leads to the contradiction with the Assumption 2. Thus, with probability 1, v>xi 6= 0 for all i : yi = 1 and v>xi = 0 for all i : yi = \u22121. Therefore, by setting u = v in Equation (2), we have\n0 = \u2212 \u2211 i:yi=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))\u03c3\u2032\u2032(w\u22171>xi)(v>xi)2 \u2264 0,\nwhere the equality holds if and only if \u2200i : yi = 1, `\u2032p(\u2212yif(xi;\u03b8\u2217)) = 0 and this further indicates that \u2200i : yi = 1, yif(xi;\u03b8\u2217) \u2265 z0 > 0. Furthermore, since \u03b8\u2217 is a critical point and thus\n0 = dL\u0302n(\u03b8\n\u2217; p)\nda0 =\n1\nn n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))(\u2212yi) = \u2212 1 n \u2211 i:yi=1 `\u2032p(\u2212yif(xi;\u03b8\u2217)) + 1 n \u2211 i:yi=\u22121 `\u2032p(\u2212yif(xi;\u03b8\u2217))\n= 1\nn \u2211 i:yi=\u22121 `\u2032p(\u2212yif(xi;\u03b8\u2217)).\nTherefore, \u2200i : yi = \u22121, yif(xi;\u03b8\u2217) \u2265 z0 > 0 and this indicates that R\u0302n(\u03b8\u2217) = 0. Proof of Claim 2: First, we define M0 = dM/2e, then\nM0 \u2265 max{r+, r\u2212}.\nIn addition, since r = |U+ \u222a U\u2212|, then max{r+, r\u2212}+ min{r+, r\u2212} \u2265 r. Therefore,\n2M0 \u2265 2 max{r+, r\u2212} > 2r \u2212 r+ \u2212 r\u2212 \u2265 2 min{r \u2212 r+, r \u2212 r\u2212} , 2K,\nwhere we define K = min{r \u2212 r+, r \u2212 r\u2212}. Since in claim 2, we assume that a\u2217j 6= 0 for all j \u2208 [M ], then there exists ai1 , ..., aiM0 , i1 < i2 < ... < iM0 having the same sign, i.e.,\nsgn(ai1) = ... = sgn(aiM0 ).\nWithout loss of generality, we assume that sgn(a1) = ... = sgn(aM0) = +1. Now we prove the claim 2. First, we consider the Hessian matrix H(w\u22171, ...,w \u2217 M0 ). Since \u03b8\u2217 is a local minima with R\u0302n(\u03b8 \u2217) > 0, then the inequality\nF (u1, ...,uM0) = M0\u2211 j=1 M0\u2211 k=1 u>j \u22072wj ,wk L\u0302n(\u03b8 \u2217; p)uk \u2265 0\nholds for all vectors u1, ...,uM0 \u2208 Rd. Since\n\u22072wj L\u0302n(\u03b8\u2217; p) = a\u2217j n n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))(\u2212yi)\u03c3\u2032\u2032 ( w\u2217j >xi ) xix > i\n+ a\u2217j 2\nn n\u2211 i=1 `\u2032\u2032p(\u2212yif(xi;\u03b8\u2217)) [ \u03c3\u2032 ( w\u2217j >xi )]2 xix > i ,\nand\n\u22072wj ,wk L\u0302n(\u03b8 \u2217; p) =\na\u2217ja \u2217 k\nn n\u2211 i=1 `\u2032\u2032p(\u2212yif(xi;\u03b8\u2217)) [ \u03c3\u2032 ( w\u2217j >xi )] [ \u03c3\u2032 ( w\u2217k >xi + b \u2217 k )] xix > i .\nThus, we have for any u1, ...,uM0 \u2208 Rd,\nF (u1, ...,uM0) = \u2212 1\nn M0\u2211 j=1 [ a\u2217j n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))yi\u03c3\u2032\u2032 ( w\u2217j >xi )( u>j xi )2]\n+ 1\nn M0\u2211 j=1 M0\u2211 k=1 [ a\u2217ja \u2217 k n\u2211 i=1 `\u2032\u2032p(\u2212yif(xi;\u03b8\u2217))\u03c3\u2032 ( w\u2217j >xi ) \u03c3\u2032 ( w\u2217k >xi + b \u2217 k )( u>j xi )( u>k xi )]\n= \u2212 1 n n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))yi M0\u2211 j=1 [ a\u2217j\u03c3 \u2032\u2032 ( w\u2217j >xi )( u>j xi )2] + 1\nn n\u2211 i=1 `\u2032\u2032p(\u2212yif(xi;\u03b8\u2217)) M0\u2211 j=1 a\u2217j\u03c3 \u2032 ( w\u2217j >xi )( u>j xi )2 . Now we find some coefficients \u03b11, ..., \u03b1M0 , not all zero, and vectors u1, ...,uM0 , not all zero vector, satisfying\nM0\u2211 j=1 \u03b1j\u03c3 \u2032 ( w\u2217j >xi ) u>j xi = 0, \u2200i \u2208 [n],\nand \u2200i : yi = \u22121 and \u2200j \u2208 [M0], u>j xi = 0.\nWe note here that if sgn(a1) = ... = sgn(aM0) = \u22121, then we need to find coefficients \u03b11, ..., \u03b1M0 , not all zero, and vectors u1, ...,uM0 , not all zero vector, satisfying\nM0\u2211 j=1 \u03b1j\u03c3 \u2032 ( w\u2217j >xi ) u>j xi = 0, \u2200i \u2208 [n],\nand \u2200i : yi = 1 and \u2200j \u2208 [M0], u>j xi = 0. Since \u03b8\u2217 is a local minima, then by Lemma 1, we have\nn\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))yi\u03c3\u2032(w\u2217j>xi)xi = 0d. (3)\nFurthermore, by the assumption that K = r\u2212max{r+, r\u2212} > 0, then the set U+\\U\u2212 is not an empty set. Thus, for \u2200v \u2208 U+\\U\u2212 \u2282 E , with probability 1, \u2200i : yi = \u22121, v>xi = 0. In addition, by the analysis presented in the proof of claim 1, we have that with probability 1, v>xi 6= 0 for all i : yi = 1. Since\nK = r\u2212max{r+, r\u2212} = |U+ \u222aU\u2212| \u2212max{|U+|, |U\u2212|} = |U+\\U\u2212|+ |U\u2212| \u2212max{|U+|, |U\u2212|} \u2264 |U+\\U\u2212|,\nthen without loss of generality, we assume that {e1, ..., eK} \u2286 U+\\U\u2212 and U+ = {e1, ..., er+}. Thus, with probability 1, \u2200j \u2208 [K], \u2200i : yi = \u22121, e>j xi = 0 and \u2200i : yi = 1, e>j xi 6= 0. Then by Equation (3), now we consider the following set of linear equations\nn\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))yi\u03c3\u2032(w\u22171>xi) ( e>1 xi ) = 0, ..., n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))yi\u03c3\u2032(w\u2217M0 >xi + b \u2217 M0) ( e>1 xi ) = 0,\n... n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))yi\u03c3\u2032(w\u22171>xi) ( e>Kxi ) = 0, ..., n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))yi\u03c3\u2032(w\u2217M0 >xi + b \u2217 M0) ( e>Kxi ) = 0.\nThese equations can be rewritten in a matrix form \u03c3\u2032(w\u22171 >x1) ( e>1 x1 ) ... \u03c3\u2032(w\u22171 >xn) ( e>1 xn ) ... ... ... \u03c3\u2032(w\u2217M0 >x1 + b \u2217 M0 ) ( e>1 x1 ) ... \u03c3\u2032(w\u2217M0 >xn + b \u2217 M0 ) ( e>1 xn ) ... ... ... \u03c3\u2032(w\u22171 >x1) ( e>Kx1 ) ... \u03c3\u2032(w\u22171 >xn) ( e>Kxn ) ... ... ...\n\u03c3\u2032(w\u2217M0 >x1 + b \u2217 M0 ) ( e>Kx1 ) ... \u03c3\u2032(w\u2217M0 >xn + b \u2217 M0 ) ( e>Kxn )  (KM0\u00d7n)\ufe38 \ufe37\ufe37 \ufe38\nP\n `\u2032p(\u2212y1f(x1;\u03b8\u2217))y1 `\u2032p(\u2212y2f(x2;\u03b8\u2217))y2 ... ... ... ...\n... `\u2032p(\u2212ynf(x1;\u03b8\u2217))yn  \ufe38 \ufe37\ufe37 \ufe38\nq\n= 0n\nor Pq = 0n.\nSince M \u2265 2n\u2206r = 2nK , then M0K \u2265 MK/2 \u2265 n. Clearly, if rank(P ) = n, we should have q = 0n and this indicates that `\u2032p(\u2212yif(xi;\u03b8\u2217)) = 0 for all i \u2208 [n] or R\u0302n(\u03b8\u2217) = 0. Thus, we only need to consider\nthe case where rank(P ) < n \u2264M0K. This means the raw vectors of the matrix P is linearly dependent and thus there exists coefficients vectors (\u03b211, ..., \u03b21K), ..., (\u03b2M01, ..., \u03b2M0K), not all zero vectors, such that\nK\u2211 s=1 M0\u2211 j=1 \u03c3\u2032(w\u2217j >xi)\u03b2js(e > s xi) = 0, \u2200i \u2208 [n],\nor M0\u2211 j=1 a\u2217j\u03c3 \u2032(w\u2217j >xi)\n( 1\na\u2217j K\u2211 s=1 \u03b2jses\n)> xi = 0, \u2200i \u2208 [n],\nby assumption that a\u2217j 6= 0 for all j = 1, ...,M0. Define uj = 1a\u2217j \u2211K s=1 \u03b2jses for j = 1, ...,M0, then we have M0\u2211 j=1 a\u2217j\u03c3 \u2032(w\u2217j >xi)u > j xi = 0, \u2200i \u2208 [n]. (4)\nFurthermore, since uj \u2208 Span({e1, ..., eK}) and with probability 1, e>j xi = 0, for \u2200i : yi = \u22121, \u2200j \u2208 [K], then \u2200j \u2208 [M ], \u2200i : yi = \u22121, u>j xi = 0. Thus, by setting uj = 1a\u2217j \u2211K s=1 \u03b2jses for j = 1, ...,M0, then we have\nF (u1, ...,uM0) = \u2212 1\nn n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))yi M0\u2211 j=1 [ a\u2217j\u03c3 \u2032\u2032 ( w\u2217j >xi )( u>j xi )2] + 1\nn n\u2211 i=1 `\u2032\u2032p(\u2212yif(xi;\u03b8\u2217)) M0\u2211 j=1 a\u2217j\u03c3 \u2032 ( w\u2217j >xi )( u>j xi )2 = \u2212 1\nn n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))yi M0\u2211 j=1 [ a\u2217j\u03c3 \u2032\u2032 ( w\u2217j >xi )( u>j xi )2] by Eq. (4) = \u2212 1\nn \u2211 i:yi=1 `\u2032p(\u2212yif(xi;\u03b8\u2217)) M0\u2211 j=1 [ a\u2217j\u03c3 \u2032\u2032 ( w\u2217j >xi )( u>j xi )2] \u2265 0. (5) In addition, since \u03c3\u2032\u2032(z) > 0 for all z \u2208 R and a\u2217j > 0 for all j \u2208 [M0], then we have\n`\u2032p(\u2212yif(xi;\u03b8\u2217)) M0\u2211 j=1 [ a\u2217j\u03c3 \u2032\u2032 ( w\u2217j >xi )( u>j xi )2] \u2265 0, \u2200i : yi = 1\nand this leads to F (u1, ...,uM0) \u2264 0.\nTogether with Eq. (5), we have F (u1, ...,uM0) = 0,\nand thus\n`\u2032p(\u2212yif(xi;\u03b8\u2217)) M0\u2211 j=1 [ a\u2217j\u03c3 \u2032\u2032 ( w\u2217j >xi )( u>j xi )2] = 0, \u2200i : yi = 1. (6)\nNow we split the index {1, ..., n} set into two disjoint subset C0, C1:\nC0 = {i \u2208 [n] : yi = 1, and \u2203j \u2208 [M0],u>j xi 6= 0}, C1 = {i \u2208 [n] : yi = 1 and \u2200j \u2208 [M0],u>j xi = 0}.\nClearly, for all i \u2208 C0, by the fact that a\u2217j > 0 for all j \u2208 [M0] and \u03c3\u2032\u2032(z) > 0 for all z \u2208 R, we have\nM0\u2211 j=1 [ a\u2217j\u03c3 \u2032\u2032 ( w\u2217j >xi )( u>j xi )2] > 0,\nand by Equation (6), we have `\u2032p(\u2212yif(xi;\u03b8\u2217)) = 0, \u2200i \u2208 C0.\nNow we need to consider the index set C1. First, we show that the following inequality holds with probability 1, |C1| < r+ \u2264 max{r+, r\u2212}. Since uj =\n1 a\u2217j\n\u2211K i=1 \u03b2jses for j = 1, ...,M0 and coefficient vectors (\u03b211, ..., \u03b21K), ..., (\u03b2M01, ..., \u03b2M0K) are\nnot all zero vectors, then the there exists a j0 \u2208 [K] such that the non-zero vector uj0 satisfy u>j0xi = 0 for all i \u2208 C1 and uj0 \u2208 Span({e1, ..., eK}). Furthermore, by assumption U+ = {e1, ..., er+}, thus we have\nu>j0xi = K\u2211 s=1 (u>j0es)(x > i es) = r+\u2211 s=1 (u>j0es)(x > i es) = 0 (7)\nholds for all i \u2208 C1. If |C1| \u2265 r+, then without loss of generality, we assume that {1, ..., r+} \u2286 C1. Thus, with probability 1, the matrix e>1 x1 ... e>r+x1... ... ...\ne>1 xr+ ... e > r+xr+  = x>1... x>r+ (e1 ... er+) has a full rank equal to r+, by the fact that {x1, ..., xr+} \u2282 Span(U+) and ( x1, ..., xr+ ) is a full rank matrix with probability 1. Thus, by Equation (7), we have e>1 x1 ... e>r+x1... ... ... e>1 xr+ ... e > r+xr+  u>j0e1... u>j0er+\n = 0d and this leads to u>j0es = 0 for all s \u2208 [K]. This contradicts with the fact that uj0 \u2208 Span({e1, ..., eK}) and uj0 is not a zero vector. Therefore, |C1| < r+ \u2264 M0. Furthermore, since `\u2032(z) = 0 if and only if z \u2264 \u2212z0 for some positive z0 > 0, then `\u2032\u2032(z) = 0 when z \u2264 \u2212z0. Now we consider the function F , since \u2200i \u2208 C0 : `\u2032p(\u2212yif(xi;\u03b8\u2217)) = 0 and `\u2032\u2032p(\u2212yif(xi;\u03b8\u2217)) = 0, then\nF (u1, ...,uM0) = \u2212 1\nn \u2211 i\u2208C1 `\u2032p(\u2212yif(xi;\u03b8\u2217)) M0\u2211 j=1 [ a\u2217j\u03c3 \u2032\u2032 ( w\u2217j >xi )( u>j xi )2] + 1\nn \u2211 i\u2208C1 `\u2032\u2032p(\u2212yif(xi;\u03b8\u2217)) M0\u2211 j=1 a\u2217j\u03c3 \u2032 ( w\u2217j >xi )( u>j xi )2 \u2265 0 holds for all u1, ...,uM0 \u2208 Span({e1, ..., eK}). Now we set uj = \u03b1je1, j = 1, ...,M0 for some scalar \u03b1j . We only need to find \u03b11, ..., \u03b1M0 such that\nM0\u2211 j=1 \u03b1ja \u2217 j\u03c3 \u2032 ( w\u2217j >xi ) e>1 xi = 0, \u2200i \u2208 C1.\nSince |C1| < r+ \u2264M0, then there exists \u03b1\u22171, ..., \u03b1\u2217M0 , not all zeros, such that\nM0\u2211 j=1 \u03b1\u2217ja \u2217 j\u03c3 \u2032 ( w\u2217j >xi ) e>1 xi = 0, \u2200i \u2208 C1.\nThen by setting uj = \u03b1 \u2217 je1, we have\nF (u1, ...,uM0) = \u2212 1\nn \u2211 i\u2208C1 `\u2032p(\u2212yif(xi;\u03b8\u2217)) M0\u2211 j=1 [ |\u03b1\u2217j |2a\u2217j\u03c3\u2032\u2032 ( w\u2217j >xi )( e>1 xi )2] \u2265 0. .\nSimilarly, since |\u03b11|, ..., |\u03b1M0 | are not all zeros, a\u2217j > 0 for all j \u2208 [M0], \u03c3\u2032\u2032(z) > 0 for all z \u2208 R and e>1 xi 6= 0 holds for all i with probability 1, then\n`\u2032p(\u2212yif(xi;\u03b8\u2217)) = 0, \u2200i \u2208 C1.\nTherefore, this indicates that\n`\u2032p(\u2212yif(xi;\u03b8\u2217)) = 0, \u2200i : yi = 1.\nFurthermore, since \u03b8\u2217 is a local minima and thus\n0 = dL\u0302n(\u03b8\n\u2217; p)\nda0 =\n1\nn n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))(\u2212yi) = \u2212 1 n \u2211 i:yi=1 `\u2032p(\u2212yif(xi;\u03b8\u2217)) + 1 n \u2211 i:yi=\u22121 `\u2032p(\u2212yif(xi;\u03b8\u2217))\n= 1\nn \u2211 i:yi=\u22121 `\u2032p(\u2212yif(xi;\u03b8\u2217)).\nThis means when `\u2032p(\u2212yif(xi;\u03b8\u2217)) = 0 holds for all i : yi = 1, we have `\u2032p(\u2212yif(xi;\u03b8\u2217)) = 0 for all i : yi = \u22121. These two together give us R\u0302n(\u03b8\u2217) = 0. Similarly, when sgn(a1) = ... = sgn(aM0) = \u22121, we have the similar the results. Therefore, \u03b8\u2217 is a local minima with R\u0302n(\u03b8 \u2217) = 0."}, {"heading": "A.3 Proof of Proposition 1", "text": "Proposition 13 Assume that the loss function `p satisfies assumption 1, the distribution PX\u00d7Y satisfies assumption 2 and 3, the network architecture satisfies assumption 4 and neurons in the network satisfy assumption 5. Assume that samples in the dataset D = {(xi, yi)}ni=1, n \u2265 1 are independently drawn from the distribution PX\u00d7Y . Assume that the neuron \u03c3(z) = z2 and the number of neurons M > r. If the real parameters \u03b8\u2217 = (\u03b8\u2217S ,\u03b8 \u2217 D) denote a local minimum of the loss function L\u0302n(\u03b8S ,\u03b8D; p) and p \u2265 6, then R\u0302n(\u03b8\u2217) = L\u0302n(\u03b8\u2217; p) = 0 holds with probability one.\nProof: We first recall some notations defined in the paper. The output of the neural network is\nf(x;\u03b8) = fS(x;\u03b8S) + fD(x;\u03b8D),\nwhere fS(x;\u03b8S) is the single layer neural network parameterized by \u03b8S , i.e.,\nfS(x;\u03b8S) = a0 + M\u2211 j=1 aj\u03c3 ( w>j x ) ,\nand fD(x;\u03b8D) is a deep neural network parameterized by \u03b8D. The empirical loss function is given by\nL\u0302n(\u03b8; p) = L\u0302n(\u03b8S ,\u03b8D; p) = 1\nn n\u2211 i=1 `p(\u2212yif(xi;\u03b8)).\nWe first assume that the \u03b8\u2217 = (\u03b8\u2217S ,\u03b8 \u2217 D) is a local minima. We next prove the following two claims: Claim 1: If \u03b8\u2217 = (\u03b8\u2217S ,\u03b8 \u2217 D) is a local minima and there exists j \u2208 [M ] such that a\u2217j = 0, then R\u0302n(\u03b8 \u2217) = 0. Claim 2: If \u03b8\u2217 = (\u03b8\u2217S ,\u03b8 \u2217 D) is a local minima and a \u2217 j 6= 0 for all j \u2208 [M ], then R\u0302n(\u03b8\u2217) = 0. (a) Proof of claim 1. We prove that if \u03b8\u2217 = (\u03b8\u2217S ,\u03b8 \u2217 D) is a local minima and there exists j \u2208 [M ] such that a\u2217j = 0, then R\u0302n(\u03b8 \u2217) = 0. Without loss of generality, we assume that a\u22171 = 0. Since \u03b8\u2217 = (\u03b8\u2217S ,\u03b8 \u2217 D) is a local minima, then there exists \u03b50 > 0 such that for any small perturbations \u2206a1, \u2206w1 on parameters a \u2217 1 and w \u2217 1, i.e., |\u2206a1|2 + \u2016\u2206w1\u201622 \u2264 \u03b520, we have\nL\u0302n(\u03b8\u0303S ,\u03b8 \u2217 D) \u2265 L\u0303n(\u03b8\u2217S ,\u03b8\u2217D),\nwhere \u03b8\u0303 = (a\u03030, a\u03031, ..., a\u0303M , w\u03031, ..., w\u0303M ), a\u03031 = a \u2217 1 + \u2206a1, w\u03031 = w \u2217 1 + \u2206w1 and a\u0303j = a \u2217 j , w\u0303j = w \u2217 j for j 6= 1. Now we consider Taylor expansion of L\u0303n(\u03b8\u0303S ,\u03b8\u2217D) at (\u03b8\u2217S ,\u03b8\u2217D). We note here that the Taylor expansion of L\u0302(\u03b8S ,\u03b8 \u2217 D; p) on \u03b8S always exists, since the empirical loss function L\u0302n has continuous derivatives with respect to fS up to the p-th order and the output of the neural network f(x;\u03b8S) is infinitely differentiable with respect to \u03b8S due to the fact that neuron activation function \u03c3 is real analytic. We first calculate the first order derivatives at the point (\u03b8\u2217S ,\u03b8 \u2217 D)\ndL\u0302n(\u03b8 \u2217)\nda1 =\n1\nn n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))(\u2212yi)\u03c3 ( w\u22171 >xi ) = 0, \u03b8\u2217 is a critical point,\n\u2207w1L\u0302n(\u03b8\u2217) = a\u22171 n n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))(\u2212yi)\u03c3\u2032 ( w\u22171 >xi ) xi = 0d, \u03b8 \u2217 is a critical point.\nNext, we calculate the second order derivatives at the point (\u03b8\u2217S ,\u03b8 \u2217 D),\nd2L\u0302(\u03b8\u2217)\nda21 =\n1\nn N\u2211 i=1 `\u2032\u2032p(\u2212yif(xi;\u03b8\u2217))\u03c32 ( w\u22171 >xi ) \u2265 0,\nd\nda1 (\u2207w1L(\u03b8\u2217)) =\n1\nn n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))(\u2212yi)\u03c3\u2032 ( w\u22171 >xi ) xi\n+ a\u22171 n n\u2211 i=1 `\u2032\u2032p(\u2212yif(xi;\u03b8\u2217))\u03c3 ( w\u22171 >xi ) \u03c3\u2032 ( w\u22171 >xi ) xi\n= 0d,\nwhere the first term equals to the zero vector by the necessary condition for a local minima presented in Lemma 1 and the second term equals to the zero vector by the assumption that a\u22171 = 0. Furthermore, by the assumption that a\u22171 = 0, we have\n\u22072w1L\u0302n(\u03b8\u2217; p) = a\u22171 n \u2207w1 [ n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8))(\u2212yi)\u03c3\u2032 ( w\u22171 >xi ) xi ] = 0d\u00d7d.\nWe further calculate the third order derivatives\nd\nda1\n[ \u22072w1L\u0302n(\u03b8\u2217; p) ] = d\nda1\n[ a\u22171\u2207w1 [ 1\nn n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8))(\u2212yi)\u03c3\u2032 ( w\u22171 >xi ) xi\n]]\n= \u2207w1\n[ 1\nn n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8))(\u2212yi)\u03c3\u2032 ( w\u22171 >xi ) xi ] + 0d\u00d7d by a \u2217 1 = 0\n= 1\nn n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8))(\u2212yi)\u03c3\u2032\u2032 ( w\u22171 >xi ) xix > i\n+ a\u22171 n n\u2211 i=1 `\u2032\u2032p(\u2212yif(xi;\u03b8)) [ \u03c3\u2032 ( w\u22171 >xi )]2 xix > i\n= 1\nn n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8))(\u2212yi)\u03c3\u2032\u2032 ( w\u22171 >xi ) xix > i by a \u2217 1 = 0\nand\n\u22073w1L\u0302n(\u03b8\u2217; p) = a\u22171\u22072w1\n[ 1\nn n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8))(\u2212yi)\u03c3\u2032 ( w\u22171 >xi ) xi ] = 0d\u00d7d\u00d7d.\nIn fact, it is easy to show that for any 2 \u2264 k \u2264 p,\n\u2207kw1L\u0302n(\u03b8\u2217; p) = a\u22171\u2207k\u22121w1\n[ 1\nn n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8))(\u2212yi)\u03c3\u2032 ( w\u22171 >xi ) xi ] = 0d\u00d7 d\u00d7 ...\u00d7 d\ufe38 \ufe37\ufe37 \ufe38\nk times\n.\nLet \u03b5 > 0, \u2206a1 = sgn(a1)\u03b5 9/4 and \u2206w1 = \u03b5u1 for u1 : \u2016u1\u20162 = 1. Clearly, when \u03b5 \u2192 0, \u2206a1 = o(\u2016\u2206w1\u20162), \u2206a1 = o(1) and \u2016\u2206w1\u2016 = o(1). Then we expand L\u0302n(\u03b8\u0303S ,\u03b8\u2217D) at the point \u03b8\u2217 up to the\nsixth order and thus as \u03b5\u2192 0,\nL\u0302n(\u03b8\u0303S ,\u03b8 \u2217 D) = L\u0302n(\u03b8 \u2217 S ,\u03b8 \u2217 D) +\n1\n2!n\nd2L\u0302n(\u03b8 \u2217)\nd2a1 (\u2206a1)\n2\n+ 1\n2n \u2206a1\u2206w\n> 1\nd\nda1\n[ D2w1L\u0302n(\u03b8 \u2217; p) ] \u2206w1 + o(|a1|2) + o(|a1|\u2016w1\u201622) + o(\u2016\u2206w1\u201652)\n= L\u0302n(\u03b8 \u2217 S ,\u03b8 \u2217 D) +\n1\n2!n\nd2L\u0302n(\u03b8 \u2217)\nd2a1 \u03b59/2 +\n1\n2n sgn(a1)\u03b5 9/4+2 n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8))\u03c3\u2032\u2032 ( w\u22171 >xi ) (u>1 xi) 2\n+ o(\u03b59/2) + o(\u03b59/4+2) + o(\u03b55)\n= L\u0302n(\u03b8 \u2217 S ,\u03b8 \u2217 D) +\n1\n2n sgn(a1)\u03b5 17/4 n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8))(\u2212yi)\u03c3\u2032\u2032 ( w\u22171 >xi ) (u>1 xi) 2 + o(\u03b517/4)\nSince \u03b5 > 0 and L\u0302n(\u03b8\u0303S ,\u03b8 \u2217 D; p) \u2265 L\u0302n(\u03b8\u2217; p) holds for any u1 : \u2016u1\u20162 = 1 and any sgn(a1) \u2208 {\u22121, 1}, then n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8))(\u2212yi)\u03c3\u2032\u2032 ( w\u22171 >xi ) (u>xi) 2 = 0, for any u \u2208 Rd. (8) Therefore, n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8))(\u2212yi)\u03c3\u2032\u2032 ( w\u22171 >xi ) xix > i = 0d\u00d7d.\nBy assumption that there exists a set of orthogonal basis E = {e1, ..., ed} in Rd and a subset U+ \u2286 E such that PX|Y (X \u2208 Span(U1)|Y = 1) = 1 and by assumption that r = |U+ \u222a U\u2212| > max{r+, r\u2212} = max{|U+|, |U\u2212|}, then the set U+\\U\u2212 is not an empty set. It is easy to show that for any vector v \u2208 U+\\U\u2212, PX\u00d7Y (v>X = 0|Y = 1) = 0. Otherwise, if p = PX\u00d7Y (v>X = 0|Y = 1) > 0, then for random vectors X1, ...,X|U+| independently drawn from the conditional distribution PX|Y=1,\nPX|Y=1 |U+|\u22c3 i=1 { v>Xi = 0 } \u2223\u2223\u2223\u2223\u2223Y = 1  = |U+|\u220f i=1 PX|Y=1 ( v>Xi = 0|Y = 1 ) = p|U+| > 0.\nFurthermore, since X1, ...,X|U+| \u2208 Span(U+), v>Xi = 0, i = 1, ..., |U+| and v \u2208 U+, then the rank of the matrix ( X1, ...,X|U+| ) is at most |U+| \u2212 1 and this indicates that the matrix is not a full rank matrix with probability p|U+| > 0. This leads to the contradiction with the Assumption 2. Thus, with probability 1, v>xi 6= 0 for all i : yi = 1 and v>xi = 0 for all i : yi = \u22121. Therefore, by setting u = v in Equation (8), we have\n0 = \u2212 \u2211 i:yi=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))\u03c3\u2032\u2032(w\u22171>xi)(v>xi)2 \u2264 0,\nwhere the equality holds if and only if \u2200i : yi = 1, `\u2032p(\u2212yif(xi;\u03b8\u2217)) = 0 and this further indicates that \u2200i : yi = 1, yif(xi;\u03b8\u2217) \u2265 z0 > 0. Furthermore, since \u03b8\u2217 is a critical point and thus\n0 = dL\u0302n(\u03b8\n\u2217; p)\nda0 =\n1\nn n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))(\u2212yi) = \u2212 1 n \u2211 i:yi=1 `\u2032p(\u2212yif(xi;\u03b8\u2217)) + 1 n \u2211 i:yi=\u22121 `\u2032p(\u2212yif(xi;\u03b8\u2217))\n= 1\nn \u2211 i:yi=\u22121 `\u2032p(\u2212yif(xi;\u03b8\u2217)).\nTherefore, \u2200i : yi = \u22121, yif(xi;\u03b8\u2217) \u2265 z0 > 0 and this indicates that R\u0302n(\u03b8\u2217) = 0.\n(b) Proof of Claim 2: To prove the claim 2, we first prove that if M > r, then there exists coefficients \u03b11, ..., \u03b1M , not all zero, such that\n(\u03b11w \u2217 1 + ...+ \u03b1Mw \u2217 M ) > xi = 0, for all i \u2208 [n].\nSince we assume that U+ \u2286 E and U\u2212 \u2286 E such that PX|Y (X \u2208 Span(U+)|Y = 1) = 1 and PX|Y (X \u2208 Span(U\u2212)|Y = \u22121) = 1, then without loss generality, we assume that xis locate in the linear span of {e1, ..., er} \u2286 {e1, ..., ed} (note that r = |U+ \u222a U\u2212|). Clearly, for any w\u22171, ...,w\u2217M , if M > r, then there exists coefficients \u03b11, ..., \u03b1M , not all zero, such that\n\u03b11w \u2217 1 + ...+ \u03b1Mw \u2217 M \u2208 Span({er+1, ..., ed}), if r < d, \u03b11w \u2217 1 + ...+ \u03b1Mw \u2217 M = 0d, if r = d.\nTherefore, if M > r, then there exists coefficients \u03b11, ..., \u03b1M , not all zero, such that\n(\u03b11w \u2217 1 + ...+ \u03b1Mw \u2217 M ) >xi = 0, for all i \u2208 [n].\nNow we prove the claim 2. First, we consider the Hessian matrix H(w\u22171, ...,w \u2217 M ). Since \u03b8 \u2217 is a local minima, then\nF (u1, ...,uM ) = M\u2211 j=1 M\u2211 k=1 u>j \u22072wj ,wk L\u0302n(\u03b8 \u2217; p)uk \u2265 0\nholds for any vectors u1, ...,uM \u2208 Rd. Since \u03c3\u2032\u2032(z) = 2 and \u03c3\u2032(z) = 2z for all z \u2208 R, then\n\u22072wj L\u0302n(\u03b8\u2217; p) = a\u2217j n n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))(\u2212yi)\u03c3\u2032\u2032 ( w\u2217j >xi ) xix > i\n+ a\u2217j 2\nn n\u2211 i=1 `\u2032\u2032p(\u2212yif(xi;\u03b8\u2217)) [ \u03c3\u2032 ( w\u2217j >xi )]2 xix > i\n= \u2212 2a\u2217j n n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))yixix>i + 4a\u2217j 2 n n\u2211 i=1 `\u2032\u2032p(\u2212yif(xi;\u03b8\u2217)) ( w\u2217j >xi )2 xix > i ,\nand\n\u22072wj ,wk L\u0302n(\u03b8 \u2217; p) =\na\u2217ja \u2217 k\nn n\u2211 i=1 `\u2032\u2032p(\u2212yif(xi;\u03b8\u2217)) [ \u03c3\u2032 ( w\u2217j >xi )] [ \u03c3\u2032 ( w\u2217k >xi )] xix > i\n= 4a\u2217ja \u2217 k\nn n\u2211 i=1 `\u2032\u2032p(\u2212yif(xi;\u03b8\u2217)) ( w\u2217j >xi )( w\u2217k >xi ) xix > i .\nThus, we have\nF (u1, ...,uM ) = \u22122 M\u2211 j=1 [ a\u2217j n n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))yi ( u>j xi )2]\n+ 4 M\u2211 j=1 M\u2211 k=1\n[ a\u2217ja \u2217 k\nn n\u2211 i=1 `\u2032\u2032p(\u2212yif(xi;\u03b8\u2217)) ( w\u2217j >xi )( w\u2217k >xi )( u>j xi )( u>k xi\n)]\n= \u2212 2 n M\u2211 j=1 [ a\u2217j n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))yi ( u>j xi )2]\n+ 4\nn n\u2211 i=1 `\u2032\u2032p(\u2212yif(xi;\u03b8\u2217))  M\u2211 j=1 a\u2217j ( w\u2217j >xi )( u>j xi )2 .\nSince there exists coefficients \u03b11, ..., \u03b1M , not all zero, such that (\u03b11w \u2217 1 + ...+ \u03b1Mw \u2217 M ) >xi = 0, for all i \u2208 [n], and a\u2217j 6= 0 for all j \u2208 [M ] then by setting uj = \u03b1ju/a\u2217j for all j \u2208 [M ], we have that the inequality\nF (u1, ...,uM ) = \u2212 2\nn M\u2211 j=1 [ a\u2217j n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))yi ( \u03b1j/a \u2217 j )2 ( u>xi )2]\n+ 4\nn n\u2211 i=1 `\u2032\u2032p(\u2212yif(xi;\u03b8\u2217))  M\u2211 j=1 \u03b1j ( w\u2217j >xi )( u>xi )2 = \u2212 2\nn M\u2211 j=1 [ a\u2217j n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))yi ( \u03b1j/a \u2217 j )2 ( u>xi )2]\n+ 4\nn n\u2211 i=1 `\u2032\u2032p(\u2212yif(xi;\u03b8\u2217))   M\u2211 j=1 \u03b1jw \u2217 j > xi  2 ( u>xi )2 = \u2212 2\nn M\u2211 j=1 ( \u03b12j/a \u2217 j ) \u00b7 n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))yi ( u>xi )2 \u2265 0\nholds for any u \u2208 Rd. Next we consider the following two cases: (1) \u2211M j=1 ( \u03b12j/a \u2217 j ) 6= 0; (2) \u2211Mj=1 (\u03b12j/a\u2217j) = 0.\nCase 1: If \u2211M\nj=1 ( \u03b12j/a \u2217 j ) 6= 0, then without loss of generality, we assume that \u2211Mj=1 (\u03b12j/a\u2217j) < 0.\nThis indicates that n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))yi ( u>xi )2 \u2265 0, for all u \u2208 Rd.\nBy the assumption that there exists two vectors er, es such that \u2200i : yi = 1, e>r xi = 0, e>s xi 6= 0 hold with probability 1 and \u2200i : yi = \u22121, e>s xi = 0, e>r xi 6= 0 hold with probability 1, then by setting u = er, we have that\n0 \u2264 n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))yi ( e>r xi )2 = \u2212 \u2211 i:yi=\u22121 `\u2032p(\u2212yif(xi;\u03b8\u2217)) ( e>r xi )2 \u2264 0,\nwhere the equality holds if and only if `\u2032p(\u2212yif(xi;\u03b8\u2217)) = 0 or yif(xi;\u03b8\u2217) \u2265 z0 > 0 holds for all i : yi = \u22121. Furthermore, since \u03b8\u2217 is a local minima and thus\n0 = dL\u0302n(\u03b8\n\u2217; p)\nda0 = n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))(\u2212yi) = \u2212 \u2211 i:yi=1 `\u2032p(\u2212yif(xi;\u03b8\u2217)) + \u2211 i:yi=\u22121 `\u2032p(\u2212yif(xi;\u03b8\u2217))\n= \u2212 \u2211 i:yi=1 `\u2032p(\u2212yif(xi;\u03b8\u2217)).\nThis means when `\u2032p(\u2212yif(xi;\u03b8\u2217)) = 0 holds for all i : yi = \u22121, we have `\u2032p(\u2212yif(xi;\u03b8\u2217)) = 0 for all i : yi = 1. These two together give us R\u0302n(\u03b8 \u2217) = 0. When \u2211M\nj=1 ( \u03b12j/a \u2217 j ) > 0, by setting u = es\nand following the similar analysis presented above, we can obtain the same result. Therefore, when\u2211M j=1 ( \u03b12j/a \u2217 j ) 6= 0, we have R\u0302n(\u03b8\u2217) = 0.\nCase 2: If \u2211M\nj=1 ( \u03b12j/a \u2217 j ) = 0, then by setting uj = (\u03b1j/a \u2217 j +vsgn(\u03b1j))u for some scalar v and vector\nu \u2208 Rd, we have\nF (v,u) = \u2212 2 n M\u2211 j=1 [ a\u2217j n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))yi ( (\u03b1j/a \u2217 j + vsgn(\u03b1j))u >xi )2]\n+ 4\nn n\u2211 i=1 `\u2032\u2032p(\u2212yif(xi;\u03b8\u2217))  M\u2211 j=1 a\u2217j ( w\u2217j >xi )( (\u03b1j/a \u2217 j + vsgn(\u03b1j))u >xi )2 = \u2212 2\nn M\u2211 j=1 [ a\u2217j n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))yi ( (\u03b1j/a \u2217 j + vsgn(\u03b1j))u >xi )2]\n+ 4\nn n\u2211 i=1 `\u2032\u2032p(\u2212yif(xi;\u03b8\u2217))   M\u2211 j=1 (\u03b1j + vsgn(\u03b1j)a \u2217 j )w \u2217 j > xi (u>xi)2  = \u2212 2\nn M\u2211 j=1 [ a\u2217j n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))yi ( (\u03b1j/a \u2217 j + vsgn(\u03b1j))u >xi )2]\n+ 4v2 n\u2211 i=1 `\u2032\u2032p(\u2212yif(xi;\u03b8\u2217))   M\u2211 j=1 sgn(\u03b1j)a \u2217 jw \u2217 j > xi  2 ( u>xi )2 , \u2212 2\nn M\u2211 j=1 [ a\u2217j n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))yi ( (\u03b1j/a \u2217 j + vsgn(\u03b1j))u >xi )2] + v2R(u),\nwhere we define\nR(u) = 4\nn n\u2211 i=1 `\u2032\u2032p(\u2212yif(xi;\u03b8\u2217))   M\u2211 j=1 sgn(\u03b1j)a \u2217 jw \u2217 j > xi  2 ( u>xi )2 . In addition, we have\nM\u2211 j=1 [ a\u2217j n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))yi ( (\u03b1j/a \u2217 j + vsgn(\u03b1j))u >xi )2]\n= n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8))yi(u>xi)2 \u00b7  M\u2211 j=1 (\u03b12j/a \u2217 j + 2vsgn(\u03b1j)\u03b1j + v 2a\u2217j )  =\nn\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8))yi(u>xi)2 \u00b7  M\u2211 j=1 (2vsgn(\u03b1j)\u03b1j + v 2a\u2217j )  = 2v\n M\u2211 j=1 |\u03b1j |  n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8))yi(u>xi)2 + v2  M\u2211 j=1 a\u2217j  n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8))yi(u>xi)2.\nTherefore, we can rewrite F (v,u) as\nF (v,u) = \u22124v n M\u2211 j=1 |\u03b1j | n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8))yi(u>xi)2 \u2212 2v2 n M\u2211 j=1 a\u2217j \u00b7 n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8))yi(u>xi)2 + v2R(u)\n, \u22124v n M\u2211 j=1 |\u03b1j | n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8))yi(u>xi)2 + v2R\u0302(u)\nSince F (v,u) \u2265 0 holds for any scalar v and vector u \u2208 Rd, then we should have\nM\u2211 j=1 |\u03b1j | n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8))yi(u>xi)2 = 0, for any u \u2208 Rd.\nSince the coefficient \u03b11, ..., \u03b1M are not all zero, then for any u \u2208 Rd, we have n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8))yi(u>xi)2 = 0.\nSince there exists two vectors er, es: \u2200i : yi = 1, e>r xi = 0 and e>s xi 6= 0 hold with probability 1 and \u2200i : yi = \u22121, e>s xi = 0 and e>r xi 6= 0 hold with probability 1, then by setting u = er, we have\n0 = n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8))yi(e>r xi)2 = \u2212 \u2211 i:yi=\u22121 `\u2032p(\u2212yif(xi;\u03b8))(e>r xi)2 \u2264 0,\nwhere the equality holds if and only if `\u2032p(\u2212yif(xi;\u03b8\u2217)) = 0 or yif(xi;\u03b8\u2217) \u2265 z0 > 0 holds for all i : yi = \u22121. Similar to the case 1, we have that `\u2032p(\u2212yif(xi;\u03b8\u2217)) = 0 holds for all i and this leads to R\u0302n(\u03b8 \u2217) = 0."}, {"heading": "A.4 Proof of Theorem 2", "text": "Theorem 4 Assume that the loss function `p satisfies assumption 1 and the network architecture satisfies assumption 4. Assume that samples in the dataset D = {(xi, yi)}ni=1, n \u2265 1 are independently drawn from a distribution satisfying assumption 6. Assume that the single layer network fS has M \u2265 1 neurons and neurons \u03c3 in the network fS are twice differentiable and satisfy \u03c3\n\u2032(z) > 0 for all z \u2208 R. If a set of real parameters \u03b8\u2217 = (\u03b8\u2217S ,\u03b8 \u2217 D) denotes a local minimum of the loss function L\u0302n(\u03b8S ,\u03b8D; p), p \u2265 3, then R\u0302n(\u03b8\u2217S ,\u03b8\u2217D) = 0 holds with probability one.\nProof: We first recall some notations defined in the paper. The output of the neural network is\nf(x;\u03b8) = fS(x;\u03b8S) + fD(x;\u03b8D),\nwhere fS(x;\u03b8S) is the single layer neural network parameterized by \u03b8S , i.e.,\nfS(x;\u03b8S) = a0 + M\u2211 j=1 aj\u03c3 ( w>j x ) ,\nand fD(x;\u03b8D) is a deep neural network parameterized by \u03b8D. The empirical loss function is given by\nL\u0302n(\u03b8; p) = L\u0302n(\u03b8S ,\u03b8D; p) = 1\nn n\u2211 i=1 `p(\u2212yif(xi;\u03b8)).\nBy the assumption that \u03b8\u2217 = (\u03b8\u2217S ,\u03b8 \u2217 D) is a local minima and by the necessary condition presented in Lemma 1, we have\nn\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))yi\u03c3\u2032(w\u2217j>xi)xi = 0d.\nThus, for any w \u2208 Rd and any j \u2208 [M ], we have n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))\u03c3\u2032(w\u2217j>xi)yi(w>xi) = 0.\nFurthermore, by assumption `\u2032p(z) \u2265 0 and the equality holds if and only if z \u2264 \u2212z0. Thus, by assumption that \u03c3\u2032(z) > 0 for all z \u2208 R and assumption that there exists a vector PX\u00d7Y (Yw>X > 0) = 1, then there exists and positive constant c > 0 such that\nyi(w >xi) > c > 0, \u2200i \u2208 [n].\nThus, we have\n0 = n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))\u03c3\u2032(w\u2217j>xi)yi(w>xi) \u2265 c n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))\u03c3\u2032(w\u2217j>xi) \u2265 0,\nwhere the equality holds if and only if `\u2032p(\u2212yif(xi;\u03b8\u2217)) = 0 for all i \u2208 [n]. Equivalently, if \u03b8\u2217 is a local minima, then yif(xi;\u03b8 \u2217) \u2265 z0 > 0 for all i \u2208 [n]. This indicates that Ln(\u03b8\u2217; p) = R\u0302n(\u03b8\u2217) = 0."}, {"heading": "B Additional Results in Section 4", "text": ""}, {"heading": "B.1 Proof of Proposition 2", "text": "Proposition 14 Assume that assumption 1 and 4 are satisfed. Assume that neurons in the network fS satisfy that \u03c3(z) = 0 for all z \u2264 0 and \u03c3(z) is piece-wise continuous on R. Then there exists a feedforward network fD and a distribution satisfying assumptions in Theorem 1 or 2 such that with probability one, the empirical loss L\u0302n(\u03b8; p), p \u2265 2 has a local minima \u03b8\u2217 = (\u03b8\u2217S ,\u03b8\u2217D) satisfying R\u0302n(\u03b8 \u2217) \u2265 min{n+,n\u2212}n , where n+ and n\u2212 are the number of positive and negative samples, respectively.\nProof: We choose the network architecture fD(x;\u03b8D) \u2261 0 for all x \u2208 Rd. Then the output of the network is\nf(x;\u03b8) = fS(x;\u03b8S) = a0 + M\u2211 j=1 aj\u03c3 ( w>j xi ) .\nNow we prove the following claim showing that if the dataset contains both positive and negative samples, then the empirical loss has a local minimum with a non-zero training error.\nClaim 1 Under the conditions in proposition 2, if the dataset contains both positive and negative samples and samples in the dataset are drawn in the space Rd\u22121\u00d7{1}\u00d7{1,\u22121}, the empirical loss has a local minimum with a non-zero training error. Furthermore, the training error is no smaller than min{n+,n\u2212}\nn .\nProof: We construct the local minimum as follows. Now we construct a local minimum \u03b8\u2217 = (\u03b8\u2217S). The key idea of constructing the local minimum having a training error no smaller than min{n+,n\u2212}n is appropriately choosing wj such that all neurons in the last layer keep inactive on all samples in the dataset. This is possible since the number of samples is bounded. Next, for any data set D = {(xi; yi)}ni=1, we define\nK = max i\u2208[n] \u2016xi\u20162.\nSince all samples in the dataset xi \u2208 Rd\u22121 \u00d7 {1}, then by choosing w\u2217j = ( w (1) j \u2217 , ..., w (d\u22121) j \u2217 , w (d) j \u2217) such that\nd\u22121\u2211 k=1 ( w (1) j \u2217)2 = 1,\nand w (d) j\n\u2217 = \u2212K \u2212 1. Since for all samples in the dataset\nw>j xi = d\u22121\u2211 k=1 w (k) j \u2217 x (k) i + w (d) j \u2217 \u2264 K \u2212K \u2212 1 = \u22121,\nthen \u03c3(w>j xi) = 0, \u2200i \u2208 [n].\nTherefore, the neural network becomes\nf(xi;\u03b8 \u2217) = a\u22170, \u2200i \u2208 [n].\nFinally, we set a\u22170 to the global minimizer of the following convex optimization problem:\nmin a\u2208R\n1\nn n\u2211 i=1 `(\u2212yia).\nThis indicates that for any a \u2208 R,\n1\nn n\u2211 i=1 `(\u2212yia) \u2265 1 n n\u2211 i=1 `(\u2212yia\u22170).\nNow we show that \u03b8\u2217 is local minimum of the empirical loss function. Now we slightly perturb the parameters a0, ..., aM ,w1, ...,wM by \u2206a0, ...,\u2206aM ,\u2206w1, ...,\u2206wM . Define\n\u03b8\u0303 = (a\u22170 + \u2206a0, ..., a \u2217 M + \u2206aM ,w \u2217 1 + \u2206w1, ...,w \u2217 M + \u2206wM ).\nThen, if \u2016\u03b8 \u2212 \u03b8\u0303\u20162 \u2264 \u03b5 and \u03b5 is positive and sufficiently small, then for \u2200j \u2208 [M ] and \u2200 \u2208 [n], we have\nw\u2217jxi + \u2206w > j xi \u2264 \u22121 + \u2016\u2206wj\u20162 \u2016xi\u20162 \u2264 \u22121 +K\u03b5 < 0.\nThis means that if \u03b5 is positive and sufficiently small, then\nf(xi; \u03b8\u0303) = a \u2217 0 + \u2206a0.\nIn addition, for all \u2206a0 \u2208 R,\n1\nn n\u2211 i=1 `(\u2212yia\u2217 + \u2206a0) \u2265 1 n n\u2211 i=1 `(\u2212yia\u22170),\ntherefore for \u03b8\u0303 : \u2016\u03b8\u0303 \u2212 \u03b8\u2217\u20162 \u2264 \u03b4(\u03b5) and any a0 \u2208 R\nL\u0302n(\u03b8\u0303) = 1\nn n\u2211 i=1 `(\u2212yif(xi; \u03b8\u0303)) = 1 n n\u2211 i=1 `(\u2212yi(a\u22170 + \u2206a0))\n\u2265 1 n n\u2211 i=1 `(\u2212yia\u22170) \u2265 1 n n\u2211 i=1 `(\u2212yif(xi;\u03b8\u2217)) = L\u0302n(\u03b8\u2217).\nThis means that \u03b8\u2217 is a local minimum of the empirical loss and f(xi;\u03b8 \u2217) = a\u22170 for all i \u2208 [n]. This further indicates that\nR\u0302n(\u03b8 \u2217) \u2265 min{n\u2212, n+}\nn .\nNow we only need to construct the data distribution satisfying assumptions in Theorem 1 and Theorem 2, respectively, such that with probability at least 1 \u2212 e\u2212\u2126(n), the dataset drawn from this distribution satisfies the assumption in claim 1.\nDistribution for Theorem 1: Now we define a distribution as follows, PX|Y=1 is a uniform distribution on the region [\u22122,\u22121] \u222a [1, 2] \u00d7 {0} \u00d7 {1} \u00d7 {0}d\u22123 and PX|Y=\u22121 is a uniform distribution on the region {0} \u00d7 [\u22122,\u22121] \u222a [1, 2] \u00d7 {1} \u00d7 {0}d\u22123. In addition, P(Y = 1) = P(Y = \u22121) = 0.5. It is easy to check that r = 3 > max{r+, r\u2212} = 2 and for any two samples independently drawn from the distribution PX|Y=1 or PX|Y=\u22121, these two samples are linearly independent. This means that this data distribution satisfies the conditions in Theorem 1. In addition, if samples in the dataset are independently drawn from this distribution, then with probability 1\u2212 1\n2n\u22121 , the dataset contains both positive and negative samples.\nDistribution for Theorem 2: Now we define a distribution as follows, PX|Y=1 is a uniform distribution on the region [\u22122,\u22121] \u00d7 {0} \u00d7 {1} \u00d7 {0}d\u22123 and PX|Y=\u22121 is a uniform distribution on the region {0}\u00d7 [\u22122,\u22121]\u00d7{1}\u00d7{0}d\u22123. It is easy to check that This means that this distribution satisfies the conditions in Theorem 2. In addition, if samples in the dataset are independently drawn from this distribution, then with probability 1\u2212 1\n2n\u22121 , the dataset contains both positive and negative samples."}, {"heading": "B.2 Proof of Proposition 3", "text": "Proposition 15 Assume that assumption 1 and 4 are satisfed. Assume that neurons in the network fS satisfy that \u03c3(z) = z for all z \u2265 0 and \u03c3(z) is piece-wise continuous on R. Then there exists a network architecture fD and a distribution satisfying assumptions in Theorem 1 such that, with probability at least 1\u2212 e\u2212\u2126(n), the empirical loss L\u0302n(\u03b8; p), p \u2265 2 has a local minima \u03b8\u2217 = (\u03b8\u2217S ,\u03b8\u2217D) with non-zero training error.\nProof: We choose the network architecture fD(x;\u03b8D) \u2261 0 for all x \u2208 Rd. Then the output of the network is\nf(x;\u03b8) = fS(x;\u03b8S) = a0 + M\u2211 j=1 aj\u03c3 ( w>j xi ) .\nNow we prove the following claim showing that if the dataset contains both positive and negative samples, then the empirical loss has a local minimum with a non-zero training error.\nClaim 2 Under the conditions in proposition 2, if the samples in the dataset are not linearly separable and samples (xi, yi) are drawn in the space Rd\u22121\u00d7{1}\u00d7{1,\u22121}, the empirical loss has a local minimum with a non-zero training error.\nProof: We construct the local minimum as follows. Now we construct a local minimum \u03b8\u2217 = (\u03b8\u2217S). The key idea of constructing the local minimum having a training error no smaller than min{n+,n\u2212}n is appropriately choosing wj such that all neurons in the last layer keep inactive on all samples in the dataset. This is possible since the number of samples is bounded. First, let w\u2217 be a global minimizer of the following convex optimization problem:\nmin w\u2208Rd n\u2211 i=1 `p(\u2212yi(w>xi)). (9)\nNext, for any data set D = {(xi; yi)}ni=1, we define\nK = max i\u2208[n] |w\u2217>xi| and K1 = max i\u2208[n] \u2016xi\u20162.\nSince all samples in the dataset xi \u2208 Rd\u22121 \u00d7 {1}, then by choosing w\u2217j = ( w (1) j \u2217 , ..., w (d\u22121) j \u2217 , w (d) j \u2217) such that\nw (1) j\n\u2217 = w(1) \u2217 , ..., w\n(d\u22121) j\n\u2217 = w(d\u22121) \u2217 , w\n(d) j\n\u2217 = w(d) \u2217 +K + 1.\nSince for all samples in the dataset\nw\u2217j >xi = w \u2217>xi +K + 1 \u2265 \u2212K +K + 1 = 1, then\n\u03c3(w>j xi) = w >xi, \u2200i \u2208 [n].\nIn addition, let a\u2217j = 1 M and a \u2217 0 = 0. Therefore, the neural network becomes\nf(xi;\u03b8 \u2217) = w>xi, \u2200i \u2208 [n].\nSince w\u2217 is the global optimizer of the convex optimization problem defined in Equation (9), this indicates that for any w \u2208 Rd,\n1\nn n\u2211 i=1 `p(\u2212yi(w>xi)) \u2265 1 n n\u2211 i=1 `p(\u2212yi(w\u2217>xi)).\nNow we show that \u03b8\u2217 is local minimum of the empirical loss function. Now we slightly perturb the parameters a0, ..., aM ,w1, ...,wM by \u2206a0, ...,\u2206aM ,\u2206w1, ...,\u2206wM . Define\n\u03b8\u0303 = (a\u22170 + \u2206a0, ..., a \u2217 M + \u2206aM ,w \u2217 1 + \u2206w1, ...,w \u2217 M + \u2206wM ).\nThen, if \u2016\u03b8 \u2212 \u03b8\u0303\u20162 \u2264 \u03b5 and \u03b5 is positive and sufficiently small, then for \u2200j \u2208 [M ] and \u2200 \u2208 [n], we have\nw\u2217jxi + \u2206w > j xi \u2265 1\u2212 \u2016\u2206wj\u20162 \u2016xi\u20162 \u2265 1\u2212K1\u03b5 > 0.\nThis means that if \u03b5 is positive and sufficiently small, then\nf(xi; \u03b8\u0303) = \u2206a0 + M\u2211 j=1 (a\u2217j + \u2206aj) ( w>xi + \u2206w > j xi ) .\nThis means that f(x; \u03b8\u0303) behave as a linear model on the dataset. Since w\u2217 corresponds to the optimal linear model minimizing the empirical loss, then\nL\u0302n(\u03b8\u0303) = 1\nn n\u2211 i=1 `p(\u2212yif(xi; \u03b8\u0303))\n\u2265 1 n n\u2211 i=1 `p(\u2212yi(w>xi)) \u2265 1 n n\u2211 i=1 `p(\u2212yif(xi;\u03b8\u2217)) = L\u0302n(\u03b8\u2217).\nThis means that \u03b8\u2217 is a local minimum of the empirical loss and f(xi;\u03b8 \u2217) = a\u22170 for all i \u2208 [n]. This further indicates that\nR\u0302n(\u03b8 \u2217) \u2265 min{n\u2212, n+}\nn .\nNow we only need to construct the data distribution satisfying assumptions in Theorem 1 such that with probability at least 1\u2212 e\u2212\u2126(n), the dataset drawn from this distribution satisfies the assumption in claim 2.\nDistribution for Theorem 1: Now we define a distribution as follows, PX|Y=1 is a uniform distribution on the region [\u22122,\u22121] \u222a [1, 2] \u00d7 {0} \u00d7 {1} \u00d7 {0}d\u22123 and PX|Y=\u22121 is a uniform distribution on the region {0} \u00d7 [\u22122,\u22121] \u222a [1, 2] \u00d7 {1} \u00d7 {0}d\u22123. In addition, P(Y = 1) = P(Y = \u22121) = 0.5. It is easy to check that r = 3 > max{r+, r\u2212} = 2 and for any two samples independently drawn from the distribution PX|Y=1 or PX|Y=\u22121, these two samples are linearly independent. This means that this data distribution satisfies the conditions in Theorem 1. In addition, if samples in the dataset are independently drawn from this distribution, then with probability 1 \u2212 e\u2212\u2126(n), the dataset contains samples in each of the following four regions: [\u22122,\u22121]\u00d7{0}\u00d7{1}\u00d7{0}d\u22123, [1, 2]\u00d7{0}\u00d7{1}\u00d7{0}d\u22123, {0}\u00d7 [1, 2]\u00d7{1}\u00d7 {0}d\u22123 and {0}\u00d7 [\u22122,\u22121]\u00d7{1}\u00d7 {0}d\u22123, which makes the samples in the dataset not linearly separable."}, {"heading": "B.3 Proof of Proposition 4", "text": "Proposition 16 Assume that assumption 1 and 4 are satisfed. Assume that there exists a constant c \u2208 R such that neurons in the network fS satisfy \u03c3(z) + \u03c3(\u2212z) \u2261 c for all z \u2208 R. Assume that the dataset D has 2n samples. Then there exists a network architecture fD and a distribution satisfying assumptions in Theorem 1 such that, with probability at least \u2126(1/n2), the empirical loss function L\u03022n(\u03b8; p) has a local minimum \u03b8 \u2217 = (\u03b8\u2217S ,\u03b8 \u2217 D) satisfying R\u03022n(\u03b8\n\u2217) \u2265 min{n\u2212,n+}2n , where n+ and n\u2212 denote the number of positive and negative samples in the dataset, respectively.\nProof: We first prove the following claim showing that when the dataset satisfies certain conditions, there exists a local minimum satisfying R\u03022n(\u03b8\n\u2217) \u2265 min{n\u2212,n+}2n . Next, we construct a data distribution such that the dataset drawn from the distribution satisfies these conditions with probability \u2126(1/n2).\nClaim 3 Assume that for each sample (xi, yi) in the dataset D = {(xi, yi)}2ni=1, there exists a sample (xj , yj) \u2208 D such that \u2016xi + xj\u20162 = 0 and yi = yj. If the function \u03c3(z) +\u03c3(\u2212z) \u2261 constant on R, then the empirical loss function L\u03022n(\u03b8) has a local minimum \u03b8 \u2217 satisfying R\u03022n(\u03b8 \u2217) \u2265 min{n\u2212,n+}2n .\nProof: Consider a single layer neural network\nf(x;\u03b8) = a0 + M\u2211 j=1 aj\u03c3(w > j x).\nNow we construct a local minimum \u03b8\u2217. Let a\u22171 = ... = a \u2217 M = \u22121, and w\u22171 = ... = w\u2217M = 0d. Thus f(x;\u03b8\u2217) = a\u22170 \u2212M\u03c3(0). Let a\u22170 be the global optimizer of the following convex optimization problem.\nmin a 2n\u2211 i=1 `p(\u2212yi(a\u2212M\u03c3(0))).\nThus, we have 2n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))(\u2212yi) = 0, (10) and this indicates that\u2211 i:yi=1 `\u2032p(\u2212(a\u22170 \u2212M\u03c3(0))) = \u2211 i:yi=\u22121 `\u2032p(a \u2217 0 \u2212M\u03c3(0)) or `\u2032p(\u2212a\u22170 +M\u03c3(0))n+ = `\u2032p(a\u22170 \u2212M\u03c3(0))n\u2212. (11) In addition, we have, for \u2200j \u2208 [M ],\n\u2202L\u03022n(\u03b8 \u2217)\naj = 2n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))(\u2212yi)\u03c3(0) = 0, by Equation (10)\n\u2207wj L\u03022n(\u03b8\u2217) = 2n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))(\u2212yi)\u03c3\u2032(0)xi,\n= \u03c3\u2032(0) 2n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))(\u2212yi)xi.\nBy assumption that for each sample (xi, yi) in the dataset, there exists a sample (xj , yj) in the dataset such that xi + xj = 0d and yi = yj , i.e., yixi + yjxj = 0d, thus we have for any j \u2208 [M ],\n\u2207wj L\u03022n(\u03b8\u2217) = \u03c3\u2032(0) 2n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))(\u2212yi)xi = 0d. (12)\nFurthermore, we have\n\u2202L\u03022n(\u03b8 \u2217)\na0 = 2n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))(\u2212yi) = 0,\nthen \u03b8\u2217 is a critical point. Now we only need to show that it is a local minimum. We prove it by definition. Consider any perturbation \u2206a1, ...,\u2206aM : |\u2206aj | < 12 for all j \u2208 [M ], \u2206w1, ...,\u2206wM \u2208 Rd and \u2206a0 \u2208 R. Define\n\u03b8\u0303 = (a\u22170 + \u2206a0, ..., a \u2217 M + \u2206aM ,w \u2217 1 + \u2206w1, ...,w \u2217 M + \u2206wM ).\nThen\n2n\u2211 i=1 `p(\u2212yif(xi; \u03b8\u0303))\u2212 2n\u2211 i=1 `p(\u2212yif(xi;\u03b8\u2217)) = 2n\u2211 i=1 [ `p(\u2212yif(xi; \u03b8\u0303))\u2212 `p(\u2212yif(xi;\u03b8\u2217)) ] \u2265\n2n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))(\u2212yi)[f(xi; \u03b8\u0303)\u2212 f(xi;\u03b8\u2217)]\n= 2n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))(\u2212yi)[f(xi; \u03b8\u0303)\u2212 a\u22170 +M\u03c3(0)]\n= 2n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))(\u2212yi)f(xi; \u03b8\u0303),\nwhere the inequality follows from the convexity of `p, the second equality follows from the fact that f(x;\u03b8\u2217) \u2261 a\u22170 \u2212M\u03c3(0) and the third equality follows from Equation (10). In addition, we have\n2n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))(\u2212yi)f(xi; \u03b8\u0303) = 2n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))(\u2212yi)  M\u2211 j=1 (a\u2217j + \u2206aj)\u03c3 ( \u2206w>j xi ) + \u2206a0\n =\n2n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))(\u2212yi)  M\u2211 j=1 (a\u2217j + \u2206aj)\u03c3 ( \u2206w>j xi ) by Eq. (10) =\nM\u2211 j=1 \u2212(a\u2217j + \u2206aj) [ 2n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))yi\u03c3 ( \u2206w>j xi )] .\nNow we consider the following term\n2n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))yi\u03c3 ( \u2206w>j xi ) .\nBy assumption that for each sample (xi, yi) in the dataset, there exists a sample (xk, yk) in the dataset such that xi + xk = 0d, yi = yk by the assumption that there exists a constant c0 such that \u03c3(z) + \u03c3(\u2212z) \u2261 c0, thus we have for any \u2206wj \u2208 Rd,\nyi\u03c3 ( \u2206w>j xi ) + yk\u03c3 ( \u2206w>j xk ) = yi\u03c3 ( \u2206w>j xi ) + yi\u03c3 ( \u2212\u2206w>j xi ) = yic0 =\nc0 2 (yi + yk),\nwhere the last equality follows from yi = yk. Therefore, we have for all \u2206wj \u2208 Rd, 2n\u2211 i=1 `p(\u2212yi(a\u22170 \u2212M\u03c3(0)))yi\u03c3 ( \u2206w>j xi ) = c0 2 2n\u2211 i=1 `p(\u2212yi(a\u22170 \u2212M\u03c3(0)))yi = 0.\nThus, we have 2n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))(\u2212yi)f(xi; \u03b8\u0303) = M\u2211 j=1 \u2212(a\u2217j + \u2206aj) [ 2n\u2211 i=1 `p(\u2212yi(a\u22170 \u2212M\u03c3(0)))yi\u03c3 ( \u2206w>j xi )] = 0,\nand this further indicates\n2n\u2211 i=1 `p(\u2212yif(xi; \u03b8\u0303))\u2212 2n\u2211 i=1 `p(\u2212yif(xi;\u03b8\u2217)) \u2265 2n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))(\u2212yi)f(xi; \u03b8\u0303) = 0.\nTherefore, this means that \u03b8\u2217 is a local minimum. Since f(x;\u03b8\u2217) = a\u22170 \u2212M\u03c3(0), then clearly,\nR\u03022n(\u03b8 \u2217) \u2265 min{n+, n\u2212}\nn .\nNow we construct the data distribution PX\u00d7Y as follows\nP(X = (1, 0), Y = 1) = P(X = (\u22121, 0), Y = 1) = P(X = (0, 1), Y = \u22121) = P(X = (0,\u22121), Y = \u22121).\nAssume that samples in the dataset D = {(xi, yi)}2ni=1 are independently draw from the data distribution PX\u00d7Y . Let n(1,0) and n(\u22121,0) denote the number of samples at the point (1, 0) and (\u22121, 0), respectively. Let n(0,1) and n(0,\u22121) denote the number of samples at the point (0, 1) and (0,\u22121), respectively. Then the probability that n(1,0) = n(\u22121,0) and n(0,1) = n(0,\u22121) is\nPX\u00d7Y [ n(1,0) = n(\u22121,0) and n(0,1) = n(0,\u22121) ] = n\u2211 i=1 ( 2n 2i )( 2i i )( 2(n\u2212 i) n\u2212 i )( 1 4 )2n =\nn\u2211 i=1\n(2n)! (2i)!(2n\u2212 2i)! (2i)! [i!]2 (2n\u2212 2i)! [(n\u2212 i)!]2\n( 1\n16\n)n = n\u2211 i=1\n(2n)! [i!(n\u2212 i)!]2 1 16n\n= (2n)!\n16n(n!)2 n\u2211 i=1\n(n!)2\n[i!(n\u2212 i)!]2 =\n(2n)!\n16n(n!)2 n\u2211 i=1 ( n i )2 = 1\n16n\n( 2n\nn\n)2 >\n1\n(n+ 1)2\nby the equality n\u2211 i=1 ( n i )2 = ( 2n n ) and the inequality (\n2n\nn\n) > 4n\nn+ 1 .\nNow we only need to check whether the distribution PX\u00d7Y satisfies the assumptions shown in Theorem 1. Clearly, r+ = r\u2212 = 1 < r = 2 and with probability 1, random vector X drawn from distribution PX|Y=1 and random vector Z drawn from distribution PX|Y=\u22121 has rank one which equals to r+ and r\u2212. Therefore, the distribution constructed here satisfies the assumptions in Theorem 1."}, {"heading": "B.4 Proof of Proposition 5", "text": "Proposition 17 Assume that assumption 1 and 4 are satisfed. Assume that neurons in fS satisfy that \u03c3 is strongly convex and twice differentiable on R and has a global minimum at z = 0. Then there exists a network architecture fD and a distribution satisfying assumptions in Theorem 2 such that with probability one, the empirical loss L\u0302n(\u03b8; p), p \u2265 2 has a local minima \u03b8\u2217 = (\u03b8\u2217S ,\u03b8\u2217D) satisfying R\u0302n(\u03b8\n\u2217) \u2265 min{n+,n\u2212}n , where n+ and n\u2212 denote the number of positive and negative samples in the dataset, respectively.\nProof: We first prove the following claim showing that if the dataset satisfies certain conditions, then the empirical loss has a local minimum satisfying R\u0302n(\u03b8\n\u2217) \u2265 min{n\u2212,n+}n . Next, we construct a data distribution such that the dataset drawn from the distribution PX\u00d7Y satisfies these conditions with probability one. Claim 4 If the matrix 1n+ \u2211 i:yi=1 xix > i \u2212 1n\u2212 \u2211 i:yi=\u22121 xix > i is positive or negative definite, then the empirical loss function L\u0302n(\u03b8) has a local minimum \u03b8 \u2217 satisfying R\u0302n(\u03b8 \u2217) \u2265 min{n\u2212,n+}n .\nProof: We prove that if the following matrix\n1\nn+ \u2211 i:yi=1 xix > i \u2212 1 n\u2212 \u2211 i:yi=\u22121 xix > i\nis either positive definite or negative definite, then there exists a local minima \u03b8\u2217 having f(x;\u03b8\u2217) \u2261 constant and this leads to R\u0302n(\u03b8\n\u2217) \u2265 min{n+,n\u2212}n . Without loss of generality, we assume that the matrix is positive definite. Consider a single layer neural network\nf(x;\u03b8) = a0 + M\u2211 j=1 aj\u03c3 ( w>j x ) .\nLet a\u22171 = ... = a \u2217 M = \u22121 and w\u22171 = ... = w\u2217M = 0d. Therefore, we have f(x;\u03b8\u2217) = a\u22170 \u2212M\u03c3(0). Let a\u22170 be the global optimizer of the following convex optimization problem.\nmin a n\u2211 i=1 `p(\u2212yi(a\u2212M\u03c3(0))).\nThus, we have n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))(\u2212yi) = 0, (13) and this indicates that\u2211 i:yi=1 `\u2032p(\u2212(a\u22170 \u2212M\u03c3(0))) = \u2211 i:yi=\u22121 `\u2032p(a \u2217 0 \u2212M\u03c3(0)) or `\u2032p(\u2212a\u22170 +M\u03c3(0))n+ = `\u2032p(a\u22170 \u2212M\u03c3(0))n\u2212. (14) In addition, since for \u2200j \u2208 [M ],\n\u2202L\u0302n(\u03b8 \u2217)\n\u2202aj = n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))(\u2212yi)\u03c3(0) = 0, by Equation (13),\n\u2207wj L\u0302n(\u03b8\u2217) = n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))(\u2212yi)\u03c3\u2032(0)xi = 0d, by \u03c3\u2032(0) = 0,\nand \u2202L\u0302n(\u03b8 \u2217)\n\u2202a0 = n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))(\u2212yi) = 0,\nthen \u03b8\u2217 is a critical point. Next we show that \u03b8\u2217 = (a\u22170, ..., a \u2217 M ,w \u2217 1, ...,w \u2217 M ) is a local minima. Consider any perturbation \u2206a1, ...,\u2206aM : |\u2206aj | < 12 for all j \u2208 [M ], \u2206w1, ...,\u2206wM \u2208 Rd and \u2206a0 \u2208 R. Define\n\u03b8\u0303 = (a\u22170 + \u2206a0, ..., a \u2217 M + \u2206aM ,w \u2217 1 + \u2206w1, ...,w \u2217 M + \u2206wM ).\nThen\nn\u2211 i=1 `p(\u2212yif(xi; \u03b8\u0303))\u2212 n\u2211 i=1 `p(\u2212yif(xi;\u03b8\u2217)) = n\u2211 i=1 [ `p(\u2212yif(xi; \u03b8\u0303))\u2212 `p(\u2212yif(xi;\u03b8\u2217)) ] \u2265\nn\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))(\u2212yi)[f(xi; \u03b8\u0303)\u2212 f(xi;\u03b8\u2217)]\n= n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))(\u2212yi)[f(xi; \u03b8\u0303)\u2212 a\u22170 +M\u03c3(0)]\n= n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))(\u2212yi)f(xi; \u03b8\u0303),\nwhere the inequality follows from the convexity of the loss function `p(z), the second equality follows from the fact that f(x;\u03b8\u2217) \u2261 a\u22170 \u2212 M\u03c3(0) and the third equality follows from Equation (14). In addition, we have\nn\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))(\u2212yi)f(xi; \u03b8\u0303) = n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))(\u2212yi)  M\u2211 j=1 (a\u2217j + \u2206aj)\u03c3 ( \u2206w>j xi ) + \u2206a0\n =\nn\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))(\u2212yi)  M\u2211 j=1 (a\u2217j + \u2206aj)\u03c3 ( \u2206w>j xi ) by Eq. (14) =\nM\u2211 j=1 \u2212(a\u2217j + \u2206aj) [ n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))yi\u03c3 ( \u2206w>j xi )] .\nNow we define the following function G : Rd \u2192 R,\nG(u) = n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))yi\u03c3 ( u>xi ) .\nNow we consider the gradient of the function G with respect to the vector u at the point 0d,\n\u2207uG(0d) = n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))yi\u03c3\u2032 (0)xi = 0d.\nFurthermore, the Hessian matrix \u22072uG(0d) satisfies\n\u22072uG(0d) = n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))yi\u03c3\u2032\u2032 (0)xix>i = \u03c3\u2032\u2032 (0) n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))yixix>i\n= \u03c3\u2032\u2032(0)  1 n+ \u2211 i:yi=1 xix > i \u2212 1 n\u2212 \u2211 i:yi=\u22121 xix > i  0, then the function G(u) = \u2211n i=1 `p(\u2212yi(a\u22170 \u2212M\u03c3(0)))yi\u03c3 ( u>xi ) has a local minima at u = 0d. This indicates that there exists \u03b5 > 0 such that for all \u2206w : \u2016\u2206w\u20162 \u2264 \u03b5, n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))yi\u03c3 ( \u2206w>xi ) \u2265 n\u2211 i=1 `p(\u2212yi(a\u22170 \u2212M\u03c3(0)))yi\u03c3 (0) = 0.\nIn addition, since a\u2217j = \u22121, |\u2206aj | < 12 , then for all \u2206wj : \u2016\u2206wj\u20162 \u2264 \u03b5, n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))(\u2212yi)f(xi; \u03b8\u0303) = M\u2211 j=1 \u2212(a\u2217j + \u2206aj) [ n\u2211 i=1 `p(\u2212yi(a\u22170 \u2212M\u03c3(0)))yi\u03c3 ( \u2206w>j xi )] \u2265 0.\nTherefore, we have n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))(\u2212yi)f(xi; \u03b8\u0303) \u2265 0, and this indicates that n\u2211 i=1 `p(\u2212yif(xi; \u03b8\u0303))\u2212 n\u2211 i=1 `p(\u2212yif(xi;\u03b8\u2217)) \u2265 0. Thus, \u03b8\u2217 is a local minima with f(x;\u03b8\u2217) = a\u22170 \u2212M\u03c3(0) = constant. Thus, n\u2211 i=1 I{yi 6= sgn(f(xi;\u03b8\u2217))} \u2265 min{n\u2212, n+} n .\nNow we define a data distribution as follows. Let PY (Y = 1) = P(Y = \u22121) = 0.5. Let PX|Y=1 be a continuous distribution (e.g., uniform distribution) defined on the interval [2, 3] and PX|Y=\u22121 be a continuous distribution defined on the interval [\u22121,\u22121/2]. Then if samples in the dataset D are drawn independently from the this distribution, the scalar 1n+ \u2211 i:yi=1 x2i \u2212 1n\u2212 \u2211 i:yi=\u22121 x 2 i > 0 if n+ > 0 and\nthe scalar 1n+ \u2211 i:yi=1 x2i \u2212 1n\u2212 \u2211 i:yi=\u22121 x 2 i < 0 if n+ = 0. This means that the dataset satisfies the conditions in the claim with probability one."}, {"heading": "B.5 Proof of Proposition 6", "text": "Proposition 18 Assume that assumption 1 is satisfied. Assume that the feedforward neural network f(x;\u03b8) has at least one hidden layer and has at least one neuron in each hidden layer. If neurons in the network f satisfy that \u03c3(z) = 0 for all z \u2264 0 and \u03c3(z) is continuous on R, then the empirical loss L\u0302n(\u03b8; p), p \u2265 2 has a local minima \u03b8\u2217 satisfying R\u0302n(\u03b8\u2217) \u2265 min{n+,n\u2212}n , where n+ and n\u2212 denote the number of positive and negative samples in the dataset, respectively.\nProof: Assume that the multilayer neural network f(x;\u03b8) has L \u2265 1 hidden layers, Ml \u2265 1 neurons in the l-th layer. Now we let the vector \u03b8l contain all parameters in the first l \u2208 [L] layers. Then the output of the neural network can be rewritten as\nf(x; a0,\u03b8L) = a0 + ML\u2211 j=1 aj\u03c3(w > j \u03a6(x;\u03b8L\u22121) + bj),\nwhere \u03a6(x;\u03b8L\u22121) = (\u03a61(x;\u03b8L\u22121), ...,\u03a6ML\u22121(x;\u03b8L\u22121)) denotes the outputs of the neurons in the layer L \u2212 1. Now we construct a local minimum \u03b8\u2217 = (a\u22170,\u03b8\u2217L). The key idea of constructing the local minimum having a training error no smaller than min{n+,n\u2212}n is appropriately choosing wj , bj such that all neurons in the last layer keep inactive on all samples in the dataset. This is possible since the outputs of the neurons in the layer L\u2212 1 are bounded. We first set \u03b8L\u22121 to any unit vector \u03b8 \u2217 L\u22121 : \u2016\u03b8\u2217L\u22121\u20162 = 1. Next, for any data set D = {(xi; yi)}ni=1, we define K = max\ni\u2208[n] \u2016\u03a6(xi;\u03b8\u2217L\u22121)\u20162.\nIn addition, it is easy to show that the function \u03d5ij(\u03b8) = \u03a6j(xi;\u03b8) is a continuous function. Now we consider the compact set C\u03b4 = {\u03b8 : \u2016\u03b8 \u2212 \u03b8\u2217L\u22121\u20162 \u2264 \u03b4}, where \u03b4 > 0 . Since each function \u03d5ij is a continuous function on the compact set C, then by the definition of continuity,\n\u2200\u03b5 > 0,\u2203\u03b4ij(\u03b5) \u2208 (0, 1) : |\u03d5ij(\u03b8)\u2212 \u03d5ij(\u03b8\u2217L\u22121)| \u2264 \u03b5 for all \u03b8 \u2208 C\u03b4ij . For a given \u03b5 > 0, let\n\u03b4(\u03b5) = min i\u2208[n],j\u2208[ML\u22121] \u03b4ij(\u03b5),\nthen for all i \u2208 [n], j \u2208 [ML\u22121] and \u2200\u03b8 \u2208 C\u03b4, |\u03d5ij(\u03b8)\u2212 \u03d5ij(\u03b8L\u22121)| \u2264 \u03b5.\nNow we set wj to some unit vector wj : \u2016wj\u20162 = 1 for all j \u2208 [ML\u22121], and we set bj to a scalar b\u2217j satisfying\nw\u2217j >\u03a6(xi;\u03b8 \u2217 L\u22121) + b \u2217 j \u2264 \u22121, for all i \u2208 [n] and all \u03b8 \u2208 C.\nTherefore, the neural network becomes\nf(xi; a0,\u03b8 \u2217 L) = a0, \u2200i \u2208 [n].\nFurthermore, for the \u03b4(\u03b5) defined above and for any parameter vector \u03b8\u0303L : \u2016\u03b8\u0303L\u2212\u03b8\u2217L\u20162 \u2264 \u03b4(\u03b5), we have for all j \u2208 [ML\u22121] and all i \u2208 [n],\n|w\u0303>j \u03a6(xi; \u03b8\u0303L\u22121) + b\u0303j \u2212w\u2217j>\u03a6(xi;\u03b8\u2217L\u22121)\u2212 b\u2217j | \u2264 |w\u0303>j \u03a6(xi; \u03b8\u0303L\u22121)\u2212 w\u0303>j \u03a6(xi;\u03b8\u2217L\u22121) + w\u0303>j \u03a6(xi;\u03b8\u2217L\u22121)\u2212w\u2217j>\u03a6(xi;\u03b8\u2217L\u22121)|+ |b\u0303j \u2212 bj | \u2264 |w\u0303>j \u03a6(xi; \u03b8\u0303L\u22121)\u2212 w\u0303>j \u03a6(xi;\u03b8\u2217L\u22121)|+ |w\u0303>j \u03a6(xi;\u03b8\u2217L\u22121)\u2212w\u2217j>\u03a6(xi;\u03b8\u2217L\u22121)|+ |b\u0303j \u2212 bj | \u2264 \u2016w\u0303j\u20162\u2016\u03a6(xi; \u03b8\u0303L\u22121)\u2212\u03a6(xi;\u03b8\u2217L\u22121)\u20162 + \u2016w\u0303j \u2212w\u2217j\u20162\u2016\u03a6(xi;\u03b8\u2217L\u22121)\u20162 + |b\u0303j \u2212 bj | \u2264 (1 + \u03b4(\u03b5)) \u221a ML\u22121\u03b5+ \u03b5K + \u03b5 \u2264 (2 \u221a ML\u22121 +K + 1)\u03b5.\nThus, if \u03b5 < 1 2(2 \u221a ML\u22121+K+1) , then for all \u03b8\u0303L : \u2016\u03b8\u0303L \u2212 \u03b8\u2217L\u20162 \u2264 \u03b4(\u03b5), \u2200j \u2208 [M ] and \u2200i \u2208 [n]\nw\u0303>j \u03a6(xi; \u03b8\u0303L\u22121) + b\u0303j \u2264 w\u2217j>\u03a6(xi;\u03b8\u2217L\u22121) + b\u2217j + 1 2 \u2264 \u22121 2 . (15)\nSince \u03c3(z) = 0 for all z \u2264 0, then this indicates that for all \u03b8\u0303L : \u2016\u03b8\u0303L \u2212 \u03b8\u2217L\u20162 \u2264 \u03b4(\u03b5),\nf(xi; a0, \u03b8\u0303L\u22121) = a0, for all i \u2208 [n].\nFinally, we set a\u22170 to the global minimizer of the following convex optimization problem:\nmin a\u2208R\n1\nn n\u2211 i=1 `(\u2212yia).\nThis indicates that for any a \u2208 R,\n1\nn n\u2211 i=1 `(\u2212yia) \u2265 1 n n\u2211 i=1 `(\u2212yia\u22170).\nTherefore, for \u03b8\u0303L : \u2016\u03b8\u0303L \u2212 \u03b8\u2217L\u20162 \u2264 \u03b4(\u03b5) and any a0 \u2208 R\nL\u0302n(a0, \u03b8\u0303L) = 1\nn n\u2211 i=1 `(\u2212yif(xi; \u03b8\u0303L)) = 1 n n\u2211 i=1 `(\u2212yia0)\n\u2265 1 n n\u2211 i=1 `(\u2212yia\u22170) \u2265 1 n n\u2211 i=1 `(\u2212yif(xi; a\u22170,\u03b8\u2217L)) = L\u0302n(a\u22170,\u03b8\u2217L).\nThis means that (a\u22170,\u03b8 \u2217 L) is a local minima and f(xi; a \u2217 0,\u03b8 \u2217 L) = a \u2217 0 for all i \u2208 [n]. This further indicates that\nR\u0302n(\u03b8 \u2217) \u2265 min{n\u2212, n+}\nn ."}, {"heading": "B.6 Proof of Proposition 7", "text": "Proposition 19 Assume that H : Rd \u2192 Rd is a feedforward neural network parameterized by \u03b8 and all neurons in H are ReLUs. Define a network f : Rd \u2192 R with identity shortcut connections as f(x;a,\u03b8, b) = a>(x + H(x;\u03b8)) + b, a \u2208 Rd, b \u2208 R. Then there exists a distribution PX\u00d7Y satisfying the assumptions in Theorem 1 such that with probability at least 1 \u2212 e\u2212\u2126(n), the empirical loss L\u0302n(a,\u03b8, b; p) = 1 n \u2211n i=1 `(\u2212yif(xi;\u03b8); p), p \u2265 2 has a local minimum with non-zero training error.\nProof: We first show that if the samples in the dataset are not linearly separable, then empirical loss has a local minimum with a non-zero training error. Next, we construct a data distribution such that n samples independently drawn from this data distribution are not linearly separable with probability at least 1\u2212 exp(\u2212\u2126(n)).\nClaim 5 If the samples in the dataset are not linearly separable, i.e., minw\u2208Rd,b\u2208R 1 n \u2211n i=1 I{yi 6= sgn(w>xi + b)} > 0, then empirical loss has a local minimum with a non-zero training error.\nProof: The proof follows from the proof of Proposition 2 in Appendix B.1 where we show that when the dataset has both positive and negative samples and all neurons in the multilayer network are ReLUs, then the empirical loss has a local minimum with a non-zero training error. Assume that the multilayer neural network H(x;\u03b8) has L \u2265 1 hidden layers, Ml \u2265 1 neurons in the l-th layer in the multilayer neural network H. Clearly, ML = d. Now we let the vector \u03b8l contain all parameters in the first l \u2208 [L] layers. Then the output of the neural network f(x;a,\u03b8, b) can be rewritten as\nf(x;a,\u03b8, b) = b+ ML\u2211 j=1 aj\u03c3(w > j \u03a6(x;\u03b8L\u22121) + bj) + a >x,\nwhere \u03a6(x;\u03b8L\u22121) = (\u03a61(x;\u03b8L\u22121), ...,\u03a6ML\u22121(x;\u03b8L\u22121)) denotes the outputs of the neurons in the layer L \u2212 1. Now we construct a local minimum (a\u2217,\u03b8\u2217, b\u2217). The whole idea of constructing the local minimum having a non-zero training error is as follows. We first appropriately choose wj , bj such that all neurons in the last layer of the multilayer network H keep inactive on all samples in the dataset. Then the neural network becomes a linear model\nf(x;a\u2217,\u03b8\u2217, b\u2217) = b\u2217 + a\u2217>x.\nNext we only need to set a\u2217, b\u2217 to the global optimizer of the convex optimization problem\nmin a\u2208Rd,b\u2208R\n1\nn n\u2211 i=1 `p ( \u2212yi(a>xi + b) ) .\nTherefore, as we have shown in the proof of Propsition 2, if we slightly perturb the parameter \u03b8\u2217, the output of the multilayer network H(x; \u03b8\u0303) on all samples are still zero and this makes f(xi;a\n\u2217, \u03b8\u0303, b\u2217) = a\u2217>xi + b\n\u2217. In addition, if we further perturb the vector a\u2217 and b\u2217, the value of the empirical loss will not decrease since a\u2217 and b\u2217 are the global optimizer of the empirical loss function. Now we present the proof. We first set \u03b8L\u22121 to any unit vector \u03b8 \u2217 L\u22121 : \u2016\u03b8\u2217L\u22121\u20162 = 1. Next, for any data set D = {(xi; yi)}ni=1, we define K = max\ni\u2208[n] \u2016\u03a6(xi;\u03b8\u2217L\u22121)\u20162.\nIn addition, it is easy to show that the function \u03d5ij(\u03b8) = \u03a6j(xi;\u03b8) is a continuous function. Now we consider the compact set C\u03b4 = {\u03b8 : \u2016\u03b8 \u2212 \u03b8\u2217L\u22121\u20162 \u2264 \u03b4}, where \u03b4 > 0 . Since each function \u03d5ij is a continuous function on the compact set C, then by the definition of continuity,\n\u2200\u03b5 > 0,\u2203\u03b4ij(\u03b5) \u2208 (0, 1) : |\u03d5ij(\u03b8)\u2212 \u03d5ij(\u03b8\u2217L\u22121)| \u2264 \u03b5 for all \u03b8 \u2208 C\u03b4ij .\nFor a given \u03b5 > 0, let \u03b4(\u03b5) = min\ni\u2208[n],j\u2208[ML\u22121] \u03b4ij(\u03b5),\nthen for all i \u2208 [n], j \u2208 [ML\u22121] and \u2200\u03b8 \u2208 C\u03b4,\n|\u03d5ij(\u03b8)\u2212 \u03d5ij(\u03b8L\u22121)| \u2264 \u03b5.\nNow we set wj to some unit vector wj : \u2016wj\u20162 = 1 for all j \u2208 [ML\u22121], and we set bj to a scalar b\u2217j satisfying\nw\u2217j >\u03a6(xi;\u03b8 \u2217 L\u22121) + b \u2217 j \u2264 \u22121, for all i \u2208 [n] and all \u03b8 \u2208 C.\nTherefore, the neural network becomes\nf(xi;a, \u03b8\u0303, b) = a >xi + b, \u2200i \u2208 [n].\nFurthermore, for the \u03b4(\u03b5) defined above and for any parameter vector \u03b8\u0303L : \u2016\u03b8\u0303L\u2212\u03b8\u2217L\u20162 \u2264 \u03b4(\u03b5), we have for all j \u2208 [ML\u22121] and all i \u2208 [n],\n|w\u0303>j \u03a6(xi; \u03b8\u0303L\u22121) + b\u0303j \u2212w\u2217j>\u03a6(xi;\u03b8\u2217L\u22121)\u2212 b\u2217j | \u2264 |w\u0303>j \u03a6(xi; \u03b8\u0303L\u22121)\u2212 w\u0303>j \u03a6(xi;\u03b8\u2217L\u22121) + w\u0303>j \u03a6(xi;\u03b8\u2217L\u22121)\u2212w\u2217j>\u03a6(xi;\u03b8\u2217L\u22121)|+ |b\u0303j \u2212 bj | \u2264 |w\u0303>j \u03a6(xi; \u03b8\u0303L\u22121)\u2212 w\u0303>j \u03a6(xi;\u03b8\u2217L\u22121)|+ |w\u0303>j \u03a6(xi;\u03b8\u2217L\u22121)\u2212w\u2217j>\u03a6(xi;\u03b8\u2217L\u22121)|+ |b\u0303j \u2212 bj | \u2264 \u2016w\u0303j\u20162\u2016\u03a6(xi; \u03b8\u0303L\u22121)\u2212\u03a6(xi;\u03b8\u2217L\u22121)\u20162 + \u2016w\u0303j \u2212w\u2217j\u20162\u2016\u03a6(xi;\u03b8\u2217L\u22121)\u20162 + |b\u0303j \u2212 bj | \u2264 (1 + \u03b4(\u03b5)) \u221a ML\u22121\u03b5+ \u03b5K + \u03b5 \u2264 (2 \u221a ML\u22121 +K + 1)\u03b5.\nThus, if \u03b5 < 1 2(2 \u221a ML\u22121+K+1) , then for all \u03b8\u0303L : \u2016\u03b8\u0303L \u2212 \u03b8\u2217L\u20162 \u2264 \u03b4(\u03b5), \u2200j \u2208 [M ] and \u2200i \u2208 [n]\nw\u0303>j \u03a6(xi; \u03b8\u0303L\u22121) + b\u0303j \u2264 w\u2217j>\u03a6(xi;\u03b8\u2217L\u22121) + b\u2217j + 1 2 \u2264 \u22121 2 . (16)\nSince \u03c3(z) = 0 for all z \u2264 0, then this indicates that for all \u03b8\u0303L : \u2016\u03b8\u0303L \u2212 \u03b8\u2217L\u20162 \u2264 \u03b4(\u03b5),\nf(xi;a, \u03b8\u0303, b) = a >xi + b, for all i \u2208 [n].\nFinally, we set a\u2217, b\u2217 to the global minimizer of the following convex optimization problem:\nmin a\u2208Rd,b\u2208R\n1\nn n\u2211 i=1 `p ( \u2212yi(a>xi + b) ) .\nThis indicates that for any a \u2208 Rd, b \u2208 R,\n1\nn n\u2211 i=1 `p(\u2212yi(a>xi + b)) \u2265 1 n n\u2211 i=1 `p(\u2212yi(a\u2217>xi + b\u2217)).\nTherefore, for \u03b8\u0303L : \u2016\u03b8\u0303L \u2212 \u03b8\u2217L\u20162 \u2264 \u03b4(\u03b5) and any a \u2208 Rd, b \u2208 R\nL\u0302n(a, \u03b8\u0303L, b; p) = 1\nn n\u2211 i=1 `p(\u2212yif(xi;a, \u03b8\u0303L, b)) = 1 n n\u2211 i=1 `p(\u2212yi(a>xi + b))\n\u2265 1 n n\u2211 i=1 `p(\u2212yi(a\u2217>xi + b\u2217)) \u2265 1 n n\u2211 i=1 `p(\u2212yif(xi; a\u22170,\u03b8\u2217L, b\u2217)) = L\u0302n(a\u2217,\u03b8\u2217L, b\u2217; p).\nThis means that (a\u2217,\u03b8\u2217L, b \u2217) is a local minima and f(xi;a \u2217,\u03b8\u2217L, b \u2217) = a\u2217>xi + b \u2217 for all i \u2208 [n]. This further indicates that\nR\u0302n(\u03b8 \u2217) \u2265 min\nw\u2208Rd,b\u2208R\n1\nn n\u2211 i=1 I{yi 6= sgn(w>xi + b)} > 0.\nNow we consider the following distribution PX\u00d7Y defined on the Rd. Let PX|Y=1 is a uniform distribution on the region [1, 2] \u222a [\u22122,\u22121] \u00d7 {0}d\u22121 and PX|Y=\u22121 is a uniform distribution on the region {0} \u00d7 [1, 2] \u222a [\u22122,\u22121] \u00d7 {0}d\u22122. In addition, let PY (Y = 1) = PY (Y = \u22121) = 0.5 Clearly, r+ = r\u2212 = 1 < r = 2 and this distribution satisfies the assumptions in Theorem 1. Furthermore, with probability at least 1 \u2212 1\n4n\u22121 , there exists at least one sample in the following four regions: [1, 2]\u00d7{0}d\u22121, [\u22122,\u22121]\u00d7{0}d\u22121, {0}\u00d7 [1, 2]\u00d7{0}d\u22122 and {0}\u00d7 [\u22122,\u22121]\u00d7{0}d\u22122 and this makes the samples in the dataset not linearly separable."}, {"heading": "B.7 Proof of Example 1", "text": "Example 2 Let the distribution PX\u00d7Y satisfy that P(Y = 1) = P(Y = \u22121) = 0.5, P(X = 5/4|Y = 1) = 1 and P(X|Y = \u22121) is a uniform distribution on the interval [0, 1]. For a linear model f(x; a, b) = ax + b, a, b \u2208 R, then every global minimum (a\u2217, b\u2217) of the population loss L(a, b) = EX\u00d7Y [(1 \u2212 Y f(X; a, b))2] satisfies PX\u00d7Y [Y 6= sgn(f(X; a\u2217, b\u2217))] \u2265 1/16.\nProof: The proof is simple. We first consider a simpler form of the problem. Given the distribution PX\u00d7Y , the optimal linear estimator E\u0302[Y |X] is\nE\u0302[Y |X] = E[Y ] + Cov(Y,X)V ar\u22121(X)(X \u2212 E[X]).\nSince E[Y ] = 0, Cov(Y,X) = E[XY ]\u2212 E[X]E[Y ] = 1, V ar(X) > 0, E[X] = 7/8, the misclassification rate is 1/16."}, {"heading": "B.8 Proof of Example 3 and 4", "text": "In this subsection, we present two counterexamples to show that neither Theorem 1 nor 2 holds if we replace the loss function with the quadratic loss.\nExample 3 Let the distribution PX\u00d7Y defined on R2 \u00d7 {\u22121, 1} satisfy that P(Y = 1) = P(Y = \u22121) = 0.5, P(X = (\u03b1, 0)|Y = 1) = P(X = (1, 0)|Y = 1) = 0.5 and P(X = (0, \u03b1)|Y = \u22121) = P(X = (0, 1)|Y = \u22121) = 0.5. Assume that samples in the dataset D = {(xi, yi)}4ni=1 are independently drawn from the distribution PX\u00d7Y . Assume that the network fS has M \u2265 2 neurons and all neurons in the network fS are quadratic neurons, i.e., \u03c3(z) = z\n2. Then there exists an \u03b1 \u2208 [0, 1] such that every global minimum of the empirical loss function L\u03024n(\u03b8) = 1 4n \u22114n i=1(1\u2212 yif(xi;\u03b8))2 has a training error greater than 1/8 with probability at least \u2126(1/n3).\nRemark: This is a counterexample for Theorem 1. It is easy to check that the distribution satisfies assumption 2 and 3, where r = 2 > max{1, 1} = max{r+, r\u2212}. Proof: Let X = (X1, X2). Set the feedforward network fD to a constant. Since the positive and negative samples locate on two orthogonal subspaces, then it is easy to check that under this distribution, for any quadratic function of the form g(X1, X2) = a1X 2 1 + a2X 2 2 + a0, there always exists a neural\nnetwork of the form f(X1, X2) = a0 + \u2211M j=1 aj(wj1X1 + wj2X2) 2 = a0 + \u2211M j=1 aj(w 2 j1X 2 1 + w 2 j2X 2 2 ), M \u2265 2 satisfying PX\u00d7Y (f(X) = g(X)) = 1.\nIn addition, for any neural network f(X1, X2) = a0+ \u2211M j=1 aj(wj1X1+wj2X2) 2, there exists a quadratic function of the form g(X1, X2) = a1X 2 1 + a2X 2 2 + a0 satisfying\nPX\u00d7Y (f(X) = g(X)) = 1.\nThis indicates that the optimal neural network f(x;\u03b8\u2217) should be the solution of\nmin a0\u2208R,a\u2208R2\n1\n4n 4n\u2211 i=1 ( 1\u2212 yi ( a0 + a1(x (1) i ) 2 + a2(x (2) i ) 2 )) .\nLet n1, n2, n3 and n4 denote the number of samples at the point (\u03b1, 0), (1, 0), (0, \u03b1) and (0, 1), respectively. We only need to focus the case where n1 = n2 = n3 = n4 = n. In this case, the optimal linear estimator should be of the form\ng(X21 , X 2 2 ; a \u2217 0, a \u2217 1, a \u2217 2) = a \u2217 1(X 2 1 \u2212 E\u0302X21 ) + a\u22172(X22 \u2212 E\u0302X22 ) = a\u22171 ( X21 \u2212 1 + \u03b12\n4\n) + a\u22172 ( X22 \u2212 1 + \u03b12\n4\n) .\nWhen \u03b1 = 1/2, then 1+1/44 = 5/16 > 1/4 = \u03b1 2 and 1+1/44 = 5/16 < 1. Therefore, (1 +\u03b1 2)/4 \u2208 (\u03b12, 1). In this case, for any a\u22171, a \u2217 2, the training error cannot be smaller than 1/4. This can be easily seen by investigating positive and negative samples separately. For positive samples at (1, 0), the output of the network is g(1, 0; a\u22170, a \u2217 1, a \u2217 2) = a \u2217 1(1\u2212 (1 +\u03b12)/4). For positive samples at (\u03b1, 0), the output of the network is g(\u03b1, 0; a\u22170, a \u2217 1, a \u2217 2) = a \u2217 1(\u03b1\n2 \u2212 (1 + \u03b12)/4). Since \u03b12 < 1+\u03b124 < 1, then if a\u22171 6= 0, then the network will misclassify all samples at (\u03b1, 0) or (1, 0). This indicates that a\u22171 = 0 or training error is no smaller than 1/4. Using the same analysis on the negative samples, we will have a\u22172 = 0 or training error is no smaller than 1/4. This indicates that the output of the network is a constant equal to zero, which has a training error 1/2. In all, the training error is no smaller than 1/4. The probability of the case where n1 = n2 = n3 = n4 is(\n4n\n2n\n)( 2n\nn )2 1 44n > 42n 2n+ 1 ( 4n n+ 1 )2 1 44n = 1 (2n+ 1)(n+ 1)2\nExample 4 Let the distribution PX\u00d7Y satisfy that P(Y = 1) = P(Y = \u22121) = 0.5, P(X = 1 + \u03b1|Y = 1) = P(X = 1 + 2\u03b1|Y = 1) = 0.5 and P(X = 0|Y = \u22121) = P(X = 1|Y = \u22121) = 0.5. Assume that samples in the dataset D = {(xi, yi)}4ni=1 are independently drawn from the distribution PX\u00d7Y . Assume that the network fS has M \u2265 1 neurons and each neuron is a linear neuron \u03c3(z) = z. If \u03b1 \u2208 [0, 1/6], then every global minimum of the empirical loss function L\u03024n(\u03b8) = 1 4n \u22114n i=1(1 \u2212 yif(xi; \u03b8))2 has a training error greater than 1/8 with probability at least \u2126(1/n3).\nRemark: This is counterexample for Theorem 4. It is easy to check that distribution is linearly separable.\nProof: Let n\u22121, n1, n1+\u03b1 denote the number of samples at the point \u22121, 1 and 1 + \u03b1. We only need to focus the case where n\u22121 = n, n1 = n and n1+\u03b1 = 2n. Since the network is a linear network, then under this distribution, the optimal linear estimator should be of the form\nf(x;\u03b8) = a\u2217 ( x\u2212 3 + 3\u03b1\n4\n) .\nIf a\u2217 = 0, then the training error is 1/2. If a\u2217 > 0, then the training error is 1/4, due to the misclassification of all points at x = 1. If a\u2217 < 0, then the training error is 3/4, due to the misclassification of all points at x = 1 + \u03b1 and x = \u22121. This means that the training error in this case should be greater or equal to 1/4. The probability of this case is(\n4n\n2n\n)( 2n\nn )2 1 44n > 42n 2n+ 1 ( 4n n+ 1 )2 1 44n = 1 (2n+ 1)(n+ 1)2"}, {"heading": "B.9 Proof of Proposition 8", "text": "Proposition 20 Let f : Rd \u2192 R denote a feedforward network parameterized by \u03b8 and let the dataset have n samples. When the loss function `p satisfies assumption 1 and p \u2265 1, we have min\u03b8 L\u0302n(\u03b8; p) = 0 if and only if min\u03b8 R\u0302n(\u03b8) = 0. Furthermore, if min\u03b8 R\u0302n(\u03b8) = 0, every global minimum \u03b8\n\u2217 of the empirical loss L\u0302n(\u03b8; p) has zero training error, i.e., R\u0302n(\u03b8 \u2217) = 0.\nRemark: Using the same proof shown as follows, we can show that Proposition 8 holds for any multilayer network architectures satisfying that for any set of parameters \u03b8 and any real numbers a, b \u2208 R, there always exists a set of parameters \u03b8\u0303 such that f(x; \u03b8\u0303) = a(f(x;\u03b8)\u2212 b) holds for all x. It is easy to check that both network architectures in Fig. 1 satisfy this condition.\nProof: We first prove the \u201conly if\u201d part. The proof is trivial since, by definition `p(z) \u2265 I{z \u2265 0}, then\nR\u0302n(\u03b8) = 1\nn n\u2211 i=1 I{yi 6= sgn(f(xi;\u03b8))} \u2264 1 n n\u2211 i=1 I{yif(xi;\u03b8) \u2264 0} \u2264 1 n n\u2211 i=1 `p(\u2212yif(xi;\u03b8)) = L\u0302n(\u03b8; p).\nTherefore, if min\u03b8 L\u0302n(\u03b8; p) = 0 then min\u03b8 R\u0302n(\u03b8) = 0. Next, we prove the \u201cif\u201d part. If min\u03b8 R\u0302n(\u03b8) = 0, then there exists a set of parameter \u03b8 \u2217 such that I{yi 6= sgn(f(x;\u03b8\u2217))} = 0 holds for all i \u2208 [n]. This indicates that f(xi;\u03b8\u2217) \u2265 0 for all i : yi = 1 and f(xi;\u03b8\n\u2217) < 0 for all i : yi = \u22121. This means that there exists two real numbers c1 < c2 such that f(xi;\u03b8 \u2217) > c2 holds for all i : yi = 1 and f(xi;\u03b8 \u2217) < c1 holds for all i : yi = \u22121. Now, we define a new network f(x; \u03b8\u0303) = \u03b1(f(x;\u03b8\u2217) \u2212 c1+c22 ). Therefore, for this network f(x; \u03b8\u0303), we have f(xi; \u03b8\u0303) > \u03b1(c2 \u2212 c1)/2 holds for all i : yi = 1 and f(xi; \u03b8\u0303) < \u2212\u03b1(c2 \u2212 c1)/2 holds for all i : yi = \u22121. Since `p(z) = 0 iff z \u2264 \u2212z0, then by choosing \u03b1 > 2z0c2\u2212c1 , we have yif(xi; \u03b8\u0303) > z0 holds for \u2200i \u2208 [n]. This means that L\u0302n(\u03b8\u0303; p) = 0. Now we need to show that there exits a set of parameter \u03b8\u0303 such that\nf(x; \u03b8\u0303) = \u03b1 ( f(x;\u03b8\u2217)\u2212 c1 + c2\n2\n) .\nSince the output of the neural network can be written as\nf(x;\u03b8) = a0 + ML\u2211 j=1 aj\u03c3(w > j \u03a6(x;\u03b8) + bj),\nwhere ML denotes the number of neurons in the last layer and \u03a6(xi;\u03b8) denotes the outputs from the previous layers. Then by shifting a0 and scaling aj , we have\nf(x; \u03b8\u0303) = \u03b1 ( f(x;\u03b8\u2217)\u2212 c1 + c2\n2 ) = a\u22170 \u2212 \u03b1(c1 + c2)\n2 + ML\u2211 j=1 \u03b1a\u2217j\u03c3(w \u2217>\u03a6(x;\u03b8\u2217) + b\u2217j )\n= a\u03030 + ML\u2211 j=1 a\u0303j\u03c3(w \u2217>\u03a6(x;\u03b8\u2217) + b\u2217j ).\nTherefore, this means that there exists a set of parameters \u03b8\u0303 such that L\u0302n(\u03b8\u0303; p) = 0, i.e., min\u03b8 L\u0302n(\u03b8; p) = 0. This means, the global minimum of the empirical loss L\u0302n(\u03b8; p) is zero. Furthermore, since R\u0302n(\u03b8) \u2264 L\u0302n(\u03b8; p) holds for all \u03b8, then every global minimum of the empirical loss has zero training error."}, {"heading": "B.10 Proof of Proposition 9", "text": "Proposition 21 Assume that the loss function is the logistic loss, i.e., `(z) = log2(1 + e z). Assume that assumptions 2-5 are satisfied. Assume that samples in the dataset D = {(xi, yi)}ni=1, n \u2265 1 are independently drawn from the distribution PX\u00d7Y . Assume that the number of neurons M in the network fS satisfies M \u2265 2 max{ n\u2206r , r+, r\u2212}, where \u2206r = r\u2212max{r+, r\u2212}. If a set of real parameters \u03b8\u2217 denotes a critical point of the empirical loss L\u0302n(\u03b8), then \u03b8 \u2217 is a saddle point.\nProof: We first recall some notations defined in the paper. The output of the neural network is\nf(x;\u03b8) = fS(x;\u03b8S) + fD(x;\u03b8D),\nwhere fS(x;\u03b8S) is the single layer neural network parameterized by \u03b8S , i.e.,\nfS(x;\u03b8S) = a0 + M\u2211 j=1 aj\u03c3 ( w>j x ) ,\nand fD(x;\u03b8D) is a deep neural network parameterized by \u03b8D. The empirical loss function is given by\nL\u0302n(\u03b8) = L\u0302n(\u03b8S ,\u03b8D) = 1\nn n\u2211 i=1 `(\u2212yif(xi;\u03b8)).\nWe assume that there exists a local minimum \u03b8\u2217 = (\u03b8\u2217S ,\u03b8 \u2217 D). We next complete the proof by proving the following two claims:\nClaim 6 If there exists j \u2208 [M ] such that a\u2217j = 0, then \u03b8\u2217 is not a local minimum.\nClaim 7 If a\u2217j 6= 0 for all j \u2208 [M ], then \u03b8\u2217 is not a local minimum.\nTherefore, these two claims contradict with the assumption that \u03b8\u2217 = (\u03b8\u2217S ,\u03b8 \u2217 D) is a local minimum. Therefore, every critical point is not a local minimum. In addition, it is very easy to show that every critical point is not a local maximum, since the loss function is strictly convex with respect to a0. Therefore, every critical point is a saddle point. (a) Proof of Claim 6. In this part, we prove that if there exists j \u2208 [M ] such that a\u2217j = 0, then \u03b8\u2217 is not a local minima. Without loss of generality, we assume that a\u22171 = 0. Using the same analysis presented in the proof of Theorem 1, we have\nn\u2211 i=1 `\u2032(\u2212yif(xi;\u03b8))(\u2212yi)\u03c3\u2032\u2032 ( w\u22171 >xi ) xix > i = 0d\u00d7d.\nBy assumption that there exists a set of orthogonal basis E = {e1, ..., ed} in Rd and a subset U+ \u2286 E such that PX|Y (X \u2208 Span(U1)|Y = 1) = 1 and by assumption that r = |U+ \u222a U\u2212| > max{r+, r\u2212} = max{|U+|, |U\u2212|}, then the set U+\\U\u2212 is not an empty set. It is easy to show that for any vector v \u2208 U+\\U\u2212, PX\u00d7Y (v>X = 0|Y = 1) = 0. We prove it by contradiction. If we assume p = PX\u00d7Y (v>X = 0|Y = 1) > 0, then for random vectors X1, ...,X|U+| independently drawn from the conditional distribution PX|Y=1,\nPX|Y=1 |U+|\u22c3 i=1 { v>Xi = 0 } \u2223\u2223\u2223\u2223\u2223Y = 1  = |U+|\u220f i=1 PX|Y=1 ( v>Xi = 0|Y = 1 ) = p|U+| > 0.\nFurthermore, since X1, ...,X|U+| \u2208 Span(U+), v>Xi = 0, i = 1, ..., |U+| and v \u2208 U+, then the rank of the matrix ( X1, ...,X|U+| ) is at most |U+| \u2212 1 and this indicates that the matrix is not a full rank matrix with probability p|U+| > 0. This leads to the contradiction with the Assumption 2. Thus, with probability 1, v>xi 6= 0 for all i : yi = 1 and v>xi = 0 for all i : yi = \u22121. Proof of Claim 7: Now we have proved that a\u2217j 6= 0 for all j \u2208 [M ]. Here, we define M0 = dM/2e. Since M0 \u2265 max{r+, r\u2212}, and max{r+, r\u2212}+ min{r+, r\u2212} \u2265 r, then\n2M0 \u2265 2 max{r+, r\u2212} > 2r \u2212 r+ \u2212 r\u2212 \u2265 2 min{r \u2212 r+, r \u2212 r\u2212} , 2K.\nThus, there exists ai1 , ..., aiM0 , i1 < i2 < ... < iM0 such that\nsgn(ai1) = ... = sgn(aiM0 ).\nWithout loss of generality, we assume that sgn(a1) = ... = sgn(aM0) = +1. Now we prove the claim 7. First, we consider the Hessian matrix H(w\u22171, ...,w \u2217 M0 ). Since \u03b8\u2217 is a local minima with R\u0302n(\u03b8 \u2217) > 0, then\nF (u1, ...,uM0) = M0\u2211 j=1 M0\u2211 k=1 u>j \u22072wj ,wk L\u0302n(\u03b8 \u2217)uk \u2265 0\nholds for any vectors u1, ...,uM0 \u2208 Rd. Since\n\u22072wj L\u0302n(\u03b8\u2217) = a\u2217j n\u2211 i=1 `\u2032(\u2212yif(xi;\u03b8\u2217))(\u2212yi)\u03c3\u2032\u2032 ( w\u2217j >xi ) xix > i\n+ a\u2217j 2 n\u2211 i=1 `\u2032\u2032(\u2212yif(xi;\u03b8\u2217)) [ \u03c3\u2032 ( w\u2217j >xi )]2 xix > i ,\nand\n\u22072wj ,wk L\u0302n(\u03b8 \u2217; p) = a\u2217ja \u2217 k n\u2211 i=1 `\u2032\u2032(\u2212yif(xi;\u03b8\u2217)) [ \u03c3\u2032 ( w\u2217j >xi )] [ \u03c3\u2032 ( w\u2217k >xi )] xix > i .\nThus, we have for any u1, ...,uM0 \u2208 Rd,\nF (u1, ...,uM0) = \u22122 n\u2211 i=1 `\u2032(\u2212yif(xi;\u03b8\u2217))yi M0\u2211 j=1 [ a\u2217j\u03c3 \u2032\u2032 (w\u2217jxi) (u>j xi)2] \n+ 4 n\u2211 i=1 `\u2032\u2032(\u2212yif(xi;\u03b8\u2217)) M0\u2211 j=1 a\u2217j\u03c3 \u2032 ( w\u2217j >xi )( u>j xi )2 . Now we find some coefficients \u03b11, ..., \u03b1M0 , not all zero and vectors u1, ...,uM0 satisfying\nM0\u2211 j=1 \u03b1j\u03c3 \u2032 ( w\u2217j >xi ) u>j xi = 0, \u2200i \u2208 [n],\nand \u2200i : yi = \u22121 and \u2200j \u2208 [M0], u>j xi = 0.\nSince \u03b8\u2217 is a local minima, then by Lemma 1, we have\nn\u2211 i=1 `\u2032(\u2212yif(xi;\u03b8\u2217))yi\u03c3\u2032(w\u2217j>xi)xi = 0d.\nConsider the orthogonal vectors e1, ..., eK from the set of orthogonal basis e1, ..., ed satisfying that, with probability 1, \u2200j \u2208 [K], \u2200i : yi = \u22121, e>j xi = 0 and \u2200i : yi = 1, e>j xi 6= 0. Then, considering the following set of linear equations\nn\u2211 i=1 `\u2032(\u2212yif(xi;\u03b8\u2217))yi\u03c3\u2032(w\u22171>xi) ( e>1 xi ) = 0, ..., n\u2211 i=1 `\u2032(\u2212yif(xi;\u03b8\u2217))yi\u03c3\u2032(w\u2217M0 >xi) ( e>1 xi ) = 0,\n... n\u2211 i=1 `\u2032(\u2212yif(xi;\u03b8\u2217))yi\u03c3\u2032(w\u22171>xi) ( e>Kxi ) = 0, ..., n\u2211 i=1 `\u2032(\u2212yif(xi;\u03b8\u2217))yi\u03c3\u2032(w\u2217M0 >xi) ( e>Kxi ) = 0.\nThese equations can be rewritten in a matrix form \u03c3\u2032(w\u22171 >x1) ( e>1 x1 ) ... \u03c3\u2032(w\u22171 >xn) ( e>1 xn ) ... ... ... \u03c3\u2032(w\u2217M0 >x1) ( e>1 x1 ) ... \u03c3\u2032(w\u2217M0 >xn) ( e>1 xn ) ... ... ... \u03c3\u2032(w\u22171 >x1) ( e>Kx1 ) ... \u03c3\u2032(w\u22171 >xn) ( e>Kxn ) ... ... ...\n\u03c3\u2032(w\u2217M0 >x1) ( e>Kx1 ) ... \u03c3\u2032(w\u2217M0 >xn) ( e>Kxn )  (KM0\u00d7n)\ufe38 \ufe37\ufe37 \ufe38\nP\n `\u2032(\u2212y1f(x1;\u03b8\u2217))y1 `\u2032(\u2212y2f(x2;\u03b8\u2217))y2 ... ... ... ...\n... `\u2032(\u2212ynf(x1;\u03b8\u2217))yn  \ufe38 \ufe37\ufe37 \ufe38\nq\n= 0n\nor Pq = 0n.\nSince M0K \u2265 MK/2 \u2265 n, then if rank(P ) = n, we should have q = 0n and this indicates that `\u2032(\u2212yif(xi;\u03b8\u2217)) = 0 for all i \u2208 [n] and this contradicts with the fact that `\u2032(z) = 11+e\u2212z > 0 for all z \u2208 R. Therefore, rank(P ) < n \u2264 M0K. This means the raw vectors of the matrix P is linearly dependent and thus we have that there exists coefficients vectors (\u03b211, ..., \u03b21K), ..., (\u03b2M01, ..., \u03b2M0K), not all zero vectors, such that\nK\u2211 s=1 M0\u2211 j=1 \u03c3\u2032(w\u2217j >xi)\u03b2js(e > s xi) = 0, \u2200i \u2208 [n],\nor M0\u2211 j=1 a\u2217j\u03c3 \u2032(w\u2217j >xi)\n( 1\na\u2217j K\u2211 s=1 \u03b2jses\n)> xi = 0, \u2200i \u2208 [n],\nDefine uj = 1 a\u2217j\n\u2211K s=1 \u03b2jses for j = 1, ...,M0, then we have\nM0\u2211 j=1 a\u2217j\u03c3 \u2032(w\u2217j >xi)u > j xi = 0, \u2200i \u2208 [n]. (17)\nFurthermore, since uj \u2208 Span({e1, ..., eK}), and with probability 1, \u2200i : yi = \u22121 and \u2200j \u2208 [K], e>j xi = 0, then we have that \u2200j \u2208 [M ] and \u2200i : yi = \u22121: u>j xi = 0. Thus,\nF (u1, ...,uM0) = \u22122 n\u2211 i=1 `\u2032(\u2212yif(xi;\u03b8\u2217))yi M0\u2211 j=1 [ a\u2217j\u03c3 \u2032\u2032 (w\u2217jxi) (u>j xi)2]  by Eq. (17)\n= \u22122 \u2211 i:yi=1 `\u2032(\u2212yif(xi;\u03b8\u2217)) M0\u2211 j=1 [ a\u2217j\u03c3 \u2032\u2032 (w\u2217jxi) (u>j xi)2]  \u2265 0. (18)\nSince \u03c3\u2032\u2032(z) > 0 for all z \u2208 R and a\u2217j > 0 for all j \u2208 [M0], then we have\n`\u2032(\u2212yif(xi;\u03b8\u2217)) M0\u2211 j=1 [ a\u2217j\u03c3 \u2032\u2032 (w\u2217jxi) (u>j xi)2] \u2265 0, \u2200i : yi = 1 and this leads to F (u1, ...,uM0) \u2264 0. Together with Eq. (18), we have\nF (u1, ...,uM0) = 0\nand thus\n`\u2032(\u2212yif(xi;\u03b8\u2217)) M0\u2211 j=1 [ a\u2217j\u03c3 \u2032\u2032 (w\u2217jxi) (u>j xi)2] = 0, \u2200i : yi = 1. (19) Now we split the index {i \u2208 [n] : yi = 1} set into two disjoint subset C0, C1:\nC0 = {i \u2208 [n] : yi = 1, and \u2203j \u2208 [M0],u>j xi 6= 0}, C1 = {i \u2208 [n] : yi = 1 and \u2200j \u2208 [M0],u>j xi = 0}.\nClearly, for all i \u2208 C0, by the fact that aj > 0 for all j \u2208 [M0] and \u03c3\u2032\u2032(z) > 0 for all z \u2208 R, we have\nM0\u2211 j=1 [ a\u2217j\u03c3 \u2032\u2032 (w\u2217jxi) (u>j xi)2] > 0, and this leads to `\u2032(\u2212yif(xi;\u03b8\u2217)) = 0, \u2200i \u2208 C0, which contradict with the fact that `\u2032(z) > 0 for all z \u2208 R. Therefore, C0 = \u2205. Now we need to consider the index set C1. First, it is easy to show that with probability 1, |C1| < r+ \u2264 M0. This is due to the fact that there exists a non-zero vector uj , such that u > j xi = 0 for all i \u2208 C1 and that\nuj \u2208 Span({e1, ..., eK}). Therefore, u>j xi = \u2211K s=1(u > j es)(x > i es) = \u2211r+ s=1(u > j es)(x > i es) = 0 holds for all i \u2208 C1. If |C1| \u2265 r+, then with probability 1, the matrix e>1 x1 ... e>r+x1... ... ... e>1 xr+ ... e > r+xr+\n has the full rank equal to r+ and this makes u > j es = 0 for all s \u2208 [k]. This contradicts with the fact that uj \u2208 Span({e1, ..., eK}) and uj is not a zero vector. Thus, |C1| < r+ \u2264 M0. Now we consider\nthe function F , since \u2200i \u2208 C0 : `\u2032(\u2212yif(xi;\u03b8\u2217)) = 0, then for all u1, ...,uM0 ,\nF (u1, ...,uM0) = \u22122 \u2211 i\u2208C1 `\u2032(\u2212yif(xi;\u03b8\u2217)) M0\u2211 j=1 [ a\u2217j\u03c3 \u2032\u2032 (w\u2217jxi) (u>j xi)2] \n+ 4 \u2211 i\u2208C1 `\u2032\u2032(\u2212yif(xi;\u03b8\u2217)) M0\u2211 j=1 a\u2217j\u03c3 \u2032 ( w\u2217j >xi )( u>j xi )2 Now we set uj = \u03b1je1, j = 1, ...,M0 for some scalar \u03b1j . Now we only need find \u03b11, ..., \u03b1M0 such that\nM0\u2211 j=1 \u03b1ja \u2217 j\u03c3 \u2032 ( w\u2217j >xi ) e>1 xi = 0, \u2200i \u2208 C1.\nSince |C1| \u2264M0 \u2212 1 < M0, then there exists \u03b1\u22171, ..., \u03b1\u2217M0 , not all zeros, such that\nM0\u2211 j=1 \u03b1\u2217ja \u2217 j\u03c3 \u2032 ( w\u2217j >xi ) e>1 xi = 0, \u2200i \u2208 C1.\nThen by setting uj = \u03b1 \u2217 je1, we have\nF (u1, ...,uM0) = \u22122 \u2211 i\u2208C1 `\u2032(\u2212yif(xi;\u03b8\u2217)) M0\u2211 j=1 [ |\u03b1\u2217j |2a\u2217j\u03c3\u2032\u2032 ( w\u2217jxi ) ( e>1 xi )2] \u2265 0. .\nSimilarly, since |\u03b11|, ..., |\u03b1M0 | are not all zeros, a\u2217j > 0 for all j \u2208 [M0], \u03c3\u2032\u2032(z) > 0 for all z \u2208 R and e>1 xi 6= 0 holds for all i with probability 1, then\n`\u2032(\u2212yif(xi;\u03b8\u2217)) = 0, \u2200i \u2208 C1.\nTherefore, this indicates that `\u2032(\u2212yif(xi;\u03b8\u2217)) = 0, \u2200i : yi = 1.\nSince `\u2032(z) > 0 holds for all z \u2208 R, then this leads to the contradiction. Therefore, \u03b8\u2217 is not a local minima."}, {"heading": "B.11 Proof of Proposition 13", "text": "Proposition 13 Assume that the loss function ` is the logistic loss, i.e., `(z) = log2(1 + e z). Assume that the network architecture satisfies assumption 4. Assume that samples in the dataset D = {(xi, yi)}ni=1, n \u2265 1 are independently drawn from a distribution satisfying assumption 6. Assume that the single layer network fS has M \u2265 1 neurons and neurons \u03c3 in the network fS are twice differentiable and satisfy \u03c3\u2032(z) > 0 for all z \u2208 R. If a set of real parameters \u03b8\u2217 = (\u03b8\u2217S ,\u03b8\u2217D) denotes a local minimum of the loss function L\u0302n(\u03b8S ,\u03b8D; p), p \u2265 3, then R\u0302n(\u03b8\u2217S ,\u03b8\u2217D) = 0 holds with probability one.\nProof: We first prove that, if a set of real parameters \u03b8\u2217 denotes a critical point, then \u03b8\u2217 is a saddle point. We prove it by contradiction. We assume that \u03b8\u2217 denotes a local minima. By assumption that \u03b8\u2217 = (\u03b8\u22171,\u03b8 \u2217 2) is a local minima and by the necessary condition presented in Lemma 1, we have\nn\u2211 i=1 `\u2032(\u2212yif(xi;\u03b8\u2217))yi\u03c3\u2032(w\u2217j>xi)xi = 0d.\nThus, for any w \u2208 Rd, we have n\u2211 i=1 `\u2032(\u2212yif(xi;\u03b8\u2217))\u03c3\u2032(w\u2217j>xi)yi(w>xi) = 0.\nFurthermore, for the cross entropy loss function, we have\n`\u2032(z) = 1\n1 + exp(\u2212z) > 0, \u2200z \u2208 R.\nThus, by assumption that \u03c3\u2032(z) > 0 for all z \u2208 R and assumption that there exists a vector w \u2208 Rd such that PX\u00d7Y (Y (w>X) > 0) = 1, then there exists a constant c such that for all samples in the dataset i \u2208 [n],\nyiw >xi > c > 0.\nThus, we have\n0 = n\u2211 i=1 `\u2032(\u2212yif(xi;\u03b8\u2217))\u03c3\u2032(w\u2217j>xi)yi(w>xi) \u2265 c n\u2211 i=1 `\u2032(\u2212yif(xi;\u03b8\u2217))\u03c3\u2032(w\u2217j>xi) > 0,\nand this leads to the contradiction."}, {"heading": "B.12 Proof of Proposition 10", "text": "Proposition 10 Assume the dataset D = {(xi, yi)}ni=1 is consisted of both positive and negative samples. Assume that f(x;\u03b8) is a feedforward network parameterized by \u03b8. Assume that the loss function is logistic, i.e., `(z) = log2 (1 + e\nz). If the real parameters \u03b8\u2217 denote a critical point of the empirical loss L\u0302n(\u03b8 \u2217), then R\u0302n(\u03b8 \u2217) > 0.\nProof: We prove a general statement claiming that the proposition 10 holds for all differentiable loss functions satisfying `\u2032(z) > 0 for all z \u2208 R. We note that the following claim holds under the assumptions in Proposition 10.\nClaim 8 If the loss function is differentiable and satisfies `\u2032(z) > 0 for all z \u2208 R, then R\u0302n(\u03b8\u2217) > 0.\nAssume that the multilayer neural network f(x;\u03b8) has L \u2265 1 hidden layers, Ml \u2265 1 neurons in the l-th layer. Now we let the vector \u03b8l contain all parameters in the first l \u2208 [L] layers. Then the output of the neural network can be rewritten as\nf(x; a0,\u03b8L) = a0 + ML\u2211 j=1 aj\u03c3(w > j \u03a6(x;\u03b8L\u22121) + bj),\nwhere \u03a6(x;\u03b8L\u22121) = (\u03a61(x;\u03b8L\u22121), ...,\u03a6ML\u22121(x;\u03b8L\u22121)) denotes the outputs of the neurons in the layer L\u2212 1. Then the empirical loss is defined as\nL\u0302n(\u03b8) = 1\nn n\u2211 i=1 `(\u2212yif(xi;\u03b8))\nIf the point \u03b8\u2217 = (a\u22170,\u03b8 \u2217 L) denotes a critical point of the empirical loss function, then we should have, for \u2200j \u2208 [ML],\n\u2202L\u0302n(\u03b8 \u2217)\n\u2202a0 =\n1\nn n\u2211 i=1 `\u2032(\u2212yif(xi;\u03b8\u2217))(\u2212yi) = 0, (20)\n\u2202L\u0302n(\u03b8 \u2217)\n\u2202aj =\n1\nn n\u2211 i=1 `\u2032(\u2212yif(xi;\u03b8\u2217))(\u2212yi)\u03c3 ( w\u2217j >\u03a6(xi;\u03b8 \u2217 L\u22121) + bj ) = 0. (21)\nIn addition, by adding Equations (20) and (21), we have\n0 = a\u22170 \u2202L\u0302n(\u03b8\n\u2217)\n\u2202a0 + ML\u2211 j=1 a\u2217j \u2202L\u0302n(\u03b8 \u2217) \u2202aj = 1 n n\u2211 i=1 `\u2032(\u2212yif(xi;\u03b8\u2217))(\u2212yi) a\u22170 + ML\u2211 j=1 a\u2217j\u03c3 ( w\u2217j >\u03a6(xi;\u03b8 \u2217 L\u22121) + bj ) = 1\nn n\u2211 i=1 `\u2032(\u2212yif(xi;\u03b8\u2217))(\u2212yi)f(xi;\u03b8\u2217). (22)\nThis indicates that if \u03b8\u2217 is a critical point of the empirical loss, then the following equation should hold,\n1\nn n\u2211 i=1 `\u2032(\u2212yif(xi;\u03b8\u2217))yif(xi;\u03b8\u2217) = 0. (23)\nHowever, if the dataset contains both positive and the negative samples, `\u2032(z) > 0 for all z \u2208 R, then this means that if R\u0302n(\u03b8 \u2217) = 0, then\n1\nn n\u2211 i=1 `\u2032(\u2212yif(xi;\u03b8\u2217))yif(xi;\u03b8\u2217) > 0. (24)\nWe note here that the assumption that the dataset contains both positive and the negative samples is to ensure that when R\u0302n(\u03b8 \u2217) = 0, there is at least one sample in the dataset satisfying\nyif(xi;\u03b8 \u2217) > 0.\nTherefore, we have the contradiction. This indicates that R\u0302n(\u03b8 \u2217) > 0."}, {"heading": "B.13 Proof of Proposition 11", "text": "Proposition 11 Assume that assumptions 1, 4 and 5 are satisfied. For any feedforward architecture fD(x;\u03b8D), every local minimum \u03b8 \u2217 = (\u03b8\u2217S ,\u03b8 \u2217 D) of the empirical loss function L\u0302n(\u03b8S ,\u03b8D; p), p \u2265 6 satisfies R\u0302n(\u03b8 \u2217) = 0 only if the matrix \u2211n i=1 \u03bbiyixix > i is neither positive nor negative definite for all\nsequences {\u03bbi \u2265 0}ni=1 satisfying \u2211 i:yi=1 \u03bbi = \u2211 i:yi=\u22121 \u03bbi > 0 and \u2016 \u2211n i=1 \u03bbiyixi\u20162 = 0.\nProof: We prove Proposition 11 by proving the following claim. Claim 9 If there exists a sequence {\u03bbi \u2265 0}ni=1 satisfying \u2211 i:yi=1 \u03bbi = \u2211 i:yi=\u22121 \u03bbi > 0 and \u2016 \u2211n i=1 \u03bbiyixi\u20162 =\n0 such that the matrix \u2211n\ni=1 \u03bbiyixix > i is positive or negative positive definite, then there exists a feed-\nforward neural architecture fD such that the empirical loss function L\u0302n(\u03b8S ,\u03b8D; p), p \u2265 6 has a local minimum with a non-zero training error.\nProof: Let D = {(xi, yi)}ni=1 denote a dataset consisting of n samples. We rewrite the sample x as x = ( x(1), ..., x(d) ) . Consider the following network,\nf(x;\u03b8) = fS(x;\u03b8S) + fD(x;\u03b8D),\nwhere\nfS(x;\u03b8S) = a0 + M\u2211 j=1 aj\u03c3(w > j xi + bj),\nand the multilayer network is defined as follows,\nfD(x;\u03b8D) = fD(x; \u03b81, ..., \u03b8d) = n\u2211 i=1 \u00b5i d\u220f k=1 1 { x(k) \u2208 [ x (k) i \u2212 \u03b8k, x (k) i + \u03b8k ]} . (25)\nWe note here that \u00b51, ..., \u00b5n are not parameters and later we will show that this function can be implemented by a multilayer network consisted of threshold units. A useful property of the function fD(x;\u03b8D) is that if all parameters \u03b8is are positive and sufficiently smalls, then for each sample (xi, yi) in the dataset,\nfD(xi;\u03b8D) = \u00b5i.\nFurthermore, if we slightly perturb all parameters, the output of the function fD on all samples remain the same. In the proof, we use these two properties to construct the local minimum with a non-zero training error.\nBy assumption, there exists a sequence {\u03bbi \u2265 0}ni=1 satisfying \u2211 i:yi=1 \u03bbi = \u2211 i:yi=\u22121 \u03bbi > 0 and\n\u2016\u2211ni=1 \u03bbiyixi\u20162 = 0 such that the matrix \u2211ni=1 \u03bbiyixix>i is positive or negative positive definite. Without loss of generality, we assume that the matrix is positive definite. Now we construct a local minimum \u03b8\u2217. Let a\u22170 = a \u2217 1 = ... = a \u2217 M = \u22121, w\u22171 = ... = w\u2217M = 0d and b\u22171 = ... = b\u2217M = 0. Now we set \u03b8\u22171, ..., \u03b8 \u2217 d to be positive and sufficiently small such that for two different samples in the dataset, e.g., xi 6= xj , the following equations holds,\nd\u220f k=1 1 { x (k) j \u2208 [ x (k) i \u2212 2\u03b8\u2217k, x (k) i + 2\u03b8 \u2217 k ]} = 0, d\u220f k=1 1 { x (k) i \u2208 [ x (k) j \u2212 2\u03b8\u2217k, x (k) j + 2\u03b8 \u2217 k ]} = 0.\nNow we choose \u00b51, ..., \u00b5n as follows. The output of the neural network on sample xi in the dataset is f(xi;\u03b8\n\u2217) = \u00b5i \u2212M\u03c3(0). We need to choose \u00b51, ..., \u00b5n to satisfy all conditions shown as follows:\n(1) There exists i \u2208 [n] such that yi(\u00b5i \u2212M\u03c3(0)) < 0.\n(2) For all i : yi = 1 and all k : yk = \u22121,\n`\u2032(\u2212yi(\u00b5i \u2212M\u03c3(0)))\u2211 j:j=1 ` \u2032(\u2212yi(\u00b5i \u2212M\u03c3(0))) = \u03bbi\u2211 j:j=1 \u03bbj , `\u2032(\u2212yk(\u00b5k \u2212M\u03c3(0)))\u2211 j:j=\u22121 ` \u2032(\u2212yi(\u00b5i \u2212M\u03c3(0))) = \u03bbk\u2211 j:j=\u22121 \u03bbj ,\nand \u2211 j:j=1 `\u2032(\u2212yi(\u00b5i \u2212M\u03c3(0))) = \u2211 j:j=\u22121 `\u2032(\u2212yi(\u00b5i \u2212M\u03c3(0))).\nNow we start from the largest element in the sequence {\u03bbi}ni=1. Since \u2211n\ni=1 \u03bbi > 0, the define the index imax as the index of the largest element, i.e.,\nimax = arg max i \u03bbi.\nLet \u03bbmax = \u03bbimax . Now we choose \u00b5imax such that\nyimax(\u00b5imax \u2212M\u03c3(0)) = \u22121.\nThus, the index imax satisfy the first condition. Then for i 6= imax, we choose \u00b5i such that\n`\u2032(\u2212yi(\u00b5i \u2212M\u03c3(0))) = \u03bbi \u03bbmax `(\u2212yimax(\u00b5imax \u2212M\u03c3(0))) = \u03bbi \u03bbmax `\u2032(1) \u2264 `\u2032(1). (26)\nWe note here that for each i \u2208 [n], there always exists a \u00b5i solving the above equation. This can be seen by the fact that `\u2032 is continuous, `\u2032p(z) \u2265 0 and `\u2032p(z) = 0 iff z \u2264 \u2212z0. This indicates that for \u2200z > \u2212z0, `\u2032p(z) > 0, i.e., `\u2032(1) > 0 and that `\u2032(\u2212z0) = 0. Since `\u2032(z) is continuous, then for \u2200r \u2208 [0, `\u2032(1)], there always exists z \u2208 R such that `\u2032(z) = r, which further indicates that for \u2200i \u2208 [n], there always exists \u00b5i \u2208 R solving the Equation (37). Under this construction, it is easy to show that the second condition is satisfied as well. Now we only need to show that \u03b8\u2217 is local minimum. We first show that \u03b8\u2217 is a critical point of the empirical loss function. Since for \u2200j \u2208 [M ],\n\u2202L\u0302n(\u03b8 \u2217)\n\u2202aj = n\u2211 i=1 `\u2032(\u2212yi(\u00b5i \u2212M\u03c3(0)))(\u2212yi)\u03c3(0)\n= \u03c3(0) n\u2211 i=1 \u03bbi \u03bbmax `\u2032(1)(\u2212yi) = \u2212 \u03c3(0)`\u2032(1) \u03bbmax n\u2211 i=1 yi\u03bbi\n= 0 by \u2211 i:yi=1 \u03bbi = \u2211 i:yi=\u22121 \u03bbi\n\u2207wj L\u0302n(\u03b8\u2217) = n\u2211 i=1 `\u2032(\u2212yi(\u00b5i \u2212M\u03c3(0)))(\u2212yi)\u03c3\u2032(0)xi\n= \u2212\u03c3\u2032(0) n\u2211 i=1 \u03bbi \u03bbmax `\u2032(1)yixi = \u2212 \u03c3\u2032(0)`\u2032(1) \u03bbmax n\u2211 i=1 \u03bbiyixi\n= 0d by \u2225\u2225\u2225\u2225\u2225 n\u2211 i=1 \u03bbiyixi \u2225\u2225\u2225\u2225\u2225 2 = 0\nand \u2202L\u0302n(\u03b8 \u2217)\n\u2202a0 = n\u2211 i=1 `\u2032(\u2212yi(\u00b5i \u2212M\u03c3(0)))(\u2212yi) = \u2212 `\u2032(1) \u03bbmax n\u2211 i=1 yi\u03bbi = 0.\nIn addition, we have stated earlier, if we slightly perturb the parameter \u03b8\u2217k in the interval [\u03b8 \u2217 k/2, 3\u03b8 \u2217 k/2], the output of the function fD(xi;\u03b8D) does not change for all i \u2208 [n], then \u03b8\u2217 is a critical point. Now we show that \u03b8\u2217 is local minimum. Consider any perturbation \u2206a1, ...,\u2206aM : |\u2206aj | < 12 for all j \u2208 [M ], \u2206w1, ...,\u2206wM \u2208 Rd, \u2206a0 \u2208 R, \u2206\u03b8k : |\u2206\u03b8k| \u2264 \u03b8k/2 for all k \u2208 [n]. Define\n\u03b8\u0303 = (a\u22170 + \u2206a0, ..., a \u2217 M + \u2206aM ,w \u2217 1 + \u2206w1, ...,w \u2217 M + \u2206wM , \u03b8 \u2217 1 + \u2206\u03b8 \u2217 1, ..., \u03b8 \u2217 d + \u2206\u03b8 \u2217 d).\nThen\nn\u2211 i=1 `(\u2212yif(xi; \u03b8\u0303))\u2212 n\u2211 i=1 `(\u2212yif(xi;\u03b8\u2217)) = n\u2211 i=1 [ `(\u2212yif(xi; \u03b8\u0303))\u2212 `(\u2212yif(xi;\u03b8\u2217)) ] \u2265\nn\u2211 i=1 `\u2032(\u2212yif(xi;\u03b8\u2217))(\u2212yi)[f(xi; \u03b8\u0303)\u2212 f(xi;\u03b8\u2217)].\nSince for each sample xi in the dataset,\nf(xi; \u03b8\u0303)\u2212 f(xi;\u03b8\u2217) = \u2206a0 + M\u2211 j=1 (a\u2217j + \u2206aj)\u03c3(\u2206w > j xi) + \u00b5i \u2212 \u00b5i\n= \u2206a0 + M\u2211 j=1 (a\u2217j + \u2206aj)\u03c3(\u2206w > j xi),\nthen\nn\u2211 i=1 `(\u2212yif(xi; \u03b8\u0303))\u2212 n\u2211 i=1 `(\u2212yif(xi;\u03b8\u2217))\n\u2265 n\u2211 i=1 `\u2032(\u2212yif(xi;\u03b8\u2217))(\u2212yi)[f(xi; \u03b8\u0303)\u2212 f(xi;\u03b8\u2217)]\n= n\u2211 i=1 `\u2032(\u2212yi(\u00b5i \u2212M\u03c3(0)))(\u2212yi)  M\u2211 j=1 (a\u2217j + \u2206aj)\u03c3 ( \u2206w>j xi ) + \u2206a0  =\nn\u2211 i=1 \u03bbi` \u2032(1) \u03bbmax (\u2212yi)  M\u2211 j=1 (a\u2217j + \u2206aj)\u03c3 ( \u2206w>j xi ) = `\u2032(1)\n\u03bbmax M\u2211 j=1 \u2212(a\u2217j + \u2206aj) [ n\u2211 i=1 \u03bbiyi\u03c3 ( \u2206w>j xi )] .\nNow we define the following function G : Rd \u2192 R,\nG(u) = n\u2211 i=1 \u03bbiyi\u03c3 ( u>xi ) .\nNow we consider the gradient of the function G with respect to the vector u at the point 0d,\n\u2207uG(0d) = n\u2211 i=1 \u03bbiyi\u03c3 \u2032 (0)xi = 0d by \u2225\u2225\u2225\u2225\u2225 n\u2211 i=1 \u03bbiyixi \u2225\u2225\u2225\u2225\u2225 2 = 0.\nFurthermore, the Hessian matrix \u22072uG(0d) satisfies\n\u22072uG(0d) = n\u2211 i=1 \u03bbiyi\u03c3 \u2032\u2032 (0)xix > i = \u03c3 \u2032\u2032 (0) n\u2211 i=1 \u03bbiyixix > i 0,\nthen the function G(u) = \u2211n i=1 \u03bbiyi\u03c3 ( u>xi ) has a local minima at u = 0d. This indicates that there\nexists \u03b5 > 0 such that for all (\u2206w1, ...,\u2206wM ) : \u221a\u2211M\nj=1 \u2016\u2206wj\u201622 \u2264 \u03b5, n\u2211 i=1 \u03bbiyi\u03c3 ( \u2206w>j xi ) \u2265 n\u2211 i=1 \u03bbiyi\u03c3 (0) = 0,\nwhere the equality holds by the fact that \u2211n\ni=1 yi\u03bbi = 1. In addition, since a \u2217 j = \u22121, |\u2206aj | < 12 , then\nfor all \u2206wj : \u2016\u2206wj\u20162 \u2264 \u03b5 and \u2206bj \u2208 R, n\u2211 i=1 `(\u2212yif(xi; \u03b8\u0303))\u2212 n\u2211 i=1 `(\u2212yif(xi;\u03b8\u2217)) \u2265 0.\nThus, \u03b8\u2217 is a local minima of the empirical loss function with f(xi;\u03b8 \u2217) = \u00b5i \u2212M\u03c3(0). Since there exists a \u00b5imax such that yimax(\u00b5imax \u2212M\u03c3(0)) = 1, then this means that the neural network makes an incorrect prediction on the sample ximax . This indicates that this local minimum has a non-zero training error.\nFinally, we present the way we construct the neural network fD. Since\nfD(x;\u03b8D) = fD(x; \u03b81, ..., \u03b8d) = n\u2211 i=1 \u00b5i d\u220f k=1 1 { x(k) \u2208 [ x (k) i \u2212 \u03b8k, x (k) i + \u03b8k ]} .\nLet \u03c3th denote the threshold unit, where \u03c3th(z) = 1 if z \u2265 0 and \u03c3th(z) = 0, otherwise. Therefore, the indicator function can be represented as follows:\n1 { x(k) \u2208 [ x\n(k) i \u2212 \u03b8k, x (k) i + \u03b8k ]} = \u03c3th ( x(k) \u2212 x(k)i + \u03b8k ) \u2212 \u03c3th ( x(k) \u2212 x(k)i \u2212 \u03b8k ) Therefore,\nd\u220f k=1 1 { x(k) \u2208 [ x (k) i \u2212 \u03b8k, x (k) i + \u03b8k ]} = \u03c3th ( d\u2211\nk=1\n[ \u03c3th ( x(k) \u2212 x(k)i + \u03b8k ) \u2212 \u03c3th ( x(k) \u2212 x(k)i \u2212 \u03b8k )] \u2212 d+ 1\n2\n)\nTherefore, we have\nfD(x;\u03b8D) = n\u2211 i=1 \u00b5i\u03c3th\n( d\u2211\nk=1\n[ \u03c3th ( x(k) \u2212 x(k)i + \u03b8k ) \u2212 \u03c3th ( x(k) \u2212 x(k)i \u2212 \u03b8k )] \u2212 d+ 1\n2\n) .\nIt is very easy to see that this is a two layer network consisted of threshold units.\nFurthermore, we note here that, in the proof shown above, we assume the only parameters in the network fD are \u03b81, ...,\u03b8d. In fact, we can prove a more general statement where the fD is of the form\nfD(x;\u03b8D) = n\u2211 i=1 \u00b5i\u03c3th\n( d\u2211\nk=1\n[ aik\u03c3th ( x(k) + uik ) + bik\u03c3th ( x(k) + vik )] + ci ) ,\nwhere aik, bik, uik, vik, ci, i \u2208 [n], k \u2208 [d] are all parameters. We can show that the neural network\nfD(x;\u03b8D) = n\u2211 i=1 \u00b5i\u03c3th\n( d\u2211\nk=1\n[ \u03c3th ( x(k) \u2212 x(k)i + \u03b8k ) \u2212 \u03c3th ( x(k) \u2212 x(k)i \u2212 \u03b8k )] \u2212 d+ 1\n2\n) ,\ndenotes a local minimum, since any slight perturbations on parameters aik, bik, uik, vik, ci, i \u2208 [n], k \u2208 [d] do not change the output of the neural network on the samples in the dataset D."}, {"heading": "B.14 Proof of Example 5", "text": "In this subsection, we present two examples to show that if either assumption 2 or 3 is not satisfied, even if the other conditions in Theorem 1 are satisfied, Theorem 1 does not hold.\nExample 5 Assume that the distribution PX\u00d7Y satisfies that PY (Y = 1) = PY (Y = \u22121), PX|Y (X = (1, 0)|Y = 1) = PX|Y (X = (\u22121, 0)|Y = 1) = 0.5 and PX|Y (X = (0, 0)|Y = \u22121). Assume that samples in the dataset D = {(xi, yi)}2ni=1 are independently drawn from the distribution PX\u00d7Y . Assume that the network fS has M \u2265 1 neurons and neurons in fS satisfy the condition that \u03c3 is analytic and has a positive second order derivative on R. There exists a feedforward network fD such that the empirical loss L\u0302n(\u03b8S ,\u03b8D) has a local minimum with non-zero training error with a probability at least \u2126(1/n 2).\nRemark: This is a counterexample where Theorem 1 does not hold, when Assumption 3 is satisfied and Assumption 2 is not satisfied. This distribution can be viewed in the following way. The positive data samples are located on the linear span of the set {(1, 0)}, the negative data samples locate on the linear span of the set {(0, 1)} and all samples are located on the linear span of the set {(1, 0), (0, 1)}. Therefore, r = 2 > max{r+, r\u2212} = 1. This means that Assumption 3 is satisfied. In addition, it is easy to check that Assumption 2 is not satisfied, since the matrix (0, 0) has rank zero and thus does not have a full rank. This means that our main results may not hold when the assumption 2 is not satisfied.\nProof: Let n1, n0, n\u22121 denote the number of samples at the point (1, 0), (0, 0), (\u22121, 0), respectively. It is easy to see that the event that n1 = n\u22121 > 0 and n0 > 0 happens with probability at least \u2126(1/n\n2). We note that this is not a tight bounded, however, we just need to show that this happens with a positive probability. Now we consider the optimization problem under the dataset where n1 = n\u22121 > 0 and n0 > 0. We first set the feedforward network fD(x;\u03b8D) to constant, i.e., fD(x;\u03b8D) \u2261 0 for x \u2208 R2. Now the whole network becomes a single layer network,\nf(x;\u03b8) = a0 + M\u2211 j=1 aj\u03c3 ( w>j x ) .\nLet a\u22171 = ... = a \u2217 M = \u22121 and w\u22171 = ... = w\u2217M = 02. Therefore, we have f(x;\u03b8\u2217) = a\u22170 \u2212M\u03c3(0). Let a\u22170 be the global optimizer of the following convex optimization problem.\nmin a 2n\u2211 i=1 `p(\u2212yi(a\u2212M\u03c3(0))).\nThus, we have n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))(\u2212yi) = 0, (27)\nand this indicates that\u2211 i:yi=1 `\u2032p(\u2212(a\u22170 \u2212M\u03c3(0))) = \u2211 i:yi=\u22121 `\u2032p(a \u2217 0 \u2212M\u03c3(0)) or `\u2032p(\u2212a\u22170 +M\u03c3(0))n+ = `\u2032p(a\u22170 \u2212M\u03c3(0))n\u2212.\n(28)\nIn addition, since for \u2200j \u2208 [M ],\n\u2202L\u0302n(\u03b8 \u2217)\n\u2202aj = 2n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))(\u2212yi)\u03c3(0) = 0, by Equation (27),\n\u2207wj L\u0302n(\u03b8\u2217) = 2n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))(\u2212yi)\u03c3\u2032(0)xi = 02, by \u2211 i:yi=1 xi = \u2211 i:yi=\u22121 xi = 02,\nand \u2202L\u0302n(\u03b8 \u2217)\n\u2202a0 = n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))(\u2212yi) = 0,\nthen \u03b8\u2217 is a critical point. Next we show that \u03b8\u2217 = (a\u22170, ..., a \u2217 M ,w \u2217 1, ...,w \u2217 M ) is a local minima. Consider any perturbation \u2206a1, ...,\u2206aM : |\u2206aj | < 12 for all j \u2208 [M ], \u2206w1, ...,\u2206wM \u2208 R2 and \u2206a0 \u2208 R. Define\n\u03b8\u0303 = (a\u22170 + \u2206a0, ..., a \u2217 M + \u2206aM ,w \u2217 1 + \u2206w1, ...,w \u2217 M + \u2206wM ).\nThen\nn\u2211 i=1 `p(\u2212yif(xi; \u03b8\u0303))\u2212 n\u2211 i=1 `p(\u2212yif(xi;\u03b8\u2217)) = n\u2211 i=1 [ `p(\u2212yif(xi; \u03b8\u0303))\u2212 `p(\u2212yif(xi;\u03b8\u2217)) ] \u2265\nn\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))(\u2212yi)[f(xi; \u03b8\u0303)\u2212 f(xi;\u03b8\u2217)]\n= n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))(\u2212yi)[f(xi; \u03b8\u0303)\u2212 a\u22170 +M\u03c3(0)]\n= n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))(\u2212yi)f(xi; \u03b8\u0303),\nwhere the inequality follows from the convexity of the loss function `p(z), the second equality follows from the fact that f(x;\u03b8\u2217) \u2261 a\u22170 \u2212 M\u03c3(0) and the third equality follows from Equation (28). In addition, we have\nn\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))(\u2212yi)f(xi; \u03b8\u0303)\n= n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))(\u2212yi)  M\u2211 j=1 (a\u2217j + \u2206aj)\u03c3 ( \u2206w>j xi ) + \u2206a0  =\nn\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))(\u2212yi)  M\u2211 j=1 (a\u2217j + \u2206aj)\u03c3 ( \u2206w>j xi ) by Eq. (28) =\nM\u2211 j=1 \u2212(a\u2217j + \u2206aj) [ n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))yi\u03c3 ( \u2206w>j xi )]\n= M\u2211 j=1 \u2212(a\u2217j + \u2206aj) [ n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))yi\u03c3 ( \u2206w (1) j x (1) i )] by x (2) i = 0,\u2200i \u2208 [n].\nNow we define the following function G : R\u2192 R,\nG(u) = n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))yi\u03c3 ( ux (1) i ) .\nNow we consider the gradient of the function G with respect to the variable u at the point u = 0,\n\u2207uG(0) = n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))yi\u03c3\u2032 (0)x(1)i = 0.\nFurthermore, the second order derivative \u22072uG(0) satisfies\n\u22072uG(0) = n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))yi\u03c3\u2032\u2032 (0) ( x (1) i )2 = \u03c3\u2032\u2032 (0) n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))yi ( x (1) i )2 = \u03c3\u2032\u2032(0)  1 n+ \u2211 i:yi=1 ( x (1) i )2 \u2212 1 n\u2212 \u2211 i:yi=\u22121 ( x (1) i\n)2 > 0, then the function G(u) = \u2211n i=1 `p(\u2212yi(a\u22170 \u2212M\u03c3(0)))yi\u03c3 ( ux (1) i ) has a local minima at u = 0. This indicates that there exists \u03b5 > 0 such that for all \u2206w : \u2016\u2206w\u20162 \u2264 \u03b5, n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))yi\u03c3 ( \u2206w>xi ) \u2265 n\u2211 i=1 `p(\u2212yi(a\u22170 \u2212M\u03c3(0)))yi\u03c3 (0) = 0.\nIn addition, since a\u2217j = \u22121, |\u2206aj | < 12 , then for all \u2206wj : \u2016\u2206wj\u20162 \u2264 \u03b5, n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))(\u2212yi)f(xi; \u03b8\u0303) = M\u2211 j=1 \u2212(a\u2217j + \u2206aj) [ n\u2211 i=1 `p(\u2212yi(a\u22170 \u2212M\u03c3(0)))yi\u03c3 ( \u2206w>j xi )] \u2265 0.\nTherefore, we have n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))(\u2212yi)f(xi; \u03b8\u0303) \u2265 0, and this indicates that n\u2211 i=1 `p(\u2212yif(xi; \u03b8\u0303))\u2212 n\u2211 i=1 `p(\u2212yif(xi;\u03b8\u2217)) \u2265 0. Thus, \u03b8\u2217 is a local minima with f(x;\u03b8\u2217) = a\u22170 \u2212M\u03c3(0) = constant. Thus,\n1\nn n\u2211 i=1 I{yi 6= sgn(f(xi;\u03b8\u2217))} \u2265 min{n\u2212, n+} n .\nSince the dataset is consisted of both positive and negative examples, then the training error is nonzero.\nExample 6 Assume that the distribution PX\u00d7Y satisfies that PY (Y = 1) = PY (Y = \u22121) and PX|Y (X = 2|Y = 1) = PX|Y (X = \u22121|Y = 1) = 0.5 and PX|Y (X = 0.5|Y = \u22121) = 1. Assume that samples in the dataset D = {(xi, yi)}2ni=1 are independently drawn from the distribution PX\u00d7Y . Assume that the network fS has M \u2265 1 neurons and neurons in fS satisfy the condition that \u03c3 is analytic and has a positive second order derivative on R. There exists a feedforward network fD such that the empirical loss L\u0302n(\u03b8S ,\u03b8D) has a local minimum with non-zero training error with probability at least \u2126(1/n2).\nRemark: This is a counterexample where Theorem 1 does not hold, when Assumption 2 is satisfied and Assumption 3 is not satisfied. This distribution can be viewed in the following way. The positive data samples locate on the linear span of the set {(1)}, the negative data samples locate on the linear span of the set {(1)} and all samples locate on the linear span of the set {(1)}. It is easy to check that assumption 2 is satisfied. However, r = 1 = max{r+, r\u2212} = 1. This means the assumption 3 is not satisfied.\nProof: Let n2, n\u22121, n0.5 denote the number of samples at the point (2), (\u22121), (0.5), respectively. It is easy to see that the event that n2 = n\u22121 > 0 and n0.5 > 0 happens with probability at least \u2126(1/n\n2). We note that this is not a tight bounded, however, we just need to show that this happens with a positive probability. Now we consider the optimization problem under the dataset where n2 = n\u22121 > 0 and n0.5 > 0. We first set the feedforward network fD(x;\u03b8D) to constant, i.e., fD(x;\u03b8D) \u2261 0 for x \u2208 R. Now the whole network becomes a single layer network,\nf(x;\u03b8) = a0 + M\u2211 j=1 aj\u03c3 (wjx) .\nLet a\u22171 = ... = a \u2217 M = \u22121 and w\u22171 = ... = w\u2217M = 0. Therefore, we have f(x;\u03b8\u2217) = a\u22170 \u2212M\u03c3(0). Let a\u22170 be the global optimizer of the following convex optimization problem.\nmin a 2n\u2211 i=1 `p(\u2212yi(a\u2212M\u03c3(0))).\nThus, we have n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))(\u2212yi) = 0, (29) and this indicates that\u2211 i:yi=1 `\u2032p(\u2212(a\u22170 \u2212M\u03c3(0))) = \u2211 i:yi=\u22121 `\u2032p(a \u2217 0 \u2212M\u03c3(0)) or `\u2032p(\u2212a\u22170 +M\u03c3(0))n+ = `\u2032p(a\u22170 \u2212M\u03c3(0))n\u2212. (30) In addition, since for \u2200j \u2208 [M ],\n\u2202L\u0302n(\u03b8 \u2217)\n\u2202aj = 2n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))(\u2212yi)\u03c3(0) = 0, by Equation (29),\n\u2207wj L\u0302n(\u03b8\u2217) = 2n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))(\u2212yi)\u03c3\u2032(0)xi = 0, by \u2211 i:yi=1 xi = \u2211 i:yi=\u22121 xi = 0,\nand \u2202L\u0302n(\u03b8 \u2217)\n\u2202a0 = n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))(\u2212yi) = 0,\nthen \u03b8\u2217 is a critical point. Next we show that \u03b8\u2217 = (a\u22170, ..., a \u2217 M , w \u2217 1, ..., w \u2217 M ) is a local minima. Consider any perturbation \u2206a1, ...,\u2206aM : |\u2206aj | < 12 for all j \u2208 [M ], \u2206w1, ...,\u2206wM \u2208 R and \u2206a0 \u2208 R. Define\n\u03b8\u0303 = (a\u22170 + \u2206a0, ..., a \u2217 M + \u2206aM , w \u2217 1 + \u2206w1, ..., w \u2217 M + \u2206wM ).\nThen\nn\u2211 i=1 `p(\u2212yif(xi; \u03b8\u0303))\u2212 n\u2211 i=1 `p(\u2212yif(xi;\u03b8\u2217)) = n\u2211 i=1 [ `p(\u2212yif(xi; \u03b8\u0303))\u2212 `p(\u2212yif(xi;\u03b8\u2217)) ] \u2265\nn\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))(\u2212yi)[f(xi; \u03b8\u0303)\u2212 f(xi;\u03b8\u2217)]\n= n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))(\u2212yi)[f(xi; \u03b8\u0303)\u2212 a\u22170 +M\u03c3(0)]\n= n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))(\u2212yi)f(xi; \u03b8\u0303),\nwhere the inequality follows from the convexity of the loss function `p(z), the second equality follows from the fact that f(x;\u03b8\u2217) \u2261 a\u22170 \u2212 M\u03c3(0) and the third equality follows from Equation (30). In addition, we have\nn\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))(\u2212yi)f(xi; \u03b8\u0303) = n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))(\u2212yi)  M\u2211 j=1 (a\u2217j + \u2206aj)\u03c3 (\u2206wjxi) + \u2206a0\n =\nn\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))(\u2212yi)  M\u2211 j=1 (a\u2217j + \u2206aj)\u03c3 (\u2206wjxi)  by Eq. (30) =\nM\u2211 j=1 \u2212(a\u2217j + \u2206aj) [ n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))yi\u03c3 (\u2206wjxi) ]\n= M\u2211 j=1 \u2212(a\u2217j + \u2206aj) [ n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))yi\u03c3 (\u2206wjxi) ] .\nNow we define the following function G : R\u2192 R,\nG(u) = n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))yi\u03c3 (uxi) .\nNow we consider the gradient of the function G with respect to the variable u at the point u = 0,\n\u2207uG(0) = n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))yi\u03c3\u2032 (0)xi\n= \u03c3\u2032(0)\n( 1\n2 `\u2032p(\u2212a\u22170 +M\u03c3(0))n+ \u2212\n1 2 `\u2032p(a \u2217 0 \u2212M\u03c3(0))n\u2212\n) = 0,\nby Equation (30). Furthermore, the second order derivative \u22072uG(0) satisfies\n\u22072uG(0) = n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))yi\u03c3\u2032\u2032 (0) (xi)2 = \u03c3\u2032\u2032 (0) n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))yi (xi)2\n= \u03c3\u2032\u2032(0)  1 n+ \u2211 i:yi=1 (xi) 2 \u2212 1 n\u2212 \u2211 i:yi=\u22121 (xi) 2  > 0, then the function G(u) = \u2211n i=1 `p(\u2212yi(a\u22170 \u2212 M\u03c3(0)))yi\u03c3 (uxi) has a local minima at u = 0. This indicates that there exists \u03b5 > 0 such that for all \u2206w : \u2016\u2206w\u20162 \u2264 \u03b5, n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))yi\u03c3 ( \u2206w>xi ) \u2265 n\u2211 i=1 `p(\u2212yi(a\u22170 \u2212M\u03c3(0)))yi\u03c3 (0) = 0.\nIn addition, since a\u2217j = \u22121, |\u2206aj | < 12 , then for all \u2206wj : \u2016\u2206wj\u20162 \u2264 \u03b5, n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))(\u2212yi)f(xi; \u03b8\u0303) = M\u2211 j=1 \u2212(a\u2217j + \u2206aj) [ n\u2211 i=1 `p(\u2212yi(a\u22170 \u2212M\u03c3(0)))yi\u03c3 ( \u2206w>j xi )] \u2265 0.\nTherefore, we have n\u2211 i=1 `\u2032p(\u2212yi(a\u22170 \u2212M\u03c3(0)))(\u2212yi)f(xi; \u03b8\u0303) \u2265 0, and this indicates that n\u2211 i=1 `p(\u2212yif(xi; \u03b8\u0303))\u2212 n\u2211 i=1 `p(\u2212yif(xi;\u03b8\u2217)) \u2265 0. Thus, \u03b8\u2217 is a local minima with f(x;\u03b8\u2217) = a\u22170 \u2212M\u03c3(0) = constant. Thus,\n1\nn n\u2211 i=1 I{yi 6= sgn(f(xi;\u03b8\u2217))} \u2265 min{n\u2212, n+} n .\nSince the dataset is consisted of both positive and negative examples, then the training error is nonzero."}, {"heading": "B.15 Proof of Lemma 2", "text": "Lemma 2 If samples in the dataset D = {(xi, yi)}ni=1 satisfies that the matrix \u2211n i=1 \u03bbiyixix > i is\nindefinite for all sequences {\u03bbi \u2265 0}ni=1 satisfying \u2211 i:yi=1 \u03bbi = \u2211 i:yi=\u22121 \u03bbi > 0, then there exists a matrix A \u2208 Rd\u00d7d and two real numbers c1 > 0 and c2 \u2208 R such that yi(x>i Axi \u2212 c2) > c1 holds for all i \u2208 [n].\nProof: For each sample xi in the dataset, let vec(xix > i ) denote the vectorization of the matrix xix > i . Since we assume that for any sequence {\u03bbi \u2265 0}ni=1 satisfying \u2211 i:yi=1 \u03bbi = \u2211 i:yi=\u22121 \u03bbi = 1, the vector\u2211n\ni=1 yi\u03bbivec(xix > i ) does not equal to the zero vector 0d2 , then we have that the convex hull of two vector sets C+ = {vec(xix>i )}i:yi=1 and C\u2212 = {vec(xix>i )}i:yi=\u22121 are two disjoint closed compact sets. By the hyperplane separation theorem, this indicates that there exists a vector w \u2208 Rd2 and two real numbers c\u03031 < c\u03032 such that w >u > c\u03032 and w >v < c\u03031 for all u \u2208 C+ and v \u2208 C\u2212. This further indicates that there exists two real numbers c1 > 0 and c2 \u2208 R such that yi(x>i Axi\u2212 c2) > c1 holds for all i \u2208 R."}, {"heading": "B.16 Proof of Proposition 12", "text": "Proposition 12 Assume that the single layer neural network fS(x;\u03b8S) has M > d neurons and assume that the neuron \u03c3 is quadratic, i.e., \u03c3(z) = z2. Assume that the dataset D = {(xi, yi)}ni=1 is consisted of both positive and negative samples. For all multilayer neural network fD parameterized by \u03b8D, every local minimum \u03b8 \u2217 = (\u03b8\u2217S ,\u03b8 \u2217 D) of the empirical loss function L\u0302n(\u03b8S ,\u03b8D; p), p \u2265 6 satisfies R\u0302n(\u03b8 \u2217) = 0 if and only if the matrix \u2211n i=1 \u03bbiyixix > i is indefinite for all sequences {\u03bbi \u2265 0}ni=1\nsatisfying \u2211\ni:yi=1 \u03bbi = \u2211 i:yi=\u22121 \u03bbi > 0."}, {"heading": "Proof:", "text": "(1) Proof of \u201cif\u201d: It follows from Lemma 2 that if the assumptions on the dataset are satisfied, there exists a set of parameter \u03b8S such that fS(x;\u03b8S) achieves zero training error and this further indicates that for any neural architecture fD, there exists a set of parameter \u03b8 \u2217 = (\u03b8\u2217S ,\u03b8 \u2217 D) such that Ln(\u03b8 \u2217; p) = 0 for all p \u2265 1. This means that the empirical loss function has a global minimum with a value equal to zero. We first assume that the \u03b8\u2217 = (\u03b8\u22171,\u03b8 \u2217 2) is a local minimum. We next prove the following two claims: Claim 1: If \u03b8\u2217 = (\u03b8\u2217S ,\u03b8 \u2217 D) is a local minimum and there exists j \u2208 [M ] such that a\u2217j = 0, then R\u0302n(\u03b8 \u2217) = 0. Claim 2: If \u03b8\u2217 = (\u03b8\u2217S ,\u03b8 \u2217 D) is a local minimum and a \u2217 j 6= 0 for all j \u2208 [M ], then R\u0302n(\u03b8\u2217) = 0. (a) Proof of claim 1. We prove that if \u03b8\u2217 = (\u03b8\u2217S ,\u03b8 \u2217 D) is a local minima and there exists j \u2208 [M ] such that a\u2217j = 0, then R\u0302n(\u03b8 \u2217) = 0. Without loss of generality, we assume that a\u22171 = 0. Since \u03b8\u2217 = (\u03b8\u2217S ,\u03b8 \u2217 D) is a local minima, then there exists \u03b50 > 0 such that for any small perturbations \u2206a1, \u2206w1 on parameters a \u2217 1 and w \u2217 1, i.e., |\u2206a1|2 + \u2016\u2206w1\u201622 \u2264 \u03b520, we have\nL\u0302n(\u03b8\u0303S ,\u03b8 \u2217 D) \u2265 L\u0303n(\u03b8\u2217S ,\u03b8\u2217D),\nwhere \u03b8\u0303 = (a\u03030, a\u03031, ..., a\u0303M , w\u03031, ..., w\u0303M ), a\u03031 = a \u2217 1 + \u2206a1, w\u03031 = w \u2217 1 + \u2206w1 and a\u0303j = a \u2217 j , w\u0303j = w \u2217 j for j 6= 1. Now we consider Taylor expansion of L\u0303n(\u03b8\u0303S ,\u03b8\u2217D) at (\u03b8\u2217S ,\u03b8\u2217D). We note here that the Taylor expansion of L\u0302(\u03b8S ,\u03b8 \u2217 D; p) on \u03b8S always exists, since the empirical loss function L\u0302n has continuous derivatives with respect to fS up to the p-th order and the output of the neural network f(x;\u03b8S) is infinitely differentiable with respect to \u03b8S due to the fact that neuron activation function \u03c3 is real analytic. We first calculate the first order derivatives at the point (\u03b8\u22171,\u03b8 \u2217 2)\ndL\u0302n(\u03b8 \u2217)\nda1 = n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))(\u2212yi)\u03c3 ( w\u22171 >xi ) = 0, \u03b8\u2217 is a critical point,\n\u2207w1L\u0302n(\u03b8\u2217) = a\u22171 n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))(\u2212yi)\u03c3\u2032 ( w\u22171 >xi ) xi = 0d, \u03b8 \u2217 is a critical point.\nNext, we calculate the second order derivatives at the point (\u03b8\u22171,\u03b8 \u2217 2),\nd2L\u0302(\u03b8\u2217)\nda21 = N\u2211 i=1 `\u2032\u2032p(\u2212yif(xi;\u03b8\u2217))\u03c32 ( w\u22171 >xi ) \u2265 0,\nd\nda1 (\u2207w1L(\u03b8\u2217)) = n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))(\u2212yi)\u03c3\u2032 ( w\u22171 >xi ) xi\n+ a\u22171 n\u2211 i=1 `\u2032\u2032p(\u2212yif(xi;\u03b8\u2217))\u03c3 ( w\u22171 >xi ) \u03c3\u2032 ( w\u22171 >xi ) xi\n= 0d,\nwhere the first term equals to the zero vector by the necessary condition for a local minima presented in Lemma 1 and the second term equals to the zero vector by the assumption that a\u22171 = 0. Furthermore, by the assumption that a\u22171 = 0, we have\n\u22072w1L\u0302n(\u03b8\u2217; p) = a\u22171\u2207w1 [ n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8))(\u2212yi)\u03c3\u2032 ( w\u22171 >xi ) xi ] = 0d\u00d7d.\nWe further calculate the third order derivatives\nd\nda1\n[ \u22072w1L\u0302n(\u03b8\u2217; p) ] = d\nda1\n[ a\u22171\u2207w1 [ n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8))(\u2212yi)\u03c3\u2032 ( w\u22171 >xi ) xi ]]\n= \u2207w1 [ n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8))(\u2212yi)\u03c3\u2032 ( w\u22171 >xi ) xi ] + 0d\u00d7d by a \u2217 1 = 0\n= n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8))(\u2212yi)\u03c3\u2032\u2032 ( w\u22171 >xi ) xix > i\n+ a\u22171 n\u2211 i=1 `\u2032\u2032p(\u2212yif(xi;\u03b8)) [ \u03c3\u2032 ( w\u22171 >xi )]2 xix > i\n= n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8))(\u2212yi)\u03c3\u2032\u2032 ( w\u22171 >xi ) xix > i by a \u2217 1 = 0\nand\n\u22073w1L\u0302n(\u03b8\u2217; p) = a\u22171\u22072w1 [ n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8))(\u2212yi)\u03c3\u2032 ( w\u22171 >xi ) xi ] = 0d\u00d7d\u00d7d.\nIn fact, it is easy to show that for any 2 \u2264 k \u2264 p,\n\u2207kw1L\u0302n(\u03b8\u2217; p) = a\u22171\u2207k\u22121w1 [ n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8))(\u2212yi)\u03c3\u2032 ( w\u22171 >xi ) xi ] = 0d\u00d7 d\u00d7 ...\u00d7 d\ufe38 \ufe37\ufe37 \ufe38\nk times\n.\nLet \u03b5 > 0, \u2206a1 = sgn(a1)\u03b5 9/4 and \u2206w1 = \u03b5u1 for u1 : \u2016u1\u20162 = 1. Clearly, when \u03b5 \u2192 0, \u2206a1 = o(\u2016\u2206w1\u20162), \u2206a1 = o(1) and \u2016\u2206w1\u2016 = o(1). Then we expand L\u0302n(\u03b8\u03031,\u03b8\u22172) at the point \u03b8\u2217 up to the sixth order and thus as \u03b5\u2192 0,\nL\u0302n(\u03b8\u03031,\u03b8 \u2217 2) = L\u0302n(\u03b8 \u2217 1,\u03b8 \u2217 2) +\n1\n2!\nd2L\u0302n(\u03b8 \u2217)\nd2a1 (\u2206a1)\n2\n+ 1\n2 \u2206a1\u2206w\n> 1\nd\nda1\n[ D2w1L\u0302n(\u03b8 \u2217; p) ] \u2206w1 + o(|a1|2) + o(|a1|\u2016w1\u201622) + o(\u2016\u2206w1\u201652)\n= L\u0302n(\u03b8 \u2217 1,\u03b8 \u2217 2) +\n1\n2!\nd2L\u0302n(\u03b8 \u2217)\nd2a1 \u03b59/2 +\n1 2 sgn(a1)\u03b5 9/4+2 n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8))\u03c3\u2032\u2032 ( w\u22171 >xi ) (u>1 xi) 2\n+ o(\u03b59/2) + o(\u03b59/4+2) + o(\u03b55)\n= L\u0302n(\u03b8 \u2217 1,\u03b8 \u2217 2) +\n1 2 sgn(a1)\u03b5 17/4 n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8))(\u2212yi)\u03c3\u2032\u2032 ( w\u22171 >xi ) (u>1 xi) 2 + o(\u03b517/4)\nSince \u03b5 > 0 and L\u0302n(\u03b8\u03031,\u03b8 \u2217 2; p) \u2265 L\u0302n(\u03b8\u2217; p) holds for any u1 : \u2016u1\u20162 = 1 and any sgn(a1) \u2208 {\u22121, 1}, then n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8))(\u2212yi)\u03c3\u2032\u2032 ( w\u22171 >xi ) (u>xi) 2 = 0, for any u \u2208 Rd. (31)\nTherefore, n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8))(\u2212yi)\u03c3\u2032\u2032 ( w\u22171 >xi ) xix > i = 0d\u00d7d. Since \u03c3\u2032\u2032(z) = 2 for all z, then\nn\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8))(\u2212yi)xix>i = 0d\u00d7d. (32)\nFurthermore, since \u03b8\u2217 is a critical point, then\n\u2202L\u0302n(\u03b8; p)\n\u2202a0 =\n1\nn n\u2211 i=1 `\u2032(\u2212yif(xi;\u03b8\u2217))(\u2212yi) = 0. (33)\nNow we assume that R\u0302n(\u03b8 \u2217) > 0. This means that there exists a index i such that yif(xi;\u03b8 \u2217) < 0 or `\u2032(\u2212yif(xi;\u03b8\u2217)) > 0. Furthermore, since `\u2032(z) \u2265 0, then by setting \u03bbi = `\u2032(\u2212yif(xi;\u03b8\u2217)), we have that there exists a sequence {\u03bbi \u2265 0}ni=1 satisfying \u2211 i:yi=1 \u03bbi = \u2211\ni:yi=\u22121 \u03bbi > 0, where the equality follows from Equation (33) and the positiveness comes from the assumption that `\u2032(\u2212yif(xi;\u03b8\u2217)) > 0 for some i, such that\nn\u2211 i=1 \u03bbiyixix > i = 0d\u00d7d,\nwhere the equality follows from Equation (32). This leads to the contradiction with our assumption that the matrix \u2211n i=1 \u03bbiyixix > i should be indefinite for all sequences {\u03bbi \u2265 0}ni=1 satisfying \u2211 i:yi=1\n\u03bbi =\u2211 i:yi=\u22121 \u03bbi > 0. Therefore, this indicates that R\u0302n(\u03b8 \u2217) = 0. (b) Proof of Claim 2: To prove the claim 2, we first show that if M > d, then there exists coefficients \u03b11, ..., \u03b1M , not all zero, such that\n(\u03b11w \u2217 1 + ...+ \u03b1Mw \u2217 M ) > xi = 0, for all i \u2208 [n].\nClearly, if M > r, then there exists coefficients \u03b11, ..., \u03b1M , not all zero, such that\n(\u03b11w \u2217 1 + ...+ \u03b1Mw \u2217 M ) = 0d, for all i \u2208 [n].\nNow we prove the claim 2. First, we consider the Hessian matrix H(w\u22171, ...,w \u2217 M ). Since \u03b8 \u2217 is a local minima, then\nF (u1, ...,uM ) = M\u2211 j=1 M\u2211 k=1 u>j \u22072wj ,wk L\u0302n(\u03b8 \u2217; p)uk \u2265 0\nholds for any vectors u1, ...,uM \u2208 Rd. Since \u03c3\u2032\u2032(z) = 2 and \u03c3\u2032(z) = 2z for all z \u2208 R, then\n\u22072wj L\u0302n(\u03b8\u2217; p) = a\u2217j n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))(\u2212yi)\u03c3\u2032\u2032 ( w\u2217j >xi ) xix > i\n+ a\u2217j 2 n\u2211 i=1 `\u2032\u2032p(\u2212yif(xi;\u03b8\u2217)) [ \u03c3\u2032 ( w\u2217j >xi )]2 xix > i\n= \u22122a\u2217j n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))yixix>i + 4a\u2217j 2 n\u2211 i=1 `\u2032\u2032p(\u2212yif(xi;\u03b8\u2217)) ( w\u2217j >xi )2 xix > i ,\nand\n\u22072wj ,wk L\u0302n(\u03b8 \u2217; p) = a\u2217ja \u2217 k n\u2211 i=1 `\u2032\u2032p(\u2212yif(xi;\u03b8\u2217)) [ \u03c3\u2032 ( w\u2217j >xi )] [ \u03c3\u2032 ( w\u2217k >xi )] xix > i\n= 4a\u2217ja \u2217 k n\u2211 i=1 `\u2032\u2032p(\u2212yif(xi;\u03b8\u2217)) ( w\u2217j >xi )( w\u2217k >xi ) xix > i .\nThus, we have\nF (u1, ...,uM ) = \u22122 M\u2211 j=1 [ a\u2217j n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))yi ( u>j xi )2]\n+ 4 M\u2211 j=1 M\u2211 k=1 [ a\u2217ja \u2217 k n\u2211 i=1 `\u2032\u2032p(\u2212yif(xi;\u03b8\u2217)) ( w\u2217j >xi )( w\u2217k >xi )( u>j xi )( u>k xi )]\n= \u22122 M\u2211 j=1 [ a\u2217j n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))yi ( u>j xi )2]\n+ 4 n\u2211 i=1 `\u2032\u2032p(\u2212yif(xi;\u03b8\u2217))  M\u2211 j=1 a\u2217j ( w\u2217j >xi )( u>j xi )2 . Since there exists coefficients \u03b11, ..., \u03b1M , not all zero, such that (\u03b11w \u2217 1 + ...+ \u03b1Mw \u2217 M ) >xi = 0, for all i \u2208 [n], and a\u2217j 6= 0 for all j \u2208 [M ] then by setting uj = \u03b1ju/a\u2217j for all j \u2208 [M ], we have that the inequality\nF (u1, ...,uM ) = \u22122 M\u2211 j=1 [ a\u2217j n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))yi ( \u03b1j/a \u2217 j )2 ( u>xi )2]\n+ 4 n\u2211 i=1 `\u2032\u2032p(\u2212yif(xi;\u03b8\u2217))  M\u2211 j=1 \u03b1j ( w\u2217j >xi )( u>xi )2 = \u22122\nM\u2211 j=1 [ a\u2217j n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))yi ( \u03b1j/a \u2217 j )2 ( u>xi )2]\n+ 4 n\u2211 i=1 `\u2032\u2032p(\u2212yif(xi;\u03b8\u2217))   M\u2211 j=1 \u03b1jw \u2217 j > xi  2 ( u>xi )2 = \u22122\nM\u2211 j=1 ( \u03b12j/a \u2217 j ) \u00b7 n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))yi ( u>xi )2 \u2265 0\nholds for any u \u2208 Rd. Next we consider the following two cases: (1) \u2211M j=1 ( \u03b12j/a \u2217 j ) 6= 0; (2) \u2211Mj=1 (\u03b12j/a\u2217j) = 0.\nCase 1: If \u2211M\nj=1 ( \u03b12j/a \u2217 j ) 6= 0, then without loss of generality, we assume that \u2211Mj=1 (\u03b12j/a\u2217j) < 0.\nThis indicates that n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))yi ( u>xi )2 \u2265 0, for all u \u2208 Rd. (34)\nSince \u03b8\u2217 is a critical point, then\n\u2202L\u0302n(\u03b8 \u2217; p)\n\u2202a0 =\n1\nn n\u2211 i=1 `\u2032(\u2212yif(xi;\u03b8\u2217))(\u2212yi) = 0. (35)\nNow we assume that R\u0302n(\u03b8 \u2217) > 0. This means that there exists a index i such that yif(xi;\u03b8 \u2217) < 0 or `\u2032(\u2212yif(xi;\u03b8\u2217)) > 0. Furthermore, since `\u2032(z) \u2265 0, then by setting \u03bbi = `\u2032(\u2212yif(xi;\u03b8\u2217)), we have that there exists a sequence {\u03bbi \u2265 0}ni=1 satisfying \u2211 i:yi=1 \u03bbi = \u2211\ni:yi=\u22121 \u03bbi > 0, where the equality follows from Equation (33) and the positiveness comes from the assumption that `\u2032(\u2212yif(xi;\u03b8\u2217)) > 0 for some i, such that\nn\u2211 i=1 \u03bbiyixix > i 0,\nwhere the positive semi-definiteness follows from the inequality (34). This leads to the contradiction with our assumption that the matrix \u2211n i=1 \u03bbiyixix > i should be indefinite for all sequences {\u03bbi \u2265 0}ni=1\nsatisfying \u2211\ni:yi=1 \u03bbi = \u2211 i:yi=\u22121 \u03bbi > 0. Therefore, this indicates that R\u0302n(\u03b8 \u2217) = 0.\nCase 2: If \u2211M\nj=1 ( \u03b12j/a \u2217 j ) = 0, then by setting uj = (\u03b1j/a \u2217 j +vsgn(\u03b1j))u for some scalar v and vector\nu \u2208 Rd, we have\nF (v,u) = \u22122 M\u2211 j=1 [ a\u2217j n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))yi ( (\u03b1j/a \u2217 j + vsgn(\u03b1j))u >xi )2]\n+ 4 n\u2211 i=1 `\u2032\u2032p(\u2212yif(xi;\u03b8\u2217))  M\u2211 j=1 a\u2217j ( w\u2217j >xi )( (\u03b1j/a \u2217 j + vsgn(\u03b1j))u >xi )2 = \u22122\nM\u2211 j=1 [ a\u2217j n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))yi ( (\u03b1j/a \u2217 j + vsgn(\u03b1j))u >xi )2]\n+ 4 n\u2211 i=1 `\u2032\u2032p(\u2212yif(xi;\u03b8\u2217))   M\u2211 j=1 (\u03b1j + vsgn(\u03b1j)a \u2217 j )w \u2217 j > xi (u>xi)2  = \u22122\nM\u2211 j=1 [ a\u2217j n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))yi ( (\u03b1j/a \u2217 j + vsgn(\u03b1j))u >xi )2]\n+ 4v2 n\u2211 i=1 `\u2032\u2032p(\u2212yif(xi;\u03b8\u2217))   M\u2211 j=1 sgn(\u03b1j)a \u2217 jw \u2217 j > xi  2 ( u>xi )2 , \u22122\nM\u2211 j=1 [ a\u2217j n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))yi ( (\u03b1j/a \u2217 j + vsgn(\u03b1j))u >xi )2] + v2R(u),\nwhere we define\nR(u) = 4 n\u2211 i=1 `\u2032\u2032p(\u2212yif(xi;\u03b8\u2217))   M\u2211 j=1 sgn(\u03b1j)a \u2217 jw \u2217 j > xi  2 ( u>xi )2 .\nIn addition, we have\nM\u2211 j=1 [ a\u2217j n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8\u2217))yi ( (\u03b1j/a \u2217 j + vsgn(\u03b1j))u >xi )2]\n= n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8))yi(u>xi)2 \u00b7  M\u2211 j=1 (\u03b12j/a \u2217 j + 2vsgn(\u03b1j)\u03b1j + v 2a\u2217j )  =\nn\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8))yi(u>xi)2 \u00b7  M\u2211 j=1 (2vsgn(\u03b1j)\u03b1j + v 2a\u2217j )  = 2v\n M\u2211 j=1 |\u03b1j |  n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8))yi(u>xi)2 + v2  M\u2211 j=1 a\u2217j  n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8))yi(u>xi)2.\nTherefore, we can rewrite F (v,u) as\nF (v,u) = 2v M\u2211 j=1 |\u03b1j | n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8))yi(u>xi)2 + v2 M\u2211 j=1 a\u2217j \u00b7 n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8))yi(u>xi)2 + v2R(u)\n, 2v M\u2211 j=1 |\u03b1j | n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8))yi(u>xi)2 + v2R\u0302(u)\nSince F (v,u) \u2265 0 holds for any scalar v and vector u \u2208 Rd, then we should have\nM\u2211 j=1 |\u03b1j | n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8))yi(u>xi)2 = 0, for any u \u2208 Rd.\nSince the coefficient \u03b11, ..., \u03b1M are not all zero, then for any u \u2208 Rd, we have n\u2211 i=1 `\u2032p(\u2212yif(xi;\u03b8))yi(u>xi)2 = 0.\nApplying the same analysis shown earlier, we have R\u0302n(\u03b8 \u2217) = 0.\nProof of \u201conly if\u201d: We prove the necessary condition by proving the following claim. Claim 10 If there exists a sequence {\u03bbi \u2265 0}ni=1 satisfying \u2211 i:yi=1 \u03bbi = \u2211 i:yi=\u22121 \u03bbi > 0 such that\nthe matrix \u2211n\ni=1 \u03bbiyixix > i is positive or negative positive semi-definite, then there exists a multilayer\nneural architecture fD such that the empirical loss function L\u0302n(\u03b8S ,\u03b8D; p), p \u2265 6 has a local minimum with a non-zero training error.\nProof: Let D = {(xi, yi)}ni=1 denote a dataset consisting of n samples. We rewrite the sample x as x = ( x(1), ..., x(d) ) . Consider the following network,\nf(x;\u03b8) = fS(x;\u03b8S) + fD(x;\u03b8D),\nwhere\nfS(x;\u03b8S) = a0 + M\u2211 j=1 aj\u03c3(w > j xi + bj),\nand the multilayer network is defined as follows,\nfD(x;\u03b8D) = fD(x; \u03b81, ..., \u03b8d) = n\u2211 i=1 \u00b5i d\u220f k=1 1 { x(k) \u2208 [ x (k) i \u2212 \u03b8k, x (k) i + \u03b8k ]} . (36)\nWe note here that \u00b51, ..., \u00b5n are not parameters and later we will show that this function can be implemented by a multilayer network consisted of threshold units. A useful property of the function fD(x;\u03b8D) is that if all parameters \u03b8is are positive and sufficiently smalls, then for each sample (xi, yi) in the dataset,\nfD(xi;\u03b8D) = \u00b5i.\nFurthermore, if we slightly perturb all parameters, the output of the function fD on all samples remain the same. In the proof, we use these two properties to construct the local minimum with a non-zero training error.\nBy assumption, there exists a sequence {\u03bbi \u2265 0}ni=1 satisfying \u2211 i:yi=1 \u03bbi = \u2211 i:yi=\u22121 \u03bbi > 0 such that\nthe matrix \u2211n\ni=1 \u03bbiyixix > i is positive or negative semi-definite. Without loss of generality, we assume\nthat the matrix is positive semi-definite. Now we construct a local minimum \u03b8\u2217. Let a\u22170 = a \u2217 1 = ... = a\u2217M = \u22121, w\u22171 = ... = w\u2217M = 0d and b\u22171 = ... = b\u2217M = 0. Now we set \u03b8\u22171, ..., \u03b8\u2217d to be positive and sufficiently small such that for two different samples in the dataset, e.g., xi 6= xj , the following equations holds,\nd\u220f k=1 1 { x (k) j \u2208 [ x (k) i \u2212 2\u03b8\u2217k, x (k) i + 2\u03b8 \u2217 k ]} = 0, d\u220f k=1 1 { x (k) i \u2208 [ x (k) j \u2212 2\u03b8\u2217k, x (k) j + 2\u03b8 \u2217 k ]} = 0.\nNow we choose \u00b51, ..., \u00b5n as follows. The output of the neural network on sample xi in the dataset is f(xi;\u03b8\n\u2217) = \u00b5i \u2212M\u03c3(0). We need to choose \u00b51, ..., \u00b5n to satisfy all conditions shown as follows:\n(1) There exists i \u2208 [n] such that yi(\u00b5i \u2212M\u03c3(0)) < 0.\n(2) For all i : yi = 1 and all k : yk = \u22121,\n`\u2032(\u2212yi(\u00b5i \u2212M\u03c3(0)))\u2211 j:j=1 ` \u2032(\u2212yi(\u00b5i \u2212M\u03c3(0))) = \u03bbi\u2211 j:j=1 \u03bbj , `\u2032(\u2212yk(\u00b5k \u2212M\u03c3(0)))\u2211 j:j=\u22121 ` \u2032(\u2212yi(\u00b5i \u2212M\u03c3(0))) = \u03bbk\u2211 j:j=\u22121 \u03bbj ,\nand \u2211 j:j=1 `\u2032(\u2212yi(\u00b5i \u2212M\u03c3(0))) = \u2211 j:j=\u22121 `\u2032(\u2212yi(\u00b5i \u2212M\u03c3(0))).\nNow we start from the largest element in the sequence {\u03bbi}ni=1. Since \u2211n\ni=1 \u03bbi > 0, the define the index imax as the index of the largest element, i.e.,\nimax = arg max i \u03bbi.\nLet \u03bbmax = \u03bbimax . Now we choose \u00b5imax such that\nyimax(\u00b5imax \u2212M\u03c3(0)) = \u22121.\nThus, the index imax satisfy the first condition. Then for i 6= imax, we choose \u00b5i such that\n`\u2032(\u2212yi(\u00b5i \u2212M\u03c3(0))) = \u03bbi \u03bbmax `(\u2212yimax(\u00b5imax \u2212M\u03c3(0))) = \u03bbi \u03bbmax `\u2032(1) \u2264 `\u2032(1). (37)\nWe note here that for each i \u2208 [n], there always exists a \u00b5i solving the above equation. This can be seen by the fact that `\u2032 is continuous, `\u2032p(z) \u2265 0 and `\u2032p(z) = 0 iff z \u2264 \u2212z0. This indicates that for \u2200z > \u2212z0, `\u2032p(z) > 0, i.e., `\u2032(1) > 0 and that `\u2032(\u2212z0) = 0. Since `\u2032(z) is continuous, then for \u2200r \u2208 [0, `\u2032(1)], there always exists z \u2208 R such that `\u2032(z) = r, which further indicates that for \u2200i \u2208 [n], there always exists \u00b5i \u2208 R solving the Equation (37). Under this construction, it is easy to show that the second condition is satisfied as well. Now we only need to show that \u03b8\u2217 is local minimum. We first show that \u03b8\u2217 is a critical point of the empirical loss function. Since for \u2200j \u2208 [M ],\n\u2202L\u0302n(\u03b8 \u2217)\n\u2202aj = n\u2211 i=1 `\u2032(\u2212yi(\u00b5i \u2212M\u03c3(0)))(\u2212yi)\u03c3(0)\n= \u03c3(0) n\u2211 i=1 \u03bbi \u03bbmax `\u2032(1)(\u2212yi) = \u2212 \u03c3(0)`\u2032(1) \u03bbmax n\u2211 i=1 yi\u03bbi\n= 0 by \u2211 i:yi=1 \u03bbi = \u2211 i:yi=\u22121 \u03bbi\n\u2207wj L\u0302n(\u03b8\u2217) = n\u2211 i=1 `\u2032(\u2212yi(\u00b5i \u2212M\u03c3(0)))(\u2212yi)\u03c3\u2032(0)xi\n= \u2212\u03c3\u2032(0) n\u2211 i=1 \u03bbi \u03bbmax `\u2032(1)yixi = \u2212 \u03c3\u2032(0)`\u2032(1) \u03bbmax n\u2211 i=1 \u03bbiyixi = 0d by \u03c3 \u2032(0) = 0\nand \u2202L\u0302n(\u03b8 \u2217)\n\u2202a0 = n\u2211 i=1 `\u2032(\u2212yi(\u00b5i \u2212M\u03c3(0)))(\u2212yi) = \u2212 `\u2032(1) \u03bbmax n\u2211 i=1 yi\u03bbi = 0.\nIn addition, we have stated earlier, if we slightly perturb the parameter \u03b8\u2217k in the interval [\u03b8 \u2217 k/2, 3\u03b8 \u2217 k/2], the output of the function fD(xi;\u03b8D) does not change for all i \u2208 [n], then \u03b8\u2217 is a critical point. Now we show that \u03b8\u2217 is local minimum. Consider any perturbation \u2206a1, ...,\u2206aM : |\u2206aj | < 12 for all j \u2208 [M ], \u2206w1, ...,\u2206wM \u2208 Rd, \u2206a0 \u2208 R, \u2206\u03b8k : |\u2206\u03b8k| \u2264 \u03b8k/2 for all k \u2208 [n]. Define\n\u03b8\u0303 = (a\u22170 + \u2206a0, ..., a \u2217 M + \u2206aM ,w \u2217 1 + \u2206w1, ...,w \u2217 M + \u2206wM , \u03b8 \u2217 1 + \u2206\u03b8 \u2217 1, ..., \u03b8 \u2217 d + \u2206\u03b8 \u2217 d).\nThen\nn\u2211 i=1 `(\u2212yif(xi; \u03b8\u0303))\u2212 n\u2211 i=1 `(\u2212yif(xi;\u03b8\u2217)) = n\u2211 i=1 [ `(\u2212yif(xi; \u03b8\u0303))\u2212 `(\u2212yif(xi;\u03b8\u2217)) ] \u2265\nn\u2211 i=1 `\u2032(\u2212yif(xi;\u03b8\u2217))(\u2212yi)[f(xi; \u03b8\u0303)\u2212 f(xi;\u03b8\u2217)].\nSince for each sample xi in the dataset,\nf(xi; \u03b8\u0303)\u2212 f(xi;\u03b8\u2217) = \u2206a0 + M\u2211 j=1 (a\u2217j + \u2206aj)\u03c3(\u2206w > j xi) + \u00b5i \u2212 \u00b5i\n= \u2206a0 + M\u2211 j=1 (a\u2217j + \u2206aj)\u03c3(\u2206w > j xi),\nthen\nn\u2211 i=1 `(\u2212yif(xi; \u03b8\u0303))\u2212 n\u2211 i=1 `(\u2212yif(xi;\u03b8\u2217))\n\u2265 n\u2211 i=1 `\u2032(\u2212yif(xi;\u03b8\u2217))(\u2212yi)[f(xi; \u03b8\u0303)\u2212 f(xi;\u03b8\u2217)] = n\u2211 i=1 `\u2032(\u2212yi(\u00b5i \u2212M\u03c3(0)))(\u2212yi)  M\u2211 j=1 (a\u2217j + \u2206aj)\u03c3 ( \u2206w>j xi ) + \u2206a0\n =\nn\u2211 i=1 \u03bbi` \u2032(1) \u03bbmax (\u2212yi)  M\u2211 j=1 (a\u2217j + \u2206aj)\u03c3 ( \u2206w>j xi ) = `\u2032(1)\n\u03bbmax M\u2211 j=1 \u2212(a\u2217j + \u2206aj) [ n\u2211 i=1 \u03bbiyi ( \u2206w>j xi )2] .\nSince by assumption that the matrix \u2211n\ni=1 \u03bbiyixix > i is positive semi-definite, then for any \u2206w > j \u2208 Rd,\nn\u2211 i=1 \u03bbiyi ( \u2206w>j xi )2 \u2265 0.\nIn addition, since a\u2217j = \u22121, |\u2206aj | < 12 , then for all \u2206wj \u2208 Rd, n\u2211 i=1 `(\u2212yif(xi; \u03b8\u0303))\u2212 n\u2211 i=1 `(\u2212yif(xi;\u03b8\u2217)) \u2265 0.\nThus, \u03b8\u2217 is a local minima of the empirical loss function with f(xi;\u03b8 \u2217) = \u00b5i \u2212M\u03c3(0). Since there exists a \u00b5imax such that yimax(\u00b5imax \u2212M\u03c3(0)) = 1, then this means that the neural network makes an incorrect prediction on the sample ximax . This indicates that this local minimum has a non-zero training error.\nFinally, we present the way we construct the neural network fD. Since\nfD(x;\u03b8D) = fD(x; \u03b81, ..., \u03b8d) = n\u2211 i=1 \u00b5i d\u220f k=1 1 { x(k) \u2208 [ x (k) i \u2212 \u03b8k, x (k) i + \u03b8k ]} .\nLet \u03c3th denote the threshold unit, where \u03c3th(z) = 1 if z \u2265 0 and \u03c3th(z) = 0, otherwise. Therefore, the indicator function can be represented as follows:\n1 { x(k) \u2208 [ x\n(k) i \u2212 \u03b8k, x (k) i + \u03b8k ]} = \u03c3th ( x(k) \u2212 x(k)i + \u03b8k ) \u2212 \u03c3th ( x(k) \u2212 x(k)i \u2212 \u03b8k ) Therefore,\nd\u220f k=1 1 { x(k) \u2208 [ x (k) i \u2212 \u03b8k, x (k) i + \u03b8k ]} = \u03c3th ( d\u2211\nk=1\n[ \u03c3th ( x(k) \u2212 x(k)i + \u03b8k ) \u2212 \u03c3th ( x(k) \u2212 x(k)i \u2212 \u03b8k )] \u2212 d+ 1\n2\n)\nTherefore, we have\nfD(x;\u03b8D) = n\u2211 i=1 \u00b5i\u03c3th\n( d\u2211\nk=1\n[ \u03c3th ( x(k) \u2212 x(k)i + \u03b8k ) \u2212 \u03c3th ( x(k) \u2212 x(k)i \u2212 \u03b8k )] \u2212 d+ 1\n2\n) .\nIt is very easy to see that this is a two layer network consisted of threshold units.\nFurthermore, we note here that, in the proof shown above, we assume the only parameters in the network fD are \u03b81, ...,\u03b8d. In fact, we can prove a more general statement where the fD is of the form\nfD(x;\u03b8D) = n\u2211 i=1 \u00b5i\u03c3th\n( d\u2211\nk=1\n[ aik\u03c3th ( x(k) + uik ) + bik\u03c3th ( x(k) + vik )] + ci ) ,\nwhere aik, bik, uik, vik, ci, i \u2208 [n], k \u2208 [d] are all parameters. We can show that the neural network\nfD(x;\u03b8D) = n\u2211 i=1 \u00b5i\u03c3th\n( d\u2211\nk=1\n[ \u03c3th ( x(k) \u2212 x(k)i + \u03b8k ) \u2212 \u03c3th ( x(k) \u2212 x(k)i \u2212 \u03b8k )] \u2212 d+ 1\n2\n) ,\ndenotes a local minimum, since any slight perturbations on parameters aik, bik, uik, vik, ci, i \u2208 [n], k \u2208 [d] do not change the output of the neural network on the samples in the dataset D."}], "year": 2018, "references": [{"title": "Deep learning", "authors": ["Y. LeCun", "Y. Bengio", "G.E. Hinton"], "venue": "Nature, 521(7553):436", "year": 2015}, {"title": "The loss surfaces of multilayer networks", "authors": ["A. Choromanska", "M. Henaff", "M. Mathieu", "G. Arous", "Y. LeCun"], "venue": "AISTATS", "year": 2015}, {"title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization", "authors": ["Y.N. Dauphin", "R. Pascanu", "C. Gulcehre", "K. Cho", "S. Ganguli", "Y. Bengio"], "venue": "Advances in neural information processing systems, pages 2933\u20132941", "year": 2014}, {"title": "Large-scale machine learning with stochastic gradient descent", "authors": ["L. Bottou"], "venue": "Proceedings of COMPSTAT\u20192010, pages 177\u2013186. Springer", "year": 2010}, {"title": "Imagenet classification with deep convolutional neural networks", "authors": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS", "year": 2012}, {"title": "Maxout networks", "authors": ["I. J Goodfellow", "D. Warde-Farley", "M. Mirza", "A. Courville", "Y. Bengio"], "venue": "arXiv preprint arXiv:1302.4389", "year": 2013}, {"title": "Regularization of neural networks using dropconnect", "authors": ["L. Wan", "M. Zeiler", "S. Zhang", "Y. Le Cun", "R. Fergus"], "venue": "ICML, pages 1058\u20131066", "year": 2013}, {"title": "and J", "authors": ["Y. Li", "J. Yosinski", "J. Clune", "H. Lipson"], "venue": "Hopcroft. Convergent learning: Do different neural networks learn the same representations? arXiv preprint arXiv:1511.07543", "year": 2015}, {"title": "Learning polynomials with neural networks", "authors": ["A. Andoni", "R. Panigrahy", "G. Valiant", "L. Zhang"], "venue": "ICML", "year": 2014}, {"title": "Provable methods for training neural networks with sparse connectivity", "authors": ["H. Sedghi", "A. Anandkumar"], "venue": "arXiv preprint arXiv:1412.2693", "year": 2014}, {"title": "Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods", "authors": ["M. Janzamin", "H. Sedghi", "A. Anandkumar"], "venue": "arXiv preprint arXiv:1506.08473", "year": 2015}, {"title": "Global optimality in tensor factorization", "authors": ["B. D Haeffele", "R. Vidal"], "venue": "deep learning, and beyond. arXiv preprint arXiv:1506.07540", "year": 2015}, {"title": "Globally optimal training of generalized polynomial neural networks with nonlinear spectral methods", "authors": ["A. Gautier", "Q.N. Nguyen", "M. Hein"], "venue": "Advances in Neural Information Processing Systems, pages 1687\u20131695", "year": 2016}, {"title": "Globally optimal gradient descent for a convnet with gaussian inputs", "authors": ["A. Brutzkus", "A. Globerson"], "venue": "arXiv preprint arXiv:1702.07966", "year": 2017}, {"title": "Learning relus via gradient descent", "authors": ["M. Soltanolkotabi"], "venue": "NIPS, pages 2004\u20132014", "year": 2017}, {"title": "Exponentially vanishing sub-optimal local minima in multilayer neural networks", "authors": ["D. Soudry", "E. Hoffer"], "venue": "arXiv preprint arXiv:1702.05777", "year": 2017}, {"title": "Learning depth-three neural networks in polynomial time", "authors": ["S. Goel", "A. Klivans"], "venue": "arXiv preprint arXiv:1709.06010", "year": 2017}, {"title": "and Y", "authors": ["S.S. Du", "J.D. Lee"], "venue": "Tian. When is a convolutional filter easy to learn? arXiv preprint arXiv:1709.06129", "year": 2017}, {"title": "Recovery guarantees for one-hiddenlayer neural networks", "authors": ["K. Zhong", "Z. Song", "P. Jain", "P. L Bartlett", "I. S Dhillon"], "venue": "arXiv preprint arXiv:1706.03175", "year": 2017}, {"title": "Convergence analysis of two-layer neural networks with relu activation", "authors": ["Y. Li", "Y. Yuan"], "venue": "NIPS, pages 597\u2013607", "year": 2017}, {"title": "Neural networks and principal component analysis: Learning from examples without local minima", "authors": ["P. Baldi", "K. Hornik"], "venue": "Neural networks, 2(1):53\u201358", "year": 1989}, {"title": "Deep learning without poor local minima", "authors": ["K. Kawaguchi"], "venue": "Advances in Neural Information Processing Systems, pages 586\u2013594", "year": 2016}, {"title": "Topology and geometry of half-rectified network optimization", "authors": ["C D. Freeman", "J. Bruna"], "venue": "ICLR", "year": 2016}, {"title": "Identity matters in deep learning", "authors": ["M. Hardt", "T. Ma"], "venue": "ICLR", "year": 2017}, {"title": "Global optimality conditions for deep neural networks", "authors": ["C. Yun", "S. Sra", "A. Jadbabaie"], "venue": "arXiv preprint arXiv:1707.02444", "year": 2017}, {"title": "The loss surface and expressivity of deep convolutional neural networks", "authors": ["Q. Nguyen", "M. Hein"], "venue": "arXiv preprint arXiv:1710.10928", "year": 2017}, {"title": "The loss surface of deep and wide neural networks", "authors": ["Q. Nguyen", "M. Hein"], "venue": "arXiv preprint arXiv:1704.08045", "year": 2017}, {"title": "Theoretical properties of the global optimizer of two layer neural network", "authors": ["D. Boob", "G. Lan"], "venue": "arXiv preprint arXiv:1710.11241", "year": 2017}, {"title": "Theoretical insights into the optimization landscape of over-parameterized shallow neural networks", "authors": ["M. Soltanolkotabi", "A. Javanmard", "J.D. Lee"], "venue": "arXiv preprint arXiv:1707.04926", "year": 2017}, {"title": "Eigenfaces vs", "authors": ["P.N. Belhumeur", "J. P Hespanha", "D.J. Kriegman"], "venue": "fisherfaces: Recognition using class specific linear projection. IEEE Transactions on pattern analysis and machine intelligence, 19(7):711\u2013720", "year": 1997}, {"title": "Sparse pca", "authors": ["C. Chennubhotla", "A. Jepson"], "venue": "extracting multi-scale structure from data. In ICCV, volume 1, pages 641\u2013647. IEEE", "year": 2001}, {"title": "Active appearance models", "authors": ["T.F. Cootes", "G.J. Edwards", "C.J. Taylor"], "venue": "IEEE Transactions on pattern analysis and machine intelligence, 23(6):681\u2013685", "year": 2001}, {"title": "Deep residual learning for image recognition", "authors": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CVPR, pages 770\u2013778", "year": 2016}, {"title": "Highway networks", "authors": ["R.K. Srivastava", "K. Greff", "J. Schmidhuber"], "venue": "arXiv preprint arXiv:1505.00387", "year": 2015}], "id": "SP:9be76db1418022f8e00663acde200b4002f34ff8", "authors": [{"name": "Shiyu Liang", "affiliations": []}, {"name": "Ruoyu Sun", "affiliations": []}, {"name": "Yixuan Li", "affiliations": []}], "abstractText": "It is widely conjectured that the reason that training algorithms for neural networks are successful because all local minima lead to similar performance; for example, see [1, 2, 3]. Performance is typically measured in terms of two metrics: training performance and generalization performance. Here we focus on the training performance of neural networks for binary classification, and provide conditions under which the training error is zero at all local minima of appropriately chosen surrogate loss functions. Our conditions are roughly in the following form: the neurons have to be increasing and strictly convex, the neural network should either be single-layered or is multi-layered with a shortcut-like connection, and the surrogate loss function should be a smooth version of hinge loss. We also provide counterexamples to show that, when these conditions are relaxed, the result may not hold.", "title": "Understanding the Loss Surface of Neural Networks for Binary Classification"}