0	39	Summarization is the task of condensing a piece of text to a shorter version that contains the main information from the original.
2	14	Extractive methods assemble summaries exclusively from passages (usually whole sentences) taken directly from the source text, while abstractive methods may generate novel words and phrases not featured in the source text – as a human-written abstract usually does.
9	16	While most recent abstractive work has focused on headline generation tasks (reducing one or two sentences to a single headline), we believe that longer-text summarization is both more challenging (requiring higher levels of abstraction while avoiding repetition) and ultimately more useful.
10	48	Therefore we apply our model to the recently-introduced CNN/ Daily Mail dataset (Hermann et al., 2015; Nallapati et al., 2016), which contains news articles (39 sentences on average) paired with multi-sentence summaries, and show that we outperform the stateof-the-art abstractive system by at least 2 ROUGE points.
18	29	The tokens of the article wi are fed one-by-one into the encoder (a single-layer bidirectional LSTM), producing a sequence of encoder hidden states hi.
23	26	Pvocab is a probability distribution over all words in the vocabulary, and provides us with our final distribution from which to predict words w: P(w) = Pvocab(w) (5) During training, the loss for timestep t is the negative log likelihood of the target word w∗t for that timestep: losst =− logP(w∗t ) (6) and the overall loss for the whole sequence is: loss = 1 T ∑ T t=0 losst (7)
27	71	Next, pgen is used as a soft switch to choose between generating a word from the vocabulary by sampling from Pvocab, or copying a word from the input sequence by sampling from the attention distribution at .
28	39	For each document let the extended vocabulary denote the union of the vocabulary, and all words appearing in the source document.
29	24	We obtain the following probability distribution over the extended vocabulary: P(w) = pgenPvocab(w)+(1− pgen)∑i:wi=w ati (9) Note that if w is an out-of-vocabulary (OOV) word, then Pvocab(w) is zero; similarly if w does not appear in the source document, then ∑i:wi=w a t i is zero.
30	14	The ability to produce OOV words is one of the primary advantages of pointer-generator models; by contrast models such as our baseline are restricted to their pre-set vocabulary.
32	81	Repetition is a common problem for sequenceto-sequence models (Tu et al., 2016; Mi et al., 2016; Sankaran et al., 2016; Suzuki and Nagata, 2016), and is especially pronounced when generating multi-sentence text (see Figure 1).
34	25	In our coverage model, we maintain a coverage vector ct , which is the sum of attention distributions over all previous decoder timesteps: ct = ∑t−1t ′=0 at ′ (10) Intuitively, ct is a (unnormalized) distribution over the source document words that represents the degree of coverage that those words have received from the attention mechanism so far.
36	24	The coverage vector is used as extra input to the attention mechanism, changing equation (1) to: eti = v T tanh(Whhi +Wsst +wccti +battn) (11) where wc is a learnable parameter vector of same length as v. This ensures that the attention mechanism’s current decision (choosing where to attend next) is informed by a reminder of its previous decisions (summarized in ct).
38	22	We find it necessary (see section 5) to additionally define a coverage loss to penalize repeatedly attending to the same locations: covlosst = ∑i min(ati,cti) (12) Note that the coverage loss is bounded; in particular covlosst ≤∑i ati = 1.
81	25	During training and at test time we truncate the article to 400 tokens and limit the length of the summary to 100 tokens for training and 120 tokens at test time.3 This is done to expedite training and testing, but we also found that truncating the article can raise the performance of the model (see section 7.1 for more details).
97	13	The output of our models is available online.6 Given that we generate plain-text summaries but Nallapati et al. (2016; 2017) generate anonymized summaries (see Section 4), our ROUGE scores are not strictly comparable.
101	21	Nevertheless, given that the disparity in the lead-3 scores is (+1.1 ROUGE-1, +2.0 ROUGE-2, +1.1 ROUGEL) points respectively, and our best model scores exceed Nallapati et al. (2016) by (+4.07 ROUGE1, +3.98 ROUGE-2, +3.73 ROUGE-L) points, we may estimate that we outperform the only previous abstractive system by at least 2 ROUGE points allround.
115	19	However, our best model does not quite surpass the ROUGE scores of the lead-3 baseline, nor the current best extractive model (Nallapati et al., 2017).
119	34	Firstly, news articles tend to be structured with the most important information at the start; this partially explains the strength of the lead-3 baseline.
120	23	Indeed, we found that using only the first 400 tokens (about 20 sentences) of the article yielded significantly higher ROUGE scores than using the first 800 tokens.
121	85	Secondly, the nature of the task and the ROUGE metric make extractive approaches and the lead3 baseline difficult to beat.
123	14	Given that the articles contain 39 sentences on average, there are many equally valid ways to choose 3 or 4 highlights in this style.
125	28	For example, smugglers profit from desperate migrants is a valid alternative abstractive summary for the first example in Figure 5, but it scores 0 ROUGE with respect to the reference summary.
136	86	But does the ease of copying make our system any less abstractive?
140	30	This is a main area for improvement, as we would like our model to move beyond simple sentence extraction.
142	28	Article sentences are truncated to form grammatically-correct shorter versions, and new sentences are composed by stitching together fragments.
155	83	Our mixture model allows the network to copy while simultaneously consulting the language model – enabling operations like stitching and truncation to be performed with grammaticality.
157	34	In this work we presented a hybrid pointergenerator architecture with coverage, and showed that it reduces inaccuracies and repetition.
158	34	We applied our model to a new and challenging longtext dataset, and significantly outperformed the abstractive state-of-the-art result.
159	21	Our model exhibits many abstractive abilities, but attaining higher levels of abstraction remains an open research question.
