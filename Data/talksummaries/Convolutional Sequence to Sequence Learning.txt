1	12	The dominant approach to date encodes the input sequence with a series of bi-directional recurrent neural networks (RNN) and generates a variable length output with another set of decoder RNNs, both of which interface via a soft-attention mechanism (Bahdanau et al., 2014; Luong et al., 2015).
2	26	In machine translation, this architecture has been demonstrated to outperform traditional phrase-based models by large margins (Sennrich et al., 2016b; Zhou et al., 2016; Wu et al., 2016;§2).
4	23	Compared to recurrent layers, convolutions create representations for fixed size contexts, however, the effective context size of the network can easily be made larger ?The source code and models are available at https: //github.com/facebookresearch/fairseq.
7	12	Convolutional networks do not depend on the computations of the previous time step and therefore allow parallelization over every element in a sequence.
9	7	Multi-layer convolutional neural networks create hierarchical representations over the input sequence in which nearby input elements interact at lower layers while distant elements interact at higher layers.
10	14	Hierarchical structure provides a shorter path to capture long-range dependencies compared to the chain structure modeled by recurrent networks, e.g. we can obtain a feature representation capturing relationships within a window of n words by applying only O(n k ) convolutional operations for kernels of width k, compared to a linear number O(n) for recurrent neural networks.
13	6	Recent work has applied convolutional neural networks to sequence modeling such as Bradbury et al. (2016) who introduce recurrent pooling between a succession of convolutional layers or Kalchbrenner et al. (2016) who tackle neural translation without attention.
15	8	Gated convolutions have been previously explored for machine translation by Meng et al. (2015) but their evaluation was restricted to a small dataset and the model was used in tandem with a traditional count-based model.
16	11	Architectures which are partially convolutional have shown strong performance on larger tasks but their decoder is still recurrent (Gehring et al., 2016).
17	13	In this paper we propose an architecture for sequence to sequence modeling that is entirely convolutional.
18	11	Our model is equipped with gated linear units (Dauphin et al., 2016) and residual connections (He et al., 2015a).
19	29	We also use attention in every decoder layer and demonstrate that each attention layer only adds a negligible amount of overhead.
21	44	We evaluate our approach on several large datasets for machine translation as well as summarization and compare to the current best architectures reported in the literature.
22	3	On WMT’16 English-Romanian translation we achieve a new state of the art, outperforming the previous best result by 1.9 BLEU.
23	50	On WMT’14 English-German we outperform the strong LSTM setup of Wu et al. (2016) by 0.5 BLEU and on WMT’14 English-French we outperform the likelihood trained system of Wu et al. (2016) by 1.6 BLEU.
24	86	Furthermore, our model can translate unseen sentences at an order of magnitude faster speed than Wu et al. (2016) on GPU and CPU hardware (§4,§5).
25	10	Sequence to sequence modeling has been synonymous with recurrent neural network based encoder-decoder architectures (Sutskever et al., 2014; Bahdanau et al., 2014).
26	78	The encoder RNN processes an input sequence x = (x1,...,xm) of m elements and returns state representations z = (z1....,zm).
27	32	The decoder RNN takes z and generates the output sequence y=(y1,...,yn) left to right, one element at a time.
28	23	To generate output y i+1, the decoder computes a new hidden state hi+1 based on the previous state h i , an embedding g i of the previous target language word y i , as well as a conditional input c i derived from the encoder output z.
29	10	Based on this generic formulation, various encoder-decoder architectures have been proposed, which differ mainly in the conditional input and the type of RNN.
30	111	Models without attention consider only the final encoder state z m by setting c i =z m for all i (Cho et al., 2014), or simply initialize the first decoder state with z m (Sutskever et al., 2014), in which case c i is not used.
31	57	Architectures with attention (Bahdanau et al., 2014; Luong et al., 2015) compute c i as a weighted sum of (z1....,zm) at each time step.
32	49	The weights of the sum are referred to as attention scores and allow the network to focus on different parts of the input sequence as it generates the output sequences.
37	18	Models with many layers often rely on shortcut or residual connections (He et al., 2015a; Zhou et al., 2016; Wu et al., 2016).
40	56	First, we embed input elements x=(x1,...,xm) in distributional space as w=(w1,...,wm), where wj 2Rf is a column in an embedding matrix D2RV⇥f .
41	105	We also equip our model with a sense of order by embedding the absolute position of input elements p=(p1,...,pm) where pj2Rf .
43	38	We proceed similarly for output elements that were already generated by the decoder network to yield output element representations that are being fed back into the decoder network g=(g1,...,gn).
