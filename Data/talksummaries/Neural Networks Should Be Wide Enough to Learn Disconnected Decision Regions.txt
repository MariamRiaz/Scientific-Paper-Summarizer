0	31	While deep learning has become state of the art in many application domains such as computer vision and natural language processing and speech recognition, the theoretical understanding of this success is steadily growing but there are still plenty of questions where there is little or no understanding.
1	30	In particular, for the question how one should construct the network e.g. choice of activation function, number of layers, number of hidden units per layer etc., there is little guidance and only limited understanding on the implications of the choice e.g. “The design of hidden units is an extremely active area of research and does not yet have many definitive guiding theoretical principles.” is a quote from the recent book on deep learning (Goodfellow et al., 2016, p. 191).
7	12	Tighter bounds on the number of linear regions are later on developed by (Arora et al., 2018; Serra et al., 2018; Charisopoulos & Maragos, 2018).
11	10	In particular, by studying a special type of networks called convolutional arithmetic circuits – also known as Sum-Product networks (Poon & Domingos, 2011), the authors show that besides a set of measure zero, all functions that can be realized by a deep network of polynomial size require exponential size in order to be realized, or even approximated by a shallow network.
13	20	Unlike most of previous work which focuses on the power of depth, (Lu et al., 2017; Hanin & Sellke, 2017) have recently shown that neural networks with ReLU activation function have to be wide enough in order to have the universal approximation property as depth increases.
34	6	This is leaky ReLU (Maas et al., 2013): σ(t) = max{t, αt} for 0 < α < 1, where typically α is fixed but it has also been optimized together with the network weights (He et al., 2015) and ELU (exponential linear unit) (Clevert et al., 2016): σ(t) = { et − 1 t < 0 t t ≥ 0. .
42	15	Definition 3.1 (Decision region) The decision region of a given class 1 ≤ j ≤ m, denoted by Cj , is defined as Cj = { x ∈ Rd ∣∣ (fL)j(x) > (fL)k(x), ∀k 6= j} .
56	7	Apart from the property of connectivity, we can also show the openness of a set when considering the pre-image of a given network.
103	7	Then every decision region Cj is an open connected subset of Rd for every 1 ≤ j ≤ m. Proof: We note that in the proof of Theorem 3.10 the Vj is a finite intersection of open half-spaces and thus a convex set.
106	6	As both sets are open convex sets, the intersection Vj ∩ σ̂1(Rn1) is again convex and open as well.
122	15	Taking both results together, it seems rather obvious that as a general guiding principle for the construction of hidden layers in neural networks one should use, at least for the first hidden layer, more units than the input dimension, as it is rather unlikely that the Bayes optimal decision regions are connected.
128	6	≥ dL−1 is not fulfilled, then the statement of the theorem might not hold as the decision regions can be disconnected.
129	9	We illustrate this via a counter-example below.
147	7	Finally, one notes in this example that except for the activation function, all the other conditions of Theorem 3.10 are still satisfied, that is, the network has pyramidal structure (2-2-2-2) and all the weight matrices (Wl)2l=1 have full rank by our construction.
149	30	We consider a binary classification task in R2 where the data points are generated so that the blue class has disconnected components on the square [−4, 4] × [−4, 4], see Figure 4 (a) for an illustration.
155	6	First, we observe that for two hidden units (n1 = 2), the network satisfies the condition of Theorem 3.10 and thus can only learn connected regions, which one can also clearly see in the figure, where one basically gets a linear separator.
158	19	We use a single image of digit 1 from the MNIST dataset to create a new artificial dataset where the underlying data generation probability measure has a similar one-dimensional structure as in (4) but now embedded in the pixel space R28×28.
160	6	We generate 2000 training images for each red/blue class by rotating the chosen digit 1 with angles ranging from [−5◦, 5◦] for the read class, and [−20◦,−15◦] ∪ [15◦, 20◦] for the blue class, see Figure 5.
161	7	Note that this is a binary classification task where the dataset has just one effective degree of freedom and the Bayes optimal decision regions are disconnected.
162	15	We train a one hidden layer network with 784 hidden units which is equal to the input dimension and leaky ReLU as activation function with α = 0.1.
163	25	The training error is zero and the resulting weight matrices have full rank, thus the conditions of Theorem 3.10 are satisfied and the decision region of class blue should be connected even though the Bayes optimal decision region is disconnected.
164	8	This can only happen by establishing a connection around the other red class.
166	10	Next, we generate an adversarial image 1 from the red class using the one step target class method (Kurakin et al., 2016; 2017) and consider the path between the source image to the adversarial image and subsequently from the adversarial image to the target one.
168	7	Figure 6 shows the complete path from the source image to the target image where the color indicates that all the intermediate images are classified as blue with high confidence (note that we turned the output of the network into probabilities by using the softmax function).
174	6	This could be handled by introducing a background class, but then it would be even more important that the classifier can produce disconnected decision regions which naturally requires a minimal width of d+ 1 of the network.
177	16	Once again, one can see that there exists a continuous path that connects two different-looking images of digit 5 (blue class) where every image along this path is classified as blue class with high confidence.
179	7	We have shown that deep neural networks (with a certain class of activation functions) need to have in general width larger than the input dimension in order to learn disconnected decision regions.
180	28	It remains an open problem if our current requirement σ(R) = R can be removed.
181	64	While our result does not resolve the question how to choose the network architecture in practice, it provides at least a guideline how to choose the width of the network.
182	52	Moreover, our result and experiments show that too narrow networks produce high confidence predictions on a path connecting the true disconnected decision regions which could be used to attack these networks using adversarial manipulation.
