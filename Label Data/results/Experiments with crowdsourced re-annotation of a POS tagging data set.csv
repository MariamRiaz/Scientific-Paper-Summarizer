0,1,label2,summary_sentences
A key to the success of probabilistic modeling is the pairing of rich probability models with fast and accurate inference algorithms.,1. Introduction,[0],[0]
Probabilistic graphical models enable this by providing a flexible class of probability distributions together with algorithms that exploit the graph structure for efficient inference.,1. Introduction,[0],[0]
"However, exact inference algorithms are only available when both the distributions involved and the graph structure are simple enough.",1. Introduction,[0],[0]
"How-
",1. Introduction,[0],[0]
"1College of Information and Computer Sciences, University of Massachusetts Amherst 2Department of Computer Science, Mount Holyoke College.",1. Introduction,[0],[0]
"Correspondence to: Kevin Winner <kwinner@cs.umass.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
ever, this situation is rare and consequently, much research today is devoted to general-purpose approximate inference techniques (e.g. Ranganath et al., 2014; Kingma & Welling, 2014; Carpenter et al., 2016).
",1. Introduction,[0],[0]
"Despite many advances in probabilistic inference, there remain relatively simple (and useful) models for which exact inference algorithms are not available.",1. Introduction,[0],[0]
This paper considers the case of graphical models with a simple structure but with (unbounded) latent count random variables.,1. Introduction,[0],[0]
"These are a natural modeling choice for many real world problems in ecology (Zonneveld, 1991; Royle, 2004; Dail & Madsen, 2011) and epidemiology (Farrington et al., 2003; Panaretos, 2007; Kvitkovicova & Panaretos, 2011).",1. Introduction,[0],[0]
"However, they pose a unique challenge for inference: even though algorithms like belief propagation (Pearl, 1986) or variable elimination (Zhang & Poole, 1994) are well defined mathematically, they cannot be implemented in an obvious way because factors have a countably infinite number of entries.",1. Introduction,[0],[0]
"As a result, approximations like truncating the support of the random variables or MCMC are applied (Royle, 2004; Gross et al., 2007; Chandler et al., 2011; Dail & Madsen, 2011; Zipkin et al., 2014; Winner et al., 2015).
",1. Introduction,[0],[0]
"Recently, Winner & Sheldon (2016) introduced a new technique for exact inference in models with latent count variables.",1. Introduction,[0],[0]
"Their approach executes the same operations as variable elimination, but with factors, which are infinite sequences of values, represented in a compact way using probability generating functions (PGFs).",1. Introduction,[0],[0]
"They developed an efficient exact inference algorithm for a specific class of Poisson hidden Markov models (HMMs) that represent a population undergoing mortality and immigration, and noisy observations of the population over time.
",1. Introduction,[0],[0]
A key open question is the extent to which PGF-based inference generalizes to a broader class of models.,1. Introduction,[0],[0]
There are two primary considerations.,1. Introduction,[0],[0]
"First, for what types of factors can the required operations (multiplication, marginalization, and conditioning) be “lifted” to PGF-based representations?",1. Introduction,[0],[0]
"Here, there is significant room for generalization: the mathematical PGF operations developed in (Winner & Sheldon, 2016) already apply to a broad class of nonPoisson immigration models, and we will generalize the models further to allow richer models of population survival and growth.",1. Introduction,[0],[0]
"Second, and more significantly, for what
types of PGFs can the requisite mathematical operations be implemented efficiently?",1. Introduction,[0],[0]
Winner & Sheldon (2016) manipulated PGFs symbolically.,1. Introduction,[0],[0]
"Their compact symbolic representation seems to rely crucially on properties of the Poisson distribution; it remains unclear whether symbolic PGF inference can be generalized beyond Poisson models.
",1. Introduction,[0],[0]
"This paper introduces a new algorithmic technique based on higher-order automatic differentiation (Griewank & Walther, 2008) for inference with PGFs.",1. Introduction,[0],[0]
A key insight is that most inference tasks do not require a full symbolic representation of the PGF.,1. Introduction,[0],[0]
"For example, the likelihood is computed by evaluating a PGF F (s) at s = 1.",1. Introduction,[0],[0]
Other probability queries can be posed in terms of derivatives F (k)(s) evaluated at either s = 0 or s = 1.,1. Introduction,[0],[0]
"In all cases, it suffices to evaluate F and its higher-order derivatives at particular values of s, as opposed to computing a compact symbolic representation of F .",1. Introduction,[0],[0]
"It may seem that this problem is then solved by standard techniques, such as higher-order forward-mode automatic differentiation (Griewank & Walther, 2008).",1. Introduction,[0],[0]
"However, the requisite PGF F is complex—it is defined recursively in terms of higher-order derivatives of other PGFs—and offthe-shelf automatic differentiation methods do not apply.",1. Introduction,[0],[0]
"We therefore develop a novel recursive procedure using building blocks of forward-mode automatic differentiation (generalized dual numbers and univariate Taylor polynomials; Griewank & Walther, 2008) to evaluate F and its derivatives.
",1. Introduction,[0],[0]
"Our algorithmic contribution leads to the first efficient exact algorithms for a class of HMMs that includes many well-known models as special cases, and has many applications.",1. Introduction,[0],[0]
"The hidden variables represent a population that undergoes three different processes: mortality (or emigration), immigration, and growth.",1. Introduction,[0],[0]
A variety of different distributional assumptions may be made about each process.,1. Introduction,[0],[0]
The models may also be viewed without this interpretation as a flexible class of models for integer-valued time series.,1. Introduction,[0],[0]
"Special cases include models from population ecology (Royle, 2004; Gross et al., 2007; Dail & Madsen, 2011), branching processes (Watson & Galton, 1875; Heathcote, 1965), queueing theory (Eick et al., 1993), and integer-valued autoregressive models (McKenzie, 2003).",1. Introduction,[0],[0]
Additional details about the relation to these models are given in Section 2.,1. Introduction,[0],[0]
"Our algorithms permit exact calculation of the likelihood for all of these models even when they are partially observed.
",1. Introduction,[0],[0]
"We demonstrate experimentally that our new exact inference algorithms are more scalable than competing approximate approaches, and support learning via exact likelihood calculations in a broad class of models for which this was not previously possible.",1. Introduction,[0],[0]
"We consider a hidden Markov model with integer latent variables N
1 , . . .",2. Model and Problem Statement,[0],[0]
", NK and integer observed variables Y 1
, . . .",2. Model and Problem Statement,[0],[0]
", YK .",2. Model and Problem Statement,[0],[0]
All variables are assumed to be non-negative.,2. Model and Problem Statement,[0],[0]
"The model is most easily understood in the context of its application to population ecology or branching processes (which are similar): in these cases, the variable Nk represents the size of a hidden population at time tk, and Yk represents the number of individuals that are observed at time tk.",2. Model and Problem Statement,[0],[0]
"However, the model is equally valid without this interpretation as a flexible class of autoregressive processes (McKenzie, 2003).
",2. Model and Problem Statement,[0],[0]
We introduce some notation to describe the model.,2. Model and Problem Statement,[0],[0]
"For an integer random variable N , write Y = ⇢ N to mean that Y ⇠ Binomial(N, ⇢).",2. Model and Problem Statement,[0],[0]
This operation is known as “binomial thinning”: the count Y is the number of “survivors” from the original count N .,2. Model and Problem Statement,[0],[0]
We can equivalently write Y = PN i=1,2. Model and Problem Statement,[0],[0]
Xi for iid Xi ⇠ Bernoulli(⇢) to highlight the fact that this is a compound distribution.,2. Model and Problem Statement,[0],[0]
"Indeed, compound distributions will play a key role: for independent integer random variables N and X , let Z = N X denote the compound random variable Z = PN i=1",2. Model and Problem Statement,[0],[0]
"Xi, where {Xi} are independent copies of X .",2. Model and Problem Statement,[0],[0]
"Now, we can describe our model as:
Nk = (Nk 1 Xk) +Mk, (1) Yk = ⇢k Nk. (2)
",2. Model and Problem Statement,[0],[0]
The variable Nk represents the population size at time tk.,2. Model and Problem Statement,[0],[0]
The random variable Nk 1 Xk 1 = PNk 1 i=1,2. Model and Problem Statement,[0],[0]
"Xk 1,i is the number of offspring of individuals from the previous time step, where Xk 1,i is the total number of individuals “caused by” the ith individual alive at time tk 1.",2. Model and Problem Statement,[0],[0]
"This definition of offspring is flexible enough to model immediate offspring, surviving individuals, and descendants of more than one generation.",2. Model and Problem Statement,[0],[0]
"The random variable Mk is the number of immigrants at time tk, and Yk is the number of individuals observed at time tk, with the assumption that each individual is observed independently with probability ⇢",2. Model and Problem Statement,[0],[0]
"k. We have left unspecified the distributions of Mk and Xk, which we term the immigration and offspring distributions, respectively.",2. Model and Problem Statement,[0],[0]
These may be arbitrary distributions over non-negative integers.,2. Model and Problem Statement,[0],[0]
"We will assume the initial condition N
0 = 0, though the model can easily be extended to accommodate arbitrary initial distributions.
",2. Model and Problem Statement,[0],[0]
Problem Statement We use lower case variables to denote specific settings of random variables.,2. Model and Problem Statement,[0],[0]
"Let yi:j = (yi, . . .",2. Model and Problem Statement,[0],[0]
", yj) and ni:j = (ni, . . .",2. Model and Problem Statement,[0],[0]
", nj).",2. Model and Problem Statement,[0],[0]
"The model above defines a joint probability mass function (pmf) p(n
1:K , y1:K ; ✓) where we introduce the vector ✓ containing parameters of all component distributions when necessary.",2. Model and Problem Statement,[0],[0]
"It is clear that the density factors according to a hidden Markov model: p(n
1:K , y1:K) =
QK k=1 p(nk |nk 1)p(yk |nk).",2. Model and Problem Statement,[0],[0]
"We will consider several inference problems that are standard for HMMs, but pose unique challenges when the hidden variables have countably infinite support.",2. Model and Problem Statement,[0],[0]
"Specifically, suppose y
1:K are observed, then we seek to:
• Compute the likelihood L(✓) = p(y 1:K ; ✓) for any ✓,
• Compute moments and values of the pmf of the filtered marginals p(nk | y1:k; ✓), for any k, ✓,
• Estimate parameters ✓ by maximizing the likelihood.
",2. Model and Problem Statement,[0],[0]
"We focus technically on the first two problems, which will enable numerical optimization to maximize the likelihood.",2. Model and Problem Statement,[0],[0]
Another standard problem is to compute smoothed marginals p(nk | y1:K ; ✓) given both past and future observations relative to time step k.,2. Model and Problem Statement,[0],[0]
"Although this is interesting, it is technically more difficult, and we defer it for future work.
",2. Model and Problem Statement,[0],[0]
Connections to Other Models This model specializes to capture many different models in the literature.,2. Model and Problem Statement,[0],[0]
The latent process of Eq.,2. Model and Problem Statement,[0],[0]
"(1) is a Galton-Watson branching process with immigration (Watson & Galton, 1875; Heathcote, 1965).",2. Model and Problem Statement,[0],[0]
"It also captures a number of different AR(1) (first-order autoregressive) processes for integer variables (McKenzie, 2003); these typically assume Xk ⇠ Bernoulli( k), i.e., that the offspring process is binomial thinning of the current individuals.",2. Model and Problem Statement,[0],[0]
"For clarity when describing this as an offspring distribution, we will refer to it as Bernoulli offspring.",2. Model and Problem Statement,[0],[0]
"With Bernoulli offspring and time-homogenous Poisson immigration, the model is an M/M/1 queue (McKenzie, 2003); with time-varying Poisson immigration it is an Mt/M/1 queue (Eick et al., 1993).",2. Model and Problem Statement,[0],[0]
"For each of these models, we contribute the first known algorithms for exact inference and likelihood calculations when the process is partially observed.",2. Model and Problem Statement,[0],[0]
"This allows estimation from data that is noisy and has variability that should not be modeled by the latent process.
",2. Model and Problem Statement,[0],[0]
Special cases of our model with noisy observations occur in statistical estimation problems in population ecology.,2. Model and Problem Statement,[0],[0]
"When immigration is zero after the first time step and Xk = 1, the population size is a fixed random variable, and we recover the N -mixture model of Royle (2004) for estimating the size of an animal population from repeated counts.",2. Model and Problem Statement,[0],[0]
"With Poisson immigration and Bernoulli offspring, we recover the basic model of Dail & Madsen (2011) for open metapopulations; extended versions with overdispersion and population growth also fall within our framework by using negative-binomial immigration and Poisson offspring.",2. Model and Problem Statement,[0],[0]
"Related models for insect populations also fall within our framework (Zonneveld, 1991; Gross et al., 2007; Winner et al., 2015).",2. Model and Problem Statement,[0],[0]
The main goal in most of this literature is parameter estimation.,2. Model and Problem Statement,[0],[0]
"Until very recently, no exact algorithms were known to compute the likelihood, so ap-
proximations such as truncating the support of the latent variables (Royle, 2004; Fiske & Chandler, 2011; Chandler et al., 2011; Dail & Madsen, 2011) or MCMC (Gross et al., 2007; Winner et al., 2015) were used.",2. Model and Problem Statement,[0],[0]
Winner & Sheldon (2016) introduced PGF-based exact algorithms for the restricted version of the model with Bernoulli offspring and Poisson immigration.,2. Model and Problem Statement,[0],[0]
We will build on that work to provide exact inference and likelihood algorithms for all of the aforementioned models.,2. Model and Problem Statement,[0],[0]
"The standard approach for inference in HMMs is the forward-backward algorithm (Rabiner, 1989), which is a special case of more general propagation or messagepassing algorithms (Pearl, 1986; Lauritzen & Spiegelhalter, 1988; Jensen et al., 1990; Shenoy & Shafer, 1990).",3. Methods,[0],[0]
"Winner & Sheldon (2016) showed how to implement the forward algorithm using PGFs for models with Bernoulli offspring and Poisson immigration.
",3. Methods,[0],[0]
Forward Algorithm,3. Methods,[0],[0]
"The forward algorithm recursively computes “messages”, which are unnormalized distributions of subsets of the variables.",3. Methods,[0],[0]
"Specifically, define ↵k(nk) :",3. Methods,[0],[0]
"= p(nk, y1:k) and k(nk)",3. Methods,[0],[0]
":= p(nk, y1:k 1).",3. Methods,[0],[0]
"These satisfy the recurrence:
k(nk) =",3. Methods,[0],[0]
"X
nk 1
↵k 1(nk 1)p(nk |nk 1), (3)
↵k(nk) = k(nk)p(yk |nk).",3. Methods,[0],[0]
"(4)
We will refer to Equation (3) as the prediction step (the value of nk is predicted based on the observations y1:k 1), and Equation (4) as the evidence step (the new evidence yk is incorporated).",3. Methods,[0],[0]
"In finite models, the forward algorithm can compute the ↵k messages for k = 1, . . .",3. Methods,[0],[0]
",K directly using Equations (3) and (4).",3. Methods,[0],[0]
"However, if nk is unbounded, this cannot be done directly; for example, ↵k(nk) is an infinite sequence, and Equation (3) contains an infinite sum.",3. Methods,[0],[0]
"Winner & Sheldon (2016) observed that, for some conditional distributions p(nk |nk 1) and p(yk |nk), the operations of the forward algorithm can be carried out using PGFs.",3.1. Forward Algorithm with PGFs,[0],[0]
"Specifically, define the PGFs k(uk) and Ak(sk) of k(nk) and ↵k(nk), respectively, as:
k(uk) := 1X
nk=0
k(nk)u nk k , (5)
Ak(sk) := 1X
nk=0
↵k(nk)s nk k .",3.1. Forward Algorithm with PGFs,[0],[0]
"(6)
The PGFs k and Ak are power series in the variables uk and sk with coefficients equal to the message entries.
",3.1. Forward Algorithm with PGFs,[0],[0]
These functions capture all relevant information about the associated distributions.,3.1. Forward Algorithm with PGFs,[0],[0]
"Technically, k and Ak are unnormalized PGFs because the coefficients do not sum to one.",3.1. Forward Algorithm with PGFs,[0],[0]
"However, the normalization constants are easily recovered by evaluating the PGF on input value 1: for example, Ak(1) =",3.1. Forward Algorithm with PGFs,[0],[0]
"P nk
↵k(nk) = p(y1:k).",3.1. Forward Algorithm with PGFs,[0],[0]
This also shows that we can recover the likelihood as AK(1) = p(y1:K).,3.1. Forward Algorithm with PGFs,[0],[0]
"After normalizing, the PGFs can be interpreted as expectations, for example Ak(sk)/Ak(1) =",3.1. Forward Algorithm with PGFs,[0],[0]
E[sNkk,3.1. Forward Algorithm with PGFs,[0],[0]
"| y1:k].
In general, it is well known that the PGF F (s) of a nonnegative integer-valued random variable X uniquely defines the entries of the probability mass function and the moments of X , which are recovered from (higher-order) derivatives of F evaluated at zero and one, respectively:
Pr(X = r)",3.1. Forward Algorithm with PGFs,[0],[0]
=,3.1. Forward Algorithm with PGFs,[0],[0]
"F (r)(0)/r!, (7)
E[X] = F (1)(1), (8)
Var(X) = F (2)(1)",3.1. Forward Algorithm with PGFs,[0],[0]
h F (1)(1),3.1. Forward Algorithm with PGFs,[0],[0]
i 2 + F (1)(1).,3.1. Forward Algorithm with PGFs,[0],[0]
"(9)
More generally, the first q moments are determined by the derivatives F (r)(1) for r  q.",3.1. Forward Algorithm with PGFs,[0],[0]
"Therefore, if we can evaluate the PGF Ak and its derivatives for sk 2 {0, 1}, we can answer arbitrary queries about the filtering distributions p(nk, y1:k), and, in particular, solve our three stated inference problems.
",3.1. Forward Algorithm with PGFs,[0],[0]
"But how can we compute values of Ak, k, and their derivatives?",3.1. Forward Algorithm with PGFs,[0],[0]
What form do these PGFs have?,3.1. Forward Algorithm with PGFs,[0],[0]
"One key result of Winner & Sheldon (2016), which we generalize here, is the fact that there is also a recurrence relation among the PGFs.",3.1. Forward Algorithm with PGFs,[0],[0]
Proposition 1.,3.1. Forward Algorithm with PGFs,[0],[0]
Consider the probability model defined in Equations (1) and (2).,3.1. Forward Algorithm with PGFs,[0],[0]
"Let Fk be the PGF of the offspring random variable Xk, and let Gk be the PGF of the immigration random variable Mk.",3.1. Forward Algorithm with PGFs,[0],[0]
"Then k and Ak satisfy the following recurrence:
k(uk) =",3.1. Forward Algorithm with PGFs,[0],[0]
"Ak 1 Fk(uk) ·Gk(uk) (10)
Ak(sk)",3.1. Forward Algorithm with PGFs,[0],[0]
"= (sk⇢k)yk
yk! · (yk)k sk(1 ⇢k)
",3.1. Forward Algorithm with PGFs,[0],[0]
"(11)
Proof.",3.1. Forward Algorithm with PGFs,[0],[0]
"A slightly less general version of Equation (10) appeared in Winner & Sheldon (2016); the general version appears in the literature on branching processes with immigration (Heathcote, 1965).",3.1. Forward Algorithm with PGFs,[0],[0]
"Equation (11) follows directly from general PGF operations outlined in (Winner & Sheldon, 2016).
",3.1. Forward Algorithm with PGFs,[0],[0]
The PGF recurrence has the same two elements as the pmf recurrence in equations (3) and (4).,3.1. Forward Algorithm with PGFs,[0],[0]
"Equation (10) is the prediction step: it describes the PGF of k(nk) = p(nk, y1:k 1) in terms of previous PGFs.",3.1. Forward Algorithm with PGFs,[0],[0]
"Equation (11) is the evidence step: it describes the PGF for ↵k(nk) =
p(nk, y1:k) in terms of the previous PGF and the new observation yk.",3.1. Forward Algorithm with PGFs,[0],[0]
"Note that the evidence step involves the ykth derivative of the PGF k from the prediction step, where yk is the observed count.",3.1. Forward Algorithm with PGFs,[0],[0]
These high-order derivatives complicate the calculation of the PGFs.,3.1. Forward Algorithm with PGFs,[0],[0]
The recurrence reveals structure about Ak and k but does not immediately imply an algorithm.,3.2. Evaluating Ak via Automatic Differentiation,[0],[0]
"Winner & Sheldon (2016) showed how to use the recurrence to compute symbolic representations of all PGFs in the special case of Bernoulli offspring and Poisson immigration: in this case, they proved that all PGFs have the form F (s) = f(s)",3.2. Evaluating Ak via Automatic Differentiation,[0],[0]
"exp(as + b), where f is a polynomial of bounded degree.",3.2. Evaluating Ak via Automatic Differentiation,[0],[0]
"Hence, they can be represented compactly and computed efficiently using the recurrence.",3.2. Evaluating Ak via Automatic Differentiation,[0],[0]
"The result is a symbolic representation, so, for example, one obtains a closed form representation of the final PGF AK , from which the likelihood, entries of the pmf, and moments can be calculated.",3.2. Evaluating Ak via Automatic Differentiation,[0],[0]
"However, the compact functional form f(s) exp(as + b) seems to rely crucially on properties of the Poisson distribution.",3.2. Evaluating Ak via Automatic Differentiation,[0],[0]
"When other distributions are used, the size of the symbolic PGF representation grows quickly with K. It is an open question whether the symbolic methods can be extended to other classes of PGFs.
",3.2. Evaluating Ak via Automatic Differentiation,[0],[0]
This motivates an alternate approach.,3.2. Evaluating Ak via Automatic Differentiation,[0],[0]
"Instead of computing Ak symbolically, we will evaluate Ak and its derivatives at particular values of sk corresponding to the queries we wish to make (cf. Equations (7)–(9)).",3.2. Evaluating Ak via Automatic Differentiation,[0],[0]
"To develop the approach, it is helpful to consider the feed-forward computation for evaluating Ak at a particular value sk.",3.2. Evaluating Ak via Automatic Differentiation,[0],[0]
"The circuit diagram in Figure 1 is a directed acyclic graph that describes this calculation; the nodes are intermediate quantities in the calculation, and the shaded rectangles illustrate the recursively nested PGFs.
",3.2. Evaluating Ak via Automatic Differentiation,[0],[0]
"Now, we can consider techniques from automatic differentiation (autodiff) to compute Ak and its derivatives.",3.2. Evaluating Ak via Automatic Differentiation,[0],[0]
"How-
ever, these will not apply directly.",3.2. Evaluating Ak via Automatic Differentiation,[0],[0]
"Note that Ak is defined in terms of higher-order derivatives of the function k, which depends on higher-order derivatives of k 1, and so forth.",3.2. Evaluating Ak via Automatic Differentiation,[0],[0]
Standard autodiff techniques cannot handle these recursively nested derivatives.,3.2. Evaluating Ak via Automatic Differentiation,[0],[0]
"Therefore, we will develop a novel algorithm.",3.2. Evaluating Ak via Automatic Differentiation,[0],[0]
We now develop basic notation and building blocks that we will assemble to construct our algorithm.,3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
"It is helpful to abstract from our particular setting and describe a general model for derivatives within a feed-forward computation, following Griewank & Walther (2008).",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
"We consider a procedure that assigns values to a sequence of variables v 0 , v 1
, . . .",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
", vn, where v0 is the input variable, vn is the output variable, and each intermediate variable vj is computed via a function 'j(vi)i j of some subset (vi)i j of the variables v
0:j 1.",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
Here the dependence relation,3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
"i j simply means that 'j depends directly on vi, and (vi)i j is the vector of variables for which that is true.",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
"Note that the dependence relation defines a directed acyclic graph G (e.g., the circuit in Figure 1), and v
0 , . . .",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
", vn is a topological ordering of G.
We will be concerned with the values of a variable v` and its derivatives with respect to some earlier variable vi.",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
"To represent this cleanly, we first introduce a notation to capture the partial computation between the assignment of vi and v`.",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
"For i  `, define fi`(v0:i) to be the value that is assigned to v` if the values of the first i variables are given by v 0:i (now treated as fixed input values).",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
"This can be defined formally in an inductive fashion:
fi`(v0:i) = '`(uij)j `, uij = ( vj if j  i fij(v0:i) if j >",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
"i
This can be interpreted as recursion with memoization for v 0:i. When '` “requests” the value of uij of vj : if j  i, this value was given as an input argument of fi`, so we just “look it up”; but if j > i, we recursively compute the correct value via the partial computation from i to j. Now, we define a notation to capture derivatives of a variable v` with respect to an earlier variable vi.",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
Definition 1 (Dual numbers).,3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
"The generalized dual number hv`, dviiq for 0  i  ` and q > 0 is the sequence consisting of v` and its first q derivatives with respect to vi:
hv`,",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
"dviiq = ✓ @p
@vpi fi`(v0:i)
◆q
p=0
We say that hv`, dviiq is a dual number of order q with respect to vi.",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
Let DRq be the set of dual numbers of order q.,3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
"We will commonly write dual numbers as:
hs, duiq = ⇣ s, ds
du , . . .",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
",
dqs
duq
⌘
in which case it is understood that s = v` and u = vi for some 0  i  `, and the function fi`(·) will be clear from context.
",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
Our treatment of dual numbers and partial computations is more explicit than what is standard.,3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
"In particular, we are explicit both about the variable v` we are differentiating and the variable vi with respect to which we are differentiating.",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
"This is important for our algorithm, and also helps distinguish our approach from traditional automatic differentiation approaches.",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
"Forward-mode autodiff computes derivatives of all variables with respect to v
0 , i.e., it computes hvj , dv0iq for j = 1, . . .",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
", n. Reverse-mode autodiff computes derivatives of vn with respect to all variables, i.e., it computes hvn, dviiq for i = n 1, . . .",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
", 0.",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
"In each case, one of the two variables is fixed, so the notation can be simplified.",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
The general idea of our algorithm will resemble forwardmode autodiff.,3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"Instead of sequentially calculating the values v
1 , . .",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
.,3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
", vn in our feed-forward computation, we will calculate dual numbers hv
1 , dvi1iq1 , . . .",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
", hvn, dviniqn , where we leave unspecified (for now) the variables with respect to which we differentiate, and the order of the dual numbers.",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
We will require three high-level operations on dual numbers.,3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"The first one is “lifting” a scalar function.
",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
Definition 2 (Lifted Function).,3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
Let f : Rm !,3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"R be a function of variables x
1 , . . .",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
", xm.",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
The qth-order lifted function Lqf : (DRq)m !,3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"DRq is the function that accepts as input dual numbers hx
1 , duiq, . . .",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
", hxm, duiq of order q with respect to the same variable u, and returns the value⌦ f(x
1 , . . .",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
", xm), du ↵ q .
",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
Lifting is the basic operation of higher-order forward mode autodiff.,3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"For functions f consisting only of “primitive operations”, the lifted function Lqf can be computed at a modest overhead relative to computing f .
",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"Proposition 2 (Griewank & Walther, 2008).",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
Let f : Rm !,3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"R be a function that consists only of the following primitive operations, where x and y are arbitrary input variables and all other numbers are constants: x",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"+ cy, x ⇤ y, x/y, xr, ln(x), exp(x), sin(x), cos(x).",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"Then Lqf can be computed in time O(q2) times the running time of f .
",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"Based on this proposition, we will write algebraic operations on dual numbers, e.g., hx, duiq⇥hy, duiq , and understand these to be lifted versions of the corresponding scalar operations.",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"The standard lifting approach is to represent dual numbers as univariate Taylor polynomials (UTPs), in which case many operations (e.g., multiplication, addition) translate directly to the corresponding operations on polynomials.",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"We will use UTPs in the proof of Theorem 1.
",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
The second operation we will require is composition.,3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
Say that variable vj separates vi from v` if all paths from vi to v` in G go through vj .,3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
Theorem 1 (Composition).,3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
Suppose vj separates vi from v`.,3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"In this case, the dual number hv`, dviiq depends only on the dual numbers hv`, dvjiq and hvj , dviiq , and we define the composition operation:
hv`, dvjiq hvj , dviiq := hv`, dviiq
If vj does not separate vi from v`, the written composition operation is undefined.",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"The composition operation can be performed in O(q2 log q) time by composing two UTPs.
",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
Proof.,3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"If all paths from vi to v` go through vj , then vj is a “bottleneck” in the partial computation fil.",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"Specifically, there exist functions F and H such that vj = F (vi) and v` = H(vj).",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"Here, the notation suppresses dependence on variables that either are not reachable from vi, or do not have a path to v`, and hence may be treated as constants because they they do not impact the dual number hv`, viiq .",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
A detailed justification of this is given in the supplementary material.,3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"Now, our goal is to compute the higher-order derivatives of v` = H(F (vi)).",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"Let ˆF and ˆH be infinite Taylor expansions about vi and vj , respectively, omitting the constant terms F (vi) and H(vj):
ˆF ("") := 1X
p=1
F (p)(vi)
p!",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"""p, ˆH("") :=
1X
p=1
H(p)(vj)
p!",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"""p.
These are polynomials in "", and the first q coefficients are given in the input dual numbers.",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"The coefficient of ""p in ˆU("") := ˆH( ˆF ("")) for p 1 is exactly dpv`/dvpi (see Wheeler, 1987, where the composition of Taylor polynomials is related directly to the higher-order chain rule known as Faà dı́ Bruno’s Formula).",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
So it suffices to compute the first q coefficients of ˆH( ˆF (✏)).,3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"This can be done by executing Horner’s method (Horner, 1819) in truncated Taylor polynomial arithmetic (Griewank & Walther, 2008), which keeps only the first q coefficients of all polynomials (i.e., it assumes ✏p = 0 for p > q).",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"After truncation, Horner’s method involves q additions and q multiplications of polynomials of degree at most q. Polynomial multiplication takes time O(q log q) using the FFT, so the overall running time is O(q2 log q).
",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
The final operation we will require is differentiation.,3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"This will support local functions '` that differentiate a previous value, e.g., v` = '`(vj) = dpvj/dvpi .",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
Definition 3 (Differential Operator).,3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"Let hs, duiq be a dual number.",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"For p  q, the differential operator Dp applied to hs, duiq returns the dual number of order q p given by:
Dphs, duiq := ⇣ dps dup , . . .",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
", dqs duq ⌘
The differential operator can be applied in O(q) time.
",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"This operation was defined in (Kalaba & Tesfatsion, 1986).",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"We will now use these operations to lift the function Ak to compute h↵k, skiq = LA hsk, dskiq), i.e., the output of Ak and its derivatives with respect to its input.",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
Algorithm 1 gives a sequence of mathematical operations to compute Ak(sk).,3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
Algorithm 2 shows the corresponding operations on dual numbers; we call this algorithm the generalized dual-number forward algorithm or GDUALFORWARD.,3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"Note that a dual number of a variable with respect to itself is simply hx, dxiq = (x, 1, 0, . . .",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
", 0); such expressions are used without explicit initialization in Algorithm 2.",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"Also, if the dual number hx, dyiq has been assigned, we will assume the scalar value x is also available, for example, to initialize a new dual variable hx, dxiq (cf.",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
the dual number on the RHS of Line 3).,3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
Note that Algorithm 1 contains a non-primitive operation on Line 5: the derivative dyk k/duykk .,3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"To evaluate this in Algorithm 2, we must manipulate the dual number of k to be taken with respect to uk, and not the original input value sk, as in forward-mode autodiff.",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"Our approach can be viewed as following a different recursive principle from either forward or reverse-mode autodiff: in the circuit diagram of Figure 1, we calculate derivatives of each nested circuit with respect to its own input, starting with the innermost circuit and working out.",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
Theorem 2.,3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"LAK computes h↵k, dskiq in time O K(q",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"+
Y )2 log(q + Y ) where Y = PK
k=1 yk is the sum of the observed counts and q is the requested number of derivatives.",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"Therefore, the likelihood can be computed in O(KY 2 log Y ) time, and the first q moments or the first q entries of the filtered marginals can be computed in time",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
O K(q,3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"+ Y )2 log(q + Y ) .
",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
Proof.,3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"To see that GDUAL-FORWARD is correct, note that it corresponds to Algorithm 1, but applies the three operations from the previous section to operate on dual numbers instead of scalars.",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
We will verify that the conditions for applying each operation are met.,3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
Lines 2–5 each use lifting of algebraic operations or the functions,3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"Fk and Gk, which are assumed to consist only of primitive operations.",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"Lines 4 and 5 apply the composition operation; here, we can verify from Figure 1 that sk 1 separates uk and ↵k 1 (Line 4) and that uk separates sk and k (Line 5).",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"The conditions for applying the differential operator on Line 5 are also met.
",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"For the running time, note that the total number of operations on dual numbers in LAK , including recursive calls, is O(K).",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"The order of the dual numbers is initially q, but increases by yk in each recursive call (Line 4).",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"Therefore, the maximum value is q + Y .",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"Each of the operations on
Algorithm 1 Ak(sk) if k = 0 then
1: return ↵k = 1 end if 2: uk = sk(1 ⇢k) 3: sk 1 = Fk(uk) 4: k = Ak 1(sk 1) ·Gk(uk) 5: ↵k = d yk
du yk k k · (sk⇢k)yk/yk! 6: return ↵k
Algorithm 2 LAk(hsk, dskiq) — GDUAL-FORWARD if k = 0 then
1: return h↵k, dskiq = (1, 0, . . .",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
", 0) end if 2: huk, dskiq = hsk, dskiq · (1 ⇢k) 3: hsk 1, dukiq+yk = LFk huk, dukiq+yk
4: h k, dukiq+yk = ⇥ LAk 1",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"hsk 1, dsk 1iq+yk hsk 1, dukiq+yk ⇤ ⇥",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"LGk huk, dukiq+yk 5: h↵k, dskiq = ⇥",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"Dyk h k, dukiq+yk huk, dskiq ⇤ ⇥ ⇢khsk, dskiq
yk/yk! 6: return h↵k, dskiq
dual numbers is O(p2 log p) for dual numbers of order p, so the total is O(K(q + Y )2 log(q + Y )).",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"In this section we describe simulation experiments to evaluate the running time of GDUAL-FORWARD against other algorithms, and to assess the ability to learn a wide variety of models for which exact likelihood calculations were not previously possible, by using GDUAL-FORWARD within a parameter estimation routine.
",4. Experiments,[0],[0]
Running time vs Y .,4. Experiments,[0],[0]
"We compared the running time of GDUAL-FORWARD with the PGF-FORWARD algorithm from (Winner & Sheldon, 2016) as well as TRUNC, the standard truncated forward algorithm (Dail & Madsen, 2011).",4. Experiments,[0],[0]
"PGF-FORWARD is only applicable to the Poisson HMM from (Winner & Sheldon, 2016), which, in our terminology, is a model with a Poisson immigration distribution and a Bernoulli offspring distribution.",4. Experiments,[0],[0]
"TRUNC applies to any choice of distributions, but is approximate.",4. Experiments,[0],[0]
"For these experiments, we restrict to Poisson HMMs for the sake of comparison with the less general PGF-FORWARD algorithm.
",4. Experiments,[0],[0]
A primary factor affecting running time is the magnitude of the counts.,4. Experiments,[0],[0]
We measured the running time for all algorithms to compute the likelihood p(y; ✓) for vectors y,4. Experiments,[0],[0]
:= y 1:K = c ⇥,4. Experiments,[0],[0]
"(1, 1, 1, 1, 1) with increasing c.",4. Experiments,[0],[0]
"In this case, Y = P k yk = 5c.",4. Experiments,[0],[0]
"PGF-FORWARD and GDUAL-FORWARD have running times O(KY 2) and O(KY 2 log Y ), respectively, which depend only on Y and not ✓.",4. Experiments,[0],[0]
"The running time of an FFT-based implementation of TRUNC is O(KN2
max
logN max ), where N max is the value used to truncate the support of each latent variable.",4. Experiments,[0],[0]
"A heuristic is required to choose N
max so that it captures most of the probability mass of p(y; ✓) but is not too big.",4. Experiments,[0],[0]
"The appropriate value depends strongly on ✓, which in practice may be unknown.",4. Experiments,[0],[0]
"In preliminary experiments with realistic immigration and offspring models (see below) and known parameters, we found that an excellent heuristic is N
max = 0.4Y/⇢, which we use here.",4. Experiments,[0],[0]
"With this heuristic, TRUNC’s running time is O(K⇢2Y 2 log Y ).
",4. Experiments,[0],[0]
"Figure 3 shows the results for ⇢ 2 {0.15, 0.85}, averaged over 20 trials with error bars showing 95% confidence intervals of the mean.",4. Experiments,[0],[0]
"GDUAL-FORWARD and TRUNC have the same asymptotic dependence on Y but GDUALFORWARD scales better empirically, and is exact.",4. Experiments,[0],[0]
"It is about 8x faster than TRUNC for the largest Y when ⇢ = 0.15, and 2x faster for ⇢ = 0.85.",4. Experiments,[0],[0]
"PGF-FORWARD is faster by a factor of log Y in theory and scales better in practice, but applies to fewer models.
",4. Experiments,[0],[0]
Running time for different ✓.,4. Experiments,[0],[0]
"We also conducted experiments where we varied parameters and used an oracle method to select N
max for TRUNC.",4. Experiments,[0],[0]
"This was done by running the algorithm for increasing values of N
max and selecting the smallest one such that the likelihood was within 10 6 of the true value (see Winner & Sheldon, 2016).
",4. Experiments,[0],[0]
"We simulated data from Poisson HMMs and measured the time to compute the likelihood p(y; ✓) for the true parameters ✓ = ( , , ⇢), where is a vector whose kth entry is the mean of the Poisson immigration distribution at time k, and and ⇢ are scalars representing the Bernoulli survival probability and detection probability, respectively, which are shared across time steps.",4. Experiments,[0],[0]
"We set and to mimic three different biological models; for each, we varied ⇢ from 0.05 to 0.95.",4. Experiments,[0],[0]
"The biological models were as follows: ‘PHMM’ follows a temporal model for insect populations (Zonneveld, 1991) with = (5.13, 23.26, 42.08, 30.09, 8.56) and = 0.26; ‘PHMM-peaked’ is similar, but sets = (0.04, 10.26, 74.93, 25.13, 4.14) so the immigration is temporally “peaked” at the middle time step; ‘NMix’ sets = (80, 0, 0, 0, 0) and = 0.4, which is similar to the N-mixture model (Royle, 2004), with no immigration following the first time step.
",4. Experiments,[0],[0]
Figure 2 shows the running time of all three methods versus ⇢.,4. Experiments,[0],[0]
"In these models, E[Y ] is proportional to ⇢, and the running times of GDUAL-FORWARD and PGF-FORWARD increase with ⇢ due to the corresponding increase in Y .",4. Experiments,[0],[0]
"PGFFORWARD is faster by a factor of log Y , but is applicable to fewer models.",4. Experiments,[0],[0]
"GDUAL-FORWARD perfoms best relative to PGF-FORWARD for the NMix model, because it is fastest when counts occur in early time steps.
",4. Experiments,[0],[0]
"Recall that the running time of TRUNC is O(N2
max
logN max ).",4. Experiments,[0],[0]
"For these models, the distribution of the hidden population depends only on and , and these are the primary factors determining N
max .",4. Experiments,[0],[0]
"Running time decreases slightly as ⇢ increases, because the observation model p(y |n; ⇢) exerts more influence restricting implausible settings of n when the detection probability is higher.
",4. Experiments,[0],[0]
Parameter Estimation.,4. Experiments,[0],[0]
"To demonstrate the flexibility of the method, we used GDUAL-FORWARD within an optimization routine to compute maximum likelihood estimates (MLEs) for models with different immigration and growth distributions.",4. Experiments,[0],[0]
"In each experiment, we generated 10 independent observation vectors for K = 7 time steps from the same model p(y; ✓), and then used the L-BFGS-B algorithm to numerically find ✓ to maximize the loglikelihood of the 10 replicates.",4. Experiments,[0],[0]
We varied the distributional forms of the immigration and offspring distributions as well as the mean R := E[Xk] of the offspring distribution.,4. Experiments,[0],[0]
"We fixed the mean immigration := E[Mk] = 6 and the de-
tection probability to ⇢ = 0.6 across all time steps.",4. Experiments,[0],[0]
"The quantity R is the “basic reproduction number”, or the average number of offspring produced by a single individual, and is of paramount importance for disease and population models.",4. Experiments,[0],[0]
"We varied R, which was also shared across time steps, between 0.2 and 1.2.",4. Experiments,[0],[0]
"The parameters and R were learned, and ⇢ was fixed to resolve ambiguity between population size and detection probability.",4. Experiments,[0],[0]
"Each experiment was repeated 50 times; a very small number of optimizer runs failed to converge after 10 random restarts and were excluded.
",4. Experiments,[0],[0]
Figure 4 shows the distribution of 50 MLE estimates for R vs. the true values for each model.,4. Experiments,[0],[0]
Results for two additional models appear in the supplementary material.,4. Experiments,[0],[0]
In all cases the distribution of the estimate is centered around the true parameter.,4. Experiments,[0],[0]
It is evident that GDUAL-FORWARD can be used effectively to produce parameter estimates across a variety of models for which exact likelihood computations were not previously possible.,4. Experiments,[0],[0]
This material is based upon work supported by the National Science Foundation under Grant No. 1617533.,Acknowledgments,[0],[0]
Graphical models with latent count variables arise in a number of areas.,abstractText,[0],[0]
"However, standard inference algorithms do not apply to these models due to the infinite support of the latent variables.",abstractText,[0],[0]
"Winner & Sheldon (2016) recently developed a new technique using probability generating functions (PGFs) to perform efficient, exact inference for certain Poisson latent variable models.",abstractText,[0],[0]
"However, the method relies on symbolic manipulation of PGFs, and it is unclear whether this can be extended to more general models.",abstractText,[0],[0]
"In this paper we introduce a new approach for inference with PGFs: instead of manipulating PGFs symbolically, we adapt techniques from the autodiff literature to compute the higher-order derivatives necessary for inference.",abstractText,[0],[0]
"This substantially generalizes the class of models for which efficient, exact inference algorithms are available.",abstractText,[0],[0]
"Specifically, our results apply to a class of models that includes branching processes, which are widely used in applied mathematics and population ecology, and autoregressive models for integer data.",abstractText,[0],[0]
Experiments show that our techniques are more scalable than existing approximate methods and enable new applications.,abstractText,[0],[0]
Exact Inference for Integer Latent-Variable Models,title,[0],[0]
"Given a graphical model, one essential problem is MAP inference, that is, finding the most likely configuration of states according to the model. Although this problem is NP-hard, large instances can be solved in practice and it is a major open question is to explain why this is true. We give a natural condition under which we can provably perform MAP inference in polynomial time—we require that the number of fractional vertices in the LP relaxation exceeding the optimal solution is bounded by a polynomial in the problem size. This resolves an open question by Dimakis, Gohari, and Wainwright. In contrast, for general LP relaxations of integer programs, known techniques can only handle a constant number of fractional vertices whose value exceeds the optimal solution. We experimentally verify this condition and demonstrate how efficient various integer programming methods are at removing fractional solutions.",text,[0],[0]
"Given a graphical model, one essential problem is MAP inference, that is, finding the most likely configuration of states according to the model.
",1. Introduction,[0],[0]
"Consider graphical models with binary random variables and pairwise interactions, also known as Ising models.",1. Introduction,[0],[0]
"For a graph G = (V,E) with node weights θ ∈ RV and edge weights W ∈ RE , the probability of a variable configura-
1Department of Electrical and Computer Engineering, University of Texas at Austin, USA 2Department of Computer Science, University of Texas at Austin, USA.",1. Introduction,[0],[0]
Correspondence to:,1. Introduction,[0],[0]
"Erik M. Lindgren <erikml@utexas.edu>, Alexandros G. Dimakis <dimakis@austin.utexas.edu>, Adam Klivans <klivans@cs.utexas.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
tion is given by
P(X = x) = 1
Z exp ∑ i∈V θixi + ∑ ij∈E",1. Introduction,[0],[0]
"Wijxixj  , (1) where Z is a normalization constant.
",1. Introduction,[0],[0]
"The MAP problem is to find the configuration x ∈ {0, 1}V that maximizes Equation (1).",1. Introduction,[0],[0]
"We can write this as an integer linear program (ILP) as follows:
max q∈RV ∪E ∑ i∈V θiqi + ∑ ij∈E",1. Introduction,[0],[0]
"Wijqij
s.t. qi ∈ {0, 1} ∀i ∈ V qij ≥ max{0, qi + qj",1. Introduction,[0],[0]
− 1} ∀ij ∈,1. Introduction,[0],[0]
E,1. Introduction,[0],[0]
qij ≤,1. Introduction,[0],[0]
"min{qi, qj} ∀ij ∈",1. Introduction,[0],[0]
"E. (2)
",1. Introduction,[0],[0]
"The MAP problem on binary, pairwise graphical models contains, as a special case, the Max-cut problem and is therefore NP-hard.",1. Introduction,[0],[0]
"For this reason, a significant amount of attention has focused on analyzing the LP relaxation of the ILP, which can be solved efficiently in practice.
",1. Introduction,[0],[0]
max q∈RV ∪E ∑ i∈V θiqi + ∑ ij∈E,1. Introduction,[0],[0]
"Wijqij
s.t. 0 ≤ qi ≤ 1 ∀i ∈ V qij ≥ max{0, qi + qj",1. Introduction,[0],[0]
− 1} ∀ij ∈,1. Introduction,[0],[0]
E,1. Introduction,[0],[0]
qij ≤,1. Introduction,[0],[0]
"min{qi, qj} ∀ij ∈ E (3)
",1. Introduction,[0],[0]
This relaxation has been an area of intense research in machine learning and statistics.,1. Introduction,[0],[0]
"In (Meshi et al., 2016), the authors state that a major open question is to identify why real world instances of Problem (2) can be solved efficiently despite the theoretical worst case complexity.
",1. Introduction,[0],[0]
"We make progress on this open problem by analyzing the fractional vertices of the LP relaxation, that is, the extreme points of the polytope with fractional coordinates.",1. Introduction,[0],[0]
Vertices of the relaxed polytope with fractional coordinates are called pseudomarginals for graphical models and pseudocodewords in coding theory.,1. Introduction,[0],[0]
"If a fractional vertex has higher objective value (i.e. likelihood) compared to the best integral one, the LP relaxation fails.",1. Introduction,[0],[0]
"We call fractional vertices with an objective value at least as good as the objective
value of the optimal integral vertex confounding vertices.",1. Introduction,[0],[0]
"Our main result is that it is possible to prune all confounding vertices efficiently when their number is polynomial.
",1. Introduction,[0],[0]
"Our contributions:
• Our first contribution is a general result on integer programs.",1. Introduction,[0],[0]
"We show that any 0-1 integer linear program (ILP) can be solved exactly in polynomial time, if the number confounding vertices is bounded by a polynomial.",1. Introduction,[0],[0]
This applies to MAP inference for a graphical model over any alphabet size and any order of connection.,1. Introduction,[0],[0]
"The same result (exact solution if the number of confounding vertices is bounded by a polynomial) was established by (Dimakis et al., 2009) for the special case of LP decoding of LDPC codes (Feldman et al., 2005).",1. Introduction,[0],[0]
"The algorithm from (Dimakis et al., 2009) relies on the special structure of the graphical models that correspond to LDPC codes.",1. Introduction,[0],[0]
In this paper we generalize this result for any ILP in the unit hypercube.,1. Introduction,[0],[0]
"Our results extend to finding all integral vertices among the M -best vertices.
",1. Introduction,[0],[0]
"• Given our condition, one may be tempted to think that we generate the top M -best vertices of a linear program (for M polynomial) and output the best integral one in this list.",1. Introduction,[0],[0]
We actually show that such an approach would be computationally intractable.,1. Introduction,[0],[0]
"Specifically, we show that it is NP-hard to produce a list of the M -best vertices if M = O(nε) for any fixed ε > 0.",1. Introduction,[0],[0]
This result holds even if the list is allowed to be approximate.,1. Introduction,[0],[0]
"This strengthens the previously known hardness result (Angulo et al., 2014) which was M = O(n) for the exact M -best vertices.",1. Introduction,[0],[0]
"In terms of achievability, the best previously known result (from (Angulo et al., 2014)) can only solve the ILP if there is at most a constant number of confounding vertices.
",1. Introduction,[0],[0]
"• We obtain a complete characterization of the fractional vertices of the local polytope for binary, pairwise graphical models.",1. Introduction,[0],[0]
We show that any variable in the fractional support must be connected to a frustrated cycle by other fractional variables in the graphical model.,1. Introduction,[0],[0]
"This is a complete structural characterization that was not previously known, to the best of our knowledge.
",1. Introduction,[0],[0]
• We develop an approach to estimate the number of confounding vertices of a half-integral polytope.,1. Introduction,[0],[0]
We use this method in an empirical evaluation of the number of confounding vertices of previously studied problems and analyze how well common integer programming techniques perform at pruning confounding vertices.,1. Introduction,[0],[0]
"For some classes of graphical models, it is possible to solve the MAP problem exactly.",2. Background and Related Work,[0],[0]
"For example see (Weller et al., 2016) for balanced and almost balanced models, (Jebara, 2009) for perfect graphs, and (Wainwright et al., 2008) for graphs with constant tree-width.
",2. Background and Related Work,[0],[0]
These conditions are often not true in practice and a wide variety of general purpose algorithms are able to solve the MAP problem for large inputs.,2. Background and Related Work,[0],[0]
"One class is belief propagation and its variants (Yedidia et al., 2000; Wainwright et al., 2003; Sontag et al., 2008).",2. Background and Related Work,[0],[0]
"Another class involves general ILP optimization methods (see e.g. (Nemhauser & Wolsey, 1999)).",2. Background and Related Work,[0],[0]
"Techniques specialized to graphical models include cutting-plane methods based on the cycle inequalities (Sontag & Jaakkola, 2007; Komodakis & Paragios, 2008; Sontag et al., 2012).",2. Background and Related Work,[0],[0]
"See also (Kappes et al., 2013) for a comparative survey of techniques.
",2. Background and Related Work,[0],[0]
"In (Weller et al., 2014), the authors investigate how pseudomarginals and relaxations relate to the success of the Bethe approximation of the partition function.
",2. Background and Related Work,[0],[0]
"There has been substantial prior work on improving inference building on these LP relaxations, especially for LDPC codes in the information theory community.",2. Background and Related Work,[0],[0]
"This work ranges from very fast solvers that exploit the special structure of the polytope (Burshtein, 2009), connections to unequal error protection (Dimakis et al., 2007), and graphical model covers (Koetter et al., 2007).",2. Background and Related Work,[0],[0]
"LP decoding currently provides the best known finite-length error-correction bounds for LDPC codes both for random (Daskalakis et al., 2008; Arora et al., 2009), and adversarial bit-flipping errors (Feldman et al., 2007).
",2. Background and Related Work,[0],[0]
"For binary graphical models, there is a body of work which tries to exploit the persistency of the LP relaxation, that is, the property that integer components in the solution of the relaxation must take the same value in the optimal solution, under some regularity assumptions (Boros & Hammer, 2002; Rother et al., 2007; Fix et al., 2012).
",2. Background and Related Work,[0],[0]
"Fast algorithms for solving large graphical models in practice include (Ihler et al., 2012; Dechter & Rish, 2003).
",2. Background and Related Work,[0],[0]
"The work most closely related to this paper involves eliminating fractional vertices (so-called pseudocodewords in coding theory) by changing the polytope or the objective function (Zhang & Siegel, 2012; Chertkov & Stepanov, 2008; Liu et al., 2012).",2. Background and Related Work,[0],[0]
"A binary integer linear program is an optimization problem of the following form.
",3. Provable Integer Programming,[0],[0]
"max x
cTx
subject to Ax ≤ b x ∈ {0, 1}n
which is relaxed to a linear program by replacing the x ∈ {0, 1}n constraint with 0 ≤ x ≤ 1.",3. Provable Integer Programming,[0],[0]
For binary integer programs with the box constraints 0 ≤,3. Provable Integer Programming,[0],[0]
"xi ≤ 1 for all i, every integral vector x is a vertex of the polytope described by the constraints of the LP relaxation.",3. Provable Integer Programming,[0],[0]
"However fraction vertices may also be in this polytope, and fractional solutions can potentially have an objective value larger than every integral vertex.
",3. Provable Integer Programming,[0],[0]
"If the optimal solution to the linear program happens to be integral, then this is the optimal solution to the original integer linear program.",3. Provable Integer Programming,[0],[0]
"If the optimal solution is fractional, then a variety of techniques are available to tighten the LP relaxation and eliminate the fractional solution.
",3. Provable Integer Programming,[0],[0]
"We establish a success condition for integer programming based on the number of confounding vertices, which to the best of our knowledge was unknown.",3. Provable Integer Programming,[0],[0]
"The algorithm used in proving Theorem 1 is a version of branch-and-bound, a classic technique in integer programming (Land & Doig, 1960) (see (Nemhauser & Wolsey, 1999) for a modern reference on integer programming).",3. Provable Integer Programming,[0],[0]
"This algorithm works by starting with a root node, then branching on a fractional coordinate by making two new linear programs with all the constraints of the parent node, with the constraint xi = 0 added to one new leaf and xi = 1 added to the other.",3. Provable Integer Programming,[0],[0]
The decision on which leaf of the tree to branch on next is based on which leaf has the best objective value.,3. Provable Integer Programming,[0],[0]
"When the best leaf is integral, we know that this is the best integral solution.",3. Provable Integer Programming,[0],[0]
"This algorithm is formally written in Algorithm 1.
",3. Provable Integer Programming,[0],[0]
Theorem 1.,3. Provable Integer Programming,[0],[0]
"Let x∗ be the optimal integral solution and let {v1, v2, . . .",3. Provable Integer Programming,[0],[0]
", vM} be the set of confounding vertices in the LP relaxation.",3. Provable Integer Programming,[0],[0]
"Algorithm 1 will find the optimal integral solution x∗ after 2M calls to an LP solver.
",3. Provable Integer Programming,[0],[0]
"Since MAP inference is a binary integer program regardless of the alphabet size of the variables and order of the clique potentials, we have the following corollary:
Corollary 2.",3. Provable Integer Programming,[0],[0]
"Given a graphical model such that the local polytope has M as cofounding variables, Algorithm 1 can find the optimal MAP configuration with 2M calls to an LP solver.
",3. Provable Integer Programming,[0],[0]
"Cutting-plane methods, which remove a fractional vertex by introducing a new constraint in the polytope may not have this property, since this cut may create new confound-
Algorithm 1 Branch and Bound test Input: an LP {min cTx :",3. Provable Integer Programming,[0],[0]
"Ax ≤ b, 0 ≤ x ≤ 1}
# branch (v, I0, I1) means v is optimal LP # with xI0 = 0 and xI1 = 1.",3. Provable Integer Programming,[0],[0]
"def LP(I0, I1): v∗ ← argmax cTx subject to:",3. Provable Integer Programming,[0],[0]
Ax ≤ b xI0 = 0,3. Provable Integer Programming,[0],[0]
"xI1 = 1
return v∗ if feasible, else return null
v ← LP(∅, ∅)",3. Provable Integer Programming,[0],[0]
"B ← {(v, ∅, ∅)} while optimal integral vertex not found:
(v, I0, I1)←",3. Provable Integer Programming,[0],[0]
"argmax(v,I0,I1)∈B c T v if v is integral: return v else: find a fractional coordinate i v(0)",3. Provable Integer Programming,[0],[0]
"← LP(I0 ∪ {i}, I1) v(1) ← LP(I0, I1 ∪ {i}) remove (v, I0, I1) from B add (v(0), I0 ∪ {i}, I1) to B if feasible add (v(1), I0, I1 ∪ {i}) to B if feasible
ing vertices.",3. Provable Integer Programming,[0],[0]
This branch-and-bound algorithm has the desirable property that it never creates a new fractional vertex.,3. Provable Integer Programming,[0],[0]
"We note that other branching algorithms, such as the algorithm presented by the authors in (Marinescu & Dechter, 2009), do not immediately allow us to prove our desired theorem.
",3. Provable Integer Programming,[0],[0]
Note that warm starting a linear program with slightly modified constraints allows subsequent calls to an LP solver to be much more efficient after the root LP has been solved.,3. Provable Integer Programming,[0],[0]
"The proof follows from the following invariants:
• At every iteration we remove at least one fractional vertex.
•",3.1. Proof of Theorem 1,[0],[0]
"Every integral vertex is in exactly one branch.
",3.1. Proof of Theorem 1,[0],[0]
•,3.1. Proof of Theorem 1,[0],[0]
"Every fractional vertex is in at most one branch.
",3.1. Proof of Theorem 1,[0],[0]
•,3.1. Proof of Theorem 1,[0],[0]
"No fractional vertices are created by the new constraints.
",3.1. Proof of Theorem 1,[0],[0]
"To see the last invariant, note that every vertex of a polytope can be identified by the set of inequality constraints that are satisfied with equality (see (Bertsimas & Tsitsiklis, 1997)).",3.1. Proof of Theorem 1,[0],[0]
"By forcing an inequality constraint to be tight, we cannot possibly introduce new vertices.",3.1. Proof of Theorem 1,[0],[0]
"As mentioned in the introduction, the algorithm used to prove Theorem 1 does not enumerate all the fractional vertices until it finds an integral vertex.",3.2. The M -Best LP Problem,[0],[0]
"Enumerating the M - best vertices of a linear program is theM -best LP problem.
",3.2. The M -Best LP Problem,[0],[0]
Definition.,3.2. The M -Best LP Problem,[0],[0]
"Given a linear program {min cTx : x ∈ P} over a polytope P and a positive integer M , the M -best LP problem is to optimize
max {v1,...,vM}⊆V (P ) M∑",3.2. The M -Best LP Problem,[0],[0]
"k=1 cT vk.
",3.2. The M -Best LP Problem,[0],[0]
"This was established by (Angulo et al., 2014) to be NP-hard when M = O(n).",3.2. The M -Best LP Problem,[0],[0]
"We strengthen this result to hardness of approximation even when M = nε for any ε > 0.
Theorem 3.",3.2. The M -Best LP Problem,[0],[0]
"It is NP-hard to approximate the M -best LP problem by a factor better than O(n ε
M ) for any fixed ε > 0.
",3.2. The M -Best LP Problem,[0],[0]
Proof.,3.2. The M -Best LP Problem,[0],[0]
"Consider the circulation polytope described in (Khachiyan et al., 2008), with the graph and weight vector w described in (Boros et al., 2011).",3.2. The M -Best LP Problem,[0],[0]
"By adding anO(logM) long series of 2×2 bipartite subgraphs, we can make it such that one long path in the original graph implies M long paths in the new graph, and thus it is NP-hard to find any of these long paths in the new graph.",3.2. The M -Best LP Problem,[0],[0]
"By adding the constraint vector wTx ≤ 0, and using the cost function −w, the vertices corresponding to the short paths have value 1/2, the vertices corresponding to the long paths have value O(1/n), and all other vertices have value 0.",3.2. The M -Best LP Problem,[0],[0]
Thus the optimal set has value O(n+ Mn ).,3.2. The M -Best LP Problem,[0],[0]
"However it is NP-hard to find a set of value greater than O(n) in polynomial time, which gives an O( nM ) approximation.",3.2. The M -Best LP Problem,[0],[0]
"Using a padding argument, we can replace n with nε.
",3.2. The M -Best LP Problem,[0],[0]
"The best known algorithm for the M -best LP problem is a generalization of the facet guessing algorithm (Dimakis et al., 2009) developed in (Angulo et al., 2014), which would require O(mM ) calls to an LP solver, where m is the number of constraints of the LP.",3.2. The M -Best LP Problem,[0],[0]
"Since we only care about integral solutions, we can find the single best integral vertex with O(M) calls to an LP solver, and if we want all of the K-best integral solutions among the top M vertices of the polytope, we can find these with O(nK",3.2. The M -Best LP Problem,[0],[0]
"+M) calls to an LP-solver, as we will see in the next section.
3.3.",3.2. The M -Best LP Problem,[0],[0]
"K-Best Integral Solutions
Finding the K-best solutions to general optimization problems has been uses in several machine learning applications.",3.2. The M -Best LP Problem,[0],[0]
Producing multiple high-value outputs can be naturally combined with post-processing algorithms that select the most desired solution using additional sideinformation.,3.2. The M -Best LP Problem,[0],[0]
"There is a significant volume of work in the general area, see (Fromer & Globerson, 2009; Batra et al., 2012) for MAP solutions in graphical models and (Eppstein, 2014) for a survey on M -best problems.
",3.2. The M -Best LP Problem,[0],[0]
We further generalize our theorem to find the K-best integral solutions.,3.2. The M -Best LP Problem,[0],[0]
Theorem 4.,3.2. The M -Best LP Problem,[0],[0]
"Under the assumption that there are less than M fractional vertices with objective value at least as good as the K-best integral solutions, we can find all of the Kbest integral solutions, O(nK",3.2. The M -Best LP Problem,[0],[0]
"+M) calls to an LP solver.
",3.2. The M -Best LP Problem,[0],[0]
The algorithm used in this theorem is Algorithm 2.,3.2. The M -Best LP Problem,[0],[0]
"It combines Algorithm 1 with the space partitioning technique used in (Murty, 1968; Lawler, 1972).",3.2. The M -Best LP Problem,[0],[0]
"If the current optimal solution in the solution tree is fractional, then we use the branching technique in Algorithm 1.",3.2. The M -Best LP Problem,[0],[0]
"If the current optimal solution in the solution tree x∗ is integral, then we branch by creating a new leaf for every i not currently constrained by the parent with the constraint",3.2. The M -Best LP Problem,[0],[0]
xi = ¬x∗i .,3.2. The M -Best LP Problem,[0],[0]
"We now describe the fractional vertices of the local polytope for binary, pairwise graphical models, which is described in Equation 3.",4. Fractional Vertices of the Local Polytope,[0],[0]
"It was shown in (Padberg, 1989) that all the vertices of this polytope are half-integral, that is, all coordinates have a value from {0, 12 , 1} (see (Weller et al., 2016) for a new proof of this).
",4. Fractional Vertices of the Local Polytope,[0],[0]
"Given a half-integral point q ∈ {0, 12 , 1} V ∪E in the local polytope, we say that a cycle C = (VC , EC) ⊆ G is frustrated if there is an odd number of edges ij ∈ EC such that qij = 0.",4. Fractional Vertices of the Local Polytope,[0],[0]
"If a point q has a frustrated cycle, then it is a pseudomarginal, as no probability distribution exists that has as its singleton and pairwise marginals the coordinates of q. Half-integral points q with a frustrated cycle do not satisfy the cycle inequalities (Sontag & Jaakkola, 2007; Wainwright et al., 2008), for all cycles C = (VC , EC), F = (VF , EF ) ⊆ C, |EF",4. Fractional Vertices of the Local Polytope,[0],[0]
| odd we must have∑ ij∈EF qi+qj−2qij− ∑ ij∈EC\EF qi+qj−2qij ≤ |FC |−1.,4. Fractional Vertices of the Local Polytope,[0],[0]
"(4)
Frustrated cycles allow a solution to be zero on negative weights in a way that is not possible for any integral solution.
",4. Fractional Vertices of the Local Polytope,[0],[0]
"We have the following theorem describing all the vertices of the local polytope for binary, pairwise graphical models.
",4. Fractional Vertices of the Local Polytope,[0],[0]
Algorithm 2 M -best Integral Input: an LP {max cTx :,4. Fractional Vertices of the Local Polytope,[0],[0]
"Ax ≤ b, 0 ≤ x ≤ 1} Input: number of solutions K
def LP(I0, I1): same as Algorithm 1
def SplitIntegral(v, I0, I1): P ← { } for i ∈",4. Fractional Vertices of the Local Polytope,[0],[0]
[n] if i /∈,4. Fractional Vertices of the Local Polytope,[0],[0]
"I0 ∪ I1: a← ¬vi I ′0, I ′",4. Fractional Vertices of the Local Polytope,[0],[0]
"1 ← copy(I0, I1)
add i to I ′a v′",4. Fractional Vertices of the Local Polytope,[0],[0]
"← LP(I ′0, I ′1) add (v′, I ′0, I ′ a) to P if feasible
return P
v ← LP(∅, ∅)",4. Fractional Vertices of the Local Polytope,[0],[0]
"B ← {(v, ∅, ∅)} results← { } while K integral vertices not found: (v, I0, I1)←",4. Fractional Vertices of the Local Polytope,[0],[0]
"argmax(v,I0,I1)∈B c
T v if v is integral:
add v to results add SplitIntegeral(v, I0, I1) to B remove (v, I0, I1) from B
else: find a fractional coordinate i v(0)",4. Fractional Vertices of the Local Polytope,[0],[0]
"← LP(I0 ∪ {i}, I1) v(1) ← LP(I0, I1 ∪ {i}) remove (v, I0, I1) from B add (v(0), I0 ∪ {i}, I1) to B if feasible add (v(1), I0, I1 ∪ {i}) to B if feasible
return results
Theorem 5.",4. Fractional Vertices of the Local Polytope,[0],[0]
"Given a point q in the local polytope, q is a vertex of this polytope if and only if q ∈ {0, 12 , 1}
V ∪E and the induced subgraph on the fractional nodes of q is such that every connected component of this subgraph contains a frustrated cycle.",4. Fractional Vertices of the Local Polytope,[0],[0]
"Every vertex q of an n-dimensional polytope is such that there are n constraints such that q satisfies them with equality, known as active constraints (see (Bertsimas & Tsitsiklis, 1997)).",4.1. Proof of Theorem 5,[0],[0]
Every integral q is thus a vertex of the local polytope.,4.1. Proof of Theorem 5,[0],[0]
"We now describe the fractional vertices of the local polytope.
Definition.",4.1. Proof of Theorem 5,[0],[0]
"Let q ∈ {0, 12 , 1} n+m be a point of the local polytope.",4.1. Proof of Theorem 5,[0],[0]
"Let GF = (VF , EF ) be an induced subgraph of points such that qi = 12 for all i ∈ VF .",4.1. Proof of Theorem 5,[0],[0]
"We say that GF is
full rank if the following system of equations is full rank.
",4.1. Proof of Theorem 5,[0],[0]
qi + qj,4.1. Proof of Theorem 5,[0],[0]
− qij = 1 ∀ij ∈ EF such that qij = 0 qij = 0,4.1. Proof of Theorem 5,[0],[0]
"∀ij ∈ EF such that qij = 0
qi",4.1. Proof of Theorem 5,[0],[0]
"− qij = 0 ∀ij ∈ EF such that qij = 1
2
qj",4.1. Proof of Theorem 5,[0],[0]
"− qij = 0 ∀ij ∈ EF such that qij = 1
2
(5)
Theorem 5 follows from the following lemmas.
",4.1. Proof of Theorem 5,[0],[0]
Lemma 6.,4.1. Proof of Theorem 5,[0],[0]
"Let q ∈ {0, 12 , 1} n+m be a point of the local polytope.",4.1. Proof of Theorem 5,[0],[0]
"Let GF = (VF , EF ) be the subgraph induced by the nodes i ∈ V such that qi = 12 .",4.1. Proof of Theorem 5,[0],[0]
"The point q is a vertex if and only if every connected component of GF is full rank.
",4.1. Proof of Theorem 5,[0],[0]
Lemma 7.,4.1. Proof of Theorem 5,[0],[0]
"Let q ∈ {0, 12 , 1} n+m be a point of the local polytope.",4.1. Proof of Theorem 5,[0],[0]
"Let GF = (VF , EF ) be a connected subgraph induced from nodes such that such that qi = 12 for all i ∈ VF .",4.1. Proof of Theorem 5,[0],[0]
"GF is full rank if and only if GF contains cycle that is full rank.
",4.1. Proof of Theorem 5,[0],[0]
Lemma 8.,4.1. Proof of Theorem 5,[0],[0]
"Let q ∈ {0, 12 , 1} n+m be a point of the local polytope.",4.1. Proof of Theorem 5,[0],[0]
"Let C = (VC , EC) be a cycle of G such that qi is fractional for all i ∈ VC .",4.1. Proof of Theorem 5,[0],[0]
"C is full rank if and only if C is a frustrated cycle.
",4.1. Proof of Theorem 5,[0],[0]
Proof of Lemma 6.,4.1. Proof of Theorem 5,[0],[0]
Suppose every connected component is full rank.,4.1. Proof of Theorem 5,[0],[0]
Then every fractional node and edge between fractional nodes is fully specified by their corresponding equations in Problem 3.,4.1. Proof of Theorem 5,[0],[0]
"It is easy to check that all integral nodes, edges between integral nodes, and edges between integral and fractional nodes is also fixed.",4.1. Proof of Theorem 5,[0],[0]
"Thus q is a vertex.
",4.1. Proof of Theorem 5,[0],[0]
Now suppose that there exists a connected component that is not full rank.,4.1. Proof of Theorem 5,[0],[0]
The only other constraints involving this connected component are those between fractional nodes and integral nodes.,4.1. Proof of Theorem 5,[0],[0]
"However, note that these constraints are always rank 1, and also introduce a new edge variable.",4.1. Proof of Theorem 5,[0],[0]
"Thus all the constraints where q is tight do not make a full rank system of equations.
",4.1. Proof of Theorem 5,[0],[0]
Proof of Lemma 7.,4.1. Proof of Theorem 5,[0],[0]
Suppose GF has a full rank cycle.,4.1. Proof of Theorem 5,[0],[0]
We will build the graph starting with the full rank cycle then adding one connected edge at a time.,4.1. Proof of Theorem 5,[0],[0]
"It is easy to see from Equations 5 that all new variables introduced to the system of equations have a fixed value, and thus the whole connected component is full rank.
",4.1. Proof of Theorem 5,[0],[0]
Now suppose GF has no full rank cycle.,4.1. Proof of Theorem 5,[0],[0]
We will again build the graph starting from the cycle then adding one connected edge at a time.,4.1. Proof of Theorem 5,[0],[0]
"If we add an edge that connects to a new node, then we added two variables and two equations, thus we did not make the graph full rank.",4.1. Proof of Theorem 5,[0],[0]
"If we add an edge between two existing nodes, then we have a cycle involving this edge.",4.1. Proof of Theorem 5,[0],[0]
"We introduce two new equations, however with
one of the equations and the other cycle equations, we can produce the other equation, thus we can increase the rank by one but we also introduced a new edge.",4.1. Proof of Theorem 5,[0],[0]
"Thus the whole graph cannot be full rank.
",4.1. Proof of Theorem 5,[0],[0]
"The proof of Lemma 8 from the following lemma.
",4.1. Proof of Theorem 5,[0],[0]
Lemma 9.,4.1. Proof of Theorem 5,[0],[0]
"Consider a collection of n vectors
v1 = (1, t1, 0, . . .",4.1. Proof of Theorem 5,[0],[0]
", 0)
",4.1. Proof of Theorem 5,[0],[0]
"v2 = (0, 1, t2, 0, . . .",4.1. Proof of Theorem 5,[0],[0]
", 0)
",4.1. Proof of Theorem 5,[0],[0]
"v3 = (0, 0, 1, t3, 0, . . .",4.1. Proof of Theorem 5,[0],[0]
", 0)
...
",4.1. Proof of Theorem 5,[0],[0]
"vn−1 = (0, . . .",4.1. Proof of Theorem 5,[0],[0]
", 0, 1, tn−1)
vn = (tn, 0, . . .",4.1. Proof of Theorem 5,[0],[0]
", 0, 1)
",4.1. Proof of Theorem 5,[0],[0]
"for ti ∈ {−1, 1}.",4.1. Proof of Theorem 5,[0],[0]
"We have rank(v1, v2, . . .",4.1. Proof of Theorem 5,[0],[0]
", vn) = n",4.1. Proof of Theorem 5,[0],[0]
"if and only if there is an odd number of vectors such that ti = 1.
",4.1. Proof of Theorem 5,[0],[0]
Proof of Lemma 9.,4.1. Proof of Theorem 5,[0],[0]
Let k be the number of vectors such that ti = 1.,4.1. Proof of Theorem 5,[0],[0]
"Let S1 = v1 and define
Si+1 = { Si − vi+1",4.1. Proof of Theorem 5,[0],[0]
"if Si(i+ 1) = 1 Si + vi+1 if Si(i+ 1) = −1
for i = 2, . .",4.1. Proof of Theorem 5,[0],[0]
.,4.1. Proof of Theorem 5,[0],[0]
", n− 1.
Note that if ti+1 = −1 then Si+1(i+2) = Si(i+1) and if ti+1 = 1 then Si+1(i+2) = −Si(i+1).",4.1. Proof of Theorem 5,[0],[0]
"Thus the number of times the sign changes is exactly the number of ti = 1 for i ∈ {2, . . .",4.1. Proof of Theorem 5,[0],[0]
", n− 1}.
",4.1. Proof of Theorem 5,[0],[0]
"Using the value of Sn−1 we can now we can check for all values of t1 and tn that the following is true.
",4.1. Proof of Theorem 5,[0],[0]
"• If k is odd then (1, 0, . . .",4.1. Proof of Theorem 5,[0],[0]
", 0) ∈ span(v1, v2, . . .",4.1. Proof of Theorem 5,[0],[0]
", vn), which allows us to create the entire standard basis, showing the vectors are full rank.
",4.1. Proof of Theorem 5,[0],[0]
"• If k is even then vn ∈ span(v1, v2, . . .",4.1. Proof of Theorem 5,[0],[0]
", vn−1) and thus the vectors are not full rank.",4.1. Proof of Theorem 5,[0],[0]
For this section we generalize generalize Theorem 1.,5. Estimating the number of Confounding Singleton Marginals,[0],[0]
We see after every iteration we potentially remove more than one confounding vertex—we remove all confounding vertices that agree with xI0 = 0 and xI1,5. Estimating the number of Confounding Singleton Marginals,[0],[0]
= 1 and are fractional with any value at coordinate i.,5. Estimating the number of Confounding Singleton Marginals,[0],[0]
"We also observe that we can
handle a mixed integer program (MIP) with the same algorithm.
",5. Estimating the number of Confounding Singleton Marginals,[0],[0]
"max x
cTx+ dT",5. Estimating the number of Confounding Singleton Marginals,[0],[0]
"z
subject to Ax+Bz ≤ b x ∈ {0, 1}n
",5. Estimating the number of Confounding Singleton Marginals,[0],[0]
"We will call a vertex (x, z) fractional if its x component is fractional.",5. Estimating the number of Confounding Singleton Marginals,[0],[0]
"For each fractional vertex (x, z), we create a half-integral vector S(x) such that
S(x)i =  0",5. Estimating the number of Confounding Singleton Marginals,[0],[0]
"if xi = 0 1 2 if xi is fractional 1 if xi = 1
For a set of vertices V , we define S(V ) to be the set {S(x) : (x, z) ∈ V }, i.e. we remove all duplicate entries.",5. Estimating the number of Confounding Singleton Marginals,[0],[0]
Theorem 10.,5. Estimating the number of Confounding Singleton Marginals,[0],[0]
"Let (x∗, z∗) be the optimal integral solution and let VC be the set of confounding vertices.",5. Estimating the number of Confounding Singleton Marginals,[0],[0]
"Algorithm 1 will find the optimal integral solution (x∗, z∗) after 2|S(VC)| calls to an LP solver.
",5. Estimating the number of Confounding Singleton Marginals,[0],[0]
"For MAP inference in graphical models, S(VC) refers to the fractional singleton marginals qV such that there exists a set of pairwise pseudomarginals qE such that (qV , qE) is a cofounding vertex.",5. Estimating the number of Confounding Singleton Marginals,[0],[0]
In this case we call qV a confounding singleton marginal.,5. Estimating the number of Confounding Singleton Marginals,[0],[0]
We develop Algorithm 3 to estimate the number of confounding singleton marginals for our experiments section.,5. Estimating the number of Confounding Singleton Marginals,[0],[0]
"It is based on the k-best enumeration method developed in (Murty, 1968; Lawler, 1972).
",5. Estimating the number of Confounding Singleton Marginals,[0],[0]
Algorithm 3 works by a branching argument.,5. Estimating the number of Confounding Singleton Marginals,[0],[0]
The root node is the original LP.,5. Estimating the number of Confounding Singleton Marginals,[0],[0]
"A leaf node is branched on by introducing a new leaf for every node in V and every element of {0, 12 , 1} such that qi 6=",5. Estimating the number of Confounding Singleton Marginals,[0],[0]
a in the parent node and the constraint {qi = a} is not in the constraints for the parent node.,5. Estimating the number of Confounding Singleton Marginals,[0],[0]
"For i ∈ V , a ∈ {0, 12 , 1}, we create the leaf such that it has all the constraints of its parents plus the constraint qi = a.
Note that Algorithm 3 actually generates a superset of the elements of S(VC), since the introduction of constraints of the type qi = 12 introduce vertices into the new polytope that were not in the original polytope.",5. Estimating the number of Confounding Singleton Marginals,[0],[0]
"This does not seem to be an issue for the experiments we consider, however this does occur for other graphs.",5. Estimating the number of Confounding Singleton Marginals,[0],[0]
An interesting question is if the vertices of the local polytope can be provably enumerated.,5. Estimating the number of Confounding Singleton Marginals,[0],[0]
"We consider a synthetic experiment on randomly created graphical models, which were also used in (Sontag & Jaakkola, 2007; Weller, 2016; Weller et al., 2014).",6. Experiments,[0],[0]
The graph topology used is the complete graph on 12 nodes.,6. Experiments,[0],[0]
"We first reparametrize the model to use the sufficient statistics
Algorithm 3 Estimate S(VC) for Binary, Pairwise Graphical Models
Input: a binary, pairwise graphical model LP
# branch (v, I0, I 1 2 , I1) means v is optimal LP # with xI0 = 0, xI 1 2 = 12 , and xI1 = 1.",6. Experiments,[0],[0]
"def LP(I0, I 1 2 , I1):
optimize LP with additional constraints: xI0 = 0 xI 1
2 = 12 xI1 = 1
return q∗ if feasible, else return null
q ← LP(∅, ∅, ∅)",6. Experiments,[0],[0]
"B ← {(q, ∅, ∅, ∅)}",6. Experiments,[0],[0]
"solution← { } while optimal integral vertex not found: (q, I0, I 1
2 , I1)←",6. Experiments,[0],[0]
"argmax(q,I0,I 1 2 ,I1)∈B objective val
add q to solution remove (q, I0, I 1
2 , I1) from B
for i ∈ V if i /∈",6. Experiments,[0],[0]
"I0 ∪ I 1 2 ∪ I1:
for a ∈ {0, 12 , 1} if qi 6=",6. Experiments,[0],[0]
"a: I ′0, I
′ 1 2 , I ′1 ← copy(I0, I 12 , I1)",6. Experiments,[0],[0]
I ′a ←,6. Experiments,[0],[0]
I ′a ∪ {i} q′,6. Experiments,[0],[0]
"← LP(I ′0, I ′1
2
, I ′1)
add (q′, I ′0, I ′ 1 2 , I ′1) to B if feasible return solution
1(xi = xj) and 1(xi = 1).",6. Experiments,[0],[0]
"The node weights are drawn θi ∼ Uniform(−1, 1) and the edge weights are drawn Wij ∼ Uniform(−w,w) for varying w.",6. Experiments,[0],[0]
The quantity w determines how strong the connections are between nodes.,6. Experiments,[0],[0]
"We do 100 draws for each choice of edge strength w.
For the complete graph, we observe that Algorithm 3 does not yield any points that do not correspond to vertices, however this does occur for other topologies.
",6. Experiments,[0],[0]
We first compare how the number of fractional singleton marginals |S(VC)| changes with the connection strengthw.,6. Experiments,[0],[0]
"In Figure 1, we plot the sample CDF of the probability that |S(VC)| is some given value.",6. Experiments,[0],[0]
We observe that |S(VC)| increases as the connection strength increases.,6. Experiments,[0],[0]
"Further we see that while most instances have a small number for |S(VC)|, there are rare instances where |S(VC)| is quite large.
",6. Experiments,[0],[0]
Now we compare how the number of cycle constraints from Equation (4) that need to be introduced to find the best integral solution changes with the number of confounding singleton marginals in Figure 2.,6. Experiments,[0],[0]
"We use the algorithm for finding the most frustrated cycle in (Sontag & Jaakkola, 2007) to introduce new constraints.",6. Experiments,[0],[0]
"We observe that each constraint seems to remove many confounding singleton
marginals.
",6. Experiments,[0],[0]
"We also observe the number of introduced confounding singleton marginals that are introduced by the cycle constraints increases with the number of confounding singleton marginals in Figure 3.
",6. Experiments,[0],[0]
Finally we compare the number of branches needed to find the optimal solution increases with the number of confounding singleton marginals in Figure 4.,6. Experiments,[0],[0]
A similar trend arises as with the number of cycle inequalities introduced.,6. Experiments,[0],[0]
"To compare the methods, note that branch-and-bound uses twice as many LP calls as there are branches.",6. Experiments,[0],[0]
"For this family of graphical models, branch-and-bound tends to require less calls to an LP solver than the cut constraints.",6. Experiments,[0],[0]
"Perhaps the most interesting follow-up question to our work is to determine when, in theory and practice, our condition on the number of confounding pseudomarginals in the LP relaxation is small.",7. Conclusion,[0],[0]
Another interesting question is to see if it is possible to prune the number of confounding pseudomarginals at a faster rate.,7. Conclusion,[0],[0]
The algorithm presented for our main theorem removes one pseudomarginal after two calls to an LP solver.,7. Conclusion,[0],[0]
Is it possible to do this at a faster rate?,7. Conclusion,[0],[0]
"From our experiments, this seems to be the case in practice.",7. Conclusion,[0],[0]
"This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No. DGE-1110007 as well as NSF Grants CCF 1344364, 1407278, 1422549, 1618689, 1018829 and ARO YIP W911NF-14-1-0258.",Acknowledgements,[0],[0]
"Given a graphical model, one essential problem is MAP inference, that is, finding the most likely configuration of states according to the model.",abstractText,[0],[0]
"Although this problem is NP-hard, large instances can be solved in practice and it is a major open question is to explain why this is true.",abstractText,[0],[0]
We give a natural condition under which we can provably perform MAP inference in polynomial time—we require that the number of fractional vertices in the LP relaxation exceeding the optimal solution is bounded by a polynomial in the problem size.,abstractText,[0],[0]
"This resolves an open question by Dimakis, Gohari, and Wainwright.",abstractText,[0],[0]
"In contrast, for general LP relaxations of integer programs, known techniques can only handle a constant number of fractional vertices whose value exceeds the optimal solution.",abstractText,[0],[0]
We experimentally verify this condition and demonstrate how efficient various integer programming methods are at removing fractional solutions.,abstractText,[0],[0]
Exact MAP Inference by Avoiding Fractional Vertices,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 694–699 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
694
Many corpora span broad periods of time. Language processing models trained during one time period may not work well in future time periods, and the best model may depend on specific times of year (e.g., people might describe hotels differently in reviews during the winter versus the summer). This study investigates how document classifiers trained on documents from certain time intervals perform on documents from other time intervals, considering both seasonal intervals (intervals that repeat across years, e.g., winter) and non-seasonal intervals (e.g., specific years). We show experimentally that classification performance varies over time, and that performance can be improved by using a standard domain adaptation approach to adjust for changes in time.",text,[0],[0]
"Language, and therefore data derived from language, changes over time (Ullmann, 1962).",1 Introduction,[0],[0]
"Word senses can shift over long periods of time (Wilkins, 1993; Wijaya and Yeniterzi, 2011; Hamilton et al., 2016), and written language can change rapidly in online platforms (Eisenstein et al., 2014; Goel et al., 2016).",1 Introduction,[0],[0]
"However, little is known about how shifts in text over time affect the performance of language processing systems.
",1 Introduction,[0],[0]
"This paper focuses on a standard text processing task, document classification, to provide insight into how classification performance varies with time.",1 Introduction,[0],[0]
We consider both long-term variations in text over time and seasonal variations which change throughout a year but repeat across years.,1 Introduction,[0],[0]
"Our empirical study considers corpora contain-
ing formal text spanning decades as well as usergenerated content spanning only a few years.
",1 Introduction,[0],[0]
"After describing the datasets and experiment design, this paper has two main sections, respectively addressing the following research questions:
1.",1 Introduction,[0],[0]
"In what ways does document classification depend on the timestamps of the documents?
2.",1 Introduction,[0],[0]
"Can document classifiers be adapted to perform better in time-varying corpora?
",1 Introduction,[0],[0]
"To address question 1, we train and test on data from different time periods, to understand how performance varies with time.",1 Introduction,[0],[0]
"To address question 2, we apply a domain adaptation approach, treating time intervals as domains.",1 Introduction,[0],[0]
"We show that in most cases this approach can lead to improvements in classification performance, even on future time intervals.",1 Introduction,[0],[0]
"Time is implicitly embedded in the classification process: classifiers are often built to be applied to future data that doesn’t yet exist, and performance on held-out data is measured to estimate performance on future data whose distribution may have changed.",1.1 Related Work,[0],[0]
"Methods exist to adjust for changes in the data distribution (covariate shift) (Shimodaira, 2000; Bickel et al., 2009), but time is not typically incorporated into such methods explicitly.
",1.1 Related Work,[0],[0]
"One line of work that explicitly studies the relationship between time and the distribution of data is work on classifying the time period in which a document was written (document dating) (Kanhabua and Nørvåg, 2008; Chambers, 2012; Kotsakos et al., 2014).",1.1 Related Work,[0],[0]
"However, this task is directed differently from our work: predicting timestamps given documents, rather than predicting information about documents given timestamps.",1.1 Related Work,[0],[0]
"Our study experiments with six corpora:
• Reviews: Three corpora containing reviews labeled with sentiment: music reviews from Amazon (He and McAuley, 2016), and hotel reviews and restaurant reviews from Yelp.1 We discarded reviews that had fewer than 10 tokens or a helpfulness/usefulness score of zero.",2 Datasets and Experimental Setup,[0],[0]
"The reviews with neutral scores were removed.
",2 Datasets and Experimental Setup,[0],[0]
"• Politics: Sentences from the American party platforms of Republicans and Democrats from 1948 to 2016, available every four years.2
• News: Newspaper articles from 1950-2014, labeled with whether the article is relevant to the US economy.3
• Twitter: Tweets labeled with whether they indicate that the user received an influenza vaccination (i.e., a flu shot) (Huang et al., 2017).
",2 Datasets and Experimental Setup,[0],[0]
Our experiments require documents to be grouped into time intervals.,2 Datasets and Experimental Setup,[0],[0]
Table 1 shows the intervals for each corpus.,2 Datasets and Experimental Setup,[0],[0]
Documents that fall outside of these time intervals were removed.,2 Datasets and Experimental Setup,[0],[0]
"We grouped documents into two types of intervals:
• Seasonal: Time intervals within a year (e.g., January through March) that may be repeated across years.
",2 Datasets and Experimental Setup,[0],[0]
"• Non-seasonal: Time intervals that do not repeat (e.g., 1997-1999).
",2 Datasets and Experimental Setup,[0],[0]
"For each dataset, we performed binary classification, implemented in sklearn (Pedregosa et al., 2011).",2 Datasets and Experimental Setup,[0],[0]
"We built logistic regression classifiers with TF-IDF weighted n-gram features (n ∈ {1, 2, 3}), removing features that appeared in less than 2 documents.",2 Datasets and Experimental Setup,[0],[0]
"Except when otherwise specified, we held out a random 10% of documents as
1https://www.yelp.com/dataset 2https://www.comparativeagendas.net/
datasets_codebooks 3https://www.crowdflower.com/ data-for-everyone/
validation data for each dataset.",2 Datasets and Experimental Setup,[0],[0]
"We used Elastic Net (combined `1 and `2) regularization (Zou and Hastie, 2005), and tuned the regularization parameters to maximize performance on the validation data.",2 Datasets and Experimental Setup,[0],[0]
We evaluated the performance using weighted F1 scores.,2 Datasets and Experimental Setup,[0],[0]
We first conduct an analysis of how classifier performance depends on the time intervals in which it is trained and applied.,3 How Does Classification Performance Vary with Time?,[0],[0]
"For each corpus, we train the classifier on each time interval and test on each time interval.",3 How Does Classification Performance Vary with Time?,[0],[0]
"We downsampled the training data within each time interval to match the number of documents in the smallest interval, so that differences in performance are not due to the size of the training data.
",3 How Does Classification Performance Vary with Time?,[0],[0]
"In all experiments, we train a classifier on a partition of 80% of the documents in the time interval, and repeat this five times on different partitions, averaging the five F1 scores to produce the final estimate.",3 How Does Classification Performance Vary with Time?,[0],[0]
"When training and testing on the same interval, we test on the held-out 20% of documents in that interval (standard cross-validation).",3 How Does Classification Performance Vary with Time?,[0],[0]
"When testing on different time intervals, we test on all documents, since they are all held-out from the training interval; however, we still train on five subsets of 80% of documents, so that the training data is identical across all experiments.
",3 How Does Classification Performance Vary with Time?,[0],[0]
"Finally, to understand why performance varies, we also qualitatively examined how the distribution of content changes across time intervals.",3 How Does Classification Performance Vary with Time?,[0],[0]
"To measure the distribution of content, we trained a topic model with 20 topics using gensim (Řehůřek and Sojka, 2010) with default parameters.",3 How Does Classification Performance Vary with Time?,[0],[0]
"We associated each document with one topic (the most probable topic in the document), and then calculated the proportion of each topic within a time period as the proportion of documents in that time period assigned to that topic.",3 How Does Classification Performance Vary with Time?,[0],[0]
"We can then visualize the extent to which the distribution of 20 topics varies by time.
",3 How Does Classification Performance Vary with Time?,[0],[0]
Jan-M ar Apr-Ju n,3 How Does Classification Performance Vary with Time?,[0],[0]
"Jul-Se p Oct-D ec
Train
JanMar",3 How Does Classification Performance Vary with Time?,[0],[0]
Apr -Jun,3 How Does Classification Performance Vary with Time?,[0],[0]
"Jul-S ep
Oct -De
c
Te st
0.948 0.912 0.913 0.910
0.916 0.949 0.914 0.909
0.916 0.912 0.952 0.910
0.916 0.914 0.918 0.945
Reviews data - music
Jan-M ar Apr-Ju n",3 How Does Classification Performance Vary with Time?,[0],[0]
"Jul-Se p Oct-D ec
Train
JanMar",3 How Does Classification Performance Vary with Time?,[0],[0]
Apr -Jun,3 How Does Classification Performance Vary with Time?,[0],[0]
"Jul-S ep
Oct -De
c
Te st
0.865 0.862 0.862 0.861
0.863 0.862 0.861 0.858
0.862 0.859 0.866 0.861
0.863 0.863 0.863 0.858
Reviews data - hotels
Jan-M ar Apr-Ju n",3 How Does Classification Performance Vary with Time?,[0],[0]
"Jul-Se p Oct-D ec
Train
JanMar",3 How Does Classification Performance Vary with Time?,[0],[0]
Apr -Jun,3 How Does Classification Performance Vary with Time?,[0],[0]
"Jul-S ep
Oct -De
c
Te st
0.898 0.806 0.750 0.769
0.795 0.876 0.745 0.787
0.794 0.795 0.900 0.767
0.791 0.790 0.731 0.891
News data - economy
Jan-M ar Apr-Ju n",3 How Does Classification Performance Vary with Time?,[0],[0]
"Jul-Se p Oct-D ec
Train
JanMar",3 How Does Classification Performance Vary with Time?,[0],[0]
Apr -Jun,3 How Does Classification Performance Vary with Time?,[0],[0]
"Jul-S ep
Oct -De
c
Te st
0.896 0.894 0.891 0.856
0.808 0.940 0.853 0.829
0.836 0.904 0.917 0.845
0.849 0.891 0.884 0.902
Twitter data - vaccine
2006 -08 2009 -11 2012 -14 2015 -17
Train
200 6-08 200 9-11 201 2-14 201 5-17 Te st
0.823 0.828 0.825 0.859
0.799 0.843 0.830 0.858
0.800 0.819 0.833 0.869
0.790 0.813 0.835 0.880
Reviews data - hotels
2006 -08 2009 -11 2012 -14 2015 -17
Train
200 6-08 200 9-11 201 2-14 201 5-17 Te st
0.829 0.838 0.869 0.883
0.814 0.856 0.870 0.883
0.815 0.842 0.884 0.894
0.814 0.839 0.875 0.902
Reviews data - restaurants
1948 -56 1960 -68 1972 -80 1984 -92",3 How Does Classification Performance Vary with Time?,[0],[0]
"1996 -20042008 -16
",3 How Does Classification Performance Vary with Time?,[0],[0]
"Train
194 8-56 196 0-68 197 2-80 198 4-92
199 6-20
04 200 8-16
Te st
0.659 0.567 0.518 0.544 0.525 0.532 0.551 0.800 0.529 0.477 0.474 0.495 0.545 0.506 0.678 0.635 0.573 0.523 0.515 0.473 0.565 0.866 0.594 0.569 0.435 0.404 0.490 0.618 0.848 0.684 0.435 0.416 0.480 0.606 0.674 0.819
Politics - US political data
1985 -89 1990 -94 1995 -99 2000 -04 2005 -09 2010 -14
",3 How Does Classification Performance Vary with Time?,[0],[0]
"Train
198 5-89 199 0-94 199 5-99 200 0-04 200 5-09 201 0-14",3 How Does Classification Performance Vary with Time?,[0],[0]
"Te st
0.876 0.758 0.783 0.794 0.777 0.756 0.764 0.883 0.771 0.802 0.789 0.748 0.759 0.760 0.905 0.798 0.806 0.763 0.760 0.756 0.770 0.926 0.805 0.771 0.773 0.767 0.783 0.826 0.900 0.778 0.773 0.750 0.778 0.810 0.786 0.897
News data - economy
Figure 1: Document classification performance when training and testing on different times of year (top) and different years (bottom).",3 How Does Classification Performance Vary with Time?,[0],[0]
Some corpora are omitted for space.,3 How Does Classification Performance Vary with Time?,[0],[0]
The top row of Figure 1 shows the test scores from training and testing on each pair of seasonal time intervals for four of the datasets.,3.1 Seasonal Variability,[0],[0]
"We observe very strong seasonal variations in the economic news corpus, with a drop in F1 score on the order of 10 when there is a mismatch in the season between training and testing.",3.1 Seasonal Variability,[0],[0]
"There is a similar, but weaker, effect on performance in the music reviews from Amazon and the vaccine tweets.",3.1 Seasonal Variability,[0],[0]
"There was virtually no difference in performance in any of the pairs in both review corpora from Yelp (restaurants, not pictured, and hotels).
",3.1 Seasonal Variability,[0],[0]
"To help understand why the performance varies, Figure 2 (left) shows the distribution of topics in each seasonal interval for two corpora: Amazon music reviews and Twitter.",3.1 Seasonal Variability,[0],[0]
"We observe very little variation in the topic distribution across seasons in the Amazon corpus, but some variation in the Twitter corpus, which may explain the large performance differences when testing on held-out seasons in the Twitter data as compared to the Amazon corpus.
",3.1 Seasonal Variability,[0],[0]
"For space, we do not show the descriptions of the topics, but instead only the shape of the distributions to show the degree of variability.",3.1 Seasonal Variability,[0],[0]
"We did qualitatively examine the differences in word features across the time periods, but had difficulty interpreting the observations and were unable to draw clear conclusions.",3.1 Seasonal Variability,[0],[0]
"Thus, characterizing the ways in which content distributions vary over time, and why this affects performance, is still an open question.",3.1 Seasonal Variability,[0],[0]
The bottom row of Figure 1 shows the test scores from training and testing on each pair of nonseasonal time intervals.,3.2 Non-seasonal Variability,[0],[0]
A strong pattern emerges in the political parties corpus: F1 scores can drop by as much as 40 points when testing on different time intervals.,3.2 Non-seasonal Variability,[0],[0]
"This is perhaps unsurprising, as this collection spans decades, and US party positions have substantially changed over time.",3.2 Non-seasonal Variability,[0],[0]
"The performance declines more when testing on time intervals that are further away in time from the training interval, suggesting that changes in party platforms shift gradually over time.",3.2 Non-seasonal Variability,[0],[0]
"In contrast, while there was a performance drop when testing outside the training interval in the economic news corpus, the drop was not gradual.",3.2 Non-seasonal Variability,[0],[0]
"In the Twitter dataset (not pictured), F1 dropped by an average of 4.9 points outside the training interval.
",3.2 Non-seasonal Variability,[0],[0]
"We observe an intriguing non-seasonal pattern that is consistent in both of the review corpora from Yelp, but not in the music review corpus from Amazon (not pictured), which is that the classification performance fairly consistently increases over time.",3.2 Non-seasonal Variability,[0],[0]
"Since we sampled the dataset so that the time intervals have the same number of reviews, this suggests something else changed over time about the way reviews are written that makes the sentiment easier to detect.
",3.2 Non-seasonal Variability,[0],[0]
The right side of Figure 2 shows the topic distribution in the Amazon and Twitter datasets across non-seasonal intervals.,3.2 Non-seasonal Variability,[0],[0]
We observe higher levels of variability across time in the non-seasonal intervals as compared to the seasonal intervals.,3.2 Non-seasonal Variability,[0],[0]
"Overall, it is clear that classifiers generally perform best when applied to the same time interval they were trained.",3.3 Discussion,[0],[0]
"Performance diminishes when applied to different time intervals, although different corpora exhibit differ patterns in the way in which the performance diminishes.",3.3 Discussion,[0],[0]
This kind of analysis can be applied to any corpus and could provide insights into characteristics of the corpus that may be helpful when designing a classifier.,3.3 Discussion,[0],[0]
We now consider how to improve classifiers when working with datasets that span different time intervals.,4 Making Classification Robust to Temporality,[0],[0]
We propose to treat this as a domain adaptation problem.,4 Making Classification Robust to Temporality,[0],[0]
"In domain adaptation, any partition of data that is expected to have a different distribution of features can be treated as a domain (Joshi et al., 2013).",4 Making Classification Robust to Temporality,[0],[0]
"Traditionally, domain adaptation is used to adapt models to a common task across rather different sets of data, e.g., a sentiment classifier for different types of products (Blitzer et al., 2007).",4 Making Classification Robust to Temporality,[0],[0]
"Recent work has also applied domain adaptation to adjust for potentially more subtle differences in data, such as adapting for differences in the demographics of authors (Volkova et al., 2013; Lynn et al., 2017).",4 Making Classification Robust to Temporality,[0],[0]
"We follow the same approach, treating time intervals as domains.
",4 Making Classification Robust to Temporality,[0],[0]
"In our experiments, we use the feature augmentation approach of Daumé III (2007) to perform domain adaptation.",4 Making Classification Robust to Temporality,[0],[0]
"Each feature is duplicated to have a specific version of the feature for every domain, as well as a domain-independent version of the feature.",4 Making Classification Robust to Temporality,[0],[0]
"In each instance, the domainindependent feature and the domain-specific feature for that instance’s domain have the same feature value, while the value is zeroed out for the domain-specific features for the other domains.
",4 Making Classification Robust to Temporality,[0],[0]
"This is equivalent to a model where the feature weights are domain specific but share a Gaussian prior across domains (Finkel and Manning, 2009).",4 Making Classification Robust to Temporality,[0],[0]
"This approach is widely used due to its simplicity, and derivatives of this approach have been used in similar work (e.g., (Lynn et al., 2017)).",4 Making Classification Robust to Temporality,[0],[0]
"Following Finkel and Manning (2009), we separately adjust the regularization strength for the domain-independent feature weights and the domain-specific feature weights.",4 Making Classification Robust to Temporality,[0],[0]
"We first examine classification performance on the datasets when grouping the seasonal time intervals (January-March, April-June, July-August, September-December) as domains and applying the feature augmentation approach for domain adaptation.",4.1 Seasonal Adaptation,[0],[0]
"As a baseline comparison, we apply the same classifier, but without domain adaptation.
",4.1 Seasonal Adaptation,[0],[0]
Results are shown in Table 2.,4.1 Seasonal Adaptation,[0],[0]
"We see that applying domain adaptation provides a small boost in three of the datasets, and has no effect on two of the datasets.",4.1 Seasonal Adaptation,[0],[0]
"If this pattern holds in other corpora, then this suggests that it does not hurt performance to apply domain adaptation across different times of year, and in some cases can lead to a small performance boost.",4.1 Seasonal Adaptation,[0],[0]
We now consider the non-seasonal time intervals (spans of years).,4.2 Non-seasonal Adaptation,[0],[0]
"In particular, we consider the scenario when one wants to apply a classifier trained on older data to future data.",4.2 Non-seasonal Adaptation,[0],[0]
"This requires a modification to the domain adaptation approach, because future data includes domains that did not exist in the training data, and thus we cannot learn domain-specific feature weights.",4.2 Non-seasonal Adaptation,[0],[0]
"To solve this, we train in the usual way, but when testing on future data, we only include the domain-independent features.",4.2 Non-seasonal Adaptation,[0],[0]
"The intuition is that the domain-independent parameters should be applicable to all domains, and so using only these features should lead to better generalizability to new domains.",4.2 Non-seasonal Adaptation,[0],[0]
"We test this hypothesis by training the classifiers on all but the last time interval, and testing on the final interval.",4.2 Non-seasonal Adaptation,[0],[0]
"For hyperparameter tuning, we used the final time interval of the training data (i.e., the penultimate interval) as the validation set.",4.2 Non-seasonal Adaptation,[0],[0]
"The intuition is that the penultimate interval is the closest to the test data and thus is expected to be most similar to it.
",4.2 Non-seasonal Adaptation,[0],[0]
Results are shown in the first three columns of Table 3.,4.2 Non-seasonal Adaptation,[0],[0]
We see that this approach leads to a small performance boost in all cases except the Twitter dataset.,4.2 Non-seasonal Adaptation,[0],[0]
"This means that this simple feature augmentation approach has the potential to make classifiers more robust to future changes in data.
",4.2 Non-seasonal Adaptation,[0],[0]
How to apply the feature augmentation technique to unseen domains is not well understood.,4.2 Non-seasonal Adaptation,[0],[0]
"By removing the domain-specific features, as we did here, the prediction model has changed, and so its behavior may be hard to predict.",4.2 Non-seasonal Adaptation,[0],[0]
"Nonetheless, this appears to be a successful approach.",4.2 Non-seasonal Adaptation,[0],[0]
We also experimented with including the seasonal features when performing non-seasonal adaptation.,4.2.1 Adding Seasonal Features,[0],[0]
"In this setting, we train the models with two domain-specific features in addition to the domain-independent features: one for the season,
and one for the non-seasonal interval.",4.2.1 Adding Seasonal Features,[0],[0]
"As above, we remove the non-seasonal features at test time; however, we retain the season-specific features in addition to the domain-independent features, as they can be reused in future years.
",4.2.1 Adding Seasonal Features,[0],[0]
The results of this approach are shown in the last column of Table 3.,4.2.1 Adding Seasonal Features,[0],[0]
We find that combining seasonal and non-seasonal features together leads to an additional performance gain in most cases.,4.2.1 Adding Seasonal Features,[0],[0]
"Our experiments suggest that time can substantially affect the performance of document classification, and practitioners should be cognizant of this variable when developing classifiers.",5 Conclusion,[0],[0]
"A simple analysis comparing pairs of time intervals can provide insights into how performance varies with time, which could be a good practice to do when initially working with a corpus.",5 Conclusion,[0],[0]
"Our experiments also suggest that simple domain adaptation techniques can help account for this variation.4
We make two practical recommendations following the insights from this work.",5 Conclusion,[0],[0]
"First, evaluation will be most accurate if the test data is as similar as possible to whatever future data the classifier will be applied to, and one way to achieve this is to select test data from the chronological end of the corpus, rather than randomly sampling data without regard to time.",5 Conclusion,[0],[0]
"Second, we observed that performance on future data tends to increase when hyperparameter tuning is conducted on later data; thus, we also recommend sampling validation data from the chronological end of the corpus.",5 Conclusion,[0],[0]
The authors thank the anonymous reviews for their insightful comments and suggestions.,Acknowledgements,[0],[0]
"This work was supported in part by the National Science Foundation under award number IIS-1657338.
4Our code is available at: https://github.com/ xiaoleihuang/Domain_Adaptation_ACL2018",Acknowledgements,[0],[0]
Many corpora span broad periods of time.,abstractText,[0],[0]
"Language processing models trained during one time period may not work well in future time periods, and the best model may depend on specific times of year (e.g., people might describe hotels differently in reviews during the winter versus the summer).",abstractText,[0],[0]
"This study investigates how document classifiers trained on documents from certain time intervals perform on documents from other time intervals, considering both seasonal intervals (intervals that repeat across years, e.g., winter) and non-seasonal intervals (e.g., specific years).",abstractText,[0],[0]
"We show experimentally that classification performance varies over time, and that performance can be improved by using a standard domain adaptation approach to adjust for changes in time.",abstractText,[0],[0]
Examining Temporality in Document Classification,title,[0],[0]
"Proceedings of the SIGDIAL 2015 Conference, pages 260–269, Prague, Czech Republic, 2-4 September 2015. c©2015 Association for Computational Linguistics",text,[0],[0]
"The amount of textual content that is produced and consumed each day all over the world, through news websites, social media, and other information sources, is constantly growing.",1 Introduction,[0],[0]
This makes the process of selecting the right content to read and quickly recognizing basic facts and topics in texts a core task for making content accessible to the users.,1 Introduction,[0],[0]
Automatic summarization strives to provide a means to this end.,1 Introduction,[0],[0]
"This paper describes our automatic summarization system, and its participation in the MultiLing 2015 summarization challenge.
",1 Introduction,[0],[0]
"Our focus has been on producing a largely language-independent solution for the MultiLing 2015 challenge that, in contrast to most attempts in this field, requires a strict minimum of languagespecific components and uses no language-specific materials for the core innovative elements.
",1 Introduction,[0],[0]
"Our motivation comes in part from Hong et al. (2014), who compares a number of single language summarization systems on the same standardized data set and shows that many complex, language-specific, highly optimized and trained
methods do not significantly out-perform simplistic algorithms that date back to the first summarization competitions in 2004.
",1 Introduction,[0],[0]
"Language-independent text summarization is generally based on sentence extractive methods: A subset of sentences in a text are identified and combined to form a summary, rather than performing more complex operations, and the primary task of summarization algorithms is to identify the set of sentences that form the best summary.",1 Introduction,[0],[0]
"In this case, algorithms differ mostly in how sentences are selected.
",1 Introduction,[0],[0]
One textual feature that has proven useful in identifying good summary sentences is the relative prominence of specific words in texts when contrasted to a reference distribution (like frequency in a large general corpus).,1 Introduction,[0],[0]
"For example, the “keyness” metric in El-Haj and Rayson (2013), singular value decomposition on a term-vector matrix (Steinberger, 2013) and neural network-derived transformations of term vectors (Kågebäck et al., 2014) have all produced significant results.",1 Introduction,[0],[0]
There are also a number of rule-based approaches like Anechitei and Ignat (2013).,1 Introduction,[0],[0]
"Hong et al. (2014) provides an overview of various current approaches, ranging from simple baseline algorithms to complex systems with many machine learning and rule-based components of various kinds.
",1 Introduction,[0],[0]
"One promising recent approach is graph theorybased schemes which construct sentence similarity graphs and use various graph techniques to determine the importance of specific sentences as a heuristic to identify good summary sentences (Barth, 2004; Li et al., 2013b; Mihalcea and Tarau, 2004).
",1 Introduction,[0],[0]
"In this paper, we describe ExB’s graphbased summarization approach and its results in two MultiLing 2015 tasks: Multilingual Singledocument Summarization and Multilingual Multidocument Summarization.",1 Introduction,[0],[0]
ExB’s submissions covered all languages in each task.,1 Introduction,[0],[0]
"Furthermore,
260
we summarize and discuss some unexpected negative experimental results, particularly in light of the problems posed by summarization tasks and their evaluation using ROUGE (Lin, 2004).",1 Introduction,[0],[0]
"The procedures used in both tasks start from similar assumptions and use a generalized framework for language-independent sentence selectionbased summarization.
",2 Process Overview,[0],[0]
We start from the same basic model as LDA approaches to text analysis: Every document contains a mixture of topics that are probabilistically indicative of the tokens present in it.,2 Process Overview,[0],[0]
"We select sentences in order to generate summaries whose topic mixtures most closely match that of the document as a whole (Blei et al., 2003).
",2 Process Overview,[0],[0]
"We construct a graph representation of the text in which each node corresponds to a sentence, and edges are weighted by a similarity metric for comparing them.",2 Process Overview,[0],[0]
"We then extract key sentences for use in summaries by applying the PageRank/TextRank algorithm, a well-studied algorithm for measuring graph centrality.",2 Process Overview,[0],[0]
"This technique has proven to be good model for similar extraction tasks in the past (Mihalcea and Tarau, 2004).
",2 Process Overview,[0],[0]
We deliberately chose not to optimize any parameters of our core algorithm for specific languages.,2 Process Overview,[0],[0]
Every parameter and design decision applied to all languages equally and was based on cross-linguistic performance.,2 Process Overview,[0],[0]
"Typically it is possible to increase evaluation performance by 2%-4% through fine tuning, but this tends to produce overfitting and the gains are lost when applied to any broader set of languages or domains.
",2 Process Overview,[0],[0]
"Our approach consists of three stages:
1.",2 Process Overview,[0],[0]
Preprocessing using common NLP tools.,2 Process Overview,[0],[0]
"This includes steps like tokenization and sentence identification, and in the multilingual summarization case, an extractor for time references like dates and specific times of day.",2 Process Overview,[0],[0]
"These tools are not entirely languageindependent.
2.",2 Process Overview,[0],[0]
"Sentence graph construction and sentence ranking as described in Sections 2.2 and 2.3 respectively.
3.",2 Process Overview,[0],[0]
Post-processing using simple and languageindependent rules for selecting the highest ranking sentences up to the desired length of text.,2 Process Overview,[0],[0]
Our processing pipeline starts with tokenization and sentence boundary detection.,2.1 Preprocessing,[0],[0]
For most languages we employ ExB’s proprietary languageindependent rule-based tokenizer.,2.1 Preprocessing,[0],[0]
"For Chinese, Japanese and Thai tokenization we use languagedependent approaches:
• Chinese is tokenized using a proprietary algorithm that relies on a small dictionary, the probability distribution of token lengths in Chinese, and a few handcrafted rules for special cases.
",2.1 Preprocessing,[0],[0]
"• For Thai, we use a dictionary containing data from NECTEC (2003) and Satayamas (2014) to calculate the optimal partition of Thai letter sequences based on a shortest path algorithm in a weighted, directed acyclic character graph using dictionary terms found in the text.
",2.1 Preprocessing,[0],[0]
"• For Japanese, we employ the CRF-based MeCab (Kudo et al., 2004; Kudo, 2013) morphological analyzer and tokenizer.",2.1 Preprocessing,[0],[0]
"MeCab is considered state-of-the-art and is currently being used in the construction of annotated reference corpora for Japanese by Maekawa et al. (2014).
",2.1 Preprocessing,[0],[0]
"Sentence boundary detection is rule-based and uses all sentence separators available in the Unicode range of the document’s main language, along with an abbreviation list and a few rules to correctly identify expressions like “p.ex.” or “...”
Finally, we use a proprietary SVM-based stemmer trained for a wide variety of languages on custom corpora.",2.1 Preprocessing,[0],[0]
"Given a set of tokenized sentences S, we construct a weighted undirected graph G = (V,E), where each vertex Vi ∈ V corresponds to a sentence in S. The weighted edges (Si, Sj , w) of the graph are defined as a subset of S × S where i",2.2 Graph construction,[0],[0]
"6= j and (w ← sim(Si, Sj))",2.2 Graph construction,[0],[0]
≥ t for a given similarity measure sim and a given threshold t.,2.2 Graph construction,[0],[0]
"We always assume a normalized similarity measure with a scale between 0 and 1.
",2.2 Graph construction,[0],[0]
"Sentence similarity is computed with the standard vector space model (Salton, 1989), where each sentence is defined by a vector of its tokens.
",2.2 Graph construction,[0],[0]
"We compared these vectors using a number of techniques:
• An unweighted bag-of-words model with sentence similarity computed using the Jacquard index.
",2.2 Graph construction,[0],[0]
"• Conventional cosine similarity of sentence vectors weighted by term frequency in the sentence.
",2.2 Graph construction,[0],[0]
"• TF-IDF weighted cosine similarity, where term frequencies in sentences are normalized with respect to the document collection.
",2.2 Graph construction,[0],[0]
"• Semantic similarity measured using the ExB Themis semantic approach described in Hänig et al. (2015).
",2.2 Graph construction,[0],[0]
"We also evaluated different settings for the threshold t. We did not optimize t separately for different languages, instead setting a single value for all languages.
",2.2 Graph construction,[0],[0]
"Surprisingly, when averaged over all 38 languages in the MSS training set, the simple bag-ofwords model with a threshold t = 0.3 produced the best result using the ROUGE-2 measure.",2.2 Graph construction,[0],[0]
"We then apply to the sentence similarity graph an iterative extension of the PageRank algorithm (Brin and Page, 1998) that we have called FairTextRank (FRank) to rank the sentences in the graph.",2.3 Sentence ranking,[0],[0]
"PageRank has been used as a ranker for an extractive summarizer before in Mihalcea and Tarau (2004), who named it TextRank when used for this purpose.",2.3 Sentence ranking,[0],[0]
"PageRank constitutes a measure of graph centrality, so intuitively we would expect it to select the most central, topical, and summarizing sentences in the text.
",2.3 Sentence ranking,[0],[0]
"Following our assumption that every document constitutes a mix of topics, we further assume that every topic corresponds to a cluster in the sentence graph.",2.3 Sentence ranking,[0],[0]
"However, PageRank is not a cluster sensitive algorithm and does not, by itself, ensure coverage of the different clusters present in any graph.",2.3 Sentence ranking,[0],[0]
"Therefore, our FRank algorithm invokes PageRank iteratively on the graph, at each step ranking all the sentences, then removing the top ranking sentence from the graph, and then running PageRank again to extract the next highest ranking sentence.",2.3 Sentence ranking,[0],[0]
"Because the most central sentence in the entire graph is also, by definition, the most central sentence in some cluster, removing it weakens
the centrality of the other sentences in that cluster and increases the likelihood that the next sentence selected will be the highest ranking sentence in another cluster.
",2.3 Sentence ranking,[0],[0]
"A similar method of removing selected sentences is used in the UWB Summarizer by Steinberger (2013), which was one of the top performing systems at MultiLing 2013.",2.3 Sentence ranking,[0],[0]
"However, the UWB Summarizer uses an LSA algorithm on a sentence-term matrix to identify representative sentences, where we have employed PageRank.
",2.3 Sentence ranking,[0],[0]
The complete algorithm is detailed in Algorithm 1.,2.3 Sentence ranking,[0],[0]
"The function adj returns the weighted adjacency matrix of the sentence graph G. An inner for-loop transforms the weighted adjacency matrix into a column-stochastic matrix where for each column c, where A[i, c] is the weight of the edge between sentence i and sentence c, the following expression holds: ∑ i∈|A|A[i, c] = 1.",2.3 Sentence ranking,[0],[0]
"Informally, each column is normalized at each iteration so that its values sum to 1.",2.3 Sentence ranking,[0],[0]
"pr is the PageRank-algorithm with the default parameters β = 0.85, a convergence threshold of 0.001 and allowed to run for at most 100 iterations as implemented in the JUNG API (O’Madadhain et al., 2010).
",2.3 Sentence ranking,[0],[0]
Algorithm 1,2.3 Sentence ranking,[0],[0]
FairTextRank 1: function FRANK(G) 2:,2.3 Sentence ranking,[0],[0]
R←,2.3 Sentence ranking,[0],[0]
[] 3: while |G| > 0,2.3 Sentence ranking,[0],[0]
"do 4: A← adj(G) 5: for (r, c)← |A|2 do 6: Anorm[r, c]← A[r,c],∑
i∈|A| A[i,c]
7: rank ← pr(Anorm) 8: v ← rank[0] 9: R← R+ v
10: G← G \ v return R",2.3 Sentence ranking,[0],[0]
The final step in processing is the production of a plain text summary.,2.4 Post-processing,[0],[0]
"Given a fixed maximum summary length, we selected the highest ranked sentences produced by the ranking algorithm until total text length was greater than the maximum allowed length, then truncated the last sentence to fit exactly the maximum allowed length.",2.4 Post-processing,[0],[0]
"Although this reduces the human readability of the summary - the last sentence is interrupted without any consideration of the reader at all - it can only increase
the score of an n-gram based evaluation metric like ROUGE.",2.4 Post-processing,[0],[0]
The Multilingual Single-document Summarization (MSS) task consisted of producing summaries for Wikipedia articles in 38 languages.,3 Single Document Summarizer,[0],[0]
All articles were provided as UTF-8 encoded plain-text files and as XML documents that mark sections and other elements of the text structure.,3 Single Document Summarizer,[0],[0]
"We took advantage of the availability of headers and section boundary information in performing this task.
",3 Single Document Summarizer,[0],[0]
There was no overlap between the training data and the evaluation data for the MSS task.,3 Single Document Summarizer,[0],[0]
The released training data consisted of the evaluation data set from MultiLing 2013 as described in Kubina et al. (2013).,3 Single Document Summarizer,[0],[0]
This training data contains 30 articles in each of 40 languages.,3 Single Document Summarizer,[0],[0]
"The MSS task itself at MultiLing 2015 used 30 articles in each of 38 languages, dropping two languages because there were not enough new articles not included in the training data.
",3 Single Document Summarizer,[0],[0]
"In addition to the preprocessing steps described in Section 2.1, for this task we applied a list of sentence filters developed specifically for Wikipedia texts:
• Skip all headers.",3 Single Document Summarizer,[0],[0]
•,3 Single Document Summarizer,[0],[0]
"Skip every sentence with with less than 2 to-
kens (mostly errors in sentence boundary detection).
",3 Single Document Summarizer,[0],[0]
•,3 Single Document Summarizer,[0],[0]
"Skip every sentence that contains double quotes.
",3 Single Document Summarizer,[0],[0]
"We then performed sentence graph construction and ranking as described in Sections 2.2 and 2.3
In the post-processing stage, we sorted the sentences selected to go into the summary in order of their position in the original article, before producing a plain text summary by concatenating them.",3 Single Document Summarizer,[0],[0]
The organizers of the MultiLing 2015 challenge measured the quality of our system’s output using five different versions of the ROUGE score.,3.1 Results,[0],[0]
We provide a summary of the results for all participants in Table 1.,3.1 Results,[0],[0]
"It shows the average ranking of each participating system over all the languages on which it was tested, as well as the number of languages on which each system was tested.",3.1 Results,[0],[0]
The systems labelled Lead and Oracles are special systems.,3.1 Results,[0],[0]
"Lead just uses the beginning of the article
as the summary and represents a very simple baseline.",3.1 Results,[0],[0]
"Oracles, on the other hand, is a cheating system that marks the upper bound for any extractive approach.
",3.1 Results,[0],[0]
Only three submissions - highlighted in bold - participated in more than 3 languages.,3.1 Results,[0],[0]
"We submitted only one run of our system, defined as a fixed set of parameters that are the same over all languages.",3.1 Results,[0],[0]
One of the other two systems that participated in all 38 languages submitted five runs.,3.1 Results,[0],[0]
"According to the frequently used ROUGE-1 and ROUGE-2 scores, our system achieved an average ranking of 3.2 and 3.3, respectively.",3.1 Results,[0],[0]
"This table shows that the CCS system performed better on average than our system, and the LCS-IESI system performed on average worse.
",3.1 Results,[0],[0]
"However, ROUGE-1 only measures matching single words, whereas ROUGE-2 measures matching bigrams.",3.1 Results,[0],[0]
More complex combinations of words are more indicative of topic matches between gold standard data and system output.,3.1 Results,[0],[0]
"We believe that ROUGE-SU4, which measures bigrams of words with some gaps as well as unigrams, would be a better measure of output quality.",3.1 Results,[0],[0]
"When manually inspecting the summaries, we have the strong impression that system runs in which our system scored well by ROUGESU4 measures, but poorly by ROUGE-2, did produce better summaries with greater readability and topic coverage.
",3.1 Results,[0],[0]
"Our system achieves a significantly better overall ranking using ROUGE-SU4 instead of ROUGE-2, even though the system was optimized to produce the highest ROUGE-2 scores.",3.1 Results,[0],[0]
Only two runs of the winning system CCS scored better than our system according to ROUGE-SU4.,3.1 Results,[0],[0]
"This underlines the robustness of our system’s underlying principles, despite the known problems with ROUGE evaluations.",3.1 Results,[0],[0]
The Multilingual Multi-document Summarization (MMS) task involves summarizing ten news articles on a single topic in a single language.,4 Multi Document Summarizer,[0],[0]
"For each language, the dataset consists of ten to fifteen topics, and ten languages were covered in all, including and expanding on the data used in the 2013 MMS task described by Li et al. (2013a).
",4 Multi Document Summarizer,[0],[0]
"The intuition guiding our approach to this task is the idea that if news articles on the same topic contain temporal references that are close together
or overlapping in time, then they are likely to describe the same event.",4 Multi Document Summarizer,[0],[0]
We therefore cluster the documents in each collection by the points in time referenced in the text rather than attempting to summarize the concatenation of the documents directly.,4 Multi Document Summarizer,[0],[0]
"This approach has the natural advantage that we can present summary information in chronological order, thereby often improving readability.",4 Multi Document Summarizer,[0],[0]
"Unfortunately, this improvement is not measurable using ROUGE-style metrics as employed in evaluating this task.
",4 Multi Document Summarizer,[0],[0]
"An official training data set with model summaries was released, but too late to inform our submission, which was not trained with any new 2015 data.",4 Multi Document Summarizer,[0],[0]
"We did, however, use data from the 2011 MultiLing Pilot including gold standard summaries (Giannakopoulos et al., 2011), which forms a part of the 2015 dataset.",4 Multi Document Summarizer,[0],[0]
"We used only the 700 documents and summaries from the 2011 task as training data, and did not use any Chinese, Spanish or Romanian materials in preparing our submission.
",4 Multi Document Summarizer,[0],[0]
"Our submission follows broadly the same procedure as for the single document summarization task, as described in Section 2 and Section 3, except for the final step, which relies on section information not present in the news articles that form the dataset for this task.",4 Multi Document Summarizer,[0],[0]
"Instead, a manual examination of the dataset revealed that the news articles all have a fixed structure: the first line is the headline, the second is the date, and the remaining lines form the main text.",4 Multi Document Summarizer,[0],[0]
"We used this underlying structure in preprocessing to identify the dateline of the news article, and we use this date to disambiguate relative time expressions in the text like “yesterday” or “next week”.",4 Multi Document Summarizer,[0],[0]
"Articles are also ordered in
time with respect to each other on the basis of the article date.
",4 Multi Document Summarizer,[0],[0]
"Furthermore, we remove in preprocessing any sentence that contains only time reference tokens because they are uninformative for summarization.
",4 Multi Document Summarizer,[0],[0]
"We then extract temporal references from the text, using ExB’s proprietary TimeRec framework described in Thomas (2012), which is available for all the languages used in this task.",4 Multi Document Summarizer,[0],[0]
"With the set of disambiguated time references in each document, we can provide a “timeframe” for each document that ranges from the earliest time referenced in the text to the latest.",4 Multi Document Summarizer,[0],[0]
"Note that this may not include the date of the document itself, if, for example, it is a retrospective article about an event that may have happened years in the past.",4 Multi Document Summarizer,[0],[0]
Ng et al. (2014) and,4.1 Time information processing,[0],[0]
Wan (2007) investigate using textural markers of time for multi-document summarization of news articles using very different algorithms.,4.1 Time information processing,[0],[0]
Our approach is more similar to Ng et al. in constructing a timeline for each document and for the collection as a whole based on references extracted from texts.,4.1 Time information processing,[0],[0]
"Once document timeframes are ordered chronologically, we organize them into groups based on their positions on a time line.",4.1 Time information processing,[0],[0]
"We explored two strategies to produce these groups:
• Least Variance Clustering (LVC):",4.1 Time information processing,[0],[0]
Grouping the documents iteratively by adding a new document to the group if the overall variance of the group doesn’t go over a threshold.,4.1 Time information processing,[0],[0]
"We set the standard deviation limit of the group
in 0.1.",4.1 Time information processing,[0],[0]
The algorithm is a divisive clustering algorithm based on the central time of the documents and the standard deviation.,4.1 Time information processing,[0],[0]
"At first the minimal central time of a document collection is subtracted from all other central times, then we compute mean, variance and standard deviation based on days as a unit and normalized by the mean.",4.1 Time information processing,[0],[0]
"Afterwards we recursively split the groups with the goal to minimize the variance of both splits until either a group consists only of one document or the recomputed standard deviation of a group is less than 0.1.
",4.1 Time information processing,[0],[0]
"• Overlapping Time Clustering (OTC): Grouping documents together if their timeframes overlap more than a certain amount, which we empirically set to 0.9 after experimenting with various values.",4.1 Time information processing,[0],[0]
"This means that if two texts A and B are grouped together, then either A’s timeframe includes at least 90% of B’s timeframe, or B’s timeframe includes 90% of A’s.",4.1 Time information processing,[0],[0]
"This approach proceeds iteratively, with each new addition to a group updating the timeframe of the group as a whole, and any text which overlaps more than 90% with this new interval is then grouped with it in the next iteration.
",4.1 Time information processing,[0],[0]
"In addition, we provide two baseline clusterings:
• One document per cluster (1PC): Each document is in a cluster by itself.
",4.1 Time information processing,[0],[0]
"• All in one cluster (AIO): All documents from one topic are clustered together.
",4.1 Time information processing,[0],[0]
"In the LVC and OTC cases, clustering is iterative and starts with the earliest document as determined by a fixed “central” date for each document.",4.1 Time information processing,[0],[0]
"We explored different ways of determining that “central” date: One was using the dateline found in preprocessing on the second line of each document, another was the median of the time references in the document.",4.1 Time information processing,[0],[0]
"Our best result used the dateline from each article and, as can be seen in Table 2, was produced by the OTC strategy.",4.1 Time information processing,[0],[0]
"This is a surprising result, as we expected LVC to perform better since variance is generally a better measure of clustering.",4.1 Time information processing,[0],[0]
"However, we found that LVC generally produced more clusters than OTC and we believe that to account for its poor performance.
",4.1 Time information processing,[0],[0]
"We experimented with a number of other ordering and clustering approaches, although they do not figure into our submission to the MMS task, but in all cases they failed to out-perform the OTC approach according to the ROUGE-2 recall measure.
",4.1 Time information processing,[0],[0]
"For all conditions, identical preprocessing was performed using ExB’s proprietary languagespecific tokenizer and sentence identifier.",4.1 Time information processing,[0],[0]
"ROUGE scores, because they are based on token n-grams, are very sensitive to discrepancies between tokenizers and stemmers.",4.1 Time information processing,[0],[0]
"In English, because most tokenizers perform very similarly, this causes fewer problems in scoring than for Arabic or other languages where tokenizers vary dramatically.",4.1 Time information processing,[0],[0]
"We used the results in Table 2 to decide which conditions to use in the competition, but we cannot be sure to what degree our results have been influenced by these kinds of ROUGE-related problems.
",4.1 Time information processing,[0],[0]
"After clustering, we perform graph-based sentence ranking as described in Sections 2.2 and 2.3 separately for each cluster.",4.1 Time information processing,[0],[0]
"We then select sentences from each cluster, ensuring that they are all represented in the final summary, so that the entire time span of the articles is covered.",4.1 Time information processing,[0],[0]
"We also order the selected sentences in the summary based on the temporal ordering of the clusters, so that summary presentation is in event order.",4.1 Time information processing,[0],[0]
"When experimenting with the challenge data we made several observations:
1.",4.2 Experimental results,[0],[0]
"Since the dataset of MMS is composed of news articles, just selecting the headlines and first sentences will produce a strong baseline with very high ROUGE scores.",4.2 Experimental results,[0],[0]
"It is difficult to beat this baseline using sentence extraction techniques.
",4.2 Experimental results,[0],[0]
2.,4.2 Experimental results,[0],[0]
The quality of the summaries varies a great deal between languages.,4.2 Experimental results,[0],[0]
"Instead of producing fine-tuned configurations for each lan-
guage that optimize ROUGE scores, we focused on increasing the performance in English - a language we can read and in which we can qualitatively evaluate the produced summaries.
",4.2 Experimental results,[0],[0]
3.,4.2 Experimental results,[0],[0]
All the results here of the time information processing are at document-level.,4.2 Experimental results,[0],[0]
"We also tried to apply the time grouping algorithms per sentence, but we noticed a drop of about 3% ROUGE-2 score on average.
",4.2 Experimental results,[0],[0]
"The most important finding is that using temporal expressions and chronological information does improve the performance of the summary system, and that the iterative FairTextRank algorithm shows a solid performance even for multiple documents.
",4.2 Experimental results,[0],[0]
"As can be seen in Table 3, our system gets ranked in middle position in the official scores of the challenge using the NPowER, MeMoG and AutoSummENG measures as described in Giannakopoulos and Karkaletsis (2013) and Giannakopoulos and Karkaletsis (2011).",4.2 Experimental results,[0],[0]
"We also note that our system out-performs all other participants in Chinese, a language for which we had no training data.",4.2 Experimental results,[0],[0]
"We feel that it is important not only to publish positive results, but also negative ones, to counter the strong publication bias identified in many areas in the natural and social sciences (Dickersin et al., 1987; Ioannidis, 2005).",5 Negative results,[0],[0]
"Since we conducted a large number of experiments in creating this system, we inevitably also came across a number of ideas that seemed good, but turned out to not improve our algorithm, at least as measured using ROUGE-2.
",5 Negative results,[0],[0]
In another challenge participation we developed a very powerful “semantic text similarity” (STS) toolkit.,5 Negative results,[0],[0]
"In SemEval 2015 Task 2 (Agirre et al., 2015), it achieved by far the highest scores for Spanish texts and the second best scores for English.",5 Negative results,[0],[0]
"Since our text summarization methodology is based on a sentence similarity graph, our intuitive hypothesis was that when using this module as opposed to simple matching-words strategies, performance should increase significantly.",5 Negative results,[0],[0]
"Matching-words strategies are used as the baseline in SemEval tasks, and it is easily out-performed by more sophisticated approaches.
",5 Negative results,[0],[0]
"Therefore, we tried out our STS module as a replacement for Jacquard and cosine similarity measures when constructing the sentence graph, while keeping all other parameters fixed.",5 Negative results,[0],[0]
"Surprisingly, it did not improve performance, and lowered ROUGE-2 scores by 2%.",5 Negative results,[0],[0]
"We also attempted to use word2vec embeddings precomputed on very large corpora (Mikolov et al., 2013) to represent words and hence compute a much finer-grained sentence similarity, but those results were 4% worse.",5 Negative results,[0],[0]
"It is possible that those systems were, in fact, better, but because ROUGE scoring focuses on word matches, any other improvement cannot be measured directly.",5 Negative results,[0],[0]
"We also attempted to include other factors such as sentence length, position, number of named entities, temporal expressions, and physical measurements into the sentence similarity score, all without seeing any increase in ROUGE scores.
",5 Negative results,[0],[0]
"Since identifying temporal expressions increases ROUGE scores, as this paper shows, we surmised that name recognition might also improve summarization.",5 Negative results,[0],[0]
"We applied our named entity recognition system, which is available in a number of different languages and won the Germeval 2014 (Benikova et al., 2014) NER challenge, and weighted more heavily sentences with detected names before extracting summary sentences.",5 Negative results,[0],[0]
"Interestingly, no matter how the weighting scheme was set up, the performance of the system always dropped by a few percent.",5 Negative results,[0],[0]
"Often, the system would select useless sentences that contain long lists of participating authors, or enumerations of entities participating in some reported event.",5 Negative results,[0],[0]
"Even when these kinds of sentences are explicitly removed, it still selects sentences that simply contain many names with little relevance to the topics of the news article.",5 Negative results,[0],[0]
"We conclude that sen-
tences describing central topics in documents are not strongly correlated with named entity usage.
",5 Negative results,[0],[0]
"Another very intuitive assumption is that filtering stop words, or down-weighting very frequent words, or using a TF-IDF based scheme with a similar effect, would improve the results.",5 Negative results,[0],[0]
"However, we did not observe any improvement by using these techniques.",5 Negative results,[0],[0]
"Nonetheless, there are strong indications that this is due to the limitations of ROUGE-2 scoring and we cannot conclude that these kinds of techniques are useless for summarization.",5 Negative results,[0],[0]
It is easy to achieve very competitive ROUGE-2 scores by just filling the summary with very frequent stop word combinations.,5 Negative results,[0],[0]
"A human would immediately recognize the uselessness of such a “summary”, but ROUGE-2 would count many bigram matches with a gold standard summary.
",5 Negative results,[0],[0]
"Finally, we considered the hypothesis that the summary system could be helped by explicitly removing very similar sentences presenting redundant information.",5 Negative results,[0],[0]
"Surprisingly, explicitly removing such sentences did not improve the performance of the system.",5 Negative results,[0],[0]
"Manually inspecting a number of summaries, we notice that very similar sentences recurring often in texts are rarely selected by the FRank algorithm.",5 Negative results,[0],[0]
We believe this is because our approach is sufficiently robust to discount these sentences on its own.,5 Negative results,[0],[0]
"In this paper we outline ExB’s largely languageindependent system for text summarization based on sentence selection, and show that it supports at least the 38 languages used in this completion without any language-specific fine-tuning.",6 Conclusions,[0],[0]
Sentences are selected using an iterative extension of PageRank calculation on a sentence similarity graph.,6 Conclusions,[0],[0]
"Our results in the MultiLing 2015 challenge have validated this approach by achieving the best scores for several languages and competitive scores for most of them, generally surpassed by only one other participating system.
",6 Conclusions,[0],[0]
"We also show that one basic summarization system can apply to different domains, different languages, and different tasks without special configuration, while retaining state-of-the-art performance.",6 Conclusions,[0],[0]
"Furthermore, for multi-document news summarization, we show that extracting temporal expressions is a useful feature for combining articles on the same topic.
",6 Conclusions,[0],[0]
"Our most relevant conclusion is that both the current evaluation methodology (based on various forms of ROUGE) as well as the current principal approach to language-independent text summarization (context-free, sentence selection based) are highly inadequate to model the vague requirements users associate with a text summarization product.
",6 Conclusions,[0],[0]
Participants in MultiLing 2015 did not receive the scripts and parameters used in producing evaluations.,6 Conclusions,[0],[0]
This made it difficult to optimize parameters and algorithms and has a significant impact on results using ROUGE measures and probably the other measures as well.,6 Conclusions,[0],[0]
"Hong et al. (2014), for example, notes values between 30.8% and 39.1% using ROUGE-1 for one well-known algorithm on one data set by different authors.",6 Conclusions,[0],[0]
It is not clear how the vastly different scores obtained for identical summaries using different ROUGE parameters correlate with the objective quality of a given summary.,6 Conclusions,[0],[0]
"We have no clear indication that ROUGE scores really capture the quality of a given summary at all.
",6 Conclusions,[0],[0]
"While it is possible to formulate summarization solutions based on sentence selection and even iteratively improve them using ROUGE scores, the actual achievable performance measured using ROUGE is very low.",6 Conclusions,[0],[0]
"We have noticed that stemming, stopword filtering and various tokenization strategies can have a very large influence on ROUGE scores, especially in morphologically richer languages than English.",6 Conclusions,[0],[0]
"More modern evaluation measures like MeMog or NPoweR might solve the problems inherent to ROUGE, however they currently lack widespread adoption in the research community.
",6 Conclusions,[0],[0]
"Nonetheless, even if these issues in evaluation can be addressed, we do not believe that summaries based on sentence selection will ever reach a quality where they could be accepted as comparable to a human written summary.",6 Conclusions,[0],[0]
We present our state of the art multilingual text summarizer capable of single as well as multi-document text summarization.,abstractText,[0],[0]
"The algorithm is based on repeated application of TextRank on a sentence similarity graph, a bag of words model for sentence similarity and a number of linguistic preand post-processing steps using standard NLP tools.",abstractText,[0],[0]
We submitted this algorithm for two different tasks of the MultiLing 2015 summarization challenge: Multilingual Singledocument Summarization and Multilingual Multi-document Summarization.,abstractText,[0],[0]
ExB Text Summarizer,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1329–1338 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1329
In this paper we present the Exemplar Encoder-Decoder network (EED), a novel conversation model that learns to utilize similar examples from training data to generate responses. Similar conversation examples (context-response pairs) from training data are retrieved using a traditional TF-IDF based retrieval model. The retrieved responses are used to create exemplar vectors that are used by the decoder to generate the response. The contribution of each retrieved response is weighed by the similarity of corresponding context with the input context. We present detailed experiments on two large data sets and find that our method outperforms state of the art sequence to sequence generative models on several recently proposed evaluation metrics. We also observe that the responses generated by the proposed EED model are more informative and diverse compared to existing state-of-the-art method.",text,[0],[0]
"With the availability of large datasets and the recent progress made by neural methods, variants of sequence to sequence learning (seq2seq) (Sutskever et al., 2014) architectures have been successfully applied for building conversational systems (Serban et al., 2016, 2017b).",1 Introduction,[0],[0]
"However, despite these methods being the stateof-the art frameworks for conversation generation, they suffer from problems such as lack of diversity in responses and generation of short, repetitive and uninteresting responses (Liu et al., 2016; Serban et al., 2016, 2017b).",1 Introduction,[0],[0]
"A large body of recent
literature has focused on overcoming such challenges (Li et al., 2016a; Lowe et al., 2017).
",1 Introduction,[0],[0]
"In part, such problems arise as all information required to generate responses needs to be captured as part of the model parameters learnt from the training data.",1 Introduction,[0],[0]
These model parameters alone may not be sufficient for generating natural conversations.,1 Introduction,[0],[0]
"Therefore, despite providing enormous amount of data, neural generative systems have been found to be ineffective for use in real world applications (Liu et al., 2016).
",1 Introduction,[0],[0]
"In this paper, we focus our attention on closed domain conversations.",1 Introduction,[0],[0]
"A characteristic feature of such conversations is that over a period of time, some conversation contexts1 are likely to have occurred previously (Lu et al., 2017b).",1 Introduction,[0],[0]
"For instance, Table 1 shows some contexts from the Ubuntu dialog corpus.",1 Introduction,[0],[0]
"Each row presents an input dialog context with its corresponding gold response followed by a similar context and response seen in training data – as can be seen, contexts for “installing dms”, “sharing files”, “blocking ufw ports” have all occurred in training data.",1 Introduction,[0],[0]
"We hypothesize that being able to refer to training responses for previously seen similar contexts could be a helpful signal to use while generating responses.
",1 Introduction,[0],[0]
"In order to exploit this aspect of closed domain conversations we build our neural encoderdecoder architecture called the Exemplar Encoder Decoder (EED), that learns to generate a response for a given context by exploiting similar contexts from training conversations.",1 Introduction,[0],[0]
"Thus, instead of having the seq2seq model learn patterns of language only from aligned parallel corpora, we assist the model by providing it closely related (similar) samples from the training data that it can refer to while generating text.
",1 Introduction,[0],[0]
"Specifically, given a context c, we retrieve a set 1We use the phrase “dialog context”, “conversation con-
text” and “context” interchangeably throughout the paper.
of context-response pairs (c(k), r(k)), 1 ≤ k ≤ K using an inverted index of training data.",1 Introduction,[0],[0]
We create an exemplar vector e(k) by encoding the response r(k) (also referred to as exemplar response) along with an encoded representation of the current context c. We then learn the importance of each exemplar vector e(k) based on the likelihood of it being able to generate the ground truth response.,1 Introduction,[0],[0]
We believe that e(k) may contain information that is helpful in generating the response.,1 Introduction,[0],[0]
"Table 1 highlights the words in exemplar responses that appear in the ground truth response as well.
",1 Introduction,[0],[0]
"Contributions: We present a novel Exemplar Encoder-Decoder (EED) architecture that makes use of similar conversations, fetched from an index of training data.",1 Introduction,[0],[0]
"The retrieved contextresponse pairs are used to create exemplar vectors which are used by the decoder in the EED model, to learn the importance of training context-response pairs, while generating responses.",1 Introduction,[0],[0]
"We present detailed experiments on the publicly benchmarked Ubuntu dialog corpus data set (Lowe et al., 2015) as well a large collection of more than 127,000 technical support conversations.",1 Introduction,[0],[0]
"We compare the performance of the EED model with the existing state of the art generative models such as HRED (Serban et al., 2016) and VHRED (Serban et al., 2017b).",1 Introduction,[0],[0]
"We find that our model out-performs these models on a wide variety of metrics such as the recently proposed Activity Entity metrics (Serban et al., 2017a) as well as Embedding-based metrics (Lowe et al., 2015).",1 Introduction,[0],[0]
"In addition, we present qualitative insights into our results and we find that exemplar based responses
are more informative and diverse.",1 Introduction,[0],[0]
The rest of the paper is organized as follows.,1 Introduction,[0],[0]
Section 2 briefly describes the recent works in neural dialogue generation The details of the proposed EED model for dialogue generation are described in detail in Section 3.,1 Introduction,[0],[0]
"In Section 4, we describe the datasets as well as the details of the models used during training.",1 Introduction,[0],[0]
We present quantitative and qualitative results of EED model in Section 5.,1 Introduction,[0],[0]
"In this section, we compare our work against other data-driven end-to-end conversation models.",2 Related Work,[0],[0]
"Endto-end conversation models can be further classified into two broad categories — generation based models and retrieval based models.
",2 Related Work,[0],[0]
Generation based models cast the problem of dialogue generation as a sequence to sequence learning problem.,2 Related Work,[0],[0]
"Initial works treat the entire context as a single long sentence and learn an encoder-decoder framework to generate response word by word (Shang et al., 2015; Vinyals and Le, 2015).",2 Related Work,[0],[0]
"This was followed by work that models context better by breaking it into conversation history and last utterance (Sordoni et al., 2015b).",2 Related Work,[0],[0]
"Context was further modeled effectively by using a hierarchical encoder decoder (HRED) model which first learns a vector representation of each utterance and then combines these representations to learn vector representation of context (Serban et al., 2016).",2 Related Work,[0],[0]
"Later, an alternative hierarchical model called VHRED (Serban et al., 2017b) was proposed, where generated responses were conditioned on latent variables.",2 Related Work,[0],[0]
"This leads to more in-
formative responses and adds diversity to response generation.",2 Related Work,[0],[0]
"Models that explicitly incorporate diversity in response generation have also been studied in literature (Li et al., 2016b; Vijayakumar et al., 2016; Cao and Clark, 2017; Zhao et al., 2017).
",2 Related Work,[0],[0]
"Our work differs from the above as none of these above approaches utilize similar conversation contexts observed in the training data explicitly.
",2 Related Work,[0],[0]
"Retrieval based models on the other hand treat the conversation context as a query and obtain a set of responses using information retrieval (IR) techniques from the conversation logs (Ji et al., 2014).",2 Related Work,[0],[0]
"There has been further work where the responses are further ranked using a deep learning based model (Yan et al., 2016a,b; Qiu et al., 2017).",2 Related Work,[0],[0]
"On the other hand of the spectrum, endto-end deep learning based rankers have also been employed to generate responses (Wu et al., 2017; Henderson et al., 2017).",2 Related Work,[0],[0]
"Recently a framework has also been proposed that uses a discriminative dialog network that ranks the candidate responses received from a response generator network and trains both the networks in an end to end manner (Lu et al., 2017a).
",2 Related Work,[0],[0]
"In contrast to the above models, we use the input contexts as well as the retrieved responses for generating the final responses.",2 Related Work,[0],[0]
"Contemporaneous to our work, a generative model for machine translation that employs retrieved translation pairs has also been proposed (Gu et al., 2017).",2 Related Work,[0],[0]
"We note that while the underlying premise of both the papers remains the same, the difference lies in the mechanism of incorporating the retrieved data.",2 Related Work,[0],[0]
A conversation consists of a sequence of utterances.,3.1 Overview,[0],[0]
"At a given point in the conversation, the utterances expressed prior to it are jointly referred to as the context.",3.1 Overview,[0],[0]
The utterance that immediately follows the context is referred to as the response.,3.1 Overview,[0],[0]
"As discussed in Section 1, given a conversational context, we wish to to generate a response by utilizing similar context-response pairs from the training data.",3.1 Overview,[0],[0]
We retrieve a set of K exemplar contextresponse pairs from an inverted index created using the training data in an off-line manner.,3.1 Overview,[0],[0]
"The input and the retrieved context-response pairs are then fed to the Exemplar Encoder Decoder (EED)
network.",3.1 Overview,[0],[0]
A schematic illustration of the EED network is presented in Figure 1.,3.1 Overview,[0],[0]
The EED encoder combines the input context and the retrieved responses to create a set of exemplar vectors.,3.1 Overview,[0],[0]
The EED decoder then uses the exemplar vectors based on the similarity between the input context and retrieved contexts to generate a response.,3.1 Overview,[0],[0]
We now provide details of each of these modules.,3.1 Overview,[0],[0]
"Given a large collection of conversations as (context, response) pairs, we index each response and its corresponding context in tf",3.2 Retrieval of Similar Context-Response Pairs,[0],[0]
− idf vector space.,3.2 Retrieval of Similar Context-Response Pairs,[0],[0]
"We further extract the last turn of a conversation and index it as an additional attribute of the context-response document pairs so as to allow directed queries based on it.
",3.2 Retrieval of Similar Context-Response Pairs,[0],[0]
"Given an input context c, we construct a query that weighs the last utterance in the context twice as much as the rest of the context and use it to retrieve the top-k similar context-response pairs from the index based on a BM25 (Robertson et al., 2009) retrieval model.",3.2 Retrieval of Similar Context-Response Pairs,[0],[0]
"These retrieved pairs form our exemplar context-response pairs (c(k), r(k)), 1 ≤ k ≤ K.",3.2 Retrieval of Similar Context-Response Pairs,[0],[0]
"Given the exemplar pairs (c(k), r(k)), 1 ≤ k ≤ K and an input context-response pair (c, r), we feed the input context c and the exemplar contexts c(1), . . .",3.3 Exemplar Encoder Network,[0],[0]
", c(K)",3.3 Exemplar Encoder Network,[0],[0]
"through an encoder to generate the embeddings as given below:
ce = Encodec(c)
c(k)e =",3.3 Exemplar Encoder Network,[0],[0]
"Encodec(c (k)), 1 ≤ k ≤",3.3 Exemplar Encoder Network,[0],[0]
"K
Note that we do not constrain our choice of encoder and that any parametrized differentiable architecture can be used as the encoder to generate the above embeddings.",3.3 Exemplar Encoder Network,[0],[0]
"Similarly, we feed the exemplar responses r(1), . . .",3.3 Exemplar Encoder Network,[0],[0]
", r(K) through a response encoder to generate response embeddings r (1) e , . . .",3.3 Exemplar Encoder Network,[0],[0]
", r (K) e , that is,
r(k)e =",3.3 Exemplar Encoder Network,[0],[0]
"Encoder(r (k)), 1 ≤ k ≤",3.3 Exemplar Encoder Network,[0],[0]
"K (1)
",3.3 Exemplar Encoder Network,[0],[0]
"Next, we concatenate the exemplar response encoding r(k)e with an encoded representation of current context ce as shown in equation 2 to create the exemplar vector e(k).",3.3 Exemplar Encoder Network,[0],[0]
"This allows us to include in-
formation about similar responses along with the encoded input context representation.
e(k) =",3.3 Exemplar Encoder Network,[0],[0]
"[ce; r (k) e ], 1 ≤ k ≤",3.3 Exemplar Encoder Network,[0],[0]
"K (2)
",3.3 Exemplar Encoder Network,[0],[0]
"The exemplar vectors e(k), 1 ≤ k ≤",3.3 Exemplar Encoder Network,[0],[0]
K are further used by the decoder for generating the ground truth response as described in the next section.,3.3 Exemplar Encoder Network,[0],[0]
Recall that we want the exemplar responses to help generate the responses based on how similar the corresponding contexts are with the input context.,3.4 Exemplar Decoder Network,[0],[0]
"More similar an exemplar context is to the input context, higher should be its effect in generating the response.",3.4 Exemplar Decoder Network,[0],[0]
"To this end, we compute the similarity scores s(k), 1 ≤ k ≤ K using the encodings computed in Section 3.3 as shown below.
s(k) =",3.4 Exemplar Decoder Network,[0],[0]
"exp(cTe c (k) e )∑K
l=1",3.4 Exemplar Decoder Network,[0],[0]
"exp(c T e c (l) e )
(3)
",3.4 Exemplar Decoder Network,[0],[0]
"Next, each exemplar vector e(k) computed in Section 3.3, is fed to a decoder, where the decoder is responsible for predicting the ground truth response from the exemplar vector.",3.4 Exemplar Decoder Network,[0],[0]
Let pdec(r|e(k)),3.4 Exemplar Decoder Network,[0],[0]
be the distribution of generating the ground truth response given the exemplar embedding.,3.4 Exemplar Decoder Network,[0],[0]
"The objective function to be maximized, is expressed as a
function of the scores s(k), the decoding distribution pdec and the exemplar vectors e(k) as shown below:
ll = K∑ k=1 s(k) log pdec(r|e(k))",3.4 Exemplar Decoder Network,[0],[0]
"(4)
Note that we weigh the contribution of each exemplar vector to the final objective based on how similar the corresponding context is to the input context.",3.4 Exemplar Decoder Network,[0],[0]
"Moreover, the similarities are differentiable function of the input and hence, trainable by back propagation.",3.4 Exemplar Decoder Network,[0],[0]
"The model should learn to assign higher similarities to the exemplar contexts, whose responses are helpful for generating the correct response.
",3.4 Exemplar Decoder Network,[0],[0]
The model description uses encoder and decoder networks that can be implemented using any differentiable parametrized architecture.,3.4 Exemplar Decoder Network,[0],[0]
We discuss our choices for the encoders and decoder in the next section.,3.4 Exemplar Decoder Network,[0],[0]
"In this section, we discuss the various encoders and the decoder used by our model.",3.5 The Encoders and Decoder,[0],[0]
The conversation context consists of an ordered sequence of utterances and each utterance can be further viewed as a sequence of words.,3.5 The Encoders and Decoder,[0],[0]
"Thus, context can be viewed as having multiple levels of
hierarchies—at the word level and then at the utterance (sentence) level.",3.5 The Encoders and Decoder,[0],[0]
"We use a hierarchical recurrent encoder—popularly employed as part of the HRED framework for generating responses and query suggestions (Sordoni et al., 2015a; Serban et al., 2016, 2017b).",3.5 The Encoders and Decoder,[0],[0]
The word-level encoder encodes the vector representations of words of an utterance to an utterance vector.,3.5 The Encoders and Decoder,[0],[0]
"Finally, the utterance-level encoder encodes the utterance vectors to a context vector.
",3.5 The Encoders and Decoder,[0],[0]
"Let (u1, . . .",3.5 The Encoders and Decoder,[0],[0]
",uN ) be the utterances present in the context.",3.5 The Encoders and Decoder,[0],[0]
"Furthermore, let (wn1, . . .",3.5 The Encoders and Decoder,[0],[0]
", wnMn) be the words present in the nth utterance for 1 ≤ n ≤",3.5 The Encoders and Decoder,[0],[0]
N .,3.5 The Encoders and Decoder,[0],[0]
"For each word in the utterance, we retrieve its corresponding embedding from an embedding matrix.",3.5 The Encoders and Decoder,[0],[0]
The word embedding for wnm will be denoted as wenm.,3.5 The Encoders and Decoder,[0],[0]
"The encoding of the nth utterance can be computed iteratively as follows:
hnm = f1(hnm−1, wenm), 1 ≤ m ≤Mn (5)
We use an LSTM (Hochreiter and Schmidhuber, 1997) to model the above equation.",3.5 The Encoders and Decoder,[0],[0]
"The last hidden state hnMn is referred to as the utterance encoding and will be denoted as hn.
",3.5 The Encoders and Decoder,[0],[0]
"The utterance-level encoder takes the utterance encodings h1, . . .",3.5 The Encoders and Decoder,[0],[0]
", hN as input and generates the encoding for the context as follows:
cen = f2(cen−1, hn), 1 ≤ n ≤",3.5 The Encoders and Decoder,[0],[0]
"N (6)
",3.5 The Encoders and Decoder,[0],[0]
"Again, we use an LSTM to model the above equation.",3.5 The Encoders and Decoder,[0],[0]
"The last hidden state ceN is referred to as the context embedding and is denoted as ce.
",3.5 The Encoders and Decoder,[0],[0]
A single level LSTM is used for embedding the response.,3.5 The Encoders and Decoder,[0],[0]
"In particular, let (w1, . . .",3.5 The Encoders and Decoder,[0],[0]
", wM ) be the sequence of words present in the response.",3.5 The Encoders and Decoder,[0],[0]
"For each word w, we retrieve the corresponding word embedding we from a word embedding matrix.",3.5 The Encoders and Decoder,[0],[0]
"The response embedding is computed from the word embeddings iteratively as follows:
rem = g(rem−1, wem), 1 ≤ m ≤M (7)
",3.5 The Encoders and Decoder,[0],[0]
"Again, we use an LSTM to model the above equation.",3.5 The Encoders and Decoder,[0],[0]
The last hidden state rem is referred to as the response embedding and is denoted as re.,3.5 The Encoders and Decoder,[0],[0]
"We conduct experiments on Ubuntu Dialogue Corpus (Lowe et al., 2015)(v2.0)2.",4.1.1 Ubuntu Dataset,[0],[0]
Ubuntu dialogue corpus has about 1M context response pairs along with a label.,4.1.1 Ubuntu Dataset,[0],[0]
The label value 1 indicates that the response associated with a context is the correct response and is incorrect otherwise.,4.1.1 Ubuntu Dataset,[0],[0]
As we are only interested in positive labeled data we work with label = 1.,4.1.1 Ubuntu Dataset,[0],[0]
Table 2 depicts some statistics for the dataset.,4.1.1 Ubuntu Dataset,[0],[0]
We also conduct our experiments on a large technical support dataset with more than 127K conversations.,4.1.2 Tech Support Dataset,[0],[0]
We will refer to this dataset as Tech Support dataset in the rest of the paper.,4.1.2 Tech Support Dataset,[0],[0]
"Tech Support dataset contains conversations pertaining to an employee seeking assistance from an agent (technical support) — to resolve problems such as password reset, software installation/licensing, and wireless access.",4.1.2 Tech Support Dataset,[0],[0]
"In contrast to Ubuntu dataset, this dataset has clearly two distinct users — employee and agent.",4.1.2 Tech Support Dataset,[0],[0]
"In our experiments we model the agent responses only.
",4.1.2 Tech Support Dataset,[0],[0]
"For each conversation in the tech support data, we sample context and response pairs to create a dataset similar to the Ubuntu dataset format.",4.1.2 Tech Support Dataset,[0],[0]
Note that multiple context-response pairs can be generated from a single conversation.,4.1.2 Tech Support Dataset,[0],[0]
"For each conversation, we sample 25% of the possible contextresponse pairs.",4.1.2 Tech Support Dataset,[0],[0]
We create validation pairs by selecting 5000 conversations randomly and sampling context response pairs).,4.1.2 Tech Support Dataset,[0],[0]
"Similarly, we create test pairs from a different subset of 5000 conversations.",4.1.2 Tech Support Dataset,[0],[0]
"The remaining conversations are used to
2https://github.com/rkadlec/ ubuntu-ranking-dataset-creator
create training context-response pairs.",4.1.2 Tech Support Dataset,[0],[0]
Table 3 depicts some statistics for this dataset:,4.1.2 Tech Support Dataset,[0],[0]
"The EED and HRED models were implemented using the PyTorch framework (Paszke et al., 2017).",4.2 Model and Training Details,[0],[0]
We initialize the word embedding matrix as well as the weights of context and response encoders from the standard normal distribution with mean 0 and variance 0.01.,4.2 Model and Training Details,[0],[0]
The biases of the encoders and decoder are initialized with 0.,4.2 Model and Training Details,[0],[0]
The word embedding matrix is shared by the context and response encoders.,4.2 Model and Training Details,[0],[0]
"For Ubuntu dataset, we use a word embedding size of 600, whereas the size of the hidden layers of the LSTMs in context and response encoders and the decoder is fixed at 1200.",4.2 Model and Training Details,[0],[0]
"For Tech support dataset, we use a word embedding size of 128.",4.2 Model and Training Details,[0],[0]
"Furthermore, the size of the hidden layers of the multiple LSTMs in context and response encoders and the decoder is fixed at 256.",4.2 Model and Training Details,[0],[0]
"A smaller embedding size was chosen for the Tech Support dataset since we observed much less diversity in the responses of the Tech Support dataset as compared to Ubuntu dataset.
",4.2 Model and Training Details,[0],[0]
Two different encoders are used for encoding the input context (not shown in Figure 1 for simplicity).,4.2 Model and Training Details,[0],[0]
The output of the first context encoder is concatenated with the exemplar response vectors to generate exemplar vectors as detailed in Section 3.3.,4.2 Model and Training Details,[0],[0]
The output of the second context encoder is used to compute the scoring function as detailed in Section 3.4.,4.2 Model and Training Details,[0],[0]
"For each input context, we retrieve 5 similar context-response pairs for Ubuntu dataset and 3 context-response pairs for Tech support dataset using the tf-idf mechanism discussed in Section 3.2.
",4.2 Model and Training Details,[0],[0]
"We use the Adam optimizer (Kingma and Ba, 2014) with a learning rate of 1e",4.2 Model and Training Details,[0],[0]
− 4 for training the model.,4.2 Model and Training Details,[0],[0]
"A batch size of 20 samples was used
during training.",4.2 Model and Training Details,[0],[0]
"In order to prevent overfitting, we use early stopping with log-likelihood on validation set as the stopping criteria.",4.2 Model and Training Details,[0],[0]
"In order to generate the samples using the proposed EED model, we identify the exemplar context that is most similar to the input context based on the learnt scoring function discussed in Section 3.4.",4.2 Model and Training Details,[0],[0]
The corresponding exemplar vector is fed to the decoder to generate the response.,4.2 Model and Training Details,[0],[0]
The samples are generated using a beam search with width 5.,4.2 Model and Training Details,[0],[0]
The average per-word log-likelihood is used to score the beams.,4.2 Model and Training Details,[0],[0]
"A traditional and popular metric used for comparing a generated sentence with a ground truth sentence is BLEU (Papineni et al., 2002) and is frequently used to evaluate machine translation.",5.1.1 Activity and Entity Metrics,[0],[0]
"The metric has also been applied to compute scores for predicted responses in conversations, but it has been found to be less indicative of actual performance (Liu et al., 2016; Sordoni et al., 2015a; Serban et al., 2017a), as it is extremely sensitive to the exact words in the ground truth response, and gives equal importance to stop words/phrases and informative words.
",5.1.1 Activity and Entity Metrics,[0],[0]
Serban et al. (2017a) recently proposed a new set of metrics for evaluating dialogue responses for the Ubuntu corpus.,5.1.1 Activity and Entity Metrics,[0],[0]
"It is important to highlight that these metrics have been specifically designed for the Ubuntu corpus and evaluate a generated response with the ground truth response by comparing the coarse level representation of an utterance (such as entities, activities, Ubuntu OS commands).",5.1.1 Activity and Entity Metrics,[0],[0]
"Here is a brief description of each metric:
• Activity: Activity metric compares the activities present in a predicted response with the ground truth response.",5.1.1 Activity and Entity Metrics,[0],[0]
Activity can be thought of as a verb.,5.1.1 Activity and Entity Metrics,[0],[0]
"Thus, all the verbs in a response are mapped to a set of manually identified list of 192 verbs.
",5.1.1 Activity and Entity Metrics,[0],[0]
•,5.1.1 Activity and Entity Metrics,[0],[0]
Entity:,5.1.1 Activity and Entity Metrics,[0],[0]
This compares the technical entities that overlap with the ground truth response.,5.1.1 Activity and Entity Metrics,[0],[0]
"A total of 3115 technical entities is identified using public resources such as Debian package manager APT.
",5.1.1 Activity and Entity Metrics,[0],[0]
•,5.1.1 Activity and Entity Metrics,[0],[0]
"Tense: This measure compares the time tense of ground truth with predicted response.
",5.1.1 Activity and Entity Metrics,[0],[0]
• Cmd:,5.1.1 Activity and Entity Metrics,[0],[0]
"This metric computes accuracy by comparing commands identified in ground truth utterance with a predicted response.
",5.1.1 Activity and Entity Metrics,[0],[0]
"Table 4 compares our model with other recent generative models (Serban et al., 2017a) — LSTM (Shang et al., 2015), HRED (Serban et al., 2016) & VHRED (Serban et al.,",5.1.1 Activity and Entity Metrics,[0],[0]
"2017b).We do not compare our model with Multi-Resolution RNN (MRNN) (Serban et al., 2017a), as MRNN explicitly utilizes the activities and entities during the generation process.",5.1.1 Activity and Entity Metrics,[0],[0]
"In contrast, the proposed EED model and the other models used for comparison are agnostic to the activity and entity information.",5.1.1 Activity and Entity Metrics,[0],[0]
"We use the standard script3 to compute the metrics.
",5.1.1 Activity and Entity Metrics,[0],[0]
"The EED model scores better than generative models on almost all of the metrics, indicating that we generate more informative responses than other state-of-the-art generative based approaches for Ubuntu corpus.",5.1.1 Activity and Entity Metrics,[0],[0]
"The results show that responses associated with similar contexts may contain the activities and entities present in the ground truth response, and thus help in response generation.",5.1.1 Activity and Entity Metrics,[0],[0]
This is discussed further in Section 5.2.,5.1.1 Activity and Entity Metrics,[0],[0]
"Additionally, we compared our proposed EED with a retrieval only baseline.",5.1.1 Activity and Entity Metrics,[0],[0]
"The retrieval baseline achieves an activity F1 score of 4.23 and entity F1 score of 2.72 compared to 4.87 and 2.99 respectively achieved by our method on the Ubuntu corpus.
",5.1.1 Activity and Entity Metrics,[0],[0]
"The Tech Support dataset is not evaluated using the above metrics, since activity and entity information is not available for this dataset.
",5.1.1 Activity and Entity Metrics,[0],[0]
3https://github.com/julianser/Ubuntu-MultiresolutionTools/blob/master/ActEntRepresentation/eval file.sh,5.1.1 Activity and Entity Metrics,[0],[0]
"Embedding metrics (Lowe et al., 2017) were proposed as an alternative to word by word comparison metrics such as BLEU.",5.1.2 Embedding Metrics,[0],[0]
"We use pre-trained Google news word embeddings4 similar to Serban et al. (2017b), for easy reproducibility as these metrics are sensitive to the word embeddings used.",5.1.2 Embedding Metrics,[0],[0]
"The three metrics of interest utilize the word vectors in ground truth response and a predicted response and are discussed below:
• Average: Average word embedding vectors are computed for the candidate response and ground truth.",5.1.2 Embedding Metrics,[0],[0]
The cosine similarity is computed between these averaged embeddings.,5.1.2 Embedding Metrics,[0],[0]
"High similarity gives as indication that ground truth and predicted response have similar words.
",5.1.2 Embedding Metrics,[0],[0]
"• Greedy: Greedy matching score finds the most similar word in predicted response to ground truth response using cosine similarity.
",5.1.2 Embedding Metrics,[0],[0]
"• Extrema: Vector extrema score computes the maximum or minimum value of each dimension of word vectors in candidate response and ground truth.
",5.1.2 Embedding Metrics,[0],[0]
"Of these, the embedding average metric is the most reflective of performance for our setup.",5.1.2 Embedding Metrics,[0],[0]
"The extrema representation, for instance, is very sensitive to text length and becomes ineffective beyond single length sentences(Forgues et al., 2014).",5.1.2 Embedding Metrics,[0],[0]
We use the publicly available script5 for all our computations.,5.1.2 Embedding Metrics,[0],[0]
"As the test outputs for HRED are not available for Technical Support dataset, we use our
4GoogleNews-vectors-negative300.bin from https:// code.google.com/archive/p/word2vec/
5https://github.com/julianser/ hed-dlg-truncated/blob/master/",5.1.2 Embedding Metrics,[0],[0]
"Evaluation/embedding_metrics.py
own implementation of HRED.",5.1.2 Embedding Metrics,[0],[0]
"Table 5 compares our model with HRED, and depicts that our model scores better on all metrics for Technical Support
dataset, and on majority of the metrics for Ubuntu dataset.
",5.1.2 Embedding Metrics,[0],[0]
"We note that the improvement achieved by the
EED model on activity and entity metrics are much more significant than those on embedding metrics.",5.1.2 Embedding Metrics,[0],[0]
"This suggests that the EED model is better able to capture the specific information (objects and actions) present in the conversations.
",5.1.2 Embedding Metrics,[0],[0]
"Finally, we evaluate the diversity of the generated responses for EED against HRED by counting the number of unique tokens, token-pairs and token-triplets present in the generated responses on Ubuntu and Tech Support dataset.",5.1.2 Embedding Metrics,[0],[0]
The results are shown in Table 6.,5.1.2 Embedding Metrics,[0],[0]
"As can be observed, the responses in EED have a larger number of distinct tokens, token-pairs and token-triplets than HRED, and hence, are arguably more diverse.",5.1.2 Embedding Metrics,[0],[0]
"Table 7 presents the responses generated by HRED, VHRED and the proposed EED for a few selected contexts along with the corresponding similar exemplar responses.",5.2 Qualitative Evaluation,[0],[0]
"As can be observed from the table, the responses generated by EED tend to be more specific to the input context as compared to the responses of HRED and VHRED.",5.2 Qualitative Evaluation,[0],[0]
"For example, in conversations 1 and 2 we find that both HRED and VHRED generate simple generic responses whereas EED generates responses with additional information such as the type of disk partition used or a command not working.",5.2 Qualitative Evaluation,[0],[0]
This is also confirmed by the quantitative results obtained using activity and entity metrics in the previous section.,5.2 Qualitative Evaluation,[0],[0]
We further observe that the exemplar responses contain informative words that are utilized by the EED model for generating the responses as highlighted in Table 7.,5.2 Qualitative Evaluation,[0],[0]
"In this work, we propose a deep learning method, Exemplar Encoder Decoder (EED), that given a conversation context uses similar contexts and corresponding responses from training data for generating a response.",6 Conclusions,[0],[0]
We show that by utilizing this information the system is able to outperform state of the art generative models on publicly available Ubuntu dataset.,6 Conclusions,[0],[0]
"We further show improvements achieved by the proposed method on a large collection of technical support conversations.
",6 Conclusions,[0],[0]
"While in this work, we apply the exemplar encoder decoder network on conversational task, the method is generic and could be used with other tasks such as question answering and machine translation.",6 Conclusions,[0],[0]
"In our future work we plan to extend
the proposed method to these other applications.",6 Conclusions,[0],[0]
We are grateful to the anonymous reviewers for their comments that helped in improving the paper.,Acknowledgements,[0],[0]
"In this paper we present the Exemplar Encoder-Decoder network (EED), a novel conversation model that learns to utilize similar examples from training data to generate responses.",abstractText,[0],[0]
Similar conversation examples (context-response pairs) from training data are retrieved using a traditional TF-IDF based retrieval model.,abstractText,[0],[0]
The retrieved responses are used to create exemplar vectors that are used by the decoder to generate the response.,abstractText,[0],[0]
The contribution of each retrieved response is weighed by the similarity of corresponding context with the input context.,abstractText,[0],[0]
We present detailed experiments on two large data sets and find that our method outperforms state of the art sequence to sequence generative models on several recently proposed evaluation metrics.,abstractText,[0],[0]
We also observe that the responses generated by the proposed EED model are more informative and diverse compared to existing state-of-the-art method.,abstractText,[0],[0]
Exemplar Encoder-Decoder for Neural Conversation Generation,title,[0],[0]
"Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 377–382, Baltimore, Maryland, USA, June 23-25 2014. c©2014 Association for Computational Linguistics",text,[0],[0]
"Training good predictive NLP models typically requires annotated data, but getting professional annotators to build useful data sets is often timeconsuming and expensive.",1 Introduction,[1.0],"['Training good predictive NLP models typically requires annotated data, but getting professional annotators to build useful data sets is often timeconsuming and expensive.']"
"Snow et al. (2008) showed, however, that crowdsourced annotations can produce similar results to annotations made by experts.",1 Introduction,[1.0],"['Snow et al. (2008) showed, however, that crowdsourced annotations can produce similar results to annotations made by experts.']"
"Crowdsourcing services such as Amazon’s Mechanical Turk has since been successfully used for various annotation tasks in NLP (Jha et al., 2010; Callison-Burch and Dredze, 2010).
",1 Introduction,[0.9999999732983762],"['Crowdsourcing services such as Amazon’s Mechanical Turk has since been successfully used for various annotation tasks in NLP (Jha et al., 2010; Callison-Burch and Dredze, 2010).']"
"However, most applications of crowdsourcing in NLP have been concerned with classification problems, such as document classification and constructing lexica (Callison-Burch and Dredze, 2010).",1 Introduction,[1.0],"['However, most applications of crowdsourcing in NLP have been concerned with classification problems, such as document classification and constructing lexica (Callison-Burch and Dredze, 2010).']"
"A large part of NLP problems, however, are structured prediction tasks.",1 Introduction,[0],[0]
"Typically, sequence labeling tasks employ a larger set of labels than classification problems, as well as complex interactions between the annotations.",1 Introduction,[0],[0]
"Disagreement among annotators is therefore potentially higher, and the task of annotating structured data thus harder.
",1 Introduction,[0],[0]
"Only a few recent studies have investigated crowdsourcing sequential tasks; specifically, named entity recognition (Finin et al., 2010; Rodrigues et al., 2013).",1 Introduction,[0],[0]
Results for this are good.,1 Introduction,[0],[0]
"However, named entities typically use only few labels (LOC, ORG, and PER), and the data contains mostly non-entities, so the complexity is manageable.",1 Introduction,[0],[0]
"The question of whether a more linguistically involved structured task like part-of-speech (POS) tagging can be crowdsourced has remained largely unaddressed.1
In this paper, we investigate how well lay annotators can produce POS labels for Twitter data.",1 Introduction,[1.000000023524669],"['The question of whether a more linguistically involved structured task like part-of-speech (POS) tagging can be crowdsourced has remained largely unaddressed.1 In this paper, we investigate how well lay annotators can produce POS labels for Twitter data.']"
"In our setup, we present annotators with one word at a time, with a minimal surrounding context (two words to each side).",1 Introduction,[1.0],"['In our setup, we present annotators with one word at a time, with a minimal surrounding context (two words to each side).']"
"Our choice of annotating Twitter data is not coincidental: with the shortlived nature of Twitter messages, models quickly lose predictive power (Eisenstein, 2013), and retraining models on new samples of more representative data becomes necessary.",1 Introduction,[0],[0]
Expensive professional annotation may be prohibitive for keeping NLP models up-to-date with linguistic and topical changes on Twitter.,1 Introduction,[0],[0]
"We use a minimum of instructions and require few qualifications.
",1 Introduction,[0],[0]
"Obviously, lay annotation is generally less reliable than professional annotation.",1 Introduction,[0],[0]
It is therefore common to aggregate over multiple annotations for the same item to get more robust annotations.,1 Introduction,[0],[0]
"In this paper we compare two aggregation schemes, namely majority voting (MV) and MACE (Hovy et al., 2013).",1 Introduction,[0],[0]
"We also show how we can use Wiktionary, a crowdsourced lexicon, to filter crowdsourced annotations.",1 Introduction,[0],[0]
"We evaluate the annotations in several ways: (a) by testing their accuracy with respect to a gold standard, (b) by evaluating the performance of POS models trained on
1One of the reviewers alerted us to an unpublished masters thesis, which uses pre-annotation to reduce tagging to fewer multiple-choice questions.",1 Introduction,[0],[0]
"See Related Work section for details.
",1 Introduction,[0],[0]
"377
the annotations across several existing data sets, as well as (c) by applying our models in downstream tasks.",1 Introduction,[0],[0]
"We show that with minimal context and annotation effort, we can produce structured annotations of near-expert quality.",1 Introduction,[0],[0]
"We also show that these annotations lead to better POS tagging models than previous models learned from crowdsourced lexicons (Li et al., 2012).",1 Introduction,[0],[0]
"Finally, we show that models learned from these annotations are competitive with models learned from expert annotations on various downstream tasks.",1 Introduction,[0],[0]
We crowdsource the training section of the data from Gimpel et al. (2011)2 with POS tags.,2 Our Approach,[0],[0]
"We use Crowdflower,3 to collect five annotations for each word, and then find the most likely label for each word among the possible annotations.",2 Our Approach,[0],[0]
See Figure 1 for an example.,2 Our Approach,[0],[0]
"If the correct label is not among the annotations, we are unable to recover the correct answer.",2 Our Approach,[0],[0]
This was the case for 1497 instances in our data (cf.,2 Our Approach,[0],[0]
the token “:” in the example).,2 Our Approach,[0],[0]
"We thus report on oracle score, i.e., the best label sequence that could possibly be found, which is correct except for the missing tokens.",2 Our Approach,[0],[0]
"Note that while we report agreement between the crowdsourced annotations and the crowdsourced annotations, our main evaluations are based on models learned from expert vs. crowdsourced annotations and downstream applications thereof (chunking and NER).",2 Our Approach,[0],[0]
We take care in evaluating our models across different data sets to avoid biasing our evaluations to particular annotations.,2 Our Approach,[0],[0]
"All the data sets used in our experiments are publicly available at http://lowlands.ku.dk/results/.
2http://www.ark.cs.cmu.edu/TweetNLP/ 3http://crowdflower.com",2 Our Approach,[0],[0]
"In order to use the annotations to train models that can be applied across various data sets, i.e., making out-of-sample evaluation possible (see Section 5), we follow Hovy et al. (2014) in using the universal tag set (Petrov et al., 2012) with 12 labels.
",3 Crowdsourcing Sequential Annotation,[0.9999999954126366],"['In order to use the annotations to train models that can be applied across various data sets, i.e., making out-of-sample evaluation possible (see Section 5), we follow Hovy et al. (2014) in using the universal tag set (Petrov et al., 2012) with 12 labels.']"
Annotators were given a bold-faced word with two words on either side and asked to select the most appropriate tag from a drop down menu.,3 Crowdsourcing Sequential Annotation,[1.0],['Annotators were given a bold-faced word with two words on either side and asked to select the most appropriate tag from a drop down menu.']
"For each tag, we spell out the name of the syntactic category, and provide a few example words.",3 Crowdsourcing Sequential Annotation,[0],[0]
See Figure 2 for a screenshot of the interface.,3 Crowdsourcing Sequential Annotation,[0],[0]
"Annotators were also told that words can belong to several classes, depending on the context.",3 Crowdsourcing Sequential Annotation,[0],[0]
"No additional guidelines were given.
",3 Crowdsourcing Sequential Annotation,[0],[0]
Only trusted annotators (in Crowdflower: Bronze skills) that had answered correctly on 4 gold tokens (randomly chosen from a set of 20 gold tokens provided by the authors) were allowed to submit annotations.,3 Crowdsourcing Sequential Annotation,[0],[0]
"In total, 177 individual annotators supplied answers.",3 Crowdsourcing Sequential Annotation,[0],[0]
We paid annotators a reward of $0.05 for 10 tokens.,3 Crowdsourcing Sequential Annotation,[1.0],['We paid annotators a reward of $0.05 for 10 tokens.']
"The full data set contains 14,619 tokens.",3 Crowdsourcing Sequential Annotation,[1.0],"['The full data set contains 14,619 tokens.']"
Completion of the task took slightly less than 10 days.,3 Crowdsourcing Sequential Annotation,[0],[0]
Contributors were very satisfied with the task (4.5 on a scale from 1 to 5).,3 Crowdsourcing Sequential Annotation,[0],[0]
"In particular, they felt instructions were clear (4.4/5), and that the pay was reasonable (4.1/5).",3 Crowdsourcing Sequential Annotation,[0],[0]
"After collecting the annotations, we need to aggregate the annotations to derive a single answer for each token.",4 Label Aggregation,[1.0],"['After collecting the annotations, we need to aggregate the annotations to derive a single answer for each token.']"
"In the simplest scheme, we choose the majority label, i.e., the label picked by most annotators.",4 Label Aggregation,[0],[0]
"In case of ties, we select the final label at random.",4 Label Aggregation,[1.0],"['In case of ties, we select the final label at random.']"
"Since this is a stochastic process, we average results over 100 runs.",4 Label Aggregation,[0],[0]
We refer to this as MAJORITY VOTING (MV).,4 Label Aggregation,[0],[0]
Note that in MV we trust all annotators to the same degree.,4 Label Aggregation,[0],[0]
"However, crowdsourcing attracts people with different mo-
tives, and not all of them are equally reliable— even the ones with Bronze level.",4 Label Aggregation,[0],[0]
"Ideally, we would like to factor this into our decision process.
",4 Label Aggregation,[0],[0]
"We use MACE4 (Hovy et al., 2013) as our second scheme to learn both the most likely answer and a competence estimate for each of the annotators.",4 Label Aggregation,[1.0],"['We use MACE4 (Hovy et al., 2013) as our second scheme to learn both the most likely answer and a competence estimate for each of the annotators.']"
"MACE treats annotator competence and the correct answer as hidden variables and estimates their parameters via EM (Dempster et al., 1977).",4 Label Aggregation,[0],[0]
"We use MACE with default parameter settings to give us the weighted average for each annotated example.
",4 Label Aggregation,[0],[0]
"Finally, we also tried applying the joint learning scheme in Rodrigues et al. (2013), but their scheme requires that entire sequences are annotated by the same annotators, which we don’t have, and it expects BIO sequences, rather than POS tags.
",4 Label Aggregation,[0.9999999696276394],"['Finally, we also tried applying the joint learning scheme in Rodrigues et al. (2013), but their scheme requires that entire sequences are annotated by the same annotators, which we don’t have, and it expects BIO sequences, rather than POS tags.']"
"Dictionaries Decoding tasks profit from the use of dictionaries (Merialdo, 1994; Johnson, 2007; Ravi and Knight, 2009) by restricting the number of tags that need to be considered for each word, also known as type constraints (Täckström et al., 2013).",4 Label Aggregation,[1.0],"['Dictionaries Decoding tasks profit from the use of dictionaries (Merialdo, 1994; Johnson, 2007; Ravi and Knight, 2009) by restricting the number of tags that need to be considered for each word, also known as type constraints (Täckström et al., 2013).']"
"We follow Li et al. (2012) in including Wiktionary information as type constraints into our decoding: if a word is found in Wiktionary, we disregard all annotations that are not licensed by the dictionary entry.",4 Label Aggregation,[0],[0]
"If the word is not found in Wiktionary, or if none of its annotations is licensed by Wiktionary, we keep the original annotations.",4 Label Aggregation,[1.0],"['If the word is not found in Wiktionary, or if none of its annotations is licensed by Wiktionary, we keep the original annotations.']"
"Since we aggregate annotations independently (unlike Viterbi decoding), we basically use Wiktionary as a pre-filtering step, such that MV and MACE only operate on the reduced annotations.",4 Label Aggregation,[0],[0]
Each of the two aggregation schemes above produces a final label sequence ŷ for our training corpus.,5 Experiments,[0],[0]
"We evaluate the resulting annotated data in three ways.
1.",5 Experiments,[0],[0]
We compare ŷ to the available expert annotation on the training data.,5 Experiments,[0],[0]
"This tells us how similar lay annotation is to professional annotation.
2.",5 Experiments,[0],[0]
"Ultimately, we want to use structured annotations for supervised training, where annotation quality influences model performance on held-out test data.",5 Experiments,[1.0],"['Ultimately, we want to use structured annotations for supervised training, where annotation quality influences model performance on held-out test data.']"
"To test this, we train a CRF model (Lafferty et al., 2001) with simple orthographic features and word clusters (Owoputi et al., 2013)
4http://www.isi.edu/publications/ licensed-sw/mace/
on the annotated Twitter data described in Gimpel et al. (2011).",5 Experiments,[0.9938929007457392],"['To test this, we train a CRF model (Lafferty et al., 2001) with simple orthographic features and word clusters (Owoputi et al., 2013) on the annotated Twitter data described in Gimpel et al. (2011).']"
"Leaving out the dedicated test set to avoid in-sample bias, we evaluate our models across three data sets: RITTER (the 10% test split of the data in Ritter et al. (2011) used in Derczynski et al. (2013)), the test set from Foster et al. (2011), and the data set described in Hovy et al. (2014).
",5 Experiments,[1.0000000234556352],"['Leaving out the dedicated test set to avoid in-sample bias, we evaluate our models across three data sets: RITTER (the 10% test split of the data in Ritter et al. (2011) used in Derczynski et al. (2013)), the test set from Foster et al. (2011), and the data set described in Hovy et al. (2014).']"
We will make the preprocessed data sets available to the public to facilitate comparison.,5 Experiments,[1.0],['We will make the preprocessed data sets available to the public to facilitate comparison.']
"In addition to a supervised model trained on expert annotations, we compare our tagging accuracy with that of a weakly supervised system (Li et al., 2012) re-trained on 400,000 unlabeled tweets to adapt to Twitter, but using a crowdsourced lexicon, namely Wiktionary, to constrain inference.",5 Experiments,[1.0],"['In addition to a supervised model trained on expert annotations, we compare our tagging accuracy with that of a weakly supervised system (Li et al., 2012) re-trained on 400,000 unlabeled tweets to adapt to Twitter, but using a crowdsourced lexicon, namely Wiktionary, to constrain inference.']"
"We use parameter settings from Li et al. (2012), as well as their Wikipedia dump, available from their project website.5
3.",5 Experiments,[0],[0]
"POS tagging is often the first step for further analysis, such as chunking, parsing, etc.",5 Experiments,[0],[0]
We test the downstream performance of the POS models from the previous step on chunking and NER.,5 Experiments,[0],[0]
"We use the models to annotate the training data portion of each task with POS tags, and use them as features in a chunking and NER model.",5 Experiments,[1.0],"['We use the models to annotate the training data portion of each task with POS tags, and use them as features in a chunking and NER model.']"
"For both tasks, we train a CRF model on the respective (POS-augmented) training set, and evaluate it on several held-out test sets.",5 Experiments,[0],[0]
"For chunking, we use the test sets from Foster et al. (2011) and Ritter et al. (2011) (with the splits from Derczynski et al. (2013)).",5 Experiments,[0],[0]
"For NER, we use data from Finin et al. (2010) and again Ritter et al. (2011).",5 Experiments,[1.0],"['For NER, we use data from Finin et al. (2010) and again Ritter et al. (2011).']"
"For chunking, we follow Sha and Pereira (2003) for the set of features, including token and POS information.",5 Experiments,[0],[0]
"For NER, we use standard features, including POS tags (from the previous experiments), indicators for hyphens, digits, single quotes, upper/lowercase, 3-character prefix and suffix information, and Brown word cluster features6 with 2,4,8,16 bitstring prefixes estimated from a large Twitter corpus (Owoputi et al., 2013).",5 Experiments,[0],[0]
We report macro-averages over all these data sets.,5 Experiments,[0],[0]
Agreement with expert annotators Table 1 shows the accuracy of each aggregation compared to the gold labels.,6 Results,[0],[0]
"The crowdsourced annotations
5https://code.google.com/p/ wikily-supervised-pos-tagger/
6http://www.ark.cs.cmu.edu/TweetNLP/
aggregated using MV agree with the expert annotations in 79.54% of the cases.",6 Results,[0],[0]
"If we pre-filter the data using Wiktionary, the agreement becomes 80.58%.",6 Results,[0],[0]
MACE leads to higher agreement with expert annotations under both conditions (79.89 and 80.75).,6 Results,[0],[0]
"The small difference indicates that annotators are consistent and largely reliable, thus confirming the Bronze-level qualification we required.",6 Results,[0],[0]
"Both schemes cannot recover the correct answer for the 1497 cases where none of the crowdsourced labels matched the gold label, i.e. y /∈ Zi.",6 Results,[1.0],"['Both schemes cannot recover the correct answer for the 1497 cases where none of the crowdsourced labels matched the gold label, i.e. y /∈ Zi.']"
"The best possible result either of them could achieve (the oracle) would be matching all but the missing labels, an agreement of 89.63%.
",6 Results,[1.0000000467197534],"['The best possible result either of them could achieve (the oracle) would be matching all but the missing labels, an agreement of 89.63%.']"
Most of the cases where the correct label was not among the annotations belong to a small set of confusions.,6 Results,[1.0],['Most of the cases where the correct label was not among the annotations belong to a small set of confusions.']
"The most frequent was mislabeling “:” and “. . .”, both mapped to X. Annotators mostly decided to label these tokens as punctuation (.).",6 Results,[0],[0]
"They also predominantly labeled your, my and this as PRON (for the former two), and a variety of labels for the latter, when the gold label is DET.
",6 Results,[0],[0]
"Effect on POS Tagging Accuracy Usually, we don’t want to match a gold standard, but we rather want to create new annotated training data.",6 Results,[0],[0]
"Crowdsourcing matches our gold standard to about 80%, but the question remains how useful this data is when training models on it.",6 Results,[1.0],"['Crowdsourcing matches our gold standard to about 80%, but the question remains how useful this data is when training models on it.']"
"After all, inter-annotator agreement among professional an-
notators on this task is only around 90% (Gimpel et al., 2011; Hovy et al., 2014).",6 Results,[0.999999992497848],"['After all, inter-annotator agreement among professional an- notators on this task is only around 90% (Gimpel et al., 2011; Hovy et al., 2014).']"
"In order to evaluate how much each aggregation scheme influences tagging performance of the resulting model, we train separate models on each scheme’s annotations and test on the same four data sets.",6 Results,[0],[0]
Table 2 shows the results.,6 Results,[0],[0]
Note that the differences between the four schemes are insignificant.,6 Results,[0],[0]
"More importantly, however, POS tagging accuracy using crowdsourced annotations are on average only 2.6% worse than gold using professional annotations.",6 Results,[0],[0]
"On the other hand, performance is much better than the weakly supervised approach by Li et al. (2012), which only relies on a crowdsourced POS lexicon.
",6 Results,[0],[0]
Downstream Performance Table 3 shows the accuracy when using the POS models trained in the previous evaluation step.,6 Results,[0],[0]
Note that we present the average over the two data sets used for each task.,6 Results,[0],[0]
Note also how the Wiktionary constraints lead to improvements in downstream performance.,6 Results,[0],[0]
"In chunking, we see that using the crowdsourced annotations leads to worse performance than using the professional annotations.",6 Results,[0],[0]
"For NER, however, we find that some of the POS taggers trained on aggregated data produce better NER performance than POS taggers trained on expert-annotated gold data.",6 Results,[0],[0]
"Since the only difference between models are the respective POS features, the results suggest that at least for some tasks, POS taggers learned from crowdsourced annotations may be as good as those learned from expert annotations.",6 Results,[0],[0]
"There is considerable work in the literature on modeling answer correctness and annotator competence as latent variables (Dawid and Skene,
1979; Smyth et al., 1995; Carpenter, 2008; Whitehill et al., 2009; Welinder et al., 2010; Yan et al., 2010; Raykar and Yu, 2012).",7 Related Work,[0],[0]
Rodrigues et al. (2013) recently presented a sequential model for this.,7 Related Work,[0],[0]
They estimate annotator competence as latent variables in a CRF model using EM.,7 Related Work,[0],[0]
"They evaluate their approach on synthetic and NER data annotated on Mechanical Turk, showing improvements over the MV baselines and the multi-label model by Dredze et al. (2009).",7 Related Work,[0],[0]
"The latter do not model annotator reliability but rather model label priors by integrating them into the CRF objective, and re-estimating them during learning.",7 Related Work,[0],[0]
"Both require annotators to supply a full sentence, while we use minimal context, which requires less annotator commitment and makes the task more flexible.",7 Related Work,[0],[0]
"Unfortunately, we could not run those models on our data due to label incompatibility and the fact that we typically do not have complete sequences annotated by the same annotators.
",7 Related Work,[0],[0]
Mainzer (2011) actually presents an earlier paper on crowdsourcing POS tagging.,7 Related Work,[0],[0]
"However, it differs from our approach in several ways.",7 Related Work,[0],[0]
It uses the Penn Treebank tag set to annotate Wikipedia data (which is much more canonical than Twitter) via a Java applet.,7 Related Work,[0],[0]
"The applet automatically labels certain categories, and only presents the users with a series of multiple choice questions for the remainder.",7 Related Work,[0],[0]
"This is highly effective, as it eliminates some sources of possible disagreement.",7 Related Work,[0],[0]
"In contrast, we do not pre-label any tokens, but always present the annotators with all labels.",7 Related Work,[0],[0]
We use crowdsourcing to collect POS annotations with minimal context (five-word windows).,8 Conclusion,[0],[0]
"While the performance of POS models learned from this data is still slightly below that of models trained on expert annotations, models learned from aggregations approach oracle performance for POS tagging.",8 Conclusion,[0],[0]
"In general, we find that the use of a dictionary tends to make aggregations more useful, irrespective of aggregation method.",8 Conclusion,[0],[0]
"For some downstream tasks, models using the aggregated POS tags perform even better than models using expert-annotated tags.",8 Conclusion,[0],[0]
We would like to thank the anonymous reviewers for valuable comments and feedback.,Acknowledgments,[0],[0]
"This research is funded by the ERC Starting Grant LOW-
LANDS",Acknowledgments,[0],[0]
No. 313695.,Acknowledgments,[0],[0]
Crowdsourcing lets us collect multiple annotations for an item from several annotators.,abstractText,[0],[0]
"Typically, these are annotations for non-sequential classification tasks.",abstractText,[0],[0]
"While there has been some work on crowdsourcing named entity annotations, researchers have largely assumed that syntactic tasks such as part-of-speech (POS) tagging cannot be crowdsourced.",abstractText,[0],[0]
This paper shows that workers can actually annotate sequential data almost as well as experts.,abstractText,[0],[0]
"Further, we show that the models learned from crowdsourced annotations fare as well as the models learned from expert annotations in downstream tasks.",abstractText,[0],[0]
Experiments with crowdsourced re-annotation of a POS tagging data set,title,[0],[0]
