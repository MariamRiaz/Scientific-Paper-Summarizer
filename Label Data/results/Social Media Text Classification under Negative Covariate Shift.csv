0,1,label2,summary_sentences
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1914–1925 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
1914",text,[0],[0]
Deep learning models work best when trained on large amounts of labeled data.,1 Introduction,[0],[0]
"However, acquiring labels is costly, motivating the need for effective semi-supervised learning techniques that leverage unlabeled examples.",1 Introduction,[0],[0]
"A widely successful semi-supervised learning strategy for neural NLP is pre-training word vectors (Mikolov et al., 2013).",1 Introduction,[0],[0]
"More recent work trains a Bi-LSTM sentence encoder to do language modeling and then incorporates its context-sensitive representations into supervised models (Dai and Le, 2015; Peters et al.,
1Code will be made available at https: //github.com/tensorflow/models/tree/ master/research/cvt_text
2018).",1 Introduction,[0],[0]
"Such pre-training methods perform unsupervised representation learning on a large corpus of unlabeled data followed by supervised training.
",1 Introduction,[0],[0]
A key disadvantage of pre-training is that the first representation learning phase does not take advantage of labeled data – the model attempts to learn generally effective representations rather than ones that are targeted towards a particular task.,1 Introduction,[0],[0]
Older semi-supervised learning algorithms like self-training do not suffer from this problem because they continually learn about a task on a mix of labeled and unlabeled data.,1 Introduction,[0],[0]
"Selftraining has historically been effective for NLP (Yarowsky, 1995; McClosky et al., 2006), but is less commonly used with neural models.",1 Introduction,[0],[0]
"This paper presents Cross-View Training (CVT), a new self-training algorithm that works well for neural sequence models.
",1 Introduction,[0],[0]
"In self-training, the model learns as normal on labeled examples.",1 Introduction,[0],[0]
"On unlabeled examples, the model acts as both a teacher that makes predictions about the examples and a student that is trained on those predictions.",1 Introduction,[0],[0]
"Although this process has shown value for some tasks, it is somewhat tautological: the model already produces the predictions it is being trained on.",1 Introduction,[0],[0]
"Recent research on computer vision addresses this by adding noise to the student’s input, training the model so it is robust to input perturbations (Sajjadi et al., 2016; Wei et al., 2018).",1 Introduction,[0],[0]
"However, applying noise is difficult for discrete inputs like text.
",1 Introduction,[0],[0]
"As a solution, we take inspiration from multiview learning (Blum and Mitchell, 1998; Xu et al., 2013) and train the model to produce consistent predictions across different views of the input.",1 Introduction,[0.950752365935201],"['The way to compute each center ci is similar to that in the Rocchio relevance feedback method in information retrieval (Rocchio, 1971; Manning et al. 2008), which uses the corresponding ds-vectors of all training positive and negative documents.']"
"Instead of only training the full model as a student, CVT adds auxiliary prediction modules – neural networks that transform vector representations into predictions – to the model and also trains them as students.",1 Introduction,[0],[0]
"The input to each student prediction module is a subset of the model’s intermediate rep-
resentations corresponding to a restricted view of the input example.",1 Introduction,[0],[0]
"For example, one auxiliary prediction module for sequence tagging is attached to only the “forward” LSTM in the model’s first BiLSTM layer, so it makes predictions without seeing any tokens to the right of the current one.
",1 Introduction,[0],[0]
CVT works by improving the model’s representation learning.,1 Introduction,[0],[0]
"The auxiliary prediction modules can learn from the full model’s predictions because the full model has a better, unrestricted view of the input.",1 Introduction,[0],[0]
"As the auxiliary modules learn to make accurate predictions despite their restricted views of the input, they improve the quality of the representations they are built on top of.",1 Introduction,[0],[0]
"This in turn improves the full model, which uses the same shared representations.",1 Introduction,[0],[0]
"In short, our method combines the idea of representation learning on unlabeled data with classic self-training.
",1 Introduction,[0],[0]
"CVT can be applied to a variety of tasks and neural architectures, but we focus on sequence modeling tasks where the prediction modules are attached to a shared Bi-LSTM encoder.",1 Introduction,[0],[0]
"We propose auxiliary prediction modules that work well for sequence taggers, graph-based dependency parsers, and sequence-to-sequence models.",1 Introduction,[0],[0]
"We evaluate our approach on English dependency parsing, combinatory categorial grammar supertagging, named entity recognition, partof-speech tagging, and text chunking, as well as English to Vietnamese machine translation.",1 Introduction,[0],[0]
CVT improves over previously published results on all these tasks.,1 Introduction,[0],[0]
"Furthermore, CVT can easily and effectively be combined with multi-task learning: we just add additional prediction modules for the different tasks on top of the shared Bi-LSTM encoder.",1 Introduction,[0],[0]
Training a unified model to jointly perform all of the tasks except machine translation improves results (outperforming a multi-task ELMo model) while decreasing the total training time.,1 Introduction,[0],[0]
We first present Cross-View Training and describe how it can be combined effectively with multi-task learning.,2 Cross-View Training,[0],[0]
See Figure 1 for an overview of the training method.,2 Cross-View Training,[0],[0]
"Let Dl = {(x1, y1), (x2, y2), ..., (xN , yN )} represent a labeled dataset and Dul = {x1, x2, ..., xM} represent an unlabeled dataset We use pθ(y|xi) to denote the output distribution over classes pro-
duced by the model with parameters θ on input xi.",2.1 Method,[0],[0]
"During CVT, the model alternates learning on a minibatch of labeled examples and learning on a minibatch of unlabeled examples.",2.1 Method,[0],[0]
"For labeled examples, CVT uses standard cross-entropy loss:
Lsup(θ) = 1 |Dl| ∑
xi,yi∈Dl
CE(yi, pθ(y|xi))
",2.1 Method,[0],[0]
"CVT adds k auxiliary prediction modules to the model, which are used when learning on unlabeled examples.",2.1 Method,[0],[0]
"A prediction module is usually a small neural network (e.g., a hidden layer followed by a softmax layer).",2.1 Method,[0],[0]
"Each one takes as input an intermediate representation hj(xi) produced by the model (e.g., the outputs of one of the LSTMs in a Bi-LSTM model).",2.1 Method,[0],[0]
It outputs a distribution over labels pjθ(y|xi).,2.1 Method,[0],[0]
"Each h
j is chosen such that it only uses a part of the input xi; the particular choice
can depend on the task and model architecture.",2.1 Method,[0],[0]
We propose variants for several tasks in Section 3.,2.1 Method,[0],[0]
"The auxiliary prediction modules are only used during training; the test-time prediction come from the primary prediction module that produces pθ.
",2.1 Method,[0],[0]
"On an unlabeled example, the model first produces soft targets pθ(y|xi) by performing inference.",2.1 Method,[0],[0]
"CVT trains the auxiliary prediction modules to match the primary prediction module on the unlabeled data by minimizing LCVT(θ) = 1|Dul| ∑ xi∈Dul ∑k j=1D(pθ(y|xi), p j θ(y|xi))
where D is a distance function between probability distributions (we use KL divergence).",2.1 Method,[0],[0]
"We hold the primary module’s prediction pθ(y|xi) fixed during training (i.e., we do not back-propagate through it) so the auxiliary modules learn to imitate the primary one, but not vice versa.",2.1 Method,[0],[0]
CVT works by enhancing the model’s representation learning.,2.1 Method,[0],[0]
"As the auxiliary modules train, the representations they take as input improve so they are useful for making predictions even when some of the model’s inputs are not available.",2.1 Method,[0],[0]
"This in turn improves the primary prediction module, which is built on top of the same shared representations.
",2.1 Method,[0],[0]
"We combine the supervised and CVT losses into the total loss, L = Lsup + LCVT, and minimize it with stochastic gradient descent.",2.1 Method,[0],[0]
"In particular, we alternate minimizing Lsup over a minibatch of labeled examples and minimizing LCVT over a minibatch of unlabeled examples.
",2.1 Method,[0],[0]
"For most neural networks, adding a few additional prediction modules is computationally cheap compared to the portion of the model building up representations (such as an RNN or CNN).",2.1 Method,[0],[0]
Therefore our method contributes little overhead to training time over other self-training approaches for most tasks.,2.1 Method,[0],[0]
CVT does not change inference time or the number of parameters in the fullytrained model because the auxiliary prediction modules are only used during training.,2.1 Method,[0],[0]
CVT can easily be combined with multi-task learning by adding additional prediction modules for the other tasks on top of the shared Bi-LSTM encoder.,2.2 Combining CVT with Multi-Task Learning,[0],[0]
"During supervised learning, we randomly select a task and then update Lsup using a minibatch of labeled data for that task.",2.2 Combining CVT with Multi-Task Learning,[0],[0]
"When learning on the unlabeled data, we optimize LCVT
jointly across all tasks at once, first running inference with all the primary prediction modules and then learning from the predictions with all the auxiliary prediction modules.",2.2 Combining CVT with Multi-Task Learning,[0],[0]
"As before, the model alternates training on minibatches of labeled and unlabeled examples.
",2.2 Combining CVT with Multi-Task Learning,[0],[0]
"Examples labeled across many tasks are useful for multi-task systems to learn from, but most datasets are only labeled with one task.",2.2 Combining CVT with Multi-Task Learning,[0],[0]
A benefit of multi-task CVT is that the model creates (artificial) all-tasks-labeled examples from unlabeled data.,2.2 Combining CVT with Multi-Task Learning,[0],[0]
This significantly improves the model’s data efficiency and training time.,2.2 Combining CVT with Multi-Task Learning,[0],[0]
"Since running prediction modules is computationally cheap, computing LCVT is not much slower for many tasks than it is for a single one.",2.2 Combining CVT with Multi-Task Learning,[0],[0]
"However, we find the all-tasks-labeled examples substantially speed up model convergence.",2.2 Combining CVT with Multi-Task Learning,[0],[0]
"For example, our model trained on six tasks takes about three times as long to converge as the average model trained on one task, a 2x decrease in total training time.",2.2 Combining CVT with Multi-Task Learning,[0],[0]
CVT relies on auxiliary prediction modules that have restricted views of the input.,3 Cross-View Training Models,[0],[0]
"In this section, we describe specific constructions of the auxiliary prediction modules that are effective for sequence tagging, dependency parsing, and sequence-tosequence learning.",3 Cross-View Training Models,[0],[0]
"All of our models use a two-layer CNN-BiLSTM (Chiu and Nichols, 2016; Ma and Hovy, 2016) sentence encoder.",3.1 Bi-LSTM Sentence Encoder,[0],[0]
It takes as input a sequence of words xi =,3.1 Bi-LSTM Sentence Encoder,[0],[0]
"[x1i , x 2 i , ..., x T i ].",3.1 Bi-LSTM Sentence Encoder,[0],[0]
"First, each word is represented as the sum of an embedding vector and the output of a character-level Convolutional Neural Network, resulting in a sequence of vectors v =",3.1 Bi-LSTM Sentence Encoder,[0],[0]
"[v1, v2, ..., vT ].",3.1 Bi-LSTM Sentence Encoder,[0],[0]
"The encoder applies a twolayer bidirectional LSTM (Graves and Schmidhuber, 2005) to these representations.",3.1 Bi-LSTM Sentence Encoder,[0],[0]
"The first layer runs a Long Short-Term Memory unit (Hochreiter and Schmidhuber, 1997) in the forward direction (taking vt as input at each step t) and the backward direction (taking vT−t+1 at each step) to produce vector sequences [ −→ h 11, −→ h 21, ...",3.1 Bi-LSTM Sentence Encoder,[0],[0]
"−→ h T1 ] and [ ←− h 11, ←− h 21, ...",3.1 Bi-LSTM Sentence Encoder,[0],[0]
←− h T1 ].,3.1 Bi-LSTM Sentence Encoder,[0],[0]
The output of the Bi-LSTM is the concatenation of these vectors: h1 =,3.1 Bi-LSTM Sentence Encoder,[0],[0]
"[ −→ h 11 ⊕←−
h 11, ..., −→ h T1 ⊕ ←−",3.1 Bi-LSTM Sentence Encoder,[0],[0]
h T1 ].,3.1 Bi-LSTM Sentence Encoder,[0],[0]
"The second Bi-LSTM layer
works the same, producing outputs h2, except it takes h1 as input instead of v.",3.1 Bi-LSTM Sentence Encoder,[0],[0]
"In sequence tagging, each token xti has a corresponding label yti .",3.2 CVT for Sequence Tagging,[0],[0]
"The primary prediction module for sequence tagging produces a probability distribution over classes for the tth label using a onehidden-layer neural network applied to the corresponding encoder outputs:
p(yt|xi) = NN(ht1 ⊕ ht2) = softmax(U · ReLU(W (ht1 ⊕ ht2))",3.2 CVT for Sequence Tagging,[0],[0]
"+ b)
",3.2 CVT for Sequence Tagging,[0],[0]
"The auxiliary prediction modules take −→ h 1(xi) and ←− h 1(xi), the outputs of the forward and backward LSTMs in the first2 Bi-LSTM layer, as inputs.",3.2 CVT for Sequence Tagging,[0],[0]
"We add the following four auxiliary prediction modules to the model (see Figure 2):
pfwdθ (y t|xi)",3.2 CVT for Sequence Tagging,[0],[0]
= NNfwd( −→ h t1(xi)),3.2 CVT for Sequence Tagging,[0],[0]
pbwdθ (y t|xi) =,3.2 CVT for Sequence Tagging,[0],[0]
"NNbwd( ←− h t1(xi))
",3.2 CVT for Sequence Tagging,[0],[0]
pfutureθ,3.2 CVT for Sequence Tagging,[0],[0]
"(y t|xi) = NNfuture( −→ h t−11 (xi))
",3.2 CVT for Sequence Tagging,[0],[0]
p past θ (y t|xi) =,3.2 CVT for Sequence Tagging,[0],[0]
NNpast( ←− h,3.2 CVT for Sequence Tagging,[0],[0]
"t+11 (xi))
The “forward” module makes each prediction without seeing the right context of the current token.",3.2 CVT for Sequence Tagging,[0],[0]
The “future” module makes each prediction without the right context or the current token itself.,3.2 CVT for Sequence Tagging,[0],[0]
"Therefore it works like a neural language model that, instead of predicting which token comes next in the sequence, predicts which class of token comes next.",3.2 CVT for Sequence Tagging,[0],[0]
The “backward” and “past” modules are analogous.,3.2 CVT for Sequence Tagging,[0],[0]
"In a dependency parse, words in a sentence are treated as nodes in a graph.",3.3 CVT for Dependency Parsing,[0],[0]
"Typed directed edges connect the words, forming a tree structure describing the syntactic structure of the sentence.",3.3 CVT for Dependency Parsing,[0],[0]
"In particular, each word xti in a sentence",3.3 CVT for Dependency Parsing,[0],[0]
"xi = x 1 i , ..., x T i receives exactly one in-going edge (u, t, r) going from word xui (called the “head”) to it (the “dependent”) of type r (the “relation”).",3.3 CVT for Dependency Parsing,[0],[0]
We use a graph-based dependency parser similar to the one from Dozat and Manning (2017).,3.3 CVT for Dependency Parsing,[0],[0]
This treats dependency parsing as a classification task where the goal is to predict which in-going edge yti =,3.3 CVT for Dependency Parsing,[0],[0]
"(u, t, r) connects to each word x t i.
First, the representations produced by the encoder for the candidate head and dependent are
2Modules taking inputs from the second Bi-LSTM layer would not have restricted views because information about the whole sentence gets propagated through the first layer.
",3.3 CVT for Dependency Parsing,[0],[0]
"LSTM LSTM ŷfuture  ŷfwd  ŷ   ŷbwd  ŷpast  Backward LSTM Forward LSTM Predict LSTM LSTM LSTM LSTM Auxiliary Prediction Modules Primary Prediction Module
passed through separate hidden layers.",3.3 CVT for Dependency Parsing,[0],[0]
A bilinear classifier applied to these representations produces a score for each candidate edge.,3.3 CVT for Dependency Parsing,[0],[0]
"Lastly, these scores are passed through a softmax layer to produce probabilities.",3.3 CVT for Dependency Parsing,[0],[0]
"Mathematically, the probability of an edge is given as:
pθ((u, t, r)|xi) ∝",3.3 CVT for Dependency Parsing,[0],[0]
"es(h u 1 (xi)⊕hu2 (xi),ht1(xi)⊕ht2(xi),r)
where s is the scoring function:
s(z1, z2, r) = ReLU(Wheadz1 + bhead)(Wr",3.3 CVT for Dependency Parsing,[0],[0]
"+W )
",3.3 CVT for Dependency Parsing,[0],[0]
"ReLU(Wdepz2 + bdep)
",3.3 CVT for Dependency Parsing,[0],[0]
The bilinear classifier uses a weight matrix Wr specific to the candidate relation as well as a weight matrix W shared across all relations.,3.3 CVT for Dependency Parsing,[0],[0]
"Note that unlike in most prior work, our dependency parser only takes words as inputs, not words and part-of-speech tags.
",3.3 CVT for Dependency Parsing,[0],[0]
"We add four auxiliary prediction modules to our model for cross-view training:
pfwd-fwdθ ((u, t, r)|xi) ∝",3.3 CVT for Dependency Parsing,[0],[0]
"es fwd-fwd(
−→ h u1 (xi), −→ h t1(xi),r)
pfwd-bwdθ ((u, t, r)|xi) ∝",3.3 CVT for Dependency Parsing,[0],[0]
"es fwd-bwd(
−→ h u1 (xi), ←− h t1(xi),r)
pbwd-fwdθ ((u, t, r)|xi) ∝",3.3 CVT for Dependency Parsing,[0],[0]
"es bwd-fwd(
←− h u1 (xi), −→ h t1(xi),r)
pbwd-bwdθ ((u, t, r)|xi) ∝",3.3 CVT for Dependency Parsing,[0],[0]
"es bwd-bwd(
←− h u1 (xi), ←− h t1(xi),r)
",3.3 CVT for Dependency Parsing,[0],[0]
Each one has some missing context (not seeing either the preceding or following words) for the candidate head and candidate dependent.,3.3 CVT for Dependency Parsing,[0],[0]
"We use an encoder-decoder sequence-to-sequence model with attention (Sutskever et al., 2014; Bahdanau et al., 2015).",3.4 CVT for Sequence-to-Sequence Learning,[0],[0]
"Each example consists of an input (source) sequence xi = x1i , ..., x T i and output (target) sequence yi = y1i , ..., y K i .",3.4 CVT for Sequence-to-Sequence Learning,[0],[0]
"The encoder’s representations are passed into an LSTM decoder using a bilinear attention mechanism (Luong et al., 2015).",3.4 CVT for Sequence-to-Sequence Learning,[0],[0]
"In particular, at each time step t the decoder computes an attention distribution over source sequence hidden states as αj ∝ eh
jWαh̄t where h̄t is the decoder’s current hidden state.",3.4 CVT for Sequence-to-Sequence Learning,[0],[0]
"The source hidden states weighted by the attention distribution form a context vector: ct = ∑ j αjh
j .",3.4 CVT for Sequence-to-Sequence Learning,[0],[0]
"Next, the context vector and current hidden state are combined into an attention vector at = tanh(Wa[ct, ht]).",3.4 CVT for Sequence-to-Sequence Learning,[0],[0]
"Lastly, a softmax layer predicts the next token in the output sequence: p(yti |y<ti , xi) = softmax(Wsat).
",3.4 CVT for Sequence-to-Sequence Learning,[0],[0]
We add two auxiliary decoders when applying CVT.,3.4 CVT for Sequence-to-Sequence Learning,[0],[0]
"The auxiliary decoders share embedding and LSTM parameters with the primary decoder, but have different parameters for the attention mechanisms and softmax layers.",3.4 CVT for Sequence-to-Sequence Learning,[0],[0]
"For the first one, we restrict its view of the input by applying attention dropout, randomly zeroing out a fraction of its attention weights.",3.4 CVT for Sequence-to-Sequence Learning,[0],[0]
"The second one is trained to predict the next word in the target sequence rather than the current one: pfutureθ (y t i |y<ti , xi) = softmax(W futures a future t−1 ).",3.4 CVT for Sequence-to-Sequence Learning,[0],[0]
"Since there is no target sequence for unlabeled examples, we cannot apply teacher forcing to get an output distribution over the vocabulary from the primary decoder at each time step.",3.4 CVT for Sequence-to-Sequence Learning,[0],[0]
"Instead, we produce hard targets for the auxiliary modules by running the primary decoder with beam search on the input sequence.",3.4 CVT for Sequence-to-Sequence Learning,[0],[0]
This idea has previously been applied to sequence-level knowledge distillation by Kim and Rush (2016).,3.4 CVT for Sequence-to-Sequence Learning,[0],[0]
"We compare Cross-View Training against several strong baselines on seven tasks:
Combinatory Categorial Grammar (CCG)",4 Experiments,[0],[0]
"Supertagging: We use data from CCGBank (Hockenmaier and Steedman, 2007).
",4 Experiments,[0],[0]
"Text Chunking: We use the CoNLL-2000 data (Tjong Kim Sang and Buchholz, 2000).
",4 Experiments,[0],[0]
"Named Entity Recognition (NER): We use the CoNLL-2003 data (Tjong Kim Sang and De Meulder, 2003).
",4 Experiments,[0],[0]
"Fine-Grained NER (FGN): We use the OntoNotes (Hovy et al., 2006) dataset.
",4 Experiments,[0],[0]
Part-of-Speech (POS),4 Experiments,[0],[0]
"Tagging: We use the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1993).
",4 Experiments,[0],[0]
"Dependency Parsing: We use the Penn Treebank converted to Stanford Dependencies version 3.3.0.
Machine Translation: We use the EnglishVietnamese translation dataset from IWSLT 2015 (Cettolo et al., 2015).",4 Experiments,[0],[0]
"We report (tokenized) BLEU scores on the tst2013 test set.
",4 Experiments,[0],[0]
"We use the 1 Billion Word Language Model Benchmark (Chelba et al., 2014) as a pool of unlabeled sentences for semi-supervised learning.",4 Experiments,[0],[0]
"We apply dropout during training, but not when running the primary prediction module to produce soft targets on unlabeled examples.",4.1 Model Details and Baselines,[0],[0]
"In addition to the auxiliary prediction modules listed in Section 3, we find it slightly improves results to add another one that sees the whole input rather than a subset (but unlike the primary prediction module, does have dropout applied to its representations).",4.1 Model Details and Baselines,[0],[0]
"Unless indicated otherwise, our models have LSTMs with 1024-sized hidden states and 512-sized projection layers.",4.1 Model Details and Baselines,[0],[0]
See the supplementary material for full training details and hyperparameters.,4.1 Model Details and Baselines,[0],[0]
"We compare CVT with the following other semi-supervised learning algorithms:
Word Dropout.",4.1 Model Details and Baselines,[0],[0]
"In this method, we only train the primary prediction module.",4.1 Model Details and Baselines,[0],[0]
"When acting as a teacher it is run as normal, but when acting as a student, we randomly replace some of the input words with a REMOVED token.",4.1 Model Details and Baselines,[0],[0]
This is similar to CVT in that it exposes the model to a restricted view of the input.,4.1 Model Details and Baselines,[0],[0]
"However, it is less data efficient.",4.1 Model Details and Baselines,[0],[0]
"By carefully designing the auxiliary prediction modules, it is possible to train the auxiliary prediction modules to match the primary one across many different views of the input a once, rather than just one view at a time.
",4.1 Model Details and Baselines,[0],[0]
Virtual Adversarial Training (VAT).,4.1 Model Details and Baselines,[0],[0]
"VAT (Miyato et al., 2016) works like word dropout, but adds noise to the word embeddings of the student instead of dropping out words.",4.1 Model Details and Baselines,[0],[0]
"Notably, the noise is chosen adversarially so it most changes the model’s prediction.",4.1 Model Details and Baselines,[0],[0]
"This method was applied successfully to semi-supervised text classification
by Miyato et al. (2017).
ELMo.",4.1 Model Details and Baselines,[0],[0]
ELMo incorporates the representations from a large separately-trained language model into a task-specific model.,4.1 Model Details and Baselines,[0],[0]
Our implementaiton follows Peters et al. (2018).,4.1 Model Details and Baselines,[0],[0]
"When combining ELMo with multi-task learning, we allow each task to learn its own weights for the ELMo embeddings going into each prediction module.",4.1 Model Details and Baselines,[0],[0]
We found applying dropout to the ELMo embeddings was crucial for achieving good performance.,4.1 Model Details and Baselines,[0],[0]
Results are shown in Table 1.,4.2 Results,[0],[0]
CVT on its own outperforms or is comparable to the best previously published results on all tasks.,4.2 Results,[0],[0]
"Figure 3 shows an example win for CVT over supervised learning.
",4.2 Results,[0],[0]
"Of the prior results listed in Table 1, only TagLM and ELMo are semi-supervised.",4.2 Results,[0],[0]
These methods first train an enormous language model on unlabeled data and incorporate the representations produced by the language model into a supervised classifier.,4.2 Results,[0],[0]
"Our base models use 1024 hidden units in their LSTMs (compared to 4096 in ELMo), require fewer training steps (around one pass over the billion-word benchmark rather than
many passes), and do not require a pipelined training procedure.",4.2 Results,[0],[0]
"Therefore, although they perform on par with ELMo, they are faster and simpler to train.",4.2 Results,[0],[0]
Increasing the size of our CVT+Multitask model so it has 4096 units in its LSTMs like ELMo improves results further so they are significantly better than the ELMo+Multi-task ones.,4.2 Results,[0],[0]
"We suspect there could be further gains from combining our method with language model pre-training, which we leave for future work.
",4.2 Results,[0],[0]
CVT + Multi-Task.,4.2 Results,[0],[0]
We train a single sharedencoder CVT model to perform all of the tasks except machine translation (as it is quite different and requires more training time than the other ones).,4.2 Results,[0],[0]
"Multi-task learning improves results on all of the tasks except fine-grained NER, sometimes by large margins.",4.2 Results,[0],[0]
Prior work on many-task NLP such as Hashimoto et al. (2017) uses complicated architectures and training algorithms.,4.2 Results,[0],[0]
"Our result shows that simple parameter sharing can be enough for effective many-task learning when the model is big and trained on a large amount of data.
",4.2 Results,[0],[0]
"Interestingly, multi-task learning works better in conjunction with CVT than with ELMo.",4.2 Results,[0],[0]
"We hypothesize that the ELMo models quickly fit to the data primarily using the ELMo vectors, which perhaps hinders the model from learning effective representations that transfer across tasks.",4.2 Results,[0],[0]
"We also believe CVT alleviates the danger of the model “forgetting” one task while training on the other ones, a well-known problem in many-task learning (Kirkpatrick et al., 2017).",4.2 Results,[0],[0]
"During multi-task CVT, the model makes predictions about unlabeled examples across all tasks, creating (artificial) all-tasks-labeled examples, so the model does not only see one task at a time.",4.2 Results,[0],[0]
"In fact, multi-task learning plus self training is similar to the Learning without Forgetting algorithm (Li and Hoiem, 2016), which trains the model to keep its predictions on an old task unchanged when learning a new task.",4.2 Results,[0],[0]
"To test the value of all-tasks-labeled examples, we trained a multi-task CVT model that only computes LCVT on one task at a time (chosen randomly for each unlabeled minibatch) instead of for all tasks in parallel.",4.2 Results,[0],[0]
"The one-at-a-time model performs substantially worse (see Table 2).
",4.2 Results,[0],[0]
Model Generalization.,4.2 Results,[0],[0]
"In order to evaluate how our models generalize to the dev set from the train set, we plot the dev vs. train accuracy for our different methods as they learn (see Figure 4).",4.2 Results,[0],[0]
"Both CVT and multi-task learning improve model generalization: for the same train accuracy, the models get better dev accuracy than purely supervised learning.",4.2 Results,[0],[0]
"Interestingly, CVT continues to improve
in dev set accuracy while close to 100% train accuracy for CCG, Chunking, and NER, perhaps because the model is still learning from unlabeled data even when it has completely fit to the train set.",4.2 Results,[0],[0]
We also show results for a smaller multi-task + CVT model.,4.2 Results,[0],[0]
"Although it generalizes at least as well as the larger one, it halts making progress on the train set earlier.",4.2 Results,[0],[0]
"This suggests it is important to use sufficiently large neural networks for multitask learning: otherwise the model does not have the capacity to fit to all the training data.
",4.2 Results,[0],[0]
Auxiliary Prediction Module Ablation.,4.2 Results,[0],[0]
We briefly explore which auxiliary prediction modules are more important for the sequence tagging tasks in Table 3.,4.2 Results,[0],[0]
"We find that both kinds of auxiliary prediction modules improve performance, but that the future and past modules improve results more than the forward and backward ones, perhaps because they see a more restricted and challenging view of the input.
",4.2 Results,[0],[0]
Training Models on Small Datasets.,4.2 Results,[0],[0]
"We explore how CVT scales with dataset size by varying the amount of training data the model has ac-
cess to.",4.2 Results,[0],[0]
"Unsurprisingly, the improvement of CVT over purely supervised learning grows larger as the amount of labeled data decreases (see Figure 5, left).",4.2 Results,[0],[0]
"Using only 25% of the labeled data, our approach already performs as well or better than a fully supervised model using 100% of the training data, demonstrating that CVT is particularly useful on small datasets.
",4.2 Results,[0],[0]
Training Larger Models.,4.2 Results,[0],[0]
"Most sequence taggers and dependency parsers in prior work use small LSTMs (hidden state sizes of around 300) because larger models yield little to no gains in performance (Reimers and Gurevych, 2017).",4.2 Results,[0],[0]
We found our own supervised approaches also do not benefit greatly from increasing the model size.,4.2 Results,[0],[0]
"In contrast, when using CVT accuracy scales better with model size (see Figure 5, right).",4.2 Results,[0],[0]
"This finding suggests the appropriate semi-supervised learning methods may enable the development of larger, more sophisticated models for NLP tasks with limited amounts of labeled data.
",4.2 Results,[0],[0]
Generalizable Representations.,4.2 Results,[0],[0]
"Lastly, we explore training the CVT+multi-task model on five tasks, freezing the encoder, and then only training a prediction module on the sixth task.",4.2 Results,[0],[0]
This tests whether the encoder’s representations generalize to a new task not seen during its training.,4.2 Results,[0],[0]
Only training the prediction module is very fast because (1) the encoder (which is by far the slowest part of the model) has to be run over each example only once and (2) we do not back-propagate into the encoder.,4.2 Results,[0],[0]
"Results are shown in Table 4.
",4.2 Results,[0],[0]
"Training only a prediction module on top of multi-task representations works remarkably well,
outperforming ELMo embeddings and sometimes even a vanilla supervised model, showing the multi-task model is building up effective representations for language.",4.2 Results,[0],[0]
"In particular, the representations could be used like skip-thought vectors (Kiros et al., 2015) to quickly train models on new tasks without slow representation learning.",4.2 Results,[0],[0]
Unsupervised Representation Learning.,5 Related Work,[0],[0]
"Early approaches to deep semi-supervised learning pretrain neural models on unlabeled data, which has been successful for applications in computer vision (Jarrett et al., 2009; LeCun et al., 2010) and NLP.",5 Related Work,[0],[0]
"Particularly noteworthy for NLP are algorithms for learning effective word embeddings (Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014) and language model pretraining (Dai and Le, 2015; Ramachandran et al., 2017; Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018).",5 Related Work,[0],[0]
"Pre-training on other tasks such as machine translation has also been studied (McCann et al., 2017).",5 Related Work,[0],[0]
"Other approaches train
“thought vectors” representing sentences through unsupervised (Kiros et al., 2015; Hill et al., 2016) or supervised (Conneau et al., 2017) learning.
",5 Related Work,[0],[0]
Self-Training.,5 Related Work,[0],[0]
"One of the earliest approaches to semi-supervised learning is self-training (Scudder, 1965), which has been successfully applied to NLP tasks such as word-sense disambiguation (Yarowsky, 1995) and parsing (McClosky et al., 2006).",5 Related Work,[0],[0]
"In each round of training, the classifier, acting as a “teacher,” labels some of the unlabeled data and adds it to the training set.",5 Related Work,[0],[0]
"Then, acting as a “student,” it is retrained on the new training set.",5 Related Work,[0],[0]
"Many recent approaches (including the consistentency regularization methods discussed below and our own method) train the student with soft targets from the teacher’s output distribution rather than a hard label, making the procedure more akin to knowledge distillation (Hinton et al., 2015).",5 Related Work,[0],[0]
"It is also possible to use multiple models or prediction modules for the teacher, such as in tri-training (Zhou and Li, 2005; Ruder and Plank, 2018).
",5 Related Work,[0],[0]
Consistency Regularization.,5 Related Work,[0],[0]
"Recent works add noise (e.g., drawn from a Gaussian distribution) or apply stochastic transformations (e.g., horizontally flipping an image) to the student’s inputs.",5 Related Work,[0],[0]
"This trains the model to give consistent predictions to nearby data points, encouraging distributional smoothness in the model.",5 Related Work,[0],[0]
"Consistency regularization has been very successful for computer vision applications (Bachman et al., 2014; Laine and Aila, 2017; Tarvainen and Valpola, 2017).",5 Related Work,[0],[0]
"However, stochastic input alterations are more difficult to apply to discrete data like text, making consistency regularization less used for natural language processing.",5 Related Work,[0],[0]
"One solution is to add noise to the model’s word embeddings (Miyato et al., 2017); we compare against this approach in our experiments.",5 Related Work,[0],[0]
"CVT is easily applicable to text because it does not require changing the student’s inputs.
",5 Related Work,[0],[0]
Multi-View Learning.,5 Related Work,[0],[0]
"Multi-view learning on data where features can be separated into distinct subsets has been well studied (Xu et al., 2013).",5 Related Work,[0],[0]
"Particularly relevant are co-training (Blum and Mitchell, 1998) and co-regularization (Sindhwani and Belkin, 2005), which trains two models with disjoint views of the input.",5 Related Work,[0],[0]
"On unlabeled data, each one acts as a “teacher” for the other model.",5 Related Work,[0],[0]
"In contrast to these methods, our approach trains a single unified model where auxiliary prediction modules see different, but not necessarily indepen-
dent views of the input.
",5 Related Work,[0],[0]
Self Supervision.,5 Related Work,[0],[0]
Self-supervised learning methods train auxiliary prediction modules on tasks where performance can be measured without human-provided labels.,5 Related Work,[0],[0]
"Recent work has jointly trained image classifiers with tasks like relative position and colorization (Doersch and Zisserman, 2017), sequence taggers with language modeling (Rei, 2017), and reinforcement learning agents with predicting changes in the environment (Jaderberg et al., 2017).",5 Related Work,[0],[0]
"Unlike these approaches, our auxiliary losses are based on self-labeling, not labels deterministically constructed from the input.
",5 Related Work,[0],[0]
Multi-Task Learning.,5 Related Work,[0],[0]
"There has been extensive prior work on multi-task learning (Caruana, 1997; Ruder, 2017).",5 Related Work,[0],[0]
"For NLP, most work has focused on a small number of closely related tasks (Luong et al., 2016; Zhang and Weiss, 2016; Søgaard and Goldberg, 2016; Peng et al., 2017).",5 Related Work,[0.9500023868292051],"['However, this assumption may not hold in practice such as in our case above, i.e., the training and the test distributions are different (Heckman 1979; Shimodaira 2000; Zadrozny 2004; Huang et al. 2007; Sugiyama et al. 2008; Bickel et al. 2009).']"
Manytask systems are less commonly developed.,5 Related Work,[0],[0]
"Collobert and Weston (2008) propose a many-task system sharing word embeddings between the tasks, Hashimoto et al. (2017) train a many-task model where the tasks are arranged hierarchically according to their linguistic level, and Subramanian et al. (2018) train a shared-encoder many-task model for the purpose of learning better sentence representations for use in downstream tasks, not for improving results on the original tasks.",5 Related Work,[0],[0]
"We propose Cross-View Training, a new method for semi-supervised learning.",6 Conclusion,[0],[0]
"Our approach allows models to effectively leverage their own predictions on unlabeled data, training them to produce effective representations that yield accurate predictions even when some of the input is not available.",6 Conclusion,[0],[0]
"We achieve excellent results across seven NLP tasks, especially when CVT is combined with multi-task learning.",6 Conclusion,[0],[0]
"We thank Abi See, Christopher Clark, He He, Peng Qi, Reid Pryzant, Yuaho Zhang, and the anonymous reviewers for their thoughtful comments and suggestions.",Acknowledgements,[0],[0]
We thank Takeru Miyato for help with his virtual adversarial training code and Emma Strubell for answering our questions about OntoNotes NER.,Acknowledgements,[0],[0]
Kevin is supported by a Google PhD Fellowship.,Acknowledgements,[0],[0]
"Unsupervised representation learning algorithms such as word2vec and ELMo improve the accuracy of many supervised NLP models, mainly because they can take advantage of large amounts of unlabeled text.",abstractText,[0],[0]
"However, the supervised models only learn from taskspecific labeled data during the main training phase.",abstractText,[0],[0]
"We therefore propose Cross-View Training (CVT), a semi-supervised learning algorithm that improves the representations of a Bi-LSTM sentence encoder using a mix of labeled and unlabeled data.",abstractText,[0],[0]
"On labeled examples, standard supervised learning is used.",abstractText,[0],[0]
"On unlabeled examples, CVT teaches auxiliary prediction modules that see restricted views of the input (e.g., only part of a sentence) to match the predictions of the full model seeing the whole input.",abstractText,[0],[0]
"Since the auxiliary modules and the full model share intermediate representations, this in turn improves the full model.",abstractText,[0],[0]
"Moreover, we show that CVT is particularly effective when combined with multitask learning.",abstractText,[0],[0]
"We evaluate CVT on five sequence tagging tasks, machine translation, and dependency parsing, achieving state-of-the-art results.1",abstractText,[0],[0]
Semi-Supervised Sequence Modeling with Cross-View Training,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2009–2019 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
2009",text,[0],[0]
"User geolocation, the task of identifying the “home” location of a user, is an integral component of many applications ranging from public health monitoring (Paul and Dredze, 2011; Chon et al., 2015; Yepes et al., 2015) and regional studies of sentiment, to real-time emergency awareness systems (De Longueville et al., 2009; Sakaki et al., 2010), which use social media as an implicit information resource about people.
",1 Introduction,[0],[0]
"Social media services such as Twitter rely on IP addresses, WiFi footprints, and GPS data to geolocate users.",1 Introduction,[0],[0]
"Third-party service providers don’t have easy access to such information, and have to rely on public sources of geolocation information such as the profile location field, which is noisy and difficult to map to a location (Hecht et al., 2011), or geotagged tweets, which are publicly available for only 1% of tweets (Cheng et al., 2010; Morstatter et al., 2013).",1 Introduction,[0],[0]
"The scarcity of publicly available
location information motivates predictive user geolocation from information such as tweet text and social interaction data.
",1 Introduction,[0],[0]
"Most previous work on user geolocation takes the form of either supervised text-based approaches (Wing and Baldridge, 2011; Han et al., 2012) relying on the geographical variation of language use, or graph-based semi-supervised label propagation relying on location homophily in user–user interactions (Davis Jr et al., 2011; Jurgens, 2013).
",1 Introduction,[0],[0]
Both text and network views are critical in geolocating users.,1 Introduction,[0],[0]
"Some users post a lot of local content, but their social network is lacking or is not representative of their location; for them, text is the dominant view for geolocation.",1 Introduction,[0],[0]
"Other users have many local social interactions, and mostly use social media to read other people’s comments, and for interacting with friends.",1 Introduction,[0],[0]
Single-view learning would fail to accurately geolocate these users if the more information-rich view is not present.,1 Introduction,[0],[0]
"There has been some work that uses both the text and network views, but it either completely ignores unlabelled data (Li et al., 2012a; Miura et al., 2017), or just uses unlabelled data in the network view (Rahimi et al., 2015b; Do et al., 2017).",1 Introduction,[0],[0]
"Given that the 1% of geotagged tweets is often used for supervision, it is crucial for geolocation models to be able to leverage unlabelled data, and to perform well under a minimal supervision scenario.
",1 Introduction,[0],[0]
"In this paper, we propose GCN, an end-to-end user geolocation model based on Graph Convolutional Networks (Kipf and Welling, 2017) that jointly learns from text and network information to classify a user timeline into a location.",1 Introduction,[0],[0]
"Our contributions are: (1) we evaluate our model under a minimal supervision scenario which is close to real world applications and show that GCN outperforms two strong baselines; (2) given sufficient supervision, we show that GCN is competitive, although the much simpler MLP-TXT+NET outper-
forms state-of-the-art models; and (3) we show that highway gates play a significant role in controlling the amount of useful neighbourhood smoothing in GCN.1",1 Introduction,[0],[0]
"We propose a transductive multiview geolocation model, GCN, using Graph Convolutional Networks (“GCN”: Kipf and Welling (2017)).",2 Model,[0],[0]
"We also introduce two multiview baselines: MLP-TXT+NET based on concatenation of text and network, and DCCA based on Deep Canonical Correlation Analysis (Andrew et al., 2013).",2 Model,[0],[0]
"Let X ∈ R|U |×|V | be the text view, consisting of the bag of words for each user in U using vocabulary V , and A ∈ 1|U",2.1 Multivew Geolocation,[0],[0]
"|×|U | be the network view, encoding user–user interactions.",2.1 Multivew Geolocation,[0],[0]
"We partition U = US ∪ UH into a supervised and heldout (unlabelled) set, US and UH , respectively.",2.1 Multivew Geolocation,[0],[0]
"The goal is to infer the location of unlabelled samples YU , given the location of labelled samples YS , where each location is encoded as a one-hot classification label, yi ∈ 1c with c being the number of target regions.
2.2 GCN GCN defines a neural network model f(X,A) with each layer:
Â = D̃− 1 2 (A+ λI)D̃− 1 2 H(l+1) = σ",2.1 Multivew Geolocation,[0],[0]
"( ÂH(l)W (l) + b ) ,
(1)
where D̃ is the degree matrix of A + λI; hyperparameter λ controls the weight of a node against its neighbourhood, which is set to 1 in the original model (Kipf and Welling, 2017); H0 = X and the din × dout matrix W (l) and dout ×",2.1 Multivew Geolocation,[0],[0]
1 matrix b are trainable layer parameters; and σ is an arbitrary nonlinearity.,2.1 Multivew Geolocation,[0],[0]
"The first layer takes an average of each sample and its immediate neighbours (labelled and unlabelled) using weights in Â, and performs a linear transformation using W and b followed by a nonlinear activation function (σ).",2.1 Multivew Geolocation,[0],[0]
"In other words, for user ui, the output of layer l is computed by:
~hl+1i = σ",2.1 Multivew Geolocation,[0],[0]
"(∑ j∈nhood(i) Âij~h l jW l + bl ) , (2)
1Code and data available at https://github.com/ afshinrahimi/geographconv
Highway GCN:
Highway GCN: ,
Output GCN:
X = BoWtext
Â
Â
Â tanh
tanh
softmax
H0
H1
Hl−1
Hl
predict location: ŷ
W l−1, bl−1, W l−1h , b l−1 h
W 1, b1, W 1h , b 1 h
W l, bl
Figure 1: The architecture of GCN geolocation model with layer-wise highway gates (W ih, b i h).",2.1 Multivew Geolocation,[0],[0]
"GCN is applied to a BoW model of user content over the @-mention graph to predict user location.
where W l and bl are learnable layer parameters, and nhood(i) indicates the neighbours of user ui.",2.1 Multivew Geolocation,[0],[0]
Each extra layer in GCN extends the neighbourhood over which a sample is smoothed.,2.1 Multivew Geolocation,[0],[0]
"For example a GCN with 3 layers smooths each sample with its neighbours up to 3 hops away, which is beneficial if location homophily extends to a neighbourhood of this size.",2.1 Multivew Geolocation,[0],[0]
"Expanding the neighbourhood for label propagation by adding multiple GCN layers can improve geolocation by accessing information from friends that are multiple hops away, but it might also lead to propagation of noisy information to users from an exponentially increasing number of expanded neighbourhood members.",2.2.1 Highway GCN,[0],[0]
"To control the required balance of how much neighbourhood information should be passed to a node, we use layer-wise gates similar to highway networks.",2.2.1 Highway GCN,[0],[0]
"In highway networks (Srivastava et al., 2015), the output of a layer is summed with its input with gating weights T (~hl):
T (~hl) = σ",2.2.1 Highway GCN,[0],[0]
( W lt ~hl + blt ) ~hl+1,2.2.1 Highway GCN,[0],[0]
= ~hl+1 ◦,2.2.1 Highway GCN,[0],[0]
T (~hl) +,2.2.1 Highway GCN,[0],[0]
"~hl ◦ (1− T (~hl)) , (3)
where ~hl is the incoming input to layer l + 1, (W lt , b l t) are gating weights and bias variables, ◦ is elementwise multiplication, and σ is the Sigmoid function.
",2.2.1 Highway GCN,[0],[0]
"2.3 DCCA Given two views X and Â (from Equation 1) of data samples, CCA (Hotelling, 1936), and its deep version (DCCA) (Andrew et al., 2013) learn functions f1(X) and f2(Â) such that the correlation between the output of the two functions is maximised:
ρ = corr(f1(X), f2(Â)) .",2.2.1 Highway GCN,[0],[0]
"(4)
The resulting representations of f1(X) and f2(Â) are the compressed representations of the two views where the uncorrelated noise between them is reduced.",2.2.1 Highway GCN,[0],[0]
"The new representations ideally represent user communities for the network view, and the language model of that community for the text view, and their concatenation is a multiview representation of data, which can be used as input for other tasks.
",2.2.1 Highway GCN,[0],[0]
"In DCCA, the two views are first projected to a lower dimensionality using a separate multilayer perceptron for each view (the f1 and f2 functions of Equation 4), the output of which is used to estimate the CCA cost:
maximise: tr(W T1 Σ12W2) subject to: W T1 Σ11W1 = W T 2 Σ22W2 =",2.2.1 Highway GCN,[0],[0]
"I (5)
where Σ11 and Σ22 are the covariances of the two outputs, and Σ12 is the cross-covariance.",2.2.1 Highway GCN,[0],[0]
"The weights W1 and W2 are the linear projections of the MLP outputs, which are used in estimating the CCA cost.",2.2.1 Highway GCN,[0.9507823732468129],['The labeled training data and the unseen test data have the same target conditional distribution p(y|x) and the marginal distributions of the positive data in both the training and testing are also the same.']
"The optimisation problem is solved by SVD, and the error is backpropagated to train the parameters of the two MLPs and the final linear projections.",2.2.1 Highway GCN,[0],[0]
"After training, the two networks are used to predict new projections for unseen data.",2.2.1 Highway GCN,[0],[0]
"The two projections of unseen data — the outputs of the two networks — are then concatenated to form a multiview sample representation, as shown in Figure 2.",2.2.1 Highway GCN,[0],[0]
"We use three existing Twitter user geolocation datasets: (1) GEOTEXT (Eisenstein et al., 2010), (2) TWITTER-US (Roller et al., 2012), and (3) TWITTER-WORLD (Han et al., 2012).",3.1 Data,[0],[0]
These datasets have been used widely for training and evaluation of geolocation models.,3.1 Data,[0],[0]
"They are all pre-partitioned into training, development and test
sets.",3.1 Data,[0],[0]
"Each user is represented by the concatenation of their tweets, and labelled with the latitude/longitude of the first collected geotagged tweet in the case of GEOTEXT and TWITTER-US, and the centre of the closest city in the case of TWITTER-WORLD.",3.1 Data,[0],[0]
"GEOTEXT and TWITTER-US cover the continental US, and TWITTER-WORLD covers the whole world, with 9k, 449k and 1.3m users, respectively.",3.1 Data,[0],[0]
"The labels are the discretised geographical coordinates of the training points using a k-d tree following Roller et al. (2012), with the number of labels equal to 129, 256, and 930 for GEOTEXT, TWITTER-US, and TWITTER-WORLD, respectively.",3.1 Data,[0],[0]
"We build matrix Â as in Equation 1 using the collapsed @-mention graph between users, where two users are connected (Aij = 1) if one mentions the other, or they co-mention another user.",3.2 Constructing the Views,[0],[0]
"The text view is a BoW model of user content with binary term frequency, inverse document frequency, and l2 normalisation of samples.",3.2 Constructing the Views,[0],[0]
"For GCN, we use highway layers to control the amount of neighbourhood information passed to a node.",3.3 Model Selection,[0],[0]
"We use 3 layers in GCN with size 300, 600, 900 for GEOTEXT, TWITTER-US and TWITTERWORLD respectively.",3.3 Model Selection,[0],[0]
"Note that the final softmax layer is also graph convolutional, which sets the radius of the averaging neighbourhood to 4.",3.3 Model Selection,[0],[0]
"The
k-d tree bucket size hyperparameter which controls the maximum number of users in each cluster is set to 50, 2400, and 2400 for the respective datasets, based on tuning over the validation set.",3.3 Model Selection,[0],[0]
"The architecture of GCN-LP is similar, with the difference that the text view is set to zero.",3.3 Model Selection,[0],[0]
"In DCCA, for the unsupervised networks we use a single sigmoid hidden layer with size 1000 and a linear output layer with size 500 for the three datasets.",3.3 Model Selection,[0],[0]
"The loss function is CCA loss, which maximises the output correlations.",3.3 Model Selection,[0],[0]
"The supervised multilayer perceptron has one hidden layer with size 300, 600, 1000 for GEOTEXT, TWITTER-US, and TWITTER-WORLD, respectively, which we set by tuning over the development sets.",3.3 Model Selection,[0],[0]
"We evaluate the models using Median error, Mean error, and Acc@161, accuracy of predicting a user within 161km or 100 miles from the known location.",3.3 Model Selection,[0],[0]
"We also compare DCCA and GCN with two baselines:
GCN-LP is based on GCN, but for input, instead of text-based features , we use one-hot encoding of a user’s neighbours, which are then convolved with their k-hop neighbours using the GCN.",3.4 Baselines,[0],[0]
"This approach is similar to label propagation in smoothing the label distribution of a user with that of its neighbours, but uses graph convolutional networks which have extra layer parameters, and also a gating mechanism to control the smoothing neighbourhood radius.",3.4 Baselines,[0],[0]
"Note that for unlabelled samples, the predicted labels are used for input after training accuracy reaches 0.2.
",3.4 Baselines,[0],[0]
"MLP-TXT+NET is a simple transductive supervised model based on a single layer multilayer perceptron where the input to the network is the concatenation of the text view X , the user content’s bag-of-words and Â (Equation 1), which represents the network view as a vector input.",3.4 Baselines,[0],[0]
"For the hidden layer we use a ReLU nonlinearity, and sizes 300, 600, and 600 for GEOTEXT, TWITTER-US, and TWITTER-WORLD, respectively.",3.4 Baselines,[0],[0]
"Deep CCA and GCN are able to provide an unsupervised data representation in different ways.
",4.1 Representation,[0],[0]
"Deep CCA takes the two text-based and networkbased views, and finds deep non-linear transformations that result in maximum correlation between the two views (Andrew et al., 2013).",4.1 Representation,[0],[0]
"The representations can be visualised using t-SNE, where we hope that samples with the same label are clustered together.",4.1 Representation,[0],[0]
"GCN, on the other hand, uses graph convolution.",4.1 Representation,[0],[0]
The representations of 50 samples from each of 4 randomly chosen labels of GEOTEXT are shown in Figure 3.,4.1 Representation,[0],[0]
"As shown, Deep CCA seems to slightly improve the representations from pure concatenation of the two views.",4.1 Representation,[0],[0]
"GCN, on the other hand, substantially improves the representations.",4.1 Representation,[0],[0]
"Further application of GCN results in more samples clumping together, which might be desirable when there is strong homophily.",4.1 Representation,[0],[0]
"To achieve good performance in supervised tasks, often large amounts of labelled data are required, which is a big challenge for Twitter geolocation, where only a small fraction of the data is geotagged (about 1%).",4.2 Labelled Data Size,[0],[0]
The scarcity of supervision indicates the importance of semi-supervised learning where unlabelled (e.g. non-geotagged) tweets are used for training.,4.2 Labelled Data Size,[0],[0]
"The three models we propose (MLP-TXT+NET, DCCA, and GCN) are all transductive semi-supervised models that use unlabelled data, however, they are different in terms of how much labelled data they require to achieve acceptable performance.",4.2 Labelled Data Size,[0],[0]
"Given that in a real-world scenario, only a small fraction of data is geotagged, we conduct an experiment to analyse the effect of labelled samples on the performance of the three geolocation models.",4.2 Labelled Data Size,[0.9536738269245947],"['Instead, it represents a set of similarity values between document d and the center of the positive documents.']"
"We provided the three models with different fractions of samples that are labelled (in terms of % of dataset samples) while using the remainder as unlabelled data, and analysed their Median error performance over the development set of GEOTEXT, TWITTER-US, and TWITTER-WORLD.",4.2 Labelled Data Size,[0],[0]
"Note that the text and network view, and the development set, remain fixed for all the experiments.",4.2 Labelled Data Size,[0],[0]
"As shown in Figure 4, when the fraction of labelled samples is less than 10% of all the samples, GCN and DCCA outperform MLP-TXT+NET, as a result of having fewer parameters, and therefore, lower supervision requirement to optimise them.",4.2 Labelled Data Size,[0],[0]
"When enough training data is available (e.g. more than 20% of all the samples), GCN and MLP-TXT+NET clearly outperform DCCA, possibly as a result of directly modelling the
interactions between network and text views.",4.2 Labelled Data Size,[0],[0]
"When all the training samples of the two larger datasets (95% and 98% for TWITTER-US and TWITTERWORLD, respectively) are available to the models, MLP-TXT+NET outperforms GCN.",4.2 Labelled Data Size,[0],[0]
Note that the number of parameters increases from DCCA to GCN and to MLP-TXT+NET.,4.2 Labelled Data Size,[0],[0]
"In 1% for GEOTEXT, DCCA outperforms GCN as a result of having fewer parameters and just a few labelled samples, insufficient to train the parameters of GCN.",4.2 Labelled Data Size,[0],[0]
"Adding more layers to GCN expands the graph neighbourhood within which the user features are averaged, and so might introduce noise, and consequently decrease accuracy as shown in Figure 5 when no gates are used.",4.3 Highway Gates,[0],[0]
"We see that by adding highway network gates, the performance of GCN slightly improves until three layers are added, but then by adding more layers the performance doesn’t change that much as gates are allowing the layer inputs to pass through the network without
much change.",4.3 Highway Gates,[0],[0]
The performance peaks at 4 layers which is compatible with the distribution of shortest path lengths shown in Figure 6.,4.3 Highway Gates,[0],[0]
"The performance of the three proposed models (MLP-TXT+NET, DCCA and GCN) is shown in Table 1.",4.4 Performance,[0],[0]
"The models are also compared with supervised text-based methods (Wing and Baldridge, 2014; Cha et al., 2015; Rahimi et al., 2017b), a network-based method (Rahimi et al., 2015a) and GCN-LP, and also joint text and network models (Rahimi et al., 2017b; Do et al., 2017; Miura et al., 2017).",4.4 Performance,[0],[0]
"MLP-TXT+NET and GCN outperform all the text- or network-only models, and also the hybrid model of Rahimi et al. (2017b), indicating that joint modelling of text and network features is important.",4.4 Performance,[0],[0]
"MLP-TXT+NET is competitive with Do et al. (2017), outperforming it on larger datasets, and underperforming on GEO-
TEXT.",4.4 Performance,[0],[0]
"However, it’s difficult to make a fair comparison as they use timezone data in their feature set.",4.4 Performance,[0],[0]
"MLP-TXT+NET outperforms GCN over TWITTERUS and TWITTER-WORLD, which are very large, and have large amounts of labelled data.",4.4 Performance,[0],[0]
In a scenario with little supervision (1% of the total samples are labelled),4.4 Performance,[0],[0]
"DCCA and GCN clearly outperform MLP-TXT+NET, as they have fewer pa-
rameters.",4.4 Performance,[0],[0]
"Except for Acc@161 over GEOTEXT where the number of labelled samples in the minimal supervision scenario is very low, GCN outperforms DCCA by a large margin, indicating that for a medium dataset where only 1% of samples are labelled (as happens in random samples of Twitter) GCN is superior to MLP-TXT+NET and DCCA, consistent with Section 4.2.",4.4 Performance,[0],[0]
"Both MLP-TXT+NET and GCN achieve state of the art results compared
to network-only, text-only, and hybrid models.",4.4 Performance,[0],[0]
"The network-based GCN-LP model, which does label propagation using Graph Convolutional Networks, outperforms Rahimi et al. (2015a), which is based on location propagation using Modified Adsorption (Talukdar and Crammer, 2009), possibly because the label propagation in GCN is parametrised.",4.4 Performance,[0],[0]
"Although the performance of MLP-TXT+NET is better than GCN and DCCA when a large amount of labelled data is available (Table 1), under a scenario where little labelled data is available (1% of data), DCCA and GCN outperform MLP-TXT+NET, mainly because the number of parameters in MLP-TXT+NET grows with the number of samples, and is much larger than GCN and DCCA.",4.5 Error Analysis,[0],[0]
"GCN outperforms DCCA and MLP-TXT+NET using 1% of data, however, the distribution of errors in the development set of TWITTER-US indicates higher error for smaller states such as Rhode Island (RI), Iowa (IA), North Dakota (ND), and Idaho (ID), which is simply because the number of labelled samples in those states is insufficient.
",4.5 Error Analysis,[0],[0]
"Although we evaluate geolocation models with Median, Mean, and Acc@161, it doesn’t mean that the distribution of errors is uniform over all locations.",4.5 Error Analysis,[0],[0]
"Big cities often attract more local online discussions, making the geolocation of users in those areas simpler.",4.5 Error Analysis,[0],[0]
"For example users in LA are more likely to talk about LA-related issues such as their sport teams, Hollywood or local events than users in the state of Rhode Island (RI), which lacks large sport teams or major events.",4.5 Error Analysis,[0],[0]
"It is also possible that people in less densely populated areas are further apart from each other, and therefore, as a result of discretisation fall in different clusters.",4.5 Error Analysis,[0],[0]
The non,4.5 Error Analysis,[0],[0]
"-uniformity in local discussions results in lower geolocation performance in less densely populated areas like Midwest U.S., and higher performance in densely populated areas such as NYC and LA as shown in Figure 7.",4.5 Error Analysis,[0],[0]
"The geographical distribution of error for GCN, DCCA and MLP-TXT+NET under the minimal supervision scenario is shown in the supplementary material.
",4.5 Error Analysis,[0],[0]
"To get a better picture of misclassification between states, we built a confusion matrix based on known state and predicted state for development users of TWITTER-US using GCN using only 1% of labelled data.",4.5 Error Analysis,[0],[0]
"There is a tendency for users to be wrongly predicted to be in CA, NY, TX, and surpris-
ingly OH.",4.5 Error Analysis,[0],[0]
"Particularly users from states such as TX, AZ, CO, and NV, which are located close to CA, are wrongly predicted to be in CA, and users from NJ, PA, and MA are misclassified as being in NY.",4.5 Error Analysis,[0],[0]
The same goes for OH and TX where users from neighbouring smaller states are misclassified to be there.,4.5 Error Analysis,[0],[0]
"Users from CA and NY are also misclassified between the two states, which might be the result of business and entertainment connections that exist between NYC and LA/SF.",4.5 Error Analysis,[0],[0]
"Interestingly, there are a number of misclassifications to FL for users from CA, NY, and TX, which might be the effect of users vacationing or retiring to FL.",4.5 Error Analysis,[0],[0]
The full confusion matrix between the U.S. states is provided in the supplementary material.,4.5 Error Analysis,[0],[0]
"In Table 2, local terms of a few regions detected by GCN under minimal supervision are shown.",4.6 Local Terms,[0],[0]
The terms that were present in the labelled data are excluded to show how graph convolutions over the social graph have extended the vocabulary.,4.6 Local Terms,[0],[0]
"For example, in case of Seattle, #goseahawks is an important term not present in the 1% labelled data but present in the unlabelled data.",4.6 Local Terms,[0],[0]
The convolution over the social graph is able to utilise such terms that don’t exist in the labelled data.,4.6 Local Terms,[0],[0]
"Previous work on user geolocation can be broadly divided into text-based, network-based and multiview approaches.
",5 Related Work,[0],[0]
Text-based geolocation uses the geographical bias in language use to infer the location of users.,5 Related Work,[0],[0]
"There are three main text-based approaches to geolocation: (1) gazetteer-based models which map geographical references in text to location, but ignore non-geographical references and vernacular uses of language (Rauch et al., 2003; Amitay et al., 2004; Lieberman et al., 2010); (2) geographical topic models that learn region-specific topics, but don’t scale to the magnitude of social media (Eisenstein et al., 2010; Hong et al., 2012; Ahmed et al., 2013); and (3) supervised models which are often framed as text classification (Serdyukov et al., 2009; Wing and Baldridge, 2011; Roller et al., 2012; Han et al., 2014) or text regression (Iso et al., 2017; Rahimi et al., 2017a).",5 Related Work,[0],[0]
"Supervised models scale well and can achieve good performance with sufficient supervision, which is not available in a real world scenario.
",5 Related Work,[0],[0]
Network-based methods leverage the location homophily assumption: nearby users are more likely to befriend and interact with each other.,5 Related Work,[0],[0]
"There are four main network-based geolocation approaches: distance-based, supervised classification, graph-based label propagation, and node embedding methods.",5 Related Work,[0],[0]
"Distance-based methods model the probability of friendship given the distance (Backstrom et al., 2010; McGee et al., 2013; Gu et al., 2012; Kong et al., 2014), supervised models use neighbourhood features to classify a user into a location (Rout et al., 2013; Malmi et al., 2015), and graph-based label-propagation models propagate the location information through the user–user graph to estimate unknown labels (Davis Jr et al., 2011; Jurgens, 2013; Compton et al., 2014).",5 Related Work,[0],[0]
"Node embedding methods build heterogeneous graphs between user–user, user–location and location– location, and learn an embedding space to minimise the distance of connected nodes, and maximise the distance of disconnected nodes.",5 Related Work,[0],[0]
"The embeddings are then used in supervised models for geolocation (Wang et al., 2017).",5 Related Work,[0],[0]
Network-based models fail to geolocate disconnected users: Jurgens et al. (2015) couldn’t geolocation 37% of users as a result of disconnectedness.,5 Related Work,[0],[0]
"Previous work on hybrid text and network methods can be broadly categorised into three main approaches: (1) incorporating text-based information such as toponyms or locations predicted from a textbased model as auxiliary nodes into the user–user graph, which is then used in network-based models (Li et al., 2012a,b; Rahimi et al., 2015b,a); (2) ensembling separately trained text- and networkbased models (Gu et al., 2012; Ren et al., 2012; Jayasinghe et al., 2016; Ribeiro and Pappa, 2017); and (3) jointly learning geolocation from several information sources such as text and network information (Miura et al., 2017; Do et al., 2017), which can capture the complementary information in text and network views, and also model the interactions between the two.",5 Related Work,[0],[0]
"None of the previous
multiview approaches — with the exception of Li et al. (2012a) and Li et al. (2012b) that only use toponyms — effectively uses unlabelled data in the text view, and use only the unlabelled information of the network view via the user–user graph.
",5 Related Work,[0],[0]
"There are three main shortcomings in the previous work on user geolocation that we address in this paper: (1) with the exception of few recent works (Miura et al., 2017; Do et al., 2017), previous models don’t jointly exploit both text and network information, and therefore the interaction between text and network views is not modelled; (2) the unlabelled data in both text and network views is not effectively exploited, which is crucial given the small amounts of available supervision; and (3) previous models are rarely evaluated under a minimal supervision scenario, a scenario which reflects real world conditions.",5 Related Work,[0],[0]
"We proposed GCN, DCCA and MLP-TXT+NET, three multiview, transductive, semi-supervised geolocation models, which use text and network information to infer user location in a joint setting.",6 Conclusion,[0],[0]
"We showed that joint modelling of text and network information outperforms network-only, text-only, and hybrid geolocation models as a result of modelling the interaction between text and network information.",6 Conclusion,[0],[0]
We also showed that GCN and DCCA are able to perform well under a minimal supervision scenario similar to real world applications by effectively using unlabelled data.,6 Conclusion,[0],[0]
"We ignored the context in which users interact with each other, and assumed all the connections to hold location homophily.",6 Conclusion,[0],[0]
"In future work, we are interested in modelling the extent to which a social interaction is caused by geographical proximity (e.g. using user–user gates).",6 Conclusion,[0],[0]
Social media user geolocation is vital to many applications such as event detection.,abstractText,[0],[0]
"In this paper, we propose GCN, a multiview geolocation model based on Graph Convolutional Networks, that uses both text and network context.",abstractText,[0],[0]
"We compare GCN to the state-of-the-art, and to two baselines we propose, and show that our model achieves or is competitive with the stateof-the-art over three benchmark geolocation datasets when sufficient supervision is available.",abstractText,[0],[0]
"We also evaluate GCN under a minimal supervision scenario, and show it outperforms baselines.",abstractText,[0],[0]
We find that highway network gates are essential for controlling the amount of useful neighbourhood expansion in GCN.,abstractText,[0],[0]
Semi-supervised User Geolocation via Graph Convolutional Networks,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 97–102 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-2016",text,[0],[0]
"Automated text simplification (ATS) tries to automatically transform (syntactically, lexically and/or semantically) complex sentences into their simpler variants without significantly altering the original meaning.",1 Introduction,[0],[0]
"It has attracted much attention recently as it could make texts more accessible to wider audiences (Aluı́sio and Gasperin, 2010; Saggion et al., 2015), and used as a pre-processing step, improve performances of various NLP tasks and systems (Vickrey and Koller, 2008; Evans, 2011; Štajner and Popović, 2016).
",1 Introduction,[0],[0]
"However, the state-of-the-art ATS systems still do not reach satisfying performances and require some human post-editing (Štajner and Popović, 2016).",1 Introduction,[0],[0]
"While the best supervised approaches generally lead to grammatical output with preserved original meaning, they are overcautious, making almost no changes to the input sentences (Specia, 2010; Štajner et al., 2015), probably due to the limited size or bad quality of parallel TS corpora used for training.",1 Introduction,[0],[0]
"The largest existing sentence-aligned TS dataset for English is the English Wikipedia – Simple English Wikipedia
(EW–SEW) dataset, which contains 160-280,000 sentence pairs, depending on whether we want to model only traditional sentence rewritings or also to model content reduction and stronger paraphrasing (Hwang et al., 2015).",1 Introduction,[0],[0]
"For Spanish, the largest existing parallel TS corpus contains only 1,000 sentence pairs thus impeding the use of fully supervised approaches.",1 Introduction,[0],[0]
"The best unsupervised lexical simplification (LS) systems for English which leverage word-embeddings (Glavaš and Štajner, 2015; Paetzold and Specia, 2016) seem to perform more lexical substitutions but at the cost of having less grammatical output and more often changed meaning.",1 Introduction,[0],[0]
"However, there have been no direct comparisons of supervised and unsupervised state-of-the-art approaches so far.
",1 Introduction,[0],[0]
"The Newsela corpora1 offers over 2,000 original news articles in English and around 250 in Spanish, manually simplified to 3–4 different complexity levels following strict guidelines (Xu et al., 2015).",1 Introduction,[0],[0]
"Although it was suggested that it has better quality than the EW–SEW corpus (Xu et al., 2015), Newsela has not yet been used for training end-to-end ATS systems, due to the lack of its sentence (and paragraph) alignments.",1 Introduction,[0],[0]
"Such alignments, between various text complexity levels, would offer large training datasets for modelling different levels of simplification, i.e. ‘mild’ simplifications (using the alignments from the neighbouring levels) and ‘heavy’ simplifications (using the alignments of level pairs: 0–3, 0–4, 1–4).
Contributions.",1 Introduction,[0],[0]
"We: (1) provide several methods for paragraph- and sentence alignment of parallel texts, and for assessing similarity level between pairs of text snippets, as freely avail-
1Freely available: https://newsela.com/data/
97
able software;2 (2) compare the performances of lexically- and semantically-based alignment methods across various text complexity levels; (3) test the hypothesis that the original order of information is preserved during manual simplification (Bott and Saggion, 2011) by offering customized MST-LIS alignment strategy (Section 3.1); and (4) show that the new sentence-alignments lead to the state-of-the-art ATS systems even in a basic phrase-based statistical machine translation (PBSMT) approach to text simplifications.",1 Introduction,[0],[0]
"The current state-of-the-art systems for automatic sentence-alignment of original and manually simplified texts are the GSWN method (Hwang et al., 2015) used for sentence-alignment of original and simple English Wikipedia, and the HMMbased method (Bott and Saggion, 2011) used for sentence-alignment of the Spanish Simplext corpus (Saggion et al., 2015).
",2 Related Work,[0],[0]
The HMM-based method can be applied to any language as it does not require any languagespecific resources.,2 Related Work,[0],[0]
"It is based on two hypotheses: (H1) that the original order of information is preserved, and (H2) that every ‘simple’ sentence has at least one corresponding ‘original’ sentence (it can have more than one in the case of ‘n-1’ or ‘nm’ alignments).
",2 Related Work,[0],[0]
"As Simple Wikipedia does not represent direct simplification of the ‘original’ Wikipedia articles (‘simple’ articles were written independently of the ‘original’ ones), GSWN method does not assume H1 or H2.",2 Related Work,[0],[0]
"The main limitations of this method are that it only allows for ‘1-1’ sentence alignments – which is very restricting for TS as it does not allow for sentence splitting (‘1-n’), and summarisation and compression (‘n-1’ and ‘n-m’) alignments – and it is language-dependent as it requires English Wiktionary.
",2 Related Work,[0],[0]
"Unlike the GSWN method, all the methods we apply are language-independent, resource-light and allow for ‘1-n’, ‘n-1’, and ‘n-m’ alignments.",2 Related Work,[0],[0]
"Similar to the HMM-method, our methods assume the hypothesis H2.",2 Related Work,[0],[0]
"We provide them in both variants, using the hypothesis H1 and without it (Section 3.1).
",2 Related Work,[0],[0]
2https://github.com/neosyon/ SimpTextAlign,2 Related Work,[0],[0]
"Having a set of ‘simple’ text snippets S and a set of ‘complex’ text snippets C, we offer two strategies (Section 3.1) to obtain the alignments (si, cj), where si ∈ S, cj ∈",3 Approach,[0],[0]
C.,3 Approach,[0],[0]
"Each alignment strategy, in turn, can use one of the three methods (Section 3.2) to calculate similarity scores between text snippets (either paragraphs or sentences).",3 Approach,[0],[0]
"Most Similar Text (MST): Given one of the similarity methods (Section 3.2), MST compares similarity scores of all possible pairs (si, cj), and aligns each si ∈ S with the closest one in C. MST with Longest Increasing Sequence (MSTLIS): MST-LIS uses the hypothesis H1.",3.1 Alignment strategies,[0],[0]
"It first uses the MST strategy, and then postprocess the output by extracting – from all obtained alignments – only those alignments li ∈ L, which contain the longest increasing sequence of offsets jk in C. In order to allow for ‘1–n’ alignments (i.e. sentence splitting), we allow for repeated offsets of C (‘complex’ text snippets) in L. The ‘simple’ text snippets not contained in L are included in the set U of unaligned snippets.",3.1 Alignment strategies,[0],[0]
"Finally, we align each um ∈ U by restricting the search space in C to those offsets of ‘complex’ text snippets that correspond to the previous and the next aligned ‘simple’ snippets.",3.1 Alignment strategies,[0],[0]
"For instance, if L = {(s1, c4), (s3, c7)} and U = {s2}, then the search space for the alignments of s2 is reduced to {c4...c7}.",3.1 Alignment strategies,[0],[0]
"We denote this strategy with an ‘*’ in the results (Table 2), e.g. C3G*.",3.1 Alignment strategies,[0],[0]
C3G: We employ the Character N -Gram,3.2 Similarity Methods,[0],[0]
"(CNG) (Mcnamee and Mayfield, 2004) similarity model (for n = 3) with log TF-IDF weighting (Salton and McGill, 1986) and compare vectors using the cosine similarity.",3.2 Similarity Methods,[0],[0]
WAVG:,3.2 Similarity Methods,[0],[0]
"We use the continuous skip-gram model (Mikolov et al., 2013b) of the TensorFlow toolkit3 to process the whole English Wikipedia and generate continuous representations of its words.4 For each text snippet, we average its word vectors to obtain a single representation of its content as this setting has shown good results
3https://www.tensorflow.org/ 4We use 300-dimensional vectors, context windows of size 10, and 20 negative words for each sample, in all our continuous word-based models.
",3.2 Similarity Methods,[0],[0]
"in other NLP tasks (e.g. for selecting out-of-thelist words (Mikolov et al., 2013a)).",3.2 Similarity Methods,[0],[0]
"Finally, the similarity between text snippets is estimated using the cosine similarity.",3.2 Similarity Methods,[0],[0]
CWASA:,3.2 Similarity Methods,[0],[0]
"We employ the Continuous Word Alignment-based Similarity Analysis (CWASA) model (Franco-Salvador et al., 2016), which finds the optimal word alignment by computing cosine similarity between continuous representations of all words (instead of averaging word vectors as in the case of WAVG).",3.2 Similarity Methods,[0],[0]
"It was originally proposed for plagiarism detection with excellent results, especially for longer text snippets.",3.2 Similarity Methods,[0],[0]
"To compare the performances of different alignment methods, we randomly selected 10 original texts (Level 0) and their corresponding simpler versions at Levels 1, 3 and 4.",4 Manual Evaluation,[0],[0]
"Instead of creating a ‘gold standard’ and then automatically evaluating the performances, we asked two annotators to rate each pair of automatically aligned paragraphs and sentences – by each of the possible six alignment methods and the HMM-based method (Bott and Saggion, 2011) – for three pairs of text complexity levels (0–1, 0–4, and 3–4) on a 0–2 scale, where: 0 – no semantic overlap in the content; 1 – partial semantic overlap (partial matches); 2 – same semantic content (good matches).",4 Manual Evaluation,[0],[0]
"This resulted in a total of 1526 paragraph- and 1086 sentence-alignments for the 0–1 pairs, and 1218 paragraph- and 1266 sentence-alignments for the 0–4 and 3–4 pairs.",4 Manual Evaluation,[0],[0]
"In the context of TS, both good- and partial matches
are important.",4 Manual Evaluation,[0],[0]
"While full semantic overlap models full paraphrases (‘1-1’ alignments), partial overlap models sentence splitting (“1-n” alignments), deleting irrelevant sentence parts, adding explanations, or summarizing (‘n-m’ alignments).",4 Manual Evaluation,[0],[0]
"Several examples of full and partial matches from the EW–SEW dataset (Hwang et al., 2015) are given in Table 1.
",4 Manual Evaluation,[0],[0]
"We expect that the automatic-alignment task is the easiest between the 0–1 text complexity levels, and much more difficult between the 0-4 levels (Level 4 is obtained after four stages of simplification and thus contains stronger paraphrases and less lexical overlap with Level 0 than Level 1 has).",4 Manual Evaluation,[0],[0]
"We also explore whether the task is equally difficult whenever we align two neighbouring levels, or the difficulty of the task depends on the level complexity (0–1 vs. 3–4).",4 Manual Evaluation,[0],[0]
"The obtained interannotator agreement, weighted Cohen’s κ (on 400 double-annotated instances) was between 0.71 and 0.74 depending on the task and levels.
",4 Manual Evaluation,[0],[0]
"The results of the manual analysis (Table 2) showed that: (1) all applied methods significantly (p < 0.001) outperformed the HMM method on both paragraph- and sentence-alignment tasks;5 (2) the methods which do not assume hypothesis H1 (C3G, CWASA, and WAVG) led to (not significantly) higher percentage of correct alignments than their counterparts which do assume
5Although some of our methods share the same percentage of good+partial matches with the HMM method on the paragraph-alignment 0–1 task, there is still significant difference in the obtained scores (in some cases, our methods led to good matches whereas the HMM only led to partial matches).
",4 Manual Evaluation,[0],[0]
"H1 (C3G*, CWASA*, WAVG*); (3) the difference in the performances of the lexical approach (C3G) and semantic approaches (CWASA and WAVG) was significant only in the 0–4 sentencealignment task, where CWASA performed significantly worse (p < 0.001) than the other two methods, and in the 0–4 paragraph-alignment task, where WAVG performed significantly worse than C3G; (4) the 2-step C3G alignment-method (C3G-2s), which first aligns paragraphs using the best paragraph-alignment method (C3G) and then within each paragraph align sentences with the best sentence-alignment method (C3G), led to more good+partial alignments than the ‘direct’ sentence-alignment C3G method.",4 Manual Evaluation,[0],[0]
"Finally, we test our new English Newsela (C3G2s) sentence-alignments (both for the neighbouring levels – neighb.",5 Extrinsic Evaluation,[0],[0]
"and for all levels – all) and Newsela sentence-alignments for neighboring levels obtained with HMM-method6 (Bott and Saggion, 2011) in the ATS task using standard PBSMT models7 in the Moses toolkit (Koehn et al., 2007).",5 Extrinsic Evaluation,[0],[0]
"We vary the training dataset and the corpus used to build language models (LMs), while keeping always the same 2,000 sentence pairs for tuning (Xu et al., 2016) and the first 70 sentence
6Given that the performance of the HMM-method was poor for non-neighboring levels (Table 2).
",5 Extrinsic Evaluation,[0],[0]
"7GIZA++ implementation of the IBM word alignment model 4 (Och and Ney, 2003), refinement and phraseextraction heuristics (Koehn et al., 2003), the minimum error rate training (Och, 2003) for tuning, and 5-gram LMs with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002).
pairs of their test set8 for our human evaluation.",5 Extrinsic Evaluation,[0],[0]
"Using that particular test set allow us to compare our (PBSMT) systems with the output of the stateof-the-art syntax-based MT (SBMT) system for TS (Xu et al., 2016) which is not freely available.",5 Extrinsic Evaluation,[0],[0]
"We compare: (1) the performance of the standard PBSMT model which uses only the already available EW–SEW dataset (Hwang et al., 2015) with the performances of the same PBSMT models but this time using the combination of the EW–SEW dataset and our newly-created Newsela datasets; (2) the latter PBSMT models (which use both EW–SEW and new Newsela datasets) against the state-of-the-art supervised ATS system (Xu et al., 2016), and one of the recently proposed unsupervised lexical simplification systems, the LightLS system (Glavaš and Štajner, 2015).9
We perform three types of human evaluation on the outputs of all systems.",5 Extrinsic Evaluation,[0],[0]
"First, we count the total number of changes made by each system (Total), counting the change of a whole phrase (e.g. “become defunct” → “was dissolved”) as one change.",5 Extrinsic Evaluation,[0],[0]
"We mark as Correct those changes that preserve the original meaning and grammaticality of the sentence (assessed by two native English speakers) and, at the same time, make the sentence easier to understand (assessed by two non-native fluent English speakers).10 Second, three native English speakers rate the grammaticality (G) and meaning preservation (M) of each sentence with at least one change on a 1–5 Likert scale (1 – very bad; 5 – very good).",5 Extrinsic Evaluation,[0],[0]
"Third, the three nonnative fluent English speakers were shown original (reference) sentences and target (output) sentences (one pair at the time) and asked whether the target sentence is: +2 – much simpler; +1 – somewhat simpler; 0 – equally difficult; -1 – somewhat more difficult; -2 – much more difficult, than the reference sentence.",5 Extrinsic Evaluation,[0],[0]
"While the correctness of changes takes into account the influence of each individual change on grammaticality, meaning and simplicity of a sentence, the Scores (G and M) and Rank (S) take into account the mutual influence of all changes within a sentence.
",5 Extrinsic Evaluation,[0],[0]
"Adding our sentence-aligned Newsela corpus
8Both freely available from: https://github.com/ cocoxu/simplification/
9We use the output of the original SBMT (Xu et al., 2016) and LightLS (Glavaš and Štajner, 2015) systems, obtained from the authors.
",5 Extrinsic Evaluation,[0],[0]
"10Those cases in which the two annotators did not agree are additionally evaluated by a third annotator to obtain majority.
",5 Extrinsic Evaluation,[0],[0]
(either neighb.,5 Extrinsic Evaluation,[0],[0]
"C3G-2l or all C3G-2l) to the currently best sentence-aligned Wiki corpus (Hwang et al., 2015) in a standard PBSMT setup significantly11 improves grammaticality (G) and meaning preservation (M), and increases the percentage of correct changes (Table 3).",5 Extrinsic Evaluation,[0],[0]
"It also significantly outperforms the state-of-the-art ATS systems by simplicity rankings (S), meaning preservation (M), and number of correct changes (Correct), while achieving almost equally good grammaticality (G).
",5 Extrinsic Evaluation,[0],[0]
The level of simplification applied in the training dataset (Newsela neighb.,5 Extrinsic Evaluation,[0],[0]
"C3G-2s vs. Newsela all C3G-2s) significantly influences G and M scores.
",5 Extrinsic Evaluation,[0],[0]
"The use of the HMM-method for aligning Newsela (instead of ours) lead to significantly worse simplifications by all five criteria.
",5 Extrinsic Evaluation,[0],[0]
"11Wilcoxon’s signed rank test, p < 0.001.
",5 Extrinsic Evaluation,[0],[0]
An example of the outputs of different ATS systems is presented in Table 4.,5 Extrinsic Evaluation,[0],[0]
"We provided several methods for paragraphand sentence-alignment from parallel TS corpora, made the software publicly available, and showed that the use of the new sentence-aligned (freely available) Newsela dataset leads to state-of-the-art ATS systems even in a basic PBSMT setup.",6 Conclusions,[0],[0]
"We also showed that lexically-based C3G method is superior to semantically-based methods (CWASA and WAVG) in aligning paraphraphs and sentences with ‘heavy’ simplifications (0–4 alignments), and that 2-step sentence alignment (aligning first paragraphs and then sentences within the paragraphs) lead to more correct alignments than the ‘direct’ sentence alignment.",6 Conclusions,[0],[0]
"This work has been partially supported by the SFB 884 on the Political Economy of Reforms at the University of Mannheim (project C4), funded by the German Research Foundation (DFG), and also by the SomEMBED TIN2015-71147-C2-1-P MINECO research project.",Acknowledgments,[0],[0]
We provide several methods for sentencealignment of texts with different complexity levels.,abstractText,[0],[0]
"Using the best of them, we sentence-align the Newsela corpora, thus providing large training materials for automatic text simplification (ATS) systems.",abstractText,[0],[0]
"We show that using this dataset, even the standard phrase-based statistical machine translation models for ATS can outperform the state-of-the-art ATS systems.",abstractText,[0],[0]
Sentence Alignment Methods for Improving Text Simplification Systems,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 360–368, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
Sentence compression is a standard NLP task where the goal is to generate a shorter paraphrase of a sentence.,1 Introduction,[0],[0]
"Dozens of systems have been introduced in the past two decades and most of them are deletion-based: generated compressions are token subsequences of the input sentences (Jing, 2000; Knight & Marcu, 2000; McDonald, 2006; Clarke & Lapata, 2008; Berg-Kirkpatrick et al., 2011, to name a few).
",1 Introduction,[0],[0]
Existing compression systems heavily use syntactic information to minimize chances of introducing grammatical mistakes in the output.,1 Introduction,[0],[0]
"A common approach is to use only some syntactic information (Jing, 2000; Clarke & Lapata, 2008,
among others) or use syntactic features as signals in a statistical model (McDonald, 2006).",1 Introduction,[0],[0]
"It is probably even more common to operate on syntactic trees directly (dependency or constituency) and generate compressions by pruning them (Knight & Marcu, 2000; Berg-Kirkpatrick et al., 2011; Filippova & Altun, 2013, among others).",1 Introduction,[0],[0]
"Unfortunately, this makes such systems vulnerable to error propagation as there is no way to recover from an incorrect parse tree.",1 Introduction,[0],[0]
"With the state-of-the-art parsing systems achieving about 91 points in labeled attachment accuracy (Zhang & McDonald, 2014), the problem is not a negligible one.",1 Introduction,[0],[0]
"To our knowledge, there is no competitive compression system so far which does not require any linguistic preprocessing but tokenization.
",1 Introduction,[0],[0]
In this paper we research the following question: can a robust compression model be built which only uses tokens and has no access to syntactic or other linguistic information?,1 Introduction,[0],[0]
"While phenomena like long-distance relations may seem to make generation of grammatically correct compressions impossible, we are going to present an evidence to the contrary.",1 Introduction,[0],[0]
"In particular, we will present a model which benefits from the very recent advances in deep learning and uses word embeddings and Long Short Term Memory models (LSTMs) to output surprisingly readable and informative compressions.",1 Introduction,[0],[0]
"Trained on a corpus of less than two million automatically extracted parallel sentences and using a standard tool to obtain word embeddings, in its best and most simple configuration it achieves 4.5 points out of 5 in readability and 3.8 points in informativeness in an extensive evaluation with human judges.",1 Introduction,[0],[0]
"We believe that this is an important result as it may suggest a new direction for sentence compression research which is less tied to modeling linguistic
360
structures, especially syntactic ones, than the compression work so far.
",1 Introduction,[0],[0]
The paper is organized as follows: Section 3 presents a competitive baseline which implements the system of McDonald (2006) for large training sets.,1 Introduction,[0],[0]
The LSTM model and its three configurations are introduced in Section 4.,1 Introduction,[0],[0]
The evaluation set-up and a discussion on wins and losses with examples are presented in Section 5 which is followed by the conclusions.,1 Introduction,[0],[0]
"The problem formulation we adopt in this paper is very simple: for every token in the input sentence we ask whether it should be kept or dropped, which translates into a sequence labeling problem with just two labels: one and zero.",2 Related Work,[0],[0]
"The deletion approach is a standard one in compression research, although the problem is often formulated over the syntactic structure and not the raw token sequence.",2 Related Work,[0],[0]
"That is, one usually drops constituents or prunes dependency edges (Jing, 2000; Knight & Marcu, 2000; McDonald, 2006; Clarke & Lapata, 2008; Berg-Kirkpatrick et al., 2011; Filippova & Altun, 2013).",2 Related Work,[0],[0]
"Thus, the relation to existing compression work is that we also use the deletion approach.
",2 Related Work,[0],[0]
Recent advances in machine learning made it possible to escape the typical paradigm of mapping a fixed dimensional input to a fixed dimensional output to mapping an input sequence onto an output sequence.,2 Related Work,[0],[0]
"Even though many of these models were proposed more than a decade ago, it is not until recently that they have empirically been shown to perform well.",2 Related Work,[0],[0]
"Indeed, core problems in natural language processing such as translation (Cho et al., 2014; Sutskever et al., 2014; Luong et al., 2014), parsing (Vinyals et al., 2014), image captioning (Vinyals et al., 2015; Xu et al., 2015), or learning to execute small programs (Zaremba & Sutskever, 2014) employed virtually the same principles—the use of Recurrent Neural Networks (RNNs).",2 Related Work,[0],[0]
"Thus, with regard to this line of research, our work comes closest to the recent machine translation work.",2 Related Work,[0],[0]
"An important difference is that we do not aim at building a model that generates compressions directly but rather a model which generates a sequence of deletion decisions.
",2 Related Work,[0],[0]
"A more complex translation model is also conceivable and may significantly advance work on compression by paraphrasing, of which there have
not been many examples yet (Cohn & Lapata, 2008).",2 Related Work,[0],[0]
"However, in this paper our goal is to demonstrate that a simple but robust deletionbased system can be built without using any linguistic features other than token boundaries.",2 Related Work,[0],[0]
We leave experiments with paraphrasing models to future work.,2 Related Work,[0],[0]
We compare our model against the system of McDonald (2006) which also formulates sentence compression as a binary sequence labeling problem.,3 Baseline,[0],[0]
"In contrast to our proposal, it makes use of a large set of syntactic features which are treated as soft evidence.",3 Baseline,[0],[0]
The presence or absence of these features is treated as signals which do not condition the output that the model can produce.,3 Baseline,[0],[0]
"Therefore the model is robust against noise present in the precomputed syntactic structures of the input sentences.
",3 Baseline,[0],[0]
The system was implemented based on the description by McDonald (2006) with two changes which were necessary due to the large size of the training data set used for model fitting.,3 Baseline,[0],[0]
"The first change was related to the learning procedure and the second one to the family of features used.
",3 Baseline,[0],[0]
"Regarding the learning procedure, the original model uses a large-margin learning framework, namely MIRA (Crammer & Singer, 2003), but with some minor changes as presented by McDonald et al. (2005).",3 Baseline,[0],[0]
"In this set-up, online learning is performed, and at each step an optimization procedure is made where K constraints are included, which correspond to the top-K solutions for a given training observation.",3 Baseline,[0],[0]
"This optimization step is equivalent to a Quadratic Programming problem if K > 1, which is time-costly to solve, and therefore not adequate for the large amount of data we used for training the model.",3 Baseline,[0],[0]
"Furthermore, in his publication McDonald states clearly that different values of K did not actually have a major impact on the final performance of the model.",3 Baseline,[0],[0]
"Consequently, and for the sake of being able to successfully train the model with largescale data, the learning procedure is implemented as a distributed structured perceptron with iterative parameter mixing (McDonald et al., 2010), where each shard is processed with MIRA and K is set to 1.
Setting K = 1 will only affect the weight update described on line 4 of Figure 3 of McDonald
(2006), which is now expressed as:
w(i+1) ← w(i)",3 Baseline,[0],[0]
"+ τ × eyt,y′ where τ = max ( 0, L(yt,y′)−w · eyt,y′ ||eyt,y′ ||2 )
eyt,y′ =",3 Baseline,[0],[0]
"F (xt,yt)− F (xt,y′) y′",3 Baseline,[0],[0]
= best(x; w(i)),3 Baseline,[0],[0]
"F (x,y) = |y|∑ j=2 f(x, I(yj−1), I(yj))
",3 Baseline,[0],[0]
The second change concerns the feature set used.,3 Baseline,[0],[0]
"While McDonald’s original model contains deep syntactic features coming from both dependency and constituency parse trees, we use only dependency-based features.",3 Baseline,[0],[0]
"Additionally, and to better compare the baseline with the LSTM models, we have included as an optional feature a 256-dimension embedding-vector representation of each input word and its syntactic parent.",3 Baseline,[0],[0]
"The vectors are pre-trained using the Skipgram model1 (Mikolov et al., 2013).",3 Baseline,[0],[0]
"Ultimately, our implementation of McDonald’s model contained 463,614 individual features, summarized in three categories: • PoS features: Joint PoS tags of selected to-
kens.",3 Baseline,[0],[0]
"Unigram, bigram and trigram PoS context of selected and dropped tokens.",3 Baseline,[0],[0]
All the previous features conjoined with one indicating if the last two selected tokens are adjacent.,3 Baseline,[0],[0]
•,3 Baseline,[0],[0]
"Deep syntactic features: Dependency labels
of taken and dropped tokens and their parent dependencies.",3 Baseline,[0],[0]
"Boolean features indicating syntactic relations between selected tokens (i.e., siblings, parents, leaves, etc.).",3 Baseline,[0],[0]
Dependency label of the least common ancestor in the dependency tree between a batch of dropped tokens.,3 Baseline,[0],[0]
All the previous features conjoined with the PoS tag of the involved tokens.,3 Baseline,[0],[0]
"• Word features: Boolean features indicating
if a group of dropped nodes contain a complete or incomplete parenthesization.",3 Baseline,[0],[0]
Wordembedding vectors of selected and dropped tokens and their syntactic parents.,3 Baseline,[0],[0]
"The model is fitted over ten epochs on the whole training data, and for model selection a small development set consisting of 5,000 previously unseen sentences is used (none of them belonging to
1https://code.google.com/p/word2vec/
the evaluation set).",3 Baseline,[0],[0]
The automated metric used for this selection was accuracy@1 which is the proportion of golden compressions which could be fully reproduced.,3 Baseline,[0],[0]
The performance on the development set plateaus when getting close to the last epoch.,3 Baseline,[0],[0]
Our approach is largely based on the sequence to sequence paradigm proposed in Sutskever et al. (2014).,4 The LSTM model,[0],[0]
We train a model that maximizes the probability of the correct output given the input sentence.,4 The LSTM model,[0],[0]
"Concretely, for each training pair (X,Y ), we will learn a parametric model (with parameters θ), by solving the following optimization problem:
θ∗ = arg max θ ∑ X,Y log p(Y |X; θ) (1)
where the sum is assumed to be over all training examples.",4 The LSTM model,[0],[0]
"To model the probability p, we use the same architecture described by Sutskever et al. (2014).",4 The LSTM model,[0],[0]
"In particular, we use a RNN based on the Long Short Term Memory (LSTM) unit (Hochreiter & Schmidhuber, 1997), designed to avoid vanishing gradients and to remember some long-distance dependences from the input sequence.",4 The LSTM model,[0],[0]
Figure 1 shows a basic LSTM architecture.,4 The LSTM model,[0],[0]
"The RNN is fed with input words Xi (one at a time), until we feed a special symbol “GO”.",4 The LSTM model,[0],[0]
"It is now a common practice (Sutskever et al., 2014; Li & Jurafsky, 2015) to start feeding the input in reversed order, as it has been shown to perform better empirically.",4 The LSTM model,[0],[0]
"During the first pass over the input, the network is expected to learn a compact, distributed representation of the input sentence, which will allow it to start generating the right predictions when the second pass starts, after the “GO” symbol is read.
",4 The LSTM model,[0],[0]
"We can apply the chain rule to decompose Equation (1) as follows:
p(Y |X; θ) = T∏ t=1 p(Yt|Y1, . . .",4 The LSTM model,[0],[0]
", Yt−1, X; θ) (2)
noting that we made no independence assumptions.",4 The LSTM model,[0],[0]
"Once we find the optimal θ∗, we construct our estimated compression Ŷ as:
Ŷ = arg max Y
p(Y |X; θ∗) (3)
LSTM cell: Let us review the sequence-tosequence LSTM model.",4 The LSTM model,[0],[0]
The Long Short Term Memory model of Hochreiter & Schmidhuber (1997) is defined as follows.,4 The LSTM model,[0],[0]
"Let xt, ht, and mt be the input, control state, and memory state at timestep t. Then, given a sequence of inputs (x1, . . .",4 The LSTM model,[0],[0]
", xT ), the LSTM computes the h-sequence (h1, . . .",4 The LSTM model,[0],[0]
", hT ) and the m-sequence (m1, . . .",4 The LSTM model,[0],[0]
",mT ) as follows
it =",4 The LSTM model,[0],[0]
"sigm(W1xt +W2ht−1) i′t = tanh(W3xt +W4ht−1) ft = sigm(W5xt +W6ht−1) ot = sigm(W7xt +W8ht−1) mt = mt−1 ft + it i′t ht = mt ot
The operator denotes element-wise multiplication, the matrices W1, . . .",4 The LSTM model,[0],[0]
",W8 and the vector h0 are the parameters of the model, and all the nonlinearities are computed element-wise.
",4 The LSTM model,[0],[0]
Stochastic gradient descent is used to maximize the training objective (Eq. (1)),4 The LSTM model,[0],[0]
w.r.t.,4 The LSTM model,[0],[0]
"all the LSTM parameters.
",4 The LSTM model,[0],[0]
Network architecture:,4 The LSTM model,[0],[0]
In these experiments we have used the architecture depicted in Figure 3.,4 The LSTM model,[0],[0]
"Following Vinyals et al. (2014), we have used three stacked LSTM layers to allow the upper layers to learn higher-order representations of the input, interleaved with dropout layers to prevent overfitting (Srivastava et al., 2014).",4 The LSTM model,[0],[0]
"The output layer is a SoftMax classifier that predicts, after the “GO” symbol is read, one of the following three
labels: 1, if a word is to be retained in the compression, 0 if a word is to be deleted, or EOS, which is the output label used for the “GO” input and the end-of-sentence final period.
",4 The LSTM model,[0],[0]
"Input representation: In the simplest implementation, that we call LSTM, the input layer has 259 dimensions.",4 The LSTM model,[0],[0]
"The first 256 contain the embedding-vector representation of the current in-
put word, pre-trained using the Skipgram model2",4 The LSTM model,[0],[0]
"(Mikolov et al., 2013).",4 The LSTM model,[0],[0]
"The final three dimensions contain a one-hot-spot representation of the goldstandard label of the previous word (during training), or the generated label of the previous word (during decoding).
",4 The LSTM model,[0],[0]
"For the LSTM+PAR architecture we first parse the input sentence, and then we provide as input, for each input word, the embedding-vector representation of that word and its parent word in the dependency tree.",4 The LSTM model,[0],[0]
"If the current input is the root node, then a special parent embedding is constructed with all nodes set to zero except for one node.",4 The LSTM model,[0],[0]
In these settings we want to test the hypothesis whether knowledge about the parent node can be useful to decide if the current constituent is relevant or not for the compression.,4 The LSTM model,[0],[0]
The dimensionality of the input layer in this case is 515.,4 The LSTM model,[0],[0]
"Similarly to McDonald (2006), syntax is used here as a soft feature in the model.
",4 The LSTM model,[0],[0]
"For the LSTM+PAR+PRES architecture, we again parse the input sentence, and use a 518-sized embedding vector, that includes: • The embedding vector for the current word
(256 dimensions).",4 The LSTM model,[0],[0]
"• The embedding vector for the parent word
(256 dimensions).",4 The LSTM model,[0],[0]
"• The label predicted for the last word (3 di-
mensions).",4 The LSTM model,[0],[0]
"• A bit indicating whether the parent word has 2https://code.google.com/p/word2vec/
already been seen and kept in the compression (1 dimension).",4 The LSTM model,[0],[0]
"• A bit indicating whether the parent word has
already been seen but discarded (1 dimension).",4 The LSTM model,[0],[0]
"• A bit indicating whether the parent word
comes later in the input (1 dimension).
",4 The LSTM model,[0],[0]
Decoding: Eq. (3) involves searching through all possible output sequences (given X).,4 The LSTM model,[0],[0]
"Contrary to the baseline, in the case of LSTMs the complete previous history is taken into account for each prediction and we cannot simplify Eq.",4 The LSTM model,[0.9508559697095067],"['However, in the CBS space, we learn a classifier that returns 1 for documents that are “close enough” to the center of the training positive documents and -1 for documents elsewhere.']"
(2) with a Markov assumption.,4 The LSTM model,[0],[0]
"Therefore, the search space at decoding time is exponential on the length of the input, and we have used a beam-search procedure as described in Figure 2.
",4 The LSTM model,[0],[0]
"Fixed parameters: For training, we unfold the network 120 times and make sure that none of our training instances is longer than that.",4 The LSTM model,[0],[0]
"The learning rate is initialized at 2, with a decay factor of 0.96 every 300,000 traning steps.",4 The LSTM model,[0],[0]
The dropping probability for the dropout layers is 0.2.,4 The LSTM model,[0],[0]
The number of nodes in each LSTM layer is always identical to the number of nodes in the input layer.,4 The LSTM model,[0],[0]
We have not tuned these parameters nor the number of stacked layers.,4 The LSTM model,[0],[0]
Both the LSTM systems we introduced and the baseline require a training set of a considerable size.,5.1 Data,[0],[0]
"In particular, the LSTM model uses 256- dimensional embeddings of token sequences and cannot be expected to perform well if trained on a thousand parallel sentences, which is the size of the commonly used data sets (Knight & Marcu, 2000; Clarke & Lapata, 2006).",5.1 Data,[0],[0]
"Following the method of Filippova & Altun (2013), we collect a much larger corpus of about two million parallel sentence-compression instances from the news where every compression is a subsequence of tokens from the input.",5.1 Data,[0],[0]
"For testing, we use the publicly released set of 10,000 sentence-compression pairs3.",5.1 Data,[0],[0]
"We take the first 200 sentences from this set for the manual evaluation with human raters, and the first 1,000 sentences for the automatic evaluation.",5.1 Data,[0],[0]
We evaluate the baseline and our systems on the 200-sentence test set in an experiment with human raters.,5.2 Experiments,[0],[0]
The raters were asked to rate readability and informativeness of compressions given the input which are the standard evaluation metrics for compression.,5.2 Experiments,[0],[0]
"The former covers the grammatical correctness, comprehensibility and fluency of the output while the latter measures the amount of important content preserved in the compression.
",5.2 Experiments,[0],[0]
"Additionally, for experiments on the development set, we used two metrics for automatic evaluation: per-sentence accuracy (i.e., how many compressions could be fully reproduced) and word-based F1-score.",5.2 Experiments,[0],[0]
The latter differs from the RASP-based relation F-score by Riezler et al. (2003) in that we simply compute the recall and precision in terms of tokens kept in the golden and the generated compressions.,5.2 Experiments,[0],[0]
"We report these results for completeness although it is the results of the human evaluation from which we draw our conclusions.
",5.2 Experiments,[0],[0]
Compression ratio:,5.2 Experiments,[0],[0]
The three versions of our system (LSTM*) and the baseline (MIRA) have comparable compression ratios (CR) which are defined as the length of the compression in characters divided over the sentence length.,5.2 Experiments,[0],[0]
"Since the
3http://storage.googleapis.com/ sentencecomp/compressiondata.json
ratios are very close, a comparison of the systems’ scores is justified (Napoles et al., 2011).
",5.2 Experiments,[0],[0]
"Automatic evaluation: A total of 1,000 sentence pairs from the test set4 were used in the automatic evaluation.",5.2 Experiments,[0],[0]
"The results are summarized in Table 1.
",5.2 Experiments,[0],[0]
"There is a significant difference in performance of the MIRA baseline and the LSTM models, both in terms of F1-score and in accuracy.",5.2 Experiments,[0],[0]
More than 30% of golden compressions could be fully regenerated by the LSTM systems which is in sharp contrast with the 20% of MIRA.,5.2 Experiments,[0],[0]
"The differences in F-score between the three versions of LSTM are not significant, all scores are close to 0.81.
",5.2 Experiments,[0],[0]
"Evaluation with humans: The first 200 sentences from the set of 1,000 used in the automatic evaluation were compressed by each of the four systems.",5.2 Experiments,[0],[0]
"Every sentence-compression pair was rated by three raters who were asked to select a rating on a five-point Likert scale, ranging from one to five.",5.2 Experiments,[0],[0]
In very few cases (around 1%),5.2 Experiments,[0],[0]
"the ratings were inconclusive (i.e., 1, 3, 5 were given to the same pair) and had to be skipped.",5.2 Experiments,[0],[0]
"Table 2 summarizes the results.
",5.2 Experiments,[0],[0]
The results indicate that the LSTM models produce more readable and more informative compressions.,5.2 Experiments,[0],[0]
"Interestingly, there is no benefit in using the syntactic information, at least not with
4We used the very first 1,000 instances.
",5.2 Experiments,[0],[0]
the amount of parallel data we had at our disposal.,5.2 Experiments,[0],[0]
"The simple LSTM model which only uses token embeddings to generate a sequence of deletion decisions significantly outperforms the baseline which was given not only embeddings but also syntactic and other features.
",5.2 Experiments,[0],[0]
Discussion: What are the wins and losses of the LSTM systems?,5.2 Experiments,[0],[0]
Figure 4 presents some of the evaluated sentence-compression pairs.,5.2 Experiments,[0],[0]
"In terms of readability, the basic LSTM system performed surprisingly well.",5.2 Experiments,[0],[0]
Only in a few cases (out of 200) did it get an average score of two or three.,5.2 Experiments,[0],[0]
"Sentences which pose difficulty to the model are the ones with quotes, intervening commas, or other uncommon punctuation patterns.",5.2 Experiments,[0],[0]
"For example, in the second sentence in Figure 4, if one removes from the input the age modifiers and the preceding commas, the words and Chris Martin are not
dropped and the output compression is grammatical, preserving both conjoined elements.
",5.2 Experiments,[0],[0]
"With regard to informativeness, the difficult cases are those where there is very little to be removed and where the model still removed more than a half to achieve the compression ratio it observed in the training data.",5.2 Experiments,[0],[0]
"For example, the only part that can be removed from the fourth sentence in Figure 4 is the modifier of police, everything else being important content.",5.2 Experiments,[0],[0]
"Similarly, in the fifth sentence the context of the event must be retained in the compression for the event to be interpreted correctly.
",5.2 Experiments,[0],[0]
"Arguably, such cases would also be difficult for other systems.",5.2 Experiments,[0],[0]
"In particular, recognizing when the context is crucial is a problem that can be solved only by including deep semantic and discourse features which has not been attempted yet.",5.2 Experiments,[0],[0]
"And
sentences with quotes (direct speech, a song or a book title, etc.) are challenging for parsers which in turn provide important signals for most compression systems.
",5.2 Experiments,[0],[0]
The bottom of Figure 4 contains examples of good compressions.,5.2 Experiments,[0],[0]
"Even though for a significant number of input sentences the compression was a continuous subsequence of tokens, there are many discontinuous compressions.",5.2 Experiments,[0],[0]
"In particular, the LSTM model learned to drop appositions, no matter how long they are, temporal expressions, optional modifiers, introductory clauses, etc.
",5.2 Experiments,[0],[0]
"Our understanding of why the extended model (LSTM+PAR+PRES) performed worse in the human evlauation than the base model is that, in the absence of syntactic features, the basic LSTM learned a model of syntax useful for compression, while LSTM++, which was given syntactic information, learned to optimize for the particular way the ”golden” set was created (tree pruning).",5.2 Experiments,[0],[0]
"While the automatic evaluation penalized all deviations from the single golden variant, in human evals there was no penalty for readable alternatives.",5.2 Experiments,[0],[0]
"We presented, to our knowledge, a first attempt at building a competitive compression system which is given no linguistic features from the input.",6 Conclusions,[0],[0]
"The two important components of the system are (1) word embeddings, which can be obtained by anyone either pre-trained, or by running word2vec on a large corpus, and (2) an LSTM model which draws on the very recent advances in research on RNNs.",6 Conclusions,[0],[0]
"The training data of about two million sentence-compression pairs was collected automatically from the Internet.
",6 Conclusions,[0],[0]
Our results clearly indicate that a compression model which is not given syntactic information explicitly in the form of features may still achieve competitive performance.,6 Conclusions,[0],[0]
The high readability and informativeness scores assigned by human raters support this claim.,6 Conclusions,[0],[0]
"In the future, we are planning to experiment with more “interesting” paraphrasing models which translate the input not into a zero-one sequence but into words.",6 Conclusions,[0],[0]
"We present an LSTM approach to deletion-based sentence compression where the task is to translate a sentence into a sequence of zeros and ones, corresponding to token deletion decisions.",abstractText,[0],[0]
"We demonstrate that even the most basic version of the system, which is given no syntactic information (no PoS or NE tags, or dependencies) or desired compression length, performs surprisingly well: around 30% of the compressions from a large test set could be regenerated.",abstractText,[0],[0]
We compare the LSTM system with a competitive baseline which is trained on the same amount of data but is additionally provided with all kinds of linguistic features.,abstractText,[0],[0]
In an experiment with human raters the LSTMbased model outperforms the baseline achieving 4.5 in readability and 3.8 in informativeness.,abstractText,[0],[0]
Sentence Compression by Deletion with LSTMs,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2453–2464 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
2453",text,[0],[0]
Sentence compression aims to produce a summary of a single sentence that retains the most important information while preserving its fluency.,1 Introduction,[0],[0]
"The task has attracted much attention due to its potential for applications such as text summarization (Jing, 2000; Madnani et al., 2007; Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011), subtitle generation (Vandeghinste and Pan, 2004; Luotolahti and Ginter, 2015), and the display of text on small-screens (Corston-Oliver, 2001).
",1 Introduction,[0],[0]
"The bulk of research on sentence compression has focused on a simplification of the task involving exclusively word deletion (Knight and Marcu, 2002; Riezler et al., 2003; Turner and Charniak, 2005; McDonald, 2006; Clarke and Lapata, 2008; Cohn and Lapata, 2009), whereas a few approaches view sentence compression as a more general text rewriting problem (Galley and McKeown, 2007; Woodsend and Lapata, 2010; Cohn and Lapata, 2013).",1 Introduction,[0],[0]
"Irrespective of how
1Publicly available for download at https://github. com/Jmallins/MOSS
the compression task is formulated, most previous work relies on syntactic information such as parse trees to help decide what to delete from a sentence or which rules to learn in order to rewrite a sentence using less words.",1 Introduction,[0],[0]
"More recently, there has been much interest in applying neural network models to natural language generation tasks, including sentence compression (Rush et al., 2015; Filippova et al., 2015; Chopra et al., 2016; Kikuchi et al., 2016).",1 Introduction,[0],[0]
Filippova et al. (2015) focus on deletion-based sentence compression which they model as a sequence labeling problem using a recurrent neural network with long short-term memory units (LSTM; Hochreiter and Schmidhuber 1997).,1 Introduction,[0],[0]
"Rush et al. (2015) capture the full gamut of rewrite operations drawing insights from encoderdecoder models recently proposed for machine translation (Bahdanau et al., 2015).
",1 Introduction,[0],[0]
"Neural network-based approaches are datadriven, relying on the ability of recurrent architectures to learn continuous features without recourse to preprocessing tools or syntactic information (e.g., part-of-speech tags, parse trees).",1 Introduction,[0],[0]
"In order to achieve good performance, they require large amounts of training data, in the region of millions of long-short sentence pairs.2 Existing compression datasets are several orders of magnitude smaller.",1 Introduction,[0],[0]
"For example, the ZiffDavis corpus (Knight and Marcu, 2002) contains 1,067 sentences and originated from a collection of news articles on computer products.",1 Introduction,[0],[0]
"Clarke and Lapata (2008) create two manual corpora sampled from written (1,433 sentences) and spoken sources (1,370 sentences).",1 Introduction,[0],[0]
Cohn and Lapata (2013) elicit manual compressions for 625 sentences taken from newspaper articles.,1 Introduction,[0],[0]
"More recently, Toutanova et al. (2016) crowdsource a larger corpus which contains manual compressions for single and multiple sentences (about 26,000 pairs of source and compressed texts).
",1 Introduction,[0],[0]
"2Rush et al. (2015) use approximately four million training instances and Filippova et al. (2015) two million.
",1 Introduction,[0],[0]
"Since large scale compression datasets do not occur naturally, they must be somehow approximated, e.g., by pairing headlines with the first sentence of a news article (Filippova and Altun, 2013; Rush et al., 2015).",1 Introduction,[0],[0]
"As a result, the training corpus construction process must be repeated and reconfigured for new languages and domains (e.g., many headline-first sentence pairs are spurious and need to be filtered using language and domain specific heuristics).",1 Introduction,[0],[0]
"And although it may be easy to automatically obtain large scale training data in the news domain, it is not clear how such data can be sourced for many other genres with different writing conventions.
",1 Introduction,[0],[0]
Our work addresses the paucity of data for sentence compression models.,1 Introduction,[0],[0]
"We argue that multilingual corpora are a rich source for learning a variety of rewrite rules across languages and that existing neural machine translation (NMT) models (Sutskever et al. 2014; Bahdanau et al. 2015) can be easily adapted to the compression task through bilingual pivoting (Mallinson et al., 2017) coupled with methods which decode the output sequence to a desired length (e.g., subject to language and genre requirements).",1 Introduction,[0],[0]
"We obtain compressions by translating a source string into a foreign language and then back-translating it into the source while controlling the translation length (Kikuchi et al., 2016).",1 Introduction,[0],[0]
"Our model can be trained for any language as long as a bilingual corpus is available, and can perform arbitrary rewrites while taking advantage of multiple pivots if these exist.",1 Introduction,[0],[0]
"We also demonstrate that models trained on multilingual data perform well out-of-domain.
",1 Introduction,[0],[0]
"Although our approach does not employ compression corpora for training, for evaluation purposes, we create MOSS, a new Multilingual Compression dataset for English, French, and German.",1 Introduction,[0],[0]
"MOSS is a parallel corpus containing documents from the European parliament proceedings, TED talks, news commentaries, and the EU bookshop.",1 Introduction,[0],[0]
"Each document is written in English, French, and German, and compressed by native speakers of the respective language who process a document at a time.",1 Introduction,[0],[0]
"We obtain five compressions per document leading to 2,000 long-short sentence pairs per language.",1 Introduction,[0],[0]
"Like previous related resources (Clarke and Lapata, 2008; Cohn and Lapata, 2013; de Loupy et al., 2010)",1 Introduction,[0],[0]
"our corpus is curated manually, however it differs from Toutanova et al. (2016) in that it contains compressions for individual sentences, not documents.
",1 Introduction,[0],[0]
There has been relatively little interest in compressing languages other than English.,1 Introduction,[0],[0]
"A few models have been proposed for Japanese (Hori and Furui, 2004; Hirao et al., 2009; Harashima and Kurohashi, 2012), including a neural network model (Hasegawa et al., 2017) which repurposes Filippova and Altun’s (2013) data construction method for Japanese.",1 Introduction,[0],[0]
"There is a compression corpus available for French (de Loupy et al., 2010), however, we are not aware of any modeling work on this language.",1 Introduction,[0],[0]
"Overall, there are no standardized datasets in languages other than English, either for training or testing.
",1 Introduction,[0],[0]
"Our contributions in this work are three-fold: a novel application of bilingual pivoting to sentence compression; corroborated by empirical results showing that our model scales across languages and text genres without additional supervision over and above what is available in the bilingual parallel data; and the release of a multilingual, multi-reference compression corpus which can be effectively used to gain insight in the compression task and facilitate further research in compression modeling.",1 Introduction,[0],[0]
"In our pivot-based sentence compression model an input sequence is first translated into a foreign language, and then back into the source language.",2 Pivot-based Neural Compression,[0],[0]
"Unlike previous paraphrasing pivoting models (Mallinson et al., 2017), we parameterize our translation models with a length feature, which allows us to produce compressed output.",2 Pivot-based Neural Compression,[0],[0]
"We define two models, performing compression in one step or alternatively in two steps which affords more flexibility in model output.",2 Pivot-based Neural Compression,[0],[0]
"In the neural encoder-decoder framework for MT (Bahdanau et al., 2015; Sutskever et al., 2014), an encoder takes in a source X =",2.1 NMT Background,[0],[0]
"(x1, ...,xTx) of length Tx and the decoder generates a target sequence (y1, ...,yTy) of length Ty.",2.1 NMT Background,[0],[0]
"Let hi be the hidden state of the source symbol at position i, obtained by concatenating the forward and backward encoder RNN hidden states, hi =",2.1 NMT Background,[0],[0]
[ −→ hi ; ←−,2.1 NMT Background,[0],[0]
hi ].,2.1 NMT Background,[0],[0]
"We deviate from previous work (Bahdanau et al., 2015; Sutskever et al., 2014) in that we initialize the decoder with the average of the hidden states, following Sennrich et al. (2017):
s0 = tanh(Winit ∑Txi=1",2.1 NMT Background,[0],[0]
"hi
Tx ) (1)
where Winit is a learnt parameter.",2.1 NMT Background,[0],[0]
"Our decoder is a conditional recurrent neural network, specifically a gated recurrent unit (GRU, Cho et al., 2014) with attention, which we denote as cGRUatt .",2.1 NMT Background,[0],[0]
"cGRUatt takes as input the previous hidden state s j−1, the source annotations C = h1, ...,hTx , and the previously decoded symbol y j−1 in order to update its hidden state s j, which is used to decode symbol y j at position j:
s j = cGRUatt(s j−1,y j−1,C)",2.1 NMT Background,[0],[0]
"(2)
cGRUatt consists of three components.",2.1 NMT Background,[0],[0]
The first combines the previously decoded symbol y j−1 and the previous hidden state s j−1 to generate an intermediate representation s′j.,2.1 NMT Background,[0],[0]
"The attention mechanism, AT T , inputs the entire context set C along with intermediate hidden state s′j in order to compute the context vector c j:
c j = AT T (C,s′j) =",2.1 NMT Background,[0],[0]
"Tx
∑ i αi jhi (3)
",2.1 NMT Background,[0],[0]
"αi j = exp(ei j)
∑Txk=1 exp(ek j) (4)
ei j = f",2.1 NMT Background,[0],[0]
"(s′j,hi) (5)
Where αi j is the normalized alignment weight between the source symbol at position i and the target symbol at position j, and f is a feedfoward neural network.
",2.1 NMT Background,[0],[0]
"Finally, we generate s j, the hidden state of cGRUatt , by using the intermediate representation s′j and",2.1 NMT Background,[0],[0]
"the context vector c j. Given s j, y j−1, and c j",2.1 NMT Background,[0],[0]
"the output probability p(y j|s j,y j−1,c j) is computed using a feedforward neural network with a softmax activation.",2.1 NMT Background,[0],[0]
"We define the probability of sequence y as:
P(y|x;θ) =",2.1 NMT Background,[0],[0]
"Ty
∏ j=1 p(y j|s j,y j−1,c j) (6)",2.1 NMT Background,[0],[0]
"To be able to produce compressed sentences, we parameterize our model with a length vector which allows to control the output length.",2.2 Length Control,[0],[0]
"Our approach is similar to the LenInit model of Kikuchi et al. (2016), however we use a GRU instead of an LSTM.",2.2 Length Control,[0],[0]
"The hidden state of the decoder consists of the average of the encoder’s hidden states but also a length vector LV , a learnt parameter, which is scaled by the desired target length Ty′ .",2.2 Length Control,[0],[0]
"We therefore rewrite Equation (1) as follows:
s′0 = tanh ( Winit [∑Txi=1 hi
Tx ;LV ·Ty′
]) (7)
As such we now define our model as:
P(y|x,Ty′ ;θ) (8)
During training, the target length is set to Ty′ = Ty.",2.2 Length Control,[0],[0]
"However, at test time, the target length generally varies according to the domain, genre, and language at hand.",2.2 Length Control,[0],[0]
We determine the target length experimentally based on a small validation set.,2.2 Length Control,[0],[0]
"Pivoting is often used in machine translation to overcome the shortage of parallel data, i,e., when there is no translation path from the source language to the target by taking advantage of paths through an intermediate language.",2.3 Pivoting,[0],[0]
"The idea dates back at least to Kay (1997), who observed that ambiguities in translating from one language onto another may be resolved if a translation into some third language is available, and has met with success in phrase-based SMT (Wu and Wang, 2007; Utiyama and Isahara, 2007) and more recently in neural MT systems (Firat et al., 2016).
",2.3 Pivoting,[0],[0]
"We use pivoting to provide a path from a source English sentence, via an intermediate foreign language, to English in a compressed form.",2.3 Pivoting,[0],[0]
"We propose to extend Mallinson et al.’s (2017) approach to multi-pivoting, where a sentence x is translated to K-best foreign pivots, Fx = { f1, ..., fK}.",2.3 Pivoting,[0],[0]
"The probability of generating compression y = y1...yTy is decomposed as:
P(y|x) =",2.3 Pivoting,[0],[0]
"Fx
∑ f
P(y| f ; −→ θ ) ·P( f |x; ←− θ ) (9)
which we approximate as the tokenwise weighted average of the pivots:
P(y|x)≈ Ty
∏ j=1
Fx ∑ f P(y j|y< j, f )P( f |x) (10)
where y< j = y1, ...y j .",2.3 Pivoting,[0],[0]
"To ensure a probability distribution, we normalize the K-best list Fx, such that the translation probabilities sum to one.",2.3 Pivoting,[0],[0]
We use beam search to decode tokens by conditioning on multiple pivoting sentences.,2.3 Pivoting,[0],[0]
"The results with the best decoding scores are considered candidate compressions.
",2.3 Pivoting,[0],[0]
"To ensure the model produces compressed output, we extend the pivoting approach in two ways.",2.3 Pivoting,[0],[0]
"In single step compression, one of the translation
models is parameterized with length information:
P(y|x,Ty′)",2.3 Pivoting,[0],[0]
"≈ F
∑ f
P(y| f ,Ty′ ; −→ θ ) · P( f |x; ←− θ )
",2.3 Pivoting,[0],[0]
"In dual-step compression, we parameterize both translation models with length information:
P(y|x,Ty′ ,Ty′′)≈ F
∑ f
P(y| f ,Ty′ ;",2.3 Pivoting,[0],[0]
"−→ θ )·P( f |x,Ty′′ ; ←− θ )
",2.3 Pivoting,[0],[0]
"We find that dual-compression performs better when the system is expected to drastically compress the source sentence (e.g., in a headline generation task).",2.3 Pivoting,[0.9567355462972581],"['Document space one-class SVM (ds-osvm): As we discussed earlier, due to the covariate shift problem in the negative training data, one solution is to drop the negative training data completely to build a one-class classifier.']"
Imposing a high compression ratio from the start tends to produce unintelligible text.,2.3 Pivoting,[0],[0]
"The model attempts to reduce the length of the source at all costs, even at the expense of being semantically faithful to the input.",2.3 Pivoting,[0],[0]
"Performing two moderate compressions in succession reduces both length and content conservatively and as a result produces more meaningful text.
",2.3 Pivoting,[0],[0]
In Figure 1 we illustrate how the pivot-based model sketched above can successfully control the output of the generated compressions.,2.3 Pivoting,[0],[0]
We show the output of a single-step compression model on three languages initialized with varying compression rates3,2.3 Pivoting,[0],[0]
(see Section 4 for details on how the models were trained and tested).,2.3 Pivoting,[0],[0]
"The compression rate (CR) is used to determine length parameter of Equation (8):
Ty′ =",2.3 Pivoting,[0],[0]
"Tx ·CR (11)
",2.3 Pivoting,[0],[0]
"The figure shows how the output length varies compared to a vanilla encoder-decoder system which uses pivoting to backtranslate the source
3The term refers to the percentage of words retained from the source sentence in the compression.
language (Mallinson et al., 2017).",2.3 Pivoting,[0],[0]
We can see that the majority of sentences are generated with length close to the desired compression rate.,2.3 Pivoting,[0],[0]
"For evaluation purposes, we created a multilingual sentence compression corpus in English, German, and French.",3 The MOSS Dataset,[0],[0]
The corpus was collated from existing document and sentence aligned multilingual datasets which vary both in terms of topic and genre.,3 The MOSS Dataset,[0],[0]
"We sampled five documents each from:
1.",3 The MOSS Dataset,[0],[0]
"Europarl, the European Parliament Proceedings Parallel Corpus (Koehn, 2005), has been used extensively in machine translation research; it contains the minutes of the European parliament and is a spoken corpus of formulaic nature; speakers take part in debating various issues concerning EU policy (e.g., taxation, environment).
2.",3 The MOSS Dataset,[0],[0]
"The TED parallel Corpus (Cettolo et al., 2012) contains transcripts in multiple languages of short talks devoted to spreading powerful ideas on a variety of topics ranging from science to business and global issues.
3.",3 The MOSS Dataset,[0],[0]
"The EU bookshop corpus (Skadiņš et al., 2014) contains publications from European institutions covering a variety of topics such as refugees, gender equality, and travel.
4.",3 The MOSS Dataset,[0],[0]
"The News Commentary Parallel Corpus contains articles downloaded from Project Syndicate, an international media organization that publishes commentary on global topics (e.g., economics, world affairs).
",3 The MOSS Dataset,[0],[0]
We obtained compressions using the Crowdflower platform.,3 The MOSS Dataset,[0],[0]
Crowdworkers were given instructions that explained the task and defined sentence compression with the aid of examples.,3 The MOSS Dataset,[0],[0]
"They
were asked to compress while preserving the most important information, ensuring the sentences remained grammatical and meaning preserving.",3 The MOSS Dataset,[0],[0]
"Annotators were encouraged to use any rewriting operations that seemed appropriate, e.g., to delete words, add new words, substitute them, or reorder them.",3 The MOSS Dataset,[0],[0]
"Annotation proceeded on a document-bydocument basis, line-by-line.",3 The MOSS Dataset,[0],[0]
Crowdworkers compressed the first twenty lines of each document and we elicited five compression per document.,3 The MOSS Dataset,[0],[0]
"Example compressions are shown in Table 1.
",3 The MOSS Dataset,[0],[0]
Table 2 presents various statistics on our corpus.,3 The MOSS Dataset,[0],[0]
"As can be seen, Europarl contains the longest sentences across languages (see column SL), TED contains the shortest sentences, while the other two corpora are somewhere in-between.",3 The MOSS Dataset,[0],[0]
"We also observe that crowdworkers compress the least when it comes to TED (see column CR), which is not surprising given the brevity of the utterances.",3 The MOSS Dataset,[0],[0]
"Overall, French speakers seem more conservative when shortening sentences compared to English and German.",3 The MOSS Dataset,[0],[0]
"In general, compression rates are genre dependent, they range from 0.58 (for English Europarl) to 0.84 (for German TED).",3 The MOSS Dataset,[0],[0]
"We also examined the degree to which crowdworkers paraphrase the source sentence using Translation Edit Rate (TER; Snover et al., 2006), a measure com-
monly used to automatically evaluate the quality of machine translation output.",3 The MOSS Dataset,[0],[0]
We used TER to compute the (average) number of edits required to change a long sentence to shorter output.,3 The MOSS Dataset,[0],[0]
"We also report the number of edits by type, i.e., the number of insertions, substitutions, deletions, and shifts needed (on average) to convert long to short sentences.",3 The MOSS Dataset,[0],[0]
We observe that crowdworkers perform a fair amount of rewriting across corpora and languages.,3 The MOSS Dataset,[0],[0]
"The most frequent rewrite operations are deletions followed by substitutions, shifts, and insertions.",3 The MOSS Dataset,[0],[0]
"Neural Machine Translation Training Nematus (Sennrich et al., 2017) was used as the machine translation system for all our experiments.",4 Experimental Setup,[0],[0]
We generally used the default settings and training procedures as specified within Nematus.,4 Experimental Setup,[0],[0]
"All networks have a hidden layer size of 1,000, and an embedding layer size of 512.",4 Experimental Setup,[0],[0]
"In addition, layer normalization (Ba et al., 2016) was used.",4 Experimental Setup,[0],[0]
"During training, we used ADAM (Kingma and Ba, 2014), a minibatch size of 80, and the training set was reshuffled between epochs.",4 Experimental Setup,[0],[0]
"We also employed early stopping.
",4 Experimental Setup,[0],[0]
"We used up to four encoder-decoder NMT models in our experiments (BLEU scores4 shown in parentheses): English→French (27.03), French→English (29.14), English→German (28.3), and German→English (31.19).",4 Experimental Setup,[0],[0]
German training/test data was taken from the WMT16 shared task and French from the WMT14 shared task.,4 Experimental Setup,[0],[0]
"The training data was 4.2 million and 39 million sentence pairs for en-de, and en-fr, respectively.",4 Experimental Setup,[0],[0]
"We also used back-translated monolingual training data, from the news domain, (Sennrich et al., 2016a) in training for the German systems.",4 Experimental Setup,[0],[0]
"The data was pre-processed using standard scripts found in MOSES (Koehn et al., 2007).",4 Experimental Setup,[0],[0]
"Rare words were split into sub-word units, using byte pair encoding (BPE; Sennrich et al. 2016b).",4 Experimental Setup,[0],[0]
"The BPE operations are shared between language directions.
",4 Experimental Setup,[0],[0]
We experimented with various model variants using one or multiple pivots.,4 Experimental Setup,[0],[0]
The compression rate (see Equation (8)) was tuned experimentally on the validation set which consists of one document from each domain (20 source sentences; 100 compression-pairs).,4 Experimental Setup,[0],[0]
"Compression rates varied from 0.55 to 0.85 and were broadly comparable to those shown in Table 2.
",4 Experimental Setup,[0],[0]
"4BLEU scores were calculated using mteval-v13a.pl.
",4 Experimental Setup,[0],[0]
"Comparison Systems We compared our model against ABS, a sequence-to-sequence attentionbased model, developed by Rush et al. (2015).",4 Experimental Setup,[0],[0]
"This model was trained on a monolingual dataset extracted from the Annotated English Gigaword corpus (Napoles et al., 2011).",4 Experimental Setup,[0],[0]
The dataset consists of approximately 4 million pairs of the first sentence from each source document and its headline.,4 Experimental Setup,[0],[0]
"We also trained LenInit (Kikuchi et al., 2016) on the same corpus which is conceptually similar to ABS but additionally controls the output length using a length embedding vector (as described in Section 2.2).5",4 Experimental Setup,[0],[0]
"Unfortunately, we could not train these models for French or German, since there are no monolingual sentence compression datasets available at a similar scale.",4 Experimental Setup,[0],[0]
An obvious workaround is to translate Gigaword to French and German and then train compression models on the translated data.,4 Experimental Setup,[0],[0]
"As the quality of the translation is relatively poor, we also translated German or French into English, compressed it with ABS and LenInit trained on the Gigaword corpus, and then translated the compressions back to French or German.",4 Experimental Setup,[0],[0]
"Finally, we include a prefix (Pfix) baseline which does not perform any rewriting but simply truncates the source sentence so that it matches the compression ratio of the validation set.",4 Experimental Setup,[0],[0]
"MOSS Evaluation We assessed model performance using three automatic metrics which represent different aspects of the compression task and have been found to correlate well with human judgments (Toutanova et al., 2016; Clarke and Lapata, 2006).",5 Results,[0],[0]
"These include a recall metric based on skip bi-grams, any pair of words in a sequence allowing for gaps of size four6 (RS-R); a recall metric based on bi-grams of dependency tree triples (D2-R); and bi-gram ROUGE (R2-F1).",5 Results,[0],[0]
"We used the Stanford neural network parser (Chen and Manning, 2014) to obtain dependency triples.
",5 Results,[0],[0]
Table 3(a) reports results on English with a model which controls the output length (L) and uses either a single pivot (SP; K = 1) or multiple pivots (MP; K = 10).,5 Results,[0],[0]
We experimented with French (fr) or German (de) as pivot languages.,5 Results,[0],[0]
All pivot-based models perform compression in a single step (see Section 2.3).,5 Results,[0],[0]
"Dual-step compres-
5We used our own implementation of ABS and LenInit which on DUC-2004 obtained ROUGE scores similar to those published in Rush et al. (2015) and Kikuchi et al. (2016).
",5 Results,[0],[0]
"6We add a begin-of-sentence marker at the start of the candidate and reference sentences.
",5 Results,[0],[0]
sion obtained inferior results which we omit for the sake of brevity.,5 Results,[0],[0]
"As can be seen, models which use a single pivot are better than those using multiple ones (German is better than French; see SPde vs SP f r).",5 Results,[0],[0]
"More pivots might introduce noise at the expense of translation quality.
",5 Results,[0],[0]
"Overall, pivot-based models outperform ABS and LenInit.",5 Results,[0],[0]
This is perhaps to be expected since these models are tested on out of domain data with different vocabulary and writing conventions; MOSS does not contain any newspaper articles.,5 Results,[0],[0]
"Unfortunately, it is not possible to train ABS and LenInt on in-domain data as compression data only exists for the headlines-first sentences pairs.",5 Results,[0],[0]
"As an upper bound, we also report how well humans agree with each other, treating one (randomly selected) reference as system output and computing how it agrees with the rest (row Gold in Table 3).",5 Results,[0],[0]
"All models lag significantly behind human performance on this task.
",5 Results,[0],[0]
"Tables 3(b) and 3(c) report results on French and German, respectively.",5 Results,[0],[0]
"For these languages, we obtained best results with English as pivot, using a single-step compression model.",5 Results,[0],[0]
"ABS and LenInit perform poorly when trained directly on translations of Gigaword into French and German; their performance improves considerably when they are trained on the Gigaword and used to compress English translations of French or German (ABSen, LenIniten).",5 Results,[0],[0]
"Again, we observe that our models (SPL ,en, MPL ,en) outperform the comparison systems across all metrics and that using a single pivot yields better compressions.",5 Results,[0],[0]
Example compressions are given in Table 4 where we show output produced by ABS and SP for each language (see the supplementary material for more examples).,5 Results,[0],[0]
"Finally, notice that automatic scores for the prefix baseline across languages are misleadingly high, since it simply repeats the source sentence up to a fixed length without performing any rewriting.
",5 Results,[0],[0]
We also elicited human judgments through the Crowdflower platform.,5 Results,[0],[0]
We asked crowdworkers to rate the grammaticality of the target compressions and whether they preserved the most important information from the source.,5 Results,[0],[0]
"In both cases, they used a five-point rating scale where a high number indicates better performance.",5 Results,[0],[0]
"We randomly selected 25 sentences from each corpus from the test portion of MOSS, i.e., 100 long-short sentence pairs per language.",5 Results,[0],[0]
"We compared compressions generated by our model (SPL ), with ABS models for the three languages, the prefix baseline, and (randomly selected) gold-standard reference (Ref) compressions from MOSS.",5 Results,[0],[0]
All systems used the length parameter to allow comparisons with approximately the same compression rates.,5 Results,[0],[0]
We collected five ratings per compression.,5 Results,[0],[0]
Our results are summarized in Table 5.,5 Results,[0],[0]
"We show mean ratings for grammaticality (Gram), importance (Imp) and their combination (column Avg).",5 Results,[0],[0]
"Across languages our model (SPL) significantly (p < 0.05) outperforms comparison systems (Pfix, ABS) on both dimensions of grammaticality and importance (significance tests were performed using a student t-test).",5 Results,[0],[0]
"All systems are significantly worse (p < 0.05) than the human reference compressions.
",5 Results,[0],[0]
"Finally, in Table 6 we analyze the output of our best model (SPL ) using the same statistics we applied to the human compressions (see Table 2).",5 Results,[0],[0]
"As can be seen, the model generally compressess more aggressively and applies more ed-
its than the crowdworkers (both compression rates and TER scores are higher for all three languages).",5 Results,[0],[0]
"Although the rate of deletions is similar to humans, insertions, substitutions and shifts happen to a greater extent for our model, indicating that it performs a good amount of paraphrasing.
",5 Results,[0.9560043704231619],"['Further, due to the highly dynamic nature of social media, it is probably impossible to label all possible negative topics.']"
"DUC-2004 Evaluation Besides MOSS, we evaluated our model on the benchmark DUC-2004 task-1 dataset.",5 Results,[0],[0]
"In this task, the aim is to create a very short summary (75 bytes) for a document.",5 Results,[0],[0]
The evaluation set consists of 500 source documents (from the New York Times and Associated Press Wire services) each paired with four humanwritten (reference) summaries.,5 Results,[0],[0]
"We follow previous work (Rush et al., 2015; Chopra et al., 2016) in compressing the first sentence of the document and presenting this as the summary.",5 Results,[0],[0]
"To make the evaluation unbiased to length, the output of all systems is cut-off after 75-characters and no bonus is given for shorter summaries.
",5 Results,[0],[0]
Our results are shown in Table 7.,5 Results,[0],[0]
"To compare with existing methods, we also report ROUGE (Lin, 2004) unigram and bigram overlap (Lin, 2004) and the longest common subsequence (ROUGE-L).9",5 Results,[0],[0]
We employed a dual step compression model (see Section 2) as preliminary experiments showed that it was superior to singlestage variants.,5 Results,[0],[0]
"We compared single and multiple pivot models against existing ABS and ABS+ (Rush et al., 2015), two encoder-decoder models trained on the English Gigaword.",5 Results,[0],[0]
"ABS+ applies minimum error rate (MERT) training as a copy-
7Our ABS implementation obtains R1-R 25.03, R2-R 8.40, and RL-R: 22.35
8Our LenInit implementation obtains R1-R 29.26, R2-R 9.56, and RL-R 25.70
9We used ROUGE version 1.5.5 with the original DUC-2004 ROUGE parameters.
",5 Results,[0],[0]
ing mechanism.,5 Results,[0],[0]
"LenEmb and LenInit include a length parameter (Kikuchi et al., 2016), whereas RAS uses a specialized recurrent neural network architecture (Elman, 1990).",5 Results,[0],[0]
We also report how well DUC-2004 abstractors agree with each other (row Gold in Table 7).,5 Results,[0],[0]
"Example compressions are given in Table 8, where we show output produced by SPL ,de and a human reference (see the supplementary material for further examples).
",5 Results,[0],[0]
Using automatic metrics we see that our model generally performs worse compared to these systems and that German is the best pivot for English.,5 Results,[0],[0]
"Although the objective of this paper is not to obtain state-of-the-art scores on this evaluation set, it is interesting to see that our model is able to compress out-of-domain.",5 Results,[0],[0]
"We do not have access to headline-first sentence pairs, while all comparison systems do.",5 Results,[0],[0]
We also elicited human judgments on the compressions of 100 lead sentences whose documents were randomly selected from the DUC-2004 test set.,5 Results,[0],[0]
"We compared the prefix baseline, our model (SPL ,de), ABS+ (Rush et al., 2015), LenEmb (Kikuchi et al., 2016), Topiary (Zajic et al., 2004), and a randomly selected reference.",5 Results,[0],[0]
Topiary came top in almost all measures in the DUC-2004 evaluation; it first compresses the lead sentence using linguistically motivated heuristics and then enhances it with topic keywords.,5 Results,[0],[0]
"Crowdworkers rated grammaticality and importance, using a five-point scale; we collected five ratings per compression.
",5 Results,[0],[0]
As shown in Table 9 ABS+ has the lead with our system following suit.,5 Results,[0],[0]
"In terms of grammaticality, ABS+ and SPL ,de are not significantly different from the gold standard or from each other (Pfix, Topiary, and LenEmb are significantly worse than Gold; p < 0.05).",5 Results,[0],[0]
"In terms of importance, pairwise differences between systems and the gold standard are not significant.",5 Results,[0],[0]
"Overall, we observe that SPL ,de performs comparably to ABS+ even though it was
not trained on any compression specific data.",5 Results,[0],[0]
Inspection of system output reveals that our model performs more paraphrasing than comparison systems (a conclusion also confirmed by the statistics in Table 6).,5 Results,[0],[0]
In this paper we have shown that multilingual corpora can be used to bootstrap compression models across languages and text genres.,6 Conclusions,[0],[0]
Our approach adapts existing neural machine translation machinery to the compression task coupled with methods which decode the output to a desired length.,6 Conclusions,[0],[0]
"An interesting direction for future work would be to train our model using reinforcement learning (Ranzato et al., 2016; Zhang and Lapata, 2017) in order to control the compression output more directly.",6 Conclusions,[0],[0]
"Moreover, although we do not use any direct supervision in our experiments, it would be interesting to incorporate it as a means of domain adaptation (Cheng et al., 2016).
",6 Conclusions,[0],[0]
Acknowledgments The authors gratefully acknowledge the support of the UK Engineering and Physical Sciences Research Council (grant EP/L016427/1; Mallinson) and the European Research Council (award number 681760; Lapata).,6 Conclusions,[0],[0]
In this paper we advocate the use of bilingual corpora which are abundantly available for training sentence compression models.,abstractText,[0],[0]
Our approach borrows much of its machinery from neural machine translation and leverages bilingual pivoting: compressions are obtained by translating a source string into a foreign language and then back-translating it into the source while controlling the translation length.,abstractText,[0],[0]
Our model can be trained for any language as long as a bilingual corpus is available and performs arbitrary rewrites without access to compression specific data.,abstractText,[0],[0]
"We release1 MOSS, a new parallel Multilingual Compression dataset for English, German, and French which can be used to evaluate compression models across languages and genres.",abstractText,[0],[0]
Sentence Compression for Arbitrary Languages via Multilingual Pivoting,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 584–594 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"The main goal of sentence simplification is to reduce the linguistic complexity of text, while still retaining its original information and meaning.",1 Introduction,[0],[0]
"The simplification task has been the subject of several modeling efforts in recent years due to its relevance for NLP applications and individuals alike (Siddharthan, 2014; Shardlow, 2014).",1 Introduction,[0],[0]
"For instance, a simplification component could be used as a preprocessing step to improve the performance of parsers (Chandrasekar et al., 1996), summarizers (Beigman Klebanov et al., 2004), and semantic role labelers (Vickrey and Koller, 2008; Woodsend and Lapata, 2014).",1 Introduction,[0],[0]
"Automatic simplification would also benefit people with low-literacy skills (Watanabe et al., 2009), such as children and
1Our code and data are publicly available at https:// github.com/XingxingZhang/dress.
non-native speakers as well as individuals with autism (Evans et al., 2014), aphasia (Carroll et al., 1999), or dyslexia (Rello et al., 2013).
",1 Introduction,[0],[0]
"The most prevalent rewrite operations which give rise to simplified text include substituting rare words with more common words or phrases, rendering syntactically complex structures simpler, and deleting elements of the original text (Siddharthan, 2014).",1 Introduction,[0],[0]
Earlier work focused on individual aspects of the simplification problem.,1 Introduction,[0],[0]
"For example, several systems performed syntactic simplification only, using rules aimed at sentence splitting (Carroll et al., 1999; Chandrasekar et al., 1996; Vickrey and Koller, 2008; Siddharthan, 2004) while others turned to lexical simplification by substituting difficult words with more common WordNet synonyms or paraphrases (Devlin, 1999; Inui et al., 2003; Kaji et al., 2002).
",1 Introduction,[0],[0]
Recent approaches view the simplification process more holistically as a monolingual textto-text generation task borrowing ideas from statistical machine translation.,1 Introduction,[0],[0]
Simplification rewrites are learned automatically from examples of complex-simple sentences extracted from online resources such as the ordinary and simple English Wikipedia.,1 Introduction,[0],[0]
"For example, Zhu et al. (2010) draw inspiration from syntax-based translation and propose a model similar to Yamada and Knight (2001) which additionally performs simplification-specific rewrite operations (e.g., sentence splitting).",1 Introduction,[0],[0]
"Woodsend and Lapata (2011) formulate simplification in the framework of Quasi-synchronous grammar (Smith and Eisner, 2006) and use integer linear programming to score the candidate translations/simplifications.",1 Introduction,[0],[0]
"Wubben et al. (2012) propose a two-stage model: initially, a standard phrase-based machine translation (PBMT) model is trained on complex-simple sentence pairs.",1 Introduction,[0],[0]
"During inference, the K-best outputs of the PBMT model are reranked according
584
to their dis-similarity to the (complex) input sentence.",1 Introduction,[0],[0]
The hybrid model developed in Narayan and Gardent (2014) also operates in two phases.,1 Introduction,[0],[0]
"Initially, a probabilistic model performs sentence splitting and deletion operations over discourse representation structures assigned by Boxer (Curran et al., 2007).",1 Introduction,[0],[0]
The resulting sentences are further simplified by a model similar to Wubben et al. (2012).,1 Introduction,[0],[0]
"Xu et al. (2016) train a syntax-based machine translation model on a large scale paraphrase dataset (Ganitkevitch et al., 2013) using simplification-specific objective functions and features to encourage simpler output.
",1 Introduction,[0],[0]
"In this paper we propose a simplification model which draws on insights from neural machine translation (Bahdanau et al., 2015; Sutskever et al., 2014).",1 Introduction,[0],[0]
Central to this approach is an encoderdecoder architecture implemented by recurrent neural networks.,1 Introduction,[0],[0]
The encoder reads the source sequence into a list of continuous-space representations from which the decoder generates the target sequence.,1 Introduction,[0],[0]
"Although our model uses the encoder-decoder architecture as its backbone, it must also meet constraints imposed by the simplification task itself, i.e., the predicted output must be simpler, preserve the meaning of the input, and grammatical.",1 Introduction,[0],[0]
"To incorporate this knowledge, the model is trained in a reinforcement learning framework (Williams, 1992): it explores the space of possible simplifications while learning to maximize an expected reward function that encourages outputs which meet simplificationspecific constraints.",1 Introduction,[0],[0]
"Reinforcement learning has been previously applied to extractive summarization (Ryang and Abekawa, 2012), information extraction (Narasimhan et al., 2016), dialogue generation (Li et al., 2016), machine translation, and image caption generation (Ranzato et al., 2016).
",1 Introduction,[0],[0]
"We evaluate our system on three publicly available datasets collated automatically from Wikipedia (Zhu et al., 2010; Woodsend and Lapata, 2011) and human-authored news articles (Xu et al., 2015b).",1 Introduction,[0],[0]
We experimentally show that the reinforcement learning framework is the key to successful generation of simplified text bringing significant improvements over strong simplification models across datasets.,1 Introduction,[0],[0]
"We will first define a basic encoder-decoder model for sentence simplification and then explain how to embed it in a reinforcement learning
framework.",2 Neural Encoder-Decoder Model,[0],[0]
Given a (complex) source sentence X =,2 Neural Encoder-Decoder Model,[0],[0]
"(x1, x2, . . .",2 Neural Encoder-Decoder Model,[0],[0]
", x|X|), our model learns to predict its simplified target Y = (y1, y2, . . .",2 Neural Encoder-Decoder Model,[0],[0]
", y|Y |).",2 Neural Encoder-Decoder Model,[0],[0]
"Inferring the target Y given the sourceX is a typical sequence to sequence learning problem, which can be modeled with attention-based encoderdecoder models (Bahdanau et al., 2015; Luong et al., 2015).",2 Neural Encoder-Decoder Model,[0],[0]
"Sentence simplification is slightly different from related sequence transduction tasks (e.g., compression) in that it can involve splitting operations.",2 Neural Encoder-Decoder Model,[0],[0]
"For example, a long source sentence (In 1883, Faur married Marie Fremiet, with whom he had two sons.)",2 Neural Encoder-Decoder Model,[0],[0]
"can be simplified as two sentences (In 1883, Faur married Marie Fremiet.",2 Neural Encoder-Decoder Model,[0],[0]
They had two sons.).,2 Neural Encoder-Decoder Model,[0],[0]
"Nevertheless, we still view the target as a sequence, i.e., two or more sequences concatenated with full stops.
",2 Neural Encoder-Decoder Model,[0],[0]
The encoder-decoder model has two parts (see left hand side in Figure 1).,2 Neural Encoder-Decoder Model,[0],[0]
"The encoder transforms the source sentence X into a sequence of hidden states (hS1 ,h S 2 , . . .",2 Neural Encoder-Decoder Model,[0],[0]
",h S |X|)",2 Neural Encoder-Decoder Model,[0],[0]
"with a Long Short-Term Memory Network (LSTM; Hochreiter and Schmidhuber 1997), while the decoder uses another LSTM to generate one word yt+1 at a time in the simplified target Y .",2 Neural Encoder-Decoder Model,[0],[0]
"Generation is conditioned on all previously generated words y1:t and a dynamically created context vector ct, which encodes the source sentence:
P (Y |X) = |Y |∏ t=1 P (yt|y1:t−1, X) (1)
P (yt+1|y1:t, X) = softmax(g(hTt , ct))",2 Neural Encoder-Decoder Model,[0],[0]
"(2)
where g(·) is a one-hidden-layer neural network with the following parametrization:
g(hTt , ct) =",2 Neural Encoder-Decoder Model,[0],[0]
"Wo tanh(Uhh T t + Whct) (3)
where Wo ∈ R|V |×d, Uh ∈ Rd×d, and Wh ∈ Rd×d; |V",2 Neural Encoder-Decoder Model,[0],[0]
| is the output vocabulary size and d the hidden unit size.,2 Neural Encoder-Decoder Model,[0],[0]
"hTt is the hidden state of the decoder LSTM which summarizes y1:t, i.e., what has been generated so far:
hTt = LSTM(yt,h T t−1) (4)
",2 Neural Encoder-Decoder Model,[0],[0]
"The dynamic context vector ct is the weighted sum of the hidden states of the source sentence:
ct = |X|∑ i=1",2 Neural Encoder-Decoder Model,[0],[0]
"αtihSi (5)
whose weights αti are determined by an attention mechanism:
αti = exp(hTt · hSi )∑ i exp(h T t · hSi )
(6)
where · is the dot product between two vectors.",2 Neural Encoder-Decoder Model,[0],[0]
We use the dot product here mainly for efficiency reasons; alternative ways to compute attention scores have been proposed in the literature and we refer the interested reader to Luong et al. (2015).,2 Neural Encoder-Decoder Model,[0],[0]
The model sketched above is usually trained by minimizing the negative log-likelihood of the training source-target pairs.,2 Neural Encoder-Decoder Model,[0],[0]
"In this section we present DRESS, our Deep REinforcement Sentence Simplification model.",3 Reinforcement Learning for Sentence Simplification,[0],[0]
"Despite successful application in numerous sequence transduction tasks (Jean et al., 2015; Chopra et al., 2016; Xu et al., 2015a), a vanilla encoder-decoder model is not ideal for sentence simplification.",3 Reinforcement Learning for Sentence Simplification,[0],[0]
"Although a number of rewrite operations (e.g., copying, deletion, substitution, word reordering) can be used to simplify text, copying is by far the most common.",3 Reinforcement Learning for Sentence Simplification,[0],[0]
We empirically found that 73% of the target words are copied from the source in the Newsela dataset.,3 Reinforcement Learning for Sentence Simplification,[0],[0]
This number further increases to 83% when considering Wikipedia-based datasets (we provide details on these datasets in Section 5).,3 Reinforcement Learning for Sentence Simplification,[0],[0]
"As a result, a generic encoder-decoder model learns to copy all too well at the expense of other rewrite operations, often parroting back the source or making only a few trivial changes.
",3 Reinforcement Learning for Sentence Simplification,[0],[0]
"To encourage a wider variety of rewrite operations while remaining fluent and faithful to the meaning of the source, we employ a reinforcement learning framework (see Figure 1).",3 Reinforcement Learning for Sentence Simplification,[0],[0]
"We view the encoder-decoder model as an agent which first reads the source sentence X; then at each step, it takes an action ŷt ∈ V (where V is the output vocabulary) according to a policy PRL(ŷt|ŷ1:t−1, X) (see Equation (2)).",3 Reinforcement Learning for Sentence Simplification,[0],[0]
"The agent continues to take actions until it produces an End Of Sentence (EOS) token yielding the action sequence Ŷ = (ŷ1, ŷ2, . . .",3 Reinforcement Learning for Sentence Simplification,[0],[0]
", ŷ|Ŷ |), which is also the simplified output of our model.",3 Reinforcement Learning for Sentence Simplification,[0],[0]
"A reward r is then received and the REINFORCE algorithm (Williams, 1992) is used to update the agent.",3 Reinforcement Learning for Sentence Simplification,[0],[0]
"In the following, we first introduce our reward and then present the details of the REINFORCE algorithm.",3 Reinforcement Learning for Sentence Simplification,[0],[0]
"The reward r(Ŷ ) for system output Ŷ is the weighted sum of the three components aimed at capturing key aspects of the target output, namely simplicity, relevance, and fluency:
r(Ŷ ) = λS rS + λR rR + λF",3.1 Reward,[0],[0]
"rF (7)
where λS , λR, λF ∈",3.1 Reward,[0],[0]
"[0, 1]; r(Ŷ ) is a shorthand for r(X,Y, Ŷ )",3.1 Reward,[0],[0]
"whereX is the source, Y the reference (or target), and Ŷ the system output.",3.1 Reward,[0],[0]
"rS , rR, and rF are shorthands for simplicity rS(X,Y, Ŷ ), relevance rR(X, Ŷ ), and fluency rF (Ŷ ).",3.1 Reward,[0],[0]
"We provide details for each reward summand below.
",3.1 Reward,[0],[0]
"Simplicity To encourage the model to apply a wide range of simplification operations, we use SARI (Xu et al., 2016), a recently proposed metric which compares System output Against References and against the Input sentence.",3.1 Reward,[0],[0]
"SARI is the arithmetic average of n-gram precision and recall of three rewrite operations: addition, copying, and deletion.",3.1 Reward,[0],[0]
It rewards addition operations where system output was not in the input but occurred in the references.,3.1 Reward,[0],[0]
"Analogously, it rewards words retained/deleted in both the system output and the references.",3.1 Reward,[0],[0]
"In experimental evaluation Xu et al. (2016) demonstrate that SARI correlates well with human judgments of simplicity, whilst correctly rewarding systems that both make changes and simplify the input.
",3.1 Reward,[0],[0]
One caveat with using SARI as a reward is the fact that it relies on the availability of multiple references which are rare for sentence simplification.,3.1 Reward,[0],[0]
"Xu et al. (2016) provide eight references for 2,350 sentences, but these are primarily for system tuning and evaluation rather than training.",3.1 Reward,[0],[0]
The majority of existing simplification datasets (see Section 5 for details) have a single reference for each source sentence.,3.1 Reward,[0],[0]
"Moreover, they are unavoidably noisy as they are mostly constructed automatically, e.g., by aligning sentences from the ordinary and simple English Wikipedias.",3.1 Reward,[0],[0]
"When relying solely on a single reference, SARI will try to reward accidental",3.1 Reward,[0],[0]
n-grams that should never have occurred in it.,3.1 Reward,[0],[0]
"To countenance the effect of noise, we apply SARI(X, Ŷ , Y ) in the expected direction, with X as the source, Ŷ the system output, and Y the reference as well as in the reverse direction with Y as the system output and Ŷ as the reference.",3.1 Reward,[0],[0]
"Assuming our system can produce reasonably good simplifications, by swapping the output
and the reference, reverse SARI can be used to estimate how good a reference is with respect to the system output.",3.1 Reward,[0],[0]
"Our first reward is therefore the weighted sum of SARI and reverse SARI:
rS=β SARI(X, Ŷ , Y )+(1−β)",3.1 Reward,[0],[0]
"SARI(X,Y, Ŷ ) (8)
Relevance",3.1 Reward,[0],[0]
"While the simplicity-based reward rS tries to encourage the model to make changes, the relevance reward rR ensures that the generated sentences preserve the meaning of the source.",3.1 Reward,[0],[0]
We use an LSTM sentence encoder to convert the source X and the predicted target Ŷ into two vectors qX and qŶ .,3.1 Reward,[0],[0]
"The relevance reward rR is simply the cosine similarity between these two vectors:
rR = cos(qX ,qŶ )",3.1 Reward,[0],[0]
= qX · qŶ ||qX || ||qŶ,3.1 Reward,[0],[0]
"||
(9)
",3.1 Reward,[0],[0]
We use a sequence auto-encoder (SAE; Dai and Le 2015) to train the LSTM sentence encoder on both the complex and simple sentences.,3.1 Reward,[0],[0]
"Specifically, the SAE uses sentence X =",3.1 Reward,[0],[0]
"(x1, . . .",3.1 Reward,[0],[0]
", x|X|) to infer itself via an encoder-decoder model (without an attention mechanism).",3.1 Reward,[0],[0]
"Firstly, an encoder LSTM convertsX into a sequence of hidden states (h1, . . .",3.1 Reward,[0],[0]
",h|X|).",3.1 Reward,[0],[0]
"Then, we use h|X| to initialize the hidden state of the decoder LSTM and recover/generate X one word at a time.
",3.1 Reward,[0],[0]
"Fluency Xu et al. (2016) observe that SARI correlates less with fluency compared to other metrics such as BLEU (Papineni et al., 2002).",3.1 Reward,[0],[0]
The fluency reward rF models the well-formedness of the generated sentences explicitly.,3.1 Reward,[0],[0]
"It is the normalized sentence probability assigned by an LSTM
language model trained on simple sentences:
rF = exp  1 |Ŷ | |Ŷ |∑ i=1",3.1 Reward,[0],[0]
"logPLM (ŷi|ŷ0:i−1)  (10)
",3.1 Reward,[0],[0]
We take the exponential of Ŷ ’s perplexity to ensure that rF ∈,3.1 Reward,[0],[0]
"[0, 1] as is the case with rS and rR.",3.1 Reward,[0],[0]
The goal of the REINFORCE algorithm is to find an agent that maximizes the expected reward.,3.2 The REINFORCE Algorithm,[0],[0]
"The training loss for one sequence is its negative expected reward:
L(θ) = −E(ŷ1,...,ŷ|Ŷ |)∼PRL(·|X)[r(ŷ1, . .",3.2 The REINFORCE Algorithm,[0],[0]
"., ŷ|Ŷ",3.2 The REINFORCE Algorithm,[0],[0]
"|)]
where PRL is our policy, i.e., the distribution produced by the encoder-decoder model (see Equation(2)) and r(·) is the reward function of an action sequence Ŷ = (ŷ1, . . .",3.2 The REINFORCE Algorithm,[0],[0]
", ŷ|Ŷ |), i.e., a generated simplification.",3.2 The REINFORCE Algorithm,[0],[0]
"Unfortunately, computing the expectation term is prohibitive, since there is an infinite number of possible action sequences.",3.2 The REINFORCE Algorithm,[0],[0]
"In practice, we approximate this expectation with a single sample from the distribution of PLR(·|X).",3.2 The REINFORCE Algorithm,[0],[0]
We refer to Williams (1992) for the full derivation of the gradients.,3.2 The REINFORCE Algorithm,[0],[0]
The gradient of L(θ) is: ∇L(θ) ≈∑|Ŷ,3.2 The REINFORCE Algorithm,[0],[0]
"|
t=1∇ logPRL(ŷt|ŷ1:t−1, X)[r(ŷ1:|Ŷ |)− bt]
To reduce the variance of gradients, we also introduce a baseline linear regression model bt to estimate the expected future reward at time t (Ranzato et al., 2016).",3.2 The REINFORCE Algorithm,[0],[0]
bt takes the concatenation of hTt and ct as input and outputs a real value as the expected reward.,3.2 The REINFORCE Algorithm,[0],[0]
"The parameters of the regressor are
trained by minimizing mean squared error.",3.2 The REINFORCE Algorithm,[0],[0]
"We do not back-propagate this error to hTt or ct during training (Ranzato et al., 2016).",3.2 The REINFORCE Algorithm,[0],[0]
"Presented in its original form, the REINFORCE algorithm starts learning with a random policy.",3.3 Learning,[0],[0]
"This assumption can make model training challenging for generation tasks like ours with large vocabularies (i.e., action spaces).",3.3 Learning,[0],[0]
"We address this issue by pre-training our agent (i.e., the encoderdecoder model) with a negative log-likelihood objective (see Section 2), making sure it can produce reasonable simplifications, thereby starting off with a policy which is better than random.",3.3 Learning,[0],[0]
"We follow prior work (Ranzato et al., 2016) in adopting a curriculum learning strategy.",3.3 Learning,[0],[0]
"In the beginning of training, we give little freedom to our agent allowing it to predict the last few words for each target sentence.",3.3 Learning,[0],[0]
"For every target sequence, we use negative log-likelihood to train the first L (initially, L = 24) tokens and apply the reinforcement learning algorithm to the (L + 1)th tokens onwards.",3.3 Learning,[0],[0]
"Every two epochs, we set L = L− 3 and the training terminates when L is 0.",3.3 Learning,[0],[0]
"Lexical substitution, the replacement of complex words with simpler alternatives, is an integral part of sentence simplification (Specia et al., 2012).",4 Lexical Simplification,[0],[0]
The model presented so far learns lexical substitution and other rewrite operations jointly.,4 Lexical Simplification,[0],[0]
"In some cases, words are predicted because they seem natural in the their context, but are poor substitutes for the content of the complex sentence.",4 Lexical Simplification,[0],[0]
"To countenance this, we learn lexical simplifications explicitly and integrate them with our reinforcement learning-based model.
",4 Lexical Simplification,[0],[0]
"We use an pre-trained encoder-decoder model (which is trained on a parallel corpus of complex and simple sentences) to obtain probabilistic word alignments, aka attention scores (see αt in Equation (6)).",4 Lexical Simplification,[0],[0]
Let X =,4 Lexical Simplification,[0],[0]
"(x1, x2, . . .",4 Lexical Simplification,[0],[0]
", x|X|) denote a source sentence and Y = (y1, y2, . . .",4 Lexical Simplification,[0],[0]
", y|Y |) a target sentence.",4 Lexical Simplification,[0],[0]
"We convert X into |X| hidden states (v1,v2, . . .",4 Lexical Simplification,[0],[0]
",v|X|) with an LSTM.",4 Lexical Simplification,[0],[0]
Note that vt ∈ Rd×1 corresponds to the context dependent representation of xt.,4 Lexical Simplification,[0],[0]
"Let αt denote the alignment scores αt1, αt2, . . .",4 Lexical Simplification,[0],[0]
", αt|X|.",4 Lexical Simplification,[0],[0]
"The lexical simplification probability of yt given the source sentence
and the alignment scores is:
PLS(yt|X,αt) =",4 Lexical Simplification,[0],[0]
"softmax(Wl st) (11)
where Wl ∈ R|V |×d and st represents the source:
st = |X|∑ i=1",4 Lexical Simplification,[0],[0]
"αtivi (12)
",4 Lexical Simplification,[0],[0]
"The lexical simplification model on its own encourages lexical substitutions, without taking into account what has been generated so far (i.e., y1:t−1) and as a result fluency could be compromised.",4 Lexical Simplification,[0],[0]
"A straightforward solution is to integrate lexical simplification with our reinforcement learning trained model (Section 3) using linear interpolation, where η ∈",4 Lexical Simplification,[0],[0]
"[0, 1]:
P (yt|y1:t−1, X) = (1− η)PRL(yt|y1:t−1, X) +",4 Lexical Simplification,[0],[0]
"η PLS(yt|X,αt) (13)",4 Lexical Simplification,[0],[0]
In this section we present our experimental setup for assessing the performance of the simplification model described above.,5 Experimental Setup,[0],[0]
"We give details on our datasets, model training, evaluation protocol, and the systems used for comparison.
",5 Experimental Setup,[0],[0]
Datasets We conducted experiments on three simplification datasets.,5 Experimental Setup,[0],[0]
"WikiSmall (Zhu et al., 2010) is a parallel corpus which has been extensively used as a benchmark for evaluating text simplification systems (Wubben et al., 2012; Woodsend and Lapata, 2011; Narayan and Gardent, 2014; Zhu et al., 2010).",5 Experimental Setup,[0],[0]
It contains automatically aligned complex and simple sentences from the ordinary and simple English Wikipedias.,5 Experimental Setup,[0],[0]
The test set consists of 100 complex-simple sentence pairs.,5 Experimental Setup,[0],[0]
"The training set contains 89,042 sentence pairs (after removing duplicates and test sentences).",5 Experimental Setup,[0],[0]
"We randomly sampled 205 pairs for development and used the remaining sentences for training.
",5 Experimental Setup,[0],[0]
"We also constructed WikiLarge, a larger Wikipedia corpus by combining previously created simplification corpora.",5 Experimental Setup,[0],[0]
"Specifically, we aggregated the aligned sentence pairs in Kauchak (2013), the aligned and revision sentence pairs in Woodsend and Lapata (2011), and Zhu’s (2010) WikiSmall dataset described above.",5 Experimental Setup,[0],[0]
We used the development and test sets created in Xu et al. (2016).,5 Experimental Setup,[0],[0]
These are complex sentences taken from WikiSmall paired with simplifications provided by Amazon Mechanical Turk workers.,5 Experimental Setup,[0],[0]
"The dataset
contains 8 (reference) simplifications for 2,359 sentences partitioned into 2,000 for development and 359 for testing.",5 Experimental Setup,[0],[0]
"After removing duplicates and sentences in development and test sets, the resulting training set contains 296,402 sentence pairs.
",5 Experimental Setup,[0],[0]
"Our third dataset is Newsela, a corpus collated by Xu et al. (2015b) who argue that Wikipediabased resources are suboptimal due to the automatic sentence alignment which unavoidably introduces errors, and their uniform writing style which leads to systems that generalize poorly.",5 Experimental Setup,[0],[0]
"Newsela2 consists of 1,130 news articles, each rewritten four times by professional editors for children at different grade levels (0 is the most complex level and 4 is simplest).",5 Experimental Setup,[0],[0]
Xu et al. (2015b) provide multiple aligned complex-simple pairs within each article.,5 Experimental Setup,[0],[0]
"We removed sentence pairs corresponding to levels 0–1, 1–2, and 2–3, since they were too similar to each other.",5 Experimental Setup,[0],[0]
"The first 1,070 documents were used for training (94,208 sentence pairs), the next 30 documents for development (1,129 sentence pairs) and the last 30 documents for testing (",5 Experimental Setup,[0],[0]
"1,076 sentence pairs).3 We are not aware of any published results on this dataset.
",5 Experimental Setup,[0],[0]
Training Details,5 Experimental Setup,[0],[0]
We trained our models on an Nvidia GPU card.,5 Experimental Setup,[0],[0]
We used the same hyperparameters across datasets.,5 Experimental Setup,[0],[0]
"We first trained an encoder-decoder model, and then performed reinforcement learning training (Section 3), and trained the lexical simplification model (Section 4).",5 Experimental Setup,[0],[0]
"Encoder-decoder parameters were uniformly initialized to [−0.1, 0.1].",5 Experimental Setup,[0],[0]
"We used Adam (Kingma and Ba, 2014) to optimize the model with learning rate 0.001; the first momentum coefficient was set to 0.9 and the second momentum coefficient to 0.999.",5 Experimental Setup,[0],[0]
"The gradient was rescaled when the norm exceeded 5 (Pascanu et al., 2013).",5 Experimental Setup,[0],[0]
Both encoder and decoder LSTMs have two layers with 256 hidden neurons in each layer.,5 Experimental Setup,[0],[0]
"We regularized all LSTMs with a dropout rate of 0.2 (Zaremba et al., 2014).",5 Experimental Setup,[0],[0]
"We initialized the encoder and decoder word embedding matrices with 300 dimensional Glove vectors (Pennington et al., 2014).
",5 Experimental Setup,[0],[0]
"During reinforcement training, we used plain stochastic gradient descent with a learning rate of 0.01.",5 Experimental Setup,[0],[0]
"We set β = 0.1, λS = 1, λR = 0.25 and λF = 0.5.4 Training details for the lexical
2https://newsela.com 3If a sentence has multiple references in the development or test set, we use the reference with highest simplicity level.",5 Experimental Setup,[0],[0]
"4Weights were tuned on the development set of the Newsela dataset and kept fixed for the other two datasets.
simplification model are identical to the encoderdecoder model except that word embedding matrices were randomly initialized.",5 Experimental Setup,[0],[0]
"The weight of the lexical simplification model was set to η = 0.1.
",5 Experimental Setup,[0],[0]
"To reduce vocabulary size, named entities were tagged with the Stanford CoreNLP (Manning et al., 2014) and anonymized with a NE@N token, where NE ∈ {PER,LOC,ORG,MISC} and N indicates NE@N is the N -th distinct NE typed entity.",5 Experimental Setup,[0],[0]
"For example, “John and Bob are . . .",5 Experimental Setup,[0],[0]
” becomes “PER@1 and PER@2 are . . .,5 Experimental Setup,[0],[0]
”.,5 Experimental Setup,[0],[0]
"At test time, we de-anonymize NE@N tokens in the output by looking them up in their source sentences.",5 Experimental Setup,[0],[0]
"Note that the de-anonymization may fail, but the chance is small (around 2% of the time on the Newsela development set).",5 Experimental Setup,[0],[0]
We replaced words occurring three times or less in the training set with UNK.,5 Experimental Setup,[0],[0]
"At test time, when our models predict UNK, we adopt the UNK replacement method proposed in Jean et al. (2015).
",5 Experimental Setup,[0],[0]
"Evaluation Following previous work (Woodsend and Lapata, 2011; Xu et al., 2016)",5 Experimental Setup,[0],[0]
we evaluated system output automatically adopting metrics widely used in the simplification literature.,5 Experimental Setup,[0],[0]
"Specifically, we used BLEU5 (Papineni et al., 2002) to assess the degree to which generated simplifications differed from gold standard references and the Flesch-Kincaid Grade Level index (FKGL; Kincaid et al. 1975) to measure the readability of the output (lower FKGL6 implies simpler output).",5 Experimental Setup,[0],[0]
"In addition, we used SARI (Xu et al., 2016), which evaluates the quality of the output by comparing it against the source and reference simplifications.7 BLEU, FKGL, and SARI are all measured at corpus-level.",5 Experimental Setup,[0],[0]
We also evaluated system output by eliciting human judgments via Amazon’s Mechanical Turk.,5 Experimental Setup,[0],[0]
"Specifically (selfreported) native English speakers were asked to rate simplifications on three dimensions: Fluency (is the output grammatical and well formed?),",5 Experimental Setup,[0],[0]
Adequacy (to what extent is the meaning expressed in the original sentence preserved in the output?) and Simplicity (is the output simpler than the original sentence?).,5 Experimental Setup,[0],[0]
"All ratings were obtained using a five point Likert scale.
",5 Experimental Setup,[0],[0]
Comparison Systems We compared our model against several systems previously proposed in the literature.,5 Experimental Setup,[0],[0]
"These include PBMT-R, a mono-
5With the default mtevalv13a.pl settings.",5 Experimental Setup,[0],[0]
6FKGL implementation at http://goo.gl/OHP7k3.,5 Experimental Setup,[0],[0]
"7We used he implementation of SARI in Xu et al. (2016).
",5 Experimental Setup,[0],[0]
"lingual phrase-based machine translation system with a reranking post-processing step8 (Wubben et al., 2012) and Hybrid, a model which first performs sentence splitting and deletion operations over discourse representation structures and then further simplifies sentences with PBMT-R (Narayan and Gardent, 2014).",5 Experimental Setup,[0],[0]
Hybrid9 is state of the art on the WikiSmall dataset.,5 Experimental Setup,[0],[0]
"Comparisons with SBMT-SARI, a syntax-based translation model trained on PPDB (Ganitkevitch et al., 2013) and tuned with SARI (Xu et al., 2016), are problematic due to the size of PPDB which is considerably larger than any of the datasets used in this work (it contains 106 million sentence pairs with 2 billion words).",5 Experimental Setup,[0],[0]
"Nevertheless, we compare10 against SBMT-SARI, but only models trained on Wikilarge, our largest dataset.",5 Experimental Setup,[0],[0]
"Since Newsela contains high quality simplifications created by professional editors, we performed the bulk of our experiments on this dataset.",6 Results,[0],[0]
"Specifically, we set out to answer two questions: (a) which neural model performs best and (b) how do neural models which are resource lean and do not have access to linguistic annotations fare against more traditional systems.",6 Results,[0],[0]
"We therefore compared the basic attention-based encoder-
8We made a good-faith effort to re-implement their system following closely the details in Wubben et al. (2012).
",6 Results,[0],[0]
"9We are grateful to Shashi Narayan for running his system on our three datasets.
",6 Results,[0],[0]
"10The output of SBMT-SARI is publicly available.
",6 Results,[0],[0]
"decoder model (EncDecA), with the deep reinforcement learning model (DRESS; Section 3), and a linear combination of DRESS and the lexical simplification model (DRESS-LS; Section 4).",6 Results,[0],[0]
"Neural models were further compared against two strong baselines, PBMT-R and Hybrid.",6 Results,[0],[0]
"Table 3 shows example output of all models on the Newsela dataset.
",6 Results,[0],[0]
The top block in Table 1 summarizes the results of our automatic evaluation.,6 Results,[0],[0]
"As can be seen, all neural models obtain higher BLEU, lower FKGL and higher SARI compared to PBMT-R. Hybrid has the lowest FKGL and highest SARI.",6 Results,[0],[0]
"Compared to EncDecA, DRESS scores lower on FKGL and higher on SARI, which indicates that the model has indeed learned to optimize the reward function which includes SARI.",6 Results,[0],[0]
"Integrating lexical simplification (DRESS-LS) yields better BLEU, but slightly worse FKGL and SARI.
",6 Results,[0],[0]
The results of our human evaluation are presented in the top block of Table 2.,6 Results,[0],[0]
We elicited judgments for 100 randomly sampled test sentences.,6 Results,[0],[0]
"Aside from comparing system output (PBMT-R, Hybrid, EncDecA, DRESS, and DRESS-LS), we also elicited ratings for the gold standard Reference as an upper bound.",6 Results,[0],[0]
"We report results for Fluency, Adequacy, and Simplicity individually and in combination (All is the average rating of the three dimensions).",6 Results,[0],[0]
"As can be seen, DRESS and DRESS-LS outperform PBMT-R and
Hybrid on Fluency, Simplicity, and overall.",6 Results,[0],[0]
"The fact that neural models (EncDecA, DRESS and DRESS-LS) fare well on Fluency, is perhaps not surprising given the recent success of LSTMs in language modeling and neural machine translation (Zaremba et al., 2014; Jean et al., 2015).
",6 Results,[0],[0]
Neural models obtain worse ratings on Adequacy but are closest to the human references on this dimension.,6 Results,[0],[0]
"DRESS-LS (and DRESS) are significantly better (p < 0.01) on Simplicity than EncDecA, PBMT-R, and Hybrid which indicates that our reinforcement learning based model is effective at creating simpler output.",6 Results,[0],[0]
Combined ratings (All) for DRESS-LS are significantly different compared to the other models but not to DRESS and the Reference.,6 Results,[0],[0]
"Nevertheless, integration of the lexical simplification model boosts performance as ratings increase almost across the board (Simplicity is slightly worse).",6 Results,[0],[0]
"Returning to our original questions, we find that neural models are more fluent than comparison systems, while performing non-trivial rewrite operations (see the SARI
scores in Table 1) which yield simpler output (see the Simplicity column in Table 2).",6 Results,[0],[0]
"Based on our judgment elicitation study, neural models trained with reinforcement learning perform best, with DRESS-LS having a slight advantage.
",6 Results,[0],[0]
We further analyzed model performance by computing various statistics on the simplified output.,6 Results,[0],[0]
We measured average sentence length and the degree to which DRESS and comparison systems perform rewriting operations.,6 Results,[0],[0]
"We approximated the latter with Translation Error Rate (TER; Snover et al. 2006), a measure commonly used to automatically evaluate the quality of machine translation output.",6 Results,[0],[0]
We used TER to compute the (average) number of edits required to change an original complex sentence to simpler output.,6 Results,[0],[0]
"We also report the number of edits by type, i.e., the number of insertions, substitutions, deletions, and shifts needed (on average) to convert complex to simple sentences.
",6 Results,[0],[0]
"As shown in Table 4, Hybrid obtains the highest TER, followed by our models (DRESS and
DRESS-LS), which indicates that they actively perform rewriting.",6 Results,[0],[0]
"Perhaps Hybrid is too aggressive when simplifying a sentence, it obtains low Fluency and Adequacy scores in human evaluation (Table 2).",6 Results,[0],[0]
"There is a strong correlation between sentence length and number of deletion operations (i.e., more deleteions lead to shorter sentences) and PBMT-R performs very few deletions.",6 Results,[0],[0]
"Overall, reinforcement learning encourages deletion (see DRESS and DRESS-LS), while performing a reasonable amount of additional operations (e.g., substitutions and shifts) compared to EncDecA and PBMT-R.
The middle blocks in Tables 1 and 2 report results on the WikiSmall dataset.",6 Results,[0],[0]
FKGL and SARI follow a similar pattern as on Newsela.,6 Results,[0],[0]
"BLEU scores for PBMT-R, Hybrid, and EncDecA are much higher compared to DRESS and DRESS-LS.",6 Results,[0],[0]
"Hybrid obtains best BLEU and SARI scores, while DRESS and DRESS-LS do very well on FKGL.",6 Results,[0],[0]
"In human evaluation, we elicited judgments on the entire WikiSmall test set (100 sentences).",6 Results,[0],[0]
"We compared DRESS-LS, with PBMT-R, Hybrid, and gold standard Reference simplifications.",6 Results,[0],[0]
"As human experiments are time consuming and expensive, we did not include other neural models besides DRESS-LS based on our Newsela study which showed that EncDecA is inferior to variants trained with reinforcement learning and that DRESS-LS is the better performing model (however, we do compare all models in Table 1).",6 Results,[0],[0]
"DRESS-LS is significantly better on Simplicity than PBMT-R, Hybrid, and the Reference.",6 Results,[0],[0]
It performs on par with PBMT-R on Fluency and worse on Adequacy (but still closer to the human Reference than PBMT-R or Hybrid).,6 Results,[0],[0]
"When combining all ratings (All in Table 2), DRESS-LS is significantly better than PBMT-R, Hybrid, and the Reference.
",6 Results,[0],[0]
The bottom blocks in Tables 1 and 2 report results on Wikilarge.,6 Results,[0],[0]
"We compared our models with PBMT-R, Hybrid, and SBMT-SARI (Xu et al., 2016).",6 Results,[0],[0]
The FKGL follows a similar pattern as in the previous datasets.,6 Results,[0],[0]
"PBMT-R and our models are best in terms of BLEU while SBMT-SARI outperforms all other systems on SARI.11 Because there are 8 references for each complex sentence in the test set, BLEU scores are much higher compared to Newsela and WikiSmall.",6 Results,[0],[0]
"In human evaluation, we again elicited judgments for 100 randomly sampled test sentences.",6 Results,[0],[0]
We randomly selected one of the 8 references as the Reference upper bound.,6 Results,[0],[0]
"On Simplicity, DRESS-LS is significantly better than all comparison systems, except Hybrid.",6 Results,[0],[0]
"On Adequacy, it is better than Hybrid but significantly worse than other comparison systems.",6 Results,[0],[0]
"On Fluency, it is on par with PBMT-R12 but better than Hybrid and SBMT-SARI.",6 Results,[0],[0]
On All dimension DRESS-LS significantly outperforms all comparison systems.,6 Results,[0],[0]
"We developed a reinforcement learning-based text simplification model, which can jointly model simplicity, grammaticality, and semantic fidelity to the input.",7 Conclusions,[0],[0]
We also proposed a lexical simplification component that further boosts performance.,7 Conclusions,[0],[0]
"Overall, we find that reinforcement learning offers a great means to inject prior knowledge to the simplification task achieving good results across three datasets.",7 Conclusions,[0],[0]
"In the future, we would like to explicitly model sentence splitting and simplify entire documents (rather than individual sentences).",7 Conclusions,[0],[0]
"Beyond sentence simplification, the reinforcement learning framework presented here is potentially applicable to generation tasks such as sentence compression (Chopra et al., 2016), generation of programming code (Ling et al., 2016), or poems (Zhang and Lapata, 2014).
",7 Conclusions,[0],[0]
"Acknowledgments We would like to thank Li Dong, Jianpeng Cheng, Shashi Narayan and the EMNLP reviewers for their valuable feedback.",7 Conclusions,[0],[0]
We are also grateful to Shashi Narayan for supplying us with the output of his system and Wei Xu for her help with this work.,7 Conclusions,[0],[0]
"The authors acknowledge the support of the European Research Council (award number 681760).
",7 Conclusions,[0],[0]
"11BLEU and SARI scores reported in Xu et al. (2016) are 72.36 and 37.91, and measured at sentence-level.
",7 Conclusions,[0],[0]
12We used more data to train PBMT-R and maybe that is why PBMT-R performs better than Xu et al. (2016) reported.,7 Conclusions,[0],[0]
Sentence simplification aims to make sentences easier to read and understand.,abstractText,[0],[0]
Most recent approaches draw on insights from machine translation to learn simplification rewrites from monolingual corpora of complex and simple sentences.,abstractText,[0],[0]
We address the simplification problem with an encoder-decoder model coupled with a deep reinforcement learning framework.,abstractText,[0],[0]
"Our model, which we call DRESS (as shorthand for Deep REinforcement Sentence Simplification), explores the space of possible simplifications while learning to optimize a reward function that encourages outputs which are simple, fluent, and preserve the meaning of the input.",abstractText,[0],[0]
Experiments on three datasets demonstrate that our model outperforms competitive simplification systems.1,abstractText,[0],[0]
Sentence Simplification with Deep Reinforcement Learning,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 1156–1168 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,[0],[0]
"Sentences with gapping (Ross, 1970) such as Paul likes coffee and Mary tea are characterized by having one or more conjuncts that contain multiple arguments or modifiers of an elided predicate.",1 Introduction,[0],[0]
"In this example, the predicate likes is elided for the relation Mary likes tea.",1 Introduction,[0],[0]
"While these sentences appear relatively infrequently in most written texts, they are often used to convey a lot of factual information that is highly relevant for language understanding (NLU) tasks such as open information extraction and semantic parsing.",1 Introduction,[0],[0]
"For example, consider the following sentence from the WSJ portion of the Penn Treebank (Marcus et al., 1993).
",1 Introduction,[0],[0]
"(1) Unemployment has reached 27.6% in Azerbaijan, 25.7% in Tadzhikistan, 22.8% in Uzbekistan, 18.8% in Turkmenia, 18% in Armenia and 16.3% in Kirgizia, [...]
To extract the information about unemployment rates in the various countries, an NLU system has to identify that the percentages indicate unemployment rates and the locational modifiers indicate the corresponding country.",1 Introduction,[0],[0]
"Given only this sentence, or this sentence and a strict surface syntax representation that does not indicate elided predicates, this is a challenging task.",1 Introduction,[0],[0]
"However, given a dependency graph that reconstructs the elided predicate for each conjunct, the problem becomes much easier and methods developed to extract information from dependency trees of clauses with canonical structures are much more likely to extract the correct information from a gapped clause.
",1 Introduction,[0],[0]
"While gapping constructions receive a lot of attention in the theoretical syntax literature (e.g., Ross 1970; Jackendoff 1971; Steedman 1990; Coppock 2001; Osborne 2006; Johnson 2014; Toosarvandani 2016; Kubota and Levine 2016), they have been almost entirely neglected by the NLP community so far.",1 Introduction,[0],[0]
"The Penn Treebank explicitly annotates gapping constructions, by coindexing arguments in the clause with a predicate and the clause with the gap, but these co-indices are not included in the standard parsing metrics
1156
and almost all parsers ignore them.1",1 Introduction,[0],[0]
"Despite the sophisticated analysis of gapping within CCG (Steedman, 1990), sentences with gapping were deemed too difficult to represent within the CCGBank (Hockenmaier and Steedman, 2007).",1 Introduction,[0],[0]
"Similarly the treebanks for the Semantic Dependencies Shared Task (Oepen et al., 2015) exclude all sentences from the Wall Street Journal that contain gapping.",1 Introduction,[0],[0]
"Finally, while the tectogrammatical layer of the Prague Dependency Treebank (Bejček et al., 2013) as well as the enhanced Universal Dependencies (UD) representation (Nivre et al., 2016) provide an analysis with reconstructed nodes for gapping constructions, there exist no methods to automatically parse to these representations.
",1 Introduction,[0],[0]
"Here, we provide the first careful analysis of parsing of gapping constructions, and we present two methods for reconstructing elided predicates in sentences with gapping within the UD framework.",1 Introduction,[0],[0]
"As illustrated in Figure 1, we first parse to a dependency tree and then reconstruct the elided material.",1 Introduction,[0],[0]
The methods differ in how much information is encoded in the dependency tree.,1 Introduction,[0],[0]
"The first method adapts an existing procedure for parsing sentences with elided function words (Seeker et al., 2012), which uses composite labels that can be deterministically turned into dependency graphs in most cases.",1 Introduction,[0],[0]
"The second method is a novel procedure that relies on the parser only to identify a gap, and then employs an unsupervised method to reconstruct the elided predicates and reattach the arguments to the reconstructed predicate.",1 Introduction,[0],[0]
We find that both methods can reconstruct elided predicates with very high accuracy from gold standard dependency trees.,1 Introduction,[0],[0]
"When applied to the output of a parser, which often fails to identify gapping, our methods achieve a sentence-level accuracy of 32% and 34%, significantly outperforming the recently proposed constituent parser by Kummerfeld and Klein (2017).",1 Introduction,[0],[0]
"Gapping constructions in English come in many forms that can be broadly classified as follows.
",2.1 Gapping constructions,[0],[0]
"1 To the best of our knowledge, the parser by Kummerfeld and Klein (2017) is the only parser that tries to output the co-indexing of constituents in clauses with gapping but they lack an explicit evaluation of their co-indexing prediction accuracy.
(2) Single predicate gaps: John bought books, and Mary flowers.
(3) Contiguous predicate-argument gap (including ACCs): Eve gave flowers to Al and Sue to Paul.",2.1 Gapping constructions,[0],[0]
"Eve gave a CD to Al and roses to Sue.
(4) Non-contiguous predicate-argument gap: Arizona elected Goldwater Senator, and Pennsylvania Schwelker .
",2.1 Gapping constructions,[0],[0]
"(Jackendoff, 1971)
(5) Verb cluster gap: I want to try to begin to write a novel and
...",2.1 Gapping constructions,[0],[0]
Mary a play. ...,2.1 Gapping constructions,[0],[0]
"Mary to write a play.
...",2.1 Gapping constructions,[0],[0]
Mary to begin to write a play. ...,2.1 Gapping constructions,[0],[0]
"Mary to try to begin to write a play.
",2.1 Gapping constructions,[0],[0]
"(Ross, 1970)
",2.1 Gapping constructions,[0],[0]
The defining characteristic of gapping constructions is that there is a clause that lacks a predicate (the gap) but still contains two or more arguments or modifiers of the elided predicate (the remnants or orphans).,2.1 Gapping constructions,[0],[0]
"In most cases, the remnants have a corresponding argument or modifier (the correspondent) in the clause with the overt predicate.
",2.1 Gapping constructions,[0],[0]
These types of gapping also make up the majority of attested constructions in other languages.,2.1 Gapping constructions,[0],[0]
"However, Wyngaerd (2007) notes that Dutch permits gaps in relative clauses, and Farudi (2013) notes that Farsi permits gaps in finite embedded clauses even if the overt predicate is not embedded.2",2.1 Gapping constructions,[0],[0]
"We work within the UD framework, which aims to provide cross-linguistically consistent dependency annotations that are useful for NLP tasks.",2.2 Target representation,[0],[0]
"UD defines two types of representation: the basic UD representation which is a strict surface syntax dependency tree and the enhanced UD representation (Schuster and Manning, 2016) which may be a graph instead of a tree and may contain additional nodes.",2.2 Target representation,[0],[0]
"The analysis of gapping in the enhanced representation makes use of copy nodes for elided predicates and additional edges for elided arguments, which we both try to automatically reconstruct in this paper.",2.2 Target representation,[0],[0]
"In the simple case in which only one predicate was elided, there is exactly one
2See Johnson (2014) or Schuster et al. (2017) for a more comprehensive overview of cross-linguistically attested gapping constructions.
copy node for the elided predicate, which leads to a structure that is identical to the structure of the same sentence without a gap.3
John bought books and Mary bought′ flowers
nsubj",2.2 Target representation,[0],[0]
"obj cc nsubj
conj
obj
If a clause contains a more complex gap, the enhanced representation contains copies for all content words that are required to attach the remnants.
... and Mary wanted′ try′ begin′ write′",2.2 Target representation,[0],[0]
"a play
cc conj
xcomp xcomp xcomp det
obj
The motivation behind this analysis is that the semantically empty markers to are not needed for interpreting the sentence and minimizing the number of copy nodes leads to less complex graphs.
",2.2 Target representation,[0],[0]
"Finally, if a core argument was elided along with the predicate, we introduce additional dependencies between the copy nodes and the shared arguments, as for example, the open clausal complement (xcomp) dependency between the copy node and Senator in the following example.
",2.2 Target representation,[0],[0]
"AZ elected G. Senator and PA elected′ S.
nsubj",2.2 Target representation,[0],[0]
"obj
xcomp cc
nsubj
conj
obj
xcomp
The rationale for not copying all arguments is again to keep the graph simple, while still encoding all relations between content words.",2.2 Target representation,[0],[0]
"Arguments can be arbitrarily complex and it seems misguided to copy entire subtrees of arguments which, e.g., could contain multiple adverbial clauses.",2.2 Target representation,[0],[0]
Note that linking to existing nodes would not work in the case of verb clusters because they do not satisfy the subtree constraint.,2.2 Target representation,[0],[0]
"Our first method adapts one of the procedures by Seeker et al. (2012), which represents gaps in dependency trees by attaching dependents of an elided predicate with composite relations.",3.1 Composite relations,[0],[0]
"These relations represent the dependency path that would
3To enhance the readability of our examples, we place the copy node in the sentence where the elided predicate would have been pronounced.",3.1 Composite relations,[0],[0]
"However, as linear order typically does not matter for extracting information with dependency patterns, our procedures only try to recover the structure of canonical sentences but not their linear order.
have existed if nothing had been elided.",3.1 Composite relations,[0],[0]
"For example, in the following sentence, the verb bought, which would have been attached to the head of the first conjunct with a conj relation, was elided from the second conjunct and hence all nodes that would have depended on the elided verb, are attached to the first conjunct using a composite relation consisting of conj and the type of argument.
",3.1 Composite relations,[0],[0]
"John bought books and Mary flowers
nsubj",3.1 Composite relations,[0],[0]
"obj
conj>cc
conj>nsubj
conj>obj
The major advantage of this approach is that the dependency tree contains information about the types of arguments and so it should be straightforward to turn dependency trees of this form into enhanced UD graphs.",3.1 Composite relations,[0],[0]
"For most dependency trees, one can obtain the enhanced UD graph by splitting the composite relations into its atomic parts and inserting copy nodes at the splitting points.4
At the same time, this approach comes with the drawback of drastically increasing the label space.",3.1 Composite relations,[0],[0]
"For sentences with more complex gaps as in (5), one has to use composite relations that consist of more than two atomic relations and theoretically, the number of composite relations is unbounded:
... and Mary a play
det
conj>xcomp>xcomp>xcomp>obj
conj>nsubj
conj>cc",3.1 Composite relations,[0],[0]
"Our second method also uses a two-step approach to resolve gaps, but compared to the previous method, it puts less work on the parser.",3.2 Orphan procedure,[0],[0]
"We first parse sentences to the basic UD v2 representation, which analyzes gapping constructions as follows.",3.2 Orphan procedure,[0],[0]
One remnant is promoted to be the head of the clause and all other remnants are attached to the promoted phrase.,3.2 Orphan procedure,[0],[0]
"For example, in this sentence, the subject of the second clause, Mary, is the head of the clause and the other remnant, flowers, is attached to Mary with the special orphan relation:
John bought books and Mary flowers
nsubj",3.2 Orphan procedure,[0],[0]
"obj cc conj orphan
4Note that this representation does not indicate conjunct boundaries, and for sentences with multiple gapped conjuncts, it is thus unclear how many copy nodes are required.
",3.2 Orphan procedure,[0],[0]
"This analysis can also be used for more complex gaps, as in the example with a gap that consists of a chain of non-finite embedded verbs in (5).
... and Mary a play
cc
conj
det
orphan
When parsing to this representation, the parser only has to identify that there is a gap but does not have to recover the elided material or determine the type of remnants.",3.2 Orphan procedure,[0],[0]
"As a second step, we use an unsupervised procedure to determine which nodes to copy and how and where to attach the remnants.",3.2 Orphan procedure,[0],[0]
"In developing this procedure, we made use of the fact that in the vast majority of cases, all arguments and modifiers that are expressed in gapped conjunct are also expressed in the full conjunct.",3.2 Orphan procedure,[0],[0]
The problem of determining which nodes to copy and which relations to use can thus be reduced to the problem of aligning arguments in the gapped conjunct to arguments in the full conjunct.,3.2 Orphan procedure,[0],[0]
"We apply the following procedure to all sentences that contain at least one orphan relation.
1.",3.2 Orphan procedure,[0],[0]
"Create a list F of arguments of the head of the full conjunct by considering all core argument dependents of the conjunct’s head as well as clausal and nominal non-core dependents, and adverbial modifiers.
2.",3.2 Orphan procedure,[0],[0]
"Create a list G of arguments in the gapped conjunct that contains the head of the gapped conjunct and all its orphan dependents.
3.",3.2 Orphan procedure,[0],[0]
"Find the highest-scoring monotonic alignment of arguments in G to arguments in F .
4.",3.2 Orphan procedure,[0],[0]
"Copy the head of the full conjunct and attach the copy node c to the head of the full conjunct with the original relation of the head of the gapped conjunct (usually conj).
5.",3.2 Orphan procedure,[0],[0]
"For each argument g ∈ G that has been aligned to f ∈ F , attach g to c with the same relation as the parent relation of f , e.g., if f is attached to the head of the full conjunct with an nsubj relation, also attach g to c with an nsubj relation.",3.2 Orphan procedure,[0],[0]
"Attach arguments g′ ∈ G that were not aligned to any token in F to c using the general dep relation.
6.",3.2 Orphan procedure,[0],[0]
"For each copy node c, add dependencies to all core arguments of the original node which do not have a corresponding remnant in the gapped clause.",3.2 Orphan procedure,[0],[0]
"For example, if the full conjunct contains a subject, an object, and an
oblique modifier but the clause with the gap, only a subject and an oblique modifier, add an object dependency between the copy node and the object in the full conjunct.
",3.2 Orphan procedure,[0],[0]
"A crucial step is the third step, determining the highest-scoring alignment.",3.2 Orphan procedure,[0],[0]
"This can be done straightforwardly with the sequence alignment algorithm by Needleman and Wunsch (1970) if one defines a similarity function sim(g, f) that returns a similarity score between the arguments g and f .",3.2 Orphan procedure,[0],[0]
"We defined sim based on the intuitions that often, parallel arguments are of the same syntactic category, that they are introduced by the same function words (e.g., the same preposition), and that they are closely related in meaning.",3.2 Orphan procedure,[0],[0]
"The first intuition can be captured by penalizing mismatching POS tags, and the other two by computing the distance between argument embeddings.",3.2 Orphan procedure,[0],[0]
We compute these embeddings by averaging over the 100- dim.,3.2 Orphan procedure,[0],[0]
"pretrained GloVe (Pennington et al., 2014) embeddings for each token in the argument.",3.2 Orphan procedure,[0],[0]
"Given the POS tags tg and tf and the argument embeddings vg and vf , sim is defined as follows.5
sim(g, f) = −‖vg",3.2 Orphan procedure,[0],[0]
− vf‖2 + 1,3.2 Orphan procedure,[0],[0]
"[tg = tf ] × pos_mismatch_penalty
We set pos_mismatch_penalty, a parameter that penalizes mismatching POS tags, to −2.6
This procedure can be used for almost all sentences with gapping constructions.",3.2 Orphan procedure,[0],[0]
"However, if parts of an argument were elided along with the main predicate, it can become necessary to copy multiple nodes.",3.2 Orphan procedure,[0],[0]
We therefore consider the alignment not only between complete arguments in the full clause and the gapped clause but also between partial arguments in the full clause and the complete arguments in the gapped clause.,3.2 Orphan procedure,[0],[0]
"For example, for the sentence “Mary wants to write a play and Sue a book” the complete arguments of the full clause are {Mary, to write a play} and the arguments of the gapped clause are {Sue, a book}.",3.2 Orphan procedure,[0],[0]
"In this case, we also consider the partial arguments {Mary, a play} and if the arguments of the gapped
5As suggested by one of the reviewers, we also ran a posthoc experiment with a simpler similarity score function without the embedding distance term, which only takes into account whether the POS tags match.",3.2 Orphan procedure,[0],[0]
"We found that quantitatively, the embeddings do not lead to significant better scores on the test set according to our metrics but qualitatively, they lead to better results for the examples with verb cluster gaps.
6We optimized this parameter on the training set by trying integer values from −1 to −15.
conjunct align better to the partial arguments, we use this alignment.",3.2 Orphan procedure,[0],[0]
"However, now that the token write is part of the dependency path between want and play, we also have to make a copy of write to reconstruct the UD graph of the gapped clause.",3.2 Orphan procedure,[0],[0]
Both methods rely on a dependency parser followed by a post-processing step.,4 Experiments,[0],[0]
We evaluated the individual steps and the end-to-end performance.,4 Experiments,[0],[0]
"We used the UD English Web Treebank v2.1 (henceforth EWT; Silveira et al., 2014; Nivre et al., 2017) for training and evaluating parsers.",4.1 Data,[0],[0]
"As the treebank is relatively small and therefore only contains very few sentences with gapping, we also extracted gapping constructions from the WSJ and Brown portions of the PTB (Marcus et al., 1993) and the GENIA corpus (Ohta et al., 2002).",4.1 Data,[0],[0]
"Further, we copied sentences from the Wikipedia page on gapping7 and from published papers on gapping.",4.1 Data,[0],[0]
"The sentences in the EWT already contain annotations with the orphan relation and copy nodes for the enhanced representation, and we manually added both of these annotations for the remaining examples.",4.1 Data,[0],[0]
"The composite relations can
7https://en.wikipedia.org/wiki/Gapping, accessed on Aug 24, 2017.
be automatically obtained from the enhanced representation by removing the copy nodes and concatenating the dependency labels, which we did to build the training and test corpus for the composite relation procedure.",4.1 Data,[0],[0]
"Table 1 shows properties of the data splits of the original treebank, the additional sentences with gapping, and their combination; Table 2 shows the number of sentences in our corpus for each of the gap types.",4.1 Data,[0],[0]
Parser We used the parser by Dozat and Manning (2017) for parsing to the two different intermediate dependency representations.,4.2 Parsing experiments,[0],[0]
"This parser is a graph-based parser (McDonald et al., 2005) that uses a biLSTM to compute token representations and then uses a multi-layer perceptron with biaffine attention to compute arc and label scores.
",4.2 Parsing experiments,[0],[0]
"Setup We trained the parser on the COMBINED training corpus with gold tokenization, and predicted fine-grained and universal part-of-speech tags, for which we used the tagger by Dozat et al. (2017).",4.2 Parsing experiments,[0],[0]
We trained the tagger on the COMBINED training corpus.,4.2 Parsing experiments,[0],[0]
"As pre-trained embeddings, we used the word2vec (Mikolov et al., 2013) embeddings that were provided for the CoNLL 2017 Shared Task (Zeman et al., 2017), and we used the same hyperparameters as Dozat et al. (2017).
",4.2 Parsing experiments,[0],[0]
Evaluation We evaluated the parseability of the two dependency representations using labeled and unlabeled attachment scores (LAS and UAS).,4.2 Parsing experiments,[0],[0]
"Further, to specifically evaluate how well parsers are able to parse gapping constructions according to the two annotation schemes, we also computed the LAS and UAS just for the head tokens of remnants (LASg and UASg).",4.2 Parsing experiments,[0],[0]
"For all our metrics, we excluded punctuation tokens.",4.2 Parsing experiments,[0],[0]
"To determine sta-
tistical significance of pairwise comparisons, we performed two-tailed approximate randomization tests (Noreen, 1989; Yeh, 2000) with an adapted version of the sigf package (Padó, 2006).
",4.2 Parsing experiments,[0],[0]
Results Table 3 shows the overall parsing results on the development and test sets of the two treebanks.,4.2 Parsing experiments,[0],[0]
"There was no significant difference between the parser that was trained on the UD representation (ORPHAN) and the parser trained on the composite representation (COMPOSITE) when tested on the EWT data sets, which is not surprising considering that there is just one sentence with gapping each in the development and the test split.",4.2 Parsing experiments,[0],[0]
"When evaluated on the GAPPING datasets, the ORPHAN parser performs significantly better (p < 0.01) in terms of labeled attachment score, which suggests that the parser trained on the COMPOSITE representation is indeed struggling with the greatly increased label space.",4.2 Parsing experiments,[0],[0]
This is further confirmed by the attachment scores of the head tokens of remnants (Table 4).,4.2 Parsing experiments,[0],[0]
The labeled attachment score of remnants is significantly higher for the ORPHAN parser than for the COMPOSITE parser.,4.2 Parsing experiments,[0],[0]
"Further, the unlabeled attachment score on the test set is also higher for the ORPHAN parser, which suggests that the COMPOSITE parser is sometimes struggling with finding the right attachment for the
multiple long-distance composite dependencies.",4.2 Parsing experiments,[0],[0]
Our second set of experiments concerns the recovery of the elided material and the reattachment of the orphans.,4.3 Recovery experiments,[0],[0]
We conducted two experiments: an oracle experiment that used gold standard dependency trees and an end-to-end experiment that used the output of the parser as input.,4.3 Recovery experiments,[0],[0]
"For all experiments, we used the COMBINED treebank.
",4.3 Recovery experiments,[0],[0]
"Evaluation Here, we evaluated dependency graphs and therefore used the labeled and unlabeled precision and recall metrics.",4.3 Recovery experiments,[0],[0]
"However, as our two procedures are only changing the attachment of orphans, we only computed these metrics for copy nodes and their dependents.",4.3 Recovery experiments,[0],[0]
"Further, we excluded punctuation and coordinating conjunctions as their attachment is usually trivial and including them would inflate scores.",4.3 Recovery experiments,[0],[0]
"Lastly, we computed the sentence-level accuracy for all sentences with gapping.",4.3 Recovery experiments,[0],[0]
"For this metric, we considered a sentence to be correct if all copy nodes and their dependents of a sentence were attached to the correct head with the correct label.
",4.3 Recovery experiments,[0],[0]
Oracle results The top part of Table 5 shows the results for the oracle experiment.,4.3 Recovery experiments,[0],[0]
Both methods are able to reconstruct the elided material and the canonical clause structure from gold dependency trees with high accuracy.,4.3 Recovery experiments,[0],[0]
"This was expected for the COMPOSITE procedure, which can make use of the composite relations in the dependency trees, but less so for the ORPHAN procedure which has to recover the structure and the types of relations.",4.3 Recovery experiments,[0],[0]
"The two methods work equally well in terms of all metrics except for the sentence-level accuracy, which is significantly higher for the COMPOSITE procedure.",4.3 Recovery experiments,[0],[0]
This difference is caused by a difference in the types of mistakes.,4.3 Recovery experiments,[0],[0]
All errors of the COMPOSITE procedure are of a structural nature and stem from copying the wrong number of nodes while the dependency labels are always correct because they are part of the dependency tree.,4.3 Recovery experiments,[0],[0]
"The majority of errors of the ORPHAN procedure stem from incorrect dependency labels, and these mistakes are scattered across more examples, which leads to the lower sentence-level accuracy.
",4.3 Recovery experiments,[0],[0]
End-to-end results The middle part of Table 5 shows the results for the end-to-end experiment.,4.3 Recovery experiments,[0],[0]
"The performance of both methods is considerably lower than in the oracle experiment, which is pri-
marily driven by the much lower recall.",4.3 Recovery experiments,[0],[0]
"Both methods assume that the parser detects the existence of a gap and if the parser fails to do so, neither method attempts to reconstruct the elided material.",4.3 Recovery experiments,[0],[0]
"In general, precision tends to be a bit higher for the ORPHAN procedure whereas recall tends to be a bit higher for the COMPOSITE method but overall and in terms of sentence-level accuracy both methods seem to perform equally well.
",4.3 Recovery experiments,[0],[0]
"Error analysis For both methods, the primary issue is low recall, which is a result of parsing errors.",4.3 Recovery experiments,[0],[0]
"When the parser correctly predicts the orphan relation, the main sources of error for the ORPHAN procedure are missing correspondents for remnants (e.g., [for good] has no correspondent in They had left the company, many for good) or that the types of argument of the remnant and its correspondent differ (e.g., in She was convicted of selling unregistered securities in Florida and of unlawful phone calls in Ohio,",4.3 Recovery experiments,[0],[0]
"[of selling unregistered securities] is an adverbial clause whereas [of unlawful phone calls] is an oblique modifier).
",4.3 Recovery experiments,[0],[0]
"Apart from the cases where the COMPOSITE procedure leads to an incorrect structure, the remaining errors are all caused by the parser predicting the wrong composite relation.",4.3 Recovery experiments,[0],[0]
Kummerfeld and Klein (henceforth K&K; 2017) recently proposed a one-endpoint-crossing graph parser that is able to directly parse to PTB-style trees with traces.,4.4 Comparison to Kummerfeld and Klein,[0],[0]
They also briefly discuss gapping constructions and their parser tries to output the co-indexing that is used for gapping constructions in the PTB.,4.4 Comparison to Kummerfeld and Klein,[0],[0]
"The EWT and all the sentences that we took from the WSJ, Brown, and GENIA treebanks already come with constituency tree annotations, and we manually annotated the remaining sentences according to the PTB guide-
lines (Bies et al., 1995).",4.4 Comparison to Kummerfeld and Klein,[0],[0]
This allowed us to train the K&K parser with exactly the same set of sentences that we used in our previous experiments.,4.4 Comparison to Kummerfeld and Klein,[0],[0]
"As this parser outputs constituency trees, we could not compute dependency graph metrics for this method.",4.4 Comparison to Kummerfeld and Klein,[0],[0]
"For the sentence-level accuracy, we considered an example to be correct if a) each argument in the gapped conjunct was the child of a single constituent node, which in return was the sibling of the full clause/verb phrase, and b) the coindexing of each argument in the gapped conjunct was correct.",4.4 Comparison to Kummerfeld and Klein,[0],[0]
"For example, the following bracketing would be considered correct despite the incorrect internal structure of the first conjunct: [S[S[NP-1 Al ] likes [NP-2 coffee ]] and [S[NP=1 Sue",4.4 Comparison to Kummerfeld and Klein,[0],[0]
],4.4 Comparison to Kummerfeld and Klein,[0],[0]
"[NP=2 tea ]]]
The last row of Table 5 shows the results of the K&K parser.",4.4 Comparison to Kummerfeld and Klein,[0],[0]
The parser failed to output the correct constituency structure or co-indexing for every single example in the development and test sets.,4.4 Comparison to Kummerfeld and Klein,[0],[0]
"The parser struggled in particular with outputting the correct co-indices: For 32.5% of the test sentences with gapping, the bracketing of the gapped clause was correct but one or more of the co-indices were missing from the output.
",4.4 Comparison to Kummerfeld and Klein,[0],[0]
"Overall these results suggest that our dependency-based approach is much more reliable at identifying gapping constructions than the parser by K&K, which, in their defense, was optimized to output traces for other phenomena.",4.4 Comparison to Kummerfeld and Klein,[0],[0]
"Our method is also faster and took only seconds to parse the test set, while the K&K parser took several hours.",4.4 Comparison to Kummerfeld and Klein,[0],[0]
One of the appeals of the ORPHAN procedure is that it can be easily applied to other languages even if there exist no annotated enhanced dependency graphs.8,5 Resolving gaps in other languages,[0],[0]
"On the one hand, this is because
8There is no theoretical reason that would prevent one from using the COMPOSITE procedure for other languages
our method does not make use of lexical information, and on the other hand, this is because we developed our method on top of the UD annotation scheme, which has already been applied to many languages and for which many treebanks exist.
",5 Resolving gaps in other languages,[0],[0]
"Currently, all treebanks but the English one lack copy nodes for gapping constructions and many of them incorrectly use the orphan relation (Droganova and Zeman, 2017) and therefore we could not evaluate our method on a large variety of languages.",5 Resolving gaps in other languages,[0],[0]
"In order to demonstrate that our method can be applied to other languages, we therefore did a case study on the Swedish UD treebank.",5 Resolving gaps in other languages,[0],[0]
"The Swedish UD treebank is an automatic conversion from a section of the Talbanken (Einarsson, 1976) with extensive manual corrections.",5 Resolving gaps in other languages,[0],[0]
"While the treebank is overall of high quality, we noticed conversion errors that led to incorrect uses of the orphan relation in 11 of the 29 sentences with orphan relations, which we excluded from our evaluation.",5 Resolving gaps in other languages,[0],[0]
We applied our gapping resolution procedure without any modifications to the remaining 18 sentences.,5 Resolving gaps in other languages,[0],[0]
We used the Swedish word2vec embeddings that were prepared for the CoNLL 2017 Shared Task.,5 Resolving gaps in other languages,[0],[0]
"Our method correctly predicts the insertion of 29 copy nodes and is able to predict the correct structure of the enhanced representation in all cases, including complex ones with elided verb clusters such as the example in Figure 2.",5 Resolving gaps in other languages,[0],[0]
"It also predicts the correct dependency label for 108/110 relations, leading to a labeled precision and labeled recall of 98.18%, which are both higher than the English numbers despite the fact that we optimized our procedure for English.",5 Resolving gaps in other languages,[0],[0]
"The main reason for the higher performance seems to be that many of the Swedish examples come from informational texts from public organizations, which are more likely to be written to be clear and unambiguous.",5 Resolving gaps in other languages,[0],[0]
"Further, the Swedish data does not contain challenging examples from the linguistic literature.
",5 Resolving gaps in other languages,[0],[0]
"As Swedish is a Germanic language like English and thus shares many structural properties, we cannot conclude that our method is applicable to any language based on just this experiment.",5 Resolving gaps in other languages,[0],[0]
"However, given that our method does not rely on language-specific structural patterns, we expect it to work well for a wide range of languages.
",5 Resolving gaps in other languages,[0],[0]
"but given that UD treebanks are annotated with orphan relations, using the the COMPOSITE procedure would require additional manual annotations in practice.",5 Resolving gaps in other languages,[0],[0]
"Gapping constructions have been little studied in NLP, but several approaches (e.g., Dukes and Habash 2011; Simkó and Vincze 2017) parse to dependency trees with empty nodes.",6 Related work,[0],[0]
"Seeker et al. (2012) compared three ways of parsing with empty heads: adding a transition that inserts empty nodes, using composite relation labels for nodes that depend on an elided node, and pre-inserting empties before parsing.",6 Related work,[0],[0]
These papers all focus on recovering nodes for elided function words such as auxiliaries; none of them attempt to recover and resolve the content word elisions of gapping.,6 Related work,[0],[0]
"Ficler and Goldberg (2016) modified PTB annotations of argument-cluster coordinations (ACCs), i.e., gapping constructions with two post-verbal orphan phrases, which make up a subset of the gapping constructions in the PTB.",6 Related work,[0],[0]
"While the modified annotation style leads to higher parsing accuracy of ACCs, it is specific to ACCs and does not generalize to other gapping constructions.",6 Related work,[0.9501220685669319],"['Thus, text classification is needed to make more sophisticated decisions to improve accuracy.']"
"Moreover, they did not reconstruct gapped ACC clauses.",6 Related work,[0],[0]
"Traditional grammarbased chart parsers (Kay, 1980; Klein and Manning, 2001) did handle empty nodes and so could in principle provide a parse of gapping sentences though additional mechanisms would be needed for reconstruction.",6 Related work,[0],[0]
"In practice, though, dealing with gapping in a grammar-based framework is not straightforward and can lead to a combinatorial explosion that slows down parsing in general, as has been noted for the English Resource Grammar (Flickinger, 2017, p.c.) and for an HPSG implementation for Norwegian (Haugereid, 2017).",6 Related work,[0],[0]
"The grammar-based parser built with augmented transition networks (Woods, 1970) provided an extension in the form of the SYSCONJ operation (Woods, 1973) to parse some gapping constructions, but also this approach lacked explicit reconstruction mechanisms and provided only limited coverage.
",6 Related work,[0],[0]
"There also exists a long line of work on postprocessing surface-syntax constituency trees to recover traces in the PTB (Johnson, 2002; Levy and Manning, 2004; Campbell, 2004; Gabbard et al., 2006), pre-processing sentences such that they contain tokens for traces before parsing (Dienes and Dubey, 2003b), or directly parsing sentences to either PTB-style trees with empty elements or pre-processed trees that can be deterministically converted to PTB-style trees (Collins,
1997; Dienes and Dubey, 2003a; Schmid, 2006; Cai et al., 2011; Hayashi and Nagata, 2016; Kato and Matsubara, 2016; Kummerfeld and Klein, 2017).",6 Related work,[0],[0]
"However, all of these works are primarily concerned with recovering traces for phenomena such as Wh-movement or control and raising constructions and, with the exception of Kummerfeld and Klein (2017), none of these works attempt to output the co-indexing that is used for analyzing gapping constructions.",6 Related work,[0],[0]
"And again, none of these works try to reconstruct elided material.
",6 Related work,[0],[0]
"Lastly, several methods have been proposed for resolving other forms of ellipsis, including VP ellipsis (Hardt, 1997; Nielsen, 2004; Lappin, 2005; McShane and Babkin, 2016) and sluicing (Anand and Hardt, 2016) but none of these methods consider gapping constructions.",6 Related work,[0],[0]
We presented two methods to recover elided predicates in sentences with gapping.,7 Conclusion,[0],[0]
Our experiments suggest that both methods work equally well in a realistic end-to-end setting.,7 Conclusion,[0],[0]
"While in general, recall is still low, the oracle experiments suggest that both methods can recover elided predicates from correct dependency trees, which suggests that as parsers become more and more accurate, the gap recovery accuracy should also increase.
",7 Conclusion,[0],[0]
We also demonstrated that our method can be used to automatically add the enhanced UD representation to UD treebanks in other languages than English.,7 Conclusion,[0],[0]
"Apart from being useful in a parsing pipeline, we therefore also expect our method to be useful for building enhanced UD treebanks.
",7 Conclusion,[0],[0]
"Reproducibility
All data, pre-trained models, system outputs as well as a package for running the enhancement procedure are available from https:// github.com/sebschu/naacl-gapping.",7 Conclusion,[0],[0]
We thank the anonymous reviewers for their thoughtful feedback.,Acknowledgments,[0],[0]
"Also thanks to Vera Gribanova and Boris Harizanov for continuous feedback throughout this project, and to Matthew Lamm for help with annotating the data.",Acknowledgments,[0],[0]
"This work was supported in part by gifts from Google, Inc. and IPSoft, Inc.",Acknowledgments,[0],[0]
The first author is also supported by a Goodan Family Graduate Fellowship.,Acknowledgments,[0],[0]
"Sentences with gapping, such as Paul likes coffee and Mary tea, lack an overt predicate to indicate the relation between two or more arguments.",abstractText,[0],[0]
"Surface syntax representations of such sentences are often produced poorly by parsers, and even if correct, not well suited to downstream natural language understanding tasks such as relation extraction that are typically designed to extract information from sentences with canonical clause structure.",abstractText,[0],[0]
"In this paper, we present two methods for parsing to a Universal Dependencies graph representation that explicitly encodes the elided material with additional nodes and edges.",abstractText,[0],[0]
We find that both methods can reconstruct elided material from dependency trees with high accuracy when the parser correctly predicts the existence of a gap.,abstractText,[0],[0]
We further demonstrate that one of our methods can be applied to other languages based on a case study on Swedish.,abstractText,[0],[0]
Sentences with Gapping: Parsing and Reconstructing Elided Predicates,title,[0],[0]
"Sentiment-oriented relation extraction (Choi et al., 2006) is concerned with recognizing sentiment polarities and comparative relations between entities from natural language text.",1 Introduction,[0],[0]
Identifying such relations often requires syntactic and semantic analysis at both sentence and phrase level.,1 Introduction,[0],[0]
"Most prior work on sentiment analysis consider either i) subjective sentence detection (Yu and Kübler, 2011), ii) polarity classification (Johansson and Moschitti, 2011; Wilson et al., 2005), or iii) comparative relation identification (Jindal and Liu, 2006; Ganapathibhotla and Liu, 2008).",1 Introduction,[0],[0]
"In practice, however, differ-
ent types of sentiment-oriented relations frequently coexist in documents.",1 Introduction,[0],[0]
"In particular, we found that more than 38% of the sentences in our test corpus contain more than one type of relations.",1 Introduction,[0],[0]
The isolated analysis approach is inappropriate because i) it sacrifices acuracy by ignoring the intricate interplay among different types of relations; ii) it could lead to conflicting predictions such as estimating a relation candidate as both negative and comparative.,1 Introduction,[0],[0]
"Therefore, in this paper, we identify instances of both sentiment polarities and comparative relations for entities of interest simultaneously.",1 Introduction,[0],[0]
"We assume that all the mentions of entities and attributes are given, and entities are disambiguated.",1 Introduction,[0],[0]
"It is a widely used assumption when evaluating a module in a pipeline system that the outputs of preceding modules are error-free.
",1 Introduction,[0],[0]
"To the best of our knowledge, the only existing system capable of extracting both comparisons and sentiment polarities is a rule-based system proposed by Ding et al. (2009).",1 Introduction,[0],[0]
We argue that it is better to tackle the task by using a unified model with structured outputs.,1 Introduction,[0],[0]
It allows us to consider a set of correlated relation instances jointly and characterize their interaction through a set of soft and hard constraints.,1 Introduction,[0],[0]
"For example, we can encode constraints to discourage an attribute to participate in a polarity relation and a comparative relation at the same time.",1 Introduction,[0],[0]
"As a result, the system extracts a set of correlated instances of sentiment-oriented relations from a given sentence.",1 Introduction,[0],[0]
"For example, with the sentence about the camera Canon 7D, “The sensor is great, but the price is higher than Nikon D7000.”",1 Introduction,[0],[0]
"the expected output is positive(Canon 7D, sensor)
155
Transactions of the Association for Computational Linguistics, 2 (2014) 155–168.",1 Introduction,[0],[0]
Action Editor: Janyce Wiebe.,1 Introduction,[0],[0]
Submitted 6/2013; Revised 11/2013; Published 4/2014.,1 Introduction,[0],[0]
"c©2014 Association for Computational Linguistics.
and preferred(Nikon D7000, Canon 7D, textitprice).
",1 Introduction,[0],[0]
"However, constructing a fully annotated training corpus for this task is labor-intensive and requires strong linguistic background.",1 Introduction,[0],[0]
"We minimize this overhead by applying a simplified annotation scheme, in which annotators mark mentions of entities and attributes, disambiguate the entities, and label instances of relations for each sentence.",1 Introduction,[0.9532021192777607],"['As stated at the beginning of the paper, this work was motivated by the real-life problem of identifying the right social media posts or documents for specific applications.']"
"Based on the new scheme, we have created a small Sentiment Relation Graph (SRG) corpus for the domains of cameras and movies, which significantly differs from the corpora used in prior work (Wei and Gulla, 2010; Kessler et al., 2010; Toprak et al., 2010; Wiebe et al., 2005; Hu and Liu, 2004) in the following ways: i) both sentiment polarities and comparative relations are annotated; ii) all mentioned entities are disambiguated; and iii) no subjective expressions are annotated, unless they are part of entity mentions.
",1 Introduction,[0],[0]
The new annotation scheme raises a new challenge for learning algorithms in that they need to automatically find textual evidences for each annotated relation during training.,1 Introduction,[0],[0]
"For example, with the sentence “I like the Rebel a little better, but that is another price jump”, simply assigning a sentimentbearing expression to the nearest relation candidate is insufficient, especially when the sentiment is not explicitly expressed.
",1 Introduction,[0],[0]
"In this paper, we propose SENTI-LSSVM, a latent structural SVM based model for sentiment-oriented relation extraction.",1 Introduction,[0],[0]
"SENTI-LSSVM is applied to find the most likely set of the relation instances expressed in a given sentence, where the latent variables are used to assign the most appropriate textual evidences to the respective instances.
",1 Introduction,[0],[0]
"In summary, the contributions of this paper are the following:
• We propose SENTI-LSSVM: the first unified statistical model with the capability of extracting instances of both binary and ternary sentimentoriented relations.
",1 Introduction,[0],[0]
"• We design a task-specific integer linear programming (ILP) formulation for inference.
",1 Introduction,[0],[0]
"• We construct a new SRG corpus as a valuable asset for the evaluation of sentiment relation
extraction.
",1 Introduction,[0],[0]
"• We conduct extensive experiments with online reviews and forum posts, showing that SENTI-LSSVM model can effectively learn from a training corpus without explicitly annotated subjective expressions and that its performance significantly outperforms state-of-the-art systems.",1 Introduction,[0],[0]
"There are ample works on analyzing sentiment polarities and entity comparisons, but the majority of them studied the two tasks in isolation.
",2 Related Work,[0],[0]
Most prior approaches for fine-grained sentiment analysis focus on polarity classification.,2 Related Work,[0],[0]
"Supervised approaches on expression-level analysis require the annotation of sentiment-bearing expressions as training data (Jin et al., 2009; Choi and Cardie, 2010; Johansson and Moschitti, 2011; Yessenalina and Cardie, 2011; Wei and Gulla, 2010).",2 Related Work,[0],[0]
"However, the corresponding annotation process is time-consuming.",2 Related Work,[0],[0]
"Although sentence-level annotations are easier to obtain, the analysis at this level cannot cope with sentences conveying relations of multiple types (McDonald et al., 2007; Täckström and McDonald, 2011; Socher et al., 2012).",2 Related Work,[0],[0]
"Lexiconbased approaches require no training data (Ku et al., 2006; Kim and Hovy, 2006; Godbole et al., 2007; Ding et al., 2008; Popescu and Etzioni, 2005; Liu et al., 2005) but suffer from inferior performance (Wilson et al., 2005; Qu et al., 2012).",2 Related Work,[0],[0]
"In contrast, our method requires no annotation of sentiment-bearing expressions for training and can predict both sentiment polarities and comparative relations.
",2 Related Work,[0],[0]
"Sentiment-oriented comparative relations have been studied in the context of user-generated discourse (Jindal and Liu, 2006; Ganapathibhotla and Liu, 2008).",2 Related Work,[0],[0]
Approaches rely on linguistically motivated rules and assume the existence of independent keywords in sentences which indicate comparative relations.,2 Related Work,[0],[0]
"Therefore, these methods fall short of extracting comparative relations based on domain dependent information.
",2 Related Work,[0],[0]
Both Johansson and Moschitti (2011) and Wu et al. (2011) formulate fine-grained sentiment analysis as a learning problem with structured outputs.,2 Related Work,[0],[0]
"However, they focus only on polarity classification
of expressions and require annotation of sentimentbearing expressions for training as well.
While ILP has been previously applied for inference in sentiment analysis (Choi and Cardie, 2009; Somasundaran and Wiebe, 2009; Wu et al., 2011), our task requires a complete ILP reformulation due to 1) the absence of annotated sentiment expressions and 2) the constraints imposed by the joint extraction of both sentiment polarity and comparative relations.",2 Related Work,[0],[0]
This section gives an overview of the whole system for extracting sentiment-oriented relation instances.,3 System Overview,[0],[0]
"Prior to presenting the system architecture, we introduce the essential concepts and the definitions of two kinds of directed hypergraphs as the representation of correlated relation instances extracted from sentences.",3 System Overview,[0],[0]
Entity.,3.1 Concepts and Definitions,[0],[0]
"An entity is an abstract or concrete thing, which needs not be of material existence.",3.1 Concepts and Definitions,[0],[0]
An entity in this paper refers to either a product or a brand.,3.1 Concepts and Definitions,[0],[0]
Attribute.,3.1 Concepts and Definitions,[0],[0]
"An attribute is an object closely associated with or belonging to an entity, such as the lens of digital camera.",3.1 Concepts and Definitions,[0],[0]
Sentiment-Oriented Relation.,3.1 Concepts and Definitions,[0],[0]
"A sentimentoriented relation is either a sentiment polarity or a comparative relation, defined on tuples of entities and attributes.",3.1 Concepts and Definitions,[0],[0]
"A sentiment polarity relation conveys either a positive or a negative attitude towards entities or their attributes, whereas a comparative relation indicates the preference of one entity over the other entity w.r.t.",3.1 Concepts and Definitions,[0],[0]
an attribute.,3.1 Concepts and Definitions,[0],[0]
Relation Instance.,3.1 Concepts and Definitions,[0],[0]
"An instance of sentiment polarity takes the form r(entity, attribute) with r ∈ {positive, negative}, such as positive(Canon 7D, sensor).",3.1 Concepts and Definitions,[0],[0]
"The polarity instances expressed in the form of unary relations, such as “Nikon D7000 is excellent.”, are denoted as binary relations r(entity, whole), where the attribute whole indicates the entity as a whole.",3.1 Concepts and Definitions,[0],[0]
"In contrast, an instance of comparative relation is in the form of preferred{entity, entity, attribute}, e.g. preferred(Canon 7D, Nikon D7000, price).",3.1 Concepts and Definitions,[0],[0]
"For brevity, we refer to an instance set of sentiment-oriented relations extracted from a
sentence as an sSoR. To represent the instances of the remaining relations, we represent them as other{entity, attribute}, such as textitpartOf{wheel, car}.",3.1 Concepts and Definitions,[0],[0]
These relations include objective relations and the subjective relations other than sentimentoriented relations.,3.1 Concepts and Definitions,[0],[0]
Mention-Based Relation Instances.,3.1 Concepts and Definitions,[0],[0]
A mentionbased relation instance refers to a tuple of entity mentions with a certain relation.,3.1 Concepts and Definitions,[0],[0]
"This concept is introduced as the representation of instances in a sentence by replacing entities with the corresponding entity mentions, such as positive(“Canon SD880i”, “wide angle view”).
",3.1 Concepts and Definitions,[0],[0]
Mention-Based Relation Graph.,3.1 Concepts and Definitions,[0],[0]
A mention-based relation graph (or MRG ) represents a collection of mention-based relation instances expressed in a sentence.,3.1 Concepts and Definitions,[0],[0]
"As illustrated in Figure 1, an MRG is a directed hypergraph G = 〈M,E〉 with a vertex set M and an edge set E. A vertex mi ∈ M denotes a mention of an entity or an attribute occurring either within the sentence or in its context.",3.1 Concepts and Definitions,[0],[0]
We say that a mention is from the context if it is mentioned in the previous sentence or is an attribute implied in the current sentence.,3.1 Concepts and Definitions,[0],[0]
"An instance of a binary relation in an MRG takes the form of a binary edge el = (mi,ma), where mi and ma denote an entity mention and an attribute mention respectively, and the type l ∈ {positive, negative, other}.",3.1 Concepts and Definitions,[0],[0]
"A ternary edge el indicating comparative relation is represented as el = (mi,mj ,ma), where two entity mentions mi and mj are compared with respect to the attribute mention ma.",3.1 Concepts and Definitions,[0],[0]
"We define the type l ∈ {better,worse} to indicate two possible directions of the relation and assume mi occurs before mj .",3.1 Concepts and Definitions,[0],[0]
"As a result, we have a set L of five relation types: positive, negative, better, worse or other.",3.1 Concepts and Definitions,[0],[0]
"According to these definitions, the annotations in the SRG corpus are actually MRGs and disambiguated entities.",3.1 Concepts and Definitions,[0],[0]
"If there are multiple mentions referring to the same entity, annotators are asked to choose the
most obvious one because it saves annotation time and is less demanding for the entity recognition and diambiguation modules.
",3.1 Concepts and Definitions,[0],[0]
Evidentiary Mention-Based Relation Graph.,3.1 Concepts and Definitions,[0],[0]
"An evidentiary mention-based relation graph, coined eMRG , extends an MRG by associating each edge with a textual evidence to support the corresponding relation assertions (see Figure 2).",3.1 Concepts and Definitions,[0],[0]
"Consequently, an edge in an eMRG is denoted by a pair (a, c), where a represents a mention-based relation instance and c is the associated textual evidence.",3.1 Concepts and Definitions,[0],[0]
It is also referred to as an evidentiary edge.,3.1 Concepts and Definitions,[0],[0]
"represented as el = (mi,mj ,ma), an MRG as an evidentiary MRG (eMRG) and the edges of eMRGs as evidentiary edges, as shown in Figure 2.",3.1 Concepts and Definitions,[0],[0]
"As illustrated by Figure 3, at the core of our system is the SENTI-LSSVM model, which extracts sets
of mention-based relationships in the form of eMRGs from sentences.",3.2 System Architecture,[0],[0]
"For a given sentence with known entity mentions, we select all possible mention sets as relation candidates, where each set includes at least one entity mention.",3.2 System Architecture,[0],[0]
Then we associate each relation candidate with a set of constituents or the whole sentence as the textual evidence candidates (cf. Section 6.1).,3.2 System Architecture,[0],[0]
"Subsequently, the inference component aims to find the most likely eMRG from all possible combinations of mention-based relation instances and their textual evidences (cf. Section 6.2).",3.2 System Architecture,[0],[0]
The representation eMRG is chosen because it characterizes exactly the model outputs by letting each edge correspond to an instance of mention-based relation and the associated textual evidence.,3.2 System Architecture,[0],[0]
"Finally, the model parameters of this model are learned by an online algorithm (cf. Section 7).
",3.2 System Architecture,[0],[0]
"Since instance sets of sentiment-oriented relations (sSoRs) are the expected outputs, we can obtain sSoRs from MRGs by using a simple rule-based algorithm.",3.2 System Architecture,[0],[0]
The algorithm essentially maps the mentions from an MRG into entities and attributes in an sSoR and label the corresponding tuples with the relation types of the edges from an MRG.,3.2 System Architecture,[0],[0]
"For instances of comparative relation, the label better or worse is mapped to the relation type preferred.",3.2 System Architecture,[0],[0]
The task of sentiment-oriented relation extraction is to determine the most likely sSoR in a sentence.,4 SENTI-LSSVM Model,[0],[0]
"Since sSoRs are derived from the corresponding MRGs as described in Section 3, the task is reduced to find the most likely MRG for each sentence.",4 SENTI-LSSVM Model,[0],[0]
"Since an MRG is created by assigning relation types to a subset of all relation candidates, which are possible tuples of mentions with unknown relation types, the number of MRGs can be extremely high.
",4 SENTI-LSSVM Model,[0],[0]
"To tackle the task, one solution is to employ an edge-factored linear model in the framework of structural SVM (Martins et al., 2009; Tsochantaridis et al., 2004).",4 SENTI-LSSVM Model,[0],[0]
"The model suggests that a bag of features should be specified for each relation candidate, and then the model predicts the most likely candidate sets along with their relation types to form the optimal MRGs.",4 SENTI-LSSVM Model,[0],[0]
"As we observed, for a relation candidate, the most informative features are the words near its entity mentions in the original text.",4 SENTI-LSSVM Model,[0],[0]
"How-
ever, if we represent a candidate by all these words, it is very likely that the instances of different relation types share overly similar features, because a mention is often involved in more than one relation candidate, as shown in Figure 2.",4 SENTI-LSSVM Model,[0],[0]
"As a consequence, the instances of different relations represented by overly similar features can easily confuse the learning algorithm.",4 SENTI-LSSVM Model,[0],[0]
"Thus, it is critical to select proper constituents or sentences as textual evidences for each relation candidate in both training and testing.
",4 SENTI-LSSVM Model,[0],[0]
"Consequently, we divide the task of sentimentoriented relation extraction into two subtasks : i) identifying the most likely MRGs; ii) assigning proper textual evidences to each edge of MRGs to support their relation assertions.",4 SENTI-LSSVM Model,[0],[0]
It is desirable to carry out the two subtasks jointly as these two subtasks could enhance each other.,4 SENTI-LSSVM Model,[0],[0]
"First, the identification of relation types requires proper textual evidences; second, the soft and hard constraints imposed by the correlated relation instances facilitate the recognition of the corresponding textual evidences.",4 SENTI-LSSVM Model,[0],[0]
"Since the eMRGs are created by attaching every MRG with a set of textual evidences, tackling the two subtasks simultaneously is equivalent to selecting the most likely eMRG from a set of eMRG candidates.",4 SENTI-LSSVM Model,[0],[0]
"It is challenging because our SRG corpus does not contain any annotation of textual evidences.
",4 SENTI-LSSVM Model,[0],[0]
"Formally, let X denote the set of all available sentences, and we define y ∈ Y(x)(x ∈ X ) as the set of labeled edges of an MRG and Y = ∪x∈XY(x).",4 SENTI-LSSVM Model,[0],[0]
"Since the assignments of textual evidences are not observed, an assignment of evidences to y is denoted by a latent variable h ∈ H(x) and H = ∪x∈XH(x).",4 SENTI-LSSVM Model,[0],[0]
"Then (y, h) corresponds to an eMRG, and (a, c) ∈ (y, h) is a labeled edge a attached with a textual evidence c.",4 SENTI-LSSVM Model,[0],[0]
"Given a labeled dataset D = {(x1, y1), ..., (xn, yn)} ∈ (X × Y)n, we aim to learn a discriminant function f : X",4 SENTI-LSSVM Model,[0],[0]
"→ Y×H that outputs the optimal eMRG (y, h) ∈ Y(x)×H(x) for a given sentence x.
Due to the introduction of latent variables, we adopt the latent structural SVM (Yu and Joachims, 2009) for structural classification.",4 SENTI-LSSVM Model,[0],[0]
"Our discriminant function is defined as
f(x) =",4 SENTI-LSSVM Model,[0],[0]
"argmax(y,h)∈Y(x)×H(x)β >Φ(x, y, h) (1)
where Φ(x, y, h) is the feature function of an eMRG (y, h) and β is the corresponding weight vector.
",4 SENTI-LSSVM Model,[0],[0]
"To ensure tractability, we also employ edge-based factorization for our model.",4 SENTI-LSSVM Model,[0],[0]
"Let Mp denote a set of entity mentions and yr(mi) be a set of edges labeled with sentiment-oriented relations incident to mi, the factorization of Φ(x, y, h) is given as
Φ(x, y, h) = ∑
(a,c)∈(y,h) Φe(x, a, c) + (2)
∑
mi∈Mp
∑
a,a′∈yr(mi),a 6=a′ Φc(a, a
′)
where Φe(x, a, c) is a local edge feature function for a labeled edge a attached with a textual evidence c and Φc(a, a′) is a feature function capturing cooccurrence of two labeled edges ami and a ′ mi incident to an entity mention mi.",4 SENTI-LSSVM Model,[0],[0]
"The following features are used in the feature functions (Equation 2):
Unigrams: As mentioned before, a textual evidence attached to an edge in MRG is either a word, phrase or sentence.",5 Feature Space,[0],[0]
"We consider all lemmatized unigrams in the textual evidence as unigram features.
",5 Feature Space,[0],[0]
"Context: Since web users usually express related sentiments about the same entity across sentence boundaries, we describe the sentiment flow using a set of contextual binary features.",5 Feature Space,[0],[0]
"For example, if entity A is mentioned in both the previous sentence and the current sentence, a set of contextual binary features are used to indicate all possible combinations of the current and the previous mentioned sentimentoriented relations regarding to entity A.
Co-occurrence: We have mentioned the cooccurrence feature in Equation 2, indicated by Φc(a, a
′).",5 Feature Space,[0],[0]
It captures the co-occurrence of two labeled edges incident to the same entity mention.,5 Feature Space,[0],[0]
"Note that the co-occurrence feature function is considered only if there is a contrast conjunction such as “but” between the non-shared entity mentions incident to the two labeled edges.
",5 Feature Space,[0],[0]
"Senti-predictors: Following the idea of (Qu et al., 2012), we encode the prediction results from the rule-based phrase-level multi-relation predictor (Ding et al., 2009) and from the bag-of-opinions predictor (Qu et al., 2010) as features based on the textual evidence.",5 Feature Space,[0],[0]
"The output of the first predictor is an integer value, while the output of the second predictor is a sentiment relation, such as “positive”,
“negative”, “better” or “worse”.",5 Feature Space,[0],[0]
"We map the relational outputs into integer values and then encode the outputs from both predictors as senti-predictor features.
",5 Feature Space,[0],[0]
Others: The commonly used part-of-speech tags are also included as features.,5 Feature Space,[0],[0]
"Moreover, for an edge candidate, a set of binary features are used to denote the types of the edge and its entity mentions.",5 Feature Space,[0],[0]
"For instance, a binary feature indicates whether an edge is a binary edge related to an entity mentioned in context.",5 Feature Space,[0],[0]
"To characterize the syntactic dependencies between two adjacent entity mentions, we use the path in the dependency tree between the heads of the corresponding constituents, the number of words and other mentions in-between as features.",5 Feature Space,[0],[0]
"Additionally, if the textual evidence is a constituent, its feature w.r.t.",5 Feature Space,[0],[0]
an edge is the dependency path to the closest mention of the edge that does not overlap with this constituent.,5 Feature Space,[0],[0]
"In order to find the best eMRG for a given sentence with a well trained model, we need to determine the most likely relation type for each relation candidate and support the corresponding assertions with proper textual evidences.",6 Structural Inference,[0.9514815766728287],"['We observed from our collaborations with social science and health science researchers that in a typical application, the researcher first need to obtain a set of posts of a particular topic that he/she wants to study, e.g., a political issue.']"
We formulate this task as an Integer Linear Programming (ILP).,6 Structural Inference,[0],[0]
"Instead of considering all constituents of a sentence, we empirically select a subset as textual evidences for each relation candidate.",6 Structural Inference,[0],[0]
"Textual evidences are selected based on the constituent trees of sentences parsed by the Stanford parser (Klein and Manning, 2003).",6.1 Textual Evidence Candidates Selection,[0],[0]
"For each mention in a sentence, we first locate a constituent in the tree with the maximal overlap by Jaccard similarity.",6.1 Textual Evidence Candidates Selection,[0],[0]
"Starting from this constituent, we consider two types of candidates: type I candidates are constituents at the highest level which contain neither any word of another mention nor any contrast conjunctions such as “but”; type II candidates are constituents at the highest level which cover exactly two mentions of an edge and do not overlap with any other mentions.",6.1 Textual Evidence Candidates Selection,[0],[0]
"For a binary edge connecting an entity mention and an attribute mention, we consider a type I candidate starting from the attribute men-
tion.",6.1 Textual Evidence Candidates Selection,[0],[0]
"For a binary edge connecting two entity mentions, we consider type I candidates starting from both mentions.",6.1 Textual Evidence Candidates Selection,[0],[0]
"Moreover, for a comparative ternary edge, we consider both type I and type II candidates starting from the attribute mention.",6.1 Textual Evidence Candidates Selection,[0],[0]
This strategy is based on our observation that these candidates often cover the most important information w.r.t.,6.1 Textual Evidence Candidates Selection,[0],[0]
the covered entity mentions.,6.1 Textual Evidence Candidates Selection,[0],[0]
"We formulate the inference problem of finding the best eMRG as an ILP problem due to its convenient integration of both soft and hard constraints.
",6.2 ILP Formulation,[0],[0]
"Given the model parameters β, we reformulate the score of an eMRG in the discriminant function (1) as follows,
β>Φ(x, y, h) = ∑
(a,c)∈(y,h) saczac +
∑
mi∈Mp
∑
a,a′∈yr(mi),a 6=a′ saa′zaa′
where sac = β>Φe(x, a, c) denotes the score of a labeled edge a attached with a textual evidence c, saa′ = β
>Φc(a, a′) is the edge co-occurrence score, the binary variable zac indicates the presence or absence of the corresponding edge, and zaa′ indicates if two edges co-occurr.",6.2 ILP Formulation,[0],[0]
"As not every edge set can form an eMRG, we require that a valid eMRG should satisfy a set of linear constraints, which form our constraint space.",6.2 ILP Formulation,[0],[0]
"Then function (1) is equivalent to
max z∈B
s>z + µzd
s.t.",6.2 ILP Formulation,[0],[0]
A   z η,6.2 ILP Formulation,[0],[0]
τ   ≤,6.2 ILP Formulation,[0],[0]
"d
z,η, τ ∈ B
where B = 2S with S = {0, 1}, and η and τ are auxiliary binary variables that help define the constraint space.",6.2 ILP Formulation,[0],[0]
"The above optimization problem takes exactly the form of an ILP because both the constraints and the objective function are linear, and all variables take only integer values.
",6.2 ILP Formulation,[0],[0]
"In the following, we consider two types of constraint space, 1) an eMRG with only binary edges and 2) an eMRG with both binary and ternary edges.
eMRG with only Binary Edges: An eMRG has only binary edges if a sentence contains no attribute mention or at most one entity mention.",6.2 ILP Formulation,[0],[0]
We expect that each edge has only one relation type and is supported by a single textual evidence.,6.2 ILP Formulation,[0],[0]
"To facilitate the formulation of constraints, we introduce ηel to denote the presence or absence of a labeled edge el, and ηec to indicate if a textual evidence c is assigned to an unlabeled edge e. Then the binary variable for the corresponding evidentiary edge zelc = ηec ∧ ηel , where the ILP formulation of conjunction can be found in (Martins et al., 2009).
",6.2 ILP Formulation,[0],[0]
Let Ce denote the set of textual evidence candidates of an unlabeled edge e.,6.2 ILP Formulation,[0],[0]
"The constraint of at most one textual evidence per edge is formulated as:
∑
c∈Ce ηec ≤ 1 (3)
",6.2 ILP Formulation,[0],[0]
"Once a textual evidence is assigned to an edge, their relation labels should match and the number of labeled edges must agree with the number of attached textual evidences.",6.2 ILP Formulation,[0],[0]
"Further, we assume that a textual evidence c conveys at most one relation so that an evidence will not be assigned to the relations of different types, which is the main problem for the structural SVM based model.",6.2 ILP Formulation,[0],[0]
"Let ηcl indicate that the textual evidence c is labeled by the relation type l. The corresponding constraints are expressed as,
∑ l∈Le ηel = ∑ c∈Ce ηec; zelc ≤ ηcl; ∑ l∈L ηcl ≤ 1
where Le denotes the set of all possible labels for an unlabeled edge e, and L is the set of all relation types of MRGs (cf. Section 3).
",6.2 ILP Formulation,[0],[0]
"In order to avoid a textual evidence being overly reused by multiple relation candidates, we first penalize the assignment of a textual evidence c to a labeled edge a by associating the corresponding zac with a fixed negative cost −µ in the objective function.",6.2 ILP Formulation,[0],[0]
"Then the selection of one textual evidence per edge a is encouraged by associating µ to zdc in the objective function, where zdc = ∨ e∈Sc ηec and Sc is the set of edges that the textual evidence c serves as a candidate.",6.2 ILP Formulation,[0],[0]
"The disjunction zdc is expressed as:
zdc ≥ ηe, e ∈",6.2 ILP Formulation,[0],[0]
"Sc zdc ≤ ∑
e∈Sc ηe
This soft constraint not only encourages one textual evidence per edge, but also keeps it eligible for multiple assignments.
",6.2 ILP Formulation,[0],[0]
"For any two labeled edge a and a′ incident to the same entity mention, the edge-to-edge cooccurrence is described by zca,a′ = za",6.2 ILP Formulation,[0],[0]
"∧ za′ .
eMRG with both Binary and Ternary Edges: If there are more than one entity mentions and at least one attribute mention in a sentence, an eMRG can potentially have both binary and ternary edges.",6.2 ILP Formulation,[0],[0]
"In this case, we assume that each mention of attributes can participate either in binary relations or in ternary relations.",6.2 ILP Formulation,[0],[0]
"The assumption holds in more than 99.9% of the sentences in our SRG corpus, thus we describe it as a set of hard constraints.",6.2 ILP Formulation,[0],[0]
"Geometrically, the assumption can be visualized as the selection between two alternative structures incident to the same attribute mention, as shown in Figure 4.",6.2 ILP Formulation,[0],[0]
"Note that, in the binary edge structure, we include not only the edges incident to the attribute mention but also the edge between the two entity mentions.
",6.2 ILP Formulation,[0],[0]
Let Sbmi be the set of all possible labeled edges in a binary edge structure of an attribute mention mi.,6.2 ILP Formulation,[0],[0]
Variable τ,6.2 ILP Formulation,[0],[0]
"bmi = ∨ el∈Sbmi
ηel indicates whether the attribute mention is associated with a binary edge structure or not.",6.2 ILP Formulation,[0],[0]
"In the same manner, we use τ tmi = ∨ el∈Stmi
ηel to indicate the association of the an attribute mention mi with an ternary edge structure from the set of all incident ternary edges Stmi .",6.2 ILP Formulation,[0],[0]
"The selection between two alternative structures is
formulated as τ bmi + τ",6.2 ILP Formulation,[0],[0]
t mi = 1.,6.2 ILP Formulation,[0],[0]
"As this influences only the edges incident to an attribute mention, we keep all the constraints introduced in the previous section unchanged except for constraint (3), which is modified as
∑ c∈Ce ηec ≤ τ",6.2 ILP Formulation,[0],[0]
"bmi ;
∑ c∈Ce ηec ≤ τ",6.2 ILP Formulation,[0],[0]
"tmi
Therefore, we can have either binary edges or ternary edges for an attribute mention.",6.2 ILP Formulation,[0],[0]
"Given a set of training sentences D = {(x1, y1), . . .",7 Learning Model Parameters,[0],[0]
", (xn, yn)}, the best weight vector β of the discriminant function (1) is found by solving the following optimization problem:
min β
1
n
n∑
i=1
[ max (ŷ,ĥ)∈Y(x)×H(x)
(β>Φ(x, ŷ, ĥ)+δ(ĥ, ŷ, y))
",7 Learning Model Parameters,[0],[0]
"− max h̄∈H(x)
β>Φ(x, y, h̄)]",7 Learning Model Parameters,[0],[0]
"+ ρ|β|] (4)
where δ(ĥ, ŷ, y) is a loss function measuring the discrepancies between an eMRG (y, h̄) with gold standard edge labels y and an eMRG (ŷ, ĥ) with inferred labeled edges ŷ and textual evidences ĥ. Due to the sparse nature of the lexical features, we apply L1 regularizer to the weight vector β, and the degree of sparsity is controlled by the hyperparameter ρ.
",7 Learning Model Parameters,[0],[0]
"Since the L1 norm in the above optimization problem is not differentiable at zero, we apply the online forward-backward splitting (FOBOS) algorithm (Duchi and Singer, 2009).",7 Learning Model Parameters,[0],[0]
"It requires two steps for updating the weight vector β by using a single training sentence x on each iteration t.
βt+ 12 = βt",7 Learning Model Parameters,[0],[0]
"− εt∆t
βt+1",7 Learning Model Parameters,[0],[0]
"= arg min β
1 2 ‖β",7 Learning Model Parameters,[0],[0]
"− βt‖2 + εtρ|β|
where ∆t is the subgradient computed without considering the L1 norm and εt is the learning rate.",7 Learning Model Parameters,[0],[0]
"For a labeled sentence x, ∆t = Φ(x, ŷ∗, ĥ∗) − Φ(x, y, h̄∗), where the feature functions of the corresponding eMRGs are inferred by solving (ŷ∗, ĥ∗) = arg max(ĥ,ŷ)∈H(x)×Y(x)[β >Φ(x, ŷ, ĥ) +",7 Learning Model Parameters,[0],[0]
"δ(ĥ, ŷ, y)] and (y, h̄∗) = arg maxh̄∈H(x) β",7 Learning Model Parameters,[0],[0]
>,7 Learning Model Parameters,[0],[0]
"Φ(x, y, h̄), as indicated in the optimization problem (4).
",7 Learning Model Parameters,[0],[0]
The former inference problem is similar to the one we considered in the previous section except the inclusion of the loss function.,7 Learning Model Parameters,[0],[0]
"We incorporate the loss function into the ILP formulation by defining the loss between an MRG (y, h) and a gold standard MRG as the sum of per-edge costs.",7 Learning Model Parameters,[0],[0]
"In our experiments, we consider a positive cost ϕ for each wrongly labeled edge a, so that if an edge a has a different label from the gold standard, we add ϕ to the coefficient sac of the corresponding variable zac in the objective function of the ILP formulation.
",7 Learning Model Parameters,[0],[0]
"In addition, since the non-positive weights of edge labels in the initial learning phrase often lead to eMRGs with many unlabeled edges, which harms the learning performance, we fix it by adding a constraint for the minimal number of labeled edges in an eMRG, ∑
a∈A
∑
c∈Ca ηac ≥ ζ (5)
where A is the set of all labeled edge candidates and ζ denotes the minimal number of labeled edges.
",7 Learning Model Parameters,[0],[0]
"Empirically, the best way to determine ζ is to make it equal to the maximal number of labeled edges in an eMRG with the restriction that a textual evidence can be assigned to at most one edge.",7 Learning Model Parameters,[0],[0]
"By considering all the edge candidates A and all the textual evidence candidates C as two vertex sets in a bipartite graph Ĝ = 〈V = (A,C), E〉 (with edges in E indicating which textual evidence can be assigned to which edge), ζ corresponds to exactly the size of a maximum matching of the bipartite graph1.
",7 Learning Model Parameters,[0],[0]
"To find the optimal eMRG (y, h̄∗), for the gold label k of each edge, we consider the following set of constraints for inference since the labels of the edges are known for the training data,
∑ c∈Ce ηec ≤ 1; ηec ≤ lck ∑ k′∈L lck′ ≤ 1; ∑ e∈Sc ηec ≤ 1
",7 Learning Model Parameters,[0],[0]
"We include also the soft constraints, which avoid a textual evidence being overly reused by multiple relations, and the constraints similar to (5) to ensure a minimal number of labeled edges and a minimal number of sentiment-oriented relations.
",7 Learning Model Parameters,[0],[0]
"1It is computed by the Hopcroft-Karp algorithm (Hopcroft and Karp, 1973) in our implementation.",7 Learning Model Parameters,[0],[0]
"For evaluation we constructed the SRG corpus, which in total consists of 1686 manually annotated online reviews and forum posts in the digital camera and movie domains2.",8 SRG Corpus,[0],[0]
"For each domain, we maintain a set of attributes and a list of entity names.
",8 SRG Corpus,[0],[0]
The annotation scheme for the sentiment representation asserts minimal linguistic knowledge from our annotators.,8 SRG Corpus,[0],[0]
"By focusing on the meanings of the sentences, the annotators make decisions based on their language intuition, not restricted by specific syntactic structures.",8 SRG Corpus,[0],[0]
"Taking the example in Figure 2, the annotators only need to mark the mentions of entities and attributes from both the sentences and the context, disambiguate them, and label (“Canon 7D”, “Nikon D7000”, price) as worse and (“Canon 7D”, “sensor”) as positive, whereas in prior work, people have annotated the sentiment-bearing expressions such as “great” and link them to the respective relation instances as well.",8 SRG Corpus,[0],[0]
"This also enables them to annotate instances of both sentiment polarity and comparative relaton, which are conveyed by not only explicit sentiment-bearing expressions like “excellent performance”, but also factual expressions implying evaluations such as “The 7V has 10x optical zoom and the 9V has 16x.”.
",8 SRG Corpus,[0],[0]
14 annotators participated in the annotation project.,8 SRG Corpus,[0],[0]
"After a short training period, annotators worked on randomly assigned documents one at a time.",8 SRG Corpus,[0],[0]
"For product reviews, the system lists all relevant information about the entity and the predefined attributes.",8 SRG Corpus,[0],[0]
"For forum posts, the system shows only the attribute list.",8 SRG Corpus,[0],[0]
"For each sentence in a document, the annotator first determines if it refers to an entity of interest.",8 SRG Corpus,[0],[0]
"If not, the sentence is marked
2The 107 camera reviews are from bestbuy.com and Amazon.com; the 667 camera forum posts are downloaded from forum.digitalcamerareview.com; the 138 movie reviews and 774 forum posts are from imdb.com and boards.ie respectively
as off-topic.",8 SRG Corpus,[0],[0]
"Otherwise, the annotator will identify the most obvious mentions, disambiguate them, and mark the MRGs.",8 SRG Corpus,[0],[0]
"We evaluate the inter-annotator agreement on sSoRs in terms of Cohen’s Kappa (κ) (Cohen, 1968).",8 SRG Corpus,[0],[0]
"An average Kappa value of 0.698 was achieved on a randomly selected set consisting of 412 sentences.
",8 SRG Corpus,[0],[0]
Table 1 shows the corpus distribution after normalizing them into sSoRs.,8 SRG Corpus,[0],[0]
Camera forum posts contain the largest proportion of comparisons because they are mainly about the recommendation of digital cameras.,8 SRG Corpus,[0],[0]
"In contrast, web users are much less interested in comparing movies, in both reviews and forums.",8 SRG Corpus,[0],[0]
"In all subsets, positive relations play a dominant role since web users intend to express more positive attitudes online than negative ones (Pang and Lee, 2007).",8 SRG Corpus,[0],[0]
This section describes the empirical evaluation of SENTI-LSSVM together with two competitive baselines on the SRG corpus.,9 Experiments,[0],[0]
"We implemented a rule-based baseline (DINGRULE) and a structural SVM (Tsochantaridis et al., 2004) baseline (SENTI-SSVM) for comparison.",9.1 Experimental Setup,[0],[0]
"The former system extends the work of Ding et al. (2009), which designed several linguisticallymotivated rules based on a sentiment polarity lexicon for relation identification and assumes there is only one type of sentiment relation in a sentence.",9.1 Experimental Setup,[0],[0]
"In our implementation, we keep all the rules of (Ding et al., 2009) and add one phrase-level rule when there are more than one mention in a sentence.",9.1 Experimental Setup,[0],[0]
The additional rule assigns sentiment-bearing words and negators to its nearest relation candidates based on the absolute surface distance between the words and the corresponding mentions.,9.1 Experimental Setup,[0],[0]
"In this case, the phraselevel sentiment-oriented relations depend only on the assigned sentiment words and negators.",9.1 Experimental Setup,[0],[0]
The latter system is based on a structural SVM and does not consider the assignment of textual evidences to relation instances during inference.,9.1 Experimental Setup,[0],[0]
"The textual features of a relation candidate are all lexical and sentiment predictor features within a surface distance of four words from the mentions of the candidate.
",9.1 Experimental Setup,[0],[0]
"Thus, this baseline does not need the inference constraints of SENTI-LSSVM for the selection of textual evidences.",9.1 Experimental Setup,[0],[0]
"To gain more insights into the model, we also evaluate the contribution of individual features of SENTI-LSSVM.",9.1 Experimental Setup,[0],[0]
"In addition, to show if identifying sentiment polarities and comparative relations jointly works better than tackling each task on its own, we train SENTI-LSSVM for each task separately and combine their predictions according to compatibility rules and the corresponding graph scores.
",9.1 Experimental Setup,[0],[0]
"For each domain and text genre, we withheld 15% documents for development and use the remaining for cross validation.",9.1 Experimental Setup,[0],[0]
The hyperparameters of all systems are tuned on the development datasets.,9.1 Experimental Setup,[0],[0]
"For all experiments of SENTI-LSSVM, we use ρ = 0.0001 for the L1 regularizer in Eq.(4) and ϕ = 0.05 for the loss function; and for SENTI-SSVM, ρ = 0.0001 and ϕ = 0.01.",9.1 Experimental Setup,[0],[0]
"Since the relation type of off-topic sentences is certainly other, we evaluate all systems with 5-fold cross-validation only on the on-topic sentences in the evaluation dataset.",9.1 Experimental Setup,[0],[0]
"Since the same sSoR can have several equivalent MRGs and the relation type other is not of our interest, we evaluate the sSoRs in terms of precision, recall and F-measure.",9.1 Experimental Setup,[0],[0]
All reported numbers are averages over the 5 folds.,9.1 Experimental Setup,[0],[0]
Table 2 shows the complete results of all systems.,9.2 Results,[0],[0]
Here our model SENTI-LSSVM outperformed all baselines in terms of the average F-measure scores and recalls by a large margin.,9.2 Results,[0],[0]
The F-measure on movie reviews is about 14% over the best baseline.,9.2 Results,[0],[0]
The rule-based system has higher precision than recall in most cases.,9.2 Results,[0],[0]
"However, simply increasing the coverage of the domain independent sentiment polarity lexicon might lead to worse performance (Taboada et al., 2011) because many sentiment oriented relations are conveyed by domain dependent expressions and factual expressions implying evaluations, such as “This camera does not have manual control.”",9.2 Results,[0],[0]
"Compared to DING-RULE, SENTI-SSVM performs better in the camera domain but worse for the movies due to many misclassification of negative relation instances as other.",9.2 Results,[0],[0]
It also wrongly predicted more positive instances as other than SENTI-LSSVM.,9.2 Results,[0],[0]
"We found that the recalls of these instances are low because they often have overly similar features with the instances of the type
other linking to the same mentions.",9.2 Results,[0],[0]
The problem gets worse in the movie domain since i),9.2 Results,[0],[0]
many sentences contain no explicit sentiment-bearing words; ii) the prior polarity of the sentiment-bearing words do not agree with their contextual polarity in the sentences.,9.2 Results,[0],[0]
Consider the following example from a forum post about the movie “Superman Returns”: “Have a look at Superman: the Animated Series or Justice League Unlimited . . .,9.2 Results,[0],[0]
that is how the characters of Superman and Lex Luthor should be.”.,9.2 Results,[0],[0]
"In contrast, our model minimizes the overlapping features by assigning them to the most likely relation candidates.",9.2 Results,[0],[0]
This leads to significantly better performance.,9.2 Results,[0],[0]
"Although SENTI-SSVM has low recall for both positive and negative relations, it achieves the highest recall for the comparative relation among all systems in the movie domain and camera reviews.",9.2 Results,[0],[0]
"Since less than 1% of all instances are for comparative relations in these document sets and all models are trained to optimize the overall accuracy, SENTILSSVM intends to trade off the minority class for the overall better performance.",9.2 Results,[0],[0]
"This advantage disappears on the camera forum posts, where the number of instances of comparative relation is 12 times more than that in the other data sets.
",9.2 Results,[0],[0]
All systems perform better in predicting positive relations than the negative ones.,9.2 Results,[0],[0]
"This corresponds well to the empirical findings in (Wilson, 2008) that people intend to use more complex expressions for negative sentiments than their affirmative counterparts.",9.2 Results,[0],[0]
It is also in accordance with the distribution of these relations in our SRG corpus which is randomly sampled from the online documents.,9.2 Results,[0],[0]
"For learning systems, it can also be explained by the fact that the training data for positive relations are considerably more than those for negative ones.",9.2 Results,[0],[0]
"The comparative relation is the hardest one to process since we found that many corresponding expressions do not contain explicit keywords for comparison.
",9.2 Results,[0],[0]
"To understand the performance of the key feature groups in our model better, we remove each group from the full SENTI-LSSVM system and evaluate the variations with movie reviews and camera forum posts, which have relatively balanced distribution of relation types.",9.2 Results,[0],[0]
"As shown in Table 3, the features from the sentiment predictors make significant contributions for both datasets.",9.2 Results,[0],[0]
"The different drops of the performance indicate that the po-
larities predicted by rules are more consistent in camera forum posts than in movie reviews.",9.2 Results,[0],[0]
Due to the complexity of expressions in the movie reviews our model cannot benefit from the unigram features but these features are a good compensation for the sentiment predictor features in camera forum posts.,9.2 Results,[0],[0]
The sharp drop by removing the context features from our model on movie reviews indicates that the sentiments in movie reviews depend highly on the relations of the previous sentences.,9.2 Results,[0],[0]
"In contrast, the sentiment-oriented relations of the previous sentences could be a reason of overfitting for camera forum data.",9.2 Results,[0],[0]
The edge co-occurrence features do not play an important role in our model since the number of co-occurred sentiment-oriented relations in the sentences with contrast conjunctions like “but” is small.,9.2 Results,[0],[0]
"However, we found that allowing the co-occurrence of any sentiment-oriented relations would harm the performance of the model.
",9.2 Results,[0],[0]
"In addition, our experiments showed that the sep-
arated approach, which trains a model for sentiment polarities and comparative relations respectively, leads to a decrease by almost 1% in terms of the F-measure averaged over all four datasets.",9.2 Results,[0],[0]
"The largest drop of F-measure is 3% on camera forum posts, since this dataset contains the largest proportion of comparative relations.",9.2 Results,[0],[0]
We found that the errors are increased when the trained models make conflicting predictions.,9.2 Results,[0],[0]
"In this case, the joint approach can take all factors into account and make more consistent decisions than the separated approaches.",9.2 Results,[0],[0]
We proposed SENTI-LSSVM model for extracting instances of both sentiment polarities and comparative relations.,10 Conclusion,[0],[0]
"For evaluating and training the model, we created an SRG corpus by using a lightweight annotation scheme.",10 Conclusion,[0],[0]
We showed that our model can automatically find textual evidences to support its relation predictions and achieves significantly better F-measure scores than alternative state-of-the-art methods.,10 Conclusion,[0],[0]
Extracting instances of sentiment-oriented relations from user-generated web documents is important for online marketing analysis.,abstractText,[0],[0]
"Unlike previous work, we formulate this extraction task as a structured prediction problem and design the corresponding inference as an integer linear program.",abstractText,[0],[0]
"Our latent structural SVM based model can learn from training corpora that do not contain explicit annotations of sentiment-bearing expressions, and it can simultaneously recognize instances of both binary (polarity) and ternary (comparative) relations with regard to entity mentions of interest.",abstractText,[0],[0]
The empirical evaluation shows that our approach significantly outperforms stateof-the-art systems across domains (cameras and movies) and across genres (reviews and forum posts).,abstractText,[0],[0]
The gold standard corpus that we built will also be a valuable resource for the community.,abstractText,[0],[0]
Senti-LSSVM: Sentiment-Oriented Multi-Relation Extraction with Latent Structural SVM,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3654–3663 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
3654",text,[0],[0]
"Sentiment analysis, a.k.a. opinion mining, is a task which aims to identify the user sentiment orientation of a product/brand/service by monitoring the online textual data, e.g., reviews and social media messages.",1 Introduction,[0],[0]
"It has attracted huge attention in both academic and industrial communities due to its widespread applications, such like recommendation (Zhang et al., 2014) and social media mining (Chambers et al., 2015).",1 Introduction,[0],[0]
"As the fundamental component in sentiment analysis, sentiment classification mainly classifies the sentiment polarity as positive or negative, and has been well-studied from both sentence-level (Kim and Hovy, 2004) and document-level (Xu et al., 2016).
",1 Introduction,[0],[0]
"∗Corresponding author
Recently, a new QA-style reviewing form, namely “customer questions & answers”, has become increasingly popular on the giant ecommerce platforms, e.g., Amazon and Taobao.",1 Introduction,[0],[0]
"In this new form, a potential customer asks question(s) about the target product/service while other experienced user(s) can provide answer(s).",1 Introduction,[0],[0]
"With the widespread of such QA-style reviews, users find a different channel to efficiently explore rich and useful information, and service providers and scholars are paying more attention to its specific characteristics comparing with traditional reviews (Wachsmuth et al., 2014; Zhou et al., 2015a).",1 Introduction,[0],[0]
"Comparing to the traditional reviews, the QA style reviews can be more informative and convincing.",1 Introduction,[0],[0]
"More importantly, because answer providers are randomly picked from the users who already purchased the target item, this new form of review can be more reliable and trustful.
",1 Introduction,[0],[0]
"Regarding QA-style sentiment analysis, one straightforward method is to directly employ an existing sentiment classification approach that works well on traditional reviews, such as RNN (Nguyen and Shirai, 2015) and LSTM (Chen et al., 2016).",1 Introduction,[0],[0]
"However, because of the significant differences between QA-style and classical reviews, existing review mining algorithms, e.g., text-based sentiment analysis/classification, should not be di-
rectly applied to this new kind of QA-style data.",1 Introduction,[0],[0]
"More detailed reasons can be found as the followings.
",1 Introduction,[0],[0]
"First, in QA-style text, the question and answer text are more likely to be two parallel units rather than a sequence form.",1 Introduction,[0],[0]
"On the one hand, for instance, in Figure 1, sentence “It’s a nice phone with high-quality screen.”",1 Introduction,[0],[0]
in Answer 1 actually does not follow sentence “How is the battery?”,1 Introduction,[0],[0]
"in Question 1 , but corresponds to sentence “Is the screen clear?”",1 Introduction,[0],[0]
in Question 1.,1 Introduction,[0],[0]
"Therefore, when the question text and answer text are presented as two units in a sequence, it is rather difficult to capture the relationship between the question and its corresponding answer due to the possible long distance between them.",1 Introduction,[0],[0]
"On the other hand, there often exists both positive and negative sentiments in answer text according to different parts of question, and this specific case should be categorized as another category named conflict.",1 Introduction,[0],[0]
"For instance, in Figure 1, Answer 1",1 Introduction,[0],[0]
“It’s a nice phone with high-quality screen.,1 Introduction,[0],[0]
But the battery is not durable.”,1 Introduction,[0],[0]
is a conflict answer to Question 1.,1 Introduction,[0],[0]
"However, when this answer text is considered as a sequence, it is highly possible to be predicted as the category of positive or negative rather than conflict.",1 Introduction,[0],[0]
"In order to address these problems, a more appropriate approach is to segment both the question and answer text into some parallel sentences, and then construct the [Q-sentence, A-sentence] units in each QA text pair to detect in-depth sentiment information.
",1 Introduction,[0],[0]
"Second, although the main sentiment polarity is usually expressed from the answer text, the question text could also carry important sentiment tips to predict the sentiment polarity of a QA text pair.",1 Introduction,[0],[0]
"For instance, in Figure 1, we could hardly estimate the sentiment polarity solely based on Answer 2.",1 Introduction,[0],[0]
"However, when we take Question 2, “Is the sun cream really effective?”, into consideration, it can be easier to label this QA text pair with a negative tag.",1 Introduction,[0],[0]
"In this study, we propose an approach to match the sentences inside the question and answer text bidirectionally.
",1 Introduction,[0],[0]
"Third, in each QA text pair, the importance degrees of different [Q-sentence, A-sentence] units can be different.",1 Introduction,[0],[0]
"For instance, in Figure 1, the [Qsentence, A-sentence] unit, i.e., sentence “Summer is coming, I’m afraid of getting darker.”",1 Introduction,[0],[0]
"in Answer 2 and sentence “No, just depending on my own experience.”",1 Introduction,[0],[0]
"in Question 2, makes tiny contribution to imply the sentiment polarity for the
QA text pair.",1 Introduction,[0],[0]
"Therefore, a well-behaved network approach should consider the importance degrees of different [Q-sentence, A-sentence] units for predicting the sentiment polarity of a QA text pair.
",1 Introduction,[0],[0]
The contribution of this paper is twofold.,1 Introduction,[0],[0]
"First, we propose a novel problem, QA-style sentiment analysis, and build a large-scale annotated corpus tailed for this task.",1 Introduction,[0],[0]
The dataset is released to motivate future investigations for this track of research.,1 Introduction,[0],[0]
"Second, we propose a hierarchical matching network model to address the challenges of QA-style sentiment classification.",1 Introduction,[0],[0]
"Specifically, we first segment both the question and answer text into sentences and construct the [Q-sentence, A-sentence] units for each QA text pair.",1 Introduction,[0],[0]
"Then, by using a QA bidirectional matching layer, we encode each [Q-sentence, A-sentence] unit for exploring sentiment information.",1 Introduction,[0],[0]
"Finally, the self-matching attention layer in the model can capture the importance of these [Q-sentence, A-sentence] matching vectors obtained from QA bidirectional matching layer, which could effectively refine the evidence for inferring the sentiment polarity of a QA text pair.",1 Introduction,[0],[0]
Experimental results show that the proposed approach significantly outperforms several strong baselines for QA-style sentiment classification.,1 Introduction,[0],[0]
Sentiment classification has become a hot research field in NLP since the pioneering work by Pang et al. (2002).,2 Related Work,[0],[0]
"In general, the research on traditional sentiment classification has been carried out in different text levels, such like word-level, documentlevel and aspect-level.
",2 Related Work,[0],[0]
Word-level sentiment classification has been studied in a long period in the research community of sentiment analysis.,2 Related Work,[0],[0]
Some early studies have devoted their efforts to predicting the sentiment polarity of a word with different learning models and resources.,2 Related Work,[0],[0]
Turney (2002) proposed an approach to predicting the sentiment polarity of words by calculating Pointwise Mutual Information (PMI) values between the seed words and the search hits.,2 Related Work,[0],[0]
"Hassan and Radev (2010) and Hassan et al. (2011) applied a Markov random walk model to determine the word polarities with a large word relatedness graph, and the synonyms and hypernyms in WordNet (Miller, 1995).",2 Related Work,[0],[0]
"More recently, some studies aim to learn better word embedding of a word rather than its polarity.",2 Related Work,[0],[0]
"Tang et al. (2014) developed three neural networks to learn word em-
bedding by incorporating sentiment polarities of text in loss functions.",2 Related Work,[0],[0]
"Zhou et al. (2015b) employed both unsupervised and supervised neural networks to learn bilingual sentiment word embedding.
",2 Related Work,[0],[0]
Document-level sentiment classification has also been studied in a long period in the research community of sentiment analysis.,2 Related Work,[0],[0]
"On one hand, many early studies have been devoted their efforts to various of aspects on learning approaches, such as supervised learning (Pang et al., 2002; Riloff et al., 2006), semi-supervised learning (Li et al., 2010; Xia et al., 2015; Li et al., 2015), and domain adaptation (Blitzer et al., 2007; He et al., 2011).",2 Related Work,[0],[0]
"On the other hand, many recent studies employ deep learning approaches to enhance the performances in sentiment classification.",2 Related Work,[0],[0]
Tang et al. (2015) proposed a user-product neural network to incorporate both user and product information for sentiment classification.,2 Related Work,[0],[0]
Xu et al. (2016) proposed a Cached Long Short-Term Memory neural networks (CLSTM) to capture the overall semantic information in long texts.,2 Related Work,[0],[0]
"More recently, Long et al. (2017) proposed a novel attention model, namely cognition-based attention, for sentiment classification.
",2 Related Work,[0],[0]
Aspect-level sentiment classification is a relatively new research area in the research community of sentiment analysis and it is a fine-grained classification task.,2 Related Work,[0],[0]
"Recently, Wang et al. (2016) proposed an attention-based LSTM neural network to aspect-level sentiment classification by exploring the connection between an aspect and the content of a sentence.",2 Related Work,[0],[0]
Tang et al. (2016) proposed a deep memory network with multiple attention-based computational layers to improve the performance.,2 Related Work,[0],[0]
"Wang et al. (2018) proposed a hierarchical attention network to explore both word-level and clause-level sentiment information towards a target aspect.
",2 Related Work,[0],[0]
"Unlike all the prior studies, this paper focuses on a very different kind of text representation, i.e., QA-style text level, for sentiment classification.",2 Related Work,[0],[0]
"To the best of our knowledge, this is the first attempt to perform sentiment classification on this text level.",2 Related Work,[0],[0]
"We collect QA text pairs from “Asking All” in Taobao (Alibaba)1, which is the world’s biggest ecommerce company.",3 Data Collection and Annotation,[0],[0]
"The QA text pairs are mainly from Beauty, Shoe and Electronic domains and each domain contains 10,000 QA text pairs.
",3 Data Collection and Annotation,[0],[0]
"We define four sentiment-related categories, i.e., positive, negative, conflict (both positive and negative sentiment) and neutral (neither positive nor negative sentiment).",3 Data Collection and Annotation,[0],[0]
"To guarantee a high annotation agreement, we propose some annotation guidelines after several times of annotation processes on a small size of data.",3 Data Collection and Annotation,[0],[0]
"Then, we ask more coders to annotate the whole data set according to these annotation guidelines.
",3 Data Collection and Annotation,[0],[0]
The annotation guidelines contain two main groups.,3 Data Collection and Annotation,[0],[0]
"One contains the guidelines which aim to distinguish the categories of neutral and nonneutral, i.e., (a) A QA text pair in which the question and the answer do not match is annotated as a neutral sample.",3 Data Collection and Annotation,[0],[0]
"In this type of samples, the answer does not reply to the question correctly.",3 Data Collection and Annotation,[0],[0]
"E1 is an example of this type where the question talks about the screen while the answer talks about the battery.
",3 Data Collection and Annotation,[0],[0]
E1: Q: Is the screen clear?,3 Data Collection and Annotation,[0],[0]
"A: The battery life is decent.
",3 Data Collection and Annotation,[0],[0]
(b) A QA text pair with an unknown or uncertain answer is annotated as a neutral sample.,3 Data Collection and Annotation,[0],[0]
"E2 is an example of this type.
",3 Data Collection and Annotation,[0],[0]
E2: Q: What about these sneakers?,3 Data Collection and Annotation,[0],[0]
"A: I don’t know, I bought it for my dad.
",3 Data Collection and Annotation,[0],[0]
(c),3 Data Collection and Annotation,[0],[0]
A QA text pair with only objective description is annotated as a neutral sample.,3 Data Collection and Annotation,[0],[0]
"E3 is an example of this type.
",3 Data Collection and Annotation,[0],[0]
E3: Q: What’s the operation system of the phone?,3 Data Collection and Annotation,[0],[0]
"A: Android.
",3 Data Collection and Annotation,[0],[0]
(d),3 Data Collection and Annotation,[0],[0]
A QA text pair which compares two different products is annotated as a neutral sample.,3 Data Collection and Annotation,[0],[0]
"In this type of samples, two products are involved and it
1https://www.taobao.com/
is sometimes difficult to tell the sentiment orientation of one product.",3 Data Collection and Annotation,[0],[0]
"E4 is an example of this type.
",3 Data Collection and Annotation,[0],[0]
E4: Q: How about this phone when compared to iPhone 6s?,3 Data Collection and Annotation,[0],[0]
"A: It’s up to you, and they’re not comparable.
",3 Data Collection and Annotation,[0],[0]
"The other group contains the guidelines which aim to distinguish the categories of positive and negative, i.e., (e) If the answer text contains sentimental expressions to question like “disappointed”, “terrible”, and so on, we annotate it as negative.",3 Data Collection and Annotation,[0],[0]
"E5 is an example of this type.
",3 Data Collection and Annotation,[0],[0]
E5: Q: How is the rock climbing shoe?,3 Data Collection and Annotation,[0],[0]
"A: I am so disappointed, my feet felt hurt when I wore them.
",3 Data Collection and Annotation,[0],[0]
(f),3 Data Collection and Annotation,[0],[0]
"If the answer text contains sentimental expressions to question like “perfect”, “satisfied”, and so on, we annotate it as positive.",3 Data Collection and Annotation,[0],[0]
"E6 is an example of this type.
",3 Data Collection and Annotation,[0],[0]
E6: Q: How about the fragrance?,3 Data Collection and Annotation,[0],[0]
"A: I am so satisfied, it smells distinctive.
",3 Data Collection and Annotation,[0],[0]
"(g) If we cannot confirm the polarity of a QA text pair only depending on answer text, we annotate the polarity according to both the question and answer text.",3 Data Collection and Annotation,[0],[0]
"For instance, E7 is an example with positive polarity, while E8 is an example with negative polarity.
",3 Data Collection and Annotation,[0],[0]
E7: Q: Will the phone get hot when gaming?,3 Data Collection and Annotation,[0],[0]
"A: No.
E8: Q: Is the sun cream really economic?",3 Data Collection and Annotation,[0],[0]
"A: No.
We assign two annotators to annotate each QA text pair, and the Kappa consistency check value of the annotation is 0.84.",3 Data Collection and Annotation,[0],[0]
"When annotators cannot reach an agreement, an expert will make the final decision, ensuring the quality of data annotation.",3 Data Collection and Annotation,[0],[0]
Table 1 shows the category distribution of the corpus.,3 Data Collection and Annotation,[0],[0]
"To motivate other scholars to investigate this novel but important task, we share the data via Github2.",3 Data Collection and Annotation,[0],[0]
"In this section, we introduce the proposed hierarchical matching network approach for QAstyle sentiment classification.",4 Methodology,[0],[0]
Figure 2 depicts the overview of the proposed approach.,4 Methodology,[0],[0]
Word Encoding Layer:,4.1 QA Bidirectional Matching Mechanism,[0],[0]
"After sentence segmentation, the question text in a QA text pair contains N sentences, SQi represents the i-th sentence in the question text.",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"Similarly, the answer text in this QA text pair contains M sentences, SAj represents the j-th sentence in the answer text.",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"We then construct [Q-sentence, A-sentence] units by pairing one sentence in the question text and one sentence in the answer text, and we obtain N*M [Q-sentence, A-sentence] units at last.
",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"Given a [SQi , SAj ] unit in this QA text pair, i.e., Q-sentence SQi with words wi,n, i ∈",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"[1, N ], n ∈
2https://github.com/clshenNLP/QASC/
[1, Ni] and A-sentence SAj with words wj,m, j ∈",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"[1,M ],m ∈",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"[1,Mj ], we first convert the words to their respective word embeddings (xi,n ∈ Rd, i ∈",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"[1, N ], n ∈",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"[1, Ni] and xj,m, j ∈",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"[1,M ],m ∈",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"[1,Mj ]).",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"We then use Bi-directional LSTM (namely Bi-LSTM), which can efficiently make use of past features (via forward states) and future features (via backward states) for a specific time step, to get contextual representations of SQi and SAj individually.",4.1 QA Bidirectional Matching Mechanism,[0],[0]
The representation of each word is formed by concatenating the forward and backward hidden states.,4.1 QA Bidirectional Matching Mechanism,[0],[0]
"For simplicity, we note contextual representation of SQi asHQi , and contextual representation of SAj as HAj respectively:
HQi =",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"[hi,1, hi,2, ..., hi,n, ..., hi,Ni ] (1)
HAj =",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"[hj,1, hj,2, ..., hj,m, ..., hj,Mj ] (2)
where hi,n ∈",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"Rd ′
denotes the word representation in SQi at time step n, hj,m ∈ Rd ′ denotes the word representation in SAj at time step m, and d ′ is the dimensionality of word representation.",4.1 QA Bidirectional Matching Mechanism,[0],[0]
QA Bidirectional Matching Layer:,4.1 QA Bidirectional Matching Mechanism,[0],[0]
"General neural network could not capture sentiment matching information in a [SQi , SAj ] unit well.",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"For the sake of solving this problem, we introduce the QA bidirectional matching layer to encapsulate the clues and interactions between SQi and SAj synchronously (Tay et al., 2017; McCann et al., 2017).",4.1 QA Bidirectional Matching Mechanism,[0],[0]
Figure 3 depicts the detail architecture of QA bidirectional matching mechanism.,4.1 QA Bidirectional Matching Mechanism,[0],[0]
"Specifically, we first calculate the bidirectional pair-wise matching matrix by using the fol-
lowing formula:
D[i,j] = (HQi)",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"> · (HAj ) (3)
",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"where D[i,j] ∈",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"RNi×Mj denotes the bidirectional matching matrix for the [SQi , SAj ] unit.",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"Each element in D[i,j] is the score that measures how well the word in SQi semantically matches the word in SAj and vice versa.
",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"Given the bidirectional matching matrix D[i,j], we use attention mechanism (Yang et al., 2016; Cui et al., 2017) to mine the sentiment matching information between question and answer from two directions, which could be seen as an Answerto-Question attention and a Question-to-Answer attention as follows.",4.1 QA Bidirectional Matching Mechanism,[0],[0]
•,4.1 QA Bidirectional Matching Mechanism,[0],[0]
"Answer-to-Question Attention: We employ row-wise operations to compute the attention weight vector αr[i,j] as follows:
U r[i,j] = tanh(Wr ·D >",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"[i,j]) (4)
αr[i,j] = softmax(w > r · U r[i,j]) (5)
where αr[i,j] ∈ R Ni is the Answer-to-Question attention weight vector regarding the importance degrees of all words in Q-sentence SQi , Wr ∈",4.1 QA Bidirectional Matching Mechanism,[0],[0]
Rd′×Mj and wr ∈ Rd ′ are weight matrices.,4.1 QA Bidirectional Matching Mechanism,[0],[0]
"After computing the Answer-to-Question attention weight vector, we can get the Answer-to-Question matching vector V r[i,j] ∈ R d′ as follows:
V r[i,j] = (HQi) ·",4.1 QA Bidirectional Matching Mechanism,[0],[0]
α r,4.1 QA Bidirectional Matching Mechanism,[0],[0]
"[i,j] (6)
• Question-to-Answer Attention: Simultaneously, we employ column-wise operations to calculate the attention weight vector αc[i,j] as follows:
U c[i,j] = tanh(Wc ·D[i,j]) (7) αc[i,j] =",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"softmax(w > c · U c[i,j]) (8)
where αc[i,j] ∈",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"R Mj is the Question-to-Answer attention weight vector regarding the importance degrees of all words in A-sentence SAj , Wc ∈ Rd′×Ni and wc ∈ Rd ′ are weight matrices.",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"After calculating the Question-to-Answer attention weight vector, we can get the Question-to-Answer matching vector V c[i,j] ∈ R d′ as follows:
V c[i,j] = (HAj ) ·",4.1 QA Bidirectional Matching Mechanism,[0],[0]
α c,4.1 QA Bidirectional Matching Mechanism,[0],[0]
"[i,j] (9)
Then, we combine Answer-to-Question and Question-to-Answer matching vectors to represent
the final bidirectional matching vector of the [SQi , SAj ] unit:
V[i,j] = V r",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"[i,j] ⊕ V c",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"[i,j] (10)
where ⊕ denotes the concatenate operator, and V[i,j] denotes the bidirectional matching vector which integrates SQi and SAj with each other.",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"Through the QA bidirectional matching layer, informative bidirectional matching vectors are generated to pinpoint the sentiment matching information in each [Q-sentence, A-sentence] unit.",4.2 Self-Matching Attention Mechanism,[0],[0]
"Intuitively, each matching vector for [Q-sentence, Asentence] unit holds different importance to a QA text pair.",4.2 Self-Matching Attention Mechanism,[0],[0]
"To better aggregate the evidence from these vectors for inferring the sentiment polarity of the QA text pair, we propose a self-matching attention layer, matching these informative vectors against themselves.",4.2 Self-Matching Attention Mechanism,[0],[0]
Self-Matching Attention Layer:,4.2 Self-Matching Attention Mechanism,[0],[0]
"As aforementioned, we have obtained N*M bidirectional matching vectors through QA bidirectional matching layer, then we calculate the attention weight vector α with these matching vectors by following formulas:
V =",4.2 Self-Matching Attention Mechanism,[0],[0]
"[V[1,1], V[1,2], ..., V[i,j], ..., V[N,M ]] (11)
",4.2 Self-Matching Attention Mechanism,[0],[0]
U = tanh(Wh · V ) (12) α =,4.2 Self-Matching Attention Mechanism,[0],[0]
"softmax(w>h · U) (13)
where α is the attention weight vector which measures the importance of these matching vectors, Wh and wh are the weight matrices.
",4.2 Self-Matching Attention Mechanism,[0],[0]
"Finally, we can get the QA text pair representation R as follows:
R = V · α (14)",4.2 Self-Matching Attention Mechanism,[0],[0]
QA text pair representationR is a high level representation which can be used for classification.,4.3 Classification Model,[0],[0]
"In our approach, we feed R to a softmax classifier:
p = softmax(Wl",4.3 Classification Model,[0],[0]
"·R+ bl) (15)
where p is a set of predicted distribution of the sentiment categories, i.e., positive, negative, neutral, and conflict.",4.3 Classification Model,[0],[0]
"Wl is the weight matrix and bl is the bias.
",4.3 Classification Model,[0],[0]
"To learn the whole model, we train an end-toend model given the training data, and the goal of
training is to minimize the cross-entropy loss, i.e.,
L(θ) =",4.3 Classification Model,[0],[0]
"− S∑
s=1 K∑ k=1",4.3 Classification Model,[0],[0]
yks ·,4.3 Classification Model,[0],[0]
"logŷks + λ‖θ‖ 2 2 (16)
where S is the number of training data.",4.3 Classification Model,[0],[0]
ys is the true sentiment label of the s-th sample.,4.3 Classification Model,[0],[0]
ŷs is the predicted sentiment label of the s-th sample.,4.3 Classification Model,[0],[0]
K is number of all sentiment categories.,4.3 Classification Model,[0],[0]
"λ is a L2regularization term, θ is the parameter set.",4.3 Classification Model,[0],[0]
"In the above equation, the model parameters are optimized by using Adam (Kingma and Ba, 2014).",4.3 Classification Model,[0],[0]
"In this section, we evaluate the performances of the proposed approach for QA-style sentiment classification.",5 Experimentation,[0],[0]
"• Data Sets: As introduced in Section 3, the annotated QA text pairs cover three different domains.",5.1 Experimental Settings,[0],[0]
"In each domain, we randomly split the data into a training set (80% in each category) and a test set (20% in each category).",5.1 Experimental Settings,[0],[0]
"In addition, we set aside 10% from the training set as the development data for parameters tuning.",5.1 Experimental Settings,[0],[0]
"• Word Segmentation and Embeddings: FudanNLP3 (Qiu et al., 2013) is employed to segment text into Chinese words and word2vec4 (Mikolov et al., 2013) is employed to pre-train word embeddings.",5.1 Experimental Settings,[0],[0]
The vector dimensionality is set to be 100.,5.1 Experimental Settings,[0],[0]
"• Sentence Segmentation: CoreNLP5 (Manning et al., 2014) is employed to segment both the question and answer text into sentences.",5.1 Experimental Settings,[0],[0]
"• Hyper-parameters: In the experiment, all outof-vocabulary words are initialized by sampling from the uniform distribution U(−0.01, 0.01).",5.1 Experimental Settings,[0],[0]
"All weight matrices are given their initial values by sampling from uniform distribution U(−0.01, 0.01).",5.1 Experimental Settings,[0],[0]
The LSTM hidden states are set to be 128 and all models are trained by mini-batch of 32 instances.,5.1 Experimental Settings,[0],[0]
The dropout rate is set to 0.2.,5.1 Experimental Settings,[0],[0]
The other hyper-parameters are tuned according to the development data.,5.1 Experimental Settings,[0],[0]
•,5.1 Experimental Settings,[0],[0]
"Evaluation Metric: The performance is evaluated using standard Accuracy and Macro-F1.
3https://github.com/FudanNLP/fnlp/ 4https://code.google.com/archive/p/word2vec/ 5http://stanfordnlp.github.io/CoreNLP/",5.1 Experimental Settings,[0],[0]
The following baseline approaches are employed for comparison.,5.2 Experimental Results,[0],[0]
Note that all the approaches share the same word embeddings for fair comparison.,5.2 Experimental Results,[0],[0]
• SVM:,5.2 Experimental Results,[0],[0]
This baseline employs support vector machine along with word embedding features.,5.2 Experimental Results,[0],[0]
The question and answer text in a QA text pair are chained as a sequence.,5.2 Experimental Results,[0],[0]
• LSTM:,5.2 Experimental Results,[0],[0]
A standard LSTM model utilizes word embeddings and concatenates the question and answer text as a sequence.,5.2 Experimental Results,[0],[0]
• Bi-LSTM:,5.2 Experimental Results,[0],[0]
A bidirectional LSTM model which concatenates the question and answer text as a sequence.,5.2 Experimental Results,[0],[0]
• Bidirectional-Match:,5.2 Experimental Results,[0],[0]
"This approach employs QA bidirectional matching mechanism, without taking the sentence segmentation strategy and selfmatching attention mechanism.",5.2 Experimental Results,[0],[0]
• AtoQ-Match:,5.2 Experimental Results,[0],[0]
"This approach takes the sentence segmentation strategy, and employs QA unidirectional matching mechanism (i.e., only using Answer-to-Question attention), but does not employ self-matching attention mechanism.",5.2 Experimental Results,[0],[0]
We average the Answer-to-Question matching vectors to represent the QA text pair.,5.2 Experimental Results,[0],[0]
•,5.2 Experimental Results,[0],[0]
QtoA-Match:,5.2 Experimental Results,[0],[0]
"This approach takes the sentence segmentation strategy, and employs QA unidirectional matching mechanism (i.e., only using Question-to-Answer attention), but does not employ self-matching attention mechanism.",5.2 Experimental Results,[0],[0]
"• Bidirectional-Match QA: This approach takes the sentence segmentation strategy, and employs QA bidirectional matching mechanism, but does not employ self-matching attention mechanism.",5.2 Experimental Results,[0],[0]
• HMN:,5.2 Experimental Results,[0],[0]
"This is our hierarchical matching network model which takes the sentence segmentation strategy and employs both QA bidirectional matching mechanism and self-matching attention mechanism.
",5.2 Experimental Results,[0],[0]
"Table 2 summarizes the experimental results of all the approaches above, and we can find that:
(1) All LSTM-based approaches are superior to SVM, indicating the effectiveness of neural network for this task.",5.2 Experimental Results,[0],[0]
"(2) The proposed approaches, with novel QA contextual representation, outperform the other baseline approaches.",5.2 Experimental Results,[0],[0]
"(3) When only employing QA bidirectional matching mechanism, Bidirectional-Match QA, which takes the sentence segmentation strategy, consistently outperforms Bidirectional-Match (without sentence segmentation) in all domains.",5.2 Experimental Results,[0],[0]
It confirms our hypothesis that sentence segmentation helps to extract the sentiment matching information between the question and answer.,5.2 Experimental Results,[0],[0]
"(4) When comparing to QA unidirectional matching mechanism, Bidirectional-Match QA, which employs QA bidirectional matching mechanism, performs better than AtoQMatch and QtoA-Match.",5.2 Experimental Results,[0],[0]
It confirms our hypothesis that both the question and answer information contribute to sentiment polarity of the QA text pair.,5.2 Experimental Results,[0],[0]
"(5) Impressively, the proposed approach HMN significantly outperforms all the other approaches in all domains (p-value<0.05 via ttest).",5.2 Experimental Results,[0],[0]
"It verifies the advantages of both QA bidirectional matching mechanism and selfmatching attention mechanism for this task.
",5.2 Experimental Results,[0],[0]
"Besides, we also implement some more recent state-of-the-art approaches for sentiment classification, which are illustrated in Table 3.",5.2 Experimental Results,[0],[0]
"This result also supports the earlier findings.
",5.2 Experimental Results,[0],[0]
"• CNN-Tensor (Lei et al., 2015):",5.2 Experimental Results,[0],[0]
"This is a stateof-the-art approach to sentence-level sentiment classification, which models n-gram interactions based on tensor product and evaluates all non-
consecutive n-gram vectors as a feature mapping operator for CNNs.",5.2 Experimental Results,[0],[0]
"• Attention-LSTM (Wang et al., 2016):",5.2 Experimental Results,[0],[0]
This is a state-of-the-art approach to aspect-level sentiment classification.,5.2 Experimental Results,[0],[0]
"In our implementation, we ignore the aspect embedding and directly use the outputs of LSTM to yield the attention.",5.2 Experimental Results,[0],[0]
• BiMPM,5.2 Experimental Results,[0],[0]
"(Wang et al., 2017):",5.2 Experimental Results,[0],[0]
"This is a state-ofthe-art approach to QA matching, which matches the question and answer from multiple perspectives.",5.2 Experimental Results,[0],[0]
"In our implementation, we use the matching representation to perform QA-style sentiment classification with a softmax classifier.",5.2 Experimental Results,[0],[0]
• HMN:,5.2 Experimental Results,[0],[0]
"The proposed hierarchical matching network which employs both QA bidirectional matching mechanism and self-matching attention mechanism, and takes the sentence segmentation strategy.
",5.2 Experimental Results,[0],[0]
Table 3 shows the comparison results of these strong baseline approaches and the proposed approach (HMN) in all domains.,5.2 Experimental Results,[0],[0]
"From this table, we can find that: (1) the approaches that take matching strategy, i.e., BiMPM and our approach (HMN), outperform other approaches.",5.2 Experimental Results,[0],[0]
"(2) The proposed approach (HMN) significantly outperforms all the other baseline approaches in terms of both Macro-F1 and Accuracy (p-value<0.05 via ttest), which confirms the initial hypotheses of this study.",5.2 Experimental Results,[0],[0]
"Table 4 shows some examples, along with the predicted categories via different approaches.",5.3 Case Study,[0],[0]
"We can find that: (1) the approaches based on matching strategy (BiMPM and HMN) are well-performed, as shown in E9, when question and answer carrying different kinds of information.",5.3 Case Study,[0],[0]
"This is a unique challenge for QA-style sentiment mining, and traditional sentiment classification approaches can hardly address this problem.",5.3 Case Study,[0],[0]
"(2) The proposed approach (HMN) performs better than other approaches when dealing with conflict instances, as shown in E10.",5.3 Case Study,[0],[0]
"To get a better understanding of our proposed hierarchical matching network for QA-style sentiment classification, we picture the attention weights obtained from Equations (5), (8) and (13).",5.4 Visualization of Attention,[0],[0]
"For
simplicity, we directly use the English translation of E11 for illustration and adopt the visualization approach presented by Yang et al. (2016), as shown in Figure 2.",5.4 Visualization of Attention,[0],[0]
"Specifically, each line is a [Qsentence, A-sentence] unit, where the red denotes the [Q-sentence, A-sentence] unit weight, the blue denotes the word weight in each [Q-sentence, Asentence], and the color depth indicates the importance of attention weights (the darker the more important).
",5.4 Visualization of Attention,[0],[0]
"From Figure 4, we can see that the QA bidirectional matching layer always assigns reasonable attention weights to words in each [Q-sentence, Asentence] unit which makes sentence from question and sentence from answer match correctly.",5.4 Visualization of Attention,[0],[0]
"In addition, the self-matching attention layer is able to select informative",5.4 Visualization of Attention,[0],[0]
"[Q-sentence, A-sentence] unit for predicting true sentiment polarity of this example.",5.4 Visualization of Attention,[0],[0]
"In this paper, we propose a novel but important sentiment analysis task, i.e., QA-style sentiment mining, and we build a large-scale highquality human annotated corpus for experiment.",6 Conclusion,[0],[0]
The dataset is shared to encourage other scholars to investigate this interesting problem.,6 Conclusion,[0],[0]
"Moreover, we propose a hierarchical matching neural network model to enable QA bidirectional matching mechanism and self-matching attention mechanism for this task.",6 Conclusion,[0],[0]
"Empirical studies show that the proposed approach significantly outperforms other strong baseline approaches in all the test domains for QA-style sentiment classification.
",6 Conclusion,[0],[0]
"In the future, we would like to investigate some other network structures to explore deeper information in each QA text pair.",6 Conclusion,[0],[0]
"Besides, we would like to test the effectiveness of the proposed approach to QA-style sentiment classification in some other languages.",6 Conclusion,[0],[0]
We would like to thank the anonymous reviewers for their valuable comments.,Acknowledgments,[0],[0]
"This work is partially supported by the National Key R&D Program of China under Grant No.2017YFB1002101 and two NSFC grants No.61331011, No.61672366.",Acknowledgments,[0],[0]
This work is also supported by the joint research project of Alibaba Group and Soochow University.,Acknowledgments,[0],[0]
"In an e-commerce environment, user-oriented question-answering (QA) text pair could carry rich sentiment information.",abstractText,[0],[0]
"In this study, we propose a novel task/method to address QA sentiment analysis.",abstractText,[0],[0]
"In particular, we create a high-quality annotated corpus with speciallydesigned annotation guidelines for QA-style sentiment classification.",abstractText,[0],[0]
"On the basis, we propose a three-stage hierarchical matching network to explore deep sentiment information in a QA text pair.",abstractText,[0],[0]
"First, we segment both the question and answer text into sentences and construct a number of [Q-sentence, Asentence] units in each QA text pair.",abstractText,[0],[0]
"Then, by leveraging a QA bidirectional matching layer, the proposed approach can learn the matching vectors of each [Q-sentence, A-sentence] unit.",abstractText,[0],[0]
"Finally, we characterize the importance of the generated matching vectors via a selfmatching attention layer.",abstractText,[0],[0]
"Experimental results, comparing with a number of state-ofthe-art baselines, demonstrate the impressive effectiveness of the proposed approach for QA-style sentiment classification.",abstractText,[0],[0]
Sentiment Classification towards Question-Answering with Hierarchical Matching Network,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2860–2865 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
NLP research relies heavily on annotated datasets for training and evaluation.,1 Introduction,[0],[0]
"The design of the annotation task can influence the decisions made by annotators in subtle ways: besides the actual features of the instance being annotated, annotators are also influenced by factors such as the user interface, wording of the question, and familiarity with the task or domain.
",1 Introduction,[0],[0]
"When collecting NLP annotations, care is usually taken to ensure that the annotations are of high quality, through careful design of label sets, annotation guidelines and training of annotators (Hovy et al., 2006), methods for aggregating annotations (Passonneau and Carpenter, 2014), and intuitive user interfaces (Stenetorp et al., 2012).
",1 Introduction,[0],[0]
"Crowdsourcing has emerged as a cheaper, faster alternative to expert NLP annotations (Snow et al.,
2008; Callison-Burch and Dredze, 2010; Graham et al., 2017), although it entails additional effort to filter out unskilled or opportunistic workers, e.g. through the collection of redundant repeated judgements for each instance, or including some trap questions with known answers (CallisonBurch and Dredze, 2010; Hoßfeld et al., 2014).",1 Introduction,[0],[0]
"In most annotation exercises, the order of presentation of instances is randomised to remove bias due to similarities in topic, style and vocabulary (Koehn and Monz, 2006; Bojar et al., 2016).
",1 Introduction,[0],[0]
"When crowdsourcing judgements, the normal practise (as used in the datasets we analyse) is for the item ordering to be randomised in creating a “HIT” (i.e. a single collection of items presented to a crowdworker for judgement), and then to have each HIT annotated by multiple workers, for quality control purposes.",1 Introduction,[0],[0]
"The order of items is generally fixed across all annotators of an individual HIT (Snow et al., 2008; Graham et al., 2017).
",1 Introduction,[0],[0]
"In this paper, we show that worker scores are affected by sequence bias, whereby the order of presentation can affect individuals’ assessment of an item.",1 Introduction,[0],[0]
"Since all workers see the instances in the same order, this affects any other inferences made from the data, including aggregated assessment or inferences about individual annotators (such as their overall quality or individual thresholds).
",1 Introduction,[0],[0]
"Possible explanations for sequence effects include:
Gambler’s fallacy: Once annotators have developed an idea of the distribution of scores/labels, they can come to expect even small sequences to follow the distribution.",1 Introduction,[0],[0]
"In particular, in binary annotation tasks, if they expect that True (1) and False (0) items are equally likely, then they believe the sequence 00000 (100% False and 0% True) is less likely than the sequence 01010 (50% False and 50% True).",1 Introduction,[0],[0]
"So if they assign 0 to an item,
2860
they may approach the next item with a prior belief that it is more likely to be a 1 than a 0.",1 Introduction,[0],[0]
"Chen et al. (2016) showed evidence for the gambler’s fallacy in decisions of loan officers, asylum judges, and baseball umpires.
",1 Introduction,[0],[0]
Sequential contrast effects: A high quality item may raise the bar for the next item.,1 Introduction,[0],[0]
"On the other hand, a bad item may make the next item seem better in comparison (Kenrick and Gutierres, 1980; Hartzmark and Shue, to appear)
",1 Introduction,[0],[0]
"Assimilation and anchoring: The annotator uses their score of the previous item as an anchor, and adjusts the score of the current item from this anchor, based on perceived similarities and differences with the previous item.",1 Introduction,[0],[0]
"If they focus on similarities between the previous and current instance, the annotations show an assimilation effect (Geiselman et al., 1984; Damisch et al., 2006).",1 Introduction,[0],[0]
"Anchoring effects may decrease as people gain experience and expertise in the task (Wilson et al., 1996).",1 Introduction,[0],[0]
"We test whether the annotation of an instance is correlated with the annotation on previous instances, conditioned on control variables such as the gold standard (i.e. expert annotations1), based on the following linear model:
Yi,t = β0 + β1Yi,t−1 + β2Gold + η (1)
where Yi,t is the annotation given by an annotator i to an instance t, and η is white Gaussian noise with zero mean.",2 Methodology,[0],[0]
We use linear regression for continuous data and logistic regression for binary data.2,2 Methodology,[0],[0]
"If there is no dependence between consecutive instances, and annotators assign labels/scores based only on the aspects of the current instance, then the data can be explained from the gold score (learning a positive β2 value) and bias term (β0), with β1 set to zero.",2 Methodology,[0],[0]
"When we use the ground truth as a control, if β1 is non-zero, it is evidence of mistakes being made by annotators due to sequential bias.",2 Methodology,[0],[0]
"A positive value of β1 can be explained by priming or anchoring, and a negative value with sequential contrast effects or the gambler’s fallacy.",2 Methodology,[0],[0]
"Accordingly, we test the statistical significance of
1For the Machine Translation dataset described in Section 3.3, we use the mean of at least fifteen crowd workers as a proxy for expert annotations.
",2 Methodology,[0],[0]
"2η is not included in the case of logistic regression
the β1 6= 0",2 Methodology,[0],[0]
to determine whether sequencing effects are present in crowdsourced text corpora.,2 Methodology,[0],[0]
"We analyse several influential datasets that have been constructed through crowdsourcing, including both binary and continuous annotation tasks: recognising textual entailment, event ordering, affective text analysis, and machine translation evaluation.",3 Experiments,[0],[0]
"First, we examine the recognising textual entailment (“RTE”) and event temporal ordering (“TEMPORAL”) datasets from Snow et al. (2008).",3.1 Recognising Textual Entailment (RTE) and Event Temporal Ordering,[0],[0]
"In the RTE task, annotators are presented with two sentences, and are asked to judge whether the second text can be inferred from the first.",3.1 Recognising Textual Entailment (RTE) and Event Temporal Ordering,[0],[0]
"With the TEMPORAL dataset, they are shown two sentences describing events, and asked to indicate which of the two events occurred first.",3.1 Recognising Textual Entailment (RTE) and Event Temporal Ordering,[0],[0]
Both datasets include both expert annotations and crowdsourced annotations constructed using Amazon Mechanical Turk (“MTurk”).,3.1 Recognising Textual Entailment (RTE) and Event Temporal Ordering,[0],[0]
"On MTurk, each RTE HIT contains 20 instances, and each TEMPORAL HIT contains 10 instances, which the workers see in sequential order.",3.1 Recognising Textual Entailment (RTE) and Event Temporal Ordering,[0],[0]
"For both tasks, each HIT was annotated by 10 workers.
",3.1 Recognising Textual Entailment (RTE) and Event Temporal Ordering,[0],[0]
"Results We use logistic regression on worker labels against labels on the previous instance in the current HIT, with the expert judgements as a control variable.",3.1 Recognising Textual Entailment (RTE) and Event Temporal Ordering,[0],[0]
"We also add an additional control, namely the percentage of True labels assigned by the worker overall, which accounts for the overall annotator bias.",3.1 Recognising Textual Entailment (RTE) and Event Temporal Ordering,[0],[0]
"To calculate this, we use scores by the worker excluding the current score, to avoid giving the model any information about the current instance.
",3.1 Recognising Textual Entailment (RTE) and Event Temporal Ordering,[0],[0]
"As shown in Table 1, over all workers (“All”), we find a small negative autocorrelation for both the RTE and TEMPORAL tasks.",3.1 Recognising Textual Entailment (RTE) and Event Temporal Ordering,[0],[0]
"One possibility
is that this is biased by opportunistic workers who assign the same label to all instances in the HIT, for which we would not expect any sequential bias effects.",3.1 Recognising Textual Entailment (RTE) and Event Temporal Ordering,[0],[0]
"When we exclude these workers (“Moderate”), the autocorrelation increases, and is highly statistically significant.",3.1 Recognising Textual Entailment (RTE) and Event Temporal Ordering,[0],[0]
"We also show results for workers with at least 60% accuracy when compared to expert annotations (“Good”), and observe a similar effect.",3.1 Recognising Textual Entailment (RTE) and Event Temporal Ordering,[0],[0]
"In the affective text analysis task (“AFFECTIVE”), annotators are asked to rate news headlines for anger, disgust, fear, joy, sadness, and surprise on a continuous scale of 0–100.",3.2 Affective text analysis,[0],[0]
"Besides these emotions, they are asked to rate sentences for (emotive) valence, i.e., how strongly negative or positive they are (−100 to +100).",3.2 Affective text analysis,[0],[0]
"In this dataset, there are 100 headlines divided into 10 HITs, with 10 workers annotating each HIT (Snow et al., 2008).",3.2 Affective text analysis,[0],[0]
"We test for autocorrelation of scores of each aspect individually, controlling for the expert scores and worker correlation with the expert scores.",3.2 Affective text analysis,[0],[0]
"We also look separately at datasets of good and bad workers, based on whether the correlation with the expert annotations is greater than 0.5.
",3.2 Affective text analysis,[0],[0]
"Results For individual emotions, we do not observe any significant autocorrelation (p ≥ 0.05).",3.2 Affective text analysis,[0],[0]
"As there are only 1000 annotations per emotion, we also look at results when combining data for all aspects.",3.2 Affective text analysis,[0],[0]
"Though we find a statistically significant negative autocorrelation for scores of the full dataset, this disappears when we filter out bad workers (Table 2).",3.2 Affective text analysis,[0],[0]
"Given the difficulty of this very subjective task, it is likely that many of workers considered ‘bad’ might have simply found this task too difficult or arbitrary, and thus become more prone to sequence effects.",3.2 Affective text analysis,[0],[0]
"When evaluating machine translation (“MT”), we tend to focus on adequacy: the extent to which the meaning of the reference translation is captured in the MT output.",3.3 Machine Translation Adequacy,[0],[0]
"In the method of Graham et al. (2015) — the current best-practise, as adopted by WMT (Bojar et al., 2016) — annotators are asked to judge the adequacy of translations using a 100- point sliding scale which is initialised at the mid point.",3.3 Machine Translation Adequacy,[0.9524919500195487],['One main assumption was that the conditional distribution of the class given a data instance is the same in the training and test data sets (Shimodaira 2000; Huang et al. 2007; Bickel et al. 2009).']
There are 3 marks on the scale dividing it into 4 quarters to aid workers with internal calibration.,3.3 Machine Translation Adequacy,[0],[0]
"They are given no other instructions or
guidelines.",3.3 Machine Translation Adequacy,[0],[0]
"In this paper, we base our analysis on the adequacy dataset of Graham et al. (2015), on SpanishEnglish newswire data from WMT 2013 (Bojar et al., 2013).",3.3 Machine Translation Adequacy,[0],[0]
"The dataset consists of 12 HITS of 100 sentence pairs each; each HIT is annotated by at least 15 workers.
",3.3 Machine Translation Adequacy,[0],[0]
HITs are designed to include quality control items to filter out poor quality scores.,3.3 Machine Translation Adequacy,[0],[0]
"In addition to 70 MT system translations, each HIT contains degraded versions of 10 of these translations, 10 reference translations by a human expert corresponding to 10 of these translations, and repeats of another 10 translations.",3.3 Machine Translation Adequacy,[0],[0]
"Good workers are assumed to give high scores to the references, similar scores to the pair of repeats, and high scores to the MT system translations when compared to corresponding degraded translations.",3.3 Machine Translation Adequacy,[0],[0]
Workers who submitted scores of clearly bad quality were rejected.,3.3 Machine Translation Adequacy,[0],[0]
"For the remaining workers, the Wilcoxon rank-sum test is used to test whether the score difference between the repeat judgements is less than the score difference between translations and the corresponding degraded versions.",3.3 Machine Translation Adequacy,[0],[0]
"We divide these workers into “good” and “moderate” based on the threshold of p < 0.05.
",3.3 Machine Translation Adequacy,[0],[0]
"To eliminate differences due to different internal scales, every individual worker’s scores are standardised by subtracting the mean and dividing by the standard deviation of their scores.",3.3 Machine Translation Adequacy,[0],[0]
"Following Graham et al. (2015), we use the average of standardised scores of at least 15 good workers as the ground truth.
",3.3 Machine Translation Adequacy,[0],[0]
"We refer to the final dataset as “MTadeq”.
Results As this is a (practically) continuous output, we use a linear regression model, whereby the current score is predicted based on the previous score, with the mean of all worker scores as control.",3.3 Machine Translation Adequacy,[0],[0]
"We also controlled for worker correlation with mean score, and position of the sentence in the HIT, but these were not significant and did not affect the autocorrelation.",3.3 Machine Translation Adequacy,[0],[0]
"As seen in Table 3, we see a small but significant positive autocorrelation for good workers.",3.3 Machine Translation Adequacy,[0],[0]
"The bias is much stronger with
bad (rejected) workers.",3.3 Machine Translation Adequacy,[0],[0]
"An interesting question is whether the bias changes as workers annotate more data, which could be ascribed to learning through the task, calibrating their internal scales, or becoming fatigued on a monotonous task.",3.3 Machine Translation Adequacy,[0],[0]
"Each HIT consists of 100 sentences, and we divide the dataset into 3 equal groups based on the position of sentence in the HIT.",3.3 Machine Translation Adequacy,[0],[0]
"As shown in Table 4, for good and moderate workers, the bias is stronger in the first group of sentences annotated, decreases in the second, and is much smaller in the last.",3.3 Machine Translation Adequacy,[0],[0]
"This could be because workers are familiarising themselves with the task earlier on, and calibrating their scale.",3.3 Machine Translation Adequacy,[0],[0]
"There is no such trend with bad quality scores, possibly because the workers are not putting in sufficient effort to produce accurate scores.
",3.3 Machine Translation Adequacy,[0],[0]
Next we assess the impact of the bias in the worst case situation.,3.3 Machine Translation Adequacy,[0],[0]
"We discretize scores into low, middle and high based on equal-frequency binning, and divide the dataset into 3 groups based on the score assigned to the previous sentence.",3.3 Machine Translation Adequacy,[0],[0]
"As shown in Table 5 we can see that the sentences in the “low” partition and the “high” partition have a difference of 0.18, which is highly significant;3 moreover, this difference is likely to be sufficiently large to alter the rankings of systems in an evaluation.",3.3 Machine Translation Adequacy,[0],[0]
"The bias remains even when we increase the number of workers and use the average score, as all workers scored the translations in the same order.",3.3 Machine Translation Adequacy,[0],[0]
"This shows that the mean is also affected by
3p < 0.001 using Welch’s two-sample t-test
sequence bias.",3.3 Machine Translation Adequacy,[0],[0]
"Thus, it is theoretically possible to exploit sequence bias to artificially deflate (or inflate) a specific system’s computed score by ordering a HIT such that the system’s output is seen consistently immediately after a bad (or good) output.",3.3 Machine Translation Adequacy,[0],[0]
"We have shown significant sequence effects across several independent crowdsourced datasets: a negative autocorrelation in the RTE and TEMPORAL datasets, and a positive autocorrelation in the MTadeq dataset.",4 Discussion and Conclusions,[0],[0]
The negative autocorrelation can be attributed either to sequential contrast effects or the gambler’s fallacy.,4 Discussion and Conclusions,[0],[0]
"These effects were not significant for the AFFECTIVE dataset, perhaps due to the nature of the annotation task, whereby annotations of one emotion are separated by six other annotations, thus limiting the potential for sequencing effects.",4 Discussion and Conclusions,[0],[0]
"It is also possible that the dataset is too small to obtain statistical significance.
",4 Discussion and Conclusions,[0],[0]
"MT judgements are subjective, and when people are asked to rate them on a continuous scale, they need time to calibrate their scale.",4 Discussion and Conclusions,[0],[0]
"We show that the sequential bias decreases for better workers as they annotate more sentences in the HIT, indicating a learning effect.",4 Discussion and Conclusions,[0],[0]
"Since the ordering of the systems is random, system scores obtained by averaging scores of all sentences translated by the system would be unbiased, assuming a sufficiently large sample of sentences.",4 Discussion and Conclusions,[0],[0]
Thus we do not expect sequential bias to have a marked effect on system rankings or other macro-level conclusions on the basis of this data.,4 Discussion and Conclusions,[0],[0]
"However, the scores of in-
dividual translations remain biased, which augurs poorly for the use of these annotations at the sentence level, such as when used in error analysis or for training automatic metrics.
",4 Discussion and Conclusions,[0],[0]
"Sequence problems can be easily addressed by adequate randomisation — providing each individual worker with a separate dataset that has been randomised, such that no two workers see the same ordered data.",4 Discussion and Conclusions,[0],[0]
"In this way sequence bias effects can be considered as independent noise sources, rather than a systematic bias, and consequently the aggregate results over several workers will remain unbiased.
",4 Discussion and Conclusions,[0],[0]
"This study has shown that sequence bias is real, and can distort evaluation and annotation exercises with crowd-workers.",4 Discussion and Conclusions,[0],[0]
"We limited our scope to binary and continuous responses, however it is likely that sequence effects are prevalent for multinomial and structured outputs, e.g., in discourse and parsing, where priming is known to have a significant effect (Reitter et al., 2006).",4 Discussion and Conclusions,[0],[0]
"Another important question for future work is whether sequence bias is detectable in expert annotators, not just crowd workers.",4 Discussion and Conclusions,[0],[0]
We thank the anonymous reviewers for their valuable feedback.,Acknowledgments,[0],[0]
This work was supported in part by the Australian Research Council.,Acknowledgments,[0],[0]
Manual data annotation is a vital component of NLP research.,abstractText,[0],[0]
"When designing annotation tasks, properties of the annotation interface can lead to unintentional artefacts in the resulting dataset, biasing the evaluation.",abstractText,[0],[0]
"In this paper, we explore sequence effects where annotations of an item are affected by the preceding items.",abstractText,[0],[0]
"Having assigned one label to an instance, the annotator may be less (or more) likely to assign the same label to the next.",abstractText,[0],[0]
"During rating tasks, seeing a low quality item may affect the score given to the next item either positively or negatively.",abstractText,[0],[0]
We see clear evidence of both types of effects using auto-correlation studies over three different crowdsourced datasets.,abstractText,[0],[0]
We then recommend a simple way to minimise sequence effects.,abstractText,[0],[0]
Sequence Effects in Crowdsourced Annotations,title,[0],[0]
"The success of recurrent neural network (RNN) models in complex tasks like machine translation and audio synthesis has inspired immense interest in learning from sequence data (Eck & Schmidhuber, 2002; Graves, 2013; Sutskever et al., 2014; Karpathy, 2015).",Introduction,[0],[0]
"Comprised of elements s
t P S , which are typically symbols from a discrete vocabulary, a sequence x “ ps1, . . .",Introduction,[0],[0]
", sT q P X has length T which can vary between different instances.",Introduction,[0],[0]
"Sentences are a popular example of such data, where each s
j is a word from the language.",Introduction,[0],[0]
"In many domains, only a tiny fraction of X (the set of possible sequences over a given vocabulary) represents sequences likely to be found in nature (ie.
1MIT Computer Science & Artificial Intelligence Laboratory.",Introduction,[0],[0]
"Correspondence to: J. Mueller <jonasmueller@csail.mit.edu>.
",Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",Introduction,[0],[0]
"Copyright 2017 by the author(s).
",Introduction,[0],[0]
those which appear realistic),Introduction,[0],[0]
.,Introduction,[0],[0]
"For example: a random sequence of words will almost never form a coherent sentence that reads naturally, and a random amino-acid sequence is highly unlikely to specify a biologically active protein.
",Introduction,[0],[0]
"In this work, we consider applications where each sequence x is associated with a corresponding outcome y P R. For example: a news article title or Twitter post can be associated with the number of shares it subsequently received online, or the amino-acid sequence of a synthetic protein can be associated with its clinical efficacy.",Introduction,[0],[0]
"We operate under the standard supervised learning setting, assuming availability of a dataset D
n",Introduction,[0],[0]
"“ tpx i , y i qun i“1 iid„ p XY
of sequence-outcome pairs.",Introduction,[0],[0]
"The marginal distribution p X
is assumed as a generative model of the natural sequences, and may be concentrated in a small subspace of X .",Introduction,[0],[0]
"Throughout this paper, p denotes both density and distribution functions depending on the referenced variable.
",Introduction,[0],[0]
"After fitting models to D n , we are presented a new sequence x0 P X (with unknown outcome), and our goal is to quickly identify a revised version that is expected to have superior outcome.",Introduction,[0],[0]
"Formally, we seek the revised sequence:
x˚ “ argmax xPC
x0
ErY | X “ xs (1)
Here, we want the set C x0 of feasible revisions to ensure that x˚ remains natural and is merely a minor revision of x0.",Introduction,[0],[0]
"Under a generative modeling perspective, these two goals are formalized as the following desiderata: p
X px˚q is not too small, and x˚ and x0 share similar underlying latent characteristics.",Introduction,[0],[0]
"When revising a sentence for example, it is imperative that the revision reads naturally (has reasonable likelihood under the distribution of realistic sentences) and retains the semantics of the original.
",Introduction,[0],[0]
This optimization is difficult because the constraint-set and objective may be highly complex and are both unknown (must be learned from data).,Introduction,[0],[0]
"For many types of sequence such as sentences, standard distance measures applied directly in the space of X or S (eg. Levenshtein distance or TF-IDF similarity) are inadequate to capture meaningful similarities, even though these can be faithfully reflected by a simple metric over an appropriately learned space of continuous latent factors (Mueller & Thyagarajan, 2016).",Introduction,[0],[0]
"In this work, we introduce a generative-modeling framework which transforms (1) into a simpler differentiable optimiza-
tion by leveraging continuous-valued latent representations learned using neural networks.",Introduction,[0],[0]
"After the generative model has been fit, our proposed procedure can efficiently revise any new sequence in a manner that satisfies the aforementioned desiderata (with high probability).",Introduction,[0],[0]
"Unlike imitation learning, our setting does not require availability of improved versions of a particular sequence.",Related Work,[0],[0]
"This prevents direct application of a sequence-to-sequence model (Sutskever et al., 2014).",Related Work,[0],[0]
"Similar to our approach, Gómez-Bombarelli et al. (2016) also utilize latent autoencoder representations in order to propose novel chemical structures via Bayesian optimization.",Related Work,[0],[0]
"However, unlike sequential bandit/reinforcement-learning settings, our learner sees no outcomes outside of the training data, neither for the new sequence it is asked to revise, nor for any of its proposed revisions of said sequence (Mueller et al., 2017).",Related Work,[0],[0]
"Our methods only require an easily-assembled dataset of sequence-outcome pairs and are thus widely applicable.
",Related Work,[0],[0]
"Combinatorial structures are often optimized via complex search heuristics such as genetic programming (Zaefferer et al., 2014).",Related Work,[0],[0]
"However, search relies on evaluating isolated changes in each iteration, whereas good revisions of a sequence are often made over a larger context (ie. altering a phrase in a sentence).",Related Work,[0],[0]
"From the vast number of possibilities, such revisions are unlikely to be found by search-procedures, and it is generally observed that such methods are outperformed by gradient-based optimization in high-dimensional continuous settings.",Related Work,[0],[0]
"Unlike combinatorial search, our framework leverages gradients in order to efficiently find good revisions at test time.",Related Work,[0],[0]
"Simonyan et al. (2014) and Nguyen et al. (2015) also proposed gradientbased optimization of inputs with respect to neural predictions, but work in this vein has been focused on conditional generation (rather than revision) and is primarily restricted to the continuous image domain (Nguyen et al., 2016).",Related Work,[0],[0]
"To identify good revisions, we first map our stochastic combinatorial optimization problem into a continuous space where the objective and constraints exhibit a simpler form.",Methods,[0],[0]
"We assume the data are generated by the probabilistic graphical model in Figure 1A. Here, latent factors Z P Rd specify a (continuous) configuration of the generative process for X,Y (both sequences and outcomes), and we adopt the prior p
Z “ Np0, Iq.",Methods,[0],[0]
"Relationships between these variables are summarized by the maps F,E,D which we parameterize using three neural networks F ,E ,D trained to enable efficient approximate inference under this model.
",Methods,[0],[0]
"The first step of our framework is to fit this model to D n
by learning the parameters of these inference networks: the encoder E , the decoder D , and the outcome-predictor F .",Methods,[0],[0]
"A good model that facilitates high-quality revision under our framework will possess the following properties: (1) Y can efficiently be inferred from Z and this relationship obeys a smooth functional form, (2) the map D produces a realistic sequence x given any z with reasonable prior probability, (3) the distribution of natural sequences is geometrically simple in the latent Z-space.",Methods,[0],[0]
"We explicitly encourage (1) by choosing F as a fairly simple feedforward network, (2) by defining D as the most-likely x given z, and (3) by endowing Z with our simple Np0, Iq prior.",Methods,[0],[0]
Another characteristic desired of our Z-representations is that they encode meaningful sequence-features such that two fundamentally similar sequences are likely to have been generated from neighboring z-values.,Methods,[0],[0]
"Applied to image data, VAE models similar to ours have been found to learn latent representations that disentangle salient characteristics such as scale, rotation, and other independent visual concepts (Higgins et al., 2016).",Methods,[0],[0]
"The latent representations of recurrent architectures trained on text (similar to the models used here) have also been shown to encode meaningful semantics, with a strong correlation between distances in the latent space and human-judged similarity between texts (Mueller & Thyagarajan, 2016).",Methods,[0],[0]
"By exploiting such simplified geometry, a basic shift in the latent vector space may be able to produce higher-quality revisions than attempts to directly manipulate the combinatorial space of sequence elements.
",Methods,[0],[0]
"After fitting a model with these desirable qualities, our strategy to revise a given sequence x0 P X is outlined in Figure 1B. First, we compute its latent representation z0 “ Epx0q using a trained encoding map.",Methods,[0],[0]
"As the latent representations z are continuous, we can employ efficient gradient-based optimization to find a nearby local optimum z˚ of F pzq (within a simple constraint-set around z0 defined later on).",Methods,[0],[0]
"To z˚, we subsequently apply a simple decoding map D (defined with respect to our learned model) in order to obtain our revised sequence x˚. Under our
assumed model, the optimization in latent representationspace attempts to identify a generative configuration which produces large values of Y (as inferred via F ).",Methods,[0],[0]
The subsequent decoding step seeks the most likely sequence produced by the optimized setting of the latent factors.,Methods,[0],[0]
"For approximate inference in the X,Z relationship, we leverage the variational autoencoder (VAE) model of Kingma & Welling (2014).",Variational Autoencoder,[0],[0]
"In our VAE, a generative model of sequences is specified by our prior over the latent values z combined with a likelihood function p
D px | zq which our decoder network D outputs in order to evaluate the likelihood of any sequence x given z P Rd.",Variational Autoencoder,[0],[0]
"Given any sequence x, our encoder network E outputs a variational approximation q
E pz | xq of the true posterior over the latent-values ppz | xq9 p
D px | zqp Z pzq.",Variational Autoencoder,[0],[0]
"As advocated by Kingma & Welling (2014) and Bowman et al. (2016), we employ the variational family q
E pz",Variational Autoencoder,[0],[0]
| xq,Variational Autoencoder,[0],[0]
"“ Npµ z|x,⌃z|x)",Variational Autoencoder,[0],[0]
"with diag-
onal covariance",Variational Autoencoder,[0],[0]
.,Variational Autoencoder,[0],[0]
"Our revision methodology employs the encoding procedure Epxq “ µ
z|x which maps a sequence to the maximum a posteriori (MAP) configuration of the latent values z (as estimated by the encoder network E ).",Variational Autoencoder,[0],[0]
"The parameters of E ,D are learned using stochastic variational inference to maximize a lower bound for the marginal likelihood of each observation in the training data:
log p X pxq • ´ “ Lrecpxq ` Lpripxq ‰ (2)",Variational Autoencoder,[0],[0]
"Lrecpxq “ ´E q
E pz|xq rlog",Variational Autoencoder,[0],[0]
"pDpx | zqs Lpripxq “ KLpqEpz | xq|| pZq
Defining z|x",Variational Autoencoder,[0],[0]
“,Variational Autoencoder,[0],[0]
"diagp⌃z|xq, the prior-enforcing KullbackLeibler divergence has a differentiable closed form expression when q
E , p Z are diagonal Gaussian distributions.",Variational Autoencoder,[0],[0]
"The reconstruction term Lrec (ie. negative log-likelihood under the decoder model) is efficiently approximated using just one Monte-Carlo sample z „ q
E pz | xq.",Variational Autoencoder,[0],[0]
"To optimize the variational lower bound over our data D
n with respect to the parameters of neural networks E ,D , we use stochastic gradients of (2) obtained via backpropagation and the reparameterization trick of Kingma & Welling (2014).
",Variational Autoencoder,[0],[0]
"Throughout, our encoder/decoder models E ,D are recurrent neural networks (RNN).",Variational Autoencoder,[0],[0]
"RNNs adapt standard feedforward neural networks for sequence data x “ ps1, . . .",Variational Autoencoder,[0],[0]
", sT q, where at each time-step t P t1, . . .",Variational Autoencoder,[0],[0]
", T u, a fixed size hiddenstate vector h
t P Rd is updated based on the next element in the input sequence.",Variational Autoencoder,[0],[0]
"To produce the approximate posterior for a given x, our encoder network E appends the following additional layers to the final RNN hidden-state (parameterized by W
µ
,W ,W v , b µ , b , b v ):
µ z|x “ WµhT ` bµ P Rd
z|x",Variational Autoencoder,[0],[0]
"“ expp´|W v ` b |q, v “ ReLUpWvhT ` bvq (3)
",Variational Autoencoder,[0],[0]
The (squared) elements of z|x P Rd form the diagonal of our approximate-posterior covariance ⌃ z|x.,Variational Autoencoder,[0],[0]
"Since Lpri is minimized at z|x “ ~1 and Lrec is likely to worsen with additional variance in encodings (as our posterior approximation is unimodal), we simply do not consider
z|x values that exceed 1 in our variational family.",Variational Autoencoder,[0],[0]
This restriction results in more stable training and also encourages the encoder and decoder to co-evolve such that the true posterior is likely closer to unimodal with variance § 1.,Variational Autoencoder,[0],[0]
"To evaluate the likelihood of a sequence, RNN D computes not only its hidden state h
t
, but also the additional output:
⇡ t “ softmaxpW ⇡ h t ` b ⇡ q (4)
",Variational Autoencoder,[0],[0]
"At each position t, ⇡ t estimates pps t | s1, . . .",Variational Autoencoder,[0],[0]
", st´1q by relying on h
t to summarize the sequence history.",Variational Autoencoder,[0],[0]
"By the factorization pps1, . . .",Variational Autoencoder,[0],[0]
", sT q “ ± T
t“1 ppst | st´1, . . .",Variational Autoencoder,[0],[0]
", s1q, we have p
D px",Variational Autoencoder,[0],[0]
"| zq “ ±T t“1 ⇡trsts, which is calculated by
specifying an initial hidden-state h0 “ z and feeding x “ ps1, . . .",Variational Autoencoder,[0],[0]
", sT q into D .",Variational Autoencoder,[0],[0]
"From a given latent configuration z, our revisions are produced by decoding a sequence via the most-likely observation, which we denote as the map:
Dpzq “ argmax",Variational Autoencoder,[0],[0]
xPX,Variational Autoencoder,[0],[0]
"p D px | zq (5)
",Variational Autoencoder,[0],[0]
"While the most-likely decoding in (5) is itself a combinatorial problem, beam search can exploit the sequentialfactorization of ppx | zq to efficiently find a good approximate solution (Wiseman & Rush, 2016; Sutskever et al., 2014).",Variational Autoencoder,[0],[0]
"For x˚ “ Dpzq P X , this decoding strategy seeks to ensure neither p
X
px˚q nor ppz | x˚q is too small.",Variational Autoencoder,[0],[0]
"In addition to the VAE component, we fit a compositional outcome-prediction model which uses a standard feed forward neural network F to implement the map F : Rd Ñ R. It is assumed that F pzq “ ErY | Z “ zs under our generative model.",Compositional Prediction of Outcomes,[0],[0]
"Rather than integrating over Z to compute ErY | X “ xs “ ≥ F pzqq
E pz | xqdz, we employ the first-order Taylor approximation F pEpxqq, where the approximation-error shrinks the more closely F resembles an affine transformation.",Compositional Prediction of Outcomes,[0],[0]
"To ensure this approximateinference step accurately estimates the conditional expectation, we jointly train E and F with the loss:
Lmsepx, yq “ ry ´ F pEpxqqs2 (6)
",Compositional Prediction of Outcomes,[0],[0]
"If the architecture of networks E ,F is specified with sufficient capacity to capture the underlying conditional relationship, then we should have F pEpxqq « ErY | X “ xs after properly learning the network parameters from a sufficiently large dataset (even F is a nonlinear map).
",Compositional Prediction of Outcomes,[0],[0]
"Enforcing Invariance
In theory, it is possible that some dimensions of z pertain solely to the outcome y and do not have any effect on the decoded sequence Dpzq.",Compositional Prediction of Outcomes,[0],[0]
"Happening to learn this sort of latent representation would be troubling, since subsequent optimization of the inferred y with respect to z might not actually lead to a superior revised sequence.",Compositional Prediction of Outcomes,[0],[0]
"To mitigate this issue, we carefully ensure the dimensionality d of our latent Z does not significantly exceed the bottleneck capacity needed to produce accurate outcome-predictions and VAE reconstructions (Gupta et al., 2016).",Compositional Prediction of Outcomes,[0],[0]
"We explicitly suppress this undesirable scenario by adding the following loss to guide training of our neural networks:
Linv “ Ez„p Z
“ F pzq ´ F pEpDpzqqq ‰2",Compositional Prediction of Outcomes,[0],[0]
"(7)
When optimizing neural network parameters with respect to this loss, we treat the parameters of D and the lefthand F pzq term as fixed, solely backpropagating Monte-Carlo estimated gradients into E ,F .",Compositional Prediction of Outcomes,[0],[0]
Driving Linv toward 0 ensures our outcome-predictions remain invariant to variation introduced by the encoding-decoding process (and this term also serves as a practical regularizer to enforce additional smoothness in our learned functions).,Compositional Prediction of Outcomes,[0],[0]
"The parameters of all components of this model (q E , p D , and F ) are learned jointly in an end-to-end fashion.",Joint Training,[0],[0]
"Training is done via stochastic gradient descent applied to minimize the following objective over the examples in D
n
:
Lpx, yq",Joint Training,[0],[0]
"“ Lrec ` priLpri ` mse 2 Y Lmse ` inv 2 Y Linv (8)
where 2 Y denotes the (empirical) variance of the outcomes, and the • 0 are constants chosen to balance the relative weight of each goal so that the overall framework produces maximally useful revisions.",Joint Training,[0],[0]
"By setting mse “ inv “ 0 at first, we can optionally leverage a separate large corpus of unlabeled examples to initially train only the VAE component of our architecture, as in the unsupervised pretraining strategy used successfully by Kiros et al. (2015); Erhan et al. (2010).
",Joint Training,[0],[0]
"In practice, we found the following training strategy to work well, in which numerous mini-batch stochastic gradient updates (typically 10-30 epochs) are applied within every one of these steps:
Step 1: Begin with inv “ pri “ 0, so Lrec and Lmse are the only training objectives.",Joint Training,[0],[0]
"We found that regardless of the precise value specified for mse, both Lrec and Lmse were often driven to their lowest possible values during this joint optimization (verified by training individually against each objective).
",Joint Training,[0],[0]
"Step 2: Grow pri from 0 to 1 following the sigmoid annealing schedule proposed by Bowman et al. (2016), which is needed to ensure the variational sequence to sequence model does not simply ignore the encodings z",Joint Training,[0],[0]
(note that the formal variational lower bound is attained at pri “ 1).,Joint Training,[0],[0]
"Step 3: Gradually increase inv linearly until Linv becomes small on average across our Monte-Carlo samples z „ p
Z .",Joint Training,[0],[0]
"Here, p
D is treated as constant with respect to Linv, and each mini-batch used in stochastic gradient descent is chosen to contain the same number of Monte-Carlo samples for estimating Linv as (sequence, outcome) pairs.",Joint Training,[0],[0]
"While the aforementioned training procedure is computationally intensive, once learned, our neural networks can be leveraged for efficient inference.",Proposing Revisions,[0],[0]
"Given user-specified constant ↵ ° 0 and a to-be-revised sequence x0, we propose the revision x˚ output by the following procedure.
",Proposing Revisions,[0],[0]
"REVISE Algorithm Input: sequence x0 P X , constant ↵ P p0, |2⇡⌃ z|x0 |´ 1 2 q Output: revised sequence x˚ P X 1)",Proposing Revisions,[0],[0]
"Use E to compute q
E pz | x0q 2)",Proposing Revisions,[0],[0]
"Define C
x0 “ z P Rd :",Proposing Revisions,[0],[0]
"q E pz | x0q • ↵ (
3) Find z˚ “ argmax zPC
x0
F pzq (gradient ascent)
4) Return x˚ “ Dpz˚q (beam search)
",Proposing Revisions,[0],[0]
"Intuitively, the level-set constraint C x0 Ñ Rd ensures that z˚, the latent configuration from which we decode x˚, is likely similar to the latent characteristics responsible for the generation of x0.",Proposing Revisions,[0],[0]
Assuming x0 and x˚ share similar latent factors implies these sequences are fundamentally similar according to the generative model.,Proposing Revisions,[0],[0]
"Note that z˚ “ Epx0q is always a feasible solution of the latent-factor optimization over z P C
x0 (for any allowed value of ↵).",Proposing Revisions,[0],[0]
"Furthermore, this constrained optimization is easy under our Gaussian approximate-posterior, since C
x0 forms a simple ellipsoid centered around Epx0q.",Proposing Revisions,[0],[0]
"To find z˚ in Step 3 of the REVISE procedure, we use gradient ascent initialized at z “ Epx0q, which can quickly reach a local maximum if F is parameterized by a simple feedforward network.",Proposing Revisions,[0],[0]
"Starting the search at Epx0q makes most sense for unimodal posterior approximations like our Gaussian q
E .",Proposing Revisions,[0],[0]
"To ensure all iterates remain in the feasible region C
x0 , we instead take gradient steps with respect to a penalized objective F pzq ` µ ¨ Jpzq where:
Jpzq “ log ” K ´ pz ´ Epx0qqT ⌃´1 z|x0pz ´ Epx0qq ı
K “ ´2 logrp2⇡qd{2|⌃ z|x|1{2↵s (9)
and 0 † µ !",Proposing Revisions,[0],[0]
"1 is gradually decreased toward 0 to en-
sure the optimization can approach the boundary of C x0 .",Proposing Revisions,[0],[0]
"In terms of resulting revision quality, we found this log barrier method outperformed other standard first-order techniques for constrained optimization such as the projected gradient and Franke-Wolfe algorithms.
",Proposing Revisions,[0],[0]
"In principle, our revision method can operate on the latent representations of a traditional deterministic autoencoder for sequences, such as the seq2seq models of Sutskever et al. (2014) and Cho et al. (2014).",Proposing Revisions,[0],[0]
"However, the VAE offers numerous practical advantages, some of which are highlighted by Bowman et al. (2016) in the context of generating more-coherent sentences.",Proposing Revisions,[0],[0]
The posterior uncertainty of the VAE encourages the network to smoothly spread the training examples across the support of the latent distribution.,Proposing Revisions,[0],[0]
"In contrast, central regions of the latent space under a traditional autoencoder can contain holes (to which no examples are mapped), and it is not straightforward to avoid these in our optimization of z˚.",Proposing Revisions,[0],[0]
"Furthermore, we introduce an adaptive variant of our decoder in §S1 which is designed to avoid poor revisions in cases where the initial sequence is already not reconstructed properly: DpEpx0qq ‰ x0.",Proposing Revisions,[0],[0]
"Here, we theoretically characterize properties of revisions obtained via our REVISE procedure (all proofs are relegated to §S3 in the Supplementary Material).",Theoretical Properties of Revision,[0],[0]
"Our results imply that in an ideal setting where our neural network inference approximations are exact, the revisions proposed by our method are guaranteed to satisfy our previously stated desiderata: x˚ is associated with an expected outcome-increase, x˚ appears natural (has nontrivial probability under p
X whenever x0 is a natural sequence), and x˚ is likely to share similar latent characteristics as x0 (since x˚ is the most likely observation generated from z˚ and q
E pz˚ | x0q • ↵ by design).",Theoretical Properties of Revision,[0],[0]
"Although exact approximations are unrealistic in practice, our theory precisely quantifies the expected degradation in the quality of proposed revisions that accompanies a decline in either the accuracy of our approximate inference techniques or the marginal likelihood of the original sequence to revise.
",Theoretical Properties of Revision,[0],[0]
"Theorems 1 and 2 below ensure that for an initial sequence x0 drawn from the natural distribution, the likelihood of the revised sequence x˚ output by our REVISE procedure under p
X has lower bound determined by the user-parameter ↵ and the probability of the original sequence p
X px0q.",Theoretical Properties of Revision,[0],[0]
"Thus, when revising a sequence x0 which looks natural (has substantial probability under p
X ), our procedure is highly likely to produce a revised sequence x˚ which also looks natural.",Theoretical Properties of Revision,[0],[0]
"The strength of this guarantee can be precisely controlled by choosing ↵ appropriately large in applications where this property is critical.
",Theoretical Properties of Revision,[0],[0]
"In each high probability statement, our bounds assume the
initial to-be-revised sequence x0 stems from the natural distribution p
X , and each result holds for any fixed constant ° 0.",Theoretical Properties of Revision,[0],[0]
"We first introduce the following assumptions: (A1) For ° 0,↵ ° 0, there exists 0 † § 1 such that:
i. With probability • 1 ´ {2 (over x „ p X ):
ppz | xq • ¨ q E pz | xq",Theoretical Properties of Revision,[0],[0]
"whenever q E pz | xq • ↵
ii.",Theoretical Properties of Revision,[0],[0]
"PrpZ R B R{2p0qq • ¨ Prp rZ R BR{2p0qq
where Z „ Np0, Iq, and rZ „ q Z
, the average encoding distribution defined by Hoffman & Johnson (2016) as:
q Z pzq",Theoretical Properties of Revision,[0],[0]
"“ E x„p
X
rq E pz | xqs (10) B
R p0q “ tz P Rd : ||z|| § Ru denotes the Euclidean ball centered around 0 with radius R defined here as: R “ maxtR1, R2u (11) with R1 “ a ´8 logr↵ ¨ p2⇡qd{2s
R2 “ maxt rR2, 2u, rR2 “ c 8 ´ 14d log ´ 8 ¯
(A2)",Theoretical Properties of Revision,[0],[0]
There exists ⌘ ° 0 (depends on ) such that with probability • 1 ´ {2 (over x0 „ pX ): ppz˚ | x˚q,Theoretical Properties of Revision,[0],[0]
§ ⌘,Theoretical Properties of Revision,[0],[0]
"This means the latent posterior is bounded at x˚, z˚ (as defined in REVISE), where both depend upon the initial tobe-revised sequence x0.",Theoretical Properties of Revision,[0],[0]
Theorem 1.,Theoretical Properties of Revision,[0],[0]
"For any ° 0, (A1) and (A2) imply:
p X px˚q",Theoretical Properties of Revision,[0],[0]
"• ↵ ⌘ ¨ p X
px0q with probability • 1 ´ (over x0 „ pX ).
",Theoretical Properties of Revision,[0],[0]
"Condition (A1) forms a generalization of absolute continuity, and is required since little can be guaranteed about our inference procedures if the variational posterior is too inaccurate.",Theoretical Properties of Revision,[0],[0]
"Equality holds in (A1) with probability 1 if the variational distributions q
E exactly represent the true posterior ( Ñ 1 as the variational approximations become more accurate over the measure p
X ).",Theoretical Properties of Revision,[0],[0]
"In practice, minimization of the reverse KL divergence (Lpri) used in our VAE formulation ensures that q
E pz",Theoretical Properties of Revision,[0],[0]
"| xq is small wherever the true posterior ppz | xq takes small values (Blei et al., 2017).",Theoretical Properties of Revision,[0],[0]
"While the bound in Theorem 1 has particularly simple form, this result hinges on assumption (A2).",Theoretical Properties of Revision,[0],[0]
One can show for example that the inequality in (A2) is satisfied if the posteriors ppz,Theoretical Properties of Revision,[0],[0]
| x˚q are Lipschitz continuous functions of z at z˚ (sharing one Lipschitz constant over all possible x˚).,Theoretical Properties of Revision,[0],[0]
"In general however, (A2) heavily depends on both the data distribution p
X and decoder model p D .",Theoretical Properties of Revision,[0],[0]
"Therefore, we provide a similar lower bound guarantee on the likelihood of our revision x˚ under p
X , which instead only relies on weaker assumption (A3) below.
",Theoretical Properties of Revision,[0],[0]
"(A3) There exists L ° 0 such that for each x P X : p D px | zq is a L-Lipschitz function of z over B R`1p0q.
",Theoretical Properties of Revision,[0],[0]
"Here, L depends on (through R), and we assume L • 1 without loss of generality.",Theoretical Properties of Revision,[0],[0]
(A3) is guaranteed to hold in the setting where we only consider sequences of finite length § T .,Theoretical Properties of Revision,[0],[0]
"This is because the probability output by our decoder model, p
D px | zq, is differentiable with bounded gradients over all z P B
R p0q under any sequence-to-sequence RNN architecture which can be properly trained using gradient methods.",Theoretical Properties of Revision,[0],[0]
"Since B
R`1p0q Ä Rd is a closed interval, p D
px | zq must be Lipschitz continuous over this set, for a given value of x.",Theoretical Properties of Revision,[0],[0]
We can simply define L to be the largest Lipschitz constant over the |S|T possible choices of x P X (|S| “ size of the vocabulary).,Theoretical Properties of Revision,[0],[0]
"In the next theorem below, user-specified constant ↵ ° 0 is defined in REVISE, and L, , R all depend on .",Theoretical Properties of Revision,[0],[0]
Theorem 2.,Theoretical Properties of Revision,[0],[0]
"For any ° 0, if (A1) and (A3) hold, then with probability • 1 ´ (over x0 „ pX ):
p X
px˚q •",Theoretical Properties of Revision,[0],[0]
"Ce ´R
Ld ¨
“ ¨ ↵ ¨ p
X px0q ‰ d`1
where constant C “ ⇡ d{2 pd2 ` 1q ¨ pd ` 1q d pd ` 2qd`1
Our final result, Theorem 3, ensures that our optimization of z˚ with respect to F is tied to the expected outcomes at x˚ “ Dpz˚q, so that large improvements in the optimization objective: F pz˚q ´ F pEpx0qq imply that our revision procedure likely produces large expected improvements in the outcome: ErY | X “ x˚s ´ ErY | X “ x0s.",Theoretical Properties of Revision,[0],[0]
"For this result, we make the following assumptions:
(A4) For any ° 0, there exists  ° 0",Theoretical Properties of Revision,[0],[0]
"such that PrpX P Kq • 1 ´ {2, where we define:
K “ tx P X : x0 “ x ùñ pXpx˚q • u (12)
as the subset of sequences whose improved versions produced by our REVISE procedure remain natural with likelihood • .",Theoretical Properties of Revision,[0],[0]
"Note that either Theorem 1 or 2 (with the corresponding assumptions) ensures that one can suitably define  such that (A4) is satisfied (by considering a sufficiently large finite subset of X ).
",Theoretical Properties of Revision,[0],[0]
(A5),Theoretical Properties of Revision,[0],[0]
"For any  ° 0, there exists ✏mse ° 0 such that PrpX P Emseq",Theoretical Properties of Revision,[0],[0]
"° 1 ´ , where we define:
Emse“ tx P X : |F",Theoretical Properties of Revision,[0],[0]
"pEpxqq ´ ErY |X “ xs| § ✏mseu (13)
(A6) For any ° 0, there exists ✏inv ° 0 such that:
|F pzq ´ F pEpDpzqqq| § ✏inv for all z P BRp0q",Theoretical Properties of Revision,[0],[0]
"Ä Rd
where R is defined in (11) and depends on .
",Theoretical Properties of Revision,[0],[0]
"Here, ✏mse and ✏inv quantify the approximation error of our neural networks for predicting expected outcomes and ensuring encoding-decoding invariance with respect to F .
",Theoretical Properties of Revision,[0],[0]
"Standard learning theory implies both ✏mse, ✏inv will be driven toward 0 if we use neural networks with sufficient capacity to substantially reduce Lmse and Linv over a large training set.",Theoretical Properties of Revision,[0],[0]
Theorem 3.,Theoretical Properties of Revision,[0],[0]
"For any ° 0, if conditions (A1), (A4), (A5), and (A6) hold, then with probability • 1 ´ ´ :
z ˚ ´ ✏ § F pz˚q ´ F pEpx0qq § z ˚ ` ✏ (14)
where
z ˚ “ ErY | X “ x˚s ´ ErY | X “ x0s ✏ “ ✏
inv ` 2✏ mse
Here, , ✏inv are defined in terms of as specified in (A4), (A6), and ✏mse is defined in terms of  as specified in (A5).",Theoretical Properties of Revision,[0],[0]
"All of our RNNs employ the Gated Recurrent Unit (GRU) of Cho et al. (2014), which contains a simple gating mechanism to effectively learn long-range dependencies across a sequence.",Experiments,[0],[0]
"Throughout, F is a simple feedforward network with 1 hidden layer and tanh activations (note that the popular ReLU activation is inappropriate for F since it has zero gradient over half its domain).",Experiments,[0],[0]
"Decoding with respect to p
D is simply done entirely greedily (ie.",Experiments,[0],[0]
a beam-search of size 1) to demonstrate our approach is not reliant on search heuristics.,Experiments,[0],[0]
§S2 contains additional details for each analysis.,Experiments,[0],[0]
"To study our methods in a setting where all aspects of performance can be quantified, we construct a natural distribution p
X over sequences of lengths 10-20 whose elements stem from the vocabulary S “ tA,B, . . .",Simulation Study,[0],[0]
", I, Ju.",Simulation Study,[0],[0]
Each sequence is generated via the probabilistic grammar of Table S1.,Simulation Study,[0],[0]
"For each sequence, the associated outcome y is simply the number of times A appears in the sequence (a completely deterministic relationship).",Simulation Study,[0],[0]
"Since A often follows C and is almost always followed by B under p
X , a procedure to generate natural revisions cannot simply insert/substitute A symbols at random positions.
",Simulation Study,[0],[0]
Table 1 compares various methods for proposing revisions.,Simulation Study,[0],[0]
"Letting
Y denote the standard deviation of outcomes in D
n , we evaluate each proposed x˚ using a rescaled version of the actual underlying outcome-improvement:
Y px˚q “ ´1 Y pErY",Simulation Study,[0],[0]
| X “ x˚s ´ ErY | X “ x0sq.,Simulation Study,[0],[0]
"Except where sample size is explicitly listed, all models were trained using n “ 10, 000 (sequence, outcome) pairs sampled from the generative grammar.",Simulation Study,[0],[0]
"Wherever appropriate, the different methods all make use of the same neural network components with latent dimension d “ 128.",Simulation Study,[0],[0]
"Other than ↵, all hyperparameters of each revision method described below were chosen so that over 1000 revisions, the Levenshtein (edit) distance dpx˚, x0q « 3.3 on average.
",Simulation Study,[0],[0]
"All three results above the line in Table 1 are based on the full model described in our joint training procedure, with new sequences proposed via our REVISE algorithm (using the setting log↵ “ ´10000).",Simulation Study,[0],[0]
"In the latter two results, this model was only trained on a smaller subset of the data.",Simulation Study,[0],[0]
We also generated revisions via this same procedure with the more conservative choice log↵ “ ´1.,Simulation Study,[0],[0]
"ADAPTIVE denotes the same approach (with log↵ “ ´10000), this time using the adaptive decoding D
x0 introduced in §S1, which is intended to slightly bias revisions toward x0.",Simulation Study,[0],[0]
The model with inv “ pri “ 0 is a similar method using a deterministic sequence-to-sequence autoencoder rather than our probabilistic VAE formulation (no variational posterior approximation or invariance-enforcing) where the latent encodings are still jointly trained to predict outcomes via F .,Simulation Study,[0],[0]
"Under this model, a revision is proposed by starting at Epx0q in the latent space, taking 1000 (unconstrained) gradient steps with respect to F , and finally applying D to the resulting z.
The above methods form an ablation study of the various components in our framework.",Simulation Study,[0],[0]
"SEARCH is a different combinatorial approach where we randomly generate 100 revisions by performing 4 random edits in x0 (each individual edit is randomly selected as one of: substitution, insertion, deletion, or no change).",Simulation Study,[0],[0]
"In this approach, we separately learn a language-model RNN L on our training sequences (Mikolov et al., 2010).",Simulation Study,[0],[0]
"Sharing the same GRU architecture as our decoder model, L directly estimates the likelihood of any given sequence under p
X .",Simulation Study,[0],[0]
"Of the randomly generated revisions, we only retain those sequences x for which Lpxq • 1|S|Lpx0q (in this case, those which are not estimated to be † 10 times less likely than the original sequence x0 under pX ).",Simulation Study,[0],[0]
"Finally, we score each remaining candidate (including x0) using the outcome-prediction model F pEpxqq, and the best is chosen as x˚. Table 1 shows that our probabilistic VAE formulation outperforms the alternative approaches, both in terms of outcome-improvement achieved as well as ensuring revi-
sions follow p X .",Simulation Study,[0],[0]
"For comparison, ´ log p X px0q had an average value of 26.8 (over these 1000 starting sequences), and changing one randomly-selected symbol in each sequence to A results in an average negative log-probability of 32.8.",Simulation Study,[0],[0]
"Thus, all of our revision methods clearly account for p
X to some degree.",Simulation Study,[0],[0]
We find that all components used in our REVISION procedure are useful in achieving superior revisions.,Simulation Study,[0],[0]
"While individual standard deviations seem large, nearly all average differences in
Y or ´ log p X values produced by different methods are statistically significant considering they are over 1000 revisions.
",Simulation Study,[0],[0]
"From Supplementary Figure S1, it is clear that ↵ controls how conservative the changes proposed by our REVISE procedure tend to be, in terms of both ´ log p
X px˚q and the edit distance dpx0, x˚q.",Simulation Study,[0],[0]
"The red curve in Figure S1A suggests that our theoretical lower bounds for p
X px˚q are overly stringent in practice (although only the averagecase is depicted in the figure).",Simulation Study,[0],[0]
"The relationship between log p
X px0q and log pXpx˚q (see Figure S1B) is best-fit by a line of slope 1.2, indicating that the linear dependence on p
X px0q in the Theorem 1 bound for pXpx˚q is reasonably accurate.",Simulation Study,[0],[0]
Figure S1C shows that the magnitude of changes in the latent space (arising from z-optimization during our REVISE procedure) only exhibits a weak correlation with the edit distance between the resulting revision and the original sequence.,Simulation Study,[0],[0]
This implies that a fixed shift in different directions in the latent space can produce drastically different degrees of change in the sequence space.,Simulation Study,[0],[0]
"To ensure a high-quality revision, it is thus crucial to carefully treat the (variational) posterior landscape when performing manipulations of Z.",Simulation Study,[0],[0]
"Next, we apply our model to „1M reviews from BeerAdvocate (McAuley et al., 2012).",Improving Sentence Positivity,[0],[0]
"Each beer review is parsed into separate sentences, and each sentence is treated as an individual sequence of words.",Improving Sentence Positivity,[0],[0]
"In order to evaluate methods using an outcome that can be obtained for any proposed revision, we choose y P r0, 1s as the VADER sentiment compound score of a given sentence (Hutto & Gilbert,
2014).",Improving Sentence Positivity,[0],[0]
"VADER is a complex rule-based sentiment analysis tool which jointly estimates polarity and intensity of English text, and larger VADER scores correspond to text that humans find more positive with high fidelity.
",Improving Sentence Positivity,[0],[0]
We applied all aforementioned approaches to produce revisions for a held-out set of 1000 test sentences.,Improving Sentence Positivity,[0],[0]
"As p
X
underlying these sentences is unknown, we report estimates thereof obtained from a RNN language-model L learned on the sentences in D
n .",Improving Sentence Positivity,[0],[0]
Table 2 demonstrates that our VAE approach achieves the greatest outcome-improvement.,Improving Sentence Positivity,[0],[0]
"Moreover, Tables 3 and S2 show that our probabilisticallyconstrained VAE revision approach produces much more coherent sentences than the other strategies.",Improving Sentence Positivity,[0],[0]
"For our final application, we assemble a dataset of „100K short sentences which are either from Shakespeare or a more contemporary source (details in §S2.3).",Revising Modern Text in the Language of Shakespeare,[0],[0]
"In this training data, each sentence is labeled with outcome y “ 0.9
if it was authored by Shakespeare and y “ 0.1 otherwise (these values are chosen to avoid the flat region of the sigmoid output layer used in network F ).",Revising Modern Text in the Language of Shakespeare,[0],[0]
"When applied in this domain, our REVISE procedure thus attempts to alter a sentence so that the author is increasingly expected to be Shakespeare rather than a more contemporary source.
",Revising Modern Text in the Language of Shakespeare,[0],[0]
"Tables 4 and S3 show revisions (of held-out sentences) proposed by our REVISE procedure with adaptive decoding (see §S1), together with sentences generated by applying the adaptive decoder at various points along an unconstrained gradient-ascent path in latent Z space (following gradients of F ).",Revising Modern Text in the Language of Shakespeare,[0],[0]
"Since the data lack similar versions of a sentence written in both contemporary and Shakespearean language, this revision task is an ambitious application of our ideas.",Revising Modern Text in the Language of Shakespeare,[0],[0]
"Without observing a continuous spectrum of outcomes or leveraging specially-designed style transfer features (Gatys et al., 2016), our REVISE procedure has to alter the underlying semantics in order to nontrivially increase the expected outcome of the revised sentence under F .",Revising Modern Text in the Language of Shakespeare,[0],[0]
"Nevertheless, we find that many of the revised sentences look realistic and resemble text written by Shakespeare.",Revising Modern Text in the Language of Shakespeare,[0],[0]
"Furthermore, these examples demonstrate how the probabilistic constraint in our REVISE optimization prevents the revision-generating latent Z configurations from straying into regions where decodings begin to look very unnatural.",Revising Modern Text in the Language of Shakespeare,[0],[0]
This paper presents an efficient method for optimizing discrete sequences when both the objective and constraints are stochastically estimated.,Discussion,[0],[0]
"Leveraging a latent-variable generative model, our procedure does not require any examples of revisions in order to propose natural-looking sequences with improved outcomes.",Discussion,[0],[0]
These characteristics are proven to hold with high probability in a theoretical analysis of VAE behavior under our controlled latent-variable manipulations.,Discussion,[0],[0]
"However, ensuring semantic similarity in textrevisions remains difficult for this approach, and might be improved via superior VAE models or utilizing additional similarity labels to shape the latent geometry.",Discussion,[0],[0]
"D., Bougares, F., Schwenk, H., and Bengio, Y. Learning phrase representations using rnn encoder-decoder for statistical machine translation.","Cho, K., van Merrienboer, B., Gulcehre, C., Bahdanau,",[0],[0]
"Empirical Methods on Natural Language Processing, 2014.
","Cho, K., van Merrienboer, B., Gulcehre, C., Bahdanau,",[0],[0]
"Eck, D. and Schmidhuber, J. A first look at music composition using lstm recurrent neural networks.","Cho, K., van Merrienboer, B., Gulcehre, C., Bahdanau,",[0],[0]
"IDSIA Technical Report, 2002.
","Cho, K., van Merrienboer, B., Gulcehre, C., Bahdanau,",[0],[0]
"Erhan, D., Bengio, Y., Courville, A., Manzagol, P., Vincent, P., and Bengio, S. Why does unsupervised pretraining help deep learning?","Cho, K., van Merrienboer, B., Gulcehre, C., Bahdanau,",[0],[0]
"Journal of Machine Learning Research, 11:625–660, 2010.","Cho, K., van Merrienboer, B., Gulcehre, C., Bahdanau,",[0],[0]
transfer using convolutional neural networks.,"Gatys, L. A., Ecker, A. S., and Bethge, M. Image style",[0],[0]
"Computer Vision and Pattern Recognition, 2016.","Gatys, L. A., Ecker, A. S., and Bethge, M. Image style",[0],[0]
"J. M., Aguilera-Iparraguirre, J., , Hirzel, T., Adams, R. P., and Aspuru-Guzik, A. Automatic chemical design using a data-driven continuous representation of molecules.","Gómez-Bombarelli, R., Duvenaud, D., Hernández-Lobato,",[0],[0]
"arXiv:1610.02415, 2016.
","Gómez-Bombarelli, R., Duvenaud, D., Hernández-Lobato,",[0],[0]
"Graves, A. Generating sequences with recurrent neural networks.","Gómez-Bombarelli, R., Duvenaud, D., Hernández-Lobato,",[0],[0]
"arXiv:1308.0850, 2013.
","Gómez-Bombarelli, R., Duvenaud, D., Hernández-Lobato,",[0],[0]
"Gupta, P., Banchs, R. E., and Rosso, P. Squeezing bottlenecks: Exploring the limits of autoencoder semantic representation capabilities.","Gómez-Bombarelli, R., Duvenaud, D., Hernández-Lobato,",[0],[0]
"Neurocomputing, 175:1001– 1008, 2016.
","Gómez-Bombarelli, R., Duvenaud, D., Hernández-Lobato,",[0],[0]
"Higgins, I., Matthey, L., Glorot, X., Pal, A., Uria, B., Blundell, C., Mohamed, S., and Lerchner, A. Early visual concept learning with unsupervised deep learning.","Gómez-Bombarelli, R., Duvenaud, D., Hernández-Lobato,",[0],[0]
"arXiv:1606.05579, 2016.","Gómez-Bombarelli, R., Duvenaud, D., Hernández-Lobato,",[0],[0]
another way to carve up the variational evidence lower bound.,"Hoffman, M. D. and Johnson, M. J. Elbo surgery: yet",[0],[0]
"NIPS Workshop on Advances in Approximate Bayesian Inference, 2016.
","Hoffman, M. D. and Johnson, M. J. Elbo surgery: yet",[0],[0]
"Hutto, C.J. and Gilbert, E. Vader: A parsimonious rulebased model for sentiment analysis of social media text.","Hoffman, M. D. and Johnson, M. J. Elbo surgery: yet",[0],[0]
"Eighth International Conference on Weblogs and Social
Media, 2014.
","Hoffman, M. D. and Johnson, M. J. Elbo surgery: yet",[0],[0]
"Karpathy, A.","Hoffman, M. D. and Johnson, M. J. Elbo surgery: yet",[0],[0]
The unreasonable effectiveness of recurrent neural networks.,"Hoffman, M. D. and Johnson, M. J. Elbo surgery: yet",[0],[0]
"Andrej Karpathy blog, 2015.","Hoffman, M. D. and Johnson, M. J. Elbo surgery: yet",[0],[0]
URL karpathy.github.io.,"Hoffman, M. D. and Johnson, M. J. Elbo surgery: yet",[0],[0]
bayes.,"Kingma, D. P. and Welling, M. Auto-encoding variational",[0],[0]
"International Conference on Learning Representations, 2014.
","Kingma, D. P. and Welling, M. Auto-encoding variational",[0],[0]
"Kiros, R., Zhu, Y., Salakhutdinov, R., Zemel, R. S., Torralba, A., Urtasun, R., and Fidler, S. Skip-thought vectors.","Kingma, D. P. and Welling, M. Auto-encoding variational",[0],[0]
"Advances in Neural Information Processing Systems, 2015.
","Kingma, D. P. and Welling, M. Auto-encoding variational",[0],[0]
"McAuley, J., Leskovec, J., and Jurafsky, D. Learning attitudes and attributes from multi-aspect reviews.","Kingma, D. P. and Welling, M. Auto-encoding variational",[0],[0]
"IEEE International Conference on Data Mining, 2012.","Kingma, D. P. and Welling, M. Auto-encoding variational",[0],[0]
"Khudanpur, S. Recurrent neural network based language model.","Mikolov, T., Karafiat, M., Burget, L., Cernocky, J., and",[0],[0]
"Interspeech, 2010.
","Mikolov, T., Karafiat, M., Burget, L., Cernocky, J., and",[0],[0]
"Mueller, J. and Thyagarajan, A. Siamese recurrent architectures for learning sentence similarity.","Mikolov, T., Karafiat, M., Burget, L., Cernocky, J., and",[0],[0]
Proc.,"Mikolov, T., Karafiat, M., Burget, L., Cernocky, J., and",[0],[0]
"AAAI Conference on Artificial Intelligence, 2016.
","Mikolov, T., Karafiat, M., Burget, L., Cernocky, J., and",[0],[0]
"Mueller, J., Reshef, D. N., Du, G., and Jaakkola, T. Learning optimal interventions.","Mikolov, T., Karafiat, M., Burget, L., Cernocky, J., and",[0],[0]
"Artificial Intelligence and Statistics, 2017.
","Mikolov, T., Karafiat, M., Burget, L., Cernocky, J., and",[0],[0]
"Nguyen, A., Yosinski, J., and Clune, J. Deep neural networks are easily fooled: High confidence predictions for unrecognizable images.","Mikolov, T., Karafiat, M., Burget, L., Cernocky, J., and",[0],[0]
"Computer Vision and Pattern Recognition, 2015.","Mikolov, T., Karafiat, M., Burget, L., Cernocky, J., and",[0],[0]
"Clune, J. Synthesizing the preferred inputs for neurons in neural networks via deep generator networks.","Nguyen, A., Dosovitskiy, A., Yosinski, J., Brox, T., and",[0],[0]
"Advances in Neural Information Processing Systems, 2016.","Nguyen, A., Dosovitskiy, A., Yosinski, J., Brox, T., and",[0],[0]
convolutional networks: Visualising image classification models and saliency maps.,"Simonyan, K., Vedaldi, A., and Zisserman, A. Deep inside",[0],[0]
"ICLR Workshop Proceedings, 2014.
","Simonyan, K., Vedaldi, A., and Zisserman, A. Deep inside",[0],[0]
"Sutskever, I., Vinyals, O., and Le, Q.V. Sequence to sequence learning with neural networks.","Simonyan, K., Vedaldi, A., and Zisserman, A. Deep inside",[0],[0]
"Advances in Neural Information Processing Systems, 2014.
","Simonyan, K., Vedaldi, A., and Zisserman, A. Deep inside",[0],[0]
"Wiseman, S. and Rush, A. M. Sequence-to-sequence learning as beam-search optimization.","Simonyan, K., Vedaldi, A., and Zisserman, A. Deep inside",[0],[0]
"Empirical Methods in Natural Language Processing, 2016.","Simonyan, K., Vedaldi, A., and Zisserman, A. Deep inside",[0],[0]
"B., and Bartz-Beielstein, T. Efficient global optimization for combinatorial problems.","Zaefferer, M., Stork, J., Friese, M., Fischbach, A., Naujoks,",[0],[0]
"Genetic and Evolutionary Computation Conference, 2014.","Zaefferer, M., Stork, J., Friese, M., Fischbach, A., Naujoks,",[0],[0]
"We present a model that, after learning on observations of (sequence, outcome) pairs, can be efficiently used to revise a new sequence in order to improve its associated outcome.",abstractText,[0],[0]
"Our framework requires neither example improvements, nor additional evaluation of outcomes for proposed revisions.",abstractText,[0],[0]
"To avoid combinatorial-search over sequence elements, we specify a generative model with continuous latent factors, which is learned via joint approximate inference using a recurrent variational autoencoder (VAE) and an outcome-predicting neural network module.",abstractText,[0],[0]
"Under this model, gradient methods can be used to efficiently optimize the continuous latent factors with respect to inferred outcomes.",abstractText,[0],[0]
"By appropriately constraining this optimization and using the VAE decoder to generate a revised sequence, we ensure the revision is fundamentally similar to the original sequence, is associated with better outcomes, and looks natural.",abstractText,[0],[0]
"These desiderata are proven to hold with high probability under our approach, which is empirically demonstrated for revising natural language sentences.",abstractText,[0],[0]
"Introduction The success of recurrent neural network (RNN) models in complex tasks like machine translation and audio synthesis has inspired immense interest in learning from sequence data (Eck & Schmidhuber, 2002; Graves, 2013; Sutskever et al., 2014; Karpathy, 2015).",abstractText,[0],[0]
"Comprised of elements s t P S , which are typically symbols from a discrete vocabulary, a sequence x “ ps1, . . .",abstractText,[0],[0]
", sT q P X has length T which can vary between different instances.",abstractText,[0],[0]
"Sentences are a popular example of such data, where each s j is a word from the language.",abstractText,[0],[0]
"In many domains, only a tiny fraction of X (the set of possible sequences over a given vocabulary) represents sequences likely to be found in nature (ie.",abstractText,[0],[0]
MIT Computer Science & Artificial Intelligence Laboratory.,abstractText,[0],[0]
Correspondence to: J. Mueller <jonasmueller@csail.mit.edu>.,abstractText,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",abstractText,[0],[0]
Copyright 2017 by the author(s).,abstractText,[0],[0]
those which appear realistic),abstractText,[0],[0]
.,abstractText,[0],[0]
"For example: a random sequence of words will almost never form a coherent sentence that reads naturally, and a random amino-acid sequence is highly unlikely to specify a biologically active protein.",abstractText,[0],[0]
"In this work, we consider applications where each sequence x is associated with a corresponding outcome y P R. For example: a news article title or Twitter post can be associated with the number of shares it subsequently received online, or the amino-acid sequence of a synthetic protein can be associated with its clinical efficacy.",abstractText,[0],[0]
"We operate under the standard supervised learning setting, assuming availability of a dataset D",abstractText,[0],[0]
Sequence to Better Sequence: Continuous Revision of Combinatorial Structures,title,[0],[0]
"The approach of training sequence generation models using likelihood maximization suffers from known failure modes, and it is notoriously difficult to ensure multi-step generated sequences have coherent global structure.",1. Introduction,[0],[0]
"For example, long short-term memory (LSTM) (Hochreiter & Schmidhuber, 1997) networks trained to predict the next character in sequences of text may produce text that has correct
1Google Brain, Mountain View, USA 2Massachusetts Institute of Technology, Cambridge, USA 3University of Cambridge, Cambridge, UK 4Max Planck Institute for Intelligent Systems, Stuttgart, Germany 5Université de Montréal, Montréal, Canada.",1. Introduction,[0],[0]
"Correspondence to: Natasha Jaques <jaquesn@mit.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"spelling, punctuation, and even a semblance of grammar, but the generated text shifts so rapidly from topic to topic, that it is almost completely nonsensical (see (Graves, 2013) for an example).",1. Introduction,[0],[0]
"Similar networks trained to predict the next note in a melody suffer from the same problem; the generated music has no consistent theme or structure, and appears wandering and random.",1. Introduction,[0],[0]
"In addition, these models are prone to excessively repeating the same output token, a problem that has also been noted in the context of recurrent dialog generation models (Li et al., 2016).
",1. Introduction,[0],[0]
"To ameliorate these problems we propose Sequence Tutor, a novel approach which uses RL to impose structure on a sequence generation RNN via task-specific rewards, while simultaneously ensuring that information learned from data is retained.",1. Introduction,[0],[0]
"This is accomplished by maintaining a fixed copy of a sequence generation RNN pre-trained on data, which is termed the Reward RNN.",1. Introduction,[0],[0]
"Rather than simply using the Reward RNN to supply part of the rewards to our model, we derive novel off-policy RL methods for sequence generation from KL-control that allow us to directly penalize Kullback Leibler (KL) divergence from the policy defined by the Reward RNN.",1. Introduction,[0],[0]
As a byproduct of minimizing KL our objective includes an entropy regularization term that encourages high entropy in the distribution of the RL model.,1. Introduction,[0],[0]
"This is ideal for sequence generation tasks such as text, music, or molecule generation, in which maintaining diversity in the samples generated by the model is critical.
",1. Introduction,[0],[0]
"Sequence Tutor effectively combines both data and taskrelated goals, without relying on either as a perfect metric of task success.",1. Introduction,[0],[0]
This is an important novel direction of research.,1. Introduction,[0],[0]
"Much previous work on combining RL and MLE has used MLE training simply as a way to bootstrap the training of an RL model (Ranzato et al., 2015; Bahdanau et al., 2016; Li et al., 2016), since training with RL from scratch is difficult.",1. Introduction,[0],[0]
"However, this approach does not encourage diversity of the generated samples, and can be problematic when task-specific rewards are incomplete or imperfect.",1. Introduction,[0],[0]
"Designing an appropriate reward definition is highly non-trivial, and often the hand-crafted rewards cannot be fully trusted (Vedantam et al., 2015; Liu et al., 2016).",1. Introduction,[0],[0]
"And yet, relying on data alone can be insufficient when the data itself contains biases, as has been shown for text data
ar X
iv :1
61 1.
02 79
6v 9
[ cs
.L G
] 1
6 O
ct 2
01 7
(Caliskan-Islam et al., 2016), or when domain-specific constraints cannot be encoded directly into MLE training.",1. Introduction,[0],[0]
"By learning a policy that trades off staying close to the data distribution while improving performance on specific metrics, Sequence Tutor reduces both of these problems.
",1. Introduction,[0],[0]
"This paper contributes to the sequence training and RL literature by a) proposing a novel method for combining MLE and RL training; b) showing the connection between KL control and sequence generation; c) deriving the explicit relationships among a generalized variant of Ψ-learning (Rawlik et al., 2012), G-learning (Fox et al., 2015), and Q-learning with log prior augmentation, and being the first to empirically compare these methods and use them with deep neural networks.
",1. Introduction,[0],[0]
We explore the usefulness of our approach for two sequence generation applications.,1. Introduction,[0],[0]
"The first, music generation, is a difficult problem in which the aesthetic beauty of generated sequences cannot be fully captured in a known reward function, but in which models trained purely on data cannot produce well-structured sequences.",1. Introduction,[0],[0]
"Through an empirical study, we show that by imposing rules of music theory on a melody generation model, Sequence Tutor is able to produce melodies which are varied, yet more harmonious, interesting, and rated as significantly more subjectively pleasing than those of the MLE model.",1. Introduction,[0],[0]
"Further, Sequence Tutor is able to significantly reduce unwanted behaviors and failure modes of the original RNN.",1. Introduction,[0],[0]
"The effectiveness of Sequence Tutor is also demonstrated for computational molecular generation, a task in which the goal is to generate novel drug-like molecules with desirable properties by outputting a string representation of the molecule encoding.",1. Introduction,[0],[0]
"However, generating valid molecules can prove difficult, as it is hard for probabilistic models to learn all the constraints that define physically realizable molecules directly from data (Gómez-Bombarelli et al., 2016).",1. Introduction,[0],[0]
"We show that Sequence Tutor is able to yield a higher percentage of valid molecules than the baseline MLE RNN, and the generated molecules score higher on metrics of druglikeness and ease of synthesis.",1. Introduction,[0],[0]
Recent work has attempted to use both MLE and RL in the context of structured prediction.,2. Related Work,[0],[0]
"While the attempts were successful, the problems of maintaining information about the data distribution and diversity in the generated samples were not addressed.",2. Related Work,[0],[0]
"MIXER (Mixed Incremental Cross-Entropy Reinforce) (Ranzato et al., 2015) uses BLEU score as a reward signal to gradually introduce a RL loss to a text translation model.",2. Related Work,[0],[0]
"Bahdanau et al. (2016) applies an actor-critic method and uses BLEU score directly to train a critic network to output the value of each word, where the actor is again initialized with the policy of an
RNN trained with next-step prediction.",2. Related Work,[0],[0]
Li et al. (2016) use RL to improve a pre-trained dialog model with heuristic rewards.,2. Related Work,[0],[0]
These approaches assume that the complete task reward specification is available.,2. Related Work,[0],[0]
"They pre-train a good policy with supervised learning so that RL can be used to learn the true task objective, since it can be difficult to reach convergence when training with pure RL.",2. Related Work,[0],[0]
"However, the original MLE policy of these models is overwritten by the RL training process.",2. Related Work,[0],[0]
"In contrast, Sequence Tutor uses rewards to correct certain properties of the generated data, while learning most information from data and maintaining this information; an important ability when the true reward function is not available or imperfect.
",2. Related Work,[0],[0]
"Reward augmented maximum likelihood (RAML) (Norouzi et al., 2016) is an approach designed to improve MLE training of a translation model by augmenting the ground truth targets with additional outputs that are within a small edit distance, and performing MLE training against those as well.",2. Related Work,[0],[0]
"The authors show that their approach is equivalent to minimizing KL-divergence between an RL exponentiated payoff distribution based on edit distance, and the MLE distribution.",2. Related Work,[0],[0]
"In contrast, our goal is generation rather than prediction, and we train an RL rather than MLE model.",2. Related Work,[0],[0]
"The RAML approach, while an important contribution, is only viable if it is possible to generate additional MLE training samples that are similar in terms of the reward function to the ground truth (i.e. samples within a small edit distance).",2. Related Work,[0],[0]
"However in some domains, including the two explored in this paper, generating similar samples with high reward is not only not possible, but in fact constitutes the entire problem under investigation.
",2. Related Work,[0],[0]
"Finally, our approach is related to KL control (Todorov, 2007; Kappen et al., 2012; Rawlik et al., 2012), a branch of stochastic optimal control (SOC) (Stengel, 1986).",2. Related Work,[0],[0]
"There is also a connection between this work and Maximum Entropy Inverse RL (Ziebart et al., 2008), which can be seen as KL control with a flat, improper prior.",2. Related Work,[0],[0]
"From KL control, we take inspiration from two off-policy, model-free methods, Ψ-learning (Rawlik et al., 2012) and G-learning (Fox et al., 2015).",2. Related Work,[0],[0]
"Both approaches are derived from a KLregularized RL objective, where an agent maximizes the reward while incurring additional penalty for divergence from some prior policy.",2. Related Work,[0],[0]
"While our methods rely on similar derivations presented in these papers, our methods have different motivations and forms from the original papers.",2. Related Work,[0],[0]
"The original Ψ-learning (Rawlik et al., 2012) restricts the prior policy to be the policy at the previous iteration and solves the original RL objective with conservative, KLregularized policy updates, similar to conservative policy gradient methods (?Peters et al., 2010; Schulman et al., 2015).",2. Related Work,[0],[0]
"The original G-learning (Fox et al., 2015) penalizes divergence from a simple uniform prior policy in order to cope with over-estimation of target Q values.",2. Related Work,[0],[0]
"These tech-
niques have not been applied to deep learning techniques or with RNNs, or as a way to improve a pre-trained MLE model.",2. Related Work,[0],[0]
"Our work is the first to explore these methods in such a context, and includes a Q-learning model with additional cross-entropy reward as a comparable alternative.",2. Related Work,[0],[0]
"To the best of our knowledge, our work is the first to provide comparisons among these three approaches.
",2. Related Work,[0],[0]
There has also been prior work in the domain of generative modeling of music.,2. Related Work,[0],[0]
"Using RNNs for this purpose has been explored in a variety of contexts, including generating Celtic folk music (Sturm et al., 2016), or improvising the blues (Eck & Schmidhuber, 2002).",2. Related Work,[0],[0]
"Often, this involves training the RNN to predict the next note in a monophonic melody; however, as mentioned above, the melodies generated by this model tend to wander and lack musical structure.",2. Related Work,[0],[0]
"Some authors have experimented with encoding musical structure into a hierarchical RNN with layers dedicated to generated the melody, drums, and chords (Chu et al., 2016).",2. Related Work,[0],[0]
"Other approaches have examined RNNs with richer expressivity, latent-variables for notes, or raw audio synthesis (Boulanger-Lewandowski et al., 2012; Gu et al., 2015; Chung et al., 2015).",2. Related Work,[0],[0]
"Recently, Wavenet produced impressive performance in generating music from raw audio using convolutional neural networks with receptive fields at various time scales (van den Oord et al., 2016).",2. Related Work,[0],[0]
"However, the authors themselves note that “even with a receptive field of several seconds, the models did not enforce long-range consistency which resulted in second-to-second variations in genre, instrumentation, and sound quality” (p. 8).
",2. Related Work,[0],[0]
"Finally, prior work has successfully performed computational molecular generation with deep neural networks.",2. Related Work,[0],[0]
Segler et al. (2017) demonstrated that an LSTM trained on sets of biologically active molecules can be used to generate novel molecules with similar properties.,2. Related Work,[0],[0]
GómezBombarelli,2. Related Work,[0],[0]
et al. (2016) trained a variational autoencoder to learn a compact embedding of molecules encoded using the SMILES notation.,2. Related Work,[0],[0]
"By interpolating in the embedding space and optimizing for desirable metrics of drug quality, the authors were able to decode molecules with high scores on these metrics.",2. Related Work,[0],[0]
"However, producing embeddings that led to valid molecules was difficult; in some cases, as little as 1% of generated sequences proved to be a valid molecule encoding.",2. Related Work,[0],[0]
"In RL, an agent interacts with an environment.",3. Background,[0],[0]
"Given the state of the environment at time t, st, the agent takes an action at according to its policy π(at|st), receives a reward r(st, at), and the environment transitions to state, st+1.The agent’s goal is to maximize reward over a sequence of actions, with a discount factor of γ applied to future rewards.",3. Background,[0],[0]
"The optimal deterministic policy π∗ is known to satisfy the
following Bellman optimality equation,
Q(st, at;π ∗)",3. Background,[0],[0]
"= r(st, at) (1)
+ γEp(st+1|st,at)[maxat+1 Q(st+1, at+1;π
∗)]
where Qπ(st, at) = Eπ[ ∑∞ t′=t",3. Background,[0],[0]
"γ
t′−tr(st′ , at′)] is the Q function of a policy π.",3. Background,[0],[0]
"In Deep Q-learning (Mnih et al., 2013), a neural network called the deep Q-network (DQN) is trained to approximate Q(s, a; θ), using the following objective,
L(θ) =",3. Background,[0],[0]
"Eβ [(r(s, a) + γmax a′ Q(s′, a′; θ−)−Q(s, a; θ))2] (2)
where β is the exploration policy, and θ− is the parameters of the target Q-network (Mnih et al., 2013) that is held fixed during the gradient computation.",3. Background,[0],[0]
"The target Q-network is updated more slowly than the Q-network; for example the moving average of θ can be used as θ−, as proposed by Lillicrap et al. (2015).",3. Background,[0],[0]
Exploration can be performed with either the -greedy method or Boltzmann sampling.,3. Background,[0],[0]
"Additional techniques such as a replay memory (Mnih et al., 2013) are used to stabilize and improve learning.",3. Background,[0],[0]
"Given a trained sequence generation RNN, we would like to impose domain-specific rewards based on the structure and quality of generated sequences, while still maintaining information about typical sequences learned from data.",4. Sequence Tutor,[0],[0]
"Therefore, we treat the trained model as a black-box prior policy, and focus on developing a method that can tune some properties of the model without interfering with the original probability distribution learned from data.",4. Sequence Tutor,[0],[0]
"The separation between the trained sequence model and the tuning method is important, as it prevents RL training from overwriting the original policy.",4. Sequence Tutor,[0],[0]
"To accomplish this task, we propose Sequence Tutor.",4. Sequence Tutor,[0],[0]
"An LSTM trained on data supplies the initial weights for three networks in the model: a recurrent Q-network and target Q-network, and a Reward RNN.",4. Sequence Tutor,[0],[0]
"The Reward RNN is held fixed during training, and treated as a prior policy which can supply the probability of a given token in a sequence as originally learned from data.
",4. Sequence Tutor,[0],[0]
"To apply RL to sequence generation, generating the next token in the sequence is treated as an action a.",4. Sequence Tutor,[0],[0]
"The state of the environment consists of all of the tokens generated so far, i.e. st = {a1, a2, ...at−1}.",4. Sequence Tutor,[0],[0]
"Given action at, we would like the reward rt to combine information about the prior policy p(at|st) as output by the Reward RNN, as well as some domain- or task-specific rewards rT .",4. Sequence Tutor,[0],[0]
Figure 1 illustrates these ideas.,4. Sequence Tutor,[0],[0]
The simplest and most naı̈ve way to incorporate information about the prior policy is to directly augment the taskspecific rewards with the output of the Reward RNN.,4.1. Q-learning with log prior augmentation,[0],[0]
"In this case, the total reward given at time t becomes:
r(s, a) = log p(a|s) + rT (a, s)/c (3)
where c is a constant controlling the emphasis placed on the task-specific rewards.",4.1. Q-learning with log prior augmentation,[0],[0]
"Given the DQN objective in Eq. 2 and modified reward function in Eq. 3, the objective and learned policy are:
L(θ) =",4.1. Q-learning with log prior augmentation,[0],[0]
Eβ,4.1. Q-learning with log prior augmentation,[0],[0]
"[(log p(a|s) + rMT (a, s)/c (4) +",4.1. Q-learning with log prior augmentation,[0],[0]
"γmax
a′ Q(s′, a′; θ−)−Q(s, a; θ))2]
πθ(a|s) = δ(a = arg max a Q(s, a; θ)).",4.1. Q-learning with log prior augmentation,[0],[0]
"(5)
",4.1. Q-learning with log prior augmentation,[0],[0]
"This modified objective forces the model to learn that the most valuable actions are those that conform to the music theory rules, but still have high probability in the original data.",4.1. Q-learning with log prior augmentation,[0],[0]
"However, the DQN learns a deterministic policy (as shown in Eq. 5), which is not ideal for sequence generation.",4.1. Q-learning with log prior augmentation,[0],[0]
"Therefore, after the model is trained, we generate sequences by sampling from the softmax function applied to the predicted Q-values.",4.1. Q-learning with log prior augmentation,[0],[0]
"If we cast sequence generation as a sequential decisionmaking problem and the desired sequence properties in terms of target rewards, the problem can be expressed as a KL control problem for a non-Markovian system.",4.2. KL Control for Sequence Generation,[0],[0]
"KL control (Todorov, 2007; Kappen et al., 2012; Rawlik et al., 2012) is a branch of stochastic optimal control (SOC) (Stengel, 1986), which studies an RL, or control, problem in which the agent tries maximizing its task reward while minimizing deviation from a prior policy.",4.2. KL Control for Sequence Generation,[0],[0]
"For our purposes, we treat a trained MLE sequence model as the prior policy, and thus the objective is to train a new policy, or sequence model, to maximize some rewards while keeping close to the original MLE model.",4.2. KL Control for Sequence Generation,[0],[0]
"We show that such KL control formulation allows us to derive additional
variants of Q-learning with minimal modifications, which give rise to different properties.",4.2. KL Control for Sequence Generation,[0],[0]
Let τ,4.2. KL Control for Sequence Generation,[0],[0]
=,4.2. KL Control for Sequence Generation,[0],[0]
{,4.2. KL Control for Sequence Generation,[0],[0]
"a1, a2, ..., at−1} represent the sequence, r(τ) the reward of the sequence, p(τ) be the prior distribution over τ given by the trained sequence model, and q(τ) be the policy of the Sequence Tutor model.",4.2. KL Control for Sequence Generation,[0],[0]
"The objective is then to maximize the following expression with respect to q(τ), where DKL represents the KL divergence of distributions:
L(q) = Eq(τ)[r(τ)]/c−DKL[q(τ)||p(τ)].",4.2. KL Control for Sequence Generation,[0],[0]
"(6)
We express q(τ) in terms of a parametrized recurrent policy πθ(at|st), i.e. q(τ) = ∏T t=1 πθ(at|st) where st = {a1, a2, ..., at−1}, indicates that the system is nonMarkovian.",4.2. KL Control for Sequence Generation,[0],[0]
The prior policy is expressed similarly p(τ) =∏T t=1 p(at|st).,4.2. KL Control for Sequence Generation,[0],[0]
"The reinforcement learning objective is the following, where Eπ[·] below indicates expectation with respect to sequences sampled from π, L(θ) =",4.2. KL Control for Sequence Generation,[0],[0]
"Eπ[ ∑
t
r(st, at)/c+ log p(at|st)− log πθ(at|st)]
",4.2. KL Control for Sequence Generation,[0],[0]
"The difference between this equation and Eq. 4 is that an entropy regularizer is now included, and thus the optimal policy is no longer deterministic.",4.2. KL Control for Sequence Generation,[0],[0]
"Below, we derive general temporal-difference based methods for the KL-control problem for sequence generation.",4.2. KL Control for Sequence Generation,[0],[0]
"Let V π(st) define the recurrent value function of the policy πθ, given by,
V π(st) =",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"Eπ[ ∞∑
t′=t
r(st′ , at′)/c+ log p(at′ |st′) (7)
− log π(at′ |st′)]
We define the generalized Ψ function, analogous toQ function for KL control, as below.",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"We call this generalized Ψ function, as it was introduced in deriving Ψ-learning (Rawlik et al., 2012), and the following derivation is a generalization to the Ψ-learning algorithm.
",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"Ψπ(st, at) =",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"r(st, at)/c+ log p(at|st) + V π(st+1) (8)
Note that the state st+1 is given deterministically by st = {a1, a2, ..., at−1} and at for sequence modeling, and thus the expressions do not contain the usual stochastic dynamics p(st+1|st, at).",4.3. Recurrent Generalized Ψ-learning,[0],[0]
The value function V π(st),4.3. Recurrent Generalized Ψ-learning,[0],[0]
"can be recursively expressed in terms of Ψπ ,
V π(st) = Eπ[Ψπ(st, at)]",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"+ H[π(.|st)] (9) = Eπ[Ψπ(st, at)− log π(at|st)]",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"(10)
Fixing Ψ(st, at) =",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"Ψπ(st, at) and constraining π to be a probability distribution, the optimal greedy policy update
π∗ can be derived, along with the corresponding optimal value function,
π∗(at|st) ∝ eΨ(st,at) (11) V ∗(st) = log ∑
at
eΨ(st,at) (12)
",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"Given Eq. 8 and 12, the following Bellman optimality equation for generalized Ψ function is derived.
Ψ∗(st, at) = r(st, at)/c+ log p(at|st) + log ∑
at+1
exp(Ψ∗(st+1, at+1))",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"(13)
The Ψ-learning loss directly follows:
LΨ(θ) =",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"Eβ [(Ψθ(st, at)− yt)2] where (14) yt = log p(at|st) + r(st, at)/c+ γ log ∑
a′
eΨ −(st+1,a ′)
",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"β corresponds to sampling sequence trajectories from an arbitrary distribution; in practice, the experience replay (Mnih et al., 2013).",4.3. Recurrent Generalized Ψ-learning,[0],[0]
Ψ− indicates that it uses the target network.,4.3. Recurrent Generalized Ψ-learning,[0],[0]
"Ψθ, i.e. πθ, is parametrized with recurrent neural networks, and for discrete actions, πθ is effectively a softmax layer on top of Ψθ.
4.4.",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"Recurrent G-learning
We can derive another algorithm by parametrizing Ψθ indirectly by Ψθ(st, at) = log p(at|st) +",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"Gθ(st, at).",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"Substituting into above equations, we get a different temporaldifference method:
LG(θ) =",4.3. Recurrent Generalized Ψ-learning,[0],[0]
Eβ,4.3. Recurrent Generalized Ψ-learning,[0],[0]
"[(Gθ(st, at)− yt)2] where (15) yt = r(st, at)/c+ γ log ∑
a′
p(a′|st+1)eG −(st+1,a ′) and
πθ(at|st) ∝",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"p(at|st) exp(Gθ(st, at))
",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"This formulation corresponds to G-learning (Fox et al., 2015), which can thus be seen as a special case of generalized Ψ-learning.",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"Unlike Ψ learning, which directly builds knowledge about the prior policy into the Ψ function, theG-function does not give the policy directly but instead needs to be dynamically mixed with the prior policy probabilities.",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"While this computation is straight-forward for discrete action domains as here, extensions to continuous action domains require additional considerations such as normalizability of Ψ-function parametrization (Gu et al., 2016).
",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"The KL control-based derivation also has another benefit in that the stochastic policies can be directly used as an exploration strategy, instead of heuristics such as -greedy or additive noise (Mnih et al., 2013; Lillicrap et al., 2015).",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"Following from the above derivations, we compare three methods for implementing Sequence Tutor: Q-learning with log prior augmentation (based on Eq. 4), generalized Ψ-learning (based on Eq. 14), and G-learning (based on Eq. 15).",4.5. Sequence Tutor implementation,[0],[0]
"A pre-trained sequence generation LSTM is used as the Reward RNN, to supply the cross entropy reward in Q-learning and the prior policy in G- and generalized Ψlearning.",4.5. Sequence Tutor implementation,[0],[0]
"These approaches are compared to both the original performance of the MLE RNN, and a model trained using only RL and no prior policy.",4.5. Sequence Tutor implementation,[0],[0]
"Model evaluation is performed every 100,000 training epochs, by generating 100 sequences and assessing the average rT and log p(a|s).",4.5. Sequence Tutor implementation,[0],[0]
"The code for Sequence Tutor, including a checkpointed version of the trained melody RNN is available at https://github.com/tensorflow/magenta/ tree/master/magenta/models/rl_tuner.",4.5. Sequence Tutor implementation,[0],[0]
"Music compositions adhere to relatively well-defined structural rules, making music an interesting sequence generation challenge.",5. Experiment I: Melody Generation,[0],[0]
"For example, music theory tells that groups of notes belong to keys, chords follow progressions, and songs have consistent structures made up of musical phrases.",5. Experiment I: Melody Generation,[0],[0]
"Our research question is therefore whether such constraints can be learned by an RNN, while still allowing it to maintain note probabilities learned from data.
",5. Experiment I: Melody Generation,[0],[0]
"To test this hypothesis, we developed several rules that we believe describe pleasant-sounding melodies, taking inspiration from a text on melodic composition (Gauldin, 1995).",5. Experiment I: Melody Generation,[0],[0]
"We do not claim these characteristics are exhaustive or strictly necessary for good composition; rather, they are an incomplete measure of task success that can simply guide the model towards traditional composition structure.",5. Experiment I: Melody Generation,[0],[0]
It is therefore crucial that the Sequence Tutor approach allows the model to retain knowledge learned from real songs in the training data.,5. Experiment I: Melody Generation,[0],[0]
"The rules comprising the music-specific reward function rT (a, s) encourage melodies to: stay in key, start with the tonic note, resolve melodic leaps, have a unique maximum and minimum note, prefer harmonious intervals, play motifs and repeat them, have a low autocorrelation at a lag of 1, 2, and 3 beats, and avoid excessively repeating notes.",5. Experiment I: Melody Generation,[0],[0]
"Interestingly, while excessively repeating tokens is a common problem in RNN sequence generation models, avoiding this behavior is also Gauldin’s first rule of melodic composition (p. 42).
",5. Experiment I: Melody Generation,[0],[0]
"To train the model, we begin by extracting monophonic melodies from a corpus of 30,000 MIDI songs and encoding them as one-hot sequences of notes1.",5. Experiment I: Melody Generation,[0],[0]
"These melodies
1More information about both the note encoding and the reward metrics is available in the supplementary material.
",5. Experiment I: Melody Generation,[0],[0]
are then used to train an LSTM with one layer of 100 cells.,5. Experiment I: Melody Generation,[0],[0]
"Optimization was performed with Adam (Kingma & Ba, 2014), a batch size of 128, initial learning rate of .5, and a stepwise learning rate decay of 0.85 every 1000 steps.",5. Experiment I: Melody Generation,[0],[0]
"Gradients were clipped to ensure the L2 norm was less than 5, and weight regularization was applied with β = 2.5×10−5.",5. Experiment I: Melody Generation,[0],[0]
"Finally, the losses for the first 8 notes of each sequence were not used to train the model, since it cannot reasonably be expected to accurately predict them with no context.",5. Experiment I: Melody Generation,[0],[0]
The trained RNN eventually obtained a validation accuracy of 92% and a log perplexity score of .2536.,5. Experiment I: Melody Generation,[0],[0]
"This model was used as described above to initialize the three sub-networks in the Sequence Tutor model.
",5. Experiment I: Melody Generation,[0],[0]
"The Sequence Tutor model was trained using a similar configuration to the one above, except with a batch size of 32, and a reward discount factor of γ=.5.",5. Experiment I: Melody Generation,[0],[0]
"The TargetQ-network’s weights θ− were gradually updated towards those of the Q-network (θ) according to the formula (1 − η)θ− + ηθ, where η = .01 is the Target-Q-network update rate.",5. Experiment I: Melody Generation,[0],[0]
"A strength of our model is that the influence of data and task-specific rewards can be explicitly controlled by adjusting the temperature parameter c. We replicated our results for a number of settings for c; we present results for c=.5 below because we believe them to be most musically pleasing, however additional results are available at https://goo.gl/cTZy8r.",5. Experiment I: Melody Generation,[0],[0]
"Similarly, we replicated the results using both -greedy and Boltzmann exploration, and present the results using -greedy exploration below.",5. Experiment I: Melody Generation,[0],[0]
"Table 1 provides quantitative results in the form of performance on the music theory rules to which we trained the model to adhere; for example, we can assess the fraction of notes played by the model which belonged to the correct key, or the fraction of melodic leaps that were resolved.",5.1. Results,[0],[0]
"The statistics were computed by randomly generating 100,000 melodies from each model.
",5.1. Results,[0],[0]
"The results above demonstrate that the application of RL is able to correct almost all of the targeted “bad behaviors” of the MLE RNN, while improving performance on the desired metrics.",5.1. Results,[0],[0]
"For example, the original LSTM model was extremely prone to repeating the same note; after applying RL, we see that the number of notes belonging to some excessively repeated segment has dropped from 63% to nearly 0% in all of the Sequence Tutor models.",5.1. Results,[0],[0]
"While the metrics for the G model did not improve as consistently, the Q and Ψ models successfully learned to adhere to most of the imposed rules.",5.1. Results,[0],[0]
The degree of improvement on these metrics is related to the magnitude of the reward given for the behavior.,5.1. Results,[0],[0]
"For example, a strong penalty of -100 was applied each time a note was excessively repeated, while a reward of only 3 was applied at the end of a melody
for unique extrema notes (which most likely explains the lack of improvement on this metric).",5.1. Results,[0],[0]
"The reward values could be adjusted to improve the metrics further, however we found that these values produced pleasant melodies.
",5.1. Results,[0],[0]
"While the metrics indicate that the targeted behaviors of the RNN have improved, it is not clear whether the models have retained information about the training data.",5.1. Results,[0],[0]
"Figure 2a plots the average log p(a|s) as produced by the Reward RNN for melodies generated by the models every 100,000 training epochs; Figure 2b plots the average rT .",5.1. Results,[0],[0]
"Included in the plot is an RL only model trained using only the music theory rewards, with no information about log p(a|s).",5.1. Results,[0],[0]
"Since each model is initialized with the weights of the trained MLE RNN, we see that as the models quickly learn to adhere to the music theory constraints, log p(a|s) falls from its initial point.",5.1. Results,[0],[0]
"For the RL only model, log p(a|s) reaches an average of -3.65, which is equivalent to an average p(a|s) of approximately 0.026, or essentially a random policy over the 38 actions with respect to the distribution defined by the Reward RNN.",5.1. Results,[0],[0]
"Figure 2a shows that each of our models (Q, Ψ, and G) attain higher log p(a|s) values than this baseline, indicating they have maintained information about the data distribution, even over 3,000,000 training steps.",5.1. Results,[0],[0]
"TheG-learning implementation scores highest on this metric, at the cost of slightly lower average rT .",5.1. Results,[0],[0]
This compromise between data probability and adherence to music theory could explain the difference in the G model’s performance on the music theory metrics in Table 1.,5.1. Results,[0],[0]
"Finally, we have verified that by increasing the c parameter it is possible to train all the models to have even higher average log p(a|s), but found that c = 0.5 produced melodies that sounded better subjectively.
",5.1. Results,[0],[0]
The question remains whether the RL-tutored models actually produce more pleasing melodies.,5.1. Results,[0],[0]
"The sample melodies used for the study are available here: goo.gl/XIYt9m;
we encourage readers to judge their quality for themselves.",5.1. Results,[0],[0]
"To more formally answer this question, we conducted a user study via Amazon Mechanical Turk in which participants were asked to rate which of two randomly selected melodies they preferred on a Likert scale.",5.1. Results,[0],[0]
A total of 192 ratings were collected; each model was involved in 92 of these comparisons.,5.1. Results,[0],[0]
Figure 3 plots the number of comparisons in which a melody from each model was selected as the most musically pleasing.,5.1. Results,[0],[0]
"A Kruskal-Wallis H test of the ratings showed that there was a statistically significant difference between the models, χ2(3) = 109.480, p < 0.001.",5.1. Results,[0],[0]
"Mann-Whitney U post-hoc tests revealed that the melodies from all three Sequence Tuner models (Q, Ψ, and G) had significantly higher ratings than the melodies of the MLE RNN, p < .001.",5.1. Results,[0],[0]
"The Q and Ψ melodies were also rated as significantly more pleasing than those of the G model, but did not differ significantly from each other.",5.1. Results,[0],[0]
"Listening to the samples produced by the MLE RNN reveals that they are sometimes dischordant and usually dull; the model tends to place rests frequently, repeat the same
0 10 20 30 40 50 60 70 80 90
Number of times preferred
Ψ
Q
G
Note RNN
M o d e l
Figure 3: The number of times a melody from each model was selected as most musically pleasing.",5.2. Discussion,[0],[0]
Error bars reflect the std. dev.,5.2. Discussion,[0],[0]
"of a binomial distribution fit to the binary win/loss data from each model.
note, and produce melodies with little variation.",5.2. Discussion,[0],[0]
"In contrast, the melodies produced by the Sequence Tutor models are more varied and interesting.",5.2. Discussion,[0],[0]
"The G model tends to produce energetic and chaotic melodies, which include sequences of repeated notes.",5.2. Discussion,[0],[0]
"This repetition is likely because theG policy as defined in Eq. 15 directly mixes p(a|s) with the output of the G network, and the MLE RNN strongly favours repeating notes.",5.2. Discussion,[0],[0]
The most pleasant melodies are generated by the Q and Ψ models.,5.2. Discussion,[0],[0]
"These melodies stay firmly in key and frequently choose more harmonious interval steps, leading to melodic and pleasant melodies.",5.2. Discussion,[0],[0]
"However, it is clear they have retained information about the training data; for example, the sample q2.wav in the sample directory ends with a seemingly familiar riff.
",5.2. Discussion,[0.9566078484385805],"['In classic machine learning, it is assumed that the training and testing data are drawn from the same distribution.']"
"While we acknowledge that the monophonic melodies generated by these models — which are based on highly simplistic rules of melodic composition — do not approach the level of artistic merit of human composers, we believe this study provides a proof-of-concept that encoding even incomplete and partially specified domain knowledge using our method can help the outputs of an LSTM adhere to a more consistent structure.",5.2. Discussion,[0],[0]
"The musical complexity of the songs is limited not just by the heuristic rules, but also by the simple monophonic encoding, which cannot represent the dynamics and expressivity of a musical performance.",5.2. Discussion,[0],[0]
"Although these melodies cannot surpass those of human musicians, attempting to train a model to generate aesthetically pleasing outputs in the absence of a better metric of human taste than log-likelihood is a problem of broader interest to the artificial intelligence community.",5.2. Discussion,[0],[0]
"As a follow-on experiment, we tested the effectiveness of Sequence Tutor for generating a higher yield of synthet-
ically accessible drug-like molecules.",6. Experiment II: Computational Molecular Generation,[0],[0]
"Organic molecules can be encoded using the commonly used SMILES representation (Weininger, 1970).",6. Experiment II: Computational Molecular Generation,[0],[0]
"For example, amphetamine can be encoded as ‘CC(N)Cc1ccccc1’, while creatine is ‘CN(CC(=O)O)C(=N)N’.",6. Experiment II: Computational Molecular Generation,[0],[0]
"Using this character encoding, it is straightforward to train an MLE RNN to generate sequences of SMILES characters; we trained such a model using the same settings as described above for the melody MLE RNN.",6. Experiment II: Computational Molecular Generation,[0],[0]
"However, only about a third of the molecules generated using this simple approach are actually valid SMILES encodings.",6. Experiment II: Computational Molecular Generation,[0],[0]
"Further, this approach does not directly optimize for metrics of molecule or drug quality.",6. Experiment II: Computational Molecular Generation,[0],[0]
"These metrics include: a) the water-octanol partition coefficient (logP), which is important in assessing the druglikeness of a molecule; b) synthetic accessibility (SA) (Ertl & Schuffenhauer, 2009), a score from 1-10 that is lower if the molecule is easier to synthesize; and c) Quantitative Estimation of Drug-likeness (QED) (Bickerton et al., 2012), a more subjective measure of drug-likeness based on abstract ideas of medicinal aesthetics.
",6. Experiment II: Computational Molecular Generation,[0],[0]
"To optimize for these metrics, while simultaneously improving the percent yield of valid molecules from the RNN, we constructed a reward function that incentivizes validity, logP, SA, and QED using an open-source library called RDkit (http://www.rdkit.org/).",6. Experiment II: Computational Molecular Generation,[0],[0]
"Included in the reward function was a penalty for molecules with unrealistically large carbon rings (size larger than 6), as per previous work (Gómez-Bombarelli et al., 2016).",6. Experiment II: Computational Molecular Generation,[0],[0]
"Finally, after observing that the model could exploit the reward function by generating the simple molecule ‘N’ repeatedly, or ‘CCCCC...’ (which produces an unrealistically high logP value), we added penalties for sequences shorter than, or with more consecutive carbon atoms than, any sequence in the training data.",6. Experiment II: Computational Molecular Generation,[0],[0]
"Sequence Tutor was then trained using these rewards, the pre-trained MLE RNN, and similar settings to the first experiment, except with -greedy exploration with = .01, a batch size of 512, and discount factor γ = .95.",6. Experiment II: Computational Molecular Generation,[0],[0]
"For this experiment, we also made use of prioritized experience replay (Schaul et al., 2015) to allow the model to more frequently learn from relatively rare valid samples.",6. Experiment II: Computational Molecular Generation,[0],[0]
"A value of c = 2.85 led to a higher yield of valid molecules with high metrics, but still encouraged the diversity of generated samples.",6. Experiment II: Computational Molecular Generation,[0],[0]
"As the Ψ algorithm produced the best results for the music generation task, we focused on using this technique for generating molecules.",6.1. Results and discussion,[0],[0]
"Table 2 shows the performance of this model against the original MLE model according to metrics of validity, drug-likeness, and synthetic accessibility.",6.1. Results and discussion,[0],[0]
"Once again, Sequence Tutor is able to significantly improve almost all of the targeted metrics.",6.1. Results and discussion,[0],[0]
"However, it should be noted that the Sequence Tutor model
tends to produce simplistic molecules involving more carbon atoms than the MLE baseline; e.g. Sequence Tutor may produce ‘SNCc1ccccc1’, while the MLE produces ‘C(=O)c1ccc(S(=O)(=O)N(C)C)c(Cl)c1’, which is the reason for the Sequence Tutor model’s lower QED scores.",6.1. Results and discussion,[0],[0]
"This effect is due to the fact that simple sequences are more likely to be valid, have high logP and SA scores, and carbon is highly likely under the distribution learned by the MLE model.",6.1. Results and discussion,[0],[0]
A higher reward for QED and further improvement of the task-specific rewards based on domain knowledge could help to alleviate these problems.,6.1. Results and discussion,[0],[0]
"Overall, the fact that Sequence Tutor can improve the percentage of valid molecules produced as well as the logP and synthetic accessibility scores serves as a proof-of-concept that Sequence Tutor may be valuable in a number of domains for imparting domain knowledge onto a sequence predictor.",6.1. Results and discussion,[0],[0]
"We have derived a novel sequence learning framework which uses RL to correct properties of sequences generated by an RNN, while maintaining information learned from MLE training on data, and ensuring the diversity of generated samples.",7. Conclusion and Future Work,[0],[0]
"By demonstrating a connection between our sequence generation approach and KL-control, we have derived three novel RL-based methods for optimizing sequence generation models.",7. Conclusion and Future Work,[0],[0]
"These methods were empirically compared in the context of a music generation task, and further demonstrated on a computational molecular generation task.",7. Conclusion and Future Work,[0],[0]
"Sequence Tutor showed promising results in terms of both adherence to task-specific rules, and subjective quality of the generated sequences.
",7. Conclusion and Future Work,[0],[0]
"We believe the Sequence Tutor approach of using RL to refine RNN models could be promising for a number of applications, including the reduction of bias in deep learning models.",7. Conclusion and Future Work,[0],[0]
"While manually writing a domain-specific reward function may seem unappealing, that approach is limited by the quality of the data that can be collected, and besides, even state-of-the-art sequence models often fail to learn all the aspects of high-level structure (van den Oord et al., 2016; Graves, 2013).",7. Conclusion and Future Work,[0],[0]
"Further, the data may contain hidden biases, as has been demonstrated for popular language models (Caliskan-Islam et al., 2016).",7. Conclusion and Future Work,[0],[0]
"In contrast to relying solely on possibly biased data, our approach allows
for encoding high-level domain knowledge into the RNN, providing a general, alternative tool for training sequence models.",7. Conclusion and Future Work,[0],[0]
"This work was supported by Google Brain, the MIT Media Lab Consortium, and Canada’s Natural Sciences and Engineering Research Council (NSERC).",ACKNOWLEDGMENTS,[0],[0]
"We thank Greg Wayne, Sergey Levine, and Timothy Lillicrap for helpful discussions on RL and stochastic optimal control and Kyle Kastner and Tim Cooijmans for valuable insight into training RNNs.",ACKNOWLEDGMENTS,[0],[0]
"This paper proposes a general method for improving the structure and quality of sequences generated by a recurrent neural network (RNN), while maintaining information originally learned from data, as well as sample diversity.",abstractText,[0],[0]
"An RNN is first pre-trained on data using maximum likelihood estimation (MLE), and the probability distribution over the next token in the sequence learned by this model is treated as a prior policy.",abstractText,[0],[0]
Another RNN is then trained using reinforcement learning (RL) to generate higher-quality outputs that account for domain-specific incentives while retaining proximity to the prior policy of the MLE RNN.,abstractText,[0],[0]
"To formalize this objective, we derive novel off-policy RL methods for RNNs from KL-control.",abstractText,[0],[0]
"The effectiveness of the approach is demonstrated on two applications; 1) generating novel musical melodies, and 2) computational molecular generation.",abstractText,[0],[0]
"For both problems, we show that the proposed method improves the desired properties and structure of the generated sequences, while maintaining information learned from data.",abstractText,[0],[0]
Sequence Tutor: Conservative Fine-Tuning of Sequence Generation Models with KL-control,title,[0],[0]
"Neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) is a deep learningbased method for translation that has recently shown promising results as an alternative to statistical ap-
proaches.",1 Introduction,[0],[0]
"NMT systems directly model the probability of the next word in the target sentence simply by conditioning a recurrent neural network on the source sentence and previously generated target words.
",1 Introduction,[0],[0]
"While both simple and surprisingly accurate, NMT systems typically need to have very high capacity in order to perform well: Sutskever et al. (2014) used a 4-layer LSTM with 1000 hidden units per layer (herein 4×1000) and Zhou et al.",1 Introduction,[0],[0]
(2016) obtained state-of-the-art results on English → French with a 16-layer LSTM with 512 units per layer.,1 Introduction,[0],[0]
"The sheer size of the models requires cutting-edge hardware for training and makes using the models on standard setups very challenging.
",1 Introduction,[0],[0]
"This issue of excessively large networks has been observed in several other domains, with much focus on fully-connected and convolutional networks for multi-class classification.",1 Introduction,[0],[0]
"Researchers have particularly noted that large networks seem to be necessary for training, but learn redundant representations in the process (Denil et al., 2013).",1 Introduction,[0],[0]
Therefore compressing deep models into smaller networks has been an active area of research.,1 Introduction,[0],[0]
"As deep learning systems obtain better results on NLP tasks, compression also becomes an important practical issue with applications such as running deep learning models for speech and translation locally on cell phones.
",1 Introduction,[0],[0]
Existing compression methods generally fall into two categories: (1) pruning and (2) knowledge distillation.,1 Introduction,[0],[0]
"Pruning methods (LeCun et al., 1990; He et al., 2014; Han et al., 2016), zero-out weights or entire neurons based on an importance criterion: LeCun et al. (1990) use (a diagonal approximation to)
",1 Introduction,[0],[0]
"ar X
iv :1
60 6.
07 94
7v 4
[ cs
.C",1 Introduction,[0],[0]
"L
] 2
2 Se
p 20
the Hessian to identify weights whose removal minimally impacts the objective function, while Han et al. (2016) remove weights based on thresholding their absolute values.",1 Introduction,[0],[0]
"Knowledge distillation approaches (Bucila et al., 2006; Ba and Caruana, 2014; Hinton et al., 2015) learn a smaller student network to mimic the original teacher network by minimizing the loss (typically L2 or cross-entropy) between the student and teacher output.
",1 Introduction,[0],[0]
"In this work, we investigate knowledge distillation in the context of neural machine translation.",1 Introduction,[0],[0]
We note that NMT differs from previous work which has mainly explored non-recurrent models in the multiclass prediction setting.,1 Introduction,[0],[0]
"For NMT, while the model is trained on multi-class prediction at the word-level, it is tasked with predicting complete sequence outputs conditioned on previous decisions.",1 Introduction,[0],[0]
"With this difference in mind, we experiment with standard knowledge distillation for NMT and also propose two new versions of the approach that attempt to approximately match the sequence-level (as opposed to word-level) distribution of the teacher network.",1 Introduction,[0],[0]
"This sequence-level approximation leads to a simple training procedure wherein the student network is trained on a newly generated dataset that is the result of running beam search with the teacher network.
",1 Introduction,[0.9617617999998078],['The data is first mapped into a transformed feature space via a kernel and then standard two-class SVM is employed to construct a hyper-plane that separates the data and the original with maximum margin.']
"We run experiments to compress a large state-ofthe-art 4 × 1000 LSTM model, and find that with sequence-level knowledge distillation we are able to learn a 2× 500 LSTM that roughly matches the performance of the full system.",1 Introduction,[0],[0]
We see similar results compressing a 2 × 500 model down to 2 × 100 on a smaller data set.,1 Introduction,[0],[0]
"Furthermore, we observe that our proposed approach has other benefits, such as not requiring any beam search at test-time.",1 Introduction,[0],[0]
As a result we are able to perform greedy decoding on the 2 × 500 model 10 times faster than beam search on the 4 × 1000 model with comparable performance.,1 Introduction,[0],[0]
"Our student models can even be run efficiently on a standard smartphone.1 Finally, we apply weight pruning on top of the student network to obtain a model that has 13× fewer parameters than the original teacher model.",1 Introduction,[0],[0]
"We have released all the code for the models described in this paper.2
1https://github.com/harvardnlp/nmt-android 2https://github.com/harvardnlp/seq2seq-attn",1 Introduction,[0],[0]
"Let s = [s1, . . .",2.1 Sequence-to-Sequence with Attention,[0],[0]
", sI ] and t =",2.1 Sequence-to-Sequence with Attention,[0],[0]
"[t1, . .",2.1 Sequence-to-Sequence with Attention,[0],[0]
.,2.1 Sequence-to-Sequence with Attention,[0],[0]
", tJ ] be (random variable sequences representing)",2.1 Sequence-to-Sequence with Attention,[0],[0]
"the source/target sentence, with I and J respectively being the source/target lengths.",2.1 Sequence-to-Sequence with Attention,[0],[0]
"Machine translation involves finding the most probable target sentence given the source:
argmax t∈T
p(t | s)
where T is the set of all possible sequences.",2.1 Sequence-to-Sequence with Attention,[0],[0]
NMT models parameterize p(t | s) with an encoder neural network which reads the source sentence and a decoder neural network which produces a distribution over the target sentence (one word at a time) given the source.,2.1 Sequence-to-Sequence with Attention,[0],[0]
"We employ the attentional architecture from Luong et al. (2015), which achieved state-ofthe-art results on English→ German translation.3",2.1 Sequence-to-Sequence with Attention,[0],[0]
Knowledge distillation describes a class of methods for training a smaller student network to perform better by learning from a larger teacher network (in addition to learning from the training data set).,2.2 Knowledge Distillation,[0],[0]
"We generally assume that the teacher has previously been trained, and that we are estimating parameters for the student.",2.2 Knowledge Distillation,[0],[0]
Knowledge distillation suggests training by matching the student’s predictions to the teacher’s predictions.,2.2 Knowledge Distillation,[0],[0]
"For classification this usually means matching the probabilities either via L2 on the log scale (Ba and Caruana, 2014) or by crossentropy (Li et al., 2014; Hinton et al., 2015).
",2.2 Knowledge Distillation,[0],[0]
"Concretely, assume we are learning a multi-class classifier over a data set of examples of the form (x, y) with possible classes V .",2.2 Knowledge Distillation,[0],[0]
"The usual training criteria is to minimize NLL for each example from the training data,
LNLL(θ) =",2.2 Knowledge Distillation,[0],[0]
"− |V|∑ k=1 1{y = k} log p(y = k |x; θ)
where 1{·} is the indicator function and p the distribution from our model (parameterized by θ).
",2.2 Knowledge Distillation,[0],[0]
"3Specifically, we use the global-general attention model with the input-feeding approach.",2.2 Knowledge Distillation,[0],[0]
"We refer the reader to the original paper for further details.
",2.2 Knowledge Distillation,[0],[0]
"This objective can be seen as minimizing the crossentropy between the degenerate data distribution (which has all of its probability mass on one class) and the model distribution p(y |x; θ).
",2.2 Knowledge Distillation,[0],[0]
"In knowledge distillation, we assume access to a learned teacher distribution q(y |x; θT ), possibly trained over the same data set.",2.2 Knowledge Distillation,[0],[0]
"Instead of minimizing cross-entropy with the observed data, we instead minimize the cross-entropy with the teacher’s probability distribution,
LKD(θ; θT )",2.2 Knowledge Distillation,[0],[0]
"=− |V|∑ k=1 q(y = k |x; θT )×
log p(y = k |x; θ)
where θT parameterizes the teacher distribution and remains fixed.",2.2 Knowledge Distillation,[0],[0]
"Note the cross-entropy setup is identical, but the target distribution is no longer a sparse distribution.4 Training on q(y |x; θT ) is attractive since it gives more information about other classes for a given data point (e.g. similarity between classes) and has less variance in gradients (Hinton et al., 2015).
",2.2 Knowledge Distillation,[0],[0]
4 In some cases the entropy of the teacher/student distribution is increased by annealing it with a temperature term τ,2.2 Knowledge Distillation,[0],[0]
"> 1
p̃(y |x) ∝",2.2 Knowledge Distillation,[0],[0]
"p(y |x) 1 τ
After testing τ ∈ {1, 1.5, 2} we found that τ = 1 worked best.
",2.2 Knowledge Distillation,[0],[0]
"Since this new objective has no direct term for the training data, it is common practice to interpolate between the two losses,
L(θ; θT )",2.2 Knowledge Distillation,[0],[0]
= (1− α)LNLL(θ),2.2 Knowledge Distillation,[0],[0]
"+ αLKD(θ; θT )
where α is mixture parameter combining the one-hot distribution and the teacher distribution.",2.2 Knowledge Distillation,[0],[0]
The large sizes of neural machine translation systems make them an ideal candidate for knowledge distillation approaches.,3 Knowledge Distillation for NMT,[0],[0]
In this section we explore three different ways this technique can be applied to NMT.,3 Knowledge Distillation for NMT,[0],[0]
"NMT systems are trained directly to minimize word NLL, LWORD-NLL, at each position.",3.1 Word-Level Knowledge Distillation,[0],[0]
"Therefore if we have a teacher model, standard knowledge distillation for multi-class cross-entropy can be applied.",3.1 Word-Level Knowledge Distillation,[0],[0]
"We define this distillation for a sentence as,
LWORD-KD = − J∑
j=1 |V|∑ k=1 q(tj = k | s, t<j)×
log p(tj = k",3.1 Word-Level Knowledge Distillation,[0],[0]
"| s, t<j)
where V is the target vocabulary set.",3.1 Word-Level Knowledge Distillation,[0],[0]
"The student can further be trained to optimize the mixture of
LWORD-KD and LWORD-NLL.",3.1 Word-Level Knowledge Distillation,[0],[0]
"In the context of NMT, we refer to this approach as word-level knowledge distillation and illustrate this in Figure 1 (left).",3.1 Word-Level Knowledge Distillation,[0],[0]
Word-level knowledge distillation allows transfer of these local word distributions.,3.2 Sequence-Level Knowledge Distillation,[0],[0]
"Ideally however, we would like the student model to mimic the teacher’s actions at the sequence-level.",3.2 Sequence-Level Knowledge Distillation,[0],[0]
"The sequence distribution is particularly important for NMT, because wrong predictions can propagate forward at testtime.
",3.2 Sequence-Level Knowledge Distillation,[0],[0]
"First, consider the sequence-level distribution specified by the model over all possible sequences t ∈ T ,
p(t | s) = J∏
j=1
p(tj | s, t<j)
for any length J .",3.2 Sequence-Level Knowledge Distillation,[0],[0]
"The sequence-level negative loglikelihood for NMT then involves matching the onehot distribution over all complete sequences,
LSEQ-NLL = − ∑ t∈T 1{t = y} log p(t | s)
=",3.2 Sequence-Level Knowledge Distillation,[0],[0]
"− J∑
j=1 |V|∑ k=1 1{yj = k} log p(tj = k",3.2 Sequence-Level Knowledge Distillation,[0],[0]
"| s, t<j)
= LWORD-NLL
where y =",3.2 Sequence-Level Knowledge Distillation,[0],[0]
"[y1, . . .",3.2 Sequence-Level Knowledge Distillation,[0],[0]
", yJ ] is the observed sequence.",3.2 Sequence-Level Knowledge Distillation,[0],[0]
"Of course, this just shows that from a negative log likelihood perspective, minimizing word-level NLL and sequence-level NLL are equivalent in this model.
",3.2 Sequence-Level Knowledge Distillation,[0],[0]
But now consider the case of sequence-level knowledge distillation.,3.2 Sequence-Level Knowledge Distillation,[0],[0]
"As before, we can simply replace the distribution from the data with a probability distribution derived from our teacher model.",3.2 Sequence-Level Knowledge Distillation,[0],[0]
"However, instead of using a single word prediction, we use q(t | s) to represent the teacher’s sequence distribution over the sample space of all possible sequences,
LSEQ-KD = − ∑ t∈T q(t | s) log p(t | s)
Note that LSEQ-KD is inherently different from LWORD-KD, as the sum is over an exponential number of terms.",3.2 Sequence-Level Knowledge Distillation,[0],[0]
"Despite its intractability, we posit
that this sequence-level objective is worthwhile.",3.2 Sequence-Level Knowledge Distillation,[0],[0]
It gives the teacher the chance to assign probabilities to complete sequences and therefore transfer a broader range of knowledge.,3.2 Sequence-Level Knowledge Distillation,[0],[0]
"We thus consider an approximation of this objective.
",3.2 Sequence-Level Knowledge Distillation,[0],[0]
"Our simplest approximation is to replace the teacher distribution q with its mode,
q(t | s) ∼",3.2 Sequence-Level Knowledge Distillation,[0],[0]
1{t,3.2 Sequence-Level Knowledge Distillation,[0],[0]
=,3.2 Sequence-Level Knowledge Distillation,[0],[0]
argmax,3.2 Sequence-Level Knowledge Distillation,[0],[0]
"t∈T q(t | s)}
Observing that finding the mode is itself intractable, we use beam search to find an approximation.",3.2 Sequence-Level Knowledge Distillation,[0],[0]
"The loss is then
LSEQ-KD",3.2 Sequence-Level Knowledge Distillation,[0],[0]
≈,3.2 Sequence-Level Knowledge Distillation,[0],[0]
"− ∑ t∈T 1{t = ŷ} log p(t | s)
=",3.2 Sequence-Level Knowledge Distillation,[0],[0]
− log p(t = ŷ,3.2 Sequence-Level Knowledge Distillation,[0],[0]
"| s)
where ŷ is now the output from running beam search with the teacher model.
",3.2 Sequence-Level Knowledge Distillation,[0],[0]
"Using the mode seems like a poor approximation for the teacher distribution q(t | s), as we are approximating an exponentially-sized distribution with a single sample.",3.2 Sequence-Level Knowledge Distillation,[0],[0]
"However, previous results showing the effectiveness of beam search decoding for NMT lead us to belief that a large portion of q’s mass lies in a single output sequence.",3.2 Sequence-Level Knowledge Distillation,[0],[0]
"In fact, in experiments we find that with beam of size 1, q(ŷ | s) (on average) accounts for 1.3% of the distribution for German→ English, and 2.3% for Thai→ English (Table 1: p(t = ŷ)).5
To summarize, sequence-level knowledge distillation suggests to: (1) train a teacher model, (2) run beam search over the training set with this model, (3) train the student network with cross-entropy on this new dataset.",3.2 Sequence-Level Knowledge Distillation,[0],[0]
Step (3) is identical to the word-level NLL process except now on the newly-generated data set.,3.2 Sequence-Level Knowledge Distillation,[0],[0]
"This is shown in Figure 1 (center).
",3.2 Sequence-Level Knowledge Distillation,[0],[0]
5Additionally there are simple ways to better approximate q(t | s).,3.2 Sequence-Level Knowledge Distillation,[0],[0]
"One way would be to consider a K-best list from beam search and renormalizing the probabilities,
q(t | s) ∼ q(t | s)∑ t∈TK q(t | s)
where TK is the K-best list from beam search.",3.2 Sequence-Level Knowledge Distillation,[0],[0]
"This would increase the training set by a factor of K. A beam of size 5 captures 2.8% of the distribution for German → English, and 3.8% for Thai → English.",3.2 Sequence-Level Knowledge Distillation,[0],[0]
Another alternative is to use a Monte Carlo estimate and sample from the teacher model (since LSEQ-KD = Et∼q(t | s)[− log p(t | s) ]).,3.2 Sequence-Level Knowledge Distillation,[0],[0]
However in practice we found the (approximate) mode to work well.,3.2 Sequence-Level Knowledge Distillation,[0],[0]
"Next we consider integrating the training data back into the process, such that we train the student model as a mixture of our sequence-level teachergenerated data (LSEQ-KD) with the original training data (LSEQ-NLL),
L = (1− α)LSEQ-NLL + αLSEQ-KD = −(1− α) log p(y |",3.3 Sequence-Level Interpolation,[0],[0]
s)−,3.3 Sequence-Level Interpolation,[0],[0]
α ∑,3.3 Sequence-Level Interpolation,[0],[0]
"t∈T q(t | s) log p(t | s)
where y is the gold target sequence.",3.3 Sequence-Level Interpolation,[0],[0]
"Since the second term is intractable, we could again apply the mode approximation from the previous section,
L = −(1− α) log p(y |",3.3 Sequence-Level Interpolation,[0],[0]
"s)− α log p(ŷ | s)
and train on both observed (y) and teachergenerated (ŷ) data.",3.3 Sequence-Level Interpolation,[0],[0]
"However, this process is nonideal for two reasons: (1) unlike for standard knowledge distribution, it doubles the size of the training data, and (2) it requires training on both the teachergenerated sequence and the true sequence, conditioned on the same source input.",3.3 Sequence-Level Interpolation,[0],[0]
"The latter concern is particularly problematic since we observe that y and ŷ are often quite different.
",3.3 Sequence-Level Interpolation,[0],[0]
"As an alternative, we propose a single-sequence approximation that is more attractive in this setting.",3.3 Sequence-Level Interpolation,[0],[0]
"This approach is inspired by local updating (Liang et al., 2006), a method for discriminative training in statistical machine translation (although to our knowledge not for knowledge distillation).",3.3 Sequence-Level Interpolation,[0],[0]
"Local updating suggests selecting a training sequence which is close to y and has high probability under the teacher model,
ỹ = argmax t∈T
sim(t,y)q(t | s)
where sim is a function measuring closeness (e.g. Jaccard similarity or BLEU (Papineni et al., 2002)).",3.3 Sequence-Level Interpolation,[0],[0]
"Following local updating, we can approximate this sequence by running beam search and choosing
ỹ",3.3 Sequence-Level Interpolation,[0],[0]
≈ argmax t∈TK,3.3 Sequence-Level Interpolation,[0],[0]
"sim(t,y)
where TK is the K-best list from beam search.",3.3 Sequence-Level Interpolation,[0],[0]
"We take sim to be smoothed sentence-level BLEU (Chen and Cherry, 2014).
",3.3 Sequence-Level Interpolation,[0],[0]
"We justify training on ỹ from a knowledge distillation perspective with the following generative process: suppose that there is a true target sequence (which we do not observe) that is first generated from the underlying data distributionD. And further suppose that the target sequence that we observe (y) is a noisy version of the unobserved true sequence: i.e. (i) t ∼ D, (ii) y ∼ (t), where (t) is, for example, a noise function that independently replaces each element in t with a random element in V with some small probability.6 In such a case, ideally the student’s distribution should match the mixture distribution,
DSEQ-Inter ∼ (1− α)D + αq(t | s)
",3.3 Sequence-Level Interpolation,[0],[0]
"In this setting, due to the noise assumption,D now has significant probability mass around a neighborhood of y (not just at y), and therefore the argmax of the mixture distribution is likely something other than y (the observed sequence) or ŷ",3.3 Sequence-Level Interpolation,[0],[0]
(the output from beam search).,3.3 Sequence-Level Interpolation,[0],[0]
We can see that ỹ is a natural approximation to the argmax of this mixture distribution between D and q(t | s) for some α.,3.3 Sequence-Level Interpolation,[0],[0]
We illustrate this framework in Figure 1 (right) and visualize the distribution over a real example in Figure 2.,3.3 Sequence-Level Interpolation,[0],[0]
"To test out these approaches, we conduct two sets of NMT experiments: high resource (English → German) and low resource (Thai→ English).
",4 Experimental Setup,[0],[0]
The English-German data comes from WMT 2014.7 The training set has 4m sentences and we take newstest2012/newstest2013 as the dev set and newstest2014 as the test set.,4 Experimental Setup,[0],[0]
"We keep the top 50k most frequent words, and replace the rest with UNK.",4 Experimental Setup,[0],[0]
The teacher model is a 4 × 1000 LSTM (as in Luong et al. (2015)) and we train two student models: 2× 300 and 2× 500.,4 Experimental Setup,[0],[0]
"The Thai-English data comes from IWSLT 2015.8 There are 90k sentences in the
6While we employ a simple (unrealistic) noise function for illustrative purposes, the generative story is quite plausible if we consider a more elaborate noise function which includes additional sources of noise such as phrase reordering, replacement of words with synonyms, etc.",4 Experimental Setup,[0],[0]
"One could view translation having two sources of variance that should be modeled separately: variance due to the source sentence (t ∼ D), and variance due to the individual translator (y ∼ (t)).
",4 Experimental Setup,[0],[0]
"7http://statmt.org/wmt14 8https://sites.google.com/site/iwsltevaluation2015/mt-track
training set",4 Experimental Setup,[0],[0]
"and we take 2010/2011/2012 data as the dev set and 2012/2013 as the test set, with a vocabulary size is 25k.",4 Experimental Setup,[0],[0]
"Size of the teacher model is 2×500 (which performed better than 4×1000, 2×750 models), and the student model is 2×100.",4 Experimental Setup,[0],[0]
"Other training details mirror Luong et al. (2015).
",4 Experimental Setup,[0],[0]
"We evaluate on tokenized BLEU with multi-bleu.perl, and experiment with the following variations:
Word-Level Knowledge Distillation (Word-KD) Student is trained on the original data and additionally trained to minimize the cross-entropy of the teacher distribution at the word-level.",4 Experimental Setup,[0],[0]
"We tested α ∈ {0.5, 0.9} and found α = 0.5 to work better.
",4 Experimental Setup,[0],[0]
"Sequence-Level Knowledge Distillation (Seq-KD) Student is trained on the teacher-generated data, which is the result of running beam search and taking the highest-scoring sequence with the teacher model.",4 Experimental Setup,[0],[0]
"We use beam size K = 5 (we did not see improvements with a larger beam).
",4 Experimental Setup,[0],[0]
Sequence-Level Interpolation (Seq-Inter) Student is trained on the sequence on the teacher’s beam that had the highest BLEU (beam size K = 35).,4 Experimental Setup,[0],[0]
"We
adopt a fine-tuning approach where we begin training from a pretrained model (either on original data or Seq-KD data) and train with a smaller learning rate (0.1).",4 Experimental Setup,[0],[0]
"For English-German we generate SeqInter data on a smaller portion of the training set (∼ 50%) for efficiency.
",4 Experimental Setup,[0],[0]
The above methods are complementary and can be combined with each other.,4 Experimental Setup,[0],[0]
"For example, we can train on teacher-generated data but still include a word-level cross-entropy term between the teacher/student (Seq-KD + Word-KD in Table 1), or fine-tune towards Seq-Inter data starting from the baseline model trained on original data (Baseline + Seq-Inter in Table 1).9",4 Experimental Setup,[0],[0]
Results of our experiments are shown in Table 1.,5 Results and Discussion,[0],[0]
"We find that while word-level knowledge distillation (Word-KD) does improve upon the baseline, sequence-level knowledge distillation (SeqKD) does better on English → German and performs similarly on Thai → English.",5 Results and Discussion,[0],[0]
"Combining them (Seq-KD + Word-KD) results in further gains for the 2 × 300 and 2 × 100 models (although not for the 2 × 500 model), indicating that these methods provide orthogonal means of transferring knowledge from the teacher to the student: Word-KD is transferring knowledge at the the local (i.e. word) level while Seq-KD is transferring knowledge at the global (i.e. sequence) level.
",5 Results and Discussion,[0],[0]
"Sequence-level interpolation (Seq-Inter), in addition to improving models trained via Word-KD and Seq-KD, also improves upon the original teacher model that was trained on the actual data but finetuned towards Seq-Inter data (Baseline + Seq-Inter).",5 Results and Discussion,[0],[0]
"In fact, greedy decoding with this fine-tuned model has similar performance (19.6) as beam search with the original model (19.5), allowing for faster decoding even with an identically-sized model.
",5 Results and Discussion,[0],[0]
"We hypothesize that sequence-level knowledge distillation is effective because it allows the student network to only model relevant parts of the teacher distribution (i.e. around the teacher’s mode) instead of ‘wasting’ parameters on trying to model the entire
9For instance, ‘Seq-KD + Seq-Inter + Word-KD’ in Table 1 means that the model was trained on Seq-KD data and finetuned towards Seq-Inter data with the mixture cross-entropy loss at the word-level.
space of translations.",5 Results and Discussion,[0],[0]
Our results suggest that this is indeed the case: the probability mass that SeqKD models assign to the approximate mode is much higher than is the case for baseline models trained on original data (Table 1: p(t = ŷ)).,5 Results and Discussion,[0],[0]
"For example, on English → German the (approximate) argmax for the 2 × 500 Seq-KD model (on average) accounts for 16.9% of the total probability mass, while the corresponding number is 0.9% for the baseline.",5 Results and Discussion,[0],[0]
"This also explains the success of greedy decoding for Seq-KD models—since we are only modeling around the teacher’s mode, the student’s distribution is more peaked and therefore the argmax is much easier to find.",5 Results and Discussion,[0],[0]
"Seq-Inter offers a compromise between the two, with the greedily-decoded sequence accounting for 7.6% of the distribution.
",5 Results and Discussion,[0],[0]
"Finally, although past work has shown that models with lower perplexity generally tend to have
higher BLEU, our results indicate that this is not necessarily the case.",5 Results and Discussion,[0],[0]
"The perplexity of the baseline 2 × 500 English→ German model is 8.2 while the perplexity of the corresponding Seq-KD model is 22.7, despite the fact that Seq-KD model does significantly better for both greedy (+4.2 BLEU) and beam search (+1.4 BLEU) decoding.",5 Results and Discussion,[0],[0]
Run-time complexity for beam search grows linearly with beam size.,5.1 Decoding Speed,[0],[0]
"Therefore, the fact that sequencelevel knowledge distillation allows for greedy decoding is significant, with practical implications for running NMT systems across various devices.",5.1 Decoding Speed,[0],[0]
"To test the speed gains, we run the teacher/student models on GPU, CPU, and smartphone, and check the average number of source words translated per second (Table 2).",5.1 Decoding Speed,[0],[0]
We use a GeForce GTX Titan X for GPU and a Samsung Galaxy 6 smartphone.,5.1 Decoding Speed,[0],[0]
"We find that we can run the student model 10 times faster with greedy decoding than the teacher model with beam search on GPU (1051.3 vs 101.9 words/sec), with similar performance.",5.1 Decoding Speed,[0],[0]
"Although knowledge distillation enables training faster models, the number of parameters for the student models is still somewhat large (Table 1: Params), due to the word embeddings which dominate most of the parameters.10 For example, on the
10Word embeddings scale linearly while RNN parameters scale quadratically with the dimension size.
2 × 500 English → German model the word embeddings account for approximately 63% (50m out of 84m) of the parameters.",5.2 Weight Pruning,[0],[0]
"The size of word embeddings have little impact on run-time as the word embedding layer is a simple lookup table that only affects the first layer of the model.
",5.2 Weight Pruning,[0],[0]
We therefore focus next on reducing the memory footprint of the student models further through weight pruning.,5.2 Weight Pruning,[0],[0]
"Weight pruning for NMT was recently investigated by See et al. (2016), who found that up to 80 − 90% of the parameters in a large NMT model can be pruned with little loss in performance.",5.2 Weight Pruning,[0],[0]
We take our best English→ German student model (2× 500 Seq-KD + Seq-Inter) and prune x% of the parameters by removing the weights with the lowest absolute values.,5.2 Weight Pruning,[0],[0]
We then retrain the pruned model on Seq-KD data with a learning rate of 0.2 and fine-tune towards Seq-Inter data with a learning rate of 0.1.,5.2 Weight Pruning,[0],[0]
"As observed by See et al. (2016), retraining proved to be crucial.",5.2 Weight Pruning,[0],[0]
"The results are shown in Table 3.
",5.2 Weight Pruning,[0],[0]
Our findings suggest that compression benefits achieved through weight pruning and knowledge distillation are orthogonal.11 Pruning 80% of the weight in the 2 × 500 student model results in a model with 13× fewer parameters than the original teacher model with only a decrease of 0.4 BLEU.,5.2 Weight Pruning,[0],[0]
"While pruning 90% of the weights results in a more appreciable decrease of 1.0 BLEU, the model is
11To our knowledge combining pruning and knowledge distillation has not been investigated before.
drastically smaller with 8m parameters, which is 26× fewer than the original teacher model.",5.2 Weight Pruning,[0],[0]
"• For models trained with word-level knowledge
distillation, we also tried regressing the student network’s top-most hidden layer at each time step to the teacher network’s top-most hidden layer as a pretraining step, noting that Romero et al. (2015) obtained improvements with a similar technique on feed-forward models.",5.3 Further Observations,[0],[0]
"We found this to give comparable results to standard knowledge distillation and hence did not pursue this further.
",5.3 Further Observations,[0],[0]
"• There have been promising recent results on eliminating word embeddings completely and obtaining word representations directly from characters with character composition models, which have many fewer parameters than word embedding lookup tables (Ling et al., 2015a; Kim et al., 2016; Ling et al., 2015b; Jozefowicz et al., 2016; Costa-Jussa and Fonollosa, 2016).",5.3 Further Observations,[0],[0]
Combining such methods with knowledge distillation/pruning to further reduce the memory footprint of NMT systems remains an avenue for future work.,5.3 Further Observations,[0],[0]
Compressing deep learning models is an active area of current research.,6 Related Work,[0],[0]
Pruning methods involve pruning weights or entire neurons/nodes based on some criterion.,6 Related Work,[0],[0]
"LeCun et al. (1990) prune weights based on an approximation of the Hessian, while Han et al. (2016) show that a simple magnitude-based pruning works well.",6 Related Work,[0],[0]
Prior work on removing neurons/nodes include Srinivas and Babu (2015) and Mariet and Sra (2016).,6 Related Work,[0],[0]
"See et al. (2016) were the first to apply pruning to Neural Machine Translation, observing that that different parts of the architecture (input word embeddings, LSTM matrices, etc.) admit different levels of pruning.",6 Related Work,[0],[0]
"Knowledge distillation approaches train a smaller student model to mimic a larger teacher model, by minimizing the loss between the teacher/student predictions (Bucila et al., 2006; Ba and Caruana, 2014; Li et al., 2014; Hinton et al., 2015).",6 Related Work,[0],[0]
"Romero et al. (2015) additionally regress on the intermediate hidden layers of the
student/teacher network as a pretraining step, while Mou et al. (2015) obtain smaller word embeddings from a teacher model via regression.",6 Related Work,[0],[0]
There has also been work on transferring knowledge across different network architectures: Chan et al. (2015b) show that a deep non-recurrent neural network can learn from an RNN; Geras et al. (2016) train a CNN to mimic an LSTM for speech recognition.,6 Related Work,[0],[0]
"Kuncoro et al. (2016) recently investigated knowledge distillation for structured prediction by having a single parser learn from an ensemble of parsers.
",6 Related Work,[0],[0]
"Other approaches for compression involve low rank factorizations of weight matrices (Denton et al., 2014; Jaderberg et al., 2014; Lu et al., 2016; Prabhavalkar et al., 2016), sparsity-inducing regularizers (Murray and Chiang, 2015), binarization of weights (Courbariaux et al., 2016; Lin et al., 2016), and weight sharing (Chen et al., 2015; Han et al., 2016).",6 Related Work,[0],[0]
"Finally, although we have motivated sequence-level knowledge distillation in the context of training a smaller model, there are other techniques that train on a mixture of the model’s predictions and the data, such as local updating (Liang et al., 2006), hope/fear training (Chiang, 2012), SEARN (Daumé III et al., 2009), DAgger (Ross et al., 2011), and minimum risk training (Och, 2003; Shen et al., 2016).",6 Related Work,[0],[0]
"In this work we have investigated existing knowledge distillation methods for NMT (which work at the word-level) and introduced two sequence-level variants of knowledge distillation, which provide improvements over standard word-level knowledge distillation.
",7 Conclusion,[0],[0]
"We have chosen to focus on translation as this domain has generally required the largest capacity deep learning models, but the sequence-to-sequence framework has been successfully applied to a wide range of tasks including parsing (Vinyals et al., 2015a), summarization (Rush et al., 2015), dialogue (Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016), NER/POS-tagging (Gillick et al., 2016), image captioning (Vinyals et al., 2015b; Xu et al., 2015), video generation (Srivastava et al., 2015), and speech recognition (Chan et al., 2015a).",7 Conclusion,[0],[0]
We anticipate that methods described in this paper can be used to similarly train smaller models in other domains.,7 Conclusion,[0],[0]
Neural machine translation (NMT) offers a novel alternative formulation of translation that is potentially simpler than statistical approaches.,abstractText,[0],[0]
"However to reach competitive performance, NMT models need to be exceedingly large.",abstractText,[0],[0]
"In this paper we consider applying knowledge distillation approaches (Bucila et al., 2006; Hinton et al., 2015) that have proven successful for reducing the size of neural models in other domains to the problem of NMT.",abstractText,[0],[0]
"We demonstrate that standard knowledge distillation applied to word-level prediction can be effective for NMT, and also introduce two novel sequence-level versions of knowledge distillation that further improve performance, and somewhat surprisingly, seem to eliminate the need for beam search (even when applied on the original teacher model).",abstractText,[0],[0]
Our best student model runs 10 times faster than its state-of-the-art teacher with little loss in performance.,abstractText,[0],[0]
It is also significantly better than a baseline model trained without knowledge distillation: by 4.2/1.7 BLEU with greedy decoding/beam search.,abstractText,[0],[0]
"Applying weight pruning on top of knowledge distillation results in a student model that has 13× fewer parameters than the original teacher model, with a decrease of 0.4 BLEU.",abstractText,[0],[0]
Sequence-Level Knowledge Distillation,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 766–777 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
766",text,[0],[0]
"Semantic parsing aims to map natural language sentences to logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowski et al., 2013).",1 Introduction,[0],[0]
"For example, the sentence “Which states border Texas?” will be mapped to answer (A, (state (A), next to (A, stateid ( texas )))).
",1 Introduction,[0],[0]
"A semantic parser needs two functions, one for structure prediction and the other for semantic grounding.",1 Introduction,[0],[0]
"Traditional semantic parsers are usually based on compositional grammar, such as CCG (Zettlemoyer and Collins, 2005, 2007), DCS (Liang et al., 2011), etc.",1 Introduction,[0],[0]
"These parsers compose structure using manually designed grammars, use lexicons for semantic grounding, and exploit fea-
tures for candidate logical forms ranking.",1 Introduction,[0],[0]
"Unfortunately, it is challenging to design grammars and learn accurate lexicons, especially in wideopen domains.",1 Introduction,[0],[0]
"Moreover, it is often hard to design effective features, and its learning process is not end-to-end.",1 Introduction,[0],[0]
"To resolve the above problems, two promising lines of work have been proposed: Semantic graph-based methods and Seq2Seq methods.
",1 Introduction,[0],[0]
"Semantic graph-based methods (Reddy et al., 2014, 2016; Bast and Haussmann, 2015; Yih et al., 2015) represent the meaning of a sentence as a semantic graph (i.e., a sub-graph of a knowledge base, see example in Figure 1) and treat semantic parsing as a semantic graph matching/generation process.",1 Introduction,[0],[0]
"Compared with logical forms, semantic graphs have a tight-coupling with knowledge bases (Yih et al., 2015), and share many commonalities with syntactic structures (Reddy et al., 2014).",1 Introduction,[0],[0]
"Therefore both the structure and semantic constraints from knowledge bases can be easily exploited during parsing (Yih et al., 2015).",1 Introduction,[0],[0]
The main challenge of semantic graph-based parsing is how to effectively construct the semantic graph of a sentence.,1 Introduction,[0],[0]
"Currently, semantic graphs
are either constructed by matching with patterns (Bast and Haussmann, 2015), transforming from dependency tree (Reddy et al., 2014, 2016), or via a staged heuristic search algorithm (Yih et al., 2015).",1 Introduction,[0],[0]
"These methods are all based on manuallydesigned, heuristic construction processes, making them hard to handle open/complex situations.
",1 Introduction,[0],[0]
"In recent years, RNN models have achieved success in sequence-to-sequence problems due to its strong ability on both representation learning and prediction, e.g., in machine translation (Cho et al., 2014).",1 Introduction,[0],[0]
"A lot of Seq2Seq models have also been employed for semantic parsing (Xiao et al., 2016; Dong and Lapata, 2016; Jia and Liang, 2016), where a sentence is parsed by translating it to linearized logical form using RNN models.",1 Introduction,[0],[0]
"There is no need for high-quality lexicons, manually-built grammars, and hand-crafted features.",1 Introduction,[0],[0]
"These models are trained end-to-end, and can leverage attention mechanism (Bahdanau et al., 2014; Luong et al., 2015) to learn soft alignments between sentences and logical forms.
",1 Introduction,[0],[0]
"In this paper, we propose a new neural semantic parsing framework – Sequence-to-Action, which can simultaneously leverage the advantages of semantic graph representation and the strong prediction ability of Seq2Seq models.",1 Introduction,[0],[0]
"Specifically, we model semantic parsing as an end-to-end semantic graph generation process.",1 Introduction,[0],[0]
"For example in Figure 1, our model will parse the sentence “Which states border Texas” by generating a sequence of actions [add variable:A, add type:state, ...].",1 Introduction,[0],[0]
"To achieve the above goal, we first design an action set which can encode the generation process of semantic graph (including node actions such as add variable, add entity, add type, edge actions such as add edge, and operation actions such as argmin, argmax, count, sum, etc.).",1 Introduction,[0],[0]
And then we design a RNN model which can generate the action sequence for constructing the semantic graph of a sentence.,1 Introduction,[0],[0]
"Finally we further enhance parsing by incorporating both structure and semantic constraints during decoding.
",1 Introduction,[0],[0]
"Compared with the manually-designed, heuristic generation algorithms used in traditional semantic graph-based methods, our sequence-toaction method generates semantic graphs using a RNN model, which is learned end-to-end from training data.",1 Introduction,[0],[0]
"Such a learnable, end-to-end generation makes our approach more effective and can fit to different situations.
",1 Introduction,[0],[0]
"Compared with the previous Seq2Seq semantic parsing methods, our sequence-to-action model predicts a sequence of semantic graph generation actions, rather than linearized logical forms.",1 Introduction,[0],[0]
"We find that the action sequence encoding can better capture structure and semantic information, and is more compact.",1 Introduction,[0],[0]
And the parsing can be enhanced by exploiting structure and semantic constraints.,1 Introduction,[0],[0]
"For example, in GEO dataset, the action add edge:next to must subject to the semantic constraint that its arguments must be of type state and state, and the structure constraint that the edge next to must connect two nodes to form a valid graph.
",1 Introduction,[0],[0]
"We evaluate our approach on three standard datasets: GEO (Zelle and Mooney, 1996), ATIS (He and Young, 2005) and OVERNIGHT (Wang et al., 2015b).",1 Introduction,[0],[0]
"The results show that our method achieves state-of-the-art performance on OVERNIGHT dataset and gets competitive performance on GEO and ATIS datasets.
",1 Introduction,[0],[0]
"The main contributions of this paper are summarized as follows:
• We propose a new semantic parsing framework – Sequence-to-Action, which models semantic parsing as an end-to-end semantic graph generation process.",1 Introduction,[0],[0]
"This new framework can synthesize the advantages of semantic graph representation and the prediction ability of Seq2Seq models.
",1 Introduction,[0],[0]
"• We design a sequence-to-action model, including an action set encoding for semantic graph generation and a Seq2Seq RNN model for action sequence prediction.",1 Introduction,[0],[0]
We further enhance the parsing by exploiting structure and semantic constraints during decoding.,1 Introduction,[0],[0]
Experiments validate the effectiveness of our method.,1 Introduction,[0],[0]
"Given a sentence X = x1, ..., x|X|, our sequenceto-action model generates a sequence of actions Y = y1, ..., y|Y | for constructing the correct semantic graph.",2 Sequence-to-Action Model for End-to-End Semantic Graph Generation,[0],[0]
Figure 2 shows an example.,2 Sequence-to-Action Model for End-to-End Semantic Graph Generation,[0],[0]
"The conditional probability P (Y |X) used in our
model is decomposed as follows:
P (Y |X) =",2 Sequence-to-Action Model for End-to-End Semantic Graph Generation,[0],[0]
"|Y |∏ t=1 P (yt|y<t, X) (1)
where y<t = y1, ..., yt−1.",2 Sequence-to-Action Model for End-to-End Semantic Graph Generation,[0],[0]
"To achieve the above goal, we need: 1) an action set which can encode semantic graph generation process; 2) an encoder which encodes natural language input X into a vector representation, and a decoder which generates y1, ..., y|Y | conditioned on the encoding vector.",2 Sequence-to-Action Model for End-to-End Semantic Graph Generation,[0],[0]
In following we describe them in detail.,2 Sequence-to-Action Model for End-to-End Semantic Graph Generation,[0],[0]
"Generally, a semantic graph consists of nodes (including variables, entities, types) and edges (semantic relations), with some universal operations (e.g., argmax, argmin, count, sum, and not).",2.1 Actions for Semantic Graph Generation,[0],[0]
"To generate a semantic graph, we define six types of actions as follows:
Add Variable Node: This kind of actions denotes adding a variable node to semantic graph.",2.1 Actions for Semantic Graph Generation,[0],[0]
"In most cases a variable node is a return node (e.g., which, what), but can also be an intermediate variable node.",2.1 Actions for Semantic Graph Generation,[0],[0]
"We represent this kind of action as add variable:A, where A is the identifier of the variable node.
",2.1 Actions for Semantic Graph Generation,[0],[0]
"Add Entity Node: This kind of actions denotes adding an entity node (e.g., Texas, New York) and is represented as add entity node:texas.",2.1 Actions for Semantic Graph Generation,[0],[0]
"An entity node corresponds to an entity in knowledge bases.
",2.1 Actions for Semantic Graph Generation,[0],[0]
"Add Type Node: This kind of actions denotes adding a type node (e.g., state, city).",2.1 Actions for Semantic Graph Generation,[0],[0]
"We represent them as add type node:state.
",2.1 Actions for Semantic Graph Generation,[0],[0]
Add Edge: This kind of actions denotes adding an edge between two nodes.,2.1 Actions for Semantic Graph Generation,[0],[0]
An edge is a binary relation in knowledge bases.,2.1 Actions for Semantic Graph Generation,[0],[0]
"This kind of actions is represented as add edge:next to.
",2.1 Actions for Semantic Graph Generation,[0],[0]
Operation Action:,2.1 Actions for Semantic Graph Generation,[0],[0]
This kind of actions denotes adding an operation.,2.1 Actions for Semantic Graph Generation,[0],[0]
"An operation can be argmax, argmin, count, sum, not, et al. Because each operation has a scope, we define two actions for an operation, one is operation start action, represented as start operation:most, and the other is operation end action, represented as end operation:most.",2.1 Actions for Semantic Graph Generation,[0],[0]
"The subgraph within the start and end operation actions is its scope.
",2.1 Actions for Semantic Graph Generation,[0],[0]
Argument Action:,2.1 Actions for Semantic Graph Generation,[0],[0]
Some above actions need argument information.,2.1 Actions for Semantic Graph Generation,[0],[0]
"For example, which nodes the add edge:next to action should connect to.",2.1 Actions for Semantic Graph Generation,[0],[0]
"In this paper, we design argument actions for add type, add edge and operation actions, and the argument actions should be put directly after its main action.
",2.1 Actions for Semantic Graph Generation,[0],[0]
"For add type actions, we put an argument action to indicate which node this type node should constrain.",2.1 Actions for Semantic Graph Generation,[0],[0]
The argument can be a variable node or an entity node.,2.1 Actions for Semantic Graph Generation,[0],[0]
"An argument action for a type node is represented as arg:A.
For add edge action, we use two argument actions: arg1 node and arg2 node, and they are represented as arg1 node:A and arg2 node:B.
We design argument actions for different operations.",2.1 Actions for Semantic Graph Generation,[0],[0]
"For operation:sum, there are three arguments: arg-for, arg-in and arg-return.",2.1 Actions for Semantic Graph Generation,[0],[0]
"For operation:count, they are arg-for and arg-return.",2.1 Actions for Semantic Graph Generation,[0],[0]
"There are two arg-for arguments for operation:most.
",2.1 Actions for Semantic Graph Generation,[0],[0]
"We can see that each action encodes both structure and semantic information, which makes it easy to capture more information for parsing and can be tightly coupled with knowledge base.",2.1 Actions for Semantic Graph Generation,[0],[0]
"Furthermore, we find that action sequence encoding is more compact than linearized logical form (See Section 4.4 for more details).",2.1 Actions for Semantic Graph Generation,[0],[0]
"Based on the above action encoding mechanism, this section describes our encoder-decoder model for mapping sentence to action sequence.",2.2 Neural Sequence-to-Action Model,[0],[0]
"Specifically, similar to the RNN model in Jia and Liang (2016), this paper employs the attentionbased sequence-to-sequence RNN model.",2.2 Neural Sequence-to-Action Model,[0],[0]
Figure 3 presents the overall structure.,2.2 Neural Sequence-to-Action Model,[0],[0]
Encoder:,2.2 Neural Sequence-to-Action Model,[0],[0]
"The encoder converts the input sequence x1, ..., xm to a sequence of contextsensitive vectors b1, ..., bm using a bidirectional RNN (Bahdanau et al., 2014).",2.2 Neural Sequence-to-Action Model,[0],[0]
"Firstly each word xi is mapped to its embedding vector, then these vectors are fed into a forward RNN and a backward RNN.",2.2 Neural Sequence-to-Action Model,[0],[0]
"The sequence of hidden states h1, ..., hm are generated by recurrently applying the recurrence:
hi = LSTM(φ (x)(xi), hi−1).",2.2 Neural Sequence-to-Action Model,[0],[0]
"(2)
The recurrence takes the form of LSTM (Hochreiter and Schmidhuber, 1997).",2.2 Neural Sequence-to-Action Model,[0],[0]
"Finally, for each input position i, we define its context-sensitive embedding as bi =",2.2 Neural Sequence-to-Action Model,[0],[0]
"[hFi , h B i ].",2.2 Neural Sequence-to-Action Model,[0],[0]
"Decoder: This paper uses the classical attentionbased decoder (Bahdanau et al., 2014), which generates action sequence y1, ..., yn, one action at a time.",2.2 Neural Sequence-to-Action Model,[0],[0]
"At each time step j, it writes yj based on the current hidden state sj , then updates the hidden state to sj+1 based on sj and yj .",2.2 Neural Sequence-to-Action Model,[0],[0]
"The decoder is formally defined by the following equations:
s1 = tanh(W (s)[hFm, h B 1 ]) (3)
eji = s T j W (a)bi (4) aji = exp(eji)∑m
i′=1 exp(eji′ ) (5)
cj =",2.2 Neural Sequence-to-Action Model,[0],[0]
m∑ i=1,2.2 Neural Sequence-to-Action Model,[0],[0]
"ajibi (6) P (yj = w|x, y1:j−1) ∝",2.2 Neural Sequence-to-Action Model,[0],[0]
"exp(Uw[sj , cj ]) (7) sj+1 = LSTM([φ (y)(yj), cj ], sj) (8)
where the normalized attention scores aji defines the probability distribution over input words, indicating the attention probability on input word i at time j; eji is un-normalized attention score.",2.2 Neural Sequence-to-Action Model,[0],[0]
"To incorporate constraints during decoding, an extra controller component is added and its details will be described in Section 3.3.",2.2 Neural Sequence-to-Action Model,[0],[0]
Action Embedding.,2.2 Neural Sequence-to-Action Model,[0],[0]
The above decoder needs the embedding of each action.,2.2 Neural Sequence-to-Action Model,[0],[0]
"As described above, each action has two parts, one for structure (e.g., add edge), and the other for semantic (e.g., next to).",2.2 Neural Sequence-to-Action Model,[0],[0]
"As a result, actions may share the same structure or semantic part, e.g., add edge:next to and add edge:loc have the same structure part, and add node:A and arg node:A have the same semantic part.",2.2 Neural Sequence-to-Action Model,[0],[0]
"To make parameters more compact, we first embed the structure part and the semantic part independently, then concatenate them to get the final embedding.",2.2 Neural Sequence-to-Action Model,[0],[0]
"For instance, φ(y)(add edge:next to ) =",2.2 Neural Sequence-to-Action Model,[0],[0]
"[ φ(y)strut( add edge ), φ (y) sem( next to )].",2.2 Neural Sequence-to-Action Model,[0],[0]
The action embeddings φ(y) are learned during training.,2.2 Neural Sequence-to-Action Model,[0],[0]
"In this section, we describe how to build a neural semantic parser using sequence-to-action model.",3 Constrained Semantic Parsing using Sequence-to-Action Model,[0],[0]
"We first describe the training and the inference of our model, and then introduce how to incorporate structure and semantic constraints during decoding.",3 Constrained Semantic Parsing using Sequence-to-Action Model,[0],[0]
Parameter Estimation.,3.1 Training,[0],[0]
"The parameters of our model include RNN parameters W (s), W (a), Uw, word embeddings φ(x), and action embeddings φ(y).",3.1 Training,[0],[0]
We estimate these parameters from training data.,3.1 Training,[0],[0]
"Given a training example with a sentence X and its action sequence Y , we maximize the likelihood of the generated sequence of actions given X .",3.1 Training,[0],[0]
"The objective function is:
n∑ i=1",3.1 Training,[0],[0]
"logP (Yi|Xi) (9)
Standard stochastic gradient descent algorithm is employed to update parameters.",3.1 Training,[0],[0]
Logical Form to Action Sequence.,3.1 Training,[0],[0]
"Currently, most datasets of semantic parsing are labeled with logical forms.",3.1 Training,[0],[0]
"In order to train our model, we
convert logical forms to action sequences using semantic graph as an intermediate representation (See Figure 4 for an overview).",3.1 Training,[0],[0]
"Concretely, we transform logical forms into semantic graphs using a depth-first-search algorithm from root, and then generate the action sequence using the same order.",3.1 Training,[0],[0]
"Specifically, entities, variables and types are nodes; relations are edges.",3.1 Training,[0],[0]
Conversely we can convert action sequence to logical form similarly.,3.1 Training,[0],[0]
"Based on the above algorithm, action sequences can be transformed into logical forms in a deterministic way, and the same for logical forms to action sequences.",3.1 Training,[0],[0]
Mechanisms for Handling Entities.,3.1 Training,[0],[0]
"Entities play an important role in semantic parsing (Yih et al., 2015).",3.1 Training,[0],[0]
"In Dong and Lapata (2016), entities are replaced with their types and unique IDs.",3.1 Training,[0],[0]
"In Jia and Liang (2016), entities are generated via attention-based copying mechanism helped with a lexicon.",3.1 Training,[0],[0]
This paper implements both mechanisms and compares them in experiments.,3.1 Training,[0],[0]
"Given a new sentence X , we predict action sequence by:
Y ∗ = argmax Y P (Y |X) (10)
where Y represents action sequence, and P (Y |X) is computed using Formula (1).",3.2 Inference,[0],[0]
Beam search is used for best action sequence decoding.,3.2 Inference,[0],[0]
Semantic graph and logical form can be derived from Y ∗ as described in above.,3.2 Inference,[0],[0]
"For decoding, we generate action sequentially.",3.3 Incorporating Constraints in Decoding,[0],[0]
"It is obviously that the next action has a strong correlation with the partial semantic graph generated to current, and illegal actions can be filtered using structure and semantic constraints.",3.3 Incorporating Constraints in Decoding,[0],[0]
"Specifically, we incorporate constraints in decoding using a controller.",3.3 Incorporating Constraints in Decoding,[0],[0]
"This procedure has two steps: 1) the controller constructs partial semantic graph using the actions generated to current; 2) the controller checks whether a new generated action can meet
all structure/semantic constraints using the partial semantic graph.
",3.3 Incorporating Constraints in Decoding,[0],[0]
Structure Constraints.,3.3 Incorporating Constraints in Decoding,[0],[0]
The structure constraints ensure action sequence will form a connected acyclic graph.,3.3 Incorporating Constraints in Decoding,[0],[0]
"For example, there must be two argument nodes for an edge, and the two argument nodes should be different (The third candidate next action in Figure 5 violates this constraint).",3.3 Incorporating Constraints in Decoding,[0],[0]
This kind of constraints are domain-independent.,3.3 Incorporating Constraints in Decoding,[0],[0]
"The controller encodes structure constraints as a set of rules.
",3.3 Incorporating Constraints in Decoding,[0],[0]
Semantic Constraints.,3.3 Incorporating Constraints in Decoding,[0],[0]
The semantic constraints ensure the constructed graph must follow the schema of knowledge bases.,3.3 Incorporating Constraints in Decoding,[0],[0]
"Specifically, we model two types of semantic constraints.",3.3 Incorporating Constraints in Decoding,[0],[0]
One is selectional preference constraints where the argument types of a relation should follow knowledge base schemas.,3.3 Incorporating Constraints in Decoding,[0],[0]
"For example, in GEO dataset, relation next to’s arg1 and arg2 should both be a state.",3.3 Incorporating Constraints in Decoding,[0],[0]
"The second is type conflict constraints, i.e., an entity/variable node’s type must be consistent, i.e., a node cannot be both of type city and state.",3.3 Incorporating Constraints in Decoding,[0],[0]
Semantic constraints are domain-specific and are automatically extracted from knowledge base schemas.,3.3 Incorporating Constraints in Decoding,[0],[0]
The controller encodes semantic constraints as a set of rules.,3.3 Incorporating Constraints in Decoding,[0],[0]
"In this section, we assess the performance of our method and compare it with previous methods.",4 Experiments,[0],[0]
"We conduct experiments on three standard datasets: GEO, ATIS and OVERNIGHT.",4.1 Datasets,[0],[0]
GEO contains natural language questions about US geography paired with corresponding Prolog database queries.,4.1 Datasets,[0],[0]
"Following Zettlemoyer and Collins (2005), we use the standard 600/280 instance splits for training/test.",4.1 Datasets,[0],[0]
"ATIS contains natural language questions of a flight database, with each question is annotated with a lambda calculus query.",4.1 Datasets,[0],[0]
"Following Zettlemoyer and Collins (2007), we use the standard 4473/448 instance splits for training/test.",4.1 Datasets,[0],[0]
OVERNIGHT contains natural language paraphrases paired with logical forms across eight domains.,4.1 Datasets,[0],[0]
We evaluate on the standard train/test splits as Wang et al. (2015b).,4.1 Datasets,[0],[0]
Following the experimental setup of Jia and Liang (2016): we use 200 hidden units and 100- dimensional word vectors for sentence encoding.,4.2 Experimental Settings,[0],[0]
The dimensions of action embedding are tuned on validation datasets for each corpus.,4.2 Experimental Settings,[0],[0]
We initialize all parameters by uniformly sampling within the interval,4.2 Experimental Settings,[0],[0]
"[-0.1, 0.1].",4.2 Experimental Settings,[0],[0]
"We train our model for a total of 30 epochs with an initial learning rate of 0.1, and halve the learning rate every 5 epochs after epoch 15.",4.2 Experimental Settings,[0],[0]
We replace word vectors for words occurring only once with an universal word vector.,4.2 Experimental Settings,[0],[0]
The beam size is set as 5.,4.2 Experimental Settings,[0],[0]
"Our model is implemented in Theano (Bergstra et al., 2010), and the codes and settings are released on Github: https://github.com/dongpobeyond/Seq2Act.
",4.2 Experimental Settings,[0],[0]
"We evaluate different systems using the standard accuracy metric, and the accuracies on different datasets are obtained as same as Jia and Liang (2016).",4.2 Experimental Settings,[0],[0]
We compare our method with state-of-the-art systems on all three datasets.,4.3 Overall Results,[0],[0]
"Because all systems using the same training/test splits, we directly use the reported best performances from their original papers for fair comparison.
",4.3 Overall Results,[0],[0]
"For our method, we train our model with three settings: the first one is the basic sequence-toaction model without constraints – Seq2Act; the second one adds structure constraints in decoding – Seq2Act (+C1); the third one is the full model which adds both structure and semantic
constraints – Seq2Act (+C1+C2).",4.3 Overall Results,[0],[0]
Semantic constraints (C2) are stricter than structure constraints (C1).,4.3 Overall Results,[0],[0]
Therefore we set that C1 should be first met for C2 to be met.,4.3 Overall Results,[0],[0]
So in our experiments we add constraints incrementally.,4.3 Overall Results,[0],[0]
The overall results are shown in Table 1-2.,4.3 Overall Results,[0],[0]
"From the overall results, we can see that:
1) By synthetizing the advantages of semantic graph representation and the prediction ability of Seq2Seq model, our method achieves stateof-the-art performance on OVERNIGHT dataset, and gets competitive performance on GEO and ATIS dataset.",4.3 Overall Results,[0],[0]
"In fact, on GEO our full model (Seq2Act+C1+C2) also gets the best test accuracy of 88.9 if under the same settings, which only falls behind Liang et al. (2011)* which uses extra handcrafted lexicons and Jia and Liang (2016)* which uses extra augmented training data.",4.3 Overall Results,[0],[0]
"On ATIS our full model gets the second best test accuracy of 85.5, which only falls behind Rabinovich et al. (2017) which uses a supervised attention strategy.",4.3 Overall Results,[0],[0]
"On OVERNIGHT, our full model gets state-of-theart accuracy of 79.0, which even outperforms Jia and Liang (2016)* with extra augmented training data.
",4.3 Overall Results,[0],[0]
2),4.3 Overall Results,[0],[0]
"Compared with the linearized logical form representation used in previous Seq2Seq baselines, our action sequence encoding is more effective for semantic parsing.",4.3 Overall Results,[0],[0]
"On all three datasets,
our basic Seq2Act model gets better results than all Seq2Seq baselines.",4.3 Overall Results,[0],[0]
"On GEO, the Seq2Act model achieve test accuracy of 87.5, better than the best accuracy 87.1 of Seq2Seq baseline.",4.3 Overall Results,[0],[0]
"On ATIS, the Seq2Act model obtains a test accuracy of 84.6, the same as the best Seq2Seq baseline.",4.3 Overall Results,[0],[0]
"On OVERNGIHT, the Seq2Act model gets a test accuracy of 78.0, better than the best Seq2Seq baseline gets 77.5.",4.3 Overall Results,[0],[0]
"We argue that this is because our action sequence encoding is more compact and can capture more information.
3) Structure constraints can enhance semantic parsing by ensuring the validity of graph using the generated action sequence.",4.3 Overall Results,[0],[0]
"In all three datasets, Seq2Act (+C1) outperforms the basic Seq2Act model.",4.3 Overall Results,[0],[0]
"This is because a part of illegal actions will be filtered during decoding.
4) By leveraging knowledge base schemas during decoding, semantic constraints are effective for semantic parsing.",4.3 Overall Results,[0],[0]
"Compared to Seq2Act and Seq2Act (+C1), the Seq2Act (+C1+C2) gets the best performance on all three datasets.",4.3 Overall Results,[0],[0]
This is because semantic constraints can further filter semantic illegal actions using selectional preference and consistency between types.,4.3 Overall Results,[0],[0]
Effect of Entity Handling Mechanisms.,4.4 Detailed Analysis,[0],[0]
"This paper implements two entity handling mechanisms – Replacing (Dong and Lapata, 2016) which identifies entities and then replaces them with their types and IDs, and attention-based Copying (Jia and Liang, 2016).",4.4 Detailed Analysis,[0],[0]
"To compare the above two mechanisms, we train and test with our full model and the results are shown in Table 3.",4.4 Detailed Analysis,[0],[0]
"We can see that, Replacing mechanism outperforms Copying in all three datasets.",4.4 Detailed Analysis,[0],[0]
"This is because Replacing is done
in preprocessing, while attention-based Copying is done during parsing and needs additional copy mechanism.",4.4 Detailed Analysis,[0],[0]
Linearized Logical Form vs. Action Sequence.,4.4 Detailed Analysis,[0],[0]
Table 4 shows the average length of linearized logical forms used in previous Seq2Seq models and the action sequences of our model on all three datasets.,4.4 Detailed Analysis,[0],[0]
"As we can see, action sequence encoding is more compact than linearized logical form encoding: action sequence is shorter on all three datasets, 35.5%, 9.2% and 28.5% reduction in length respectively.",4.4 Detailed Analysis,[0],[0]
The main advantage of a shorter/compact encoding is that it will reduce the influence of long distance dependency problem.,4.4 Detailed Analysis,[0],[0]
We perform error analysis on results and find there are mainly two types of errors.,4.5 Error Analysis,[0],[0]
Unseen/Informal Sentence Structure.,4.5 Error Analysis,[0],[0]
Some test sentences have unseen syntactic structures.,4.5 Error Analysis,[0],[0]
"For example, the first case in Table 5 has an unseen
and informal structure, where entity word “Iowa” and relation word “borders” appear ahead of the question words “how many”.",4.5 Error Analysis,[0],[0]
"For this problem, we can employ sentence rewriting or paraphrasing techniques (Chen et al., 2016; Dong et al., 2017) to transform unseen sentence structures into normal ones.",4.5 Error Analysis,[0],[0]
Under-Mapping.,4.5 Error Analysis,[0],[0]
"As Dong and Lapata (2016) discussed, the attention model does not take the alignment history into consideration, makes some words are ignored during parsing.",4.5 Error Analysis,[0],[0]
"For example in the second case in Table 5, “first class” is ignored during the decoding process.",4.5 Error Analysis,[0],[0]
"This problem can be further solved using explicit word coverage models used in neural machine translation (Tu et al., 2016; Cohn et al., 2016)",4.5 Error Analysis,[0],[0]
"Semantic parsing has received significant attention for a long time (Kate and Mooney, 2006; Clarke et al., 2010; Krishnamurthy and Mitchell, 2012; Artzi and Zettlemoyer, 2013; Berant and Liang, 2014; Quirk et al., 2015; Artzi et al., 2015; Reddy et al., 2017).",5 Related Work,[0],[0]
"Traditional methods are mostly based on the principle of compositional semantics, which first trigger predicates using lexicons and then compose them using grammars.",5 Related Work,[0],[0]
"The prominent grammars include SCFG (Wong and Mooney, 2007; Li et al., 2015), CCG (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011; Cai and Yates, 2013), DCS (Liang et al., 2011; Berant et al., 2013), etc.",5 Related Work,[0],[0]
"As discussed above, the main drawback of grammar-based methods is that they rely on high-quality lexicons, manually-built grammars, and hand-crafted features.
",5 Related Work,[0],[0]
"In recent years, one promising direction of semantic parsing is to use semantic graph as representation.",5 Related Work,[0],[0]
Thus semantic parsing is modeled as a semantic graph generation process.,5 Related Work,[0],[0]
"Ge and Mooney (2009) build semantic graph by trans-
forming syntactic tree.",5 Related Work,[0],[0]
Bast and Haussmann (2015) identify the structure of a semantic query using three pre-defined patterns.,5 Related Work,[0],[0]
"Reddy et al. (2014, 2016) use Freebase-based semantic graph representation, and convert sentences to semantic graphs using CCG or dependency tree.",5 Related Work,[0],[0]
Yih et al. (2015) generate semantic graphs using a staged heuristic search algorithm.,5 Related Work,[0],[0]
"These methods are all based on manually-designed, heuristic generation process, which may suffer from syntactic parse errors (Ge and Mooney, 2009; Reddy et al., 2014, 2016), structure mismatch (Chen et al., 2016), and are hard to deal with complex sentences (Yih et al., 2015).
",5 Related Work,[0],[0]
"One other direction is to employ neural Seq2Seq models, which models semantic parsing as an end-to-end, sentence to logical form machine translation problem.",5 Related Work,[0],[0]
"Dong and Lapata (2016), Jia and Liang (2016) and Xiao et al. (2016) transform word sequence to linearized logical forms.",5 Related Work,[0],[0]
One main drawback of these methods is that it is hard to capture and exploit structure and semantic constraints using linearized logical forms.,5 Related Work,[0],[0]
"Dong and Lapata (2016) propose a Seq2Tree model to capture the hierarchical structure of logical forms.
",5 Related Work,[0],[0]
It has been shown that structure and semantic constraints are effective for enhancing semantic parsing.,5 Related Work,[0],[0]
Krishnamurthy et al. (2017) use type constraints to filter illegal tokens.,5 Related Work,[0],[0]
Liang et al. (2017) adopt a Lisp interpreter with pre-defined functions to produce valid tokens.,5 Related Work,[0],[0]
Iyyer et al. (2017) adopt type constraints to generate valid actions.,5 Related Work,[0],[0]
"Inspired by these approaches, we also incorporate both structure and semantic constraints in our neural sequence-to-action model.
",5 Related Work,[0],[0]
"Transition-based approaches are important in both dependency parsing (Nivre, 2008; Henderson et al., 2013) and AMR parsing (Wang et al., 2015a).",5 Related Work,[0],[0]
"In semantic parsing, our method has a tight-coupling with knowledge bases, and con-
straints can be exploited for more accurate decoding.",5 Related Work,[0],[0]
"We believe this can also be used to enhance previous transition based methods and may also be used in other parsing tasks, e.g., AMR parsing.",5 Related Work,[0],[0]
"This paper proposes Sequence-to-Action, a method which models semantic parsing as an end-to-end semantic graph generation process.",6 Conclusions,[0],[0]
"By leveraging the advantages of semantic graph representation and exploiting the representation learning and prediction ability of Seq2Seq models, our method achieved significant performance improvements on three datasets.",6 Conclusions,[0],[0]
"Furthermore, structure and semantic constraints can be easily incorporated in decoding to enhance semantic parsing.
",6 Conclusions,[0],[0]
"For future work, to solve the problem of the lack of training data, we want to design weakly supervised learning algorithm using denotations (QA pairs) as supervision.",6 Conclusions,[0],[0]
"Furthermore, we want to collect labeled data by designing an interactive UI for annotation assist like (Yih et al., 2016), which uses semantic graphs to annotate the meaning of sentences, since semantic graph is more natural and can be easily annotated without the need of expert knowledge.",6 Conclusions,[0],[0]
This research work is supported by the National Key Research and Development Program of China under Grant No.2017YFB1002104; and the National Natural Science Foundation of China under Grants no. 61572477 and 61772505.,Acknowledgments,[0],[0]
"Moreover, we sincerely thank the reviewers for their valuable comments.",Acknowledgments,[0],[0]
"This paper proposes a neural semantic parsing approach – Sequence-to-Action, which models semantic parsing as an endto-end semantic graph generation process.",abstractText,[0],[0]
Our method simultaneously leverages the advantages from two recent promising directions of semantic parsing.,abstractText,[0],[0]
"Firstly, our model uses a semantic graph to represent the meaning of a sentence, which has a tight-coupling with knowledge bases.",abstractText,[0],[0]
"Secondly, by leveraging the powerful representation learning and prediction ability of neural network models, we propose a RNN model which can effectively map sentences to action sequences for semantic graph generation.",abstractText,[0],[0]
Experiments show that our method achieves state-of-the-art performance on OVERNIGHT dataset and gets competitive performance on GEO and ATIS datasets.,abstractText,[0],[0]
Sequence-to-Action: End-to-End Semantic Graph Generation for Semantic Parsing,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 698–707 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1065",text,[0],[0]
"Recently, Neural Machine Translation (NMT) with the attention-based encoder-decoder framework (Bahdanau et al., 2015) has achieved significant improvements in translation quality of many language pairs (Bahdanau et al., 2015; Luong et al., 2015a; Tu et al., 2016; Wu et al., 2016).",1 Introduction,[0],[0]
"In a conventional NMT model, an encoder reads in source sentences of various lengths, and transforms them into sequences of intermediate hidden vector representations.",1 Introduction,[0],[0]
"After weighted by attention operations, combined hidden vectors are used by the decoder to generate translations.",1 Introduction,[0],[0]
"In most of cases, both encoder and decoder are implemented as recurrent neural networks (RNNs).
",1 Introduction,[0],[0]
"∗Contribution during internship at Microsoft Research.
",1 Introduction,[0],[0]
Many methods have been proposed to further improve the sequence-to-sequence NMT model since it was first proposed by Sutskever et al. (2014) and Bahdanau et al. (2015).,1 Introduction,[0],[0]
"Previous work ranges from addressing the problem of out-ofvocabulary words (Jean et al., 2015), designing attention mechanism (Luong et al., 2015a), to more efficient parameter learning (Shen et al., 2016), using source-side syntactic trees for better encoding (Eriguchi et al., 2016) and so on.",1 Introduction,[0],[0]
All these NMT models employ a sequential recurrent neural network for target generations.,1 Introduction,[0],[0]
"Although in theory RNN is able to remember sufficiently long history, we still observe substantial incorrect translations which violate long-distance syntactic constraints.",1 Introduction,[0],[0]
This suggests that it is still very challenging for a linear RNN to learn models that effectively capture many subtle long-range word dependencies.,1 Introduction,[0],[0]
"For example, Figure 1 shows an incorrect translation related to the long-distance dependency.",1 Introduction,[0],[0]
"The translation fragment in italic is locally fluent around the word is, but from a global view the translation is ungrammatical.",1 Introduction,[0],[0]
"Actually, this part of translation should be mostly affected by the distant plural noun foreigners rather than words Venezuelan government nearby.
",1 Introduction,[0],[0]
"Fortunately, such long-distance word correspondence can be well addressed and modeled by syntactic dependency trees.",1 Introduction,[0],[0]
"In Figure 1, the head word foreigners in the partial dependency tree (top dashed box) can provide correct structural context for the next target word, with this information it is more likely to generate the correct word will rather than is.",1 Introduction,[0],[0]
"This structure has been successfully applied to significantly improve the performance of statistical machine translation (Shen et al., 2008).",1 Introduction,[0],[0]
"On the NMT side, introducing target syntactic structures could help solve the problem of ungrammatical output because it can bring two advantages over state-of-the-art NMT models:
698
a) syntactic trees can be used to model the grammatical validity of translation candidates; b) partial syntactic structures can be used as additional context to facilitate future target word prediction.
",1 Introduction,[0],[0]
"However, it is not trivial to build and leverage syntactic structures on the target side in current NMT framework.",1 Introduction,[0],[0]
"Several practical challenges arise:
(1) How to model syntactic structures such as dependency parse trees with recurrent neural network;
(2) How to efficiently perform both target word generation and syntactic structure construction tasks simultaneously in a single neural network;
(3) How to effectively leverage target syntactic context to help target word generation.
",1 Introduction,[0],[0]
"To address these issues, we propose and empirically evaluate a novel Sequence-to-Dependency Neural Machine Translation (SD-NMT) model in our paper.",1 Introduction,[0],[0]
"An SD-NMT model encodes source inputs with bi-directional RNNs and associates them with target word prediction via attention mechanism as in most NMT models, but it comes with a new decoder which is able to jointly generate target translations and construct their syntactic dependency trees.",1 Introduction,[0],[0]
"The key difference from conventional NMT decoders is that we use two RNNs, one for translation generation and the other for dependency parse tree construction, in which incremental parsing is performed with the arc-standard shift-reduce algorithm proposed by Nivre (2004).
",1 Introduction,[0],[0]
"We will describe in detail how these two RNNs work interactively in Section 3.
",1 Introduction,[0],[0]
We evaluate our method on publicly available data sets with Chinese-English and JapaneseEnglish translation tasks.,1 Introduction,[0],[0]
Experimental results show that our model significantly improves translation accuracy over the conventional NMT and SMT baseline systems.,1 Introduction,[0],[0]
"As a new paradigm to machine translation, NMT is an end-to-end framework (Sutskever et al., 2014; Bahdanau et al., 2015) which directly models the conditional probability P (Y |X) of target translation Y = y1,y2,...,yn given source sentence X = x1,x2,...,xm.",2.1 Neural Machine Translation,[0],[0]
An NMT model consists of two parts: an encoder and a decoder.,2.1 Neural Machine Translation,[0],[0]
"Both of them utilize recurrent neural networks which can be a Gated Recurrent Unit (GRU) (Cho et al., 2014) or a Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) in practice.",2.1 Neural Machine Translation,[0],[0]
"The encoder bidirectionally encodes a source sentence into a sequence of hidden vectorsH = h1,h2,...,hm with a forward RNN and a backward RNN.",2.1 Neural Machine Translation,[0],[0]
"Then the decoder predicts target words one by one with probability
P (Y |X) = n∏
j=1
P (yj|y<j, H) (1)
Typically, for the jth target word, the probability P (yj |y<j , H) is computed as
P (yj|y<j, H) = g(sj, yj−1, cj) (2)
where g is a nonlinear function that outputs the probability of yj , and sj is the RNN hidden state.",2.1 Neural Machine Translation,[0],[0]
"The context cj is calculated at each timestamp j based on H by the attention network
cj = m∑
k=1
ajkhk (3)
ajk = exp(ejk)∑m i=1 exp(eji)
(4)
ejk = v T a tanh(Wasj−1 + Uahk) (5)
where va, Wa, Ua are the weight matrices.",2.1 Neural Machine Translation,[0],[0]
The attention mechanism is effective to model the correspondences between source and target.,2.1 Neural Machine Translation,[0],[0]
We use a shift-reduce transition-based dependency parser to build the syntactic structure for the target language in our work.,2.2 Dependency Tree Construction,[0],[0]
"Specially, we adopt the arcstandard algorithm (Nivre, 2004) to perform incremental parsing during the translation process.",2.2 Dependency Tree Construction,[0],[0]
"In this algorithm, a stack and a buffer are maintained to store the parsing state over which three kinds of transition actions are applied.",2.2 Dependency Tree Construction,[0],[0]
"Let w0 and w1 be two topmost words in the stack, and w̄ be the current new word in a sequence of input, three transition actions are described as below.
",2.2 Dependency Tree Construction,[0],[0]
"• Shift(SH) : Push w̄ to the stack.
",2.2 Dependency Tree Construction,[0],[0]
• Left-Reduce(LR(d)),2.2 Dependency Tree Construction,[0],[0]
": Link w0 and w1 with dependency label d as w0
d−→w1, and reduce them to the head w0.
",2.2 Dependency Tree Construction,[0],[0]
•,2.2 Dependency Tree Construction,[0],[0]
Right-Reduce(RR(d)),2.2 Dependency Tree Construction,[0],[0]
": Link w0 andw1 with dependency label d as w0
d←−w1, and reduce them to the head w1.
",2.2 Dependency Tree Construction,[0],[0]
"During parsing, an specific structure is used to record the dependency relationship between different words of input sentence.",2.2 Dependency Tree Construction,[0],[0]
The parsing finishes when the stack is empty and all input words are consumed.,2.2 Dependency Tree Construction,[0],[0]
"As each word must be pushed to the stack once and popped off once, the number of actions needed to parse a sentence is always 2n, where n is the length of the sentence (Nivre, 2004).",2.2 Dependency Tree Construction,[0],[0]
"Because each valid transition action sequence corresponds to a unique dependency tree, a dependency tree can also be equivalently represented by a sequence of transition actions.",2.2 Dependency Tree Construction,[0],[0]
An SD-NMT model is an extension to the conventional NMT model augmented with syntactic structural information of target translation.,3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"Given a source sentenceX = x1,x2,..,xm, its target translation Y = y1,y2,..,yn and Y ’s dependency parse tree T , the goal of the extension is to enable us to compute the joint probability P (Y, T |X).",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"As in most structural learning tasks, the full prediction of Y and T is further decomposed into a chain of smaller predictions.",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"For translation Y , it is generated in the left-to-right order as y1, y2, .., yn following the way in a normal sequence-to-sequence model.",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"For Y ’s parse tree T , instead of directly modeling the tree itself, we predict a parsing action sequence A which can map Y to T .",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"Thus at
top level our SD-NMT model can be formulated as
P (Y, T |X) = P (Y,A|X) = P (y1y2..yn, a1, a2..al|X)(6)
where A = a1,a2,..,aj ,..,al 1 with length l (l = 2n), aj ∈ {SH,RR(d),LR(d)}2.
",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"Two recurrent neural networks, Word-RNN and Action-RNN, are used to model generation processes of translation sequence Y and parsing action sequence A respectively.",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"Figure 2 shows an example how translation Y and its parsing actions are predicted step by step.
",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"Because the lengths of Word-RNN and ActionRNN are different, they are designed to work in a mutually dependent way: a target word is only allowed to be generated when the SH action is predicted in the action sequence.",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"In this way, we can perform incremental dependency parsing for translation Y and at the same time track the partial parsing status through the translation generation process.
",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"For notational clarity, we introduce a virtual translation sequence Ŷ =ŷ1,ŷ2,..,ŷj ,..,ŷl for WordRNN which has the same length l with transition action sequence.",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"ŷj is defined as
ŷj",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"= { yvj δ(SH, aj) = 1 yvj−1 δ(SH, aj) = 0
where δ(SH, aj) is 1 when aj = SH, otherwise 0.",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"vj is the index of Y , computed by vj =∑j
i=1",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"δ(SH, ai).",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"Apparently the mapping from Ŷ 1In the rest of this paper, aj represents the transition action, rather than the attention weight in Equation 4. 2RR(d) refers to a set of RR actions augmented with dependency labels so as to LR(d).
",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
𝑇𝑖𝑚𝑒𝑠𝑡𝑎𝑚𝑝 1 2 3 4 𝑗,3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"− 1
𝒋
to Y is deterministic, and Y can be easily derived given Ŷ and A.
With the notation of Ŷ , the sequence probability of Y and A can be written as
P (A|X, Ŷ<l)",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"= l∏
j=1
P (aj|a<j, X, Ŷ<j) (7)
P (Ŷ |X,A≤l)",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"= l∏
j=1
P (ŷj|ŷ<j, X,A≤j)δ(SH,aj)
(8)
where Ŷ<j refers to the subsequence ŷ1, ŷ2, .., ŷj−1, and A≤j to a1, a2, .., aj .",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"Based on Equation 7 and 8, the overall joint model can be computed as
P (Y, T |X) = P (A|X, Ŷ<l)× P (Ŷ |X,A≤l) (9)
",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"As we have two RNNs in our model, the termination condition is also different from a conventional NMT model.",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"In decoding, we maintain a stack to track the parsing configuration, and our model terminates once the Word-RNN predicts a special ending symbol EOS and all the words in the stack have been reduced.
",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
Figure 3 (a) gives an overview of our SD-NMT model.,3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"Due to space limitation, the detailed interconnections between two RNNs are only illustrated at timestamp j. The encoder of our model
follows standard bidirectional RNN configuration.",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"At timestamp j during decoding, our model first predicts an action aj by Action-RNN, then WordRNN checks the condition gate δ according to aj .",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"If aj = SH, the Word-RNN will generate a new state (solid arrow) and predict a new target word yvj , otherwise it just copies previous state (dashed arrow) to the current state.",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"For example, at timestamp 3, a3 6=",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"SH, the state of Word-RNN is copied from its previous one.",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"Meanwhile, ŷ3 = y2 is used as the immediate proceeding word in translation history.
",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"When computing attention scores, we extend Equation 5 by replacing the decoder hidden state with the concatenation of Word-RNN hidden state s and Action-RNN hidden state s′ (gray boxes in Figure 3).",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"The new attention score is then updated as
ejk = v T a tanh(Wa[sj−1; s ′",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
j−1] + Uahk) (10),3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"Syntax has been proven useful for sentence generation task (Dyer et al., 2016).",3.1 Syntactic Context for Target Word Prediction,[0],[0]
We propose to leverage target syntax to help translation generation.,3.1 Syntactic Context for Target Word Prediction,[0],[0]
"In our model, the syntactic context Kj at timestamp j is defined as a vector which is computed by a feed-forward network based on current
parsing configuration of Action-RNN.",3.1 Syntactic Context for Target Word Prediction,[0],[0]
"Denote that w0 and w1 are two topmost words in the stack, w0l and w1l are their leftmost modifiers in the partial tree,w0r andw1r their rightmost modifiers respectively.",3.1 Syntactic Context for Target Word Prediction,[0],[0]
We define two unigram features and four bigram features.,3.1 Syntactic Context for Target Word Prediction,[0],[0]
The unigram features are w0 and w1 which are represented by the word embedding vectors.,3.1 Syntactic Context for Target Word Prediction,[0],[0]
"The bigram features are w0w0l, w0w0r, w1w1l and w1w1r.",3.1 Syntactic Context for Target Word Prediction,[0],[0]
"Each of them is computed by bhc = tanh(WbEwh + UbEwhc), h ∈ {0, 1}, c ∈ {l, r}.",3.1 Syntactic Context for Target Word Prediction,[0],[0]
"These kinds of feature template have beeb proven effective in dependency parsing task (Zhang and Clark, 2008).",3.1 Syntactic Context for Target Word Prediction,[0],[0]
"Based on these features, the syntactic context vector Kj is computed as
Kj = tanh(Wk[Ew0;Ew1] + Uk[b0l; b0r; b1l; b1r]) (11)
where Wk, Uk, Wb, Ub are the weight matrices, E stands for the embedding matrix.",3.1 Syntactic Context for Target Word Prediction,[0],[0]
Figure 2 (b) gives an overview of the construction of Kj .,3.1 Syntactic Context for Target Word Prediction,[0],[0]
"Note that zero vector is used for padding the words which are not available in the partial tree, so that all the K vectors have the same input size in computation.
",3.1 Syntactic Context for Target Word Prediction,[0],[0]
"Adding Kj to Equation 2, the probabilities of transition action and word in Equation 7 and 8 are then updated as
P (aj|a<j, X, Ŷ<j) = g(s′j, aj−1, cj,Kj) (12) P (ŷj|ŷ<j, X,A≤j) = g(sj, ŷj−1, cj,Kj) (13)
",3.1 Syntactic Context for Target Word Prediction,[0],[0]
"After each prediction step in Word-RNN and Action-RNN, the syntax context vector K will be updated accordingly.",3.1 Syntactic Context for Target Word Prediction,[0],[0]
Note that K is not used to calculate the recurrent states s in this work.,3.1 Syntactic Context for Target Word Prediction,[0],[0]
"For SD-NMT model, we use the sum of loglikelihoods of word sequence and action sequence as objective function for training algorithm, so that the joint probability of target translations and their parsing trees can be maximized:
J(θ) = ∑
(X,Y,A)∈D log P (A|X, Ŷ<l)+
log P (Ŷ |X,A≤l) (14)
We also use mini-batch for model training.",3.2 Model Training and Decoding,[0],[0]
"As the target dependency trees are known in the bilingual corpus during training, we pre-compute the partial tree state and syntactic context at each time
stamp for each training instance.",3.2 Model Training and Decoding,[0],[0]
"Thus it is easy for the model to process multiple trees in one batch.
",3.2 Model Training and Decoding,[0],[0]
"In the decoding process of an SD-NMT model, the score of each search path is the sum of log probabilities of target word sequence and transition action sequence normalized by the sequence length:
score",3.2 Model Training and Decoding,[0],[0]
"= 1
l
l∑
j=1
log P (aj |a<j , X, Ŷ<j)+
1
n
l∑
j=1
δ(SH, aj) log P (ŷj |ŷ<j , X,A≤j) (15)
where n is word sequence length and l is action sequence length.",3.2 Model Training and Decoding,[0],[0]
"The experiments are conducted on the ChineseEnglish task as well as the Japanese-English translation tasks where the same data set from WAT 2016 ASPEC corpus (Nakazawa et al., 2016) 3 is used for a fair comparison with other work.",4 Experiments,[0],[0]
"In addition to evaluate translation performance, we also investigate the quality of dependency parsing as a by-product and the effect of parsing quality against translation quality.",4 Experiments,[0],[0]
"In the Chinese-English task, the bilingual training data consists of a set of LDC datasets, 4 which has around 2M sentence pairs.",4.1 Setup,[0],[0]
"We use NIST2003 as the development set, and the testsets contain NIST2005, NIST2006, NIST2008 and NIST2012.",4.1 Setup,[0],[0]
"All English words are lowercased.
",4.1 Setup,[0],[0]
"In the Japanese-English task, we use top 1M sentence pairs from ASPEC Japanese-English corpus.",4.1 Setup,[0],[0]
"The development data contains 1,790 sentences, and the test data contains 1,812 sentences with single reference per source sentence.
",4.1 Setup,[0],[0]
"To train SD-NMT model, the target dependency tree references are needed.",4.1 Setup,[0],[0]
"As there is no golden annotation of parse trees over the target training data, we use pseudo parsing results as the target dependency references, which are got from an in-house developed arc-eager dependency parser based on work in (Zhang and Nivre, 2011).
3http://orchid.kuee.kyoto-u.ac.jp/ASPEC/ 4LDC2003E14, LDC2005T10, LDC2005E83, LDC2006E26, LDC2006E34, LDC2006E85, LDC2006E92, LDC2003E07, LDC2002E18, LDC2005T06, LDC2003E07, LDC2004T07, LDC2004T08, LDC2005T06
In the neural network training, the vocabulary size is limited to 30K high frequent words for both source and target languages.",4.1 Setup,[0],[0]
"All low frequent words are normalized into a special token unk and post-processed by following the work in (Luong et al., 2015b).",4.1 Setup,[0],[0]
The size of word embedding and transition action embedding is set to 512.,4.1 Setup,[0],[0]
The dimensions of the hidden states for all RNNs are set to 1024.,4.1 Setup,[0],[0]
"All model parameters are initialized randomly with Gaussian distribution (Glorot and Bengio, 2010) and trained on a NVIDIA Tesla K40 GPU.",4.1 Setup,[0],[0]
The stochastic gradient descent (SGD) algorithm is used to tune parameters with a learning rate of 1.0.,4.1 Setup,[0],[0]
The batch size is set to 96.,4.1 Setup,[0],[0]
"In the update procedure, Adadelta (Zeiler, 2012) algorithm is used to automatically adapt the learning rate.",4.1 Setup,[0],[0]
"The beam sizes for both word prediction and transition action prediction are set to 12 in decoding.
",4.1 Setup,[0],[0]
"The baselines in our experiments are a phrasal system and a neural translation system, denoted by HPSMT and RNNsearch respectively.",4.1 Setup,[0],[0]
"HPSMT is an in-house implementation of the hierarchical phrase-based model (Chiang, 2005), where a 4- gram language model is trained using the modified Kneser-Ney smoothing (Kneser and Ney, 1995) algorism over the English Gigaword corpus (LDC2009T13) plus the target data from the bilingual corpus.",4.1 Setup,[0],[0]
"RNNsearch is an in-house implementation of the attention-based neural machine translation model (Bahdanau et al., 2015) using the same parameter settings as our SD-NMT model including word embedding size, hidden vector dimension, beam size, as well as the same mechanism for OOV word processing.
",4.1 Setup,[0],[0]
"The evaluation results are reported with the case-insensitive IBM BLEU-4 (Papineni et al., 2002).",4.1 Setup,[0],[0]
A statistical significance test is performed using the bootstrap resampling method proposed by Koehn (2004) with a 95% confidence level.,4.1 Setup,[0],[0]
"For Japanese-English task, we use the official eval-
uation procedure provided by WAT 2016.5, where both BLEU and RIBES (Isozaki et al., 2010) are used for evaluation.",4.1 Setup,[0],[0]
We evaluate our method on the Chinese-English translation task.,4.2 Evaluation on Chinese-English Translation,[0],[0]
The evaluation results over all NIST test sets against baselines are listed in Table 1.,4.2 Evaluation on Chinese-English Translation,[0],[0]
"Generally, RNNsearch outperforms HPSMT by 3.78 BLEU points on average while SD-NMT surpasses RNNsearch 2.03 BLUE point gains on average, which shows that NMT models usually achieve better results than SMT models, and our proposed sequence-to-dependency NMT model performs much better than traditional sequence-tosequence NMT model.
",4.2 Evaluation on Chinese-English Translation,[0],[0]
We also investigate the effect of syntactic knowledge context by excluding its computation in Equation 12 and 13.,4.2 Evaluation on Chinese-English Translation,[0],[0]
The alternative model is denoted by SD-NMT\K.,4.2 Evaluation on Chinese-English Translation,[0],[0]
"According to Table 1, SD-NMT\K outperforms RNNsearch by 0.54 BLEU points but degrades SD-NMT by 1.49 BLEU points on average, which demonstrates that the long distance dependencies captured by the target syntactic knowledge context, such as leftmost/rightmost children together with their dependency relationships, really bring strong positive effects on the prediction of target words.
",4.2 Evaluation on Chinese-English Translation,[0],[0]
"In addition to translation quality, we compare the perplexity (PPL) changes on the development set in terms of numbers of training mini-batches for RNNsearch and SD-NMT in Figure 4.",4.2 Evaluation on Chinese-English Translation,[0],[0]
"We can see that the PPL of SD-NMT is initially higher than that of RNNsearch, but decreased to be lower over time.",4.2 Evaluation on Chinese-English Translation,[0],[0]
This is mainly because the quality of parse tree is too poor at the beginning which degrades translation quality and leads to higher PPL.,4.2 Evaluation on Chinese-English Translation,[0],[0]
"After some training iterations, the SD-NMT
5http://lotus.kuee.kyoto-u.ac.jp/WAT/evaluation/index .html
model learns reasonable inferences of parse trees which begins to help target word generation and leads to lower PPL.
25 17.26 15.74
26 18.76 16.58
27 17.62 15.88
In our experiments, the time cost of SD-NMT is two times of that for RNNsearch due to a more complicated model structure.",4.2 Evaluation on Chinese-English Translation,[0],[0]
But we think it is a worthy trade to pursue high quality translations.,4.2 Evaluation on Chinese-English Translation,[0],[0]
"In this section, we report results on the JapaneseEnglish translation task.",4.3 Evaluation on Japanese-English Translation,[0],[0]
"To ensure fair comparisons, we use the same training data and follow the pre-processing steps recommended in WAT 20166.",4.3 Evaluation on Japanese-English Translation,[0],[0]
Table 2 shows the comparison results from 8 systems with the evaluation metrics of BLEU and RIBES.,4.3 Evaluation on Japanese-English Translation,[0],[0]
The results in the first 3 rows are produced by SMT systems taken from the official WAT 2016.,4.3 Evaluation on Japanese-English Translation,[0],[0]
"The remaining results are produced by NMT systems, among which the bottom two row results are taken from our in-house NMT systems and others refer to the work in (Cromieres, 2016;
6http://lotus.kuee.kyoto-u.ac.jp/WAT/baseline/data PreparationJE.html
Cromieres et al., 2016) that are the competitive NMT results on WAT 2016.",4.3 Evaluation on Japanese-English Translation,[0],[0]
"According to Table 2, NMT results still outperform SMT results similar to our Chinese-English evaluation results.",4.3 Evaluation on Japanese-English Translation,[0],[0]
"The SD-NMT model significantly outperforms most other NMT models, which shows that our proposed approach to modeling target dependency tree benefit NMT systems since our RNNsearch baseline achieves comparable performance with the single layer attention-based NMT system in (Cromieres, 2016).",4.3 Evaluation on Japanese-English Translation,[0],[0]
"Note that our SD-NMT gets comparable results with the 4 single-layer ensemble model in (Cromieres, 2016; Cromieres et al., 2016).",4.3 Evaluation on Japanese-English Translation,[0],[0]
We believe SD-NMT can get more improvements with an ensemble of multiple models in future experiments.,4.3 Evaluation on Japanese-English Translation,[0],[0]
The interaction effect between dependency tree conduction and target word generation is investigated in this section.,4.4 Effect of the Parsing Accuracy upon Translation Quality,[0],[0]
The experiments are conducted on the Chinese-English task over multiple test sets.,4.4 Effect of the Parsing Accuracy upon Translation Quality,[0],[0]
We evaluate how the quality of dependency trees affect the performance of translation.,4.4 Effect of the Parsing Accuracy upon Translation Quality,[0],[0]
"In the decoding phase of SD-NMT, beam search is applied to the generations of both transition and actions as illustrated in Equation 15.",4.4 Effect of the Parsing Accuracy upon Translation Quality,[0],[0]
"Intuitively, the larger the beam size of action prediction is, the better the dependency tree quality is.",4.4 Effect of the Parsing Accuracy upon Translation Quality,[0],[0]
"We fix the beam size for generating target words to 12, and change the beam size for action prediction to see the difference.",4.4 Effect of the Parsing Accuracy upon Translation Quality,[0],[0]
Figure 5 shows the evaluation results of all test sets.,4.4 Effect of the Parsing Accuracy upon Translation Quality,[0],[0]
There is a tendency for BLEU scores to increase with the growth of action prediction beam size.,4.4 Effect of the Parsing Accuracy upon Translation Quality,[0],[0]
"The reason is that the translation quality increases as the quality of dependency tree improves, which shows the construction of dependency trees can boost the generation of target
4 38.77 40.64 32.06 30.63
6 38.93 41.32 32.63 31.07
8 39.34 41.52 32.88 31.32
10 39.32 41.65 32.82 31.41 12 39.38 41.81 33.06 31.43
words, and vice versa we believe.",4.4 Effect of the Parsing Accuracy upon Translation Quality,[0],[0]
"As a by-product, the quality of dependency trees not only affects the performance of target word generation, but also influences the possible downstream processors or tasks such as text analyses.",4.5 Quality Estimation of Dependency Tree Construction,[0],[0]
The direct evaluation of tree quality is not feasible due to the unavailable golden references.,4.5 Quality Estimation of Dependency Tree Construction,[0],[0]
So we resort to estimating the consistency between the by-products and the parsing results of our standalone dependency parser with state-of-the-art performance.,4.5 Quality Estimation of Dependency Tree Construction,[0],[0]
"The higher the consistency is, the closer the performance of by-product is to the standalone parser.",4.5 Quality Estimation of Dependency Tree Construction,[0],[0]
"To reduce the influence of ill-formed data as much as possible, we build the evaluation data set by heuristically selecting 360 SD-NMT translation results together with their dependency trees from NIST test sets where both source- and target-side do not contain unk and have a length of 20-30.",4.5 Quality Estimation of Dependency Tree Construction,[0],[0]
We then take the parsing results of the stand-alone parser for these translations as references to indirectly estimate the quality of byproducts.,4.5 Quality Estimation of Dependency Tree Construction,[0],[0]
"We get a UAS (unlabeled attachment score) of 94.96% and a LAS (labeled attachment score) of 93.92%, which demonstrates that the dependency trees produced by SD-NMT are much similar with the parsing results from the standalone parser.",4.5 Quality Estimation of Dependency Tree Construction,[0],[0]
"In this section, we give a case study to explain how our method works.",4.6 Translation Example,[0],[0]
Figure 6 shows a translation example from the NIST testsets.,4.6 Translation Example,[0],[0]
"SMT and RNNsearch refer to the translation results from the
baselines HPSMT and NMT.",4.6 Translation Example,[0],[0]
"For our SD-NMT model, we list both the generated translation and its corresponding dependency tree.",4.6 Translation Example,[0],[0]
"We find that the translation of SMT is disfluent and ungrammatical, whereas RNNsearch is better than SMT.",4.6 Translation Example,[0],[0]
"Although the translation of RNNsearch is locally fluent around word “have” in the rectangle, both its grammar is incorrect and its meaning is inaccurate from a global view.",4.6 Translation Example,[0],[0]
The word “have” should be in a singular form as its subject is “safety” rather than “workers”.,4.6 Translation Example,[0],[0]
"For our SD-NMT model, we can see that the translation is much better than baselines and the dependency tree is reasonable.",4.6 Translation Example,[0],[0]
"The reason is that after generating the word “workers”, the previous subtree in the gray region is transformed to the syntactic context which can guide the generation of the next word as illustrated by the dashed arrow.",4.6 Translation Example,[0],[0]
Thus our model is more likely to generate the correct verb “is” with singular form.,4.6 Translation Example,[0],[0]
"In addition, the global structure helps the model correctly identify the inverted sentence pattern of the former translated part and make better choices for the future translation (“only when .. can ..” in our translation, “only when .. will ..”",4.6 Translation Example,[0],[0]
"in the reference), which remains a challenge for conventional NMT model.",4.6 Translation Example,[0],[0]
"Incorporating linguistic knowledge into machine translation has been extensively studied in Statistic Machine Translation (SMT) (Galley et al., 2006; Shen et al., 2008; Liu et al., 2006).",5 Related Work,[0],[0]
Liu et al. (2006) proposed a tree-to-string alignment template for SMT to leverage source side syntactic information.,5 Related Work,[0],[0]
Shen et al. (2008) proposed a target dependency language model for SMT to employ target-side structured information.,5 Related Work,[0],[0]
"These methods show promising improvement for SMT.
",5 Related Work,[0],[0]
"Recently, neural machine translation (NMT) has achieved better performance than SMT in many language pairs (Luong et al., 2015a; Zhang et al., 2016; Shen et al., 2016; Wu et al., 2016; Neubig, 2016).",5 Related Work,[0],[0]
"In a vanilla NMT model, source and target sentences are treated as sequences where the syntactic knowledge of both sides is neglected.",5 Related Work,[0],[0]
Some effort has been done to incorporate source syntax into NMT.,5 Related Work,[0],[0]
Eriguchi et al. (2016) proposed a tree-to-sequence attentional NMT model where source-side parse tree was used and achieved promising improvement.,5 Related Work,[0],[0]
"Intuitively, adding source syntactic information to
[Source] 只有施工人员的安全得到了保证 , 才能继续施工 .",5 Related Work,[0],[0]
[Reference] only when the safety of the workers is guaranteed will they continue with the project .,5 Related Work,[0],[0]
"[HPSMT] only safety is assured of construction personnel , to continue construction .",5 Related Work,[0],[0]
[RNNsearch] only when the safety of construction workers have been guaranteed to continue construction .,5 Related Work,[0],[0]
"[SD-NMT] only when the safety of the workers is guaranteed can we continue to work .
",5 Related Work,[0],[0]
"NMT is straightforward, because the source sentence is definitive and easy to attach extra information.",5 Related Work,[0],[0]
"However, it is non-trivial to add target syntax as target words are uncertain in decoding process.",5 Related Work,[0],[0]
"Up to now, there is few work that attempts to build and leverage target syntactic information for NMT.
",5 Related Work,[0],[0]
There has been work that incorporates syntactic information into NLP tasks with neural networks.,5 Related Work,[0],[0]
Dyer et al. (2016) presented a RNN grammar for parsing and language modeling.,5 Related Work,[0],[0]
"They replaced SH with a set of generative actions to generate words under a Stack LSTM framework (Dyer et al., 2015), which achieves promising results for language modeling on the Penn Treebank data.",5 Related Work,[0],[0]
"In our work, we propose to involve target syntactic trees into NMT model to jointly learn target translation and dependency parsing where target syntactic context over the parse tree is used to improve the translation quality.",5 Related Work,[0],[0]
"In this paper, we propose a novel string-todependency translation model over NMT.",6 Conclusion and Future Work,[0],[0]
Our model jointly performs target word generation and arc-standard dependency parsing.,6 Conclusion and Future Work,[0],[0]
"Experimental results show that our method can boost the two procedures and achieve significant improvements on the translation quality of NMT systems.
",6 Conclusion and Future Work,[0],[0]
"In future work, along this research direction, we will try to integrate other prior knowledge, such as
semantic information, into NMT systems.",6 Conclusion and Future Work,[0],[0]
"In addition, we will apply our method to other sequenceto-sequence tasks, such as text summarization, to verify the effectiveness.",6 Conclusion and Future Work,[0],[0]
We are grateful to the anonymous reviewers for their insightful comments.,Acknowledgments,[0],[0]
We also thank Shujie Liu and Zhirui Zhang for the helpful discussions.,Acknowledgments,[0],[0]
"Nowadays a typical Neural Machine Translation (NMT) model generates translations from left to right as a linear sequence, during which latent syntactic structures of the target sentences are not explicitly concerned.",abstractText,[0],[0]
"Inspired by the success of using syntactic knowledge of target language for improving statistical machine translation, in this paper we propose a novel Sequence-to-Dependency",abstractText,[0],[0]
"Neural Machine Translation (SD-NMT) method, in which the target word sequence and its corresponding dependency structure are jointly constructed and modeled, and this structure is used as context to facilitate word generations.",abstractText,[0],[0]
Experimental results show that the proposed method significantly outperforms state-of-the-art baselines on Chinese-English and JapaneseEnglish translation tasks.,abstractText,[0],[0]
Sequence-to-Dependency Neural Machine Translation,title,[0],[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1296–1306, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics",text,[0],[0]
"Sequence-to-Sequence learning with deep neural networks (herein, seq2seq) (Sutskever et al., 2011; Sutskever et al., 2014) has rapidly become a very useful and surprisingly general-purpose tool for natural language processing.",1 Introduction,[0],[0]
"In addition to demonstrating impressive results for machine translation (Bahdanau et al., 2015), roughly the same model and training have also proven to be useful for sentence compression (Filippova et al., 2015), parsing (Vinyals et al., 2015), and dialogue systems (Serban et al., 2016), and they additionally underlie other
text generation applications, such as image or video captioning (Venugopalan et al., 2015; Xu et al., 2015).
",1 Introduction,[0],[0]
"The dominant approach to training a seq2seq system is as a conditional language model, with training maximizing the likelihood of each successive target word conditioned on the input sequence and the gold history of target words.",1 Introduction,[0],[0]
"Thus, training uses a strictly word-level loss, usually cross-entropy over the target vocabulary.",1 Introduction,[0],[0]
"This approach has proven to be very effective and efficient for training neural language models, and seq2seq models similarly obtain impressive perplexities for word-generation tasks.
",1 Introduction,[0],[0]
"Notably, however, seq2seq models are not used as conditional language models at test-time; they must instead generate fully-formed word sequences.",1 Introduction,[0],[0]
"In practice, generation is accomplished by searching over output sequences greedily or with beam search.",1 Introduction,[0],[0]
"In this context, Ranzato et al. (2016) note that the combination of the training and generation scheme just described leads to at least two major issues:
1.",1 Introduction,[0],[0]
"Exposure Bias: the model is never exposed to its own errors during training, and so the inferred histories at test-time do not resemble the gold training histories.
",1 Introduction,[0],[0]
2.,1 Introduction,[0],[0]
"Loss-Evaluation Mismatch: training uses a word-level loss, while at test-time we target improving sequence-level evaluation metrics, such as BLEU (Papineni et al., 2002).
",1 Introduction,[0],[0]
"We might additionally add the concern of label bias (Lafferty et al., 2001) to the list, since wordprobabilities at each time-step are locally normalized, guaranteeing that successors of incorrect his-
1296
tories receive the same mass as do the successors of the true history.
",1 Introduction,[0],[0]
"In this work we develop a non-probabilistic variant of the seq2seq model that can assign a score to any possible target sequence, and we propose a training procedure, inspired by the learning as search optimization (LaSO) framework of Daumé III and Marcu (2005), that defines a loss function in terms of errors made during beam search.",1 Introduction,[0],[0]
"Furthermore, we provide an efficient algorithm to backpropagate through the beam-search procedure during seq2seq training.
",1 Introduction,[0],[0]
"This approach offers a possible solution to each of the three aforementioned issues, while largely maintaining the model architecture and training efficiency of standard seq2seq learning.",1 Introduction,[0],[0]
"Moreover, by scoring sequences rather than words, our approach also allows for enforcing hard-constraints on sequence generation at training time.",1 Introduction,[0],[0]
"To test out the effectiveness of the proposed approach, we develop a general-purpose seq2seq system with beam search optimization.",1 Introduction,[0],[0]
"We run experiments on three very different problems: word ordering, syntactic parsing, and machine translation, and compare to a highlytuned seq2seq system with attention (Luong et al., 2015).",1 Introduction,[0],[0]
"The version with beam search optimization shows significant improvements on all three tasks, and particular improvements on tasks that require difficult search.",1 Introduction,[0],[0]
"The issues of exposure bias and label bias have received much attention from authors in the structured prediction community, and we briefly review some of this work here.",2 Related Work,[0],[0]
"One prominent approach to combating exposure bias is that of SEARN (Daumé III et al., 2009), a meta-training algorithm that learns a search policy in the form of a cost-sensitive classifier trained on examples generated from an interpolation of an oracle policy and the model’s current (learned) policy.",2 Related Work,[0],[0]
"Thus, SEARN explicitly targets the mismatch between oracular training and non-oracular (often greedy) test-time inference by training on the output of the model’s own policy.",2 Related Work,[0],[0]
"DAgger (Ross et al., 2011) is a similar approach, which differs in terms of how training examples are generated and aggregated, and there have additionally been impor-
tant refinements to this style of training over the past several years (Chang et al., 2015).",2 Related Work,[0],[0]
"When it comes to training RNNs, SEARN/DAgger has been applied under the name “scheduled sampling” (Bengio et al., 2015), which involves training an RNN to generate the t+ 1’st token in a target sequence after consuming either the true t’th token, or, with probability that increases throughout training, the predicted t’th token.
",2 Related Work,[0],[0]
"Though technically possible, it is uncommon to use beam search when training with SEARN/DAgger.",2 Related Work,[0],[0]
"The early-update (Collins and Roark, 2004) and LaSO (Daumé III and Marcu, 2005) training strategies, however, explicitly account for beam search, and describe strategies for updating parameters when the gold structure becomes unreachable during search.",2 Related Work,[0],[0]
"Early update and LaSO differ primarily in that the former discards a training example after the first search error, whereas LaSO resumes searching after an error from a state that includes the gold partial structure.",2 Related Work,[0],[0]
"In the context of feed-forward neural network training, early update training has been recently explored in a feedforward setting by Zhou et al. (2015) and Andor et al. (2016).",2 Related Work,[0],[0]
"Our work differs in that we adopt a LaSO-like paradigm (with some minor modifications), and apply it to the training of seq2seq RNNs (rather than feed-forward networks).",2 Related Work,[0],[0]
"We also note that Watanabe and Sumita (2015) apply maximumviolation training (Huang et al., 2012), which is similar to early-update, to a parsing model with recurrent components, and that Yazdani and Henderson (2015) use beam-search in training a discriminative, locally normalized dependency parser with recurrent components.
",2 Related Work,[0],[0]
Recently authors have also proposed alleviating exposure bias using techniques from reinforcement learning.,2 Related Work,[0],[0]
"Ranzato et al. (2016) follow this approach to train RNN decoders in a seq2seq model, and they obtain consistent improvements in performance, even over models trained with scheduled sampling.",2 Related Work,[0],[0]
"As Daumé III and Marcu (2005) note, LaSO is similar to reinforcement learning, except it does not require “exploration” in the same way.",2 Related Work,[0],[0]
"Such exploration may be unnecessary in supervised text-generation, since we typically know the gold partial sequences at each time-step.",2 Related Work,[0],[0]
"Shen et al. (2016) use minimum risk training (approximated by
sampling) to address the issues of exposure bias and loss-evaluation mismatch for seq2seq MT, and show impressive performance gains.
",2 Related Work,[0],[0]
"Whereas exposure bias results from training in a certain way, label bias results from properties of the model itself.",2 Related Work,[0],[0]
"In particular, label bias is likely to affect structured models that make sub-structure predictions using locally-normalized scores.",2 Related Work,[0],[0]
"Because the neural and non-neural literature on this point has recently been reviewed by Andor et al. (2016), we simply note here that RNN models are typically locally normalized, and we are unaware of any specifically seq2seq work with RNNs that does not use locally-normalized scores.",2 Related Work,[0],[0]
"The model we introduce here, however, is not locally normalized, and so should not suffer from label bias.",2 Related Work,[0],[0]
"We also note that there are some (non-seq2seq) exceptions to the trend of locally normalized RNNs, such as the work of Sak et al. (2014) and Voigtlaender et al. (2015), who train LSTMs in the context of HMMs for speech recognition using sequence-level objectives; their work does not consider search, however.",2 Related Work,[0],[0]
"In the simplest seq2seq scenario, we are given a collection of source-target sequence pairs and tasked with learning to generate target sequences from source sequences.",3 Background and Notation,[0],[0]
"For instance, we might view machine translation in this way, where in particular we attempt to generate English sentences from (corresponding) French sentences.",3 Background and Notation,[0],[0]
"Seq2seq models are part of the broader class of “encoder-decoder” models (Cho et al., 2014), which first use an encoding model to transform a source object into an encoded representation x. Many different sequential (and non-sequential) encoders have proven to be effective for different source domains.",3 Background and Notation,[0],[0]
"In this work we are agnostic to the form of the encoding model, and simply assume an abstract source representation x.
Once the input sequence is encoded, seq2seq models generate a target sequence using a decoder.",3 Background and Notation,[0],[0]
The decoder is tasked with generating a target sequence of words from a target vocabulary V .,3 Background and Notation,[0],[0]
"In particular, words are generated sequentially by conditioning on the input representation x and on the previously generated words or history.",3 Background and Notation,[0],[0]
"We use the notation w1:T to refer to an arbitrary word sequence
of length T , and the notation y1:T to refer to the gold (i.e., correct) target word sequence for an input x.
Most seq2seq systems utilize a recurrent neural network (RNN) for the decoder model.",3 Background and Notation,[0],[0]
"Formally, a recurrent neural network is a parameterized nonlinear function RNN that recursively maps a sequence of vectors to a sequence of hidden states.",3 Background and Notation,[0],[0]
"Let m1, . . .",3 Background and Notation,[0],[0]
",mT be a sequence of T vectors, and let h0 be some initial state vector.",3 Background and Notation,[0],[0]
"Applying an RNN to any such sequence yields hidden states ht at each time-step t, as follows:
ht ← RNN(mt,ht−1;θ),
where θ is the set of model parameters, which are shared over time.",3 Background and Notation,[0],[0]
"In this work, the vectors mt will always correspond to the embeddings of a target word sequence w1:T , and so we will also write ht ← RNN(wt,ht−1;θ), with wt standing in for its embedding.
RNN decoders are typically trained to act as conditional language models.",3 Background and Notation,[0],[0]
"That is, one attempts to model the probability of the t’th target word conditioned on x and the target history by stipulating that p(wt|w1:t−1,x) = g(wt,ht−1,x), for some parameterized function g typically computed with an affine layer followed by a softmax.",3 Background and Notation,[0],[0]
"In computing these probabilities, the state ht−1 represents the target history, and h0 is typically set to be some function of x.",3 Background and Notation,[0],[0]
"The complete model (including encoder) is trained, analogously to a neural language model, to minimize the cross-entropy loss at each time-step while conditioning on the gold history in the training data.",3 Background and Notation,[0],[0]
"That is, the model is trained to minimize − ln∏Tt=1 p(yt|y1:t−1,x).
",3 Background and Notation,[0],[0]
"Once the decoder is trained, discrete sequence generation can be performed by approximately maximizing the probability of the target sequence under the conditional distribution, ŷ1:T = argbeamw1:T ∏T t=1",3 Background and Notation,[0],[0]
"p(wt|w1:t−1,x), where we use the notation argbeam to emphasize that the decoding process requires heuristic search, since the RNN model is non-Markovian.",3 Background and Notation,[0],[0]
"In practice, a simple beam search procedure that explores K prospective histories at each time-step has proven to be an effective decoding approach.",3 Background and Notation,[0],[0]
"However, as noted above, decoding in this manner after conditional languagemodel style training potentially suffers from the is-
sues of exposure bias and label bias, which motivates the work of this paper.",3 Background and Notation,[0],[0]
We begin by making one small change to the seq2seq modeling framework.,4 Beam Search Optimization,[0],[0]
"Instead of predicting the probability of the next word, we instead learn to produce (non-probabilistic) scores for ranking sequences.",4 Beam Search Optimization,[0],[0]
"Define the score of a sequence consisting of history w1:t−1 followed by a single word wt as f(wt,ht−1,x), where f is a parameterized function examining the current hidden-state of the relevant RNN at time t− 1 as well as the input representation x.",4 Beam Search Optimization,[0],[0]
"In experiments, our f will have an identical form to g but without the final softmax transformation (which transforms unnormalized scores into probabilities), thereby allowing the model to avoid issues associated with the label bias problem.
",4 Beam Search Optimization,[0],[0]
"More importantly, we also modify how this model is trained.",4 Beam Search Optimization,[0],[0]
Ideally we would train by comparing the gold sequence to the highest-scoring complete sequence.,4 Beam Search Optimization,[0],[0]
"However, because finding the argmax sequence according to this model is intractable, we propose to adopt a LaSO-like (Daumé III and Marcu, 2005) scheme to train, which we will refer to as beam search optimization (BSO).",4 Beam Search Optimization,[0],[0]
"In particular, we define a loss that penalizes the gold sequence falling off the beam during training.1 The proposed training approach is a simple way to expose the model to incorrect histories and to match the training procedure to test generation.",4 Beam Search Optimization,[0],[0]
"Furthermore we show that it can be implemented efficiently without changing the asymptotic run-time of training, beyond a factor of the beam size K.",4 Beam Search Optimization,[0],[0]
We now formalize this notion of a search-based loss for RNN training.,4.1 Search-Based Loss,[0],[0]
"Assume we have a set St of K candidate sequences of length t. We can calculate a score for each sequence in St using a scoring function f parameterized with an RNN, as above, and we define the sequence ŷ(K)1:t ∈St to be the K’th ranked
1Using a non-probabilistic model further allows us to incur no loss (and thus require no update to parameters) when the gold sequence is on the beam; this contrasts with models based on a CRF loss, such as those of Andor et al. (2016) and Zhou et al. (2015), though in training those models are simply not updated when the gold sequence remains on the beam.
",4.1 Search-Based Loss,[0],[0]
sequence in St according to f .,4.1 Search-Based Loss,[0],[0]
"That is, assuming distinct scores,
|{ŷ(k)1:t ∈St |",4.1 Search-Based Loss,[0],[0]
"f(ŷ (k) t , ĥ
(k) t−1) >",4.1 Search-Based Loss,[0],[0]
"f(ŷ (K) t , ĥ (K) t−1)}| =",4.1 Search-Based Loss,[0],[0]
"K − 1,
where ŷ(k)t is the t’th token in ŷ (k) 1:t , ĥ
(k)
t−1 is the RNN state corresponding to its t− 1’st step, and where we have omitted the x argument to f for brevity.
",4.1 Search-Based Loss,[0],[0]
We now define a loss function that gives loss each time the score of the gold prefix y1:t does not exceed that of ŷ(K)1:,4.1 Search-Based Loss,[0],[0]
"t by a margin:
L(f) = T∑
t=1
∆(ŷ (K) 1:t ) [ 1− f(yt,ht−1) + f(ŷ(K)t , ĥ (K) t−1) ] .
",4.1 Search-Based Loss,[0],[0]
"Above, the ∆(ŷ(K)1:t ) term denotes a mistake-specific cost-function, which allows us to scale the loss depending on the severity of erroneously predicting ŷ",4.1 Search-Based Loss,[0],[0]
"(K) 1:t ; it is assumed to return 0 when the margin requirement is satisfied, and a positive number otherwise.",4.1 Search-Based Loss,[0],[0]
It is this term that allows us to use sequencerather than word-level costs in training (addressing the 2nd issue in the introduction).,4.1 Search-Based Loss,[0],[0]
"For instance, when training a seq2seq model for machine translation, it may be desirable to have ∆(ŷ(K)1:t ) be inversely related to the partial sentence-level BLEU score of ŷ(K)1:",4.1 Search-Based Loss,[0],[0]
"t with y1:t; we experiment along these lines in Section 5.3.
",4.1 Search-Based Loss,[0],[0]
"Finally, because we want the full gold sequence to be at the top of the beam at the end of search, when t=T we modify the loss to require the score of y1:T to exceed the score of the highest ranked incorrect prediction by a margin.
",4.1 Search-Based Loss,[0],[0]
"We can optimize the loss L using a two-step process: (1) in a forward pass, we compute candidate sets St and record margin violations (sequences with non-zero loss); (2) in a backward pass, we backpropagate the errors through the seq2seq RNNs.",4.1 Search-Based Loss,[0],[0]
"Unlike standard seq2seq training, the first-step requires running search (in our case beam search) to find margin violations.",4.1 Search-Based Loss,[0],[0]
The second step can be done by adapting back-propagation through time (BPTT).,4.1 Search-Based Loss,[0],[0]
We next discuss the details of this process.,4.1 Search-Based Loss,[0],[0]
"In order to minimize this loss, we need to specify a procedure for constructing candidate sequences ŷ(k)1:t
at each time step t so that we find margin violations.",4.2 Forward: Find Violations,[0],[0]
We follow LaSO (rather than early-update 2; see Section 2) and build candidates in a recursive manner.,4.2 Forward: Find Violations,[0],[0]
"If there was no margin violation at t−1, then St is constructed using a standard beam search update.",4.2 Forward: Find Violations,[0],[0]
"If there was a margin violation, St is constructed as the K best sequences assuming the gold history y1:t−1 through time-step t−1.
",4.2 Forward: Find Violations,[0],[0]
"Formally, assume the function succ maps a sequence w1:t−1 ∈Vt−1 to the set of all valid sequences of length t that can be formed by appending to it a valid word w∈V .",4.2 Forward: Find Violations,[0],[0]
"In the simplest, unconstrained case, we will have
succ(w1:t−1) = {w1:t−1, w | w ∈ V}.
",4.2 Forward: Find Violations,[0],[0]
"As an important aside, note that for some problems it may be preferable to define a succ function which imposes hard constraints on successor sequences.",4.2 Forward: Find Violations,[0],[0]
"For instance, if we would like to use seq2seq models for parsing (by emitting a constituency or dependency structure encoded into a sequence in some way), we will have hard constraints on the sequences the model can output, namely, that they represent valid parses.",4.2 Forward: Find Violations,[0],[0]
"While hard constraints such as these would be difficult to add to standard seq2seq at training time, in our framework they can naturally be added to the succ function, allowing us to train with hard constraints; we experiment along these lines in Section 5.3, where we refer to a model trained with constrained beam search as ConBSO.
",4.2 Forward: Find Violations,[0],[0]
"Having defined an appropriate succ function, we specify the candidate set as:
St = topK { succ(y1:t−1) violation at t−1⋃K
k=1",4.2 Forward: Find Violations,[0],[0]
succ(ŷ,4.2 Forward: Find Violations,[0],[0]
"(k) 1:t−1) otherwise,
where we have a margin violation at t−1 iff f(yt−1,ht−2)",4.2 Forward: Find Violations,[0],[0]
<,4.2 Forward: Find Violations,[0],[0]
"f(ŷ (K) t−1 , ĥ (K)
t−2) + 1, and where topK considers the scores given by f .",4.2 Forward: Find Violations,[0],[0]
"This search procedure is illustrated in the top portion of Figure 1.
",4.2 Forward: Find Violations,[0],[0]
"In the forward pass of our training algorithm, shown as the first part of Algorithm 1, we run this version of beam search and collect all sequences and their hidden states that lead to losses.
",4.2 Forward: Find Violations,[0],[0]
"2We found that training with early-update rather than (delayed) LaSO did not work well, even after pre-training.",4.2 Forward: Find Violations,[0],[0]
Given the success of early-update in many NLP tasks this was somewhat surprising.,4.2 Forward: Find Violations,[0],[0]
We leave this question to future work.,4.2 Forward: Find Violations,[0],[0]
Once we have collected margin violations we can run backpropagation to compute parameter updates.,4.3 Backward: Merge Sequences,[0],[0]
Assume a margin violation occurs at time-step t between the predicted history ŷ(K)1:,4.3 Backward: Merge Sequences,[0],[0]
t and the gold history y1:t.,4.3 Backward: Merge Sequences,[0],[0]
"As in standard seq2seq training we must back-propagate this error through the gold history; however, unlike seq2seq we also have a gradient for the wrongly predicted history.
",4.3 Backward: Merge Sequences,[0],[0]
"Recall that to back-propagate errors through an RNN we run a recursive backward procedure — denoted below by BRNN — at each time-step t, which accumulates the gradients of next-step and future losses with respect to ht.",4.3 Backward: Merge Sequences,[0],[0]
"We have:
∇htL ← BRNN(∇htLt+1,∇ht+1L),
where Lt+1 is the loss at step t+ 1, deriving, for instance, from the score f(yt+1,ht).",4.3 Backward: Merge Sequences,[0],[0]
"Running this BRNN procedure from t=T − 1 to t= 0 is known as back-propagation through time (BPTT).
",4.3 Backward: Merge Sequences,[0],[0]
"In determining the total computational cost of back-propagation here, first note that in the worst case there is one violation at each time-step, which leads to T independent, incorrect sequences.",4.3 Backward: Merge Sequences,[0],[0]
Since we need to call BRNN O(T ),4.3 Backward: Merge Sequences,[0],[0]
"times for each sequence, a naive strategy of running BPTT for each incorrect sequence would lead to an O(T 2) backward pass, rather than the O(T ) time required for the standard seq2seq approach.
",4.3 Backward: Merge Sequences,[0],[0]
"Fortunately, our combination of search-strategy and loss make it possible to efficiently share BRNN operations.",4.3 Backward: Merge Sequences,[0],[0]
"This shared structure comes
naturally from the LaSO update, which resets the beam in a convenient way.
",4.3 Backward: Merge Sequences,[0],[0]
We informally illustrate the process in Figure 1.,4.3 Backward: Merge Sequences,[0],[0]
The top of the diagram shows a possible sequence of ŷ(k)1:t formed during search with a beam of size 3 for the target sequence y = “a red dog runs quickly today.”,4.3 Backward: Merge Sequences,[0],[0]
"When the gold sequence falls off the beam at t= 4, search resumes with S5 = succ(y1:4), and so all subsequent predicted sequences have y1:4 as a prefix and are thus functions of h4.",4.3 Backward: Merge Sequences,[0],[0]
"Moreover, because our loss function only involves the scores of the gold prefix and the violating prefix, we end up with the relatively simple computation tree shown at the bottom of Figure 1.",4.3 Backward: Merge Sequences,[0],[0]
"It is evident that we can backpropagate in a single pass, accumulating gradients from sequences that diverge from the gold at the time-step that precedes their divergence.",4.3 Backward: Merge Sequences,[0],[0]
"The second half of Algorithm 1 shows this explicitly for a single sequence, though it is straightforward to extend the algorithm to operate in batch.3",4.3 Backward: Merge Sequences,[0],[0]
"We run experiments on three different tasks, comparing our approach to the seq2seq baseline, and to other relevant baselines.",5 Data and Methods,[0],[0]
"While the method we describe applies to seq2seq RNNs in general, for all experiments we use the global attention model of Luong et al. (2015) — which consists of an LSTM (Hochreiter and Schmidhuber, 1997) encoder and an LSTM decoder with a global attention model — as both the baseline seq2seq model (i.e., as the model that computes the g in Section 3) and as the model that computes our sequence-scores f(wt,ht−1,x).",5.1 Model,[0],[0]
"As in Luong et al. (2015), we also use “input feeding,” which involves feeding the attention distribution from the previous time-step into the decoder at the current step.",5.1 Model,[0],[0]
"This model architecture has been found to be highly performant for neural machine translation and other seq2seq tasks.
",5.1 Model,[0],[0]
"3We also note that because we do not update the parameters until after the T ’th search step, our training procedure differs slightly from LaSO (which is online), and in this aspect is essentially equivalent to the “delayed LaSO update” of Björkelund and Kuhn (2014).
",5.1 Model,[0],[0]
"Algorithm 1 Seq2seq Beam-Search Optimization 1: procedure BSO(x,Ktr, succ) 2: /*FORWARD*/",5.1 Model,[0],[0]
3:,5.1 Model,[0],[0]
"Init empty storage ŷ1:T and ĥ1:T ; init S1 4: r ← 0; violations← {0} 5: for t = 1, . . .",5.1 Model,[0],[0]
", T do 6: K =Ktr if t 6=T else argmax
k:ŷ (k) 1:t 6=y1:t
f(ŷ (k) t , ĥ
(k) t−1)
7: if f(yt,ht−1) <",5.1 Model,[0],[0]
"f(ŷ (K) t , ĥ
(K) t−1) + 1 then
8: ĥr:t−1 ← ĥ (K)
",5.1 Model,[0],[0]
"r:t−1 9: ŷr+1:t ← ŷ(K)r+1:t
10: Add t to violations 11:",5.1 Model,[0],[0]
r ← t 12: St+1,5.1 Model,[0],[0]
← topK(succ(y1:t)) 13: else 14: St+1 ← topK( ⋃K k=1,5.1 Model,[0],[0]
succ(ŷ,5.1 Model,[0],[0]
(k) 1:t )),5.1 Model,[0],[0]
"16: grad hT ← 0; grad ĥT ← 0 17: for t = T − 1, . . .",15: /*BACKWARD*/,[0],[0]
", 1 do 18: grad ht←BRNN(∇htLt+1, grad ht+1) 19: grad ĥt←BRNN(∇ĥtLt+1, grad ĥt+1) 20: if t− 1 ∈ violations then 21: grad ht ← grad ht + grad ĥt 22: grad ĥt ← 0
To distinguish the models we refer to our system as BSO (beam search optimization) and to the baseline as seq2seq.",15: /*BACKWARD*/,[0],[0]
"When we apply constrained training (as discussed in Section 4.2), we refer to the model as ConBSO.",15: /*BACKWARD*/,[0],[0]
"In providing results we also distinguish between the beam size Ktr with which the model is trained, and the beam size Kte which is used at test-time.",15: /*BACKWARD*/,[0],[0]
"In general, if we plan on evaluating with a beam of size Kte it makes sense to train with a beam of size Ktr = Kte + 1, since our objective requires the gold sequence to be scored higher than the last sequence on the beam.",15: /*BACKWARD*/,[0],[0]
Here we detail additional techniques we found necessary to ensure the model learned effectively.,5.2 Methodology,[0],[0]
"First, we found that the model failed to learn when trained from a random initialization.4 We therefore found it necessary to pre-train the model using a standard, word-level cross-entropy loss as described in Sec-
4This may be because there is relatively little signal in the sparse, sequence-level gradient, but this point requires further investigation.
tion 3.",5.2 Methodology,[0],[0]
"The necessity of pre-training in this instance is consistent with the findings of other authors who train non-local neural models (Kingsbury, 2009; Sak et al., 2014; Andor et al., 2016; Ranzato et al., 2016).5
Similarly, it is clear that the smaller the beam used in training is, the less room the model has to make erroneous predictions without running afoul of the margin loss.",5.2 Methodology,[0],[0]
"Accordingly, we also found it useful to use a “curriculum beam” strategy in training, whereby the size of the beam is increased gradually during training.",5.2 Methodology,[0],[0]
"In particular, given a desired training beam size Ktr, we began training with a beam of size 2, and increased it by 1 every 2 epochs until reaching Ktr.
",5.2 Methodology,[0],[0]
"Finally, it has been established that dropout (Srivastava et al., 2014) regularization improves the performance of LSTMs (Pham et al., 2014; Zaremba et al., 2014), and in our experiments we run beam search under dropout.6
For all experiments, we trained both seq2seq and BSO models with mini-batch Adagrad (Duchi et al., 2011) (using batches of size 64), and we renormalized all gradients so they did not exceed 5 before updating parameters.",5.2 Methodology,[0],[0]
"We did not extensively tune learning-rates, but we found initial rates of 0.02 for the encoder and decoder LSTMs, and a rate of 0.1 or 0.2 for the final linear layer (i.e., the layer tasked with making word-predictions at each timestep) to work well across all the tasks we considered.",5.2 Methodology,[0],[0]
Code implementing the experiments described below can be found at https://github.com/ harvardnlp/BSO.7,5.2 Methodology,[0],[0]
Our experiments are primarily intended to evaluate the effectiveness of beam search optimization over standard seq2seq training.,5.3 Tasks and Results,[0],[0]
"As such, we run experiments with the same model across three very dif-
5Andor et al. (2016) found, however, that pre-training only increased convergence-speed, but was not necessary for obtaining good results.
",5.3 Tasks and Results,[0],[0]
"6However, it is important to ensure that the same mask applied at each time-step of the forward search is also applied at the corresponding step of the backward pass.",5.3 Tasks and Results,[0],[0]
"We accomplish this by pre-computing masks for each time-step, and sharing them between the partial sequence LSTMs.
",5.3 Tasks and Results,[0],[0]
"7Our code is based on Yoon Kim’s seq2seq code, https: //github.com/harvardnlp/seq2seq-attn.
ferent problems: word ordering, dependency parsing, and machine translation.",5.3 Tasks and Results,[0],[0]
"While we do not include all the features and extensions necessary to reach state-of-the-art performance, even the baseline seq2seq model is generally quite performant.
",5.3 Tasks and Results,[0],[0]
"Word Ordering The task of correctly ordering the words in a shuffled sentence has recently gained some attention as a way to test the (syntactic) capabilities of text-generation systems (Zhang and Clark, 2011; Zhang and Clark, 2015; Liu et al., 2015; Schmaltz et al., 2016).",5.3 Tasks and Results,[0],[0]
"We cast this task as seq2seq problem by viewing a shuffled sentence as a source sentence, and the correctly ordered sentence as the target.",5.3 Tasks and Results,[0],[0]
"While word ordering is a somewhat synthetic task, it has two interesting properties for our purposes.",5.3 Tasks and Results,[0],[0]
"First, it is a task which plausibly requires search (due to the exponentially many possible orderings), and, second, there is a clear hard constraint on output sequences, namely, that they be a permutation of the source sequence.",5.3 Tasks and Results,[0],[0]
For both the baseline and BSO models we enforce this constraint at testtime.,5.3 Tasks and Results,[0],[0]
"However, we also experiment with constraining the BSO model during training, as described in Section 4.2, by defining the succ function to only allow successor sequences containing un-used words in the source sentence.
",5.3 Tasks and Results,[0],[0]
"For experiments, we use the same PTB dataset (with the standard training, development, and test splits) and evaluation procedure as in Zhang and Clark (2015) and later work, with performance reported in terms of BLEU score with the correctly ordered sentences.",5.3 Tasks and Results,[0],[0]
"For all word-ordering experiments we use 2-layer encoder and decoder LSTMs, each with 256 hidden units, and dropout with a rate of 0.2 between LSTM layers.",5.3 Tasks and Results,[0],[0]
"We use simple 0/1 costs in defining the ∆ function.
",5.3 Tasks and Results,[0],[0]
We show our test-set results in Table 1.,5.3 Tasks and Results,[0],[0]
"We see that on this task there is a large improvement at each beam size from switching to BSO, and a further improvement from using the constrained model.
",5.3 Tasks and Results,[0],[0]
"Inspired by a similar analysis in Daumé III and Marcu (2005), we further examine the relationship between Ktr and Kte when training with ConBSO in Table 2.",5.3 Tasks and Results,[0],[0]
"We see that larger Ktr hurt greedy inference, but that results continue to improve, at least initially, when using a Kte that is (somewhat) bigger than Ktr − 1.
",5.3 Tasks and Results,[0],[0]
Dependency Parsing,5.3 Tasks and Results,[0],[0]
"We next apply our model to dependency parsing, which also has hard constraints and plausibly benefits from search.",5.3 Tasks and Results,[0],[0]
"We treat dependency parsing with arc-standard transitions as a seq2seq task by attempting to map from a source sentence to a target sequence of source sentence words interleaved with the arc-standard, reduce-actions in its parse.",5.3 Tasks and Results,[0],[0]
"For example, we attempt to map the source sentence
But it was the Quotron problems that ...
to the target sequence
",5.3 Tasks and Results,[0],[0]
"But it was @L SBJ @L DEP the Quotron problems @L NMOD @L NMOD that ...
We use the standard Penn Treebank dataset splits with Stanford dependency labels, and the standard UAS/LAS evaluation metric (excluding punctuation) following Chen and Manning (2014).",5.3 Tasks and Results,[0],[0]
"All models thus see only the words in the source and, when decoding, the actions it has emitted so far; no other features are used.",5.3 Tasks and Results,[0],[0]
"We use 2-layer encoder and decoder LSTMs with 300 hidden units per layer
and dropout with a rate of 0.3 between LSTM layers.",5.3 Tasks and Results,[0],[0]
"We replace singleton words in the training set with an UNK token, normalize digits to a single symbol, and initialize word embeddings for both source and target words from the publicly available word2vec (Mikolov et al., 2013) embeddings.",5.3 Tasks and Results,[0],[0]
"We use simple 0/1 costs in defining the ∆ function.
",5.3 Tasks and Results,[0],[0]
"As in the word-ordering case, we also experiment with modifying the succ function in order to train under hard constraints, namely, that the emitted target sequence be a valid parse.",5.3 Tasks and Results,[0],[0]
"In particular, we constrain the output at each time-step to obey the stack constraint, and we ensure words in the source are emitted in order.
",5.3 Tasks and Results,[0],[0]
We show results on the test-set in Table 3.,5.3 Tasks and Results,[0],[0]
"BSO and ConBSO both show significant improvements over seq2seq, with ConBSO improving most on UAS, and BSO improving most on LAS.",5.3 Tasks and Results,[0],[0]
"We achieve a reasonable final score of 91.57 UAS, which lags behind the state-of-the-art, but is promising for a general-purpose, word-only model.
",5.3 Tasks and Results,[0],[0]
"Translation We finally evaluate our model on a small machine translation dataset, which allows us to experiment with a cost function that is not 0/1, and to consider other baselines that attempt to mitigate exposure bias in the seq2seq setting.",5.3 Tasks and Results,[0],[0]
"We use the dataset from the work of Ranzato et al. (2016), which uses data from the German-to-English portion of the IWSLT 2014 machine translation evaluation campaign (Cettolo et al., 2014).",5.3 Tasks and Results,[0],[0]
"The data comes from translated TED talks, and the dataset contains roughly 153K training sentences, 7K development sentences, and 7K test sentences.",5.3 Tasks and Results,[0],[0]
"We use the same preprocessing and dataset splits as Ranzato et
al. (2016), and like them we also use a single-layer LSTM decoder with 256 units.",5.3 Tasks and Results,[0],[0]
We also use dropout with a rate of 0.2 between each LSTM layer.,5.3 Tasks and Results,[0],[0]
"We emphasize, however, that while our decoder LSTM is of the same size as that of Ranzato et al. (2016), our results are not directly comparable, because we use an LSTM encoder (rather than a convolutional encoder as they do), a slightly different attention mechanism, and input feeding (Luong et al., 2015).
",5.3 Tasks and Results,[0],[0]
"For our main MT results, we set ∆(ŷ(k)1:t ) to 1−SB(ŷ(K)r+1:t, yr+1:t), where r is the last margin violation and SB denotes smoothed, sentence-level BLEU (Chen and Cherry, 2014).",5.3 Tasks and Results,[0],[0]
This setting of ∆ should act to penalize erroneous predictions with a relatively low sentence-level BLEU score more than those with a relatively high sentence-level BLEU score.,5.3 Tasks and Results,[0],[0]
"In Table 4 we show our final results and those from Ranzato et al. (2016).8 While we start with an improved baseline, we see similarly large increases in accuracy as those obtained by DAD and MIXER, in particular when Kte > 1.
",5.3 Tasks and Results,[0],[0]
"We further investigate the utility of these sequence-level costs in Table 5, which compares using sentence-level BLEU costs in defining ∆ with using 0/1 costs.",5.3 Tasks and Results,[0],[0]
"We see that the more sophisticated sequence-level costs have a moderate effect on BLEU score.
",5.3 Tasks and Results,[0],[0]
"8Some results from personal communication.
",5.3 Tasks and Results,[0],[0]
"r+1:t, yr+1:t) (bottom), and
Ktr = 6.
",5.3 Tasks and Results,[0],[0]
"Timing Given Algorithm 1, we would expect training time to increase linearly with the size of the beam.",5.3 Tasks and Results,[0],[0]
"On the above MT task, our highly tuned seq2seq baseline processes an average of 13,038 tokens/second (including both source and target tokens) on a GTX 970 GPU.",5.3 Tasks and Results,[0],[0]
"For beams of size Ktr = 2, 3, 4, 5, and 6, our implementation processes on average 1,985, 1,768, 1,709, 1,521, and 1,458 tokens/second, respectively.",5.3 Tasks and Results,[0],[0]
"Thus, we appear to pay an initial constant factor of ≈ 3.3 due to the more complicated forward and backward passes, and then training scales with the size of the beam.",5.3 Tasks and Results,[0],[0]
"Because we batch beam predictions on a GPU, however, we find that in practice training time scales sub-linearly with the beam-size.",5.3 Tasks and Results,[0],[0]
"We have introduced a variant of seq2seq and an associated beam search training scheme, which addresses exposure bias as well as label bias, and moreover allows for both training with sequencelevel cost functions as well as with hard constraints.",6 Conclusion,[0],[0]
Future work will examine scaling this approach to much larger datasets.,6 Conclusion,[0],[0]
We thank Yoon Kim for helpful discussions and for providing the initial seq2seq code on which our implementations are based.,Acknowledgments,[0],[0]
We thank Allen Schmaltz for help with the word ordering experiments.,Acknowledgments,[0],[0]
We also gratefully acknowledge the support of a Google Research Award.,Acknowledgments,[0],[0]
Sequence-to-Sequence (seq2seq) modeling has rapidly become an important generalpurpose NLP tool that has proven effective for many text-generation and sequence-labeling tasks.,abstractText,[0],[0]
"Seq2seq builds on deep neural language modeling and inherits its remarkable accuracy in estimating local, next-word distributions.",abstractText,[0],[0]
"In this work, we introduce a model and beamsearch training scheme, based on the work of Daumé III and Marcu (2005), that extends seq2seq to learn global sequence scores.",abstractText,[0],[0]
"This structured approach avoids classical biases associated with local training and unifies the training loss with the test-time usage, while preserving the proven model architecture of seq2seq and its efficient training approach.",abstractText,[0],[0]
"We show that our system outperforms a highlyoptimized attention-based seq2seq system and other baselines on three different sequence to sequence tasks: word ordering, parsing, and machine translation.",abstractText,[0],[0]
Sequence-to-Sequence Learning as Beam-Search Optimization,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1842–1852 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1842
Sequence-to-sequence Models for Cache Transition Systems
Xiaochang Peng1, Linfeng Song1, Daniel Gildea1, Giorgio Satta2 1University of Rochester 2University of Padua {xpeng,lsong10,gildea}@cs.rochester.edu,
satta@dei.unipd.it
Abstract
In this paper, we present a sequenceto-sequence based approach for mapping natural language sentences to AMR semantic graphs. We transform the sequence to graph mapping problem to a word sequence to transition action sequence problem using a special transition system called a cache transition system. To address the sparsity issue of neural AMR parsing, we feed feature embeddings from the transition state to provide relevant local information for each decoder state. We present a monotonic hard attention model for the transition framework to handle the strictly left-to-right alignment between each transition state and the current buffer input focus. We evaluate our neural transition model on the AMR parsing task, and our parser outperforms other sequence-to-sequence approaches and achieves competitive results in comparison with the best-performing models.1",text,[0],[0]
"Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism where the meaning of a sentence is encoded as a rooted, directed graph.",1 Introduction,[0],[0]
Figure 1 shows an example of an AMR in which the nodes represent the AMR concepts and the edges represent the relations between the concepts.,1 Introduction,[0],[0]
"AMR has been used in various applications such as text summarization (Liu et al., 2015), sentence compression (Takase et al., 2016), and event extraction (Huang et al., 2016).
",1 Introduction,[0],[0]
"1The implementation of our parser is available at https://github.com/xiaochang13/CacheTransition-Seq2seq
want-01
person
go-01
ARG0
ARG0
ARG1
name
“John”
name
op1
Figure 1: An example of AMR graph representing the meaning of: “John wants to go”
The task of AMR graph parsing is to map natural language strings to AMR semantic graphs.",1 Introduction,[0],[0]
"Different parsers have been developed to tackle this problem (Flanigan et al., 2014; Wang et al., 2015b,a; Peng et al., 2015; Artzi et al., 2015; Pust et al., 2015; van Noord and Bos, 2017).",1 Introduction,[0],[0]
"On the other hand, due to the limited amount of labeled data and the large output vocabulary, the sequence-to-sequence model has not been very successful on AMR parsing.",1 Introduction,[0],[0]
Peng et al. (2017) propose a linearization approach that encodes labeled graphs as sequences.,1 Introduction,[0],[0]
"To address the data sparsity issue, low-frequency entities and tokens are mapped to special categories to reduce the vocabulary size for the neural models.",1 Introduction,[0],[0]
Konstas et al. (2017) use self-training on a huge amount of unlabeled text to lower the out-of-vocabulary rate.,1 Introduction,[0],[0]
"However, the final performance still falls behind the best-performing models.
",1 Introduction,[0],[0]
The best performing AMR parsers model graph structures directly.,1 Introduction,[0],[0]
"One approach to modeling graph structures is to use a transition system to build graphs step by step, as shown by the system
of Wang and Xue (2017), which is currently the top performing system.",1 Introduction,[0],[0]
"This raises the question of whether the advantages of neural and transitionbased system can be combined, as for example with the syntactic parser of Dyer et al. (2015), who use stack LSTMs to capture action history information in the transition state of the transition system.",1 Introduction,[0],[0]
"Ballesteros and Al-Onaizan (2017) apply stack-LSTM to transition-based AMR parsing and achieve competitive results, which shows that local transition state information is important for predicting transition actions.
",1 Introduction,[0],[0]
"Instead of linearizing the target AMR graph to a sequence structure, Buys and Blunsom (2017) propose a sequence-to-action-sequence approach where the reference AMR graph is replaced with an action derivation sequence by running a deterministic oracle algorithm on the training sentence, AMR graph pairs.",1 Introduction,[0],[0]
"They use a separate alignment probability to explicitly model the hard alignment from graph nodes to sentence tokens in the buffer.
",1 Introduction,[0],[0]
Gildea et al. (2018) propose a special transition framework called a cache transition system to generate the set of semantic graphs.,1 Introduction,[0],[0]
"They adapt the stack-based parsing system by adding a working set, which they refer to as a cache, to the traditional stack and buffer.",1 Introduction,[0],[0]
"Peng et al. (2018) apply the cache transition system to AMR parsing and design refined action phases, each modeled with a separate feedforward neural network, to deal with some practical implementation issues.
",1 Introduction,[0],[0]
"In this paper, we propose a sequence-to-actionsequence approach for AMR parsing with cache transition systems.",1 Introduction,[0],[0]
"We want to take advantage of the sequence-to-sequence model to encode wholesentence context information and the history action sequence, while using the transition system to constrain the possible output.",1 Introduction,[0],[0]
"The transition system can also provide better local context information than the linearized graph representation, which is important for neural AMR parsing given the limited amount of data.
",1 Introduction,[0],[0]
"More specifically, we use bi-LSTM to encode two levels of input information for AMR parsing: word level and concept level, each refined with more general category information such as lemmatization, POS tags, and concept categories.
",1 Introduction,[0],[0]
We also want to make better use of the complex transition system to address the data sparsity issue for neural AMR parsing.,1 Introduction,[0],[0]
"We extend the hard attention model of Aharoni and Goldberg (2017),
which deals with the nearly-monotonic alignment in the morphological inflection task, to the more general scenario of transition systems where the input buffer is processed from left-to-right.",1 Introduction,[0],[0]
"When we process the buffer in this ordered manner, the sequence of target transition actions are also strictly aligned left-to-right according to the input order.",1 Introduction,[0],[0]
"On the decoder side, we augment the prediction of output action with embedding features from the current transition state.",1 Introduction,[0],[0]
Our experiments show that encoding information from the transition state significantly improves sequenceto-sequence models for AMR parsing.,1 Introduction,[0],[0]
"We adopt the transition system of Gildea et al. (2018), which has been shown to have good coverage of the graphs found in AMR.
",2 Cache Transition Parser,[0],[0]
"A cache transition parser consists of a stack, a cache, and an input buffer.",2 Cache Transition Parser,[0],[0]
"The stack is a sequence σ of (integer, concept) pairs, as explained below, with the topmost element always at the rightmost position.",2 Cache Transition Parser,[0],[0]
"The buffer is a sequence of ordered concepts β containing a suffix of the input concept sequence, with the first element to be read as a newly introduced concept/vertex of the graph.",2 Cache Transition Parser,[0],[0]
(We use the terms concept and vertex interchangeably in this paper.),2 Cache Transition Parser,[0],[0]
"Finally, the cache is a sequence of concepts η =",2 Cache Transition Parser,[0],[0]
"[v1, . . .",2 Cache Transition Parser,[0],[0]
", vm].",2 Cache Transition Parser,[0],[0]
"The element at the leftmost position is called the first element of the cache, and the element at the rightmost position is called the last element.
",2 Cache Transition Parser,[0],[0]
"Operationally, the functioning of the parser can be described in terms of configurations and transitions.",2 Cache Transition Parser,[0],[0]
"A configuration of our parser has the form:
C = (σ, η, β,Gp)
where σ, η and β are as described above, and Gp is the partial graph that has been built so far.",2 Cache Transition Parser,[0],[0]
"The initial configuration of the parser is ([], [$, . . .",2 Cache Transition Parser,[0],[0]
", $], [c1, . . .",2 Cache Transition Parser,[0],[0]
", cn], ∅), meaning that the stack and the partial graph are initially empty, and the cache is filled with m occurrences of the special symbol $.",2 Cache Transition Parser,[0],[0]
The buffer is initialized with all the graph vertices constrained by the order of the input sentence.,2 Cache Transition Parser,[0],[0]
"The final configuration is ([], [$, . . .",2 Cache Transition Parser,[0],[0]
", $], [], G), where the stack and the cache are as in the initial configuration and the buffer is empty.",2 Cache Transition Parser,[0],[0]
"The constructed graph is the target AMR graph.
",2 Cache Transition Parser,[0],[0]
"In the first step, which is called concept identification, we map the input sentence w1:n′ = w1, . . .",2 Cache Transition Parser,[0],[0]
", wn′ to a sequence of concepts c1:n = c1, . . .",2 Cache Transition Parser,[0],[0]
", cn.",2 Cache Transition Parser,[0],[0]
"We decouple the problem of concept identification from the transition system and initialize the buffer with a recognized concept sequence from another classifier, which we will introduce later.",2 Cache Transition Parser,[0],[0]
"As the sequence-to-sequence model uses all possible output actions as the target vocabulary, this can significantly reduce the target vocabulary size.",2 Cache Transition Parser,[0],[0]
"The transitions of the parser are specified as follows.
1.",2 Cache Transition Parser,[0],[0]
"Pop pops a pair (i, v) from the stack, where the integer i records the position in the cache that it originally came from.",2 Cache Transition Parser,[0],[0]
"We place concept v in position i in the cache, shifting the remainder of the cache one position to the right, and discarding the last element in the cache.
2.",2 Cache Transition Parser,[0],[0]
"Shift signals that we will start processing the next input concept, which will become a new vertex in the output graph.
3. PushIndex(i) shifts the next input concept out of the buffer and moves it into the last position of the cache.",2 Cache Transition Parser,[0],[0]
"We also take out the concept vi appearing at position i in the cache and push it onto the stack σ, along with the integer i recording its original position in the cache.2
2Our transition design is different from Peng et al. (2018) in two ways: the PushIndex phase is initiated before making all the arc decisions; the newly introduced concept is placed at the last cache position instead of the leftmost buffer position, which essentially increases the cache size by 1.
4.",2 Cache Transition Parser,[0],[0]
"Arc(i, d, l) builds an arc with direction d and label l between the rightmost concept and the i-th concept in the cache.",2 Cache Transition Parser,[0],[0]
The label l is NULL if no arc is made and we use the action NOARC in this case.,2 Cache Transition Parser,[0],[0]
Otherwise we decompose the arc decision into two actions ARC and d-l. We consider all arc decisions between the rightmost cache concept and each of the other concepts in the cache.,2 Cache Transition Parser,[0],[0]
"We can consider this phase as first making a binary decision whether there is an arc, and then predicting the label in case there is one, between each concept pair.
",2 Cache Transition Parser,[0],[0]
"Given the sentence “John wants to go” and the recognized concept sequence “Per want-01 go-01” (person name category Per for “John”), our cache transition parser can construct the AMR graph shown in Figure 1 using the run shown in Figure 2 with cache size of 3.",2 Cache Transition Parser,[0],[0]
"We use the following oracle algorithm (Nivre, 2008) to derive the sequence of actions that leads to the gold AMR graph for a cache transition parser with cache size m. The correctness of the oracle is shown by Gildea et al. (2018).
",2.1 Oracle Extraction Algorithm,[0],[0]
"Let EG be the set of edges of the gold graph G. We maintain the set of vertices that is not yet shifted into the cache as S, which is initialized with all vertices in G. The vertices are ordered according to their aligned position in the word sequence and the unaligned vertices are listed according to their order in the depth-first traversal of the graph.",2.1 Oracle Extraction Algorithm,[0],[0]
"The oracle algorithm can look into
EG to decide which transition to take next, or else to decide that it should fail.",2.1 Oracle Extraction Algorithm,[0],[0]
"This decision is based on the mutually exclusive rules listed below.
1.",2.1 Oracle Extraction Algorithm,[0],[0]
"ShiftOrPop phase: the oracle chooses transition Pop, in case there is no edge (vm, v) in EG such that vertex v is in S, or chooses transition Shift and proceeds to the next phase.
2.",2.1 Oracle Extraction Algorithm,[0],[0]
"PushIndex phase: in this phase, the oracle first chooses a position i (as explained below) in the cache to place the candidate concept and removes the vertex at this position and places its index, vertex pair onto the stack.",2.1 Oracle Extraction Algorithm,[0],[0]
"The oracle chooses transition PushIndex(i) and proceeds to the next phase.
3. ArcBinary, ArcLabel phases: between the rightmost cache concept and each concept in the cache, we make a binary decision about whether there is an arc between them.",2.1 Oracle Extraction Algorithm,[0],[0]
"If there is an arc, the oracle chooses its direction and label.",2.1 Oracle Extraction Algorithm,[0],[0]
"After arc decisions to m−1 cache concepts are made, we jump to the next step.
4.",2.1 Oracle Extraction Algorithm,[0],[0]
"If the stack and buffer are both empty, and the cache is in the initial state, the oracle finishes with success, otherwise we proceed to the first step.
",2.1 Oracle Extraction Algorithm,[0],[0]
We use the equation below to choose the cache concept to take out in the step PushIndex(i).,2.1 Oracle Extraction Algorithm,[0],[0]
For j ∈,2.1 Oracle Extraction Algorithm,[0],[0]
"[|β|], we write βj to denote the j-th vertex in β.",2.1 Oracle Extraction Algorithm,[0],[0]
"We choose a vertex vi∗ in η such that:
i∗ = argmax i∈[m] min {j | (vi, βj) ∈ EG}
In words, vi∗ is the concept from the cache whose closest neighbor in the buffer β is furthest forward in β.",2.1 Oracle Extraction Algorithm,[0],[0]
"We move out of the cache vertex vi∗ and push it onto the stack, for later processing.
",2.1 Oracle Extraction Algorithm,[0],[0]
"For each training example (x1:n, g), the transition system generates the output AMR graph g from the input sequence x1:n through an oracle sequence a1:q ∈ Σ∗a, where Σa is the union of all possible actions.",2.1 Oracle Extraction Algorithm,[0],[0]
"We model the probability of the output with the action sequence:
P (a1:q|x1:n) = q∏
t=1
P (at|a1, . . .",2.1 Oracle Extraction Algorithm,[0],[0]
", at−1, x1:n; θ)
which we estimate using a sequence-to-sequence model, as we will describe in the next section.",2.1 Oracle Extraction Algorithm,[0],[0]
"Shown in Figure 3, our sequence-to-sequence model takes a word sequence w1:n′ and its mapped concept sequence c1:n as the input, and the action sequence a1:q as the output.",3 Soft vs Hard Attention for Sequence-to-action-sequence,[0],[0]
"It uses two BiLSTM encoders, each encoding an input sequence.",3 Soft vs Hard Attention for Sequence-to-action-sequence,[0],[0]
"As the two encoders have the same structure, we only introduce the encoder for the word sequence in detail below.",3 Soft vs Hard Attention for Sequence-to-action-sequence,[0],[0]
"Given an input word sequence w1:n′ , we use a bidirectional LSTM to encode it.",3.1 BiLSTM Encoder,[0],[0]
"At each step j, the current hidden states ←− h wj",3.1 BiLSTM Encoder,[0],[0]
and −→ h wj are generated from the previous hidden states ←− h wj+1,3.1 BiLSTM Encoder,[0],[0]
"and −→ h wj−1,
and the representation vector xj of the current input word wj :
←− h wj = LSTM( ←− h wj+1, xj) −→ h wj = LSTM( −→ h wj−1, xj)
",3.1 BiLSTM Encoder,[0],[0]
"The representation vector xj is the concatenation of the embeddings of its word, lemma, and POS tag, respectively.",3.1 BiLSTM Encoder,[0],[0]
"Then the hidden states of both directions are concatenated as the final hidden state for word wj :
hwj =",3.1 BiLSTM Encoder,[0],[0]
"[ ←− h wj ; −→ h wj ]
Similarly, for the concept sequence, the final hidden state for concept cj is:
hcj =",3.1 BiLSTM Encoder,[0],[0]
[ ←− h cj ;,3.1 BiLSTM Encoder,[0],[0]
−→ h cj ],3.1 BiLSTM Encoder,[0],[0]
"We use an attention-based LSTM decoder (Bahdanau et al., 2014) with two attention memories Hw and Hc, where Hw is the concatenation of the state vectors of all input words, and Hc for input concepts correspondingly:
",3.2 LSTM Decoder with Soft Attention,[0],[0]
Hw =,3.2 LSTM Decoder with Soft Attention,[0],[0]
[h w 1 ;h w 2 ; . . .,3.2 LSTM Decoder with Soft Attention,[0],[0]
;h w n′,3.2 LSTM Decoder with Soft Attention,[0],[0]
],3.2 LSTM Decoder with Soft Attention,[0],[0]
"(1)
",3.2 LSTM Decoder with Soft Attention,[0],[0]
Hc =,3.2 LSTM Decoder with Soft Attention,[0],[0]
[h c 1;h c 2; . . .,3.2 LSTM Decoder with Soft Attention,[0],[0]
";h c n] (2)
",3.2 LSTM Decoder with Soft Attention,[0],[0]
"The decoder yields an action sequence a1, a2, . . .",3.2 LSTM Decoder with Soft Attention,[0],[0]
", aq as the output by calculating a sequence of hidden states s1, s2 . . .",3.2 LSTM Decoder with Soft Attention,[0],[0]
", sq recurrently.",3.2 LSTM Decoder with Soft Attention,[0],[0]
"While generating the t-th output action, the decoder considers three factors: (1) the previous hidden state of the LSTM model st−1; (2) the embedding of the previous generated action et−1; and (3) the previous context vectors for words µwt−1 and concepts µ c t−1, which are calculated using Hw and Hc, respectively.",3.2 LSTM Decoder with Soft Attention,[0],[0]
"When t = 1, we initialize µ0 as a zero vector, and set e0 to the embedding of the start token “〈s〉”.",3.2 LSTM Decoder with Soft Attention,[0],[0]
"The hidden state s0 is initialized as:
s0 = Wd[ ←− h w1 ; −→ h wn ; ←− h c1; −→ h cn] + bd,
where Wd and bd are model parameters.",3.2 LSTM Decoder with Soft Attention,[0],[0]
"For each time-step t, the decoder feeds the concatenation of the embedding of previous action et−1 and the previous context vectors for words µwt−1 and concepts µ c t−1 into the LSTM model to update its hidden state.
",3.2 LSTM Decoder with Soft Attention,[0],[0]
"st = LSTM(st−1, [et−1;µwt−1;µ c t−1]) (3)
",3.2 LSTM Decoder with Soft Attention,[0],[0]
Then the attention probabilities for the word sequence and the concept sequence are calculated similarly.,3.2 LSTM Decoder with Soft Attention,[0],[0]
"Take the word sequence as an example, αwt,i on h w",3.2 LSTM Decoder with Soft Attention,[0],[0]
i ∈,3.2 LSTM Decoder with Soft Attention,[0],[0]
"Hw for time-step t is calculated as:
t,i = v T c tanh(Whh w i +Wsst + bc)
αwt,i = exp( t,i)∑N j=1 exp( t,j)
",3.2 LSTM Decoder with Soft Attention,[0],[0]
"Wh,Ws, vc and bc are model parameters.",3.2 LSTM Decoder with Soft Attention,[0],[0]
The new context vector µwt =,3.2 LSTM Decoder with Soft Attention,[0],[0]
∑n i=1,3.2 LSTM Decoder with Soft Attention,[0],[0]
"α w t,ih w",3.2 LSTM Decoder with Soft Attention,[0],[0]
i .,3.2 LSTM Decoder with Soft Attention,[0],[0]
"The calculation of µct follows the same procedure, but with a different set of model parameters.
",3.2 LSTM Decoder with Soft Attention,[0],[0]
"The output probability distribution over all actions at the current state is calculated by:
PΣa = softmax(Va[st;µ w t ;µ c t ] + ba), (4)
where Va and ba are learnable parameters, and the number of rows in Va represents the number of all actions.",3.2 LSTM Decoder with Soft Attention,[0],[0]
The symbol Σa is the set of all actions.,3.2 LSTM Decoder with Soft Attention,[0],[0]
"When we process each buffer input, the next few transition actions are closely related to this input position.",3.3 Monotonic Hard Attention for Transition Systems,[0],[0]
"The buffer maintains the order information of the input sequence and is processed strictly left-to-right, which essentially encodes a monotone alignment between the transition action sequence and the input sequence.
",3.3 Monotonic Hard Attention for Transition Systems,[0],[0]
"As we have generated a concept sequence from the input word sequence, we maintain two hard attention pointers, lw and lc, to model monotonic attention to word and concept sequences respectively.",3.3 Monotonic Hard Attention for Transition Systems,[0],[0]
"The update to the decoder state now relies on a single position of each input sequence in contrast to Equation 3:
st = LSTM(st−1, [et−1;hwlw ;h c lc ]) (5)
Control Mechanism.",3.3 Monotonic Hard Attention for Transition Systems,[0],[0]
Both pointers are initialized as 0 and advanced to the next position deterministically.,3.3 Monotonic Hard Attention for Transition Systems,[0],[0]
We move the concept attention focus lc to the next position after arc decisions to all the other m − 1 cache concepts are made.,3.3 Monotonic Hard Attention for Transition Systems,[0],[0]
"We move the word attention focus lw to its aligned position in case the new concept is aligned, otherwise we don’t move the word focus.",3.3 Monotonic Hard Attention for Transition Systems,[0],[0]
"As shown in Figure 4, after we have made arc decisions from concept want-01 to the other cache concepts, we move the concept focus to the next concept go-01.",3.3 Monotonic Hard Attention for Transition Systems,[0],[0]
"As this concept is aligned, we move the word focus to its aligned position go in the word sequence and skip the unaligned word to.",3.3 Monotonic Hard Attention for Transition Systems,[0],[0]
"Another difference of our model with Buys and Blunsom (2017) is that we extract features from the current transition state configuration Ct:
ef (Ct) =",3.4 Transition State Features for Decoder,[0],[0]
"[ef1(Ct); ef2(Ct); · · · ; efl(Ct)]
where l is the number of features extracted from Ct and efk(Ct) (k = 1, . . .",3.4 Transition State Features for Decoder,[0],[0]
", l) represents the embedding for the k-th feature, which is learned during training.",3.4 Transition State Features for Decoder,[0],[0]
"These feature embeddings are concatenated as ef (Ct), and fed as additional input to the decoder.",3.4 Transition State Features for Decoder,[0],[0]
"For the soft attention decoder:
st = LSTM(st−1, [et−1;µwt−1;µ c t−1; ef (Ct)])
and for the hard attention decoder:
st = LSTM(st−1, [et−1;hwlw ;h c lc ; ef (Ct)])
We use the following features in our experiments:
1.",3.4 Transition State Features for Decoder,[0],[0]
"Phase type: indicator features showing which phase the next transition is.
2.",3.4 Transition State Features for Decoder,[0],[0]
ShiftOrPop features: token features3 for the rightmost cache concept and the leftmost buffer concept.,3.4 Transition State Features for Decoder,[0],[0]
"Number of dependencies to words on the right, and the top three dependency labels for them.
3.",3.4 Transition State Features for Decoder,[0],[0]
ArcBinary or ArcLabel features: token features for the rightmost concept and the current cache concept it makes arc decisions to.,3.4 Transition State Features for Decoder,[0],[0]
"Word, concept and dependency distance between the two concepts.",3.4 Transition State Features for Decoder,[0],[0]
The labels for the two most recent outgoing arcs for these two concepts and their first incoming arc and the number of incoming arcs.,3.4 Transition State Features for Decoder,[0],[0]
"Dependency label between the two positions if there is a dependency arc between them.
",3.4 Transition State Features for Decoder,[0],[0]
4.,3.4 Transition State Features for Decoder,[0],[0]
"PushIndex features: token features for the leftmost buffer concept and all the concepts in the cache.
",3.4 Transition State Features for Decoder,[0],[0]
The phase type features are deterministic from the last action output.,3.4 Transition State Features for Decoder,[0],[0]
"For example, if the last action output is Shift, the current phase type would be PushIndex.",3.4 Transition State Features for Decoder,[0],[0]
We only extract corresponding features for this phase and fill all the other feature types with -NULL- as placeholders.,3.4 Transition State Features for Decoder,[0],[0]
"The features for other phases are similar.
",3.4 Transition State Features for Decoder,[0],[0]
"3Concept, concept category at the specified position in concept sequence.",3.4 Transition State Features for Decoder,[0],[0]
"And the word, lemma, POS tag at the aligned input position.",3.4 Transition State Features for Decoder,[0],[0]
"We train our models using the cross-entropy loss, over each oracle action sequence a∗1, . . .",4.1 Training and Decoding,[0],[0]
", a ∗ q :
",4.1 Training and Decoding,[0],[0]
"L = − q∑
t=1
logP (a∗t |a∗1, . . .",4.1 Training and Decoding,[0],[0]
", a∗t−1, X; θ), (6)
where X represents the input word and concept sequences, and θ is the model parameters.",4.1 Training and Decoding,[0],[0]
"Adam (Kingma and Ba, 2014) with a learning rate of 0.001 is used as the optimizer, and the model that yields the best performance on the dev set is selected to evaluate on the test set.",4.1 Training and Decoding,[0],[0]
Dropout with rate 0.3 is used during training.,4.1 Training and Decoding,[0],[0]
Beam search with a beam size of 10 is used for decoding.,4.1 Training and Decoding,[0],[0]
"Both training and decoding use a Tesla K20X GPU.
",4.1 Training and Decoding,[0],[0]
Hidden state sizes for both encoder and decoder are set to 100.,4.1 Training and Decoding,[0],[0]
"The word embeddings are initialized from Glove pretrained word embeddings (Pennington et al., 2014) on Common Crawl, and are not updated during training.",4.1 Training and Decoding,[0],[0]
"The embeddings for POS tags and features are randomly initialized, with the sizes of 20 and 50, respectively.",4.1 Training and Decoding,[0],[0]
"As the AMR data is very sparse, we collapse some subgraphs or spans into categories based on the alignment.",4.2 Preprocessing and Postprocessing,[0],[0]
"We define some special categories such as named entities (NE), dates (DATE), single rooted subgraphs involving multiple concepts (MULT)4, numbers (NUMBER) and phrases (PHRASE).",4.2 Preprocessing and Postprocessing,[0],[0]
The phrases are extracted based on the multiple-to-one alignment in the training data.,4.2 Preprocessing and Postprocessing,[0],[0]
One example phrase is more than which aligns to a single concept more-than.,4.2 Preprocessing and Postprocessing,[0],[0]
"We first collapse spans and subgraphs into these categories based on the alignment from the JAMR aligner (Flanigan et al., 2014), which greedily aligns a span of words to AMR subgraphs using a set of heuristics.",4.2 Preprocessing and Postprocessing,[0],[0]
"This categorization procedure enables the parser to capture mappings from continuous spans on the sentence side to connected subgraphs on the AMR side.
",4.2 Preprocessing and Postprocessing,[0],[0]
"We use the semi-Markov model from Flanigan et al. (2016) as the concept identifier, which jointly segments the sentence into a sequence of spans and maps each span to a subgraph.",4.2 Preprocessing and Postprocessing,[0],[0]
"During decoding, our output has categories, and we need to map
4For example, verbalization of “teacher” as “(person :ARG0-of teach-01)”, or “minister” as “(person :ARG0-of (have-org-role-91 :ARG2 minister))”.
each category to the corresponding AMR concept or subgraph.",4.2 Preprocessing and Postprocessing,[0],[0]
"We save a table Q which shows the original subgraph each category is collapsed from, and map each category to its original subgraph representation.",4.2 Preprocessing and Postprocessing,[0],[0]
"We also use heuristic rules to generate the target-side AMR subgraph representation for NE, DATE, and NUMBER based on the source side tokens.",4.2 Preprocessing and Postprocessing,[0],[0]
"We evaluate our system on the released dataset (LDC2015E86) for SemEval 2016 task 8 on meaning representation parsing (May, 2016).",5 Experiments,[0],[0]
"The dataset contains 16,833 training, 1,368 development, and 1,371 test sentences which mainly cover domains like newswire, discussion forum, etc.",5 Experiments,[0],[0]
"All parsing results are measured by Smatch (version 2.0.2) (Cai and Knight, 2013).",5 Experiments,[0],[0]
We categorize the training data using the automatic alignment and dump a template for date entities and frequent phrases from the multiple to one alignment.,5.1 Experiment Settings,[0],[0]
We also generate an alignment table from tokens or phrases to their candidate targetside subgraphs.,5.1 Experiment Settings,[0],[0]
"For the dev and test data, we first extract the named entities using the Illinois Named Entity Tagger (Ratinov and Roth, 2009) and extract date entities by matching spans with the date template.",5.1 Experiment Settings,[0],[0]
We further categorize the dataset with the categories we have defined.,5.1 Experiment Settings,[0],[0]
"After categorization, we use Stanford CoreNLP",5.1 Experiment Settings,[0],[0]
"(Manning et al., 2014) to get the POS tags and dependencies of the categorized dataset.",5.1 Experiment Settings,[0],[0]
We run the oracle algorithm separately for training and dev data (with alignment) to get the statistics of individual phases.,5.1 Experiment Settings,[0],[0]
We use a cache size of 5 in our experiments.,5.1 Experiment Settings,[0],[0]
Individual Phase Accuracy We first evaluate the prediction accuracy of individual phases on the dev oracle data assuming gold prediction history.,5.2 Results,[0],[0]
"The four transition phases ShiftOrPop, PushIndex, ArcBinary, and ArcLabel account for 25%, 12.5%,
50.1%, and 12.4% of the total transition actions respectively.",5.2 Results,[0],[0]
Table 1 shows the phase-wise accuracy of our sequence-to-sequence model.,5.2 Results,[0],[0]
Peng et al. (2018) use a separate feedforward network to predict each phase independently.,5.2 Results,[0],[0]
We use the same alignment from the SemEval dataset as in Peng et al. (2018) to avoid differences resulting from the aligner.,5.2 Results,[0],[0]
"Soft+feats shows the performance of our sequence-to-sequence model with soft attention and transition state features, while Hard+feats is using hard attention.",5.2 Results,[0],[0]
"We can see that the hard attention model outperforms the soft attention model in all phases, which shows that the single-pointer attention finds more relevant information than the soft attention on the relatively small dataset.",5.2 Results,[0],[0]
"The sequence-to-sequence models perform better than the feedforward model of Peng et al. (2018) on ShiftOrPop and ArcBinary, which shows that the whole-sentence context information is important for the prediction of these two phases.",5.2 Results,[0],[0]
"On the other hand, the sequence-tosequence models perform worse than the feedforward models on PushIndex and ArcLabel.",5.2 Results,[0],[0]
"One possible reason is that the model tries to optimize the overall accuracy, while these two phases account for fewer than 25% of the total transition actions and might be less attended to during the update.
",5.2 Results,[0],[0]
Impact of Different Components Table 2 shows the impact of different components for the sequence-to-sequence model.,5.2 Results,[0],[0]
We can see that the transition state features play a very important role for predicting the correct transition action.,5.2 Results,[0],[0]
This is because different transition phases have very different prediction behaviors and need different types of local information for the prediction.,5.2 Results,[0],[0]
"Relying on the sequence-to-sequence model alone does not perform well in disambiguating these choices, while the transition state can enforce direct constraints.",5.2 Results,[0],[0]
"We can also see that while the hard attention only attends to one position of the input, it performs slightly better than the soft attention model, while the time complexity is lower.
",5.2 Results,[0],[0]
Impact of Different Cache Sizes The cache size of the transition system can be optimized as a trade-off between coverage of AMR graphs and the prediction accuracy.,5.2 Results,[0],[0]
"While larger cache size increases the coverage of AMR graphs, it complicates the prediction procedure with more cache decisions to make.",5.2 Results,[0],[0]
From Table 3 we can see that,5.2 Results,[0],[0]
the hard attention model performs best with cache size 5.,6 0.69 0.64 0.66,[0],[0]
"The soft attention model also achieves best performance with the same cache size.
",6 0.69 0.64 0.66,[0],[0]
Comparison with other Parsers Table 4 shows the comparison with other AMR parsers.,6 0.69 0.64 0.66,[0],[0]
The first three systems are some competitive neural models.,6 0.69 0.64 0.66,[0],[0]
We can see that our parser significantly outperforms the sequence-to-action-sequence model of Buys and Blunsom (2017).,6 0.69 0.64 0.66,[0],[0]
Konstas et al. (2017) use a linearization approach that linearizes the AMR graph to a sequence structure and use selftraining on 20M unlabeled Gigaword sentences.,6 0.69 0.64 0.66,[0],[0]
"Our model achieves better results without using additional unlabeled data, which shows that relevant information from the transition system is very useful for the prediction.",6 0.69 0.64 0.66,[0],[0]
"Our model also outperforms the stack-LSTM model by Ballesteros and Al-Onaizan (2017), while their model is evaluated on the previous release of LDC2014T12.
",6 0.69 0.64 0.66,[0],[0]
We also show the performance of some of the best-performing models.,6 0.69 0.64 0.66,[0],[0]
"While our hard attention achieves slightly lower performance in comparison with Wang et al. (2015a) and Wang and Xue (2017), it is worth noting that their approaches of using WordNet, semantic role labels and word cluster features are complimentary to ours.",6 0.69 0.64 0.66,[0],[0]
The alignment from the aligner and the concept identification identifier also play an important role for improving the performance.,6 0.69 0.64 0.66,[0],[0]
"Wang and Xue (2017) propose to improve AMR parsing by improving the alignment and concept identification, which can also be combined with our system to improve the performance of a sequence-to-sequence model.
",6 0.69 0.64 0.66,[0],[0]
"Dealing with Reentrancy Reentrancy is an important characteristic of AMR, and we evaluate the Smatch score only on the reentrant edges following Damonte et al. (2017).",6 0.69 0.64 0.66,[0],[0]
From Table 5 we can see that our hard attention model significantly outperforms the feedforward model of Peng et al. (2018) in predicting reentrancies.,6 0.69 0.64 0.66,[0],[0]
"This is because predicting reentrancy is directly related to the ArcBinary phase of the cache transition system since it decides to make multiple arc decisions to the same vertex, and we can see from Table 1 that the hard attention model has significantly better prediction accuracy in this phase.",6 0.69 0.64 0.66,[0],[0]
"We also compare the reentrancy results of our transition system with two other systems, Damonte et al. (2017) and JAMR, where these statistics are available.",6 0.69 0.64 0.66,[0],[0]
"From Table 5, we can see that our cache transition system slightly outperforms these two systems in predicting reentrancies.
",6 0.69 0.64 0.66,[0],[0]
"Figure 5 shows a reentrancy example where JAMR and the feedforward network of Peng et al. (2018) do not predict well, while our system predicts the correct output.",6 0.69 0.64 0.66,[0],[0]
"JAMR fails to predict the reentrancy arc from desire-01 to i, and connects the wrong arc from “live-01” to “-” instead of from “desire-01”.",6 0.69 0.64 0.66,[0],[0]
"The feedforward model of Peng et al. (2018) fails to predict the two arcs from desire-01
Sentence: I have no desire to live in any city .
and live-01 to i. This error is because their feedforward ArcBinary classifier does not model longterm dependency and usually prefers making arcs between words that are close and not if they are distant.",6 0.69 0.64 0.66,[0],[0]
"Our classifier, which encodes both word and concept sequence information, can accurately predict the reentrancy through the two arc decisions shown in Figure 5.",6 0.69 0.64 0.66,[0],[0]
"When desire-01 and live01 are shifted into the cache respectively, the transition system makes a left-going arc from each of them to the same concept i, thus creating the reentrancy as desired.",6 0.69 0.64 0.66,[0],[0]
"In this paper, we have presented a sequence-toaction-sequence approach for cache transition systems and applied it to AMR parsing.",6 Conclusion,[0],[0]
"To address the data sparsity issue for neural AMR parsing, we show that the transition state features are very helpful in constraining the possible output and improving the performance of sequence-to-sequence models.",6 Conclusion,[0],[0]
We also show that the monotonic hard attention model can be generalized to the transitionbased framework and outperforms the soft attention model when limited data is available.,6 Conclusion,[0],[0]
"While
we are focused on AMR parsing in this paper, in future work our cache transition system and the presented sequence-to-sequence models can be potentially applied to other semantic graph parsing tasks (Oepen et al., 2015; Du et al., 2015; Zhang et al., 2016; Cao et al., 2017).",6 Conclusion,[0],[0]
"We gratefully acknowledge the assistance of Hao Zhang from Google, New York for the monotonic hard attention idea and the helpful comments and suggestions.",Acknowledgments,[0],[0]
"In this paper, we present a sequenceto-sequence based approach for mapping natural language sentences to AMR semantic graphs.",abstractText,[0],[0]
We transform the sequence to graph mapping problem to a word sequence to transition action sequence problem using a special transition system called a cache transition system.,abstractText,[0],[0]
"To address the sparsity issue of neural AMR parsing, we feed feature embeddings from the transition state to provide relevant local information for each decoder state.",abstractText,[0],[0]
We present a monotonic hard attention model for the transition framework to handle the strictly left-to-right alignment between each transition state and the current buffer input focus.,abstractText,[0],[0]
"We evaluate our neural transition model on the AMR parsing task, and our parser outperforms other sequence-to-sequence approaches and achieves competitive results in comparison with the best-performing models.1",abstractText,[0],[0]
Sequence-to-sequence Models for Cache Transition Systems,title,[0],[0]
"Proceedings of the SIGDIAL 2017 Conference, pages 103–114, Saarbrücken, Germany, 15-17 August 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"Goal oriented dialogue systems help users with accomplishing tasks, like making restaurant reservations or booking flights, by interacting with them in natural language.",1 Introduction,[0],[0]
The capability to understand user utterances and break them down into task specific semantics is a key requirement for these systems.,1 Introduction,[0],[0]
"This is accomplished in the spoken language understanding module, which typically parses user utterances into semantic frames, composed of domains, intents and slots (Tur and De Mori, 2011), that can then be processed by downstream dia-
logue system components.",1 Introduction,[0],[0]
An example semantic frame is shown for a restaurant reservation related query in Figure 1.,1 Introduction,[0],[0]
"As the complexity of the task supported by a dialogue system increases, there is a need for an increased back and forth interaction between the user and the agent.",1 Introduction,[0],[0]
"For example, a restaurant reservation task might require the user to specify a restaurant name, date, time and number of people required for the reservation.",1 Introduction,[0],[0]
"Additionally, based on reservation availability, the user might need to negotiate on date, time, or any other attribute with the agent.",1 Introduction,[0],[0]
This puts the burden of parsing in-dialogue contextual user utterances on the language understanding module.,1 Introduction,[0],[0]
The complexity increases further when the system supports more than one task and the user is allowed to have goals spanning multiple domains within the same dialogue.,1 Introduction,[0],[0]
"Natural language utterances are often ambiguous, and the context from previous user and system turns could help resolve the errors arising from these ambiguities.
",1 Introduction,[0],[0]
"In this paper, we explore approaches to improve dialogue context modeling within a Recurrent Neural Network (RNN) based spoken language understanding system.",1 Introduction,[0],[0]
"We propose a novel model architecture to improve dialogue context modeling for spoken language understanding on a
103
multi-domain dialogue dataset.",1 Introduction,[0],[0]
The proposed architecture is an extension of Hierarchical Recurrent Encoder Decoders (HRED),1 Introduction,[0],[0]
"(Sordoni et al., 2015), where we combine the query level encodings with a representation of the current utterance, before feeding it into the session level encoder.",1 Introduction,[0],[0]
"We compare the performance of this model to a RNN tagger injected with just the previous turn context and a single hop memory network that uses an attention weighted combination of the dialogue context (Chen et al., 2016; Weston et al., 2014).",1 Introduction,[0],[0]
"Furthermore, we describe a dialogue recombination technique to enhance the complexity of the training dataset by injecting synthetic domain switches, to create a better match with the mixed domain dialogues in the test dataset.",1 Introduction,[0],[0]
"This is, in principle, a multi-turn extension of (Jia and Liang, 2016).",1 Introduction,[0],[0]
"Instead of inducing and composing grammars to synthetically enhance single turn text, we combine single domain dialogue sessions into multi-domain dialogues to provide richer context during training.",1 Introduction,[0],[0]
"The task of understanding a user utterance is typically broken down into 3 tasks: domain classification, intent classification and slot-filling (Tur and De Mori, 2011).",2 Related Work,[0],[0]
"Most modern approaches to Spoken language understanding involve training machine learning models on labeled training data (Young, 2002; Hahn et al., 2011; Wang et al., 2005, among others).",2 Related Work,[0],[0]
"More recently, recurrent neural network (RNN) based approaches have been shown to perform exceedingly well on spoken language understanding tasks (Mesnil et al., 2015; Hakkani-Tür et al., 2016; Kurata et al., 2016, among others).",2 Related Work,[0],[0]
"RNN based approaches have also been applied successfully to other tasks for di-
alogue systems, like dialogue state tracking (Henderson, 2015; Henderson et al., 2014; Perez and Liu, 2016, among others), policy learning (Su et al., 2015) and system response generation (Wen et al., 2015, 2016, among others).",2 Related Work,[0],[0]
"In parallel, joint modeling of tasks and addition of contextual signals has been shown to result in performance gains for several applications.",2 Related Work,[0],[0]
"Modeling domain, intent and slots in a joint RNN model was shown to result in reduction of overall frame error rates (Hakkani-Tür et al., 2016).",2 Related Work,[0],[0]
"Joint modeling of intent classification and language modeling showed promising improvements in intent recognition, especially in the presence of noisy speech recognition (Liu and Lane, 2016).",2 Related Work,[0],[0]
"Similarly, models incorporating more context from dialogue history (Chen et al., 2016) or semantic context from the frame (Dauphin et al., 2014; Bapna et al., 2017) tend to outperform models without context and have shown potential for greater generalization on spoken language understanding and related tasks.",2 Related Work,[0],[0]
"(Dhingra et al., 2016) show improved performance on an informational dialogue agent by incorporating knowledge base context into their dialogue system.",2 Related Work,[0],[0]
"Using dialogue context was shown to boost performance for end to end dialogue (Bordes and Weston, 2016) and next utterance prediction (Serban et al., 2015).",2 Related Work,[0],[0]
"In the next few sections, we describe the proposed model architecture, the dataset and our dialogue recombination approach.",2 Related Work,[0],[0]
This is followed by experimental results and analysis.,2 Related Work,[0],[0]
We compare the performance of 3 model architectures for encoding dialogue context on a multidomain dialogue dataset.,3 Model Architecture,[0],[0]
"Let the dialogue be a sequence of system and user utterances Dt =
{u1, u2...ut} and at time step t we are trying to output the parse of a user utterance ut, given Dt.",3 Model Architecture,[0],[0]
"Let any utterance uk be a sequence of tokens given by {xk1, xk2...xknk}.",3 Model Architecture,[0],[0]
"We divide the model into 2 components, the context encoder that acts on Dt to produce a vector representation of the dialogue context denoted by ht = H(Dt), and the tagger, which takes the dialogue context encoding ht, and the current utterance ut as input and produces the domain, intent and slot annotations as output.",3 Model Architecture,[0],[0]
In this section we describe the architectures of the context encoders used for our experiments.,3.1 Context Encoder Architectures,[0],[0]
We compare the performance of 3 different architectures that encode varying levels of dialogue context.,3.1 Context Encoder Architectures,[0],[0]
This is the baseline context encoder architecture.,3.1.1 Previous Utterance Encoder,[0],[0]
"We feed the embeddings corresponding to tokens in the previous system utterance, ut−1 = {xt−11 , xt−12 ...",3.1.1 Previous Utterance Encoder,[0],[0]
"xt−1nt−1}, into a single Bidirectional RNN (BiRNN) layer with Gated Recurrent Unit (GRU) (Chung et al., 2014) cells and 128 dimensions (64 in each direction).",3.1.1 Previous Utterance Encoder,[0],[0]
The embeddings are shared with the tagger.,3.1.1 Previous Utterance Encoder,[0],[0]
"The final state of the context encoder GRU is used as the dialogue context.
",3.1.1 Previous Utterance Encoder,[0],[0]
ht = BiGRUc(ut−1) (1),3.1.1 Previous Utterance Encoder,[0],[0]
"This architecture is identical to the approach described in (Chen et al., 2016).",3.1.2 Memory Network,[0],[0]
"We encode all dialogue context utterances, {u1, u2...ut−1}, into memory vectors denoted by {m1, m2, ...mt−1} using a Bidirectional GRU (BiGRU) encoder with 128 dimensions (64 in each direction).",3.1.2 Memory Network,[0],[0]
"To add temporal context to the dialogue history utter-
ances, we append special positional tokens to each utterance.
",3.1.2 Memory Network,[0],[0]
"mk = BiGRUm(uk) for 0 ≤ k ≤ t−1 (2)
We also encode the current utterance with another BiGRU encoder with 128 dimensions (64 in each direction), into a context vector denoted by c, as in equation 3.",3.1.2 Memory Network,[0],[0]
"This is conceptually depicted in Figure 2
c = BiGRUc(ut) (3)
Let M be a matrix with the ith row given by mi.",3.1.2 Memory Network,[0],[0]
"We obtain the cosine similarity between each memory vector, mi, and the context vector c. The softmax of this similarity is used as an attention distribution over the memory M , and an attention weighted sum of M is used to produce the dialogue context vector",3.1.2 Memory Network,[0],[0]
ht (Equation 4).,3.1.2 Memory Network,[0],[0]
"This is conceptually depicted in Figure 3.
",3.1.2 Memory Network,[0],[0]
"a = softmax(Mc)
ht = aT M (4)",3.1.2 Memory Network,[0],[0]
"We enhance the memory network architecture described above by adding a session encoder (Sordoni et al., 2015) that temporally combines a joint representation of the current utterance encoding, c, (Eq. 3) and the memory vectors, {m1, m2...mt−1}, (Eq. 2).",3.1.3 Sequential Dialogue Encoder Network,[0],[0]
"We combine the context vector c with each memory vector mk, for 1 ≤ k ≤",3.1.3 Sequential Dialogue Encoder Network,[0],[0]
"nk, by concatenating and passing them through a feed forward layer (FF) to produce 128 dimensional context encodings, denoted by {g1, g2...",3.1.3 Sequential Dialogue Encoder Network,[0],[0]
"gt−1} (Eq. 5).
",3.1.3 Sequential Dialogue Encoder Network,[0],[0]
"gk = sigmoid(FF (mk, c))",3.1.3 Sequential Dialogue Encoder Network,[0],[0]
"for 0 ≤ k ≤ t−1 (5) These context encodings are fed as token level inputs into the session encoder, which is a 128 di-
Figure 4: Architecture of the Sequential Dialogue Encoder Network.",3.1.3 Sequential Dialogue Encoder Network,[0],[0]
"The feed-forward networks share weights across all memories.
",3.1.3 Sequential Dialogue Encoder Network,[0],[0]
mensional BiGRU layer.,3.1.3 Sequential Dialogue Encoder Network,[0],[0]
"The final state of the session encoder represents the dialogue context encoding ht (Eq. 6).
",3.1.3 Sequential Dialogue Encoder Network,[0],[0]
"ht = BiGRUs({g1, g2, ...gt−1}) (6)",3.1.3 Sequential Dialogue Encoder Network,[0],[0]
The architecture is depicted in Figure 4.,3.1.3 Sequential Dialogue Encoder Network,[0],[0]
"For all our experiments we use a stacked BiRNN tagger to jointly model domain classification, intent classification and slot-filling, similar to the approach described in (Hakkani-Tür et al., 2016).",3.2 Tagger Architecture,[0],[0]
We feed learned 256 dimensional embeddings corresponding to the current utterance tokens into the tagger.,3.2 Tagger Architecture,[0],[0]
The first RNN layer uses GRU cells with 256 dimensions (128 in each direction) as in equation 7.,3.2 Tagger Architecture,[0],[0]
"The token embeddings are fed into the token level inputs of the first RNN layer to produce the token level outputs o1 = {o11, o12...o1nt}.
o1 = BiGRU1(ut) (7)
",3.2 Tagger Architecture,[0],[0]
"The second layer uses Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) cells with 256 dimensions (128 in both dimensions).",3.2 Tagger Architecture,[0],[0]
We use a LSTM based second layer since that improved slot-filling performance on the validation set for all architectures.,3.2 Tagger Architecture,[0],[0]
We apply dropout to the outputs of both layers.,3.2 Tagger Architecture,[0],[0]
The initial states of both forward and backward LSTMs of the second tagger layer are initialized with the dialogue encoding ht as in equation 8.,3.2 Tagger Architecture,[0],[0]
"The token level outputs of the first RNN layer, o1, are fed as input
into the second RNN layer to produce token level outputs o2 = {o21, o22...o2nt} and the final state s2.
",3.2 Tagger Architecture,[0],[0]
"o2, s2 = BiLSTM2(o1, ht) (8)
The final state of the second layer, s2, is used as input to classification layers for domain and intent classification.
",3.2 Tagger Architecture,[0],[0]
"pdomain = softmax(Us2)
pintent = sigmoid(V s2) (9)
",3.2 Tagger Architecture,[0],[0]
"The token level outputs of the second layer, o2, are used as input to a softmax layer that outputs the IOB slot labels.",3.2 Tagger Architecture,[0],[0]
"This results in a softmax layer with 2N+1 dimensions for a domain with N slots.
",3.2 Tagger Architecture,[0],[0]
psloti = softmax(So 2 i ) for 0 ≤,3.2 Tagger Architecture,[0],[0]
"i ≤ nt
(10) The architecture is depicted in Figure 5.",3.2 Tagger Architecture,[0],[0]
"We crowd sourced multi-turn dialogue sessions for 3 tasks: buying movie tickets, searching for a restaurant and reserving tables at a restaurant.",4 Dataset,[0],[0]
Our data collection process comprises of two steps: (i) Generating user-agent interactions comprising of dialog acts and slots based on the interplay of a simulated user and a rule based dialogue policy.,4 Dataset,[0],[0]
(ii) Using a crowd sourcing platform to elicit natural language utterances that align with the semantics of the generated interactions.,4 Dataset,[0],[0]
The goal of the spoken language understanding module of our dialogue system is to map each user utterance into frame based semantics that can be processed by the downstream components.,4 Dataset,[0],[0]
Tables describing the intents and slots present in the dataset can be found in the appendix.,4 Dataset,[0],[0]
"We use a stochastic agenda-based user simulator (Schatzmann et al., 2007; Shah et al., 2016) for interplay with our rule based system policy.",4 Dataset,[0],[0]
"The user goal is specified in terms of a tuple of slots, which denote the user constraints.",4 Dataset,[0],[0]
"Some constraints might be unspecified, in which case the user is indifferent to the value of those slots.",4 Dataset,[0],[0]
"At any given turn, the simulator samples a user dialogue act from a set of acceptable actions based on (i) the user goal and agenda that includes slots that still need to be specified, (ii) a randomly chosen user profile (co-operative/aggressive, verbose/succinct etc.)",4 Dataset,[0],[0]
"and (iii) the previous user and
system actions.",4 Dataset,[0],[0]
"Based on the chosen user dialogue act, the rule based policy might make a backend call to inquire for restaurant or movie availability.",4 Dataset,[0],[0]
"Based on the user act and the backend response the system responds back with a dialogue act or a combination of dialogue acts, based on a hand designed rule based policy.",4 Dataset,[0],[0]
These generated interactions were then translated to their natural language counterparts and sent out to crowdworkers for paraphrasing into natural language human-machine dialogues.,4 Dataset,[0],[0]
The simulator and policy were also extended to handle multiple goals spanning different domains.,4 Dataset,[0],[0]
"In this set-up, the user goal for the simulator would include multiple tasks and slot values could be conditioned on the previous task, for example, the simulator would ask for booking a table ”after the movie”, or search for a restaurant ”near the theater”.",4 Dataset,[0],[0]
The set of slots supported by the simulator is enumerated in Table 1.,4 Dataset,[0],[0]
"We collected 1319 dialogues for restaurant reservation, 976 dialogues for finding restaurants and 1048 dialogues for buying movie tickets.",4 Dataset,[0],[0]
"All single domain datasets were
used for training.",4 Dataset,[0],[0]
"The multi-domain simulator was used to collect 467 dialogues for training, 50 for validation and 273 for the test set.",4 Dataset,[0],[0]
"Since the natural language dialogues were paraphrased versions of known dialogue- act and slot combinations, they were automatically labeled.",4 Dataset,[0],[0]
"These labels were verified by an expert annotator, and turns with missing annotations were manually annotated by the expert.",4 Dataset,[0],[0]
"As described in the previous section, we train our models on a large set of single domain dialogue datasets and a small set of multi-domain dialogues.",5 Dialogue Recombination,[0],[0]
"These models are then evaluated on a test set composed of multi-domain dialogues, where the user attempts to fulfill multiple goals spanning several domains.",5 Dialogue Recombination,[0],[0]
This results in a distribution drift that might result in performance degradation.,5 Dialogue Recombination,[0],[0]
"To counter this drift in the training-test data distributions we device a dialogue recombination scheme to generate multi-domain dialogues from single domain training datasets.
",5 Dialogue Recombination,[0],[0]
"The key idea behind the recombination approach is the conditional independence of sub-dialogues aimed at performing distinct tasks (Grosz and Sidner, 1986).",5 Dialogue Recombination,[0],[0]
"We exploit the presence of task intents, or intents that denote a switch in the primary task the user is trying to perform, since they are a strong indicator of a switch in the focus of the dialogue.",5 Dialogue Recombination,[0],[0]
"We exploit the independence of the sub-dialogue following these intents from the previous dialogue context, to generate synthetic dialogues with multi-domain context.",5 Dialogue Recombination,[0],[0]
"The recombination process is described as follows: Let a dialogue d be defined as a sequence of turns and corresponding semantic labels (domain, intent and slot annotations) {(td1, fd1), (td2, fd2), ...(tdnd , fdnd}.",5 Dialogue Recombination,[0],[0]
"To obtain a re-combined dataset composed of dialogues from dataset dataset1 and dataset2, we repeat the following steps 10000 times, for each combination of (dataset1, dataset2) from the three single domain datasets.
",5 Dialogue Recombination,[0],[0]
"• Sample dialogues x and y from dataset1 and dataset2 respectively.
• Find the first user utterance labeled with a task intent in y. Let this be turn l.
•",5 Dialogue Recombination,[0],[0]
Randomly sample an insertion point in dialogue x.,5 Dialogue Recombination,[0],[0]
"Let this be turn k.
•",5 Dialogue Recombination,[0],[0]
"The new recombined dialogue is {(tx1, fx1), ...(txk, fxk), (tyl, fyl), ...(tyny , fyny)}.
",5 Dialogue Recombination,[0],[0]
A sample dialogue generated using the above procedure is described in table 2.,5 Dialogue Recombination,[0],[0]
We drop the utterances from dialogue x following the insertion point (turn k) in the recombined dialogue since these turns become ambiguous or confusing in the absence of preceding context.,5 Dialogue Recombination,[0],[0]
In a sense our approach is one of partial dialogue recombination.,5 Dialogue Recombination,[0],[0]
"We compare the domain classification, intent classification and slot-filling performances, and the overall frame error rates of the encoder-decoder, memory network and sequential dialogue encoder network on the dataset described above.",6 Experiments,[0],[0]
"The frame error rate of a SLU system is the percentage of utterances where it makes a wrong prediction i.e. any of domain, intent or slot is predicted incorrectly.",6 Experiments,[0],[0]
We trained all 3 models with RMSProp for 100000 training steps with a batch size of 100.,6 Experiments,[0],[0]
We started with a learning rate of 0.0003 which was decayed by a factor of 0.95 every 3000 steps.,6 Experiments,[0],[0]
Gradient norms were clipped if they exceed a magnitude of 2.5.,6 Experiments,[0],[0]
"All model and optimization hyper-parameters were chosen based on a grid search, to minimize validation set frame error rates.
",6 Experiments,[0],[0]
"We restrict the model vocabularies to contain only tokens occurring more than 10 times in the training set, to prevent over-fitting to training set entities.",6 Experiments,[0],[0]
Digits were replaced with a special ”#” token to allow better generalization to unseen numbers.,6 Experiments,[0],[0]
The dialogue history was padded to 40 utterances for batch processing.,6 Experiments,[0],[0]
We report results with and without the recombined dataset in Table 3.,6 Experiments,[0],[0]
"The encoder decoder model trained on just the previous turn context performs worst on almost all metrics, irrespective of the presence of recombined data.",7 Results,[0],[0]
"This can be explained by worse performance on in-dialogue utterances, where just the previous turn context isn’t sufficient to accurately identify the domain, and in several cases, the intents and slots of the utterance.",7 Results,[0],[0]
"The memory network is the best performing model in the absence of recombined data, indicating that
the model is able to encode additional context effectively to improve performance on all tasks, even when only a small amount of multi-domain data is available.",7 Results,[0],[0]
The Sequential dialogue encoder network performs slightly worse than the memory network in the absence of recombined data.,7 Results,[0],[0]
This could be explained by the model over-fitting to the single domain context seen during training and failure to utilize context effectively in a multi-domain setting.,7 Results,[0],[0]
In the presence of recombined dialogues it outperforms all other implementations.,7 Results,[0],[0]
"Apart from increasing the noise in the dialogue context, adding recombined dialogues to the training set increases the average turn length of the training data, bringing it closer to that of the test dialogues.",7 Results,[0],[0]
"Our augmentation approach is, in spirit, an extension of the data recombination described in (Jia and Liang, 2016) to conversations.",7 Results,[0],[0]
"We hypothesize that the presence of synthetic con-
text has a regularization-like effect on the models.",7 Results,[0],[0]
"Similar effects were observed by (Jia and Liang, 2016), where training with longer, syntheticallyaugmented utterances resulted in improved semantic parsing performance on a simpler test set.",7 Results,[0],[0]
This is also supported by the observation that performance improvements obtained by addition of recombined data increase as the complexity of the model increases.,7 Results,[0],[0]
"Table 4 demonstrates an example dialogue from the test set, along with the gold and model annotations from all 3 models.",8 Discussion and Conclusions,[0],[0]
"We observe that Encoder Decoder (ED) and Sequential Dialogue Encoder Network (SDEN) are able to successfully identify the domain, intent and slots, while the Memory Network (MN) fails to identify the movie name.
",8 Discussion and Conclusions,[0],[0]
"Looking at the attention distributions, we notice that the MN attention is very diffused, whereas SDEN is focusing on the most recent last 2 utterances, which directly identify the domain and the presence of the movie slot in the final user utterance.",8 Discussion and Conclusions,[0],[0]
ED is also able to identify the presence of a movie in the final user utterance from the previous utterance context.,8 Discussion and Conclusions,[0],[0]
Table 5 displays another example where the SDEN model outperforms both MN and ED.,8 Discussion and Conclusions,[0],[0]
Constrained to just the previous utterance ED is unable to correctly identify the domain of the user utterance.,8 Discussion and Conclusions,[0],[0]
"The MN model correctly identifies the domain, using its strong focus on the task-intent bearing utterance, but it is unable to identify the presence of a restaurant in the user utterance.",8 Discussion and Conclusions,[0],[0]
This highlights its failure to combine context from multiple history utterances.,8 Discussion and Conclusions,[0],[0]
"On the other hand, as indicated by its attention distribution on the final
two utterances, SDEN is able to successfully combine context from the dialogue to correctly identify the domain and the restaurant name from the user utterance, despite the presence of several outof-vocabulary tokens.",8 Discussion and Conclusions,[0],[0]
The above two examples hint that SDEN performs better in scenarios where multiple history utterances encode complementary information that could be useful to interpret user utterances.,8 Discussion and Conclusions,[0],[0]
"This is usually the case in more natural goal oriented dialogues, where several tasks and sub tasks go in and out of the focus of the conversation (Grosz, 1979).",8 Discussion and Conclusions,[0],[0]
"On the other hand, we also observed that SDEN performs significantly worse in the absence of recombined data.",8 Discussion and Conclusions,[0],[0]
Due to its complex architecture and a much larger set of parameters SDEN is prone to over-fitting in low data scenarios.,8 Discussion and Conclusions,[0],[0]
"In this paper, we collect a multi-domain dataset of goal oriented human-machine conversations and analyze and compare the SLU performance of multiple neural network based model architectures that can encode varying amounts of context.",8 Discussion and Conclusions,[0],[0]
"Our experiments suggest that encoding more context from the dialogue, and enabling the model to combine contextual information in a sequential order results in a reduction in overall frame error rate.",8 Discussion and Conclusions,[0],[0]
"We also introduce a data augmentation scheme to generate longer dialogues with richer context, and empirically demonstrate that it results in performance improvement for multiple model architectures.",8 Discussion and Conclusions,[0],[0]
"We would like to thank Pararth Shah, Abhinav Rastogi, Anna Khasin and Georgi Nikolov for their help with the user-machine conversation data collection and labeling.",9 Acknowledgements,[0],[0]
We would also like to thank the anonymous reviewers for their insightful comments.,9 Acknowledgements,[0],[0]
Spoken Language Understanding (SLU) is a key component of goal oriented dialogue systems that would parse user utterances into semantic frame representations.,abstractText,[0],[0]
Traditionally SLU does not utilize the dialogue history beyond the previous system turn and contextual ambiguities are resolved by the downstream components.,abstractText,[0],[0]
"In this paper, we explore novel approaches for modeling dialogue context in a recurrent neural network (RNN) based language understanding system.",abstractText,[0],[0]
"We propose the Sequential Dialogue Encoder Network, that allows encoding context from the dialogue history in chronological order.",abstractText,[0],[0]
"We compare the performance of our proposed architecture with two context models, one that uses just the previous turn context and another that encodes dialogue context in a memory network, but loses the order of utterances in the dialogue history.",abstractText,[0],[0]
Experiments with a multi-domain dialogue dataset demonstrate that the proposed architecture results in reduced semantic frame error rates.,abstractText,[0],[0]
Sequential Dialogue Context Modeling for Spoken Language Understanding,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 496–505 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1046",text,[0],[0]
Conversational agents include task-oriented dialog systems and non-task-oriented chatbots.,1 Introduction,[0],[0]
"Dialog systems focus on helping people complete specific tasks in vertical domains (Young et al., 2010), while chatbots aim to naturally and meaningfully converse with humans on open domain topics (Ritter et al., 2011).",1 Introduction,[0],[0]
Existing work on building chatbots includes generation -based methods and retrieval-based methods.,1 Introduction,[0],[0]
"Retrieval based chatbots enjoy the advantage of informative and fluent responses, because they select a proper response for
∗ Corresponding Author
the current conversation from a repository with response selection algorithms.",1 Introduction,[0],[0]
"While most existing work on retrieval-based chatbots studies response selection for single-turn conversation (Wang et al., 2013) which only considers the last input message, we consider the problem in a multi-turn scenario.",1 Introduction,[0],[0]
"In a chatbot, multi-turn response selection takes a message and utterances in its previous turns as input and selects a response that is natural and relevant to the whole context.
",1 Introduction,[0],[0]
The key to response selection lies in inputresponse matching.,1 Introduction,[0],[0]
"Different from single-turn conversation, multi-turn conversation requires matching between a response and a conversation context in which one needs to consider not only the matching between the response and the input message but also matching between responses and utterances in previous turns.",1 Introduction,[0],[0]
"The challenges of the task include (1) how to identify important information (words, phrases, and sentences) in context, which is crucial to selecting a proper response and leveraging relevant information in matching; and (2) how to model relationships among the utterances in the context.",1 Introduction,[0],[0]
Table 1 illustrates the challenges with an example.,1 Introduction,[0],[0]
"First, “hold a drum class” and “drum” in context are very important.",1 Introduction,[0],[0]
"Without them, one may find responses relevant to the message (i.e., the fifth utterance of the context) but nonsense in the context (e.g., “what lessons do you want?”).",1 Introduction,[0],[0]
"Second, the message highly depends on the second utterance in the context, and
496
the order of the utterances matters in response selection: exchanging the third utterance and the fifth utterance may lead to different responses.",1 Introduction,[0],[0]
"Existing work, however, either ignores relationships among utterances when concatenating them together (Lowe et al., 2015), or loses important information in context in the process of converting the whole context to a vector without enough supervision from responses (e.g., by a hierarchical RNN (Zhou et al., 2016)).
",1 Introduction,[0],[0]
"We propose a sequential matching network (SMN), a new context based matching model that can tackle both challenges in an end-to-end way.",1 Introduction,[0],[0]
The reason that existing models lose important information in the context is that they first represent the whole context as a vector and then match the context vector with a response vector.,1 Introduction,[0],[0]
"Thus, responses in these models connect with the context until the final step in matching.",1 Introduction,[0],[0]
"To avoid information loss, SMN matches a response with each utterance in the context at the beginning and encodes important information in each pair into a matching vector.",1 Introduction,[0],[0]
The matching vectors are then accumulated in the utterances’ temporal order to model their relationships.,1 Introduction,[0],[0]
The final matching degree is computed with the accumulation of the matching vectors.,1 Introduction,[0],[0]
"Specifically, for each utterance-response pair, the model constructs a word-word similarity matrix and a sequence-sequence similarity matrix by the word embeddings and the hidden states of a recurrent neural network with gated recurrent units (GRU) (Chung et al., 2014) respectively.",1 Introduction,[0],[0]
"The two matrices capture important matching information in the pair on a word level and a segment (word subsequence) level respectively, and the information is distilled and fused as a matching vector through an alternation of convolution and pooling operations on the matrices.",1 Introduction,[0],[0]
"By this means, important information from multiple levels of granularity in context is recognized under sufficient supervision from the response and carried into matching with minimal loss.",1 Introduction,[0],[0]
The matching vectors are then uploaded to another GRU to form a matching score for the context and the response.,1 Introduction,[0],[0]
The GRU accumulates the pair matching in its hidden states in the chronological order of the utterances in context.,1 Introduction,[0],[0]
It models relationships and dependencies among the utterances in a matching fashion and has the utterance order supervise the accumulation of pair matching.,1 Introduction,[0],[0]
"The matching degree of the context and the response is computed by a logit
model with the hidden states of the GRU. SMN extends the powerful “2D” matching paradigm in text pair matching for single-turn conversation to context based matching for multi-turn conversation, and enjoys the advantage of both important information in utterance-response pairs and relationships among utterances being sufficiently preserved and leveraged in matching.
",1 Introduction,[0],[0]
"We test our model on the Ubuntu dialogue corpus (Lowe et al., 2015) which is a large scale publicly available English data set for research in multi-turn conversation.",1 Introduction,[0],[0]
"The results show that our model can significantly outperform state-ofthe-art methods, and improvement to the best baseline model on R10@1 is over 6%.",1 Introduction,[0],[0]
"In addition to the Ubuntu corpus, we create a human-labeled Chinese data set, namely the Douban Conversation Corpus, and test our model on it.",1 Introduction,[0],[0]
"In contrast to the Ubuntu corpus in which data is collected from a specific domain and negative candidates are randomly sampled, conversations in this data come from the open domain, and response candidates in this data set are collected from a retrieval engine and labeled by three human judges.",1 Introduction,[0],[0]
"On this data, our model improves the best baseline model by 3% on R10@1 and 4% on P@1.",1 Introduction,[0],[0]
"As far as we know, Douban Conversation Corpus is the first human-labeled data set for multi-turn response selection and could be a good complement to the Ubuntu corpus.",1 Introduction,[0],[0]
"We have released Douban Conversation Corups and our source code at https://github.com/MarkWuNLP/ MultiTurnResponseSelection
Our contributions in this paper are three-folds: (1) the proposal of a new context based matching model for multi-turn response selection in retrieval-based chatbots; (2) the publication of a large human-labeled data set to research communities; (3) empirical verification of the effectiveness of the model on public data sets.",1 Introduction,[0.9531026928071838],['Step 2: Compute the similarity vector cbs-vd (center-based similarity space vector) for each document d ∈D based on its set of document space vectors Rd and the corresponding centroids C of the positive documents.']
"Recently, building a chatbot with data driven approaches (Ritter et al., 2011; Ji et al., 2014) has drawn significant attention.",2 Related Work,[0],[0]
"Existing work along this line includes retrieval-based methods (Hu et al., 2014; Ji et al., 2014; Wang et al., 2015; Yan et al., 2016; Wu et al., 2016b; Zhou et al., 2016; Wu et al., 2016a) and generation-based methods (Shang et al., 2015; Serban et al., 2015; Vinyals and Le, 2015; Li et al., 2015, 2016; Xing et al.,
2016; Serban et al., 2016).",2 Related Work,[0],[0]
"Our work is a retrievalbased method, in which we study context-based response selection.
",2 Related Work,[0],[0]
"Early studies of retrieval-based chatbots focus on response selection for single-turn conversation (Wang et al., 2013; Ji et al., 2014; Wang et al., 2015; Wu et al., 2016b).",2 Related Work,[0],[0]
"Recently, researchers have begun to pay attention to multi-turn conversation.",2 Related Work,[0],[0]
"For example, Lowe et al. (2015) match a response with the literal concatenation of context utterances.",2 Related Work,[0],[0]
Yan et al. (2016) concatenate context utterances with the input message as reformulated queries and perform matching with a deep neural network architecture.,2 Related Work,[0],[0]
Zhou et al. (2016) improve multi-turn response selection with a multi-view model including an utterance view and a word view.,2 Related Work,[0],[0]
"Our model is different in that it matches a response with each utterance at first and accumulates matching information instead of sentences by a GRU, thus useful information for matching can be sufficiently retained.",2 Related Work,[0],[0]
"Suppose that we have a data set D = {(yi, si, ri)}Ni=1, where si = {ui,1, . . .",3.1 Problem Formalization,[0],[0]
", ui,ni} represents a conversation context with {ui,k}nik=1 as utterances.",3.1 Problem Formalization,[0],[0]
"ri is a response candidate and yi ∈ {0, 1} denotes a label.",3.1 Problem Formalization,[0],[0]
"yi = 1 means ri is a proper response for si, otherwise yi = 0.",3.1 Problem Formalization,[0],[0]
"Our goal is to learn a matching model g(·, ·) with D. For any context-response pair (s, r), g(s, r) measures the matching degree between s and r.",3.1 Problem Formalization,[0],[0]
"We propose a sequential matching network (SMN) to model g(·, ·).",3.2 Model Overview,[0],[0]
"Figure 1 gives the architecture.
",3.2 Model Overview,[0],[0]
SMN first decomposes context-response matching into several utterance-response pair matching and then all pairs matching are accumulated as a context based matching through a recurrent neural network.,3.2 Model Overview,[0],[0]
SMN consists of three layers.,3.2 Model Overview,[0],[0]
"The first layer matches a response candidate with each utterance in the context on a word level and a segment level, and important matching information from the two levels is distilled by convolution, pooling and encoded in a matching vector.",3.2 Model Overview,[0],[0]
The matching vectors are then fed into the second layer where they are accumulated in the hidden states of a recurrent neural network with GRU following the chronological order of the utterances in the context.,3.2 Model Overview,[0],[0]
"The third layer calculates the final matching score with the hidden states of the second layer.
",3.2 Model Overview,[0],[0]
SMN enjoys several advantages over existing models.,3.2 Model Overview,[0],[0]
"First, a response candidate can match each utterance in the context at the very beginning, thus matching information in every utteranceresponse pair can be sufficiently extracted and carried to the final matching score with minimal loss.",3.2 Model Overview,[0],[0]
"Second, information extraction from each utterance is conducted on different levels of granularity and under sufficient supervision from the response, thus semantic structures that are useful for response selection in each utterance can be well identified and extracted.",3.2 Model Overview,[0],[0]
"Third, matching and utterance relationships are coupled rather than separately modeled, thus utterance relationships (e.g., order), as a kind of knowledge, can supervise the formation of the matching score.
",3.2 Model Overview,[0],[0]
"By taking utterance relationships into account, SMN extends the “2D” matching that has proven effective in text pair matching for single-turn response selection to sequential “2D” matching for
context based matching in response selection for multi-turn conversation.",3.2 Model Overview,[0],[0]
"In the following sections, we will describe details of the three layers.",3.2 Model Overview,[0],[0]
"Given an utterance u in a context s and a response candidate r, the model looks up an embedding table and represents u and r as U =",3.3 Utterance-Response Matching,[0],[0]
"[eu,1, . . .",3.3 Utterance-Response Matching,[0],[0]
", eu,nu ] and R =",3.3 Utterance-Response Matching,[0],[0]
"[er,1, . . .",3.3 Utterance-Response Matching,[0],[0]
", er,nr ] respectively, where eu,i, er,i ∈ Rd are the embeddings of the i-th word of u and r respectively.",3.3 Utterance-Response Matching,[0],[0]
U ∈ Rd×nu and R ∈ Rd×nr are then used to construct a word-word similarity matrix M1 ∈ Rnu×nr and a sequence-sequence similarity matrix M2 ∈ Rnu×nr which are two input channels of a convolutional neural network (CNN).,3.3 Utterance-Response Matching,[0],[0]
"The CNN distills important matching information from the matrices and encodes the information into a matching vector v.
Specifically, ∀i, j, the (i, j)-th element of M1 is defined by
e1,i,j = e > u,i · er,j .",3.3 Utterance-Response Matching,[0],[0]
"(1)
M1 models the matching between u and r on a word level.
",3.3 Utterance-Response Matching,[0],[0]
"To construct M2, we first employ a GRU to transform U and R to hidden vectors.",3.3 Utterance-Response Matching,[0],[0]
Suppose that Hu =,3.3 Utterance-Response Matching,[0],[0]
"[hu,1, . . .",3.3 Utterance-Response Matching,[0],[0]
", hu,nu ] are the hidden vectors of U, then ∀i, hu,i ∈",3.3 Utterance-Response Matching,[0],[0]
"Rm is defined by
zi = σ(Wzeu,i +Uzhu,i−1) ri = σ(Wreu,",3.3 Utterance-Response Matching,[0],[0]
"i +Urhu,i−1)
h̃u,i = tanh(Wheu,i +Uh(ri hu,i−1)) hu,i = zi h̃u,i + (1− zi) hu,i−1, (2)
",3.3 Utterance-Response Matching,[0],[0]
"where hu,0 = 0, zi and ri are an update gate and a reset gate respectively, σ(·) is a sigmoid function, and Wz, Wh, Wr, Uz, Ur,Uh are parameters.",3.3 Utterance-Response Matching,[0],[0]
"Similarly, we have Hr =",3.3 Utterance-Response Matching,[0],[0]
"[hr,1, . . .",3.3 Utterance-Response Matching,[0],[0]
", hr,nr ] as the hidden vectors of R. Then, ∀i, j, the (i, j)-th element of M2 is defined by
e2,i,j = h > u,iAhr,j , (3)
where A ∈ Rm×m is a linear transformation.",3.3 Utterance-Response Matching,[0],[0]
"∀i, GRU models the sequential relationship and the dependency among words up to position i and encodes the text segment until the i-th word to a hidden vector.",3.3 Utterance-Response Matching,[0],[0]
"Therefore, M2 models the matching between u and r on a segment level.",3.3 Utterance-Response Matching,[0],[0]
M1 and M2 are then processed by a CNN to form v. ∀f,3.3 Utterance-Response Matching,[0],[0]
"= 1, 2, CNN regards Mf as
an input channel, and alternates convolution and max-pooling operations.",3.3 Utterance-Response Matching,[0],[0]
"Suppose that z(l,f) =[ z",3.3 Utterance-Response Matching,[0],[0]
"(l,f)",3.3 Utterance-Response Matching,[0],[0]
"i,j ] I(l,f)×J(l,f) denotes the output of feature maps of type-f on layer-l, where z(0,f) =",3.3 Utterance-Response Matching,[0],[0]
"Mf , ∀f = 1, 2.",3.3 Utterance-Response Matching,[0],[0]
"On the convolution layer, we employ a 2D convolution operation with a window size r (l,f)",3.3 Utterance-Response Matching,[0],[0]
"w × r(l,f)h , and define z (l,f)",3.3 Utterance-Response Matching,[0],[0]
"i,j as
z (l,f)",3.3 Utterance-Response Matching,[0],[0]
"i,j = σ(
Fl−1∑
f ′=0
r (l,f) w∑
s=0
r (l,f) h∑
t=0
W (l,f) s,t · z(l−1,f ′) i+s,j+t + b",3.3 Utterance-Response Matching,[0],[0]
"l,k), (4)
where σ(·) is a ReLU, W(l,",3.3 Utterance-Response Matching,[0],[0]
"f) ∈ Rr(l,f)w ×r(l,f)h and bl,k are parameters, and Fl−1 is the number of feature maps on the (l − 1)-th layer.",3.3 Utterance-Response Matching,[0],[0]
"A max pooling operation follows a convolution operation and can be formulated as
z (l,f)",3.3 Utterance-Response Matching,[0],[0]
"i,j = max
p (l,f)",3.3 Utterance-Response Matching,[0],[0]
w >,3.3 Utterance-Response Matching,[0],[0]
"s≥0 max p (l,f) h >t≥0 zi+s,j+t, (5)
where p(l,f)w and p (l,f) h are the width and the height of the 2D pooling respectively.",3.3 Utterance-Response Matching,[0],[0]
The output of the final feature maps are concatenated and mapped to a low dimensional space with a linear transformation as the matching vector v ∈,3.3 Utterance-Response Matching,[0],[0]
"Rq.
",3.3 Utterance-Response Matching,[0],[0]
"According to Equation (1), (3), (4), and (5), we can see that by learning word embedding and parameters of GRU from training data, words or segments in an utterance that are useful for recognizing the appropriateness of a response may have high similarity with some words or segments in the response and result in high value areas in the similarity matrices.",3.3 Utterance-Response Matching,[0],[0]
These areas will be transformed and selected by convolution and pooling operations and carry important information in the utterance to the matching vector.,3.3 Utterance-Response Matching,[0],[0]
This is how our model identifies important information in context and leverage it in matching under the supervision of the response.,3.3 Utterance-Response Matching,[0],[0]
We consider multiple channels because we want to capture important matching information on multiple levels of granularity of text.,3.3 Utterance-Response Matching,[0],[0]
"Suppose that [v1, . . .",3.4 Matching Accumulation,[0],[0]
", vn] is the output of the first layer (corresponding to n pairs), at the second layer, a GRU takes [v1, . . .",3.4 Matching Accumulation,[0],[0]
", vn] as an input and encodes the matching sequence into its hidden states Hm =",3.4 Matching Accumulation,[0],[0]
"[h ′ 1, . . .",3.4 Matching Accumulation,[0],[0]
", h ′ n] ∈",3.4 Matching Accumulation,[0],[0]
Rq×n with a detailed parameterization similar to Equation (2).,3.4 Matching Accumulation,[0],[0]
"This layer has two functions: (1) it models the dependency and the temporal relationship of utterances in the
context; (2) it leverages the temporal relationship to supervise the accumulation of the pair matching as a context based matching.",3.4 Matching Accumulation,[0],[0]
"Moreover, from Equation (2), we can see that the reset gate (i.e., ri) and the update gate (i.e., zi) control how much information from the previous hidden state and the current input flows to the current hidden state, thus important matching vectors (corresponding to important utterances) can be accumulated while noise in the vectors can be filtered out.",3.4 Matching Accumulation,[0],[0]
"With [h′1, . . .",3.5 Matching Prediction and Learning,[0],[0]
", h ′ n",3.5 Matching Prediction and Learning,[0],[0]
"], we define g(s, r) as
g(s, r) =",3.5 Matching Prediction and Learning,[0],[0]
softmax(W2L[h ′,3.5 Matching Prediction and Learning,[0],[0]
"1, . . .",3.5 Matching Prediction and Learning,[0],[0]
", h ′ n] + b2), (6)
where W2 and b2 are parameters.",3.5 Matching Prediction and Learning,[0],[0]
"We consider three parameterizations for L[h′1, . . .",3.5 Matching Prediction and Learning,[0],[0]
", h ′ n",3.5 Matching Prediction and Learning,[0],[0]
]: (1) only the last hidden state is used.,3.5 Matching Prediction and Learning,[0],[0]
"Then L[h′1, . . .",3.5 Matching Prediction and Learning,[0],[0]
", h ′ n",3.5 Matching Prediction and Learning,[0],[0]
] = h ′,3.5 Matching Prediction and Learning,[0],[0]
n. (2) the hidden states are linearly combined.,3.5 Matching Prediction and Learning,[0],[0]
"Then, L[h′1, . . .",3.5 Matching Prediction and Learning,[0],[0]
", h ′ n",3.5 Matching Prediction and Learning,[0],[0]
"] =∑n
i=1wih ′",3.5 Matching Prediction and Learning,[0],[0]
"i, where wi ∈ R. (3) we follow (Yang et al., 2016) and employ an attention mechanism to combine the hidden states.",3.5 Matching Prediction and Learning,[0],[0]
"Then, L[h′1, . . .",3.5 Matching Prediction and Learning,[0],[0]
", h ′ n] is defined as
ti = tanh(W1,1hui,nu +W1,2h ′",3.5 Matching Prediction and Learning,[0],[0]
"i + b1), αi =",3.5 Matching Prediction and Learning,[0],[0]
exp(t>i ts)∑,3.5 Matching Prediction and Learning,[0],[0]
"i(exp(t > i ts)) ,
L[h′1, . . .",3.5 Matching Prediction and Learning,[0],[0]
", h ′ n] =
n∑
i=1
αih ′",3.5 Matching Prediction and Learning,[0],[0]
"i, (7)
where W1,1 ∈ Rq×m,W1,2 ∈ Rq×q and b1 ∈",3.5 Matching Prediction and Learning,[0],[0]
Rq are parameters.,3.5 Matching Prediction and Learning,[0],[0]
"h′i and hui,nu are the i-th matching vector and the final hidden state of the i-th utterance respectively.",3.5 Matching Prediction and Learning,[0],[0]
ts ∈,3.5 Matching Prediction and Learning,[0],[0]
"Rq is a virtual context vector which is randomly initialized and jointly learned in training.
",3.5 Matching Prediction and Learning,[0],[0]
"Both (2) and (3) aim to learn weights for {h′1, . . .",3.5 Matching Prediction and Learning,[0],[0]
", h′n} from training data and highlight the effect of important matching vectors in the final matching.",3.5 Matching Prediction and Learning,[0],[0]
"The difference is that weights in (2) are static, because the weights are totally determined by the positions of utterances, while weights in (3) are dynamically computed by the matching vectors and utterance vectors.",3.5 Matching Prediction and Learning,[0],[0]
"We denote our model with the three parameterizations of L[h′1, . . .",3.5 Matching Prediction and Learning,[0],[0]
", h ′ n]",3.5 Matching Prediction and Learning,[0],[0]
"as SMNlast, SMNstatic, and SMNdynamic, and empirically compare them in experiments.
",3.5 Matching Prediction and Learning,[0],[0]
"We learn g(·, ·) by minimizing cross entropy withD. Let Θ denote the parameters of SMN, then the objective function L(D,Θ) of learning can be
formulated as
− N∑
i=1
",3.5 Matching Prediction and Learning,[0],[0]
"[yilog(g(si, ri))",3.5 Matching Prediction and Learning,[0],[0]
"+ (1− yi)log(1− g(si, ri))] .",3.5 Matching Prediction and Learning,[0],[0]
(8),3.5 Matching Prediction and Learning,[0],[0]
"In practice, a retrieval-based chatbot, to apply the matching approach to the response selection, one needs to retrieve a number of response candidates from an index beforehand.",4 Response Candidate Retrieval,[0],[0]
"While candidate retrieval is not the focus of the paper, it is an important step in a real system.",4 Response Candidate Retrieval,[0],[0]
"In this work, we exploit a heuristic method to obtain response candidates from the index.",4 Response Candidate Retrieval,[0],[0]
"Given a message un with {u1, . . .",4 Response Candidate Retrieval,[0],[0]
", un−1} utterances in its previous turns, we extract the top 5 keywords from {u1, . . .",4 Response Candidate Retrieval,[0],[0]
", un−1} based on their tf-idf scores1 and expand un with the keywords.",4 Response Candidate Retrieval,[0],[0]
Then we send the expanded message to the index and retrieve response candidates using the inline retrieval algorithm of the index.,4 Response Candidate Retrieval,[0],[0]
"Finally, we use g(s, r) to rerank the candidates and return the top one as a response to the context.",4 Response Candidate Retrieval,[0],[0]
We tested our model on a publicly available English data set and a Chinese data set published with this paper.,5 Experiments,[0],[0]
"The English data set is the Ubuntu Corpus (Lowe et al., 2015) which contains multi-turn dialogues collected from chat logs of the Ubuntu Forum.",5.1 Ubuntu Corpus,[0],[0]
"The data set consists of 1 million context-response pairs for training, 0.5 million pairs for validation, and 0.5 million pairs for testing.",5.1 Ubuntu Corpus,[0],[0]
"Positive responses are true responses from humans, and negative ones are randomly sampled.",5.1 Ubuntu Corpus,[0],[0]
"The ratio of the positive and the negative is 1:1 in training, and 1:9 in validation and testing.",5.1 Ubuntu Corpus,[0],[0]
"We used the copy shared by Xu et al. (2016) 2 in which numbers, urls, and paths are replaced by special placeholders.",5.1 Ubuntu Corpus,[0],[0]
"We followed (Lowe et al., 2015) and employed recall at position k in n candidates (Rn@k) as evaluation metrics.
",5.1 Ubuntu Corpus,[0],[0]
"1Tf is word frequency in the context, while idf is calculated using the entire index.
",5.1 Ubuntu Corpus,[0],[0]
2https://www.dropbox.com/s/ 2fdn26rj6h9bpvl/ubuntudata.zip?dl=0,5.1 Ubuntu Corpus,[0],[0]
"The Ubuntu Corpus is a domain specific data set, and response candidates are obtained from negative sampling without human judgment.",5.2 Douban Conversation Corpus,[0],[0]
"To further verify the efficacy of our model, we created a new data set with open domain conversations, called the Douban Conversation Corpus.",5.2 Douban Conversation Corpus,[0],[0]
Response candidates in the test set of the Douban Conversation Corpus are collected following the procedure of a retrieval-based chatbot and are labeled by human judges.,5.2 Douban Conversation Corpus,[0],[0]
It simulates the real scenario of a retrievalbased chatbot.,5.2 Douban Conversation Corpus,[0],[0]
"We publish it to research communities to facilitate the research of multi-turn response selection.
",5.2 Douban Conversation Corpus,[0],[0]
"Specifically, we crawled 1.1 million dyadic dialogues (conversation between two persons) longer than 2 turns from Douban group3 which is a popular social networking service in China.",5.2 Douban Conversation Corpus,[0],[0]
"We randomly sampled 0.5 million dialogues for creating a training set, 25 thousand dialouges for creating a validation set, and 1, 000 dialogues for creating a test set, and made sure that there is no overlap between the three sets.",5.2 Douban Conversation Corpus,[0],[0]
"For each dialogue in training and validation, we took the last turn as a positive response for the previous turns as a context and randomly sampled another response from the 1.1 million data as a negative response.",5.2 Douban Conversation Corpus,[0],[0]
"There are 1 million context-response pairs in the training set and 50 thousand pairs in the validation set.
",5.2 Douban Conversation Corpus,[0],[0]
"To create the test set, we first crawled 15 million post-reply pairs from Sina Weibo4 which is the largest microblogging service in China and indexed the pairs with Lucene5.",5.2 Douban Conversation Corpus,[0],[0]
"We took the last turn of each Douban dyadic dialogue in the test set as a message, retrieved 10 response candidates from the index following the method in Section 4, and finally formed a test set with 10, 000 context-response pairs.",5.2 Douban Conversation Corpus,[0],[0]
We recruited three labelers to judge if a candidate is a proper response to the context.,5.2 Douban Conversation Corpus,[0],[0]
A proper response means the response can naturally reply to the message given the whole context.,5.2 Douban Conversation Corpus,[0],[0]
Each pair received three labels and the majority of the labels were taken as the final decision.,5.2 Douban Conversation Corpus,[0],[0]
Table 2 gives the statistics of the three sets.,5.2 Douban Conversation Corpus,[0],[0]
"Note that the Fleiss’ kappa (Fleiss, 1971) of the labeling is 0.41, which indicates that the three labelers reached a relatively high agreement.
",5.2 Douban Conversation Corpus,[0],[0]
"Besides Rn@ks, we also followed the conven-
3https://www.douban.com/group 4http://weibo.com/ 5https://lucenenet.apache.org/
tion of information retrieval and employed mean average precision (MAP) (Baeza-Yates et al., 1999), mean reciprocal rank (MRR) (Voorhees et al., 1999), and precision at position 1 (P@1) as evaluation metrics.",5.2 Douban Conversation Corpus,[0],[0]
"We did not calculate R2@1 because in Douban corpus one context could have more than one correct responses, and we have to randomly sample one for R2@1, which may bring bias to evaluation.",5.2 Douban Conversation Corpus,[0],[0]
"When using the labeled set, we removed conversations with all negative responses or all positive responses, as models make no difference with them.",5.2 Douban Conversation Corpus,[0],[0]
"There are 6, 670 contextresponse pairs left in the test set.",5.2 Douban Conversation Corpus,[0],[0]
"We considered the following baselines:
Basic models: models in (Lowe et al., 2015) and (Kadlec et al., 2015) including TF-IDF, RNN, CNN, LSTM and BiLSTM.
Multi-view: the model proposed by Zhou et al. (2016) that utilizes a hierarchical recurrent neural network to model utterance relationships.
",5.3 Baseline,[0],[0]
"Deep learning to respond (DL2R): the model proposed by Yan et al. (2016) that reformulates the message with other utterances in the context.
",5.3 Baseline,[0],[0]
"Advanced single-turn matching models: since BiLSTM does not represent the state-ofthe-art matching model, we concatenated the utterances in a context and matched the long text with a response candidate using more powerful models including MV-LSTM (Wan et al., 2016)",5.3 Baseline,[0],[0]
"(2D matching), Match-LSTM (Wang and Jiang, 2015), Attentive-LSTM (Tan et al., 2015) (two attention based models), and Multi-Channel which is described in Section 3.3.",5.3 Baseline,[0],[0]
Multi-Channel is a simple version of our model without considering utterance relationships.,5.3 Baseline,[0],[0]
"We also appended the top 5 tf-idf words in context to the input message, and computed the score between the expanded message and a response with Multi-Channel, denoted as Multi-Channelexp.",5.3 Baseline,[0],[0]
"For baseline models, if their results are available in existing literature (e.g., those on the Ubuntu corpus), we just copied the numbers, otherwise we implemented the models following the settings in the literatures.",5.4 Parameter Tuning,[0],[0]
"All models were implemented using Theano (Theano Development Team, 2016).",5.4 Parameter Tuning,[0],[0]
"Word embeddings were initialized by the results of word2vec (Mikolov et al., 2013) which ran on the training data, and the dimensionality of word vectors is 200.",5.4 Parameter Tuning,[0],[0]
"For Multi-Channel and layer one of our model, we set the dimensionality of the hidden states of GRU as 200.",5.4 Parameter Tuning,[0],[0]
"We tuned the window size of convolution and pooling in {(2, 2), (3, 3)(4, 4)} and chose (3, 3) finally.",5.4 Parameter Tuning,[0],[0]
The number of feature maps is 8.,5.4 Parameter Tuning,[0],[0]
"In layer two, we set the dimensionality of matching vectors and the hidden states of GRU as 50.",5.4 Parameter Tuning,[0],[0]
"The parameters were updated by stochastic gradient descent with Adam algorithm (Kingma and Ba, 2014) on a single Tesla K80 GPU.",5.4 Parameter Tuning,[0],[0]
"The initial learning rate is 0.001, and the parameters of Adam, β1 and β2 are 0.9 and 0.999 respectively.",5.4 Parameter Tuning,[0],[0]
We employed early-stopping as a regularization strategy.,5.4 Parameter Tuning,[0],[0]
"Models were trained in minibatches with a batch size of 200, and the maximum utterance length is 50.",5.4 Parameter Tuning,[0],[0]
"We set the maximum context length (i.e., number of utterances) as 10, because the performance of models does not improve on contexts longer than 10 (details are shown in the Section 5.6).",5.4 Parameter Tuning,[0],[0]
"We padded zeros if the number of utterances in a context is less than 10, otherwise we kept the last 10 utterances.",5.4 Parameter Tuning,[0],[0]
Table 3 shows the evaluation results on the two data sets.,5.5 Evaluation Results,[0],[0]
"Our models outperform baselines
greatly in terms of all metrics on both data sets, with the improvements being statistically significant (t-test with p-value ≤ 0.01, except R10@5 on Douban Corpus).",5.5 Evaluation Results,[0],[0]
Even the state-of-the-art singleturn matching models perform much worse than our models.,5.5 Evaluation Results,[0],[0]
The results demonstrate that one cannot neglect utterance relationships and simply perform multi-turn response selection by concatenating utterances together.,5.5 Evaluation Results,[0],[0]
"Our models achieve significant improvements over Multi-View, which justified our “matching first” strategy.",5.5 Evaluation Results,[0],[0]
"DL2R is worse than our models, indicating that utterance reformulation with heuristic rules is not a good method for utilizing context information.",5.5 Evaluation Results,[0],[0]
"Rn@ks are low on the Douban Corpus as there are multiple correct candidates for a context (e.g., if there are 3 correct responses, then the maximumR10@1 is 0.33).",5.5 Evaluation Results,[0],[0]
SMNdynamic is only slightly better than SMNstatic and SMNlast.,5.5 Evaluation Results,[0],[0]
"The reason might be that the GRU can select useful signals from the matching sequence and accumulate them in the final state with its gate mechanism, thus the efficacy of an attention mechanism is not obvious for the task at hand.",5.5 Evaluation Results,[0],[0]
Visualization: we visualize the similarity matrices and the gates of GRU in layer two using an example from the Ubuntu corpus to further clarify how our model identifies important information in the context and how it selects important matching vectors with the gate mechanism of GRU as described in Section 3.3 and Section 3.4.,5.6 Further Analysis,[0],[0]
The example is {u1: how can unzip many rar ( number for example ) files at once; u2: sure you can do that in bash; u3: okay how?,5.6 Further Analysis,[0],[0]
"u4: are the files all
in the same directory?",5.6 Further Analysis,[0],[0]
u5:,5.6 Further Analysis,[0],[0]
yes they all are; r: then the command glebihan should extract them all from/to that directory}.,5.6 Further Analysis,[0],[0]
It is from the test set and our model successfully ranked the correct response to the top position.,5.6 Further Analysis,[0],[0]
"Due to space limitation, we only visualized M1, M2 and the update gate (i.e. z) in Figure 2.",5.6 Further Analysis,[0],[0]
"We can see that in u1 important words including “unzip”, “rar”, “files” are recognized and carried to matching by “command”, “extract”, and “directory” in r, while u3 is almost useless and thus little information is extracted from it.",5.6 Further Analysis,[0],[0]
"u1 is crucial to response selection and nearly all information from u1 and r flows to the hidden state of GRU, while other utterances are less informative and the corresponding gates are almost “closed” to keep the information from u1 and r until the final state.
",5.6 Further Analysis,[0],[0]
"Model ablation: we investigate the effect of different parts of SMN by removing them one by one from SMNlast, shown in Table 4.",5.6 Further Analysis,[0],[0]
"First, replacing the multi-channel “2D” matching with a neural tensor network (NTN) (Socher et al., 2013) (denoted as ReplaceM ) makes the performance drop dramatically.",5.6 Further Analysis,[0],[0]
This is because NTN only matches a pair by an utterance vector and a response vector and loses important information in the pair.,5.6 Further Analysis,[0],[0]
"Together with the visualization, we can conclude that “2D” matching plays a key role in the “matching first” strategy as it captures the important matching information in each pair with minimal loss.",5.6 Further Analysis,[0],[0]
"Second, the performance drops slightly when replacing the GRU for matching accumulation with a multi-layer perceptron (denoted as ReplaceA).",5.6 Further Analysis,[0],[0]
This indicates that utterance relationships are useful.,5.6 Further Analysis,[0],[0]
"Finally, we left only one channel in matching
and found that M2 is a little more powerful than M1 and we achieve the best results with both of them (except on R10@5 on the Douban Corpus).
",5.6 Further Analysis,[0],[0]
Performance across context length: we study how our model (SMNlast) performs across the length of contexts.,5.6 Further Analysis,[0],[0]
Figure 3 shows the comparison on MAP in different length intervals on the Douban corpus.,5.6 Further Analysis,[0],[0]
"Our model consistently performs better than the baselines, and when contexts become longer, the gap becomes larger.",5.6 Further Analysis,[0],[0]
"The results demonstrate that our model can well capture the dependencies, especially long dependencies, among utterances in contexts.
",5.6 Further Analysis,[0],[0]
Maximum context length: we investigate the influence of maximum context length for SMN.,5.6 Further Analysis,[0],[0]
Figure 4 shows the performance of SMN on Ubuntu Corpus and Douban Corpus with respect to maximum context length.,5.6 Further Analysis,[0],[0]
"From Figure 4, we find that performance improves significantly when the maximum context length is lower than 5, and becomes stable after the context length reaches 10.",5.6 Further Analysis,[0],[0]
"This indicates that context information is important for multi-turn response selection, and we can set the maximum context length as 10 to balance effectiveness and efficiency.
",5.6 Further Analysis,[0],[0]
"Error analysis: although SMN outperforms baseline methods on the two data sets, there are
still several problems that cannot be handled perfectly.
(1) Logical consistency.",5.6 Further Analysis,[0],[0]
"SMN models the context and response on the semantic level, but pays little attention to logical consistency.",5.6 Further Analysis,[0],[0]
This leads to several DSATs in the Douban Corpus.,5.6 Further Analysis,[0],[0]
"For example, given a context {a: Does anyone know Newton jogging shoes?",5.6 Further Analysis,[0],[0]
b: 100 RMB on Taobao.,5.6 Further Analysis,[0],[0]
a: I know that.,5.6 Further Analysis,[0],[0]
"I do not want to buy it because that is a fake which is made in Qingdao ,b: Is it the only reason you do not want to buy it? }, SMN gives a large score to the response { It is not a fake.",5.6 Further Analysis,[0],[0]
I just worry about the date of manufacture}.,5.6 Further Analysis,[0],[0]
"The response is inconsistent with the context on logic, as it claims that the jogging shoes are not fake.",5.6 Further Analysis,[0],[0]
"In the future, we shall explore the logic consistency problem in retrieval-based chatbots.
",5.6 Further Analysis,[0],[0]
(2) No correct candidates after retrieval.,5.6 Further Analysis,[0],[0]
"In the experiment, we prepared 1000 contexts for testing, but only 667 contexts have correct candidates after candidate response retrieval.",5.6 Further Analysis,[0],[0]
"This indicates that there is still room for candidate retrieval components to improve, and only expanding the input message with several keywords in context may not be a perfect approach for candidate retrieval.",5.6 Further Analysis,[0],[0]
"In the future, we will consider advanced methods for retrieving candidates.",5.6 Further Analysis,[0],[0]
We present a new context based model for multiturn response selection in retrieval-based chatbots.,6 Conclusion and Future Work,[0],[0]
Experiment results on open data sets show that the model can significantly outperform the stateof-the-art methods.,6 Conclusion and Future Work,[0],[0]
"Besides, we publish the first human-labeled multi-turn response selection data set to research communities.",6 Conclusion and Future Work,[0],[0]
"In the future, we shall study how to model logical consistency of responses and improve candidate retrieval.",6 Conclusion and Future Work,[0],[0]
We appreciate valuable comments provided by anonymous reviewers and our discussions with Zhao Yan.,7 Acknowledgment,[0],[0]
"This work was supported by the National Natural Science Foundation of China (Grand Nos. 61672081, U1636211, 61370126), Beijing Advanced Innovation Center for Imaging Technology (No.BAICIT-2016001), National High Technology Research and Development Program of China (No.2015AA016004), and the Fund of the State Key Laboratory of Software Development Environment (No.SKLSDE-2015ZX-16).",7 Acknowledgment,[0],[0]
We study response selection for multiturn conversation in retrieval-based chatbots.,abstractText,[0],[0]
"Existing work either concatenates utterances in context or matches a response with a highly abstract context vector finally, which may lose relationships among utterances or important contextual information.",abstractText,[0],[0]
We propose a sequential matching network (SMN) to address both problems.,abstractText,[0],[0]
"SMN first matches a response with each utterance in the context on multiple levels of granularity, and distills important matching information from each pair as a vector with convolution and pooling operations.",abstractText,[0],[0]
The vectors are then accumulated in a chronological order through a recurrent neural network (RNN) which models relationships among utterances.,abstractText,[0],[0]
The final matching score is calculated with the hidden states of the RNN.,abstractText,[0],[0]
An empirical study on two public data sets shows that SMN can significantly outperform stateof-the-art methods for response selection in multi-turn conversation.,abstractText,[0],[0]
Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-Based Chatbots,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2764–2768 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
2764",text,[0],[0]
"Over the past decade the state of the art in language modeling has shifted from N-gram models to feed-forward networks (Bengio et al., 2006), and then to recurrent neural networks (RNNs) that read a list of words sequentially and predict the next word at each position.",1 Introduction,[0],[0]
"Starting with standard recurrent networks (Mikolov et al., 2010) the sequential modeling approach was later improved using the long-short-term memory (LSTM) architecture of (Hochreiter and Schmidhuber, 1997) for further gains (Sundermeyer et al., 2012; Medennikov et al., 2016; Xiong et al., 2017).",1 Introduction,[0],[0]
RNN models give two fundamental advantages over the old N-gram framework.,1 Introduction,[0],[0]
"First, the continuous-space embedding of word identities allows word similarities to be exploited for generalization (Bengio et al., 2006; Mikolov et al., 2013).",1 Introduction,[0],[0]
"Second, the recurrent architecture allows, in principle at least,
an unlimited history to condition the prediction of next words.
",1 Introduction,[0],[0]
"The potential advantage of unlimited history, however, is not commonly used to its full benefit, since the language model (LM) is typically “reset” at the start of each utterance in current stateof-the-art recognition systems (Saon et al., 2017; Xiong et al., 2018).",1 Introduction,[0],[0]
"This presumes that each utterance is independent of the others, and clearly violates what we know about how language and conversation works, as discussed in the next section.",1 Introduction,[0],[0]
"Consequently, there have been many proposals to inject information from a longer context into standard LM architectures, going back to Ngram models (Bellegarda, 2004), or to generalize N-grams LMs to operate across utterance boundaries and speakers (Ji and Bilmes, 2004).",1 Introduction,[0],[0]
"Based on the RNN framework, (Mikolov and Zweig, 2012) proposed augmenting network inputs with a more slowly varying context vector that would encode longer-range properties of the history, such as a latent semantic indexing vector.",1 Introduction,[0],[0]
The problem with these approaches is that the modeler has to make design decisions about how to encapsulate contextual information as network inputs.,1 Introduction,[0],[0]
"Therefore, our approach here is to simply provide the entire conversation history as input to a standard LSTM-LM, and let the network learn the information that is relevant to next-word prediction.
",1 Introduction,[0],[0]
"We start by discussing linguistic phenomena that could potentially help in conversational LM (Section 2), followed by a description of the LSTM model we propose to capture them (Section 3).",1 Introduction,[0],[0]
"Section 4 describes the data and recognition system we used to test our models, with results reported in Section 5.",1 Introduction,[0],[0]
We end with conclusions and future directions.,1 Introduction,[0],[0]
Here we review a few of the conversation-level phenomena that could be used for predicting words from longer context.,2 Conversation-level Phenomena,[0],[0]
"Perhaps the most widely studied effect is topical coherence, or the tendency of words that are semantically related to one or more underlying topics to appear together in the conversation.",2 Conversation-level Phenomena,[0],[0]
"Consequently, topic-related words are bound to re-occur across utterances, or certain related words appear to trigger one another (such as “children” and “school”).",2 Conversation-level Phenomena,[0],[0]
"This should be especially true for conversations in the Switchboard (and Fischer) corpora, which were collected by pairing up strangers to talk about a mutually agreeable topic.
",2 Conversation-level Phenomena,[0],[0]
"Another phenomenon that could lead to words reoccurring is lexical entrainment (Brennan and Clark, 1996), or the tendency of conversants to adopt the same words and phrases.",2 Conversation-level Phenomena,[0],[0]
"Entrainment can also apply to speaking style, so the use of common discourse particles, syntactic patterns (like question tags), or even disfluencies could be triggered across speakers.
",2 Conversation-level Phenomena,[0],[0]
"Other phenomena operate more locally, but across speaker turn boundaries.",2 Conversation-level Phenomena,[0],[0]
"Linguistic conversation analysis has long noted that utterance types come in adjacency pairs (Schegloff, 1968), with preferences for certain pairs over others (like a statement is preferentially followed by agreement rather than disagreement).",2 Conversation-level Phenomena,[0],[0]
"Therefore, words in an utterance should be more predicable based on the previous utterance.",2 Conversation-level Phenomena,[0],[0]
"In the past, this has been modeled by conditioning utterance words on an underlying dialog act label, which in turn is conditioned on adjacent dialog act labels via a dialog act grammar (Stolcke et al., 2000).
",2 Conversation-level Phenomena,[0],[0]
"A good part of conversational behavior has to do with how turn-taking is negotiated (Sacks et al., 1974).",2 Conversation-level Phenomena,[0],[0]
"Speakers use special discourse devices, such as backchannel words and pause fillers, to signal when they want to take the floor, or to signal that the other party should keep the floor.",2 Conversation-level Phenomena,[0],[0]
"Conversants also anticipate the ends of turns and jump in before the other speaker is completely done, making for very efficient use of time.",2 Conversation-level Phenomena,[0],[0]
"As a result of all of these mechanisms, a good portion of conversations consists of overlapping (simultaneous) speaking.",2 Conversation-level Phenomena,[0],[0]
"It was shown (Shriberg et al., 2001) that such overlap locations can be partly predicted by word-based language models.",2 Conversation-level Phenomena,[0],[0]
"This suggests reversing the modeling and using overlap (the tim-
ing of utterances) to help predict the words.",2 Conversation-level Phenomena,[0],[0]
"Our baseline language model is a standard LSTM that models utterances independently from one another, i.e., the history at the onset of each utterance is the start-of-sentence token.",3 Models,[0],[0]
"In fact, we used two version of this basic LSTM-LM:
• Word inputs encoded with one-hot vectors, combined with a jointly trained embedding layer
• Words encoded by multiple-hot vectors corresponding to the letter trigrams making up the words.
",3 Models,[0],[0]
Both types of LSTM-LMs use three 1000- dimensional hidden layers with recurrence.,3 Models,[0],[0]
"The word embedding layer is also of size 1000, and the letter-trigram encoding has size 7190 (the number of unique trigrams in our vocabulary).
",3 Models,[0],[0]
"The main addition for session-level modeling is that the LSTM history consists of all the utterances preceding the current utterance, followed by all words in the current utterance preceding the word to be predicted.",3 Models,[0],[0]
"The preceding utterances are serialized in the order of their onset times, so that the flow of words within an utterance is not disrupted.",3 Models,[0],[0]
The resulting total word history and nextword prediction is depicted in Figure 1.,3 Models,[0],[0]
"Information about utterance boundaries is encoded using a boundary tag, similar to the start-of-sentence token that is commonly used in LMs.
",3 Models,[0],[0]
Several of the conversational phenomena described in Section 2 refer to turn-taking between speakers; to capture this in the model we augment the word input encoding with an extra bit that indicates whether a speaker change occurred.,3 Models,[0],[0]
"This bit is turned on only for the start-of-utterance token.
",3 Models,[0],[0]
"We also want to capture some information about utterance overlap, since, as described earlier,
speech overlap interacts with word choice.",3 Models,[0],[0]
"Possible events to model would be overlap (exceedings a time threshold) at the starts and ends of utterances, or maybe a continuous measure of such overlaps.",3 Models,[0],[0]
"As a first proof of concept we chose to encode only one type of overlap, i.e., when the utterance in question is completely overlapped temporally by the other speaker’s turn.",3 Models,[0],[0]
"This is typical of backchannel acknowledgments (“uh-huh”) and short utterances that attempt to grab the floor (“um”, “but”).",3 Models,[0],[0]
Complete utterance overlap is also encoded by an additional input bit that is turned on for the start-of-utterance token.,3 Models,[0],[0]
"We used a single bidirectional LSTM acoustic model in experiments reported here, trained on the commonly used conversational telephone speech corpora (Switchboard, Fisher, CallHome English), estimating frame-level posterior probabilities for 9000 context-dependent phone units.",4.1 Recognition system,[0],[0]
"The system decodes speech utterances using a 4-gram language model, generating lattices.",4.1 Recognition system,[0],[0]
"These are then expanded to 500-best lists, which in turn are rescored using the various LMs.
",4.1 Recognition system,[0],[0]
"The recognition system and the N-gram LM used in decoding have a vocabulary of 165k words, but the LSTM-LMs are trained on only the 38k words occurring at least twice in the indomain conversational training data.",4.1 Recognition system,[0],[0]
Words outside of the LSTM-LM vocabulary are penalized in rescoring with a constant weight that is empirically optimized on the development set.,4.1 Recognition system,[0],[0]
"Language model training uses the Switchboard1, BBN Switchboard-2, Fisher, and English CallHome transcripts (about 23 million words in total) as well as the UW conversational Web corpus (Bulyko et al., 2003) for pre-training (see below).",4.2 Data,[0],[0]
The N-gram LM used for N-best generation also includes the LDC Hub4 (Broadcast News) corpus.,4.2 Data,[0],[0]
The Switchboard-1 and Switchboard-2 portions of the NIST 2002 CTS test set were used for tuning and development.,4.2 Data,[0],[0]
"Evaluation is carried out on the NIST 2000 CTS test set, consisting of Switchboard (SWB) and CallHome (CH) subsets.
",4.2 Data,[0],[0]
"As an expedient, we refrained from resegmenting utterances based on forced alignments of words, and instead use utterance boundaries as
given in the available transcripts (corresponding to the audio segments used in acoustic training).",4.2 Data,[0],[0]
"Similarly, in testing, we use the presegmented utterances provided by NIST.",4.2 Data,[0],[0]
"No doubt there are inconsistencies in how the different corpora define utterance units, and a consistent, alignment-based resegmentation of all training and test data based on the durations nonspeech regions and/or lexical tagging might give improved results.",4.2 Data,[0],[0]
"All LSTM-LMs are trained using the Microsoft Cognitive Toolkit, or CNTK (Yu et al., 2014; Microsoft Research, 2016) on a Linux-based multiGPU server farm.",4.3 Model training,[0],[0]
"Training is parallelized using CNTK’s distributed stochastic gradient descent (SGD) with 1-bit gradient quantization (Seide et al., 2014).",4.3 Model training,[0],[0]
"We use the CNTK “FsAdaGrad” learning algorithm, which is an implementation of Adam (Kingma and Ba, 2015).
",4.3 Model training,[0],[0]
"All LSTM-LMs are pretrained for one or two epochs on a large corpus of “conversational Web” data (Bulyko et al., 2003), followed by normal training to convergence on the in-domain data.",4.3 Model training,[0],[0]
"Each utterance in the Web data is treated as a single session for purposes of session-based LM, i.e., the extra bits for speaker change and overlap are never turned on.",4.3 Model training,[0],[0]
"When evaluating the session-based LMs on speech test data, the true utterance contexts are not known, and we must use hypothesized words for word histories preceding the current utterance.",5 Results,[0],[0]
"In our case, the histories were obtained using the output of our best recognition system, which uses a combination of acoustic models (Xiong et al., 2018), but excluding the session-based LM.1",5 Results,[0],[0]
"Per-
1We also omitted the final confusion network rescoring stage described in (Xiong et al., 2018).
",5 Results,[0],[0]
"plexity was evaluated on reference transcripts, as is customary.
",5 Results,[0],[0]
"Table 1 shows the effect of session-level modeling and of optional model elements on perplexity, based on LSTMs using letter-trigram encoding.",5 Results,[0],[0]
Baseline is the standard utterance-scope LSTMLM.,5 Results,[0],[0]
"We see a large perplexity reduction of 17- 21% by conditioning on session history words, with smaller incremental reductions from adding speaker change and overlap information.
",5 Results,[0],[0]
The last two table rows show that some of the perplexity gain over the baseline is negated by the use of errorful recognition output for the conversation history.,5 Results,[0],[0]
"It does not make much difference whether the recognized word history is generated by just the subsystem being rescored (“single system”, with 6% word error on SWB) or the full recognition system using multiple acoustic models (“full system”, with about 5% word error rate on SWB and 10% on CH).",5 Results,[0],[0]
"Using recognition output as history, the perplexity degrades about 6% relative for SWB, and 11% on CH, relative to using the true word histories.",5 Results,[0],[0]
"Even with the more errorful recognition on CH, the session-based LM still gives a perplexity reduction of 14% relative to the baseline.
",5 Results,[0],[0]
"Table 2 presents recognition results, comparing baseline LSTM-LMs to the full session-based LSTM-LMs.",5 Results,[0],[0]
Both the letter-trigram and one-word word encoding versions are reported.,5 Results,[0],[0]
"The different models may also be used jointly, using loglinear score combination in rescoring, shown in the third section of the table.",5 Results,[0],[0]
"We also tried iterating the session LM rescoring, after the recognized word histories were updated from the first rescoring pass (shown as “2nd iteration” in the table).
",5 Results,[0],[0]
"Results show that the session-based LM yields between 1% and 4% relative word error reduction for the two word encodings, and test sets.",5 Results,[0],[0]
"When the two word encoding types are combined by log-
linear combination of model scores, the gain from session-based modeling is preserved.",5 Results,[0],[0]
"Iterating the session LM rescoring to improve the word histories did not give consistent gains.
",5 Results,[0],[0]
"Even though the session-based LSTM subsumes all the information used in the standard LSTM, there is an additional gain to be had from combining those two model types (last row in the table).",5 Results,[0],[0]
"Thus, the overall gain from adding the session-based models to the two baseline models is 3-5% relative word error reduction.",5 Results,[0],[0]
"We have proposed a simple generalization of utterance-level LSTM language models aimed at capturing conversational phenomena that operate across utterances and speakers, such as lexical entrainment, adjacency pairs, speech overlap, and topical coherence.",6 Conclusion and Future Work,[0],[0]
"To capture non-local conditioning information, the LSTM-LM is trained to read the entire sequence of utterances making up a conversation, along with side information encoding speaker changes and overlap of utterances.",6 Conclusion and Future Work,[0],[0]
"This is found to reduce perplexity by about 25%, most of which is retained when errorful recognition output is used to represent the word history in previous utterances.",6 Conclusion and Future Work,[0],[0]
"The session-based LM yields up to 5% relative reduction in word error when the utterance- and session-based LMs are combined.
",6 Conclusion and Future Work,[0],[0]
It would be worthwhile to investigate which conversational phenomena are actually being exploited by the session LSTM model.,6 Conclusion and Future Work,[0],[0]
"The ease with which additional information can be input to the LSTM-LM also suggests encoding other conditioning information, such a more details about utterance timing, as well as semantic features that capture topical coherence.",6 Conclusion and Future Work,[0],[0]
"We propose to generalize language models for conversational speech recognition to allow them to operate across utterance boundaries and speaker changes, thereby capturing conversation-level phenomena such as adjacency pairs, lexical entrainment, and topical coherence.",abstractText,[0],[0]
"The model consists of a long-shortterm memory (LSTM) recurrent network that reads the entire word-level history of a conversation, as well as information about turn taking and speaker overlap, in order to predict each next word.",abstractText,[0],[0]
"The model is applied in a rescoring framework, where the word history prior to the current utterance is approximated with preliminary recognition results.",abstractText,[0],[0]
"In experiments in the conversational telephone speech domain (Switchboard) we find that such a model gives substantial perplexity reductions over a standard LSTM-LM with utterance scope, as well as improvements in word error rate.",abstractText,[0],[0]
Session-level Language Modeling for Conversational Speech,title,[0],[0]
"Deep learning techniques have been very successful in several domains, like object recognition in images (e.g Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; Szegedy et al., 2015; He et al., 2016), machine translation (e.g. Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016;",1 Introduction,[0],[0]
"Gehring et al., 2016) and speech recognition (e.g. Graves et al., 2013; Hannun et al., 2014; Chorowski et al., 2015; Chan et al., 2016; Collobert et al., 2016).",1 Introduction,[0],[0]
Several arguments have been brought forward to justify these empirical results.,1 Introduction,[0],[0]
"From a representational point of view, it has been argued that deep networks can efficiently
1Université of Montréal, Montréal, Canada 2DeepMind, London, United Kingdom 3Google Brain, Mountain View, United States 4CIFAR Senior Fellow.",1 Introduction,[0],[0]
"Correspondence to: Laurent Dinh <laurent.dinh@umontreal.ca>.
",1 Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1 Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1 Introduction,[0],[0]
"approximate certain functions (e.g. Montufar et al., 2014; Raghu et al., 2016).",1 Introduction,[0],[0]
"Other works (e.g Dauphin et al., 2014; Sagun et al., 2014; Choromanska et al., 2015) have looked at the structure of the error surface to analyze how trainable these models are.",1 Introduction,[0],[0]
"Finally, another point of discussion is how well these models can generalize (Nesterov & Vial, 2008; Keskar et al., 2017; Zhang et al., 2017).",1 Introduction,[0],[0]
"These correspond, respectively, to low approximation, optimization and estimation error as described by Bottou (2010).
",1 Introduction,[0],[0]
Our work focuses on the analysis of the estimation error.,1 Introduction,[0],[0]
"In particular, different approaches had been used to look at the question of why stochastic gradient descent results in solutions that generalize well (Bottou & LeCun, 2005; Bottou & Bousquet, 2008).",1 Introduction,[0],[0]
"For example, Duchi et al. (2011); Nesterov & Vial (2008); Hardt et al. (2016); Bottou et al. (2016); Gonen & Shalev-Shwartz (2017) rely on the concept of stochastic approximation or uniform stability (Bousquet & Elisseeff, 2002).",1 Introduction,[0],[0]
"Another conjecture that was recently (Keskar et al., 2017) explored, but that could be traced back to Hochreiter & Schmidhuber (1997), relies on the geometry of the loss function around a given solution.",1 Introduction,[0],[0]
"It argues that flat minima, for some definition of flatness, lead to better generalization.",1 Introduction,[0],[0]
"Our work focuses on this particular conjecture, arguing that there are critical issues when applying the concept of flat minima to deep neural networks, which require rethinking what flatness actually means.
",1 Introduction,[0],[0]
"While the concept of flat minima is not well defined, having slightly different meanings in different works, the intuition is relatively simple.",1 Introduction,[0],[0]
"If one imagines the error as a onedimensional curve, a minimum is flat if there is a wide region around it with roughly the same error, otherwise the minimum is sharp.",1 Introduction,[0],[0]
"When moving to higher dimensional spaces, defining flatness becomes more complicated.",1 Introduction,[0],[0]
In Hochreiter & Schmidhuber (1997) it is defined as the size of the connected region around the minimum where the training loss is relatively similar.,1 Introduction,[0],[0]
"Chaudhari et al. (2017) relies, in contrast, on the curvature of the second order structure around the minimum, while Keskar et al. (2017) looks at the maximum loss in a bounded neighbourhood of the minimum.",1 Introduction,[0],[0]
"All these works rely on the fact that flatness results in robustness to low precision arithmetic or noise in the parameter space, which, using an minimum description length-based argument, suggests a low expected overfitting.
",1 Introduction,[0],[0]
"However, several common architectures and parametrizations in deep learning are already at odds with this conjecture, requiring at least some degree of refinement in the statements made.",1 Introduction,[0],[0]
"In particular, we show how the geometry of the associated parameter space can alter the ranking between prediction functions when considering several measures of flatness/sharpness.",1 Introduction,[0],[0]
"We believe the reason for this contradiction stems from the Bayesian arguments about KLdivergence made to justify the generalization ability of flat minima (Hinton & Van Camp, 1993).",1 Introduction,[0],[0]
"Indeed, KullbackLiebler divergence is invariant to change of parameters whereas the notion of ”flatness” is not.",1 Introduction,[0],[0]
"The demonstrations of Hochreiter & Schmidhuber (1997) are approximately based on a Gibbs formalism and rely on strong assumptions and approximations that can compromise the applicability of the argument, including the assumption of a discrete function space.",1 Introduction,[0],[0]
"For conciseness, we will restrict ourselves to supervised scalar output problems, but several conclusions in this paper can apply to other problems as well.",2 Definitions of flatness/sharpness,[0],[0]
We will consider a function f that takes as input an element x from an input space X and outputs a scalar y.,2 Definitions of flatness/sharpness,[0],[0]
We will denote by fθ the prediction function.,2 Definitions of flatness/sharpness,[0],[0]
"This prediction function will be parametrized by a parameter vector θ in a parameter space Θ. Often, this prediction function will be over-parametrized and two parameters (θ, θ′) ∈ Θ2 that yield the same prediction function everywhere, ∀x ∈ X , fθ(x) = fθ′(x), are called observationally equivalent.",2 Definitions of flatness/sharpness,[0],[0]
The model is trained to minimize a continuous loss function L which takes as argument the prediction function fθ.,2 Definitions of flatness/sharpness,[0],[0]
"We will often think of the loss L as a function of θ and adopt the notation L(θ).
",2 Definitions of flatness/sharpness,[0],[0]
"The notion of flatness/sharpness of a minimum is relative, therefore we will discuss metrics that can be used to compare the relative flatness between two minima.",2 Definitions of flatness/sharpness,[0],[0]
"In this section we will formalize three used definitions of flatness in
the literature.
",2 Definitions of flatness/sharpness,[0],[0]
Hochreiter & Schmidhuber (1997) defines a flat minimum as ”a large connected region in weight space where the error remains approximately constant”.,2 Definitions of flatness/sharpness,[0],[0]
"We interpret this formulation as follows:
Definition 1.",2 Definitions of flatness/sharpness,[0],[0]
"Given > 0, a minimum θ, and a loss L, we define C(L, θ, ) as the largest (using inclusion as the partial order over the subsets of Θ) connected set containing θ such that ∀θ′ ∈ C(L, θ, ), L(θ′) < L(θ) + .",2 Definitions of flatness/sharpness,[0],[0]
"The - flatness will be defined as the volume of C(L, θ, ).",2 Definitions of flatness/sharpness,[0],[0]
"We will call this measure the volume -flatness.
",2 Definitions of flatness/sharpness,[0],[0]
"In Figure 1, C(L, θ, ) will be the purple line at the top of the red area if the height is and its volume will simply be the length of the purple line.
",2 Definitions of flatness/sharpness,[0],[0]
Flatness can also be defined using the local curvature of the loss function around the minimum if it is a critical point 1.,2 Definitions of flatness/sharpness,[0],[0]
Chaudhari et al. (2017); Keskar et al. (2017) suggest that this information is encoded in the eigenvalues of the Hessian.,2 Definitions of flatness/sharpness,[0],[0]
"However, in order to compare how flat one minimum versus another, the eigenvalues need to be reduced to a single number.",2 Definitions of flatness/sharpness,[0],[0]
"Here we consider the spectral norm and trace of the Hessian, two typical measurements of the eigenvalues of a matrix.
",2 Definitions of flatness/sharpness,[0],[0]
Additionally Keskar et al. (2017) defines the notion of - sharpness.,2 Definitions of flatness/sharpness,[0],[0]
"In order to make proofs more readable, we will slightly modify their definition.",2 Definitions of flatness/sharpness,[0],[0]
"However, because of norm equivalence in finite dimensional space, our results will transfer to the original definition in full space as well.",2 Definitions of flatness/sharpness,[0],[0]
"Our modified definition is the following:
Definition 2.",2 Definitions of flatness/sharpness,[0],[0]
"Let B2( , θ) be an Euclidean ball centered on a minimum θ with radius .",2 Definitions of flatness/sharpness,[0],[0]
"Then, for a non-negative valued loss function L, the -sharpness will be defined as proportional to
maxθ′∈B2( ,θ) ( L(θ′)− L(θ) ) 1 + L(θ) .
",2 Definitions of flatness/sharpness,[0],[0]
"In Figure 1, if the width of the red area is 2 then the height of the red area is maxθ′∈B2( ,θ) ( L(θ′)− L(θ) ) .
",2 Definitions of flatness/sharpness,[0],[0]
-sharpness can be related to the spectral norm of the Hessian.,2 Definitions of flatness/sharpness,[0],[0]
"Indeed, a second-order Taylor expansion of L around a critical point minimum is written
L(θ′) = L(θ) + 1
2 (θ′ − θ) (∇2L)(θ)(θ′ − θ)T
+ o(‖θ′",2 Definitions of flatness/sharpness,[0],[0]
"− θ‖22).
",2 Definitions of flatness/sharpness,[0],[0]
"In this second order approximation, the -sharpness at θ
1In this paper, we will often assume that is the case when dealing with Hessian-based measures in order to have them welldefined.
would be ∣∣∣∣∣∣(∇2L)(θ)∣∣∣∣∣∣ 2 2
2 ( 1 + L(θ) ) .",2 Definitions of flatness/sharpness,[0],[0]
"Before moving forward to our results, in this section we first introduce the notation used in the rest of paper.",3 Properties of Deep Rectified Networks,[0],[0]
"Most of our results, for clarity, will be on the deep rectified feedforward networks with a linear output layer that we describe below, though they can easily be extended to other architectures (e.g. convolutional, etc.).",3 Properties of Deep Rectified Networks,[0],[0]
Definition 3.,3 Properties of Deep Rectified Networks,[0],[0]
"GivenK weight matrices (θk)k≤K with nk = dim ( vec(θk) ) and n = ∑K k=1 nk, the output y of a deep rectified feedforward networks with a linear output layer is:
y = φrect ( φrect ( · · ·φrect(x · θ1) · · · ) · θK−1 ) · θK ,
where
• x is the input to the model, a high-dimensional vector
• φrect is the rectified elementwise activation function (Jarrett et al., 2009; Nair & Hinton, 2010; Glorot et al., 2011), which is the positive part (zi)i 7→ (max(zi, 0))i.
",3 Properties of Deep Rectified Networks,[0],[0]
"• vec reshapes a matrix into a vector.
",3 Properties of Deep Rectified Networks,[0],[0]
"Note that in our definition we excluded the bias terms, usually found in any neural architecture.",3 Properties of Deep Rectified Networks,[0],[0]
"This is done mainly for convenience, to simplify the rendition of our arguments.",3 Properties of Deep Rectified Networks,[0],[0]
"However, the arguments can be extended to the case that includes biases (see Appendix) .",3 Properties of Deep Rectified Networks,[0],[0]
Another choice is that of the linear output layer.,3 Properties of Deep Rectified Networks,[0],[0]
"Having an output activation function does not affect our argument either: since the loss is a function of the output activation, it can be rephrased as a function of linear pre-activation.
",3 Properties of Deep Rectified Networks,[0],[0]
"Deep rectifier models have certain properties that allows us in section 4 to arbitrary manipulate the flatness of a minimum.
",3 Properties of Deep Rectified Networks,[0],[0]
"An important topic for optimization of neural networks is understanding the non-Euclidean geometry of the parameter space as imposed by the neural architecture (see, for example Amari, 1998).",3 Properties of Deep Rectified Networks,[0],[0]
"In principle, when we take a step in parameter space what we expect to control is the change in the behavior of the model (i.e. the mapping of the input x to the output y).",3 Properties of Deep Rectified Networks,[0],[0]
"In principle we are not interested in the parameters per se, but rather only in the mapping they represent.
",3 Properties of Deep Rectified Networks,[0],[0]
"If one defines a measure for the change in the behavior of the model, which can be done under some assumptions, then, it can be used to define, at any point in the parameter space, a metric that says what is the equivalent change in the parameters for a unit of change in the behavior of the model.",3 Properties of Deep Rectified Networks,[0],[0]
"As it turns out, for neural networks, this metric is not constant over Θ. Intuitively, the metric is related to the curvature, and since neural networks can be highly nonlinear, the curvature will not be constant.",3 Properties of Deep Rectified Networks,[0],[0]
See Amari (1998); Pascanu & Bengio (2014) for more details.,3 Properties of Deep Rectified Networks,[0],[0]
"Coming back to the concept of flatness or sharpness of a minimum, this metric should define the flatness.
",3 Properties of Deep Rectified Networks,[0],[0]
"However, the geometry of the parameter space is more complicated.",3 Properties of Deep Rectified Networks,[0],[0]
"Regardless of the measure chosen to compare two instantiations of a neural network, because of the structure of the model, it also exhibits a large number of symmetric configurations that result in exactly the same behavior.",3 Properties of Deep Rectified Networks,[0],[0]
"Because the rectifier activation has the non-negative homogeneity property, as we will see shortly, one can construct a continuum of points that lead to the same behavior, hence the metric is singular.",3 Properties of Deep Rectified Networks,[0],[0]
"Which means that one can exploit these directions in which the model stays unchanged to shape the neighbourhood around a minimum in such a way that, by most definitions of flatness, this property can be controlled.",3 Properties of Deep Rectified Networks,[0],[0]
"See Figure 2 for a visual depiction, where the flatness (given here as the distance between the different level curves) can be changed by moving along the curve.
",3 Properties of Deep Rectified Networks,[0],[0]
"Let us redefine, for convenience, the non-negative homogeneity property (Neyshabur et al., 2015; Lafond et al., 2016) below.",3 Properties of Deep Rectified Networks,[0],[0]
"Note that beside this property, the reason for studying the rectified linear activation is for its widespread adoption (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; Szegedy et al., 2015; He et al., 2016).
",3 Properties of Deep Rectified Networks,[0],[0]
Definition 4.,3 Properties of Deep Rectified Networks,[0],[0]
"A given a function φ is non-negative homogeneous if
∀(z, α) ∈",3 Properties of Deep Rectified Networks,[0],[0]
"R× R+, φ(αz) = αφ(z)
.
",3 Properties of Deep Rectified Networks,[0],[0]
Theorem 1.,3 Properties of Deep Rectified Networks,[0],[0]
"The rectified function φrect(x) = max(x, 0) is non-negative homogeneous.
",3 Properties of Deep Rectified Networks,[0],[0]
Proof.,3 Properties of Deep Rectified Networks,[0],[0]
"Follows trivially from the constraint that α > 0, given that x >",3 Properties of Deep Rectified Networks,[0],[0]
"0⇒ αx > 0, iff α > 0.
",3 Properties of Deep Rectified Networks,[0],[0]
"For a deep rectified neural network it means that:
φrect ( x · (αθ1) ) · θ2 = φrect(x · θ1) · (αθ2),
meaning that for this one (hidden) layer neural network, the parameters (αθ1, θ2) is observationally equivalent to (θ1, αθ2).",3 Properties of Deep Rectified Networks,[0],[0]
"This observational equivalence similarly holds for convolutional layers.
",3 Properties of Deep Rectified Networks,[0],[0]
"Given this non-negative homogeneity, if (θ1, θ2) 6= (0, 0) then { (αθ1, α −1θ2), α > 0 }
is an infinite set of observationally equivalent parameters, inducing a strong nonidentifiability in this learning scenario.",3 Properties of Deep Rectified Networks,[0],[0]
"Other models like deep linear networks (Saxe et al., 2013), leaky rectifiers (He et al., 2015) or maxout networks (Goodfellow et al., 2013) also have this non-negative homogeneity property.
",3 Properties of Deep Rectified Networks,[0],[0]
"In what follows we will rely on such transformations, in particular we will rely on the following definition:
Definition 5.",3 Properties of Deep Rectified Networks,[0],[0]
"For a single hidden layer rectifier feedforward network we define the family of transformations
Tα : (θ1, θ2) 7→ (αθ1, α−1θ2)
which we refer to as a α-scale transformation.
",3 Properties of Deep Rectified Networks,[0],[0]
"Note that a α-scale transformation will not affect the generalization, as the behavior of the function is identical.",3 Properties of Deep Rectified Networks,[0],[0]
"Also while the transformation is only defined for a single layer rectified feedforward network, it can trivially be extended to any architecture having a single rectified network as a submodule, e.g. a deep rectified feedforward network.",3 Properties of Deep Rectified Networks,[0],[0]
For simplicity and readability we will rely on this definition.,3 Properties of Deep Rectified Networks,[0],[0]
In this section we exploit the resulting strong nonidentifiability to showcase a few shortcomings of some definitions of flatness.,4 Deep Rectified networks and flat minima,[0],[0]
"Although α-scale transformation does not affect the function represented, it allows us to significantly decrease several measures of flatness.",4 Deep Rectified networks and flat minima,[0],[0]
"For another definition of flatness, α-scale transformation show that all minima are equally flat.",4 Deep Rectified networks and flat minima,[0],[0]
Theorem 2.,4.1 Volume -flatness,[0],[0]
"For a one-hidden layer rectified neural network of the form
y = φrect(x · θ1) ·",4.1 Volume -flatness,[0],[0]
"θ2,
and a minimum θ = (θ1, θ2), such that θ1 6= 0 and θ2 6= 0, ∀ > 0",4.1 Volume -flatness,[0],[0]
"C(L, θ, ) has an infinite volume.
",4.1 Volume -flatness,[0],[0]
"We will not consider the solution θ where any of the weight matrices θ1, θ2 is zero, θ1 = 0 or θ2 = 0, as it results in a constant function which we will assume to give poor training performance.",4.1 Volume -flatness,[0],[0]
"For α > 0, the α-scale transformation Tα : (θ1, θ2) 7→ (αθ1, α−1θ2) has Jacobian determinant αn1−n2 , where once again n1 = dim ( vec(θ1) ) and n2 =
dim ( vec(θ2) ) .",4.1 Volume -flatness,[0],[0]
Note that the Jacobian determinant of this linear transformation is the change in the volume induced by Tα and Tα ◦ Tβ = Tαβ .,4.1 Volume -flatness,[0],[0]
"We show below that there is a connected region containing θ with infinite volume and where the error remains approximately constant.
",4.1 Volume -flatness,[0],[0]
Proof.,4.1 Volume -flatness,[0],[0]
We will first introduce a small region with approximately constant error around θ with non-zero volume.,4.1 Volume -flatness,[0],[0]
Given > 0,4.1 Volume -flatness,[0],[0]
"and if we consider the loss function continuous with respect to the parameter, C(L, θ, ) is an open set containing θ.",4.1 Volume -flatness,[0],[0]
"Since we also have θ1 6= 0 and θ2 6= 0, let",4.1 Volume -flatness,[0],[0]
r > 0,4.1 Volume -flatness,[0],[0]
"such that the L∞ ball B∞(r, θ) is in C(L, θ, ) and has empty intersection with {θ′, θ′1 = 0}.",4.1 Volume -flatness,[0],[0]
Let v = (2r)n1+n2 > 0,4.1 Volume -flatness,[0],[0]
"the volume of B∞(r, θ).
",4.1 Volume -flatness,[0],[0]
"Since the Jacobian determinant of Tα is the multiplicative change of induced by Tα, the volume of Tα ( B∞(r, θ) ) is vαn1−n2 .",4.1 Volume -flatness,[0],[0]
"If n1 6= n2, we can arbitrarily grow the volume of Tα ( B∞(r, θ) ) , with error within an -interval of L(θ), by having α tends to +∞ if n1 > n2 or to 0 otherwise.
",4.1 Volume -flatness,[0],[0]
"If n1 = n2, ∀α′ > 0, Tα′ ( B∞(r, θ) ) has volume v. Let
C ′ =",4.1 Volume -flatness,[0],[0]
"⋃ α′>0 Tα′ ( B∞(r, θ) ) .",4.1 Volume -flatness,[0],[0]
"C ′ is a connected region where the error remains approximately constant, i.e. within an -interval of L(θ).
",4.1 Volume -flatness,[0],[0]
Let α = 2 ‖θ1‖∞+r‖θ1‖∞−r .,4.1 Volume -flatness,[0],[0]
"Since
B∞(r, θ) = B∞(r, θ1)×B∞(r, θ2),
where × is the Cartesian set product, we have
Tα ( B∞(r, θ) )",4.1 Volume -flatness,[0],[0]
"= B∞(αr, αθ1)×B∞(α−1r, α−1θ2).
",4.1 Volume -flatness,[0],[0]
"Therefore, Tα ( B∞(r, θ) ) ∩B∞(r, θ) = ∅ (see Figure 3).
",4.1 Volume -flatness,[0],[0]
"Similarly, B∞(r, θ), Tα ( B∞(r, θ) ) , T 2α ( B∞(r, θ) ) , . . .
are disjoint and have volume v.",4.1 Volume -flatness,[0],[0]
"We have also T kα ( B∞(r ′, θ) )",4.1 Volume -flatness,[0],[0]
"= Tαk ( B∞(r ′, θ) ) ∈ C ′. The volume of C ′ is then lower bounded by 0 <",4.1 Volume -flatness,[0],[0]
v + v + v + · · · and is therefore infinite.,4.1 Volume -flatness,[0],[0]
"C(L, θ, ) has then infinite volume too, making the volume -flatness of θ infinite.
",4.1 Volume -flatness,[0],[0]
This theorem can generalize to rectified neural networks in general with a similar proof.,4.1 Volume -flatness,[0],[0]
"Given that every minimum has an infinitely large region (volume-wise) in which the error remains approximately constant, that means that every minimum would be infinitely flat according to the volume -flatness.",4.1 Volume -flatness,[0],[0]
"Since all minima are equally flat, it is not possible to use volume -flatness to gauge the generalization property of a minimum.",4.1 Volume -flatness,[0],[0]
"The non-Euclidean geometry of the parameter space, coupled with the manifolds of observationally equal behavior of the model, allows one to move from one region of the parameter space to another, changing the curvature of the model without actually changing the function.",4.2 Hessian-based measures,[0],[0]
"This approach has been used with success to improve optimization, by moving from a region of high curvature to a region of well behaved
curvature (e.g. Desjardins et al., 2015; Salimans & Kingma, 2016).",4.2 Hessian-based measures,[0],[0]
"In this section we look at two widely used measures of the Hessian, the spectral radius and trace, showing that either of these values can be manipulated without actually changing the behavior of the function.",4.2 Hessian-based measures,[0],[0]
"If the flatness of a minimum is defined by any of these quantities, then it could also be easily manipulated.
",4.2 Hessian-based measures,[0],[0]
Theorem 3.,4.2 Hessian-based measures,[0],[0]
"The gradient and Hessian of the loss L with respect to θ can be modified by Tα.
",4.2 Hessian-based measures,[0],[0]
"Proof.
",4.2 Hessian-based measures,[0],[0]
"L(θ1, θ2) = L(αθ1, α −1θ2),
we have then by differentiation (∇L)(θ1, θ2) =",4.2 Hessian-based measures,[0],[0]
"(∇L)(αθ1, α−1θ2) [ αIn1 0
0 α−1In2 ] ⇔ (∇L)(αθ1, α−1θ2) =",4.2 Hessian-based measures,[0],[0]
"(∇L)(θ1, θ2) [ α−1In1 0
0 αIn2 ] and
(∇2L)(αθ1, α−1θ2)
=
[ α−1In1 0
0 αIn2
] (∇2L)(θ1, θ2) [ α−1In1 0
0 αIn2
] .
",4.2 Hessian-based measures,[0],[0]
"Sharpest direction Through these transformations we can easily find, for any critical point which is a minimum with non-zero Hessian, an observationally equivalent parameter whose Hessian has an arbitrarily large spectral norm.
",4.2 Hessian-based measures,[0],[0]
Theorem 4.,4.2 Hessian-based measures,[0],[0]
"For a one-hidden layer rectified neural network of the form
y = φrect(x · θ1) ·",4.2 Hessian-based measures,[0],[0]
"θ2,
and critical point θ = (θ1, θ2) being a minimum for L, such that (∇2L)(θ) 6= 0, ∀M > 0,∃α > 0, ∣∣∣∣∣∣(∇2L)(Tα(θ))∣∣∣∣∣∣2 ≥ M where ∣∣∣∣∣∣(∇2L)(Tα(θ))∣∣∣∣∣∣2 is
the spectral norm of (∇2L) ( Tα(θ) ) .
",4.2 Hessian-based measures,[0],[0]
Proof.,4.2 Hessian-based measures,[0],[0]
"The trace of a symmetric matrix is the sum of its eigenvalues and a real symmetric matrix can be diagonalized in R, therefore if the Hessian is non-zero, there is one nonzero positive diagonal element.",4.2 Hessian-based measures,[0],[0]
"Without loss of generality, we will assume that this non-zero element of value γ > 0 corresponds to an element in θ1.",4.2 Hessian-based measures,[0],[0]
"Therefore the Frobenius norm
∣∣∣∣∣∣(∇2L)(Tα(θ))∣∣∣∣∣∣F of (∇2L)(αθ1, α−1θ2)
=
[ α−1In1 0
0 αIn2
] (∇2L)(θ1, θ2) [ α−1In1 0
0 αIn2
] .
is lower bounded by α−2γ.
",4.2 Hessian-based measures,[0],[0]
"Since all norms are equivalent in finite dimension, there exists a constant r > 0",4.2 Hessian-based measures,[0],[0]
such that r|||A|||F ≤,4.2 Hessian-based measures,[0],[0]
|||A|||2 for all symmetric matrices A.,4.2 Hessian-based measures,[0],[0]
So by picking α <,4.2 Hessian-based measures,[0],[0]
"√ rγ M , we are
guaranteed that ∣∣∣∣∣∣(∇2L)(Tα(θ))∣∣∣∣∣∣2 ≥M .
",4.2 Hessian-based measures,[0],[0]
Any minimum with non-zero Hessian will be observationally equivalent to a minimum whose Hessian has an arbitrarily large spectral norm.,4.2 Hessian-based measures,[0],[0]
"Therefore for any minimum in the loss function, if there exists another minimum that generalizes better then there exists another minimum that generalizes better and is also sharper according the spectral norm of the Hessian.",4.2 Hessian-based measures,[0],[0]
The spectral norm of critical points’ Hessian becomes as a result less relevant as a measure of potential generalization error.,4.2 Hessian-based measures,[0],[0]
"Moreover, since the spectral norm lower bounds the trace for a positive semi-definite symmetric matrix, the same conclusion can be drawn for the trace.
",4.2 Hessian-based measures,[0],[0]
Further properties of the Hessian are analyzed in Appendix.,4.2 Hessian-based measures,[0],[0]
We have redefined for > 0,4.3 -sharpness,[0],[0]
"the -sharpness of Keskar et al. (2017) as follow
maxθ′∈B2( ,θ) ( L(θ′)− L(θ) )",4.3 -sharpness,[0],[0]
"1 + L(θ)
where B2( , θ) is the Euclidean ball of radius centered on θ.",4.3 -sharpness,[0],[0]
This modification will demonstrate more clearly the issues of that metric as a measure of probable generalization.,4.3 -sharpness,[0],[0]
"If we use K = 2 and (θ1, θ2) corresponding to a non-constant function, i.e. θ1 6= 0 and θ2 6= 0, then we can define α = ‖θ1‖2 .",4.3 -sharpness,[0],[0]
"We will now consider the observationally equivalent parameter Tα(θ1, θ2) =",4.3 -sharpness,[0],[0]
"( θ1‖θ1‖2 , α −1θ2).
Given that ‖θ1‖2 ≤",4.3 -sharpness,[0],[0]
"‖θ‖2, we have that (0, α−1θ2) ∈ B2 ( , Tα(θ) ) , making the maximum loss in this neighborhood at least as high as the best constant-valued function,
incurring relatively high sharpness.",4.3 -sharpness,[0],[0]
"Figure 4 provides a visualization of the proof.
",4.3 -sharpness,[0],[0]
For rectified neural network every minimum is observationally equivalent to a minimum that generalizes as well but with high -sharpness.,4.3 -sharpness,[0],[0]
This also applies when using the full-space -sharpness used by Keskar et al. (2017).,4.3 -sharpness,[0],[0]
"We can prove this similarly using the equivalence of norms in finite dimensional vector spaces and the fact that for c > 0, > 0, ≤ (c + 1) (see Keskar et al. (2017)).",4.3 -sharpness,[0],[0]
"We have not been able to show a similar problem with random subspace -sharpness used by Keskar et al. (2017), i.e. a restriction of the maximization to a random subspace, which could relate to the notion of wide valleys described by Chaudhari et al. (2017).
",4.3 -sharpness,[0],[0]
"By exploiting the non-Euclidean geometry and nonidentifiability of rectified neural networks, we were able to demonstrate some of the limits of using typical definitions of minimum’s flatness as core explanation for generalization.",4.3 -sharpness,[0],[0]
"In the previous section 4 we explored the case of a fixed parametrization, that of deep rectifier models.",5 Allowing reparametrizations,[0],[0]
In this section we demonstrate a simple observation.,5 Allowing reparametrizations,[0],[0]
"If we are allowed to change the parametrization of some function f , we can obtain arbitrarily different geometries without affecting how the function evaluates on unseen data.",5 Allowing reparametrizations,[0],[0]
The same holds for reparametrization of the input space.,5 Allowing reparametrizations,[0],[0]
The implication is that the correlation between the geometry of the parameter space (and hence the error surface) and the behavior of a given function is meaningless if not preconditioned on the specific parametrization of the model.,5 Allowing reparametrizations,[0],[0]
One thing that needs to be considered when relating flatness of minima to their probable generalization is that the choice of parametrization and its associated geometry are arbitrary.,5.1 Model reparametrization,[0],[0]
"Since we are interested in finding a prediction function in a given family of functions, no reparametrization of this family should influence generalization of any of these functions.",5.1 Model reparametrization,[0],[0]
"Given a bijection g onto θ, we can define new transformed parameter η = g−1(θ).",5.1 Model reparametrization,[0],[0]
"Since θ and η represent in different space the same prediction function, they should generalize as well.
",5.1 Model reparametrization,[0],[0]
Let’s call Lη = L ◦ g the loss function with respect to the new parameter η.,5.1 Model reparametrization,[0],[0]
"We generalize the derivation of Subsec-
tion 4.2:
Lη(η) = L ( g(η) ) ⇒",5.1 Model reparametrization,[0],[0]
"(∇Lη)(η) = (∇L) ( g(η) ) (∇g)(η)
⇒ (∇2Lη)(η) =",5.1 Model reparametrization,[0],[0]
"(∇g)(η)T (∇2L) ( g(η) ) (∇g)(η)
+ (∇L) ( g(η) ) (∇2g)(η).
",5.1 Model reparametrization,[0],[0]
"At a differentiable critical point, we have by definition (∇L) ( g(η) )",5.1 Model reparametrization,[0],[0]
"= 0, therefore the transformed Hessian at a
critical point becomes
(∇2Lη)(η) =",5.1 Model reparametrization,[0],[0]
"(∇g)(η)T (∇2L) ( g(η) ) (∇g)(η).
",5.1 Model reparametrization,[0],[0]
This means that by reparametrizing the problem we can modify to a large extent the geometry of the loss function so as to have sharp minima of L in θ correspond to flat minima ofLη in η = g−1(θ) and conversely.,5.1 Model reparametrization,[0],[0]
Figure 5 illustrates that point in one dimension.,5.1 Model reparametrization,[0],[0]
"Several practical (Dinh et al., 2014; Rezende & Mohamed, 2015; Kingma et al., 2016; Dinh et al., 2016) and theoretical works (Hyvärinen & Pajunen, 1999) show how powerful bijections can be.",5.1 Model reparametrization,[0],[0]
"We can also note that the formula for the transformed Hessian at a critical point also applies if g is not invertible, g would just need to be surjective over Θ in order to cover exactly the same family of prediction functions
{fθ, θ ∈ Θ} = {fg(η), η ∈ g−1(Θ)}.
",5.1 Model reparametrization,[0],[0]
"We show in Appendix, bijections that allow us to perturb the relative flatness between a finite number of minima.
Instances of commonly used reparametrization are batch normalization (Ioffe & Szegedy, 2015), or the virtual batch normalization variant (Salimans et al., 2016), and weight normalization (Badrinarayanan et al., 2015; Salimans & Kingma, 2016; Arpit et al., 2016).",5.1 Model reparametrization,[0],[0]
Im et al. (2016) have plotted how the loss function landscape was affected by batch normalization.,5.1 Model reparametrization,[0],[0]
"However, we will focus on weight normalization reparametrization as the analysis will be simpler, but the intuition with batch normalization will be similar.",5.1 Model reparametrization,[0],[0]
"Weight normalization reparametrizes a nonzero weight w as w = s v‖v‖2 with the new parameter being the scale s and the unnormalized weight v 6= 0.
",5.1 Model reparametrization,[0],[0]
"Since we can observe that w is invariant to scaling of v, reasoning similar to Section 3 can be applied with the simpler transformations T ′α : v 7→ αv for α 6= 0.",5.1 Model reparametrization,[0],[0]
"Moreover, since this transformation is a simpler isotropic scaling, the conclusion that we can draw can be actually more powerful with respect to v:
• every minimum has infinite volume -sharpness; • every minimum is observationally equivalent to an
infinitely sharp minimum and to an infinitely flat minimum when considering nonzero eigenvalues of the Hessian;
• every minimum is observationally equivalent to a minimum with arbitrarily low full-space and random subspace -sharpness and a minimum with high full-space -sharpness.
",5.1 Model reparametrization,[0],[0]
This further weakens the link between the flatness of a minimum and the generalization property of the associated prediction function when a specific parameter space has not been specified and explained beforehand.,5.1 Model reparametrization,[0],[0]
"As we conclude that the notion of flatness for a minimum in the loss function by itself is not sufficient to determine its generalization ability in the general case, we can choose to focus instead on properties of the prediction function instead.",5.2 Input representation,[0],[0]
"Motivated by some work in adversarial examples (Szegedy et al., 2014; Goodfellow et al., 2015) for deep neural networks, one could decide on its generalization property by analyzing the gradient of the prediction function on examples.",5.2 Input representation,[0],[0]
"Intuitively, if the gradient is small on typical points from the distribution or has a small Lipschitz constant, then a small change in the input should not incur a large change in the prediction.
",5.2 Input representation,[0],[0]
But this infinitesimal reasoning is once again very dependent of the local geometry of the input space.,5.2 Input representation,[0],[0]
"For an invertible preprocessing ξ−1, e.g. feature standardization, whitening or gaussianization (Chen & Gopinath, 2001), we will call fξ = f ◦ ξ",5.2 Input representation,[0],[0]
the prediction function on the preprocessed input u = ξ−1(x).,5.2 Input representation,[0],[0]
"We can reproduce the derivation in Section 5 to obtain
∂fξ ∂uT
( ξ(u) )",5.2 Input representation,[0],[0]
= ∂f ∂xT ( ξ(u) ),5.2 Input representation,[0],[0]
"∂ξ ∂uT (u).
",5.2 Input representation,[0],[0]
"As we can alter significantly the relative magnitude of the gradient at each point, analyzing the amplitude of the gradient of the prediction function might prove problematic if the choice of the input space have not been explained beforehand.",5.2 Input representation,[0],[0]
"This remark applies in applications involving images, sound or other signals with invariances (Larsen et al., 2015).",5.2 Input representation,[0],[0]
"For example, Theis et al. (2016) show for images how a small drift of one to four pixels can incur a large difference in terms of L2 norm.",5.2 Input representation,[0],[0]
"It has been observed empirically that minima found by standard deep learning algorithms that generalize well tend to be flatter than found minima that did not generalize well (Chaudhari et al., 2017; Keskar et al., 2017).",6 Discussion,[0],[0]
"However, when following several definitions of flatness, we have shown that the conclusion that flat minima should generalize better than sharp ones cannot be applied as is without further context.",6 Discussion,[0],[0]
Previously used definitions fail to account for the complex geometry of some commonly used deep architectures.,6 Discussion,[0],[0]
"In particular, the non-identifiability of the model induced by symmetries, allows one to alter the flatness of a minimum without affecting the function it represents.",6 Discussion,[0],[0]
Additionally the whole geometry of the error surface with respect to the parameters can be changed arbitrarily under different parametrizations.,6 Discussion,[0],[0]
"In the spirit of (Swirszcz et al., 2016), our work indicates that more care is needed to define flatness to avoid degeneracies of the geometry of the model under study.",6 Discussion,[0],[0]
"Also such a concept can not be divorced from the
particular parametrization of the model or input space.",6 Discussion,[0],[0]
"The authors would like to thank Grzegorz Świrszcz for an insightful discussion on the paper, Harm De Vries, Yann Dauphin, Jascha Sohl-Dickstein and César Laurent for useful discussions about optimization, Danilo Rezende for explaining universal approximation using normalizing flows and Kyle Kastner, Adriana Romero, Junyoung Chung, Nicolas Ballas, Aaron Courville, George Dahl, Yaroslav Ganin, Prajit Ramachandran, Çağlar Gülçehre, Ahmed Touati and the ICML reviewers for useful feedback.",Acknowledgements,[0],[0]
"Despite their overwhelming capacity to overfit, deep learning architectures tend to generalize relatively well to unseen data, allowing them to be deployed in practice.",abstractText,[0],[0]
"However, explaining why this is the case is still an open area of research.",abstractText,[0],[0]
"One standing hypothesis that is gaining popularity, e.g. Hochreiter & Schmidhuber (1997); Keskar et al. (2017), is that the flatness of minima of the loss function found by stochastic gradient based methods results in good generalization.",abstractText,[0],[0]
This paper argues that most notions of flatness are problematic for deep models and can not be directly applied to explain generalization.,abstractText,[0],[0]
"Specifically, when focusing on deep networks with rectifier units, we can exploit the particular geometry of parameter space induced by the inherent symmetries that these architectures exhibit to build equivalent models corresponding to arbitrarily sharper minima.",abstractText,[0],[0]
"Furthermore, if we allow to reparametrize a function, the geometry of its parameters can change drastically without affecting its generalization properties.",abstractText,[0],[0]
Sharp Minima Can Generalize For Deep Nets,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2820–2825 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics
We approach this problem from two angles: First, we describe several techniques for speeding up an NMT beam search decoder, which obtain a 4.4x speedup over a very efficient baseline decoder without changing the decoder output. Second, we propose a simple but powerful network architecture which uses an RNN (GRU/LSTM) layer at bottom, followed by a series of stacked fully-connected layers applied at every timestep. This architecture achieves similar accuracy to a deep recurrent model, at a small fraction of the training and decoding cost. By combining these techniques, our best system achieves a very competitive accuracy of 38.3 BLEU on WMT EnglishFrench NewsTest2014, while decoding at 100 words/sec on single-threaded CPU. We believe this is the best published accuracy/speed trade-off of an NMT system.",text,[0],[0]
"Attentional sequence-to-sequence models have become the new standard for machine translation over the last two years, and with the unprecedented improvements in translation accuracy
comes a new set of technical challenges.",1 Introduction,[0],[0]
"One of the biggest challenges is the high training and decoding costs of these neural machine translation (NMT) system, which is often at least an order of magnitude higher than a phrase-based system trained on the same data.",1 Introduction,[0],[0]
"For instance, phrasal MT systems were able achieve single-threaded decoding speeds of 100-500 words/sec on decade-old CPUs (Quirk and Moore, 2007), while Jean et al. (2015) reported single-threaded decoding speeds of 8-10 words/sec on a shallow NMT system.",1 Introduction,[0],[0]
"Wu et al. (2016) was able to reach CPU decoding speeds of 100 words/sec for a deep model, but used 44 CPU cores to do so.",1 Introduction,[0],[0]
"There has been recent work in speeding up decoding by reducing the search space (Kim and Rush, 2016), but little in computational improvements.
",1 Introduction,[0],[0]
"In this work, we consider a production scenario which requires low-latency, high-throughput NMT decoding.",1 Introduction,[0],[0]
"We focus on CPU-based decoders, since GPU/FPGA/ASIC-based decoders require specialized hardware deployment and logistical constraints such as batch processing.",1 Introduction,[0],[0]
Efficient CPU decoders can also be used for ondevice mobile translation.,1 Introduction,[0],[0]
"We focus on singlethreaded decoding and single-sentence processing, since multiple threads can be used to reduce latency but not total throughput.
",1 Introduction,[0],[0]
"We approach this problem from two angles: In Section 4, we describe a number of techniques for improving the speed of the decoder, and obtain a 4.4x speedup over a highly efficient baseline.",1 Introduction,[0],[0]
"These speedups do not affect decoding results, so they can be applied universally.",1 Introduction,[0],[0]
"In Section 5, we describe a simple but powerful network architecture which uses a single RNN (GRU/LSTM) layer at the bottom with a large number of fully-connected (FC) layers on top, and obtains improvements similar to a deep RNN model at a fraction of the training and decoding cost.
",1 Introduction,[0],[0]
2820,1 Introduction,[0],[0]
"The data set we evaluate on in this work is WMT English-French NewsTest2014, which has 380M words of parallel training data and a 3003 sentence test set.",2 Data Set,[0],[0]
The NewsTest2013 set is used for validation.,2 Data Set,[0],[0]
"In order to compare our architecture to past work, we train a word-based system without any data augmentation techniques.",2 Data Set,[0],[0]
"The network architecture is very similar to Bahdanau et al. (2014), and specific details of layer size/depth are provided in subsequent sections.",2 Data Set,[0],[0]
"We use an 80k source/target vocab and perform standard unk-replacement (Jean et al., 2015) on out-of-vocabulary words.",2 Data Set,[0],[0]
Training is performed using an in-house toolkit.,2 Data Set,[0],[0]
"Our baseline decoder is a standard beam search decoder (Sutskever et al., 2014) with several straightforward performance optimizations:
• It is written in pure C++, with no heap allocation done during the core search.",3 Baseline Decoder,[0],[0]
• A candidate list is used to reduce the output softmax from 80k to ~500.,3 Baseline Decoder,[0],[0]
"We run word alignment (Brown et al., 1993) on the training and keep the top 20 context-free translations for each source word in the test sentence.",3 Baseline Decoder,[0],[0]
•,3 Baseline Decoder,[0],[0]
"The Intel MKL library is used for matrix multiplication, as it is the fastest floating point matrix multiplication library for CPUs.",3 Baseline Decoder,[0],[0]
• Early stopping is performed when the top partial hypothesis has a log-score of δ = 3.0 worse than the best completed hypothesis.,3 Baseline Decoder,[0],[0]
• Batching of matrix multiplication is applied when possible.,3 Baseline Decoder,[0],[0]
"Since each sentence is decoded separately, we can only batch over the hypotheses in the beam as well as the input vectors on the source side.",3 Baseline Decoder,[0],[0]
This section describes a number of speedups that can be made to a CPU-based attentional sequenceto-sequence beam decoder.,4 Decoder Speed Improvements,[0],[0]
"Crucially, none of these speedups affect the actual mathematical computation of the decoder, so they can be applied to any network architecture with a guarantee that they will not affect the results.1
1Some speedups apply quantization which leads to small random perturbations, but these change the BLEU score by less than 0.02.
",4 Decoder Speed Improvements,[0],[0]
The model used here is similar to the original implementation of Bahdanau et al. (2014).,4 Decoder Speed Improvements,[0],[0]
"The exact target GRU equation is:
dij = tanh(Wahi−1 +",4 Decoder Speed Improvements,[0],[0]
"Vaxi)·tanh(Uasj) αij = edij∑ j′ e dij′
ci = ∑
j
αijsj
ui = σ(Wuhi−1 + Vuxi +",4 Decoder Speed Improvements,[0],[0]
"Uuci + bu) ri = σ(Wrhi−1 + Vrxi + Urci + br)
ĥi = σ(ri (Whhi−1) +",4 Decoder Speed Improvements,[0],[0]
"Vhxi + Uhci + bh) hi = uihi−1 + (1− ui)ĥi
Where W∗, V∗, U∗, b∗ are learned parameters, sj is the hidden vector of the jth source word, hi−1 is the previous target recurrent vector, xi is the target input (e.g., embedding of previous word).
",4 Decoder Speed Improvements,[0],[0]
"We also denote the various hyperparameters: b for the beam size, r for the recurrent hidden size, e is the embedding size, |S| for the source sentence length, and |T | for the target sentence length, |E| is the vocab size.",4 Decoder Speed Improvements,[0],[0]
"Although CPU-based matrix multiplication libraries are highly optimized, they typically only operate on 32/64-bit floats, even though DNNs can almost always operate on much lower precision without degredation of accuracy (Han et al., 2016).",4.1 16-Bit Matrix Multiplication,[0],[0]
"However, low-precision math (1-bit to 7-bit) is difficult to implement efficiently on the CPU, and even 8-bit math has limited support in terms of vectorized (SIMD) instruction sets.",4.1 16-Bit Matrix Multiplication,[0],[0]
"Here, we use 16-bit fixed-point integer math, since it has firstclass SIMD support and requires minimal changes to training.",4.1 16-Bit Matrix Multiplication,[0],[0]
"Training is still performed with 32-bit floats, but we clip the weights to the range [-1.0, 1.0] the relu activation to [0.0, 10.0] to ensure that all values fit into 16-bits with high precision.",4.1 16-Bit Matrix Multiplication,[0],[0]
"A reference implementation of 16-bit multiplication in C++/SSE2 is provided in the supplementary material, with a thorough description of lowlevel details.2
A comparison between our 16-bit integer implementation and Intel MKL’s 32-bit floating point multiplication is given in Figure 1.",4.1 16-Bit Matrix Multiplication,[0],[0]
"We can see that 16-bit multiplication is 2x-3x faster than 32- bit multiplication for batch sizes between 2 and 8, which is the typical range of the beam size",4.1 16-Bit Matrix Multiplication,[0],[0]
"b. We
2Included as ancillary file in Arxiv submission, on right side of submission page.
",4.1 16-Bit Matrix Multiplication,[0],[0]
"are able to achieve greater than a 2x speedup in certain cases because we pre-process the weight matrix offline to have optimal memory layout, which is a capability BLAS libraries do not have.",4.1 16-Bit Matrix Multiplication,[0],[0]
"In the first hidden layer on the source and target sides, xi corresponds to word embeddings.",4.2 Pre-Compute Embeddings,[0],[0]
"Since this is a closed set of values that are fixed after training, the vectors V xi can be pre-computed (Devlin et al., 2014) for each word in the vocabulary and stored in a lookup table.",4.2 Pre-Compute Embeddings,[0],[0]
"This can only be applied to the first hidden layer.
",4.2 Pre-Compute Embeddings,[0],[0]
"Pre-computation does increase the memory cost of the model, since we must store r × 3 floats per word instead of e.",4.2 Pre-Compute Embeddings,[0],[0]
"However, if we only compute the k most frequently words (e.g., k = 8, 000), this reduces the pre-computation memory by 90% but still results in 95%+ token coverage due to the Zipfian distribution of language.",4.2 Pre-Compute Embeddings,[0],[0]
"The attention context computation in the GRU can be re-factored as follows:
Uci = U( ∑ j αijsj) = ∑ j αij(Usj)
Crucially, the hidden vector representation sj is only dependent on the source sentence, while aij is dependent on the target hypothesis.",4.3 Pre-Compute Attention,[0],[0]
"Therefore, the original computation Uci requires total |T |× b multiplications per sentence, but the re-factored versionUsj only requires total |S|multiplications.",4.3 Pre-Compute Attention,[0],[0]
"The expectation over α must still be computed at each target timestep, but this is much less expensive than the multiplication by U .",4.3 Pre-Compute Attention,[0],[0]
"For the element-wise vector functions use in the GRU, we can use vectorized instructions (SSE/AVX) for the add and multiply functions, and lookup tables for sigmoid and tanh.
Reference implementations in C++ are provided in the supplementary material.",4.4 SSE & Lookup Tables,[0],[0]
"In the GRU equation, for the first target hidden layer, xi represents the previously generated word, and hi−1 encodes the hypothesis up to two words before the current word.",4.5 Merge Recurrent States,[0],[0]
"Therefore, if two partial hypotheses in the beam only differ by the last emitted word, their hi−1 vectors will be identical.",4.5 Merge Recurrent States,[0],[0]
"Thus, we can perform matrix multiplication Whi−1 only on the unique hi−1 vectors in the beam at each target timestep.",4.5 Merge Recurrent States,[0],[0]
"For a beam size of b = 6, we measured that the ratio of unique hi−1 compared to total hi−1 is approximately 70%, averaged over several language pairs.",4.5 Merge Recurrent States,[0],[0]
This can only be applied to the first target hidden layer.,4.5 Merge Recurrent States,[0],[0]
"Cumulative results from each of the preceding speedups are presented in Table 1, measured on WMT English-French NewsTest2014.",4.6 Speedup Results,[0],[0]
"The NMT architecture evaluated here uses 3-layer 512- dimensional bidirectional GRU for the source, and a 1-layer 1024-dimensional attentional GRU for the target.",4.6 Speedup Results,[0],[0]
Each sentence is decoded independently with a beam of 6.,4.6 Speedup Results,[0],[0]
"Since these speedups are all mathematical identities excluding quantization noise, all outputs achieve 36.2 BLEU and are 99.9%+ identical.
",4.6 Speedup Results,[0],[0]
"The largest improvement is from 16-bit matrix multiplication, but all speedups contribute a significant amount.",4.6 Speedup Results,[0],[0]
"Overall, we are able to achieve a 4.4x speedup over a fast baseline decoder.",4.6 Speedup Results,[0],[0]
"Although the absolute speed is impressive, the model only uses one target layer and is several BLEU behind the SOTA, so the next goal is to maximize model accuracy while still achieving speeds greater than some target, such as 100 words/sec.",4.6 Speedup Results,[0],[0]
"In NMT, like in many other deep learning tasks, accuracy can be greatly improved by adding more hidden layers, but training and decoding time increase significantly (Luong et al., 2014; Zhou et al., 2016; Wu et al., 2016).",5 Model Improvements,[0],[0]
"Several past works have noted that convolutional neural networks (CNNs) are significantly less expensive than RNNs, and replaced the source and/or target side with a CNN-based architecture (Gehring et al., 2016; Kalchbrenner et al., 2016).",5 Model Improvements,[0],[0]
"However, these works have found it is difficult to replace the target side of the model with CNN layers while maintaining high accuracy.",5 Model Improvements,[0],[0]
"The use of a recurrent target is especially important to track attentional coverage and ensure fluency.
",5 Model Improvements,[0],[0]
"Here, we propose a mixed model which uses an RNN layer at the bottom to both capture fullsentence context and perform attention, followed by a series of fully-connected (FC) layers applied on top at each timestep.",5 Model Improvements,[0],[0]
The FC layers can be interpreted as a CNN without overlapping stride.,5 Model Improvements,[0],[0]
"Since each FC layer consists of a single matrix multiplication, it is 1/6th the cost of a GRU (or 1/8th an LSTM).",5 Model Improvements,[0],[0]
"Additionally, several of the speedups from Section 4 can only be applied to the first layer, so there is strong incentive to only use a single target RNN.
",5 Model Improvements,[0],[0]
"To avoid vanishing gradients, we use ResNet-
style skip connections (He et al., 2016).",5 Model Improvements,[0],[0]
"These allow very deep models to be trained from scratch and do not require any additional matrix multiplications, unlike highway networks (Srivastava et al., 2015).",5 Model Improvements,[0],[0]
"With 5 intermediate FC layers, target timestep i is computed as:
hBi = AttGRU(h B i−1, xi, S)
h1i = relu(W 1hBi )",5 Model Improvements,[0],[0]
h2i = relu(W 2h1i ) h3i = relu(W 3h2i,5 Model Improvements,[0],[0]
+,5 Model Improvements,[0],[0]
h 1 i ),5 Model Improvements,[0],[0]
h4i = relu(W 4h3i ),5 Model Improvements,[0],[0]
"h5i = relu(W 5h4i + h 3 i ) hTi = tanh(W Th5i ) or GRU(h T i−1, h 5 i )
yi = softmax(V hTi )
",5 Model Improvements,[0],[0]
"Where xi is the target input embedding, S is the set of source hidden vectors used for attention, and V is the target output vocabulary matrix.",5 Model Improvements,[0],[0]
"The superscripts hB and hT simply denote the “bottom” and “top” hidden layers, while the numbered layers hn represent the intermediate fully-connected layers.
",5 Model Improvements,[0],[0]
"We follow He et al. (2016) and only use skip connections on every other FC layer, but do not use batch normalization.",5 Model Improvements,[0],[0]
"The same pattern can be used for more FC layers, and the FC layers can be a different size than the bottom or top hidden layers.",5 Model Improvements,[0],[0]
The top hidden layer can be an RNN or an FC layer.,5 Model Improvements,[0],[0]
"It is important to use relu activations
(opposed to tanh) for ResNet-style skip connections.",5 Model Improvements,[0],[0]
The GRUs still use tanh.,5 Model Improvements,[0],[0]
"Results using the mixed RNN+FC architecture are shown in Table 2, using all speedups.",5.1 Model Results,[0],[0]
"We have found that the benefit of using RNN+FC layers on the source is minimal, so we only perform ablation on the target.",5.1 Model Results,[0],[0]
"For the source, we use a 3-layer 512- dim bidi GRU in all models (S1)-(S6).
",5.1 Model Results,[0],[0]
Model (S1) and (S2) are one and two layer baselines.,5.1 Model Results,[0],[0]
"Model (S4), which uses 7 intermediate FC layers, has similar decoding cost to (S2) while doubling the improvement over (S1) to 1.2 BLEU.",5.1 Model Results,[0],[0]
We see minimal benefit from using a GRU on the top layer (S5) or using more FC layers (S6).,5.1 Model Results,[0],[0]
"In (E1) and (E2) we present 2 and 3 model ensembles of (S4), trained from scratch with different random seeds.",5.1 Model Results,[0],[0]
"We can see that the 2-model ensemble improves results by 0.9 BLEU, but the 3-model ensemble has little additional improvment.",5.1 Model Results,[0],[0]
"Although not presented here, we have found these improvement from decoder speedups and RNN+FC to be consistent across many language pairs.
",5.1 Model Results,[0],[0]
"All together, we were able to achieve a BLEU score of 38.3 while decoding at 100 words/sec on a single CPU core.",5.1 Model Results,[0],[0]
"As a point of comparison, Wu et al. (2016) achieves similar BLEU scores on this test set (37.9 to 38.9) and reports a CPU decoding speed of ~100 words/sec (0.2226 sents/sec), but parallelizes this decoding across 44 CPU cores.",5.1 Model Results,[0],[0]
"System (S7), which is our re-implementation of Wu et al. (2016), decodes at 28 words/sec on one CPU core, using all of the speedups described in Section 4.",5.1 Model Results,[0],[0]
"Zhou et al. (2016) has a similar computational cost to (S7), but we were not able to replicate those results in terms of accuracy.
",5.1 Model Results,[0],[0]
"Although we are comparing an ensemble to a single model, we can see ensemble (E1) is over 3x faster to decode than the single model (S7).",5.1 Model Results,[0],[0]
"Additionally, we have found that model (S4) is roughly 3x faster to train than (S7) using the same GPU resources, so (E1) is also 1.5x faster to train than a single model (S7).",5.1 Model Results,[0],[0]
"Attentional sequence-to-sequence models have become the new standard for machine translation, but one challenge of such models is a significant increase in training and decoding cost compared to phrase-based systems.",abstractText,[0],[0]
"Here, we focus on efficient decoding, with a goal of achieving accuracy close the state-of-the-art in neural machine translation (NMT), while achieving CPU decoding speed/throughput close to that of a phrasal decoder.",abstractText,[0],[0]
"We approach this problem from two angles: First, we describe several techniques for speeding up an NMT beam search decoder, which obtain a 4.4x speedup over a very efficient baseline decoder without changing the decoder output.",abstractText,[0],[0]
"Second, we propose a simple but powerful network architecture which uses an RNN (GRU/LSTM) layer at bottom, followed by a series of stacked fully-connected layers applied at every timestep.",abstractText,[0],[0]
"This architecture achieves similar accuracy to a deep recurrent model, at a small fraction of the training and decoding cost.",abstractText,[0],[0]
"By combining these techniques, our best system achieves a very competitive accuracy of 38.3 BLEU on WMT EnglishFrench NewsTest2014, while decoding at 100 words/sec on single-threaded CPU.",abstractText,[0],[0]
We believe this is the best published accuracy/speed trade-off of an NMT system.,abstractText,[0],[0]
Sharp Models on Dull Hardware: Fast and Accurate Neural Machine Translation Decoding on the CPU,title,[0],[0]
"Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1030–1035, Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics
We present the first dynamic programming (DP) algorithm for shift-reduce constituency parsing, which extends the DP idea of Huang and Sagae (2010) to context-free grammars. To alleviate the propagation of errors from part-of-speech tagging, we also extend the parser to take a tag lattice instead of a fixed tag sequence. Experiments on both English and Chinese treebanks show that our DP parser significantly improves parsing quality over non-DP baselines, and achieves the best accuracies among empirical linear-time parsers.",text,[0],[0]
"Incremental parsing has gained popularity in both dependency (Nivre, 2004; Zhang and Clark, 2008) and constituency parsing (Zhu et al., 2013; Wang and Xue, 2014).",1 Introduction,[0],[0]
"However, the greedy or beam search algorithms used in these parsers can only explore a tiny fraction of trees among exponentially many candidates.",1 Introduction,[0],[0]
"To alleviate this problem, Huang and Sagae (2010) propose a dynamic programming (DP) algorithm, reducing the search space to a polynomial size by merging equivalent states.",1 Introduction,[0],[0]
"This idea has been extended by Kuhlmann et al. (2011) and Cohen et al. (2011) to other dependency parsing paradigms.
",1 Introduction,[0],[0]
"In constituency parsing, however, DP has not yet been applied to incremental parsing, and the bigger search space in constituency parsing suggests a potentially even bigger advantage by DP.",1 Introduction,[0],[0]
"However, with unary rules and more-than-binary branchings, constituency parsing presents challenges not found in dependency parsing that must be addressed before applying DP.",1 Introduction,[0],[0]
"Thus, we first present an odd-even
shift-reduce constituency parser which always finishes in same number of steps, eliminating the complicated asynchronicity issue in previous work (Zhu et al., 2013; Wang and Xue, 2014), and then develop dynamic programming on top of that.",1 Introduction,[0],[0]
"Secondly, to alleviate the error propagation from POS tagging, we also extends the algorithm to take a tagging sausage lattice as input, which is a compromise between pipeline and joint approaches (Hatori et al., 2011; Li et al., 2011; Wang and Xue, 2014).
",1 Introduction,[0],[0]
"Our DP parser achieves state-of-the-art performances on both Chinese and English treebanks (at 90.8% on PTB and 83.9% on CTB, the latter being the highest in literature).",1 Introduction,[0],[0]
One major challenge in constituency parsing is unary rules.,2 Odd-Even Shift-Reduce CFG Parser,[0],[0]
"Unlike dependency parsing where shiftreduce always finishes in 2n−1 steps, existing incremental constituency parsers (Zhu et al., 2013; Wang and Xue, 2014) reach the goal state (full parse tree) in different steps due to different number of unary rules.",2 Odd-Even Shift-Reduce CFG Parser,[0],[0]
"So we propose a new, synchronized, “oddeven” system to reach the goal in the same 4n − 2 steps.",2 Odd-Even Shift-Reduce CFG Parser,[0],[0]
"A state is notated p = 〈S,Q〉, where S is a stack of trees ..., s1, s0, and Q is a queue of wordtag pairs.",2 Odd-Even Shift-Reduce CFG Parser,[0],[0]
"At even steps (when step index is even) we can choose one of the three standard actions
• sh: shift the head of Q, a word-tag pair (t, w), onto S as a singleton tree t(w);
• rexx: combine the top two trees on the stack and replace them with a new tree x(s1, s0), x being the root nonterminal, headed on s0;
• rexy: similar to rexx but headed on s1; and at odd steps we can choose two new actions:
1030
• unx : replace s0 with a new tree x(s0) with x being the root nonterminal;
• st: no action.",2 Odd-Even Shift-Reduce CFG Parser,[0],[0]
Figure 1 shows the deductive system.,2 Odd-Even Shift-Reduce CFG Parser,[0],[0]
"Note that we alternate between standard shift-reduce actions in even steps and unary actions (unx or st) in odd steps, and the first action must be sh, followed by a unx or st, and followed by another sh.",2 Odd-Even Shift-Reduce CFG Parser,[0],[0]
"Continuing this procedure, we can always achieve the goal in 2(2n− 1) steps.
",2 Odd-Even Shift-Reduce CFG Parser,[0],[0]
"In practice, we have larger than two-way rules and multi-level unary rules, so we binarize them and collapse multi-level unary rules into one level, for example,
NP
S
VP
PPNPV
=⇒
NP+S
VP
PPVP′
NPV
Following Huang and Sagae (2010), we represent feature templates as functions",2 Odd-Even Shift-Reduce CFG Parser,[0],[0]
"f(·, ·) on stack S and queue Q. Table 1 shows the 43 feature templates we use in this paper, all adopted from Zhu et al. (2013).",2 Odd-Even Shift-Reduce CFG Parser,[0],[0]
"They are combinations of the 32 atomic features f̃(S,Q) (e.g. s0.t and s0.c denote the head tag and
syntactic category of tree s0, resp., and s0.lc.w is the head word of its leftmost child).",2 Odd-Even Shift-Reduce CFG Parser,[0],[0]
"The key idea towards DP is the merging of equivalent states, after which the stacks are organized in a “graph-structured stack” (GSS)(Tomita, 1988).",3 Dynamic Programming,[0],[0]
"Following Huang and Sagae (2010), “equivalent states” ∼ in a same beam are defined by the atomic features f̃(S,Q) and the span of s0:
〈S,Q〉 ∼ 〈S′, Q′〉 ⇔ f̃(S,Q) = f̃(S′, Q′) and s0.span = s′0.span.
Similarly, for each state p, π(p) is a set of predictor states, each of which can be combined with p in a rexx or re x y action.",3 Dynamic Programming,[0],[0]
"For each action, we have different operations on π(p).",3 Dynamic Programming,[0],[0]
"If a state pmakes a sh action and generates a state p′, then π(p′) = {p}.",3 Dynamic Programming,[0],[0]
"If two shifted states p′ and p′′ are equivalent, p′ ∼ p′′, we merge π(p′) and π(p′′).",3 Dynamic Programming,[0],[0]
"If a state p makes a reduce (rexx or re x y) action, p tries to combine with every p′ ∈ π(p), and each combination generates a state r with π(r) = π(p′).",3 Dynamic Programming,[0],[0]
"If two reduced states are equivalent, we only keep one predictor states, as their predictor states are identical.",3 Dynamic Programming,[0],[0]
"If a state p fires an unx or a st action resulting in a state u, we copy the predictor states π(u) = π(p).",3 Dynamic Programming,[0],[0]
"Similar to reduce actions, if two resulting states after applying an unx or a st action are equivalent, we only keep the best one with highest score (the recombined ones are only useful for searching k-best trees).
",3 Dynamic Programming,[0],[0]
"input (T1, w1)...(Tn, wn)
axioms 0 : 〈 , (t, w1)...(Tn, wn)({</s>}, </s>)〉 : 0,∀ t ∈ T1
sh",3 Dynamic Programming,[0],[0]
"l : 〈S, (t, w)|(T ′, w′)|Q〉 : (c, v) l+1 : 〈S|t(w), (t′, w′)|Q〉 : (c+csh, 0) t′ ∈ T ′, l is even
Figure 3: Extended shift-reduce deductive system with tagging sausage lattice, only showing sh.
",3 Dynamic Programming,[0],[0]
"In order to compute all the scores in GSS, for each state p, we calculate the prefix score, c, which is the total cost of the best action sequence from the initial state to the end of state p, and the inside score v, which is the score since the last shift (Figure 2).
",3 Dynamic Programming,[0],[0]
"The new mechanism beyond Huang and Sagae (2010) is the non-trivial dynamic programming treatment of unary actions (unx and st), which is not found in dependency parsing.",3 Dynamic Programming,[0],[0]
Note that the score calculation is quite different from shift in the sense that unary actions are more like reduces.,3 Dynamic Programming,[0],[0]
It is easy to extend our deductive system to take tagging sausage lattices as input.,4 Incorporating Tag Lattices,[0],[0]
The key difference is that the tag t associated with each word in the input sequence becomes a set of tags T .,4 Incorporating Tag Lattices,[0],[0]
"Thus, in the sh action, we split the state with all the possible tags t′ in the tagset T ′ for the second word on the queue.",4 Incorporating Tag Lattices,[0],[0]
"Figure 3 shows the deductive system, where we only change the sh action, input and axiom.",4 Incorporating Tag Lattices,[0],[0]
"For simplicity reasons we only present one word look
87.5
88
88.5
89
89.5
90
2 4 6 8 10 12 14 16 18
F 1
o n
t h
e d
e v s
e t
iteration
11th
15th
DP non-DP
Figure 4: The learning curves of non-DP and DP parsers on the development set.",4 Incorporating Tag Lattices,[0],[0]
"DP achieves the best performance at 11th iteration with 89.8%, while non-DP gets its optimal iteration at 15th with a lower F1 89.5%.
ahead (we just need to know the tag of the first word on the queue), but in practice, we use a look ahead of 4 words (q0..q3, see Table 1), so each shift actually splits the tagset of the 5th word on the queue (q4).",4 Incorporating Tag Lattices,[0],[0]
We evaluate our parsers on both Penn English Treebank (PTB) and Chinese Treebank (CTB).,5 Experiments,[0],[0]
"For PTB, we use sections 02-21 as the training, section 24 as the dev set, and section 23 as the test.",5 Experiments,[0],[0]
"For CTB, we use the version of 5.1, articles 001-270 and 440- 1151 as the training data, articles 301-325 as the dev set, and articles 271-300 as the test set.
",5 Experiments,[0],[0]
"Besides training with gold POS tags, we add k-best automatic tagging results to the training set using a MaxEnt model with ten-way jackknifing (Collins, 2000).",5 Experiments,[0],[0]
"And we automatically tag the dev and test sets with k-best tagging sequences us-
ing the MaxEnt POS tagger (at 97.1% accuracy on English, and 94.5% on Chinese) trained on the training set.",5 Experiments,[0],[0]
We set k to 20 for English.,5 Experiments,[0],[0]
"And we run two sets of experiments, 1-best vs. 20-best, for Chinese to address the tagging issue.",5 Experiments,[0],[0]
"We train our parsers using “max-violation perceptron” (Huang et al., 2012) (which has been shown to converge much faster than “early-update” of Collins and Roark (2004)) with minibatch parallelization (Zhao and Huang, 2013) on the head-out binarized and unary-collapsed training set.",5 Experiments,[0],[0]
"We finally debinarize the trees to recover the collapsed unary rules.
",5 Experiments,[0],[0]
"We evaluate parser performance with EVALB including labeled precision (LP), labeled recall (LR), and bracketing F1.",5 Experiments,[0],[0]
"We use a beam size of 32, and pick the optimal iteration number based on the performances on the dev set.
",5 Experiments,[0],[0]
"Our baseline is the shift-reduce parser without state recombination (henceforth “non-DP”), and our dynamic programming parser (henceforth “DP”) is the extension of the baseline.",5 Experiments,[0],[0]
Figure 4 shows the learning curves on the PTB dev set.,5.1 Learning Curves and Search Quality,[0],[0]
"With a same beam width, DP parser achieves a better performance (89.8%, peaking at the 11th iteration) and converges faster than non-DP.",5.1 Learning Curves and Search Quality,[0],[0]
"Picking the optimal iterations for DP and non-DP models, we test each with various beam size, and plot the F1 curves in Figure 5.",5.1 Learning Curves and Search Quality,[0],[0]
"Again, DP is always better than non-DP, with 0.5% difference at beam of 64.",5.1 Learning Curves and Search Quality,[0],[0]
Table 2 shows the final results on the PTB test set.,5.2 Final Results on English,[0],[0]
The last column shows the empirical time complexity.,5.2 Final Results on English,[0],[0]
"Our baseline parser achieves a competitive score, which is higher than Berkeley even with a linear time complexity, and is comparable to Zhu et al. (2013).",5.2 Final Results on English,[0],[0]
"Our DP parser improves the F1 score by 0.5 points over the non-DP, and achieves the best F1 score among empirical linear-time parsers.",5.2 Final Results on English,[0],[0]
"To alleviate the propagation of errors from POS tagging, we run sausage lattice parsing on both Chinese and English, where Chinese tagging accuracy significantly lag behind English.
",5.3 Sausage Lattice Parsing,[0],[0]
Table 3 shows the F1 score and POS tagging accuracy of all parsing models on the Chinese 5.1 test set.,5.3 Sausage Lattice Parsing,[0],[0]
"Our MaxEnt POS tagger achieves an accuracy of 94.5% on 1-best outputs, and an oracle score of 97.1% on 20-best results.",5.3 Sausage Lattice Parsing,[0],[0]
"The average number of
tags for each word in the 20-best list is 1.1.",5.3 Sausage Lattice Parsing,[0],[0]
The joint tagging and parsing approach of Wang and Xue (2014) improves the F1 score from 80.1% to 83.6% (see lines 4 and 5).,5.3 Sausage Lattice Parsing,[0],[0]
"We instead use sausage lattices, a much cheaper way.",5.3 Sausage Lattice Parsing,[0],[0]
The non-DP (1-best POS) and non-DP (20-best POS) lines show the effectiveness of using sausage lattices (+1.1 for tagging and +2.6 for parsing).,5.3 Sausage Lattice Parsing,[0],[0]
"As Wang and Xue (2014) is a non-DP model, it is comparable to our non-DP results.",5.3 Sausage Lattice Parsing,[0],[0]
"With the help of 20-best tagging lattices, we achieve the same tagging accuracy at 95.5%, but still 0.4 worse on the F1 score than the joint model.",5.3 Sausage Lattice Parsing,[0],[0]
It suggests that we need a larger k to catch up the gap.,5.3 Sausage Lattice Parsing,[0],[0]
"But our DP model boosts the performance further to the best score at 83.9% with a similar set of features.
",5.3 Sausage Lattice Parsing,[0],[0]
The last two lines (non-DP and DP) in Table 2 show our English lattice parsing results.,5.3 Sausage Lattice Parsing,[0],[0]
"So we run another baseline with the non-DP English parser on 1-best POS tags, and the baseline achieves a tagging accuracy at 97.11 and an F1 score at 90.1.",5.3 Sausage Lattice Parsing,[0],[0]
"Comparing to the tagging accuracy (97.15) and F1 score (90.3) of our non-DP lattice parser, sausage lattice parsing doesn’t help the tagging accuracy, but helps parsing a little by 0.2 points.",5.3 Sausage Lattice Parsing,[0],[0]
"The statistics show that 2 percent of POS tags in the lattice parsing result are different from the baseline, and those differences lead to a slight improvement on parsing.",5.3 Sausage Lattice Parsing,[0],[0]
"In this paper, we present a dynamic programming algorithm based on graph-structured stack (GSS) for shift-reduce constituency parsing, and extend the algorithm to take tagging sausage lattices as input.",6 Conclusions,[0],[0]
"Experiments on both English and Chinese treebanks show that our DP parser outperforms almost all other parsers except of Carreras et al. (2008), which runs in a much higher time complexity.",6 Conclusions,[0],[0]
We thank the anonymous reviewers for comments.,Acknowledgment,[0],[0]
"Haitao Mi is supported by DARPA HR0011-12C-0015 (BOLT), and Liang Huang is supported by DARPA FA8750-13-2-0041 (DEFT), NSF IIS1449278, and a Google Faculty Research Award.",Acknowledgment,[0],[0]
The views and findings in this paper are those of the authors and are not endorsed by the DARPA.,Acknowledgment,[0],[0]
"We present the first dynamic programming (DP) algorithm for shift-reduce constituency parsing, which extends the DP idea of Huang and Sagae (2010) to context-free grammars.",abstractText,[0],[0]
"To alleviate the propagation of errors from part-of-speech tagging, we also extend the parser to take a tag lattice instead of a fixed tag sequence.",abstractText,[0],[0]
"Experiments on both English and Chinese treebanks show that our DP parser significantly improves parsing quality over non-DP baselines, and achieves the best accuracies among empirical linear-time parsers.",abstractText,[0],[0]
Shift-Reduce Constituency Parsing with Dynamic Programming and POS Tag Lattice,title,[0],[0]
This article deals with the estimation of the regression vector β ∈,1. Introduction,[0],[0]
"Rp in the linear regression model y = Xβ + w, where X ∈ Rn×p is a known design matrix with unit Euclidean norm columns, w is the noise vector and y is the observation vector.",1. Introduction,[0],[0]
"Throughout this article, we assume that the entries of the noise w are independent, zero mean and Gaussian distributed with variance σ2.",1. Introduction,[0],[0]
We consider the high dimensional and sample starved scenario of n < p or n p where classical techniques like ordinary least squares (OLS) are no longer applicable.,1. Introduction,[0],[0]
This problem of estimating high dimensional vectors in sample starved scenarios is ill-posed even in the absence of noise unless strong structural assumptions are made on X and β.,1. Introduction,[0],[0]
A widely used and practically valid assumption is sparsity.,1. Introduction,[0],[0]
The vector β ∈,1. Introduction,[0],[0]
Rp is sparse if the support of β given by S = supp(β) = {k : βk 6= 0} has cardinality k0 = card(S),1. Introduction,[0],[0]
"p.
*Equal contribution 1Department of Electrical Engineering, IIT Madras, India 2Department of Electrical Engineering, IIT Madras, India.",1. Introduction,[0],[0]
"Correspondence to: Sreejith Kallummil <sreejith.k.venugopal@gmail.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"A number of algorithms like least absolute shrinkage and selection operator (LASSO)(Tropp, 2006; Tibshirani, 1996), Dantzig selector (DS)(Candes & Tao, 2007), subspace pursuit (SP)(Dai & Milenkovic, 2009), OMP (Pati et al., 1993; Mallat & Zhang, 1993; Tropp, 2004; Cai & Wang, 2011), elastic net (Zou & Hastie, 2005) etc. are proposed to efficiently estimate β.",1. Introduction,[0],[0]
Tuning the hyper parameters of aforementioned algorithms to achieve optimal performance require a priori knowledge of signal parameters like sparsity k0 or noise statistics like σ2 etc.,1. Introduction,[0],[0]
"Unfortunately, these parameters are rarely known a priori.",1. Introduction,[0],[0]
"To the best of our knowledge, no computationally efficient technique to estimate k0 is reported in open literature.",1. Introduction,[0],[0]
"However, limited success on the estimation of σ2 has been reported in literature (Dicker, 2014; Fan et al., 2012; Dicker & Erdogdu, 2016; Bayati et al., 2013).",1. Introduction,[0],[0]
"However, the performance of these σ2 estimates when used for tuning hyper parameters in LASSO, DS, OMP etc. are largely unknown.",1. Introduction,[0],[0]
"Generalised techniques for hyper parameter selection like cross validation (CV)(Arlot et al., 2010), re-sampling (Meinshausen & Bühlmann, 2010) etc. are computationally challenging.",1. Introduction,[0],[0]
"Further, CV is reported to have poor variable selection behaviour(Chichignoud et al., 2016; Arlot et al., 2010).",1. Introduction,[0],[0]
"Indeed, algorithms that are oblivious to signal and noise statistics are also proposed in literature.",1. Introduction,[0],[0]
"This include algorithms inspired or related to LASSO like square root LASSO(Belloni et al., 2011), AV∞ (Chichignoud et al., 2016), approximate message passing (Mousavi et al., 2013; Bayati et al., 2013) etc. and ridge regression inspired techniques like least squares adaptive thresholding (LAT), ridge adaptive thresholding (RAT)(Wang et al., 2016) etc.",1. Introduction,[0],[0]
"However, most of existing signal and noise statistics oblivious sparse recovery techniques have only large sample performance guarantees.",1. Introduction,[0],[0]
"Further, many of these techniques assume that design matrix X is sampled from a random ensemble, a condition which is rarely satisfied in practice.",1. Introduction,[0],[0]
This article present a novel technique called residual ratio thresholding (RRT) for finding a “good” estimate of support S from the data dependent/adaptive sequence of supports generated by OMP.,1.1. Contributions of this paper,[0],[0]
"RRT is analytically shown to accomplish exact support recovery, (i.e., identifying S) under the same finite sample and deterministic constraints on X like
restricted isometry constants (RIC) or mutual coherence required by OMP with a priori knowledge of k0 or σ2.",1.1. Contributions of this paper,[0],[0]
"However, the signal to noise ratio (SNR=‖Xβ‖22/nσ2) required for support recovery using RRT is slightly higher than that of OMP with a priori knowledge of k0 or σ2.",1.1. Contributions of this paper,[0],[0]
This extra SNR requirement is shown to decrease with the increase in sample size n. RRT and OMP with a priori knowledge of k0 or σ2 are shown to be equivalent as n → ∞ in terms of the SNR required for support recovery.,1.1. Contributions of this paper,[0],[0]
RRT involves a tuning parameter α that can be set independent of ambient SNR or noise statistics.,1.1. Contributions of this paper,[0],[0]
The hyper parameter α in RRT have an interesting semantic interpretation of being the high SNR upper bound on support recovery error.,1.1. Contributions of this paper,[0],[0]
"Also RRT is asymptotically tuning free in the sense that a very wide range of α deliver similar performances as n → ∞. Numerical simulations indicate that RRT can deliver a highly competitive performance when compared to OMP having a priori knowledge of k0 or σ2, OMP with k0 estimated using CV and the recently proposed LAT algorithm.",1.1. Contributions of this paper,[0],[0]
"Further, RRT also delivered a highly competitive performance when applied to identify outliers in real data sets, an increasingly popular application of sparse estimation algorithms(Mitra et al., 2010; 2013).
",1.1. Contributions of this paper,[0],[0]
The remainder of this article is organised as follows.,1.1. Contributions of this paper,[0],[0]
In section 2 we discuss OMP algorithm.,1.1. Contributions of this paper,[0],[0]
RRT algorithm is presented in Section 3.,1.1. Contributions of this paper,[0],[0]
Section 4 presents theoretical performance guarantees for RRT.,1.1. Contributions of this paper,[0],[0]
Section 5 presents numerical simulation results.,1.1. Contributions of this paper,[0],[0]
"All the proofs are provided in the supplementary material.
1.2.",1.1. Contributions of this paper,[0],[0]
Notations used ‖x‖q =,1.1. Contributions of this paper,[0],[0]
( p∑ k=1 |xk|q ),1.1. Contributions of this paper,[0],[0]
1 q is the lq norm of x ∈,1.1. Contributions of this paper,[0],[0]
Rp. 0n is the n × 1 zero vector and In is the n × n identity matrix.,1.1. Contributions of this paper,[0],[0]
span(X) is the column space of X. X† =,1.1. Contributions of this paper,[0],[0]
(XTX)−1XT is the Moore-Penrose pseudo inverse of X. XJ denotes the sub-matrix of X formed using the columns indexed by J .,1.1. Contributions of this paper,[0],[0]
"N (u,C) represents a Gaussian random vector (R.V) with mean u and covariance matrix C. B(a, b) denotes a Beta R.V with parameters a and b. a ∼ b implies that a and b are identically distributed.",1.1. Contributions of this paper,[0],[0]
[p] represents the floor operator.,1.1. Contributions of this paper,[0],[0]
φ represents the null set.,1.1. Contributions of this paper,[0],[0]
"For any two sets J1 and J2, J1/J2 denotes the set difference.",1.1. Contributions of this paper,[0],[0]
"a
P→ b represents the convergence of R.V a to R.V b in probability.",1.1. Contributions of this paper,[0],[0]
"OMP (Algorithm 1) starts with a null support estimate and in each iteration it adds that column index to the current support which is the most correlated with the previous residual rk−1, i.e., tk = arg max
j |XTj rk−1|.",2. Orthogonal Matching Pursuit (OMP),[0],[0]
"Then a
LS estimate of β restricted to the current support Skomp is
Algorithm 1 Orthogonal matching pursuit Input: Observation y, matrix X Initialize Somp0 = φ.",2. Orthogonal Matching Pursuit (OMP),[0],[0]
"k = 1 and residual r0 = y repeat
Identify the next column tk = arg max j |XTj rk−1|",2. Orthogonal Matching Pursuit (OMP),[0],[0]
Expand current support Skomp = Sk−1omp ∪ tk,2. Orthogonal Matching Pursuit (OMP),[0],[0]
"Restricted LS estimate: β̂Skomp = X † Skomp y.
β̂{1,...,p}/Skomp = 0p−k.
",2. Orthogonal Matching Pursuit (OMP),[0],[0]
Update residual: rk = y −Xβ̂ =,2. Orthogonal Matching Pursuit (OMP),[0],[0]
(In −Pk)y.,2. Orthogonal Matching Pursuit (OMP),[0],[0]
"Increment k ← k + 1.
",2. Orthogonal Matching Pursuit (OMP),[0],[0]
until stopping condition (SC) is true Output: Support estimate Ŝ = Skomp.,2. Orthogonal Matching Pursuit (OMP),[0],[0]
"Vector estimate β̂
computed as an intermediate estimate of β and this estimate is used to update the residual.",2. Orthogonal Matching Pursuit (OMP),[0],[0]
"Note that Pk in Algorithm 1 refers to XSkompX † Skomp , the projection matrix onto span(XSkomp).",2. Orthogonal Matching Pursuit (OMP),[0],[0]
"Since the residual r k is orthogonal to span(XSkomp), X T j r
k",2. Orthogonal Matching Pursuit (OMP),[0],[0]
= 0,2. Orthogonal Matching Pursuit (OMP),[0],[0]
for all j ∈ Skomp.,2. Orthogonal Matching Pursuit (OMP),[0],[0]
"Consequently, tk+1 /∈",2. Orthogonal Matching Pursuit (OMP),[0],[0]
"Skomp, i.e., the same index will not be selected in two different iterations.",2. Orthogonal Matching Pursuit (OMP),[0],[0]
"Hence, Sk+1omp ⊃ Skomp, i.e. the support sequence is monotonically increasing.",2. Orthogonal Matching Pursuit (OMP),[0],[0]
"The monotonicity of Skomp in turn implies that the residual norm ‖rk‖2 is a non increasing function of k, i.e, ‖rk+1‖2 ≤ ‖rk‖2.
",2. Orthogonal Matching Pursuit (OMP),[0],[0]
"Most of the theoretical properties of OMP are derived assuming a priori knowledge of true sparsity level k0 in which case OMP stops after exactly k0 iterations(Tropp, 2004; Wang, 2015).",2. Orthogonal Matching Pursuit (OMP),[0],[0]
"When k0 is not known, one has to rely on stopping conditions (SC) based on the properties of the residual rk as k varies.",2. Orthogonal Matching Pursuit (OMP),[0],[0]
"For example, one can stop OMP iterations once the residual power is too low compared to the expected noise power.",2. Orthogonal Matching Pursuit (OMP),[0],[0]
"Mathematically, when the noise w is l2 bounded, i.e., ‖w‖2 ≤ 2 for some a priori known 2, then OMP can be stopped if ‖rk‖2 ≤ 2.",2. Orthogonal Matching Pursuit (OMP),[0],[0]
"For a Gaussian noise vector w ∼ N (0n, σ2In), σ = σ √ n+ 2 √ n log(n) satisfies(Cai & Wang, 2011)
",2. Orthogonal Matching Pursuit (OMP),[0],[0]
P(‖w‖2 ≤ σ),2. Orthogonal Matching Pursuit (OMP),[0],[0]
"≥ 1− 1
n , (1)
i.e., Gaussian noise is l2 bounded with a very high probability.",2. Orthogonal Matching Pursuit (OMP),[0],[0]
"Consequently, one can stop OMP iterations in Gaussian noise once ‖rk‖2 ≤ σ .
",2. Orthogonal Matching Pursuit (OMP),[0],[0]
A number of deterministic recovery guarantees are proposed for OMP.,2. Orthogonal Matching Pursuit (OMP),[0],[0]
Among these guarantees the conditions based on RIC are the most popular.,2. Orthogonal Matching Pursuit (OMP),[0],[0]
"RIC of order j denoted by δj is defined as the smallest value of δ such that
(1− δ)‖b‖22 ≤ ‖Xb‖22 ≤ (1 + δ)‖b‖22",2. Orthogonal Matching Pursuit (OMP),[0],[0]
"(2)
hold true for all b ∈",2. Orthogonal Matching Pursuit (OMP),[0],[0]
Rp with ‖b‖0 = card(supp(b)),2. Orthogonal Matching Pursuit (OMP),[0],[0]
≤,2. Orthogonal Matching Pursuit (OMP),[0],[0]
"j. A smaller value of δj implies that X act as a near orthogonal
matrix for all j sparse vectors b.",2. Orthogonal Matching Pursuit (OMP),[0],[0]
Such a situation is ideal for the recovery of a j-sparse vector b using any sparse recovery technique.,2. Orthogonal Matching Pursuit (OMP),[0],[0]
"The latest RIC based support recovery guarantee using OMP is given in Lemma 1(Liu et al., 2017).
",2. Orthogonal Matching Pursuit (OMP),[0],[0]
Lemma 1.,2. Orthogonal Matching Pursuit (OMP),[0],[0]
OMP with k0 iterations or SC ‖rk‖2 ≤,2. Orthogonal Matching Pursuit (OMP),[0],[0]
‖w‖2 can recover any k0 sparse vector β provided that δk0+1 < 1/,2. Orthogonal Matching Pursuit (OMP),[0],[0]
"√ k0 + 1 and ‖w‖2 ≤ omp =
βmin √ 1− δk0+1  1−√k0 + 1δk0+1 1 + √ 1− δ2k0+1 − √ k0 + 1δk0+1 .",2. Orthogonal Matching Pursuit (OMP),[0],[0]
"Since P(‖w‖2 < σ) ≥ 1− 1/n when w ∼ N (0n, σ2In), it follows from Lemma 1 that OMP with k0 iterations or SC ‖rk‖2 ≤ σ can recover any k0-sparse vector β with probability greater than 1 − 1/n provided that δk0+1 < 1/ √ k0 + 1 and σ ≤ omp.",2. Orthogonal Matching Pursuit (OMP),[0],[0]
Lemma 1 implies that OMP with a priori knowledge of k0 or σ2 can recover support S once the matrix satisfies the regularity condition δk0+1 < 1/ √ k0 + 1 and the SNR is high.,2. Orthogonal Matching Pursuit (OMP),[0],[0]
It is also known that this RIC condition is worst case necessary.,2. Orthogonal Matching Pursuit (OMP),[0],[0]
"Consequently, Lemma 1 is one of the best deterministic guarantee for OMP available in literature.",2. Orthogonal Matching Pursuit (OMP),[0],[0]
"Note that the mutual incoherence condition given by µX = max
j 6=k |XTj Xk|",2. Orthogonal Matching Pursuit (OMP),[0],[0]
< 1/(2k0,2. Orthogonal Matching Pursuit (OMP),[0],[0]
"− 1)
also ensures exact support recovery at high SNR.",2. Orthogonal Matching Pursuit (OMP),[0],[0]
Note that the a priori knowledge of k0 or σ2 required to materialise the recovery guarantees in Lemma 1 are not available in practical problems.,2. Orthogonal Matching Pursuit (OMP),[0],[0]
"Further, k0 and σ2 are very difficult to estimate.",2. Orthogonal Matching Pursuit (OMP),[0],[0]
This motivates the proposed RRT algorithm which does not require a priori knowledge of k0 or σ2.,2. Orthogonal Matching Pursuit (OMP),[0],[0]
RRT is a novel signal and noise statistics oblivious technique to estimate the support S based on the behaviour of the residual ratio statistic RR(k) =,3. Residual Ratio Thresholding (RRT),[0],[0]
‖rk‖2/‖rk−1‖2 as k increases from k = 1 to a predefined value k = kmax > k0.,3. Residual Ratio Thresholding (RRT),[0],[0]
"As aforementioned, identifying the support using the behaviour of ‖rk‖2 requires a priori knowledge of σ2.",3. Residual Ratio Thresholding (RRT),[0],[0]
"However, as we will show in this section, support detection using RR(k) does not require a priori knowledge of σ2.",3. Residual Ratio Thresholding (RRT),[0],[0]
"Since the residual norms are non negative and non increasing, RR(k) always satisfy 0 ≤",3. Residual Ratio Thresholding (RRT),[0],[0]
RR(k) ≤ 1.,3. Residual Ratio Thresholding (RRT),[0],[0]
Consider running kmax > k0 iterations of OMP and let {Skomp}kmaxk=1 be the support sequence generated by OMP.,3.1. Minimal superset and implications,[0],[0]
"Recall that Skomp is monotonically increasing.
",3.1. Minimal superset and implications,[0],[0]
Definition 1:-,3.1. Minimal superset and implications,[0],[0]
"The minimal superset in the OMP support sequence {Skomp}kmaxk=1 is given by Skminomp , where kmin = min({k : S ⊆ Skomp}).",3.1. Minimal superset and implications,[0],[0]
"When the set {k : S ⊆ Skomp} = φ, we set kmin =∞ and Skminomp = φ.
",3.1. Minimal superset and implications,[0],[0]
"In words, minimal superset is the smallest superset of support S present in a particular realization of the support estimate sequence {Skomp}kmaxk=1 .",3.1. Minimal superset and implications,[0],[0]
Note that both kmin and Skminomp are unobservable random variables.,3.1. Minimal superset and implications,[0],[0]
"Since card(Skomp) = k, Skomp for k < k0 cannot satisfy S ⊆ Skomp and hence kmin ≥ k0.",3.1. Minimal superset and implications,[0],[0]
"Further, the monotonicity of Skomp implies that S ⊂ Skomp for all k ≥ kmin.",3.1. Minimal superset and implications,[0],[0]
"Case 1:- When kmin = k0, then Sk0omp = S and Skomp ⊃ S for k ≥ k0, i.e., S is present in the solution path.",3.1. Minimal superset and implications,[0],[0]
"Further, when kmin = k0, it is true that Skomp ⊆ S for k ≤ k0.",3.1. Minimal superset and implications,[0],[0]
Case 2:-,3.1. Minimal superset and implications,[0],[0]
"When k0 < kmin ≤ kmax, then Skomp 6= S for all k and Sompk ⊃ S for k ≥ kmin, i.e., S is not present in the solution path.",3.1. Minimal superset and implications,[0],[0]
"However, a superset of S is present.",3.1. Minimal superset and implications,[0],[0]
Case 3:-,3.1. Minimal superset and implications,[0],[0]
"When kmin = ∞, then Skomp 6⊇ S for all k, i.e., neither S nor a superset of S is present in {Skomp}kmaxk=1 .",3.1. Minimal superset and implications,[0],[0]
"To summarize, exact support recovery using any OMP based scheme including the signal and noise statistics aware schemes is possible only if kmin = k0.",3.1. Minimal superset and implications,[0],[0]
"Whenever kmin > k0, it is possible to estimate true support S without having any false negatives.",3.1. Minimal superset and implications,[0],[0]
"However, one then has to suffer from false positives.",3.1. Minimal superset and implications,[0],[0]
"When kmin =∞, any support in {Skomp}kmaxk=1 has to suffer from false negatives and all supports Skomp for k > k0 − 1 has to suffer from false positives also.",3.1. Minimal superset and implications,[0],[0]
Note that the matrix and SNR conditions required for exact support recovery in Lemma 1 automatically implies that kmin = k0.,3.1. Minimal superset and implications,[0],[0]
"We formulate the proposed RRT scheme assuming that kmin = k0.
3.2.",3.1. Minimal superset and implications,[0],[0]
"Behaviour of RR(k0)
",3.1. Minimal superset and implications,[0],[0]
"Next we consider the behaviour of residual ratio statistic at the k0 iteration, i.e., RR(k0) =",3.1. Minimal superset and implications,[0],[0]
‖rk0‖2/‖rk0−1‖2 under the assumption that ‖w‖2 ≤ omp and δk0+1 < 1/ √ k0 + 1 which ensures kmin = k0 and Skomp ⊆ S for all k ≤ k0.,3.1. Minimal superset and implications,[0],[0]
"Since Xβ = XSβS ∈ span(XS), (In − Pk)Xβ 6=",3.1. Minimal superset and implications,[0],[0]
0n if S 6⊆ Skomp and (In − Pk)Xβ = 0n if S ⊆ Skomp.,3.1. Minimal superset and implications,[0],[0]
This along with the monotonicity of Skomp implies the following.,3.1. Minimal superset and implications,[0],[0]
(In − Pk)Xβ 6= 0n for k < kmin = k0 and (In − Pk)Xβ = 0n for k ≥ kmin = k0.,3.1. Minimal superset and implications,[0],[0]
Thus rk = (In − Pk)y =,3.1. Minimal superset and implications,[0],[0]
(In − Pk)XSβS +,3.1. Minimal superset and implications,[0],[0]
"(In − Pk)w for k < kmin = k0, whereas, rk = (In − Pk)w for k ≥ kmin = k0.",3.1. Minimal superset and implications,[0],[0]
"Consequently, at k = k0, the numerator ‖rk0‖2 ofRR(k0) contains contribution only from the noise term ‖(In−Pk0)w‖2, whereas, the denominator ‖rk0−1‖2 in RR(k0) contain contributions from both the signal term i.e., (In−Pk)XSβS and the noise term (In−Pk)w.",3.1. Minimal superset and implications,[0],[0]
"This behaviour of RR(k0) along with the fact that ‖w‖2
P→ 0 as σ2 → 0 implies the following theorem.
",3.1. Minimal superset and implications,[0],[0]
Theorem 1.,3.1. Minimal superset and implications,[0],[0]
Assume that the matrix X satisfies the RIC constraint δk0+1 < 1/ √ k0 + 1 and kmax > k0.,3.1. Minimal superset and implications,[0],[0]
Then a).,3.1. Minimal superset and implications,[0],[0]
RR(kmin) P→ 0 as σ2 → 0. b).,3.1. Minimal superset and implications,[0],[0]
"lim σ2→0 P(kmin = k0) = 1.
",3.1. Minimal superset and implications,[0],[0]
"Algorithm 2 Residual ratio thresholding Input: Observation y, matrix X Step 1: Run kmax iterations of OMP.",3.1. Minimal superset and implications,[0],[0]
"Step 2: Compute RR(k) for k = 1, . . .",3.1. Minimal superset and implications,[0],[0]
", kmax.",3.1. Minimal superset and implications,[0],[0]
Step 3: Estimate kRRT = max{k : RR(k) ≤ ΓαRRT,3.1. Minimal superset and implications,[0],[0]
(k)} Output: Support estimate Ŝ = SkRRTomp .,3.1. Minimal superset and implications,[0],[0]
Vector estimate β̂(SkRRTomp ),3.1. Minimal superset and implications,[0],[0]
"= X
† SkRRTomp y, β̂({1, . . .",3.1. Minimal superset and implications,[0],[0]
", p}/SkRRTomp ) = 0p−kRRT .
",3.1. Minimal superset and implications,[0],[0]
3.3.,3.1. Minimal superset and implications,[0],[0]
Behaviour of RR(k) for k >,3.1. Minimal superset and implications,[0],[0]
"kmin
",3.1. Minimal superset and implications,[0],[0]
Next we discuss the behaviour of RR(k) for k > kmin.,3.1. Minimal superset and implications,[0],[0]
By the definition of kmin we have S ⊆ Skomp which implies that rk = (In − Pk)w for k ≥ kmin.,3.1. Minimal superset and implications,[0],[0]
"The absence of signal terms in numerator and the denominator of RR(k) = ‖(In−Pk)w‖2‖(In−Pk−1)w‖2 for k > kmin implies that even when ‖w‖2 → 0 or σ2 → 0, RR(k) for k > kmin does not converge to zero.",3.1. Minimal superset and implications,[0],[0]
"This behaviour of RR(k) for k > kmin is captured in Theorem 2 where we provide explicit σ2 or SNR independent lower bounds on RR(k) for k > kmin.
Theorem 2.",3.1. Minimal superset and implications,[0],[0]
"Let Fa,b(x) denotes the cumulative distribution function of a B(a, b) random variable.",3.1. Minimal superset and implications,[0],[0]
"Then ∀σ2 > 0,
ΓαRRT (k) =",3.1. Minimal superset and implications,[0],[0]
"√ F−1n−k
2 ,0.5
( α
kmax(p− k + 1)
) satisfies
P(RR(k) > ΓαRRT",3.1. Minimal superset and implications,[0],[0]
"(k),∀k > kmin)",3.1. Minimal superset and implications,[0],[0]
≥ 1− α.,3.1. Minimal superset and implications,[0],[0]
"(3)
Theorem 2 states that the residual ratio statistic RR(k) for k > kmin is lower bounded by the deterministic sequence {ΓαRRT (k)} kmax k=kmin+1
with a high probability (for small values of α).",3.1. Minimal superset and implications,[0],[0]
Please note that kmin is itself a R.V. Note that the sequence ΓαRRT (k) is dependent only on the matrix dimensions n,3.1. Minimal superset and implications,[0],[0]
and,3.1. Minimal superset and implications,[0],[0]
p.,3.1. Minimal superset and implications,[0],[0]
"Further, Theorem 2 does not make any assumptions on the noise variance σ2 or the design matrix X. Theorem 2 is extremely non trivial considering the fact that the support estimate sequence {Skomp} kmax k=1 produced by OMP is adaptive and data dependent.
",3.1. Minimal superset and implications,[0],[0]
Lemma 2.,3.1. Minimal superset and implications,[0],[0]
"The following important properties of ΓαRRT (k) are direct consequences of the monotonicity of CDF and the fact that a Beta R.V take values only in [0, 1]. 1). ΓαRRT",3.1. Minimal superset and implications,[0],[0]
(k) is defined only in the interval α ∈,3.1. Minimal superset and implications,[0],[0]
"[0, kmax(p− k + 1)].",3.1. Minimal superset and implications,[0],[0]
2). 0,3.1. Minimal superset and implications,[0],[0]
≤ ΓαRRT,3.1. Minimal superset and implications,[0],[0]
(k) ≤ 1. 3). ΓαRRT,3.1. Minimal superset and implications,[0],[0]
(k) is a monotonically increasing function of α. 4). ΓαRRT,3.1. Minimal superset and implications,[0],[0]
(k) = 0,3.1. Minimal superset and implications,[0],[0]
when α = 0 and Γ α RRT (k) = 1 when α = kmax(p− k + 1).,3.1. Minimal superset and implications,[0],[0]
"From Theorem 1, it is clear that P(kmin = k0) and
P(Sompk0 = S) increases with increasing SNR (or decreasing σ2), whereas, RR(kmin) decreases to zero with increasing SNR.",3.4. Residual ratio thresholding framework,[0],[0]
"At the same time, for small values of α like α = 0.01, RR(k) for k > kmin is lower bounded by ΓαRRT (k) with a very high probability at all SNR.",3.4. Residual ratio thresholding framework,[0],[0]
"Hence, finding the last index k such that RR(k) ≤ ΓαRRT",3.4. Residual ratio thresholding framework,[0],[0]
"(k), i.e., kRRT = max{k : RR(k) ≤ ΓαRRT",3.4. Residual ratio thresholding framework,[0],[0]
(k)} gives k0 and equivalently Sk0omp = S with a probability increasing with increasing SNR.,3.4. Residual ratio thresholding framework,[0],[0]
This motivates the proposed signal and noise statistics oblivious RRT algorithm presented in Algorithm 2.,3.4. Residual ratio thresholding framework,[0],[0]
Remark 1.,3.4. Residual ratio thresholding framework,[0],[0]
An important aspect regarding the RRT in Algorithm 2 is the choice of kRRT when the set {k : RR(k) ≤ ΓαRRT,3.4. Residual ratio thresholding framework,[0],[0]
(k)} = φ.,3.4. Residual ratio thresholding framework,[0],[0]
This situation happens only at very low SNR.,3.4. Residual ratio thresholding framework,[0],[0]
When {k : RR(k) ≤ ΓαRRT,3.4. Residual ratio thresholding framework,[0],[0]
"(k)} = φ for a given value of α, we increase the value of α to the smallest value αnew > α such that {k : RR(k) ≤ ΓαnewRRT (k)} 6= φ.",3.4. Residual ratio thresholding framework,[0],[0]
"Mathematically, we set kRRT = max{k : RR(k) < ΓαnewRRT",3.4. Residual ratio thresholding framework,[0],[0]
"(k)}, where αnew = min
a>α {a : {k : RR(k) ≤ ΓαRRT",3.4. Residual ratio thresholding framework,[0],[0]
"(k)} 6= φ}.
",3.4. Residual ratio thresholding framework,[0],[0]
"Since α = p kmax gives ΓαRRT (1) = 1 and RR(1) ≤ 1, a value of αnew ≤ pkmax always exists.",3.4. Residual ratio thresholding framework,[0],[0]
"αnew can be easily computed by first pre-computing {ΓaRRT (k)} kmax k=1 for say 100 prefixed values of a in the interval (α, pkmax].",3.4. Residual ratio thresholding framework,[0],[0]
Remark 2.,3.4. Residual ratio thresholding framework,[0],[0]
RRT requires performing kmax iterations of OMP.,3.4. Residual ratio thresholding framework,[0],[0]
All the quantities required for RRT including RR(k) and the final estimates can be computed while performing these kmax iterations itself.,3.4. Residual ratio thresholding framework,[0],[0]
"Consequently, RRT has complexity O(kmaxnp).",3.4. Residual ratio thresholding framework,[0],[0]
"As we will see later, a good choice of kmax is kmax =",3.4. Residual ratio thresholding framework,[0],[0]
[0.5(n + 1)] which results in a complexity order O(n2p).,3.4. Residual ratio thresholding framework,[0],[0]
This complexity is approximately n/k0 times higher than the O(npk0) complexity of OMP when k0 or σ2 are known a priori.,3.4. Residual ratio thresholding framework,[0],[0]
This is the computational cost being paid for not knowing k0 or σ2 a priori.,3.4. Residual ratio thresholding framework,[0],[0]
"In contrast, L fold CV requires running (1− 1/L)n iterations of OMP L times resulting in a O(L(1 − 1/L)n2p) = O(Ln2p) complexity, i.e., RRT is L times computationally less complex than CV.",3.4. Residual ratio thresholding framework,[0],[0]
Remark 3.,3.4. Residual ratio thresholding framework,[0],[0]
RRT algorithm is developed only assuming that the support sequence generated by the sparse recovery algorithm is monotonically increasing.,3.4. Residual ratio thresholding framework,[0],[0]
"Apart from OMP, algorithms such as orthogonal least squares(Wen et al., 2017) and OMP with thresholding(Yang & de Hoog, 2015) also produce monotonic support sequences.",3.4. Residual ratio thresholding framework,[0],[0]
RRT principle can be directly applied to operate these algorithms in a signal and noise statistics oblivious fashion.,3.4. Residual ratio thresholding framework,[0],[0]
In this section we present support recovery guarantees for RRT and compare it with the results available for OMP with a priori knowledge of k0 or σ2.,4. Analytical Results for RRT,[0],[0]
"The first result in this section deals with the finite sample and finite SNR performance for RRT.
Theorem 3.",4. Analytical Results for RRT,[0],[0]
"Let kmax ≥ k0 and suppose that the matrix X satisfies δk0+1 <
1√ k0+1 .",4. Analytical Results for RRT,[0],[0]
Then RRT can recover the true support S with probability greater than 1− 1/n− α provided that σ,4. Analytical Results for RRT,[0],[0]
"< min( omp, rrt), where
rrt = ΓαRRT (k0)
√ 1− δk0βmin
1 + ΓαRRT (k0) .",4. Analytical Results for RRT,[0],[0]
"(4)
Theorem 3 implies that RRT can identify the support S at a higher SNR or lower noise level than that required by OMP with a priori knowledge of k0 and σ2.",4. Analytical Results for RRT,[0],[0]
"For small values of α like α = 0.01, the probability of exact support recovery, i.e., 1− α− 1/n is similar to that of the 1− 1/n probability of exact support recovery in Lemma 1.",4. Analytical Results for RRT,[0],[0]
"Also please note that the RRT framework does not impose any extra conditions on the design matrix X. Consequently, the only appreciable difference between RRT and OMP with a priori knowledge of k0 and σ2 is in the extra SNR required by RRT which is quantified next using the metric extra = omp/ rrt.",4. Analytical Results for RRT,[0],[0]
"Note that the larger the value of extra, larger should be the SNR or equivalently smaller should be the noise level required for RRT to accomplish exact support recovery.",4. Analytical Results for RRT,[0],[0]
Substituting the values of omp and rrt and using the bound δk0 ≤,4. Analytical Results for RRT,[0],[0]
δk0,4. Analytical Results for RRT,[0],[0]
"+1 gives
extra ≤ 1 + 1ΓαRRT (k0)
1 +
√ 1−δ2k0+1
1− √ k0+1δk0+1
.",4. Analytical Results for RRT,[0],[0]
"(5)
Note that
√ 1−δ2k0+1
1− √ k0+1δk0+1
= (
1−δk0+1 1− √ k0+1δk0+1 )√ 1+δk0+1 1−δk0+1
≥ 1.",4. Analytical Results for RRT,[0],[0]
"Consequently,
extra ≤ 0.5 ( 1 + 1
ΓαRRT (k0)
) .",4. Analytical Results for RRT,[0],[0]
"(6)
Since 0 ≤ ΓαRRT (k0) ≤ 1, it follows that 0.5 (
1 + 1ΓαRRT (k0)
) is always greater than or equal to one.
",4. Analytical Results for RRT,[0],[0]
"However, extra decreases with the increase in ΓαRRT (k0).",4. Analytical Results for RRT,[0],[0]
"In particular, when ΓαRRT (k0) = 1, there is no extra SNR requirement.",4. Analytical Results for RRT,[0],[0]
Remark 4.,4. Analytical Results for RRT,[0],[0]
RRT algorithm involves two hyper parameters viz.,4. Analytical Results for RRT,[0],[0]
kmax and α.,4. Analytical Results for RRT,[0],[0]
Exact support recovery using RRT requires only that kmax ≥ k0.,4. Analytical Results for RRT,[0],[0]
"However, k0 is an unknown quantity.",4. Analytical Results for RRT,[0],[0]
"In our numerical simulations, we set kmax = min(p, [0.5(rank(X) + 1)]).",4. Analytical Results for RRT,[0],[0]
This choice is motivated by the facts that k0 <,4. Analytical Results for RRT,[0],[0]
"[0.5(rank(X)+1)] is a necessary condition for exact support recovery using any sparse estimation algorithm(Elad, 2010) when n < p and min(n, p) is the maximum possible number of iterations in OMP.",4. Analytical Results for RRT,[0],[0]
"Since evaluating rank(X) requires extra computations, one can always use rank(X) ≤ n to set kmax = min(p, [0.5(n+1)]).",4. Analytical Results for RRT,[0],[0]
"Please note that this choice of kmax is independent of the operating SNR, design matrix and the vector to be estimated and the user is not required to tune this parameter.",4. Analytical Results for RRT,[0],[0]
"Hence, α is the only user specified hyper parameter in RRT algorithm.",4. Analytical Results for RRT,[0],[0]
"Next we discuss the behaviour of RRT as n→∞. From (6), it is clear that the extra SNR required for support recovery using RRT decreases with increasing ΓαRRT (k0).",4.1. Large sample behaviour of RRT,[0],[0]
"However, by Lemma 2 increasing ΓαRRT (k0) requires an increase in the value of α.",4.1. Large sample behaviour of RRT,[0],[0]
"However, increasing α decreases the probability of support recovery given by 1 − α − 1/n.",4.1. Large sample behaviour of RRT,[0],[0]
"In other words, one cannot have exact support recovery using RRT at lower SNR without increasing the probability of error in the process.",4.1. Large sample behaviour of RRT,[0],[0]
An answer to this conundrum is available in the large sample regime where it is possible to achieve both α,4.1. Large sample behaviour of RRT,[0],[0]
≈ 0 and ΓαRRT (k0),4.1. Large sample behaviour of RRT,[0],[0]
"≈ 1, i.e., no extra SNR requirement and no decrease in probability of support recovery.",4.1. Large sample behaviour of RRT,[0],[0]
The following theorem states the conditions required for ΓαRRT (k0),4.1. Large sample behaviour of RRT,[0],[0]
≈ 1 for large values of n. Theorem 4.,4.1. Large sample behaviour of RRT,[0],[0]
"Define klim = lim
n→∞ k0/n, plim =
lim n→∞ log(p)/n and αlim = lim n→∞ log(α)/n.",4.1. Large sample behaviour of RRT,[0],[0]
"Let
kmax = min(p, [0.5(n + 1)]).",4.1. Large sample behaviour of RRT,[0],[0]
"Then ΓαRRT (k0) =√ F−1n−k0
2 ,0.5
( α
kmax(p− k0 + 1)
) satisfies the following
asymptotic limits.",4.1. Large sample behaviour of RRT,[0],[0]
Case 1:-).,4.1. Large sample behaviour of RRT,[0],[0]
"lim
n→∞ ΓαRRT (k0) = 1, whenever klim < 0.5,
plim = 0 and αlim = 0.",4.1. Large sample behaviour of RRT,[0],[0]
Case 2:-).,4.1. Large sample behaviour of RRT,[0],[0]
0,4.1. Large sample behaviour of RRT,[0],[0]
"< lim
n→∞ ΓαRRT (k0) < 1 if klim < 0.5,
αlim = 0 and plim > 0.",4.1. Large sample behaviour of RRT,[0],[0]
"In particular, lim n→∞ ΓαRRT (k0) = exp( −plim1−klim ).",4.1. Large sample behaviour of RRT,[0],[0]
"Case 3:- lim
n→∞ ΓαRRT (k0) = 0",4.1. Large sample behaviour of RRT,[0],[0]
"if klim < 0.5, αlim = 0 and
plim =∞.
Theorem 4 states that all choices of (n, p, k0) satisfying plim = 0 and klim < 0.5 can result in lim
n→∞ ΓαRRT (k0) = 1
provided that the parameter α satisfies αlim = 0.",4.1. Large sample behaviour of RRT,[0],[0]
"Note that αlim = 0 for a wide variety of α including α = constant, α = 1/nδ for some δ > 0, α = 1/ log(n) etc.",4.1. Large sample behaviour of RRT,[0],[0]
"It is interesting to see which (n, p, k0) scenario gives plim = 0 and klim < 0.5.",4.1. Large sample behaviour of RRT,[0],[0]
Note that exact recovery in n < p scenario is possible only if k0 ≤,4.1. Large sample behaviour of RRT,[0],[0]
[0.5(n + 1)].,4.1. Large sample behaviour of RRT,[0],[0]
"Thus, the assumption klim < 0.5 will be satisfied in all interesting problem scenarios.
",4.1. Large sample behaviour of RRT,[0],[0]
"Regime 1:- lim n→∞ ΓαRRT (k0) = 1 in low dimensional regression problems with p fixed and n → ∞ or all (n, p) → (∞,∞) with lim
n→∞ p/n ≤ 1.
",4.1. Large sample behaviour of RRT,[0],[0]
"Regime 2:- lim n→∞ ΓαRRT (k0) = 1 in high dimensional case with p increases sub exponentially with n as exp(nδ) for some δ < 1 or p increases polynomially w.r.t n, i.e., p = nδ for some δ > 1.",4.1. Large sample behaviour of RRT,[0],[0]
"In both cases, plim = lim n→∞ log(nδ)/n = 0 and plim = lim n→∞ log(exp(nδ))/n = 0.",4.1. Large sample behaviour of RRT,[0],[0]
"Regime 3:- lim n→∞ ΓαRRT (k0) = 1 in the extreme high dimensional case where (n, p, k0) → (∞,∞,∞) satisfy-
",4.1. Large sample behaviour of RRT,[0],[0]
ing n,4.1. Large sample behaviour of RRT,[0],[0]
≥ ck0 log(p) for some constant c > 0.,4.1. Large sample behaviour of RRT,[0],[0]
"Here plim = lim
n→∞ log(p)/n ≤",4.1. Large sample behaviour of RRT,[0],[0]
"lim n→∞
1
ck0 = 0 and klim =
lim n→∞ 1/c log(p) = 0.",4.1. Large sample behaviour of RRT,[0],[0]
Note that the sampling regime n,4.1. Large sample behaviour of RRT,[0],[0]
"≈ 2k0 log(p) is the best known asymptotic guarantee available for OMP(Fletcher & Rangan, 2012).",4.1. Large sample behaviour of RRT,[0],[0]
"Regime 4:- Consider a sampling regime where (n, p) → (∞,∞) such that k0 is fixed and n = ck0 log(p), i.e., p is exponentially increasing with n. Here plim = 1/(ck0) and klim = 0.",4.1. Large sample behaviour of RRT,[0],[0]
"Consequently, lim
n→∞ ΓαRRT (k0) = exp ( −1 ck0 ) <
1.",4.1. Large sample behaviour of RRT,[0],[0]
"A good example of this sampling regime is (Tropp & Gilbert, 2007) where it was shown that OMP can recover a (not every) particular k0 dimensional signal from n random measurements (in noiseless case) when n = ck0 log(p).",4.1. Large sample behaviour of RRT,[0],[0]
Note that c ≤ 20 for all k0 and c,4.1. Large sample behaviour of RRT,[0],[0]
≈ 4 for large k0.,4.1. Large sample behaviour of RRT,[0],[0]
"Even if we assume that only n = 4k0 log(p) measurements are sufficient for recovering a k0 sparse signal, we have lim n→∞
ΓαRRT",4.1. Large sample behaviour of RRT,[0],[0]
(k0) = exp(−0.125),4.1. Large sample behaviour of RRT,[0],[0]
"= 0.9512 for k0 = 5 (i.e., extra ≤ 1.0257) and lim
n→∞ ΓαRRT (k0) = exp(−0.125)",4.1. Large sample behaviour of RRT,[0],[0]
"=
0.9753 for k0 = 10 (i.e., extra ≤ 1.0127).
",4.1. Large sample behaviour of RRT,[0],[0]
Note that ΓαRRT,4.1. Large sample behaviour of RRT,[0],[0]
"(k0)→ 1 as n→∞ implies that extra → 1 and min( omp, rrt)→ 1.",4.1. Large sample behaviour of RRT,[0],[0]
This asymptotic behaviour of ΓαRRT,4.1. Large sample behaviour of RRT,[0],[0]
"(k0) and extra imply the large sample consistency of RRT as stated in the following theorem.
",4.1. Large sample behaviour of RRT,[0],[0]
Theorem 5.,4.1. Large sample behaviour of RRT,[0],[0]
"Suppose that the sample size n → ∞ such that the matrix X satisfies δk0+1 <
1√ k0+1 , σ ≤ omp and plim = 0.",4.1. Large sample behaviour of RRT,[0],[0]
"Then, a).",4.1. Large sample behaviour of RRT,[0],[0]
OMP running k0 iterations and OMP with SC ‖rk‖2,4.1. Large sample behaviour of RRT,[0],[0]
"≤ σ are large sample consistent, i.e.. lim
n→∞ P(Ŝ = S) = 1.
b).",4.1. Large sample behaviour of RRT,[0],[0]
"RRT with hyper parameter α satisfying lim n→∞ α = 0 and αlim = 0 is also large sample consistent.
",4.1. Large sample behaviour of RRT,[0],[0]
"Theorem 5 implies that at large sample sizes, RRT can accomplish exact support recovery under the same SNR and matrix conditions required by OMP with a priori knowledge of k0 or σ2.",4.1. Large sample behaviour of RRT,[0],[0]
Theorem 5 has a very important corollary.,4.1. Large sample behaviour of RRT,[0],[0]
Remark 5.,4.1. Large sample behaviour of RRT,[0],[0]
"Theorem 1 implies that all choices of α satisfying α→ 0 and αlim = 0 deliver similar performances as n→ ∞. Note that the range of adaptations satisfying α → 0 and αlim = 0 include α = 1/ log(n), α = 1/nδ for δ > 0 etc.",4.1. Large sample behaviour of RRT,[0],[0]
"Since a very wide range of tuning parameters deliver similar results as n → ∞, RRT is in fact asymptotically tuning free.",4.1. Large sample behaviour of RRT,[0],[0]
Remark 6.,4.1. Large sample behaviour of RRT,[0],[0]
"Based on the large sample analysis of RRT, one can make the following guidelines on the choice of α.",4.1. Large sample behaviour of RRT,[0],[0]
"When the sample size n is large, one can choose α as a function of n that satisfies both lim
n→∞ α = 0 and αlim = 0.",4.1. Large sample behaviour of RRT,[0],[0]
"Also since
the support recovery guarantees are of the form 1−1/n−α, it does not make sense to choose a value of α that decays to zero faster than 1/",4.1. Large sample behaviour of RRT,[0],[0]
"n. Hence, it is preferable to choose values of α that decreases to zero slower than 1/n",4.1. Large sample behaviour of RRT,[0],[0]
"like
α = 1/ log(n), α = 1/ √ n etc.",4.1. Large sample behaviour of RRT,[0],[0]
"Having discussed the large sample behaviour of RRT, we next discuss the finite sample and high SNR behaviour of RRT.",4.2. A high SNR operational interpretation of α,[0],[0]
Define the events support recovery error E = {Ŝ 6= S} and false positive F = card(Ŝ/S) > 0,4.2. A high SNR operational interpretation of α,[0],[0]
and missed discovery or false negativeM = card(S/Ŝ),4.2. A high SNR operational interpretation of α,[0],[0]
> 0.,4.2. A high SNR operational interpretation of α,[0],[0]
"The following theorem characterizes the likelihood of these events as SNR increases to infinity or σ2 → 0.
",4.2. A high SNR operational interpretation of α,[0],[0]
Theorem 6.,4.2. A high SNR operational interpretation of α,[0],[0]
Let kmax > k0 and the matrix X satisfies δk0+1 < 1/,4.2. A high SNR operational interpretation of α,[0],[0]
√ k0 + 1.,4.2. A high SNR operational interpretation of α,[0],[0]
"Then, a).",4.2. A high SNR operational interpretation of α,[0],[0]
"lim σ2→0
P(M) = 0. b).",4.2. A high SNR operational interpretation of α,[0],[0]
"lim
σ2→0 P(E) = lim σ2→0 P(F) ≤ α.
",4.2. A high SNR operational interpretation of α,[0],[0]
"Theorem 6 states that when the matrix X allows for exact support recovery in the noiseless or low noise situation, RRT will not suffer from missed discoveries.",4.2. A high SNR operational interpretation of α,[0],[0]
"Under such favourable conditions, α is a high SNR upper bound on both the probability of error and the probability of false positives.",4.2. A high SNR operational interpretation of α,[0],[0]
"Please note that such explicit characterization of hyper parameters are not available for hyper parameters in Square root LASSO, RAT, LAT etc.",4.2. A high SNR operational interpretation of α,[0],[0]
"In this section, we provide extensive numerical simulations comparing the performance of RRT with state of art sparse recovery techniques.",5. Numerical Simulations,[0],[0]
"In particular, we compare the performance of RRT with OMP with k0 estimated using five fold CV and the least squares adaptive thresholding (LAT) proposed in (Wang et al., 2016).",5. Numerical Simulations,[0],[0]
"In synthetic data sets, we also compare RRT with OMP running exactly k0 itera-
tions and OMP with SC ‖rk‖2 ≤ σ",5. Numerical Simulations,[0],[0]
"√ n+ 2 √ n log(n)(Cai & Wang, 2011).",5. Numerical Simulations,[0],[0]
"These algorithms are denoted in Figures 1-4 by “CV”, “LAT”, “OMP1” and “OMP2” respectively.",5. Numerical Simulations,[0],[0]
RRT1 and RRT2 represent RRT with parameter α set to α = 1/ log(n) and α = 1/ √ n respectively.,5. Numerical Simulations,[0],[0]
"By Theorem 5, RRT1 and RRT2 are large sample consistent.",5. Numerical Simulations,[0],[0]
The synthetic data sets are generated as follows.,5.1. Synthetic data sets,[0],[0]
"We consider two models for the matrix X. Model 1 sample each entry of the design matrix X ∈ Rn×p independently according toN (0, 1).",5.1. Synthetic data sets,[0],[0]
Matrix X in Model 2 is formed by concatenating In with a n× n Hadamard matrix,5.1. Synthetic data sets,[0],[0]
"Hn, i.e., X =",5.1. Synthetic data sets,[0],[0]
"[In,Hn].",5.1. Synthetic data sets,[0],[0]
"This matrix guarantee exact support recovery using OMP at high SNR once k0 < 1+ √ n
2 (Elad, 2010).",5.1. Synthetic data sets,[0],[0]
The columns of X in both models are normalised to have unit l2-norm.,5.1. Synthetic data sets,[0],[0]
"Based on the choice of X and support S , we conduct 4 experiments.",5.1. Synthetic data sets,[0],[0]
"Experiments 1-2 involve matrix of model 1 with (n, p) given
by (200, 300) and (200, 900) respectively with support S sampled randomly from the set {1, . . .",5.1. Synthetic data sets,[0],[0]
", p}.",5.1. Synthetic data sets,[0],[0]
"Experiment 3 and 4 involve matrix of model 2 with (n = 128, p = 256).",5.1. Synthetic data sets,[0],[0]
"For experiment 3, support S is sampled randomly from the set {1, . . .",5.1. Synthetic data sets,[0],[0]
", p}, whereas, in experiment 4, support S is fixed at {1, 2, . . .",5.1. Synthetic data sets,[0],[0]
", k0}.",5.1. Synthetic data sets,[0],[0]
"The noise w is sampled according toN (0n, σ2In) with σ2 = 1.",5.1. Synthetic data sets,[0],[0]
The non zero entries of β are randomly assigned βj = ±1.,5.1. Synthetic data sets,[0],[0]
"Subsequently, these entries are scaled to achieve SNR = ‖Xβ‖22/n = 3.",5.1. Synthetic data sets,[0],[0]
The number of non zero entries k0 in all experiments are fixed at six.,5.1. Synthetic data sets,[0],[0]
"We compare the algorithms in terms of the l2 error, the number of false positives and the number of false negatives produced in 100 runs of each experiment.
",5.1. Synthetic data sets,[0],[0]
"From the box plots given in Figures 1-4, it is clear that RRT with both values of α perform very similar to OMP1.",5.1. Synthetic data sets,[0],[0]
They differ only in one run of experiment 3 where RRT1 and RRT2 suffer from a false negative.,5.1. Synthetic data sets,[0],[0]
"Further, RRT1 and RRT2 outperform CV and LAT in all the four experiments in terms of all the three metrics considered for evaluation.",5.1. Synthetic data sets,[0],[0]
"This is primarily because LAT and CV are more prone to make false positives, whereas RRT1 and RRT2 does not report any false positives.",5.1. Synthetic data sets,[0],[0]
OMP2 consistently made false negatives which explains its poor performance in terms of l2 error.,5.1. Synthetic data sets,[0],[0]
"We have observed that once the SNR is made slightly higher, OMP2 delivers a performance similar to OMP1.",5.1. Synthetic data sets,[0],[0]
Also note that RRT with two significantly different choices of α viz.,5.1. Synthetic data sets,[0],[0]
α = 1/ √ n and α = 1/ log(n) delivered similar performances.,5.1. Synthetic data sets,[0],[0]
This observation is in agreement with the claim of asymptotic tuning freeness made in Remark 5.,5.1. Synthetic data sets,[0],[0]
Similar trends are also visible in the simulation results presented in supplementary materials.,5.1. Synthetic data sets,[0],[0]
"We next consider the application of sparse estimation techniques including RRT to identify outliers in low dimensional or full column rank (i.e., n > p) real life data sets, an approach first considered in (Mitra et al., 2010; 2013).",5.2. Outlier detection in real data sets,[0],[0]
"Consider a robust regression model of the form y = Xβ + w + gout with usual interpretations for X, β and w.",5.2. Outlier detection in real data sets,[0],[0]
The extra term gout ∈,5.2. Outlier detection in real data sets,[0],[0]
Rn represents the gross errors in the regression model that cannot be modelled using the distributional assumptions on w. Outlier detection problem in linear regression refers to the identification of the support Sg = supp(gout).,5.2. Outlier detection in real data sets,[0],[0]
"Since X has full rank, one can always annihilate the signal component Xβ by projecting onto a subspace orthogonal to span(X).",5.2. Outlier detection in real data sets,[0],[0]
"This will result in a simple linear regression model of the form given by
ỹ =",5.2. Outlier detection in real data sets,[0],[0]
(In −XX†)y =,5.2. Outlier detection in real data sets,[0],[0]
"(In −XX†)gout + (In −XX†)w, (7) i.e., identifying Sg in robust regression is equivalent to a sparse support identification problem in linear regression.",5.2. Outlier detection in real data sets,[0],[0]
"Even though this is a regression problem with n observa-
tions and n variables, the design matrix (In−XX†) in (7) is rank deficient (i.e., rank(In−XX†) = n−rank(X) < n).",5.2. Outlier detection in real data sets,[0],[0]
"Hence, classical techniques based on LS are not useful for identifying Sg.",5.2. Outlier detection in real data sets,[0],[0]
"Since card(Sg) and variance of w are unknown, we only consider the application of RRT, OMP with CV and LAT in detecting Sg .",5.2. Outlier detection in real data sets,[0],[0]
We consider four widely studied real life data sets and compare the outliers identified by these algorithms with the existing and widely replicated studies on these data sets.,5.2. Outlier detection in real data sets,[0],[0]
More details on these data sets are given in the supplementary materials.,5.2. Outlier detection in real data sets,[0],[0]
"The outliers detected by the aforementioned algorithms and outliers reported in existing literature are tabulated in TABLE 1.
",5.2. Outlier detection in real data sets,[0],[0]
"Among the four data sets considered, outliers detected by RRT and existing results are in consensus in two data sets viz.",5.2. Outlier detection in real data sets,[0],[0]
Stack loss and Stars data sets.,5.2. Outlier detection in real data sets,[0],[0]
"In AR2000 data set, RRT identifies all the outliers.",5.2. Outlier detection in real data sets,[0],[0]
"However, RRT also include observations 14 and 50 as outliers.",5.2. Outlier detection in real data sets,[0],[0]
These identifications can be potential false positives.,5.2. Outlier detection in real data sets,[0],[0]
"In Brain and Body Weight data set, RRT agrees with the existing results in 4 observations.",5.2. Outlier detection in real data sets,[0],[0]
"However, RRT misses two observations viz.",5.2. Outlier detection in real data sets,[0],[0]
14 and 17 which are claimed to be outliers by existing results.,5.2. Outlier detection in real data sets,[0],[0]
LAT agrees with RRT in all data sets except the stack loss data set where it missed outlier indices 1 and 3.,5.2. Outlier detection in real data sets,[0],[0]
CV correctly identified all the outliers identified by other algorithms in all four data sets.,5.2. Outlier detection in real data sets,[0],[0]
"However, it made lot of false positives in three data sets.",5.2. Outlier detection in real data sets,[0],[0]
"To summarize, among all the three algorithms considered, RRT delivered an outlier detection performance which is the most similar to the results reported in literature.",5.2. Outlier detection in real data sets,[0],[0]
This article proposed a novel signal and noise statistics independent sparse recovery technique based on OMP called residual ratio thresholding and derived finite and large sample guarantees for the same.,6. Conclusions,[0],[0]
Numerical simulations in real and synthetic data sets demonstrates a highly competitive performance of RRT when compared to OMP with a priori knowledge of signal and noise statistics.,6. Conclusions,[0],[0]
The RRT technique developed in this article can be used to operate sparse recovery techniques that produce a monotonic sequence of support estimates in a signal and noise statistics oblivious fashion.,6. Conclusions,[0],[0]
"However, the support estimate sequence generated by algorithms like LASSO, DS, SP etc. are not monotonic in nature.",6. Conclusions,[0],[0]
"Hence, extending the concept of RRT to operate sparse estimation techniques that produce non monotonic support sequence in a signal and noise statistics oblivious fashion is an interesting direction of future research.",6. Conclusions,[0],[0]
Orthogonal matching pursuit (OMP) is a widely used algorithm for recovering sparse high dimensional vectors in linear regression models.,abstractText,[0],[0]
The optimal performance of OMP requires a priori knowledge of either the sparsity of regression vector or noise statistics.,abstractText,[0],[0]
Both these statistics are rarely known a priori and are very difficult to estimate.,abstractText,[0],[0]
"In this paper, we present a novel technique called residual ratio thresholding (RRT) to operate OMP without any a priori knowledge of sparsity and noise statistics and establish finite sample and large sample support recovery guarantees for the same.",abstractText,[0],[0]
Both analytical results and numerical simulations in real and synthetic data sets indicate that RRT has a performance comparable to OMP with a priori knowledge of sparsity and noise statistics.,abstractText,[0],[0]
Signal and Noise Statistics Oblivious Orthogonal Matching Pursuit ,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4822–4828 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
4822",text,[0],[0]
An important aspect of natural language processing involves understanding events mentioned in text.,1 Introduction,[0],[0]
"Towards this end, event detection (ED) is the task of locating event triggers (usually verbs or nouns) within a given text, and classifying them among a given set of event types.",1 Introduction,[0],[0]
This task remains challenging due to the inherent ambiguity and flexibility of natural languages.,1 Introduction,[0],[0]
"The current state-of-the-art methods for ED have involved applying deep learning (DL) models to automatically extract feature representations of the text, and then treating the task as a classification problem (Chen et al., 2015; Nguyen and Grishman, 2015b).
",1 Introduction,[0],[0]
The major intuition in this paper is that the task of ED is closely related to the task of word sense disambiguation (WSD) whose datasets can help to improve the performance of the DL models for ED.,1 Introduction,[0],[0]
"This is due to the goal of WSD to determine the sense of a word within a particular context, given a set of possible senses that the word can take on.",1 Introduction,[0],[0]
"Our intuition is based on the two following aspects:
(i) Similar Context Modeling:",1 Introduction,[0],[0]
"Given a word in a context/sentence, both ED and WSD models need
to select/predict a correct label in a list of candidate labels for the word.",1 Introduction,[0],[0]
"For WSD, the candidate labels are the possible senses (e.g, sense ids in WordNet) that the word of interest can have, while for ED, they are the set of predetermined event types (e.g, the event subtypes in the ACE 2005 dataset1).",1 Introduction,[0],[0]
"Consider the word “fired” in the following sentence as an example:
The boss fired his secretary today.",1 Introduction,[0],[0]
"For WSD, there are 12 possible senses for the verb “fire” in WordNet in which the correct label for the word “fired” in this case is the sense id “fire%2:41:00::” (i.e, “terminate the employment of ”).",1 Introduction,[0],[0]
"The ED task in the ACE 2005 dataset, on the other hand, involves 33 possible event subtypes with “End-Position” as the correct event subtype/label for the word “fired” in our example.
",1 Introduction,[0],[0]
"In order to make such label predictions, both ED and WSD need to model the word itself and its context (i.e, the words “fired”, “boss”, and “secretary” in the example).",1 Introduction,[0],[0]
"This similar modeling allows the same DL model to be adopted for both ED and WSD, facilitating the use of WSD data to improve the feature representations for ED via parameter/representation tying.
",1 Introduction,[0],[0]
"(ii) Close Semantic Consideration: As there are some overlaps between the semantic differentiation in WSD and ED, the knowledge/information from WSD about a particular word in a context can help to make a better prediction for that word in ED.",1 Introduction,[0],[0]
"For instance, in the example above, the knowledge from WSD that the word “fired” is referring to a termination of employment would clearly help ED to identify “End-Position” as the correct event type (rather than the incorrect event type “Attack”) for “fired” in this case.
",1 Introduction,[0],[0]
"How can we exploit this intuition to improve the performance of the DL models for ED with WSD
1 https://www.ldc.upenn.edu/collaborations/past-projects/
ace
data?",1 Introduction,[0],[0]
"In this work, we propose a novel method based on representation matching to transfer the knowledge learned from the WSD data to the DL models for ED.",1 Introduction,[0],[0]
"In particular, two separate deep learning models are employed to model the context for WSD and ED.",1 Introduction,[0],[0]
"The two models share the network architecture, but involve different parameters that are specific to the tasks.",1 Introduction,[0],[0]
"We then transfer the knowledge from the WSD network to the ED network by ensuring that the feature representations learned by the two networks on the same contexts are similar to each other.
",1 Introduction,[0],[0]
We demonstrate the effectiveness of the proposed method on two widely used datasets for ED.,1 Introduction,[0],[0]
"To the best of our knowledge, this is the first work to study the transfer learning/multi-task learning methods for WSD and ED with DL.",1 Introduction,[0],[0]
"We consider the typical setting where we have two separate datasets Dwsd = {Wwsdi , pwsdi , ywsdi } for WSD and Ded = {W edi , pedi , yedi } for ED.",2 Model,[0],[0]
"Here, W edi is the i-the sentence of D
ed, pedi is the index of the word of interest for event type prediction in W edi , and y ed i is the corresponding event type label.",2 Model,[0],[0]
"The same conventions apply for Wwsdi , p wsd",2 Model,[0],[0]
"i , y wsd i .",2 Model,[0],[0]
"Also, let Y
wsd and Y ed be the label sets for WSD and ED respectively (i.e, ywsdi ∈ Y wsd and yedi ∈",2 Model,[0],[0]
Y ed).,2 Model,[0],[0]
"Our goal is to transfer the knowledge learned from the Dwsd dataset to improve the performance of the ED models trained on the Ded dataset (multi-task learning).
",2 Model,[0],[0]
"In the following, we will first describe the deep learning architectures to transform the sentences W in the datasets Dwsd and Ded into representation vectors.",2 Model,[0],[0]
We only focus on the deep learning architectures proposed for ED in the literature to achieve compatible comparisons for ED.,2 Model,[0],[0]
The proposed multi-task learning method for ED with the WSD dataset will follow.,2 Model,[0],[0]
Consider a sentence W in the datasets Dwsd or Ded that is represented as a sequence of tokens W =,2.1 Computing the Feature Representations,[0],[0]
"[w0, w1, . .",2.1 Computing the Feature Representations,[0],[0]
.,2.1 Computing the Feature Representations,[0],[0]
", wt].",2.1 Computing the Feature Representations,[0],[0]
Let p be the index of the word of interest in this sentence.,2.1 Computing the Feature Representations,[0],[0]
"The context for wp in W is constructed by taking the word itself, the n preceding words, and the n following words (padding or truncating when necessary).",2.1 Computing the Feature Representations,[0],[0]
The tokens in the context are re-indexed to form an instance V =,2.1 Computing the Feature Representations,[0],[0]
"[v0, v1, . . .",2.1 Computing the Feature Representations,[0],[0]
", vn, . . .",2.1 Computing the Feature Representations,[0],[0]
", v2n−1, v2n],
where vn corresponds to wp in W .",2.1 Computing the Feature Representations,[0],[0]
"Encoding The first step to prepare the instance V for the deep learning models is to map each token vj in V into two real-valued vectors, which are then concatenated to form a vector representation xj for vj (Nguyen and Grishman, 2015b; Chen et al., 2015):
1.",2.1 Computing the Feature Representations,[0],[0]
"The word embedding of vj obtained by looking up the token vj in the pre-trained word embedding table (Mikolov et al., 2013a).
2.",2.1 Computing the Feature Representations,[0],[0]
The position embedding vector for vj : obtained by looking up the relative distance,2.1 Computing the Feature Representations,[0],[0]
j − n of vj with respect to the token of interest vn in a position embedding table (randomly initialized),2.1 Computing the Feature Representations,[0],[0]
"(Chen et al., 2015; Nguyen and Grishman, 2015a).
",2.1 Computing the Feature Representations,[0],[0]
"It is important to note that, different from the prior works (Nguyen and Grishman, 2015b; Liu et al., 2017), we do not include the entity type label of each token into its representation.",2.1 Computing the Feature Representations,[0],[0]
"This is a more realistic setting for our work as the golden entity mentions do not always exist in practice, especially for the datasets in WSD.
",2.1 Computing the Feature Representations,[0],[0]
"Once each token vj is converted into the representation vector xj , the instance V becomes a sequence of vectors X =",2.1 Computing the Feature Representations,[0],[0]
"[x0, x1, . . .",2.1 Computing the Feature Representations,[0],[0]
", xn, . . .",2.1 Computing the Feature Representations,[0],[0]
", x2n−1, x2n] that would be fed into the one of the following deep learning models to learn a feature representation R for V .
",2.1 Computing the Feature Representations,[0],[0]
"Typical Deep Learning Models for ED
1.",2.1 Computing the Feature Representations,[0],[0]
CNN:,2.1 Computing the Feature Representations,[0],[0]
"This is the convolutional neural networks in(Nguyen and Grishman, 2015b; Chen et al., 2015).",2.1 Computing the Feature Representations,[0],[0]
It features convolution operations that are performed over the k consecutive vectors (k-grams) inX and followed by a max-pooling layer to generate the representation vector R for V .,2.1 Computing the Feature Representations,[0],[0]
"Multiple window values k are used to enhance the coverage of the model over the hidden k-grams in the context.
",2.1 Computing the Feature Representations,[0],[0]
"2. NCNN (Nguyen and Grishman, 2016d):",2.1 Computing the Feature Representations,[0],[0]
This model is similar to CNN.,2.1 Computing the Feature Representations,[0],[0]
"The only difference is instead of running the convolution over the k consecutive vectors, NCNN convolutes over the k arbitrarily non-consecutive k vectors in V .",2.1 Computing the Feature Representations,[0],[0]
"This helps NCNN to explicitly model the non-consecutive words in the context to improve ED.
3.",2.1 Computing the Feature Representations,[0],[0]
BiRNN:,2.1 Computing the Feature Representations,[0],[0]
"This is the bidirectional recurrent neural network (RNN) for event extraction in (Nguyen et al., 2016a).",2.1 Computing the Feature Representations,[0],[0]
"The model is
composed of two recurrent neural networks (RNN), where one runs forward and the other runs backward through the input sequence V .",2.1 Computing the Feature Representations,[0],[0]
The hidden vectors produced by the two networks are then concatenated at each position in the context.,2.1 Computing the Feature Representations,[0],[0]
The vector at the position of n for the word of interest is used as the representation vectorR for V .,2.1 Computing the Feature Representations,[0],[0]
"Due to the property of RNN, R encodes the information over the whole input V with a greater focus on vn.
4.",2.1 Computing the Feature Representations,[0],[0]
CNN+BiRNN:,2.1 Computing the Feature Representations,[0],[0]
"In this model (Feng et al., 2016), X is passed through both a CNN and a BiRNN whose results are concatenated to produce the hidden representation R for ED.",2.1 Computing the Feature Representations,[0],[0]
"The expectation is to take advantage of the modeling abilities from both the CNN and BiRNN architectures for ED.
",2.1 Computing the Feature Representations,[0],[0]
"In practice, the representation vector R (obtained from one of the deep learning models above) is also concatenated with the word embeddings of the tokens surrounding the token of interest wn to improve its expressiveness (Chen et al., 2015; Nguyen and Grishman, 2016d).",2.1 Computing the Feature Representations,[0],[0]
"We would use this extended version when we refer to R in the following.
",2.1 Computing the Feature Representations,[0],[0]
"In the final step, the representation vector R is fed into a feed-forward neural network followed by a softmax layer to perform predictions for ED and WSD.
",2.1 Computing the Feature Representations,[0],[0]
"For convenience, we denote the whole process that a DL model M is used to compute the representation vector R for the input sentence W with the token index p of interest as: R =M(W,p).",2.1 Computing the Feature Representations,[0],[0]
The previous section has described the deep learning methods that can be employed to train the models for ED and WSD separately.,2.2 Multi-task Learning Models,[0],[0]
"This section presents our proposed method to transfer the knowledge from the WSD dataset to improve the performance for ED.
",2.2 Multi-task Learning Models,[0],[0]
"A typical method for transfer learning/multitask learning in NLP is to alternate the training process for the parameter-shared models of the related tasks (possibly with different datasets) (Guo et al., 2016; Li et al., 2015; Liu et al., 2016).",2.2 Multi-task Learning Models,[0],[0]
"For instance, in (Guo et al., 2016), the authors use the same deep learning model to learn the feature representations for the text inputs of two related tasks.",2.2 Multi-task Learning Models,[0],[0]
This is then followed by task-specific output layers to perform the corresponding tasks.,2.2 Multi-task Learning Models,[0],[0]
"Note that
the two tasks in (Guo et al., 2016) are provided with two different datasets of different text inputs, thereby being similar to the setting we consider in this work.",2.2 Multi-task Learning Models,[0],[0]
"In order to learn the parameters for this model, in each iteration, (Guo et al., 2016) select one of the tasks with some probabilities, sample a mini-batch of examples in the dataset of the chosen task, and update the model parameters using the objective function specific to the chosen task.",2.2 Multi-task Learning Models,[0],[0]
"Consequently, the model parameters for feature representation learning are updated at every iteration while only the model parameters in the output layer for the chosen task are updated at the current iteration.
",2.2 Multi-task Learning Models,[0],[0]
"It has been demonstrated in (Guo et al., 2016) that the alternating method (called ALT) is more effective than pre-training the network on a related task and fine-tuning it on the expected task.",2.2 Multi-task Learning Models,[0],[0]
We thereby consider ALT as the baseline for multitask learning in our work.,2.2 Multi-task Learning Models,[0],[0]
"However, we argue that this baseline is not effective enough to transfer the knowledge from the WSD dataset to ED in our case.",2.2 Multi-task Learning Models,[0],[0]
This stems from its employment of a single DL model to induce the representations for the text inputs in both tasks.,2.2 Multi-task Learning Models,[0],[0]
"In our case of WSD and ED, although there are some overlap between the semantic differentiation of the two tasks, the labels in the WSD datasets (i.e, the sense ids) tend to be more fine-grained and exhaustive than those in ED.",2.2 Multi-task Learning Models,[0],[0]
"For instance, for the word “fire”, there might be 12 WSD labels for it in WordNet while the number of possible event types for “fire” in the ACE 2005 dataset is only 2 (i.e, “End-Position” and “Attack”).",2.2 Multi-task Learning Models,[0],[0]
"Eventually, if a single DL model is used to compute the representations for the text inputs in both WSD and ED, the model would suffer from a confusion to distinguish such subtlety in the semantic differentiation.
",2.2 Multi-task Learning Models,[0],[0]
"In order to overcome this issue, we propose to employ two versions Mwsd and M ed of the same DL model (with different model parameters) to compute the feature representations for WSD and ED respectively.",2.2 Multi-task Learning Models,[0],[0]
We then transfer the knowledge from Mwsd to M ed by encouraging the representations generated by the two versions Mwsd and M ed on the same text inputs to be similar.,2.2 Multi-task Learning Models,[0],[0]
"Formally, let (W t, pt, yt) be an example in the Dwsd or Ded dataset (t ∈ {wsd, ed}).",2.2 Multi-task Learning Models,[0],[0]
"Also, let Rwsd and Red be the representations for (W t, pt) induced by Mwsd and M ed respectively:
Rwsd =Mwsd(W t, pt), Red =M ed(W t, pt)
",2.2 Multi-task Learning Models,[0],[0]
"Such representation vectors are then followed by a task-specific output layer F t (i.e, feed-forward neural networks followed by a softmax layer) to compute the probability distribution over the possible labels for (W t, pt): P t(Y t|Rt)",2.2 Multi-task Learning Models,[0],[0]
"= F t(Rt) where Y t is the label set for the t task.
",2.2 Multi-task Learning Models,[0],[0]
"If the two models Mwsd and M ed were trained separately, the objective function for the t task for the current example would be the negative loglikelihood: Ct(W t, pt, yt) =",2.2 Multi-task Learning Models,[0],[0]
− logP t(yt|Rt).,2.2 Multi-task Learning Models,[0],[0]
"In this work, instead of just optimizing this objective, we optimize the joint function:
Ct(W t, pt, yt) =",2.2 Multi-task Learning Models,[0],[0]
"− logP t(yt|Rt)
+ λ 1
dR dR∑ i=0 ( Rwsdi −Redi )2 where λ is a trade-off parameter and dR is the dimension of the representation vectors.
",2.2 Multi-task Learning Models,[0],[0]
"The second term in the joint objective function enforces that the feature representations learned by Mwsd and M ed on the same input context (W t, pt) are close to each other (t ∈ {wsd, ed}).",2.2 Multi-task Learning Models,[0],[0]
"One the one hand, this representation matching schema helps the two models to communicate to each other so the knowledge from one model can be passed to the other one.",2.2 Multi-task Learning Models,[0],[0]
"On the other hand, the use of two separate models leaves a flexibility for the models to induce the task-specific structures.
",2.2 Multi-task Learning Models,[0],[0]
"Presumably, the objective function (2.2) can simultaneously improve the performance for both tasks of consideration.",2.2 Multi-task Learning Models,[0],[0]
"However, in our case of ED and WSD, it turns out this mechanism actually worsen the performance of the WSD models that were trained separately.",2.2 Multi-task Learning Models,[0],[0]
"We attribute this to the fact that the semantic differentiation in ED is more coarse-grained that that of WSD, causing the ineffectiveness of the datasets for ED to improve WSD performance.",2.2 Multi-task Learning Models,[0],[0]
"Eventually, we will just focus on the ED performance in the experiments.",2.2 Multi-task Learning Models,[0],[0]
"We use the Semcor dataset (Miller et al., 1994) as the dataset for WSD in this work.",3.1 Parameters and Datasets,[0],[0]
"This dataset was extracted from the Brown Corpus, and manually annotated with WordNet senses.",3.1 Parameters and Datasets,[0],[0]
"We evaluate the models on two different datasets for ED:
1.",3.1 Parameters and Datasets,[0],[0]
ACE 2005:,3.1 Parameters and Datasets,[0],[0]
This dataset has 33 event subtypes.,3.1 Parameters and Datasets,[0],[0]
"We use the same data split with
the prior work (Chen et al., 2015; Nguyen and Grishman, 2015b).",3.1 Parameters and Datasets,[0],[0]
"In particular, 40 newswire documents are used for testing, 30 other documents are reserved for validation, and the 529 remaining documents form the training data.
",3.1 Parameters and Datasets,[0],[0]
2.,3.1 Parameters and Datasets,[0],[0]
TAC 2015:,3.1 Parameters and Datasets,[0],[0]
"This dataset was released in the Event Nugget Detection Evaluation of the 2015 Text Analysis Conference (TAC) (Mitamura et al., 2015).",3.1 Parameters and Datasets,[0],[0]
It comes with 38 event subtypes.,3.1 Parameters and Datasets,[0],[0]
We follow the data split in the official evaluation to achieve compatible comparison.,3.1 Parameters and Datasets,[0],[0]
"As TAC 2015 does not have a development set, we use the best parameters tuned on ACE 2005 for the experiments with TAC 2015.
",3.1 Parameters and Datasets,[0],[0]
"We use the pre-trained word embeddings provided by (Nguyen and Grishman, 2016d).",3.1 Parameters and Datasets,[0],[0]
"For CNN, NCNN and CNN+BiRNN, we employ filter sizes of {2, 3, 4, 5} with 300 filters for each size as in (Nguyen and Grishman, 2015b), while Gated Recurrent Units (Cho et al., 2014) with 300 hidden units are applied in BiRNN and CNN+BiRNN (as do (Nguyen and Grishman, 2016d)).",3.1 Parameters and Datasets,[0],[0]
"For the other parameters, the best values suggested by the development data include: a dropout rate of 0.5, a feed-forward neural network with one hidden layer of 1200 hidden units for the output layers, and the penalty rate λ of 0.01 for both CNN and BiRNN, 0.6 for NCNN, and 0.7 for CNN+BiRNN in the proposed transfer learning method (called MATCHING).",3.1 Parameters and Datasets,[0],[0]
"For simplicity, the same hyper-parameters are used for the two versions of the same network architecture in the MATCHING method.",3.1 Parameters and Datasets,[0],[0]
"We utilize Adadelta (Zeiler, 2012) with back-propagation to train the models in this work.",3.1 Parameters and Datasets,[0],[0]
"In this section, we compare the proposed MATCHING method with the transfer learning baseline ALT in (Guo et al., 2016) and the separate training mechanism for ED (called SEPARATE) employed in the previous work for ED (Chen et al., 2015; Nguyen and Grishman, 2015b).",3.2 Experiments,[0],[0]
"Note that in the SEPARATE method, the models are only trained on the datasets for ED without utilizing any transfer learning techniques with external datasets.",3.2 Experiments,[0],[0]
"We report the performance when each of the DL methods in Section 2.1 is used as the network to learn the feature representations for ED and WSD.
",3.2 Experiments,[0],[0]
"Tables 1 and 2 present the performance (i.e, F1 scores) of the models on the ACE 2005 and TAC 2015 datasets respectively.",3.2 Experiments,[0],[0]
The first observation is that the proposed transfer learning method MATCHING is consistently better than the baseline method ALT across different deep learning models and datasets with large performance gap.,3.2 Experiments,[0],[0]
This is significantly with p < 0.05 and confirms our hypothesis in Section 2.2 about the advantage of the proposed MATCHING over the alternating training method ALT for ED and WSD.,3.2 Experiments,[0],[0]
"In fact, the performance of the ALT method is even worse than the traditional SEPARATE method also over different network architectures and datasets.",3.2 Experiments,[0],[0]
"Consequently, training a single deep learning model on a combination of ED and WSD data (as in ALT) does not automatically enable the model to learn to exploit the similar structures of the two tasks.",3.2 Experiments,[0],[0]
"In contrast, it hinders the model’s ability to effectively extract hidden representations for ED.
",3.2 Experiments,[0],[0]
"Comparing MATCHING and SEPARATE, we see that MATCHING helps to improve SEPARATE with respect to difference choices of the DL models.",3.2 Experiments,[0],[0]
The performance improvement is significant for CNN and BiRNN on ACE 2005 and for all the models on TAC 2015.,3.2 Experiments,[0],[0]
"Such results demonstrate the effectiveness of the WSD dataset for ED and the ability of the proposed method MATCHING to promote knowledge transferring between WSD and ED to improve ED performance.
",3.2 Experiments,[0],[0]
"Regarding the best reported performance, our best performance on ACE (i.e, 71.2% with CNN) is comparable with the recent state-of-the-art performance (i.e, Table 1).",3.2 Experiments,[0],[0]
"However, we note that such work heavily relies on the manual annotation of the entity mentions in the documents.",3.2 Experiments,[0],[0]
Our current work do not employ such information to better reflect the realistic setting.,3.2 Experiments,[0],[0]
"For the TAC 2015 dataset, our best performance is 60.7% with CNN+BiRNN although the performance of the other models is also very close.",3.2 Experiments,[0],[0]
"This performance is better than the best performance that has been reported on the TAC 2015 (i.e, Table 2).",3.2 Experiments,[0],[0]
"Prior works on ED include statistical models with manual feature engineering(Ahn, 2006; Ji and Grishman, 2008; Hong et al., 2011; Li et al., 2013; Venugopal et al., 2014; Li et al., 2015), followed by neural network models, such as CNNs (Nguyen and Grishman, 2015b; Chen et al., 2015; Nguyen
et al., 2016b,e; Chen et al., 2017), RNNs (Nguyen et al., 2016a; Jagannatha and Yu, 2016), and attention-based methods (Liu et al., 2017; Nguyen and Nguyen, 2018b).
",4 Related Work,[0],[0]
"A similar trend exists in methods proposed for WSD, with feature based methods (Miller et al., 1994; Zhong and Ng, 2010; Taghipour and Ng, 2015) succeeded recently by deep learning methods (Yuan et al., 2016; Raganato et al., 2017).
",4 Related Work,[0],[0]
"For multi-task learning in NLP, methods have been proposed for jointly modeling structured prediction tasks (Hatori et al., 2012; Li et al., 2011; Bohnet and Nivre, 2012; Henderson et al., 2013; Lluı́s et al., 2013; Duong et al., 2015), and for sequence-to-sequence problems (Dong et al., 2015; Luong et al., 2015; Liu et al., 2016; Klerke et al., 2016).",4 Related Work,[0],[0]
"The prior work to solve multiple NLP tasks using an unified architecture includes (Collobert and Weston, 2008; Guo et al., 2016).",4 Related Work,[0],[0]
"We present a method that improves the performance of deep learning models for ED by training two different versions of the same network architecture for ED and WSD, while encouraging the knowledge transfer between the two versions via representation matching.",5 Conclusion,[0],[0]
The proposed method produces better results across a variety of deep learning models.,5 Conclusion,[0],[0]
Event detection (ED) and word sense disambiguation (WSD) are two similar tasks in that they both involve identifying the classes (i.e. event types or word senses) of some word in a given sentence.,abstractText,[0],[0]
"It is thus possible to extract the knowledge hidden in the data for WSD, and utilize it to improve the performance on ED.",abstractText,[0],[0]
"In this work, we propose a method to transfer the knowledge learned on WSD to ED by matching the neural representations learned for the two tasks.",abstractText,[0],[0]
Our experiments on two widely used datasets for ED demonstrate the effectiveness of the proposed method.,abstractText,[0],[0]
Similar but not the Same: Word Sense Disambiguation Improves Event Detection via Neural Representation Matching,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 137–149 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
137",text,[0],[0]
Image captioning attracts considerable attention in both natural language processing and computer vision.,1 Introduction,[0],[0]
The task aims to generate a description in natural language grounded on the input image.,1 Introduction,[0],[0]
It is a very challenging yet interesting task.,1 Introduction,[0],[0]
"On the one hand, it has to identify the objects in the image, associate the objects, and express them in a fluent sentence, each of which is a difficult subtask.",1 Introduction,[0],[0]
"On the other hand, it combines two important fields in artificial intelligence, namely, natural language processing and computer vision.",1 Introduction,[0],[0]
"More importantly, it has a wide range of applications, including text-based image retrieval, helping visually impaired people see (Wu et al., 2017), humanrobot interaction (Das et al., 2017), etc.
Models based on the encoder-decoder framework have shown success in image captioning.",1 Introduction,[0],[0]
"According to the pivot representation, they can be
∗Equal Contributions 1",1 Introduction,[0],[0]
"The code is available at https://github.com/
lancopku/simNet
roughly categorized into models based on visual information (Vinyals et al., 2015; Chen and Zitnick, 2015; Mao et al., 2014; Karpathy and Li, 2015, 2017), and models based on conceptual information (Fang et al., 2015; You et al., 2016; Wu et al., 2016).",1 Introduction,[0],[0]
"The later explicitly provides the visual words (e.g. dog, sit, red) to the decoder instead of the image features, and is more effective in image captioning according to the evaluation on benchmark datasets.",1 Introduction,[0],[0]
"However, the models based on conceptual information have a major drawback that it is hard for the model to associate the details with the specific objects in the image, because the visual words are inherently unordered in semantics.",1 Introduction,[0],[0]
Figure 1 shows an example.,1 Introduction,[0],[0]
"For semantic attention, although open is provided as a visual word, due to the insufficient use of visual information, the model gets confused about what objects open should be associated with and thus discards open in the caption.",1 Introduction,[0],[0]
"The model may even associate the details incorrectly, which is the case
for the position of the dog.",1 Introduction,[0],[0]
"In contrast, models based on the visual information often are accurate in details but have difficulty in describing the image comprehensively and tend to only describe a subregion.
",1 Introduction,[0],[0]
"In this work, we get the best of both worlds and integrate visual attention and semantic attention for generating captions that are both detailed and comprehensive.",1 Introduction,[0],[0]
We propose a Stepwise ImageTopic Merging Network as the decoder to guide the information flow between the image and the extracted topics.,1 Introduction,[0],[0]
"At each time step, the decoder first extracts focal information from the image.",1 Introduction,[0],[0]
"Then, it decides which topics are most probable for the time step.",1 Introduction,[0],[0]
"Finally, it attends differently to the visual information and the conceptual information to generate the output word.",1 Introduction,[0],[0]
"Hence, the model can efficiently merge the two kinds of information, leading to outstanding results in image captioning.
",1 Introduction,[0],[0]
"Overall, the main contributions of this work are:
• We propose a novel approach that can effectively merge the information in the image and the topics to generate cohesive captions that are both detailed and comprehensive.",1 Introduction,[0],[0]
"We refine and combine two previous competing attention mechanisms, namely visual attention and semantic attention, with an importancebased merging gate that effectively combines
and balances the two kinds of information.
",1 Introduction,[0],[0]
"• The proposed approach outperforms the state-of-the-art methods substantially on two benchmark datasets, Flickr30k and COCO, in terms of SPICE, which correlates the best with human judgments.",1 Introduction,[0],[0]
Systematic analysis shows that the merging gate contributes the most to the overall improvement.,1 Introduction,[0],[0]
A large number of systems have been proposed for image captioning.,2 Related Work,[0],[0]
"Neural models based on the encoder-decoder framework have been attracting increased attention in the last few years in several multi-discipline tasks, such as neural image/video captioning (NIC) and visual question answering (VQA) (Vinyals et al., 2015; Karpathy and Li, 2015; Venugopalan et al., 2015; Zhao et al., 2016; Zhang et al., 2017).",2 Related Work,[0],[0]
"State-of-theart neural approaches (Anderson et al., 2018; Liu et al., 2018; Lu et al., 2018) incorporate the attention mechanism in machine translation (Bahdanau et al., 2014) to generate grounded image captions.",2 Related Work,[0],[0]
"Based on what they attend to, the models can be categorized into visual attention models and semantic attention models.
",2 Related Work,[0],[0]
Visual attention models pay attention to the image features generated by CNNs.,2 Related Work,[0],[0]
"CNNs are typically pre-trained on the image recognition task to extract general visual signals (Xu et al., 2015; Chen et al., 2017; Lu et al., 2017).",2 Related Work,[0],[0]
The visual attention is expected to find the most relevant image regions in generating the caption.,2 Related Work,[0],[0]
"Most recently, image features based on predicted bounding boxes are used (Anderson et al., 2018; Lu et al., 2018).",2 Related Work,[0],[0]
The advantages are that the attention no longer needs to find the relevant generic regions by itself but instead find relevant bounding boxes that are object orientated and can serve as semantic guides.,2 Related Work,[0],[0]
"However, the drawback is that predicting bounding boxes is difficult, which requires large datasets (Krishna et al., 2017) and complex models (Ren et al., 2015, 2017a).
",2 Related Work,[0],[0]
"Semantic attention models pay attention to a predicted set of semantic concepts (Fang et al., 2015; You et al., 2016; Wu et al., 2016).",2 Related Work,[0],[0]
"The semantic concepts are the most frequent words in the captions, and the extractor can be trained using various methods but typically is only trained on the given image captioning dataset.",2 Related Work,[0],[0]
"This kind
of approach can be seen as the extension of the earlier template-based slotting-filling approaches (Farhadi et al., 2010; Kulkarni et al., 2013).
",2 Related Work,[0],[0]
"However, few work studies how to combine the two kinds of attention models to take advantage of both of them.",2 Related Work,[0],[0]
"On the one hand, due to the limited number of visual features, it is hard to provide comprehensive information to the decoder.",2 Related Work,[0],[0]
"On the other hand, the extracted semantic concepts are unordered, making it hard for the decoder to portray the details of the objects correctly.
",2 Related Work,[0],[0]
This work focuses on combining the visual attention and the semantic attention efficiently to address their drawbacks and make use of their merits.,2 Related Work,[0],[0]
"The visual attention is designed to focus on the attributes and the relationships of the objects, while the semantic attention only includes words that are objects so that the extracted topics could be more accurate.",2 Related Work,[0],[0]
The combination is controlled by the importance-based merging mechanism that decides at each time step which kind of information should be relied on.,2 Related Work,[0],[0]
The goal is to generate image captions that are both detailed and comprehensive.,2 Related Work,[0],[0]
"Our proposed model consists of an image encoder, a topic extractor, and a stepwise merging decoder.",3 Approach,[0],[0]
Figure 3 shows a sketch.,3 Approach,[0],[0]
We first briefly introduce the image encoder and the topic extractor.,3 Approach,[0],[0]
"Then, we introduce the proposed stepwise image-topic merging decoder in detail.",3 Approach,[0],[0]
"For an input image, the image encoder expresses the image as a series of visual feature vectors V = {v1,v2, . . .",3.1 Image Encoder,[0],[0]
",vk},vi ∈",3.1 Image Encoder,[0],[0]
Rg.,3.1 Image Encoder,[0],[0]
Each feature corresponds to a different perspective of the image.,3.1 Image Encoder,[0],[0]
The visual features serve as descriptive guides of the objects in the image for the decoder.,3.1 Image Encoder,[0],[0]
"We use a
ResNet152 (He et al., 2016), which is commonly used in image captioning, to generate the visual features.",3.1 Image Encoder,[0],[0]
"The output of the last convolutional layer is used as the visual information:
V =W V,ICNN(I) (1)
where I is the input image, and W V,I shrinks the last dimension of the output.2",3.1 Image Encoder,[0],[0]
"Typically, identifying an object requires a combination of visual features, and considering the limited capacity of the visual features, it is hard for the conventional decoder to describe the objects in the image comprehensively.",3.2 Topic Extractor,[0],[0]
An advance in image captioning is to provide the decoder with the semantic concepts in the image directly so that the decoder is equipped with an overall perspective of the image.,3.2 Topic Extractor,[0],[0]
"The semantic concepts can be objects (e.g. person, car), attributes (e.g. off, electric), and relationships (e.g. using, sitting).",3.2 Topic Extractor,[0],[0]
"We only use the words that are objects in this work, the reason of which is explained later.",3.2 Topic Extractor,[0],[0]
We call such words topics.,3.2 Topic Extractor,[0],[0]
"The topic extractor concludes a list of candidate topic embeddings T = {w1,w2, . . .",3.2 Topic Extractor,[0],[0]
",wm},wi ∈",3.2 Topic Extractor,[0],[0]
"Re from the image, where e is the dimension of the topic word embeddings.",3.2 Topic Extractor,[0],[0]
"Following common practice (Fang et al., 2015; You et al., 2016), we adopt the weakly-supervised approach of Multiple Instance Learning (Zhang et al., 2006) to build a topic extractor.",3.2 Topic Extractor,[0],[0]
"Due to limited space, please refer to Fang et al. (2015) for detailed explanation.
",3.2 Topic Extractor,[0],[0]
"Different from existing work that uses all the most frequent words in the captions as valid semantic concepts or visual words, we only include the object words (nouns) in the topic word list.",3.2 Topic Extractor,[0],[0]
"Existing work relies on attribute words and rela-
2For conciseness, all the bias terms of linear transformations in this paper are omitted.
",3.2 Topic Extractor,[0],[0]
tionship words to provide visual information to the decoder.,3.2 Topic Extractor,[0],[0]
"However, it not only complicates the extracting procedure but also contributes little to the generation.",3.2 Topic Extractor,[0],[0]
"For an image containing many objects, the decoder is likely to combine the attributes with the objects arbitrarily, as such words are specific to certain objects but are provided to the decoder unordered.",3.2 Topic Extractor,[0],[0]
"In contrast, our model has visual information as additional input and we expect that the decoder should refer to the image for such kind of information instead of the extracted concepts.",3.2 Topic Extractor,[0],[0]
The essential component of the decoder is the proposed stepwise image-topic merging network.,3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"The decoder is based on an LSTM (Hochreiter and Schmidhuber, 1997).",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"At each time step, it combines the textual caption, the attentive visual information, and the attentive conceptual information as the context for generating an output word.",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"The goal is achieved by three modules, the visual attention, the topic attention, and the merging gate.
",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
Visual Attention as Output The visual attention attends to attracting parts of the image based on the state of the LSTM decoder.,3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"In existing work (Xu et al., 2015), only the previous hidden state ht−1 ∈ Rd of the LSTM is used in computation of the visual attention:
Zt = tanh(W Z,V V ⊕W Z,hht−1) (2) αt = softmax(Ztwα,Z) (3)
where W Z,V ∈ Rk×g,W Z,h ∈",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"Rk×d,wα,Z ∈",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
Rk are the learnable parameters.,3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"We denote the matrix-vector addition as ⊕, which is calculated by adding the vector to each column of the matrix.",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
αt ∈,3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
Rk is the attentive weights of V and the attentive visual input,3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
zt ∈,3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"Rg is calculated as
zt = V αt (4)
",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"The visual input zt and the embedding of the previous output word yt−1 are the input of the LSTM.
",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"ht = LSTM( [ zt yt−1 ] ,ht−1) (5)
",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"However, there is a noticeable drawback that the previous output word yt−1, which is a much stronger indicator than the previous hidden state ht−1, is not used in the attention.",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"As zt is used as the input, we call it input attention.",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"To overcome that drawback, we add another attention that incorporates the current hidden state ht, which is
based on the last generated word yt−1:
Z̃t = tanh(W̃ Z,V V ⊕ W̃ Z,hht) (6)
α̃t = softmax(Z̃tw̃α,Z) (7)
z̃t = V α̃t (8)
The procedure resembles the input attention, and we call it output attention.",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
It is worth mentioning that the output attention is essentially the same with the spatial visual attention proposed by Lu et al. (2017).,3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"However, they did not see it from the input-output point of view nor combine it with the input attention.
",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"The attentive visual output is further transformed to rt = tanh(W s,zz̃t),W s,z ∈ Re×g, which is of the same dimension as the topic word embedding to simplify the following procedure.
",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
Topic Attention,3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"In an image caption, different parts concern different topics.",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"In the existing work (You et al., 2016), the conceptual information is attended based on the previous output word:
βt = softmax(T TUyt−1) (9)
whereU ∈ Re×e,βt ∈ Rm.",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
The profound issue is that this approach neglects the visual information.,3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
It should be beneficial to provide the attentive visual information when selecting topics.,3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
The hidden state of the LSTM contains both the information of previous words and the attentive input visual information.,3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"Therefore, the model attends to the topics based on the hidden state of the LSTM:
Qt = tanh(WQ,TT ⊕WQ,hht) (10) βt = softmax(Qtwβ,Q) (11)
where WQ,T ∈ Rm×e,WQ,h ∈ Rm×d,wβ,",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
Q ∈ Rm are the parameters to be learned.,3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
βt ∈,3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"Rm is the weight of the topics, from which the attentive conceptual output qt ∈",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"Re is calculated:
qt = Tβt (12)
",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
The topic attention qt and the hidden state ht are combined as the contextual information,3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"st:
st = tanh(W s,qqt +W s,hht) (13)
where W s,q ∈ Re×e,W s,h ∈ Re×d are learnable parameters.
",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
Merging Gate,3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
We have prepared both the visual information rt and the contextual information st.,3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
It is not reasonable to treat the two kinds of information equally when the decoder generates different types of words.,3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"For example, when generating descriptive words (e.g., behind, red), rt should matter more than st.",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"However, when generating
object words (e.g., people, table), st is more important.",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"We introduce a novel score-based merging mechanism to make the model adaptively learn to adjust the balance:
γt = σ(S(st)− S(rt))",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"(14) ct = γtst + (1− γt)rt (15)
where σ is the sigmoid function, γt ∈",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"[0, 1] indicates how important the topic attention is compared to the visual attention, and S is the scoring function.",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
The scoring function needs to evaluate the importance of the topic attention.,3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
Noticing that Eq. (10) and Eq.,3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"(11) have a similar purpose, we define S similarly:
S(st) = tanh(W S,hht +W S,sst) ·wS (16) S(rt) =",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"tanh(W S,hht +W S,rrt) ·wS (17) where · denotes dot product of vectors, W S,s ∈",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"Rm×e,W S,r ∈ Rm×e are the parameters to be learned, and W S,h,ws share the weights of WQ,h,wβ,Q from Eq.",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"(10) and Eq. (11), respectively.
",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"Finally, the output word is generated by:
yt ∼ pt = softmax(W p,cct) (18) where each value of pt ∈",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
R|D| is a probability indicating how likely the corresponding word in vocabulary D is the current output word.,3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"The whole model is trained using maximum log likelihood and the loss function is the cross entropy loss.
",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"In all, our proposed approach encourages the model to take advantage of all the available information.",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
The adaptive merging mechanism makes the model weigh the information elaborately.,3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"We describe the datasets and the metrics used for evaluation, followed by the training details and the evaluation of the proposed approach.",4 Experiment,[0],[0]
There are several datasets containing images and their captions.,4.1 Datasets and Metrics,[0],[0]
"We report results on the popular Microsoft COCO (Chen et al., 2015) dataset and the Flickr30k (Young et al., 2014) dataset.",4.1 Datasets and Metrics,[0],[0]
"They contain 123,287 images and 31,000 images, respectively, and each image is annotated with 5 sentences.",4.1 Datasets and Metrics,[0],[0]
We report results using the widely-used publicly-available splits in the work of Karpathy and Li (2015).,4.1 Datasets and Metrics,[0],[0]
"There are 5,000 images each in the validation set and the test set for COCO, 1,000 images for Flickr30k.
",4.1 Datasets and Metrics,[0],[0]
"We report results using the COCO captioning evaluation toolkit (Chen et al., 2015) that reports the widely-used automatic evaluation metrics SPICE, CIDEr, BLEU, METEOR, and ROUGE.",4.1 Datasets and Metrics,[0],[0]
"SPICE (Anderson et al., 2016), which is based on scene graph matching, and CIDEr (Vedantam et al., 2015), which is based on n-gram matching, are specifically proposed for evaluating image captioning systems.",4.1 Datasets and Metrics,[0],[0]
They both incorporate the consensus of a set of references for an example.,4.1 Datasets and Metrics,[0],[0]
"BLEU (Papineni et al., 2002) and METOR (Banerjee and Lavie, 2005) are originally proposed for machine translation evaluation.",4.1 Datasets and Metrics,[0],[0]
"ROUGE (Lin and Hovy, 2003; Lin, 2004) is designed for automatic evaluation of extractive text summarization.",4.1 Datasets and Metrics,[0],[0]
"In the related studies, it is concluded that SPICE correlates the best with human judgments with a remarkable margin over the other metrics, and is expert in judging detailedness, where the other metrics show negative correlations, surprisingly; CIDEr and METEOR follows with no particular precedence, followed by ROUGE-L, and BLEU4, in that order (Anderson et al., 2016; Vedantam et al., 2015).",4.1 Datasets and Metrics,[0],[0]
"Following common practice, the CNN used is the ResNet152 model (He et al., 2016) pre-trained on ImageNet.3 There are 2048 7 × 7 feature maps, and we project them into 512 feature maps, i.e. g is 512.",4.2 Settings,[0],[0]
The word embedding size e is 256 and the hidden size d of the LSTM is 512.,4.2 Settings,[0],[0]
"We only keep caption words that occur at least 5 times in the training set, resulting in 10,132 words for COCO and 7,544 for Flickr30k.",4.2 Settings,[0],[0]
"We use the topic extractor pre-trained by Fang et al. (2015) for 1,000 concepts on COCO.",4.2 Settings,[0],[0]
We only use 568 manuallyannotated object words as topics.,4.2 Settings,[0],[0]
"For an image, only the top 5 topics are selected, which means m is 5.",4.2 Settings,[0],[0]
"The same topic extractor is used for Flickr30k, as COCO provides adequate generality.",4.2 Settings,[0],[0]
The caption words and the topic words share the same embeddings.,4.2 Settings,[0],[0]
"In training, we first train the model without visual attention (freezing the CNN parameters) for 20 epochs with the batch size of 80.",4.2 Settings,[0],[0]
The learning rate for the LSTM is 0.0004.,4.2 Settings,[0],[0]
"Then, we switch to jointly train the full model with a learning rate of 0.00001, which exponentially decays with the number of epochs so that it is halved every 50 epochs.",4.2 Settings,[0],[0]
"We also use momen-
3We use the pre-trained model from torchvision.
",4.2 Settings,[0],[0]
tum of 0.8 and weight decay of 0.999.,4.2 Settings,[0],[0]
"We use Adam (Kingma and Ba, 2014) for parameter optimization.",4.2 Settings,[0],[0]
"For fair comparison, we adopt early stop based on CIDEr within maximum 50 epochs.",4.2 Settings,[0],[0]
"We compare our approach with various representative systems on Flickr30k and COCO, including the recently proposed NBT that is the state-of-theart on the two datasets in comparable settings.",4.3 Results,[0],[0]
Table 1 shows the result on Flickr30k.,4.3 Results,[0],[0]
"As we can see, our model outperforms the comparable systems in terms of all of the metrics except BLEU-4.",4.3 Results,[0],[0]
"Moreover, our model overpasses the state-of-theart with a comfortable margin in terms of SPICE, which is shown to correlate the best with human judgments (Anderson et al., 2016).
",4.3 Results,[0],[0]
Table 2 shows the results on COCO.,4.3 Results,[0],[0]
"Among the directly comparable models, our model is arguably the best and outperforms the existing models except in terms of BLEU-4.",4.3 Results,[0],[0]
"Most encouragingly, our model is also competitive with Up-Down (Ander-
son et al., 2018), which uses much larger dataset, Visual Genome (Krishna et al., 2017), with dense annotations to train the object detector, and directly optimizes CIDEr.",4.3 Results,[0],[0]
"Especially, our model outperforms the state-of-the-art substantially in SPICE and METEOR.",4.3 Results,[0],[0]
Breakdown of SPICE Fscores over various subcategories (see Table 3) shows that our model is in dominant lead in almost all subcategories.,4.3 Results,[0],[0]
"It proves the effectiveness of our approach and indicates that our model is quite data efficient.
",4.3 Results,[0],[0]
"For the methods that directly optimize CIDEr, it is intuitive that CIDEr can improve significantly.",4.3 Results,[0],[0]
The similar improvement of BLEU-4 is evidence that optimizing CIDEr leads to more ngram matching.,4.3 Results,[0],[0]
"However, it comes to our notice that the improvements of SPICE, METEOR, and ROUGE-L are far less significant, which suggests there may be a gaming situation where the n-gram matching is wrongfully exploited by the model in reinforcement learning.",4.3 Results,[0],[0]
"As shown by Liu et al. (2017), it is most reasonable to jointly optimize
all the metrics at the same time.",4.3 Results,[0],[0]
"We also evaluate the proposed model on the COCO evaluation server, the results of which are shown in Appendix A.1, due to limited space.",4.3 Results,[0],[0]
"In this section, we analyze the contribution of each component in the proposed approach, and give examples to show the strength and the potential improvements of the model.",5 Analysis,[0],[0]
"The analysis is conducted on the test set of COCO.
",5 Analysis,[0],[0]
Topic Extraction,5 Analysis,[0],[0]
The motivation of using objects as topics is that they are easier to identify so that the generation suffers less from erroneous predictions.,5 Analysis,[0],[0]
"This can be proved by the F-score of the identified topics in the test set, which is shown in Table 4.",5 Analysis,[0],[0]
Using top-5 object words is at least as good as using top-10 all words.,5 Analysis,[0],[0]
"However, using top-10 all words introduces more erroneous visual words to the generation.",5 Analysis,[0],[0]
"As shown in Ta-
ble 5, when extracting all words, providing more words to the model indeed increases the captioning performance.",5 Analysis,[0],[0]
"However, even when top-20 all words are used, the performance is still far behind using only top-5 object words and seems to reach the performance ceiling.",5 Analysis,[0],[0]
"It proves that for semantic attention, it is also important to limit the absolute number of incorrect visual words instead of merely the precision or the recall.",5 Analysis,[0],[0]
It is also interesting to check whether using other kind of words can reach the same effect.,5 Analysis,[0],[0]
"Unfortunately, in our experiments, only using verbs or adjectives as semantic concepts works poorly.
",5 Analysis,[0],[0]
"To examine the contributions of the submodules in our model, we conduct a series of experiments.",5 Analysis,[0],[0]
The results are summarized in Table 3.,5 Analysis,[0],[0]
"To help with the understanding of the differences, we also report the breakdown of SPICE F-scores.
",5 Analysis,[0],[0]
Visual Attention,5 Analysis,[0],[0]
"Our input attention achieves similar results to previous work (Xu et al., 2015),
if not better.",5 Analysis,[0],[0]
"Using only the output attention is much more effective than using only the input attention, with substantial improvements in all metrics, showing the impact of information gap caused by delayed input in attention.",5 Analysis,[0],[0]
"Combining the input attention and the output attention can further improve the results, especially in color and size descriptions.
",5 Analysis,[0],[0]
Topic Attention,5 Analysis,[0],[0]
"As expected, compared with visual attention, the topic attention is better at identifying objects but worse at identifying attributes.",5 Analysis,[0],[0]
"We also apply the merging gate to the topic attention, but it now merges qt and ht instead of st and rt.",5 Analysis,[0],[0]
"With the merging gate, the model can balance the information in caption text and extracted topics, resulting in better overall scores.",5 Analysis,[0],[0]
"While it overpasses the conventional visual attention, it lags behind the output attention.
",5 Analysis,[0],[0]
"Merging Gate Combing the visual attention and the topic attention directly indeed results in a huge boost in performance, which confirms our motivation.",5 Analysis,[0],[0]
"However, directly combining them also causes lower scores in attributes, color, count, and size, showing that the advantages are not fully made use of.",5 Analysis,[0],[0]
"The most dramatic improvements come from applying the merging gate to the combined attention, showing that the proposed balance mechanism can adaptively combine the two kinds of information and is essential to the overall performance.",5 Analysis,[0],[0]
"The average merging gate value summarized in Figure 4 suggests the same.
",5 Analysis,[0],[0]
We give some examples in the left plot of Figure 5 to illustrate the differences between the models more intuitively.,5 Analysis,[0],[0]
"From the examples, it is clear that the proposed simNet generates the best captions in that more objects are described and many informative and detailed attributes are included, such as the quantity and the color.
",5 Analysis,[0],[0]
Visualization Figure 6 shows the visualization of the topic attention and the visual attention with running examples.,5 Analysis,[0],[0]
"As we can see, the topic attention is active when generating a phrase containing the related topic.",5 Analysis,[0],[0]
"For example, bathroom is always most attended when generating a bathroom.",5 Analysis,[0],[0]
The merging gate learns to direct the information flow efficiently.,5 Analysis,[0],[0]
"When generating words such as on and a, it gives lower weight to the topic attention and prefers the visual attention.",5 Analysis,[0],[0]
"As to the visual attention, the output attention is much more focused than the input attention.",5 Analysis,[0],[0]
"As we hypothesized, the conventional input attention lacks the information of the last generated word and does not know what to look for exactly.",5 Analysis,[0],[0]
"For example, when generating bathroom, the input attention does not know the previous generated word is a, and it loses its focus, while the output attention is relatively more concentrated.",5 Analysis,[0],[0]
"Moreover, the merging gate learns to overcome the erroneous topics, as shown in the second example.",5 Analysis,[0],[0]
"When generating chair, the topic attention is focused on a wrong object bed, while the visual attention attends correctly to the chair, and especially the output attention attends to the armrest.",5 Analysis,[0],[0]
"The merging gate effectively remedies
the misleading information from the topic attention and outputs a lower weight, resulting in the model correctly generating the word chair.
",5 Analysis,[0],[0]
Error Analysis,5 Analysis,[0],[0]
We conduct error analysis using the proposed (full) model on the test set to provide insights on how the model may be improved.,5 Analysis,[0],[0]
We find 123 out of 1000 generated captions that are not satisfactory.,5 Analysis,[0],[0]
"There are mainly three types of errors, i.e. distance (32, 26%), movement (22, 18%), and object (60, 49%), with 9 (7%) other errors.",5 Analysis,[0],[0]
Distance error takes place when there is a lot of objects and the model cannot grasp the foreground and the background relationship.,5 Analysis,[0],[0]
Movement error means that the model fails to describe whether the objects are moving.,5 Analysis,[0],[0]
"Those two kinds of errors are hard to eliminate, as they are fundamental problems of computer vision waiting to be resolved.",5 Analysis,[0],[0]
"Object error happens when there are incorrect extracted topics, and the merging gate regards the topic as grounded in the image.",5 Analysis,[0],[0]
"In the given example, the incorrect topic is garden.",5 Analysis,[0],[0]
The tricky part is that the topic is seemingly correct according to the image features or otherwise the proposed model will choose other topics.,5 Analysis,[0],[0]
A more powerful topic extractor may help with the problem but it is unlikely to be completely avoided.,5 Analysis,[0],[0]
We propose the stepwise image-topic merging network to sequentially and adaptively merge the visual and the conceptual information for improved image captioning.,6 Conclusions,[0],[0]
"To our knowledge, we are the first to combine the visual and the semantic attention to achieve substantial improvements.",6 Conclusions,[0],[0]
We introduce the stepwise merging mechanism to efficiently guide the two kinds of information when generating the caption.,6 Conclusions,[0],[0]
"The experimental results demonstrate the effectiveness of the proposed approach, which substantially outperforms the stateof-the-art image captioning methods in terms of SPICE on COCO and Flickr30k datasets.",6 Conclusions,[0],[0]
Quantitative and qualitative analysis show that the generated captions are both detailed and comprehensive in comparison with the existing methods.,6 Conclusions,[0],[0]
This work was supported in part by National Natural Science Foundation of China (No. 61673028).,Acknowledgments,[0],[0]
We thank all the anonymous reviewers for their constructive comments and suggestions.,Acknowledgments,[0],[0]
Xu Sun is the corresponding author of this paper.,Acknowledgments,[0],[0]
A.1 Results on COCO Evaluation Server Table 6 shows the performance on the online COCO evaluation server4.,A Supplementary Material,[0],[0]
"We put it in the appendix because the results are incomplete and the SPICE metric is not available for our submission, which correlates the best with human evaluation.",A Supplementary Material,[0],[0]
"The SPICE metrics are only available at the leaderboard on the COCO dataset website5, which, unfortunately, has not been updated for more than a year.",A Supplementary Material,[0],[0]
"Our submission does not directly optimize CIDEr, use model ensemble, or use extra training data.",A Supplementary Material,[0],[0]
"The three techniques typically result in orthogonal improvements (Lu et al., 2017; Rennie et al., 2017; Anderson et al., 2018).",A Supplementary Material,[0],[0]
"Moreover, the SPICE results are missing, in which the proposed model has the most advantage.",A Supplementary Material,[0],[0]
"Nonetheless, our model is second only to Up-Down (Anderson et al., 2018) and surpasses almost all the other models in published work, especially when 40 references are considered.
",A Supplementary Material,[0],[0]
"4https://competitions.codalab.org/ competitions/3221
5http://cocodataset.org/ #captions-leaderboard",A Supplementary Material,[0],[0]
The encode-decoder framework has shown recent success in image captioning.,abstractText,[0],[0]
"Visual attention, which is good at detailedness, and semantic attention, which is good at comprehensiveness, have been separately proposed to ground the caption on the image.",abstractText,[0],[0]
"In this paper, we propose the Stepwise Image-Topic Merging Network (simNet) that makes use of the two kinds of attention at the same time.",abstractText,[0],[0]
"At each time step when generating the caption, the decoder adaptively merges the attentive information in the extracted topics and the image according to the generated context, so that the visual information and the semantic information can be effectively combined.",abstractText,[0],[0]
The proposed approach is evaluated on two benchmark datasets and reaches the state-of-the-art performances.1,abstractText,[0],[0]
simNet: Stepwise Image-Topic Merging Network for Generating Detailed and Comprehensive Image Captions,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 845–855 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
845",text,[0],[0]
Teaching machines to answer arbitrary usergenerated questions is a long-term goal of natural language processing.,1 Introduction,[0],[0]
"For a wide range of questions, existing information retrieval methods are capable of locating documents that are likely to contain the answer.",1 Introduction,[0],[0]
"However, automatically extracting the answer from those texts remains an open challenge.",1 Introduction,[0],[0]
"The recent success of neural models at answering questions given a related paragraph (Wang et al., 2017c; Tan et al., 2017) suggests they have the potential to be a key part of
∗Work completed while interning at the Allen Institute for Artificial Intelligence
a solution to this problem.",1 Introduction,[0],[0]
"Most neural models are unable to scale beyond short paragraphs, so typically this requires adapting a paragraph-level model to process document-level input.
",1 Introduction,[0],[0]
There are two basic approaches to this task.,1 Introduction,[0],[0]
"Pipelined approaches select a single paragraph from the input documents, which is then passed to the paragraph model to extract an answer (Joshi et al., 2017; Wang et al., 2017a).",1 Introduction,[0],[0]
"Confidence based methods apply the model to multiple paragraphs and return the answer with the highest confidence (Chen et al., 2017a).",1 Introduction,[0],[0]
"Confidence methods have the advantage of being robust to errors in the (usually less sophisticated) paragraph selection step, however they require a model that can produce accurate confidence scores for each paragraph.",1 Introduction,[0],[0]
"As we shall show, naively trained models often struggle to meet this requirement.
",1 Introduction,[0],[0]
In this paper we start by proposing an improved pipelined method which achieves state-of-the-art results.,1 Introduction,[0],[0]
"Then we introduce a method for training models to produce accurate per-paragraph confidence scores, and we show how combining this method with multiple paragraph selection further increases performance.
",1 Introduction,[0],[0]
Our pipelined method focuses on addressing the challenges that come with training on documentlevel data.,1 Introduction,[0],[0]
We use a linear classifier to select which paragraphs to train and test on.,1 Introduction,[0],[0]
"Since annotating entire documents is expensive, data of this sort is typically distantly supervised, meaning only the answer text, not the answer spans, are known.",1 Introduction,[0],[0]
"To handle the noise this creates, we use a summed objective function that marginalizes the model’s output over all locations the answer text occurs.",1 Introduction,[0],[0]
"We apply this approach with a model design that integrates some recent ideas in reading comprehension models, including selfattention (Cheng et al., 2016) and bi-directional attention (Seo et al., 2016).
",1 Introduction,[0],[0]
Our confidence method extends this approach to better handle the multi-paragraph setting.,1 Introduction,[0],[0]
Previous approaches trained the model on questions paired with paragraphs that are known a priori to contain the answer.,1 Introduction,[0],[0]
"This has several downsides: the model is not trained to produce low confidence scores for paragraphs that do not contain an answer, and the training objective does not require confidence scores to be comparable between paragraphs.",1 Introduction,[0],[0]
"We resolve these problems by sampling paragraphs from the context documents, including paragraphs that do not contain an answer, to train on.",1 Introduction,[0],[0]
"We then use a shared-normalization objective where paragraphs are processed independently, but the probability of an answer candidate is marginalized over all paragraphs sampled from the same document.",1 Introduction,[0],[0]
"This requires the model to produce globally correct output even though each paragraph is processed independently.
",1 Introduction,[0],[0]
"We evaluate our work on TriviaQA (Joshi et al., 2017) in the wiki, web, and unfiltered setting.",1 Introduction,[0],[0]
Our model achieves a nearly 10 point lead over published prior work.,1 Introduction,[0],[0]
"We additionally perform an ablation study on our pipelined method, and we show the effectiveness of our multi-paragraph methods on a modified version of SQuAD (Rajpurkar et al., 2016) where only the correct document, not the correct paragraph, is known.",1 Introduction,[0],[0]
"Finally, we combine our model with a web search backend to build a demonstration end-to-end QA system1, and show it performs well on questions from the TREC question answering task (Voorhees et al., 1999).",1 Introduction,[0],[0]
We release our code2 to facilitate future work.,1 Introduction,[0],[0]
"In this section we propose a pipelined QA system, where a single paragraph is selected and passed to a paragraph-level question answering model.",2 Pipelined Method,[0],[0]
"If there is a single source document, we select the paragraph with the smallest TF-IDF cosine distance with the question.",2.1 Paragraph Selection,[0],[0]
Document frequencies are computed using the individual paragraphs within the document.,2.1 Paragraph Selection,[0],[0]
"If there are multiple input documents, we found it beneficial to use a linear classifier that uses the same TF-IDF score, whether the paragraph was the first in its document, how
1https://documentqa.allenai.org 2https://github.com/allenai/document-qa
many tokens preceded it, and the number of question words it includes as features.",2.1 Paragraph Selection,[0],[0]
The classifier is trained on the distantly supervised objective of selecting paragraphs that contain at least one answer span.,2.1 Paragraph Selection,[0],[0]
"On TriviaQA web, relative to truncating the document as done by prior work, this improves the chance of the selected text containing the correct answer from 83.1% to 85.1%.",2.1 Paragraph Selection,[0],[0]
In a distantly supervised setup we label all text spans that match the answer text as being correct.,2.2 Handling Noisy Labels,[0],[0]
This can lead to training the model to select unwanted answer spans.,2.2 Handling Noisy Labels,[0],[0]
Figure 1 contains an example.,2.2 Handling Noisy Labels,[0],[0]
"To handle this difficulty, we use a summed objective function similar to the one from Kadlec et al. (2016), that optimizes the negative loglikelihood of selecting any correct answer span.",2.2 Handling Noisy Labels,[0],[0]
"The models we consider here work by independently predicting the start and end token of the answer span, so we take this approach for both predictions.",2.2 Handling Noisy Labels,[0],[0]
"For example, the objective for predicting the answer start token becomes − log (∑ a∈A pa
) where A is the set of tokens that start an answer and pi is the answer-start probability predicted by the model for token i.",2.2 Handling Noisy Labels,[0],[0]
"This objective has the advantage of being agnostic to how the model distributes probability mass across the possible answer spans, allowing the model to focus on only the most relevant spans.",2.2 Handling Noisy Labels,[0],[0]
"We use a model with the following layers (shown in Figure 2):
Embedding: We embed words using pretrained word vectors.",2.3 Model,[0],[0]
"We concatenate these with character-derived word embeddings, which are
produced by embedding characters using a learned embedding matrix and then applying a convolutional neural network and max-pooling.
",2.3 Model,[0],[0]
"Pre-Process: A shared bi-directional GRU (Cho et al., 2014) is used to process the question and passage embeddings.
",2.3 Model,[0],[0]
"Attention: The attention mechanism from the Bi-Directional Attention Flow (BiDAF) model (Seo et al., 2016) is used to build a queryaware context representation.",2.3 Model,[0],[0]
"Let hi and qj be the vector for context word i and question word j, and nq and nc be the lengths of the question and context respectively.",2.3 Model,[0],[0]
"We compute attention between context word i and question word j as:
aij = w1 · hi +w2 · qj +w3 · (hi qj)
where w1, w2, and w3 are learned vectors and is element-wise multiplication.",2.3 Model,[0],[0]
"We then compute an attended vector ci for each context token as:
pij = eaij∑nq j=1 e aij ci = nq∑ j=1 qjpij
We also compute a query-to-context vector qc:
mi = max 1≤j≤nq",2.3 Model,[0],[0]
"aij
pi = emi∑nc i=1",2.3 Model,[0],[0]
e mi qc = nc∑ i=1,2.3 Model,[0],[0]
"hipi
The final vector for each token is built by concatenating hi, ci, hi ci, and qc ci.",2.3 Model,[0],[0]
"In our model we subsequently pass the result through a linear layer with ReLU activations.
",2.3 Model,[0],[0]
Self-Attention:,2.3 Model,[0],[0]
Next we use a layer of residual self-attention.,2.3 Model,[0],[0]
The input is passed through another bi-directional GRU.,2.3 Model,[0],[0]
"Then we apply the same attention mechanism, only now between the passage and itself.",2.3 Model,[0],[0]
In this case we do not use query-tocontext attention and we set aij = −inf,2.3 Model,[0],[0]
"if i = j.
As before, we pass the concatenated output through a linear layer with ReLU activations.",2.3 Model,[0],[0]
"The result is then summed with the original input.
",2.3 Model,[0],[0]
Prediction:,2.3 Model,[0],[0]
"In the last layer of our model a bidirectional GRU is applied, followed by a linear layer to compute answer start scores for each token.",2.3 Model,[0],[0]
The hidden states are concatenated with the input and fed into a second bi-directional GRU and linear layer to predict answer end scores.,2.3 Model,[0],[0]
"The softmax function is applied to the start and end scores to produce answer start and end probabilities.
",2.3 Model,[0],[0]
"Dropout: We apply variational dropout (Gal and Ghahramani, 2016) to the input to all the GRUs and the input to the attention mechanisms at a rate of 0.2.",2.3 Model,[0],[0]
"We adapt this model to the multi-paragraph setting by using the un-normalized and un-exponentiated (i.e., before the softmax operator is applied) score given to each span as a measure of the model’s confidence.",3 Confidence Method,[0],[0]
"For the boundary-based models we use here, a span’s score is the sum of the start and end score given to its start and end token.",3 Confidence Method,[0],[0]
At test time we run the model on each paragraph and select the answer span with the highest confidence.,3 Confidence Method,[0],[0]
"This is the approach taken by Chen et al. (2017a).
",3 Confidence Method,[0],[0]
"Our experiments in Section 5 show that these confidence scores can be very poor if the model is only trained on answer-containing paragraphs, as done by prior work.",3 Confidence Method,[0],[0]
"Table 1 contains some qualitative examples of the errors that occur.
",3 Confidence Method,[0],[0]
We hypothesize that there are two key sources of error.,3 Confidence Method,[0],[0]
"First, for models trained with the softmax objective, the pre-softmax scores for all spans can be arbitrarily increased or decreased by a constant value without changing the resulting softmax probability distribution.",3 Confidence Method,[0],[0]
"As a result, nothing prevents models from producing scores that are arbitrarily all larger or all smaller for one paragraph
than another.",3 Confidence Method,[0],[0]
"Second, if the model only sees paragraphs that contain answers, it might become too confident in heuristics or patterns that are only effective when it is known a priori that an answer exists.",3 Confidence Method,[0],[0]
"For example, the model might become too reliant on selecting answers that match semantic type the question is asking about, causing it be easily distracted by other entities of that type when they appear in irrelevant text.",3 Confidence Method,[0],[0]
"This kind of error has also been observed when distractor sentences are added to the context (Jia and Liang, 2017)
",3 Confidence Method,[0],[0]
"We experiment with four approaches to training models to produce comparable confidence scores, shown in the following subsections.",3 Confidence Method,[0],[0]
In all cases we will sample paragraphs that do not contain an answer as additional training points.,3 Confidence Method,[0],[0]
In this approach a modified objective function is used where span start and end scores are normalized across all paragraphs sampled from the same context.,3.1 Shared-Normalization,[0],[0]
This means that paragraphs from the same context use a shared normalization factor in the final softmax operations.,3.1 Shared-Normalization,[0],[0]
We train on this objective by including multiple paragraphs from the same context in each mini-batch.,3.1 Shared-Normalization,[0],[0]
"The key idea is that this will force the model to produce scores that are comparable between paragraphs, even though it does not have access to information about what other paragraphs are being considered.",3.1 Shared-Normalization,[0],[0]
"As an alternative to the previous method, we experiment with concatenating all paragraphs sam-
pled from the same context together during training.",3.2 Merge,[0],[0]
A paragraph separator token with a learned embedding is added before each paragraph.,3.2 Merge,[0],[0]
We also experiment with allowing the model to select a special “no-answer” option for each paragraph.,3.3 No-Answer Option,[0],[0]
"First we re-write our objective as:
− log (
esa∑n i=1",3.3 No-Answer Option,[0],[0]
"e si
)",3.3 No-Answer Option,[0],[0]
"− log ( egb∑n j=1 e gj ) =
− log ( esa+gb∑n
i=1",3.3 No-Answer Option,[0],[0]
"∑n j=1 e si+gj ) where sj and gj are the scores for the start and end bounds produced by the model for token j, and a and b are the correct start and end tokens.",3.3 No-Answer Option,[0],[0]
"We have the model compute another score, z, to represent the weight given to a “no-answer” possibility.",3.3 No-Answer Option,[0],[0]
"Our revised objective function becomes:
− log
( (1− δ)ez + δesa+gb
ez + ∑n
i=1",3.3 No-Answer Option,[0],[0]
∑n j=1 e si+gj ) where δ is 1 if an answer exists and 0 otherwise.,3.3 No-Answer Option,[0],[0]
"If there are multiple answer spans we use the same objective, except the numerator includes the summation over all answer start and end tokens.
",3.3 No-Answer Option,[0],[0]
We compute z by adding an extra layer at the end of our model.,3.3 No-Answer Option,[0],[0]
"We build input vectors by taking the summed hidden states of the RNNs used to predict the start/end token scores weighed by the start/end probabilities, and using a learned attention vector on the output of the self-attention layer.
",3.3 No-Answer Option,[0],[0]
These vectors are fed into a two layer network with an 80 dimensional hidden layer and ReLU activations that produces z as its only output.,3.3 No-Answer Option,[0],[0]
"As a final baseline, we consider training models with the sigmoid loss objective function.",3.4 Sigmoid,[0],[0]
"That is, we compute a start/end probability for each token by applying the sigmoid function to the start/end scores of each token.",3.4 Sigmoid,[0],[0]
A cross entropy loss is used on each individual probability.,3.4 Sigmoid,[0],[0]
"The intuition is that, since the scores are being evaluated independently of one another, they are more likely to be comparable between different paragraphs.",3.4 Sigmoid,[0],[0]
"We evaluate our approach on four datasets: TriviaQA unfiltered (Joshi et al., 2017), a dataset of questions from trivia databases paired with documents found by completing a web search of the questions; TriviaQA wiki, the same dataset but only including Wikipedia articles; TriviaQA web, a dataset derived from TriviaQA unfiltered by treating each question-document pair where the document contains the question answer as an individual training point; and SQuAD (Rajpurkar et al., 2016), a collection of Wikipedia articles and crowdsourced questions.",4.1 Datasets,[0],[0]
"We note that for TriviaQA web we do not subsample as was done by Joshi et al. (2017), instead training on the all 530k training examples.",4.2 Preprocessing,[0],[0]
"We also observe that TriviaQA documents often contain many small paragraphs, so we restructure the documents by merging consecutive paragraphs together up to a target size.",4.2 Preprocessing,[0],[0]
We use a maximum paragraph size of 400 unless stated otherwise.,4.2 Preprocessing,[0],[0]
Paragraph separator tokens with learned embeddings are added between merged paragraphs to preserve formatting information.,4.2 Preprocessing,[0],[0]
"We are also careful to mark all spans of text that would be considered an exact match by the official evaluation script, which includes some minor text pre-processing, as answer spans, not just spans that are an exact string match with the answer text.",4.2 Preprocessing,[0],[0]
Our confidence-based approaches are trained by sampling paragraphs from the context during training.,4.3 Sampling,[0],[0]
"For SQuAD and TriviaQA web we take
the top four paragraphs as judged by our paragraph ranking function (see Section 2.1).",4.3 Sampling,[0],[0]
We sample two different paragraphs from those four each epoch to train on.,4.3 Sampling,[0],[0]
"Since we observe that the higherranked paragraphs are more likely to contain the context needed to answer the question, we sample the highest ranked paragraph that contains an answer twice as often as the others.",4.3 Sampling,[0],[0]
"For the merge and shared-norm approaches, we additionally require that at least one of the paragraphs contains an answer span, and both of those paragraphs are included in the same mini-batch.",4.3 Sampling,[0],[0]
"For TriviaQA wiki we repeat the process but use the top 8 paragraphs, and for TriviaQA unfiltered we use the top 16, because much more context is given in these settings.",4.3 Sampling,[0],[0]
"We train the model with the Adadelta optimizer (Zeiler, 2012) with a batch size 60 for TriviaQA and 45 for SQuAD.",4.4 Implementation,[0],[0]
At test time we select the most probable answer span of length less than or equal to 8 for TriviaQA and 17 for SQuAD.,4.4 Implementation,[0],[0]
The GloVe 300 dimensional word vectors released by Pennington et al. (2014) are used for word embeddings.,4.4 Implementation,[0],[0]
"On SQuAD, we use a dimensionality of size 100 for the GRUs and of size 200 for the linear layers employed after each attention mechanism.",4.4 Implementation,[0],[0]
"We found for TriviaQA, likely because there is more data, using a larger dimensionality of 140 for each GRU and 280 for the linear layers is beneficial.",4.4 Implementation,[0],[0]
"During training, we maintain an exponential moving average of the weights with a decay rate of 0.999.",4.4 Implementation,[0],[0]
We use the weight averages at test time.,4.4 Implementation,[0],[0]
We do not update the word vectors during training.,4.4 Implementation,[0],[0]
"First, we do an ablation study on TriviaQA web to show the effects of our proposed methods for our pipeline model.",5.1 TriviaQA Web and TriviaQA Wiki,[0],[0]
We start with a baseline following the one used by Joshi et al. (2017).,5.1 TriviaQA Web and TriviaQA Wiki,[0],[0]
"This
system uses BiDAF (Seo et al., 2016) as the paragraph model, and selects a random answer span from each paragraph each epoch to train on.",5.1 TriviaQA Web and TriviaQA Wiki,[0],[0]
"The first 400 tokens of each document are used during training, and the first 800 during testing.",5.1 TriviaQA Web and TriviaQA Wiki,[0],[0]
"When using the TF-IDF paragraph selection approach, we instead break the documents into paragraphs of size 400 when training and 800 when testing, and select the top-ranked paragraph to feed into the model.",5.1 TriviaQA Web and TriviaQA Wiki,[0],[0]
"As shown in Table 2, our baseline outperforms the results reported by Joshi et al. (2017) significantly, likely because we are not subsampling the data.",5.1 TriviaQA Web and TriviaQA Wiki,[0],[0]
We find both TF-IDF ranking and the sum objective to be effective.,5.1 TriviaQA Web and TriviaQA Wiki,[0],[0]
"Using our refined model increases the gain by another 4 points.
",5.1 TriviaQA Web and TriviaQA Wiki,[0],[0]
Next we show the results of our confidencebased approaches.,5.1 TriviaQA Web and TriviaQA Wiki,[0],[0]
"For this comparison we split documents into paragraphs of at most 400 tokens, and rank them using TF-IDF cosine distance.",5.1 TriviaQA Web and TriviaQA Wiki,[0],[0]
"Then we measure the performance of our proposed approaches as the model is used to independently process an increasing number of these paragraphs, and the highest confidence answer is selected as the final output.",5.1 TriviaQA Web and TriviaQA Wiki,[0],[0]
"The results are shown in Figure 3.
",5.1 TriviaQA Web and TriviaQA Wiki,[0],[0]
"On this dataset even the model trained without any of the proposed training methods (“none”) im-
proves as more paragraphs are used, showing it does a passable job at focusing on the correct paragraph.",5.1 TriviaQA Web and TriviaQA Wiki,[0],[0]
"The no-answer option training approach lead to a significant improvement, and the sharednorm and merge approaches are even better.
",5.1 TriviaQA Web and TriviaQA Wiki,[0],[0]
We use the shared-norm approach for evaluation on the TriviaQA test sets.,5.1 TriviaQA Web and TriviaQA Wiki,[0],[0]
"We found that increasing the paragraph size to 800 at test time, and to 600 during training, was slightly beneficial, allowing our model to reach 66.04 EM and 70.98 F1 on the dev set.",5.1 TriviaQA Web and TriviaQA Wiki,[0],[0]
"As shown in Table 3, our model is firmly ahead of prior work on both the TriviaQA web and TriviaQA wiki test sets.",5.1 TriviaQA Web and TriviaQA Wiki,[0],[0]
"Since our submission, a few additional entries have been added to the public leader for this dataset5, although to the best of our knowledge these results have not yet been published.",5.1 TriviaQA Web and TriviaQA Wiki,[0],[0]
Next we apply our confidence methods to TriviaQA unfiltered.,5.2 TriviaQA Unfiltered,[0],[0]
"This dataset is of particular interest because the system is not told which document contains the answer, so it provides a plausible simulation of answering a question using a document
4Comparison made of 5/01/2018.",5.2 TriviaQA Unfiltered,[0],[0]
"5https://competitions.codalab.org/competitions/17208
retrieval system.",5.2 TriviaQA Unfiltered,[0],[0]
We show the same graph as before for this dataset in Figure 4.,5.2 TriviaQA Unfiltered,[0],[0]
"Our methods have an even larger impact on this dataset, probably because there are many more relevant and irrelevant paragraphs for each question, making paragraph selection more important.
",5.2 TriviaQA Unfiltered,[0],[0]
"Note the naively trained model starts to lose performance as more paragraphs are used, showing that errors are being caused by the model being overly confident in incorrect extractions.",5.2 TriviaQA Unfiltered,[0],[0]
We achieve a score of 61.55 EM and 67.61 F1 on the dev set.,5.2 TriviaQA Unfiltered,[0],[0]
"This advances the only prior result reported for this dataset, 50.6 EM and 57.3 F1 from Wang et al. (2017b), by 10 points.",5.2 TriviaQA Unfiltered,[0],[0]
"We additionally evaluate our model on SQuAD. SQuAD questions were not built to be answered independently of their context paragraph, which makes it unclear how effective of an evaluation tool they can be for document-level question answering.",5.3 SQuAD,[0],[0]
"To assess this we manually label 500 random questions from the training set.
",5.3 SQuAD,[0],[0]
"We categorize questions as:
1.",5.3 SQuAD,[0],[0]
"Context-independent, meaning it can be understood independently of the paragraph.
2.",5.3 SQuAD,[0],[0]
"Document-dependent, meaning it can be understood given the article’s title.",5.3 SQuAD,[0],[0]
"For example, “What individual is the school named after?”",5.3 SQuAD,[0],[0]
"for the document “Harvard University”.
3.",5.3 SQuAD,[0],[0]
"Paragraph-dependent, meaning it can only be understood given its paragraph.",5.3 SQuAD,[0],[0]
"For example, “What was the first step in the reforms?”.
",5.3 SQuAD,[0],[0]
"We find 67.4% of the questions to be contextindependent, 22.6% to be document-dependent,
and the remaining 10% to be paragraphdependent.",5.3 SQuAD,[0],[0]
There are many document-dependent questions because questions are frequently about the subject of the document.,5.3 SQuAD,[0],[0]
"Since a reasonably high fraction of the questions can be understood given the document they are from, and to isolate our analysis from the retrieval mechanism used, we choose to evaluate on the document-level.",5.3 SQuAD,[0],[0]
"We build documents by concatenating all the paragraphs in SQuAD from the same article together into a single document.
",5.3 SQuAD,[0],[0]
"Given the correct paragraph (i.e., in the standard SQuAD setting)",5.3 SQuAD,[0],[0]
our model reaches 72.14 EM and 81.05 F1 and can complete 26 epochs of training in less than five hours.,5.3 SQuAD,[0],[0]
"Most of our variations to handle the multi-paragraph setting caused a minor (up to half a point) drop in performance, while the sigmoid version fell behind by a point and a half.
",5.3 SQuAD,[0],[0]
We graph the document-level performance in Figure 5.,5.3 SQuAD,[0],[0]
"For SQuAD, we find it crucial to employ one of the suggested confidence training techniques.",5.3 SQuAD,[0],[0]
The base model starts to drop in performance once more than two paragraphs are used.,5.3 SQuAD,[0],[0]
"However, the shared-norm approach is able to reach a peak performance of 72.37 F1 and 64.08 EM given 15 paragraphs.",5.3 SQuAD,[0],[0]
"Given our estimate that 10% of the questions are ambiguous if the paragraph is unknown, our approach appears to have adapted to the document-level task very well.
",5.3 SQuAD,[0],[0]
"Finally, we compare the shared-norm model with the document-level result reported by Chen et al. (2017a).",5.3 SQuAD,[0],[0]
"We re-evaluate our model using the documents used by Chen et al. (2017a), which consist of the same Wikipedia articles SQuAD was built from, but downloaded at different dates.",5.3 SQuAD,[0],[0]
The advantage of this dataset is that it does not allow the model to know a priori which paragraphs were filtered out during the construction of SQuAD.,5.3 SQuAD,[0],[0]
"The disadvantage is that some of the articles have been edited since the questions were written, so some questions may no longer be answerable.",5.3 SQuAD,[0],[0]
"Our model achieves 59.14 EM and 67.34 F1 on this dataset, which significantly outperforms the 49.7 EM reported by Chen et al. (2017a).",5.3 SQuAD,[0],[0]
We perform one final experiment that tests our model as part of an end-to-end question answering system.,5.4 Curated TREC,[0],[0]
"For document retrieval, we re-implement the pipeline from Joshi et al. (2017).",5.4 Curated TREC,[0],[0]
"Given a question, we retrieve up to 10 web documents us-
7https://github.com/brmson/yodaqa/wiki/Benchmarks
ing a Bing web search of the question, and all Wikipedia articles about entities the entity linker TAGME (Ferragina and Scaiella, 2010) identifies in the question.",5.4 Curated TREC,[0],[0]
"We then use our linear paragraph ranker to select the 16 most relevant paragraphs from all these documents, which are passed to our model to locate the final answer span.",5.4 Curated TREC,[0],[0]
We choose to use the shared-norm model trained on the TriviaQA unfiltered dataset since it is trained using multiple web documents as input.,5.4 Curated TREC,[0],[0]
We use the same heuristics as Joshi et al. (2017) to filter out trivia or QA websites to ensure questions cannot be trivially answered using webpages that directly address the question.,5.4 Curated TREC,[0],[0]
"A demo of the system is publicly available8.
",5.4 Curated TREC,[0],[0]
"We find accuracy on the TriviaQA unfiltered questions remains almost unchanged (within half a percent exact match score) when using our document retrieval method instead of the given documents, showing our pipeline does a good job of producing evidence documents that are similar to the ones in the training data.
",5.4 Curated TREC,[0],[0]
"We test the system on questions from the TREC QA tasks (Voorhees et al., 1999), in particular a curated set of questions from Baudiš (2015), the same dataset used in Chen et al. (2017a).",5.4 Curated TREC,[0],[0]
"We apply our system to the 694 test questions without retraining on the train questions.
",5.4 Curated TREC,[0],[0]
"We compare against DrQA (Chen et al., 2017a) and YodaQA (Baudiš, 2015).",5.4 Curated TREC,[0],[0]
"It is important to note that these systems use different document corpora (Wikipedia for DrQA, and Wikipedia, several knowledge bases, and optionally Bing web search for YodaQA) and different training data (SQuAD and the TREC training questions for DrQA, and TREC only for YodaQA), so we cannot make assertions about the relative performance of individual components.",5.4 Curated TREC,[0],[0]
"Nevertheless, it is instructive to show how the methods we experiment with in this work can advance an end-to-end QA system.
",5.4 Curated TREC,[0],[0]
The results are listed in Table 4.,5.4 Curated TREC,[0],[0]
"Our method outperforms prior work, breaking the 50% accu-
8https://documentqa.allenai.org/
racy mark.",5.4 Curated TREC,[0],[0]
This is a strong proof-of-concept that neural paragraph reading combined with existing document retrieval methods can advance the stateof-the-art on general question answering.,5.4 Curated TREC,[0],[0]
"It also shows that, despite the noise, the data from TriviaQA is sufficient to train models that can be effective on out-of-domain QA tasks.",5.4 Curated TREC,[0],[0]
We found that models that have only been trained on answer-containing paragraphs can perform very poorly in the multi-paragraph setting.,5.5 Discussion,[0],[0]
"The results were particularly bad for SQuAD; we think this is partly because the paragraphs are shorter, so the model had less exposure to irrelevant text.
",5.5 Discussion,[0],[0]
"The shared-norm approach consistently outperformed the other methods, especially on SQuAD and TriviaQA unfiltered, where many paragraphs were needed to reach peak performance.",5.5 Discussion,[0],[0]
"Figures 3, 4, and 5 show this technique has a minimal effect on the performance when only one paragraph is used, suggesting the model’s per-paragraph performance is preserved.",5.5 Discussion,[0],[0]
"Meanwhile, it can be seen the accuracy of the shared-norm model never drops as more paragraphs are added, showing it successfully resolves the problem of being distracted by irrelevant text.
",5.5 Discussion,[0],[0]
"The no-answer and merge approaches were moderately effective, we suspect because they at least expose the model to more irrelevant text.",5.5 Discussion,[0],[0]
"However, these methods do not address the fundamental issue of requiring confidence scores to be comparable between independent applications of the model to different paragraphs, which is why we think they lagged behind.",5.5 Discussion,[0],[0]
"The sigmoid objective function reduces the paragraph-level performance considerably, especially on the TriviaQA datasets.",5.5 Discussion,[0],[0]
"We suspect this is because it is vulnerable to label noise, as discussed in Section 2.2.",5.5 Discussion,[0],[0]
We perform an error analysis by labeling 200 random TriviaQA web dev-set errors made by the shared-norm model.,5.6 Error Analysis,[0],[0]
"We found 40.5% of the er-
rors were caused because the document did not contain sufficient evidence to answer the question, and 17% were caused by the correct answer not being contained in the answer key.",5.6 Error Analysis,[0],[0]
"The distribution of the remaining errors is shown in Table 5.
",5.6 Error Analysis,[0],[0]
"We found quite a few cases where a sentence contained the answer, but the model was unable to extract it due to complex syntactic structure or paraphrasing.",5.6 Error Analysis,[0],[0]
"Two kinds of multi-sentence reading errors were also common: cases that required connecting multiple statements made in a single paragraph, and long-range coreference cases where a sentence’s subject was named in a previous paragraph.",5.6 Error Analysis,[0],[0]
"Finally, some questions required background knowledge, or required the model to extract answers that were only stated indirectly (e.g., examining a list to extract the nth element).",5.6 Error Analysis,[0],[0]
"Overall, these results suggest good avenues for improvement are to continue advancing the sentence and paragraph level reading comprehension abilities of the model, and adding a mechanism to handle document-level coreferences.",5.6 Error Analysis,[0],[0]
Reading Comprehension Datasets.,6 Related Work,[0],[0]
"The state of the art in reading comprehension has been rapidly advanced by neural models, in no small part due to the introduction of many large datasets.",6 Related Work,[0],[0]
"The first large scale datasets for training neural reading comprehension models used a Cloze-style task, where systems must predict a held out word from a piece of text (Hermann et al., 2015; Hill et al., 2015).",6 Related Work,[0],[0]
"Additional datasets including SQuAD (Rajpurkar et al., 2016), WikiReading (Hewlett et al., 2016), MS Marco (Nguyen et al., 2016) and TriviaQA (Joshi et al., 2017) provided more realistic questions.",6 Related Work,[0],[0]
"Another dataset of trivia questions, Quasar-T (Dhingra et al., 2017), was introduced recently that uses ClueWeb09 (Callan et al., 2009) as its source for documents.",6 Related Work,[0],[0]
"In this work we choose to focus on SQuAD because it is well studied, and TriviaQA because it is more challenging and features documents and multi-document contexts (Quasar T is similar, but was released after we started work on this project).
",6 Related Work,[0],[0]
Neural Reading Comprehension.,6 Related Work,[0],[0]
"Neural reading comprehension systems typically use some form of attention (Wang and Jiang, 2016), although alternative architectures exist (Chen et al., 2017a; Weissenborn et al., 2017b).",6 Related Work,[0],[0]
"Our model follows this approach, but includes some recent advances such as variational dropout (Gal
and Ghahramani, 2016) and bi-directional attention (Seo et al., 2016).",6 Related Work,[0],[0]
"Self-attention has been used in several prior works (Cheng et al., 2016; Wang et al., 2017c; Pan et al., 2017).",6 Related Work,[0],[0]
Our approach to allowing a reading comprehension model to produce a per-paragraph no-answer score is related to the approach used in the BiDAFT,6 Related Work,[0],[0]
"(Min et al., 2017) model to produce per-sentence classification scores, although we use an attentionbased method instead of max-pooling.
",6 Related Work,[0],[0]
"Open QA. Open question answering has been the subject of much research, especially spurred by the TREC question answering track (Voorhees et al., 1999).",6 Related Work,[0],[0]
"Knowledge bases can be used, such as in (Berant et al., 2013), although the resulting systems are limited by the quality of the knowledge base.",6 Related Work,[0],[0]
"Systems that try to answer questions using natural language resources such as YodaQA (Baudiš, 2015) typically use pipelined methods to retrieve related text, build answer candidates, and pick a final output.
",6 Related Work,[0],[0]
"Neural Open QA. Open question answering with neural models was considered by Chen et al. (2017a), where researchers trained a model on SQuAD and combined it with a retrieval engine for Wikipedia articles.",6 Related Work,[0],[0]
Our work differs because we focus on explicitly addressing the problem of applying the model to multiple paragraphs.,6 Related Work,[0],[0]
"A pipelined approach to QA was recently proposed by Wang et al. (2017a), where a ranker model is used to select a paragraph for the reading comprehension model to process.",6 Related Work,[0],[0]
"More recent work has considered evidence aggregation techniques (Wang et al., 2017b; Swayamdipta et al., 2017).",6 Related Work,[0],[0]
"Our work shows paragraph-level models that produce well-calibrated confidence scores can effectively exploit large amounts of text without aggregation, although integrating aggregation techniques could further improve our results.",6 Related Work,[0],[0]
"We have shown that, when using a paragraph-level QA model across multiple paragraphs, our training method of sampling non-answer-containing paragraphs while using a shared-norm objective function can be very beneficial.",7 Conclusion,[0],[0]
"Combining this with our suggestions for paragraph selection, using the summed training objective, and our model design allows us to advance the state of the art on TriviaQA.",7 Conclusion,[0],[0]
"As shown by our demo, this work can be directly applied to building deep-learningpowered open question answering systems.",7 Conclusion,[0],[0]
We introduce a method of adapting neural paragraph-level question answering models to the case where entire documents are given as input.,abstractText,[0],[0]
"Most current question answering models cannot scale to document or multi-document input, and naively applying these models to each paragraph independently often results in them being distracted by irrelevant text.",abstractText,[0],[0]
We show that it is possible to significantly improve performance by using a modified training scheme that teaches the model to ignore non-answer containing paragraphs.,abstractText,[0],[0]
"Our method involves sampling multiple paragraphs from each document, and using an objective function that requires the model to produce globally correct output.",abstractText,[0],[0]
We additionally identify and improve upon a number of other design decisions that arise when working with document-level data.,abstractText,[0],[0]
"Experiments on TriviaQA and SQuAD shows our method advances the state of the art, including a 10 point gain on TriviaQA.",abstractText,[0],[0]
Simple and Effective Multi-Paragraph Reading Comprehension,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 554–558 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
554",text,[0],[0]
"We present new evidence that the SimpleQuestions benchmark (Bordes et al., 2015) can be nearly solved by standard methods.",1 Introduction,[0],[0]
"First, we show that ambiguity in the data bounds performance; there are often questions have more than one equally plausible interpretation.",1 Introduction,[0],[0]
"Second, we introduce a baseline that sets a new state-of-the-art performance level, despite using standard methods.",1 Introduction,[0],[0]
"Finally, we report an empirical analysis showing that the upperbound is loose.
",1 Introduction,[0],[0]
"The simple questions task involves mapping an English question (e.g. “Who wrote Gulliver’s travels?”) to an analogous Freebase (Bollacker et al., 2008) query, used to answer the question.",1 Introduction,[0],[0]
The query consists of a Freebase relation (e.g. /film/film/story by) and subject (e.g. 090s 0,1 Introduction,[0],[0]
[gulliver’s travels]).,1 Introduction,[0],[0]
"To understand how we might bound performance on the SimpleQuestions dataset, our first contribution in this paper, consider the following examples:
a. who wrote gulliver’s travels?",1 Introduction,[0],[0]
"(film/film/story by, 090s 0",1 Introduction,[0],[0]
"[gulliver’s travels,
TV miniseries])
b. Name a character from gullivers travels.",1 Introduction,[0],[0]
"(book/book/characters, 0btc7 [ gulliver’s travels])
",1 Introduction,[0],[0]
"In example (a) the phrase “Gulliver’s travels” is mapped to a TV miniseries, while in (b) it is mapped to a book.",1 Introduction,[0],[0]
"This introduces an unintended ambiguity, since either mapping is equally plausible for both examples (i.e. both books and TV miniseries have authors and characters).",1 Introduction,[0],[0]
"We introduce a method for automatically identifying many such ambiguities in the data, for both the entities and relations, and show that performance is upperbounded at 83.4%.
",1 Introduction,[0],[0]
"Our second main contribution is a baseline that sets a new state-of-the-art performance level, despite using standard methods.",1 Introduction,[0],[0]
Our approach includes (1) a CRF used to tag the mention of the subject in a question and (2) a BiLSTM used to classify the Freebase relation.,1 Introduction,[0],[0]
"Despite its simplicity, this approach achieves 78.1% accuracy for predicting Freebase subject-relation queries, surpassing all previous models.
",1 Introduction,[0],[0]
"Finally, we present an empirical error analysis of this model which shows the upperbound is loose and that there is likely not much more than 4% of performance to be gained with future work on the data.",1 Introduction,[0],[0]
"Together, these results suggest that the SimpleQuestions dataset is nearly solved.",1 Introduction,[0],[0]
Our code and pretrained models are available at github.com/PetrochukM/ Simple-Question-Answering.,1 Introduction,[0],[0]
"Single-relation factoid questions (simple questions) are common in many settings (e.g. Microsoft’s search query logs (Yih et al., 2014) and WikiAnswers web questions (Fader et al., 2013)).",2 Background,[0],[0]
"The SimpleQuestions dataset is one of the most
commonly used benchmarks for studying such questions.
",2 Background,[0],[0]
The Freebase knowledge graph (KG) provides the facts for answering the questions in the SimpleQuestions dataset.,2 Background,[0],[0]
"It includes 3 billion triples of the form (subject, relation, object) (e.g. [04b5zb (Fires Creek), location/location/containedby, 0f80hy (Nantahala National Forest)]).",2 Background,[0],[0]
"We denote such triples as (s, r, o).
",2 Background,[0],[0]
"The SimpleQuestions task is to rewrite questions into subject-relation pairs of the form (subject, relation), denoted in this paper as (s, r).",2 Background,[0],[0]
Each pair defines a graph query that can be used to answer the corresponding natural language question.,2 Background,[0],[0]
The subject is a Freebase object with a identifier called an MID (e.g. 04b5zb ).,2 Background,[0],[0]
"Freebase objects also typically include one or more string aliases (e.g. MID 04b5zb is named “Fires Creek”), which we will use later when computing our upper bounds.",2 Background,[0],[0]
The relation is an object property (e.g. location/location/containedby) defined by the Freebase ontology.,2 Background,[0],[0]
"For example, the question “which forest is fires creek in” corresponds with the subject-relation pair (04b5zb",2 Background,[0],[0]
"[Fires Creek], location/location/containedby).",2 Background,[0],[0]
"Finally, the SimpleQuestions task is evaluated on subject-relation pair accuracy.
",2 Background,[0],[0]
"The SimpleQuestions dataset provides a set of 108,442 simple questions; each question is accompanied by a ground truth triple (s, r, o).",2 Background,[0],[0]
This dataset also provides two subsets of Freebase: FB2M and FB5M.1,2 Background,[0],[0]
The ambiguity in the SimpleQuestions dataset likely comes from the way the data was created.,3 Dataset Ambiguity and Upperbound,[0],[0]
Annotators were shown a single Freebase triple and asked to write a question.,3 Dataset Ambiguity and Upperbound,[0],[0]
"For example, given any of the following triples:
• (0btc7 [Gulliver’s Travels, Book], book/written work/author, o3 dj",3 Dataset Ambiguity and Upperbound,[0],[0]
"[Dean Swift])
",3 Dataset Ambiguity and Upperbound,[0],[0]
• (06znpjr,3 Dataset Ambiguity and Upperbound,[0],[0]
"[Gulliver’s Travels, American film], film/film/written by, 03whnyn",3 Dataset Ambiguity and Upperbound,[0],[0]
"[Nicholas Stroller])
1The FB2M and FB5M subsets of Freebase KG can complete 7,188,636 and 7,688,234 graph queries respectively; therefore, the FB5M subset is 6.9% larger than the FB2M subset.",3 Dataset Ambiguity and Upperbound,[0],[0]
"More previous research has cited FB2M numbers than FB5M; therefore, we report our numbers on FB2M.
• (06znpjr",3 Dataset Ambiguity and Upperbound,[0],[0]
"[Gulliver’s Travels, American film], film/film/story by, o3 dj [Dean Swift])
",3 Dataset Ambiguity and Upperbound,[0],[0]
The annotator might reasonably contribute the question “who wrote gulliver’s travels?”,3 Dataset Ambiguity and Upperbound,[0],[0]
"However, adding all of these pairs to the data is problematic.",3 Dataset Ambiguity and Upperbound,[0],[0]
"Systems are evaluated on producing the correct subject-relation pair, and cannot learn a deterministic mapping that would get these three examples correct.",3 Dataset Ambiguity and Upperbound,[0],[0]
"In this section, we present a simple heuristic method for finding many such instances of ambiguity, and use it to upper bound performance on this benchmark.",3 Dataset Ambiguity and Upperbound,[0],[0]
"Given an example question q with the ground truth (s, r, o), our goal is to determine the set of all other subject-relation pairs that are equally supported by the text in q.
We first determine a string alias a for the subject by matching a phrase in q with a Freebase alias for s, in our example yielding “gulliver’s travels”.",3.1 Approach,[0],[0]
"For 97% of questions q, some string alias a exactly matched a question q phrase.",3.1 Approach,[0],[0]
"We then find all other Freebase entities that share this alias a and add them to a set S, in our example S is the subject column of Table 1.
",3.1 Approach,[0],[0]
We define an abstract predicate p (e.g. “who wrote e?”) as q with alias a abstracted.,3.1 Approach,[0],[0]
"We determine the set of potential relations R as the relations p co-occurs with in the SimpleQuestions dataset, in our example R is the relation column of Table 2.
",3.1 Approach,[0],[0]
"Finally, if there exists a subject-relation pair (s, r) ∈ KG such that r ∈ R ∧ s ∈ S",3.1 Approach,[0],[0]
"we de-
fine that as an accurate semantic interpretation of q. q is unanswerable if there exists multiple valid subject-relation pairs (s, r).",3.1 Approach,[0],[0]
"In our example above, the question is unanswerable because of the many different subject, relation pairs that co-occur with “gulliver’s travels” and “who wrote e?”",3.1 Approach,[0],[0]
We find that 33.9% of examples in the SimpleQuestions dataset are unanswerable.,3.2 Results,[0],[0]
"In these cases, we can predict a majority baseline (i.e. always guess the most commonly seen Freebase entity or relation), yielding an upperbound of 85.2%.
",3.2 Results,[0],[0]
"Finally, we also found that 1.8% of example questions were noisy.",3.2 Results,[0],[0]
"For example, “Which book is written about?” does not reference the corresponding ground truth subject 01n7q (california).",3.2 Results,[0],[0]
"We also consider these examples unanswerable, yielding a final upperbound of 83.4%.",3.2 Results,[0],[0]
"Our second main contribution is a baseline that sets a new state-of-the-art performance level, despite using standard methods.",4 Baseline Model,[0],[0]
"Our approach includes (1) a CRF tagger to determine the subject alias, and (2) a BiLSTM to classify the relation.",4 Baseline Model,[0],[0]
Given a question q (e.g. “who wrote gulliver’s travels?”),4.1 Approach,[0],[0]
"our model must predict the corresponding subject-relation pair (s, r).",4.1 Approach,[0],[0]
"We predict (s, r) with a pipeline that first runs top-k subject recognition and then relation classification.
",4.1 Approach,[0],[0]
We make use of two learned distributions.,4.1 Approach,[0],[0]
"The subject recognition model P (a|q) ranges over text spans A within the question q, in our example A includes the correct subject “gulliver’s travels”.",4.1 Approach,[0],[0]
"This distribution is modeled with a CRF, as defined in more detail below.",4.1 Approach,[0],[0]
"The relation classification model P (r|q, a) will be used to select a Freebase relation r that matches q.",4.1 Approach,[0],[0]
"The distribution ranges over all relations in Freebase that co-occur with a subject that is named a. It is modeled with an LSTM, that encodes q, again as defined in more detail below.
",4.1 Approach,[0],[0]
"Given these distributions, we predict the final subject-relation pair (s, r) as follows.",4.1 Approach,[0],[0]
"First, we determine the most likely subject alias a according to P (a|q) that also matches a subject alias in the KG.",4.1 Approach,[0],[0]
"We define set S as all Freebase entities named a, in our example S is the subject column of Table
1.",4.1 Approach,[0],[0]
"Second, we define all potential relations R such that ∀(s, r) ∈",4.1 Approach,[0],[0]
KG{r ∈ R ∧ s ∈ S}.,4.1 Approach,[0],[0]
"Using the relation classification model p(r|q, a), we predict the most likely relation rmax ∈ R.
Now, the answer candidates are subject-relation pairs such that (s, rmax) ∈",4.1 Approach,[0],[0]
KG{r ∈ R ∧ s ∈ S}.,4.1 Approach,[0],[0]
"In our example question, if rmax is film/film/story by then S includes both subjects 06znpjr",4.1 Approach,[0],[0]
"(Gullivers Travels, American film) and 02py9bj (Gullivers Travels, French film).",4.1 Approach,[0],[0]
"Because there is no explicit linguistic signal to disambiguate this choice, we pick the subject that cooccurs most often with rmax in Freebase.",4.1 Approach,[0],[0]
"Our approach requires two models, in this section we cover training and configuring these models.
",4.2 Model Details,[0],[0]
Top-K Subject Recognition,4.2 Model Details,[0],[0]
We model top-k subject recognition P (a|q) using a linear-chain conditional random field tagger (CRF) with a conditional log likelihood loss objective.,4.2 Model Details,[0],[0]
"k candidates are inferred with the top-k Viterbi algorithm.
",4.2 Model Details,[0],[0]
Our model is trained on a dataset of questions each with their corresponding subject alias span delimited with IO tagging.,4.2 Model Details,[0],[0]
"The gold standard subject alias spans are determined by heuristically matching a phrase in the question with a Freebase alias for the subject.
",4.2 Model Details,[0],[0]
All hyperparameters are hand tuned and then a limited set are further tuned with grid search to increase validation accuracy.,4.2 Model Details,[0],[0]
In total we evaluated at most 100 hyperparameter configurations.,4.2 Model Details,[0],[0]
"The word embeddings are initialized with GloVe (Pennington et al., 2014) and frozen.",4.2 Model Details,[0],[0]
"Adam (Kingma and Ba, 2014), initialized with a learning rate of 0.001, is employed to optimize the model weights.",4.2 Model Details,[0],[0]
"Finally, we halve the learning rate if the validation accuracy has not improved in 3 epochs.
",4.2 Model Details,[0],[0]
Relation Classification,4.2 Model Details,[0],[0]
"The relation classification distribution P (r|q, a) is modeled with a one layer BiLSTM batchnorm softmax classifier.",4.2 Model Details,[0],[0]
"The BiLSTM encodes an abstract predicate string (e.g. “who wrote e?”), as described in Section 4.1.",4.2 Model Details,[0],[0]
"The last LSTM output vector is provided as input to an output block consisting of batch normalization, ReLU, and softmax.
",4.2 Model Details,[0],[0]
"All hyperparameters are hand tuned and then a limited set are further tuned with Hyperband (Li et al., 2017) to increase validation accuracy.",4.2 Model Details,[0],[0]
"Hyperband is allowed at most 30 epochs per model
and a total of 1000 epochs.",4.2 Model Details,[0],[0]
In total we evaluated at most 500 hyperparameter configurations.,4.2 Model Details,[0],[0]
"The word embeddings are initialized with FastText (Bojanowski et al., 2017) and frozen.",4.2 Model Details,[0],[0]
"We use the AMSGrad variant of Adam (Reddi et al., 2018), initialized with an learning rate of 0.001.",4.2 Model Details,[0],[0]
"Finally, we double the batch size (Smith et al., 2017) if the validation accuracy has not improved in 3 epochs.",4.2 Model Details,[0],[0]
"Finally, we present our results on the SimpleQuestions test set.
",4.3 Results,[0],[0]
SimpleQuestions Task,4.3 Results,[0],[0]
"Our model achieves 78.1% accuracy on the SimpleQuestions test set, a new state-of-the-art without ensembling or data augmentation (Table 3).",4.3 Results,[0],[0]
"These results suggest that relatively standard architectures work well when carefully tuned, and approach the level set by our upper bound earlier in the paper.",4.3 Results,[0],[0]
"This further confirms the results of Mohammed et al. 2017.
",4.3 Results,[0],[0]
"Further Qualitative Analysis We also analyze the remaining errors, to point toward directions for future work.
",4.3 Results,[0],[0]
"In Section 3, we showed that questions can provide equal evidence for multiple subject-relation
1Türe and Jojic 2017 reported a 86.8% accuracy but we and Mohammed et al. 2017 have not been able to replicate their results.",4.3 Results,[0],[0]
"Wang et al. 2017 scored 77.5% but removed 0.5% of the test examples.
pairs.",4.3 Results,[0],[0]
"To remove this ambiguity, we count any of these options as correct, and our performance jumps to 91.5%.
",4.3 Results,[0],[0]
The remaining 8.5% error comes from a number of sources.,4.3 Results,[0],[0]
"First, we find that 1.9% of examples were incorrect due to noise, as described in Section 3.",4.3 Results,[0],[0]
"To better understand the remaining 6.5% gap, we do an empirical error analysis on a sample of 50 negative examples.
",4.3 Results,[0],[0]
"First we found that for 14 of 50 cases the question provided equal linguistic evidence for both the ground truth options and the predicted subject-relation pair, similar to the dataset ambiguity found in Section 3, suggesting that our upper bound is loose.",4.3 Results,[0],[0]
We note that Section 3 did not cover all possible question-subject-relation pair ambiguities.,4.3 Results,[0],[0]
"The approach relied on exact string matching to discover ambiguity; therefore, missing other paraphrases.",4.3 Results,[0],[0]
"For example, the abstract predicate “what classification is e” had more examples than “what classification is the e” allowing our approach to programmatically define more subject-relation pair ambiguities for the former predicate than the latter.
",4.3 Results,[0],[0]
The remaining 36 of 50 cases were linguistic mistakes by our model.,4.3 Results,[0],[0]
"Among the 36 cases, we identified these error cases:
• Low Shot (16 of 36)",4.3 Results,[0],[0]
"The relation label was seen in the training data less than 10 times.
",4.3 Results,[0],[0]
• Span Identification (14 of 36),4.3 Results,[0],[0]
"The subject span was incorrectly labeled.
",4.3 Results,[0],[0]
• Noise (2 of 36),4.3 Results,[0],[0]
"The question did not make grammatical sense.
",4.3 Results,[0],[0]
"Together, this error analysis shows that the upperbound is loose.",4.3 Results,[0],[0]
There is likely not much more than 4% of performance to be gained with future work on the data.,4.3 Results,[0],[0]
The SimpleQuestions dataset is one of the most commonly used benchmarks for studying singlerelation factoid questions.,5 Conclusions and Future Work,[0],[0]
"In this paper, we presented new evidence to suggest that this benchmark can be nearly solved by standard methods.",5 Conclusions and Future Work,[0],[0]
"These results suggest there is likely not much more than 4% to be gained with future work on the data.
",5 Conclusions and Future Work,[0],[0]
"Finally, other KG (e.g. Freebase) query datasets should consider providing a set of correct subjectrelation pairs when there is ambiguity in the linguistic input.",5 Conclusions and Future Work,[0],[0]
The SimpleQuestions dataset is one of the most commonly used benchmarks for studying single-relation factoid questions.,abstractText,[0],[0]
"In this paper, we present new evidence that this benchmark can be nearly solved by standard methods.",abstractText,[0],[0]
"First, we show that ambiguity in the data bounds performance at 83.4%; many questions have more than one equally plausible interpretation.",abstractText,[0],[0]
"Second, we introduce a baseline that sets a new state-of-the-art performance level at 78.1% accuracy, despite using standard methods.",abstractText,[0],[0]
"Finally, we report an empirical analysis showing that the upperbound is loose; roughly a quarter of the remaining errors are also not resolvable from the linguistic signal.",abstractText,[0],[0]
"Together, these results suggest that the SimpleQuestions dataset is nearly solved.",abstractText,[0],[0]
SimpleQuestions Nearly Solved: A New Upperbound and Baseline Approach,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4273–4283 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
4273",text,[0],[0]
"Neural machine translation (NMT), typically with an attention-based encoder-decoder framework (Bahdanau et al., 2015), has recently become the dominant approach to machine translation and already been deployed for online translation services (Wu et al., 2016).",1 Introduction,[0],[0]
"Recurrent neural networks (RNN), e.g., LSTMs (Hochreiter and Schmidhuber, 1997) or GRUs (Chung et al., 2014), are widely used as the encoder and decoder for NMT.",1 Introduction,[0],[0]
"In order to alleviate the gradient
∗Corresponding author.
vanishing issue found in simple recurrent neural networks (SRNN) (Elman, 1990), recurrent units in LSTMs or GRUs normally introduce different gates to create shotcuts for gradient information to pass through.
",1 Introduction,[0],[0]
"Notwithstanding the capability of these gated recurrent networks in learning long-distance dependencies, they use remarkably more matrix transformations (i.e., more parameters) than SRNN.",1 Introduction,[0],[0]
"And with many non-linear functions modeling inputs, hidden states and outputs, they are also less transparent than SRNN.",1 Introduction,[0],[0]
"These make NMT which is based on these gated RNNs suffer from not only inefficiency in training and inference due to recurrency and heavy computation in recurrent units (Vaswani et al., 2017) but also difficulty in producing interpretable models (Lee et al., 2017).",1 Introduction,[0],[0]
"These also hinder the deployment of NMT models particularly on memory- and computationlimited devices.
",1 Introduction,[0],[0]
"In this paper, our key interest is to simplify recurrent units in RNN-based NMT.",1 Introduction,[0],[0]
"In doing so, we want to investigate how further we can advance RNN-based NMT in terms of the number of parameters (i.e., memory consumption), running speed and interpretability.",1 Introduction,[0],[0]
This simplification shall preserve the capability of modeling longdistance dependencies in LSTMs/GRUs and the expressive power of recurrent non-linearities in SRNN.,1 Introduction,[0],[0]
"The simplification shall also reduce computation load and physical memory consumption in recurrent units on the one hand and allow us to take a good look into the inner workings of RNNs on the other hand.
",1 Introduction,[0],[0]
"In order to achieve this goal, we propose an addition-subtraction twin-gated recurrent network (ATR) for NMT.",1 Introduction,[0],[0]
"In the recurrent units of ATR, we only keep the very essential weight matrices: one over the input and the other over the history (similar to SRNN).",1 Introduction,[0],[0]
"Comparing with previous
RNN variants (e.g., LSTM or GRU), we have the smallest number of weight matrices.",1 Introduction,[0],[0]
This will reduce the computation load of matrix multiplication.,1 Introduction,[0],[0]
ATR also uses gates to bypass the vanishing gradient problem so as to capture long-range dependencies.,1 Introduction,[0],[0]
"Specifically, we use the addition and subtraction operations between the weighted history and input to estimate an input and forget gate respectively.",1 Introduction,[0],[0]
"These add-sub operations not only distinguish the two gates so that we do not need to have different weight matrices for them, but also make the two gates dynamically correlate to each other.",1 Introduction,[0],[0]
"Finally, we remove some non-linearities in recurrent units.
",1 Introduction,[0],[0]
"Due to these simplifications, we can easily show that each new state in ATR is an unnormalized weighted sum of previous inputs, similar to recurrent additive networks (Lee et al., 2017).",1 Introduction,[0],[0]
This property not only allows us to trace each state back to those inputs which contribute more but also establishes unnormalized forward self-attention between the current state and all its previous inputs.,1 Introduction,[0],[0]
"The self-attention mechanism has already proved very useful in non-recurrent NMT (Vaswani et al., 2017).
",1 Introduction,[0],[0]
We build our NMT systems on the proposed ATR with a single-layer encoder and decoder.,1 Introduction,[0],[0]
Experiments on WMT14 English-German and English-French translation tasks show that our model yields competitive results compared with GRU/LSTM-based NMT.,1 Introduction,[0],[0]
"When we integrate an orthogonal context-aware encoder (still single layer) into ATR-based NMT, our model (yielding 24.97 and 39.06 BLEU on English-German and English-French translation respectively) is even comparable to deep RNN and non-RNN NMT models which are all with multiple encoder/decoder layers.",1 Introduction,[0],[0]
"In-depth analyses demonstrate that ATR is more efficient than LSTM/GRU in terms of NMT training and decoding speed.
",1 Introduction,[0],[0]
"We adapt our model to other language translation and natural language processing tasks, including NIST Chinese-English translation, natural language inference and Chinese word segmentation.",1 Introduction,[0],[0]
Our conclusions still hold on all these tasks.,1 Introduction,[0],[0]
"The most widely used RNN models are LSTM (Hochreiter and Schmidhuber, 1997) and GRU (Chung et al., 2014), both of which are good at handling gradient vanishing problem, a
notorious bottleneck of the simple RNN (Elman, 1990).",2 Related Work,[0],[0]
"The design of gates in our model follows the gate philosophy in LSTM/GRU.
",2 Related Work,[0],[0]
Our work is closely related to the recurrent additive network (RAN) proposed by Lee et al. (2017).,2 Related Work,[0],[0]
"They empirically demonstrate that many non-linearities commonly used in RNN transition dynamics can be removed, and that recurrent hidden states computed as purely the weighted sum of input vectors can be quite efficient in language modeling.",2 Related Work,[0],[0]
Our work follows the same spirit of simplifying recurrent units as they do.,2 Related Work,[0],[0]
But our proposed ATR is significantly different from RAN in three aspects.,2 Related Work,[0],[0]
"First, ATR is simpler than RAN with even fewer parameters.",2 Related Work,[0],[0]
There are only two weight matrices in ATR while four different weight matrices in the simplest version of RAN (two for each gate in RAN).,2 Related Work,[0],[0]
"Second, since the only difference between the input and forget gate in ATR is the addition/subtraction operation between the history and input, the two gates can be learned to be highly correlated as shown in our analysis.",2 Related Work,[0],[0]
"Finally, although RAN is verified effective in language modeling, our experiments show that ATR is better than RAN in machine translation in terms of both speed and translation quality.
",2 Related Work,[0],[0]
"To speed up RNN models, a line of work has attempted to remove recurrent connections.",2 Related Work,[0],[0]
"For example, Bradbury et al. (2016) propose the quasirecurrent neural network (QRNN) which uses convolutional layers and a minimalist recurrent pooling function to improve parallelism.",2 Related Work,[0],[0]
"Very recently, Lei and Zhang (2017) propose a simple recurrent unit (SRU).",2 Related Work,[0],[0]
"With the cuDNN optimization, their RNN model can be trained as fast as CNNs.",2 Related Work,[0],[0]
"However, to obtain promising results, QRNN and SRU have to use deep architectures.",2 Related Work,[0],[0]
"In practice, 4-layer QRNN encoder and decoder are used to gain translation quality that is comparable to that of singlelayer LSTM/GRU NMT.",2 Related Work,[0],[0]
"In particular, our onelayer model achieves significantly higher performance than a 10-layer SRU system.
",2 Related Work,[0],[0]
"Finally, our work is also related to the efforts in developing alternative architectures for NMT models.",2 Related Work,[0],[0]
Zhou et al. (2016) introduce fast-forward connections between adjacent stacked RNN layers to ease gradient propagation.,2 Related Work,[0],[0]
Wang et al. (2017a) propose a linear associate unit to reduce the gradient propagation length along layers in deep NMT.,2 Related Work,[0],[0]
"Gehring et al. (2017b) and Vaswani et al. (2017) explore purely convolutional and attentional archi-
×",2 Related Work,[0],[0]
"+
× × tanh
σ tanhσ",2 Related Work,[0],[0]
"σ
ct
htht−1
ct−1
xt
×
tanhσ
ht
xt
ht−1
σ
×",2 Related Work,[0],[0]
"1-
× +
ht
xt
ht−1",2 Related Work,[0],[0]
"× +
σ×σ+
Concatenate CopyNeural Network Layer Pointwise Operation Vector Transfer
(a) LSTM
× + ×",2 Related Work,[0],[0]
"× tanh σ tanhσ σ
ct
htht−1
ct−1
xt
× tanhσ
ht
xt
ht−1
σ
×",2 Related Work,[0],[0]
"1-
× +
ht
xt
ht−1 × +
σ-
×σ+
Concatenate CopyNeural Network Layer Pointwise Operation Vector Transfer
(b) GRU
× + ×",2 Related Work,[0],[0]
"× tanh σ tanhσ σ
ct htht−1 ct−1 xt
× tanhσ
ht
xt
ht−1
σ
×",2 Related Work,[0],[0]
1-,2 Related Work,[0],[0]
×,2 Related Work,[0],[0]
"+
ht
xt
ht−1 × +
σ×σ+
Concatenate CopyNeural Network Layer Pointwise Operation Vector Transfer
(c) ATR
Figure 1: Architecture for LSTM, GRU and ATR.",2 Related Work,[0],[0]
c∗ indicates the memory cell specific to the LSTM network.,2 Related Work,[0],[0]
"x∗ and h∗ denote the input and output hidden states respectively.
tectures as alternatives to RNNs for neural translation.",2 Related Work,[0],[0]
"With careful configurations, their deep models achieve state-of-the-art performance on various datasets.",2 Related Work,[0],[0]
"Given a sequence x = {x1,x2,. . .",3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
",xT }, RNN updates the hidden state ht recurrently as follows:
ht = φ(ht−1,xt) (1)
where ht−1 is the previous hidden state, which is considered to store information from all previous inputs, and xt is the current input.",3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
"The function φ(·) is a non-linear recurrent function, abstracting away from details in recurrent units.
",3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
GRU can be considered as a simplified version of LSTM.,3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
"In this paper, theoretically, we use GRU as our benchmark and propose a new recurrent unit to further simplify it.",3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
"The GRU function is defined as follows (see Figure 1b):
zt = σ(Wzxt +Uzht−1) (2)
rt = σ(Wrxt +Urht−1) (3)
h̃t = tanh(Whxt +Uh(rt ht−1))",3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
"(4) ht = zt ht−1 + (1− zt) h̃t (5)
where denotes an element-wise multiplication.",3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
The reset gate rt and update gate zt enable manageable information flow from the history and the current input to the new state respectively.,3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
"Despite the success of these two gates in handling gradient flow, they consume extensive matrix transformations and weight parameters.
",3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
We argue that many of these matrix transformations are not essential.,3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
"We therefore propose an addition-subtraction twin-gated recurrent unit
(ATR), formulated as follows (see Figure 1c):
pt = Whht−1, qt = Wxxt (6)
it = σ(pt + qt) (7)
ft = σ(pt − qt) (8) ht = it qt + ft ht−1 (9)
",3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
The hidden state ht in ATR is a weighted mixture of both the current input qt and the history ht−1 controlled by an input gate it and a forget gate ft respectively.,3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
"Notice that we use the transformed representation qt for the current input rather than the raw vector xt due to the potential mismatch in dimensions between ht and xt.
Similar to GRU, we use gates, especially the forget gate, to control the back-propagated gradient flow to make sure gradients will neither vanish nor explode.",3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
"We also preserve the non-linearities of SRNN in ATR but only in the two gates.
",3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
There are three significant differences of ATR from GRU.,3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
Some of these differences are due to the simplifications introduced in ATR.,3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
"First, we squeeze the number of weight matrices in gate calculation from four to two (see Equation (2&3) and (7&8)).",3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
"In all existing gated RNNs, the inputs to gates are weighted sum of the previous hidden state and input.",3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
"In order to distinguish these gates, the weight matrices over the previous hidden state and the current input should be different for different gates.",3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
The number of different weight matrices in gates is therefore 2|#gates| in previous gated RNNs.,3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
"Different from them, ATR introduces different operations (i.e., addition and subtraction) between the weighted history and input to distinguish the input and forget gate.",3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
"Therefore, the weight matrices over the previous state/input in the two gates can be the same in ATR.",3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
"Second, we keep the very essential non-linearities, only in the two gates.",3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
"In ATR, the role of qt is similar to that of h̃t in GRU (see Equation (4)).",3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
"However, we completely wipe out the recurrent non-linearity
1
of h̃t in qt (i.e., qt = Wxxt).",3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
Lee et al. (2017) show that this non-linearity is not necessary in language modeling.,3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
We further empirically demonstrate that it is neither essential in machine translation.,3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
"Third, in GRU the gates for h̃t and ht−1 are coupled and normalized to 1 while we do not explicitly associate the two gates in ATR.",3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
"Instead, they can be learned to be correlated in an implicit way, as shown in the next subsection and our empirical analyis in Section 5.1.",3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
"Unlike GRU, we use an addition and subtraction operation over the transformed current input qt and history pt to differentiate the two gates in ATR.",3.1 Twin-Gated Mechanism,[0],[0]
"As the two gates have the same weights for their input components with only a single difference in the operation between the input components, they act like twins.",3.1 Twin-Gated Mechanism,[0],[0]
"We term the two gates in ATR as twin gates and the procedure, shown in Equation (7&8), as the twin-gated mechanism.",3.1 Twin-Gated Mechanism,[0],[0]
This mechanism endows our model with the following two advantages: 1) Both addition and subtraction operations are completely linear so that fast computation can be expected; and 2),3.1 Twin-Gated Mechanism,[0],[0]
"No other weight parameters are introduced for gates so that our model is more memory-compact.
",3.1 Twin-Gated Mechanism,[0],[0]
A practical question for the twin-gated mechanism is whether twin gates are really capable of dynamically weighting the input and history information.,3.1 Twin-Gated Mechanism,[0],[0]
"To this end, we plot the surface of onedimensional σ(x + y)",3.1 Twin-Gated Mechanism,[0],[0]
− σ(x − y) in Figure 2.,3.1 Twin-Gated Mechanism,[0],[0]
"It is clear that both gates are highly non-linearly correlated, and that there are regions where σ(x+ y) is equal to, greater or smaller than σ(x − y).",3.1 Twin-Gated Mechanism,[0],[0]
"In other words, by adapting the distribution of input
and forget gates, the twin-gated mechanism has the potential to automatically seek suitable regions in Figure 2 to control its preference between the new and past information.",3.1 Twin-Gated Mechanism,[0],[0]
"We argue that the input and forget gates are negatively correlated after training, and empirically show their actual correlation in Section 5.1.",3.1 Twin-Gated Mechanism,[0],[0]
"Here we provide a systematical comparison of computations in LSTM, GRU, RAN and our ATR with respect to the number of weight matrices and matrix transformations.",3.2 Computation Analysis,[0],[0]
"Notice that all these units are building blocks of RNNs so that the total computational complexity and the minimum number of sequential operations required are unchanged, i.e. O(n · d2) and O(n) respectively where n is the sequence length and d is the dimensionality of hidden states.",3.2 Computation Analysis,[0],[0]
"However, the actual number of matrix transformations in the unit indeed significantly affects the running speed of RNN in practice.
",3.2 Computation Analysis,[0],[0]
We summarize the results in Table 1.,3.2 Computation Analysis,[0],[0]
"LSTM contains three different gates and a cell state, including 4 different neural layers with 8 weight matrices and transformations.",3.2 Computation Analysis,[0],[0]
"GRU simplifies LSTM by removing a gate, but still involves two gates and a candidate hidden state.",3.2 Computation Analysis,[0],[0]
It includes 3 different neural layers with 6 weight matrices and transformations.,3.2 Computation Analysis,[0],[0]
RAN further simplifies GRU by removing the non-linearity in the state transition and therefore contains 4 weight matrices in its simplest version.,3.2 Computation Analysis,[0],[0]
"Although our ATR also has two gates, however, there are only 2 weight matrices and transformations, accounting for only a third and a quarter of those in GRU and LSTM respectively.",3.2 Computation Analysis,[0],[0]
"To the best of our knowledge, ATR has the smallest number of weight transformations in existing gated RNN units.",3.2 Computation Analysis,[0],[0]
We provide a detailed and empirical analysis on the speed in Section 5.2.,3.2 Computation Analysis,[0],[0]
An appealing property of the proposed ATR is its interpretability.,3.3 Interpretability Analysis of Hidden States,[0],[0]
"This can be demonstrated by rolling out Equation (9) as follows:
ht = it qt + ft ht−1
= it Wtxt + t−1∑ k=1
ik (
t−k∏ l=1 fk+l
)",3.3 Interpretability Analysis of Hidden States,[0],[0]
"Wxxk
≈ t∑
k=1
gk Wxxk
(10)
where gk can be considered as an approximate weight assigned to the k-th input.",3.3 Interpretability Analysis of Hidden States,[0],[0]
"Similar to the RAN model (Lee et al., 2017), the hidden state in ATR is a component-wise weighted sum of the inputs.",3.3 Interpretability Analysis of Hidden States,[0],[0]
"This not only enables ATR to build up essential dependencies between preceding inputs and the current hidden state, but also allows us to easily detect which previous words have the promising impacts on the current state.",3.3 Interpretability Analysis of Hidden States,[0],[0]
"This desirable property obviously makes ATR highly interpretable.
",3.3 Interpretability Analysis of Hidden States,[0],[0]
"Additionally, this form of weighted sum is also related to self-attention (Vaswani et al., 2017).",3.3 Interpretability Analysis of Hidden States,[0],[0]
It can be considered as a forward unnormalized selfattention where each hidden state attends to all its previous positions.,3.3 Interpretability Analysis of Hidden States,[0],[0]
"As the self-attention mechanism has proved very useful in NMT (Vaswani et al., 2017), we conjecture that such property of ATR partially contributes to its success in machine translation as shown in our experiments.",3.3 Interpretability Analysis of Hidden States,[0],[0]
We visualize the dependencies captured by Equation (10) in Section 5.3.,3.3 Interpretability Analysis of Hidden States,[0],[0]
We conducted our main experiments on WMT14 English-German and English-French translation tasks.,4.1 Setup,[0],[0]
"Translation quality is measured by casesensitive BLEU-4 metric (Papineni et al., 2002).",4.1 Setup,[0],[0]
"Details about each dataset are as follows:
English-German To compare with previous reported results (Luong et al., 2015b; Jean et al., 2015; Zhou et al., 2016; Wang et al., 2017a), we used the same training data of WMT 2014, which consist of 4.5M sentence pairs.",4.1 Setup,[0],[0]
"We used the newstest2013 as our dev set, and the newstest2014 as our test set.
",4.1 Setup,[0],[0]
English-French We used the WMT 2014 training data.,4.1 Setup,[0],[0]
This corpora contain 12M selected sentence pairs.,4.1 Setup,[0],[0]
"We used the concatenation of newstest2012 and newstest2013 as our dev set, and the newstest2014 as our test set.
",4.1 Setup,[0],[0]
"The used NMT system is an attention-based encoder-decoder system, which employs a bidirectional recurrent network as its encoder and a two-layer hierarchical unidirectional recurrent network as its decoder, companied with an additive attention mechanism (Bahdanau et al., 2015).",4.1 Setup,[0],[0]
We replaced the recurrent unit with our proposed ATR model.,4.1 Setup,[0],[0]
"More details are given in Appendix A.1.
",4.1 Setup,[0],[0]
"We also conducted experiments on ChineseEnglish translation, natural language inference and Chinese word segmentation.",4.1 Setup,[0],[0]
Details and experiment results are provided in Appendix A.2.,4.1 Setup,[0],[0]
We set the maximum length of training instances to 80 words for both English-German and EnglishFrench task.,4.2 Training,[0],[0]
"We used the byte pair encoding compression algorithm (Sennrich et al., 2016) to reduce the vocabulary size as well as to deal with the issue of rich morphology.",4.2 Training,[0],[0]
We set the vocabulary size of both source and target languages to 40K for all translation tasks.,4.2 Training,[0],[0]
"All out-of-vocabulary words were replaced with a token “unk”.
",4.2 Training,[0],[0]
We used 1000 hidden units for both encoder and decoder.,4.2 Training,[0],[0]
All word embeddings had dimensionality 620.,4.2 Training,[0],[0]
We initialized all model parameters randomly according to a uniform distribution ranging from -0.08 to 0.08.,4.2 Training,[0],[0]
"These tunable parameters were then optimized using Adam algorithm (Kingma and Ba, 2015) with the two momentum parameters set to 0.9 and 0.999 respectively.",4.2 Training,[0],[0]
Gradient clipping 5.0 was applied to avoid the gradient explosion problem.,4.2 Training,[0],[0]
We trained all models with a learning rate 5e−4 and batch size 80.,4.2 Training,[0],[0]
We decayed the learning rate with a factor of 0.5 between each training epoch.,4.2 Training,[0],[0]
Translations were generated by a beam search algorithm that was based on loglikelihood scores normalized by sentence length.,4.2 Training,[0],[0]
We used a beam size of 10 in all the experiments.,4.2 Training,[0],[0]
"We also applied dropout for English-German and English-French tasks on the output layer to avoid over-fitting, and the dropout rate was set to 0.2.
",4.2 Training,[0],[0]
"To train deep NMT models, we adopted the GNMT architecture (Wu et al., 2016).",4.2 Training,[0],[0]
"We kept all the above settings, except the dimensionality
of word embedding and hidden state which we set to be 512.",4.2 Training,[0],[0]
The translation results are shown in Table 2.,4.3 Results on English-German Translation,[0],[0]
We also provide results of several existing systems that are trained with comparable experimental settings to ours.,4.3 Results on English-German Translation,[0],[0]
"In particular, our single model yields a detokenized BLEU score of 21.99.",4.3 Results on English-German Translation,[0],[0]
"In order to show that the proposed model can be orthogonal to previous methods that improve LSTM/GRU-based NMT, we integrate a singlelayer context-aware (CA) encoder (Zhang et al., 2017b) into our system.",4.3 Results on English-German Translation,[0],[0]
"The ATR+CA system further reaches 22.7 BLEU, outperforming the winner system (Buck et al., 2014) by a substantial improvement of 2 BLEU points.",4.3 Results on English-German Translation,[0],[0]
"Enhanced with the deep GNMT architecture, the GNMT+ATR system yields a gain of 0.89 BLEU points over the RNNSearch+ATR+CA and 1.6 BLEU points over the RNNSearch + ATR.",4.3 Results on English-German Translation,[0],[0]
"Notice that different from our system which was trained on the parallel corpus alone, the winner system used a huge mono-
lingual text to enhance its language model.
",4.3 Results on English-German Translation,[0],[0]
"Compared with the existing LSTM-based (Luong et al., 2015a) deep NMT system, our shallow/deep model achieves a gain of 2.41/3.26 tokenized BLEU points respectively.",4.3 Results on English-German Translation,[0],[0]
"Under the same training condition, our ATR outperforms RAN by a margin of 0.34 tokenized BLEU points, and achieves competitive results against its GRU/LSTM counterpart.",4.3 Results on English-German Translation,[0],[0]
"This suggests that although our ATR is much simpler than GRU, LSTM and RAN, it still possesses strong modeling capacity.
",4.3 Results on English-German Translation,[0],[0]
"In comparison to several advanced deep NMT models, such as the Google NMT (8 layers, 24.61 tokenized BLEU)",4.3 Results on English-German Translation,[0],[0]
"(Wu et al., 2016) and the LAU-connected NMT (4 layers, 23.80 tokenized BLEU)",4.3 Results on English-German Translation,[0],[0]
"(Wang et al., 2017a), the performance of our shallow model (23.31) is competitive.",4.3 Results on English-German Translation,[0],[0]
"Particularly, when replacing LSTM in the Google NMT with our ATR model, the GNMT+ATR system achieves a BLEU score of 24.16, merely 0.45 BLEU points lower.",4.3 Results on English-German Translation,[0],[0]
"Notice that although all systems use the same training data of WMT14, the
tokenization of these work might be different from ours.",4.3 Results on English-German Translation,[0],[0]
"However, the overall results can indicate the competitive strength of our model.",4.3 Results on English-German Translation,[0],[0]
"In addition, SRU (Lei and Zhang, 2017), a recent proposed efficient recurrent unit, obtains a BLEU score of 20.70 with 10 layers, far more behind ATR’s.
",4.3 Results on English-German Translation,[0],[0]
We further ensemble eight likelihood-trained models with different random initializations for the ATR+CA system.,4.3 Results on English-German Translation,[0],[0]
The variance in the tokenized BLEU scores of these models is 0.07.,4.3 Results on English-German Translation,[0],[0]
"As can be seen from Table 2, the ensemble system achieves a tokenized and detokenized BLEU score of 24.97 and 24.33 respectively, obtaining a gain of 1.66 and 1.63 BLEU points over the single model.",4.3 Results on English-German Translation,[0],[0]
"The final result of the ensemble system, to the best of our knowledge, is a very promising result that can be reached by single-layer NMT systems on WMT14 English-German translation.",4.3 Results on English-German Translation,[0],[0]
"Unlike the above translation task, the WMT14 English-French translation task provides a significant larger dataset.",4.4 Results on English-French Translation,[0],[0]
"The full training data have approximately 36M sentence pairs, from which we only used 12M instances for experiments following previous work (Jean et al., 2015; Gehring et al., 2017a; Luong et al., 2015b; Wang et al., 2017a).",4.4 Results on English-French Translation,[0],[0]
"We show the results in Table 3.
",4.4 Results on English-French Translation,[0],[0]
"Our shallow model achieves a tokenized BLEU score of 36.89 and 37.88 when it is equipped
with the CA encoder, outperforming almost all the listed systems, except the Google NMT (Wu et al., 2016), the ConvS2S (Gehring et al., 2017b) and the Transformer (Vaswani et al., 2017).",4.4 Results on English-French Translation,[0],[0]
"Enhanced with the deep GNMT architecture, the GNMT+ATR system reaches a BLEU score of 38.59, which beats the base model version of the Transformer by a margin of 0.49 BLEU points.",4.4 Results on English-French Translation,[0],[0]
"When we use four ensemble models (the variance in the tokenized BLEU scores of these ensemble models is 0.16), the ATR+CA system obtains another gain of 0.47 BLEU points, reaching a tokenized BLEU score of 39.06, which is comparable with several state-of-the-art systems.",4.4 Results on English-French Translation,[0],[0]
We provide an illustration of the actual relation between the learned input and forget gate in Figure 3.,5.1 Analysis on Twin-Gated Mechanism,[0],[0]
"Clearly, these two gates show strong negative correlation.",5.1 Analysis on Twin-Gated Mechanism,[0],[0]
"When the input gate opens with high values, the forget gate prefer to be close.",5.1 Analysis on Twin-Gated Mechanism,[0],[0]
"Quantitatively, on the whole test set, the Pearson’s r of the input and forget gate is -0.9819, indicating a high correlation.",5.1 Analysis on Twin-Gated Mechanism,[0],[0]
"As mentioned in Section 3.2, ATR has much fewer model parameters and matrix transformations.",5.2 Analysis on Speed and Model Parameters,[0],[0]
"We
provide more details in this section by comparing against the following two NMT systems:
• DeepRNNSearch (GRU): a deep GRUequipped RNNSearch model (Wu et al., 2016) with 5 layers.",5.2 Analysis on Speed and Model Parameters,[0],[0]
"We set the dimension of word embedding and hidden state to 620 and 1000 respectively.
",5.2 Analysis on Speed and Model Parameters,[0],[0]
"• Transformer: a purely attentional translator (Vaswani et al., 2017).",5.2 Analysis on Speed and Model Parameters,[0],[0]
We set the dimension of word embedding and filter size to 512 and 2048 respectively.,5.2 Analysis on Speed and Model Parameters,[0],[0]
"The model was trained with a minibatch size of 256.
",5.2 Analysis on Speed and Model Parameters,[0],[0]
We also compare with the GRU and LSTM-based RNNSearch.,5.2 Analysis on Speed and Model Parameters,[0],[0]
"Without specific mention, all other experimental settings for all these models are the same as for our model.",5.2 Analysis on Speed and Model Parameters,[0],[0]
"We implement all these models using the Theano library, and test the speed on one GeForce GTX TITAN X GPU card.",5.2 Analysis on Speed and Model Parameters,[0],[0]
"We show the results on Table 4.
",5.2 Analysis on Speed and Model Parameters,[0],[0]
"We observe that the Transformer achieves the best training speed, processing 4961 words per second.",5.2 Analysis on Speed and Model Parameters,[0],[0]
This is reasonable since the Transformer can be trained in full parallelization.,5.2 Analysis on Speed and Model Parameters,[0],[0]
"On the contrary, DeepRNNSearch is the slowest system.",5.2 Analysis on Speed and Model Parameters,[0],[0]
"As RNN performs sequentially, stacking more layers of RNNs inevitably reduces the training efficiency.",5.2 Analysis on Speed and Model Parameters,[0],[0]
"However, this situation becomes the reverse when it comes to the decoding procedure.",5.2 Analysis on Speed and Model Parameters,[0],[0]
The Transformer merely generates 44 words per second while DeepRNNSearch reaches 70.,5.2 Analysis on Speed and Model Parameters,[0],[0]
"This is because during decoding, all these beam search-
based systems must generate translation one word after another.",5.2 Analysis on Speed and Model Parameters,[0],[0]
Therefore the parallelization advantage of the Transformer disappears.,5.2 Analysis on Speed and Model Parameters,[0],[0]
"In comparison to DeepRNNSearch, the Transformer spends extra time on performing self-attention over all previous hidden states.
",5.2 Analysis on Speed and Model Parameters,[0],[0]
"Our model with the CA structure, using only 63.1M parameters, processes 3993 words per second during training and generates 186 words per second during decoding, which yields substantial speed improvements over the GRU- and LSTMequipped RNNSearch.",5.2 Analysis on Speed and Model Parameters,[0],[0]
This is due to the light matrix computation in recurrent units of ATR.,5.2 Analysis on Speed and Model Parameters,[0],[0]
Notice that the speed increase of ATR over GRU and LSTM does not reach 3x.,5.2 Analysis on Speed and Model Parameters,[0],[0]
"This is because at each decoding step, there are mainly two types of computation: recurrent unit and softmax layer.",5.2 Analysis on Speed and Model Parameters,[0],[0]
"The latter consumes the most calculation, which, however, is the same for different models (LSTM/GRU/ATR).",5.2 Analysis on Speed and Model Parameters,[0],[0]
"As shown in Section 3.3, a hidden state in our ATR can be formulated as a weighted sum of the previous inputs.",5.3 Analysis on Dependency Modeling,[0],[0]
"In this section, we quantitatively analyze the weights gk in Equation (10) induced from Equation (13).",5.3 Analysis on Dependency Modeling,[0],[0]
"Inspired by Lee et al. (2017), we visualize the captured dependencies of an example in Figure 4 where we connect each word to the corresponding previous word with the highest weight gk.
",5.3 Analysis on Dependency Modeling,[0],[0]
"Obviously, our model can discover strong local dependencies.",5.3 Analysis on Dependency Modeling,[0],[0]
"For example, the token “unglück@@” and “lichen” should be a
single word.",5.3 Analysis on Dependency Modeling,[0],[0]
Our model successfully associates “unglück@@” closely to the generation of “lichen” during decoding.,5.3 Analysis on Dependency Modeling,[0],[0]
"In addition, our model can also detect non-consecutive longdistance dependencies.",5.3 Analysis on Dependency Modeling,[0],[0]
"Particularly, the prediction of “Parteien” relies heavily on the token “unglücklichen”, which actually entails an amod linguistic dependency relationship.",5.3 Analysis on Dependency Modeling,[0],[0]
These captured dependencies make our model more interpretable than LSTM/GRU.,5.3 Analysis on Dependency Modeling,[0],[0]
This paper has presented a twin-gated recurrent network (ATR) to simplify neural machine translation.,6 Conclusion and Future Work,[0],[0]
"There are only two weight matrices and matrix transformations in recurrent units of ATR, making it efficient in physical memory usage and running speed.",6 Conclusion and Future Work,[0],[0]
"To avoid the gradient vanishing problem, ATR introduces a twin-gated mechanism to generate an input gate and forget gate through linear addition and subtraction operation respectively, without introducing any additional parameters.",6 Conclusion and Future Work,[0],[0]
"The simplifications allow ATR to produce interpretable results.
",6 Conclusion and Future Work,[0],[0]
Experiments on English-German and EnglishFrench translation tasks demonstrate the effectiveness of our model.,6 Conclusion and Future Work,[0],[0]
"They also show that ATR can be orthogonal to and applied with methods that improve LSTM/GRU-based NMT, indicated by the promising performance of the ATR+CA system.",6 Conclusion and Future Work,[0],[0]
Further analyses reveal that ATR can be trained more efficiently than GRU.,6 Conclusion and Future Work,[0],[0]
"It is also able to transparently model long-distance dependencies.
",6 Conclusion and Future Work,[0],[0]
We also adapt our ATR to other natural language processing tasks.,6 Conclusion and Future Work,[0],[0]
"Experiments show encouraging performance of our model on ChineseEnglish translation, natural language inference and Chinese word segmentation, demonstrating its generality and applicability on various NLP tasks.
",6 Conclusion and Future Work,[0],[0]
"In the future, we will continue to examine the effectiveness of ATR on different neural models for NMT, such as the hierarchical NMT model (Su et al., 2018b) as well as the generative NMT
model (Su et al., 2018a).",6 Conclusion and Future Work,[0],[0]
"We are also interested in adapting our ATR to summarization, semantic parsing etc.",6 Conclusion and Future Work,[0],[0]
"The authors were supported by National Natural Science Foundation of China (Grants No. 61672440, 61622209 and 61861130364), the Fundamental Research Funds for the Central Universities (Grant No. ZK1024), and Scientific Research Project of National Language Committee of China (Grant No. YB135-49).",Acknowledgments,[0],[0]
Biao Zhang greatly acknowledges the support of the Baidu Scholarship.,Acknowledgments,[0],[0]
We also thank the reviewers for their insightful comments.,Acknowledgments,[0],[0]
"In this paper, we propose an additionsubtraction twin-gated recurrent network (ATR) to simplify neural machine translation.",abstractText,[0],[0]
The recurrent units of ATR are heavily simplified to have the smallest number of weight matrices among units of all existing gated RNNs.,abstractText,[0],[0]
"With the simple addition and subtraction operation, we introduce a twin-gated mechanism to build input and forget gates which are highly correlated.",abstractText,[0],[0]
"Despite this simplification, the essential non-linearities and capability of modeling long-distance dependencies are preserved.",abstractText,[0],[0]
"Additionally, the proposed ATR is more transparent than LSTM/GRU due to the simplification.",abstractText,[0],[0]
"Forward self-attention can be easily established in ATR, which makes the proposed network interpretable.",abstractText,[0],[0]
Experiments on WMT14 translation tasks demonstrate that ATR-based neural machine translation can yield competitive performance on English-German and English-French language pairs in terms of both translation quality and speed.,abstractText,[0],[0]
"Further experiments on NIST Chinese-English translation, natural language inference and Chinese word segmentation verify the generality and applicability of ATR on different natural language processing tasks.",abstractText,[0],[0]
Simplifying Neural Machine Translation with Addition-Subtraction Twin-Gated Recurrent Networks,title,[0],[0]
Several machine learning settings are concerned with performing predictions in a very large discrete label space.,1. Introduction,[0],[0]
"From extreme multi-class classification to language modeling, one commonly used approach to this problem reduces it to a series of choices in a tree-structured model, where the leaves typically correspond to labels.",1. Introduction,[0],[0]
"While this allows for faster prediction, and is in many cases necessary to make the models tractable, the performance of the system can depend significantly on the structure of the tree used, e.g. (Mnih & Hinton, 2009).
",1. Introduction,[0],[0]
"Instead of relying on possibly costly heuristics (Mnih &
1New York University, New York, New York, USA 2Massachussets Institute of Technology, Cambridge, Massachussets, USA.",1. Introduction,[0],[0]
Correspondence to:,1. Introduction,[0],[0]
"Yacine Jernite <jernite@cs.nyu.edu>.
Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"Hinton, 2009), extrinsic hierarchies (Morin & Bengio, 2005) which can badly generalize across different data sets, or purely random trees, we provide an efficient datadependent algorithm for tree construction and training.",1. Introduction,[0],[0]
"Inspired by the LOM tree algorithm (Choromanska & Langford, 2015) for binary trees, we present an objective function which favors high-quality node splits, i.e. balanced and easily separable.",1. Introduction,[0],[0]
"In contrast to previous work, our objective applies to trees of arbitrary width and leads to guarantees on model accuracy.",1. Introduction,[0],[0]
"Furthermore, we show how to successfully optimize it in the setting when the data representation needs to be learned simultaneously with the classification tree.
",1. Introduction,[0],[0]
"Finally, the multi-class classification problem is closely related to that of conditional density estimation (Ram & Gray, 2011; Bishop, 2006) since both need to consider all labels (at least implicitly) during learning and at prediction time.",1. Introduction,[0],[0]
"Both problems present similar difficulties when dealing with very large label spaces, and the techniques that we present in this work can be applied indiscriminately to either.",1. Introduction,[0],[0]
"Indeed, we show how to adapt our algorithm to efficiently solve the conditional density estimation problem of learning a language model which uses a tree structured objective.
",1. Introduction,[0],[0]
"This paper is organized as follows: Section 2 discusses related work, Section 3 outlines the necessary background and defines the flat and tree-structured objectives for multi-class classification and density estimation, Section 4 presents the objective and the optimization algorithm, Section ??",1. Introduction,[0],[0]
"contains theoretical results, Section 5 adapts the algorithm to the problem of language modeling, Section 6 reports empirical results on the Flickr tag prediction dataset and Gutenberg text corpus, and finally Section 7 concludes the paper.",1. Introduction,[0],[0]
Supplementary material contains additional material and proofs of theoretical statements of the paper.,1. Introduction,[0],[0]
We also release the C++ implementation of our algorithm1.,1. Introduction,[0],[0]
The multi-class classification problem has been addressed in the literature in a variety of ways.,2. Related Work,[0],[0]
"Some examples include i) clustering methods (Bengio et al., 2010; Madzarov et al., 2009; Weston et al., 2013) ((Bengio et al., 2010)
1https://github.com/yjernite/fastTextLearnTree
was later improved in (Deng et al., 2011)), ii) sparse output coding (Zhao & Xing, 2013), iii) variants of error correcting output codes (Hsu et al., 2009), iv) variants of iterative least-squares (Agarwal et al., 2014), v) a method based on guess-averse loss functions (Beijbom et al., 2014), and vi) classification trees (Beygelzimer et al., 2009b; Choromanska & Langford, 2015; Daume et al., 2016) (that includes the Conditional Probability Trees (Beygelzimer et al., 2009a) when extended to the classification setting).
",2. Related Work,[0],[0]
"The recently proposed LOM tree algorithm (Choromanska & Langford, 2015) differs significantly from other similar hierarchical approaches, like for example Filter Trees (Beygelzimer et al., 2009b) or random trees (Breiman, 2001), in that it addresses the problem of learning good-quality binary node partitions.",2. Related Work,[0],[0]
"The method results in low-entropy trees and instead of using an inefficient enumerate-and-test approach, see e.g: (Breiman et al., 1984), to find a good partition or expensive brute-force optimization (Agarwal et al., 2013), it searches the space of all possible partitions with SGD (Bottou, 1998).",2. Related Work,[0],[0]
"Another work (Daume et al., 2016) uses a binary tree to map an example to a small subset of candidate labels and makes a final prediction via a more tractable one-against-all classifier, where this subset is identified with the proposed Recall Tree.",2. Related Work,[0],[0]
"A notable approach based on decision trees also include FastXML (Prabhu & Varma, 2014) (and its slower and less accurate at prediction predecessor (Agarwal et al., 2013)).",2. Related Work,[0],[0]
It is based on optimizing the rank-sensitive loss function and shows an advantage over some other ranking and NLP-based techniques in the context of multi-label classification.,2. Related Work,[0],[0]
"Other related approaches include the SLEEC classifier (Bhatia et al., 2015) for extreme multi-label classification that learns embeddings which preserve pairwise distances between only the nearest label vectors and ranking approaches based on negative sampling (Weston et al., 2011).",2. Related Work,[0],[0]
"Another tree approach (Kontschieder et al., 2015) shows no computational speed up but leads to significant improvements in prediction accuracy.
",2. Related Work,[0],[0]
Conditional density estimation can also be challenging in settings where the label space is large.,2. Related Work,[0],[0]
The underlying problem here consists in learning a probability distribution over a set of random variables given some context.,2. Related Work,[0],[0]
"For example, in the language modeling setting one can learn the probability of a word given the previous text, either by making a Markov assumption and approximating the left context by the last few words seen (n-grams e.g. (Jelinek & Mercer, 1980; Katz, 1987), feed-forward neural language models (Mnih & Teh, 2012; Mikolov et al., 2011; Schwenk & Gauvain, 2002)), or by attempting to learn a low-dimensional representation of the full history (RNNs (Mikolov et al., 2010; Mirowski & Vlachos, 2015; Tai et al., 2015; Kumar et al., 2015)).",2. Related Work,[0],[0]
"Both the recurrent and feed-forward Neural Probabilistic Language Models (NPLM) (Bengio et al., 2003) simultaneously learn a distributed representation for words and the probability function for word sequences, expressed in terms of these repre-
sentations.",2. Related Work,[0],[0]
"The major drawback of these models is that they can be slow to train, as they grow linearly with the vocabulary size (anywhere between 10,000 and 1M words), which can make them difficult to apply (Mnih & Teh, 2012).",2. Related Work,[0],[0]
A number of methods have been proposed to overcome this difficulty.,2. Related Work,[0],[0]
"Works such as LBL (Mnih & Hinton, 2007) or Word2Vec (Mikolov et al., 2013) reduce the model to its barest bones, with only one hidden layer and no nonlinearities.",2. Related Work,[0],[0]
"Another proposed approach has been to only compute the NPLM probabilities for a reduced vocabulary size, and use hybrid neural-n-gram model (Schwenk & Gauvain, 2005) at prediction time.",2. Related Work,[0],[0]
"Other avenues to reduce the cost of computing gradients for large vocabularies include using different sampling techniques to approximate it (Bengio & Sénécal, 2003; Bengio & Senecal, 2008; Mnih & Teh, 2012), replacing the likelihood objective by a contrastive one (Gutmann & Hyvärinen, 2012) or spherical loss (de Brébisson & Vincent, 2016), relying on self-normalizing models (Andreas & Klein, 2015), taking advantage of data sparsity (Vincent et al., 2015), or using clustering-based methods (Grave et al., 2016).",2. Related Work,[0],[0]
"It should be noted however that most of these techniques (to the exception of (Grave et al., 2016)) do not provide any speed up at test time.
",2. Related Work,[0],[0]
"Similarly to the classification case, there have also been a significant number of works that use tree structured models to accelerate computation of the likelihood and gradients (Morin & Bengio, 2005; Mnih & Hinton, 2009; Djuric et al., 2015; Mikolov et al., 2013).",2. Related Work,[0],[0]
"These use various heuristics to build a hierarchy, from using ontologies (Morin & Bengio, 2005) to Huffman coding (Mikolov et al., 2013).",2. Related Work,[0],[0]
"One algorithm which endeavors to learn a binary tree structure along with the representation is presented in (Mnih & Hinton, 2009).",2. Related Work,[0],[0]
"They iteratively learn word representations given a fixed tree structure, and use a criterion that trades off between making a balanced tree and clustering the words based on their current embedding.",2. Related Work,[0],[0]
"The application we present in the second part of our paper is most closely related to the latter work, and uses a similar embedding of the context.",2. Related Work,[0],[0]
"However, where their setting is limited to binary trees, we work with arbitrary width, and provide a tree building objective which is both less computationally costly and comes with theoretical guarantees.",2. Related Work,[0],[0]
"In this section, we define the classification and loglikelihood objectives we wish to maximize.",3. Background,[0],[0]
"Let X be an input space, and V a label space.",3. Background,[0],[0]
"Let P be a joint distribution over samples in (X ,V), and let fΘ : X → Rdr be a function mapping every input x ∈ X to a representation r ∈ Rdr , and parametrized by Θ (e.g. as a neural network).
",3. Background,[0],[0]
We consider two objectives.,3. Background,[0],[0]
"Let g be a function that takes an input representation r ∈ Rdr , and predicts for it a label g(r) ∈ V .",3. Background,[0],[0]
"The classification objective is defined as the expected proportion of correctly classified examples:
Oclass(Θ, g) = E(x,y)∼P [ 1[g ◦ fΘ(x) = y] ] (1)
Now, let pθ(·|r) define a conditional probability distribution (parametrized by θ) over V for any r ∈ Rdr .",3. Background,[0],[0]
"The density estimation task consists in maximizing the expected log-likelihood of samples from (X ,V):
Oll(Θ, θ) = E(x,y)∼P [ log pθ(y|fΘ(x))",3. Background,[0],[0]
"]
(2)
Tree-Structured Classification and Density Estimation Let us now show how to express the objectives in Equations 1 and 2 when using tree-structured prediction functions (with fixed structure) as illustrated in Figure 1.
Consider a tree T of depth D and arity M with K = |V| leaf nodes and N internal nodes.",3. Background,[0],[0]
"Each leaf l corresponds to a label, and can be identified with the path cl from the root to the leaf.",3. Background,[0],[0]
"In the rest of the paper, we will use the following notations:
cl = ((cl1,1, c l 1,2), . . .",3. Background,[0],[0]
", (c l d,1, c l d,2), . . .",3. Background,[0],[0]
", (c l D,1, c l D,2)), (3)
where cld,1 ∈",3. Background,[0],[0]
"[1, N ] correspond to the node index at depth d, and cld,2 ∈",3. Background,[0],[0]
"[1,M ] indicates which child of cld,1 is next in the path.",3. Background,[0],[0]
"In that case, our classification and density estimation problems are reduced to choosing the right child of a node or defining a probability distribution over children given x ∈ X respectively.
",3. Background,[0],[0]
We then need to replace g and pθ with node decision functions (gn)Nn=1 and conditional probability distributions (pθn) N n=1 respectively.,3. Background,[0],[0]
"Given such a tree and representation function, our objective functions then become:
Oclass(Θ, g) = E(x,y)∼P [ D∏ d=1 1[gcld,1 ◦ fΘ(x) =",3. Background,[0],[0]
"c l d,2] ] (4)
",3. Background,[0],[0]
"Oll(Θ, θ) = E(x,y)∼P",3. Background,[0],[0]
[,3. Background,[0],[0]
"D∑ d=1 log pθ cl d,1 (cld,2|fΘ(x))",3. Background,[0],[0]
"]
(5)
",3. Background,[0],[0]
"The tree objectives defined in Equations 4 and 5 can be optimized in the space of parameters of the representation
and node functions using standard gradient ascent methods.",3. Background,[0],[0]
"However, they also implicitly depend on the tree structure T .",3. Background,[0],[0]
"In the rest of the paper, we provide a surrogate objective function which determines the structure of the tree and, as we show theoretically (Section ??)",3. Background,[0],[0]
", maximizes the criterion in Equation 4 and, as we show empirically (Sections 5 and 6), maximizes the criterion in Equation 5.",3. Background,[0],[0]
"In this section, we introduce a per-node objective Jn which leads to good quality trees when maximized, and provide an algorithm to optimize it.",4. Learning Tree-Structured Objectives,[0],[0]
"We define the node objective Jn for node n as:
Jn = 2
M K∑ i=1 q",4.1. Objective function,[0],[0]
(n),4.1. Objective function,[0],[0]
i M∑ j=1 |p(n)j − p (n) j|i,4.1. Objective function,[0],[0]
"|, (6)
where q(n)i denotes the proportion of nodes reaching node n that are of class i, p(n)j|i is the probability that an example of class i reaching n will be sent to its jth child, and p(n)j is the probability that an example of any class reaching n will be sent to its jth child.",4.1. Objective function,[0],[0]
"Note that we have:
∀j ∈",4.1. Objective function,[0],[0]
"[1,M ], p(n)j = K∑ i=1",4.1. Objective function,[0],[0]
q,4.1. Objective function,[0],[0]
(n),4.1. Objective function,[0],[0]
i p (n) j|i .,4.1. Objective function,[0],[0]
"(7)
The objective in Equation 6 reduces to the LOM tree objective in the case of M = 2.
",4.1. Objective function,[0],[0]
"At a high level, maximizing the objective encourages the conditional distribution for each class to be as different as possible from the global one; so the node decision function needs to be able to discriminate between examples of the different classes.",4.1. Objective function,[0],[0]
The objective thus favors balanced and pure node splits.,4.1. Objective function,[0],[0]
"To wit, we call a split at node n perfectly balanced when the global distribution p(n)· is uniform, and perfectly pure when each p(n)·|i takes value either 0 or 1, as all data points from the same class reaching node n are sent to the same child.
",4.1. Objective function,[0],[0]
In Section ??,4.1. Objective function,[0],[0]
we discuss the theoretical properties of this objective in details.,4.1. Objective function,[0],[0]
We show that maximizing it leads to perfectly balanced and perfectly pure splits.,4.1. Objective function,[0],[0]
"We also derive the boosting theorem that shows the number of internal nodes that the tree needs to have to reduce the classification error below any arbitrary threshold, under the assumption that the objective is “weakly” optimized in each node of the tree.
",4.1. Objective function,[0],[0]
Remark 1.,4.1. Objective function,[0],[0]
"In the rest of the paper, we use node functions gn which take as input a data representation r ∈ Rdr and output a distribution over children of n (for example using a soft-max function).",4.1. Objective function,[0],[0]
"When used in the classification setting, gn sends the data point to the child with the highest predicted probability.",4.1. Objective function,[0],[0]
"With this notation, and representa-
Algorithm 1 Tree Learning Algorithm Input Input representation function: f with parameters
Θf .",4.1. Objective function,[0],[0]
Node decisions functions (gn)Kn=1 with parameters (Θn)Kn=1.,4.1. Objective function,[0],[0]
"Gradient step size .
",4.1. Objective function,[0],[0]
"Ouput Learned M -ary tree, parameters Θf and (Θn)Kn=1.
procedure InitializeNodeStats () for n = 1 to N do
for i = 1 to K do SumProbasn,i ← 0",4.1. Objective function,[0],[0]
"Countsn,i ← 0
procedure NodeCompute (w, n, i, target) p← gn(w) SumProbasn,i ← SumProbasn,i + p Countsn,i ← Countsn,i + 1 //",4.1. Objective function,[0],[0]
"Gradient step in the node parameters Θn ← Θn + ∂ptarget∂Θn return ∂ptarget∂w
InitializeNodeStats () for Each batch b do
// AssignLabels () re-builds the tree based on the // current node statistics AssignLabels ({1, . .",4.1. Objective function,[0],[0]
.,4.1. Objective function,[0],[0]
",K}, root) for each example (x, i) in b do
Compute input representation w = f(x) ∆w← 0 for d = 1 to D do
// ci1,...,D is the current path from the root to i Set node id and target: (n, j)← cid ∆w← ∆w + NodeCompute (w, n, i, j)
//",4.1. Objective function,[0],[0]
Gradient step in the parameters of f Θf ← Θf + ∂f∂Θf,4.1. Objective function,[0],[0]
"∆w
tion function fΘ, we can write:
p (n) j := E(x,y)∼P",4.1. Objective function,[0],[0]
[gn ◦ fΘ(x)],4.1. Objective function,[0],[0]
"(8)
and p
(n) j|i",4.1. Objective function,[0],[0]
":= E(x,y)∼P",4.1. Objective function,[0],[0]
[gn ◦ fΘ(x)|y = i].,4.1. Objective function,[0],[0]
"(9)
An intuitive geometric interpretation of probabilities p(n)j and p(n)j|i can be found in the Supplementary material.",4.1. Objective function,[0],[0]
In this section we present an algorithm for simultaneously building the classification tree and learning the data representation.,4.2. Algorithm,[0],[0]
We aim at maximizing the accuracy of the tree as defined in Equation 4 by maximizing the objective Jn of Equation 6 at each node of the tree (the boosting theorem that will be presented in Section ??,4.2. Algorithm,[0],[0]
"shows the connection between the two).
",4.2. Algorithm,[0],[0]
"Algorithm 2 Label Assignment Algorithm Input labels currently reaching the node
node ID n Ouput Lists of labels now assigned to the node’s children
procedure CheckFull (full, assigned, count, j) if |assignedj | ≡ 2 mod (M − 1) then
count← count− (M − 1) if count = 0 then
full← full ∪ {j} if count = 1 then
count← 0 for j′ s.t.",4.2. Algorithm,[0],[0]
"|assignedj′ | ≡ 1 mod (M − 1) do
full← full ∪ {j′}
procedure AssignLabels (labels, n) //",4.2. Algorithm,[0],[0]
"first, compute p(n)j and p (n) j|i .
",4.2. Algorithm,[0],[0]
"pavg0 ← 0 count← 0 for i in labels do
pavg0 ← p avg 0 + SumProbasn,i count← count +",4.2. Algorithm,[0],[0]
"Countsn,i pavgi ← SumProbasn,i/Countsn,i
pavg0 ← p avg 0 /count //",4.2. Algorithm,[0],[0]
"then, assign each label to a child of n unassigned← labels full← ∅ count← (|unassigned| − (M − 1)) for j = 1 to M do
assignedj ← ∅ while unassigned 6= ∅",4.2. Algorithm,[0],[0]
"do//
∂Jn ∂p (n) j|i is given in Equation 10
(i∗, j∗)← argmax i∈unassigned,j 6∈full ( ∂Jn ∂p (n) j|i )",4.2. Algorithm,[0],[0]
"if n = root then
ci ∗",4.2. Algorithm,[0],[0]
"← (n, j∗)
else ci ∗",4.2. Algorithm,[0],[0]
"← (ci∗ , (n, j∗))",4.2. Algorithm,[0],[0]
assignedj∗ ← assignedj∗ ∪ {i∗} unassigned← unassigned \ {i∗} CheckFull,4.2. Algorithm,[0],[0]
"(full, assigned, count, j∗)
for j = 1 to M do AssignLabels (assignedj , childn,j , d+ 1) return assigned
Let us now show how we can efficiently optimize Jn.",4.2. Algorithm,[0],[0]
"The gradient of Jn with respect to the conditional probability distributions is (see proof of Lemma 1 in the Supplement):
∂Jn
∂p (n) j|i
= 2
M q
(n)",4.2. Algorithm,[0],[0]
i (1− q (n) i ),4.2. Algorithm,[0],[0]
sign(p (n) j|i,4.2. Algorithm,[0],[0]
− p (n) j ).,4.2. Algorithm,[0],[0]
"(10)
Then, according to Equation 10, increasing the likelihood of sending label i to any child j of n such that p(n)j|i > p (n) j increases the objective Jn.",4.2. Algorithm,[0],[0]
"Note that we only need to con-
sider the labels i for which q(n)i > 0, that is, labels i which reach node n in the current tree.
",4.2. Algorithm,[0],[0]
"We also want to make sure that we have a well-formed M - ary tree at each step, which means that the number of labels assigned to any node is always congruent to 1 modulo (M − 1).",4.2. Algorithm,[0],[0]
"Algorithm 2 provides such an assignment by greedily choosing the label-child pair (i, j) such that j still has room for labels with the highest value of ∂Jn
∂p (n) j|i .
",4.2. Algorithm,[0],[0]
"The global procedure, described in Algorithm 1, is then the following.
",4.2. Algorithm,[0],[0]
"• At the start of each batch, re-assign targets for each node prediction function, starting from the root and going down the tree.",4.2. Algorithm,[0],[0]
"At each node, each label is more likely to be re-assigned to the child it has had most affinity with in the past (Algorithm 2).",4.2. Algorithm,[0],[0]
"This can be seen as a form of hierarchical on-line clustering.
",4.2. Algorithm,[0],[0]
•,4.2. Algorithm,[0],[0]
Every example now has a unique path depending on its label.,4.2. Algorithm,[0],[0]
"For each sample, we then take a gradient step at each node along the assigned path (see Algorithm 1).
",4.2. Algorithm,[0],[0]
Lemma 1.,4.2. Algorithm,[0],[0]
"Algorithm 2 finds the assignment of nodes to children for a fixed depth tree which most increases Jn under well-formedness constraints.
",4.2. Algorithm,[0],[0]
Remark 2.,4.2. Algorithm,[0],[0]
"An interesting feature of the algorithm, is that since the representation of examples from different classes are learned together, there is intuitively less of a risk of getting stuck in a specific tree configuration.",4.2. Algorithm,[0],[0]
"More specifically, if two similar classes are initially assigned to different children of a node, the algorithm is less likely to keep this initial decision since the representations for examples of both classes will be pulled together in other nodes.
",4.2. Algorithm,[0],[0]
"Next, we provide a theoretical analysis of the objective introduced in Equation 6.",4.2. Algorithm,[0],[0]
Proofs are deferred to the Supplementary material.,4.2. Algorithm,[0],[0]
"In this section, we first analyze theoretical properties of the objective Jn as regards node quality, then prove a boosting statement for the global tree accuracy.",5. Theoretical Results,[0],[0]
"We start by showing that maximizing Jn in every node of the tree leads to high-quality nodes, i.e. perfectly balanced and perfectly pure node splits.",5.1. Properties of the objective function,[0],[0]
"Let us first introduce some formal definitions.
",5.1. Properties of the objective function,[0],[0]
Definition 1 (Balancedness factor).,5.1. Properties of the objective function,[0],[0]
"The split in node n of the tree is β(n)-balanced if
β(n)",5.1. Properties of the objective function,[0],[0]
"≤ min j={1,2,...,M} p (n) j ,
where β(n) ∈ (0, 1M ] is a balancedness factor.
",5.1. Properties of the objective function,[0],[0]
A split is perfectly balanced if and only if β(n),5.1. Properties of the objective function,[0],[0]
= 1M .,5.1. Properties of the objective function,[0],[0]
Definition 2 (Purity factor).,5.1. Properties of the objective function,[0],[0]
"The split in node n of the tree is α(n)-pure if
1
M M∑ j=1 K∑ i=1",5.1. Properties of the objective function,[0],[0]
q,5.1. Properties of the objective function,[0],[0]
(n),5.1. Properties of the objective function,[0],[0]
"i min ( p (n) j|i , 1− p (n) j|i )",5.1. Properties of the objective function,[0],[0]
"≤ α(n),
where α(n) ∈",5.1. Properties of the objective function,[0],[0]
"[0, 1M ) is a purity factor.
",5.1. Properties of the objective function,[0],[0]
"A split is perfectly pure if and only if α(n) = 0.
",5.1. Properties of the objective function,[0],[0]
The following lemmas characterize the range of the objective Jn and link it to the notions of balancedness and purity of the split.,5.1. Properties of the objective function,[0],[0]
Lemma 2.,5.1. Properties of the objective function,[0],[0]
"The objective function Jn lies in the interval[ 0, 4M ( 1− 1M )] .
",5.1. Properties of the objective function,[0],[0]
"Let J∗ denotes the highest possible value of Jn, i.e. J∗ = 4 M ( 1− 1M ) .",5.1. Properties of the objective function,[0],[0]
Lemma 3.,5.1. Properties of the objective function,[0],[0]
"The objective function Jn admits the highest value, i.e. Jn = J∗, if and only if the split in node n is perfectly balanced, i.e. β(n)",5.1. Properties of the objective function,[0],[0]
"= 1M , and perfectly pure, i.e. α(n) = 0.
",5.1. Properties of the objective function,[0],[0]
We next show Lemmas ??,5.1. Properties of the objective function,[0],[0]
and ??,5.1. Properties of the objective function,[0],[0]
"which analyze balancedness and purity of a node split in isolation, i.e. we analyze resp. balancedness and purity of a node split when resp.",5.1. Properties of the objective function,[0],[0]
purity and balancedness is fixed and perfect.,5.1. Properties of the objective function,[0],[0]
"We show that in such isolated setting increasing Jn leads to a more balanced and more pure split.
",5.1. Properties of the objective function,[0],[0]
Lemma 4.,5.1. Properties of the objective function,[0],[0]
"If a split in node n is perfectly pure, then
β(n) ∈
[ 1 M − √ M(J∗ − Jn) 2 , 1 M ] .
",5.1. Properties of the objective function,[0],[0]
Lemma 5.,5.1. Properties of the objective function,[0],[0]
"If a split in node n is perfectly balanced, then α(n) ≤ (J∗ − Jn)/2.
",5.1. Properties of the objective function,[0],[0]
Next we provide a bound on the classification error for the tree.,5.1. Properties of the objective function,[0],[0]
"In particular, we show that if the objective is “weakly” optimized in each node of the tree, where this weak advantage is captured in a form of the Weak Hypothesis Assumption, then our algorithm will amplify this weak advantage to build a tree achieving any desired level of accuracy.",5.1. Properties of the objective function,[0],[0]
"Denote y(x) to be a fixed target function with domain X , which assigns the data point x to its label, and let P be a fixed target distribution over X .",5.2. Error bound,[0],[0]
"Together y and P induce a distribution on labeled pairs (x, y(x)).",5.2. Error bound,[0],[0]
Let t(x) be the label assigned to data point x by the tree.,5.2. Error bound,[0],[0]
"We denote as (T ) the error of tree T , i.e. (T ) := Ex∼P",5.2. Error bound,[0],[0]
[∑K i=1,5.2. Error bound,[0],[0]
1[t(x),5.2. Error bound,[0],[0]
"= i, y(x) 6=",5.2. Error bound,[0],[0]
"i]
] (1− (T ) refers to the accuracy as given by Equation 4).",5.2. Error bound,[0],[0]
"Then the following theorem holds
Theorem 1.",5.2. Error bound,[0],[0]
"The Weak Hypothesis Assumption says that for any distribution P over the data, at each node n of the tree T there exists a partition",5.2. Error bound,[0],[0]
"such that Jn ≥ γ, where
γ ∈",5.2. Error bound,[0],[0]
"[ M 2 minj=1,2,...,M pj , 1− M2 minj=1,2,...,M pj ] .
",5.2. Error bound,[0],[0]
"Under the Weak Hypothesis Assumption, for any κ ∈",5.2. Error bound,[0],[0]
"[0, 1], to obtain (T ) ≤ κ",5.2. Error bound,[0],[0]
"it suffices to have a tree with
N ≥ ( 1
κ
) 16[M(1−2γ)+2γ](M−1)",5.2. Error bound,[0],[0]
"log2 eM 2γ2 lnK
internal nodes.
",5.2. Error bound,[0],[0]
The above theorem shows the number of splits that suffice to reduce the multi-class classification error of the tree below an arbitrary threshold κ.,5.2. Error bound,[0],[0]
"As shown in the proof of the above theorem, the Weak Hypothesis Assumption implies that all pjs satisfy: pj ∈",5.2. Error bound,[0],[0]
"[ 2γM , M(1−2γ)+2γ M ].",5.2. Error bound,[0],[0]
Below we show a tighter version of this bound when assuming that each node induces balanced split.,5.2. Error bound,[0],[0]
Corollary 1.,5.2. Error bound,[0],[0]
"The Weak Hypothesis Assumption says that for any distribution P over the data, at each node n of the tree T there exists a partition such that Jn ≥ γ, where γ ∈ R+.
",5.2. Error bound,[0],[0]
"Under the Weak Hypothesis Assumption and when all nodes make perfectly balanced splits, for any κ ∈",5.2. Error bound,[0],[0]
"[0, 1], to obtain (T ) ≤ κ",5.2. Error bound,[0],[0]
"it suffices to have a tree with
N ≥ ( 1
κ
) 16(M−1)",5.2. Error bound,[0],[0]
"log2 eM 2γ2 lnK
internal nodes.",5.2. Error bound,[0],[0]
"We now show how to adapt the algorithm presented in Section 4 for conditional density estimation, using the example of language modeling.
",6. Extension to Density Estimation,[0],[0]
Hierarchical Log Bi-Linear Language Model (HLBL),6. Extension to Density Estimation,[0],[0]
"We take the same approach to language modeling as (Mnih & Hinton, 2009).",6. Extension to Density Estimation,[0],[0]
"First, using the chain rule and an order T Markov assumption we model the probability of a sentence w = (w1, w2, . . .",6. Extension to Density Estimation,[0],[0]
", wn) as:
p(w1, w2, . . .",6. Extension to Density Estimation,[0],[0]
", wn) = n∏ t=1 p(wt|wt−T,...,t−1)
Similarly to their work, we also use a low dimensional representation of the context (wt−T,...,t−1).",6. Extension to Density Estimation,[0],[0]
"In this setting, each word w in the vocabulary V has an embedding Uw ∈ Rdr .",6. Extension to Density Estimation,[0],[0]
"A given context x = (wt−T , . . .",6. Extension to Density Estimation,[0],[0]
", wt−1) corresponding to position t is then represented by a context embedding vector rx such that
rx = T∑ k=1 RkUwt−k ,
where U ∈ R|V|×dr is the embedding matrix, and Rk ∈ Rdr×dr is the transition matrix associated with the kth context word.
",6. Extension to Density Estimation,[0],[0]
"The most straight-forward way to define a probability function is then to define the distribution over the next word given the context representation as a soft-max, as done in (Mnih & Hinton, 2007).",6. Extension to Density Estimation,[0],[0]
"That is:
p(wt = i|x) = σi(r>x U + b)
= exp(r>x Ui + bi)∑
w∈V exp(r > x Uw + bw)
,
where bw is the bias for word w.",6. Extension to Density Estimation,[0],[0]
"However, the complexity of computing this probability distribution in this setting is O(|V |×dr), which can be prohibitive for large corpora and vocabularies.
",6. Extension to Density Estimation,[0],[0]
"Instead, (Mnih & Hinton, 2009) takes a hierarchical approach to the problem.",6. Extension to Density Estimation,[0],[0]
"They construct a binary tree, where each word w ∈ V corresponds to some leaf of the tree, and can thus be identified with the path from the root to the corresponding leaf by making a sequence of choices of going left versus right.",6. Extension to Density Estimation,[0],[0]
"This corresponds to the treestructured log-likelihood objective presented in Equation 5 for the case where M = 2, and fΘ(x) = rx.",6. Extension to Density Estimation,[0],[0]
"Thus, if ci is the path to word i as defined in Expression 3, then:
log p(wt = i|x) = D∑ d=1 log σcid,2((r > x U cid,1 + bc i d,1) (11)
",6. Extension to Density Estimation,[0],[0]
"In this binary case, σ is the sigmoid function, and for all non-leaf nodes n ∈",6. Extension to Density Estimation,[0],[0]
"{1, 2, . . .",6. Extension to Density Estimation,[0],[0]
", N}, we have Un ∈ Rdr and bn ∈ Rdr .",6. Extension to Density Estimation,[0],[0]
The cost of computing the likelihood of word w is then reduced to O(log(|V|) × dr).,6. Extension to Density Estimation,[0],[0]
"In their work, the authors start the training procedure by using a random tree, then alternate parameter learning with using a clusteringbased heuristic to rebuild their hierarchy.",6. Extension to Density Estimation,[0],[0]
"We expand upon their method by providing an algorithm which allows for using hierarchies of arbitrary width, and jointly learns the tree structure and the model parameters.
",6. Extension to Density Estimation,[0],[0]
"Using our Algorithm We may use Algorithm 1 as is to learn a good tree structure for classification: that is, a model that often predictswt to be the most likely word after seeing the context (wt−T , . . .",6. Extension to Density Estimation,[0],[0]
", wt−1).",6. Extension to Density Estimation,[0],[0]
"However, while this could certainly learn interesting representations and tree structure, there is no guarantee that such a model would achieve a good average log-likelihood.",6. Extension to Density Estimation,[0],[0]
"Intuitively, there are often several valid possibilities for a word given its immediate left context, which a classification objective does not necessarily take into account.",6. Extension to Density Estimation,[0],[0]
"Yet another option would be to learn a tree structure that maximizes the classification objective, then fine-tune the model parameters using the log-likelihood objective.",6. Extension to Density Estimation,[0],[0]
"We tried this method, but initial tests of this approach did not do much better than the use of random trees.",6. Extension to Density Estimation,[0],[0]
"Instead, we present here a small modification of Algorithm 1 which is equivalent to log-likelihood training when restricted to the fixed tree setting, and can be shown to increase the value of the node objectives Jn: by replacing the gradients with respect to ptarget by those with respect to log ptarget.",6. Extension to Density Estimation,[0],[0]
"Then, for a given tree structure, the algorithm takes a gradient step with respect to the
log-likelihood of the samples:
∂Jn
∂ log p (n) j|i
= 2
M q
(n)",6. Extension to Density Estimation,[0],[0]
i (1−q (n) i ),6. Extension to Density Estimation,[0],[0]
sign(p (n) j|i −p (n) j )p (n) j|i .,6. Extension to Density Estimation,[0],[0]
"(12)
Lemma 1 extends to the new version of the algorithm.",6. Extension to Density Estimation,[0],[0]
We ran experiments to evaluate both the classification and density estimation version of our algorithm.,7. Experiments,[0],[0]
"For classification, we used the YFCC100M dataset (Thomee et al., 2016), which consists of a set of a hundred million Flickr pictures along with captions and tag sets split into 91M training, 930K validation and 543K test examples.",7. Experiments,[0],[0]
We focus here on the problem of predicting a picture’s tags given its caption.,7. Experiments,[0],[0]
"For density estimation, we learned a logbilinear language model on the Gutenberg novels corpus, and compared the perplexity to that obtained with other flat and hierarchical losses.",7. Experiments,[0],[0]
Experimental settings are described in greater detail in the Supplementary material.,7. Experiments,[0],[0]
"We follow the setting of (Joulin et al., 2016) for the YFCC100M tag prediction task: we only keep the tags which appear at least a hundred times, which leaves us with a label space of size 312K. We compare our results to those obtained with the FastText software (Joulin et al., 2016), which uses a binary hierarchical softmax objective based on Huffman coding (Huffman trees are designed to minimize the expected depth of their leaves weighed by frequencies and have been shown to work well with word embedding systems (Mikolov et al., 2013)), and to the Tagspace system (Weston et al., 2014), which uses a sampling-based margin loss (this allows for training in tractable time, but does not help at test time, hence the long times reported).",7.1. Classification,[0],[0]
We also extend the FastText software to use Huffman trees of arbitrary width.,7.1. Classification,[0],[0]
All models use a bagof-word embedding representation of the caption text; the parameters of the input representation function fΘ which we learn are the word embeddings Uw ∈ Rd (as in Section 5) and a caption representation is obtained by summing the embeddings of its words.,7.1. Classification,[0],[0]
We experimented with embeddings of dimension d = 50 and d = 200.,7.1. Classification,[0],[0]
"We predict one tag for each caption, and report the precision as well as the training and test times in Table 1.
",7.1. Classification,[0],[0]
"Our implementation is based on the FastText open source version2, to which we added M -ary Huffman and learned tree objectives.",7.1. Classification,[0],[0]
"Table 1 reports the best accuracy we obtained with a hyper-parameter search using this version on our system so as to provide the most meaningful comparison, even though the accuracy is less than that reported in (Joulin et al., 2016).
",7.1. Classification,[0],[0]
We gain a few different insights from Table 1.,7.1. Classification,[0],[0]
"First, al-
2https://github.com/facebookresearch/fastText
though wider trees are theoretically slower (remember that the theoretical complexity isO(M logM (N)) for anM -ary tree with N labels), they run incomparable time in practice and always perform better.",7.1. Classification,[0],[0]
"Using our algorithm to learn the structure of the tree also always leads to more accurate models, with a gain of up to 3.3 precision points in the smaller 5-ary setting.",7.1. Classification,[0],[0]
"Further, both the importance of having wider trees and learning the structure seems to be less when the node prediction functions become more expressive.",7.1. Classification,[0],[0]
"At a high level, one could imagine that in that setting, the model can learn to use different dimensions of the input representation for different nodes, which would minimize the negative impact of having to learn a representation which is suited to more nodes.
",7.1. Classification,[0],[0]
"Another thing to notice is that since prediction time only depends on the expected depth of a label, our models which learned balanced trees are nearly as fast as Huffman coding which is optimal in that respect (except for the dimension 200, 20-ary tree, but the tree structure had not stabilized yet in that setting).",7.1. Classification,[0],[0]
"Given all of the above remarks, our algorithm especially shines in settings where computational complexity and prediction time are highly constrained at test time, such as mobile devices or embedded systems.",7.1. Classification,[0],[0]
"We also ran language modeling experiments on the Gutenberg novel corpus3, which has about 50M tokens and a vocabulary of 250,000 words.
",7.2. Density Estimation,[0],[0]
"One notable difference from the previous task is that the language modeling setting can drastically benefit from the use of GPU computing, which can make using a flat softmax tractable (if not fast).",7.2. Density Estimation,[0],[0]
"While our algorithm requires
3http://www.gutenberg.org/
more flexibility and thus does not benefit as much from the use of GPUs, a small modification of Algorithm 2 (described in the Supplementary material) allows it to run under a maximum depth constraint and remain competitive.",7.2. Density Estimation,[0],[0]
"The results presented in this section are obtained using this modified version, which learns 65-ary trees of depth 3.
",7.2. Density Estimation,[0],[0]
"Table 2 presents perplexity results for different loss functions, along with the time spent on computing and learning the objective (softmax parameters for the flat version, hierarchical softmax node parameters for the fixed tree, and hierarchical softmax structure and parameters for our algorithm).",7.2. Density Estimation,[0],[0]
"The learned tree model is nearly three and seven times as fast at train and test time respectively as the flat objective without losing any points of perplexity.
",7.2. Density Estimation,[0],[0]
Huffman coding does not apply to trees where all of the leaves are at the same depth.,7.2. Density Estimation,[0],[0]
"Instead, we use the following heuristic as a baseline, inspired by (Mnih & Hinton, 2009): we learn word embeddings using FastText, perform a hierarchical clustering of the vocabulary based on these, then use the resulting tree to learn a new language model.",7.2. Density Estimation,[0],[0]
We call this approach “Clustering Tree”.,7.2. Density Estimation,[0],[0]
"However, for all hyper-parameter settings, this tree structure did worse
than a random one.",7.2. Density Estimation,[0],[0]
We conjecture that its poor performance is because such a tree structure means that the deepest node decisions can be quite difficult.,7.2. Density Estimation,[0],[0]
"Finally, we also ran density estimation experiments on the Penn TreeBank data set, which consists of 1M tokens with a vocabulary size of 10,000, with sensibly similar performance results and a speedup factor of two (see supplementary material).",7.2. Density Estimation,[0],[0]
"It should be noted that running a softmax on a label set of this size (only 10K) fits comfortably on most modern GPUs (hence the comparatively smaller speed gain).
",7.2. Density Estimation,[0],[0]
Figure 2 shows the evolution of the test perplexity for a few epochs.,7.2. Density Estimation,[0],[0]
"It appears that most of the relevant tree structure can be learned in one epoch: from the second epoch on, the learned hierarchical soft-max performs similarly to the flat one.",7.2. Density Estimation,[0],[0]
"Figure 3 shows a part of the tree learned on the Gutenberg dataset, which appears to make semantic and syntactic sense.",7.2. Density Estimation,[0],[0]
"In this paper, we introduced a provably accurate algorithm for jointly learning tree structure and data representation for hierarchical prediction.",8. Conclusion,[0],[0]
"We applied it to a multiclass classification and a density estimation problem, and showed our models’ ability to achieve favorable accuracy in competitive times in both settings.",8. Conclusion,[0],[0]
We consider multi-class classification where the predictor has a hierarchical structure that allows for a very large number of labels both at train and test time.,abstractText,[0],[0]
"The predictive power of such models can heavily depend on the structure of the tree, and although past work showed how to learn the tree structure, it expected that the feature vectors remained static.",abstractText,[0],[0]
We provide a novel algorithm to simultaneously perform representation learning for the input data and learning of the hierarchical predictor.,abstractText,[0],[0]
Our approach optimizes an objective function which favors balanced and easilyseparable multi-way node partitions.,abstractText,[0],[0]
"We theoretically analyze this objective, showing that it gives rise to a boosting style property and a bound on classification error.",abstractText,[0],[0]
We next show how to extend the algorithm to conditional density estimation.,abstractText,[0],[0]
"We empirically validate both variants of the algorithm on text classification and language modeling, respectively, and show that they compare favorably to common baselines in terms of accuracy and running time.",abstractText,[0],[0]
Simultaneous Learning of Trees and Representations for Extreme Classification and Density Estimation,title,[0],[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2173–2182, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics",text,[0],[0]
Verbs are famously both complex and variable.,1 Introduction,[0],[0]
"They express the semantics of an event as well the relational information among participants in that event, and they display a rich range of syntactic and semantic behaviour (Jackendoff, 1972; Gruber, 1976; Levin, 1993).",1 Introduction,[0],[0]
Verbs play a key role at almost every level of linguistic analysis.,1 Introduction,[0],[0]
"Information related to their predicate argument structure can benefit many NLP tasks (e.g. parsing, semantic role labelling, information extraction) and applications (e.g. machine translation,
text mining) as well as research on human language acquisition and processing (Korhonen, 2010).",1 Introduction,[0],[0]
"Precise methods for representing and understanding verb semantics will undoubtedly be necessary for machines to interpret the meaning of sentences with similar accuracy to humans.
",1 Introduction,[0],[0]
"Numerous algorithms for acquiring word representations from text and/or more structured knowledge bases have been developed in recent years (Mikolov et al., 2013; Pennington et al., 2014; Faruqui et al., 2015).",1 Introduction,[0],[0]
"These representations (or embeddings) typically contain powerful features that are applicable to many language applications (Collobert and Weston, 2008; Turian et al., 2010).",1 Introduction,[0],[0]
"Nevertheless, the predominant approaches to distributed representation learning apply a single learning algorithm and representational form for all words in a vocabulary.",1 Introduction,[0],[0]
"This is despite evidence that applying different learning algorithms to word types such as nouns, adjectives and verbs can significantly increase the ultimate usefulness of representations (Schwartz et al., 2015).
",1 Introduction,[0],[0]
One factor behind the lack of more nuanced word representation learning methods is the scarcity of satisfactory ways to evaluate or analyse representations of particular word types.,1 Introduction,[0],[0]
"Resources such as MEN (Bruni et al., 2014), Rare Words (Luong et al., 2013) and SimLex-999 (Hill et al., 2015) focus either on words from a single class or small samples of different word types, with automatic approaches already reaching or surpassing the inter-annotator agreement ceiling.",1 Introduction,[0],[0]
"Consequently, for word classes such as verbs, whose semantics is critical for language understanding, it is practically impossible to achieve statistically robust analyses and comparisons between different
2173
representation learning architectures.",1 Introduction,[0],[0]
"To overcome this barrier to verb semantics research, we introduce SimVerb-3500 – an extensive intrinsic evaluation resource that is unprecedented in both size and coverage.",1 Introduction,[0],[0]
"SimVerb-3500 includes 827 verb types from the University of South Florida Free Association Norms (USF) (Nelson et al., 2004), and at least 3 member verbs from each of the 101 top-level VerbNet classes (Kipper et al., 2008).",1 Introduction,[0],[0]
"This coverage enables researchers to better understand the complex diversity of syntactic-semantic verb behaviours, and provides direct links to other established semantic resources such as WordNet (Miller, 1995) and PropBank (Palmer et al., 2005).",1 Introduction,[0],[0]
"Moreover, the large standardised development and test sets in SimVerb-3500 allow for principled tuning of hyperparameters, a critical aspect of achieving strong performance with the latest representation learning architectures.
",1 Introduction,[0],[0]
"In § 2, we discuss previous evaluation resources targeting verb similarity.",1 Introduction,[0],[0]
"We present the new SimVerb-3500 data set along with our design choices and the pair selection process in § 3, while the annotation process is detailed in § 4.",1 Introduction,[0],[0]
"In § 5 we report the performance of a diverse range of popular representation learning architectures, together with benchmark performance on existing evaluation sets.",1 Introduction,[0],[0]
"In § 6, we show how SimVerb-3500 enables a variety of new linguistic analyses, which were previously impossible due to the lack of coverage and scale in existing resources.",1 Introduction,[0],[0]
A natural way to evaluate representation quality is by judging the similarity of representations assigned to similar words.,2 Related Work,[0],[0]
The most popular evaluation sets at present consist of word pairs with similarity ratings produced by human annotators.1,2 Related Work,[0],[0]
"Nevertheless, we find that all available datasets of this kind are insufficient for judging verb similarity due to their small size or narrow coverage of verbs.
",2 Related Work,[0],[0]
"In particular, a number of word pair evaluation sets are prominent in the distributional semantics
1In some existing evaluation sets pairs are scored for relatedness which has some overlap with similarity.",2 Related Work,[0],[0]
SimVerb-3500 focuses on similarity as this is a more focused semantic relation that seems to yield a higher agreement between human annotators.,2 Related Work,[0],[0]
"For a broader discussion see (Hill et al., 2015).
literature.",2 Related Work,[0],[0]
"Representative examples include RG-65 (Rubenstein and Goodenough, 1965) and WordSim-353 (Finkelstein et al., 2002; Agirre et al., 2009) which are small (65 and 353 word pairs, respectively).",2 Related Work,[0],[0]
"Larger evaluation sets such as the Rare Words evaluation set (Luong et al., 2013) (2034 word pairs) and the evaluations sets from Silberer and Lapata (2014) are dominated by noun pairs and the former also focuses on low-frequency phenomena.",2 Related Work,[0],[0]
"Therefore, these datasets do not provide a representative sample of verbs (Hill et al., 2015).
",2 Related Work,[0],[0]
"Two datasets that do focus on verb pairs to some extent are the data set of Baker et al. (2014) and Simlex-999 (Hill et al., 2015).",2 Related Work,[0],[0]
"These datasets, however, still contain a limited number of verb pairs (134 and 222, respectively), making them unrepresentative of the rich variety of verb semantic phenomena.
",2 Related Work,[0],[0]
In this paper we provide a remedy for this problem by presenting a more comprehensive and representative verb pair evaluation resource.,2 Related Work,[0],[0]
"In this section, we discuss the design principles behind SimVerb-3500.",3 The SimVerb-3500 Data Set,[0],[0]
We first demonstrate that a new evaluation resource for verb similarity is a necessity.,3 The SimVerb-3500 Data Set,[0],[0]
"We then describe how the final verb pairs were selected with the goal to be representative, that is, to guarantee a wide coverage of two standard semantic resources: USF and VerbNet.",3 The SimVerb-3500 Data Set,[0],[0]
"Hill et al. (2015) argue that comprehensive highquality evaluation resources have to satisfy the following three criteria: (C1) Representative (the resource covers the full range of concepts occurring in natural language); (C2) Clearly defined (it clearly defines the annotated relation, e.g., similarity); (C3) Consistent and reliable (untrained native speakers must be able to quantify the target relation consistently relying on simple instructions).
",3.1 Design Motivation,[0],[0]
"Building on the same annotation guidelines as Simlex-999 that explicitly targets similarity, we ensure that criteria C2 and C3 are satisfied.",3.1 Design Motivation,[0],[0]
"However, even SimLex, as the most extensive evaluation resource for verb similarity available at present, is still of limited size, spanning only 222 verb pairs and 170
distinct verb lemmas in total.",3.1 Design Motivation,[0],[0]
"Given that 39 out of the 101 top-level VerbNet classes are not represented at all in SimLex, while 20 classes have only one member verb,2 one may conclude that the criterion C1 is not at all satisfied with current resources.
",3.1 Design Motivation,[0],[0]
There is another fundamental limitation of all current verb similarity evaluation resources: automatic approaches have reached or surpassed the interannotator agreement ceiling.,3.1 Design Motivation,[0],[0]
"For instance, while the average pairwise correlation between annotators on SL-222 is Spearman’s ρ correlation of 0.717, the best performing automatic system reaches ρ = 0.727 (Mrkšić et al., 2016).",3.1 Design Motivation,[0],[0]
"SimVerb-3500 does not inherit this anomaly (see Tab. 2) and demonstrates that there still exists an evident gap between the human and system performance.
",3.1 Design Motivation,[0],[0]
"In order to satisfy C1-C3, the new SimVerb-3500 evaluation set contains similarity ratings for 3,500 verb pairs, containing 827 verb types in total and 3 member verbs for each top-level VerbNet class.",3.1 Design Motivation,[0],[0]
The rating scale goes from 0 (not similar at all) to 10 (synonymous).,3.1 Design Motivation,[0],[0]
We employed the SimLex-999 annotation guidelines.,3.1 Design Motivation,[0],[0]
"In particular, we instructed annotators to give low ratings to antonyms, and to distinguish between similarity and relatedness.",3.1 Design Motivation,[0],[0]
"Pairs that are related but not similar (e.g., to snore / to snooze, to walk / to crawl) thus have a fairly low rating.",3.1 Design Motivation,[0],[0]
Several example pairs are provided in Tab. 1.,3.1 Design Motivation,[0],[0]
"To ensure a wide coverage of a variety of syntacticosemantic phenomena (C1), the choice of verb pairs is steered by two standard semantic resources available online: (1) the USF norms data set3 (Nelson et al., 2004), and (2) the VerbNet verb lexicon4 (Kipper et al., 2004; Kipper et al., 2008).
",3.2 Choice of Verb Pairs and Coverage,[0],[0]
The USF norms data set (further USF) is the largest database of free association collected for English.,3.2 Choice of Verb Pairs and Coverage,[0],[0]
"It was generated by presenting human subjects with one of 5, 000 cue concepts and asking them to write the first word coming to mind that is associated with that concept.",3.2 Choice of Verb Pairs and Coverage,[0],[0]
"Each cue concept c was normed in
2Note that verbs in VerbNet are soft clustered, and one verb type may be associated with more than one class.",3.2 Choice of Verb Pairs and Coverage,[0],[0]
"When computing coverage, we assume that such verbs attribute to counts of all their associated classes.
",3.2 Choice of Verb Pairs and Coverage,[0],[0]
"3http://w3.usf.edu/FreeAssociation/ 4http://verbs.colorado.edu/verb-index/
this way by over 10 participants, resulting in a set of associates a for each cue, for a total of over 72, 000 (c, a) pairs.",3.2 Choice of Verb Pairs and Coverage,[0],[0]
"For each such pair, the proportion of participants who produced associate a when presented with cue c can be used as a proxy for the strength of association between the two words.
",3.2 Choice of Verb Pairs and Coverage,[0],[0]
The norming process guarantees that two words in a pair have a degree of semantic association which correlates well with semantic relatedness and similarity.,3.2 Choice of Verb Pairs and Coverage,[0],[0]
"Sampling from the USF set ensures that both related but non-similar pairs (e.g., to run / to sweat) as well as similar pairs (e.g., to reply / to respond) are represented in the final list of pairs.",3.2 Choice of Verb Pairs and Coverage,[0],[0]
"Further, the rich annotations of the output USF data (e.g., concreteness scores, association strength) can be directly combined with the SimVerb-3500 similarity scores to yield additional analyses and insight.
",3.2 Choice of Verb Pairs and Coverage,[0],[0]
VerbNet (VN) is the largest online verb lexicon currently available for English.,3.2 Choice of Verb Pairs and Coverage,[0],[0]
"It is hierarchical, domain-independent, and broad-coverage.",3.2 Choice of Verb Pairs and Coverage,[0],[0]
VN is organised into verb classes extending the classes from Levin (1993) through further refinement to achieve syntactic and semantic coherence among class members.,3.2 Choice of Verb Pairs and Coverage,[0],[0]
"According to the official VerbNet guidelines,5 “Verb Classes are numbered according to shared semantics and syntax, and classes which share a toplevel number (9-109) have corresponding semantic relationships.”",3.2 Choice of Verb Pairs and Coverage,[0],[0]
"For instance, all verbs from the toplevel Class 9 are labelled “Verbs of Putting”, all verbs from Class 30 are labelled “Verbs of Perception”, while Class 39 contains “Verbs of Ingesting”.
",3.2 Choice of Verb Pairs and Coverage,[0],[0]
"Among others, three basic types of information are covered in VN: (1) verb subcategorization frames (SCFs), which describe the syntactic realization of the predicate-argument structure (e.g. The window broke), (2) selectional preferences (SPs), which capture the semantic preferences verbs have for their
5http://verbs.colorado.edu/verb-index/VerbNet_Guidelines.pdf
arguments (e.g. a breakable physical object broke) and (3) lexical-semantic verb classes (VCs) which provide a shared level of abstraction for verbs similar in their (morpho-)syntactic and semantic properties (e.g. BREAK verbs, sharing the VN class 45.1, and the top-level VN class 45).6",3.2 Choice of Verb Pairs and Coverage,[0],[0]
"The basic overview of the VerbNet structure already suggests that measuring verb similarity is far from trivial as it revolves around a complex interplay between various semantic and syntactic properties.
",3.2 Choice of Verb Pairs and Coverage,[0],[0]
The wide coverage of VN in SimVerb-3500 assures the wide coverage of distinct verb groups/classes and their related linguistic phenomena.,3.2 Choice of Verb Pairs and Coverage,[0],[0]
"Finally, VerbNet enables further connections of SimVerb-3500 to other important lexical resources such as FrameNet (Baker et al., 1998), WordNet (Miller, 1995), and PropBank (Palmer et al., 2005) through the sets of mappings created by the SemLink project initiative (Loper et al., 2007).7
Sampling Procedure We next sketch the complete sampling procedure which resulted in the final set of 3500 distinct verb pairs finally annotated in a crowdsourcing study (§ 4).",3.2 Choice of Verb Pairs and Coverage,[0],[0]
(Step 1),3.2 Choice of Verb Pairs and Coverage,[0],[0]
We extracted all possible verb pairs from USF based on the associated POS tags available as part of USF annotations.,3.2 Choice of Verb Pairs and Coverage,[0],[0]
"To ensure that semantic association between verbs in a pair is not accidental, we then discarded all such USF pairs that had been associated by 2 or less participants in USF.",3.2 Choice of Verb Pairs and Coverage,[0],[0]
(Step 2),3.2 Choice of Verb Pairs and Coverage,[0],[0]
"We then manually cleaned and simplified the list of pairs by removing all pairs with multi-word verbs (e.g., quit / give up), all pairs that contained the non-infinitive form of a verb (e.g., accomplished / finished, hidden / find), removing all pairs containing at least one auxiliary verb (e.g., must / to see, must / to be).",3.2 Choice of Verb Pairs and Coverage,[0],[0]
"The first two steps resulted in 3,072 USF-based verb pairs.",3.2 Choice of Verb Pairs and Coverage,[0],[0]
"(Step 3) After this stage, we noticed that several toplevel VN classes are not part of the extracted set.",3.2 Choice of Verb Pairs and Coverage,[0],[0]
"For instance, 5 VN classes did not have any member verbs included, 22 VN classes had only 1 verb, and 6 VN classes had 2 verbs included in the current set.
",3.2 Choice of Verb Pairs and Coverage,[0],[0]
We resolved the VerbNet coverage issue by sampling from such ’under-represented’ VN classes directly.,3.2 Choice of Verb Pairs and Coverage,[0],[0]
"Note that this step is not related to USF at
6https://verbs.colorado.edu/verb-index/vn/break-45.1.php 7https://verbs.colorado.edu/semlink/
all.",3.2 Choice of Verb Pairs and Coverage,[0],[0]
For each such class we sampled additional verb types until the class was represented by 3 or 4 member verbs (chosen randomly).8,3.2 Choice of Verb Pairs and Coverage,[0],[0]
"Following that, we sampled at least 2 verb pairs for each previously ’under-represented’ VN class by pairing 2 member verbs from each such class.",3.2 Choice of Verb Pairs and Coverage,[0],[0]
"This procedure resulted in 81 additional pairs, now 3,153 in total.",3.2 Choice of Verb Pairs and Coverage,[0],[0]
"(Step 4) Finally, to complement this set with a sample of entirely unassociated pairs, we followed the SimLex-999 setup.",3.2 Choice of Verb Pairs and Coverage,[0],[0]
"We paired up the verbs from the 3,153 associated pairs at random.",3.2 Choice of Verb Pairs and Coverage,[0],[0]
"From these random parings, we excluded those that coincidentally occurred elsewhere in USF (and therefore had a degree of association).",3.2 Choice of Verb Pairs and Coverage,[0],[0]
We sampled the remaining 347 pairs from this resulting set of unassociated pairs.,3.2 Choice of Verb Pairs and Coverage,[0],[0]
(Output),3.2 Choice of Verb Pairs and Coverage,[0],[0]
"The final SimVerb-3500 data set contains 3,500 verb pairs in total, covering all associated verb pairs from USF, and (almost) all top-level VerbNet classes.",3.2 Choice of Verb Pairs and Coverage,[0],[0]
"All pairs were manually checked post-hoc by the authors plus 2 additional native English speakers to verify that the final data set does not contain unknown or invalid verb types.
",3.2 Choice of Verb Pairs and Coverage,[0],[0]
Frequency Statistics,3.2 Choice of Verb Pairs and Coverage,[0],[0]
"The 3,500 pairs consist of 827 distinct verbs.",3.2 Choice of Verb Pairs and Coverage,[0],[0]
"29 top-level VN classes are represented by 3 member verbs, while the three most represented classes cover 79, 85, and 93 member verbs.",3.2 Choice of Verb Pairs and Coverage,[0],[0]
"40 verbs are not members of any VN class.
",3.2 Choice of Verb Pairs and Coverage,[0],[0]
"We performed an initial frequency analysis of SimVerb-3500 relying on the BNC counts available online (Kilgarriff, 1997).9 After ranking all BNC verbs according to their frequency, we divided the list into quartiles: Q1 (most frequent verbs in BNC) - Q4 (least frequent verbs in BNC).",3.2 Choice of Verb Pairs and Coverage,[0],[0]
"Out of the 827 SimVerb-3500 verb types, 677 are contained in Q1, 122 in Q2, 18 in Q3, 4 in Q4 (to enroll, to hitchhike, to implode, to whelp), while 6 verbs are not covered in the BNC list.",3.2 Choice of Verb Pairs and Coverage,[0],[0]
"2,818 verb pairs contain Q1 verbs, while there are 43 verb pairs with both verbs not in Q1.",3.2 Choice of Verb Pairs and Coverage,[0],[0]
"Further empirical analyses are provided in § 6.10
8The following three VN classes are exceptions: (1) Class 56, consisting of words that are dominantly tagged as nouns, but can be used as verbs exceptionally (e.g., holiday, summer, honeymoon); (2) Class 91, consisting of 2 verbs (count, matter); (3) Class 93, consisting of 2 single word verbs (adopt, assume).
",3.2 Choice of Verb Pairs and Coverage,[0],[0]
"9https://www.kilgarriff.co.uk/bnc-readme.html 10Annotations such as VerbNet class membership, relations between WordNet synsets of each verb, and frequency statistics are available as supplementary material.",3.2 Choice of Verb Pairs and Coverage,[0],[0]
"We employ the Prolific Academic (PA) crowdsourcing platform,11 an online marketplace very similar to Amazon Mechanical Turk and to CrowdFlower.",4 Word Pair Scoring,[0],[0]
"Following the SimLex-999 annotation guidelines, we had each of the 3500 verb pairs rated by at least 10 annotators.",4.1 Survey Structure,[0],[0]
"To distribute the workload, we divided the 3500 pairs into 70 tranches, with 79 pairs each.",4.1 Survey Structure,[0],[0]
"Out of the 79 pairs, 50 are unique to one tranche, while 20 manually chosen pairs are in all tranches to ensure consistency.",4.1 Survey Structure,[0],[0]
"The remaining 9 are duplicate pairs displayed to the same participant multiple times to detect inconsistent annotations.
",4.1 Survey Structure,[0],[0]
Participants see 7-8 pairs per page.,4.1 Survey Structure,[0],[0]
Pairs are rated on a scale of 0-6 by moving a slider.,4.1 Survey Structure,[0],[0]
"The first page shows 7 pairs, 5 unique ones and 2 from the consistency set.",4.1 Survey Structure,[0],[0]
The following pages are structured the same but display one extra pair from the previous page.,4.1 Survey Structure,[0],[0]
Participants are explicitly asked to give these duplicate pairs the same rating.,4.1 Survey Structure,[0],[0]
"We use them as quality control so that we can identify and exclude participants giving several inconsistent answers.
",4.1 Survey Structure,[0],[0]
Checkpoint Questions,4.1 Survey Structure,[0],[0]
The survey contains three control questions in which participants are asked to select the most similar pair out of three choices.,4.1 Survey Structure,[0],[0]
"For instance, the first checkpoint is: Which of these pairs of words is the *most* similar?",4.1 Survey Structure,[0],[0]
1. to run / to jog 2.,4.1 Survey Structure,[0],[0]
to run / to walk 3.,4.1 Survey Structure,[0],[0]
to jog / to sweat.,4.1 Survey Structure,[0],[0]
One checkpoint occurs right after the instructions and the other two later in the survey.,4.1 Survey Structure,[0],[0]
The purpose is to check that annotators have understood the guidelines and to have another quality control measure for ensuring that they are paying attention throughout the survey.,4.1 Survey Structure,[0],[0]
"If just one of the checkpoint questions is answered incorrectly, the survey ends immediately and all scores from the annotator in question are discarded.
",4.1 Survey Structure,[0],[0]
"Participants 843 raters participated in the study, producing over 65,000 ratings.",4.1 Survey Structure,[0],[0]
"Unlike other crowdsourcing platforms, PA collects and stores detailed demographic information from the participants upfront.",4.1 Survey Structure,[0],[0]
This information was used to carefully select the pool of eligible participants.,4.1 Survey Structure,[0],[0]
"We restricted the pool to native English speakers with a 90% approval
11https://prolific.ac/ (We chose PA for logistic reasons.)
rate (maximum rate on PA), of age 18-50, born and currently residing in the US (45% out of 843 raters), UK (53%), or Ireland (2%).",4.1 Survey Structure,[0],[0]
"54% of the raters were female and 46% male, with the average age of 30.",4.1 Survey Structure,[0],[0]
Participants took 8 minutes on average to complete the survey containing 79 questions.,4.1 Survey Structure,[0],[0]
"We excluded ratings of annotators who (a) answered one of the checkpoint questions incorrectly (75% of exclusions); (b) did not give equal ratings to duplicate pairs; (c) showed suspicious rating patterns (e.g., randomly alternating between two ratings or using one single rating throughout).",4.2 Post-Processing,[0],[0]
The final acceptance rate was 84%.,4.2 Post-Processing,[0],[0]
We then calculated the average of all ratings from the accepted raters ( ≥ 10 ) for each pair.,4.2 Post-Processing,[0],[0]
"The score was finally scaled linearly from the 0-6 to the 0-10 interval as in (Hill et al., 2015).",4.2 Post-Processing,[0],[0]
Inter-Annotator Agreement We employ two measures.,5 Analysis,[0],[0]
"IAA-1 (pairwise) computes the average pairwise Spearman’s ρ correlation between any two raters – a common choice in previous data collection in distributional semantics (Padó et al., 2007; Reisinger and Mooney, 2010a; Silberer and Lapata, 2014; Hill et al., 2015).
",5 Analysis,[0],[0]
A complementary measure would smooth individual annotator effects.,5 Analysis,[0],[0]
"For this aim, our IAA-2 (mean) measure compares the average correlation of a human rater with the average of all the other raters.",5 Analysis,[0],[0]
"SimVerb-3500 obtains ρ = 0.84 (IAA-1) and ρ = 0.86 (IAA-2), a very good agreement compared to other benchmarks (see Tab. 2).
",5 Analysis,[0],[0]
Vector Space Models We compare the performance of prominent representation models on SimVerb-3500.,5 Analysis,[0],[0]
"We include: (1) unsupervised models that learn from distributional information in text, including the skip-gram negative-sampling model (SGNS) with various contexts (BOW = bag of words; DEPS = dependency contexts) as in Levy and Goldberg (2014), the symmetric-pattern based vectors by Schwartz et al. (2015), and count-based PMIweighted vectors (Baroni et al., 2014); (2) Models that rely on linguistic hand-crafted resources or curated knowledge bases.",5 Analysis,[0],[0]
"Here, we use sparse binary vectors built from linguistic resources (Non-
Distributional, (Faruqui and Dyer, 2015)), and vectors fine-tuned to a paraphrase database (Paragram, (Wieting et al., 2015)) further refined using linguistic constraints (Paragram+CF, (Mrkšić et al., 2016)).",5 Analysis,[0],[0]
"Descriptions of these models are in the supplementary material.
",5 Analysis,[0],[0]
Comparison to SimLex-999 (SL-222) 170 pairs from SL-222 also appear in SimVerb-3500.,5 Analysis,[0],[0]
The correlation between the two data sets calculated on the shared pairs is ρ = 0.91.,5 Analysis,[0],[0]
"This proves, as expected, that the ratings are consistent across the two data sets.
",5 Analysis,[0],[0]
Tab. 3 shows a comparison of models’ performance on SimVerb-3500 against SL-222.,5 Analysis,[0],[0]
"Since the number of evaluation pairs may influence the results, we ideally want to compare sets of equal size for a fair comparison.",5 Analysis,[0],[0]
"Picking one random subset of 222 pairs would bias the results towards the selected pairs, and even using 10-fold cross-validation we found variations up to 0.05 depending on which subsets were used.",5 Analysis,[0],[0]
"Therefore, we employ a 2-level 10-fold crossvalidation where new random subsets are picked in each iteration of each model.",5 Analysis,[0],[0]
The numbers reported as CV-222 are averages of these ten 10-fold crossvalidation runs.,5 Analysis,[0],[0]
"The reported results come very close to the correlation on the full data set for all models.
",5 Analysis,[0],[0]
"Most models perform much better on SL-222, especially those employing additional databases or linguistic resources.",5 Analysis,[0],[0]
The performance of the best scoring Paragram+CF model is even on par with the IAA-1 of 0.72.,5 Analysis,[0],[0]
"The same model obtains the highest score on SV-3500 (ρ = 0.628), with a clear gap to IAA-1 of 0.84.",5 Analysis,[0],[0]
"We attribute these differences in
performance largely to SimVerb-3500 being a more extensive and diverse resource in terms of verb pairs.
",5 Analysis,[0],[0]
Development Set A common problem in scored word pair datasets is the lack of a standard split to development and test sets.,5 Analysis,[0],[0]
"Previous works often optimise models on the entire dataset, which leads to overfitting (Faruqui et al., 2016) or use custom splits, e.g., 10-fold cross-validation (Schwartz et al., 2015), which make results incomparable with others.",5 Analysis,[0],[0]
"The lack of standard splits stems mostly from small size and poor coverage – issues which we have solved with SimVerb-3500.
",5 Analysis,[0],[0]
"Our development set contains 500 pairs, selected to ensure a broad coverage in terms of similarity ranges (i.e., non-similar and highly similar pairs, as well as pairs of medium similarity are represented) and top-level VN classes (each class is represented by at least 1 member verb).",5 Analysis,[0],[0]
"The test set includes the remaining 3,000 verb pairs.",5 Analysis,[0],[0]
The performances of representation learning architectures on the dev and test sets are reported in Tab. 3.,5 Analysis,[0],[0]
"The ranking of models is identical on the test and the full SV-3500 set, with slight differences in ranking on the development set.",5 Analysis,[0],[0]
The large coverage and scale of SimVerb-3500 enables model evaluation based on selected criteria.,6 Evaluating Subsets,[0],[0]
"In this section, we showcase a few example analyses.
",6 Evaluating Subsets,[0],[0]
"Frequency In the first analysis, we select pairs based on their lemma frequency in the BNC corpus and form three groups, with 390-490 pairs in each group (Fig. 1).",6 Evaluating Subsets,[0],[0]
"The results from Fig. 1 suggest that the performance of all models improves as the frequency of the verbs in the pair increases, with much steeper curves for the purely distributional models (e.g., SGNS and SymPat).",6 Evaluating Subsets,[0],[0]
"The non-distributional non data-driven model of Faruqui and Dyer (2015) is only slightly affected by frequency.
",6 Evaluating Subsets,[0],[0]
"WordNet Synsets Intuitively, representations for verbs with more diverse usage patterns are more difficult to learn with statistical models.",6 Evaluating Subsets,[0],[0]
"To examine this hypothesis, we resort to WordNet (Miller, 1995), where different semantic usages of words are listed as so-called synsets.",6 Evaluating Subsets,[0],[0]
"Fig. 2 shows a clear downward trend for all models, confirming that polysemous
verbs are more difficult for current verb representation models.",6 Evaluating Subsets,[0],[0]
"Nevertheless, approaches which use additional information beyond corpus co-occurrence are again more robust.",6 Evaluating Subsets,[0],[0]
"Their performance only drops substantially for verbs with more than 10 synsets, while the performance of other models deteriorates already when tackling verbs with more than 5 synsets.
",6 Evaluating Subsets,[0],[0]
VerbNet Classes Another analysis enabled by SimVerb-3500 is investigating the connection between VerbNet classes and human similarity judgments.,6 Evaluating Subsets,[0],[0]
We find that verbs in the same top-level VerbNet class are often not assigned high similarity score.,6 Evaluating Subsets,[0],[0]
"Out of 1378 pairs where verbs share the top-level VerbNet class, 603 have a score lower than 5.",6 Evaluating Subsets,[0],[0]
Tab. 4 reports scores per VerbNet class.,6 Evaluating Subsets,[0],[0]
"When a verb be-
longs to multiple classes, we count it for each class (see Footnote 2).",6 Evaluating Subsets,[0],[0]
"We run the analysis on the five largest VN classes, each with more than 100 pairs with paired verbs belonging to the same class.
",6 Evaluating Subsets,[0],[0]
"The results indicate clear differences between classes (e.g., Class 31 vs Class 51), and suggest that further developments in verb representation learning should also focus on constructing specialised representations at the finer-grained level of VN classes.
",6 Evaluating Subsets,[0],[0]
"Lexical Relations SimVerb-3500 contains relation annotations (e.g., antonyms, synonyms, hyper/hyponyms, no relation) for all pairs extracted automatically from WordNet.",6 Evaluating Subsets,[0],[0]
"Evaluating per-relation subsets, we observe that some models draw their strength from good performance across different re-
lations.",6 Evaluating Subsets,[0],[0]
"Others have low performance on these pairs, but do very well on synonyms and hyper-/hyponyms.",6 Evaluating Subsets,[0],[0]
"Selected results of this analysis are in Tab. 5.12
Human Agreement Motivated by the varying performance of computational models regarding frequency and ambiguous words with many synsets, we analyse what disagreement effects may be captured in human ratings.",6 Evaluating Subsets,[0],[0]
"We therefore compute the average standard deviation of ratings per subset: avgstdd(S) = 1n ∑ p∈S σ(rp), where S is one subset of pairs, n is the number of pairs in this subset, p is one pair, and rp are all human ratings for this pair.
",6 Evaluating Subsets,[0],[0]
"12 Evaluation based on Spearman’s ρ may be problematic with certain categories, e.g., with antonyms.",6 Evaluating Subsets,[0],[0]
"It evaluates pairs according to their ranking; for antonyms the ranking is arbitrary - every antonym pair should have a very low rating, hence they are not included in Tab. 5.",6 Evaluating Subsets,[0],[0]
"A similar effect occurs with highly ranked synonyms, but to a much lesser degree than with antonyms.
",6 Evaluating Subsets,[0],[0]
"While the standard deviation of ratings is diverse for individual pairs, overall the average standard deviations per subset are almost identical.",6 Evaluating Subsets,[0],[0]
"For both the frequency and the WordNet synset analyses it is around ≈1.3 across all subsets, and with only little difference for the subsets based on VerbNet.",6 Evaluating Subsets,[0],[0]
"The only subsets where we found significant variations is the grouping by relations, where ratings tend to be more similar especially on antonyms (0.86) and pairs with no relation (0.92), much less similar on synonyms (1.34) and all other relations (≈1.4).",6 Evaluating Subsets,[0],[0]
These findings suggest that humans are much less influenced by frequency or polysemy in their understanding of verb semantics compared to computational models.,6 Evaluating Subsets,[0],[0]
"SimVerb-3500 is a verb similarity resource for analysis and evaluation that will be of use to researchers involved in understanding how humans or machines represent the meaning of verbs, and, by extension, scenes, events and full sentences.",7 Conclusions,[0],[0]
"The size and coverage of syntactico-semantic phenomena in SimVerb3500 makes it possible to compare the strengths and weaknesses of various representation models via statistically robust analyses on specific word classes.
",7 Conclusions,[0],[0]
"To demonstrate the utility of SimVerb-3500, we conducted a selection of analyses with existing representation-learning models.",7 Conclusions,[0],[0]
One clear conclusion is that distributional models trained on raw text (e.g. SGNS) perform very poorly on low frequency and highly polysemous verbs.,7 Conclusions,[0],[0]
"This degradation in performance can be partially mitigated by focusing models on more principled distributional contexts, such as those defined by symmetric patterns.",7 Conclusions,[0],[0]
"More generally, the finding suggests that, in order to model the diverse spectrum of verb semantics, we may require algorithms that are better suited to fast learning from few examples (Lake et al., 2011), and have some flexibility with respect to sense-level distinctions (Reisinger and Mooney, 2010b; Vilnis and McCallum, 2015).",7 Conclusions,[0],[0]
"In future work we aim to apply such methods to the task of verb acquisition.
",7 Conclusions,[0],[0]
"Beyond the preliminary conclusions from these initial analyses, the benefit of SimLex-3500 will become clear as researchers use it to probe the relationship between architectures, algorithms and representation quality for a wide range of verb classes.",7 Conclusions,[0],[0]
"Better under-
standing of how to represent the full diversity of verbs should in turn yield improved methods for encoding and interpreting the facts, propositions, relations and events that constitute much of the important information in language.",7 Conclusions,[0],[0]
This work is supported by the ERC Consolidator Grant LEXICAL (648909).,Acknowledgments,[0],[0]
"Verbs play a critical role in the meaning of sentences, but these ubiquitous words have received little attention in recent distributional semantics research.",abstractText,[0],[0]
"We introduce SimVerb-3500, an evaluation resource that provides human ratings for the similarity of 3,500 verb pairs.",abstractText,[0],[0]
"SimVerb-3500 covers all normed verb types from the USF free-association database, providing at least three examples for every VerbNet class.",abstractText,[0],[0]
This broad coverage facilitates detailed analyses of how syntactic and semantic phenomena together influence human understanding of verb meaning.,abstractText,[0],[0]
"Further, with significantly larger development and test sets than existing benchmarks, SimVerb-3500 enables more robust evaluation of representation learning architectures and promotes the development of methods tailored to verbs.",abstractText,[0],[0]
We hope that SimVerb-3500 will enable a richer understanding of the diversity and complexity of verb semantics and guide the development of systems that can effectively represent and interpret this meaning.,abstractText,[0],[0]
SimVerb-3500: A Large-Scale Evaluation Set of Verb Similarity,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2072–2082 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
2072",text,[0],[0]
"An agent executing a sequence of instructions must address multiple challenges, including grounding the language to its observed environment, reasoning about discourse dependencies, and generating actions to complete high-level goals.",1 Introduction,[0],[0]
"For example, consider the environment and instructions in Figure 1, in which a user describes moving chemicals between beakers and mixing chemicals together.",1 Introduction,[0],[0]
"To execute the second instruction, the agent needs to resolve sixth beaker and last one to objects in the environment.",1 Introduction,[0],[0]
"The third instruction requires resolving it to the rightmost beaker mentioned in the second instruction, and reasoning about the set of actions required to mix the colors in the beaker to brown.",1 Introduction,[0],[0]
"In this paper, we describe a model and learning approach to map sequences of instructions to actions.",1 Introduction,[0],[0]
"Our model considers previous utterances and the world state to select actions, learns to combine simple actions to achieve complex goals, and can be trained using
goal states without access to demonstrations.",1 Introduction,[0],[0]
"The majority of work on executing sequences of instructions focuses on mapping instructions to high-level formal representations, which are then evaluated to generate actions (e.g., Chen and Mooney, 2011; Long et al., 2016).",1 Introduction,[0],[0]
"For example, the third instruction in Figure 1 will be mapped to mix(prev_arg1), indicating that the mix action should be applied to first argument of the previous action (Long et al., 2016; Guu et al., 2017).",1 Introduction,[0],[0]
"In contrast, we focus on directly generating the sequence of actions.",1 Introduction,[0],[0]
"This requires resolving references without explicitly modeling them, and learning the sequences of actions required to complete high-level actions; for example, that mixing requires removing everything in the beaker and replacing with the same number of brown items.
",1 Introduction,[0],[0]
A key challenge in executing sequences of instructions is considering contextual cues from both the history of the interaction and the state of the world.,1 Introduction,[0],[0]
"Instructions often refer to previously
mentioned objects (e.g., it in Figure 1) or actions (e.g., do it again).",1 Introduction,[0],[0]
"The world state provides the set of objects the instruction may refer to, and implicitly determines the available actions.",1 Introduction,[0],[0]
"For example, liquid can not be removed from an empty beaker.",1 Introduction,[0],[0]
Both types of contexts continuously change during an interaction.,1 Introduction,[0],[0]
"As new instructions are given, the instruction history expands, and as the agent acts the world state changes.",1 Introduction,[0],[0]
"We propose an attentionbased model that takes as input the current instruction, previous instructions, the initial world state, and the current state.",1 Introduction,[0],[0]
"At each step, the model computes attention encodings of the different inputs, and predicts the next action to execute.
",1 Introduction,[0],[0]
We train the model given instructions paired with start and goal states without access to the correct sequence of actions.,1 Introduction,[0],[0]
"During training, the agent learns from rewards received through exploring the environment with the learned policy by mapping instructions to sequences of actions.",1 Introduction,[0],[0]
"In practice, the agent learns to execute instructions gradually, slowly correctly predicting prefixes of the correct sequences of increasing length as learning progress.",1 Introduction,[0],[0]
A key challenge is learning to correctly select actions that are only required later in execution sequences.,1 Introduction,[0],[0]
"Early during learning, these actions receive negative updates, and the agent learns to assign them low probabilities.",1 Introduction,[0],[0]
"This results in an exploration problem in later stages, where actions that are only required later are not sampled during exploration.",1 Introduction,[0],[0]
"For example, in the ALCHEMY domain shown in Figure 1, the agent behavior early during execution of instructions can be accomplished by only using POP actions.",1 Introduction,[0],[0]
"As a result, the agent quickly learns a strong bias against PUSH actions, which in practice prevents the policy from exploring them again.",1 Introduction,[0],[0]
"We address this with a learning algorithm that observes the reward for all possible actions for each visited state, and maximizes the immediate expected reward.
",1 Introduction,[0],[0]
"We evaluate our approach on SCONE (Long et al., 2016), which includes three domains, and is used to study recovering predicate logic meaning representations for sequential instructions.",1 Introduction,[0],[0]
"We study the problem of generating a sequence of low-level actions, and re-define the set of actions for each domain.",1 Introduction,[0],[0]
"For example, we treat the beakers in the ALCHEMY domain as stacks and use only POP and PUSH actions.",1 Introduction,[0],[0]
"Our approach robustly learns to execute sequential instructions with up to 89.1% task-completion
accuracy for single instruction, and 62.7% for complete sequences.",1 Introduction,[0],[0]
Our code is available at https://github.com/clic-lab/scone.,1 Introduction,[0],[0]
"Task and Notation Let S be the set of all possible world states, X be the set of all natural language instructions, and A be the set of all actions.",2 Technical Overview,[0],[0]
"An instruction x̄ ∈ X of length |x̄| is a sequence of tokens 〈x1, ...x|x̄|〉.",2 Technical Overview,[0],[0]
Executing an action modifies the world state following a transition function T : S ×A → S.,2 Technical Overview,[0],[0]
"For example, the ALCHEMY domain includes seven beakers that contain colored liquids.",2 Technical Overview,[0],[0]
The world state defines the content of each beaker.,2 Technical Overview,[0],[0]
We treat each beaker as a stack.,2 Technical Overview,[0],[0]
"The actions are POP N and PUSH N C, where 1 ≤ N ≤ 7 is the beaker number and C is one of six colors.",2 Technical Overview,[0],[0]
"There are a total of 50 actions, including the STOP action.",2 Technical Overview,[0],[0]
"Section 6 describes the domains in detail.
",2 Technical Overview,[0],[0]
"Given a start state s1 and a sequence of instructions 〈x̄1, . . .",2 Technical Overview,[0],[0]
", x̄n〉, our goal is to generate the sequence of actions specified by the instructions starting from s1.",2 Technical Overview,[0],[0]
We treat the execution of a sequence of instructions as executing each instruction in turn.,2 Technical Overview,[0],[0]
"The execution ē of an instruction x̄i starting at a state s1 and given the history of the instruction sequence 〈x̄1, . . .",2 Technical Overview,[0],[0]
", x̄i−1〉 is a sequence of state-action pairs ē = 〈(s1, a1), ..., (sm, am)〉, where",2 Technical Overview,[0],[0]
"ak ∈ A, sk+1 = T (sk, ak).",2 Technical Overview,[0],[0]
"The final action am is the special action STOP, which indicates the execution has terminated.",2 Technical Overview,[0],[0]
"The final state is then sm, as T (sk, STOP) = sk.",2 Technical Overview,[0],[0]
"Executing a sequence of instructions in order generates a sequence 〈ē1, ..., ēn〉, where ēi is the execution of instruction x̄i.",2 Technical Overview,[0],[0]
"When referring to states and actions in an indexed execution ēi, the k-th state and action are si,k and ai,",2 Technical Overview,[0],[0]
"k. We execute instructions one after the other: ē1 starts at the interaction initial state s1 and si+1,1 = si,|ēi|, where si+1,1 is the start state of ēi+1 and si,|ēi| is the final state of ēi.
",2 Technical Overview,[0],[0]
Model We model the agent with a neural network policy (Section 4).,2 Technical Overview,[0],[0]
"At step k of executing the i-th instruction, the model input is the current instruction x̄i, the previous instructions 〈x̄1, . . .",2 Technical Overview,[0],[0]
", x̄i−1〉, the world state s1 at the beginning of executing x̄i, and the current state sk.",2 Technical Overview,[0],[0]
The model predicts the next action ak to execute.,2 Technical Overview,[0],[0]
"If ak = STOP, we switch to the next instruction, or if at the end of the instruction sequence, terminate.",2 Technical Overview,[0],[0]
"Otherwise, we update the state to sk+1 = T (sk, ak).",2 Technical Overview,[0],[0]
"The model uses attention to
process the different inputs and a recurrent neural network (RNN) decoder to generate actions (Bahdanau et al., 2015).
",2 Technical Overview,[0],[0]
"Learning We assume access to a set of N instruction sequences, where each instruction in each sequence is paired with its start and goal states.",2 Technical Overview,[0],[0]
"During training, we create an example for each instruction.",2 Technical Overview,[0],[0]
"Formally, the training set is {(x̄(j)i , s (j) i,1 , 〈x̄ (j) 1 , . . .",2 Technical Overview,[0],[0]
", x̄ (j) i−1〉, g (j) i )} N,n(j)",2 Technical Overview,[0],[0]
"j=1,i=1, where x̄(j)i is an instruction, s (j) i,1 is a start state, 〈x̄(j)1 , . . .",2 Technical Overview,[0],[0]
", x̄ (j) i−1〉 is the instruction history, g (j) i is the goal state, and n(j) is the length of the j-th instruction sequence.",2 Technical Overview,[0],[0]
This training data contains no evidence about the actions and intermediate states required to execute each instruction.1,2 Technical Overview,[0],[0]
We use a learning method that maximizes the expected immediate reward for a given state (Section 5).,2 Technical Overview,[0],[0]
"The reward accounts for task-completion and distance to the goal via potential-based reward shaping.
",2 Technical Overview,[0],[0]
"Evaluation We evaluate exact task completion for sequences of instructions on a test set {(s(j)1 , 〈x̄ (j) 1 , . . .",2 Technical Overview,[0],[0]
", x̄ (j) nj 〉, g(j))}Nj=1, where g(j) is the oracle goal state of executing instructions x̄
(j) 1 , . . .",2 Technical Overview,[0],[0]
",x̄ (j) nj in order starting from s (j) 1 .",2 Technical Overview,[0],[0]
We also evaluate single-instruction task completion using per-instruction annotated start and goal states.,2 Technical Overview,[0],[0]
"Executing instructions has been studied using the SAIL corpus (MacMahon et al., 2006) with focus on navigation using high-level logical representations (Chen and Mooney, 2011; Chen, 2012; Artzi and Zettlemoyer, 2013; Artzi et al., 2014) and lowlevel actions (Mei et al., 2016).",3 Related Work,[0],[0]
"While SAIL includes sequences of instructions, the data demonstrates limited discourse phenomena, and instructions are often processed in isolation.",3 Related Work,[0],[0]
"Approaches that consider as input the entire sequence focused on segmentation (Andreas and Klein, 2015).",3 Related Work,[0],[0]
"Recently, other navigation tasks were proposed with focus on single instructions (Anderson et al., 2018; Janner et al., 2018).",3 Related Work,[0],[0]
We focus on sequences of environment manipulation instructions and modeling contextual cues from both the changing environment and instruction history.,3 Related Work,[0],[0]
"Manipulation using single-sentence instructions has been stud-
1This training set is a subset of the data used in previous work (Section 6, Guu et al., 2015), in which training uses all instruction sequences of length 1 and 2.
ied using the Blocks domain (Bisk et al., 2016, 2018; Misra et al., 2017; Tan and Bansal, 2018).",3 Related Work,[0],[0]
Our work is related to the work of Branavan et al. (2009) and Vogel and Jurafsky (2010).,3 Related Work,[0],[0]
"While both study executing sequences of instructions, similar to SAIL, the data includes limited discourse dependencies.",3 Related Work,[0],[0]
"In addition, both learn with rewards computed from surface-form similarity between text in the environment and the instruction.",3 Related Work,[0],[0]
"We do not rely on such similarities, but instead use a state distance metric.
",3 Related Work,[0],[0]
"Language understanding in interactive scenarios that include multiple turns has been studied with focus on dialogue for querying database systems using the ATIS corpus (Hemphill et al., 1990; Dahl et al., 1994).",3 Related Work,[0],[0]
Tür et al. (2010) surveys work on ATIS.,3 Related Work,[0],[0]
"Miller et al. (1996), Zettlemoyer and Collins (2009), and Suhr et al. (2018) modeled context dependence in ATIS for generating formal representations.",3 Related Work,[0],[0]
"In contrast, we focus on environments that change during execution and directly generating environment actions, a scenario that is more related to robotic agents than database query.
",3 Related Work,[0],[0]
"The SCONE corpus (Long et al., 2016) was designed to reflect a broad set of discourse context-dependence phenomena.",3 Related Work,[0],[0]
"It was studied extensively using logical meaning representations (Long et al., 2016; Guu et al., 2017; Fried et al., 2018).",3 Related Work,[0],[0]
"In contrast, we are interested in directly generating actions that modify the environment.",3 Related Work,[0],[0]
"This requires generating lower-level actions and learning procedures that are otherwise hardcoded in the logic (e.g., mixing action in Figure 1).",3 Related Work,[0],[0]
"Except for Fried et al. (2018), previous work on SCONE assumes access only to the initial and final states during training.",3 Related Work,[0],[0]
"This form of supervision does not require operating the agent manually to acquire the correct sequence of actions, a difficult task in robotic agents with complex control.",3 Related Work,[0],[0]
"Goal state supervision has been studied for instructional language (e.g., Branavan et al., 2009; Artzi and Zettlemoyer, 2013; Bisk et al., 2016), and more extensively in question answering when learning with answer annotations only (e.g., Clarke et al., 2010; Liang et al., 2011; Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014, 2015; Liang et al., 2017).",3 Related Work,[0],[0]
"We map sequences of instructions 〈x̄1, . . .",4 Model,[0],[0]
", x̄n〉 to actions by executing the instructions in or-
der.",4 Model,[0],[0]
"The model generates an execution ē = 〈(s1, a1), . . .",4 Model,[0],[0]
", (smi , ami)〉 for each instruction x̄i.",4 Model,[0],[0]
"The agent context, the information available to the agent at step k, is s̃k = (x̄i, 〈x̄1, . . .",4 Model,[0],[0]
", x̄i−1〉, sk, ē[: k]), where ē[: k] is the execution up until but not including step k.",4 Model,[0],[0]
"In contrast to the world state, the agent context also includes instructions and the execution so far.",4 Model,[0],[0]
"The agent policy πθ(s̃k, a) is modeled as a probabilistic neural network parametrized by θ, where s̃k is the agent context at step k and a is an action.",4 Model,[0],[0]
"To generate executions, we generate one action at a time, execute the action, and observe the new world state.",4 Model,[0],[0]
"In step k of executing the i-th instruction, the network inputs are the current utterance x̄i, the previous instructions 〈x̄1, . . .",4 Model,[0],[0]
", x̄i−1〉, the initial state s1 at beginning of executing x̄i, and the current state sk.",4 Model,[0],[0]
"When executing a sequence of instructions, the initial state s1 is either the state at the beginning of executing the sequence or the final state of the execution of the previous instruction.",4 Model,[0],[0]
"Figure 2 illustrates our architecture.
",4 Model,[0],[0]
We generate continuous vector representations for all inputs.,4 Model,[0],[0]
"Each input is represented as a set of vectors that are then processed with an attention function to generate a single vector representation (Luong et al., 2015).",4 Model,[0],[0]
"We assume access to a domain-specific encoding function ENC(s) that, given a state s, generates a set of vectors S representing the objects in the state.",4 Model,[0],[0]
"For example, in the ALCHEMY domain, a vector is generated for each beaker using an RNN.",4 Model,[0],[0]
"Section 6 describes the different domains and their encoding functions.
",4 Model,[0],[0]
"We use a single bidirectional RNN with a long short-term memory (LSTM; Hochreiter and Schmidhuber, 1997) recurrence to encode the instructions.",4 Model,[0],[0]
"All instructions x̄1,. . .",4 Model,[0],[0]
",x̄i are encoded
with a single RNN by concatenating them to x̄′.",4 Model,[0],[0]
"We use two delimiter tokens: one separates previous instructions, and the other separates the previous instructions from the current one.",4 Model,[0],[0]
"The forward LSTM RNN hidden states are computed as:2
−−→ hj+1 = −−−−−→ LSTME ( φI(x′j+1); −→ hj ) ,
where φI is a learned word embedding function and −−−−−→ LSTME is the forward LSTM recurrence function.",4 Model,[0],[0]
We use a similar computation to compute the backward hidden states ←− hj .,4 Model,[0],[0]
"For each token x′j in x̄ ′, a vector representation h′j =[−→
hj ; ←− hj ] is computed.",4 Model,[0],[0]
"We then create two sets of vectors, one for all the vectors of the current instruction and one for the previous instructions:
Xc = {h′j} J+|x̄i| j=J Xp = {h′j}j<Jj=0
where J is the index in x̄′ where the current instruction x̄i begins.",4 Model,[0],[0]
"Separating the vectors to two sets will allows computing separate attention on the current instruction and previous ones.
",4 Model,[0],[0]
"To compute each input representation during decoding, we use a bi-linear attention function (Luong et al., 2015).",4 Model,[0],[0]
"Given a set of vectors H , a query vector hq, and a weight matrix W, the attention function ATTEND(H,hq,W) computes a context vector z:
αi ∝ exp(hᵀiWh q) : i = 0, . . .",4 Model,[0],[0]
", |H|
z = |H|∑ i=1 αihi .
",4 Model,[0],[0]
"2To simplify the notation, we omit the memory cell (often denoted as cj) from all LSTM descriptions.",4 Model,[0],[0]
"We use only the hidden state hj to compute the intended representations (e.g., for the input text tokens).",4 Model,[0],[0]
"All LSTMs in this paper use zero vectors as initial hidden state h0 and initial cell memory c0.
",4 Model,[0],[0]
We use a decoder to generate actions.,4 Model,[0],[0]
"At each time step k, we compute an input representation using the attention function, update the decoder state, and compute the next action to execute.",4 Model,[0],[0]
"Attention is first computed over the vectors of the current instruction, which is then used to attend over the other inputs.",4 Model,[0],[0]
"We compute the context vectors zck and z p k for the current instruction and previous instructions:
zck = ATTEND(X c,hdk−1,W c) zpk = ATTEND(X p,",4 Model,[0],[0]
"[hdk−1, z c k],W p) ,
where hdk−1 is the decoder hidden state for step k− 1, and Xc and Xp are the sets of vector representations for the current instruction and previous instructions.",4 Model,[0],[0]
Two attention heads are used over both the initial and current states.,4 Model,[0],[0]
"This allows the model to attend to more than one location in a state at once, for example when transferring items from one beaker to another in ALCHEMY.",4 Model,[0],[0]
"The current state is computed by the transition function sk = T (sk−1, ak−1), where sk−1 and ak−1 are the state and action at step k − 1.",4 Model,[0],[0]
"The context vectors for the initial state s1 and the current state sk are:
zs1,k =",4 Model,[0],[0]
"[ATTEND(ENC(s1), [h d k−1, z c k],W sb,1);
",4 Model,[0],[0]
"ATTEND(ENC(s1),",4 Model,[0],[0]
"[hdk−1, z c k],W sb,2)]
zsk,k =",4 Model,[0],[0]
"[ATTEND(ENC(sk), [h d k−1, z c k],W sc,1);
ATTEND(ENC(sk),",4 Model,[0],[0]
"[hdk−1, z c k],W sc,2)] ,
where all W∗,∗ are learned weight matrices.
",4 Model,[0],[0]
"We concatenate all computed context vectors with an embedding of the previous action ak−1 to create the input for the decoder:
hk = tanh([z c k; z p k; z s 1,k; z s k,k;φ O(ak−1)]W d + bd) hdk = LSTM D ( hk;h d k−1 ) ,
where φO is a learned action embedding function and LSTMD is the LSTM decoder recurrence.
",4 Model,[0],[0]
"Given the decoder state hdk, the next action ak is predicted with a multi-layer perceptron (MLP).",4 Model,[0],[0]
"The actions in our domains decompose to an action type and at most two arguments.3 For example, the action PUSH 1 B in ALCHEMY has the type PUSH and two arguments: a beaker number and a color.",4 Model,[0],[0]
Section 6 describes the actions of each domain.,4 Model,[0],[0]
"The probability of an action is:
3We use a NULL argument for unused arguments.
",4 Model,[0],[0]
"hak = tanh(h d kW a)
sk,aT = h a kbaT sk,a1 = h a kba1 sk,a2 = h a kba2
p(ak = aT (a1, a2) | s̃k; θ) ∝ exp(sk,aT + sk,a1 + sk,a2) ,
where aT , a1, and a2 are an action type, first argument, and second argument.",4 Model,[0],[0]
"If the predicted action is STOP, the execution is complete.",4 Model,[0],[0]
"Otherwise, we execute the action ak to generate the next state sk+1, and update the agent context s̃k to s̃k+1 by appending the pair (sk, ak) to the execution ē and replacing the current state with sk+1.
",4 Model,[0],[0]
"The model parameters θ include: the embedding functions φI and φO; the recurrence parameters for −−−−−→ LSTME , ←−−−−− LSTME , and LSTMD; WC , WP , Wsb,1, Wsb,2, Wsc,1, Wsc,2, Wd, Wa, and bd; and the domain dependent parameters, including the parameters of the encoding function ENC and the action type, first argument, and second argument weights baT , ba1 , and ba2 .",4 Model,[0],[0]
We estimate the policy parameters θ using an exploration-based learning algorithm that maximizes the immediate expected reward.,5 Learning,[0],[0]
"Broadly speaking, during learning, we observe the agent behavior given the current policy, and for each visited state compute the expected immediate reward by observing rewards for all actions.",5 Learning,[0],[0]
"We assume access to a set of training examples {(x̄(j)i , s (j) i,1 , 〈x̄ (j) 1 , . . .",5 Learning,[0],[0]
", x̄ (j) i−1〉, g (j) i )} N,n(j)",5 Learning,[0],[0]
"j=1,i=1, where each instruction x̄(j)i is paired with a start state s
(j) i,1 , the previous instructions in the sequence
〈x̄(j)1 , .",5 Learning,[0],[0]
. .,5 Learning,[0],[0]
", x̄ (j) i−1〉, and a goal state g (j) i .
",5 Learning,[0],[0]
Reward The reward R(j)i : S × S ×,5 Learning,[0],[0]
"A → R is defined for each example j and instruction i:
R (j) i (s, a, s ′) = P (j) i (s, a, s ′)",5 Learning,[0],[0]
+ φ,5 Learning,[0],[0]
(j) i (s ′)− φ(j)i,5 Learning,[0],[0]
"(s) ,
where s is a source state, a is an action, and s′ is a target state.4 P (j)i (s, a, s
′) is a problem reward and φ
(j)",5 Learning,[0],[0]
i (s ′)− φ(j)i,5 Learning,[0],[0]
(s) is a shaping term.,5 Learning,[0],[0]
"The problem reward P (j)i (s, a, s
′) is positive for stopping at the goal g(j)i and negative for stopping in an incorrect
4While the reward function is defined for any state-actionstate tuple, in practice, it is used during learning with tuples that follow the system dynamics, s′ = T (s, a).
",5 Learning,[0],[0]
"Algorithm 1 SESTRA: Single-step Reward Observation.
",5 Learning,[0],[0]
"Input: Training data {(x̄(j)i , s (j) i,1 , 〈x̄ (j) 1 , . . .",5 Learning,[0],[0]
", x̄ (j) i−1〉,
g (j) i )}
N,n(j)
j=1,i=1, learning rate µ, entropy regularization coefficient λ, episode limit horizon M .",5 Learning,[0],[0]
"Definitions: πθ is a policy parameterized by θ, BEG is a special action to use for the first decoder step, and STOP indicates end of an execution.",5 Learning,[0],[0]
"T (s, a) is the state transition function, H is an entropy function, R(j)i (s, a, s
′) is the reward function for example j and instruction i, and RMSPROP divides each weight by a running average of its squared gradient (Tieleman and Hinton, 2012).
",5 Learning,[0],[0]
Output: Parameters θ defining a learned policy πθ .,5 Learning,[0],[0]
"1: for t = 1, . . .",5 Learning,[0],[0]
", T, j = 1, . . .",5 Learning,[0],[0]
", N do 2: for i = 1, . . .",5 Learning,[0],[0]
", n(j) do 3: ē← 〈 〉, k ← 0, a0 ← BEG 4: » Rollout up to STOP or episode limit.",5 Learning,[0],[0]
5:,5 Learning,[0],[0]
"while ak 6= STOP ∧ k < M do 6: k ← k + 1 7: s̃k ← (x̄i, 〈x̄1, . . .",5 Learning,[0],[0]
", x̄i−1〉, sk, ē[: k]) 8",5 Learning,[0],[0]
: » Sample an action from policy.,5 Learning,[0],[0]
"9: ak ∼ πθ(s̃k, ·) 10: sk+1 ← T (sk, ak) 11:",5 Learning,[0],[0]
ē←,5 Learning,[0],[0]
"[ē; 〈(sk, ak)〉] 12: ∆← 0̄ 13: for k′ = 1, . . .",5 Learning,[0],[0]
", k do 14: » Compute the entropy of πθ(s̃k′ , ·).",5 Learning,[0],[0]
"15: ∆← ∆ + λ∇θH(πθ(s̃k′ , ·)) 16: for a ∈",5 Learning,[0],[0]
"A do 17: s′ ← T (sk′ , a) 18: » Compute gradient for action a. 19: ∆← ∆ +R(j)i (sk′ , a, s ′)∇θπθ(s̃k′ , a)
20: θ ← θ + µRMSPROP",5 Learning,[0],[0]
"( ∆
k ) 21: return θ
state or taking an invalid action:
P (j) i (s, a, s ′) =  1.0 a = STOP ∧ s′ = g(j)i −1.0",5 Learning,[0],[0]
a = STOP ∧ s′ 6= g(j)i,5 Learning,[0],[0]
"−1.0− δ s = s′
−δ otherwise
,
where δ is a verbosity penalty.",5 Learning,[0],[0]
"The case s = s′ indicates that a was invalid in state s, as in this domain, all valid actions except STOP modify the state.",5 Learning,[0],[0]
We use a potential-based shaping term φ(j)i,5 Learning,[0],[0]
"(s
′)− φ(j)i",5 Learning,[0],[0]
"(s) (Ng et al., 1999), where φ
(j) i (s) =",5 Learning,[0],[0]
"−||s− g (j) i || computes the edit distance between the state s and the goal, measured over the objects in each state.",5 Learning,[0],[0]
"The shaping term densifies the reward, providing a meaningful signal for learning in nonterminal states.
",5 Learning,[0],[0]
Objective We maximize the immediate expected reward over all actions and use entropy regularization.,5 Learning,[0],[0]
"The gradient is approximated by sampling an execution ē = 〈(s1, a1), . . .",5 Learning,[0],[0]
", (sk, ak)〉 using our current policy:
∇θJ = 1
k k∑ k′=1 (∑ a∈A",5 Learning,[0],[0]
"R (sk, a, T (sk, a))∇θπ(s̃k, a)
+λ∇θH(π(s̃k, ·)) ) ,
where H(π(s̃k, ·) is the entropy term.
",5 Learning,[0],[0]
Algorithm Algorithm 1 shows the Single-step Reward Observation (SESTRA) learning algorithm.,5 Learning,[0],[0]
We iterate over the training data T times (line 1).,5 Learning,[0],[0]
"For each example j and turn i, we first perform a rollout by sampling an execution ē from πθ with at most M actions (lines 5-11).",5 Learning,[0],[0]
"If the rollout reaches the horizon without predicting STOP, we set the problem reward P (j)i to−1.0 for the last step.",5 Learning,[0],[0]
"Given the sampled states visited, we compute the entropy (line 15) and observe the immediate reward for all actions (line 19) for each step.",5 Learning,[0],[0]
"Entropy and rewards are used to accumulate the gradient, which is applied to the parameters using RMSPROP (Dauphin et al., 2015) (line 20).
",5 Learning,[0],[0]
Discussion Observing the rewards for all actions for each visited state addresses an on-policy learning exploration problem.,5 Learning,[0],[0]
"Actions that consistently receive negative reward early during learning will be visited with very low probability later on, and in practice, often not explored at all.",5 Learning,[0],[0]
"Because the network is randomly initialized, these early negative rewards are translated into strong general biases that are not grounded well in the observed context.",5 Learning,[0],[0]
Our algorithm exposes the agent to such actions later on when they receive positive rewards even though the agent does not explore them during rollout.,5 Learning,[0],[0]
"For example, in ALCHEMY, POP actions are sufficient to complete the first steps of good executions.",5 Learning,[0],[0]
"As a result, early during learning, the agent learns a strong bias against PUSH actions.",5 Learning,[0],[0]
"In practice, the agent then will not explore PUSH actions again.",5 Learning,[0],[0]
"In our algorithm, as the agent learns to roll out the correct POP prefix, it is then exposed to the reward for the first PUSH even though it likely sampled another POP.",5 Learning,[0],[0]
"It then unlearns its bias towards predicting POP.
",5 Learning,[0],[0]
"Our learning algorithm can be viewed as a costsensitive variant of the oracle in DAGGER (Ross et al., 2011), where it provides the rewards for all actions instead of an oracle action.",5 Learning,[0],[0]
"It is also related to Locally Optimal Learning to Search (LOLS; Chang et al., 2015) with two key distinctions: (a) instead of using different roll-in and roll-out policies, we use the model policy; and (b) we branch at each step, instead of once, but do not rollout
from branched actions since we only optimize the immediate reward.",5 Learning,[0],[0]
Figure 3 illustrates the comparison.,5 Learning,[0],[0]
"Our summation over immediate rewards for all actions is related the summation of estimated Q-values for all actions in the Mean Actor-Critic algorithm (Asadi et al., 2017).",5 Learning,[0],[0]
"Finally, our approach is related to Misra et al. (2017), who also maximize the immediate reward, but do not observe rewards for all actions for each state.",5 Learning,[0],[0]
"SCONE has three domains: ALCHEMY, SCENE, and TANGRAMS.",6 SCONE Domains and Data,[0],[0]
Each interaction contains five instructions.,6 SCONE Domains and Data,[0],[0]
Table 1 shows data statistics.,6 SCONE Domains and Data,[0],[0]
Table 2 shows discourse reference analysis.,6 SCONE Domains and Data,[0],[0]
"State encodings are detailed in the Supplementary Material.
",6 SCONE Domains and Data,[0],[0]
"ALCHEMY Each environment in ALCHEMY contains seven numbered beakers, each containing up to four colored chemicals in order.",6 SCONE Domains and Data,[0],[0]
Figure 1 shows an example.,6 SCONE Domains and Data,[0],[0]
"Instructions describe pouring chemicals between and out of beakers, and mixing beakers.",6 SCONE Domains and Data,[0],[0]
We treat all beakers as stacks.,6 SCONE Domains and Data,[0],[0]
"There
are two action types: PUSH and POP.",6 SCONE Domains and Data,[0],[0]
"POP takes a beaker index, and removes the top color.",6 SCONE Domains and Data,[0],[0]
"PUSH takes a beaker index and a color, and adds the color at the top of the beaker.",6 SCONE Domains and Data,[0],[0]
"To encode a state, we encode each beaker with an RNN, and concatenate the last output with the beaker index embedding.",6 SCONE Domains and Data,[0],[0]
"The set of vectors is the state embedding.
",6 SCONE Domains and Data,[0],[0]
"SCENE Each environment in SCENE contains ten positions, each containing at most one person defined by a shirt color and an optional hat color.",6 SCONE Domains and Data,[0],[0]
"Instructions describe adding or removing people, moving a person to another position, and moving a person’s hat to another person.",6 SCONE Domains and Data,[0],[0]
"There are four action types: ADD_PERSON, ADD_HAT, REMOVE_PERSON, and REMOVE_HAT.",6 SCONE Domains and Data,[0],[0]
ADD_PERSON and ADD_HAT take a position to place the person or hat and the color of the person’s shirt or hat.,6 SCONE Domains and Data,[0],[0]
REMOVE_PERSON,6 SCONE Domains and Data,[0],[0]
and REMOVE_HAT take the position to remove a person or hat from.,6 SCONE Domains and Data,[0],[0]
"To encode a state, we use a bidirectional RNN over the ordered positions.",6 SCONE Domains and Data,[0],[0]
The input for each position is a concatenation of the color embeddings for the person and hat.,6 SCONE Domains and Data,[0],[0]
"The set of RNN hidden states is the state embedding.
",6 SCONE Domains and Data,[0],[0]
TANGRAMS Each environment in TANGRAMS is a list containing at most five unique objects.,6 SCONE Domains and Data,[0],[0]
"Instructions describe removing or inserting an object into a position in the list, or swapping the positions of two items.",6 SCONE Domains and Data,[0],[0]
There are two action types: INSERT and REMOVE.,6 SCONE Domains and Data,[0],[0]
"INSERT takes the position to insert an object, and the object identifier.",6 SCONE Domains and Data,[0],[0]
REMOVE takes an object position.,6 SCONE Domains and Data,[0],[0]
We embed each object by concatenating embeddings for its type and position.,6 SCONE Domains and Data,[0],[0]
The resulting set is the state embedding.,6 SCONE Domains and Data,[0],[0]
"Evaluation Following Long et al. (2016), we evaluate task completion accuracy using exact match between the final state and the annotated goal state.",7 Experimental Setup,[0],[0]
"We report accuracy for complete interactions (5utts), the first three utterances of each interaction (3utts), and single instructions (Inst).",7 Experimental Setup,[0],[0]
"For single instructions, execution starts from the annotated start state of the instruction.
",7 Experimental Setup,[0],[0]
Systems We report performance of ablations and two baseline systems:,7 Experimental Setup,[0],[0]
"POLICYGRADIENT: policy gradient with cumulative episodic reward without a baseline, and CONTEXTUALBANDIT: the contextual bandit approach of Misra et al. (2017).",7 Experimental Setup,[0],[0]
"Both systems use the reward with the
shaping term and our model.",7 Experimental Setup,[0],[0]
We also report supervised learning results (SUPERVISED) by heuristically generating correct executions and computing maximum-likelihood estimate using contextaction demonstration pairs.,7 Experimental Setup,[0],[0]
Only the supervised approach uses the heuristically generated labels.,7 Experimental Setup,[0],[0]
"Although the results are not comparable, we also report the performance of previous approaches to SCONE.",7 Experimental Setup,[0],[0]
All three approaches generate logical representations based on lambda calculus.,7 Experimental Setup,[0],[0]
"In contrast to our approach, this requires an ontology of hand built symbols and rules to evaluate the logical forms.",7 Experimental Setup,[0],[0]
"Fried et al. (2018) uses supervised learning with annotated logical forms.
",7 Experimental Setup,[0],[0]
"Training Details For test results, we run each experiment five times and report results for the model with best validation interaction accuracy.",7 Experimental Setup,[0],[0]
"For ablations, we do the same with three experiments.",7 Experimental Setup,[0],[0]
We use a batch size of 20.,7 Experimental Setup,[0],[0]
We stop training using a validation set sampled from the training data.,7 Experimental Setup,[0],[0]
We hold the validation set constant for each domain for all experiments.,7 Experimental Setup,[0],[0]
"We use patience over the average reward, and select the best model using interaction-level (5utts) validation accuracy.",7 Experimental Setup,[0],[0]
"We tune λ, δ, and M on the development set.",7 Experimental Setup,[0],[0]
The selected values and other implementation details are described in the Supplementary Material.,7 Experimental Setup,[0],[0]
Table 3 shows test results.,8 Results,[0],[0]
"Our approach significantly outperforms POLICYGRADIENT and CONTEXTUALBANDIT, both of which suffer due to biases learned early during learning, hindering later exploration.",8 Results,[0],[0]
"This problem does not appear in TANGRAMS, where no action type is dominant at the beginning of executions, and all methods perform well.",8 Results,[0],[0]
"POLICYGRADIENT completely fails to learn ALCHEMY and SCENE due to observing only negative total rewards early during learning.
",8 Results,[0],[0]
"Using a baseline, for example with an actor-critic method, will potentially close the gap to CONTEXTUALBANDIT.",8 Results,[0],[0]
"However, it is unlikely to address the on-policy exploration problem.
",8 Results,[0],[0]
"Table 4 shows development results, including model ablation studies.",8 Results,[0],[0]
Removing previous instructions (– previous instructions) or both states (– current and initial state) reduces performance across all domains.,8 Results,[0],[0]
Removing only the initial state (– initial state) or the current state (– current state) shows mixed results across the domains.,8 Results,[0],[0]
"Providing access to both initial and current states increases performance for ALCHEMY, but reduces performance on the other domains.",8 Results,[0],[0]
We hypothesize that this is due to the increase in the number of parameters outweighing what is relatively marginal information for these domains.,8 Results,[0],[0]
"In our development and test results we use a single architecture across the three domains, the full approach, which has the highest interactive-level accuracy when averaged across the three domains (62.7 5utts).",8 Results,[0],[0]
We also report mean and standard deviation for our approach over five trials.,8 Results,[0],[0]
"We observe exceptionally high variance in performance on SCENE, where some experiments fail to learn and training performance remains exceptionally low (Figure 4).",8 Results,[0],[0]
"This highlights the sensitivity of the model to the random effects of initialization, dropout, and ordering of training examples.
",8 Results,[0],[0]
We analyze the instruction-level errors made by our best models when the agent is provided the correct initial state for the instruction.,8 Results,[0],[0]
We study fifty examples in each domain to identify the type of failures.,8 Results,[0],[0]
Table 5 shows the counts of major error categories.,8 Results,[0],[0]
We consider multiple reference resolution errors.,8 Results,[0],[0]
State reference errors indicate a failure to resolve a reference to the world state.,8 Results,[0],[0]
"For example, in ALCHEMY, the phrase leftmost red beaker specifies a beaker in the environment.",8 Results,[0],[0]
"If the model picked the correct action, but the wrong beaker, we count it as a state reference.",8 Results,[0],[0]
"We distinguish between multi-turn reference errors that should be feasible, and these that that are impossible to solve without access to states before executing previous utterances, which are not provided to our model.",8 Results,[0],[0]
"For example, in TANGRAMS, the instruction put it back in the same place refers to a previouslyremoved item.",8 Results,[0],[0]
"Because the agent only has access to the world state after following this instruction, it does not observe what kind of item was previously removed, and cannot identify the item to add.",8 Results,[0],[0]
"We
also find a significant number of errors due to ambiguous or incorrect instructions.",8 Results,[0],[0]
"For example, the SCENE instruction person in green appears on the right end is ambiguous.",8 Results,[0],[0]
"In the annotated goal, it is interpreted as referring to a person already in the environment, who moves to the 10th position.",8 Results,[0],[0]
"However, it can also be interpreted as a new person in green appearing in the 10th position.
",8 Results,[0],[0]
We also study performance with respect to multi-turn coreference by observing whether the model was able to identify the correct referent for each occurrence included in the analysis in Table 2.,8 Results,[0],[0]
"The models were able to correctly resolve 92.3%, 88.7%, and 76.0% of references in ALCHEMY, SCENE, and TANGRAMS respectively.
",8 Results,[0],[0]
"Finally, we include attention visualization for examples from the three domains in the Supplementary Material.",8 Results,[0],[0]
"We propose a model to reason about contextdependent instructional language that display strong dependencies both on the history of the
interaction and the state of the world.",9 Discussion,[0],[0]
"Future modeling work may include using intermediate world states from previous turns in the interaction, which is required for some of the most complex references in the data.",9 Discussion,[0],[0]
"We propose to train our model using SESTRA, a learning algorithm that takes advantage of single-step reward observations to overcome learned biases in on-policy learning.",9 Discussion,[0],[0]
Our learning approach requires additional reward observations in comparison to conventional reinforcement learning.,9 Discussion,[0],[0]
"However, it is particularly suitable to recovering from biases acquired early during learning, for example due to biased action spaces, which is likely to lead to incorrect blame assignment in neural network policies.",9 Discussion,[0],[0]
"When the domain and model are less susceptible to such biases, the benefit of the additional reward observations is less pronounced.",9 Discussion,[0],[0]
"One possible direction for future work is to use an estimator to predict rewards for all actions, rather than observing them.",9 Discussion,[0],[0]
"This research was supported by the NSF (CRII1656998), Schmidt Sciences, and cloud computing credits from Amazon.",Acknowledgements,[0],[0]
We thank John Langford and Dipendra Misra for helpful and insightful discussions with regards to our learning algorithm.,Acknowledgements,[0],[0]
We also thank the anonymous reviewers for their helpful comments.,Acknowledgements,[0],[0]
We propose a learning approach for mapping context-dependent sequential instructions to actions.,abstractText,[0],[0]
We address the problem of discourse and state dependencies with an attention-based model that considers both the history of the interaction and the state of the world.,abstractText,[0],[0]
"To train from start and goal states without access to demonstrations, we propose SESTRA, a learning algorithm that takes advantage of singlestep reward observations and immediate expected reward maximization.",abstractText,[0],[0]
"We evaluate on the SCONE domains, and show absolute accuracy improvements of 9.8%25.3% across the domains over approaches that use high-level logical representations.",abstractText,[0],[0]
Situated Mapping of Sequential Instructions to Actions with Single-step Reward Observation,title,[0],[0]
"1International Computer Science Institute and Department of Statistics, University of California at Berkeley, USA 2Department of Computer Science, Rensselaer Polytechnic Institute, USA. Correspondence to: Shusen Wang <shusen@berkeley.edu>, Alex Gittens <gittea@rpi.edu>, Michael W. Mahoney <mmahoney@stat.berkeley.edu>.
Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).
lem while greatly mitigating the statistical risks incurred by sketching.",text,[0],[0]
Regression is one of the most fundamental problems in machine learning.,1. Introduction,[0],[0]
The simplest and most thoroughly studied regression model is least squares regression (LSR).,1. Introduction,[0],[0]
Given features X =,1. Introduction,[0],[0]
[xT1 ; . . .,1. Introduction,[0],[0]
",x T n ] ∈ Rn×d and responses y =",1. Introduction,[0],[0]
"[y1, . . .",1. Introduction,[0],[0]
", yn] T ∈ Rn, the LSR problem minw ‖Xw",1. Introduction,[0],[0]
− y‖22 can be solved in O(nd2) time using the QR decomposition or in O(ndt) time using accelerated gradient descent algorithms.,1. Introduction,[0],[0]
"Here, t is the number of iterations, which depends on the initialization, the condition number of X, and the stopping criterion.
",1. Introduction,[0],[0]
"This paper considers the n d problem, where there is much redundancy in X. Matrix sketching, as used within Randomized Linear Algebra (RLA) (Mahoney, 2011; Woodruff, 2014), works by reducing the size of X without losing too much information; this operation can be modeled as taking actual rows or linear combinations of the rows of X with a sketching matrix S to form the sketch STX.",1. Introduction,[0],[0]
Here S ∈,1. Introduction,[0],[0]
"Rn×s satisfies d < s n so that STX generically has the same rank but much fewer rows as X. Sketching has been used to speed up LSR (Drineas et al., 2006; 2011; Clarkson & Woodruff, 2013; Meng & Mahoney, 2013; Nelson & Nguyên, 2013) by solving the sketched LSR problem minw ‖STXw",1. Introduction,[0],[0]
− STy‖22 instead of the original LSR problem.,1. Introduction,[0],[0]
"Solving sketched LSR costs either O(sd2 + Ts) time using the QR decomposition or O(sdt + Ts) time using accelerated gradient descent algorithms, where t is as defined previously1 and Ts is the time cost of sketching.",1. Introduction,[0],[0]
"For example, Ts = O(nd log s) when S is the subsampled randomized Hadamard transform (Drineas et al., 2011), and Ts = O(nd) when S is a CountSketch matrix (Clarkson & Woodruff, 2013).
",1. Introduction,[0],[0]
"There has been much work in RLA on analyzing the quality of sketched LSR with different sketching methods and different objectives; see the reviews (Mahoney, 2011;
1The condition number of XTSSTX is very close to that of XTX, and thus the number of iterations t is almost unchanged.
",1. Introduction,[0],[0]
"Woodruff, 2014) and the references therein.
",1. Introduction,[0],[0]
"The concept of sketched LSR originated in the theoretical computer science literature, e.g., (Drineas et al., 2006; 2011), where the behavior of sketched LSR was studied from an optimization perspective.",1. Introduction,[0],[0]
Let w? be the optimal LSR solution and w̃ be the solution to sketched LSR.,1. Introduction,[0],[0]
"This line of work established that if s = O(d/ + poly(d)), then the objective function value ‖Xw̃ − y ∥∥2 2 is at most
times worse than ‖Xw?−y ∥∥2 2 .",1. Introduction,[0],[0]
"These works also bounded ‖w̃−w?‖22 in terms of the difference in the objective function values and the condition number of XTX.
",1. Introduction,[0],[0]
"A more recent line of work has studied sketched LSR from a statistical perspective: (Ma et al., 2015; Raskutti & Mahoney, 2016; Pilanci & Wainwright, 2015; Wang et al., 2016b) considered statistical properties of sketched LSR like the bias and variance.",1. Introduction,[0],[0]
"In particular, Pilanci & Wainwright (2015) showed that sketched LSR has much higher variance than the optimal solution.
",1. Introduction,[0],[0]
Both of these perspectives are important and of practical interest.,1. Introduction,[0],[0]
The optimization perspective is relevant when the data can be taken as deterministic values.,1. Introduction,[0],[0]
"The statistical perspective is relevant in machine learning and statistics applications where the data are random, and the regression coefficients are therefore themselves random variables.
",1. Introduction,[0],[0]
"In practice, regularized regression, e.g., ridge regression and LASSO, exhibit more attractive bias-variance tradeoffs and generalization errors than vanilla LSR.",1. Introduction,[0],[0]
"Furthermore, the matrix generalization of LSR, where multiple responses are to be predicted, is often more useful than LSR.",1. Introduction,[0],[0]
"However, the properties of sketched regularized matrix regression are largely unknown.",1. Introduction,[0],[0]
"Hence, the question: How, if at all, does our understanding of the optimization and statistical properties of sketched LSR generalize to sketched regularized regression?",1. Introduction,[0],[0]
"We answer this question for sketched matrix ridge regression (MRR).
",1. Introduction,[0],[0]
Recall that X is n×,1. Introduction,[0],[0]
d. Let Y ∈ Rn×m denote the matrix of corresponding responses.,1. Introduction,[0],[0]
"We study the MRR problem
min W
{ f(W) , 1
n",1. Introduction,[0],[0]
"∥∥XW −Y∥∥2 F + γ‖W‖2F } , (1)
which has optimal solution
W? =",1. Introduction,[0],[0]
(XTX + nγId) †XTY.,1. Introduction,[0],[0]
"(2)
Here, (·)† denotes the Moore-Penrose inversion operation.
",1. Introduction,[0],[0]
"LSR is a special case of MRR, with m = 1 and γ = 0.",1. Introduction,[0],[0]
"The optimal solution W? can be obtained in O(nd2 + nmd) time using a QR decomposition of X. Sketching can be applied to MRR in two ways:
Wc = (XTSSTX + nγId) †(XTSSTY), (3) Wh = (XTSSTX + nγId) †XTY.",1. Introduction,[0],[0]
"(4)
Following the convention of Pilanci & Wainwright (2015); Wang et al. (2016a), we call Wc classical sketch and Wh Hessian sketch, which approximate the optimal solution W?.",1. Introduction,[0],[0]
Table 1 lists the time costs of the three solutions to MRR.,1. Introduction,[0],[0]
We first study classical and Hessian sketches from the optimization perspective.,1.1. Main Results and Contributions,[0],[0]
"Theorems 1 and 2 show that
• Classical sketch achieves relative error in the objective value.",1.1. Main Results and Contributions,[0],[0]
"With sketch size s = Õ(d/ ), the objective satisfies f(Wc)",1.1. Main Results and Contributions,[0],[0]
"≤ (1 + )f(W?).
",1.1. Main Results and Contributions,[0],[0]
• Hessian sketch does not achieve relative error in the objective value.,1.1. Main Results and Contributions,[0],[0]
"In particular, if 1n‖Y‖ 2 F is much
larger than f(W?), then f(Wh) can be far larger than f(W?).
",1.1. Main Results and Contributions,[0],[0]
"• For both classical and Hessian sketch, the relative quality of approximation improves as the regularization parameter γ increases.
",1.1. Main Results and Contributions,[0],[0]
"We then study classical and Hessian sketches from the statistical perspective, by modeling Y = XW0 + Ξ as the sum of a true linear model and random noise, decomposing the risk R(W) = E‖XW",1.1. Main Results and Contributions,[0],[0]
"− XW0‖2F into bias and variance terms, and bounding these terms.",1.1. Main Results and Contributions,[0],[0]
"We draw the following conclusions (see Theorems 4, 5, 6):
•",1.1. Main Results and Contributions,[0],[0]
The bias of the classical sketch can be nearly as small as that of the optimal solution.,1.1. Main Results and Contributions,[0],[0]
"The variance is Θ ( n s ) times that of the optimal solution; this bound
is optimal.",1.1. Main Results and Contributions,[0],[0]
"Therefore over-regularization, i.e., large γ, should be used to supress the variance.",1.1. Main Results and Contributions,[0],[0]
"(As γ increases, the bias increases, and the variance decreases.)
",1.1. Main Results and Contributions,[0],[0]
"• Since Y is not sketched with Hessian sketch, the variance of Hessian sketch can be close to the optimal solution.",1.1. Main Results and Contributions,[0],[0]
"However, Hessian sketch has high bias, especially when nγ is small compared to ‖X‖22.",1.1. Main Results and Contributions,[0],[0]
"This indicates that over-regularization is necessary for Hessian sketch to have low bias.
",1.1. Main Results and Contributions,[0],[0]
Our empirical evaluations bear out these theoretical results.,1.1. Main Results and Contributions,[0],[0]
"In particular, in Section 4, we show in Figure 2 that even when the regularization parameter γ is fine-tuned, the risks of classical sketch and Hessian sketch are worse than that
of the optimal solution by an order of magnitude.",1.1. Main Results and Contributions,[0],[0]
"This is an empirical demonstration of the fact that the near-optimal properties of sketching from the optimization perspective are much less relevant in a statistical setting than its suboptimal statistical properties.
",1.1. Main Results and Contributions,[0],[0]
"We propose to use model averaging, which averages the solutions of g sketched MRR problems, to attain lower optimization and statistical errors.",1.1. Main Results and Contributions,[0],[0]
"Without ambiguity, we denote classical and Hessian sketches with model averaging by Wc and Wh, respectively.",1.1. Main Results and Contributions,[0],[0]
"Theorems 7, 8, 10, 11 give the following results:
• Classical Sketch.",1.1. Main Results and Contributions,[0],[0]
"Assume the sketch size s = Õ(d ) and ≤ 1g ; then the bound on f(W
c)",1.1. Main Results and Contributions,[0],[0]
− f(W?) is proportional to g .,1.1. Main Results and Contributions,[0],[0]
Assume that s = Õ,1.1. Main Results and Contributions,[0],[0]
"( d 2 ) and
2 ≤ 1 g ; the bias does not increase; the variance bound is proportional to 1g .
",1.1. Main Results and Contributions,[0],[0]
•,1.1. Main Results and Contributions,[0],[0]
Hessian Sketch.,1.1. Main Results and Contributions,[0],[0]
"Assume that s = Õ(d ) and ≤ 1 g2 ;
then the bound on f(Wh)",1.1. Main Results and Contributions,[0],[0]
− f(W?) is proportional to g2 .,1.1. Main Results and Contributions,[0],[0]
Assume that s = Õ,1.1. Main Results and Contributions,[0],[0]
"( d 2 ); the variance does not increase; if, additionally, ≤ 1g and nγ is much smaller than the squared spectral norm of X, then the bias bound is proportional to g .
",1.1. Main Results and Contributions,[0],[0]
"Note that classical sketch with uniform sampling and model averaging is very well known as bagging (Breiman, 1996) (or pasting (Breiman, 1999) or bootstrap aggregating).",1.1. Main Results and Contributions,[0],[0]
"Different from bagging, our model averaging approach is not limited to uniform sampling.
",1.1. Main Results and Contributions,[0],[0]
Classical sketch with model averaging has three immediate applications.,1.1. Main Results and Contributions,[0],[0]
"In the single-machine setting,
• Classical sketch with model averaging offers a way to improve the statistical performance in the presence of heavy noise.",1.1. Main Results and Contributions,[0],[0]
"Assume the sketch size is s = Õ( √ nd).
",1.1. Main Results and Contributions,[0],[0]
"As g grows larger than ns , the variance of the averaged solution can be even lower than the optimal solution.",1.1. Main Results and Contributions,[0],[0]
See Remark 1 for further discussion.,1.1. Main Results and Contributions,[0],[0]
"This observation is in accordance with the observation that bagging reduces variance.
",1.1. Main Results and Contributions,[0],[0]
"In the distributed setting, the feature-response pairs (x1,y1), · · · , (xn,yn) ∈",1.1. Main Results and Contributions,[0],[0]
Rd × Rm are divided among g machines.,1.1. Main Results and Contributions,[0],[0]
"Assuming that the data have been shuffled randomly, each machine contains a sketch constructed by uniformly sampled rows from the dataset without replacement.",1.1. Main Results and Contributions,[0],[0]
"In this setting, the model averaging procedure will communicate the g local models only once to return the final estimate; this process has very low communication complexity and latency, and it suggests two further applications of classical sketch with model averaging:
• Model Averaging for Machine Learning.",1.1. Main Results and Contributions,[0],[0]
"If a lowprecision solution is acceptable, the averaged solution can be used in lieu of distributed numerical optimization algorithms requiring multiple rounds of communication.",1.1. Main Results and Contributions,[0],[0]
"If ng is big enough compared to d and the row coherence of X is small, then “one-shot” model averaging has bias and variance comparable to the optimal solution.
",1.1. Main Results and Contributions,[0],[0]
• Model Averaging for Optimization.,1.1. Main Results and Contributions,[0],[0]
"If a highprecision solution to MRR is required, then an iterative numerical optimization algorithm must be used.",1.1. Main Results and Contributions,[0],[0]
"The cost of such numerical optimization algorithms heavily depends on the quality of the initialization.2
A good initialization saves lots of iterations.",1.1. Main Results and Contributions,[0],[0]
"The averaged model is provably close to the optimal solution, so model averaging provides a high-quality initialization for more expensive algorithms.",1.1. Main Results and Contributions,[0],[0]
"The body of work on sketched LSR mentioned earlier (Drineas et al., 2006; 2011; Clarkson & Woodruff, 2013; Meng & Mahoney, 2013; Nelson & Nguyên, 2013) shares many similarities with our results.",1.2. Prior Work,[0],[0]
"However, the theories of sketched LSR developed from the optimization perspective do not obviously extend to MRR, and the statistical analysis of LSR and MRR differ: among other differences, LSR is unbiased while MRR has a nontrivial bias and therefore has a bias-variance tradeoff that must be considered.
",1.2. Prior Work,[0],[0]
"Lu et al. (2013) has considered a different application of sketching to ridge regression: they assume d n, reduce the number of features in X using sketching, and conduct statistical analysis.",1.2. Prior Work,[0],[0]
"Our setting differs in that we consider n d, reduce the number of samples by sketching, and allow for multiple responses.
",1.2. Prior Work,[0],[0]
"The model averaging analyzed in this paper is similar in spirit to the AVGM algorithm of (Zhang et al., 2013).",1.2. Prior Work,[0],[0]
"When classical sketch is used with uniform row sampling without replacement, our model averaging procedure is a special case of AVGM.",1.2. Prior Work,[0],[0]
"However, our results do not follow from those of (Zhang et al., 2013): first, we make no assumption on the data, whereas they assumed x1, · · · ,xn are i.i.d. from an unknown distribution; second, our results apply to many other sketching ensembles than uniform sampling without replacement; and third, we provide both optimization and statistical perspectives, whereas they provide only a statistical perspective.",1.2. Prior Work,[0],[0]
"Our results clearly indicate that the
2For example, the conjugate gradient method satisfies ‖W(t)−W?‖2F ‖W(0)−W?‖2
F
≤ θt1; the stochastic block coordinate descent (Tu
et al., 2016) satisfies Ef(W (t))−f(W?)
f(W(0))−f(W?)",1.2. Prior Work,[0],[0]
≤ θ t 2.,1.2. Prior Work,[0],[0]
"Here W(t) is the
output of the t-th iteration; θ1, θ2 ∈ (0, 1) depend on the condition number of XTX + nγId and some other factors.
",1.2. Prior Work,[0],[0]
"performance critically depends on the row coherence of X; this dependence is not captured in (Zhang et al., 2013).",1.2. Prior Work,[0],[0]
"For similar reasons, our work is different from the divide-andconquer kernel ridge regression algorithm of (Zhang et al., 2015).
",1.2. Prior Work,[0],[0]
Iterative Hessian sketch has been studied by Pilanci & Wainwright (2015); Wang et al. (2016a).,1.2. Prior Work,[0],[0]
"By way of comparison, all the algorithms in this paper are “one-shot” rather than iterative.",1.2. Prior Work,[0],[0]
"Upon completion of this paper, we noticed the contemporary works (Avron et al., 2016; Thanei et al., 2017).",1.2. Prior Work,[0],[0]
"Avron et al. (2016) studied classical sketch from the optimization perspective, and Thanei et al. (2017) studied LSR with model averaging.",1.2. Prior Work,[0],[0]
Section 2 defines our notation and introduces the sketching schemes we consider.,1.3. Paper Organization,[0],[0]
Section 3 presents our theoretical results.,1.3. Paper Organization,[0],[0]
Section 4 conducts experiments to verify our theories and demonstrates the usefulness of model averaging.,1.3. Paper Organization,[0],[0]
"Proofs of our claims and more empirical evaluations can be found in the technical report version (Wang et al., 2017).",1.3. Paper Organization,[0],[0]
"Throughout, we take In to be the n×n identity matrix and 0 to be a vector or matrix of all zeroes of the appropriate size.",2. Preliminaries,[0],[0]
Given a matrix A =,2. Preliminaries,[0],[0]
"[aij ], the i-th row is denoted by ai:, and a:j denotes the j-th column.",2. Preliminaries,[0],[0]
"The Frobenius and spectral norms of A are written as, respectively, ‖A‖F and ‖A‖2.",2. Preliminaries,[0],[0]
"The set {1, 2, · · · , n} is written [n].",2. Preliminaries,[0],[0]
"Let O, Ω, and Θ be the standard asymptotic notation.",2. Preliminaries,[0],[0]
"Let Õ conceal logarithm factors.
",2. Preliminaries,[0],[0]
"Throughout, we fix X ∈ Rn×d as our matrix of features.",2. Preliminaries,[0],[0]
"We set ρ = rank(X) and write the SVD of X as X = UΣVT , where U, Σ, V are respectively n × ρ, ρ × ρ, and d × ρ matrices.",2. Preliminaries,[0],[0]
We let σ1 ≥ · · · ≥ σρ > 0 be the singular values of X. The Moore-Penrose inverse of X is defined by X† = VΣ−1UT .,2. Preliminaries,[0],[0]
The row leverage scores of X are li = ‖u:i‖22 for i ∈,2. Preliminaries,[0],[0]
[n].,2. Preliminaries,[0],[0]
The row coherence of X is µ(X) =,2. Preliminaries,[0],[0]
nρ maxi ‖u:,2. Preliminaries,[0],[0]
i‖ 2 2.,2. Preliminaries,[0],[0]
"Throughout, we let µ be shorthand for µ(X).
",2. Preliminaries,[0],[0]
Matrix sketching turns big matrices into smaller ones without losing too much information useful in tasks like linear regression.,2. Preliminaries,[0],[0]
We denote the process of sketching a matrix X ∈ Rn×d by X′ = STX.,2. Preliminaries,[0],[0]
"Here, S ∈ Rn×s is called a sketching matrix and X′ ∈ Rs×d is called a sketch of X. In practice, except for Gaussian projection (where the entries of S are i.i.d. sampled fromN (0, 1/s)), the sketching matrix S is not formed explicitly.",2. Preliminaries,[0],[0]
"Matrix sketching can be accomplished by random sampling or random projection.
",2. Preliminaries,[0],[0]
"Random sampling corresponds to sampling rows of X
i.i.d.",2. Preliminaries,[0],[0]
"with replacement according to given row sampling probabilities p1, · · · , pm ∈ (0, 1).",2. Preliminaries,[0],[0]
"The corresponding (random) sketching matrix S ∈ Rn×s has exactly one non-zero entry per column, whose position indicates the index of the selected row; in practice, this S is not explicitly formed.",2. Preliminaries,[0],[0]
Uniform sampling fixes p1 = · · · = pn = 1n .,2. Preliminaries,[0],[0]
"Leverage score sampling sets pi proportional to the (exact or approximate (Drineas et al., 2012)) row leverage scores li of X.",2. Preliminaries,[0],[0]
"In practice shrinked leverage score sampling can be a better choice than leverage score sampling (Ma et al., 2015).",2. Preliminaries,[0],[0]
The sampling probabilities of shrinked leverage score sampling are defined by pi = 12 ( li∑n j=1,2. Preliminaries,[0],[0]
"lj + 1n ) .3
Gaussian projection is also well-known as the prototypical Johnson-Lindenstrauss transform (Johnson & Lindenstrauss, 1984).",2. Preliminaries,[0],[0]
"Let G ∈ Rm×s be a standard Gaussian matrix, i.e., each entry is sampled independently from N (0, 1).",2. Preliminaries,[0],[0]
"The matrix S = 1√
s G is a Gaussian projection
matrix.",2. Preliminaries,[0],[0]
"It takes O(nds) time to apply S ∈ Rn×s to any n × d dense matrix, which makes Gaussian projection inefficient relative to other forms of sketching.
",2. Preliminaries,[0],[0]
"Subsampled randomized Hadamard transform (SRHT) (Drineas et al., 2011; Lu et al., 2013; Tropp, 2011) is a more efficient alternative to Gaussian projection.",2. Preliminaries,[0],[0]
"Let Hn ∈ Rn×n be the Walsh-Hadamard matrix with +1 and −1 entries, D ∈ Rn×n be a diagonal matrix with diagonal entries sampled uniformly from {+1,−1}, and P ∈ Rn×s be the uniform row sampling matrix defined above.",2. Preliminaries,[0],[0]
"The matrix S = 1√
n DHnP ∈",2. Preliminaries,[0],[0]
"Rn×s is an SRHT matrix, and
can be applied to any n × d matrix in O(nd log s) time.",2. Preliminaries,[0],[0]
"In practice, the subsampled randomized Fourier transform (SRFT) (Woolfe et al., 2008) is often used in lieu of the SRHT, because the SRFT exists for all values of n, whereas Hn exists only for some values of n. Their performance and theoretical analyses are very similar.
CountSketch can be applied to any X ∈ Rn×d in O(nd) time (Charikar et al., 2004; Clarkson & Woodruff, 2013; Meng & Mahoney, 2013; Nelson & Nguyên, 2013; Pham & Pagh, 2013; Weinberger et al., 2009).",2. Preliminaries,[0],[0]
"Though more efficient to apply, CountSketch requires a bigger sketch size than Gaussian projections, SRHT, and leverage score sampling to attain the same theoretical guarantees.",2. Preliminaries,[0],[0]
"The readers can refer to (Woodruff, 2014) for a detailed description of CountSketch.",2. Preliminaries,[0],[0]
"Sections 3.1 and 3.2 analyze sketched MRR from, respectively, optimization and statistical perspectives.",3. Main Results,[0],[0]
"Sec-
3In fact, pi can be any convex combination of li∑n j=1",3. Main Results,[0],[0]
"lj and 1 n
(Ma et al., 2015).",3. Main Results,[0],[0]
"We use the weight 1 2 for simplicity; our conclusions extend in a straightforward manner to other weightings.
tions 3.3 and 3.4 capture the impacts of model averaging on, respectively, the optimization and statistical properties of sketched MRR.
",3. Main Results,[0],[0]
We described six sketching methods in Section 2.,3. Main Results,[0],[0]
"For simplicity, in this section, we refer to leverage score sampling, shrinked leverage score sampling, Gaussian projection, and SRHT as the four sketching methods; and we will mention explicitly uniform sampling and CountSketch.",3. Main Results,[0],[0]
The notation defined in Table 2 are used throughout.,3. Main Results,[0],[0]
"Theorem 1 shows that f(Wc), the objective value of classical sketch, is very close to the optimal objective value f(W?).",3.1. Sketched MRR: Optimization Perspective,[0],[0]
The approximation quality improves as γ increases.,3.1. Sketched MRR: Optimization Perspective,[0],[0]
Theorem 1 (Classical Sketch).,3.1. Sketched MRR: Optimization Perspective,[0],[0]
"For the four sketching methods with s = Õ ( βd ) , uniform sampling with s =
O ( µβd log d ) , and CountSketch with s = O ( βd2 ) , the inequality
f(Wc)− f(W?)",3.1. Sketched MRR: Optimization Perspective,[0],[0]
"≤ f(W?)
holds with probability at least 0.9.
",3.1. Sketched MRR: Optimization Perspective,[0],[0]
The corresponding guarantee for the performance of Hessian sketch is given in Theorem 2.,3.1. Sketched MRR: Optimization Perspective,[0],[0]
"It is weaker than the guarantee for classical sketch, especially when 1n‖Y‖ 2 F is far larger than f(W?).",3.1. Sketched MRR: Optimization Perspective,[0],[0]
"If Y is nearly noiseless—Y is wellexplained by a linear combination of the columns of X— and γ is small, then f(W?) is close to zero, and consequently f(W?) can be far smaller than 1n‖Y‖ 2 F .",3.1. Sketched MRR: Optimization Perspective,[0],[0]
"Therefore, in this case which is ideal for MRR, f(Wh) is not close to f(W?) and our theory suggests Hessian sketch does not perform as well as classical sketch.",3.1. Sketched MRR: Optimization Perspective,[0],[0]
"This is verified by our experiments, which show that unless γ is big or a large portion of Y is outside the column space of X, the ratio f(W
h) f(W?) can be large.
",3.1. Sketched MRR: Optimization Perspective,[0],[0]
Theorem 2 (Hessian Sketch).,3.1. Sketched MRR: Optimization Perspective,[0],[0]
"For the four sketching methods with s = Õ
( β2d ) , uniform sampling with s =
O ( µβ2d log d ) , and CountSketch with s = O(β 2d2
), the inequality
f(Wh)− f(W?) ≤",3.1. Sketched MRR: Optimization Perspective,[0],[0]
( ‖Y‖2F n,3.1. Sketched MRR: Optimization Perspective,[0],[0]
"− f(W?) ) .
holds with probability at least 0.9.
",3.1. Sketched MRR: Optimization Perspective,[0],[0]
These two results imply that f(Wc) and f(Wh) can be close to f(W?).,3.1. Sketched MRR: Optimization Perspective,[0],[0]
"When this is the case, curvature of the objective function ensures that the sketched solutions Wc and Wh are close to the optimal solution W?.",3.1. Sketched MRR: Optimization Perspective,[0],[0]
Lemma 3 studies the Mahalanobis distance ‖M(W−W?)‖2F .,3.1. Sketched MRR: Optimization Perspective,[0],[0]
"Here M is any non-singular matrix; in particular, it can be the identity matrix or (XTX)1/2.",3.1. Sketched MRR: Optimization Perspective,[0],[0]
Lemma 3.,3.1. Sketched MRR: Optimization Perspective,[0],[0]
"Let f be the objective function of MRR defined in (1), W ∈ Rd×m be arbitrary, and W?",3.1. Sketched MRR: Optimization Perspective,[0],[0]
be the optimal solution defined in (2).,3.1. Sketched MRR: Optimization Perspective,[0],[0]
"For any non-singular matrix M, the Mahalanobis distance satisfies 1
n
∥∥M(W −W?)∥∥2 F ≤ f(W)− f(W ?)
σ2min",3.1. Sketched MRR: Optimization Perspective,[0],[0]
[ (XTSSTX + nγId)1/2M−1 ] .,3.1. Sketched MRR: Optimization Perspective,[0],[0]
"By choosing M = (XTX)1/2, we can bound 1n‖XW − XW?‖2F in terms of the difference in the objective values:
",3.1. Sketched MRR: Optimization Perspective,[0],[0]
1 n ∥∥XW −XW?∥∥2 F ≤,3.1. Sketched MRR: Optimization Perspective,[0],[0]
β,3.1. Sketched MRR: Optimization Perspective,[0],[0]
"[ f(W)− f(W?) ] .
",3.1. Sketched MRR: Optimization Perspective,[0],[0]
"With Lemma 3, we can directly apply Theorems 1 or 2 to bound 1n‖XW c −XW?‖2F or 1n‖XW h −XW?‖2F .",3.1. Sketched MRR: Optimization Perspective,[0],[0]
We consider the following fixed design model.,3.2. Sketched MRR: Statistical Perspective,[0],[0]
"Let X ∈ Rn×d be the observed feature matrix, W0 ∈ Rd×m be the true and unknown model, Ξ ∈ Rn×m contain unknown random noise, and
Y = XW0 + Ξ (5)
be the observed responses.",3.2. Sketched MRR: Statistical Perspective,[0],[0]
"We make the following standard weak assumptions on the noise:
E[Ξ] = 0 and E[ΞΞT ] = ξ2In.",3.2. Sketched MRR: Statistical Perspective,[0],[0]
"We observe X and Y and seek to estimate W0.
",3.2. Sketched MRR: Statistical Perspective,[0],[0]
"We can evaluate the quality of the estimate by the risk: R(W) = 1 n E ∥∥XW −XW0∥∥2F , (6)
where the expectation is taken w.r.t.",3.2. Sketched MRR: Statistical Perspective,[0],[0]
"the noise Ξ. We study the risk functions R(W?), R(Wc), and R(Wh) in the following.",3.2. Sketched MRR: Statistical Perspective,[0],[0]
Theorem 4 (Bias-Variance Decomposition).,3.2. Sketched MRR: Statistical Perspective,[0],[0]
We consider the data model described in this subsection.,3.2. Sketched MRR: Statistical Perspective,[0],[0]
"Let W be W?, Wc, or Wh, as defined in (2), (3), (4), respectively; then the risk function can be decomposed as
R(W) = bias2(W) + var(W).
",3.2. Sketched MRR: Statistical Perspective,[0],[0]
Recall the SVD of X: X = UΣVT .,3.2. Sketched MRR: Statistical Perspective,[0],[0]
"The bias and variance terms can be written as
bias ( W? )",3.2. Sketched MRR: Statistical Perspective,[0],[0]
=,3.2. Sketched MRR: Statistical Perspective,[0],[0]
"γ √ n ∥∥∥(Σ2 + nγIρ)−1ΣVTW0∥∥∥
F ,
var ( W? )",3.2. Sketched MRR: Statistical Perspective,[0],[0]
"= ξ2
n ∥∥∥(Iρ + nγΣ−2)−1∥∥∥2 F ,
bias ( Wc ) =",3.2. Sketched MRR: Statistical Perspective,[0],[0]
"γ √ n ∥∥∥(ΣUTSSTUΣ + nγIρ)†ΣVTW0∥∥∥
F ,
var ( Wc ) = ξ2
n ∥∥∥(UTSSTU + nγΣ−2)†UTSST∥∥∥2 F ,
bias ( Wh ) =",3.2. Sketched MRR: Statistical Perspective,[0],[0]
"γ √ n ∥∥∥(Σ−2 + UTSSTU−Iρnγ )
· ( UTSSTU + nγΣ−2 )† ΣVTW0 ∥∥∥ F ,
var ( Wh ) = ξ2
n ∥∥∥(UTSSTU + nγΣ−2)†∥∥∥2 F .
",3.2. Sketched MRR: Statistical Perspective,[0],[0]
Theorem 5 provides upper and lower bounds on the bias and variance of the classical sketch.,3.2. Sketched MRR: Statistical Perspective,[0],[0]
"In particular, we see that that bias(Wc) is within a factor of (1± ) of bias(W?).",3.2. Sketched MRR: Statistical Perspective,[0],[0]
"However, var(Wc) is Θ(ns ) times worse than var(W ?).",3.2. Sketched MRR: Statistical Perspective,[0],[0]
Theorem 5 (Classical Sketch).,3.2. Sketched MRR: Statistical Perspective,[0],[0]
"For Gaussian projection and SRHT sketching with s = Õ( d 2 ), uniform sampling with s = O(µd log d 2 ), or CountSketch with s = O( d2
2 ), the inequalities
1− ≤",3.2. Sketched MRR: Statistical Perspective,[0],[0]
"bias(W c)
bias(W?) ≤ 1",3.2. Sketched MRR: Statistical Perspective,[0],[0]
"+ ,
(1− )n s ≤ var(W
c)
var(W?)",3.2. Sketched MRR: Statistical Perspective,[0],[0]
≤ (1 + )n,3.2. Sketched MRR: Statistical Perspective,[0],[0]
"s
hold with probability at least 0.9.
",3.2. Sketched MRR: Statistical Perspective,[0],[0]
"For shrinked leverage score sampling with s = O(d log d 2 ), these inequalities, except for the lower bound on the variance,4 hold with probability at least 0.9.
",3.2. Sketched MRR: Statistical Perspective,[0],[0]
Theorem 6 establishes similar upper and lower bounds on the bias and variance of Hessian sketch.,3.2. Sketched MRR: Statistical Perspective,[0],[0]
The situation is the reverse of that with classical sketch: the variance of Wh is close to that of W?,3.2. Sketched MRR: Statistical Perspective,[0],[0]
"if s is large enough, but as the regularization parameter γ goes to zero, bias(Wh) becomes much larger than bias(W?).",3.2. Sketched MRR: Statistical Perspective,[0],[0]
Theorem 6 (Hessian Sketch).,3.2. Sketched MRR: Statistical Perspective,[0],[0]
"For the four sketching methods with s = Õ( d 2 ), uniform sampling with s = O(µd log d 2 ), and CountSketch with s = O( d2
2 ), the inequalities
bias(Wh) bias(W?) ≤",3.2. Sketched MRR: Statistical Perspective,[0],[0]
"(1 + )
( 1 +
‖X‖22 nγ
) ,
1− ≤ var(W h)
var(W?) ≤ 1",3.2. Sketched MRR: Statistical Perspective,[0],[0]
"+
hold with probability at least 0.9.",3.2. Sketched MRR: Statistical Perspective,[0],[0]
Further assume that σ2ρ ≥ nγ .,3.2. Sketched MRR: Statistical Perspective,[0],[0]
"Then
bias(Wh)
bias(W?)",3.2. Sketched MRR: Statistical Perspective,[0],[0]
"≥ 1 1 + ( σ2ρ nγ − 1 )
holds with probability at least 0.9.
",3.2. Sketched MRR: Statistical Perspective,[0],[0]
The lower bound on the bias shows that Hessian sketch can suffer from a much higher bias than the optimal solution.,3.2. Sketched MRR: Statistical Perspective,[0],[0]
"The gap between bias(Wh) and bias(W?) can be
4For shrinked leverage score sampling, ‖S‖22 does not enjoy nontrivial lower bound.",3.2. Sketched MRR: Statistical Perspective,[0],[0]
"This is why we do not have a lower bound on the variance.
lessened by increasing the regularization parameter γ, but such over-regularization increases the baseline bias(W?) itself.",3.2. Sketched MRR: Statistical Perspective,[0],[0]
"It is also worth mentioning that unlike bias(W?) and bias(Wc), bias(Wh) is not monotonically increasing with γ, as is empirically verified in Figure 2.
",3.2. Sketched MRR: Statistical Perspective,[0],[0]
"In sum, our theories show that classical and Hessian sketches are not statistically comparable to the optimal solutions: classical sketch has too high a variance, and Hessian sketch has too high a bias for reasonable amounts of regularization.",3.2. Sketched MRR: Statistical Perspective,[0],[0]
"In practice, the regularization parameter γ should be tuned to optimize the prediction accuracy.",3.2. Sketched MRR: Statistical Perspective,[0],[0]
"Our experiments in Figure 2 show that even with fine-tuned γ, the risks of classical and Hessian sketches can be higher than the risk of the optimal solution by an order of magnitude.",3.2. Sketched MRR: Statistical Perspective,[0],[0]
"Formally speaking, minγ R(Wc) minγ R(W?) and minγ R(Wh) minγ R(W?) hold in practice.
",3.2. Sketched MRR: Statistical Perspective,[0],[0]
"Our empirical study in Figure 2 suggests classical and Hessian sketches both require over-regularization, i.e., setting γ larger than what is best for the optimal solution W?.",3.2. Sketched MRR: Statistical Perspective,[0],[0]
"Formally speaking, argminγ R(W c) > argminγ R(W ?) and argminγ R(W h) > argminγ R(W
?).",3.2. Sketched MRR: Statistical Perspective,[0],[0]
"Although this is the case for both types of sketches, the underlying explanations are different.",3.2. Sketched MRR: Statistical Perspective,[0],[0]
"Classical sketch has a high variance, so a large γ is required to supress the variance (its variance is non-increasing with γ).",3.2. Sketched MRR: Statistical Perspective,[0],[0]
"Hessian sketch has very high bias when γ is small, so a reasonably large γ is necessary to lower its bias.",3.2. Sketched MRR: Statistical Perspective,[0],[0]
We consider model averaging as an approach to increasing the accuracy of sketched MRR solutions.,3.3. Model Averaging: Optimization Perspective,[0],[0]
"The model averaging procedure is straightforward: one independently draws g sketching matrices S1, · · · ,Sg ∈ Rn×s, uses these to form g sketched MRR solutions, denoted by {Wci} g i=1 or {Whi} g i=1, and averages these solutions to obtain the fi-
nal estimate Wc = 1g ∑g i=1",3.3. Model Averaging: Optimization Perspective,[0],[0]
W c,3.3. Model Averaging: Optimization Perspective,[0],[0]
i or W h = 1g ∑g i=1,3.3. Model Averaging: Optimization Perspective,[0],[0]
W h,3.3. Model Averaging: Optimization Perspective,[0],[0]
i .,3.3. Model Averaging: Optimization Perspective,[0],[0]
"Practical applications of model averaging are enumerated in Section 1.1.
",3.3. Model Averaging: Optimization Perspective,[0],[0]
Theorems 7 and 8 present guarantees on the optimization accuracy of using model averaging to combine the classical or Hessian sketch solutions.,3.3. Model Averaging: Optimization Perspective,[0],[0]
We can contrast these with the guarantees provided for sketched MRR in Theorems 1 and 2.,3.3. Model Averaging: Optimization Perspective,[0],[0]
"For classical sketch with model averaging, we see that when ≤ 1g , the bound on f(W
h)−f(W?) is proportional to /g.",3.3. Model Averaging: Optimization Perspective,[0],[0]
"From Lemma 3 we can see that the distances from Wc to W? also decreases accordingly.
",3.3. Model Averaging: Optimization Perspective,[0],[0]
Theorem 7 (Classical Sketch with Model Averaging).,3.3. Model Averaging: Optimization Perspective,[0],[0]
"For the four methods, let s = Õ
( βd ) ; for uniform sampling, let
s = O ( µβd log d ) .",3.3. Model Averaging: Optimization Perspective,[0],[0]
"Then the inequality
f(Wc)− f(W?)",3.3. Model Averaging: Optimization Perspective,[0],[0]
≤,3.3. Model Averaging: Optimization Perspective,[0],[0]
"( g + β2 2 ) f(W?)
holds with probability at least 0.8.
",3.3. Model Averaging: Optimization Perspective,[0],[0]
"For Hessian sketch with model averaging, if β2 ≤ 1 g2 , then the bound on f(Wh)− f(W?) is proportional to g2 .
",3.3. Model Averaging: Optimization Perspective,[0],[0]
Theorem 8 (Hessian Sketch with Model Averaging).,3.3. Model Averaging: Optimization Perspective,[0],[0]
"For the four methods let s = Õ
( β2d ) , and for uniform sam-
pling let s = O ( µβ2d log d ) , then the inequality
f(Wh)− f(W?) ≤",3.3. Model Averaging: Optimization Perspective,[0],[0]
( g2 + 2 β2 ) ( ‖Y‖2F n,3.3. Model Averaging: Optimization Perspective,[0],[0]
"− f(W?) ) .
holds with probability at least 0.8.",3.3. Model Averaging: Optimization Perspective,[0],[0]
Model averaging also has the salutatory property of reducing the risks of the classical and Hessian sketch solutions.,3.4. Model Averaging: Statistical Perspective,[0],[0]
"Our first result conducts a bias-variance decomposition for the averaged solution of sketched MRR.
",3.4. Model Averaging: Statistical Perspective,[0],[0]
Theorem 9 (Bias-Variance Decomposition).,3.4. Model Averaging: Statistical Perspective,[0],[0]
We consider the fixed design model (5).,3.4. Model Averaging: Statistical Perspective,[0],[0]
"The risk function defined in (6) can be decomposed as
R(W) = bias2(W) + var(W).
",3.4. Model Averaging: Statistical Perspective,[0],[0]
The bias and variance terms are bias ( Wc ) =,3.4. Model Averaging: Statistical Perspective,[0],[0]
γ √ n ∥∥∥1,3.4. Model Averaging: Statistical Perspective,[0],[0]
g g∑ i=1,3.4. Model Averaging: Statistical Perspective,[0],[0]
( ΣUTSiS T i UΣ + nγIρ,3.4. Model Averaging: Statistical Perspective,[0],[0]
),3.4. Model Averaging: Statistical Perspective,[0],[0]
"† ΣVTW0 ∥∥∥ F , var ( Wc ) = ξ2
n ∥∥∥1",3.4. Model Averaging: Statistical Perspective,[0],[0]
g g∑ i=1,3.4. Model Averaging: Statistical Perspective,[0],[0]
( UTSiS T,3.4. Model Averaging: Statistical Perspective,[0],[0]
i U + nγΣ −2)†UTSiSTi,3.4. Model Averaging: Statistical Perspective,[0],[0]
∥∥∥2,3.4. Model Averaging: Statistical Perspective,[0],[0]
"F ,
bias ( Wh ) =",3.4. Model Averaging: Statistical Perspective,[0],[0]
γ √ n ∥∥∥1,3.4. Model Averaging: Statistical Perspective,[0],[0]
g g∑ i=1,3.4. Model Averaging: Statistical Perspective,[0],[0]
( Σ−2 + UTSiS T i U− Iρ nγ ) · ( UTSiS T i U + nγΣ −2)†ΣVTW0∥∥∥,3.4. Model Averaging: Statistical Perspective,[0],[0]
"F ,
var ( Wh ) = ξ2
n ∥∥∥1",3.4. Model Averaging: Statistical Perspective,[0],[0]
g g∑ i=1,3.4. Model Averaging: Statistical Perspective,[0],[0]
( UTSiS T,3.4. Model Averaging: Statistical Perspective,[0],[0]
"i U + nγΣ −2)†∥∥∥2 F .
",3.4. Model Averaging: Statistical Perspective,[0],[0]
"Theorems 10 and 11 provide upper bounds on the bias and variance of model-averaged sketched MRR for, respectively, classical sketch and Hessian sketch.",3.4. Model Averaging: Statistical Perspective,[0],[0]
We can contrast them with Theorems 5 and 6 to see the statistical benefits of model averaging.,3.4. Model Averaging: Statistical Perspective,[0],[0]
Theorem 10 (Classical Sketch with Model Averaging).,3.4. Model Averaging: Statistical Perspective,[0],[0]
"For shrinked leverage score sampling, Gaussian projection, SRHT with s = Õ",3.4. Model Averaging: Statistical Perspective,[0],[0]
"( d 2 ) , or uniform sampling with
s = O ( µd log d 2 ) , the inequalities
bias(Wc) bias(W?)",3.4. Model Averaging: Statistical Perspective,[0],[0]
≤ 1,3.4. Model Averaging: Statistical Perspective,[0],[0]
"+ , var(Wc)
",3.4. Model Averaging: Statistical Perspective,[0],[0]
var(W?),3.4. Model Averaging: Statistical Perspective,[0],[0]
≤ n s (√ 1+ /g,3.4. Model Averaging: Statistical Perspective,[0],[0]
"g + )2
hold with probability at least 0.8.",3.4. Model Averaging: Statistical Perspective,[0],[0]
Remark 1.,3.4. Model Averaging: Statistical Perspective,[0],[0]
"From this result, we see that if ≤ 1√g , then the variance is proportional to 1g .",3.4. Model Averaging: Statistical Perspective,[0],[0]
"If g and s are at least
g = O (n s ) and s",3.4. Model Averaging: Statistical Perspective,[0],[0]
"= Õ (√ nd ) ,
then the risk R(Wc) is close to R(W?).",3.4. Model Averaging: Statistical Perspective,[0],[0]
"If g and s are larger, then the variance var(Wc) can even be even lower than var(W?).
",3.4. Model Averaging: Statistical Perspective,[0],[0]
Theorem 11 shows that model averaging decreases the bias of Hessian sketch without increasing the variance.,3.4. Model Averaging: Statistical Perspective,[0],[0]
"For Hessian sketch without model averaging, recall that bias(Wh)
is larger than bias(W?) by a factor of O(‖X‖22/(nγ)).",3.4. Model Averaging: Statistical Perspective,[0],[0]
"Theorem 11 shows that model averaging reduces this ratio by a factor of g when ≤ 1 g .
",3.4. Model Averaging: Statistical Perspective,[0],[0]
Theorem 11 (Hessian Sketch with Model Averaging).,3.4. Model Averaging: Statistical Perspective,[0],[0]
"For the four methods with s = Õ ( d 2 ) , or uniform sampling
with s = O ( µd log d 2 ) , the inequalities
bias(Wh) bias(W?)",3.4. Model Averaging: Statistical Perspective,[0],[0]
≤ 1,3.4. Model Averaging: Statistical Perspective,[0],[0]
+,3.4. Model Averaging: Statistical Perspective,[0],[0]
"+ ( g + 2 )‖X‖22 nγ ,
var(Wh) var(W?)",3.4. Model Averaging: Statistical Perspective,[0],[0]
≤ 1,3.4. Model Averaging: Statistical Perspective,[0],[0]
"+
hold with probability at least 0.8.",3.4. Model Averaging: Statistical Perspective,[0],[0]
"Following (Ma et al., 2015; Yang et al., 2016), we constructed X ∈ Rn×d to have condition number κ(XTX) = 1012 and high row coherence, fixed w0 =",4. Sketched Ridge Regression Experiments,[0],[0]
"[10.2d; 0.110.6d; 10.2d], and set y = Xw0 +ε ∈ Rn, where the entries of ε ∈",4. Sketched Ridge Regression Experiments,[0],[0]
Rn were i.i.d.,4. Sketched Ridge Regression Experiments,[0],[0]
"sampled from N (0, ξ2).",4. Sketched Ridge Regression Experiments,[0],[0]
"The details of this data model are given in the technical report version (Wang et al., 2017).",4. Sketched Ridge Regression Experiments,[0],[0]
Let S ∈,4. Sketched Ridge Regression Experiments,[0],[0]
Rn×s be any of the six sketching methods considered in this paper.,4. Sketched Ridge Regression Experiments,[0],[0]
"We fix n = 105, d = 500, and s = 5, 000.",4. Sketched Ridge Regression Experiments,[0],[0]
"Because the analytical expressions involve the random sketching matrix S, we randomly generate S, repeat this procedure 10 times, and report the averaged results.
",4. Sketched Ridge Regression Experiments,[0],[0]
"In Figure 1, we plot the objective function value f(w) = 1 n‖Xw",4. Sketched Ridge Regression Experiments,[0],[0]
"− y‖ 2 2 + γ‖w‖22 against γ, under different settings of noise intensity ξ.",4. Sketched Ridge Regression Experiments,[0],[0]
"The results verify our theory: classical sketch wc is always close to optimal; Hessian sketch wh is much worse than the optimal when γ is small and y is mostly in the column space of X.
We conducted experiments on synthetic data to verify Theorems 5 and 6 and to show the effects of classical and Hessian sketching on the bias and variance.",4. Sketched Ridge Regression Experiments,[0],[0]
We set the noise intensity to be ξ = 0.1.,4. Sketched Ridge Regression Experiments,[0],[0]
"In Figure 2, we plot the analytical expressions for the squared bias, variance, and risk stated in Theorem 4 against the regularization parameter γ.",4. Sketched Ridge Regression Experiments,[0],[0]
"The results of this experiment match our theory: classical sketch magnified the variance, and Hessian sketch increased the bias.",4. Sketched Ridge Regression Experiments,[0],[0]
"Even if γ is fine-tuned, the risks of classical and Hessian sketches can be much higher than those of the optimal solution.5",4. Sketched Ridge Regression Experiments,[0],[0]
Our experiments also indicate that classical and Hessian sketches require setting γ larger than the best regularization parameter for the optimal solution W?.,4. Sketched Ridge Regression Experiments,[0],[0]
We studied sketched matrix ridge regression (MRR) from optimization and statistical perspectives.,5. Conclusions,[0],[0]
"Using classical sketch, by taking a large enough sketch, one can obtain an -accurate approximate solution.",5. Conclusions,[0],[0]
"Counterintuitively and in contrast to classical sketch, the relative error of Hessian sketch increases as the responses Y are better approximated by linear combinations of the columns of X. Both classical and Hessian sketches can have statistical risks that are worse than the risk of the optimal solution by an order of magnitude.",5. Conclusions,[0],[0]
We proposed the use of model averaging to attain better optimization and statistical properties.,5. Conclusions,[0],[0]
"We have shown that model averaging leads to substantial improvements in the theoretical error bounds, suggesting applications in distributed optimization and machine learning.
",5. Conclusions,[0.9510432939583935],"['We assume that the covariate shift problem occurs mainly in the negative training and test data, and no or minimum covariate shift exists in the positive training and test data.']"
"5In the experiment yielding Figure 2, Hessian sketch had lower risk than classical sketch.",5. Conclusions,[0],[0]
"This is not generally true: if we used a smaller ξ, so that the variance is dominated by bias, then classical sketch results in lower risks than Hessian sketch.",5. Conclusions,[0],[0]
We thank the anonymous reviewers for their helpful suggestions.,Acknowledgements,[0],[0]
We thank the Army Research Office and the Defense Advanced Research Projects Agency for partial support of this work.,Acknowledgements,[0],[0]
We address the statistical and optimization impacts of using classical sketch versus Hessian sketch to solve approximately the Matrix Ridge Regression (MRR) problem.,abstractText,[0],[0]
"Prior research has considered the effects of classical sketch on least squares regression (LSR), a strictly simpler problem.",abstractText,[0],[0]
"We establish that classical sketch has a similar effect upon the optimization properties of MRR as it does on those of LSR—namely, it recovers nearly optimal solutions.",abstractText,[0],[0]
"In contrast, Hessian sketch does not have this guarantee; instead, the approximation error is governed by a subtle interplay between the “mass” in the responses and the optimal objective value.",abstractText,[0],[0]
"For both types of approximations, the regularization in the sketched MRR problem gives it significantly different statistical properties from the sketched LSR problem.",abstractText,[0],[0]
"In particular, there is a bias-variance trade-off in sketched MRR that is not present in sketched LSR.",abstractText,[0],[0]
"We provide upper and lower bounds on the biases and variances of sketched MRR; these establish that the variance is significantly increased when classical sketches are used, while the bias is significantly increased when using Hessian sketches.",abstractText,[0],[0]
"Empirically, sketched MRR solutions can have risks that are higher by an order-of-magnitude than those of the optimal MRR solutions.",abstractText,[0],[0]
We establish theoretically and empirically that model averaging greatly decreases this gap.,abstractText,[0],[0]
"Thus, in the distributed setting, sketching combined with model averaging is a powerful technique that quickly obtains near-optimal solutions to the MRR probInternational Computer Science Institute and Department of Statistics, University of California at Berkeley, USA Department of Computer Science, Rensselaer Polytechnic Institute, USA.",abstractText,[0],[0]
Correspondence to: Shusen Wang,abstractText,[0],[0]
"<shusen@berkeley.edu>, Alex Gittens <gittea@rpi.edu>, Michael W. Mahoney <mmahoney@stat.berkeley.edu>.",abstractText,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",abstractText,[0],[0]
Copyright 2017 by the author(s).,abstractText,[0],[0]
lem while greatly mitigating the statistical risks incurred by sketching.,abstractText,[0],[0]
"Sketched Ridge Regression: Optimization Perspective,  Statistical Perspective, and Model Averaging",title,[0],[0]
"Topological Data Analysis (TDA) is an emerging trend in data science, grounded on topological methods to design descriptors for complex data—see e.g. (Carlsson, 2009) for an introduction to the subject.",1. Introduction,[0],[0]
"The descriptors of TDA can be used in various contexts, in particular statistical learning and geometric inference, where they provide useful insight into the structure of data.",1. Introduction,[0],[0]
"Applications of TDA can be found in a number of scientific areas, including computer vision (Li et al., 2014), materials science (Hiraoka et al., 2016), and brain science (Singh et al., 2008), to name
1INRIA Saclay 2CREST, ENSAE, Université Paris Saclay.",1. Introduction,[0],[0]
Correspondence to:,1. Introduction,[0],[0]
"Mathieu Carrière <mathieu.carriere@inria.fr>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
a few.,1. Introduction,[0],[0]
"The tools developed in TDA are built upon persistent homology theory (Edelsbrunner & Harer, 2010; Oudot, 2015), and their main output is a descriptor called persistence diagram (PD), which encodes the topology of a space at all scales in the form of a point cloud with multiplicities in the plane R2—see Section 2.1 for more details.
PDs as features.",1. Introduction,[0],[0]
"The main strength of PDs is their stability with respect to perturbations of the data (Chazal et al., 2009b; 2013).",1. Introduction,[0],[0]
"On the downside, their use in learning tasks is not straightforward.",1. Introduction,[0],[0]
"Indeed, a large class of learning methods, such as SVM or PCA, requires a Hilbert structure on the descriptors space, which is not the case for the space of PDs.",1. Introduction,[0],[0]
"Actually, many simple operators of Rn, such as addition, average or scalar product, have no analogues in that space.",1. Introduction,[0],[0]
"Mapping PDs to vectors in Rn or in some infinite-dimensional Hilbert space is one possible approach to facilitate their use in discriminative settings.
",1. Introduction,[0],[0]
Related work.,1. Introduction,[0],[0]
"A series of recent contributions have proposed kernels for PDs, falling into two classes.",1. Introduction,[0],[0]
"The first class of methods builds explicit feature maps: One can, for instance, compute and sample functions extracted from PDs (Bubenik, 2015; Adams et al., 2017; Robins & Turner, 2016); sort the entries of the distance matrices of the PDs (Carrière et al., 2015); treat the PD points as roots of a complex polynomial, whose coefficients are concatenated (Fabio & Ferri, 2015).",1. Introduction,[0],[0]
"The second class of methods, which is more relevant to our work, defines implicitly feature maps by focusing instead on building kernels for PDs.",1. Introduction,[0],[0]
"For instance, Reininghaus et al. (2015) use solutions of the heat differential equation in the plane and compare them with the usual L2(R2) dot product.",1. Introduction,[0],[0]
"Kusano et al. (2016) handle a PD as a discrete measure on the plane, and follow by using kernel mean embeddings with Gaussian kernels—see Section 4 for precise definitions.",1. Introduction,[0],[0]
"Both kernels are provably stable, in the sense that the metric they induce in their respective reproducing kernel Hilbert space (RKHS) is bounded above by the distance between PDs.",1. Introduction,[0],[0]
"Although these kernels are injective, there is no evidence that their induced RKHS distances are discriminative and therefore follow the geometry of the bottleneck distances, which are more widely accepted distances to compare PDs.
Contributions.",1. Introduction,[0],[0]
"In this article, we use the sliced Wasserstein (SW) distance (Rabin et al., 2011) to define a new ker-
nel for PDs, which we prove to be both stable and discriminative.",1. Introduction,[0],[0]
"Specifically, we provide distortion bounds on the SW distance that quantify its ability to mimic bottleneck distances between PDs.",1. Introduction,[0],[0]
"This is in contrast to other kernels for PDs, which only focus on stability.",1. Introduction,[0],[0]
"We also propose a simple approximation algorithm to speed up the computation of that kernel, confirm experimentally its discriminative power and show that it outperforms experimentally both proposals of (Kusano et al., 2016) and (Reininghaus et al., 2015) in several supervised classification problems.",1. Introduction,[0],[0]
"We briefly review in this section relevant material on TDA, notably persistence diagrams, and technical properties of positive and negative definite kernel functions.",2. Background on TDA and Kernels,[0],[0]
"Persistent homology (Zomorodian & Carlsson, 2005; Edelsbrunner & Harer, 2008; Oudot, 2015) is a technique inherited from algebraic topology for computing stable signatures on real-valued functions.",2.1. Persistent Homology,[0],[0]
"Given f : X → R as input, persistent homology outputs a planar point set with multiplicities, called the persistence diagram of f and denoted by Dg f .",2.1. Persistent Homology,[0],[0]
See Figure 1 for an example.,2.1. Persistent Homology,[0],[0]
"To understand the meaning of each point in this diagram, it suffices to know that, to compute Dg f , persistent homology considers the family of sublevel sets of f , i.e. the sets of the form f−1((−∞, t]) for t ∈ R, and it records the topological events (e.g. creation or merge of a connected component, creation or filling of a loop, void, etc.) that occur in f−1((−∞, t]) as t ranges from −∞ to +∞. Then, each point p ∈",2.1. Persistent Homology,[0],[0]
"Dg f represents the lifespan of a particular topological feature (connected component, loop, void, etc.), with its creation and destruction times as coordinates.",2.1. Persistent Homology,[0],[0]
"See again Figure 1 for an illustration.
",2.1. Persistent Homology,[0],[0]
"For the interested reader, we point out that the mathematical tool used by persistent homology to track the topological events in the family of sublevel sets is homological algebra, which turns the parametrized family of sublevel sets into a parametrized family of vector spaces and linear maps.",2.1. Persistent Homology,[0],[0]
"Computing persistent homology then boils down to computing a family of bases for the vector spaces, which are compatible with the linear maps.
Distance between PDs.",2.1. Persistent Homology,[0],[0]
We now define the pth diagram distance between PDs.,2.1. Persistent Homology,[0],[0]
"Let p ∈ N and Dg1,Dg2 be two PDs.",2.1. Persistent Homology,[0],[0]
Let Γ : Dg1 ⊇ A → B ⊆ Dg2 be a partial bijection between Dg1 and Dg2.,2.1. Persistent Homology,[0],[0]
"Then, for any point x ∈ A, the cost of x is defined as c(x) := ‖x − Γ(x)‖p∞, and for any point y ∈ (Dg1 t Dg2) \",2.1. Persistent Homology,[0],[0]
"(A t B), the cost of y is defined as c′(y) := ‖y",2.1. Persistent Homology,[0],[0]
"− π∆(y)‖p∞, where π∆ is the projection onto the diagonal ∆ = {(x, x) | x ∈ R}.",2.1. Persistent Homology,[0],[0]
"The cost c(Γ)
is defined as: c(Γ) := ( ∑ x c(x) + ∑ y c ′(y))1/p.",2.1. Persistent Homology,[0],[0]
"We then define the pth diagram distance dp as the cost of the best partial bijection between the PDs:
dp(Dg1,Dg2) = inf Γ c(Γ).
",2.1. Persistent Homology,[0],[0]
"In the particular case p = +∞, the cost of Γ is defined as c(Γ)",2.1. Persistent Homology,[0],[0]
:= max{maxx δ(x) + maxy δ′(y)}.,2.1. Persistent Homology,[0],[0]
The corresponding distance d∞ is often called the bottleneck distance.,2.1. Persistent Homology,[0],[0]
One can show that dp → d∞ when p → +∞. A fundamental property of PDs is their stability with respect to (small) perturbations of their originating functions.,2.1. Persistent Homology,[0],[0]
"Indeed, the stability theorem (Bauer & Lesnick, 2015; Chazal et al., 2009a; 2016; Cohen-Steiner et al., 2007) asserts that for any f, g : X → R, we have
d∞(Dg f, Dg g) ≤ ‖f",2.1. Persistent Homology,[0],[0]
"− g‖∞, (1)
See again Figure 1 for an illustration.
",2.1. Persistent Homology,[0],[0]
"In practice, PDs can be used as descriptors for data via the choice of appropriate filtering functions f , e.g. distance to the data in the ambient space, eccentricity, curvature, etc.",2.1. Persistent Homology,[0],[0]
"The main strengths of the obtained descriptors are: (a) to be provably stable as mentioned previously; (b) to be invariant under reparametrization of the data; and (c) to encode information about the topology of the data, which is complementary and of an essentially different nature compared to geometric or statistical quantities.",2.1. Persistent Homology,[0],[0]
"These properties have made persistence diagrams useful in a variety of contexts, including the ones mentioned in the introduction of the paper.",2.1. Persistent Homology,[0],[0]
"For further details on persistent homology and on applications of PDs, the interested reader can refer e.g. to (Oudot, 2015) and the references therein.",2.1. Persistent Homology,[0],[0]
Positive Definite Kernels.,2.2. Kernel Methods,[0],[0]
"Given a set X , a function k : X × X → R is called a positive definite kernel if for all integers n, for all families x1, ..., xn of points in X , the matrix [k(xi, xj)]i,j is itself positive semi-definite.",2.2. Kernel Methods,[0],[0]
For brevity we will refer to positive definite kernels as kernels in the rest of the paper.,2.2. Kernel Methods,[0],[0]
"It is known that kernels generalize scalar products, in the sense that, given a kernel k, there exists a Reproducing Kernel Hilbert Space (RKHS) Hk and a feature map φ : X → Hk such that k(x1, x2) = 〈φ(x1), φ(x2)〉Hk .",2.2. Kernel Methods,[0],[0]
"A kernel k also induces a distance dk on X that can be computed as the Hilbert norm of the difference between two embeddings:
d2k(x1, x2) def. = k(x1, x1) +",2.2. Kernel Methods,[0],[0]
"k(x2, x2)− 2 k(x1, x2).
",2.2. Kernel Methods,[0],[0]
"We will be particularly interested in this distance, since one of the goals we will aim for will be that of designing a kernel k for persistence diagrams such that dk has low distortion with respect to d1.
",2.2. Kernel Methods,[0],[0]
Negative Definite and RBF Kernels.,2.2. Kernel Methods,[0],[0]
A standard way to construct a kernel is to exponentiate the negative of a Euclidean distance.,2.2. Kernel Methods,[0],[0]
"Indeed, the Gaussian kernel for vectors with parameter σ",2.2. Kernel Methods,[0],[0]
"> 0 does follow that template approach: kσ(x, y) = exp ( −‖x−y‖ 2
2σ2
) .",2.2. Kernel Methods,[0],[0]
"An important theo-
rem of Berg et al. (1984) (Theorem 3.2.2, p.74) states that such an approach to build kernels, namely setting
kσ(x, y) def. =",2.2. Kernel Methods,[0],[0]
"exp ( −f(x, y)
2σ2
) ,
for an arbitrary function f can only yield a valid positive definite kernel for all σ > 0",2.2. Kernel Methods,[0],[0]
"if and only if f is a negative semi-definite function, namely that, for all integers n, ∀x1, ..., xn ∈ X , ∀a1, ..., an ∈ Rn such that ∑ i ai = 0,∑
i,j aiajf(xi, xj) ≤ 0.",2.2. Kernel Methods,[0],[0]
"Unfortunately, as observed in Appendix A of Reininghaus et al. (2014), d1 is not negative semi-definite (it only suffices to sample a family of point clouds to observe experimentally that more often than not the inequality above will be violated for a particular weight vector a).",2.2. Kernel Methods,[0],[0]
"In this article, we use an approximation of d1 with the Sliced Wasserstein distance, which is provably negative semi-definite, and we use it to define a RBF kernel that can be easily tuned thanks to its bandwidth parameter σ.
2.3.",2.2. Kernel Methods,[0],[0]
"Wasserstein distance for unnormalized measures on R
The Wasserstein distance (Villani, 2009, §6) is a distance between probability measures.",2.2. Kernel Methods,[0],[0]
"For reasons that will become clear in the next section, we will focus on a variant of that distance: the 1-Wasserstein distance for nonnegative, not necessarily normalized, measures on the real line (Santambrogio, 2015, §2).",2.2. Kernel Methods,[0],[0]
"Let µ and ν be two nonnegative mea-
sures on the real line such that |µ| = µ(R) and |ν| = ν(R) are equal to the same number r.",2.2. Kernel Methods,[0],[0]
"We define the three following objects:
W(µ, ν) = inf P∈Π(µ,ν)",2.2. Kernel Methods,[0],[0]
"∫∫ R×R |x− y|P (dx,dy) (2)
Qr(µ, ν) =",2.2. Kernel Methods,[0],[0]
"r ∫ R |M−1(x)−N−1(x)|dx (3)
L(µ, ν) = inf f∈1−Lipschitz ∫ R f(x)[µ(dx)− ν(dx)]",2.2. Kernel Methods,[0],[0]
"(4)
where Π(µ, ν) is the set of measures on R2 with marginals µ and ν, and M−1 and N−1 the generalized quantile functions of the probability measures µ/r and ν/r respectively.
",2.2. Kernel Methods,[0],[0]
Proposition 2.1.,2.2. Kernel Methods,[0],[0]
We haveW = Qr = L. Additionally (i),2.2. Kernel Methods,[0],[0]
"Qr is negative definite on the space of measures of mass r; (ii) for any three positive measures µ, ν, γ such that |µ| = |ν|, we have L(µ+ γ, ν + γ) = L(µ, ν).
",2.2. Kernel Methods,[0],[0]
"Equation (2) is the generic Kantorovich formulation of optimal transport, which is easily generalized to other cost functions and spaces, the variant being that we consider an unnormalized mass by reflecting it directly in the set",2.2. Kernel Methods,[0],[0]
Π.,2.2. Kernel Methods,[0],[0]
The equality between (2) and (3) is only valid for probability measures on the real line.,2.2. Kernel Methods,[0],[0]
"Because the cost function | · | is homogeneous, we see that the scaling factor r can be removed when considering the quantile function and multiplied back.",2.2. Kernel Methods,[0],[0]
"The equality between (2) and (4) is due to the well known Kantorovich duality for a distance cost (Villani, 2009, Particular case 5.4) which can also be trivially generalized to unnormalized measures, proving therefore the main statement of the proposition.",2.2. Kernel Methods,[0],[0]
"The definition of Qr shows that the Wasserstein distance is the l1 norm of
rM−1 − rN−1, and is therefore a negative definite kernel (as the l1 distance between two direct representations of µ and ν as functions rM−1 and rN−1), proving point (i).",2.2. Kernel Methods,[0],[0]
"The second statement is immediate.
",2.2. Kernel Methods,[0],[0]
We conclude with an important practical remark: for two unnormalized uniform empirical measures µ = ∑n i=1,2.2. Kernel Methods,[0],[0]
"δxi
and ν",2.2. Kernel Methods,[0],[0]
= ∑n i=1,2.2. Kernel Methods,[0],[0]
"δyi of the same size, with ordered x1 ≤
· · · ≤ xn and y1 ≤ · · · ≤",2.2. Kernel Methods,[0],[0]
"yn, one has: L(µ, ν) =∑n i=1 |xi−yi| = ‖X−Y ‖1, whereX = (x1, ..., xn) ∈",2.2. Kernel Methods,[0],[0]
"Rn and Y = (y1, ..., yn) ∈ Rn.",2.2. Kernel Methods,[0],[0]
"In this section we define a new kernel between PDs, called the Sliced Wasserstein (SW) kernel, based on the Sliced Wasserstein metric of Rabin et al. (2011).",3. The Sliced Wasserstein Kernel,[0],[0]
"The idea underlying this metric is to slice the plane with lines passing through the origin, to project the measures onto these lines whereW is computed, and to integrate those distances over all possible lines.",3. The Sliced Wasserstein Kernel,[0],[0]
Formally: Definition 3.1.,3. The Sliced Wasserstein Kernel,[0],[0]
"Given θ ∈ R2 with ‖θ‖2 = 1, let L(θ) denote the line {λ θ | λ ∈ R}, and let πθ : R2 → L(θ) be the orthogonal projection onto L(θ).",3. The Sliced Wasserstein Kernel,[0],[0]
"Let Dg1,Dg2 be two PDs, and let µθ1 := ∑ p∈Dg1 δπθ(p) and µ θ 1∆ :=∑
p∈Dg1 δπθ◦π∆(p), and similarly for µ θ 2, where π∆ is the orthogonal projection onto the diagonal.",3. The Sliced Wasserstein Kernel,[0],[0]
"Then, the Sliced Wasserstein distance is defined as:
SW(Dg1,Dg2) def. =
1
2π ∫ S1 W(µθ1 + µθ2∆, µθ2 + µθ1∆)dθ.
",3. The Sliced Wasserstein Kernel,[0],[0]
"Note that, by symmetry, one can restrict on the half-circle [−π2 , π2 ] and normalize by π instead of 2π.",3. The Sliced Wasserstein Kernel,[0],[0]
"SinceQr is negative semi-definite, we can deduce that SW itself is negative semi-definite: Lemma 3.2.",3. The Sliced Wasserstein Kernel,[0],[0]
Let X be the set of bounded and finite PDs.,3. The Sliced Wasserstein Kernel,[0],[0]
"Then, SW is negative semi-definite on X .",3. The Sliced Wasserstein Kernel,[0],[0]
Proof.,3. The Sliced Wasserstein Kernel,[0],[0]
"Let n ∈ N∗, a1, ..., an ∈ R such that ∑ i ai = 0 and
Dg1, ...,Dgn ∈ X .",3. The Sliced Wasserstein Kernel,[0],[0]
"Given 1 ≤ i ≤ n, we let µ̃θi := µθi +∑ q∈Dgk,k 6=i δπθ◦π∆(q), µ̃ θ ij∆ := ∑ p∈Dgk,k 6=i,j
δπθ◦π∆(p) and d = ∑ i |Dgi|.",3. The Sliced Wasserstein Kernel,[0],[0]
"Then:∑
i,j
aiajW(µθi + µθj∆, µθj + µθi∆)
",3. The Sliced Wasserstein Kernel,[0],[0]
"= ∑ i,j aiajL(µθi + µθj∆, µθj + µθi∆)
",3. The Sliced Wasserstein Kernel,[0],[0]
"= ∑ i,j aiajL(µθi + µθj∆",3. The Sliced Wasserstein Kernel,[0],[0]
"+ µθij∆, µθj + µθi∆ + µθij∆)
= ∑",3. The Sliced Wasserstein Kernel,[0],[0]
"i,j aiajL(µ̃θi , µ̃θj ) = ∑ i,j aiajQd(µ̃θi , µ̃θj ) ≤ 0
",3. The Sliced Wasserstein Kernel,[0],[0]
"The result follows by linearity of integration.
",3. The Sliced Wasserstein Kernel,[0],[0]
"Hence, the theorem of Berg et al. (1984) allows us to define a valid kernel with:
kSW(Dg1,Dg2) def. = exp
( −SW(Dg1,Dg2)
2σ2
) .",3. The Sliced Wasserstein Kernel,[0],[0]
"(5)
Metric equivalence.",3. The Sliced Wasserstein Kernel,[0],[0]
"We now give the main theoretical result of this article, which states that SW is equivalent to d1.",3. The Sliced Wasserstein Kernel,[0],[0]
"This has to be compared with (Reininghaus et al., 2015) and (Kusano et al., 2016), which only prove stability and injectivity.",3. The Sliced Wasserstein Kernel,[0],[0]
"Our equivalence result states that the kSW, in addition to be stable and injective, also preserves the metric between PDs, which should intuitively lead to an improvement of the classification power.",3. The Sliced Wasserstein Kernel,[0],[0]
"This intuition is illustrated in Section 4 and Figure 4, where we show an improvement of classification accuracies on several benchmark applications.
",3. The Sliced Wasserstein Kernel,[0],[0]
Theorem 3.3.,3. The Sliced Wasserstein Kernel,[0],[0]
"Let X be the set of bounded PDs with cardinalities bounded by N ∈ N∗. Let Dg1,Dg2 ∈ X .",3. The Sliced Wasserstein Kernel,[0],[0]
"Then, one has:
d1(Dg1,Dg2)
2M ≤ SW(Dg1,Dg2) ≤ 2
√ 2d1(Dg1,Dg2),
where M = 1 + 2N(2N − 1).
",3. The Sliced Wasserstein Kernel,[0],[0]
Proof.,3. The Sliced Wasserstein Kernel,[0],[0]
"Let sθ : Dg1 ∪π∆(Dg2)→ Dg2 ∪π∆(Dg1) be the one-to-one bijection between Dg1 ∪ π∆(Dg2) and Dg2 ∪ π∆(Dg1) induced by W(µθ1 + µθ2∆, µθ2 + µθ1∆), and let s be the one-to-one bijection between Dg1 ∪ π∆(Dg2) and Dg2 ∪ π∆(Dg1) induced by the partial bijection achieving d1(Dg1,Dg2).
",3. The Sliced Wasserstein Kernel,[0],[0]
Upper bound.,3. The Sliced Wasserstein Kernel,[0],[0]
Recall that ‖θ‖2 = 1.,3. The Sliced Wasserstein Kernel,[0],[0]
"We have: W(µθ1 + µθ2∆, µθ2 + µθ1∆) = ∑ |〈p− sθ(p), θ〉|
≤ ∑ |〈p− s(p), θ〉| ≤ √ 2 ∑ ‖p− s(p)‖∞ ≤ 2 √
2d1(Dg1,Dg2),
where the sum is taken over all p ∈ Dg1 ∪ π∆(Dg2).",3. The Sliced Wasserstein Kernel,[0],[0]
"The upper bound follows by linearity.
",3. The Sliced Wasserstein Kernel,[0],[0]
Lower bound.,3. The Sliced Wasserstein Kernel,[0],[0]
"The idea is to use the fact that sθ is a piecewise-constant function of θ, and that it has at most 2+2N(2N −1) critical values Θ0, ...,ΘM in [−π2 , π2 ].",3. The Sliced Wasserstein Kernel,[0],[0]
"Indeed, it suffices to look at all θ such that 〈p1−p2, θ〉 = 0 for some p1, p2 in Dg1 ∪ π∆(Dg2) or Dg2 ∪ π∆(Dg1).",3. The Sliced Wasserstein Kernel,[0],[0]
"Then:∫ Θi+1
Θi
∑ |〈p− sθ(p), θ〉|dθ
= ∑ ‖p− sΘi(p)‖2 ∫ Θi+1",3. The Sliced Wasserstein Kernel,[0],[0]
"Θi |cos(∠(p− sΘi(p), θ))|dθ
≥ ∑ ‖p− sΘi(p)‖2(Θi+1 −Θi)2/2π
≥ (Θi+1 −Θi)2d1(Dg1,Dg2)/2π,
where the sum is again taken over all p ∈ Dg1 ∪ π∆(Dg2), and where the inequality used to lower bound the integral of the cosine is obtained by concavity.",3. The Sliced Wasserstein Kernel,[0],[0]
"The lower bound follows then from the Cauchy-Schwarz inequality.
",3. The Sliced Wasserstein Kernel,[0],[0]
"Note that the lower bound depends on the cardinalities of the PDs, and it becomes close to 0 if the PDs have a large number of points.",3. The Sliced Wasserstein Kernel,[0],[0]
"On the other hand, the upper bound is oblivious to the cardinality.",3. The Sliced Wasserstein Kernel,[0],[0]
"A corollary of Theorem 3.3 is that dkSW , the distance induced by kSW in its RKHS, is also equivalent to d1 in a broader sense: there exist continuous, positive and monotone functions g, h such that g(0) = h(0) = 0 and g ◦",3. The Sliced Wasserstein Kernel,[0],[0]
d1 ≤ dkSW ≤ h ◦,3. The Sliced Wasserstein Kernel,[0],[0]
d1.,3. The Sliced Wasserstein Kernel,[0],[0]
"When the condition on the cardinalities of PDs is relaxed, e.g. when we only assume the PDs to be finite and bounded, with no uniform bound, the feature map φSW associated to kSW remains continuous and injective w.r.t.",3. The Sliced Wasserstein Kernel,[0],[0]
d1.,3. The Sliced Wasserstein Kernel,[0],[0]
"This means that kSW can be turned into a universal kernel by considering exp(kSW) (cf Theorem 1 in (Kwitt et al., 2015)).",3. The Sliced Wasserstein Kernel,[0],[0]
"This can be useful in a variety of tasks, including tests on distributions of PDs.
",3. The Sliced Wasserstein Kernel,[0],[0]
Computation.,3. The Sliced Wasserstein Kernel,[0],[0]
"In practice, we propose to approximate kSW in O(N log(N)) time using Algorithm 1.",3. The Sliced Wasserstein Kernel,[0],[0]
"This algorithm first samples M directions in the half-circle S+1 ; it then computes, for each sample θi and for each PD Dg, the scalar products between the points of Dg and θi, to sort them next in a vector Vθi(Dg).",3. The Sliced Wasserstein Kernel,[0],[0]
"Finally, the `1-norm between the vectors is averaged over the sampled directions: SWM (Dg1,Dg2) = 1 M ∑M i=1 ‖Vθi(Dg1) − Vθi(Dg2)‖1.",3. The Sliced Wasserstein Kernel,[0],[0]
Note that one can easily adapt the proof of Lemma 3.2 to show that SWM is negative semi-definite by using the linearity of the sum.,3. The Sliced Wasserstein Kernel,[0],[0]
"Hence, this approximation remains a kernel.",3. The Sliced Wasserstein Kernel,[0],[0]
"If the two PDs have cardinalities bounded by N , then the running time of this procedure is O(MN log(N)).",3. The Sliced Wasserstein Kernel,[0],[0]
"This approximation of kSW is useful since, as shown in Section 4, we have observed empirically that just a few directions are sufficient to get good classification accuracies.",3. The Sliced Wasserstein Kernel,[0],[0]
"Note that the exact computation of kSW is also possible in O(N2log(N)) time using the algorithm described in (Carrière et al., 2017).",3. The Sliced Wasserstein Kernel,[0],[0]
"In this section, we compare kSW to kPSS and kPWG on several benchmark applications for which PDs have been proven useful.",4. Experiments,[0],[0]
We compare these kernels in terms of classification accuracies and computational cost.,4. Experiments,[0],[0]
"We review first our experimental setting, and then all our tasks.
",4. Experiments,[0],[0]
"Experimental setting All kernels are handled with the LIBSVM (Chang & Lin, 2011) implementation of C-SVM, and results are averaged over 10 runs on a 2.4GHz Intel Xeon E5530 Quad Core.",4. Experiments,[0],[0]
"The
Algorithm 1 Computation of SWM Input: Dg1 = {p11 ... p1N1}, Dg2 = {p21 ...",4. Experiments,[0],[0]
"p2N2},M .",4. Experiments,[0],[0]
Add π∆(Dg1) to Dg2 and vice-versa.,4. Experiments,[0],[0]
Let SWM = 0; θ = −π/2; s = π/M ; for i = 1 ...,4. Experiments,[0],[0]
"M do
Store the products 〈p1k, θ〉 in an array V1; Store the products 〈p2k, θ〉 in an array V2; Sort V1 and V2 in ascending order; SWM = SWM + s‖V1 − V2‖1; θ = θ + s;
end for Output: (1/π)SWM ;
",4. Experiments,[0],[0]
"cost factor C is cross-validated in the following grid: {0.001, 0.01, 0.1, 1, 10, 100, 1000}.",4. Experiments,[0],[0]
"Table 1 summarizes the number of labels, and the number of training and test instances for each task.",4. Experiments,[0],[0]
Figure 2 illustrate how we use PDs to represent complex data.,4. Experiments,[0],[0]
"We first describe the two baselines we considered, along with their parameterization, followed by our proposal.
PSS.",4. Experiments,[0],[0]
"The Persistence Scale Space kernel kPSS (Reininghaus et al., 2015) is defined as the scalar product of the two solutions of the heat diffusion equation with initial Dirac sources located at the PD points.",4. Experiments,[0],[0]
"It has the following closed form expression: kPSS(Dg1,Dg2) =
1 8πt ∑ p∈Dg1 ∑ q∈Dg2 exp ( −‖p−q‖ 2 8t )",4. Experiments,[0],[0]
"− exp ( −‖p−q̄‖ 2 8t ) , where q̄ = (y, x) is the symmetric of q = (x, y) along the diagonal.",4. Experiments,[0],[0]
"Since there is no clear heuristic on how to tune t, this parameter is chosen in the applications by ten-fold cross-validation with random 50%-50% training-test splits and with the following set of NPSS = 13 values: 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100, 500 and 1000.
PWG.",4. Experiments,[0],[0]
"Let K, p, ρ be positive parameters.",4. Experiments,[0],[0]
Let kρ be the Gaussian kernel with parameter ρ and associated RKHS Hρ.,4. Experiments,[0],[0]
"Let Dg1,Dg2 be two PDs, and let µ1 := ∑ x∈Dg1 arctan(Kd∞(x,∆) p)kρ(·, x) ∈",4. Experiments,[0],[0]
Hρ be the kernel mean embedding of Dg1 weigthed by the diagonal distances.,4. Experiments,[0],[0]
"Let µ2 be defined similarly.
",4. Experiments,[0],[0]
Let τ > 0.,4. Experiments,[0],[0]
The Persistence Weighted Gaussian kernel kPWG,4. Experiments,[0],[0]
"(Kusano et al., 2016; 2017) is defined as kPWG(Dg1,Dg2) = exp ( −‖µ1−µ2‖Hρ2τ2 ) , i.e. the Gaussian kernel with parameter τ on Hρ.",4. Experiments,[0],[0]
"The authors in (Kusano et al., 2016) provide heuristics to compute K, ρ and τ and give a rule of thumb to tune p.",4. Experiments,[0],[0]
"Hence, in the applications we select p according to the rule of thumb, and we use ten-fold cross-validation with random 50%-50% training-test splits to chose K, ρ and τ .",4. Experiments,[0],[0]
"The ranges of possible values is obtained by multiplying the values computed with the heuristics with the following range of 5 factors: 0.01, 0.1, 1, 10 and 100, leading to NPWG = 5× 5× 5 = 125 different sets of parameters.
",4. Experiments,[0],[0]
Parameters for kSW.,4. Experiments,[0],[0]
"The kernel we propose has only one parameter, the bandwidth σ in Eq.",4. Experiments,[0],[0]
"(5), which we choose using ten-fold cross-validation with random 50%- 50% training-test splits.",4. Experiments,[0],[0]
"The range of possible values is obtained by computing the squareroot of the median, the first and the last deciles of all SW(Dgi,Dgj) in the training set, then by multiplying these values by the following range of 5 factors: 0.01, 0.1, 1, 10 and 100, leading to NSW = 5× 3 = 15 possible values.
",4. Experiments,[0],[0]
Parameter Tuning.,4. Experiments,[0],[0]
"The bandwidth of kSW is, in practice, easier to tune than the parameters of its two competitors when using grid search.",4. Experiments,[0],[0]
"Indeed, as is the case for all infinitely divisible kernels, the Gram matrix does not need to be recomputed for each choice of σ, since it only suffices to compute all the Sliced Wasserstein distances between PDs in the training set once.",4. Experiments,[0],[0]
"On the contrary, neither kPSS nor kPWG share this property, and require recomputations for each hyperparameter choice.",4. Experiments,[0],[0]
Note however that this improvement may no longer hold if one uses other methods to tune parameters.,4. Experiments,[0],[0]
"For instance, using kPWG without cross-validation is possible with the heuristics given by the authors in (Kusano et al., 2016), and leads to smaller training times, but also to worse accuracies.",4. Experiments,[0],[0]
"Our first task, whose goal is to produce point classifiers for 3D shapes, follows that presented in (Carrière et al., 2015).
",4.1. 3D shape segmentation,[0],[0]
Data.,4.1. 3D shape segmentation,[0],[0]
"We use some categories of the mesh segmentation benchmark of Chen et al. (Chen et al., 2009), which contains 3D shapes classified in several categories (“airplane”, “human”, “ant”...).",4.1. 3D shape segmentation,[0],[0]
"For each category, our goal is to design a classifier that can assign, to each point in the shape, a
label that describes the relative location of that point in the shape.",4.1. 3D shape segmentation,[0],[0]
"For instance, possible labels are, for the human category, “head”, “torso”, “arm”...",4.1. 3D shape segmentation,[0],[0]
"To train classifiers, we compute a PD per point using the geodesic distance function to this point—see (Carrière et al., 2015) for details.",4.1. 3D shape segmentation,[0],[0]
"We use 1-dimensional persistent homology (0-dimensional would not be informative since the shapes are connected, leading to solely one point with coordinates (0,+∞) per PD).",4.1. 3D shape segmentation,[0],[0]
"For each category, the training set contains one hundredth of the points of the first five 3D shapes, and the test set contains one hundredth of the points of the remaining shapes in that category.",4.1. 3D shape segmentation,[0],[0]
Points in training and test sets are evenly sampled.,4.1. 3D shape segmentation,[0],[0]
See Figure 2.,4.1. 3D shape segmentation,[0],[0]
"Here, we focus on comparison between PDs, and not on achieving state-of-the-art results.",4.1. 3D shape segmentation,[0],[0]
"It has been proven that PDs bring complementary information to classical descriptors in this task—see (Carrière et al., 2015), hence reinforcing their discriminative power with appropriate kernels is of great interest.",4.1. 3D shape segmentation,[0],[0]
"Finally, since data points are in R3, we set the p parameter of kPWG to 5.
Results.",4.1. 3D shape segmentation,[0],[0]
Classification accuracies are given in Table 2.,4.1. 3D shape segmentation,[0],[0]
"For most categories, kSW outperforms competing kernels by a significant margin.",4.1. 3D shape segmentation,[0],[0]
The variance of the results over the run is also less than that of its competitors.,4.1. 3D shape segmentation,[0],[0]
"However, training times are not better in general.",4.1. 3D shape segmentation,[0],[0]
"Hence, we also provide the results for an approximation of kSW with 10 directions.",4.1. 3D shape segmentation,[0],[0]
"As one can see from Table 2 and from Figure 3, this approximation leaves the accuracies almost unchanged, while the training times become comparable with the ones of the
other competitors.",4.1. 3D shape segmentation,[0],[0]
"Moreover, according to Figure 3, using even less directions would slightly decrease the accuracies, but still outperform the competitors performances, while decreasing even more the training times.",4.1. 3D shape segmentation,[0],[0]
"In our second experiment, we use synthetized data.",4.2. Orbit recognition,[0],[0]
"The goal is to retrieve parameters of dynamical system orbits, following an experiment proposed in (Adams et al., 2017).
",4.2. Orbit recognition,[0],[0]
Data.,4.2. Orbit recognition,[0],[0]
"We study the linked twist map, a discrete dynamical system modeling fluid flow.",4.2. Orbit recognition,[0],[0]
"It was used in (Hertzsch et al., 2007) to model flows in DNA microarrays.",4.2. Orbit recognition,[0],[0]
"Its orbits can be computed given a parameter r > 0 and initial positions (x0, y0) ∈",4.2. Orbit recognition,[0],[0]
"[0, 1]× [0, 1] as follows:
{ xn+1 =",4.2. Orbit recognition,[0],[0]
xn,4.2. Orbit recognition,[0],[0]
"+ ryn(1− yn) mod 1 yn+1 = yn + rxn+1(1− xn+1) mod 1
",4.2. Orbit recognition,[0],[0]
"Depending on the values of r, the orbits may exhibit very different behaviors.",4.2. Orbit recognition,[0],[0]
"For instance, as one can see in Figure 2, when r is 2, there seems to be no interesting topological features in the orbit, while voids form when r is 1.",4.2. Orbit recognition,[0],[0]
"Following (Adams et al., 2017), we use 5 different parameters r = 2.5, 3.5, 4, 4.1, 4.3, that act as labels.",4.2. Orbit recognition,[0],[0]
"For each parameter, we generate 100 orbits with 1000 points and random initial positions.",4.2. Orbit recognition,[0],[0]
"We then compute the PDs of the distance functions to the point clouds with the GUDHI
library (The GUDHI Project, 2015) and we use them (in all homological dimensions) to produce an orbit classifier that predicts the parameter values, by training over a 70%-30% training-test split of the data.",4.2. Orbit recognition,[0],[0]
"Since data points are in R2, we set the p parameter of kPWG to 4.
Results.",4.2. Orbit recognition,[0],[0]
"Since the PDs contain thousands of points, we use kernel approximations to speed up the computation of the Gram matrices.",4.2. Orbit recognition,[0],[0]
"In order for the approximation error to be bounded by 10−3, we use an approximation of kSW with 6 directions (as one can see from Figure 3, this has a small impact on the accuracy), we approximate kPWG with 1000 random Fourier features (Rahimi & Recht, 2008), and we approximate kPSS using Fast Gauss Transform (Morariu et al., 2009) with a normalized error of 10−10.",4.2. Orbit recognition,[0],[0]
One can see from Table 2 that the accuracy is increased a lot with kSW.,4.2. Orbit recognition,[0],[0]
"Concerning training times, there is also a large improvement since we tune the parameters with grid search.",4.2. Orbit recognition,[0],[0]
"Indeed, each Gram matrix needs not be recomputed for each parameter when using kSW.",4.2. Orbit recognition,[0],[0]
"Our last experiment is inspired from (Reininghaus et al., 2015) and (Li et al., 2014).",4.3. Texture classification,[0],[0]
"We use the OUTEX00000 data base (Ojala et al., 2002) for texture classification.
Data.",4.3. Texture classification,[0],[0]
"PDs are obtained for each texture image by computing first the sign component of CLBP descriptors (Guo et al., 2010) with radius R = 1 and P = 8 neighbors for each image, and then compute the persistent homology of this descriptor using the GUDHI library (The GUDHI Project, 2015).",4.3. Texture classification,[0],[0]
See Figure 2.,4.3. Texture classification,[0],[0]
"Note that, contrary to the experiment of (Reininghaus et al., 2015), we do not downsample the images to 32× 32 images, but keep the original 128 × 128 images.",4.3. Texture classification,[0],[0]
"Following (Reininghaus et al., 2015), we restrict the focus to 0-dimensional persistent homology.",4.3. Texture classification,[0],[0]
We also use the first 50%-50% training-test split given in the database to produce classifiers.,4.3. Texture classification,[0],[0]
"Since data points are in R2, we set the p parameter of kPWG to 4.
",4.3. Texture classification,[0],[0]
Results We use the same approximation procedure as in Section 4.2.,4.3. Texture classification,[0],[0]
"According to Figure 3, even though the approximation of SW is rough, this has again a small impact on the accuracy, while reducing the training time by a significant margin.",4.3. Texture classification,[0],[0]
"As one can see from Table 2, using kPSS leads to almost state-of-the-art results (Ojala et al., 2002; Guo et al., 2010), closely followed by the accuracies of kSW and kPWG.",4.3. Texture classification,[0],[0]
"The best timing is given by kSW, again because we use grid search.",4.3. Texture classification,[0],[0]
"Hence, kSW almost achieves the best result, and its training time is better than the ones of its competitors, due to the grid search parameter tuning.
",4.3. Texture classification,[0],[0]
Metric Distortion.,4.3. Texture classification,[0],[0]
"To illustrate the equivalence theorem, we also show in Figure 4 a scatter plot where each point
represents the comparison of two PDs taken from the Airplane segmentation data set.",4.3. Texture classification,[0],[0]
Similar plots can be obtained with the other datasets considered here.,4.3. Texture classification,[0],[0]
"For all points, the x-axis quantifies the first diagram distance d1 for that pair, while the y-axis is the logarithm of the RKHS distance induced by either kSW, kPSS, kPWG or a Gaussian kernel directly applied to d1, to obtain comparable quantities.",4.3. Texture classification,[0],[0]
We use the parameters given by the cross-validation procedure described above.,4.3. Texture classification,[0],[0]
"One can see that the distances induced by kSW are less spread than the others, suggesting that the metric induced by kSW is more discriminative.",4.3. Texture classification,[0],[0]
"Moreover the distances given by kSW and the Gaussian kernel on d1 exhibit the same behavior, suggesting that kSW is the best natural equivalent of a Gaussian kernel for PDs.",4.3. Texture classification,[0],[0]
"In this article, we introduce the Sliced Wasserstein kernel, a new kernel for PDs that is provably equivalent to the first diagram distance between PDs.",5. Conclusion,[0],[0]
"We provide fast algorithms to approximate it, and show on several datasets substantial improvements in accuracy and training times (when tuning parameters is done with grid search) over competing kernels.",5. Conclusion,[0],[0]
"A particularly appealing property of that kernel is that it is infinitely divisible, substantially facilitating the tuning of parameters through cross validation.
",5. Conclusion,[0],[0]
Acknowledgements.,5. Conclusion,[0],[0]
We thank the anonymous referees for their insightful comments.,5. Conclusion,[0],[0]
SO was supported by ERC grant Gudhi and by ANR project TopData.,5. Conclusion,[0],[0]
MC was supported by a chaire de l’IDEX Paris Saclay.,5. Conclusion,[0],[0]
"Persistence diagrams (PDs) play a key role in topological data analysis (TDA), in which they are routinely used to describe topological properties of complicated shapes.",abstractText,[0],[0]
PDs enjoy strong stability properties and have proven their utility in various learning contexts.,abstractText,[0],[0]
"They do not, however, live in a space naturally endowed with a Hilbert structure and are usually compared with non-Hilbertian distances, such as the bottleneck distance.",abstractText,[0],[0]
"To incorporate PDs in a convex learning pipeline, several kernels have been proposed with a strong emphasis on the stability of the resulting RKHS distance w.r.t.",abstractText,[0],[0]
perturbations of the PDs.,abstractText,[0],[0]
"In this article, we use the Sliced Wasserstein approximation of the Wasserstein distance to define a new kernel for PDs, which is not only provably stable but also discriminative (with a bound depending on the number of points in the PDs)",abstractText,[0],[0]
w.r.t.,abstractText,[0],[0]
the first diagram distance between PDs.,abstractText,[0],[0]
"We also demonstrate its practicality, by developing an approximation technique to reduce kernel computation time, and show that our proposal compares favorably to existing kernels for PDs on several benchmarks.",abstractText,[0],[0]
Sliced Wasserstein Kernel for Persistence Diagrams,title,[0],[0]
Establishing maps (e.g. pointwise correspondences) across object collections is a fundamental problem spanning many scientific domains.,1. Introduction,[0],[0]
"High-quality maps facilitating information propagation and transformation are key to applications ranging from 3D reconstruction with partial scans (Huber & Hebert, 2001), data-driven geometry completion and reconstruction (Pauly et al., 2005), texture transfer (Schreiner et al., 2004; Kraevoy & Sheffer, 2004), to comparative biology (Boyer et al., 2011; Gao et al., 2017), joint dataanalysis (Huang et al., 2011; Kim et al., 2012; Wang et al., 2013; 2014; Huang et al., 2014), and data exploration and organization (Kim et al., 2012; Huang et al., 2014).
",1. Introduction,[0],[0]
High quality object maps are generally difficult to compute.,1. Introduction,[0],[0]
"Prior work on map computation focused on optimizing maps between pairs of objects; see (van Kaick et al., 2010) for
1Department of Computer Science, The University of Texas at Austin 2Department of Statistics, The University of Chicago 3Institute for Interdisciplinary Information Sciences, Tsinghua Univesity.",1. Introduction,[0],[0]
"Correspondence to: Chandrajit Bajaj <bajaj@cs.utexas.edu>, Qixing Huang",1. Introduction,[0],[0]
"<huangqx@cs.utexas.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"a standard survey and (Kim et al., 2011; Mandad et al., 2017) for some recent advances.",1. Introduction,[0],[0]
"Despite the significant progress, state-of-the-art techniques tends to hit a barrier on the quality of maps that are computed in a pairwise manner.",1. Introduction,[0],[0]
"Building upon the availability of big-data, a recent line of research (Kim et al., 2012; Nguyen et al., 2011; Ovsjanikov et al., 2012; Huang et al., 2012; Huang & Guibas, 2013; Huang et al., 2014; Chen et al., 2014; Zhou et al., 2015a;b; Shen et al., 2016; Leonardos et al., 2017; Huang et al., 2017) considered computing many pairwise maps jointly among a collection of objects.",1. Introduction,[0],[0]
"The promise of these approaches hinges upon the observation that one way to obtain a high quality pairwise map between dissimilar objects is to choose a path connecting these objects but consisting of consecutive similar shapes: maps between similar objects are typically of higher quality, and so is the resulted composed map.",1. Introduction,[0],[0]
"From a regularization perspective, joint map computation leverages the generic consistency of a network of maps among multiple objects, in which composition of maps along cycles are expected to be close to the identity map.
",1. Introduction,[0],[0]
"However, the performance of these data-driven approaches relies predominantly on the homogeneity of the object collection, i.e. the input objects fall into the same category or sub-category (e.g. Chairs, Cars, and Human models).",1. Introduction,[0],[0]
"In the presence of heterogeneous data, where the input objects fall into multiple underlying categories, applying existing data-driven approaches without the category label information tend to produce unsatisfactory results.",1. Introduction,[0],[0]
"In this setting, even though existing methods are able to suppress the noise in intra-cluster maps within a single cluster, jointly computed maps for the entire object collection leads are often significantly worse.",1. Introduction,[0],[0]
One explanation is that high fraction of incorrect inter-cluster maps tends to “contaminate” the regularization effect of intra-cluster maps.,1. Introduction,[0],[0]
A natural resolution is to employ a two-stage cascadic strategy that identifies the underlying clusters before computing the intra- and inter-cluster maps.,1. Introduction,[0],[0]
"Unfortunately, such clustering requires accurate quantification of the object similarities, which is a difficult problem in its own right.",1. Introduction,[0],[0]
"Meanwhile, the error produced in the clustering stage is unlikely remedied by the consistency-based regularization.
",1. Introduction,[0],[0]
"In this paper, we propose to solve the mapping and clustering problems simultaneously.",1. Introduction,[0],[0]
"Instead of explicitly relying on certain pairwise similarity and/or map distortion
scores, we identify the underlying clusters based on the consistency of intra- and inter-cluster maps, inspired by the observation that maps tend to be more consistent along cycles within a cluster than across clusters.",1. Introduction,[0],[0]
"This discrepancy has been observed in many different contexts, and appears to be a consequence of the energy landscape of almost all optimization-based pairwise matching algorithms.",1. Introduction,[0],[0]
"The matching energy functional between objects in the same underlying cluster tends to have simple energy landscapes with easily identifiable global optimums, resulting in fairly cycle-consistent intra-cluster maps; in contrast, the highly non-convex energy landscape for dissimilar objects from different clusters leads to more “random” maps due to random initialization and/or multiple strong local minimums, for which cycle-consistency is much less often observed.",1. Introduction,[0],[0]
This map consistency argument is the foundation of our simultaneous mapping and clustering (SMAC) algorithm.,1. Introduction,[0],[0]
We validate the map consistency argument through a motivating example (see Figure 1) on a real dataset from SHREC07,1.1. Motivating Example,[0],[0]
"Watertight benchmark (Giorgi et al., 2007).",1.1. Motivating Example,[0],[0]
"This dataset consists of 38 shapes: the first 18 are Human models and the remaining 20 are Fourleg models (e.g., Horses and Dogs).",1.1. Motivating Example,[0],[0]
"Each shape is represented as a discrete metric space with 1024 sample points generated from farthest-point sampling (Eldar et al., 1997).",1.1. Motivating Example,[0],[0]
"We compute pairwise blended intrinsic maps (Kim et al., 2011) for all objects in this collection and use these maps to compute two similarity scores for each object pair: a map distortion score that measures the squared sum of geodesic distortions across all pointpairs (c.f. (Bronstein et al., 2006)), and a cycle-consistency score that is the median value (among other options) of the distortion scores of all 3-cycles to which the pair belongs, where the distortion of a 3-cycle is defined as the squared sum of the geodesic distances between each point and its image propagated along the 3-cycle.
",1.1. Motivating Example,[0],[0]
Figure 1 illustrates the distributions of the map distortion scores (Left) and the cycle-consistency scores (Right) on the 38 models.,1.1. Motivating Example,[0],[0]
"The cycle-consistency scores clearly reveal the
underlying cluster structure and in fact better separates the two clusters of models (Human vs. Fourleg) than the mapdistortion scores (intra-cluster blocks in the right figure are darker in blue).",1.1. Motivating Example,[0],[0]
"The superior cluster separation is verified by comparing the results of spectral clustering (Ng et al., 2002; Lei & Rinaldo, 2015) using the two similarity scores: spectral clustering based on the cycle-consistency scores recovers the two underlying clusters perfectly, whereas the same procedure using the map distortion scores incorrectly puts two Fourleg models in the cluster of Human models.",1.1. Motivating Example,[0],[0]
This motivating example illustrates the effectiveness and superiority of the map consistency score as a quantification of object similarity.,1.1. Motivating Example,[0],[0]
"Motivated by the example above, we propose an algorithm for simultaneous mapping and clustering (SMAC).",1.2. Approach Overview,[0],[0]
"Our SMAC algorithm takes as input (i) an object collection that falls into multiple clusters, and (ii) some noisy maps precomputed between object pairs, and outputs the underlying clusters together with improved maps between all pairs of objects.",1.2. Approach Overview,[0],[0]
"Our SMAC algorithm builds upon the equivalence between map consistency and the low-rank property of a data matrix with consistent maps in its blocks (c.f. (Huang & Guibas, 2013)).",1.2. Approach Overview,[0],[0]
"We show that this low-rank property still holds in the setting of multiple disjoint collections of consistent intra-cluster maps, though the rank is expected to be higher due to multiple clusters.",1.2. Approach Overview,[0],[0]
"Based on this observation, the first step of our approach simultaneously recovers the underlying clusters and intra-cluster maps by spectral decomposition.",1.2. Approach Overview,[0],[0]
We show that properly “rounding off” the leading eigenvectors recovers the ground-truth clusters and intracluster maps in a single pass.,1.2. Approach Overview,[0],[0]
We then construct inter-cluster maps from the recovered intra-cluster maps.,1.2. Approach Overview,[0],[0]
"Our theoretical analysis establishes sharp exact recovery conditions for both steps under a fairly general noise model, using some novel tight L∞-stability bounds for eigen-decompositions.",1.2. Approach Overview,[0],[0]
"Joint object matching, i.e., simultaneously estimating maps among a collection of objects, is an emerging field across many scientific problems.",1.3. Related Works,[0],[0]
"Earlier works use combinatorial optimizations (Nguyen et al., 2011; Kim et al., 2012; Huang et al., 2012).",1.3. Related Works,[0],[0]
"More recent works (Huang & Guibas, 2013; Huang et al., 2014; Chen et al., 2014; Wang & Singer, 2013) rely on convex or non-convex optimization techniques.",1.3. Related Works,[0],[0]
"However, all these methods assume that the underlying object collection is homogeneous (all objects belong to a single category).",1.3. Related Works,[0],[0]
"For a heterogeneous object collection where the objects fall into multiple distinctive clusters, existing methods usually rarely succeed in producing both high-quality intra- and inter-cluster maps.
",1.3. Related Works,[0],[0]
Clustering and in particular spectral clustering is another well-studied topic.,1.3. Related Works,[0],[0]
"We refer to (Lei & Rinaldo, 2015; Rohe et al., 2011) for some recent advances and to (Filippone et al., 2008; Luxburg, 2007; Fortunato, 2010) for surveys.",1.3. Related Works,[0],[0]
"Our approach falls into the general category of graph-based clustering, but the pairwise information we utilize is of “functional” rather than “scalar” nature.",1.3. Related Works,[0],[0]
"Instead of the more common approach that derives affinity scores from the pairwise maps for clustering, our SMAC algorithm discovers the cluster structure based purely on the consistency of pairwise maps and demonstrates improved empirical performance.",1.3. Related Works,[0],[0]
"This strategy is reminiscent of heterogeneous multireference alignment (Boumal et al., 2017) and simultaneous alignment and classification (Lederman & Singer, 2016) for synchronization problems in Cryo-Electron Microscopy; in this context, our residue-based clustering strategy is closest in nature to learning group actions (Gao et al., 2016).
",1.3. Related Works,[0],[0]
Our approach relies on tight L∞-type bounds on leading eigenvectors of perturbed matrices.,1.3. Related Works,[0],[0]
"Though the stability of eigenvalues and eigenspaces are well-studied, element-wise eigenvector stability appears to be a much harder problem; see recent survey (O’Rourke et al., 2016).",1.3. Related Works,[0],[0]
We introduce new stability bounds to tackle this technical difficulty.,1.3. Related Works,[0],[0]
"We use lower bold letters a, b, c,u,v,w, · · · to denote vectors, and upper letters A,B,C, · · · for matrices.",1.4. Mathematical Notation,[0],[0]
"For a block matrix X ∈ Rn1m1×n2m2 , we use Xij ∈ Rm1×m2 to denote its ij-th block; the ij-th element of a matrix A is denoted as aij .",1.4. Mathematical Notation,[0],[0]
With ⊗ we denote the Kronecker product.,1.4. Mathematical Notation,[0],[0]
"For a symmetric matrix A ∈ Rn×n, we always sort the eigenvalues in non-increasing order, i.e., λ1(A) ≥ · · · ≥ λn(A).",1.4. Mathematical Notation,[0],[0]
"Matrix norms ‖ · ‖F , ‖ · ‖1, ‖ · ‖2, and ‖ · ‖∞ will be used for a matrix A ∈ Rn1×n2 , of which ‖ · ‖2 = σmax(A) is the maximum singular value of A, and the spectral norm ‖ · ‖2 is sometimes simplified as ‖ · ‖.",1.4. Mathematical Notation,[0],[0]
We denote Pm for the set of permutation matrices of dimension m×m.,1.4. Mathematical Notation,[0],[0]
"For simplicity, we focus on describing and analyzing our algorithm under the setting where pair-wise maps are given by permutations.",2. Algorithm,[0],[0]
"In Section 4, we show how to modify the algorithm for partially similar objects.
",2. Algorithm,[0],[0]
We first describe the problem setup.,2. Algorithm,[0],[0]
"Consider n objects S = {S1, · · · , Sn} each represented by m points.",2. Algorithm,[0],[0]
"With G = (S, E) we denote an observation graph among S. An initial map X inij ∈",2. Algorithm,[0],[0]
Pm is pre-computed on each edge (,2. Algorithm,[0],[0]
"i, j) ∈ E using an off-the-shelf pairwise object matching algorithm from Si to Sj .",2. Algorithm,[0],[0]
"We also assume the objects in S are partitioned into k ≥ 2 clusters, but k is unknown.",2. Algorithm,[0],[0]
"Our goal is to identify the underlying clusters, and in the mean-
Algorithm 1 PermSMAC: Simultaneously mapping and clustering Input: Observation graph G = (S, E) and initial pairwise
maps X inij , (i, j) ∈ E Output:",2. Algorithm,[0],[0]
"Underlying clusters S = c1 ∪ · · · ∪ ck and opti-
mized pairwise maps Xij , 1 ≤",2. Algorithm,[0],[0]
"i, j ≤ n 1: {Step 1} Simultaneously compute the intra-cluster
maps and extract the underlying clusters: 2: {Step 1.1} Form data matrix based on (1).",2. Algorithm,[0],[0]
"3: {Step 1.2} Compute the critical value r =
argmax 2≤i≤nm
λi−λi+1",2. Algorithm,[0],[0]
"λi+λi+1 .
",2. Algorithm,[0],[0]
4: {Step 1.3} LetU ∈ Rnm×r store the leading r eigenvectors of X .,2. Algorithm,[0],[0]
"Compute pair-wise maps X?ij by solving (2) 5: {Step 1.4} Use fij(X?ij) as the affinity score and apply single-linkage clustering to obtain the underlying clusters 6: {Step 2} compute the inter-cluster maps by solving (6)
while improve all pairwise maps between objects in S. As a basis for identifying the underlying clusters, we assume that the intra-cluster maps are more accurate (in terms of cycle-consistency) than inter-cluster maps.
",2. Algorithm,[0],[0]
Our algorithm proceeds in two major steps.,2. Algorithm,[0],[0]
The first step simultaneously extracts the underlying clusters and computes intra-cluster maps.,2. Algorithm,[0],[0]
The second step then computes inter-cluster maps.,2. Algorithm,[0],[0]
Now we introduce these two steps in details.,2. Algorithm,[0],[0]
Algorithm 1 provides the pseudo code.,2. Algorithm,[0],[0]
"Motivated from prior works for map synchronization (Pachauri et al., 2013; Shen et al., 2016), we use a block data matrix X ∈ Rnm×nm to encode the input maps:
Xij =
{ (X inij − 1m11
T ) (i, j) ∈ E 0",2.1. Step I: Extract underlying clusters and compute intra-cluster maps.,[0],[0]
"otherwise
(1)
Our approach is motivated by the empirical observation that the leading eigen-vectors of X reveal the underlying clusters and simultaneously denoise intra-cluster maps if intra-cluster maps are more accurate than inter-cluster maps.",2.1. Step I: Extract underlying clusters and compute intra-cluster maps.,[0],[0]
"We provide the algorithmic details below; an analysis is provided in Section 3.
",2.1. Step I: Extract underlying clusters and compute intra-cluster maps.,[0],[0]
"Given the data matrix, we first estimates the number of stable eigen-vectors as
r =",2.1. Step I: Extract underlying clusters and compute intra-cluster maps.,[0],[0]
argmax m≤i≤nm λi,2.1. Step I: Extract underlying clusters and compute intra-cluster maps.,[0],[0]
"− λi+1 |λi|+ |λi+1| .
",2.1. Step I: Extract underlying clusters and compute intra-cluster maps.,[0],[0]
"Here we search within the range of [m,nm], as we expect multiple underlying clusters.",2.1. Step I: Extract underlying clusters and compute intra-cluster maps.,[0],[0]
"Let U ∈ Rnm×r store the
top r eigen-vectors of X , and divide U into n matrices U1, . . .",2.1. Step I: Extract underlying clusters and compute intra-cluster maps.,[0],[0]
", Un of shape m×r such that U = (UT1 , · · · , UTn )T .",2.1. Step I: Extract underlying clusters and compute intra-cluster maps.,[0],[0]
"We then compute the estimated map X?ij along each edge (i, j) ∈ E by solving a linear assignment problem, i.e.,
X?ij = min X∈Pm fij(X), fij(X) := ‖X ·",2.1. Step I: Extract underlying clusters and compute intra-cluster maps.,[0],[0]
"Ui − Uj‖2F (2)
for all 1 ≤",2.1. Step I: Extract underlying clusters and compute intra-cluster maps.,[0],[0]
"i, j ≤",2.1. Step I: Extract underlying clusters and compute intra-cluster maps.,[0],[0]
n. Note that (2) admits an exact solution via linear assignment.,2.1. Step I: Extract underlying clusters and compute intra-cluster maps.,[0],[0]
"In fact,
X?ij = argmin X∈Pm ‖XUi",2.1. Step I: Extract underlying clusters and compute intra-cluster maps.,[0],[0]
"− Uj‖2F (3)
= argmin X∈Pm (‖Ui‖2F + ‖Uj‖2F",2.1. Step I: Extract underlying clusters and compute intra-cluster maps.,[0],[0]
"− 2〈XUi, Uj〉) (4)
= argmax X∈Pm",2.1. Step I: Extract underlying clusters and compute intra-cluster maps.,[0],[0]
"〈X,UjUTi 〉.",2.1. Step I: Extract underlying clusters and compute intra-cluster maps.,[0],[0]
"(5)
Intuitively, UjUTi provides an approximation of the underlying map Xij , and the linear assignment procedure projects this approximation onto the space of permutations.
",2.1. Step I: Extract underlying clusters and compute intra-cluster maps.,[0],[0]
"For clustering, we treat the residual score fij(X?ij), 1 ≤ i, j ≤ n",2.1. Step I: Extract underlying clusters and compute intra-cluster maps.,[0],[0]
"as the distance measure between Si and Sj , and apply single-linkage clustering (Gower & Ross, 1969) to obtain the underlying clusters.",2.1. Step I: Extract underlying clusters and compute intra-cluster maps.,[0],[0]
We set k =,2.1. Step I: Extract underlying clusters and compute intra-cluster maps.,[0],[0]
"[ rm−1 ] as the number of desired clusters.
",2.1. Step I: Extract underlying clusters and compute intra-cluster maps.,[0],[0]
"Empirically, we found that when the input inter-cluster maps are inaccurate, the quality of estimated inter-cluster maps appear to be much more noisy than estimated intra-cluster maps.",2.1. Step I: Extract underlying clusters and compute intra-cluster maps.,[0],[0]
"This motivates us to re-estimate inter-cluster maps as a second step, described in Section 2.2 below.",2.1. Step I: Extract underlying clusters and compute intra-cluster maps.,[0],[0]
We estimate the inter-cluster maps between each pair of clusters independently.,2.2. Step II: Estimate inter-cluster maps.,[0],[0]
"Specifically, consider two clusters cs and ct .",2.2. Step II: Estimate inter-cluster maps.,[0],[0]
Let Sis ∈ cs and Sit ∈,2.2. Step II: Estimate inter-cluster maps.,[0],[0]
"ct be a pair of objects selected from each cluster, respectively.",2.2. Step II: Estimate inter-cluster maps.,[0],[0]
"We optimize the inter-cluster map X interst , represented as a pairwise object map Xisit , by solving the following linear assignment:
X interst = argmin X∈Pm ∑ i∈cs,j∈ct,(i,j)∈E ‖X −XjitX inij Xiis‖1
= argmax X∈Pm ∑ i∈cs,j∈ct,(i,j)∈E 〈X,XjitX inij Xiis〉.",2.2. Step II: Estimate inter-cluster maps.,[0],[0]
"(6)
Note that it is possible to jointly optimize X interst among all pairs of clusters.",2.2. Step II: Estimate inter-cluster maps.,[0],[0]
"However, since the number of clusters is usually significantly smaller than the size of each cluster, we found the gain of doing so is insignificant.",2.2. Step II: Estimate inter-cluster maps.,[0],[0]
We first describe our noise model in Section 3.1.,3. Analysis,[0],[0]
"We then analyze our method under this model and present the exact
recovery conditions for both underlying clusters and the pairwise maps in Section 3.2.",3. Analysis,[0],[0]
Our analysis is based on a set of new stability bounds of eigen-decompositions.,3. Analysis,[0],[0]
The proofs of all Lemma’s and Theorem’s with technical details can be referred to in the supplemental material.,3. Analysis,[0],[0]
"We consider two models, one for the pairwise map and the other one for the observation graph and the underlying cluster structure.
",3.1. Model for Analysis,[0],[0]
Map model.,3.1. Model for Analysis,[0],[0]
"We generalize the map model described in (Shen et al., 2016) to multiple clusters: Suppose there are k underlying clusters.",3.1. Model for Analysis,[0],[0]
"With cs, 1 ≤ s ≤ k",3.1. Model for Analysis,[0],[0]
we denote the vertex indices of the s-th cluster.,3.1. Model for Analysis,[0],[0]
"In other words, we have {1, · · · , n} = c1 ∪ c2 ∪ · · · ∪ ck.",3.1. Model for Analysis,[0],[0]
"Given an observation graph, the input pairwise maps are independent, and they follow
X inij =",3.1. Model for Analysis,[0],[0]
"{ Im with probability ηij UPm with probability 1− ηij
where UPm denotes a random permutation matrix satisfying
E[UPm ] = 1
m 11T .",3.1. Model for Analysis,[0],[0]
"(7)
ηij depends on the edge type: ηij = p if (i, j) is an intra-cluster edge, i.e., (i, j) ∈ E ∩ (∪1≤s≤kcs × cs), and ηij = q if (i, j) is an inter-cluster edge, i.e., (i, j) ∈ E ∩ (∪1≤s6=t≤kcs × ct).",3.1. Model for Analysis,[0],[0]
We assume p > q. Remark 1.,3.1. Model for Analysis,[0],[0]
"Note that one can also generalize the model by assuming there exist underlying permutations Pi, 1 ≤ i ≤ n, so that the ground-truth map Xij = PjPTi .",3.1. Model for Analysis,[0],[0]
"Nevertheless, it turns out that the two models are identical (Shen et al., 2016) .",3.1. Model for Analysis,[0],[0]
"Hence we adopt the simpler form for convenience.
",3.1. Model for Analysis,[0],[0]
Model for the observation graph and clusters.,3.1. Model for Analysis,[0],[0]
"To obtain concise and interpretable exact recovery conditions, we are particularly interested in analyzing our algorithm when the observation graph and underlying clusters are generated from the following established model: assume n = n0k, and the size of each underlying cluster |cs| = n0, 1 ≤ s ≤ k; the observation graph is generated from the Erdős-Rényi G(n, t) with edge selection probability t.",3.1. Model for Analysis,[0],[0]
"However, the stability bounds we introduce in the supplemental material can be used for analyzing more general noisy models, e.g., sizes of the underlying clusters are different.",3.1. Model for Analysis,[0],[0]
We begin by analyzing the leading eigenvalues and eigenvectors of E[X] to gain insights on the necessary conditions for map and cluster recovery.,3.2. Map and Cluster Recovery,[0],[0]
"To make our discussion more general, we first assume the underlying cluster and the observation graph are fixed.",3.2. Map and Cluster Recovery,[0],[0]
"Consider then the following (p, q)-
reweighted normalized adjacency matrix A(p, q) ∈",3.2. Map and Cluster Recovery,[0],[0]
"Rn×n:
(A(p, q))ij =  ",3.2. Map and Cluster Recovery,[0],[0]
"p (i, j) ∈ E ∩ ∪1≤s≤m(cs × cs),q",3.2. Map and Cluster Recovery,[0],[0]
"(i, j) ∈ E ∩ ∪1≤s 6=t≤m(cs × ct). 0",3.2. Map and Cluster Recovery,[0],[0]
"otherwise
(8) It is clear that
E[X] = A(p, q)⊗",3.2. Map and Cluster Recovery,[0],[0]
"(Im − 1
m 11T )
and thus the non-zero eigenvalues of E[X] are non-zero eigenvalues ofA(p, q) with multiplicitym−1.",3.2. Map and Cluster Recovery,[0],[0]
"Furthermore, let ( 1√
m 1, Hm) be an orthonormal basis for Rm.",3.2. Map and Cluster Recovery,[0],[0]
"Then
the leading k(m − 1) eigenvectors of E[X] are given by Sk ⊗Hm.",3.2. Map and Cluster Recovery,[0],[0]
"This leads two conditions on the eigenvalues and eigenvectors of A(p, q) for map and cluster recovery:
• Eigenvalue separation.",3.2. Map and Cluster Recovery,[0],[0]
"Since our method leverages the largest eigengap, we assume that λk(A(p, q))",3.2. Map and Cluster Recovery,[0],[0]
"− λk+1(A(p, q)) has the largest eigengap.",3.2. Map and Cluster Recovery,[0],[0]
"Define
γ =
max 1≤i6=k≤n−1
λi(A(p, q))− λi+1(A(p, q))
λk(A(p, q))− λk+1(A(p, q)) ,
Then a necessary condition for map recovery is γ < 1.
",3.2. Map and Cluster Recovery,[0],[0]
• Eigenvector separation.,3.2. Map and Cluster Recovery,[0],[0]
We further assume that the underlying clusters can be recovered by reading off the rows of Sk.,3.2. Map and Cluster Recovery,[0],[0]
"Formally, consider rows of Sk as coordinates of the corresponding objects, and define
dintra = max 1≤s≤k max i,j∈cs
‖(ei − ej)TSk‖, (9)
dinter = min 1≤s<t≤k min i∈cs,j∈ct
‖(ei − ej)TSk‖. (10)
dintra and dinter essentially measure the maximum distance within each cluster and the minimum distance between different clusters, respectively.",3.2. Map and Cluster Recovery,[0],[0]
"Thus, a necessary condition for cluster recovery is that dintra < µ · dinter for some small constant µ.
Under these two conditions, it is easy to see that when X ≈ E[X], we have Ui ≈ (eTi Sk) ⊗ Hm and UjUTi",3.2. Map and Cluster Recovery,[0],[0]
"≈ (eTj SkS T k ei)(Im− 1m11
T ).",3.2. Map and Cluster Recovery,[0],[0]
"It follows that both the underlying clusters and intra-cluster maps can be exactly recovered.
",3.2. Map and Cluster Recovery,[0],[0]
These two separation conditions are quite general.,3.2. Map and Cluster Recovery,[0],[0]
"In fact, they hold for the noisy model described in Section 3.1.",3.2. Map and Cluster Recovery,[0],[0]
"To gain some further insight, one can show that (c.f. (Le et al., 2017)) with high probability
λ1(A(p, q))",3.2. Map and Cluster Recovery,[0],[0]
=  q + p−qk +O( 1√ nt ),3.2. Map and Cluster Recovery,[0],[0]
i = 1 p−q k +O( 1√ nt ) 2 ≤,3.2. Map and Cluster Recovery,[0],[0]
i ≤ k,3.2. Map and Cluster Recovery,[0],[0]
"O( 1√
nt ) k + 1 ≤",3.2. Map and Cluster Recovery,[0],[0]
"i ≤ n
indicating that γ = o(1).",3.2. Map and Cluster Recovery,[0],[0]
"Moreover, under this model
A(p, q)",3.2. Map and Cluster Recovery,[0],[0]
"≈ (p− q)Ik ⊗ (11T ) + q(11T ).
",3.2. Map and Cluster Recovery,[0],[0]
"If p and q are well-separated, the top k eigenvectors of A(p, q) approximate Ik⊗1, meaning dintra ≈ 0 and dinter ≈ 1.",3.2. Map and Cluster Recovery,[0],[0]
"Now we formally state the exact recovery conditions:
Theorem 3.1.",3.2. Map and Cluster Recovery,[0],[0]
(Intra-Cluster and Map Recovery),3.2. Map and Cluster Recovery,[0],[0]
Assume t = Ω( log(n)n ).,3.2. Map and Cluster Recovery,[0],[0]
Consider the noise model described in Section 3.1.,3.2. Map and Cluster Recovery,[0],[0]
"There exists an absolute constant cintra such that PermSMAC recovers the underlying intra-cluster maps and the underlying clusters with high probability if
p− q ≥ cintrak √ log(n)
nt .",3.2. Map and Cluster Recovery,[0],[0]
"(11)
Remark 2.",3.2. Map and Cluster Recovery,[0],[0]
Note that the gap between p and q is used to ensure the recovery of the underlying clusters.,3.2. Map and Cluster Recovery,[0],[0]
"Moreover, the recovery rate matches the information theoretic lower bound established in (Chen et al., 2016) up to O( √ log(n), indicating the tightness of our condition for PermSMAC.
",3.2. Map and Cluster Recovery,[0],[0]
The following theorem provides an exact recovery condition for the inter-cluster maps.,3.2. Map and Cluster Recovery,[0],[0]
"Compared with the previous lower bound, the lower bound on q for inter-cluster map recovery is significantly cruder.",3.2. Map and Cluster Recovery,[0],[0]
"This shows the advantage of recovering inter-cluster maps as a separate step.
",3.2. Map and Cluster Recovery,[0],[0]
Theorem 3.2.,3.2. Map and Cluster Recovery,[0],[0]
"There exists an absolute constant cinter > 0, so that when q ≥ cinterk √ log(n)
n2t ,
PermSMAC recovers the underlying inter-cluster maps with high probability.",3.2. Map and Cluster Recovery,[0],[0]
In this section we extend the algorithm to handle partially (as opposed to fully) similar objects.,4. Partial Matching,[0],[0]
Each input object Si (1 ≤ i ≤ n) in this setting has mi ∈ N+,4. Partial Matching,[0],[0]
"points, where the mi’s vary across the collection.",4. Partial Matching,[0],[0]
"Consequently, the pairwise maps X inij ∈ {0, 1}mj×mi are no longer permutation matrices since some rows and columns may only contain zero elements.",4. Partial Matching,[0],[0]
"We propose the following modified algorithm for SMAC in this partial matching setting.
",4. Partial Matching,[0],[0]
Step I: Extract underlying clusters and compute intracluster maps.,4. Partial Matching,[0],[0]
"Forming the data matrix and leading eigenvector computation stay the same as in the full matching setting, except we replace X inij − 1m11
T by X inij in (1).",4. Partial Matching,[0],[0]
The first difference occurring in the partial matching setting is that we cannot apply (2) to obtain pair-wise maps and affinity scores for clustering.,4. Partial Matching,[0],[0]
Our strategy is to apply single linkage clustering on the rows of U .,4. Partial Matching,[0],[0]
"Specifically, the distance measure between two points p, p′ is given by
‖up − up′‖2, where up is the corresponding row of p in U .",4. Partial Matching,[0],[0]
The number of output clusters is set as r. Each output cluster of this single-linkage procedure collects a set of matched points among the input objects.,4. Partial Matching,[0],[0]
We merge two clusters if the objects they belong to overlap.,4. Partial Matching,[0],[0]
"Suppose we finally obtain k clusters c1, · · · , ck.",4. Partial Matching,[0],[0]
"For each cluster ci, we introduce a binary matrix Yi ∈ {0, 1}ni×ri , whose columns encode the enclosed point clusters.",4. Partial Matching,[0],[0]
Then it is easy to see that the blocks of YiY Ti describe intra-cluster maps.,4. Partial Matching,[0],[0]
"Note that ri = m in the full setting, but in the partial setting ri is usually larger than max1≤i≤nmi, due to partial similarity.
",4. Partial Matching,[0],[0]
Step II: Compute inter-cluster maps.,4. Partial Matching,[0],[0]
"In the partial setting, we encode the inter-cluster map from cluster cs to cluster ct as a matrix Xst ∈ {0, 1}rt×rs .",4. Partial Matching,[0],[0]
"Consider a object pair (i, j) ∈ E , where i ∈ cs and j ∈ ct.",4. Partial Matching,[0],[0]
"With Ei,s and Ej,t we denote the index matrices that extract the corresponding blocks in Ys and Yt.",4. Partial Matching,[0],[0]
"It is easy to see that the entries Y Tt Ej,tX in ij E T",4. Partial Matching,[0],[0]
"i,sYs provide cues for the inter-cluster map Xst.",4. Partial Matching,[0],[0]
"Similar to the full map setting, we compute
Cst = ∑
i∈cs,j∈ct,(i,j)∈E
Y Tt Ej,tX in ij E T",4. Partial Matching,[0],[0]
"i,sYs.
",4. Partial Matching,[0],[0]
"Since the inter-cluster maps may not be permutation matrices either, we apply a simple thresholding to obtain the inter-cluster maps:
Xst =",4. Partial Matching,[0],[0]
"Cst > βst,
where βst is set as 0.9 times the maximum element of Cst in our experiments.",4. Partial Matching,[0],[0]
"In this section, we evaluate our approach on both synthetic (Section 5.1) and real datasets (Section 5.2 and Section 5.3).",5. Experimental Results,[0],[0]
"For baseline comparison, we consider state-of-the-art approaches for clustering and joint mapping in each domain.",5. Experimental Results,[0],[0]
We apply the model described in Section 3 to generate the synthetic data sets for our experiments.,5.1. Experimental Results on Synthetic Instances,[0],[0]
"Below we summarize how the procedure depends on the model parameters:
• The observation graph G. We employ a standard twocommunity stochastic block model (Abbe et al., 2016) which enables us to fully control the vertex degree and the spectral gap.",5.1. Experimental Results on Synthetic Instances,[0],[0]
"We use this model to generate three observation graphs G1,G2,G3.",5.1. Experimental Results on Synthetic Instances,[0],[0]
"All of them have n = 300 vertices, but the vertex degrees and spectral gaps vary.",5.1. Experimental Results on Synthetic Instances,[0],[0]
"Specifically, G1 is the clique graph.",5.1. Experimental Results on Synthetic Instances,[0],[0]
"G2 is a sparse graph, whose average vertex degree is 50 and the spectral gap is 0.1.",5.1. Experimental Results on Synthetic Instances,[0],[0]
"The average vertex degree and spectral gap of G3 are 50 and 0.5, respectively.",5.1. Experimental Results on Synthetic Instances,[0],[0]
"In
our experiment, we treat G1 as the default observation graph.",5.1. Experimental Results on Synthetic Instances,[0],[0]
"We also study the influence of the observation graphs on the performance of our algorithm.
",5.1. Experimental Results on Synthetic Instances,[0],[0]
• Number of clusters k.,5.1. Experimental Results on Synthetic Instances,[0],[0]
"Without loss of generality, we allocate each object into a underlying cluster with probability 1k .",5.1. Experimental Results on Synthetic Instances,[0],[0]
"For each observation graph, we generate and fix one underlying cluster throughout our experiments.
",5.1. Experimental Results on Synthetic Instances,[0],[0]
"• Other parameters m, γ, p, q.",5.1. Experimental Results on Synthetic Instances,[0],[0]
We fix the number of points on each object as m = 30.,5.1. Experimental Results on Synthetic Instances,[0],[0]
"In the partial matching setting, we follow the protocol (Chen et al., 2014) to generate the input objects so that the expected size of each object is mγ.",5.1. Experimental Results on Synthetic Instances,[0],[0]
"We sample the ratio of correct inter-cluster maps q = exp(− i10 ), 15 ≤ i ≤ 50.",5.1. Experimental Results on Synthetic Instances,[0],[0]
"Since p > q, we sample the ratio of correct intra-cluster maps so that p− q = i100 , 1 ≤ i ≤ 30.
",5.1. Experimental Results on Synthetic Instances,[0],[0]
"We now study the empirical phase transition curves when varying p and q under different k, γ and observation graphs.
",5.1. Experimental Results on Synthetic Instances,[0],[0]
Varying k.,5.1. Experimental Results on Synthetic Instances,[0],[0]
"The first two rows of Figure 2 show the phase transition curves of map and cluster recovery for k = 2, 4, 6.",5.1. Experimental Results on Synthetic Instances,[0],[0]
"Across all configurations, our approach tolerates a significant portion of noise in the input maps.",5.1. Experimental Results on Synthetic Instances,[0],[0]
"The fraction of noise that our approach can handle reduces as k increases, which is consistent with the exact recovery conditions in Section 3.",5.1. Experimental Results on Synthetic Instances,[0],[0]
"In addition, phase transitions with respect to mapping recovery and cluster recovery roughly align.",5.1. Experimental Results on Synthetic Instances,[0],[0]
"The subtle differences are two-fold: when p and q are close, cluster recovery breaks as there is no cue for clustering; likewise, map recovery breaks when q approaches 0.
",5.1. Experimental Results on Synthetic Instances,[0],[0]
Versus mapping only.,5.1. Experimental Results on Synthetic Instances,[0],[0]
"Figure 2 compares our approach to state-of-the-art map recovery technique (Huang & Guibas, 2013).",5.1. Experimental Results on Synthetic Instances,[0],[0]
"SMAC clearly exhibits a clear advantage in the regime, where q is small and p is significantly larger than q.",5.1. Experimental Results on Synthetic Instances,[0],[0]
"This is expected through our exact recovery condition.
",5.1. Experimental Results on Synthetic Instances,[0],[0]
Varying observation graph.,5.1. Experimental Results on Synthetic Instances,[0],[0]
"Figure 3 shows phase transition curves of map and cluster recovery when varying the observation graphs (k = 2, γ = 1).",5.1. Experimental Results on Synthetic Instances,[0],[0]
Our approach tolerates a larger fraction of incorrect maps for larger vertex degrees.,5.1. Experimental Results on Synthetic Instances,[0],[0]
"Moreover, when vertex degrees are comparable, a small spectral gap means higher recovery rate.
",5.1. Experimental Results on Synthetic Instances,[0],[0]
Varying γ.,5.1. Experimental Results on Synthetic Instances,[0],[0]
Figure 4 shows the phase transition curves when varying the overlapping ratio γ.,5.1. Experimental Results on Synthetic Instances,[0],[0]
"We again show three configurations, i.e., γ = 1, γ = 0.8 and γ = 0.6.",5.1. Experimental Results on Synthetic Instances,[0],[0]
"Still, our approach can tolerate a large rate of noise in the input maps.",5.1. Experimental Results on Synthetic Instances,[0],[0]
"Moreover, the rate reduces as γ becomes smaller.",5.1. Experimental Results on Synthetic Instances,[0],[0]
"This is expected, as low overlapping ratio introduces weak signal for mapping and cluster recovery.",5.1. Experimental Results on Synthetic Instances,[0],[0]
We proceed to evaluate SMAC on 3D shapes.,5.2. Experimental Results on 3D Shapes,[0],[0]
We consider two datasets for this task.,5.2. Experimental Results on 3D Shapes,[0],[0]
"The first dataset collects four categories of 3D shapes from SHREC07-Watertight (Giorgi et al., 2007), namely, Human, Fourleg, Armadillo and Teddy.",5.2. Experimental Results on 3D Shapes,[0],[0]
These categories appear to have both similar global structures and local geometric details.,5.2. Experimental Results on 3D Shapes,[0],[0]
"However, the intercategory variability is salient.",5.2. Experimental Results on 3D Shapes,[0],[0]
The second dataset is more fine-grained.,5.2. Experimental Results on 3D Shapes,[0],[0]
"It has 10 underlying shape collections from FAUST training dataset (Bogo et al., 2014), where each collection consists of different poses of the same human subject (10 poses per collection).",5.2. Experimental Results on 3D Shapes,[0],[0]
"For evaluating shape maps, we follow the protocol of (Kim et al., 2011) by collecting statistics on the geodesic distortion of predicted correspondences with respect to human annotated feature correspondences.
",5.2. Experimental Results on 3D Shapes,[0],[0]
Mapping performance.,5.2. Experimental Results on 3D Shapes,[0],[0]
Figure 5(c) plots the accuracy of predicted correspondences of our approach versus the input.,5.2. Experimental Results on 3D Shapes,[0],[0]
"For a detailed assessment, we separate the statistics of intra-cluster maps and inter-cluster maps.",5.2. Experimental Results on 3D Shapes,[0],[0]
"We consider two approaches for baseline comparison: the first applies (Huang & Guibas, 2013) to the entire dataset, and the second applies (Huang & Guibas, 2013) to each category in isolation then applies the third step of our approach to compute intercluster maps.",5.2. Experimental Results on 3D Shapes,[0],[0]
The second baseline may be considered as a performance upper bound.,5.2. Experimental Results on 3D Shapes,[0],[0]
"Our proposed SMAC is significantly better than mapping without clustering (which is seriously affected by the noise in inter-cluster maps) and competitive against the second baseline.
",5.2. Experimental Results on 3D Shapes,[0],[0]
Clustering performance.,5.2. Experimental Results on 3D Shapes,[0],[0]
"As shown in Table 1, our approach correctly identifies all underlying clusters in SHREC07 and FAUST.",5.2. Experimental Results on 3D Shapes,[0],[0]
We also applied two baseline clustering approaches on the same dataset.,5.2. Experimental Results on 3D Shapes,[0],[0]
"The first approach performs k-means on the spectral shape descriptors (Rustamov, 2007).",5.2. Experimental Results on 3D Shapes,[0],[0]
"This approach only yields 84.6% and 72.0%, respectively.",5.2. Experimental Results on 3D Shapes,[0],[0]
The second approach utilizes the mapping distortion as an affinity measure and applies spectral clustering.,5.2. Experimental Results on 3D Shapes,[0],[0]
"This approach yields 94.9% and 74.0%, respectively, which are better than the first baseline.",5.2. Experimental Results on 3D Shapes,[0],[0]
"However, our approach is still better, which shows the advantage of using the cycle-consistency constraint for clustering.",5.2. Experimental Results on 3D Shapes,[0],[0]
"Finally, we evaluate our approach on two datasets of 2D images.",5.3. Experimental Results on 2D Images,[0],[0]
The first dataset (Figure 6(Left)) consists of 600 internet images of Notre Dame.,5.3. Experimental Results on 2D Images,[0],[0]
"These images naturally fall into 3 categories, each of which collects images from a similar camera pose (Snavely et al., 2006).",5.3. Experimental Results on 2D Images,[0],[0]
"The second dataset (Figure 6(Right)) collects 600 internet images of 4 landmark churches in Europe (Amiens Cathedral (200 im-
ages), York Minster (200 images), Duomo (100 images) and Westminster Abbey (100 images)).",5.3. Experimental Results on 2D Images,[0],[0]
"As inter-cluster maps do not make much sense here, we only evaluate clustering results and intra-cluster maps in this experiment.",5.3. Experimental Results on 2D Images,[0],[0]
"We sample 400 SIFT features (Lowe, 2004) for each image and apply SIFT flow (Liu et al., 2011) to establish pairwise correspondences between the features.",5.3. Experimental Results on 2D Images,[0],[0]
"We manually mark feature correspondences for evaluation.
",5.3. Experimental Results on 2D Images,[0],[0]
Mapping performance.,5.3. Experimental Results on 2D Images,[0],[0]
Figure 6 compares our approach with the two baseline approaches introduced in Section 5.2.,5.3. Experimental Results on 2D Images,[0],[0]
The relative performance is consistent.,5.3. Experimental Results on 2D Images,[0],[0]
"Specifically, due to the small-overlapping region across different clusters, intercluster maps are rather noisy, so applying joint-mapping directly leads to sub-optimal results.",5.3. Experimental Results on 2D Images,[0],[0]
"In addition, our approach is competitive against the approach of computing intra-cluster and inter-cluster maps in a sequential manner.
",5.3. Experimental Results on 2D Images,[0],[0]
Clustering performance.,5.3. Experimental Results on 2D Images,[0],[0]
"Finally, we evaluate our approach in terms of clustering accuracy.",5.3. Experimental Results on 2D Images,[0],[0]
"We choose two base-
line approaches, where the first baseline approach performs k-means on image descriptors.",5.3. Experimental Results on 2D Images,[0],[0]
"In this case, we employ GIST (Oliva & Torralba, 2001).",5.3. Experimental Results on 2D Images,[0],[0]
The second baseline uses the residual of SIFT flow as the affinity score for clustering.,5.3. Experimental Results on 2D Images,[0],[0]
"As shown in Table 1, our approach leads to a clustering accuracy of 99.3% and 96.1% on Notre Dame and Church, respectively.",5.3. Experimental Results on 2D Images,[0],[0]
"They are higher than those of the top performing baselines, i.e., 94.5% and 92.1%, respectively.",5.3. Experimental Results on 2D Images,[0],[0]
We have introduced SMAC for simultaneously computing consistent maps across a heterogeneous data collection and identifying the underlying clusters.,6. Conclusions,[0],[0]
The key idea is to leverage the higher self-consistency within intra-cluster maps than inter-cluster maps.,6. Conclusions,[0],[0]
Enforcing this variation of consistency allows us to denoise the input maps in a sequential manner while simultaneously identifying the underlying cluster structures.,6. Conclusions,[0],[0]
"The approach is based on spectral decomposition, for which we provided tight exact recovery conditions for both the input maps and the underling clusters.",6. Conclusions,[0],[0]
"Experimental results on synthetic data sets justify our exact recovery conditions, and experimental results on real data sets demonstrate the efficacy of our approach.
",6. Conclusions,[0],[0]
Acknowledgement.,6. Conclusions,[0],[0]
"This research by Chandrajit Bajaj was supported in part by a grant from the NIH, R01 GM117594.",6. Conclusions,[0],[0]
"Tingran Gao acknowledges support from Simons Math+X Investigators Award 400837, DARPA D15AP00109, and NSF IIS 1546413.",6. Conclusions,[0],[0]
"Qixing Huang would like to acknowledge support for this research from NSF DMS-1700234, NSF CIP-1729486, and NSF IIS-1618648.",6. Conclusions,[0],[0]
"We introduce a principled approach for simultaneous mapping and clustering (SMAC) for establishing consistent maps across heterogeneous object collections (e.g., 2D images or 3D shapes).",abstractText,[0],[0]
"Our approach takes as input a heterogeneous object collection and a set of maps computed between some pairs of objects, and outputs a homogeneous object clustering together with a new set of maps possessing optimal intraand inter-cluster consistency.",abstractText,[0],[0]
Our approach is based on the spectral decomposition of a data matrix storing all pairwise maps in its blocks.,abstractText,[0],[0]
We additionally provide tight theoretical guarantees for the accuracy of SMAC under established noise models.,abstractText,[0],[0]
We also demonstrate the usefulness of our approach on synthetic and real datasets.,abstractText,[0],[0]
SMAC: Simultaneous Mapping and Clustering Using Spectral Decompositions,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2347–2356, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
"Applications using social media data, such as reviews, discussion posts, and (micro) blogs are becoming increasingly popular.",1 Introduction,[1.0],"['Applications using social media data, such as reviews, discussion posts, and (micro) blogs are becoming increasingly popular.']"
"We observed from our collaborations with social science and health science researchers that in a typical application, the researcher first need to obtain a set of posts of a particular topic that he/she wants to study, e.g., a political issue.",1 Introduction,[0],[0]
Keyword search is often used as the first step.,1 Introduction,[1.0],['Keyword search is often used as the first step.']
"However, that is not sufficient due to low precision and low recall.",1 Introduction,[0],[0]
A post containing the keyword “politics” may not be a political post while a post that does not contain the keyword may be a political post.,1 Introduction,[0],[0]
"Thus,
text classification is needed to make more sophisticated decisions to improve accuracy.
",1 Introduction,[0],[0]
"For classification, the user first manually labels a set of relevant posts (positive data) about the political issue and irrelevant posts (negative data) not about the political issue and then builds a classifier by running a learning algorithm, e.g. SVM or naïve Bayes.",1 Introduction,[0],[0]
"However, the resulting classifier may not be satisfactory.",1 Introduction,[0],[0]
There may be many reasons.,1 Introduction,[0],[0]
"One key reason we observed is that the labeled negative training data is not fully representative of the negative test data.
",1 Introduction,[1.0000000244181064],['One key reason we observed is that the labeled negative training data is not fully representative of the negative test data.']
"Let the user-interested topic be P (positive), and the set of all other irrelevant topics discussed in a social media source be T = {T1, T2, …, Tn}, which forms the negative data.",1 Introduction,[1.0],"['Let the user-interested topic be P (positive), and the set of all other irrelevant topics discussed in a social media source be T = {T1, T2, …, Tn}, which forms the negative data.']"
n is usually large.,1 Introduction,[0],[0]
"However, due to the labor-intensive effort of manual labeling, the user can label only a certain number of training posts.",1 Introduction,[0],[0]
Then the labeled negative training posts may cover only a small number of irrelevant topics S of T (S ⊆ T) as negative.,1 Introduction,[0],[0]
"Further, due to the highly dynamic nature of social media, it is probably impossible to label all possible negative topics.",1 Introduction,[0],[0]
"In testing, when posts of other negative topics in T−S show up, their classification can be unpredictable.",1 Introduction,[0],[0]
"For example, in an application, the training data has no negative examples about sports.",1 Introduction,[0],[0]
"However, in testing, some sports posts show up.",1 Introduction,[0],[0]
"These unexpected sports posts may be classified arbitrarily, which results in low classification accuracy.",1 Introduction,[0],[0]
"In this paper, we aim to solve this problem.
",1 Introduction,[0],[0]
"In machine learning, this problem is called covariate shift, a type of sample selection bias.",1 Introduction,[1.0],"['In machine learning, this problem is called covariate shift, a type of sample selection bias.']"
"In classic machine learning, it is assumed that the training and testing data are drawn from the same distribution.",1 Introduction,[0],[0]
"However, this assumption may not hold in practice such as in our case above, i.e., the training and the test distributions are different (Heckman 1979; Shimodaira 2000; Zadrozny 2004; Huang et al. 2007; Sugiyama et al. 2008; Bickel et al. 2009).",1 Introduction,[0],[0]
"In general, the sample selection bias problem is not solvable because the two
2347
distributions can be arbitrarily far apart from each other.",1 Introduction,[0.9999999905999303],"['In general, the sample selection bias problem is not solvable because the two 2347 distributions can be arbitrarily far apart from each other.']"
Various assumptions were made to solve special cases of the problem.,1 Introduction,[0],[0]
One main assumption was that the conditional distribution of the class given a data instance is the same in the training and test data sets (Shimodaira 2000; Huang et al. 2007; Bickel et al. 2009).,1 Introduction,[0],[0]
"This gives the covariate shift problem.
",1 Introduction,[0],[0]
"In this paper, we focus on a special case of the covariate shift problem.",1 Introduction,[0],[0]
"We assume that the covariate shift problem occurs mainly in the negative training and test data, and no or minimum covariate shift exists in the positive training and test data.",1 Introduction,[0],[0]
"This assumption is reasonable because the user knows the type of posts/documents that s/he is looking for and can label many of them.
",1 Introduction,[0],[0]
"Following the notations in (Bickel et al. 2009), our special case of the covariate shift problem can be stated formally as follows: let the set of training examples be {(x1, y1), (x2, y2), …, (xk, yk)}, where xi is the data/feature vector and yi is the class label of xi.",1 Introduction,[1.0],"['Following the notations in (Bickel et al. 2009), our special case of the covariate shift problem can be stated formally as follows: let the set of training examples be {(x1, y1), (x2, y2), …, (xk, yk)}, where xi is the data/feature vector and yi is the class label of xi.']"
"Let the set of test cases be {xk+1, xk+2, …, xn}, which have no class labels.",1 Introduction,[0],[0]
"Since we are interested in binary classification, yi is either 1 (positive class) or -1",1 Introduction,[0],[0]
(negative class).,1 Introduction,[0],[0]
The labeled training data and the unseen test data have the same target conditional distribution p(y|x) and the marginal distributions of the positive data in both the training and testing are also the same.,1 Introduction,[0],[0]
"But the marginal distributions of the negative data in the training and testing are different, i.e., 𝑝!(𝐱!)",1 Introduction,[0],[0]
"≠ 𝑝!(𝐱!), where L, T, and – represent the labeled training data, test data, and the negative class respectively.
",1 Introduction,[0],[0]
Existing methods for addressing the covariate shift problem basically work as follows (see the Related Work section).,1 Introduction,[0],[0]
"First, they estimate the bias of the training data based on the given test data using some statistical techniques.",1 Introduction,[0],[0]
"Then, a classifier is trained on a weighted version of the original training set based on the estimated bias.",1 Introduction,[0],[0]
"Requiring the test data to be available in training is, however, a major weakness.",1 Introduction,[0],[0]
"In the social media post classification setting, the system needs to constantly classify the incoming data.",1 Introduction,[0],[0]
"It is infeasible to perform training constantly.
",1 Introduction,[0],[0]
"In this paper, we propose a novel learning technique that does not need the test data to be available during training due to the specific nature of our problem, i.e., the positive training data does not have the covariate shift issue.
",1 Introduction,[0],[0]
"One obvious solution to this problem is oneclass classification (Schölkopf et al. 1999; Tax and Duin, 1999a), i.e., one-class SVM.",1 Introduction,[0],[0]
"We simply discard the negative training posts/documents
completely because they have the covariate shift problem.",1 Introduction,[0],[0]
"Although this is a valid solution, as we will see in the evaluation section, the models built based on one-class SVM perform poorly.",1 Introduction,[0],[0]
"Although it is conceivable to use an unsupervised method such clustering, SVD (Alter et al., 2000) or LDA (Blei et al., 2003), supervised learning usually give much higher accuracy.
",1 Introduction,[0],[0]
"In our proposed method, instead of performing supervised learning in the original document space based on n-grams, we perform learning in a similarity space.",1 Introduction,[0],[0]
"Thus, the key novelty of the method is the transformation from the original document space (DS) to a center-based similarity space (CBS).",1 Introduction,[0],[0]
"In the new space, the covariate shift problem is significantly mitigated, which enables us to build more accurate classifiers.",1 Introduction,[0],[0]
"The reason for this is that in CBS based learning the vectors in the similarity space enable SVM (which is the learning algorithm that we use) to find a good boundary of the positive class data based on similarity and to separate it from all possible negative class data, including those negative data that is not represented in training.",1 Introduction,[0],[0]
"We will explain this in greater detail in Section 3.5 after we present the proposed algorithm, which we call CBS-L (for CBS Learning).
",1 Introduction,[0],[0]
"This paper makes three contributions: First, it formulates a special case of the covariate shift problem.",1 Introduction,[0],[0]
This case occurs frequently in social media data classification as we discussed above.,1 Introduction,[0],[0]
"Second, it proposes a novel CBS space based learning method, CBS-L, which avoids the covariate shift problem to a large extent because it is able to find a good similarity boundary of the positive data.",1 Introduction,[0],[0]
"Third, it experimentally demonstrates the effectiveness of the proposed method.",1 Introduction,[0],[0]
Traditional supervised learning assumes that the training and test examples are drawn from the same distribution.,2 Related Work,[0],[0]
"However, this assumption can be violated in many applications.",2 Related Work,[0],[0]
This is especially the case for social media data because of the high topic diversity and constant changes of topics.,2 Related Work,[0],[0]
"This problem is known as covariate shift, which is a form of sample selection bias.
",2 Related Work,[0],[0]
Sample selection bias was first introduced in econometrics by Heckman (1979).,2 Related Work,[0],[0]
It came into the field of machine learning through the work of Zadrozny (2004).,2 Related Work,[0],[0]
"The main approach in machine learning is to first estimate the distribution bias of the training data based on the test data, and then learn using weighted training examples to
compensate for the bias (Bickel et al. 2009).",2 Related Work,[0],[0]
"For example, Shimodaira (2000) and Sugiyama and Muller (2005) proposed to estimate the training and test data distributions using kernel density estimation.",2 Related Work,[0],[0]
The estimated density ratio is then used to generate weighted training examples.,2 Related Work,[0],[0]
"Dudik et al. (2005) and Bickel and Scheffer (2007) used maximum entropy density estimation, while Huang et al. (2007) proposed kernel mean matching.",2 Related Work,[0],[0]
Sugiyama et al. (2008) and Tsuboi et al. (2008) estimated the weights for the training instances by minimizing the KullbackLeibler divergence between the test and the weighted training distributions.,2 Related Work,[0],[0]
Bickel et al. (2009) proposed an integrated model.,2 Related Work,[0],[0]
"As we discussed in the introduction, the need for the test data at the training time is a major weakness for social media data classification.",2 Related Work,[0],[0]
"The proposed technique CBS-L doesn’t have this restriction.
",2 Related Work,[0],[0]
"As mentioned in the introduction, one-class classification is a suitable approach to solve the problem.",2 Related Work,[0],[0]
Tax and Duin (1999a and 1999b) proposed a model for one-class classification called Support Vector Data Description (SVDD) to seek a hyper-sphere around the positive data that encompasses points in the data with the minimum radius.,2 Related Work,[0],[0]
"In order to balance between model over-fitting and under-fitting, Tax and Duin (2001) proposed a method that tries to use artificially generated outliers to optimize the model parameters.",2 Related Work,[0],[0]
"However, their experiments suggest that the procedure to generate artificial outliers in a hyper-sphere is only feasible for up to 30 dimensions.",2 Related Work,[0],[0]
"Also, as pointed out by (Khan and Madden, 2010; 2014), one drawback of their methods is that they often require a large dataset and the methods become very inefficient in high dimensional feature spaces.",2 Related Work,[0],[0]
"Since text documents are usually represented in a much higher dimensional space, these methods are less suitable for text applications.",2 Related Work,[0],[0]
Manevitz and Yousef (2001) performed one-class text classification using one-class SVM as proposed by Schölkopf et al. (1999).,2 Related Work,[0],[0]
The method is based on identifying outlier data that are representative of the second class.,2 Related Work,[0],[0]
"Instead of assuming the origin is the only member of the outlier class, it assumes those data points with few non-zero entries are also outliers.",2 Related Work,[0],[0]
"However, as reported in the paper, their methods produce quite weak results (Schölkopf et al., 1999; 2000).",2 Related Work,[0],[0]
Li et al. (2003) presented an improved version of one-class SVM for detecting anomalies.,2 Related Work,[0],[0]
Their idea is to consider all data points that are close to the origin as outliers.,2 Related Work,[0],[0]
"Both (Yang and Madden, 2007) and (Tian and
Gu, 2010) tried to refine Schölkopf’s models by searching optimal parameters.",2 Related Work,[0],[0]
"Luo et al., (2007) proposed a cost-sensitive one-class SVM algorithm for intrusion detection.",2 Related Work,[0],[0]
"We will see in the experiment section that one-class classification is far inferior to our proposed CBS-L method.
",2 Related Work,[0],[0]
"In this work, we propose to represent documents in the similarity space and thus it is related to works on document representation.",2 Related Work,[0],[0]
"Alternative document representations have been proposed in the past and have been shown to perform well in many applications (Radev et al., 2000; He et al., 2004; Lebanon 2006; Ranzato and Szummer, 2008, Wang and Domeniconi, 2008).",2 Related Work,[0],[0]
"In (Radev et al., 2000), although the centroid sentence/document vector was computed, it was not transformed to a similarity space vector representation.",2 Related Work,[0],[0]
Wang and Domeniconi (2008) proposed to use external knowledge to build semantic kernels for documents in order to improve text classification.,2 Related Work,[0],[0]
"In our problem, the main difficulty is that testing negative documents cannot be well covered in training.",2 Related Work,[0],[0]
"It is not clear how the enriched document representations could help solve our problem.
",2 Related Work,[0],[0]
"Our work is also related to learning from positive and unlabeled examples, also known as PU learning (Denis, 1998; Yu et al. 2002; Liu et al. 2003; Lee and Liu, 2003; Elkan and Noto, 2008; Li et al. 2010).",2 Related Work,[0],[0]
"In this learning model, there is a set of labeled positive training data and a set of unlabeled data, but there is no labeled negative training data.",2 Related Work,[0],[0]
"Clearly, their setting is different from ours too.",2 Related Work,[0],[0]
"There is also no guarantee that the unlabeled data has the same distribution as the future test data.
",2 Related Work,[0],[0]
Our problem is also very different from domain adaption as we work in the same domain.,2 Related Work,[0],[0]
"Due to the use of document similarity, our method has some resemblance to learning to rank (Li, 2011; Liu, 2011).",2 Related Work,[0],[0]
"However, CBS-L is very different because we perform supervised classification.",2 Related Work,[0],[0]
"Our similarity is also center-based rather than pair-wise document similarity, which is also used in (Qian and Liu 2013) for spam detection.",2 Related Work,[0],[0]
"We now formulate the proposed supervised learning in the CBS space, called CSB-L. The key difference between CBS learning and the classic document space (DS) learning is in the document representation, which applies to both training and testing documents or posts.",3 The Proposed CBS Learning,[0],[0]
"In the next subsection, we first give the intuitive idea
and a simple example.",3 The Proposed CBS Learning,[0],[0]
The detailed algorithm follows.,3 The Proposed CBS Learning,[0],[0]
"In Section 3.5, we explain why CBS-L is better than DS-based learning when unexpected negative data appear in the test set.",3 The Proposed CBS Learning,[0],[0]
"In the proposed CBS-L formulation, each document d is still represented as a feature vector, but the vector no longer represents the document d itself based on n-grams.",3.1 Basic Idea,[0],[0]
"Instead, it represents a set of similarity values between document d and the center of the positive documents.",3.1 Basic Idea,[0],[0]
"Specifically, the learning consists of the following steps:
1.",3.1 Basic Idea,[0],[0]
"Each document d (in the positive or negative class) is first represented with a set of document representations, i.e., document space vectors (ds-vectors) based on the document itself as in traditional text classification.",3.1 Basic Idea,[0],[0]
Each vector denotes one representation of the document.,3.1 Basic Idea,[0],[0]
"For example, one representation may be based on only unigrams, and another representation may be based on only bigrams.",3.1 Basic Idea,[0],[0]
"For simplicity, we use only one representation/vector x (e.g., unigrams) here to represent d. Note that we use bold lower case letters to represent vectors.",3.1 Basic Idea,[0],[0]
Each feature in a ds-vector is called a ds-feature.,3.1 Basic Idea,[0],[0]
2.,3.1 Basic Idea,[0],[0]
A center vector c is then computed for each document representation for the positive class documents using the ds-vectors of all positive and negative documents of that representation.,3.1 Basic Idea,[0],[0]
c is thus also a ds-vector.,3.1 Basic Idea,[0],[0]
3.,3.1 Basic Idea,[0],[0]
Each document d in the positive and negative class is then transformed to a center-based similarity space vector sd (called a cbs-vector).,3.1 Basic Idea,[0],[0]
"sd consists of a set of similarity values between document d’s set of ds-vectors, i.e., {x} in our case here (since we use only one representation), and the set of corresponding positive class center vectors, i.e., {c} in our case:
sd =Sim({x}, {c}),
where Sim is a similarity function consisting of a set of similarity measures.",3.1 Basic Idea,[0],[0]
Each feature in sd is called an cbs-feature.,3.1 Basic Idea,[0],[0]
sd still has the same original class label as d. Let us see an actual example.,3.1 Basic Idea,[0],[0]
"We assume that our single center vector for the positive class has been computed (see Section 3.2) based on the unigram representation of documents: c: 1:1 2:1 6:2 where y:z represents a ds-feature y (e.g., a word) and its feature value (e.g., term frequency, tf).",3.1 Basic Idea,[0],[0]
"We want to transform the follow-
ing positive document d1 and negative document d2 (ds-vectors) to their cbs-vectors (the first number is the class):
d1: 1 1:2 2:1 3:1 d2: -1 2:2 3:1 5:2 If we use cosine as the first similarity measure in Sim, we can generate a cbs-feature 1:0.50 for d1 (as cosine(c, d1) = 0.50) and a cbsfeature 1:0.27 for d2 (as cosine(c, d2) = 0.27).",3.1 Basic Idea,[0],[0]
"If we have more similarity measures, more cbs-features will be produced.",3.1 Basic Idea,[0],[0]
"The resulting cbs-vectors for d1 and d2 with their class labels, 1 and -1, are:
d1: 1 1:0.50 … d2: -1 1:0.27 … 4.",3.1 Basic Idea,[0],[0]
"We now have a binary classification problem
in the CBS space.",3.1 Basic Idea,[0],[0]
"This step simply runs a classification algorithm, e.g., SVM, to build a classifier.",3.1 Basic Idea,[0],[0]
We use SVM in our work.,3.1 Basic Idea,[0],[0]
We are given a binary text classification problem.,3.2 CBS Based Learning,[0],[0]
"Let D = {(d1, y1), (d2, y2), …, (dn, yn)} be the set of training examples, where di is a document and yi ∈ {1, -1} is its class label.",3.2 CBS Based Learning,[1.0],"['Let D = {(d1, y1), (d2, y2), …, (dn, yn)} be the set of training examples, where di is a document and yi ∈ {1, -1} is its class label.']"
Traditional classification directly uses D to build a binary classifier.,3.2 CBS Based Learning,[0],[0]
"However, in the CBS space, we learn a classifier that returns 1 for documents that are “close enough” to the center of the training positive documents and -1 for documents elsewhere.
",3.2 CBS Based Learning,[0],[0]
We now detail the proposed technique.,3.2 CBS Based Learning,[0],[0]
"As we mentioned above, instead of using one single dsvector to represent a document di ∈D, we use a set Rd of p ds-vectors Rd = {𝐱!!",3.2 CBS Based Learning,[0],[0]
", 𝐱!!",3.2 CBS Based Learning,[0],[0]
", …, 𝐱!!}.",3.2 CBS Based Learning,[0],[0]
Each vector 𝐱!!,3.2 CBS Based Learning,[0],[0]
"denotes one document space representation of the document, e.g., unigram representation.",3.2 CBS Based Learning,[0],[0]
"We then compute the center of positive training documents, which is represented as a set of 𝑝 centroids C = {c1, c2, …, cp}, each of which corresponds to one document space representation in Rd.",3.2 CBS Based Learning,[1.0],"['We then compute the center of positive training documents, which is represented as a set of 𝑝 centroids C = {c1, c2, …, cp}, each of which corresponds to one document space representation in Rd.']"
"The way to compute each center ci is similar to that in the Rocchio relevance feedback method in information retrieval (Rocchio, 1971; Manning et al. 2008), which uses the corresponding ds-vectors of all training positive and negative documents.",3.2 CBS Based Learning,[0],[0]
The detail will be given below.,3.2 CBS Based Learning,[0],[0]
"Based on Rd for document d and the center C, we can transform a document d from its document space representations Rd to one center-based similarity vector cbs-v by applying a similarity function 𝑆𝑖𝑚 on each element 𝐱!!",3.2 CBS Based Learning,[0],[0]
of Rd and its corresponding center ci,3.2 CBS Based Learning,[0],[0]
.,3.2 CBS Based Learning,[0],[0]
"We now detail document transformation.
",3.2 CBS Based Learning,[0],[0]
"Training document transformation: The train-
ing data transformation from ds-vectors to cbsvectors performs the following two steps:
Step 1: Compute the set C of centroids for the positive class.",3.2 CBS Based Learning,[0],[0]
Each centroid vector ci∈C is for one document representation 𝐱!!.,3.2 CBS Based Learning,[0],[0]
"And it is computed by applying the Rocchio method to the corresponding ds-vectors of all documents in both positive and negative training data.
",3.2 CBS Based Learning,[0],[0]
"𝐜! = 𝛼 𝐷!
𝐱!!
",3.2 CBS Based Learning,[0],[0]
"𝐱!!𝐝𝐬!!∈!!                         
          ",3.2 CBS Based Learning,[0],[0]
− 𝛽 |𝐷,3.2 CBS Based Learning,[0],[0]
"− 𝐷!| 𝐱!!
𝐱!!𝐱!!∈!!!!
",3.2 CBS Based Learning,[0],[0]
where 𝐷! is the set of documents in the positive class and |.| is the size function.,3.2 CBS Based Learning,[0],[0]
"𝛼 and 𝛽 are parameters, which are usually set empirically.",3.2 CBS Based Learning,[0],[0]
"It is reported that using tf-idf representation, 𝛼 = 16 and 𝛽 = 4 usually work quite well (Buckley et al. 1994).",3.2 CBS Based Learning,[0],[0]
"The subtraction is used to reduce the influence of those terms that are not discriminative (i.e., terms appearing in both positive and negative documents).",3.2 CBS Based Learning,[0],[0]
"Step 2: Compute the similarity vector cbs-vd (center-based similarity space vector) for each document d ∈D based on its set of document space vectors Rd and the corresponding centroids C of the positive documents.
",3.2 CBS Based Learning,[0],[0]
"cbs-vd = Sim(Rd, C)
",3.2 CBS Based Learning,[0],[0]
"Sim has a set of similarity measures, and each measure mj is applied to p document representations 𝐱!!",3.2 CBS Based Learning,[0],[0]
in Rd and their corresponding centers 𝐜!,3.2 CBS Based Learning,[0],[0]
in C to generate p similarity features (cbs-features) in cbs-vd.,3.2 CBS Based Learning,[0],[0]
"We discuss the dsfeatures and similarity measures for computing cbs-features in the next two subsections.
",3.2 CBS Based Learning,[0],[0]
"Complexity: The data transformation step is clearly linear in the number of examples, i.e., n. Test document transformation: For each test document d, we can use step 2 above to produce a cbs-vector for d.",3.2 CBS Based Learning,[0],[0]
"In order to compute cbs-features (center-based similarity space features) for each document, we need to have the ds-features of a document and the center of the positive class.",3.3 DS-Features,[0],[0]
"We discuss dsfeatures first, which are extracted from each document itself.
",3.3 DS-Features,[0],[0]
"Since our task is document classification, we use the popular unigram, bigram and trigram
with tf-idf weighting as the ds-features for a document.",3.3 DS-Features,[0],[0]
These three types of ds-features also give us three different document representations.,3.3 DS-Features,[0],[0]
Ds-vectors are transformed into cbs-vectors by applying a set of similarity measures on each document space vector and the corresponding center vector.,3.4 CBS-Features,[0],[0]
"In this work, we employed five similarity measures from (Cha, 2007) to gauge the similarity of two vectors.",3.4 CBS-Features,[0],[0]
"Based on these measures, we produce 15 CBS features using the unigram, bigram, and trigrams representations of each document.",3.4 CBS-Features,[0],[0]
"The similarity measures we used are listed in Table 1, where P and Q are two vectors and d represents the dimension of P and Q.",3.4 CBS-Features,[1.0],"['The similarity measures we used are listed in Table 1, where P and Q are two vectors and d represents the dimension of P and Q.']"
"We now try to explain why CBS learning (CBSL) can deal with the covariate shift problem, and thus can perform better than document space learning.",3.5 Why Does CBS Space Learning Work?,[0],[0]
"The reason is that due to the use of similarity features, CBS-L is essentially trying to generate a boundary for the positive training data because similarity is not directional and thus covers all directions in a spherical shape in the space.",3.5 Why Does CBS Space Learning Work?,[0],[0]
"In classification, the negative data from anywhere or direction outside the spherical shape can be detected.",3.5 Why Does CBS Space Learning Work?,[0],[0]
The covariate shift problem will not affect the classification much.,3.5 Why Does CBS Space Learning Work?,[1.0],['The covariate shift problem will not affect the classification much.']
Many types of documents that are not represented in the negative training data will still be detected due to their low similarity.,3.5 Why Does CBS Space Learning Work?,[0],[0]
"For example, in Figure 1, we want to build a SVM classifier to separate positive data represented as black squares and negative data represented as empty circles.",3.5 Why Does CBS Space Learning Work?,[1.0],"['For example, in Figure 1, we want to build a SVM classifier to separate positive data represented as black squares and negative data represented as empty circles.']"
"The constructed CBS-L classifier would look like a circle (in dashed line) in the original document space
covering the positive data.",3.5 Why Does CBS Space Learning Work?,[0],[0]
The size of this (boundary) circle depends on the separation margin between the two classes.,3.5 Why Does CBS Space Learning Work?,[0],[0]
"Although data points represented by empty triangles are not represented in the negative training data (which has only empty circles) in building the classifier, our classifier is able to identify them as not positive at the test time because they are outside the boundary circle.
",3.5 Why Does CBS Space Learning Work?,[0],[0]
"If we had used the document space (DS) features to build a SVM classifier, the classifier would be a line (see Figure 1) between the positive data (black squares) and the negative data (empty circles).",3.5 Why Does CBS Space Learning Work?,[0],[0]
"This line unfortunately will not be able to identify data points represented as empty triangles as not positive because the triangles actually lie on the positive side and would be classified as positive, which is clearly wrong.",3.5 Why Does CBS Space Learning Work?,[0],[0]
"In this section, we evaluate the proposed learning in the center-based similarity space (CBS-L) and compare it with baselines.",4 Experiments,[0],[0]
"As stated at the beginning of the paper, this work was motivated by the real-life problem of identifying the right social media posts or documents for specific applications.",4.1 Experimental Dataset,[0],[0]
"For an effective evaluation, we need a large number of classes in the data to reflect the topic richness and diversity of the social media.",4.1 Experimental Dataset,[0],[0]
The whole data also has to be labeled for evaluation.,4.1 Experimental Dataset,[0],[0]
"Using online reviews of a large number of products is a natural choice because there are many types of products and services and there is no need to do manual labeling, which is very labor intensive, time consuming, and error prone.",4.1 Experimental Dataset,[0],[0]
"We obtained the Amazon review database from the authors of (Jindal and Liu 2008), and constructed a dataset with reviews of 50 types of products, which we also call 50 topics.",4.1 Experimental Dataset,[0],[0]
Each topic (a type of products) have 1000 reviews.,4.1 Experimental Dataset,[1.0],['Each topic (a type of products) have 1000 reviews.']
"For each topic, we randomly sampled 700 reviews/documents for training and the remaining 300 reviews for testing.",4.1 Experimental Dataset,[0],[0]
"Note that although we use this product review collection, we
do not perform sentiment classification.",4.1 Experimental Dataset,[0],[0]
"Instead, we still perform the traditional topic based classification.",4.1 Experimental Dataset,[0],[0]
"That is, given a review, the system decides what type of product the review is about.",4.1 Experimental Dataset,[0],[0]
"In our experiments, we use every topic as the positive class.",4.1 Experimental Dataset,[0],[0]
This gives us 50 classification results.,4.1 Experimental Dataset,[0],[0]
We use three baselines in our evaluation.,4.2 Baselines,[0],[0]
"Document space one-class SVM (ds-osvm): As we discussed earlier, due to the covariate shift problem in the negative training data, one solution is to drop the negative training data completely to build a one-class classifier.",4.2 Baselines,[0],[0]
One-class SVM is the state-of-the-art one-class classification algorithm.,4.2 Baselines,[1.0],['One-class SVM is the state-of-the-art one-class classification algorithm.']
We apply one-class SVM to the documents in the document space as one of the baselines.,4.2 Baselines,[0],[0]
"One-class SVM was first introduced by Schölkopf et al. (1999; 2000), which is based on the assumption that the origin is the only member of the second class.",4.2 Baselines,[1.0],"['One-class SVM was first introduced by Schölkopf et al. (1999; 2000), which is based on the assumption that the origin is the only member of the second class.']"
The data is first mapped into a transformed feature space via a kernel and then standard two-class SVM is employed to construct a hyper-plane that separates the data and the original with maximum margin.,4.2 Baselines,[0],[0]
"As mentioned earlier, there is also the support vector data description (SVDD) formulation for one-class classification proposed by Tax and Duin (1999a; 1999b).",4.2 Baselines,[0],[0]
SVDD seeks to distinguish the positive class from all other possible data in space.,4.2 Baselines,[0],[0]
It basically finds a hyper-sphere around the positive class data that contains almost all points in the data set with the minimum radius.,4.2 Baselines,[0],[0]
"It has been shown that the use of Gaussian kernel makes SVDD and One-class SVM equivalent, and the results reported in (Khan and Madden, 2014) demonstrate that SVDD and One-class SVM are comparable when the Gaussian kernel is applied.",4.2 Baselines,[0],[0]
"Thus in this paper, we just use oneclass SVM, which is one of the SVM-based classification tools in the LIBSVM1 library (version 3.20) (Chang and Lin, 2011).
",4.2 Baselines,[0],[0]
"Center-based similarity space one-class SVM (cbs-osvm): Instead of applying one-class SVM to documents in the original document space, this baseline applies it to the CBS space after the documents are transformed to CBS vectors.
",4.2 Baselines,[0],[0]
SVM:,4.2 Baselines,[0],[0]
This baseline is the SVM applied in the original document space.,4.2 Baselines,[0],[0]
"Although in this case, there is covariate shift problem, we want to see how serious the problem might be, and how the proposed CBS-L technique can deal with the 1 http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/
problem.",4.2 Baselines,[0],[0]
We use the SVM tool in LIBSVM.,4.2 Baselines,[0],[0]
"As Khan and Madden (2014) pointed out that one-class SVM performs the best when Gaussian kernel is used, we use Gaussian kernel as well.",4.3 Kernels and Parameters,[0],[0]
"Manevitz and Yousef (2001) applied one-class SVM to text classification, and the authors reported that one-class SVM works the best with binary feature weighting scheme compared to tf or tf-idf weighting schemes.",4.3 Kernels and Parameters,[0],[0]
"Also, they reported that a small number of features (10) with highest document frequency performed the best with Gaussian kernel.",4.3 Kernels and Parameters,[0],[0]
"We also use binary representation, but found that 10 features are already too many in our case.",4.3 Kernels and Parameters,[0],[0]
"In fact, 5 features give the best results.",4.3 Kernels and Parameters,[0],[0]
Using a small number of features is intuitive because to find the boundary of a very high dimensional space is very difficult.,4.3 Kernels and Parameters,[0],[0]
"We also tried more features but they were poorer.
",4.3 Kernels and Parameters,[0],[0]
"For SVM classification in the document space, we use the linear kernel as it has been shown by many researchers that the linear kernel performs the best (e.g., Joachims, 1998; Colas and Brazdil, 2006).",4.3 Kernels and Parameters,[0],[0]
"We experimented with RBF kernels extensively, but they did not perform well with the traditional document representation.",4.3 Kernels and Parameters,[0],[0]
"The term weighting scheme is tf-idf (Colas and Brazdil, 2006) with no feature selection.
",4.3 Kernels and Parameters,[0],[0]
"For our proposed method CBS-L, we use tf-idf
values of unigram, bigram and trigram to represent a document in three ways in the document space.",4.3 Kernels and Parameters,[0],[0]
"As mentioned earlier, five document similarity functions are used to transform document space vectors to CBS space vectors.",4.3 Kernels and Parameters,[0],[0]
"And in order to filter out less useful features for the center vector of the positive class, we performed feature selection in the document space using the classic information gain method (Yang and Pedersen, 1997) to empirically choose the most effective 100 features for the positive class.
",4.3 Kernels and Parameters,[0],[0]
"For all the kernels, we use the default parameter settings in the LIBSVM systems.",4.3 Kernels and Parameters,[0],[0]
"We tried to tune the parameters, but did not get better results.",4.3 Kernels and Parameters,[0],[0]
We now present the experiment results.,4.4 Results,[0],[0]
"As mentioned above, we treat each topic as the positive class.",4.4 Results,[0],[0]
This gives 50 tests.,4.4 Results,[0],[0]
"To test the effect of covariate shift, we also vary the number of topics in the negative class.",4.4 Results,[0],[0]
"We used 10, 20, 30, and 40 topics in the training negative class.",4.4 Results,[0],[0]
"The test set always has 49 topics of negative data.
",4.4 Results,[0],[0]
"For each setting, we give three sets of results for the positive class, which is the target topic data that we are interested in obtaining through classification.",4.4 Results,[0],[0]
"Each set of results includes the standard measures of precision, recall, and F1score for the positive class.",4.4 Results,[0],[0]
The three sets are: 1.,4.4 Results,[0],[0]
"In-training: In this case, the test negative data
contains only data from those topics used in training.",4.4 Results,[0],[0]
This is the classical supervised learning setting where the training and test data are randomly drawn from the same distribution.,4.4 Results,[0],[0]
2.,4.4 Results,[0],[0]
"Not-in-training: In this case, the test negative set contains only data from the other topics not used in training.",4.4 Results,[0],[0]
The classical setting of supervised learning does not deal with this problem.,4.4 Results,[0],[0]
This represents covariate shift.,4.4 Results,[0],[0]
3.,4.4 Results,[0],[0]
"Combined: In this case, the test data contains both in-training and not-in-training negative topics.",4.4 Results,[0],[0]
"Due to the use of not-in-training test data, this is also not the classical setting.",4.4 Results,[0],[0]
"Due to a large number of experiment results, we cannot report all the details.",4.4 Results,[0],[0]
Table 2 summarizes the results.,4.4 Results,[0],[0]
"Notice that for ds-osvm, it does not make sense to have in-training and not-intraining results because it does not use any training negative data.",4.4 Results,[0],[0]
"Thus, there is only one set of results for “Combined,” which is duplicated in the table for easy comparison.",4.4 Results,[0],[0]
"However, note that cbs-osvm uses negative data for training in order to compute the center for the positive class.
",4.4 Results,[0],[0]
"From the table, we can make the following observations (since there are many numbers, we only focus on F1-scores).",4.4 Results,[0],[0]
1.,4.4 Results,[0],[0]
"The proposed CBS-L method performs mark-
edly better than all baselines.",4.4 Results,[0],[0]
"For the results of in-training, not-in-training, and combined, CBS-L is consistently better in all cases than all baselines.",4.4 Results,[0],[0]
"Even for in-training, CBS-L perform better than SVM.",4.4 Results,[0],[0]
This clearly shows the superiority of the proposed CBS-L method.,4.4 Results,[0],[0]
2.,4.4 Results,[0],[0]
ds-osvm performs poorly.,4.4 Results,[0],[0]
cbs-osvm is much better because it uses the negative data in feature selection and center computation.,4.4 Results,[0],[0]
3.,4.4 Results,[0],[0]
SVM in the document space performed poorly (Combined) when only a small number of negative topics are used in training.,4.4 Results,[0],[0]
It gets better than both one-class SVM baselines when more negative topics are used in training (see the reason in the next point).,4.4 Results,[0],[0]
4.,4.4 Results,[0],[0]
"Finally, we can also see that with the number of training negative topics increases, the results of the combined case of both SVM and CBS-L improve.",4.4 Results,[0],[0]
"This is expected because with the increased number of negative topics for training, the number of not-in-training negative topics for testing decreases and the covariate shift problem gets smaller.",4.4 Results,[0],[0]
"We can also see that cbs-osvm, SVM and CBS-L’s F1-scores for not-in-training improve with the increased training negative topics due to the same reason.",4.4 Results,[0],[0]
"However, their F1-scores drop for in-training because with more negative
topics, the data becomes more skewed, which hurts in-training classification.
",4.4 Results,[0],[0]
"To give a flavor of the detailed results for each topic (product), we give the full results for one setting with 30 randomly selected topics as the training negative data (Table 3).",4.4 Results,[0],[0]
The results in the table are F1-scores of the combined case.,4.4 Results,[0],[0]
The ability to get relevant posts accurately about a topic from social media is a challenging problem.,5 Conclusion,[0],[0]
This paper attempted to solve this problem by identifying and dealing with the technical issue of covariate shift.,5 Conclusion,[0],[0]
The key idea of our technique is to transform document representation from the traditional n-gram feature space to a similarity based space.,5 Conclusion,[0],[0]
Our experimental results show that the proposed method CBS-L outperformed strong baselines by large margins.,5 Conclusion,[0],[0]
"This research was partially supported by the NSF grants IIS-1111092 and IIS-1407927, and a Google faculty award.
",Acknowledgments,[0],[0]
"Reference
Alter, O., Brown, P.O. and Bostein, D. 2000.",Acknowledgments,[0],[0]
Singular Value Decomposition for GenomeWide Expression Data Processing and Modeling.,Acknowledgments,[0],[0]
Proc.,Acknowledgments,[0],[0]
"Nat',l Academy of Science, vol. 97, no. 18, pp. 10101-10106, Aug.
Blei, D. Ng, A. and Jordan, M., 2003.",Acknowledgments,[0],[0]
"Latent dirichlet allocation, The Journal of Machine Learning Research, 3, p.993-1022, 3/1/2003
Buckley, C., Salton, G., Allan, J. 1994.",Acknowledgments,[0],[0]
"The Effect of Adding Relevance Information in a Relevance Feedback Environment, Proceedings of SIGIR Conference.
",Acknowledgments,[0],[0]
"Bickel, S., Bruckner, M., and Scheffer. 2009.",Acknowledgments,[0],[0]
T. Discriminative learning under covariate shift.,Acknowledgments,[0],[0]
"Journal of Machine Learning Research.
",Acknowledgments,[0],[0]
Bickel S. and Scheffer T. 2007.,Acknowledgments,[0],[0]
Dirichletenhanced spam filtering based on biased samples.,Acknowledgments,[0],[0]
"Advances in Neural Information Processing Systems.
Cha, S.-H. 2007.",Acknowledgments,[0],[0]
Comprehensive Survey on Distance/Similarity Measures between Probability Density Functions.,Acknowledgments,[0],[0]
"International Journal of Mathematical Models and Methods in Applied Sciences, 1(4):300--307.
",Acknowledgments,[0],[0]
"Chang, C-C. and Lin, C-J. 2011.",Acknowledgments,[0],[0]
"LIBSVM: a
library for support vector machines.",Acknowledgments,[0],[0]
"ACM Transactions on Intelligent Systems and Technology, 2:27:1--27:27, http://www.csie.",Acknowledgments,[0],[0]
ntu.edu.,Acknowledgments,[0],[0]
"tw/~cjlin/libsvm
Colas, F. and Brazdil.",Acknowledgments,[0],[0]
P. 2006.,Acknowledgments,[0],[0]
Comparison of SVM and some older classification algorithms in text classification tasks.,Acknowledgments,[0],[0]
Artificial Intelligence in Theory and Practice.,Acknowledgments,[0],[0]
"IFIP International Federation for Information Processing, pp. 169-178.
",Acknowledgments,[0],[0]
"Denis, F., PAC learning from positive statistical queries.",Acknowledgments,[0],[0]
"ALT, 1998.
",Acknowledgments,[0],[0]
"Radev, D., Jing, H. and Budzikowska, M. 2000.",Acknowledgments,[0],[0]
"Centroid-based summarization of multiple documents: Sentence extraction, utility-based evaluation, and user studies.",Acknowledgments,[0],[0]
"In ANLP/NAACL Workshop on Summarization, Seattle, April.
Dudik, M., Schapire, R., and Phillips, S. 2005.",Acknowledgments,[0],[0]
Correcting sample selection bias in maximum entropy density estimation.,Acknowledgments,[0],[0]
"Advances in Neural Information Processing Systems.
",Acknowledgments,[0],[0]
"Elkan, C. and Noto, K. 2008.",Acknowledgments,[0],[0]
Learning classifiers from only positive and unlabeled data.,Acknowledgments,[0],[0]
"KDD, 213-220.
",Acknowledgments,[0],[0]
"He, X., Cai, D., Liu, H. and Ma, W.-Y. 2004.",Acknowledgments,[0],[0]
Locality Preserving Indexing for Document Representation.,Acknowledgments,[0],[0]
Proc.,Acknowledgments,[0],[0]
"Of SIGIR.
",Acknowledgments,[0],[0]
"Heckman, J. 1979.",Acknowledgments,[0],[0]
Sample selection bias as a specification error.,Acknowledgments,[0],[0]
"Econometrica, 47:153– 161.
Huang, J., Smola, A. and Gretton, A., Borgwardt K., and Scholkopf B. 2007.",Acknowledgments,[0],[0]
Correcting sample selection bias by unlabeled data.,Acknowledgments,[0],[0]
"Advances in Neural Information Processing Systems.
Joachims, T. 1998.",Acknowledgments,[0],[0]
Text categorization with support vector machines:,Acknowledgments,[0],[0]
Learning with many relevant features.,Acknowledgments,[0],[0]
"ECML.
",Acknowledgments,[0],[0]
"Jindal, N. and Liu, B. 2008.",Acknowledgments,[0],[0]
Opinion Spam and Analysis.,Acknowledgments,[0],[0]
"Proceedings of the ACM Conference on Web Search and Data Mining.
Khan, S., and Madden, M. 2010.",Acknowledgments,[0],[0]
A survey of recent trends in one class classification.,Acknowledgments,[0],[0]
"Artificial Intelligence and Cognitive Science, volume 6206 of Lecture Notes in Computer Science.",Acknowledgments,[0],[0]
"188–197.
",Acknowledgments,[0],[0]
"Khan, S. and Madden, M. 2014.",Acknowledgments,[0],[0]
One-Class Classification: Taxonomy of Study and Review of Techniques.,Acknowledgments,[0],[0]
"The Knowledge Engineering Review, 1-30.
Lebanon, G. 2006.",Acknowledgments,[0],[0]
"Sequential document repre-
sentations and simplicial curves.",Acknowledgments,[0],[0]
UAI.,Acknowledgments,[0],[0]
"Lee, W. S. and Liu, B. 2003.",Acknowledgments,[0],[0]
"Learning with Posi-
tive and Unlabeled Examples Using Weighted Logistic Regression.",Acknowledgments,[0],[0]
"ICML.
",Acknowledgments,[0],[0]
"Li, H. 2011.",Acknowledgments,[0],[0]
Learning to Rank for Information Retrieval and Natural Language Processing.,Acknowledgments,[0],[0]
"Morgan & Claypool publishers.
",Acknowledgments,[0],[0]
"Li, K., Huang, H., Tian, S. and Xu, W. 2003.",Acknowledgments,[0],[0]
Improving One-class SVM for anomaly detection.,Acknowledgments,[0],[0]
Proc.,Acknowledgments,[0],[0]
"of the Second International conference on Machine Learning and Cybernetics, volume 5, pages 3077–3081.
",Acknowledgments,[0],[0]
"Li, X., Liu, B. and Ng.",Acknowledgments,[0],[0]
S.-K. 2010.,Acknowledgments,[0],[0]
Negative Training Data can be Harmful to Text Classification.,Acknowledgments,[0],[0]
"EMNLP.
",Acknowledgments,[0],[0]
"Liu, B, Dai, Y., Li, X., Lee, W-S. and Yu.",Acknowledgments,[0],[0]
P. 2003.,Acknowledgments,[0],[0]
Building text classifiers using positive and unlabeled examples.,Acknowledgments,[0],[0]
"ICDM.
Liu.",Acknowledgments,[0],[0]
T. 2011.,Acknowledgments,[0],[0]
Learning to Rank for Information Retrieval.,Acknowledgments,[0],[0]
"Springer.
",Acknowledgments,[0],[0]
"Luo, J., Ding, L., Pan, Z., Ni, G. and Hu, G. 2007.",Acknowledgments,[0],[0]
Research on cost-sensitive learning in one-class anomaly detection algorithms.,Acknowledgments,[0],[0]
"Autonomic and Trusted Computing, volume 4610 of Lecture Notes in Computer Science.
",Acknowledgments,[0],[0]
"Manevitz, L. and Yousef.",Acknowledgments,[0],[0]
M. 2001.,Acknowledgments,[0],[0]
One-class SVMs for document classification.,Acknowledgments,[0],[0]
"Journal of Machine Learning research.
",Acknowledgments,[0],[0]
"Manning, C. D., Prabhakar R., and Hinrich, S. 2008.",Acknowledgments,[0],[0]
Introduction to Information Retrieval.,Acknowledgments,[0],[0]
"Cambridge University Press.
",Acknowledgments,[0],[0]
"Qian, T. and Liu, B. 2013.",Acknowledgments,[0],[0]
Identifying Multiple Userids of the Same Author.,Acknowledgments,[0],[0]
"EMNLP.
",Acknowledgments,[0],[0]
"Ranzato, M. and Szummer, M. 2008.",Acknowledgments,[0],[0]
Semisupervised learning of compact document representations with deep networks.,Acknowledgments,[0],[0]
"ICML.
",Acknowledgments,[0],[0]
"Rocchio, J. 1971.",Acknowledgments,[0],[0]
Relevant feedback in information retrieval.,Acknowledgments,[0],[0]
In G. Salton (ed.).,Acknowledgments,[0],[0]
"The smart retrieval system: experiments in automatic document processing.
",Acknowledgments,[0],[0]
"Schölkopf, B., Williamson, R., Smola, A., Taylor, J. and Platt, J. 2000.",Acknowledgments,[0],[0]
Support vector method for novelty detection.,Acknowledgments,[0],[0]
"Neural Information Processing Systems, pages 582–588.
",Acknowledgments,[0],[0]
"Schölkopf, B., Platt, J., Shawe-Taylor, J., Smola, A. and Williamson, R. 1999.",Acknowledgments,[0],[0]
Estimating the support of a high-dimensional distribution.,Acknowledgments,[0],[0]
"Technical Report, Microsoft Research, MSRTR-99-87.
",Acknowledgments,[0],[0]
"Shimodaira, H. 2000.",Acknowledgments,[0],[0]
Improving predictive inference under covariate shift by weighting the log-likelihood function.,Acknowledgments,[0],[0]
"Journal of Statistical Planning and Inference, 90:227– 244.
Sugiyama, M. and Muller, K.-R. 2005.",Acknowledgments,[0],[0]
Inputdependent estimation of generalization error under covariate shift.,Acknowledgments,[0],[0]
"Statistics and Decision, 23(4):249–279.
Sugiyama, M., Nakajima, S., Kashima, H., von Bunau P., and Kawanabe M. 2008.",Acknowledgments,[0],[0]
Direct importance estimation with model selection and its application to covariate shift adaptation.,Acknowledgments,[0],[0]
"Advances in Neural Information Processing Systems.
Tax, D. and Duin, R. 1999a.",Acknowledgments,[0],[0]
Data domain description using support vectors.,Acknowledgments,[0],[0]
"Proceedings ESAN99, Brussels. 251-256
Tax, D. and Duin, R. 1999b.",Acknowledgments,[0],[0]
Support vector domain description.,Acknowledgments,[0],[0]
"Pattern Recognition Letters 20. 1191-1199
Tax, D. and Duin, R. 2001.",Acknowledgments,[0],[0]
Uniform object generation for optimizing one-class classifiers.,Acknowledgments,[0],[0]
"J. of Machine Learning Research, 2:155–173.
Tian, J. and Gu, H. 2010.",Acknowledgments,[0],[0]
Anomaly detection combining one-class SVMs and particle swarm optimization algorithms.,Acknowledgments,[0],[0]
"Nonlinear Dynamics, 61(1-2): 303–310.
",Acknowledgments,[0],[0]
"Tsuboi, J., Kashima, H., Hido, S., Bickel, S., and Sugiyama, M. 2008.",Acknowledgments,[0],[0]
Direct density ratio estimation for large-scale covariate shift adaptation.,Acknowledgments,[0],[0]
"Proceedings of the SIAM International Conference on Data Mining (SDM).
",Acknowledgments,[0],[0]
"Wang, P. and Domeniconi, C. 2008.",Acknowledgments,[0],[0]
"Building semantic kernels for text classification using Wikipedia, KDD.
Yang, Y. and Pedersen, J. O. 1997.",Acknowledgments,[0],[0]
A comparative study on feature selection in text categorization.,Acknowledgments,[0],[0]
"ICML.
",Acknowledgments,[0],[0]
"Yang, L., and Madden, M. 2007.",Acknowledgments,[0],[0]
One-class support vector machine calibration using particle swarm optimization.,Acknowledgments,[0],[0]
"AICS, Dublin.
",Acknowledgments,[0],[0]
"Yu, H., Han, J. and Chang, K. 2002.",Acknowledgments,[0],[0]
PEBL:,Acknowledgments,[0],[0]
"Positive example based learning for Web page classification using SVM. KDD, 239-248.
",Acknowledgments,[0],[0]
"Zadrozny, B. 2004.",Acknowledgments,[0],[0]
"Learning and evaluating classifiers under s ample selection bias, ICML.",Acknowledgments,[0],[0]
"In a typical social media content analysis task, the user is interested in analyzing posts of a particular topic.",abstractText,[0],[0]
Identifying such posts is often formulated as a classification problem.,abstractText,[0],[0]
"However, this problem is challenging.",abstractText,[0],[0]
One key issue is covariate shift.,abstractText,[0],[0]
"That is, the training data is not fully representative of the test data.",abstractText,[0],[0]
"We observed that the covariate shift mainly occurs in the negative data because topics discussed in social media are highly diverse and numerous, but the user-labeled negative training data may cover only a small number of topics.",abstractText,[0],[0]
This paper proposes a novel technique to solve the problem.,abstractText,[0],[0]
The key novelty of the technique is the transformation of document representation from the traditional ngram feature space to a center-based similarity (CBS) space.,abstractText,[0],[0]
"In the CBS space, the covariate shift problem is significantly mitigated, which enables us to build much better classifiers.",abstractText,[0],[0]
Experiment results show that the proposed approach markedly improves classification.,abstractText,[0],[0]
Social Media Text Classification under Negative Covariate Shift,title,[0],[0]
