{"sections": [{"heading": "1. Introduction", "text": "Assume we are given a set of items from a general metric space (X , \u03b4), but we neither have access to the representation of the data nor to the distances between data points. Instead, we have access to an oracle that we can actively ask a triplet comparison: given any triplet of items (xi, xj , xk) in the metric space X , is it true that\n\u03b4(xi, xj) < \u03b4(xi, xk) ?\nSuch a comparison-based framework has become popular in recent years, for example in the context of crowd-sourcing applications (Tamuz et al., 2011; Heikinheimo and Ukkonen, 2013; Ukkonen et al., 2015), and more generally, whenever humans are supposed to give feedback or when constructing an explicit distance or similarity function is difficult (Wilber et al., 2015; Zhang et al., 2015; Wah et al., 2015; Balcan et al., 2016; Kleindessner and von Luxburg, 2017).\n1Department of Computer Science, University of Tu\u0308bingen, Germany 2Max Planck Institute for Intelligent Systems, Tu\u0308bingen, Germany. Correspondence to: Siavash Haghiri <haghiri@informatik.uni-tuebingen.de>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nIn the present work, we consider classification and regression problems in a comparison-based setting where we are given the labels y1, . . . , yn of unknown objects x1, . . . , xn, and we can actively query triplet comparisons between objects. An indirect way to solve such problems is to first construct an \u201cordinal embedding\u201d of the data points in a (typically low-dimensional) Euclidean space that satisfies the set of triplet comparisons, and then to apply standard machine learning methods to the Euclidean data representation. However, this approach is often not satisfactory because this new representation necessarily introduces distortions in the data. Furthermore, all existing ordinal embedding methods are painfully slow, even on moderate-sized datasets (Agarwal et al., 2007; van der Maaten and Weinberger, 2012; Terada and von Luxburg, 2014). In addition, one has to estimate the embedding dimension, which is a challenging task by itself (Kleindessner and Luxburg, 2015).\nAs an alternative, we solve the classification/regression problems by a new random forest algorithm that requires only triplet comparisons. Standard random forests (Biau and Scornet, 2016) are one of the most popular and successful classification/regression algorithms in Euclidean spaces (Ferna\u0301ndez-Delgado et al., 2014). However, they heavily rely on the underlying vector space structure. In our comparison-based setting we need a completely different tree building strategy. We use the recently described comparison tree (Haghiri et al., 2017) for this purpose (which in Euclidean cases would be distantly related to linear decision trees (Kane et al., 2017b; Ezra and Sharir, 2017; Kane et al., 2017a)). A comparison-based random forest (CompRF) consists of a collection of comparison trees built on the training set.\nWe study the proposed CompRF both from a theoretical and a practical point of view. In Section 3, we give sufficient conditions under which a slightly simplified variant of the comparison-based forest is statistically consistent. In Section 4, we apply the CompRF to various datasets. In the first set of experiments we compare our random forests to traditional CART forests on Euclidean data. In the second set of experiments, the distances between objects are known while their representation is missing. Finally, we consider a case in which only triplet comparisons are available."}, {"heading": "2. Comparison-Based Random Forests", "text": "Random forests, first introduced in Breiman (2001), are one of the most popular algorithms for classification and regression in Euclidean spaces. In a comprehensive study on more than 100 classification tasks, random forests show the best performance among many other general purpose methods (Ferna\u0301ndez-Delgado et al., 2014). However, standard random forests heavily rely on the vector space representation of the underlying data points, which is not available in a comparison-based framework. Instead, we propose a comparison-based random forest algorithm for classification and regression tasks. The main ingredient is the comparison tree, which only uses of triplet comparisons and does not rely on Euclidean representation or distances between items.\nLet us recap the CART random forest: The input consists of a labeled set Dn = {(x1, y1), (x2, y2), . . . , (xn, yn)} \u2282 Rd \u00d7 R. To build an individual tree, we first draw a random subsample Ds of an points from Dn. Second, we select a random subset Dims of size mtry of all possible dimensions {1, 2, . . . , d}. The tree is then built based on recursive, axis-aligned splits along a dimension randomly chosen from Dims. The exact splitting point along this direction is determined via the CART criterion, which also involves the labels of the subset Ds of points (see Biau and Scornet (2016) for details). The tree is grown until each cell contains at most n0 points\u2014these cells then correspond to the leaf nodes of the tree. To estimate a regression function m(x), each individual tree routes the query point to the appropriate leaf and outputs the average response over all points in this leaf. The random forest aggregates M such trees. Let us denote the prediction of tree i at point x by mi(x,\u0398i, Dn), where \u0398i encodes the randomness in the tree construction. Then the final forest estimation at x is the average result over all trees (for classification, the average is replaced by a majority vote):\nmM,n(x; (\u0398i)1\u2264i\u2264M , Dn) = 1\nM M\u2211 i=1 mi(x,\u0398i, Dn) .\nThe general consensus in the literature is that CART forests are surprisingly robust to parameter choices. Consequently, people use explicit rules of thumb, for example to set mtry = dd/3e, and n0 = 5 (resp. n0 = 1) for regression (resp. classification) tasks.\nWe now suggest to replace CART trees by comparison trees, leading to comparison-based random forests (CompRF). Comparison trees have originally been designed to find nearest neighbors by recursively splitting the search space into smaller subspaces. Inspired by the CART criterion, we propose a supervised variant of the comparison tree, which we refer to as \u201csupervised comparison tree.\u201d\nFor classification, the supervised comparison tree construc-\nAlgorithm 1 CompTree(S, n0): Supervised comparison tree construction Input: Labeled data S and maximum leaf size n0 Output: Comparison tree T\n1: T.root\u2190 S 2: if |S| > n0 then 3: Sample distinct (x1, y1), (x2, y2) \u2208 S s.t. y1 6= y2 (if all points have the same label choose randomly) 4: S1 \u2190 {(x, y) \u2208 S : \u03b4(x, x1) \u2264 \u03b4(x, x2)} 5: T.leftpivot\u2190 x1, T.rightpivot\u2190 x2 6: T.leftchild\u2190 CompTree(S1, n0) 7: T.rightchild\u2190 CompTree(S\\S1, n0) 8: end if 9: Return T\ntion for a labeled set S \u2282 X \u00d7 {0, 1} is as follows (see Algorithm 1 and Figure 1): we randomly choose two pivot points x1 and x2 with different labels y1 and y2 among the points in S (the case where all the points in S have the same label is trivial). For every remaining point (x, y) \u2208 S, we request the triplet comparison \u201c\u03b4(x, x1) < \u03b4(x, x2).\u201d The answer to this query determines the relative position of x with respect to the generalized hyperplane separating x1 and x2. We assign the points closer to x1 to the first child node of S and the points closer to x2 to the other one. We now recurse the algorithm on the child nodes until less than n0 points remain in every leaf node of the tree.\nThe supervised pivot selection is analogous to the CART criterion. However, instead of a costly optimization over the choice of split, it only requires to choose pivots with different labels. In Section 4.1, we empirically show that the supervised split procedure leads to a better performance than the CART forests for classification tasks.\nFor regression, it is not obvious how the pivot selection should depend on the output values. Here we use an unsupervised version of the forest (unsupervised CompRF): we choose the pivots x1, x2 without considering y1, y2.\nThe final comparison-based forest consists of M independently constructed comparison trees. To assign a label to a query point, we traverse every tree to a leaf node, then we aggregate all the items in the leaf nodes of M trees to estimate the label of the query item. For classification, the final label is the majority vote over the labels of the accumulated set (in the multiclass case we use a one vs. one approach). For regression we use the mean output value.\nIntuitive comparison: The general understanding is that the efficiency of CART random forests is due to: (1) the randomness due to subsampling of dimensions and data points (Breiman, 1996); (2) the CART splitting criterion that exploits the label information already in the tree construction (Breiman et al., 1984). A weakness of CART splits is that\nAlgorithm 2 CompRF (Dn, q,M, n0, r): CompRF prediction at query q Input: Labeled dataset Dn \u2282 X \u00d7 {0, 1}, query q \u2208 X ,\nleaf size n0, trees M and subsampling ratio r. Output: yq = label prediction for q\n1: Set C = \u2205 as the list of predictor items 2: for j=1,. . . ,M do 3: Take a random subsample Ds \u2282 Dn, s.t., |Ds||Dn| = r 4: Tj \u2190 CompTree(Ds, n0) 5: Given q, traverse the tree Tj to the leaf node Nj 6: C \u2190 C \u222aNj 7: end for 8: Return MajorityVote({y|(x, y) \u2208 C})\nthey are necessarily axis-aligned, and thus not well-adapted to the geometry of the data.\nIn comparison trees, randomness is involved in the tree construction as well. But once a splitting direction has been determined by choosing the pivot points, the exact splitting point along this direction cannot be influenced any more, due to the lack of a vector space representation. On the other hand, the comparison tree splits are well adapted to the data geometry by construction, giving some advantage to the comparison trees.\nAll in all, the comparison-based forest is a promising candidate with slightly different strengths and weaknesses than CART forest. Our empirical comparison in Section 4.1 reveals that it performs surprisingly well and can even outperform CART forests in certain settings."}, {"heading": "3. Theoretical Analysis", "text": "Despite their intensive use in practice, theoretical questions regarding the consistency of the original procedure of Breiman (2001) are still under investigation. Most of the research focuses on simplified models in which the construction of the forest does not depend on the training set at all (Biau, 2012), or only via the xis but not the yis (Biau et al., 2008; Ishwaran and Kogalur, 2010; Denil et al., 2013).\nRecent efforts nearly closed this gap, notably Scornet et al. (2015), where it is shown that the original algorithm is consistent in the context of additive regression models and under suitable assumptions. However, there is no previous work on the consistency of random forests constructed only with triplet comparisons.\nAs a first step in this direction, we investigate the consistency of individual comparison trees, which is the first building block in the study of random forests consistency. As it is common in the theoretical literature on random forests, we consider a slightly modified version of the comparison tree. We assume that the pivot points are not randomly drawn from the underlying sample but according to the true distribution of the data. In this setting, we show that, when the number of observations grows to infinity, (i) the diameter of the cells converges to zero in probability, and (ii) each cell contains an arbitrarily large number of observations. Using a result of Devroye et al. (1996), we deduce that the associated classifier is consistent. The challenging part of the proof is to obtain control over the diameter of the cells. Intuitively, as in Dasgupta and Freund (2008, Lemma 12), it suffices to show that each cut has a larger probability to decrease the diameter of the current cell than that of leaving it unchanged. To prove this in our case is very challenging since both the position and the decrease in diameter caused by the next cut depend on the geometry of the cell."}, {"heading": "3.1. Continuous Comparison Tree", "text": "As it is the case for most theoretical results on random forests, we carry out our analysis in a Euclidean setting (however, the comparison-forest only has indirect access to the Euclidean metric via triplet queries). We assume that the input space is X = [0, 1]d with distance \u03b4 given by the Euclidean norm, that is, \u03b4 (x, y) = \u2016x\u2212 y\u2016. Let X be a random variable with support included in [0, 1]d. We assume that the observations X1, . . . , Xn \u2208 [0, 1]d are drawn independently according to the distribution of X . We make the following assumptions:\nAssumption 3.1. The random variable X \u2208 [0, 1]d has density f with respect to the Lebesgue measure on [0, 1]d.\nAdditionally, f is finite and bounded away from 0.\nFor any x, y \u2208 Rd, let us define\n\u2206(x, y) := { z \u2208 Rd | \u03b4 (x, z) = \u03b4 (y, z) } .\nIn the Euclidean setting, \u2206(x, y) is a hyperplane that separates Rd in two half-spaces. We call Hx (resp. Hy) the open half-space containing x (resp. y). The set S1 in Algorithm 1 corresponds to S \u2229Hx1 .\nWe can now define the continuous comparison tree.\nDefinition 1 (Continuous comparison tree). A continuous comparison tree is a random infinite binary tree T 0 obtained via the following iterative construction:\n\u2022 The root of T 0 is [0, 1]d;\n\u2022 Assuming that level ` of T 0 has been built already, then level `+ 1 is built as follows: For every cell C at height `, drawX1, X2 \u2208 C independently according to the distribution of X restricted to C. The children of C are defined as the closure of C \u2229HX1 and C \u2229HX2 .\nFor any sequence (pn)n\u22650, a truncated, continuous comparison tree T 0(pn) consists of the first bpnc levels of T 0.\nFrom a mathematical point of view, the continuous tree has a number of advantages. (i) Its construction does not depend on the responses Y1, . . . , Yn. Such a simplification is quite common because data-dependent random tree structures are notoriously difficult to analyze (Biau et al., 2008). (ii) Its construction is formally independent of the finite set of data points, but \u201cclose in spirit\u201d: Rather than sampling the pivots among the data points in a cell, pivots are independently sampled according to the underlying distribution. Whenever a cell contains a large number of sample points, both distributions are close, but they may drift apart when the diameter of the cells go to 0. (iii) In the continuous comparison tree, we stop splitting cells at height bpnc, whereas in the discrete setting we stop if there is less than n0 observations in the current cell. As a consequence, T 0(pn) is a perfect binary tree: each interior node has exactly 2 children. This is typically not the case for comparison trees."}, {"heading": "3.2. Consistency", "text": "To each realization of T 0(pn) is associated a partition of [0, 1]d into disjoint cells A1,n, A2,n, . . . , A2pn ,n . For any x \u2208 [0, 1]d, letA(x) be the cell of T 0(pn) containing x. Let us assume that the responses (Yi)1\u2264i\u2264n are binary labels. We consider the classifier defined by majority vote in each cell, that is,\ngn(x) :=\n{ 1 if \u2211 Xi\u2208A(x) 1Yi=1 \u2265 \u2211 Xi\u2208A(x) 1Yi=0\n0 otherwise.\nDefine Ln := P (gn(X) 6= Y |Dn) . Following Devroye et al. (1996), we say that the classifier gn is consistent if\nE [Ln] = P (gn(X) 6= Y ) \u2212\u2212\u2212\u2212\u2212\u2192 n\u2192+\u221e L? ,\nwhere L? is the Bayes error probability. Our main result is the consistency of the classifier associated with the continuous comparison tree truncated to a logarithmic height. Theorem 2 (Consistency of comparison-based trees). Under Assumption 3.1, the classifier associated to the continuous, truncated tree T 0(\u03b1 log n) is consistent for any constant 0 < \u03b1 < 1/ log 2.\nIn particular, since each individual tree is consistent, a random forest with base tree T 0(pn) is also consistent. Theorem 2 is a first step towards explaining why comparisonbased trees perform well without having access to the representation of the points. Also note that, even though the continuous tree is a simplified version of the discrete tree, they are quite similar and share all important characteristics. In particular, they roughly have the same depth\u2014 with high probability, the comparison tree has logarithmic depth (Haghiri et al., 2017, Theorem 1)."}, {"heading": "3.3. Sketch of the Proof", "text": "Since the construction of T 0(pn) does not depend on the labels, we can use Theorem 6.1 of Devroye et al. (1996). It gives sufficient conditions for classification rules based on space partitioning to be consistent. In particular, we have to show that the partition satisfies two properties: first, the leaf cells should be small enough, so that local changes of the distribution can be detected; second, the leaf cells should contain a sufficiently large number of points so that averaging among the labels makes sense. More precisely, we have to show that (1) diamA(X) \u2192 0 in probability, where diamA := supx,y\u2208A \u03b4 (x, y) is the diameter of A, and (2) N(X) \u2192 \u221e in probability, where N(X) is the number of sample points in the cell containing X . The second point is simple, because it is sufficient to show that the number of cells in the partition associated to T 0(\u03b1 log n) is o (n) (according to Lemma 20.1 in Devroye et al. (1996) and the remark that follows). Proving (1) is much more challenging. A sketch of the proof of (1) follows\u2014note that the complete version of the proof of Theorem 2 can be found in the supplementary material.\nThe critical part in proving (1) is to show that, for any cell of the continuous comparison tree, the diameter of its descendants at least k levels below is halved with high probability. More precisely, the following proposition shows that this probability is lower bounded by 1\u2212 \u03b4, where \u03b4 is exponentially decreasing in k. Proposition 3 (Diameter control). Let C be a cell of T 0(X) such that diamC \u2264 D. Then, under Assumption 3.1, the probability that there exists a descendant of C\nwhich is more than k levels below and yet has diameter greater than D/2 is at most \u03b4 = c\u03b3k, where c > 0 and \u03b3 \u2208 (0, 1) are constants depending only on the dimension d and the density f .\nThe proof of Proposition 3 amounts to showing that the probability of decreasing the diameter of any given cell is higher than the probability of keeping it unchanged\u2014see the supplementary material.\nAssuming Proposition 3, the rest of the proof goes as follows. Let us set \u03b5 \u2208 (0, 1). We are going to show that\nP (diamA(X) > \u03b5) \u2212\u2192 0 when n\u2192 +\u221e .\nLet \u0393 be the path in T 0(\u03b1 log n) that goes from the root to the leaf A of maximum diameter. This path has length bpnc according to the definition of T 0(\u03b1 log n). The root, which consists of the set [0, 1]d, has diameter \u221a d. This means that\nwe need to divide the diameter of this cell \u03c0 = \u2308 log2 \u221a d/\u03b5 \u2309 times to obtain cells with diameter smaller than \u03b5. Let us set k = bpn/\u03c0c and pick cells ( C(j) ) 0\u2264j\u2264\u03c0 along \u0393 such that C(0) = [0, 1]d, C(\u03c0) = A, and such that there are more than k levels between C(j) and C(j+1). Then we can prove that P (diamA > \u03b5) is smaller than\n\u03c0\u2211 j=1 P\n( diamC(j) > \u221a d\n2j \u2223\u2223\u2223\u2223\u2223diamC(j\u22121) \u2264 \u221a d 2j\u22121 ) .\nFurthermore, according to Prop. 3, the quantity in the last expression is upper bounded by \u03c0c\u03b3k. Since k = O (log n) and \u03b3 \u2208 (0, 1), we can conclude."}, {"heading": "4. Experiments", "text": "In this section, we first examine comparison-based forests in the Euclidean setting. Secondly, we apply the CompRF method to non-Euclidean datasets with a general metric available. Finally we run experiments in the setting where we are only given triplet comparisons."}, {"heading": "4.1. Euclidean Setting", "text": "Here we examine the performance of CompRF on classification and regression tasks in the Euclidean setting, and compare it against CART random forests as well as the KNN classifier as a baseline. As distance function for KNN and CompRF we use the standard Euclidean distance. Since the CompRF only has access to distance comparisons, the amount of information it uses is considerably lower than the information available to the CART forest. Hence, the goal of this experiment is not to show that comparison-based random forests can perform better, but rather to find out whether the performance is still acceptable.\nTo emphasize the role of supervised pivot selection, we report the performance of the unsupervised CompRF algo-\nrithm in classification tasks as well. The tree structure in the unsupervised CompRF chooses the pivot points uniformly at random without considering the labels.\nFor the sake of simplicity, we do not perform subsampling when building the CompRF trees. We report some experiments concerning the role of subsampling in Section 3.2 of supplementary material. All other parameters of CompRF are adjusted by cross-validation."}, {"heading": "4.1.1. CLASSIFICATION", "text": "We use four classification datasets. MNIST (LeCun et al., 1998) and Gisette are handwritten digit datasets. Isolet and UCIHAR are speech recognition and human activity recognition datasets respectively (Lichman, 2013). Details of the datasets are shown in the first three rows of Table 1.\nParameters of CompRF: We examine the behaviour of the CompRF with respect to the choice of the leaf size n0 and the number of treesM . We perform 10-fold cross-validation over n0 \u2208 {1, 4, 16, 64} and M \u2208 {1, 4, 16, 64, 256}. In Figure 2 we report the resulting cross validation error. Similar to the recommendation for CART forests (Biau and Scornet, 2016), we achieve the best performance when the leaf size is small, that is (n0 = 1). Moreover, there is no significant improvement for M greater than 100.\nComparison between CompRF, CART and KNN: Table 1 shows the average and standard deviation of classification error for 10 independent runs of CompRF, CART forest and KNN. Training and test sets are given in the respective\ndatasets. The parameters n0 and M of CompRF and CART, and k of KNN are chosen by 10-fold cross validation on the training set. Note that KNN is not randomized, thus there is no standard deviation to report.\nThe results show that, surprisingly, CompRF can slightly outperform the CART forests for classification tasks even though it uses considerably less information. The reason might be that the CompRF splits are better adapted to the geometry of the data than the CART splits. While the CART criterion for selecting the exact splitting point can be very informative for regression (see below), for classification it seems that a simple data dependent splitting criterion as in the supervised CompRF can be as efficient. Conversely, we see that unsupervised splitting as in the unsupervised CompRF is clearly worse than supervised splitting."}, {"heading": "4.1.2. REGRESSION", "text": "Next we consider regression tasks on four datasets. Online news popularity (ONP) is a dataset of articles with the popularity of the article as target (Fernandes et al., 2015). Boston is a dataset of properties with the estimated value as target variable. ForestFire is a dataset meant to predict the burned area of forest fires, in the northeast region of Portugal (Cortez and Morais, 2007). WhiteWine (Wine) is a subset of wine quality dataset (Cortez et al., 2009). Details of the datasets are shown in the first two rows of Table 2.\nSince the regression datasets have no separate training and test set, we assign 90% of the items to the training and the remaining 10% to the test set. In order to remove the effect\nof the fixed partitioning, we repeat the experiments 10 times with random training/test set assignments. Note that we use CompRF with unsupervised tree construction for regression.\nParameters of CompRF: We report the behaviour of the CompRF with respect to the parameters n0 and M . We perform 10-fold cross-validation with the same range of parameters as in the previous section. Figure 3 shows the average root mean squared error (RMSE) over the 10 folds. The cross-validation is performed for 10 random training/test set assignments. The figure corresponds to the first assignment out of 10 (the behaviour for the other training/test set assignments is similar). The CompRF algorithm shows the best performance with n0 = 1 for the Boston and ForestFire datasets, however larger values of n0 lead to better performance for other datasets. We believe that the main reason for this variance is the unsupervised tree construction in the CompRF algorithm for regression.\nComparison between CompRF and CART: Table 2 shows the average and standard deviation of the RMSE for the CompRF and CART forests over the 10 runs with random training/test set assignment. For each combination of training and test sets we tuned parameters independently by cross validation. CompRF is constructed with unsupervised splitting, while the CART forests are built using a supervised criterion. We can see that on the Boston and Wine datasets, the performance of the CART forest is substantially better than the CompRF. In this case, ignoring the Euclidean representation of the data and just relying on the comparison-based trees leads to a significant decrease in performance. However the performance of our method on the other two datasets is quite the same as CART forests. We can conclude that in some cases the CART criterion can be essential for regression. However, note that if we are just\ngiven a comparison-based setting\u2014without actual vector space representation\u2014it is hardly possible to propose an efficient supervised criterion for splitting."}, {"heading": "4.2. Metric, non-Euclidean Setting", "text": "In this set of experiments we aim to demonstrate the performance of the CompRF in general metric spaces. We choose graph-structured data for this experiment. Each data-point is a graph, and as a distance between graphs we use graphbased kernel functions. In particular, the Weisfeiler-Lehman graph kernels are a family of graph kernels that have promising results on various graph datasets (Shervashidze et al., 2011). We compute the WL-subtree and WL-edge kernels on four of the datasets reported in Shervashidze et al. (2011): MUTAG, ENZYMES, NCI1 and NCI109. In order to evaluate triplet comparisons based on the graph kernels, we first convert the kernel matrix to a distance matrix in the standard way (expressing the Gram matrix in terms of distances).\nWe compare supervised and unsupervised CompRF with the Kernel SVM and KNN classifier in Table 3. Note that in this setting, CART forests are not applicable as they would require an explicit vector space representation. Parameters of the Kernel SVM and k of the KNN classifier are adjusted with 10-fold cross-validation on training sets.\nWe set the parameters of the CompRF to n0 = 1 and M = 200, as it shows acceptable performance in the Euclidean setting. We assign 90% of the items as training and the remaining 10% as the test set. The experiment is repeated 10 times with random training/test assignments. The average and standard deviation of classification error is reported in Table 3. The CompRF algorithm outperforms the kernel SVM on the MUTAG and ENZYMES datasets.\nHowever, it has slightly lower performance on the other two datasets. However, note that the kernel SVM requires a lot of background knowledge (one has to construct a kernel in the first place, which can be difficult), whereas our CompRF algorithm neither uses the explicit distance values nor requires them to satisfy the axioms of a kernel function."}, {"heading": "4.3. Comparison-Based Setting", "text": "Now we assume that the distance metric is unknown and inaccessible directly, but we can actively ask for triplet comparisons. In this setting, the major competitors to comparison-based forests are indirect methods that first use ordinal embedding to a Euclidean space, and then classify the data in the Euclidean space. As practical active ordinal embedding methods do not really exist we settle for a batch setting in this case. After embedding, we use CART forests and the KNN classifier in the Euclidean space.\nComparing various ordinal embedding algorithms, such as GNMDS (Agarwal et al., 2007), LOE (Terada and von Luxburg, 2014) and TSTE (van der Maaten and Weinberger, 2012) shows that the TSTE in combination with a classifier consistently outperforms the others (see Section 3.1 in the supplement). Therefore, we here only report the comparison with the TSTE embedding algorithm. We choose the embedding dimension by 2-fold cross-validation in the range of d \u2208 {10, 20, 30, 40, 50} (embedding in more than 50 dimensions is impossible in practice due to the running time of the TSTE). We also adjust k of the KNN classifier in the cross-validation process.\nWe design a comparison-based scenario based on Euclidean datasets. First, we let CompRF choose the desired triplets to construct the forest and classify the test points. The embedding methods are used in two different scenarios: once with exactly the same triplets as in the CompRF algorithm, and\nonce with a completely random set of triplets of the same size as the one used by CompRF.\nThe size of our datasets by far exceeds the number of points that embedding algorithms, particularly TSTE, can handle. To reduce the size of the datasets, we choose the first two classes, then we subsample 1000 items. Isolet has already less than 1000 items in first two classes. We assign half of the dataset as training and the other half as test set. Bar plots in Figure 4 show the classification error of the CompRF in comparison with embedding methods with various numbers of trees in the forests (M ). We set n0 = 1 for the CompRF.\nIn each set of bars, which corresponds to a restricted comparison-based regime, CompRF outperforms embedding methods or has the same performance. Another significant advantage of CompRF in comparison with the embedding is the low computation cost. A simple demonstration is provided in Section 3.3 of the supplementary material."}, {"heading": "5. Conclusion and Future Work", "text": "We propose comparison-based forests for classification and regression tasks. This method only requires comparisons of distances as input. From a practical point of view, it works surprisingly well in all kinds of circumstances (Euclidean spaces, metric spaces, comparison-based setting) and is much simpler and more efficient than some of its competitors such as ordinal embeddings.\nWe have proven consistency in a simplified setting. As future work, this analysis should be extended to more realistic situations, namely tree construction depending on the sample; forests with inconsistent trees, but the forest is still consistent; and finally the supervised splits. In addition, it would be interesting to propose a comparison-based supervised tree construction for the regression tasks."}, {"heading": "Acknowledgements", "text": "The authors thank Debarghya Goshdastidar and Michae\u0308l Perrot for fruitful discussions. This work has been supported by the German Research Foundation DFG (SFB 936/ Z3), the Institutional Strategy of the University of Tu\u0308bingen (DFG ZUK 63), and the International Max Planck Research School for Intelligent Systems (IMPRS-IS)."}], "year": 2018, "references": [{"title": "Generalized non-metric multidimensional scaling", "authors": ["S. Agarwal", "J. Wills", "L. Cayton", "G. Lanckriet", "D. Kriegman", "S. Belongie"], "venue": "In AISTATS,", "year": 2007}, {"title": "Learning combinatorial functions from pairwise comparisons", "authors": ["M.F. Balcan", "E. Vitercik", "C. White"], "venue": "In COLT,", "year": 2016}, {"title": "Analysis of a random forests model", "authors": ["G. Biau"], "year": 2012}, {"title": "A random forest guided", "authors": ["G. Biau", "E. Scornet"], "venue": "tour. Test,", "year": 2016}, {"title": "Consistency of random forests and other averaging classifiers", "authors": ["G. Biau", "L. Devroye", "G. Lugosi"], "year": 2015}, {"title": "Classification and regression trees", "authors": ["L. Breiman", "J. Friedman", "C.J. Stone", "R.A. Olshen"], "venue": "CRC press,", "year": 1984}, {"title": "A Data Mining Approach to Predict Forest Fires using Meteorological Data", "authors": ["P. Cortez", "A.J.R. Morais"], "venue": "In Portuguese Conference on Artificial Intelligence,", "year": 2007}, {"title": "Modeling wine preferences by data mining from physicochemical properties", "authors": ["P. Cortez", "A. Cerdeira", "F. Almeida", "T. Matos", "J. Reis"], "venue": "Decision Support Systems,", "year": 2009}, {"title": "Random projection trees and low dimensional manifolds", "authors": ["S. Dasgupta", "Y. Freund"], "venue": "In STOC,", "year": 2008}, {"title": "Consistency of online random forests", "authors": ["M. Denil", "D. Matheson", "N. Freitas"], "venue": "In ICML,", "year": 2013}, {"title": "A probabilistic theory of pattern recognition", "authors": ["L. Devroye", "L. Gy\u00f6rfi", "G. Lugosi"], "year": 1996}, {"title": "A nearly quadratic bound for the decision tree complexity of k-sum", "authors": ["E. Ezra", "M. Sharir"], "venue": "In LIPIcs-Leibniz International Proceedings in Informatics,", "year": 2017}, {"title": "A proactive intelligent decision support system for predicting the popularity of online news", "authors": ["K. Fernandes", "P. Vinagre", "P. Cortez"], "venue": "In Portuguese Conference on Artificial Intelligence,", "year": 2015}, {"title": "Do we need hundreds of classifiers to solve real world classification problems", "authors": ["M. Fern\u00e1ndez-Delgado", "E. Cernadas", "S. Barro", "D. Amorim"], "year": 2014}, {"title": "Comparison-based nearest neighbor search", "authors": ["S. Haghiri", "D. Ghoshdastidar", "U. von Luxburg"], "venue": "In AISTATS,", "year": 2017}, {"title": "The crowd-median algorithm", "authors": ["H. Heikinheimo", "A. Ukkonen"], "venue": "In HCOMP,", "year": 2013}, {"title": "Consistency of random survival", "authors": ["H. Ishwaran", "U.B. Kogalur"], "venue": "forests. Statistics & probability letters,", "year": 2010}, {"title": "Near-optimal linear decision trees for k-sum and related problems", "authors": ["D.M. Kane", "S. Lovett", "S. Moran"], "venue": "arXiv preprint arXiv:1705.01720,", "year": 2017}, {"title": "Active classification with comparison queries", "authors": ["D.M. Kane", "S. Lovett", "S. Moran", "J. Zhang"], "venue": "Foundations Of Computer Science (FOCS),", "year": 2017}, {"title": "Dimensionality estimation without distances", "authors": ["M. Kleindessner", "U. Luxburg"], "venue": "In AISTATS, pages 471\u2013479,", "year": 2015}, {"title": "Kernel functions based on triplet comparisons", "authors": ["M. Kleindessner", "U. von Luxburg"], "venue": "In NIPS,", "year": 2017}, {"title": "Gradientbased learning applied to document recognition", "authors": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "year": 1998}, {"title": "Consistency of random forests", "authors": ["E. Scornet", "G. Biau", "J.-P. Vert"], "venue": "The Annals of Statistics,", "year": 2015}, {"title": "Adaptively learning the crowd kernel", "authors": ["O. Tamuz", "C. Liu", "S. Belongie", "O. Shamir", "A. Kalai"], "venue": "In ICML,", "year": 2011}, {"title": "Local ordinal embedding", "authors": ["Y. Terada", "U. von Luxburg"], "venue": "In ICML, pages 847\u2013855,", "year": 2014}, {"title": "Crowdsourced nonparametric density estimation using relative distances", "authors": ["A. Ukkonen", "B. Derakhshan", "H. Heikinheimo"], "venue": "In HCOMP,", "year": 2015}, {"title": "Stochastic triplet embedding", "authors": ["L. van der Maaten", "K. Weinberger"], "venue": "In Machine Learning for Signal Processing (MLSP), pages", "year": 2012}, {"title": "Learning localized perceptual similarity metrics for interactive categorization", "authors": ["C. Wah", "S. Maji", "S. Belongie"], "venue": "In Winter Conference on Applications of Computer Vision (WACV),", "year": 2015}, {"title": "Learning concept embeddings with combined human-machine expertise", "authors": ["M. Wilber", "I.S. Kwak", "D. Kriegman", "S. Belongie"], "venue": "In ICCV,", "year": 2015}, {"title": "Jointly learning multiple measures of similarities from triplet comparisons", "authors": ["L. Zhang", "S. Maji", "R. Tomioka"], "venue": "arXiv preprint arXiv:1503.01521,", "year": 2015}], "id": "SP:f774cdbaa9f55aad3da680198d76acb7347258d1", "authors": [{"name": "Siavash Haghiri", "affiliations": []}, {"name": "Damien Garreau", "affiliations": []}, {"name": "Ulrike von Luxburg", "affiliations": []}], "abstractText": "Assume we are given a set of items from a general metric space, but we neither have access to the representation of the data nor to the distances between data points. Instead, suppose that we can actively choose a triplet of items (A,B,C) and ask an oracle whether item A is closer to item B or to item C. In this paper, we propose a novel random forest algorithm for regression and classification that relies only on such triplet comparisons. In the theory part of this paper, we establish sufficient conditions for the consistency of such a forest. In a set of comprehensive experiments, we then demonstrate that the proposed random forest is efficient both for classification and regression. In particular, it is even competitive with other methods that have direct access to the metric representation of the data.", "title": "Comparison-Based Random Forests"}