0,1,label2,summary_sentences
Two important challenges in reinforcement learning (RL) are the problems of representation learning and of automatic discovery of skills.,1. Introduction,[0],[0]
"Proto-value functions (PVFs) are a well-known solution for the problem of representation learning (Mahadevan, 2005; Mahadevan & Maggioni, 2007); while the problem of skill discovery is generally posed under the options framework (Sutton et al., 1999; Precup, 2000), which models skills as options.
",1. Introduction,[0],[0]
"In this paper, we tie together representation learning and option discovery by showing how PVFs implicitly define options.",1. Introduction,[0],[0]
One of our main contributions is to introduce the concepts of eigenpurpose and eigenbehavior.,1. Introduction,[0],[0]
Eigenpurposes are intrinsic reward functions that incentivize the agent to traverse the state space by following the principal directions of the learned representation.,1. Introduction,[0],[0]
"Each intrinsic reward function leads to a different eigenbehavior, which is
1University of Alberta 2Google DeepMind.",1. Introduction,[0],[0]
"Correspondence to: Marlos C. Machado <machado@ualberta.ca>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
the optimal policy for that reward function.,1. Introduction,[0],[0]
In this paper we introduce an algorithm for option discovery that leverages these ideas.,1. Introduction,[0],[0]
"The options we discover are task-independent because, as PVFs, the eigenpurposes are obtained without any information about the environment’s reward structure.",1. Introduction,[0],[0]
"We first present these ideas in the tabular case and then show how they can be generalized to the function approximation case.
",1. Introduction,[0],[0]
"Exploration, while traditionally a separate problem from option discovery, can also be addressed through the careful construction of options (McGovern & Barto, 2001; Şimşek et al., 2005; Solway et al., 2014; Kulkarni et al., 2016).",1. Introduction,[0],[0]
"In this paper, we provide evidence that not all options capable of accelerating planning are useful for exploration.",1. Introduction,[0],[0]
We show that options traditionally used in the literature to speed up planning hinder the agents’ performance if used for random exploration during learning.,1. Introduction,[0],[0]
"Our options have two important properties that allow them to improve exploration: (i) they operate at different time scales, and (ii) they can be easily sequenced.",1. Introduction,[0],[0]
Having options that operate at different time scales allows agents to make finely timed actions while also decreasing the likelihood the agent will explore only a small portion of the state space.,1. Introduction,[0],[0]
"Moreover, because our options are defined across the whole state space, multiple options are available in every state, which allows them to be easily sequenced.",1. Introduction,[0],[0]
"We generally indicate random variables by capital letters (e.g., R
t ), vectors by bold letters (e.g., ✓), functions by lowercase letters (e.g., v), and sets by calligraphic font (e.g., S).",2. Background,[0],[0]
"In the RL framework (Sutton & Barto, 1998), an agent aims to maximize cumulative reward by taking actions in an environment.",2.1. Reinforcement Learning,[0],[0]
These actions affect the agent’s next state and the rewards it experiences.,2.1. Reinforcement Learning,[0],[0]
We use the MDP formalism throughout this paper.,2.1. Reinforcement Learning,[0],[0]
"An MDP is a 5-tuple hS,A, r, p, i.",2.1. Reinforcement Learning,[0],[0]
"At time t the agent is in state s
t 2 S where it takes action a
t 2 A that leads to the next state s t+1 2 S according to the transition probability kernel p(s0|s, a), which encodes Pr(S t+1 = s 0|S t = s,A t
= a).",2.1. Reinforcement Learning,[0],[0]
The agent also observes a reward R t+1 ⇠,2.1. Reinforcement Learning,[0],[0]
"r(s, a).",2.1. Reinforcement Learning,[0],[0]
"The agent’s goal is to learn a
policy µ : S ⇥",2.1. Reinforcement Learning,[0],[0]
A !,2.1. Reinforcement Learning,[0],[0]
"[0, 1] that maximizes the expected discounted return G
t
.",2.1. Reinforcement Learning,[0],[0]
"= E p,µ ⇥P1 k=0",2.1. Reinforcement Learning,[0],[0]
"k R t+k+1|st ⇤ , where
2 [0, 1) is the discount factor.
",2.1. Reinforcement Learning,[0],[0]
"It is common to use the policy improvement theorem (Bellman, 1957) when learning to maximize G
t .",2.1. Reinforcement Learning,[0],[0]
"One technique is to alternate between solving the Bellman equations for the action-value function q
µk(s, a),
q µk(s, a) .",2.1. Reinforcement Learning,[0],[0]
"= E µk,p
⇥",2.1. Reinforcement Learning,[0],[0]
"G
t |S",2.1. Reinforcement Learning,[0],[0]
"t = s,A t = a
⇤
=
X
s 0 ,r
p(s 0 , r|s, a) ⇥",2.1. Reinforcement Learning,[0],[0]
"r +
X
a
0
µ
k
(a 0|s0)q µk(s 0 , a 0 )
",2.1. Reinforcement Learning,[0],[0]
"⇤
and making the next policy, µ k+1",2.1. Reinforcement Learning,[0],[0]
", greedy w.r.t. qµk ,
µ k+1 .",2.1. Reinforcement Learning,[0],[0]
=,2.1. Reinforcement Learning,[0],[0]
"argmax
a2A q µk(s, a),
until converging to an optimal policy µ⇤.
",2.1. Reinforcement Learning,[0],[0]
Sometimes it is not feasible to learn a value for each stateaction pair due to the size of the state space.,2.1. Reinforcement Learning,[0],[0]
"Generally, this is addressed by parameterizing q
µ (s, a) with a set of weights ✓ 2 Rn such that q
µ (s, a) ⇡ q µ (s, a,✓).",2.1. Reinforcement Learning,[0],[0]
"It is common to approximate q
µ through a linear function, i.e., q
µ (s, a,✓) =",2.1. Reinforcement Learning,[0],[0]
"✓> (s, a), where (s, a) denotes a linear feature representation of state s when taking action a.",2.1. Reinforcement Learning,[0],[0]
The options framework extends RL by introducing temporally extended actions called skills or options.,2.2. The Options Framework,[0],[0]
An option ! is a 3-tuple !,2.2. The Options Framework,[0],[0]
"= hI,⇡, T i where I 2 S denotes the option’s initiation set, ⇡ : A⇥S !",2.2. The Options Framework,[0],[0]
"[0, 1] denotes the option’s policy, and T 2 S denotes the option’s termination set.",2.2. The Options Framework,[0],[0]
After the agent decides to follow option !,2.2. The Options Framework,[0],[0]
"from a state in I, actions are selected according to ⇡ until the agent reaches a state in T .",2.2. The Options Framework,[0],[0]
"Intuitively, options are higher-level actions that extend over several time steps, generalizing MDPs to semiMarkov decision processes (SMDPs) (Puterman, 1994).
",2.2. The Options Framework,[0],[0]
"Traditionally, options capable of moving agents to bottleneck states are sought after.",2.2. The Options Framework,[0],[0]
"Bottleneck states are those states that connect different densely connected regions of the state space (e.g., doorways) (Şimşek & Barto, 2004; Solway et al., 2014).",2.2. The Options Framework,[0],[0]
"They have been shown to be very efficient for planning as these states are the states most frequently visited when considering the shortest distance between any two states in an MDP (Solway et al., 2014).",2.2. The Options Framework,[0],[0]
"Proto-value functions (PVFs) are learned representations that capture large-scale temporal properties of an environment (Mahadevan, 2005; Mahadevan & Maggioni, 2007).",2.3. Proto-Value Functions,[0],[0]
"They are obtained by diagonalizing a diffusion model, which is constructed from the MDP’s transition matrix.",2.3. Proto-Value Functions,[0],[0]
"A diffusion model captures information flow on a graph, and
it is commonly defined by the combinatorial graph Laplacian matrix L = D A, where A is the graph’s adjacency matrix and D the diagonal matrix whose entries are the row sums of A. Notice that the adjacency matrix A easily generalizes to a weight matrix W .",2.3. Proto-Value Functions,[0],[0]
"PVFs are defined to be the eigenvectors obtained after the eigendecomposition of L. Different diffusion models can be used to generate PVFs, such as the normalized graph Laplacian L = D 12 (D A)D 12 , which we use in this paper.",2.3. Proto-Value Functions,[0],[0]
"PVFs capture the large-scale geometry of the environment, such as symmetries and bottlenecks.",3. Option Discovery through the Laplacian,[0],[0]
"They are task independent, in the sense that they do not use information related to reward functions.",3. Option Discovery through the Laplacian,[0],[0]
"Moreover, they are defined over the whole state space since each eigenvector induces a realvalued mapping over each state.",3. Option Discovery through the Laplacian,[0],[0]
We can imagine that options with these properties should also be useful.,3. Option Discovery through the Laplacian,[0],[0]
"In this section we show how to use PVFs to discover options.
",3. Option Discovery through the Laplacian,[0],[0]
Let us start with an example.,3. Option Discovery through the Laplacian,[0],[0]
Consider the traditional 4- room domain depicted in Figure 1c.,3. Option Discovery through the Laplacian,[0],[0]
Gray squares represent walls and white squares represent accessible states.,3. Option Discovery through the Laplacian,[0],[0]
"Four actions are available: up, down, right, and left.",3. Option Discovery through the Laplacian,[0],[0]
The transitions are deterministic and the agent is not allowed to move into a wall.,3. Option Discovery through the Laplacian,[0],[0]
"Ideally, we would like to discover options that move the agent from room to room.",3. Option Discovery through the Laplacian,[0],[0]
"Thus, we should be able to automatically distinguish between the different rooms in the environment.",3. Option Discovery through the Laplacian,[0],[0]
"This is exactly what PVFs do, as depicted in Figure 2 (left).",3. Option Discovery through the Laplacian,[0],[0]
"Instead of interpreting a PVF as a basis function, we can interpret the PVF in our example as a desire to reach the highest point of the plot, corresponding to the centre of the room.",3. Option Discovery through the Laplacian,[0],[0]
"Because the sign of an eigenvector is arbitrary, a PVF can also be interpreted as a desire to reach the lowest point of the plot, corresponding to the opposite room.",3. Option Discovery through the Laplacian,[0],[0]
"In this paper we use the eigenvectors in both directions (i.e., both signs).
",3. Option Discovery through the Laplacian,[0],[0]
An eigenpurpose formalizes the interpretation above by defining an intrinsic reward function.,3. Option Discovery through the Laplacian,[0],[0]
"We can see it as defining a purpose for the agent, that is, to maximize the discounted sum of these rewards.
",3. Option Discovery through the Laplacian,[0],[0]
Definition 3.1 (Eigenpurpose).,3. Option Discovery through the Laplacian,[0],[0]
"An eigenpurpose is the intrinsic reward function re
i
(s, s 0 ) of a proto-value function
e 2 R|S| such that
r e i (s, s 0 )",3. Option Discovery through the Laplacian,[0],[0]
"= e > ( (s0) (s)), (1)
where (x) denotes the feature representation of state x.
Notice that an eigenpurpose, in the tabular case, can be written as re
i
(s, s 0 )",3. Option Discovery through the Laplacian,[0],[0]
"= e[s 0 ] e[s].
",3. Option Discovery through the Laplacian,[0],[0]
"We can now define a new MDP to learn the option associated with the purpose, Me
i = hS,A[{?}, re i , p, i, where
the reward function is defined as in (1) and the action set is augmented by the action terminate (?), which allows the agent to leave Me
i without any cost.",3. Option Discovery through the Laplacian,[0],[0]
The state space and the transition probability kernel remain unchanged from the original problem.,3. Option Discovery through the Laplacian,[0],[0]
"The discount rate can be chosen arbitrarily, although it impacts the timescale the option encodes.
",3. Option Discovery through the Laplacian,[0],[0]
"With Me i we define a new state-value function ve ⇡ (s), for policy ⇡, as the expected value of the cumulative discounted intrinsic reward if the agent starts in state s and follows policy ⇡ until termination.",3. Option Discovery through the Laplacian,[0],[0]
"Similarly, we define a new action-value function qe
⇡ (s, a) as the expected value of the cumulative discounted intrinsic reward if the agent starts in state s, takes action a, and then follows policy ⇡ until termination.",3. Option Discovery through the Laplacian,[0],[0]
"We can also describe the optimal value function for any eigenpurpose obtained through e:
v e ⇤(s) = max
⇡
v e ⇡ (s) and qe⇤(s, a) = max ⇡ q e ⇡ (s, a).
",3. Option Discovery through the Laplacian,[0],[0]
"These definitions naturally lead us to eigenbehaviors.
",3. Option Discovery through the Laplacian,[0],[0]
Definition 3.2 (Eigenbehavior).,3. Option Discovery through the Laplacian,[0],[0]
An eigenbehavior is a policy e : S !,3. Option Discovery through the Laplacian,[0],[0]
"A that is optimal with respect to the eigenpurpose re
i
, i.e., e(s) = argmax a2A",3. Option Discovery through the Laplacian,[0],[0]
"q e ⇤(s, a).
",3. Option Discovery through the Laplacian,[0],[0]
"Finding the optimal policy ⇡e⇤ now becomes a traditional RL problem, with a different reward function.",3. Option Discovery through the Laplacian,[0],[0]
"Importantly, this reward function tends to be dense, avoiding challenging situations due to exploration issues.",3. Option Discovery through the Laplacian,[0],[0]
"In this paper we use policy iteration to solve for an optimal policy.
",3. Option Discovery through the Laplacian,[0],[0]
"If each eigenpurpose defines an option, its corresponding eigenbehavior is the option’s policy.",3. Option Discovery through the Laplacian,[0],[0]
"Thus, we need to define the option’s initiation and termination set.",3. Option Discovery through the Laplacian,[0],[0]
"An option should be available in every state where it is possible to achieve its purpose, and to terminate when it is achieved.
",3. Option Discovery through the Laplacian,[0],[0]
"When defining the MDP to learn the option, we augmented the agent’s action set with the terminate action, allowing the agent to interrupt the option anytime.",3. Option Discovery through the Laplacian,[0],[0]
"We want options to terminate when the agent achieves its purpose, i.e., when it is unable to accumulate further positive intrinsic rewards.",3. Option Discovery through the Laplacian,[0],[0]
"With the defined reward function, this happens when the agent reaches the state with largest value in the eigenpurpose (or a local maximum when < 1).",3. Option Discovery through the Laplacian,[0],[0]
Any subsequent reward will be negative.,3. Option Discovery through the Laplacian,[0],[0]
"We are able to formalize this con-
dition by defining q",3. Option Discovery through the Laplacian,[0],[0]
"(s,?)",3. Option Discovery through the Laplacian,[0],[0]
.= 0,3. Option Discovery through the Laplacian,[0],[0]
"for all e. When the terminate action is selected, control is returned to the higher level policy (Dietterich, 2000).",3. Option Discovery through the Laplacian,[0],[0]
"An option following a policy e terminates when qe
(s, a)  0 for all a 2 A.",3. Option Discovery through the Laplacian,[0],[0]
"We define the initiation set to be all states in which there exists an action a 2 A such that qe
(s, a) > 0.",3. Option Discovery through the Laplacian,[0],[0]
"Thus, the option’s policy is ⇡e(s) =",3. Option Discovery through the Laplacian,[0],[0]
"argmax
a2A",3. Option Discovery through the Laplacian,[0],[0]
"[{?} q e ⇡ (s, a).",3. Option Discovery through the Laplacian,[0],[0]
We refer to the options discovered with our approach as eigenoptions.,3. Option Discovery through the Laplacian,[0],[0]
"The eigenoption corresponding to the example at the beginning of this section is depicted in Figure 2 (right).
",3. Option Discovery through the Laplacian,[0],[0]
"For any eigenoption, there is always at least one state in which it terminates, as we now show.",3. Option Discovery through the Laplacian,[0],[0]
Theorem 3.1 (Option’s Termination).,3. Option Discovery through the Laplacian,[0],[0]
"Consider an eigenoption o = hI
o
,⇡
o , T o",3. Option Discovery through the Laplacian,[0],[0]
i and < 1.,3. Option Discovery through the Laplacian,[0],[0]
"Then, in an MDP with finite state space, T
o
is nonempty.
",3. Option Discovery through the Laplacian,[0],[0]
Proof.,3. Option Discovery through the Laplacian,[0],[0]
"We can write the Bellman equation in the matrix form: v = r+ Tv, where v is a finite column vector with one entry per state encoding its value function.",3. Option Discovery through the Laplacian,[0],[0]
"From (1) we have r = Tw w with w = (s)>e, where e denotes the eigenpurpose of interest.",3. Option Discovery through the Laplacian,[0],[0]
"Therefore:
v +w = Tw + Tv
= (1 )Tw + T (v +w) = (1 )(I T ) 1Tw.
",3. Option Discovery through the Laplacian,[0],[0]
||v +w||1 = (1 )||(I,3. Option Discovery through the Laplacian,[0],[0]
T ),3. Option Discovery through the Laplacian,[0],[0]
1Tw||1 ||v +w||1  (1 )||(I,3. Option Discovery through the Laplacian,[0],[0]
"T ) 1T ||1||w||1
||v +w||1  (1 ) 1 (1 ) ||w||1
||v +w||1  ||w||1
We can shift w by any finite constant without changing the reward, i.e., Tw w = T (w+ ) (w+ )",3. Option Discovery through the Laplacian,[0],[0]
"because T1 = 1 since P j T i,j
= 1.",3. Option Discovery through the Laplacian,[0],[0]
"Hence, we can assume w 0.",3. Option Discovery through the Laplacian,[0],[0]
"Let s ⇤ = argmax
s
w
s ⇤ , so that w s ⇤ = ||w||1.",3. Option Discovery through the Laplacian,[0],[0]
"Clearly vs⇤ 
0, otherwise ||v +w||1 |vs⇤ +",3. Option Discovery through the Laplacian,[0],[0]
ws⇤ | = vs⇤ +,3. Option Discovery through the Laplacian,[0],[0]
"ws⇤ > w
s ⇤ = ||w||1, arriving at a contradiction.
",3. Option Discovery through the Laplacian,[0],[0]
This result is applicable in both the tabular and linear function approximation case.,3. Option Discovery through the Laplacian,[0],[0]
An algorithm that does not rely on knowing the underlying graph is provided in Section 5.,3. Option Discovery through the Laplacian,[0],[0]
"We used three MDPs in our empirical study (c.f. Figure 1): an open room, an I-Maze, and the 4-room domain.",4. Empirical Evaluation,[0],[0]
Their transitions are deterministic and gray squares denote walls.,4. Empirical Evaluation,[0],[0]
"Agents have access to four actions: up, down, right, and left.",4. Empirical Evaluation,[0],[0]
"When an action that would have taken the agent into a wall is chosen, the agent’s state does not change.",4. Empirical Evaluation,[0],[0]
"We demonstrate three aspects of our framework:1
• How the eigenoptions present specific purposes.",4. Empirical Evaluation,[0],[0]
"Interestingly, options leading to bottlenecks are not the first ones we discover.
",4. Empirical Evaluation,[0],[0]
"• How eigenoptions improve exploration by reducing the expected number of steps required to navigate between any two states.
",4. Empirical Evaluation,[0],[0]
• How eigenoptions help agents to accumulate reward faster.,4. Empirical Evaluation,[0],[0]
We show how few options may hurt the agents’ performance while enough options speed up learning.,4. Empirical Evaluation,[0],[0]
"In the PVF theory, the “smoothest” eigenvectors, corresponding to the smallest eigenvalues, are preferred (Mahadevan & Maggioni, 2007).",4.1. Discovered Options,[0],[0]
"The same intuition applies to eigenoptions, with the eigenpurposes corresponding to the smallest eigenvalues being preferred.",4.1. Discovered Options,[0],[0]
"Figures 3, 4, and 5 depict the first eigenoptions discovered in the three domains used for evaluation.
",4.1. Discovered Options,[0],[0]
"Eigenoptions do not necessarily look for bottleneck states, 1Python code can be found at: https://github.com/mcmachado/options
allowing us to apply our algorithm in many environments in which there are no obvious, or meaningful, bottlenecks.",4.1. Discovered Options,[0],[0]
"We discover meaningful options in these environments, such as walking down a corridor, or going to the corners of an open room.",4.1. Discovered Options,[0],[0]
"Interestingly, doorways are not the first options we discover in the 4-room domain (the fifth eigenoption is the first to terminate at the entrance of a doorway).",4.1. Discovered Options,[0],[0]
"In the next sections we provide empirical evidence that eigenoptions are useful, and often more so than bottleneck options.",4.1. Discovered Options,[0],[0]
"A major challenge for agents to explore an environment is to be decisive, avoiding the dithering commonly observed in random walks (Machado & Bowling, 2016; Osband et al., 2016).",4.2. Exploration,[0],[0]
Options provide such decisiveness by operating in a higher level of abstraction.,4.2. Exploration,[0],[0]
"Agents performing a random walk, when equipped with options, are expected to cover larger distances in the state space, navigating back and forth between subgoals instead of dithering around the starting state.",4.2. Exploration,[0],[0]
"However, options need to satisfy two conditions to improve exploration: (1) they have to be available in several parts of the state space, ensuring the agent always has access to many different options; and (2) they have to operate at different time scales.",4.2. Exploration,[0],[0]
"For instance, in the 4-room domain, it is unlikely an agent randomly selects enough primitive actions leading it to a corner if all options move the agent between doorways.",4.2. Exploration,[0],[0]
"An important result in this section is to show that it is very unlikely for an agent to explore the whole environment if it keeps going back and forth between similar high-level goals.
",4.2. Exploration,[0],[0]
Eigenoptions satisfy both conditions.,4.2. Exploration,[0],[0]
"As demonstrated in Section 4.1, eigenoptions are often defined in the whole state space, allowing sequencing.",4.2. Exploration,[0],[0]
"Moreover, PVFs can be seen as a “frequency” basis, with different PVFs being associated with different frequencies (Mahadevan & Maggioni, 2007).",4.2. Exploration,[0],[0]
"The corresponding eigenoptions also operate
at different frequencies, with the length of a trajectory until termination varying.",4.2. Exploration,[0],[0]
This behavior can be seen when comparing the second and fourth eigenoptions in the 10 ⇥ 10 grid (Figure 3).,4.2. Exploration,[0],[0]
"The fourth eigenoption terminates, on expectation, twice as often as the second eigenoption.
",4.2. Exploration,[0],[0]
In this section we show that eigenoptions improve exploration.,4.2. Exploration,[0],[0]
"We do so by introducing a new metric, which we call diffusion time.",4.2. Exploration,[0],[0]
Diffusion time encodes the expected number of steps required to navigate between two states randomly chosen in the MDP while following a random walk.,4.2. Exploration,[0],[0]
A small expected number of steps implies that it is more likely that the agent will reach all states with a random walk.,4.2. Exploration,[0],[0]
"We discuss how this metric can be computed in the Appendix.
Figure 6 depicts, for our the three environments, the diffusion time with options and the diffusion time using only primitive actions.",4.2. Exploration,[0],[0]
"We add options incrementally in order of increasing eigenvalue when computing the diffusion time for different sets of options.
",4.2. Exploration,[0],[0]
"The first options added hurt exploration, but when enough options are added, exploration is greatly improved when compared to a random walk using only primitive actions.",4.2. Exploration,[0],[0]
"The fact that few options hurt exploration may be surprising at first, based on the fact that few useful options are generally sought after in the literature.",4.2. Exploration,[0],[0]
"However, this is a major difference between using options for planning and for learning.",4.2. Exploration,[0],[0]
"In planning, options shortcut the agents’ trajectories, pruning the search space.",4.2. Exploration,[0],[0]
All other actions are still taken into consideration.,4.2. Exploration,[0],[0]
"When exploring, a uniformly random policy over options and primitive actions skews where
agents spend their time.",4.2. Exploration,[0],[0]
"Options that are much longer than primitive actions reduce the likelihood that an agent will deviate much from the options’ trajectories, since sampling an option may undo dozens of primitive actions.",4.2. Exploration,[0],[0]
"This biasing is often observed when fewer options are available.
",4.2. Exploration,[0],[0]
The discussion above can be made clearer with an example.,4.2. Exploration,[0],[0]
"In the 4-room domain, if the only options available are those leading the agent to doorways (c.f. Appendix), it is less likely the agent will reach the outer corners.",4.2. Exploration,[0],[0]
To do so the agent would have to select enough consecutive primitive actions without sampling an option.,4.2. Exploration,[0],[0]
"Also, it is very likely agents will be always moving between rooms, never really exploring inside a room.",4.2. Exploration,[0],[0]
These issues are mitigated with eigenoptions.,4.2. Exploration,[0],[0]
"The first eigenoptions lead agents to individual rooms, but other eigenoptions operate in different time scales, allowing agents to explore different parts of rooms.
",4.2. Exploration,[0],[0]
"Figure 6d supports the intuition that options leading to bottleneck states are not sufficient, by themselves, for exploration.",4.2. Exploration,[0],[0]
It shows how the diffusion time in the 4-room domain is increased when only bottleneck options are used.,4.2. Exploration,[0],[0]
"As in the PVF literature, the ideal number of options to be used by an agent can be seen as a model selection problem.",4.2. Exploration,[0],[0]
We now illustrate the usefulness of our options when the agent’s goal is to accumulate reward.,4.3. Accumulating Rewards,[0],[0]
We also study the impact of an increasing number of options in such a task.,4.3. Accumulating Rewards,[0],[0]
"In these experiments, the agent starts at the bottom left cor-
ner and its goal is to reach the top right corner.",4.3. Accumulating Rewards,[0],[0]
"The agent observes a reward of 0 until the goal is reached, when it observes a reward of +1.",4.3. Accumulating Rewards,[0],[0]
"We used Q-Learning (Watkins & Dayan, 1992) (↵ = 0.1, = 0.9) to learn a policy over primitive actions.",4.3. Accumulating Rewards,[0],[0]
"The behavior policy chooses uniformly over primitive actions and options, following them until termination.",4.3. Accumulating Rewards,[0],[0]
"Figure 7 depicts, after learning for a given number of episodes, the average over 100 trials of the agents’ final performance.",4.3. Accumulating Rewards,[0],[0]
"Episodes were 100 time steps long, and we learned for 250 episodes in the 10 ⇥ 10 grid and in the I-Maze, and for 500 episodes in the 4-room domain.
",4.3. Accumulating Rewards,[0],[0]
In most scenarios eigenoptions improve performance.,4.3. Accumulating Rewards,[0],[0]
"As in the previous section, exceptions occur when only a few options are added to the agent’s action set.",4.3. Accumulating Rewards,[0],[0]
The best results were obtained using 64 options.,4.3. Accumulating Rewards,[0],[0]
"Despite being an additional parameter, our results show that the agent’s performance is fairly robust across different numbers of options.
",4.3. Accumulating Rewards,[0],[0]
Eigenoptions are task-independent by construction.,4.3. Accumulating Rewards,[0],[0]
Additional results in the appendix show how the same set of eigenoptions is able to speed-up learning in different tasks.,4.3. Accumulating Rewards,[0],[0]
"In the appendix we also compare eigenoptions to random options, that is, options that use a random state as subgoal.",4.3. Accumulating Rewards,[0],[0]
So far we have assumed that agents have access to the adjacency matrix representing the underlying MDP.,5. Approximate Option Discovery,[0],[0]
"However, in practical settings this is generally not true.",5. Approximate Option Discovery,[0],[0]
"In fact, the number of states in these settings is often so large that agents rarely visit the same state twice.",5. Approximate Option Discovery,[0],[0]
"These problems are generally tackled with sample-based methods and some sort of function approximation.
",5. Approximate Option Discovery,[0],[0]
In this section we propose a sample-based approach for option discovery that asymptotically discovers eigenoptions.,5. Approximate Option Discovery,[0],[0]
We then extend this algorithm to linear function approximation.,5. Approximate Option Discovery,[0],[0]
We provide anecdotal evidence in Atari 2600 games that this relatively naı̈ve sample-based approach to function approximation discovers purposeful options.,5. Approximate Option Discovery,[0],[0]
"In the online setting, agents must sample trajectories.",5.1. Sample-based Option Discovery,[0],[0]
"Naturally, one can sample trajectories until one is able to perfectly construct the MDP’s adjacency matrix, as suggested by Mahadevan & Maggioni (2007).",5.1. Sample-based Option Discovery,[0],[0]
"However, this approach does not easily extend to linear function approximation.",5.1. Sample-based Option Discovery,[0],[0]
"In this section we provide an approach that does not build the adjacency matrix allowing us to extend the concept of eigenpurposes to linear function approximation.
",5.1. Sample-based Option Discovery,[0],[0]
"In our algorithm, a sample transition is added to a matrix T if it was not previously encountered.",5.1. Sample-based Option Discovery,[0],[0]
"The transition is added as the difference between the current and previous observations, i.e., (s0) (s).",5.1. Sample-based Option Discovery,[0],[0]
"In the tabular case we define (s) to be the one-hot encoding of state s. Once enough transitions have been sampled, we perform a singular value decomposition on the matrix T such that T = U⌃V
>.",5.1. Sample-based Option Discovery,[0],[0]
"We use the columns of V , which correspond to the right-eigenvectors of T , to generate the eigenpurposes.",5.1. Sample-based Option Discovery,[0],[0]
"The intrinsic reward and the termination criterion for an eigenbehavior are the same as before.
",5.1. Sample-based Option Discovery,[0],[0]
Matrix T is known as the incidence matrix.,5.1. Sample-based Option Discovery,[0],[0]
"If all transitions in the graph are sampled once, for tabular representations, this algorithm discovers the same options we obtain with the combinatorial Laplacian.",5.1. Sample-based Option Discovery,[0],[0]
"The theorem below states the equivalence between the obtained eigenpurposes.
",5.1. Sample-based Option Discovery,[0],[0]
Theorem 5.1.,5.1. Sample-based Option Discovery,[0],[0]
"Consider the SVD of T = U T ⌃ T V > T , with each row of T consisting of the difference between observations, i.e., (s0) (s).",5.1. Sample-based Option Discovery,[0],[0]
"In the tabular case, if all transitions in the MDP have been sampled once, the orthonormal eigenvectors of L are the columns of V >
T
.
Proof.",5.1. Sample-based Option Discovery,[0],[0]
"Given the SVD decomposition of a matrix A = U⌃V
>, the columns of V are the eigenvectors of A > A (Strang, 2005).",5.1. Sample-based Option Discovery,[0],[0]
"We know that T>T = 2L, where L = D W (Lemma 5.1, c.f. Appendix).",5.1. Sample-based Option Discovery,[0],[0]
"Thus, the columns of V
T are the eigenvectors of T>T , which can be rewritten as 2(D W ).",5.1. Sample-based Option Discovery,[0],[0]
"Therefore, the columns of V
T are also the eigenvectors of L.
There is a trade-off between reconstructing the adjacency matrix and constructing the incidence matrix.",5.1. Sample-based Option Discovery,[0],[0]
"In MDPs in which states are sparsely connected, such as the I-Maze, the latter is preferred since it has fewer transitions than states.",5.1. Sample-based Option Discovery,[0],[0]
"However, what makes this result interesting is the fact that our algorithm can be easily generalized to linear function approximation.",5.1. Sample-based Option Discovery,[0],[0]
An adjacency matrix is not very useful when the agent has access only to features of the state.,5.2. Function Approximation,[0],[0]
"However, we can use the intuition about the incidence matrix to propose an algorithm compatible with linear function approximation.
",5.2. Function Approximation,[0],[0]
"In fact, to apply the algorithm proposed in the previous section, we just need to define what constitutes a new transition.",5.2. Function Approximation,[0],[0]
"We define two vectors, t and t0, to be identical if and only if t t0 = 0.",5.2. Function Approximation,[0],[0]
We then use a set data structure to avoid duplicates when storing (s0) (s).,5.2. Function Approximation,[0],[0]
"This is a naı̈ve approach, but it provides encouraging evidence eigenoptions generalize to linear function approximation.",5.2. Function Approximation,[0],[0]
"We expect more involved methods to perform even better.
",5.2. Function Approximation,[0],[0]
"We tested our method in the ALE (Bellemare et al., 2013).",5.2. Function Approximation,[0],[0]
"The agent’s representation consists of the emulator’s RAM state (1,024 bits).",5.2. Function Approximation,[0],[0]
"The final incidence matrix in which we ran the SVD had 25,000 rows, which we sampled uniformly from the set of observed transitions.",5.2. Function Approximation,[0],[0]
"We provide further details of the experimental setup in the appendix.
",5.2. Function Approximation,[0],[0]
"In the tabular case we start selecting eigenpurposes generated by the eigenvectors with smallest eigenvalue, because these are the “smoothest” ones.",5.2. Function Approximation,[0],[0]
"However, it is not clear such intuition holds here because we are in the function approximation setting and the matrix of transitions does not contain all possible transitions.",5.2. Function Approximation,[0],[0]
"Therefore, we analyzed, for each game, all 1,024 discovered options.
",5.2. Function Approximation,[0],[0]
We approximate these options greedily ( = 0) with the ALE emulator’s look-ahead.,5.2. Function Approximation,[0],[0]
"The next action a0 for an eigenpurpose e is selected as argmax b2A R s 0 p(s 0|s, b) re i (s, s 0 ).
",5.2. Function Approximation,[0],[0]
"Even with such a myopic action selection mechanism we
were able to obtain options that clearly demonstrate intent.",5.2. Function Approximation,[0],[0]
"In FREEWAY, a game in which a chicken is expected to cross the road while avoiding cars, we observe options in which the agent clearly wants to reach a specific lane in the street.",5.2. Function Approximation,[0],[0]
Figure 8 (left) depicts where the chicken tends to be when the option is executed.,5.2. Function Approximation,[0],[0]
On the right we see a histogram representing the chicken’s height during an episode.,5.2. Function Approximation,[0],[0]
"We can clearly see how the chicken’s height varies for different options, and how a random walk over primitive actions (rand) does not explore the environment properly.",5.2. Function Approximation,[0],[0]
"Remarkably, option #445 scores 28 points at the end of the episode, without ever explicitly taking the reward signal into consideration.",5.2. Function Approximation,[0],[0]
"This performance is very close to those obtained by state-of-the-art algorithms.
",5.2. Function Approximation,[0],[0]
"In MONTEZUMA’S REVENGE, a game in which the agent needs to navigate through a room to pickup a key so it can open a door, we also observe the agent having the clear intent of reaching particular positions on the screen, such as staircases, ropes and doors (Figure 9).",5.2. Function Approximation,[0],[0]
"Interestingly, the options we discover are very similar to those handcrafted by Kulkarni et al. (2016) when evaluating the usefulness of options to tackle such a game.",5.2. Function Approximation,[0],[0]
A video of the highlighted options can be found online.2,5.2. Function Approximation,[0],[0]
Most algorithms for option discovery can be seen as topdown approaches.,6. Related Work,[0],[0]
"Agents use trajectories leading to informative rewards3 as a starting point, decomposing and refining them into options.",6. Related Work,[0],[0]
"There are many approaches based on this principle, such as methods that use the observed rewards to generate intrinsic rewards leading to new value functions (e.g., McGovern & Barto, 2001; Menache et al., 2002; Konidaris & Barto, 2009), methods that use the observed rewards to climb a gradient (e.g., Mankowitz et al., 2016; Vezhnevets et al., 2016; Bacon et al., 2017), or to do
2 https://youtu.be/2BVicx4CDWA
3We define an informative reward to be the signal that informs the agent it has reached a goal.",6. Related Work,[0],[0]
"For example, when trying to escape from a maze, we consider 0 to be an informative reward if the agent observes rewards of value 1 in every time step it is inside the maze.",6. Related Work,[0],[0]
"A different example is a positive reward observed by an agent that typically observes rewards of value 0.
",6. Related Work,[0],[0]
"probabilistic inference (Daniel et al., 2016).",6. Related Work,[0],[0]
"However, such approaches are not applicable in large state spaces with sparse rewards.",6. Related Work,[0],[0]
"If informative rewards are unlikely to be found by an agent using only primitive actions, requiring long or specific sequences of actions, options are equally unlikely to be discovered.
",6. Related Work,[0],[0]
"Our algorithm can be seen as a bottom-up approach, in which options are constructed before the agent observes any informative reward.",6. Related Work,[0],[0]
These options are composed to generate the desired policy.,6. Related Work,[0],[0]
"Options discovered this way tend to be independent of an agent’s intention, and are potentially useful in many different tasks (Gregor et al., 2016).",6. Related Work,[0],[0]
"Such options can also be seen as being useful for exploration by allowing agents to commit to a behavior for an extended period of time (Machado & Bowling, 2016).",6. Related Work,[0],[0]
"Among the approaches to discover options without using extrinsic rewards are the use of global or local graph centrality measures (Şimşek & Barto, 2004; Şimşek et al., 2005; Şimşek & Barto, 2008) and clustering of states (Mannor et al., 2004; Bacon, 2013; Lakshminarayanan et al., 2016).",6. Related Work,[0],[0]
"Interestingly, Şimşek et al. (2005) and Lakshminarayanan et al. (2016) also use the graph Laplacian in their algorithm, but to identify bottleneck states.
",6. Related Work,[0],[0]
Baranes & Oudeyer (2013) and Moulin-Frier & Oudeyer (2013) show how one can build policies to explicitly assist agents to explore the environment.,6. Related Work,[0],[0]
The proposed algorithms self-generate subgoals in order to maximize learning progress.,6. Related Work,[0],[0]
The policies built can be seen as options.,6. Related Work,[0],[0]
"Recently, Solway et al. (2014) proved that “optimal hierarchy minimizes the geometric mean number of trial-and-error attempts necessary for the agent to discover the optimal policy for any selected task (...)”.",6. Related Work,[0],[0]
"Our experiments confirm this result, although we propose diffusion time as a different metric to evaluate how options improve exploration.
",6. Related Work,[0],[0]
The idea of discovering options by learning to control parts of the environment is also related to our work.,6. Related Work,[0],[0]
"Eigenpurposes encode different rates of change in the agents representation of the world, while the corresponding options aim at maximizing such change.",6. Related Work,[0],[0]
Others have also proposed ways to discover options based on the idea of learning to control the environment.,6. Related Work,[0],[0]
"Hengst (2002), for instance, proposes an algorithm that explicitly models changes in the variables that form the agent’s representation.",6. Related Work,[0],[0]
"Recently, Gregor et al. (2016) proposed an algorithm in which agents discover options by maximizing a notion of empowerment (Salge et al., 2014), where the agent aims at getting to states with a maximal set of available intrinsic options.
",6. Related Work,[0],[0]
"Continual Curiosity driven Skill Acquisition (CCSA) (Kompella et al., In Press) is the closest approach to ours.",6. Related Work,[0],[0]
CCSA also discovers skills that maximize an intrinsic reward obtained by some extracted representation.,6. Related Work,[0],[0]
"While we use PVFs, CCSA uses Incremental Slow Feature Analysis
(SFA) (Kompella et al., 2011) to define the intrinsic reward function.",6. Related Work,[0],[0]
"Sprekeler (2011) has shown that, given a specific choice of adjacency function, PVFs are equivalent to SFA (Wiskott & Sejnowski, 2002).",6. Related Work,[0],[0]
SFA becomes an approximation of PVFs if the function space used in the SFA does not allow arbitrary mappings from the observed data to an embedding.,6. Related Work,[0],[0]
"Our method differs in how we define the initiation and termination sets, as well as in the objective being maximized.",6. Related Work,[0],[0]
"CCSA acquires skills that produce a large variation in the slow-feature outputs, leading to options that seek for bottlenecks.",6. Related Work,[0],[0]
"Our approach does not seek for bottlenecks, focusing on traversing different directions of the learned representation.",6. Related Work,[0],[0]
"Being able to properly abstract MDPs into SMDPs can reduce the overall expense of learning (Sutton et al., 1999; Solway et al., 2014), mainly when the learned options are reused in multiple tasks.",7. Conclusion,[0],[0]
"On the other hand, the wrong hierarchy can hinder the agents’ learning process, moving the agent away from desired goal states.",7. Conclusion,[0],[0]
"Current algorithms for option discovery often depend on an initial informative reward signal, which may not be readily available in large MDPs.",7. Conclusion,[0],[0]
"In this paper, we introduced an approach that is effective in different environments, for a multitude of tasks.
",7. Conclusion,[0],[0]
"Our algorithm uses the graph Laplacian, being directly related to the concept of proto-value functions.",7. Conclusion,[0],[0]
The learned representation informs the agent what are meaningful options to be sought after.,7. Conclusion,[0],[0]
The discovered options can be seen as traversing each one of the dimensions in the learned representation.,7. Conclusion,[0],[0]
We believe successful algorithms in the future will be able to simultaneously discover representations and options.,7. Conclusion,[0],[0]
"Agents will use their learned representation to discover options, which will be used to further explore the environment, improving the agent’s representation.
",7. Conclusion,[0],[0]
"Interestingly, the options first discovered by our approach do not necessarily find bottlenecks, which are commonly sought after.",7. Conclusion,[0],[0]
"In this paper we showed how bottleneck options can hinder exploration strategies if naively added to the agent’s action set, and how the options we discover can help an agent to explore.",7. Conclusion,[0],[0]
"Also, we have shown how the discovered options can be used to accumulate reward in a multitude of tasks, leveraging their exploratory properties.
",7. Conclusion,[0],[0]
There are several exciting avenues for future work.,7. Conclusion,[0],[0]
"As noted, SFA can be seen as an approximation to PVFs.",7. Conclusion,[0],[0]
It would be interesting to compare such an approach to eigenoptions.,7. Conclusion,[0],[0]
It would also be interesting to see if the options we discover can be generated incrementally and with incomplete graphs.,7. Conclusion,[0],[0]
"Finally, one can also imagine extensions to the proposed algorithm where a hierarchy of options is built.",7. Conclusion,[0],[0]
"The authors would like to thank Will Dabney, Rémi Munos and Csaba Szepesvári for useful discussions.",Acknowledgements,[0],[0]
This work was supported by grants from Alberta Innovates Technology Futures and the Alberta Machine Intelligence Institute (Amii).,Acknowledgements,[0],[0]
Computing resources were provided by Compute Canada through CalculQuébec.,Acknowledgements,[0],[0]
Representation learning and option discovery are two of the biggest challenges in reinforcement learning (RL).,abstractText,[0],[0]
Proto-value functions (PVFs) are a well-known approach for representation learning in MDPs.,abstractText,[0],[0]
In this paper we address the option discovery problem by showing how PVFs implicitly define options.,abstractText,[0],[0]
"We do it by introducing eigenpurposes, intrinsic reward functions derived from the learned representations.",abstractText,[0],[0]
The options discovered from eigenpurposes traverse the principal directions of the state space.,abstractText,[0],[0]
They are useful for multiple tasks because they are discovered without taking the environment’s rewards into consideration.,abstractText,[0],[0]
"Moreover, different options act at different time scales, making them helpful for exploration.",abstractText,[0],[0]
We demonstrate features of eigenpurposes in traditional tabular domains as well as in Atari 2600 games.,abstractText,[0],[0]
A Laplacian Framework for Option Discovery in Reinforcement Learning,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632–642, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
"The semantic concepts of entailment and contradiction are central to all aspects of natural language meaning (Katz, 1972; van Benthem, 2008), from the lexicon to the content of entire texts.",1 Introduction,[0],[0]
"Thus, natural language inference (NLI) — characterizing and using these relations in computational systems (Fyodorov et al., 2000; Condoravdi et al., 2003; Bos and Markert, 2005; Dagan et al., 2006; MacCartney and Manning, 2009) — is essential in tasks ranging from information retrieval to semantic parsing to commonsense reasoning.
",1 Introduction,[0],[0]
"NLI has been addressed using a variety of techniques, including those based on symbolic logic, knowledge bases, and neural networks.",1 Introduction,[0],[0]
"In recent years, it has become an important testing ground
for approaches employing distributed word and phrase representations.",1 Introduction,[0],[0]
"Distributed representations excel at capturing relations based in similarity, and have proven effective at modeling simple dimensions of meaning like evaluative sentiment (e.g., Socher et al. 2013), but it is less clear that they can be trained to support the full range of logical and commonsense inferences required for NLI (Bowman et al., 2015; Weston et al., 2015b; Weston et al., 2015a).",1 Introduction,[0],[0]
"In a SemEval 2014 task aimed at evaluating distributed representations for NLI, the best-performing systems relied heavily on additional features and reasoning capabilities (Marelli et al., 2014a).
",1 Introduction,[0],[0]
"Our ultimate objective is to provide an empirical evaluation of learning-centered approaches to NLI, advancing the case for NLI as a tool for the evaluation of domain-general approaches to semantic representation.",1 Introduction,[0],[0]
"However, in our view, existing NLI corpora do not permit such an assessment.",1 Introduction,[0],[0]
"They are generally too small for training modern data-intensive, wide-coverage models, many contain sentences that were algorithmically generated, and they are often beset with indeterminacies of event and entity coreference that significantly impact annotation quality.
",1 Introduction,[0],[0]
"To address this, this paper introduces the Stanford Natural Language Inference (SNLI) corpus, a collection of sentence pairs labeled for entailment, contradiction, and semantic independence.",1 Introduction,[0],[0]
"At 570,152 sentence pairs, SNLI is two orders of magnitude larger than all other resources of its type.",1 Introduction,[0],[0]
"And, in contrast to many such resources, all of its sentences and labels were written by humans in a grounded, naturalistic context.",1 Introduction,[0],[0]
"In a separate validation phase, we collected four additional judgments for each label for 56,941 of the examples.",1 Introduction,[0],[0]
"Of these, 98% of cases emerge with a threeannotator consensus, and 58% see a unanimous consensus from all five annotators.
",1 Introduction,[0],[0]
"In this paper, we use this corpus to evaluate
632
a variety of models for natural language inference, including rule-based systems, simple linear classifiers, and neural network-based models.",1 Introduction,[0],[0]
We find that two models achieve comparable performance: a feature-rich classifier model and a neural network model centered around a Long Short-Term Memory network (LSTM; Hochreiter and Schmidhuber 1997).,1 Introduction,[0],[0]
"We further evaluate the LSTM model by taking advantage of its ready support for transfer learning, and show that it can be adapted to an existing NLI challenge task, yielding the best reported performance by a neural network model and approaching the overall state of the art.",1 Introduction,[0],[0]
"To date, the primary sources of annotated NLI corpora have been the Recognizing Textual Entailment (RTE) challenge tasks.1 These are generally high-quality, hand-labeled data sets, and they have stimulated innovative logical and statistical models of natural language reasoning, but their small size (fewer than a thousand examples each) limits their utility as a testbed for learned distributed representations.",2 A new corpus for NLI,[0],[0]
"The data for the SemEval 2014 task called Sentences Involving Compositional Knowledge (SICK) is a step up in terms of size, but only to 4,500 training examples, and its partly automatic construction introduced some spurious patterns into the data (Marelli et al. 2014a, §6).",2 A new corpus for NLI,[0],[0]
"The Denotation Graph entailment set (Young et al., 2014) contains millions of examples of entailments between sentences and artificially constructed short phrases, but it was labeled using fully automatic methods, and is noisy enough that it is probably suitable only as a source of sup-
1http://aclweb.org/aclwiki/index.php?",2 A new corpus for NLI,[0],[0]
"title=Textual_Entailment_Resource_Pool
plementary training data.",2 A new corpus for NLI,[0],[0]
"Outside the domain of sentence-level entailment, Levy et al. (2014) introduce a large corpus of semi-automatically annotated entailment examples between subject–verb– object relation triples, and the second release of the Paraphrase Database (Pavlick et al., 2015) includes automatically generated entailment annotations over a large corpus of pairs of words and short phrases.
",2 A new corpus for NLI,[0],[0]
Existing resources suffer from a subtler issue that impacts even projects using only humanprovided annotations: indeterminacies of event and entity coreference lead to insurmountable indeterminacy concerning the correct semantic label (de Marneffe et al. 2008,2 A new corpus for NLI,[0],[0]
§4.3; Marelli et al. 2014b).,2 A new corpus for NLI,[0],[0]
"For an example of the pitfalls surrounding entity coreference, consider the sentence pair A boat sank in the Pacific Ocean and A boat sank in the Atlantic Ocean.",2 A new corpus for NLI,[0],[0]
"The pair could be labeled as a contradiction if one assumes that the two sentences refer to the same single event, but could also be reasonably labeled as neutral if that assumption is not made.",2 A new corpus for NLI,[0],[0]
"In order to ensure that our labeling scheme assigns a single correct label to every pair, we must select one of these approaches across the board, but both choices present problems.",2 A new corpus for NLI,[0],[0]
"If we opt not to assume that events are coreferent, then we will only ever find contradictions between sentences that make broad universal assertions, but if we opt to assume coreference, new counterintuitive predictions emerge.",2 A new corpus for NLI,[0],[0]
"For example, Ruth Bader Ginsburg was appointed to the US Supreme Court and I had a sandwich for lunch today would unintuitively be labeled as a contradiction, rather than neutral, under this assumption.
",2 A new corpus for NLI,[0],[0]
"Entity coreference presents a similar kind of indeterminacy, as in the pair A tourist visited New
York and A tourist visited the city.",2 A new corpus for NLI,[0],[0]
"Assuming coreference between New York and the city justifies labeling the pair as an entailment, but without that assumption the city could be taken to refer to a specific unknown city, leaving the pair neutral.",2 A new corpus for NLI,[0],[0]
"This kind of indeterminacy of label can be resolved only once the questions of coreference are resolved.
",2 A new corpus for NLI,[0],[0]
"With SNLI, we sought to address the issues of size, quality, and indeterminacy.",2 A new corpus for NLI,[0],[0]
"To do this, we employed a crowdsourcing framework with the following crucial innovations.",2 A new corpus for NLI,[0],[0]
"First, the examples were grounded in specific scenarios, and the premise and hypothesis sentences in each example were constrained to describe that scenario from the same perspective, which helps greatly in controlling event and entity coreference.2 Second, the prompt gave participants the freedom to produce entirely novel sentences within the task setting, which led to richer examples than we see with the more proscribed string-editing techniques of earlier approaches, without sacrificing consistency.",2 A new corpus for NLI,[0],[0]
"Third, a subset of the resulting sentences were sent to a validation task aimed at providing a highly reliable set of annotations over the same data, and at identifying areas of inferential uncertainty.",2 A new corpus for NLI,[0],[0]
We used Amazon Mechanical Turk for data collection.,2.1 Data collection,[0],[0]
"In each individual task (each HIT), a worker was presented with premise scene descriptions from a pre-existing corpus, and asked to supply hypotheses for each of our three labels— entailment, neutral, and contradiction—forcing the data to be balanced among these classes.
",2.1 Data collection,[0],[0]
The instructions that we provided to the workers are shown in Figure 1.,2.1 Data collection,[0],[0]
"Below the instructions were three fields for each of three requested sentences, corresponding to our entailment, neutral, and contradiction labels, a fourth field (marked optional) for reporting problems, and a link to an FAQ page.",2.1 Data collection,[0],[0]
That FAQ grew over the course of data collection.,2.1 Data collection,[0],[0]
"It warned about disallowed techniques (e.g., reusing the same sentence for many different prompts, which we saw in a few cases), provided guidance concerning sentence length and
2 Issues of coreference are not completely solved, but greatly mitigated.",2.1 Data collection,[0],[0]
"For example, with the premise sentence A dog is lying in the grass, a worker could safely assume that the dog is the most prominent thing in the photo, and very likely the only dog, and build contradicting sentences assuming reference to the same dog.
complexity (we did not enforce a minimum length, and we allowed bare NPs as well as full sentences), and reviewed logistical issues around payment timing.",2.1 Data collection,[0],[0]
"About 2,500 workers contributed.
",2.1 Data collection,[0],[0]
"For the premises, we used captions from the Flickr30k corpus (Young et al., 2014), a collection of approximately 160k captions (corresponding to about 30k images) collected in an earlier crowdsourced effort.3",2.1 Data collection,[0],[0]
"The captions were not authored by the photographers who took the source images, and they tend to contain relatively literal scene descriptions that are suited to our approach, rather than those typically associated with personal photographs (as in their example: Our trip to the Olympic Peninsula).",2.1 Data collection,[0],[0]
"In order to ensure that the label for each sentence pair can be recovered solely based on the available text, we did not use the images at all during corpus collection.
",2.1 Data collection,[0],[0]
"Table 2 reports some key statistics about the collected corpus, and Figure 2 shows the distributions of sentence lengths for both our source hypotheses and our newly collected premises.",2.1 Data collection,[0],[0]
"We observed that while premise sentences varied considerably in length, hypothesis sentences tended to be as
3 We additionally include about 4k sentence pairs from a pilot study in which the premise sentences were instead drawn from the VisualGenome corpus (under construction; visualgenome.org).",2.1 Data collection,[0],[0]
"These examples appear only in the training set, and have pair identifiers prefixed with vg in our corpus.
Data set sizes: Training pairs 550,152 Development pairs 10,000 Test pairs 10,000
Sentence length:",2.1 Data collection,[0],[0]
"Premise mean token count 14.1 Hypothesis mean token count 8.3
Parser output:",2.1 Data collection,[0],[0]
"Premise ‘S’-rooted parses 74.0% Hypothesis ‘S’-rooted parses 88.9% Distinct words (ignoring case) 37,026
Table 2:",2.1 Data collection,[0],[0]
Key statistics for the raw sentence pairs in SNLI.,2.1 Data collection,[0],[0]
"Since the two halves of each pair were collected separately, we report some statistics for both.
short as possible while still providing enough information to yield a clear judgment, clustering at around seven words.",2.1 Data collection,[0],[0]
"We also observed that the bulk of the sentences from both sources were syntactically complete rather than fragments, and the frequency with which the parser produces a parse rooted with an ‘S’ (sentence) node attests to this.",2.1 Data collection,[0],[0]
"In order to measure the quality of our corpus, and in order to construct maximally useful testing and development sets, we performed an additional round of validation for about 10% of our data.",2.2 Data validation,[0],[0]
"This validation phase followed the same basic form as the Mechanical Turk labeling task used to label the SICK entailment data: we presented workers with pairs of sentences in batches of five, and asked them to choose a single label for each pair.",2.2 Data validation,[0],[0]
"We supplied each pair to four annotators, yielding five labels per pair including the label used by the original author.",2.2 Data validation,[0],[0]
"The instructions were similar to the instructions for initial data collection shown in Figure 1, and linked to a similar FAQ.",2.2 Data validation,[0],[0]
"Though we initially used a very restrictive qualification (based on past approval rate) to select workers for the validation task, we nonetheless discovered (and deleted) some instances of random guessing in an early batch of work, and subsequently instituted a fully closed qualification restricted to about 30 trusted workers.
",2.2 Data validation,[0],[0]
"For each pair that we validated, we assigned a gold label.",2.2 Data validation,[0],[0]
"If any one of the three labels was chosen by at least three of the five annotators, it was
LHS RHS 0 0 0",2.2 Data validation,[0],[0]
"1 1 39 1 2 42 1011 1/2/00 3 156 7980 3 4 1095 29471 4 5 3882 61196 5 6 12120 74094 6 7 26514 93600 7 8 37434 85851 8 9 44028 61359 9 10 49245 46711 10 11 50919 33241 11 12 48363 22844 12 13 43314 15994 13 14 38121 11047 14 15 33183 7601 5 16 27621 5312 16 17 23250 3732 17 18 20247 2631 18 19 18513 1878 19 20 16386 1325 20 21 13746 911 21 22 12066 642 22 23 9183 449 23 24 7131 357 24
25 6198 217 25
26 5007 168 26
27 3963 138 27
28 3438 84 28
29 2631 67 29
30 1959 46 30 31 1956 26 31 32 1434 31 32 33 1086 23 33 34 912 16 34 35 897 19 35 36 774 8 36 37 453 12 37 38 618 4 38 39 291 5 39 40 330 2 40 41 249 4 41 42 180 2 42 43 225 1 43 44 162 1 44 45 108 1 48 46 87 1 51 47 60 2 55 48 36 1 56 49 90 1 60 50 21 1 62 51 66 52 51 53 36 54 24 55 63 56 18 57 15 58 6 59 27 60 6 61 3 62 3 63 3 64 6 65 3 66 3 67 6 68 6 69 18 70 15 71 3 72 15 73 3 75 15 79 3 82 15
0 10,000 20,000 30,000 40,000 50,000 60,000 70,000 80,000 90,000 100,000
0 5 10 15 20 25 30 35 40
N um
be r
of se
nt en
ce s
Sentence length (tokens)
",2.2 Data validation,[0],[0]
"Premise Hypothesis
Figure 2: The distribution of sentence length.
chosen as the gold label.",2.2 Data validation,[0],[0]
"If there was no such consensus, which occurred in about 2% of cases, we assigned the placeholder label ‘-’.",2.2 Data validation,[0],[0]
"While these unlabeled examples are included in the corpus distribution, they are unlikely to be helpful for the standard NLI classification task, and we do not include them in either training or evaluation in the experiments that we discuss in this paper.
",2.2 Data validation,[0],[0]
The results of this validation process are summarized in Table 3.,2.2 Data validation,[0],[0]
"Nearly all of the examples received a majority label, indicating broad consensus about the nature of the data and categories.",2.2 Data validation,[0],[0]
The gold-labeled examples are very nearly evenly distributed across the three labels.,2.2 Data validation,[0],[0]
"The Fleiss κ scores (computed over every example with a full five annotations) are likely to be conservative given our large and unevenly distributed pool of annotators, but they still provide insights about the levels of disagreement across the three semantic classes.",2.2 Data validation,[0],[0]
This disagreement likely reflects not just the limitations of large crowdsourcing efforts but also the uncertainty inherent in naturalistic NLI.,2.2 Data validation,[0],[0]
"Regardless, the overall rate of agreement is extremely high, suggesting that the corpus is sufficiently high quality to pose a challenging but realistic machine learning task.",2.2 Data validation,[0],[0]
Table 1 shows a set of randomly chosen validated examples from the development set with their labels.,2.3 The distributed corpus,[0],[0]
"Qualitatively, we find the data that we collected draws fairly extensively on commonsense knowledge, and that hypothesis and premise sentences often differ structurally in significant ways, suggesting that there is room for improvement beyond superficial word alignment models.",2.3 The distributed corpus,[0],[0]
"We also find the sentences that we collected to be largely
635
fluent, correctly spelled English, with a mix of full sentences and caption-style noun phrase fragments, though punctuation and capitalization are often omitted.
",2.3 The distributed corpus,[0],[0]
"The corpus is available under a CreativeCommons Attribution-ShareAlike license, the same license used for the Flickr30k source captions.",2.3 The distributed corpus,[0],[0]
"It can be downloaded at: nlp.stanford.edu/projects/snli/
Partition We distribute the corpus with a prespecified train/test/development split.",2.3 The distributed corpus,[0],[0]
The test and development sets contain 10k examples each.,2.3 The distributed corpus,[0],[0]
"Each original ImageFlickr caption occurs in only one of the three sets, and all of the examples in the test and development sets have been validated.
",2.3 The distributed corpus,[0],[0]
"Parses The distributed corpus includes parses produced by the Stanford PCFG Parser 3.5.2 (Klein and Manning, 2003), trained on the standard training set as well as on the Brown Corpus (Francis and Kucera 1979), which we found to improve the parse quality of the descriptive sentences and noun phrases found in the descriptions.",2.3 The distributed corpus,[0],[0]
The most immediate application for our corpus is in developing models for the task of NLI.,3 Our data as a platform for evaluation,[0],[0]
"In par-
ticular, since it is dramatically larger than any existing corpus of comparable quality, we expect it to be suitable for training parameter-rich models like neural networks, which have not previously been competitive at this task.",3 Our data as a platform for evaluation,[0],[0]
"Our ability to evaluate standard classifier-base NLI models, however, was limited to those which were designed to scale to SNLI’s size without modification, so a more complete comparison of approaches will have to wait for future work.",3 Our data as a platform for evaluation,[0],[0]
"In this section, we explore the performance of three classes of models which could scale readily: (i) models from a well-known NLI system, the Excitement Open Platform; (ii) variants of a strong but simple feature-based classifier model, which makes use of both unlexicalized and lexicalized features, and (iii) distributed representation models, including a baseline model and neural network sequence models.",3 Our data as a platform for evaluation,[0],[0]
"The first class of models is from the Excitement Open Platform (EOP, Padó et al. 2014; Magnini et al. 2014)—an open source platform for RTE research.",3.1 Excitement Open Platform models,[0],[0]
EOP is a tool for quickly developing NLI systems while sharing components such as common lexical resources and evaluation sets.,3.1 Excitement Open Platform models,[0],[0]
"We evaluate on two algorithms included in the distribution: a simple edit-distance based algorithm and a classifier-based algorithm, the latter both in a bare form and augmented with EOP’s full suite of lexical resources.
",3.1 Excitement Open Platform models,[0],[0]
"Our initial goal was to better understand the difficulty of the task of classifying SNLI corpus inferences, rather than necessarily the performance of a state-of-the-art RTE system.",3.1 Excitement Open Platform models,[0],[0]
"We approached this by running the same system on several data sets: our own test set, the SICK test data, and the standard RTE-3 test set (Giampiccolo et al., 2007).",3.1 Excitement Open Platform models,[0],[0]
We report results in Table 4.,3.1 Excitement Open Platform models,[0],[0]
"Each of the models
was separately trained on the training set of each corpus.",3.1 Excitement Open Platform models,[0],[0]
All models are evaluated only on 2-class entailment.,3.1 Excitement Open Platform models,[0],[0]
"To convert 3-class problems like SICK and SNLI to this setting, all instances of contradiction and unknown are converted to nonentailment.",3.1 Excitement Open Platform models,[0],[0]
"This yields a most-frequent-class baseline accuracy of 66% on SNLI, and 71% on SICK.",3.1 Excitement Open Platform models,[0],[0]
"This is intended primarily to demonstrate the difficulty of the task, rather than necessarily the performance of a state-of-the-art RTE system.",3.1 Excitement Open Platform models,[0],[0]
"The edit distance algorithm tunes the weight of the three caseinsensitive edit distance operations on the training set, after removing stop words.",3.1 Excitement Open Platform models,[0],[0]
"In addition to the base classifier-based system distributed with the platform, we train a variant which includes information from WordNet (Miller, 1995) and VerbOcean (Chklovski and Pantel, 2004), and makes use of features based on tree patterns and dependency tree skeletons (Wang and Neumann, 2007).",3.1 Excitement Open Platform models,[0],[0]
"Unlike the RTE datasets, SNLI’s size supports approaches which make use of rich lexicalized features.",3.2 Lexicalized Classifier,[0],[0]
We evaluate a simple lexicalized classifier to explore the ability of non-specialized models to exploit these features in lieu of more involved language understanding.,3.2 Lexicalized Classifier,[0],[0]
"Our classifier implements 6 feature types; 3 unlexicalized and 3 lexicalized:
1.",3.2 Lexicalized Classifier,[0],[0]
"The BLEU score of the hypothesis with respect to the premise, using an n-gram length between 1 and 4.
2.",3.2 Lexicalized Classifier,[0],[0]
"The length difference between the hypothesis and the premise, as a real-valued feature.
3.",3.2 Lexicalized Classifier,[0],[0]
"The overlap between words in the premise and hypothesis, both as an absolute count and a percentage of possible overlap, and both over all words and over just nouns, verbs, adjectives, and adverbs.
4.",3.2 Lexicalized Classifier,[0],[0]
"An indicator for every unigram and bigram in the hypothesis.
5.",3.2 Lexicalized Classifier,[0],[0]
"Cross-unigrams: for every pair of words across the premise and hypothesis which share a POS tag, an indicator feature over the two words.
",3.2 Lexicalized Classifier,[0],[0]
6.,3.2 Lexicalized Classifier,[0],[0]
Cross,3.2 Lexicalized Classifier,[0],[0]
"-bigrams: for every pair of bigrams across the premise and hypothesis which share a POS tag on the second word, an indicator feature over the two bigrams.
",3.2 Lexicalized Classifier,[0],[0]
"We report results in Table 5, along with ablation studies for removing the cross-bigram features (leaving only the cross-unigram feature) and
for removing all lexicalized features.",3.2 Lexicalized Classifier,[0],[0]
"On our large corpus in particular, there is a substantial jump in accuracy from using lexicalized features, and another from using the very sparse cross-bigram features.",3.2 Lexicalized Classifier,[0],[0]
The latter result suggests that there is value in letting the classifier automatically learn to recognize structures like explicit negations and adjective modification.,3.2 Lexicalized Classifier,[0],[0]
"A similar result was shown in Wang and Manning (2012) for bigram features in sentiment analysis.
",3.2 Lexicalized Classifier,[0],[0]
It is surprising that the classifier performs as well as it does without any notion of alignment or tree transformations.,3.2 Lexicalized Classifier,[0],[0]
"Although we expect that richer models would perform better, the results suggest that given enough data, cross bigrams with the noisy part-of-speech overlap constraint can produce an effective model.",3.2 Lexicalized Classifier,[0],[0]
SNLI is suitably large and diverse to make it possible to train neural network models that produce distributed representations of sentence meaning.,3.3 Sentence embeddings and NLI,[0],[0]
"In this section, we compare the performance of three such models on the corpus.",3.3 Sentence embeddings and NLI,[0],[0]
"To focus specifically on the strengths of these models at producing informative sentence representations, we use sentence embedding as an intermediate step in the NLI classification task: each model must produce a vector representation of each of the two sentences without using any context from the other sentence, and the two resulting vectors are then passed to a neural network classifier which predicts the label for the pair.",3.3 Sentence embeddings and NLI,[0],[0]
"This choice allows us to focus on existing models for sentence embedding, and it allows us to evaluate the ability of those models to learn useful representations of meaning (which may be independently useful for subsequent tasks), at the cost of excluding from con-
sideration possible strong neural models for NLI that directly compare the two inputs at the word or phrase level.
",3.3 Sentence embeddings and NLI,[0],[0]
"Our neural network classifier, depicted in Figure 3 (and based on a one-layer model in Bowman et al. 2015), is simply a stack of three 200d tanh layers, with the bottom layer taking the concatenated sentence representations as input and the top layer feeding a softmax classifier, all trained jointly with the sentence embedding model itself.
",3.3 Sentence embeddings and NLI,[0],[0]
"We test three sentence embedding models, each set to use 100d phrase and sentence embeddings.",3.3 Sentence embeddings and NLI,[0],[0]
Our baseline sentence embedding model simply sums the embeddings of the words in each sentence.,3.3 Sentence embeddings and NLI,[0],[0]
"In addition, we experiment with two simple sequence embedding models: a plain RNN and an LSTM RNN (Hochreiter and Schmidhuber, 1997).
",3.3 Sentence embeddings and NLI,[0],[0]
"The word embeddings for all of the models are initialized with the 300d reference GloVe vectors (840B token version, Pennington et al. 2014) and fine-tuned as part of training.",3.3 Sentence embeddings and NLI,[0],[0]
"In addition, all of the models use an additional tanh neural network layer to map these 300d embeddings into the lower-dimensional phrase and sentence embedding space.",3.3 Sentence embeddings and NLI,[0],[0]
"All of the models are randomly initialized using standard techniques and trained using AdaDelta (Zeiler, 2012) minibatch SGD until performance on the development set stops improving.",3.3 Sentence embeddings and NLI,[0],[0]
"We applied L2 regularization to all models, manually tuning the strength coefficient λ for each, and additionally applied dropout (Srivastava et al., 2014) to the inputs and outputs of the sen-
tence embedding models (though not to its internal connections) with a fixed dropout rate.",3.3 Sentence embeddings and NLI,[0],[0]
"All models were implemented in a common framework for this paper, and the implementations will be made available at publication time.
",3.3 Sentence embeddings and NLI,[0],[0]
The results are shown in Table 6.,3.3 Sentence embeddings and NLI,[0],[0]
"The sum of words model performed slightly worse than the fundamentally similar lexicalized classifier— while the sum of words model can use pretrained word embeddings to better handle rare words, it lacks even the rudimentary sensitivity to word order that the lexicalized model’s bigram features provide.",3.3 Sentence embeddings and NLI,[0],[0]
"Of the two RNN models, the LSTM’s more robust ability to learn long-term dependencies serves it well, giving it a substantial advantage over the plain RNN, and resulting in performance that is essentially equivalent to the lexicalized classifier on the test set (LSTM performance near the stopping iteration varies by up to 0.5% between evaluation steps).",3.3 Sentence embeddings and NLI,[0],[0]
"While the lexicalized model fits the training set almost perfectly, the gap between train and test set accuracy is relatively small for all three neural network models, suggesting that research into significantly higher capacity versions of these models would be productive.",3.3 Sentence embeddings and NLI,[0],[0]
Figure 4 shows a learning curve for the LSTM and the lexicalized and unlexicalized feature-based models.,3.4 Analysis and discussion,[0],[0]
"It shows that the large size of the corpus is crucial to both the LSTM and the lexicalized model, and suggests that additional data would yield still better performance for both.",3.4 Analysis and discussion,[0],[0]
"In addition, though the LSTM and the lexicalized model show similar performance when trained on the current full corpus, the somewhat steeper slope for the LSTM hints that its ability to learn arbitrarily structured representations of sentence meaning may give it an advantage over the more constrained lexicalized model on still larger datasets.
",3.4 Analysis and discussion,[0],[0]
"We were struck by the speed with which the lexicalized classifier outperforms its unlexicalized
33.33 40.08 47.63 57.72 67.42 71.88 73.95 76.78 78.22
counterpart.",3.4 Analysis and discussion,[0],[0]
"With only 100 training examples, the cross-bigram classifier is already performing better.",3.4 Analysis and discussion,[0],[0]
"Empirically, we find that the top weighted features for the classifier trained on 100 examples tend to be high precision entailments; e.g., playing → outside (most scenes are outdoors), a banana → person eating.",3.4 Analysis and discussion,[0],[0]
"If relatively few spurious entailments get high weight—as it appears is the case— then it makes sense that, when these do fire, they boost accuracy in identifying entailments.
",3.4 Analysis and discussion,[0],[0]
There are revealing patterns in the errors common to all the models considered here.,3.4 Analysis and discussion,[0],[0]
"Despite the large size of the training corpus and the distributional information captured by GloVe initialization, many lexical relationships are still misanalyzed, leading to incorrect predictions of independent, even for pairs that are common in the training corpus like beach/surf and sprinter/runner.",3.4 Analysis and discussion,[0],[0]
"Semantic mistakes at the phrasal level (e.g., predicting contradiction for A male is placing an order in a deli/A man buying a sandwich at a deli) indicate that additional attention to compositional semantics would pay off.",3.4 Analysis and discussion,[0],[0]
"However, many of the persistent problems run deeper, to inferences that depend on world knowledge and contextspecific inferences, as in the entailment pair A race car driver leaps from a burning car/A race car driver escaping danger, for which both the lexicalized classifier and the LSTM predict neutral.",3.4 Analysis and discussion,[0],[0]
"In other cases, the models’ attempts to shortcut
this kind of inference through lexical cues can lead them astray.",3.4 Analysis and discussion,[0],[0]
"Some of these examples have qualities reminiscent of Winograd schemas (Winograd, 1972; Levesque, 2013).",3.4 Analysis and discussion,[0],[0]
"For example, all the models wrongly predict entailment for A young girl throws sand toward the ocean/A girl can’t stand the ocean, presumably because of distributional associations between throws and can’t stand.
",3.4 Analysis and discussion,[0],[0]
Analysis of the models’ predictions also yields insights into the extent to which they grapple with event and entity coreference.,3.4 Analysis and discussion,[0],[0]
"For the most part, the original image prompts contained a focal element that the caption writer identified with a syntactic subject, following information structuring conventions associating subjects and topics in English (Ward and Birner, 2004).",3.4 Analysis and discussion,[0],[0]
"Our annotators generally followed suit, writing sentences that, while structurally diverse, share topic/focus (theme/rheme) structure with their premises.",3.4 Analysis and discussion,[0],[0]
"This promotes a coherent, situation-specific construal of each sentence pair.",3.4 Analysis and discussion,[0],[0]
"This is information that our models can easily take advantage of, but it can lead them astray.",3.4 Analysis and discussion,[0],[0]
"For instance, all of them stumble with the amusingly simple case A woman prepares ingredients for a bowl of soup/A soup bowl prepares a woman, in which prior expectations about parallelism are not met.",3.4 Analysis and discussion,[0],[0]
"Another headline example of this type is A man wearing padded arm protection is being bitten by a German shepherd dog/A man bit a dog, which all the models wrongly diagnose as entailment, though the sentences report two very different stories.",3.4 Analysis and discussion,[0],[0]
A model with access to explicit information about syntactic or semantic structure should perform better on cases like these.,3.4 Analysis and discussion,[0],[0]
"To the extent that successfully training a neural network model like our LSTM on SNLI forces that model to encode broadly accurate representations of English scene descriptions and to build an entailment classifier over those relations, we should expect it to be readily possible to adapt the trained model for use on other NLI tasks.",4 Transfer learning with SICK,[0],[0]
"In this section, we evaluate on the SICK entailment task using a simple transfer learning method (Pratt et al., 1991) and achieve competitive results.
",4 Transfer learning with SICK,[0],[0]
"To perform transfer, we take the parameters of the LSTM RNN model trained on SNLI and use them to initialize a new model, which is trained from that point only on the training portion of SICK.",4 Transfer learning with SICK,[0],[0]
"The only newly initialized parameters are
softmax layer parameters and the embeddings for words that appear in SICK, but not in SNLI (which are populated with GloVe embeddings as above).",4 Transfer learning with SICK,[0],[0]
"We use the same model hyperparameters that were used to train the original model, with the exception of the L2 regularization strength, which is re-tuned.",4 Transfer learning with SICK,[0],[0]
We additionally transfer the accumulators that are used by AdaDelta to set the learning rates.,4 Transfer learning with SICK,[0],[0]
"This lowers the starting learning rates, and is intended to ensure that the model does not learn too quickly in its first few epochs after transfer and destroy the knowledge accumulated in the pre-transfer phase of training.
",4 Transfer learning with SICK,[0],[0]
The results are shown in Table 7.,4 Transfer learning with SICK,[0],[0]
"Training on SICK alone yields poor performance, and the model trained on SNLI fails when tested on SICK data, labeling more neutral examples as contradictions than correctly, possibly as a result of subtle differences in how the labeling task was presented.",4 Transfer learning with SICK,[0],[0]
"In contrast, transferring SNLI representations to SICK yields the best performance yet reported for an unaugmented neural network model, surpasses the available EOP models, and approaches both the overall state of the art at 84.6% (Lai and Hockenmaier, 2014) and the 84% level of interannotator agreement, which likely represents an approximate performance ceiling.",4 Transfer learning with SICK,[0],[0]
"This suggests that the introduction of a large high-quality corpus makes it possible to train representation-learning models for sentence meaning that are competitive with the best hand-engineered models on inference tasks.
",4 Transfer learning with SICK,[0],[0]
"We attempted to apply this same transfer evaluation technique to the RTE-3 challenge, but found that the small training set (800 examples) did not allow the model to adapt to the unfamiliar genre of text used in that corpus, such that no training configuration yielded competitive performance.",4 Transfer learning with SICK,[0],[0]
Further research on effective transfer learning on small data sets with neural models might facilitate improvements here.,4 Transfer learning with SICK,[0],[0]
"Natural languages are powerful vehicles for reasoning, and nearly all questions about meaningfulness in language can be reduced to questions of entailment and contradiction in context.",5 Conclusion,[0],[0]
"This suggests that NLI is an ideal testing ground for theories of semantic representation, and that training for NLI tasks can provide rich domain-general semantic representations.",5 Conclusion,[0],[0]
"To date, however, it has not been possible to fully realize this potential due to the limited nature of existing NLI resources.",5 Conclusion,[0],[0]
"This paper sought to remedy this with a new, largescale, naturalistic corpus of sentence pairs labeled for entailment, contradiction, and independence.",5 Conclusion,[0],[0]
"We used this corpus to evaluate a range of models, and found that both simple lexicalized models and neural network models perform well, and that the representations learned by a neural network model on our corpus can be used to dramatically improve performance on a standard challenge dataset.",5 Conclusion,[0],[0]
We hope that SNLI presents valuable training data and a challenging testbed for the continued application of machine learning to semantic representation.,5 Conclusion,[0],[0]
"We gratefully acknowledge support from a Google Faculty Research Award, a gift from Bloomberg L.P., the Defense Advanced Research Projects Agency (DARPA)",Acknowledgments,[0],[0]
Deep Exploration and Filtering of Text (DEFT) Program under Air Force Research Laboratory (AFRL) contract no.,Acknowledgments,[0],[0]
"FA875013-2-0040, the National Science Foundation under grant no.",Acknowledgments,[0],[0]
"IIS 1159679, and the Department of the Navy, Office of Naval Research, under grant no.",Acknowledgments,[0],[0]
N00014-10-1-0109.,Acknowledgments,[0],[0]
"Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of Google, Bloomberg L.P., DARPA, AFRL NSF, ONR, or the US government.",Acknowledgments,[0],[0]
We also thank our many excellent Mechanical Turk contributors.,Acknowledgments,[0],[0]
"Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations.",abstractText,[0],[0]
"However, machine learning research in this area has been dramatically limited by the lack of large-scale resources.",abstractText,[0],[0]
"To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning.",abstractText,[0],[0]
"At 570K pairs, it is two orders of magnitude larger than all other resources of its type.",abstractText,[0],[0]
"This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time.",abstractText,[0],[0]
A large annotated corpus for learning natural language inference,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1237–1247 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1114",text,[0],[0]
"Natural language processing (NLP) plays an important role in artificial intelligence, which has been extensively studied for many decades.",1 Introduction,[0],[0]
"Conventional NLP techniques include the rule-based symbolic approaches widely used about two decades ago, and the more recent statistical approaches relying on feature engineering and statistical models.",1 Introduction,[0],[0]
"In the recent years, deep learning approach has achieved huge successes in many applications, ranging from speech recognition to image classification.",1 Introduction,[0],[0]
"It is drawing increasing attention in the NLP community.
",1 Introduction,[0],[0]
"In this paper, we are interested in a fundamental problem in NLP, namely named entity recognition (NER) and mention detection (MD).",1 Introduction,[0],[0]
"NER and MD are very challenging tasks in NLP, laying the foundation of almost every NLP application.",1 Introduction,[0],[0]
"NER and MD are tasks of identifying entities (named and/or nominal) from raw text, and classifying the detected entities into one of the pre-defined categories such as person (PER), organization (ORG), location (LOC), etc.",1 Introduction,[0],[0]
"Some tasks focus on named entities only, while the others also detect nominal mentions.",1 Introduction,[0],[0]
"Moreover, nested mentions may need to be extracted too.",1 Introduction,[0],[0]
"For example,
[Sue]PER and her [brother]PER N studied in [University of [Toronto]LOC ]ORG.
where Toronto is a LOC entity, embedded in another longer ORG entity University of Toronto.
",1 Introduction,[0],[0]
"Similar to many other NLP problems, NER and MD is formulated as a sequence labeling problem, where a tag is sequentially assigned to each word in the input sentence.",1 Introduction,[0],[0]
"It has been extensively studied in the NLP community (Borthwick et al., 1998).",1 Introduction,[0],[0]
The core problem is to model the conditional probability of an output sequence given an arbitrary input sequence.,1 Introduction,[0],[0]
"Many hand-crafted features are combined with statistical models, such as conditional random fields (CRFs) (Nguyen et al., 2010), to compute conditional probabilities.",1 Introduction,[0],[0]
"More recently, some popular neural networks, including convolutional neural networks (CNNs) and recurrent neural networks (RNNs), are proposed to solve sequence labelling problems.",1 Introduction,[0],[0]
"In the inference stage, the learned models compute the conditional probabilities and the output sequence is generated by the Viterbi decoding algorithm (Viterbi, 1967).
",1 Introduction,[0],[0]
"In this paper, we propose a novel local detection approach for solving NER and MD problems.",1 Introduction,[0],[0]
"The idea can be easily extended to many other se-
1237
quence labeling problems, such as chunking, partof-speech tagging (POS).",1 Introduction,[0],[0]
"Instead of globally modeling the whole sequence in training and jointly decode the entire output sequence in test, our method examines all word segments (up to a certain length) in a sentence.",1 Introduction,[0],[0]
A word segment will be examined individually based on the underlying segment itself and its left and right contexts in the sentence so as to determine whether this word segment is a valid named entity and the corresponding label if it is.,1 Introduction,[0],[0]
This approach conforms to the way human resolves an NER problem.,1 Introduction,[0],[0]
"Given any word fragment and its contexts in a sentence or paragraph, people accurately determine whether this word segment is a named entity or not.",1 Introduction,[0],[0]
People rarely conduct a global decoding over the entire sentence to make such a decision.,1 Introduction,[0],[0]
The key to making an accurate local decision for each individual fragment is to have full access to the fragment itself as well as its complete contextual information.,1 Introduction,[0],[0]
The main pitfall to implement this idea is that we can not easily encode the segment and its contexts in models since they are of varying lengths in natural languages.,1 Introduction,[0],[0]
Many feature engineering techniques have been proposed but all of these methods will inevitably lead to information loss.,1 Introduction,[0],[0]
"In this work, we propose to use a recent fixed-size encoding method, namely fixed-size ordinally forgetting encoding (FOFE) (Zhang et al., 2015a,b), to solve this problem.",1 Introduction,[0],[0]
The FOFE method is a simple recursive encoding method.,1 Introduction,[0],[0]
FOFE theoretically guarantees (almost) unique and lossless encoding of any variable-length sequence.,1 Introduction,[0],[0]
"The left and the right contexts for each word segment are encoded by FOFE method, and then a simple neural network can be trained to make a precise recognition for each individual word segment based on the fixed-size presentation of the contextual information.",1 Introduction,[0],[0]
This FOFE-based local detection approach is more appealing to NER and MD.,1 Introduction,[0],[0]
"Firstly, feature engineering is almost eliminated.",1 Introduction,[0],[0]
"Secondly, under this local detection framework, nested mention is handled with little modification.",1 Introduction,[0],[0]
"Next, it makes better use of partially-labeled data available from many application scenarios.",1 Introduction,[0],[0]
Sequence labeling model requires all entities in a sentence to be labeled.,1 Introduction,[0],[0]
"If only some (not all) entities are labeled, it is not effective to learn a sequence labeling model.",1 Introduction,[0],[0]
"However, every single labeled entity, along with its contexts, may be used to learn the proposed model.",1 Introduction,[0],[0]
"At last, due to the simplicity of
FOFE, simple neural networks, such as multilayer perceptrons, are sufficient for recognition.",1 Introduction,[0],[0]
These models are much faster to train and easier to tune.,1 Introduction,[0],[0]
"In the test stage, all possible word segments from a sentence may be packed into a mini-batch, jointly recognized in parallel on GPUs.",1 Introduction,[0],[0]
"This leads to a very fast decoding process as well.
",1 Introduction,[0],[0]
"In this paper, we have applied this FOFE-based local detection approach to several popular NER and MD tasks, including the CoNLL 2003 NER task and TAC-KBP2015 and TAC-KBP2016 Trilingual Entity Discovery and Linking (EDL) tasks.",1 Introduction,[0],[0]
Our proposed method has yielded strong performance in all of these examined tasks.,1 Introduction,[0],[0]
It has been a long history of research involving neural networks (NN).,2 Related Work,[0],[0]
"In this section, we briefly review some recent NN-related research work in NLP, which may be relevant to our work.
",2 Related Work,[0],[0]
"The success of word embedding (Mikolov et al., 2013; Liu et al., 2015) encourages researchers to focus on machine-learned representation instead of heavy feature engineering in NLP.",2 Related Work,[0],[0]
"Using word embedding as the typical feature representation for words, NNs become competitive to traditional approaches in NER.",2 Related Work,[0],[0]
"Many NLP tasks, such as NER, chunking and part-of-speech (POS) tagging can be formulated as sequence labeling tasks.",2 Related Work,[0],[0]
"In (Collobert et al., 2011), deep convolutional neural networks (CNN) and conditional random fields (CRF) are used to infer NER labels at a sentence level, where they still use many hand-crafted features to improve performance, such as capitalization features explicitly defined based on first-letter capital, non-initial capital and so on.
",2 Related Work,[0],[0]
"Recently, recurrent neural networks (RNNs) have demonstrated the ability in modeling sequences (Graves, 2012).",2 Related Work,[0],[0]
Huang et al. (2015) built on the previous CNN-CRF approach by replacing CNNs with bidirectional Long Short-Term Memory (B-LSTM).,2 Related Work,[0],[0]
"Though they have reported improved performance, they employ heavy feature engineering in that work, most of which is language-specific.",2 Related Work,[0],[0]
"There is a similar attempt in (Rondeau and Su, 2016) with full-rank CRF.",2 Related Work,[0],[0]
"CNNs are used to extract character-level features automatically in (dos Santos et al., 2015).
",2 Related Work,[0],[0]
Gazetteer is a list of names grouped by the predefined categories.,2 Related Work,[0],[0]
"Gazetteer is shown to be one of the most effective external knowledge sources
to improve NER performance (Sang and Meulder, 2003).",2 Related Work,[0],[0]
"Thus, gazetteer is widely used in many NER systems.",2 Related Work,[0],[0]
"In (Chiu and Nichols, 2016), stateof-the-art performance on a popular NER task, i.e., CoNLL2003, is achieved by incorporating a large gazetteer.",2 Related Work,[0],[0]
"Different from previous ways to use a set of bits to indicate whether a word is in gazetteer or not, they have encoded a match in BIOES (Begin, Inside, Outside, End, Single) annotation, which captures positional information.
",2 Related Work,[0],[0]
"Interestingly enough, none of these recent successes in NER was achieved by a vanilla RNN.",2 Related Work,[0],[0]
"Rather, these successes are often established by sophisticated models combining CNNs, LSTMs and CRFs in certain ways.",2 Related Work,[0],[0]
"In this paper, based on recent work in (Zhang et al., 2015a,b) and (Zhang et al., 2016), we propose a novel but simple solution to NER by applying DNN on top of FOFEbased features.",2 Related Work,[0],[0]
"This simpler approach can achieve performance very close to state-of-the-art on various NER and MD tasks, without using any external knowledge or feature engineering.",2 Related Work,[0],[0]
"In this section, we will briefly review some background techniques, which are important to our proposed NER and mention detection approach.",3 Preliminary,[0],[0]
"It is well known that neural network is a universal approximator under certain conditions (Hornik, 1991).",3.1 Deep Feedforward Neural Networks,[0],[0]
A feedforward neural network (FFNN) is a weighted graph with a layered architecture.,3.1 Deep Feedforward Neural Networks,[0],[0]
Each layer is composed of several nodes.,3.1 Deep Feedforward Neural Networks,[0],[0]
Successive layers are fully connected.,3.1 Deep Feedforward Neural Networks,[0],[0]
Each node applies a function on the weighted sum of the lower layer.,3.1 Deep Feedforward Neural Networks,[0],[0]
An NN can learn by adjusting its weights in a process called back-propagation.,3.1 Deep Feedforward Neural Networks,[0],[0]
The learned NN may be used to generalize and extrapolate to new inputs that have not been seen during training.,3.1 Deep Feedforward Neural Networks,[0],[0]
FFNN is a powerful computation model.,3.2 Fixed-size Ordinally Forgetting Encoding,[0],[0]
"However, it requires fixed-size inputs and lacks the ability of capturing long-term dependency.",3.2 Fixed-size Ordinally Forgetting Encoding,[0],[0]
"Because most NLP problems involves variablelength sequences of words, RNNs/LSTMs are more popular than FFNNs in dealing with these problems.",3.2 Fixed-size Ordinally Forgetting Encoding,[0],[0]
"The Fixed-size Ordinally Forgetting Encoding (FOFE), originally proposed in (Zhang et al., 2015a,b), nicely overcomes the limitations
of FFNNs because it can uniquely and losslessly encode a variable-length sequence of words into a fixed-size representation.
",3.2 Fixed-size Ordinally Forgetting Encoding,[0],[0]
"Give a vocabulary V , each word can be represented by a one-hot vector.",3.2 Fixed-size Ordinally Forgetting Encoding,[0],[0]
FOFE mimics bag-ofwords (BOW) but incorporates a forgetting factor to capture positional information.,3.2 Fixed-size Ordinally Forgetting Encoding,[0],[0]
It encodes any sequence of variable length composed by words in V .,3.2 Fixed-size Ordinally Forgetting Encoding,[0],[0]
"Let S = w1, w2, w3, ..., wT denote a sequence of T words from V , and et be the one-hot vector of the t-th word in S, where 1 ≤ t ≤ T .",3.2 Fixed-size Ordinally Forgetting Encoding,[0],[0]
"The FOFE of each partial sequence zt from the first word to the t-th word is recursively defined as:
zt = { 0, if t = 0 α · zt−1 + et, otherwise
(1)
where the constant α is called forgetting factor, and it is picked between 0 and 1 exclusively.",3.2 Fixed-size Ordinally Forgetting Encoding,[0],[0]
"Obviously, the size of zt is |V |, and it is irrelevant to the length of original sequence, T .
",3.2 Fixed-size Ordinally Forgetting Encoding,[0],[0]
Here’s an example.,3.2 Fixed-size Ordinally Forgetting Encoding,[0],[0]
"Assume that we have three words in our vocabulary, e.g. A, B, C, whose one-hot representations are [1, 0, 0], [0, 1, 0] and [0, 0, 1] respectively.",3.2 Fixed-size Ordinally Forgetting Encoding,[0],[0]
"When calculating from left to right, the FOFE for the sequence “ABC” is [α2, α, 1] and that of “ABCBC” is [α4, α+α3, 1+ α2].
",3.2 Fixed-size Ordinally Forgetting Encoding,[0],[0]
"The word sequences can be unequivocally recovered from their FOFE representations (Zhang et al., 2015a,b).",3.2 Fixed-size Ordinally Forgetting Encoding,[0],[0]
"The uniqueness of FOFE representation is theoretically guaranteed by the following two theorems:
Theorem 1.",3.2 Fixed-size Ordinally Forgetting Encoding,[0],[0]
If the forgetting factor α satisfies 0,3.2 Fixed-size Ordinally Forgetting Encoding,[0],[0]
"< α ≤ 0.5, FOFE is unique for any countable vocabulary V and any finite value T .
",3.2 Fixed-size Ordinally Forgetting Encoding,[0],[0]
Theorem 2.,3.2 Fixed-size Ordinally Forgetting Encoding,[0],[0]
"For 0.5 < α < 1, given any finite value T and any countable vocabulary V , FOFE is almost unique everywhere, except only a finite set of countable choices of α.
",3.2 Fixed-size Ordinally Forgetting Encoding,[0],[0]
"Though in theory uniqueness is not guaranteed when α is chosen from 0.5 to 1, in practice the chance of hitting such scenarios is extremely slim, almost impossible due to quantization errors in the system.",3.2 Fixed-size Ordinally Forgetting Encoding,[0],[0]
"Furthermore, in natural languages, normally a word does not appear repeatedly within a near context.",3.2 Fixed-size Ordinally Forgetting Encoding,[0],[0]
"Simply put, FOFE is capable of uniquely encoding any sequence of arbitrary length, serving as a fixed-size but theoretically lossless representation for any sequence.",3.2 Fixed-size Ordinally Forgetting Encoding,[0],[0]
Kim et al. (2016) model morphology in the character level since this may provide some additional advantages in dealing with unknown or out-ofvocabulary (OOVs) words in a language.,3.3 Character-level Models in NLP,[0],[0]
"In the literature, convolutional neural networks (CNNs) have been widely used as character-level models in NLP (Kim et al., 2016).",3.3 Character-level Models in NLP,[0],[0]
A trainable character embedding is initialized based on a set of possible characters.,3.3 Character-level Models in NLP,[0],[0]
"When a word fragment comes, character vectors are retrieved according to its spelling to construct a matrix.",3.3 Character-level Models in NLP,[0],[0]
This matrix can be viewed as a single-channel image.,3.3 Character-level Models in NLP,[0],[0]
"CNN is applied to generate a more abstract representation of the word fragment.
",3.3 Character-level Models in NLP,[0],[0]
The above FOFE method can be easily extended to model character-level feature in NLP.,3.3 Character-level Models in NLP,[0],[0]
"Any word, phrase or fragment can be viewed as a sequence of characters.",3.3 Character-level Models in NLP,[0],[0]
"Based on a pre-defined set of all possible characters, we apply the same FOFE method to encode the sequence of characters.",3.3 Character-level Models in NLP,[0],[0]
"This always leads to a fixed-size representation, irrelevant to the number of characters in question.",3.3 Character-level Models in NLP,[0],[0]
"For example, a word fragment of “Walmart” may be viewed as a sequence of seven characters: ‘W’, ‘a’, ‘l’, ‘m’, ‘a’, ‘r’, ‘t’.",3.3 Character-level Models in NLP,[0],[0]
The FOFE codes of character sequences are always fixed-sized and they can be directly fed to an FFNN for morphology modeling.,3.3 Character-level Models in NLP,[0],[0]
"As described above, our FOFE-based local detection approach for NER, called FOFE-NER hereafter, is motivated by the way how human actually infers whether a word segment in text is an entity or mention, where the entity types of the
other entities in the same sentence is not a must.",4 FOFE-based Local Detection for NER,[0],[0]
"Particularly, the dependency between adjacent entities is fairly weak in NER.",4 FOFE-based Local Detection for NER,[0],[0]
"Whether a fragment is an entity or not, and what class it may belong to, largely depend on the internal structure of the fragment itself as well as the left and right contexts in which it appears.",4 FOFE-based Local Detection for NER,[0],[0]
"To a large extent, the meaning and spelling of the underlying fragment are informative to distinguish named entities from the rest of the text.",4 FOFE-based Local Detection for NER,[0],[0]
"Contexts play a very important role in NER or MD when it involves multi-sense words/phrases or out-of-vocabulary (OOV) words.
",4 FOFE-based Local Detection for NER,[0],[0]
"As shown in Figure 1, our proposed FOFENER method will examine all possible fragments in text (up to a certain length) one by one.",4 FOFE-based Local Detection for NER,[0],[0]
"For each fragment, it uses the FOFE method to fully encode the underlying fragment itself, its left context and right context into some fixed-size representations, which are in turn fed to an FFNN to predict whether the current fragment is NOT a valid entity mention (NONE), or its correct entity type (PER, LOC, ORG and so on) if it is a valid mention.",4 FOFE-based Local Detection for NER,[0],[0]
This method is appealing because the FOFE codes serves as a theoretically lossless representation of the hypothesis and its full contexts.,4 FOFE-based Local Detection for NER,[0],[0]
"FFNN is used as a universal approximator to map from text to the entity labels.
",4 FOFE-based Local Detection for NER,[0],[0]
"In this work, we use FOFE to explore both word-level and character-level features for each fragment and its contexts.",4 FOFE-based Local Detection for NER,[0],[0]
"FOFE-NER generates several word-level features for each fragment hypothesis and its left and right contexts as follows:
• Bag-of-word (BoW) of the fragment, e.g.
bag-of-word vector of ‘Toronto’, ‘Maple’ and ‘Leafs’ in Figure 1.
",4.1 Word-level Features,[0],[0]
"• FOFE code for left context including the fragment, e.g. FOFE code of the word sequence of “... puck from space for the Toronto Maple Leafs” in Figure 1.
",4.1 Word-level Features,[0],[0]
"• FOFE code for left context excluding the fragment, e.g. the FOFE code of the word sequence of “... puck from space for the” in Figure 1..
• FOFE code for right context including the fragment, e.g. the FOFE code of the word sequence of “... against opener home ’ Leafs Maple Toronto” in Figure 1.
",4.1 Word-level Features,[0],[0]
"• FOFE code for right context excluding the fragment, e.g. the FOFE code of the word sequence of “... against opener home ” in Figure 1.
",4.1 Word-level Features,[0],[0]
"Moreover, all of the above word features are computed for both case-sensitive words in raw text as well as case-insensitive words in normalized lower-case text.",4.1 Word-level Features,[0],[0]
"These FOFE codes are projected to lower-dimension dense vectors based on two projection matrices, Ws and Wi, for casesensitive and case-insensitive FOFE codes respectively.",4.1 Word-level Features,[0],[0]
"These two projection matrices are initialized by word embeddings trained by word2vec, and fine-tuned during the learning of the neural networks.
",4.1 Word-level Features,[0],[0]
"Due to the recursive computation of FOFE codes in eq.(1), all of the above FOFE codes can be jointly computed for one sentence or document in a very efficient manner.",4.1 Word-level Features,[0],[0]
"On top of the above word-level features, we also augment character-level features for the underlying segment hypothesis to further model its morphological structure.",4.2 Character-level Features,[0],[0]
"For the example in Figure 1, the current fragment, Toronto Maple Leafs, is considered as a sequence of case-sensitive characters, i.e. “{‘T’, ‘o’, ..., ‘f’ , ‘s’ }”, we then add the following character-level features for this fragment: • Left-to-right FOFE code of the character se-
quence of the underlying fragment.",4.2 Character-level Features,[0],[0]
"That is the FOFE code of the sequence, “‘T’, ‘o’, ..., ‘f’ , ‘s’ ”.
",4.2 Character-level Features,[0],[0]
•,4.2 Character-level Features,[0],[0]
Right-to-left FOFE code of the character sequence of the underlying fragment.,4.2 Character-level Features,[0],[0]
"That is
the FOFE code of the sequence, “‘s’ , ‘f’ , ..., ‘o’, ‘T’ ”.
",4.2 Character-level Features,[0],[0]
"These case-sensitive character FOFE codes are also projected by another character embedding matrix, which is randomly initialized and finetuned during model training.
",4.2 Character-level Features,[0],[0]
"Alternatively, we may use the character CNNs, as described in Section 3.3, to generate characterlevel features for each fragment hypothesis as well.",4.2 Character-level Features,[0],[0]
"Obviously, the above FOFE-NER model will take each sentence of words, S =",5 Training and Decoding Algorithm,[0],[0]
"[w1, w2, w3, ..., wm], as input, and examine all continuous subsequences [wi, wi+1, wi+2, ..., wj ] up to n words in S for possible entity types.",5 Training and Decoding Algorithm,[0],[0]
"All sub-sequences longer than n words are considered as non-entities in this work.
",5 Training and Decoding Algorithm,[0],[0]
"When we train the model, based on the entity labels of all sentences in the training set, we will generate many sentence fragments up to n words.",5 Training and Decoding Algorithm,[0],[0]
"These fragments fall into three categories: • Exact-match with an entity label, e.g., the
fragment “Toronto Maple Leafs” in the previous example.
",5 Training and Decoding Algorithm,[0],[0]
"• Partial-overlap with an entity label, e.g., “for the Toronto”.
",5 Training and Decoding Algorithm,[0],[0]
"• Disjoint with all entity label, e.g. “from space for”.
",5 Training and Decoding Algorithm,[0],[0]
"For all exact-matched fragments, we generate the corresponding outputs based on the types of the matched entities in the training set.",5 Training and Decoding Algorithm,[0],[0]
"For both partial-overlap and disjoint fragments, we introduce a new output label, NONE, to indicate that these fragments are not a valid entity.",5 Training and Decoding Algorithm,[0],[0]
"Therefore, the output nodes in the neural networks contains all entity types plus a rejection option denoted as NONE.
",5 Training and Decoding Algorithm,[0],[0]
"During training, we implement a producerconsumer software design such that a thread fetches training examples, computes all FOFE codes and packs them as a mini-batch while the other thread feeds the mini-batches to neural networks and adjusts the model parameters and all projection matrices.",5 Training and Decoding Algorithm,[0],[0]
"Since “partial-overlap” and “disjoint” significantly outnumber “exact-match”, they are down-sampled so as to balance the data set.
",5 Training and Decoding Algorithm,[0],[0]
"During inference, all fragments not longer than
n words are all fed to FOFE-NER to compute their scores over all entity types.",5 Training and Decoding Algorithm,[0],[0]
"In practice, these fragments can be packed as one mini-batch so that we can compute them in parallel on GPUs.",5 Training and Decoding Algorithm,[0],[0]
"As the NER result, the FOFE-NER model will return a subset of fragments only if: i) they are recognized as a valid entity type (not NONE); AND ii) their NN scores exceed a global pruning threshold.
Occasionally, some partially-overlapped or nested fragments may occur in the above pruned prediction results.",5 Training and Decoding Algorithm,[0],[0]
"We can use one of the following simple post-processing methods to remove overlappings from the final results:
1. highest-first: We check every word in a sentence.",5 Training and Decoding Algorithm,[0],[0]
"If it is contained by more than one fragment in the pruned results, we only keep the one with the maximum NN score and discard the rest.
2.",5 Training and Decoding Algorithm,[0],[0]
longest-first: We check every word in a sentence.,5 Training and Decoding Algorithm,[0],[0]
"If it is contained by more than one fragment in the pruned results, we only keep the longest fragment and discard the rest.
",5 Training and Decoding Algorithm,[0],[0]
"Either of these strategies leads to a collection of non-nested, non-overlapping, non-NONE entity labels.
",5 Training and Decoding Algorithm,[0],[0]
"In some tasks, it may require to label all nested entities.",5 Training and Decoding Algorithm,[0],[0]
This has imposed a big challenge to the sequence labeling methods.,5 Training and Decoding Algorithm,[0],[0]
"However, the above post-processing can be slightly modified to generate nested entities’ labels.",5 Training and Decoding Algorithm,[0],[0]
"In this case, we first run either highest-first or longest-first to generate the first round result.",5 Training and Decoding Algorithm,[0],[0]
"For every entity survived in this round, we will recursively run either highestfirst or longest-first on all entities in the original set, which are completely contained by it.",5 Training and Decoding Algorithm,[0],[0]
This will generate more prediction results.,5 Training and Decoding Algorithm,[0],[0]
This process may continue to allow any levels of nesting.,5 Training and Decoding Algorithm,[0],[0]
"For example, for a sentence of “w1 w2 w3 w4 w5”, if the model first generates the prediction results after the global pruning, as [“w2w3”, PER, 0.7], [“w3w4”, LOC, 0.8], [“w1w2w3w4”, ORG, 0.9], if we choose to run highest-first, it will generate the first entity label as [“w1w2w3w4”, ORG, 0.9].",5 Training and Decoding Algorithm,[0],[0]
"Secondly, we will run highest-first on the two fragments that are completely contained by the first one, i.e., [“w2w3”, PER, 0.7], [“w3w4”, LOC, 0.8], then we will generate the second nested entity label as [“w3w4”, LOC, 0.8].",5 Training and Decoding Algorithm,[0],[0]
"Fortunately, in any real NER and MD tasks, it is pretty rare to have overlapped predictions in the NN outputs.
",5 Training and Decoding Algorithm,[0],[0]
"Therefore, the extra expense to run this recursive post-processing method is minimal.",5 Training and Decoding Algorithm,[0],[0]
"As we know, CRF brings marginal performance gain to all taggers (but not limited to NER) because of the dependancies (though fairly weak) between entity types.",6 Second-Pass Augmentation,[0],[0]
We may easily add this level of information to our model by introducing another pass of FOFE-NER.,6 Second-Pass Augmentation,[0],[0]
"We call it 2nd-pass FOFENER.
",6 Second-Pass Augmentation,[0],[0]
"In 2nd-pass FOFE-NER, another set of model is trained on outputs from the first-pass FOFENER, including all predicted entities.",6 Second-Pass Augmentation,[0],[0]
"For example, given a sentence
S =",6 Second-Pass Augmentation,[0],[0]
"[w1, w2, ...wi, ...wj , ...wn]
and an underlying word segment [wi, ..., wj ] in the second pass, every predicted entity outside this segment is substituted by its entity type predicted from the first pass.",6 Second-Pass Augmentation,[0],[0]
"For example, in the first pass, a sentence like “Google has also recruited Fei-Fei Li, director of the AI lab at Stanford University.” is predicted as: “<ORG> has also recruited FeiFei Li, director of the AI lab at<ORG>.”",6 Second-Pass Augmentation,[0],[0]
"In 2ndpass FOFE-NER, when examining the segment “Fei-Fei Li”, the predicted entity types <ORG> are used to replace the actual named entities.",6 Second-Pass Augmentation,[0],[0]
"The 2nd-pass FOFE-NER model is trained on the outputs of the first pass, where all detected entities are replaced by their predicted types as above.
",6 Second-Pass Augmentation,[0],[0]
"During inference, the results returned by the 1st-pass model are substituted in the same way.",6 Second-Pass Augmentation,[0],[0]
"The scores for each hypothesis from 1st-pass model and 2nd-pass model are linear interpolated and then decoded by either highest-first or longestfirst to generate the final results of 2nd-pass FOFE-NER.
",6 Second-Pass Augmentation,[0],[0]
"Obviously, 2nd-pass FOFE-NER may capture the semantic roles of other entities while filtering out unwanted constructs and sparse combonations.",6 Second-Pass Augmentation,[0],[0]
"On the other hand, it enables longer context expansion, since FOFE memorizes contextual information in an unselective decaying fashion.",6 Second-Pass Augmentation,[0],[0]
"In this section, we evaluate the effectiveness of our proposed methods on several popular NER and MD tasks, including CoNLL 2003 NER task and TAC-KBP2015 and TAC-KBP2016 Trilingual Entity Discovery and Linking (EDL) tasks.
",7 Experiments,[0],[0]
We have made our codes available at https:// github.com/xmb-cipher/fofe-ner for readers to reproduce the results in this paper.,7 Experiments,[0],[0]
"The CoNLL-2003 dataset (Sang and Meulder, 2003) consists of newswire from the Reuters RCV1 corpus tagged with four types of nonnested named entities: location (LOC), organization (ORG), person (PER), and miscellaneous (MISC).
",7.1 CoNLL 2003 NER task,[0],[0]
"The top 100,000 words, are kept as vocabulary, including punctuations.",7.1 CoNLL 2003 NER task,[0],[0]
"For the case-sensitive embedding, an OOV is mapped to <unk> if it contains no upper-case letter and <UNK> otherwise.",7.1 CoNLL 2003 NER task,[0],[0]
We perform grid search on several hyperparameters using a held-out dev set.,7.1 CoNLL 2003 NER task,[0],[0]
Here we summarize the set of hyper-parameters used in our experiments: i),7.1 CoNLL 2003 NER task,[0],[0]
"Learning rate: initially set to 0.128 and is multiplied by a decay factor each epoch so that it reaches 1/16 of the initial value at the end of the training; ii) Network structure: 3 fully-connected layers of 512 nodes with ReLU activation, randomly initialized based on a uniform distribution between − √ 6
Ni+No and
√ 6
Ni+No (Glorot et al., 2011); iii) Character embeddings: 64 dimensions, randomly initialized.",7.1 CoNLL 2003 NER task,[0],[0]
iv) mini-batch: 512; v),7.1 CoNLL 2003 NER task,[0],[0]
"Dropout rate: initially set to 0.4, slowly decreased during training until it reaches 0.1 at the end.",7.1 CoNLL 2003 NER task,[0],[0]
"vi) Number of epochs: 128; vii)Embedding matrices case-sensitive and caseinsensitive word embeddings of 256 dimensions, trained from Reuters RCV1; viii)",7.1 CoNLL 2003 NER task,[0],[0]
We stick to the official data train-dev-test partition.,7.1 CoNLL 2003 NER task,[0],[0]
ix),7.1 CoNLL 2003 NER task,[0],[0]
Forgetting factor α = 0.5.,7.1 CoNLL 2003 NER task,[0],[0]
"1
We have investigated the performance of our method on the CoNLL-2003 dataset by using different combinations of the FOFE features (both word-level and character-level).",7.1 CoNLL 2003 NER task,[0],[0]
The detailed comparison results are shown in Table 1.,7.1 CoNLL 2003 NER task,[0],[0]
"In Table 2, we have compared our best performance with some top-performing neural network systems on this task.",7.1 CoNLL 2003 NER task,[0],[0]
"As we can see from Table 2, our system (highest-first decoding) yields very strong performance (90.85 in F1 score) in this task, outperforming most of neural network models reported on this
1The choice of the forgetting factor α is empirical.",7.1 CoNLL 2003 NER task,[0],[0]
"We’ve evaluatedα = 0.5, 0.6, 0.7, 0.8 on a development set in some early experiments.",7.1 CoNLL 2003 NER task,[0],[0]
It turns out that α = 0.5 is the best.,7.1 CoNLL 2003 NER task,[0],[0]
"As a result, α = 0.5 is used for all NER/MD tasks throughout this paper.
dataset.",7.1 CoNLL 2003 NER task,[0],[0]
"More importantly, we have not used any hand-crafted features in our systems, and all features (either word or char level) are automatically derived from the data.",7.1 CoNLL 2003 NER task,[0],[0]
Highest-first and longestfirst perform similarly.,7.1 CoNLL 2003 NER task,[0],[0]
"In (Chiu and Nichols, 2016)2, a slightly better performance (91.62 in F1 score) is reported but a customized gazetteer is used in theirs.",7.1 CoNLL 2003 NER task,[0],[0]
"Given a document collection in three languages (English, Chinese and Spanish), the KBP2015 trilingual EDL task (Ji et al., 2015) requires to automatically identify entities (including nested entities) from a source collection of textual documents in multiple languages as in Table 3, and classify them into one of the following pre-defined five types: Person (PER), Geo-political Entity (GPE), Organization (ORG), Location (LOC) and Facility (FAC).",7.2 KBP2015 EDL Task,[0],[0]
"The corpus consists of news articles and discussion forum posts published in recent years, related but non-parallel across languages.
",7.2 KBP2015 EDL Task,[0],[0]
Three models are trained and evaluated independently.,7.2 KBP2015 EDL Task,[0],[0]
"Unless explicitly listed, hyperparameters follow those used for CoNLL2003 as described in section 7.1 and 2nd-pass model is not used.",7.2 KBP2015 EDL Task,[0],[0]
"Three sets of word embeddings of 128 dimensions are derived from English Gigaword (Parker et al., 2011), Chinese Gigaword (Graff and Chen, 2005) and Spanish Gigaword (Mendonca et al., 2009) respectively.",7.2 KBP2015 EDL Task,[0],[0]
Some language-specific modifications are made:,7.2 KBP2015 EDL Task,[0],[0]
"• Chinese: Because Chinese segmentation is
not reliable, we label Chinese at character level.",7.2 KBP2015 EDL Task,[0],[0]
The analogous roles of case-sensitive word-embedding and case-sensitive wordembedding are played by character embedding and word-embedding in which the character appears.,7.2 KBP2015 EDL Task,[0],[0]
"Neither Char FOFE features nor Char CNN features are used for Chinese.
",7.2 KBP2015 EDL Task,[0],[0]
• Spanish:,7.2 KBP2015 EDL Task,[0],[0]
Character set of Spanish is a super set of that of English.,7.2 KBP2015 EDL Task,[0],[0]
"When building character-level features, we use the mod function to hash each character’s UTF8 encoding into a number between 0 (inclusive) and 128 (exclusive).
",7.2 KBP2015 EDL Task,[0],[0]
"As shown in Table 4, our FOFE-based local detection method has obtained fairly strong perfor-
2In their work, they have used a combination of trainingset and dev-set to train the model, differing from all other systems (including ours) in Table 2.
mance in the KBP2015 dataset.",7.2 KBP2015 EDL Task,[0],[0]
"The overall trilingual entity discovery performance is slightly better than the best systems participated in the official KBP2015 evaluation, with 73.9 vs. 72.4 as measured by F1 scores.",7.2 KBP2015 EDL Task,[0],[0]
Outer and inner decodings are longest-first and highest-first respectively.,7.2 KBP2015 EDL Task,[0],[0]
"In KBP2016, the trilingual EDL task is extended to detect nominal mentions of all 5 entity types for all three languages.",7.3 KBP2016 EDL task,[0],[0]
"In our experiments, for simplicity, we treat nominal mention types as some extra entity types and detect them along with named entities together with a single model.",7.3 KBP2016 EDL task,[0],[0]
No official training set is provided in KBP2016.,7.3.1 Data Description,[0],[0]
We make use of three sets of training data:,7.3.1 Data Description,[0],[0]
"• Training and evaluation data in KBP2015:
as described in 7.2
• Machine-labeled Wikipedia (WIKI): When terms or names are first mentioned in a Wikipedia article they are often linked to the corresponding Wikipedia page by hyperlinks, which clearly highlights the possible named entities with well-defined boundary in the text.",7.3.1 Data Description,[0],[0]
We have developed a program to automatically map these hyperlinks into KBP annotations by exploring the infobox (if existing) of the destination page and/or examining the corresponding Freebase types.,7.3.1 Data Description,[0],[0]
"In this way, we have created a fairly large amount of weakly-supervised trilingual training data for the KBP2016 EDL task.",7.3.1 Data Description,[0],[0]
"Meanwhile, a gazeteer is created and used in KBP2016.
",7.3.1 Data Description,[0],[0]
"• In-house dataset: A set of 10,000 English and Chinese documents is manually labeled using some annotation rules similar to the KBP 2016 guidelines.
",7.3.1 Data Description,[0],[0]
"We split the available data into training, validation and evaluation sets in a ratio of 90:5:5.",7.3.1 Data Description,[0],[0]
"The models are trained for 256 epochs if the in-house data is not used, and 64 epochs otherwise.",7.3.1 Data Description,[0],[0]
"In our first set of experiments, we investigate the effect of using different training data sets on the final entity discovery performance.",7.3.2 Effect of various training data,[0],[0]
Different training runs are conducted on different combinations of the aforementioned data sources.,7.3.2 Effect of various training data,[0],[0]
"In Table 6, we have summarized the official English entity dis-
covery results from several systems we submitted to KBP2016 EDL evaluation round I and II.",7.3.2 Effect of various training data,[0],[0]
"The first system, using only the KBP2015 data to train the model, has achieved 0.697 in F1 score in the official KBP2016 English evaluation data.",7.3.2 Effect of various training data,[0],[0]
"After adding the weakly labeled data, WIKI, we can see the entity discovery performance is improved to 0.718 in F1 score.",7.3.2 Effect of various training data,[0],[0]
"Moreover, we can see that it yields even better performance by using the KBP2015 data and the in-house data sets to train our models, giving 0.750 in F1 score.",7.3.2 Effect of various training data,[0],[0]
The official best results of our system are summarized in Table 5.,7.3.3 The official trilingual EDL performance in KBP2016,[0],[0]
We have broken down the system performance according to different languages and categories of entities (named or nominal).,7.3.3 The official trilingual EDL performance in KBP2016,[0],[0]
"Our system, achieving 0.718 in F1 score in the KBP2016 trilingual EDL track, ranks second among all participants.",7.3.3 The official trilingual EDL performance in KBP2016,[0],[0]
"Note that our result is produced by a single system while the top system is a combination of two different models, each of which is based on 5-fold cross-validation (Liu et al., 2016).",7.3.3 The official trilingual EDL performance in KBP2016,[0],[0]
"In this paper, we propose a novel solution to NER and MD by applying FFNN on top of FOFE features.",8 Conclusion,[0],[0]
"This simple local-detection based approach has achieved almost state-of-the-art performance on various NER and MD tasks, without using any external knowledge or feature engineering.",8 Conclusion,[0],[0]
"This work is supported mainly by a research donation from iFLYTEK Co., Ltd., Hefei, China, and partially by a discovery grant from Natural Sciences and Engineering Research Council (NSERC) of Canada.",Acknowledgement,[0],[0]
"In this paper, we study a novel approach for named entity recognition (NER) and mention detection (MD) in natural language processing.",abstractText,[0],[0]
"Instead of treating NER as a sequence labeling problem, we propose a new local detection approach, which relies on the recent fixed-size ordinally forgetting encoding (FOFE) method to fully encode each sentence fragment and its left/right contexts into a fixedsize representation.",abstractText,[0],[0]
"Subsequently, a simple feedforward neural network (FFNN) is learned to either reject or predict entity label for each individual text fragment.",abstractText,[0],[0]
"The proposed method has been evaluated in several popular NER and MD tasks, including CoNLL 2003 NER task and TAC-KBP2015 and TAC-KBP2016 Tri-lingual Entity Discovery and Linking (EDL) tasks.",abstractText,[0],[0]
Our method has yielded pretty strong performance in all of these examined tasks.,abstractText,[0],[0]
This local detection approach has shown many advantages over the traditional sequence labeling methods.,abstractText,[0],[0]
A Local Detection Approach for Named Entity Recognition and Mention Detection,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 652–662 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics
We introduce MeSys, a meaning-based approach, for solving English math word problems (MWPs) via understanding and reasoning in this paper. It first analyzes the text, transforms both body and question parts into their corresponding logic forms, and then performs inference on them. The associated context of each quantity is represented with proposed role-tags (e.g., nsubj, verb, etc.), which provides the flexibility for annotating an extracted math quantity with its associated context information (i.e., the physical meaning of this quantity). Statistical models are proposed to select the operator and operands. A noisy dataset is designed to assess if a solver solves MWPs mainly via understanding or mechanical pattern matching. Experimental results show that our approach outperforms existing systems on both benchmark datasets and the noisy dataset, which demonstrates that the proposed approach understands the meaning of each quantity in the text more.",text,[0],[0]
"The math word problem (MWP) (see Figure 1) is frequently chosen to study natural language understanding and simulate human problem solving (Bakman, 2007; Hosseini et al., 2014; Liang et al., 2016) for the following reasons: (1) the answer to the MWP cannot be simply extracted by performing keyword/pattern matching.",1 Introduction,[0],[0]
"It thus shows the merit of understanding and inference.
",1 Introduction,[0],[0]
"(2) An MWP usually possesses less complicated syntax and requires less amount of domain knowledge, so the researchers can focus on the task of understanding and reasoning.",1 Introduction,[0],[0]
(3) The body part of MWP that provides the given information for solving the problem consists of only a few sentences.,1 Introduction,[0],[0]
The understanding and reasoning procedures thus could be more efficiently checked.,1 Introduction,[0],[0]
"(4) The MWP solver has its own applications such as Computer Math Tutor (for students in primary school) and Helper for Math in Daily Life (for adults who are not good in solving mathematics related real problems).
",1 Introduction,[0],[0]
"According to the approaches used to identify entities, quantities, and to select operations and operands, previous MWP solvers can be classified into: (1) Rule-based approaches (Mukherjee and Garain, 20081; Hosseini et al., 2014), which make all related decisions based on a set of rules; (2) Purely statistics-based approaches (Kushman et al., 2014; Roy et al., 2015; Zhou et al., 2015; Upadhyay et al., 2016), in which all related decisions are done via a statistical classifier; (3) DNNbased approaches (Ling et al., 2017; Wang et al., 2017), which map the given text into the corresponding math operation/equation via a DNN; and (4) Mixed approaches, which identify entities and quantities with rules, yet, decide operands and operations via statistical/DNN classifiers.",1 Introduction,[0],[0]
"This category can be further divided into two subtypes: (a) Without understanding (Roy and Roth, 2015; Koncel-Kedziorski et al., 2015; Huang et al., 2017; Shrivastava et al., 2017), which does not check the entity-attribute consistency between each quantity and the target of the given question; and (b) With understanding (Lin et al., 2015; Mitra and Baral, 2016; Roy and Roth, 2017), which also checks the entity-attribute consistency while solving the problem.",1 Introduction,[0],[0]
1,1 Introduction,[0],[0]
"It is a survey paper which reviews most of the rule-based approaches before 2008.
",1 Introduction,[0],[0]
"652
However, a widely covered rule-set is difficult to construct for the rule-based approach.",1 Introduction,[0],[0]
"Also, it is awkward in resolving ambiguity problem.",1 Introduction,[0],[0]
"In contrast, the performance of purely statistics-based approaches deteriorates significantly when the MWP includes either irrelevant information or information gaps (Hosseini et al., 2014), as it is solved without first understanding the meaning.
",1 Introduction,[0],[0]
"For the category (4a), since the physical meaning is only implicitly utilized and the result is not generated via inference, it would be difficult to explain how the answer is obtained in a human comprehensible way.",1 Introduction,[0],[0]
"Therefore, the categories (2), (3) and (4a) belong to the less favored direct translation",1 Introduction,[0],[0]
"approach2 (Pape, 2004).
",1 Introduction,[0],[0]
"In contrast, the approaches of (4b) can avoid the problems mentioned above.",1 Introduction,[0],[0]
"However, among them, Mitra and Baral (2016) merely handled Addition and Subtraction.",1 Introduction,[0],[0]
Only the meaning-based framework proposed by Lin et al. (2015) can handle general MWPs via understanding and reasoning.,1 Introduction,[0],[0]
"Therefore, it is possible to explain how the answer is obtained in a human comprehensible way (Huang et al., 2015).",1 Introduction,[0],[0]
"However, although their design looks promising, only a few Chinese MWPs had been tested and performance was not evaluated.",1 Introduction,[0],[0]
"Accordingly, it is hard to make a fair comparison between their approach and other state-of-the-art methods.",1 Introduction,[0],[0]
"In addition, in their prototype system, the desired operands of arithmetic operations are identified with predefined lexicosyntactic patterns and ad-hoc rules.",1 Introduction,[0],[0]
"Reusing the patterns/rules designed for Chinese in another language is thus difficult even if it is possible.
",1 Introduction,[0],[0]
"In this paper, we adopt the framework proposed by Lin et al. (2015) to solve English MWPs (for its potential in solving difficult/complex MWPs and providing more human comprehensible explanations).",1 Introduction,[0],[0]
"Additionally, we make the following improvements: (1) A new statistical model is proposed to select operands for arithmetic operations, and its model parameters can be automatically learnt via weakly supervised learning (Artzi and Zettlemoyer, 2013).",1 Introduction,[0],[0]
(2) A new informative and robust feature-set is proposed to select the desired arithmetic operation.,1 Introduction,[0],[0]
(3) We show the proposed approach significantly outperforms other existing systems on the common benchmark datasets reported in the literature.,1 Introduction,[0],[0]
"(4) A noisy dataset with 2 According to (Pape, 2004), the meaning-based approach of solving MWPs achieves the best performance among various behaviors adopted by middle school children.
more irrelevant quantities in MWPs is created and released.",1 Introduction,[0],[0]
It could be used to check if an approach really understands what a given MWP looks for.,1 Introduction,[0],[0]
(5) An experiment is conducted to compare various approaches on this new dataset.,1 Introduction,[0],[0]
The superior performance of our system demonstrates that the proposed meaning-based approach has good potential in handling difficult/complex MWPs.,1 Introduction,[0],[0]
"The adopted meaning-based framework (Lin et al., 2015) is a pipeline with following four stages (see Figure 2): (1) Language Analysis, (2) Solution Type Identification, (3) Logic Form Transformation and (4) Logic Inference.",2 System Description,[0],[0]
"We use the Stanford CoreNLP suite (Manning et al., 2014) as the language analysis module.",2 System Description,[0],[0]
The other three modules are briefly described below.,2 System Description,[0],[0]
"Last, we adopt the weakly supervised learning (Artzi and Zettlemoyer, 2013; Kushman et al., 2014) to automatically learn the model parameters without manually annotating each MWP with the adopted solution type and selected operands benchmark.",2 System Description,[0],[0]
"After language analysis, each MWP is assigned with a specific solution type (such as Addition, Multiplication, etc.) which indicates the stereotype math operation pattern that should be adopted to solve this problem.",2.1 Solution Type Identification (STI),[0],[0]
We classify the English MWPs released by Hosseini et al. (2014) and Roy and Roth (2015) into 6 different types:,2.1 Solution Type Identification (STI),[0],[0]
"Addition, Subtraction, Multiplication, Division, Sum and TVQ-F3.",2.1 Solution Type Identification (STI),[0],[0]
"An SVM (Chang and Lin, 2011) is used to identify the solution type with 26 features.",2.1 Solution Type Identification (STI),[0],[0]
"Most of them are derived from some important properties associated with each quantity.
",2.1 Solution Type Identification (STI),[0],[0]
"3 TVQ-F means to get the final state of a Time-VariantQuantity that involves both Addition and Subtraction.
",2.1 Solution Type Identification (STI),[0],[0]
"In addition to the properties Entity4 and Verb (Hosseini et al., 2014) associated with the quantity, we also introduce a new property Time which encodes the tense and aspect of a verb into an integer to specify a point in the timeline.",2.1 Solution Type Identification (STI),[0],[0]
"We assign 2, 4, and 6 to the tenses Past, Present and Future, respectively, and then adjust it with the aspectvalues -1, 0 and 1 for Perfect, Simple, and Progressive, respectively.
",2.1 Solution Type Identification (STI),[0],[0]
Another property Anchor is associated with the unknown quantity asked in the question sentence.,2.1 Solution Type Identification (STI),[0],[0]
"If the subject of the question sentence is a noun phrase (e.g., “how many apples does John have?”), Anchor is the subject (i.e., John).",2.1 Solution Type Identification (STI),[0],[0]
"If the subject is an expletive nominal (e.g. “how many apples are there in the box?”), then Anchor is the associated nominal modifier nmod (i.e., “box”).",2.1 Solution Type Identification (STI),[0],[0]
"Otherwise, Anchor is set to “Unknown”.
",2.1 Solution Type Identification (STI),[0],[0]
"Inspired by (Hosseini et al., 2014), we transform Verb to Verb-Class (VC) which is positive, negative or stative.",2.1 Solution Type Identification (STI),[0],[0]
A verb is positive/negative if it increases/decreases the associated quantity of the subject.,2.1 Solution Type Identification (STI),[0],[0]
"For example, in the sentence “Tom borrowed 3 dollars from Mike”, the verb is positive because the money of subject “Tom” increases.
",2.1 Solution Type Identification (STI),[0],[0]
"However, a positive verb does not always imply the Addition operation.",2.1 Solution Type Identification (STI),[0],[0]
"If the question is “How much money does Mike have now?” for the above body sentence, the operation should be Subtraction.",2.1 Solution Type Identification (STI),[0],[0]
"Two new properties Anchor-Role (AR) and Action (A) are thus proposed: ARi indicates the role that Anchor associated with qi, and is set to nsubj/obj/nmod/φ.",2.1 Solution Type Identification (STI),[0],[0]
"Ai is determined by following rules: (1) Ai=positive if (VCi, ARi) is either (positive, nsubj) or (negative, obj/nmod).",2.1 Solution Type Identification (STI),[0],[0]
"(2) Ai=negative if (VCi, ARi) is either (negative,
4",2.1 Solution Type Identification (STI),[0],[0]
"In our works, the term “Entity” also includes the unit of the quantity (e.g., “cup of coffee”).
nsubj) or (positive, obj/nmod).",2.1 Solution Type Identification (STI),[0],[0]
"(3) Otherwise, Ai=VCi.
To rule out the noisy quantities introduced by irrelevant information, we further associate each known quantity with the property Relevance (R) according to the unknown quantity asked in the question sentence.",2.1 Solution Type Identification (STI),[0],[0]
"Let qi denote the i-th known quantity, Ei denote the entity of qi, Xi denote the property X of qi, qU denote the unknown quantity asked, and XU denote the property X of qU. Ri is specified with following rules: (1) Ri=2",2.1 Solution Type Identification (STI),[0],[0]
(DirectlyRelated) if either {Anchor is Unknown & Ei entails EU} or {Anchor is not Unknown & ARi≠φ & Ei entails EU} (2) Ri=1,2.1 Solution Type Identification (STI),[0],[0]
"(Indirectly-Related) if there is a qj which maps5 to qi and Rj=2 (i.e., qj is Directly-Related).",2.1 Solution Type Identification (STI),[0],[0]
"(3) Ri=0 (Unrelated) otherwise.
",2.1 Solution Type Identification (STI),[0],[0]
The solution type is identified by an SVM based on 26 binary features.,2.1 Solution Type Identification (STI),[0],[0]
"Let the symbols p, n, s, A, E, R, T, V, SB, SQ and wQ stand for positive, negative, stative, Action, Entity, Relevance, Time, Verb, “a body sentence”, “the question sentence” and “a word in question sentence” respectively.",2.1 Solution Type Identification (STI),[0],[0]
"Also, let I(x) be the indicator function to check if x is true.",2.1 Solution Type Identification (STI),[0],[0]
"The 26 features are briefly described as follows:
(1) VCU=p; (2) ∃Ri=2 s.t.",2.1 Solution Type Identification (STI),[0],[0]
Ai=p; (3) ∃Ri=2 s.t.,2.1 Solution Type Identification (STI),[0],[0]
Ai=n; (4) ∃Ri=2 s.t.,2.1 Solution Type Identification (STI),[0],[0]
Ai=s; (5) ∑𝑖𝑖,2.1 Solution Type Identification (STI),[0],[0]
I( Ri =2) > 2; (6) ∑𝑖𝑖,2.1 Solution Type Identification (STI),[0],[0]
"I( Ri=2 & Ai ∈{p, n} )",2.1 Solution Type Identification (STI),[0],[0]
= 2; (7) ∃Ri=2 s.t.,2.1 Solution Type Identification (STI),[0],[0]
Ai=p & TU<Ti; (8) ∃Ri=2 s.t.,2.1 Solution Type Identification (STI),[0],[0]
Ai=n & TU<Ti; (9) ∃Ri=2 s.t.,2.1 Solution Type Identification (STI),[0],[0]
Ai=s & Ti=max Tj; (10) ∃Ri=2 s.t.,2.1 Solution Type Identification (STI),[0],[0]
Ai=s & Ti<TU; (11) TU ≥ max Ti; (12) TU ≤,2.1 Solution Type Identification (STI),[0],[0]
"min Ti; (13) ∀Ri=2, Vi are the same; (14) ∀Ri=2",2.1 Solution Type Identification (STI),[0],[0]
"s.t. Ti=TU; (15) ∀Ri=2, Ti are the same; (16) ∃Ri=2, ∃Rj=1 s.t. qi maps to qj & qi > qj;
5",2.1 Solution Type Identification (STI),[0],[0]
"That is, 𝑞𝑞𝑖𝑖 is linked to a directly-related quantity 𝑞𝑞𝑗𝑗 under an expression such as “2 pencils weigh 30 grams”.
(17) ∃Ri=2, ∃Rj=1 s.t. qi maps to qj & qi is associated with a word “each/every/per/a/an”; (18) ∃Ri=2, ∃Rj=1 s.t. qi maps to qj & qj is associated with a word “each/every/per/a/an”; (19) ∃qi, qj, qk s.t. Ri = Rj = Rk =2 & Vi = Vj = Vk; (20) ∃wQ ∈{total, in all, altogether, sum}; (21) ∃wQ ∈{more, than} or ∃wQ s.t. wQ-POS=RBR; (22) ∃wQ =“left”; (23).",2.1 Solution Type Identification (STI),[0],[0]
∃qi appears in SQ; (24) “the rest V EU” appears in SB (V for any verb); (25) “each NN” appears in SQ (NN for any noun); (26) AnchorU,2.1 Solution Type Identification (STI),[0],[0]
is Unknown/nmod & VCU = s.,2.1 Solution Type Identification (STI),[0],[0]
"The results of language analysis are transformed into a logic form, which is expressed with the first-order logic (FOL) formalism (Russell and Norvig, 2009).",2.2 Logic Form Transformation (LFT),[0],[0]
Figure 3 shows how to transform the sentence (a) “Pack 100 candies into 5 boxes.” into the corresponding logic form (d).,2.2 Logic Form Transformation (LFT),[0],[0]
"First, the dependency tree (b) is transformed into the semantic representation tree (c) adopted by Lin et al., (2015).",2.2 Logic Form Transformation (LFT),[0],[0]
"Afterwards, according to the procedure proposed in (Lin et al., 2015), the domaindependent logic expressions are generated in (d).
",2.2 Logic Form Transformation (LFT),[0],[0]
"The domain-dependent logic expressions are related to crucial generic math facts, such as quantities and relations between quantities.",2.2 Logic Form Transformation (LFT),[0],[0]
"The FOL function quan(quanid, unit6,entity)=number is for describing the quantity fact.",2.2 Logic Form Transformation (LFT),[0],[0]
The first argument denotes its unique identifier.,2.2 Logic Form Transformation (LFT),[0],[0]
The other arguments and the function value describe its meaning.,2.2 Logic Form Transformation (LFT),[0],[0]
"Another FOL predicate qmap(mapid, quanid1, quanid2) (denotes the mapping from quanid1 to quanid2) is for describing a relation between two quantity facts, where the first argument is a unique identifier to represent this relation.
",2.2 Logic Form Transformation (LFT),[0],[0]
"The role-tags (e.g., verb, dobj, etc.) associated with quanid and mapid denote entity attributes (i.e., the physical meaning of the quantity), are created to help the logic inference module find the 6 This second argument denotes the associated unit used to count the entity.",2.2 Logic Form Transformation (LFT),[0],[0]
"It is set to “#” if the unit of the entity is not specified.
solution.",2.2 Logic Form Transformation (LFT),[0],[0]
"For example, quan(q2,#,box) = 5 & verb(q2,pack) &… means that q2 is the quantity of boxes being packed.",2.2 Logic Form Transformation (LFT),[0],[0]
"With those role-tags, the system can select the operands more reliably, and the inference engine can also derive new quantities to solve complex MWPs which require multi-step arithmetic operations (see section 2.3).
",2.2 Logic Form Transformation (LFT),[0],[0]
The question in the MWP is also transformed into an FOL-like utility function according to the solution type to ask the logic inference module to find out the answer.,2.2 Logic Form Transformation (LFT),[0],[0]
"For example, the utility function instance Division(quan(q1, #, candy), quan(q2, #, box)) asks the inference module to divide “100 candies” by “5 boxes”.",2.2 Logic Form Transformation (LFT),[0],[0]
"Since associated operands must be specified before calling those utility functions, a statistical model (see section 2.4) is used to identify the appropriate quantities.",2.2 Logic Form Transformation (LFT),[0],[0]
"The logic inference module adopts the inference engine from (Lin et al., 2015).",2.3 Logic Inference,[0],[0]
Figure 4 shows how it uses inference rules to derive new facts from the initial facts directly provided from the description.,2.3 Logic Inference,[0],[0]
The MWP (a) provides some facts (b) generated from the LFT module.,2.3 Logic Inference,[0],[0]
"An inference rule (c) 7 , which implements the common sense that people must pay money to buy something, is unified with the given facts (b) and derives new facts (d).",2.3 Logic Inference,[0],[0]
"The facts associated with q6 can be interpreted as “Mary paid 0.5 dollar for two puddings”.
",2.3 Logic Inference,[0],[0]
"The inference engine (IE) also provides 5 utility functions, including Addition, Subtraction, Multiplication and Division, and Sum.",2.3 Logic Inference,[0],[0]
The first four utilities all return a value by performing the named math operation on its two input arguments.,2.3 Logic Inference,[0],[0]
"On the other hand, Sum(function,condition) returns the sum of the values of FOL function instances which can be unified with the first argument (i.e., function) and satisfy the second argument (i.e., condition).",2.3 Logic Inference,[0],[0]
"For example, according to 7 In the inference rule, $q is a meta symbol to ask the inference engine to generate a unique identifier for the newly derived quantity fact.
",2.3 Logic Inference,[0],[0]
(a) A sandwich is priced at $0.75.,2.3 Logic Inference,[0],[0]
A pudding is priced at $0.25.,2.3 Logic Inference,[0],[0]
Tim bought 2 sandwiches and 4 puddings.,2.3 Logic Inference,[0],[0]
Mary bought 2 puddings.,2.3 Logic Inference,[0],[0]
"How much money should Tim pay?
(b) …price(sandwich,0.75)&price(pudding,0.25)… quan(q1,#,sandwich)=2&verb(q1,buy)&nsubj(q1,Tim)… quan(q2,#,pudding)=4&verb(q2,buy)&nsubj(q2,Tim)… quan(q3,#,pudding)=2&verb(q3,buy)&nsubj(q3,Mary)… ASK Sum(quan(?q,dollar,#),verb(?q,pay)&nsubj(?q,Tim))
",2.3 Logic Inference,[0],[0]
"(c) quan(?q,?u,?o)&verb(?q,buy)&nsubj(?q,?a)&price(?o,?p)  quan($q,dollar,#)=quan(?q,?u,?o)×?p & verb($q,pay) & nsubj($q,?a) (d) quan(q4,dollar,#)=1.5&verb(q4,pay)&nsubj(q4,Tim)… quan(q5,dollar,#)=1&verb(q5,pay)&nsubj(q5,Tim)…
quan(q6,dollar,#)=0.5&verb(q6,pay)&nsubj(q6,Mary)
",2.3 Logic Inference,[0],[0]
"Figure 2: A logic inference example
(a) A sandwich is priced at $0.75.",2.3 Logic Inference,[0],[0]
A pudding is priced at $0.25.,2.3 Logic Inference,[0],[0]
Tim bought 2 sandwiches and 4 puddings.,2.3 Logic Inference,[0],[0]
Mary bought 2 puddings.,2.3 Logic Inference,[0],[0]
"How much money should Tim pay?
(b) price(sandwich,0.75)&price(pudding,0.25) quan(q1,#,sandwich)=2&verb(q1,buy)&nsubj(q1,Tim) quan(q2,#,pudding)=4&verb(q2,buy)&nsubj(q2,Tim) quan(q3,#,pudding)=2&verb(q3,buy)&nsubj(q3,Mary) ASK Sum(quan(?q,dollar,#),verb(?q,pay)&nsubj(?q,Tim))
",2.3 Logic Inference,[0],[0]
"(c) quan(?q,?u,?o)&verb(?q,buy)&nsubj(?q,?a)&price(?o,?p) quan($q,dollar,#)=quan(?q,?u,?o)×?p & verb($q,pay) & nsubj($q,?a) (d) quan(q4,dollar,#)=1.5&verb(q4,pay)&nsubj(q4,Tim)",2.3 Logic Inference,[0],[0]
"quan(q5,dollar,#)=1&verb(q5,pay)&nsubj(q5,Tim)
",2.3 Logic Inference,[0],[0]
"quan(q6,dollar,#)=0.5&verb(q6,pay)&nsubj(q6,Mary)
",2.3 Logic Inference,[0],[0]
"Figure 4: A logic inference example
the last line in Figure 4(b), three newly derived quantity facts q4, q5 and q6 (in 4(d)) can be unified with the first argument quan(?q,dollar,#) in 4(c), but only q4 and q5 satisfy the second argument verb(?q,pay)&nsubj(?q,Tim).",2.3 Logic Inference,[0],[0]
"As a result, the answer 2.5 is returned by taking sum on the values of the quantity facts quan(q4,dollar,#) and quan(q5,dollar,#).",2.3 Logic Inference,[0],[0]
The most error-prone part in the LFT module is instantiating the utility function of math operation especially if many irrelevant quantity facts appear in the given MWP.,2.4 Probabilistic Operand Selection,[0],[0]
Figure 5 shows the LFT module needs to select two quantity facts (among 4) for Addition.,2.4 Probabilistic Operand Selection,[0],[0]
"Please note that the question quantity qQ, transformed from “how many flowers”, is not a candidate for operand selection.
",2.4 Probabilistic Operand Selection,[0],[0]
"Lin et al., (2015) used predefined lexicosyntactic patterns and ad-hoc rules to instantiate utility functions.",2.4 Probabilistic Operand Selection,[0],[0]
"However, their rule-based approach fails when the MWP involves more quantities.",2.4 Probabilistic Operand Selection,[0],[0]
"Therefore, we propose a statistical model to select operands for the utility functions Addition, Subtraction, Multiplication and Division.",2.4 Probabilistic Operand Selection,[0],[0]
"The operand selection procedure can be regarded as finding the most likely configuration (𝑜𝑜1𝑛𝑛, 𝑟𝑟), where 𝑜𝑜1𝑛𝑛 = 𝑜𝑜1,⋯ , 𝑜𝑜𝑛𝑛 is a sequence of random indicators which denote if the corresponding quantity will be selected as an operand, and 𝑟𝑟 is a tri-state variable to represent the relation between the values of two operands (i.e., 𝑟𝑟 = −1, 0 or 1 ; which denote that the first operand is less than, equal to, or greater than the second operand, respectively).",2.4 Probabilistic Operand Selection,[0],[0]
"Given a solution type 𝑠𝑠, the MWP logic expressions 𝕃𝕃 and the 𝑛𝑛 quantities 𝑞𝑞1𝑛𝑛 = 𝑞𝑞1,⋯ , 𝑞𝑞𝑛𝑛 in 𝕃𝕃.",2.4 Probabilistic Operand Selection,[0],[0]
"The procedure is formulated as:
𝑃𝑃(𝑟𝑟, 𝑜𝑜1𝑛𝑛|𝑞𝑞1𝑛𝑛,𝕃𝕃, 𝑠𝑠) ≈ 𝑃𝑃(𝑟𝑟|𝑠𝑠) × 𝑃𝑃(𝑜𝑜1𝑛𝑛|𝑞𝑞1𝑛𝑛,𝕃𝕃, 𝑠𝑠), (1)
𝑃𝑃(𝑟𝑟|𝑠𝑠) simply refers to Relative Frequency (as it has only a few parameters and we have enough training samples). 𝑃𝑃(𝑜𝑜1𝑛𝑛|𝑞𝑞1𝑛𝑛,𝕃𝕃",2.4 Probabilistic Operand Selection,[0],[0]
", 𝑠𝑠) is further derived as:
𝑃𝑃(𝑜𝑜1𝑛𝑛|𝑞𝑞1𝑛𝑛,𝕃𝕃, 𝑠𝑠) ≈ ∏ 𝑃𝑃(𝑜𝑜𝑖𝑖|𝑞𝑞𝑖𝑖 ,𝕃𝕃, 𝑠𝑠)𝑛𝑛𝑖𝑖=1 ≈ ∏ 𝑃𝑃�𝑜𝑜𝑖𝑖�Φ(𝑞𝑞𝑖𝑖 ,𝕃𝕃, 𝑠𝑠)�,𝑛𝑛𝑖𝑖=1 (2)
where Φ(∙) is a feature extraction function to map 𝑞𝑞𝑖𝑖 and its context into a feature vector.",2.4 Probabilistic Operand Selection,[0],[0]
"Here, the probabilistic factor 𝑃𝑃�𝑜𝑜𝑖𝑖�Φ(𝑞𝑞𝑖𝑖,𝕃𝕃, 𝑠𝑠)� is obtained via an SVM classifier (Chang and Lin, 2011).",2.4 Probabilistic Operand Selection,[0],[0]
"Φ(∙) extracts total 25 features (specified as follows, and 24 of them are binary) for 𝑞𝑞𝑖𝑖. The following 11 of them are independent on the question in the MWP:
1.",2.4 Probabilistic Operand Selection,[0],[0]
"Four features to indicate if 𝑠𝑠 is Addition, Subtraction, Multiplication or Division.",2.4 Probabilistic Operand Selection,[0],[0]
2.,2.4 Probabilistic Operand Selection,[0],[0]
A feature to indicate if 𝑞𝑞𝑖𝑖 is within a qmap(…).,2.4 Probabilistic Operand Selection,[0],[0]
3.,2.4 Probabilistic Operand Selection,[0],[0]
A feature to indicate if 𝑞𝑞𝑖𝑖 = 1. 4.,2.4 Probabilistic Operand Selection,[0],[0]
"Five features to indicate if 𝑛𝑛 < 2, 𝑛𝑛 = 2, 𝑛𝑛 =
3, 𝑛𝑛 = 4 or 𝑛𝑛 > 4; where 𝑛𝑛 is the number of quantities in Eq (1).
Φ(∙) also extracts features by matching the logic expressions of 𝑞𝑞𝑖𝑖 with those of question quantity qQ to check the role-tag consistencies between 𝑞𝑞𝑖𝑖 and qQ. Another fourteen features are extracted with three indicator functions 𝐼𝐼𝑚𝑚(⋅), 𝐼𝐼𝑒𝑒(⋅), 𝐼𝐼∃(⋅) and one tri-state function 𝑇𝑇𝑚𝑚(⋅) as follows:
[ 𝐼𝐼𝑚𝑚(𝑞𝑞𝑖𝑖 , qQ, entity), 𝐼𝐼𝑒𝑒(𝑞𝑞𝑖𝑖 , qQ, entity), 𝐼𝐼𝑚𝑚(𝑞𝑞𝑖𝑖 , qQ, verb), 𝐼𝐼𝑒𝑒(𝑞𝑞𝑖𝑖 , qQ, verb), 𝐼𝐼∃(qQ, nsubj),𝑇𝑇𝑚𝑚(𝑞𝑞𝑖𝑖 , qQ, nsubj), 𝐼𝐼∃(qQ, modifier), 𝐼𝐼𝑚𝑚(𝑞𝑞𝑖𝑖 , qQ, modifier), 𝐼𝐼∃(qQ, place), 𝐼𝐼𝑚𝑚(𝑞𝑞𝑖𝑖 , qQ, place), 𝐼𝐼∃(qQ, temporal), 𝐼𝐼𝑚𝑚(𝑞𝑞𝑖𝑖 , qQ, temporal), 𝐼𝐼∃(qQ, xcomp), 𝐼𝐼𝑚𝑚(𝑞𝑞𝑖𝑖 , qQ, xcomp)",2.4 Probabilistic Operand Selection,[0],[0]
"]
where the indicator functions 𝐼𝐼𝑚𝑚(𝑥𝑥,𝑦𝑦, 𝑧𝑧) checks if the 𝑧𝑧 of 𝑥𝑥 matches the 𝑧𝑧 of 𝑦𝑦, 𝐼𝐼𝑒𝑒(𝑥𝑥,𝑦𝑦, 𝑧𝑧) checks if the 𝑧𝑧 of 𝑥𝑥 entails the 𝑧𝑧 of 𝑦𝑦 and 𝐼𝐼∃(𝑦𝑦, 𝑧𝑧) checks if the 𝑧𝑧 of 𝑦𝑦 exists.",2.4 Probabilistic Operand Selection,[0],[0]
"𝑇𝑇𝑚𝑚(𝑞𝑞𝑖𝑖, qQ, nsubj) returns “exactmatch” (if nsubj of 𝑞𝑞𝑖𝑖 matches nsubj of qQ ), “quasi-match” (if nsubj of qQ does not exist or is a plural pronoun), and “unmatch”.",2.4 Probabilistic Operand Selection,[0],[0]
𝐼𝐼𝑒𝑒(⋅) uses the WordNet hypernym and hyponym relationship to judge whether one entity/verb entails another one or not via checking if they are in an inherited hypernym-path in WordNet.,2.4 Probabilistic Operand Selection,[0],[0]
"The entity, verb and nsubj of a quantity are determined according to the logic expressions.",2.4 Probabilistic Operand Selection,[0],[0]
"The modifier, place, temporal and xcomp of a quantity are extracted from the dependency tree with some lexico-syntactic patterns.",2.4 Probabilistic Operand Selection,[0],[0]
"For example, the modifier and place of the quantity in the sentence “There are 30 red flowers in the garden.”",2.4 Probabilistic Operand Selection,[0],[0]
are “red” and “garden” respectively.,2.4 Probabilistic Operand Selection,[0],[0]
"The temporal
and xcomp of a quantity are extracted according to the dependency relations “tmod” (i.e., temporal modifier) and “xcomp” (i.e., open clausal complement), respectively.",2.4 Probabilistic Operand Selection,[0],[0]
The AI2 dataset provided by Hosseini et al. (2014) and the IL dataset released by Roy and Roth (2015) are adopted to compare our approach with other state-of-the-art methods.,3 Datasets for Performance Evaluation,[0],[0]
"The AI2 dataset has 395 MWPs on addition and subtraction, with 121 MWPs containing irrelevant information (Hosseini et al., 2014).",3 Datasets for Performance Evaluation,[0],[0]
It is the most popular one for comparing different approaches.,3 Datasets for Performance Evaluation,[0],[0]
"On the other hand, the IL dataset consists of 562 elementary MWPs which can be solved by one of the four arithmetic operations (i.e., +, −, ×, and ÷) without any irrelevant quantity.",3 Datasets for Performance Evaluation,[0],[0]
"It is the first publicly available dataset for comparing performances that covers all four arithmetic operations.
",3 Datasets for Performance Evaluation,[0],[0]
"However, the difficulty of solving an MWP depends not only on the number of arithmetic operations required, but also on how many irrelevant quantities inside, and even on how the quantities are described.",3 Datasets for Performance Evaluation,[0],[0]
One way to test if a proposed approach solves the MWPs with understanding is to check whether it is robust to those irrelevant quantities.,3 Datasets for Performance Evaluation,[0],[0]
"Therefore, it is desirable to have a big enough dataset that contains irrelevant quantities which are created under different situations (e.g., confusing with an irrelevant agent, entity, or modifier, etc.) and allow us to probe the system weakness from different angles.",3 Datasets for Performance Evaluation,[0],[0]
We thus create a new dataset with more irrelevant quantities8.,3 Datasets for Performance Evaluation,[0],[0]
"But before we do that, we need to know how difficult the task of solving the given MWPs is.",3 Datasets for Performance Evaluation,[0],[0]
"Therefore, we first propose a way to measure how easy that a system solves the problem by simply guessing.",3 Datasets for Performance Evaluation,[0],[0]
"We propose to adopt the Perplexity to measure the task difficulty, which evaluates how likely a solver will get the correct answer by guessing.",3.1 Perplexity-flavor Measure,[0],[0]
"Every MWP in the datasets can be associated with a solution expression template, such as “⊡ + ⊡” or “⊡−⊡”, where the symbol ⊡ represents a slot to hold a quantity.",3.1 Perplexity-flavor Measure,[0],[0]
The solution can be obtained by placing correct quantities at appropriate slots.,3.1 Perplexity-flavor Measure,[0],[0]
"A 8 The IL dataset does not include any irrelevant information; on the other hand, the AI2 dataset only contains 121 MWPS with irrelevant information (but not systematically created).
",3.1 Perplexity-flavor Measure,[0],[0]
random baseline is to solve an MWP by guessing.,3.1 Perplexity-flavor Measure,[0],[0]
"It first selects a solution expression template according to the prior distribution of the templates and then places quantities into the selected template according to the uniform distribution.
",3.1 Perplexity-flavor Measure,[0],[0]
The expected accuracy of the random baseline on solving an MWP is a trivial combination and permutation exercise9.,3.1 Perplexity-flavor Measure,[0],[0]
"For example, the expected accuracy of solving an MWP associated with “⊡ + ⊡” template is 𝑝𝑝⊡+⊡ ×",3.1 Perplexity-flavor Measure,[0],[0]
"𝐶𝐶𝑛𝑛 2 −1 , where the factor 𝑝𝑝⊡+⊡ denotes the prior probability of the template “⊡ + ⊡” and 𝑛𝑛 is the total number of quantities (including irrelevant ones) in the MWP.",3.1 Perplexity-flavor Measure,[0],[0]
"On the other hand, expected accuracy of solving an MWP associated with “⊡−⊡”10 template is 𝑝𝑝⊡−⊡ × 𝑃𝑃𝑛𝑛 2 −1 .",3.1 Perplexity-flavor Measure,[0],[0]
Let 𝐴𝐴𝑖𝑖 denote the expected accuracy of solving the 𝑖𝑖-th MWP in a dataset.,3.1 Perplexity-flavor Measure,[0],[0]
"The accuracy of the random baseline on the dataset of size 𝑁𝑁 is then computed as 𝐴𝐴 = (1/𝑁𝑁) × ∑ 𝐴𝐴𝑖𝑖𝑁𝑁𝑖𝑖=1 .
",3.1 Perplexity-flavor Measure,[0],[0]
"The word “Accuracy” comprises the opposite sense of the word “Perplexity”11 (i.e., in the sense of how hard a prediction problem is).",3.1 Perplexity-flavor Measure,[0],[0]
"The lower the Accuracy is, the higher the Perplexity is.",3.1 Perplexity-flavor Measure,[0],[0]
"Therefore, we transform the Accuracy measure into a Perplexity-Flavor measure (PP) via the formula: PP = 2− log2 𝐴𝐴 For instance, the Perplexity-Flavor measures of AI2 and IL datasets are 4.46 and 8.32 respectively.",3.1 Perplexity-flavor Measure,[0],[0]
"Human Math/Science tests have been considered more suitable for judging AI progress than Turing test (Clark and Etzioni, 2016).",3.2 Noisy Dataset,[0],[0]
"In our task, solving MWPs is mainly regarded as a test for intelligence (not just for creating a Math Solver package).",3.2 Noisy Dataset,[0],[0]
"By injecting various irrelevant quantities into original MWPs, a noisy dataset is thus created to assess if a solver solves the MWPs mainly via understanding or via mechanical/statistical pattern matching.",3.2 Noisy Dataset,[0],[0]
"If a system solves an MWP mainly via pattern matching, it would have difficulty in solving a similar MWP augmented from the original one with some irrelevant quantities.",3.2 Noisy Dataset,[0],[0]
"Therefore, we first create a noisy dataset by selecting some 9 Let 𝐶𝐶𝑛𝑛 𝑘𝑘 denote 𝑘𝑘-combinations of 𝑛𝑛 and 𝑃𝑃𝑛𝑛 𝑘𝑘 denote 𝑘𝑘- permutations of 𝑛𝑛. 10",3.2 Noisy Dataset,[0],[0]
"We assume the operands have different values and, therefore, they are not permutable for the subtraction operator.",3.2 Noisy Dataset,[0],[0]
11,3.2 Noisy Dataset,[0],[0]
"The Perplexity of a uniform distribution over k discrete events (such as casting a fair k-sided dice) is k.
MWPs that can be correctly solved, and then augmenting each of them with an additional noisy sentence which involves an irrelevant quantity.",3.2 Noisy Dataset,[0],[0]
"This dataset is created to examine if the solver knows that this newly added quantity is irrelevant.
",3.2 Noisy Dataset,[0],[0]
Figure 6 shows how we inject noise into an MWP (a).,3.2 Noisy Dataset,[0],[0]
"(a.1) is created by associating an irrelevant quantity to a new subject (i.e., Mary).",3.2 Noisy Dataset,[0],[0]
Here the ellipse symbol “…” denotes unchanged text.,3.2 Noisy Dataset,[0],[0]
"(a.2) is obtained by associating an irrelevant quantity to a new entity (i.e., books).",3.2 Noisy Dataset,[0],[0]
"In addition, we also change modifiers (such as yellow, red, …) to add new noisy sentence (not shown here).",3.2 Noisy Dataset,[0],[0]
"Since the noisy dataset is not designed to assess the lexicon coverage rate of a solver, we reuse the words in the original dataset as much as possible while adding new subjects, entities and modifiers.
136 MWPs that both Illinois Math Solver 12 (Roy and Roth, 2016) and our system can correctly solve are selected from the AI2 and IL datasets.",3.2 Noisy Dataset,[0],[0]
This subset is denoted as OSS (Original Sub-Set).,3.2 Noisy Dataset,[0],[0]
"Afterwards, based on the 136 MWPs of OSS, we create a noisy dataset of 396 MWPs by adding irrelevant quantities.",3.2 Noisy Dataset,[0],[0]
This noisy dataset is named as NDS13.,3.2 Noisy Dataset,[0],[0]
"Table 1 lists the size of MWPs, Perplexities (PP), and the average numbers of quantities in each MWP of these two datasets.",3.2 Noisy Dataset,[0],[0]
"We compare our approach with (Roy and Roth, 2015) and (Roy and Roth, 2017) because they achieved the state-of-the-art performance on the IL dataset.",4 Experimental Results and Discussion,[0],[0]
"In the approach of (Roy and Roth, 2015), each quantity in the MWP was associated with a quantity schema whose attributes are extracted from the context of the quantity.",4 Experimental Results and Discussion,[0],[0]
"Based on the attributes, several statistical classifiers were used to select operands and determine the operator.",4 Experimental Results and Discussion,[0],[0]
"They also reported the performances on the AI2 dataset to compare their approach with those 12 We submit MWPs to Illinois Math Solver (https://cogcomp.cs.illinois.edu/page/demo_view/Math) in May and June, 2017. 13",4 Experimental Results and Discussion,[0],[0]
The noisy dataset can be downloaded from https://github.com /chaochun/nlu-mwp-noise-dataset.,4 Experimental Results and Discussion,[0],[0]
"It includes 102 Addition, 147 Subtraction, 101 Multiplication and 46 Division MWPs.
of others (e.g., Kushman et al. (2014), which is a purely statistical approach that aligns the text with various pre-extracted equation templates).",4 Experimental Results and Discussion,[0],[0]
"Roy and Roth (2017) further introduced the concept of Unit Dependency Graphs to reinforce the consistency of physical units among selected operands associated with the same operator.
",4 Experimental Results and Discussion,[0],[0]
"To compare the performance of the statistical method with the DNN approach, we only implement a Bi-directional RNN-based Solution Type Identifier (as our original statistical Operand Selector is relatively much better).",4 Experimental Results and Discussion,[0],[0]
"It consists of a word embedding layer (for both body and question parts), and a bidirectional GRU layer as an encoder.",4 Experimental Results and Discussion,[0],[0]
"We apply the attention mechanism to scan all hidden state sequence of body by the last hidden state of question to pay more attention to those more important (i.e., more similar between the body and the question) words.",4 Experimental Results and Discussion,[0],[0]
"Lastly, it outputs the solution type by a softmax function.",4 Experimental Results and Discussion,[0],[0]
"We train it for 100 epochs, with mini-batch-size = 128 and learning-rate = 0.001; the number of nodes in the hidden layer is 200, and the drop-out rate is 0.714.
",4 Experimental Results and Discussion,[0],[0]
"We follow the same n-fold cross-validation evaluation setting adopted in (Roy and Roth, 2015) exactly.",4 Experimental Results and Discussion,[0],[0]
"Therefore, various performances could be directly compared.",4 Experimental Results and Discussion,[0],[0]
"Table 2 lists the accuracies of different systems in solving the MWPs 14 Since the dataset is not large enough for splitting a development set, we choose those hyper parameters based on the test set in coarse grain.",4 Experimental Results and Discussion,[0],[0]
"Therefore, the DNN performance we show here might be a bit optimistic.
of various datasets.",4 Experimental Results and Discussion,[0],[0]
"The performance of (Roy and Roth, 2017) system is directly delivered by their code15.",4 Experimental Results and Discussion,[0],[0]
"The last two rows are extracted from (Roy and Roth, 2015).",4 Experimental Results and Discussion,[0],[0]
"The results show that our performances of the statistical approach significantly outperform that of our DNN approach and other systems on every dataset.
",4 Experimental Results and Discussion,[0],[0]
The performances of STI and LFT modules are listed in Table 3.,4 Experimental Results and Discussion,[0],[0]
"As described in section 2, the benchmark for both solution type and the operand selection benchmark are automatically determined by weakly supervised learning.",4 Experimental Results and Discussion,[0],[0]
"The first and second rows of Table 3 show the solution type accuracies of our statistical and DNN approaches, respectively.",4 Experimental Results and Discussion,[0],[0]
The third row shows the operand selection accuracy obtained by given the solution type benchmark.,4 Experimental Results and Discussion,[0],[0]
"Basically, LFT accuracies are from 92% to 95%, and the system accuracies are dominated by STI.",4 Experimental Results and Discussion,[0],[0]
"We analyzed errors resulted from our statistical STI on AI2 and IL datasets, respectively.",4 Experimental Results and Discussion,[0],[0]
"For AI2, major errors come from: (1) failure of ruling out some irrelevant quantities (40%), and (2) making confusion between TVQ-F and Sum these two solution types (20%) for those cases that only involve addition operation (however, both types would return the same answer).",4 Experimental Results and Discussion,[0],[0]
"For IL, major errors come from: (1) requiring additional information (35%), and (2) not knowing PartWhole relation (17%).",4 Experimental Results and Discussion,[0],[0]
"Table 4 shows a few examples for different STI error types.
",4 Experimental Results and Discussion,[0],[0]
The left-half of Table 5 shows the performances on the OSS and NDS datasets.,4 Experimental Results and Discussion,[0],[0]
"Recall that OSS is created by selecting some MWPs which both Illinois Math Solver (Roy and Roth, 2016) and our system16 can correctly solve.",4 Experimental Results and Discussion,[0],[0]
"Therefore, both systems have 100% accuracy in solving the OSS dataset.",4 Experimental Results and Discussion,[0],[0]
"However, these two systems behave very differently while solving the noisy dataset.",4 Experimental Results and Discussion,[0],[0]
The much higher accuracy of our system on the noisy dataset shows that our meaning-based approach understands the meaning of each quantity more.,4 Experimental Results and Discussion,[0],[0]
"Therefore, it is less confused17 with the irrelevant quantities.
",4 Experimental Results and Discussion,[0],[0]
One MWP in the noisy dataset that confuses Illinois Math Solver (IMS) is “Tom has 9 yellow balloons.,4 Experimental Results and Discussion,[0],[0]
Sara has 8 yellow balloons.,4 Experimental Results and Discussion,[0],[0]
Bob has 5 yellow flowers.,4 Experimental Results and Discussion,[0],[0]
How many yellow balloons do 15 https://github.com/CogComp/arithmetic. 16,4 Experimental Results and Discussion,[0],[0]
"In evaluating the performances on OSS and NDS datasets, our system is trained on the folds 2-5 of the IL dataset.",4 Experimental Results and Discussion,[0],[0]
"17 Since the gap between two different types of approaches is quite big, those 396 examples on OSS and 196 examples on NDS are sufficient to confirm the conclusion.
",4 Experimental Results and Discussion,[0],[0]
"they have in total?”, where the underlined sentence is the added noisy sentence.",4 Experimental Results and Discussion,[0],[0]
"The solver sums all quantities and gives the wrong answer 22, which reveals that IMS cannot understand that the quantity “5 yellow flowers” is irrelevant to the question “How many yellow balloons?”.",4 Experimental Results and Discussion,[0],[0]
"On the contrary, our system avoids this mistake.
",4 Experimental Results and Discussion,[0],[0]
"Although the meaning of each quantity is explicitly checked in our LFT module, our system still cannot correctly solve all MWPs in NDS.",4 Experimental Results and Discussion,[0],[0]
"The error analysis reveals that the top-4 error sources are STI, LFT, CoreNLP and incorrect problem construction (for 27%, 27%, 18%, 18%), which indicates that our STI and LFT still cannot completely prevent the damage caused from the noisy sentences (which implies that more consistency check for quantity meaning should be done).",4 Experimental Results and Discussion,[0],[0]
"The remaining errors are due to incorrect quantity extraction, lacking common-sense or not knowing entailment relationship between two entities.
",4 Experimental Results and Discussion,[0],[0]
A similar experiment is performed to check if the DNN approach will be affected by the noisy information more.,4 Experimental Results and Discussion,[0],[0]
We first select 124 MWPs (denoted as OSS′) from OSS that can be correctly solved by both our statistical and DNN approaches and then filter out 350 derived MWPs (denotes as NDS′) from NDS.,4 Experimental Results and Discussion,[0],[0]
"The right-half of Table 5 shows that the performance of the DNN approach drops more than the statistical approach does in the noisy dataset, which indicates that our statistical approach is less sensitive to the irrelevant quantities and more close to human’s approach.",4 Experimental Results and Discussion,[0],[0]
"To the best of our knowledge, MWP solvers proposed before 2014 all adopted the rule-based approach.",5 Related Work,[0],[0]
Mukherjee and Garain (2008) had given a good survey for all related approaches before 2008.,5 Related Work,[0],[0]
"Afterwards, Ma et al. (2010) proposed a MSWPAS system to simulate human arithmetic multi-step addition and subtraction behavior without evaluation.",5 Related Work,[0],[0]
"Besides, Liguda and Pfeiffer (2012) proposed a model based on augmented semantic networks, and claimed that it could solve multi-step MWPs and complex equation systems and was more robust to irrelevant information (also no evaluation).
",5 Related Work,[0],[0]
"Recently, Hosseini et al. (2014) proposed a Container-Entity based approach, which solved the MWP with a sequence of state transition.",5 Related Work,[0],[0]
"And Kushman et al. (2014) proposed the first statistical approach, which heuristically extracts some algebraic templates from labeled equations, and then aligns them with the given sentence.",5 Related Work,[0],[0]
"Since no semantic analysis is conducted, the performance is quite limited.
",5 Related Work,[0],[0]
"In more recent researches (Roy and Roth, 2015; Koncel-Kedziorski et al., 2015; Roy and Roth, 2017), quantities in an MWP were associated with attributes extracted from their contexts.",5 Related Work,[0],[0]
"Based on the attributes, several statistical classifiers were used to select operands and determine operators to solve multi-step MWPs.",5 Related Work,[0],[0]
"Since the physical meaning of each quantity is not explicitly considered in getting the answer, the reasoning process cannot be explained in a human comprehensible way.",5 Related Work,[0],[0]
"Besides, Shi et al. (2015) attacked the number word problem, which only deal with numbers, with a semantic parser.",5 Related Work,[0],[0]
"Mitra and Baral (2016) mapped MWPs into three types of problems, including Part-Whole, Change and Comparison.",5 Related Work,[0],[0]
Each problem was associated with a generic formula.,5 Related Work,[0],[0]
They used a log-linear model to determine how to instantiate the formula with quantities and solve the only one Unknown variable.,5 Related Work,[0],[0]
They achieved the best performance on the AI2 dataset.,5 Related Work,[0],[0]
"However, their approach cannot handle Multiplication or Division related MWPs.",5 Related Work,[0],[0]
"Recently, DNN-based approaches (Ling et al, 2017; Wang et al, 2017) have emerged.",5 Related Work,[0],[0]
"However, they only attacked algebraic word problems, and required a very large training-set.
",5 Related Work,[0],[0]
"Our proposed approach mainly differs from those previous approaches in combining the statistical framework with logic inference, and also in
adopting the meaning-based statistical approach for selecting the desired operands.",5 Related Work,[0],[0]
"A meaning-based logic form represented with role-tags (e.g., nsubj, verb, etc.) is first proposed to associate the extracted math quantity with its physical meaning, which then can be used to identify the desired operands and filter out irrelevant quantities.",6 Conclusion,[0],[0]
"Afterwards, a statistical framework is proposed to perform understanding and reasoning based on those logic expressions.",6 Conclusion,[0],[0]
"We further compare the performance with a typical DNN approach, the results show the proposed approach is still better.",6 Conclusion,[0],[0]
"We will try to integrate domain concepts into the DNN approach to improve the learning efficiency in the future.
",6 Conclusion,[0],[0]
The main contributions of our work are: (1) Adopting a meaning-based approach to solve English math word problems and showing its superiority over other state-of-the-art systems on common datasets.,6 Conclusion,[0],[0]
(2) Proposing a statistical model to select operands by explicitly checking the meanings of quantities against the meaning of the question sentence.,6 Conclusion,[0],[0]
(3) Designing a noisy dataset to test if a system solves the problems by understanding.,6 Conclusion,[0],[0]
(4) Proposing a perplexity-flavor measure to assess the complexity of a dataset.,6 Conclusion,[0],[0]
"We introduce MeSys, a meaning-based approach, for solving English math word problems (MWPs) via understanding and reasoning in this paper.",abstractText,[0],[0]
"It first analyzes the text, transforms both body and question parts into their corresponding logic forms, and then performs inference on them.",abstractText,[0],[0]
"The associated context of each quantity is represented with proposed role-tags (e.g., nsubj, verb, etc.), which provides the flexibility for annotating an extracted math quantity with its associated context information (i.e., the physical meaning of this quantity).",abstractText,[0],[0]
Statistical models are proposed to select the operator and operands.,abstractText,[0],[0]
A noisy dataset is designed to assess if a solver solves MWPs mainly via understanding or mechanical pattern matching.,abstractText,[0],[0]
"Experimental results show that our approach outperforms existing systems on both benchmark datasets and the noisy dataset, which demonstrates that the proposed approach understands the meaning of each quantity in the text more.",abstractText,[0],[0]
A Meaning-based Statistical English Math Word Problem Solver,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 221–232 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics
Resolving abstract anaphora is an important, but difficult task for text understanding. Yet, with recent advances in representation learning this task becomes a more tangible aim. A central property of abstract anaphora is that it establishes a relation between the anaphor embedded in the anaphoric sentence and its (typically non-nominal) antecedent. We propose a mention-ranking model that learns how abstract anaphors relate to their antecedents with an LSTM-Siamese Net. We overcome the lack of training data by generating artificial anaphoric sentence– antecedent pairs. Our model outperforms state-of-the-art results on shell noun resolution. We also report first benchmark results on an abstract anaphora subset of the ARRAU corpus. This corpus presents a greater challenge due to a mixture of nominal and pronominal anaphors and a greater range of confounders. We found model variants that outperform the baselines for nominal anaphors, without training on individual anaphor data, but still lag behind for pronominal anaphors. Our model selects syntactically plausible candidates and – if disregarding syntax – discriminates candidates using deeper features.",text,[0],[0]
"Current research in anaphora (or coreference) resolution is focused on resolving noun phrases referring to concrete objects or entities in the real
†Leo Born, Juri Opitz and Anette Frank contributed equally to this work.
",1 Introduction,[0],[0]
"world, which is arguably the most frequently occurring type.",1 Introduction,[0],[0]
"Distinct from these are diverse types of abstract anaphora (AA) (Asher, 1993) where reference is made to propositions, facts, events or properties.",1 Introduction,[0],[0]
"An example is given in (1) below.1
While recent approaches address the resolution of selected abstract shell nouns (Kolhatkar and Hirst, 2014), we aim to resolve a wide range of abstract anaphors, such as the NP this trend in (1), as well as pronominal anaphors (this, that, or it).
",1 Introduction,[0],[0]
"Henceforth, we refer to a sentence that contains an abstract anaphor as the anaphoric sentence (AnaphS), and to a constituent that the anaphor refers to as the antecedent (Antec) (cf. (1)).
",1 Introduction,[0],[0]
"(1) Ever-more powerful desktop computers, designed with one or more microprocessors as their ”brains”, are expected to increasingly take on functions carried out by more expensive minicomputers and mainframes. ”",1 Introduction,[0],[0]
"[Antec The guys that make traditional hardware are really being obsoleted by microprocessor-based machines]”, said Mr. Benton.",1 Introduction,[0],[0]
"[AnaphS As a result of this trendAA, longtime powerhouses HP, IBM and Digital Equipment Corp. are scrambling to counterattack with microprocessor-based systems of their own.]
",1 Introduction,[0],[0]
A major obstacle for solving this task is the lack of sufficient amounts of annotated training data.,1 Introduction,[0],[0]
We propose a method to generate large amounts of training instances covering a wide range of abstract anaphor types.,1 Introduction,[0],[0]
"This enables us to use neural methods which have shown great success in related tasks: coreference resolution (Clark and Manning, 2016a), textual entailment (Bowman et al., 2016), learning textual similarity (Mueller and Thyagarajan, 2016), and discourse relation sense classification (Rutherford et al., 2017).
",1 Introduction,[0],[0]
"Our model is inspired by the mention-ranking model for coreference resolution (Wiseman et al., 2015; Clark and Manning, 2015, 2016a,b) and combines it with a Siamese Net (Mueller and Thyagarajan, 2016), (Neculoiu et al., 2016) for
1Example drawn from ARRAU (Uryupina et al., 2016).
",1 Introduction,[0],[0]
"221
learning similarity between sentences.",1 Introduction,[0],[0]
"Given an anaphoric sentence (AntecS in (1)) and a candidate antecedent (any constituent in a given context, e.g. being obsoleted by microprocessor-based machines in (1)), the LSTM-Siamese Net learns representations for the candidate and the anaphoric sentence in a shared space.",1 Introduction,[0],[0]
These representations are combined into a joint representation used to calculate a score that characterizes the relation between them.,1 Introduction,[0],[0]
The learned score is used to select the highest-scoring antecedent candidate for the given anaphoric sentence and hence its anaphor.,1 Introduction,[0],[0]
We consider one anaphor at a time and provide the embedding of the context of the anaphor and the embedding of the head of the anaphoric phrase to the input to characterize each individual anaphor – similar to the encoding proposed by Zhou and Xu (2015) for individuating multiply occurring predicates in SRL.,1 Introduction,[0],[0]
With deeper inspection we show that the model learns a relation between the anaphor in the anaphoric sentence and its antecedent.,1 Introduction,[0],[0]
"Fig. 1 displays our architecture.
",1 Introduction,[0],[0]
"In contrast to other work, our method for generating training data is not confined to specific types of anaphora such as shell nouns (Kolhatkar and Hirst, 2014) or anaphoric connectives (Stede and Grishina, 2016).",1 Introduction,[0],[0]
It produces large amounts of instances and is easily adaptable to other languages.,1 Introduction,[0],[0]
"This enables us to build a robust, knowledge-lean model for abstract anaphora resolution that easily extends to multiple languages.
",1 Introduction,[0],[0]
We evaluate our model on the shell noun resolution dataset of Kolhatkar et al. (2013b) and show that it outperforms their state-of-the-art results.,1 Introduction,[0],[0]
"Moreover, we report results of the model (trained on our newly constructed dataset) on unrestricted abstract anaphora instances from the ARRAU corpus (Poesio and Artstein, 2008; Uryupina et al., 2016).",1 Introduction,[0],[0]
"To our knowledge this provides the first state-of-the-art benchmark on this data subset.
",1 Introduction,[0],[0]
Our TensorFlow2 implementation of the model and scripts for data extraction are available at: https://github.com/amarasovic/ neural-abstract-anaphora.,1 Introduction,[0],[0]
"Abstract anaphora has been extensively studied in linguistics and shown to exhibit specific properties in terms of semantic antecedent types, their degrees of abstractness, and general dis-
2Abadi et al. (2015)
course properties (Asher, 1993; Webber, 1991).",2 Related and prior work,[0],[0]
"In contrast to nominal anaphora, abstract anaphora is difficult to resolve, given that agreement and lexical match features are not applicable.",2 Related and prior work,[0],[0]
"Annotation of abstract anaphora is also difficult for humans (Dipper and Zinsmeister, 2012), and thus, only few smaller-scale corpora have been constructed.",2 Related and prior work,[0],[0]
"We evaluate our models on a subset of the ARRAU corpus (Uryupina et al., 2016) that contains abstract anaphors and the shell noun corpus used in Kolhatkar et al. (2013b).3 We are not aware of other freely available abstract anaphora datasets.
",2 Related and prior work,[0],[0]
Little work exists for the automatic resolution of abstract anaphora.,2 Related and prior work,[0],[0]
"Early work (Eckert and Strube, 2000; Strube and Müller, 2003; Byron, 2004; Müller, 2008) has focused on spoken language, which exhibits specific properties.",2 Related and prior work,[0],[0]
"Recently, event coreference has been addressed using feature-based classifiers (Jauhar et al., 2015; Lu and Ng, 2016).",2 Related and prior work,[0],[0]
"Event coreference is restricted to a subclass of events, and usually focuses on coreference between verb (phrase) and noun (phrase) mentions of similar abstractness levels (e.g. purchase – acquire) with no special focus on (pro)nominal anaphora.",2 Related and prior work,[0],[0]
"Abstract anaphora typically involves a full-fledged clausal antecedent that is referred to by a highly abstract (pro)nominal anaphor, as in (1).
",2 Related and prior work,[0],[0]
Rajagopal et al. (2016) proposed a model for resolution of events in biomedical text that refer to a single or multiple clauses.,2 Related and prior work,[0],[0]
"However, instead of selecting the correct antecedent clause(s) (our task) for a given event, their model is restricted to classifying the event into six abstract categories: this these changes, responses, analysis, context, finding, observation, based on its surrounding context.",2 Related and prior work,[0],[0]
"While related, their task is not comparable to the full-fledged abstract anaphora resolution task, since the events to be classified are known to be coreferent and chosen from a set of restricted abstract types.
",2 Related and prior work,[0],[0]
More related to our work is Anand and Hardt (2016) who present an antecedent ranking account for sluicing using classical machine learning based on a small training dataset.,2 Related and prior work,[0],[0]
"They employ features modeling distance, containment, discourse structure, and – less effectively – content and lexical correlates.4
Closest to our work is Kolhatkar et al. (2013b)
",2 Related and prior work,[0],[0]
3We thank the authors for making their data available.,2 Related and prior work,[0],[0]
"4Their data set was not publicized.
",2 Related and prior work,[0],[0]
"(KZH13) and Kolhatkar and Hirst (2014) (KH14) on shell noun resolution, using classical machine learning techniques.",2 Related and prior work,[0],[0]
"Shell nouns are abstract nouns, such as fact, possibility, or issue, which can only be interpreted jointly with their shell content (their embedded clause as in (2) or antecedent as in (3)).",2 Related and prior work,[0],[0]
KZH13 refer to shell nouns whose antecedent occurs in the prior discourse as anaphoric shell nouns (ASNs),2 Related and prior work,[0],[0]
"(cf. (3)), and cataphoric shell nouns (CSNs) otherwise (cf. (2)).5
(2) Congress has focused almost solely on the fact that [special education is expensive - and that it takes away money from regular education.",2 Related and prior work,[0],[0]
"]
(3) Environmental Defense",2 Related and prior work,[0],[0]
[...] notes that [Antec Mowing the lawn with a gas mower produces as much pollution [...] as driving a car 172 miles.],2 Related and prior work,[0],[0]
"[AnaphS This fact may [...] explain the recent surge in the sales of [...] old-fashioned push mowers [...]].
KZH13 presented an approach for resolving six typical shell nouns following the observation that CSNs are easy to resolve based on their syntactic structure alone, and the assumption that ASNs share linguistic properties with their embedded (CSN) counterparts.",2 Related and prior work,[0],[0]
"They manually developed rules to identify the embedded clause (i.e. cataphoric antecedent) of CSNs and trained SVMrank (Joachims, 2002) on such instances.",2 Related and prior work,[0],[0]
The trained SVMrank model is then used to resolve ASNs.,2 Related and prior work,[0],[0]
"KH14 generalized their method to be able to create training data for any given shell noun, however, their method heavily exploits the specific properties of shell nouns and does not apply to other types of abstract anaphora.
",2 Related and prior work,[0],[0]
Stede and Grishina (2016) study a related phenomenon for German.,2 Related and prior work,[0],[0]
They examine inherently anaphoric connectives (such as demzufolge – according to which) that could be used to access their abstract antecedent in the immediate context.,2 Related and prior work,[0],[0]
"Yet, such connectives are restricted in type, and the study shows that such connectives are often ambiguous with nominal anaphors and require sense disambiguation.",2 Related and prior work,[0],[0]
"We conclude that they cannot be easily used to acquire antecedents automatically.
",2 Related and prior work,[0],[0]
"In our work, we explore a different direction: we construct artificial training data using a general pattern that identifies embedded sentence constituents, which allows us to extract relatively secure training data for abstract anaphora that captures a wide range of anaphora-antecedent rela-
5We follow this terminology for their approach and data representation.
tions, and apply this data to train a model for the resolution of unconstrained abstract anaphora.
",2 Related and prior work,[0],[0]
Recent work in entity coreference resolution has proposed powerful neural network-based models that we will adapt to the task of abstract anaphora resolution.,2 Related and prior work,[0],[0]
"Most relevant for our task is the mention-ranking neural coreference model proposed in Clark and Manning (2015), and their improved model in Clark and Manning (2016a), which integrates a loss function (Wiseman et al., 2015) which learns distinct feature representations for anaphoricity detection and antecedent ranking.
",2 Related and prior work,[0],[0]
Siamese Nets distinguish between similar and dissimilar pairs of samples by optimizing a loss over the metric induced by the representations.,2 Related and prior work,[0],[0]
"It is widely used in vision (Chopra et al., 2005), and in NLP for semantic similarity, entailment, query normalization and QA (Mueller and Thyagarajan, 2016; Neculoiu et al., 2016; Das et al., 2016).",2 Related and prior work,[0],[0]
"Given an anaphoric sentence s with a marked anaphor (mention) and a candidate antecedent c, the mention-ranking (MR) model assigns the pair (c, s) a score, using representations produced by an LSTM-Siamese Net.",3 Mention-Ranking Model,[0],[0]
The highest-scoring candidate is assigned to the marked anaphor in the anaphoric sentence.,3 Mention-Ranking Model,[0],[0]
"Fig. 1 displays the model.
",3 Mention-Ranking Model,[0],[0]
"We learn representations of an anaphoric sentence s and a candidate antecedent c using a bidirectional Long Short-Term Memory (Hochreiter and Schmidhuber, 1997; Graves and Schmidhuber, 2005).",3 Mention-Ranking Model,[0],[0]
"One bi-LSTM is applied to the anaphoric sentence s and a candidate antecedent c, hence the term siamese.",3 Mention-Ranking Model,[0],[0]
"Each word is represented with a vector wi constructed by concatenating embeddings of the word, of the context of the anaphor (average of embeddings of the anaphoric phrase, the previous and the next word), of the head of the anaphoric phrase6, and, finally, an embedding of the constituent tag of the candidate, or the S constituent tag if the word is in the anaphoric sentence.",3 Mention-Ranking Model,[0],[0]
"For each sequence s or c, the word vectors wi are sequentially fed into the bi-LSTM, which produces outputs from the forward pass, −→ hi, and outputs ←−",3 Mention-Ranking Model,[0],[0]
hi from the backward pass.,3 Mention-Ranking Model,[0],[0]
The final output of the i-th word is defined as hi =,3 Mention-Ranking Model,[0],[0]
[ ←− hi ; −→ hi ].,3 Mention-Ranking Model,[0],[0]
"To get a representation of the full sequence, hs or hc, all outputs are averaged, except for those that correspond to padding tokens.
",3 Mention-Ranking Model,[0],[0]
"6Henceforth we refer to it as embedding of the anaphor.
",3 Mention-Ranking Model,[0],[0]
"To prevent forgetting the constituent tag of the sequence, we concatenate the corresponding tag embedding with hs or hc (we call this a shortcut for the tag information).",3 Mention-Ranking Model,[0],[0]
"The resulting vector is fed into a feed-forward layer of exponential linear units (ELUs) (Clevert et al., 2016) to produce the final representation h̃s or h̃c of the sequence.
",3 Mention-Ranking Model,[0],[0]
"From h̃c and h̃s we compute a vector hc,s =",3 Mention-Ranking Model,[0],[0]
[|h̃c − h̃s|; h̃c h̃s],3 Mention-Ranking Model,[0],[0]
"(Tai et al., 2015), where |–| denotes the absolute values of the element-wise subtraction, and the element-wise multiplication.",3 Mention-Ranking Model,[0],[0]
"Then hc,s is fed into a feed-forward layer of ELUs to obtain the final joint representation, h̃c,s, of the pair (c, s).",3 Mention-Ranking Model,[0],[0]
"Finally, we compute the score for the pair (c, s) that represents relatedness between them, by applying a single fully connected linear layer to the joint representation:
score(c, s) = W h̃c,s + b ∈ R, (1) where W is a 1 × d weight matrix, and d the dimension of the vector h̃c,s.
We train the described mention-ranking model with the max-margin training objective from Wiseman et al. (2015), used for the antecedent ranking subtask.",3 Mention-Ranking Model,[0],[0]
"Suppose that the training set D = {(ai, si, T (ai),N (ai)}ni=1, where ai is the i-th abstract anaphor, si the corresponding anaphoric sentence, T (ai) the set of antecedents of ai and N (ai) the set of candidates that are not antecedents (negative candidates).",3 Mention-Ranking Model,[0],[0]
"Let t̃i = arg maxt∈T (ai) score(ti, si) be the highest scor-
ing antecedent of ai.",3 Mention-Ranking Model,[0],[0]
"Then the loss is given by n∑ i=1 max(0, max c∈N (ai) {1+score(c, si)−score(t̃i, si)}).",3 Mention-Ranking Model,[0],[0]
"We create large-scale training data for abstract anaphora resolution by exploiting a common construction, consisting of a verb with an embedded sentence (complement or adverbial) (cf. Fig. 2).",4 Training data construction,[0],[0]
"We detect this pattern in a parsed corpus, ’cut off’ the S′ constituent and replace it with a suitable anaphor to create the anaphoric sentence (AnaphS), while S yields the antecedent (Antec).",4 Training data construction,[0],[0]
"This method covers a wide range of anaphoraantecedent constellations, due to diverse semantic or discourse relations that hold between the clause hosting the verb and the embedded sentence.
",4 Training data construction,[0],[0]
"First, the pattern applies to verbs that embed sentential arguments.",4 Training data construction,[0],[0]
"In (4), the verb doubt establishes a specific semantic relation between the embedding sentence and its sentential complement.
",4 Training data construction,[0],[0]
"(4) He doubts [S′ [S a Bismarckian super state will emerge that would dominate Europe], but warns of ”a risk of profound change in the [..] European Community from a Germany that is too strong, even if democratic”].
",4 Training data construction,[0],[0]
"From this we extract the artificial antecedent A Bismarckian super state will emerge that would dominate Europe, and its corresponding anaphoric sentence He doubts this, but warns of ”a risk of profound change ... even if democratic”, which we construct by randomly choosing one of a predefined set of appropriate anaphors (here: this, that, it),",4 Training data construction,[0],[0]
cf. Table 1.,4 Training data construction,[0],[0]
"The second row in Table 1 is used when the head of S′ is filled by an overt complementizer (doubts that), as opposed to (4).",4 Training data construction,[0],[0]
"The remaining rows in Table 1 apply to adverbial clauses of different types.
",4 Training data construction,[0],[0]
"Adverbial clauses encode specific discourse relations with their embedding sentences, often indicated by their conjunctions.",4 Training data construction,[0],[0]
"In (5), for example, the causal conjunction as relates a cause (embedded sentence) and its effect (embedding sentence):
(5) There is speculation that property casualty firms will sell even more munis",4 Training data construction,[0],[0]
[S′ as [S they scramble to raise cash to pay claims related to Hurricane Hugo,4 Training data construction,[0],[0]
[..],4 Training data construction,[0],[0]
"]].
We randomly replace causal conjunctions because, as with appropriately adjusted anaphors, e.g. because of that, due to this or therefore that make the causal relation explicit in the anaphor.7
Compared to the shell noun corpus of KZH13, who made use of a carefully constructed set of extraction patterns, a downside of our method is that our artificially created antecedents are uniformly of type",4 Training data construction,[0],[0]
"S. However, the majority of abstract anaphora antecedents found in the existing datasets are of type S. Also, our models are intended to induce semantic representations, and so we expect syntactic form to be less critical, compared to a feature-based model.8",4 Training data construction,[0],[0]
"Finally, the general extraction pattern in Fig. 2, covers a much wider range of anaphoric types.
",4 Training data construction,[0],[0]
"Using this method we generated a dataset of artificial anaphoric sentence–antecedent pairs from the WSJ part of the PTB Corpus (Marcus et al., 1993), automatically parsed using the Stanford Parser (Klein and Manning, 2003).",4 Training data construction,[0],[0]
"We evaluate our model on two types of anaphora: (a) shell noun anaphora and (b) (pro)nominal abstract anaphors extracted from ARRAU.
",5.1 Datasets,[0],[0]
a. Shell noun resolution dataset.,5.1 Datasets,[0],[0]
"For comparability we train and evaluate our model for shell noun resolution, using the original training (CSN) and test (ASN) corpus of Kolhatkar et al. (2013a,b).9
7In case of ambiguous conjunctions (e.g. as interpreted as causal or temporal), we generally choose the most frequent interpretation.
",5.1 Datasets,[0],[0]
"8This also alleviates problems with languages like German, where (non-)embedded sentences differ in surface position of the finite verb.",5.1 Datasets,[0],[0]
"We can either adapt the order or ignore it, when producing anaphoric sentence – antecedent pairs.
",5.1 Datasets,[0],[0]
"9We thank the authors for providing the available data.
",5.1 Datasets,[0],[0]
"We follow the data preparation and evaluation protocol of Kolhatkar et al. (2013b) (KZH13).
",5.1 Datasets,[0],[0]
The CSN corpus was constructed from the NYT corpus using manually developed patterns to identify the antecedent of cataphoric shell nouns (CSNs).,5.1 Datasets,[0],[0]
"In KZH13, all syntactic constituents of the sentence that contains both the CSN and its antecedent were considered as candidates for training a ranking model.",5.1 Datasets,[0],[0]
Candidates that differ from the antecedent in only one word or one word and punctuation were as well considered as antecedents10.,5.1 Datasets,[0],[0]
To all other candidates we refer to as negative candidates.,5.1 Datasets,[0],[0]
"For every shell noun, KZH13 used the corresponding part of the CSN data to train SVMrank.
",5.1 Datasets,[0],[0]
The ASN corpus serves as the test corpus.,5.1 Datasets,[0],[0]
"It was also constructed from the NYT corpus, by selecting anaphoric instances with the pattern ”this 〈shell noun〉” for all covered shell nouns.",5.1 Datasets,[0],[0]
"For validation, Kolhatkar et al. (2013a) crowdsourced annotations for the sentence which contains the antecedent, which KZH13 refer to as a broad region.",5.1 Datasets,[0],[0]
Candidates for the antecedent were obtained by using all syntactic constituents of the broad region as candidates and ranking them using the SVMrank model trained on the CSN corpus.,5.1 Datasets,[0],[0]
The top 10 ranked candidates were presented to the crowd workers and they chose the best answer that represents the ASN antecedent.,5.1 Datasets,[0],[0]
The workers were encouraged to select None when they did not agree with any of the displayed answers and could provide information about how satisfied they were with the displayed candidates.,5.1 Datasets,[0],[0]
"We consider this dataset as gold, as do KZH13, although it may be biased towards the offered candidates.11
b. Abstract anaphora resolution data set.",5.1 Datasets,[0],[0]
"We use the automatically constructed data from the WSJ corpus (Section 4) for training.12 Our test data for unrestricted abstract anaphora resolution is obtained from the ARRAU corpus (Uryupina et al., 2016).",5.1 Datasets,[0],[0]
"We extracted all abstract anaphoric instances from the WSJ part of ARRAU that are marked with the category abstract or plan,13 and call the subcorpus ARRAU-AA.
10We obtained this information from the authors directly.",5.1 Datasets,[0],[0]
"11The authors provided us with the workers’ annotations of the broad region, antecedents chosen by the workers and links to the NYT corpus.",5.1 Datasets,[0],[0]
"The extraction of the anaphoric sentence and the candidates had to be redone.
",5.1 Datasets,[0],[0]
12We excluded any documents that are part of ARRAU.,5.1 Datasets,[0],[0]
"13ARRAU distinguishes abstract anaphors and (mostly)
pronominal anaphors referring to an action or plan, as plan.
",5.1 Datasets,[0],[0]
Candidates extraction.,5.1 Datasets,[0],[0]
"Following KZH13, for every anaphor we create a list of candidates by extracting all syntactic constituents from sentences which contain antecedents.",5.1 Datasets,[0],[0]
"Candidates that differ from antecedents in only one word, or one word and punctuation, were as well considered as antecedents.",5.1 Datasets,[0],[0]
"Constituents that are not antecedents are considered as negative candidates.
",5.1 Datasets,[0],[0]
Data statistics.,5.1 Datasets,[0],[0]
"Table 2 gives statistics of the datasets: the number of anaphors (row 1), the median length (in tokens) of antecedents (row 2), the median length (in tokens) for all anaphoric sentences (row 3), the median of the number of antecedents and candidates that are not antecedents (negatives) (rows 4–5), the number of pronominal and nominal anaphors (rows 6–7).",5.1 Datasets,[0],[0]
"Both training sets, artificial and CSN, have only one possible antecedent for which we accept two minimal variants differing in only one word or one word and punctuation.",5.1 Datasets,[0],[0]
"On the contrary, both test sets by design allow annotation of more than one antecedent that differ in more than one word.",5.1 Datasets,[0],[0]
"Every anaphor in the artificial training dataset is pronominal, whereas anaphors in CSN and ASN are nominal only.",5.1 Datasets,[0],[0]
"ARRAU-AA has a mixture of nominal and pronominal anaphors.
",5.1 Datasets,[0],[0]
Data pre-,5.1 Datasets,[0],[0]
processing.,5.1 Datasets,[0],[0]
Other details can be found in Supplementary Materials.,5.1 Datasets,[0],[0]
"Following KZH13, we report success@n (s@n), which measures whether the antecedent, or a candidate that differs in one word14, is in the first n ranked candidates, for n ∈ {1, 2, 3, 4}.",5.2 Baselines and evaluation metrics,[0],[0]
"Additionally, we report the preceding sentence baseline
14We obtained this information in personal communication with one of the authors.
(PSBL) that chooses the previous sentence for the antecedent and TAGbaseline (TAGBL) that randomly chooses a candidate with the constituent tag label in {S, VP, ROOT, SBAR}.",5.2 Baselines and evaluation metrics,[0],[0]
For TAGBL we report the average of 10 runs with 10 fixed seeds.,5.2 Baselines and evaluation metrics,[0],[0]
"PSBL always performs worse than the KZH13 model on the ASN, so we report it only for ARRAU-AA.",5.2 Baselines and evaluation metrics,[0],[0]
Hyperparameters tuning.,5.3 Training details for our models,[0],[0]
"We recorded performance with manually chosen HPs and then tuned HPs with Tree-structured Parzen Estimators (TPE) (Bergstra et al., 2011)15.",5.3 Training details for our models,[0],[0]
TPE chooses HPs for the next (out of 10) trails on the basis of the s@1 score on the devset.,5.3 Training details for our models,[0],[0]
As devsets we employ the ARRAUAA corpus for shell noun resolution and the ASN corpus for unrestricted abstract anaphora resolution.,5.3 Training details for our models,[0],[0]
For each trial we record performance on the test set.,5.3 Training details for our models,[0],[0]
We report the best test s@1 score in 10 trials if it is better than the scores from default HPs.,5.3 Training details for our models,[0],[0]
The default HPs and prior distributions for HPs used by TPE are given below.,5.3 Training details for our models,[0],[0]
"The (exact) HPs we used can be found in Supplementary Materials.
",5.3 Training details for our models,[0],[0]
Input representation.,5.3 Training details for our models,[0],[0]
"To construct word vectors wi as defined in Section 3, we used 100-dim.",5.3 Training details for our models,[0],[0]
"GloVe word embeddings pre-trained on the Gigaword and Wikipedia (Pennington et al., 2014), and did not fine-tune them.",5.3 Training details for our models,[0],[0]
"Vocabulary was built from the words in the training data with frequency in {3, U(1, 10)}, and OOV words were replaced with an UNK token.",5.3 Training details for our models,[0],[0]
"Embeddings for tags are initialized with values drawn from the uniform distribution U(− 1√
d+t , 1√ d+t
) , where t is the number of
tags16 and d ∈ {50, qlog-U(30, 100)} the size of the tag embeddings.17 We experimented with removing embeddings for tag, anaphor and context.
",5.3 Training details for our models,[0],[0]
Weights initialization.,5.3 Training details for our models,[0],[0]
"The size of the LSTMs hidden states was set to {100, qlog-U(30, 150)}.",5.3 Training details for our models,[0],[0]
"We initialized the weight matrices of the LSTMs with random orthogonal matrices (Henaff et al., 2016), all other weight matrices with the initialization proposed in He et al. (2015).",5.3 Training details for our models,[0],[0]
"The first feed-forward layer size is set to a value in {400, qlog-U(200, 800)}, the second to a value in {1024, qlog-U(400, 2000)}.",5.3 Training details for our models,[0],[0]
"Forget biases in the LSTM were initialized with 1s (Józefowicz et al., 2015), all other biases with 0s.
15https://github.com/hyperopt/hyperopt.",5.3 Training details for our models,[0],[0]
16We used a list of tags obtained from the Stanford Parser.,5.3 Training details for our models,[0],[0]
"17qlog-U is the so-called qlog-uniform distribution.
",5.3 Training details for our models,[0],[0]
Optimization.,5.3 Training details for our models,[0],[0]
"We trained our model in minibatches using Adam (Kingma and Ba, 2015) with the learning rate of 10−4 and maximal batch size 64.",5.3 Training details for our models,[0],[0]
"We clip gradients by global norm (Pascanu et al., 2013), with a clipping value in {1.0, U(1, 100)}.",5.3 Training details for our models,[0],[0]
"We train for 10 epochs and choose the model that performs best on the devset.
Regularization.",5.3 Training details for our models,[0],[0]
"We used the l2-regularization with λ ∈ {10−5, log-U(10−7, 10−2)}.",5.3 Training details for our models,[0],[0]
"Dropout (Srivastava et al., 2014) with a keep probability kp ∈ {0.8, U(0.5, 1.0)} was applied to the outputs of the LSTMs, both feed-forward layers and optionally to the input with kp ∈ U(0.8, 1.0).",5.3 Training details for our models,[0],[0]
Table 3 provides the results of the mentionranking model (MR-LSTM) on the ASN corpus using default HPs.,6.1 Results on shell noun resolution dataset,[0],[0]
"Column 2 states which model produced the results: KZH13 refers to the best reported results in Kolhatkar et al. (2013b) and TAGBL is the baseline described in Section 5.2.
",6.1 Results on shell noun resolution dataset,[0],[0]
"In terms of s@1 score, MR-LSTM outperforms both KZH13’s results and TAGBL without even necessitating HP tuning.",6.1 Results on shell noun resolution dataset,[0],[0]
"For the outlier reason we tuned HPs (on ARRAU-AA) for different variants of the architecture: the full architecture, without embedding of the context of the anaphor (ctx), of the anaphor (aa), of both constituent tag em-
bedding and shortcut (tag,cut), dropping only the shortcut (cut), using only word embeddings as input (ctx,aa,tag,cut), without the first (ffl1) and second (ffl2) layer.",6.1 Results on shell noun resolution dataset,[0],[0]
"From Table 4 we observe: (1) with HPs tuned on ARRAU-AA, we obtain results well beyond KZH13, (2) all ablated model variants perform worse than the full model, (3) a large performance drop when omitting syntactic information (tag,cut) suggests that the model makes good use of it.",6.1 Results on shell noun resolution dataset,[0],[0]
"However, this could also be due to a bias in the tag distribution, given that all candidates stem from the single sentence that contains antecedents.",6.1 Results on shell noun resolution dataset,[0],[0]
"The median occurrence of the S tag among both antecedents and negative candidates is 1, thus the model could achieve 50.00 s@1 by picking S-type constituents, just as TAGBL achieves 42.02 for reason and 48.66 for possibility.
",6.1 Results on shell noun resolution dataset,[0],[0]
Tuning of HPs gives us insight into how different model variants cope with the task.,6.1 Results on shell noun resolution dataset,[0],[0]
"For example, without tuning the model with and without syntactic information achieves 71.27 and 19.68 (not shown in table) s@1 score, respectively, and with tuning: 87.78 and 68.10.",6.1 Results on shell noun resolution dataset,[0],[0]
"Performance of 68.10 s@1 score indicates that the model is able to learn without syntactic guidance, contrary to the 19.68 s@1 score before tuning.",6.1 Results on shell noun resolution dataset,[0],[0]
"Table 5 shows the performance of different variants of the MR-LSTM with HPs tuned on the ASN corpus (always better than the default HPs), when evaluated on 3 different subparts of the ARRAUAA: all 600 abstract anaphors, 397 nominal and 203 pronominal ones.",6.2 Results on the ARRAU corpus,[0],[0]
"HPs were tuned on the ASN corpus for every variant separately, without shuffling of the training data.",6.2 Results on the ARRAU corpus,[0],[0]
"For the best performing variant, without syntactic information (tag,cut), we report the results with HPs that yielded the best s@1 test score for all anaphors (row 4), when training with those HPs on shuffled training data (row 5), and with HPs that yielded the best s@1
score for pronominal anaphors (row 6).",6.2 Results on the ARRAU corpus,[0],[0]
"The MR-LSTM is more successful in resolving nominal than pronominal anaphors, although the training data provides only pronominal ones.",6.2 Results on the ARRAU corpus,[0],[0]
"This indicates that resolving pronominal abstract anaphora is harder compared to nominal abstract anaphora, such as shell nouns.",6.2 Results on the ARRAU corpus,[0],[0]
"Moreover, for shell noun resolution in KZH13’s dataset, the MR-LSTM achieved s@1 scores in the range 76.09–93.14, while the best variant of the model achieves 51.89 s@1 score for nominal anaphors in ARRAU-AA.",6.2 Results on the ARRAU corpus,[0],[0]
"Although lower performance is expected, since we do not have specific training data for individual nominals in ARRAU-AA, we suspect that the reason for better performance for shell noun resolution in KZH13 is due to a larger number of positive candidates in ASN (cf. Table 2, rows: antecedents/negatives).
",6.2 Results on the ARRAU corpus,[0],[0]
We also note that HPs that yield good performance for resolving nominal anaphors are not necessarily good for pronominal ones (cf. rows 4–6 in Table 5).,6.2 Results on the ARRAU corpus,[0],[0]
"Since the TPE tuner was tuned on the nominal-only ASN data, this suggest that it would be better to tune HPs for pronominal anaphors on a different dataset or stripping the nouns in ASN.
",6.2 Results on the ARRAU corpus,[0],[0]
"Contrary to shell noun resolution, omitting syntactic information boosts performance in ARRAUAA.",6.2 Results on the ARRAU corpus,[0],[0]
"We conclude that when the model is provided with syntactic information, it learns to pick S-type candidates, but does not continue to learn deeper features to further distinguish them or needs more data to do so.",6.2 Results on the ARRAU corpus,[0],[0]
"Thus, the model is not able to point to exactly one antecedent, resulting in a lower s@1 score, but does well in picking a few good candidates, which yields good s@2-4 scores.",6.2 Results on the ARRAU corpus,[0],[0]
"This is what we can observe from row 2 vs. row 6 in Table 5: the MR-LSTM without context embedding
(ctx) achieves a comparable s@2 score with the variant that omits syntactic information, but better s@3-4 scores.",6.2 Results on the ARRAU corpus,[0],[0]
"Further, median occurrence of tags not in {S, VP, ROOT, SBAR} among top-4 ranked candidates is 0 for the full architecture, and 1 when syntactic information is omitted.",6.2 Results on the ARRAU corpus,[0],[0]
"The need for discriminating capacity of the model is more emphasized in ARRAU-AA, given that the median occurrence of S-type candidates among negatives is 2 for nominal and even 3 for pronominal anaphors, whereas it is 1 for ASN.",6.2 Results on the ARRAU corpus,[0],[0]
"This is in line with the lower TAGBL in ARRAU-AA.
",6.2 Results on the ARRAU corpus,[0],[0]
"Finally, not all parts of the architecture contribute to system performance, contrary to what is observed for reason.",6.2 Results on the ARRAU corpus,[0],[0]
"For nominal anaphors, the anaphor (aa) and feed-forward layers (ffl1, ffl2) are beneficial, for pronominals only the second ffl.",6.2 Results on the ARRAU corpus,[0],[0]
"We finally analyze deeper aspects of the model: (1) whether a learned representation between the anaphoric sentence and an antecedent establishes a relation between a specific anaphor we want to resolve and the antecedent and (2) whether the maxmargin objective enforces a separation of the joint representations in the shared space.
",6.3 Exploring the model,[0],[0]
(1) We claim that by providing embeddings of both the anaphor and the sentence containing the anaphor we ensure that the learned relation between antecedent and anaphoric sentence is dependent on the anaphor under consideration.,6.3 Exploring the model,[0],[0]
Fig. 3 illustrates the heatmap for an anaphoric sentence with two anaphors.,6.3 Exploring the model,[0],[0]
The i-th column of the heatmap corresponds to absolute differences between the output of the bi-LSTM for the i-th word in the anaphoric sentence when the first vs. second anaphor is resolved.,6.3 Exploring the model,[0],[0]
"Stronger color indi-
cates larger difference, the blue rectangle represents the column for the head of the first anaphor, the dashed blue rectangle the column for the head of the second anaphor.",6.3 Exploring the model,[0],[0]
"Clearly, the representations differ when the first vs. second anaphor is being resolved and consequently, joint representations with an antecedent will differ too.
",6.3 Exploring the model,[0],[0]
(2) It is known that the max-margin objective separates the best-scoring positive candidate from the best-scoring negative candidate.,6.3 Exploring the model,[0],[0]
"To investigate what the objective accomplishes in the MRLSTM model, we analyze the joint representations of candidates and the anaphoric sentence (i.e., outputs of ffl2) after training.",6.3 Exploring the model,[0],[0]
"For a randomly chosen instance from ARRAU-AA, we plotted outputs of ffl2 with the tSNE algorithm (v.d. Maaten and Hinton, 2008).",6.3 Exploring the model,[0],[0]
Fig. 4 illustrates that the joint representation of the first ranked candidate and the anaphoric sentence is clearly separated from other joint representations.,6.3 Exploring the model,[0],[0]
This shows that the maxmargin objective separates the best scoring positive candidate from the best scoring negative candidate by separating their respective joint representations with the anaphoric sentence.,6.3 Exploring the model,[0],[0]
"We presented a neural mention-ranking model for the resolution of unconstrained abstract anaphora, and applied it to two datasets with different types of abstract anaphora: the shell noun dataset and a subpart of ARRAU with (pro)nominal abstract anaphora of any type.",7 Conclusions,[0],[0]
To our knowledge this work is the first to address the unrestricted abstract anaphora resolution task with a neural network.,7 Conclusions,[0],[0]
"Our model also outperforms state-of-the-art results on the shell noun dataset.
",7 Conclusions,[0],[0]
"In this work we explored the use of purely artificially created training data and how far it can bring
us.",7 Conclusions,[0],[0]
"In future work, we plan to investigate mixtures of (more) artificial and natural data from different sources (e.g. ASN, CSN).
",7 Conclusions,[0],[0]
"On the more challenging ARRAU-AA, we found model variants that surpass the baselines for the entire and the nominal part of ARRAU-AA, although we do not train models on individual (nominal) anaphor training data like the related work for shell noun resolution.",7 Conclusions,[0],[0]
"However, our model still lags behind for pronominal anaphors.",7 Conclusions,[0],[0]
"Our results suggest that models for nominal and pronominal anaphors should be learned independently, starting with tuning of HPs on a more suitable devset for pronominal anaphors.
",7 Conclusions,[0],[0]
"We show that the model can exploit syntactic information to select plausible candidates, but that when it does so, it does not learn how to distinguish candidates of equal syntactic type.",7 Conclusions,[0],[0]
"By contrast, if the model is not provided with syntactic information, it learns deeper features that enable it to pick the correct antecedent without narrowing down the choice of candidates.",7 Conclusions,[0],[0]
"Thus, in order to improve performance, the model should be enforced to first select reasonable candidates and then continue to learn features to distinguish them, using a larger training set that is easy to provide.
",7 Conclusions,[0],[0]
"In future work we will design such a model, and offer it candidates chosen not only from sentences containing the antecedent, but the larger context.",7 Conclusions,[0],[0]
This work has been supported by the German Research Foundation as part of the Research Training Group Adaptive Preparation of Information from Heterogeneous Sources (AIPHES) under grant No.,Acknowledgments,[0],[0]
GRK 1994/1.,Acknowledgments,[0],[0]
We would like to thank anonymous reviewers for useful comments and especially thank Todor Mihaylov for the model implementations advices and everyone in the Computational Linguistics Group for helpful discussion.,Acknowledgments,[0],[0]
"Resolving abstract anaphora is an important, but difficult task for text understanding.",abstractText,[0],[0]
"Yet, with recent advances in representation learning this task becomes a more tangible aim.",abstractText,[0],[0]
A central property of abstract anaphora is that it establishes a relation between the anaphor embedded in the anaphoric sentence and its (typically non-nominal) antecedent.,abstractText,[0],[0]
We propose a mention-ranking model that learns how abstract anaphors relate to their antecedents with an LSTM-Siamese Net.,abstractText,[0],[0]
We overcome the lack of training data by generating artificial anaphoric sentence– antecedent pairs.,abstractText,[0],[0]
Our model outperforms state-of-the-art results on shell noun resolution.,abstractText,[0],[0]
We also report first benchmark results on an abstract anaphora subset of the ARRAU corpus.,abstractText,[0],[0]
This corpus presents a greater challenge due to a mixture of nominal and pronominal anaphors and a greater range of confounders.,abstractText,[0],[0]
"We found model variants that outperform the baselines for nominal anaphors, without training on individual anaphor data, but still lag behind for pronominal anaphors.",abstractText,[0],[0]
Our model selects syntactically plausible candidates and – if disregarding syntax – discriminates candidates using deeper features.,abstractText,[0],[0]
A Mention-Ranking Model for Abstract Anaphora Resolution,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 818–827 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1076",text,[0],[0]
This paper presents a minimal but surprisingly effective span-based neural model for constituency parsing.,1 Introduction,[0],[0]
"Recent years have seen a great deal of interest in parsing architectures that make use of recurrent neural network (RNN) representations of input sentences (Vinyals et al., 2015).",1 Introduction,[0],[0]
"Despite evidence that linear RNN decoders are implicitly able to respect some nontrivial well-formedness constraints on structured outputs (Graves, 2013), researchers have consistently found that the best performance is achieved by systems that explicitly require the decoder to generate well-formed tree structures (Chen and Manning, 2014).
",1 Introduction,[0],[0]
There are two general approaches to ensuring this structural consistency.,1 Introduction,[0],[0]
The most common is to encode the output as a sequence of operations within a transition system which constructs trees incrementally.,1 Introduction,[0],[0]
"This transforms the parsing problem back into a sequence-to-sequence problem, while making it easy to force the decoder to take only actions guaranteed to produce well-formed
outputs.",1 Introduction,[0],[0]
"However, transition-based models do not admit fast dynamic programs and require careful feature engineering to support exact search-based inference (Thang et al., 2015).",1 Introduction,[0],[0]
"Moreover, models with recurrent state require complex training procedures to benefit from anything other than greedy decoding (Wiseman and Rush, 2016).
",1 Introduction,[0],[0]
"An alternative line of work focuses on chart parsers, which use log-linear or neural scoring potentials to parameterize a tree-structured dynamic program for maximization or marginalization (Finkel et al., 2008; Durrett and Klein, 2015).",1 Introduction,[0],[0]
"These models enjoy a number of appealing formal properties, including support for exact inference and structured loss functions.",1 Introduction,[0],[0]
"However, previous chart-based approaches have required considerable scaffolding beyond a simple well-formedness potential, e.g. pre-specification of a complete context-free grammar for generating output structures and initial pruning of the output space with a weaker model (Hall et al., 2014).",1 Introduction,[0],[0]
"Additionally, we are unaware of any recent chartbased models that achieve results competitive with the best transition-based models.
",1 Introduction,[0],[0]
"In this work, we present an extremely simple chart-based neural parser based on independent scoring of labels and spans, and show how this model can be adapted to support a greedy topdown decoding procedure.",1 Introduction,[0],[0]
"Our goal is to preserve the basic algorithmic properties of span-oriented (rather than transition-oriented) parse representations, while exploring the extent to which neural representational machinery can replace the additional structure required by existing chart parsers.",1 Introduction,[0],[0]
"On the Penn Treebank, our approach outperforms a number of recent models for chart-based and transition-based parsing—including the state-ofthe-art models of Cross and Huang (2016) and Liu and Zhang (2016)—achieving an F1 score of 91.79.",1 Introduction,[0],[0]
"We additionally obtain a strong F1 score of 82.23 on the French Treebank.
818",1 Introduction,[0],[0]
A constituency tree can be regarded as a collection of labeled spans over a sentence.,2 Model,[0],[0]
"Taking this view as a guiding principle, we propose a model with two components, one which assigns scores to span labels and one which assigns scores directly to span existence.",2 Model,[0],[0]
"The former is used to determine the labeling of the output, and the latter provides its structure.
",2 Model,[0],[0]
At the core of both of these components is the issue of span representation.,2 Model,[0],[0]
"Given that a span’s correct label and its quality as a constituent depend heavily on the context in which it appears, we naturally turn to recurrent neural networks as a starting point, since they have previously been shown to capture contextual information suitable for use in a variety of natural language applications (Bahdanau et al., 2014; Wang et al., 2015)
",2 Model,[0],[0]
"In particular, we run a bidirectional LSTM over the input to obtain context-sensitive forward and backward encodings for each position i, denoted by fi and bi, respectively.",2 Model,[0],[0]
"Our representation of the span (i, j) is then the concatenatation the vector differences fj",2 Model,[0],[0]
− fi and bi − bj .,2 Model,[0],[0]
"This corresponds to a bidirectional version of the LSTMMinus features first proposed by Wang and Chang (2016).
",2 Model,[0],[0]
"On top of this base, our label and span scoring functions are implemented as one-layer feedforward networks, taking as input the concatenated span difference and producing as output either a vector of label scores or a single span score.",2 Model,[0],[0]
"More formally, letting sij denote the vector representation of span (i, j), we define
slabels(i, j) = V`g(W`sij + b`),
sspan(i, j) = v > s g(Wssij + bs),
where g denotes an elementwise nonlinearity.",2 Model,[0],[0]
"For notational convenience, we also let the score of an individual label ` be denoted by
slabel(i, j, `) =",2 Model,[0],[0]
"[slabels(i, j)]`,
where the right-hand side is the corresponding element of the label score vector.
",2 Model,[0],[0]
"One potential issue is the existence of unary chains, corresponding to nested labeled spans with the same endpoints.",2 Model,[0],[0]
We take the common approach of treating these as additional atomic labels alongside all elementary nonterminals.,2 Model,[0],[0]
"To accommodate n-ary trees, our inventory additionally includes a special empty label ∅ used for spans that
are not themselves full constituents but arise during the course of implicit binarization.
",2 Model,[0],[0]
Our model shares several features in common with that of Cross and Huang (2016).,2 Model,[0],[0]
"In particular, our representation of spans and the form of our label scoring function were directly inspired by their work, as were our handling of unary chains and our use of an empty label.",2 Model,[0],[0]
"However, our approach differs in its treatment of structural decisions, and consequently, the inference algorithms we describe below diverge significantly from their transition-based framework.",2 Model,[0],[0]
Our basic model is compatible with traditional chart-based dynamic programming.,3 Chart Parsing,[0],[0]
"Representing a constituency tree T by its labeled spans,
T := {(`t, (it, jt)) : t = 1, . . .",3 Chart Parsing,[0],[0]
", |T |},
we define the score of a tree to be the sum of its constituent label and span scores,
stree(T ) = ∑
(`,(i,j))∈T [slabel(i, j, `) +",3 Chart Parsing,[0],[0]
"sspan(i, j)] .
",3 Chart Parsing,[0],[0]
"To find the tree with the highest score for a given sentence, we use a modified CKY recursion.",3 Chart Parsing,[0],[0]
"As with classical chart parsing, the running time of our procedure is O(n3) for a sentence of length n.",3 Chart Parsing,[0],[0]
"The base case is a span (i, i + 1) consisting of a single word.",3.1 Dynamic Program for Inference,[0],[0]
"Since every valid tree must include all singleton spans, possibly with an empty label, we need not consider the span score in this case and perform only a single maximization over the choice of label:
sbest(i, i + 1) = max ` [slabel(i, i + 1, `)] .
",3.1 Dynamic Program for Inference,[0],[0]
"For a general span (i, j), we define the score of the split (i, k, j) as the sum of its subspan scores,
ssplit(i, k, j) =",3.1 Dynamic Program for Inference,[0],[0]
"sspan(i, k) +",3.1 Dynamic Program for Inference,[0],[0]
"sspan(k, j).",3.1 Dynamic Program for Inference,[0],[0]
"(1)
For convenience, we also define an augmented split score incorporating the scores of the corresponding subtrees,
s̃split(i, k, j) = ssplit(i, k, j)
+ sbest(i, k) + sbest(k, j).
",3.1 Dynamic Program for Inference,[0],[0]
"Using these quantities, we can then write the general joint label and split decision as
sbest(i, j) = max `,k",3.1 Dynamic Program for Inference,[0],[0]
"[slabel(i, j, `) + s̃split(i, k, j)] .
(2)
Because our model assigns independent scores to labels and spans, this maximization decomposes into two disjoint subproblems, greatly reducing the size of the state space:
sbest(i, j) = max ` [slabel(i, j, `)]
+ max k",3.1 Dynamic Program for Inference,[0],[0]
"[s̃split(i, k, j)] .
We also note that the span scores sspan(i, j) for each span (i, j) in the sentence can be computed once at the beginning of the procedure and shared across different subproblems with common left or right endpoints, allowing for a quadratic rather than cubic number of span score computations.",3.1 Dynamic Program for Inference,[0],[0]
Training the model under this inference scheme is accomplished using a margin-based approach.,3.2 Margin Training,[0],[0]
"When presented with an example sentence and its corresponding parse tree T ∗, we compute the best prediction under the current model using the above dynamic program,
T̂ = argmax T",3.2 Margin Training,[0],[0]
"[stree(T )] .
",3.2 Margin Training,[0],[0]
"If T̂ = T ∗, then our prediction was correct and no changes need to be made.",3.2 Margin Training,[0],[0]
"Otherwise, we incur a hinge penalty of the form
max ( 0, 1− stree(T ∗)",3.2 Margin Training,[0],[0]
"+ stree(T̂ ) )
to encourage the model to keep a margin of at least 1 between the gold tree and the best alternative.",3.2 Margin Training,[0],[0]
"The loss to be minimized is then the sum of penalties across all training examples.
",3.2 Margin Training,[0],[0]
"Prior work has found that it can be beneficial in a variety of applications to incorporate a structured loss function into this margin objective, replacing the hinge penalty above with one of the form
max ( 0, ∆(T̂ , T ∗)− stree(T ∗)",3.2 Margin Training,[0],[0]
"+ stree(T̂ ) )
",3.2 Margin Training,[0],[0]
for a loss function ∆ that measures the similarity between the prediction T̂ and the reference T ∗.,3.2 Margin Training,[0],[0]
Here we take ∆ to be a Hamming loss on labeled spans.,3.2 Margin Training,[0],[0]
"To incorporate this loss into the training objective, we modify the dynamic program of
Section 3.1 to support loss-augmented decoding (Taskar et al., 2005).",3.2 Margin Training,[0],[0]
"Since the label decisions are isolated from the structural decisions, it suffices to replace every occurrence of the label scoring function slabel(i, j, `) by
slabel(i, j, `)",3.2 Margin Training,[0],[0]
"+ 1(` 6= `∗ij),
where `∗ij is the label of span (i, j) in the gold tree T ∗.",3.2 Margin Training,[0],[0]
"This has the effect of requiring larger margins between the gold tree and predictions that contain more mistakes, offering a greater degree of robustness and better generalization.",3.2 Margin Training,[0],[0]
"While we have so far motivated our model from the perspective of classical chart parsing, it also allows for a novel inference algorithm in which trees are constructed greedily from the top down.",4 Top-Down Parsing,[0],[0]
"At a high level, given a span, we independently assign it a label and pick a split point, then repeat this process for the left and right subspans; the recursion bottoms out with length-one spans that can no longer be split.",4 Top-Down Parsing,[0],[0]
"Figure 1 gives an illustration of the process, which we describe in more detail below.
",4 Top-Down Parsing,[0],[0]
"The base case is again a singleton span (i, i+1), and follows the same form as the base case for the chart parser.",4 Top-Down Parsing,[0],[0]
"In particular, we select the label ̂̀that satisfies
̂̀= argmax ` [slabel(i, i + 1, `)] ,
omitting span scores from consideration since singleton spans cannot be split.
",4 Top-Down Parsing,[0],[0]
"To construct a tree over a general span (i, j), we aim to solve the maximization problem
(̂̀, k̂) =",4 Top-Down Parsing,[0],[0]
"argmax `,k",4 Top-Down Parsing,[0],[0]
"[slabel(i, j, `) + ssplit(i, k, j)] ,
where ssplit(i, k, j) is defined as in Equation (1).",4 Top-Down Parsing,[0],[0]
"The independence of our label and span scoring functions again yields the decomposed form
̂̀= argmax `",4 Top-Down Parsing,[0],[0]
"[slabel(i, j, `)] ,
k̂ = argmax k
[ssplit(i, k, j)] , (3)
leading to a significant reduction in the size of the state space.
To generate a tree for the whole sentence, we call this procedure on the full sentence span (0, n) and return the result.",4 Top-Down Parsing,[0],[0]
"As there are O(n) spans each
requiring one label evaluation and at most n − 1 split point evaluations, the running time of the procedure is O(n2).
",4 Top-Down Parsing,[0],[0]
"The algorithm outlined here bears a strong resemblance to the chart parsing dynamic program discussed in Section 3, but differs in one key aspect.",4 Top-Down Parsing,[0],[0]
"When performing inference from the bottom up, we have already computed the scores of all of the subtrees below the current span, and we can take this knowledge into consideration when selecting a split point.",4 Top-Down Parsing,[0],[0]
"In contrast, when producing a tree from the top down, we can only select a split point based on top-level evaluations of span quality, without knowing anything about the subtrees that will be generated below them.",4 Top-Down Parsing,[0],[0]
"This difference is manifested in the augmented split score s̃split used in the definition of sbest in Equation (2), where the scores of the subtrees associated with a split point are included in the chart recursion but necessarily excluded from the top-down recursion.
",4 Top-Down Parsing,[0],[0]
"While this apparent deficiency may be a cause for concern, we demonstrate the surprising empirical result in Section 6 that there is no loss in per-
formance when moving from the globally-optimal chart parser to the greedy top-down procedure.",4 Top-Down Parsing,[0],[0]
"As with the chart parsing formulation, we also use a margin-based method for learning under the topdown model.",4.1 Margin Training,[0],[0]
"However, rather than requiring separation between the scores of full trees, we instead enforce a local margin at every decision point.
",4.1 Margin Training,[0],[0]
"For a span (i, j) occurring in the gold tree, let `∗ and k∗ represent the correct label and split point, and let ̂̀and k̂ be the predictions made by computing the maximizations in Equation (3).",4.1 Margin Training,[0],[0]
"If ̂̀ 6= `∗, meaning the prediction is incorrect, we incur a hinge penalty of the form
max ( 0, 1− slabel(i, j, `∗) +",4.1 Margin Training,[0],[0]
"slabel(i, j, ̂̀) ) .
",4.1 Margin Training,[0],[0]
"Similarly, if k̂ 6= k∗, we incur a hinge penalty of the form
max ( 0, 1− ssplit(i, k∗, j) + ssplit(i, k̂, j) ) .
",4.1 Margin Training,[0],[0]
"To obtain the loss for a given training example, we trace out the actions corresponding to the gold tree and accumulate the above penalties over all decision points.",4.1 Margin Training,[0],[0]
"As before, the total loss to be minimized is the sum of losses across all training examples.
",4.1 Margin Training,[0],[0]
"Loss augmentation is also beneficial for the local decisions made by the top-down model, and can be implemented in a manner akin to the one discussed in Section 3.2.",4.1 Margin Training,[0],[0]
"The hinge penalties given above are only defined for spans (i, j) that appear in the example tree.",4.2 Training with Exploration,[0],[0]
"The model must therefore be constrained at training time to follow decisions that exactly reproduce the gold tree, since supervision cannot be provided otherwise.",4.2 Training with Exploration,[0],[0]
"As a result, the model is never exposed to its mistakes, which can lead to a lack of calibration and poor performance at test time.
",4.2 Training with Exploration,[0],[0]
"To circumvent this issue, a dynamic oracle can be defined to inform the model about correct behavior even after it has deviated from the gold tree.",4.2 Training with Exploration,[0],[0]
"Cross and Huang (2016) propose such an oracle for a related transition-based parsing system, and prove its optimality for the F1 metric on labeled spans.",4.2 Training with Exploration,[0],[0]
"We adapt their result here to obtain a dynamic oracle for the present model with similar guarantees.
",4.2 Training with Exploration,[0],[0]
"The oracle for labeling decisions carries over without modification: the correct label for a span is the label assigned to that span if it is part of the gold tree, or the empty label ∅ otherwise.
",4.2 Training with Exploration,[0],[0]
"For split point decisions, the oracle can be broken down into two cases.",4.2 Training with Exploration,[0],[0]
"If a span (i, j) appears as a constituent in the gold tree T , we let b(i, j) denote the collection of its interior boundary points.",4.2 Training with Exploration,[0],[0]
"For example, if the constituent over (1, 7) has children spanning (1, 3), (3, 6), and (6, 7), then we would have the two interior boundary points, b(1, 7) =",4.2 Training with Exploration,[0],[0]
"{3, 6}.",4.2 Training with Exploration,[0],[0]
The oracle for a span appearing in the gold tree is then precisely the output of this function.,4.2 Training with Exploration,[0],[0]
"Otherwise, for spans (i, j) not corresponding to gold constituents, we must instead identify the smallest enclosing gold constituent:
(i∗, j∗) = min{(i′, j′) ∈ T : i′ ≤",4.2 Training with Exploration,[0],[0]
"i < j ≤ j′}, where the minimum is taken with respect to the partial ordering induced by span length.",4.2 Training with Exploration,[0],[0]
"The output of the oracle is then the set of interior boundary points of this enclosing span that also lie inside the original, {k ∈ b(i∗, j∗) : i < k",4.2 Training with Exploration,[0],[0]
"< j}.
",4.2 Training with Exploration,[0],[0]
"The proof of correctness is similar to the proof in Cross and Huang (2016); we refer to the Dynamic Oracle section in their paper for a more detailed discussion.
",4.2 Training with Exploration,[0],[0]
"As presented, the dynamic oracle for split point decisions returns a collection of one or more splits rather than a single correct answer.",4.2 Training with Exploration,[0],[0]
"Any of these is a valid choice, with different splits corresponding to different binarizations of the original n-ary tree.",4.2 Training with Exploration,[0],[0]
"We choose to use the leftmost split point for consistency in our implementation, but remark that the oracle split with the highest score could also be chosen at training time to allow for additional flexibility.
",4.2 Training with Exploration,[0],[0]
"Having defined the dynamic oracle for our system, we note that training with exploration can be implemented by a single modification to the procedure described in Section 4.1.",4.2 Training with Exploration,[0],[0]
"Local penalties are accumulated as before, but instead of tracing out the decisions required to produce the gold tree, we instead follow the decisions predicted by the model.",4.2 Training with Exploration,[0],[0]
"In this way, supervision is provided at states within the prediction procedure that are more likely to arise at test time when greedy inference is performed.",4.2 Training with Exploration,[0],[0]
The model presented in Section 2 is designed to be as simple as possible.,5 Scoring and Loss Alternatives,[0],[0]
"However, there are many variations of the label and span scoring functions that could be explored; we discuss some of the options here.",5 Scoring and Loss Alternatives,[0],[0]
"Our basic model treats the empty label, elementary nonterminals, and unary chains each as atomic units, obscuring similarities between unary chains and their component nonterminals or between different unary chains with common prefixes or suffixes.",5.1 Top-Middle-Bottom Label Scoring,[0],[0]
"To address this lack of structure, we consider an alternative scoring scheme in which labels are predicted in three parts: a top nonterminal, a middle unary chain, and a bottom nonterminal (each of which is possibly empty).1",5.1 Top-Middle-Bottom Label Scoring,[0],[0]
"This not only allows for parameter sharing across labels with common subcomponents, but also has the added benefit of allowing the model to produce novel unary chains at test time.
",5.1 Top-Middle-Bottom Label Scoring,[0],[0]
"1In more detail, ∅ decomposes as (∅, ∅, ∅), X decomposes as (X , ∅, ∅), X–Y decomposes as (X , ∅, Y ), and X–Z1– · · · –Zk–Y decomposes as (X , Z1– · · · –Zk, Y ).
",5.1 Top-Middle-Bottom Label Scoring,[0],[0]
"More precisely, we introduce the decomposition
slabel(i, j, (`t, `m, `b))",5.1 Top-Middle-Bottom Label Scoring,[0],[0]
"=
stop(i, j, `t) +",5.1 Top-Middle-Bottom Label Scoring,[0],[0]
"smiddle(i, j, `m) +",5.1 Top-Middle-Bottom Label Scoring,[0],[0]
"sbottom(i, j, `b),
where stop, smiddle, and sbottom are independent one-layer feedforward networks of the same form as slabel that output vectors of scores for all label tops, label middle chains, and label bottoms encountered in the training corpus, respectively.",5.1 Top-Middle-Bottom Label Scoring,[0],[0]
"The best label for a span (i, j) is then computed by solving the maximization problem
max `t,`m,`b [slabel(i, j, (`t, `m, `b))] ,
which decomposes into three independent subproblems corresponding to the three label components.",5.1 Top-Middle-Bottom Label Scoring,[0],[0]
"The final label is obtained by concatenating `t, `m, and `b, with empty components being omitted from the concatenation.",5.1 Top-Middle-Bottom Label Scoring,[0],[0]
The basic model uses the same span scoring function sspan to assign a score to the left and right subspans of a given span.,5.2 Left and Right Span Scoring,[0],[0]
"One simple extension is to replace this by a pair of distinct left and right feedforward networks of the same form, giving the decomposition
ssplit(i, k, j) = sleft(i, k) + sright(k, j).",5.2 Left and Right Span Scoring,[0],[0]
"Since span scores are only used to score splits in our model, we also consider directly scoring a split by feeding the concatenation of the span representations of the left and right subspans through a single feedforward network, giving
ssplit(i, k, j) = v > s g (Ws[sik; skj ] + bs) .
",5.3 Span Concatenation Scoring,[0],[0]
"This is similar to the structural scoring function used by Cross and Huang (2016), although whereas they additionally include features for the outside spans (0, i) and (j, n) in their concatenation, we omit these from our implementation, finding that they do not improve performance.",5.3 Span Concatenation Scoring,[0],[0]
"Inspired by the success of deep biaffine scoring in recent work by Dozat and Manning (2016) for dependency parsing, we also consider a split scoring function of a similar form for our model.",5.4 Deep Biaffine Span Scoring,[0],[0]
"Specifically, we let hik = fleft(sik) and hkj =
fright(skj) be deep left and right span representations obtained by passing the child vectors through corresponding left and right feedforward networks.",5.4 Deep Biaffine Span Scoring,[0],[0]
"We then define the biaffine split scoring function
ssplit(i, k, j) = h >",5.4 Deep Biaffine Span Scoring,[0],[0]
"ikWshkj + v > lefthik + v > righthkj ,
which consists of the sum of a bilinear form between the two hidden representations together with two inner products.",5.4 Deep Biaffine Span Scoring,[0],[0]
The three-way label scoring scheme described in Section 5.1 offers one path towards the incorporation of label structure into the model.,5.5 Structured Label Loss,[0],[0]
We additionally consider a structured Hamming loss on labels.,5.5 Structured Label Loss,[0],[0]
"More specifically, given two labels `1 and `2 consisting of zero or more nonterminals, we define the loss as |`1 \ `2|+ |`2 \ `1|, treating each label as a multiset of nonterminals.",5.5 Structured Label Loss,[0],[0]
This structured loss can be incorporated into the training process using the methods described in Sections 3.2 and 4.1.,5.5 Structured Label Loss,[0],[0]
We first describe the general setup used for our experiments.,6 Experiments,[0],[0]
"We use the Penn Treebank (Marcus et al., 1993) for our English experiments, with standard splits of sections 2-21 for training, section 22 for development, and section 23 for testing.",6 Experiments,[0],[0]
"We use the French Treebank from the SPMRL 2014 shared task (Seddah et al., 2014) with its provided splits for our French experiments.",6 Experiments,[0],[0]
"No token preprocessing is performed, and only a single <UNK> token is used for unknown words at test time.",6 Experiments,[0],[0]
The inputs to our system are concatenations of 100-dimensional word embeddings and 50-dimensional part-of-speech embeddings.,6 Experiments,[0],[0]
"In the case of the French Treebank, we also include 50-dimensional embeddings of each morphological tag.",6 Experiments,[0],[0]
"We use automatically predicted tags for training and testing, obtaining predicted part-ofspeech tags for the Penn Treebank using the Stanford tagger (Toutanova et al., 2003) with 10-way jackknifing, and using the provided predicted partof-speech and morphological tags for the French Treebank.",6 Experiments,[0],[0]
Words are replaced by <UNK> with probability 1/(1+freq(w)),6 Experiments,[0],[0]
"during training, where freq(w) is the frequency of w in the training data.
",6 Experiments,[0],[0]
We use a two-layer bidirectional LSTM for our base span features.,6 Experiments,[0],[0]
"Dropout with a ratio selected from {0.2, 0.3, 0.4} is applied to all non-recurrent
connections of the LSTM, including its inputs and outputs.",6 Experiments,[0],[0]
"We tie the hidden dimension of the LSTM and all feedforward networks, selecting a size from {150, 200, 250}.",6 Experiments,[0],[0]
"All parameters (including word and tag embeddings) are randomly initialized using Glorot initialization (Glorot and Bengio, 2010), and are tuned on development set performance.",6 Experiments,[0],[0]
"We use the Adam optimizer (Kingma and Ba, 2014) with its default settings for optimization, with a batch size of 10.",6 Experiments,[0],[0]
"Our system is implemented in C++ using the DyNet neural network library (Neubig et al., 2017).
",6 Experiments,[0],[0]
We begin by training the minimal version of our proposed chart and top-down parsers on the Penn Treebank.,6 Experiments,[0],[0]
"Out of the box, we obtain test F1 scores of 91.69 for the chart parser and 91.58 for the topdown parser.",6 Experiments,[0],[0]
"The higher of these matches the recent state-of-the-art score of 91.7 reported by Liu and Zhang (2016), demonstrating that our simple neural parsing system is already capable of achieving strong results.
",6 Experiments,[0],[0]
"Building on this, we explore the effects of different split scoring functions when using either the basic 0-1 label loss or the structured label loss discussed in Section 5.5.",6 Experiments,[0],[0]
"Our results are presented in Tables 1a and 1b.
",6 Experiments,[0],[0]
"We observe that regardless of the label loss, the minimal and deep biaffine split scoring schemes perform a notch below the left-right and concatenation scoring schemes.",6 Experiments,[0],[0]
"That the minimal scoring scheme performs worse than the left-right scheme is unsurprising, since the latter is a strict generalization of the former.",6 Experiments,[0],[0]
"It is evident, however,
that joint scoring of left and right subspans is not required for strong results—in fact, the left-right scheme which scores child subspans in isolation slightly outperforms the concatenation scheme in all but one case, and is stronger than the deep biaffine scoring function across the board.
",6 Experiments,[0],[0]
"Comparing results across the choice of label loss, however, we find that fewer trends are apparent.",6 Experiments,[0],[0]
"The scores obtained by training with a 0-1 loss are all within 0.1 of those obtained using a structured Hamming loss, being slightly higher in four out of eight cases and slightly lower in the other half.",6 Experiments,[0],[0]
"This leads us to conclude that the more elementary approach is sufficient when selecting atomic labels from a fixed inventory.
",6 Experiments,[0],[0]
We also perform the same set of experiments under the setting where the top-middle-bottom label scoring function described in Section 5.1 is used in place of an atomic label scoring function.,6 Experiments,[0],[0]
"These results are shown in Tables 1c and 1d.
",6 Experiments,[0],[0]
"A priori, we might expect that exposing additional structure would allow the model to make better predictions, but on the whole we find that the scores in this set of experiments are worse than those in the previous set.",6 Experiments,[0],[0]
"Trends similar to before hold across the different choices of scoring functions, though in this case the minimal setting has scores closer to those of the left-right setting, even exceeding its performance in the case of a chart parser with a 0-1 label loss.
",6 Experiments,[0],[0]
"Our final test results are given in Table 2, along with the results of other recent single-model parsers trained without external parse data.",6 Experiments,[0],[0]
"We
achieve a new state-of-the-art F1 score of 91.79 with our best model.",6 Experiments,[0],[0]
"Interestingly, we observe that our parsers have a noticeably higher gap between precision and recall than do other top parsers, likely owing to the structured label loss which penalizes mismatching nonterminals more heavily than it does a nonterminal and empty label mismatch.",6 Experiments,[0],[0]
"In addition, there is little difference between the best top-down model and the best chart model, indicating that global normalization is not required to achieve strong results.",6 Experiments,[0],[0]
"Processing one sentence at a time on a c4.4xlarge Amazon EC2 instance, our best chart and top-down parsers operate at speeds of 20.3 sentences per second and 75.5 sentences per second, respectively, as measured on the test set.
",6 Experiments,[0],[0]
"We additionally train parsers on the French Treebank using the same settings from our English experiments, selecting the best model of each type based on development performance.",6 Experiments,[0],[0]
We list our test results along with those of several other recent papers in Table 3.,6 Experiments,[0],[0]
"Although we fall short of the scores obtained by Cross and Huang (2016), we achieve competitive performance relative to the neural CRF parser of Durrett and Klein (2015).",6 Experiments,[0],[0]
"Many early successful approaches to constituency parsing focused on rich modeling of correlations in the output space, typically by engineering proabilistic context-free grammars with state spaces enriched to capture long-distance dependencies and lexical phenomena (Collins, 2003; Klein and Manning, 2003; Petrov and Klein, 2007).",7 Related Work,[0],[0]
"By contrast, the approach we have described here continues a recent line of work on direct modeling of correlations in the input space, by using rich feature representations to parameterize local potentials that interact with a comparatively unconstrained structured decoder.",7 Related Work,[0],[0]
"As noted in the introduction, this class of feature-based tree scoring functions can be implemented with either a linear transition system (Chen and Manning, 2014) or a global decoder (Finkel et al., 2008).",7 Related Work,[0],[0]
"Kiperwasser and Goldberg (2016) describe an approach closely related to ours but targeted at dependency formalisms, and which easily accommodates both sparse log-linear scoring models (Hall et al., 2014) and deep neural potentials (Henderson, 2004; Ballesteros et al., 2016).
",7 Related Work,[0],[0]
"The best-performing constituency parsers in the last two years have largely been transition-based rather than global; examples include the models of Dyer et al. (2016), Cross and Huang (2016) and Liu and Zhang (2016).",7 Related Work,[0],[0]
"The present work takes many of the insights developed in these models (e.g. the recurrent representation of spans (Kiperwasser and Goldberg, 2016), and the use of a dynamic oracle and exploration policy during training (Goldberg and Nivre, 2013)) and extends these insights to span-oriented models, which support a wider range of decoding procedures.",7 Related Work,[0],[0]
"Our approach differs from other recent chart-based neural models (e.g. Durrett and Klein (2015)) in the use of a recurrent input representation, structured loss function, and comparatively simple parameterization of the scoring function.",7 Related Work,[0],[0]
"In addition to the globally optimal decoding procedures for which these models were designed, and in contrast to the left-to-right decoder typically employed by transition-based models, our model admits an additional greedy top-to-bottom inference procedure.",7 Related Work,[0],[0]
"We have presented a minimal span-oriented parser that uses a recurrent input representation to score
trees with a sum of independent potentials on their constituent spans and labels.",8 Conclusion,[0],[0]
Our model supports both exact chart-based decoding and a novel top-down inference procedure.,8 Conclusion,[0],[0]
"Both approaches achieve state-of-the-art performance on the Penn Treebank, and our best model achieves competitive performance on the French Treebank.",8 Conclusion,[0],[0]
"Our experiments show that many of the key insights from recent neural transition-based approaches to parsing can be easily ported to the chart parsing setting, resulting in a pair of extremely simple models that nonetheless achieve excellent performance.",8 Conclusion,[0],[0]
We would like to thank Nick Altieri and the anonymous reviewers for their valuable comments and suggestions.,Acknowledgments,[0],[0]
MS is supported by an NSF Graduate Research Fellowship.,Acknowledgments,[0],[0]
JA is supported by a Facebook graduate fellowship and a Berkeley AI / Huawei fellowship.,Acknowledgments,[0],[0]
"In this work, we present a minimal neural model for constituency parsing based on independent scoring of labels and spans.",abstractText,[0],[0]
"We show that this model is not only compatible with classical dynamic programming techniques, but also admits a novel greedy top-down inference algorithm based on recursive partitioning of the input.",abstractText,[0],[0]
"We demonstrate empirically that both prediction schemes are competitive with recent work, and when combined with basic extensions to the scoring model are capable of achieving state-of-the-art single-model performance on the Penn Treebank (91.79 F1) and strong performance on the French Treebank (82.23 F1).",abstractText,[0],[0]
A Minimal Span-Based Neural Constituency Parser,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1318–1328 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1318",text,[0],[0]
"Temporal relation (TempRel) extraction is an important task for event understanding, and it has drawn much attention in the natural language processing (NLP) community recently (UzZaman et al., 2013; Chambers et al., 2014; Llorens et al., 2015; Minard et al., 2015; Bethard et al., 2015, 2016, 2017; Leeuwenberg and Moens, 2017; Ning et al., 2017, 2018a,b).
",1 Introduction,[0],[0]
"Initiated by TimeBank (TB) (Pustejovsky et al., 2003b), a number of TempRel datasets have been collected, including but not limited to the verbclause augmentation to TB (Bethard et al., 2007),
1The dataset is publicly available at https:// cogcomp.org/page/publication_view/834.
",1 Introduction,[0],[0]
"TempEval1-3 (Verhagen et al., 2007, 2010; UzZaman et al., 2013), TimeBank-Dense (TB-Dense) (Cassidy et al., 2014), EventTimeCorpus (Reimers et al., 2016), and datasets with both temporal and other types of relations (e.g., coreference and causality) such as CaTeRs (Mostafazadeh et al., 2016) and RED (O’Gorman et al., 2016).",1 Introduction,[0],[0]
"These datasets were annotated by experts, but most still suffered from low inter-annotator agreements (IAA).",1 Introduction,[0],[0]
"For instance, the IAAs of TB-Dense, RED and THYME-TimeML (Styler IV et al., 2014) were only below or near 60% (given that events are already annotated).",1 Introduction,[0],[0]
"Since a low IAA usually indicates that the task is difficult even for humans (see Examples 1-3), the community has been looking into ways to simplify the task, by reducing the label set, and by breaking up the overall, complex task into subtasks (e.g., getting agreement on which event pairs should have a relation, and then what that relation should be) (Mostafazadeh et al., 2016; O’Gorman et al., 2016).",1 Introduction,[0],[0]
"In contrast to other existing datasets, Bethard et al. (2007) achieved an agreement as high as 90%, but the scope of its annotation was narrowed down to a very special verb-clause structure.
(e1, e2), (e3, e4), and (e5, e6): TempRels that are difficult even for humans.",1 Introduction,[0],[0]
Note that only relevant events are highlighted here.,1 Introduction,[0],[0]
Example 1: Serbian police tried to eliminate the proindependence Kosovo Liberation Army and (e1:restore) order.,1 Introduction,[0],[0]
At least 51 people were (e2:killed) in clashes between Serb police and ethnic Albanians in the troubled region.,1 Introduction,[0],[0]
"Example 2: Service industries (e3:showed) solid job gains, as did manufacturers, two areas expected to be hardest (e4:hit) when the effects of the Asian crisis hit the American economy.",1 Introduction,[0],[0]
"Example 3: We will act again if we have evidence he is (e5:rebuilding) his weapons of mass destruction capabilities, senior officials say.",1 Introduction,[0],[0]
"In a bit of television diplomacy, Iraq’s deputy foreign minister (e6:responded) from Baghdad in less than one hour, saying that . . .
",1 Introduction,[0],[0]
"This paper proposes a new approach to handling
these issues in TempRel annotation.",1 Introduction,[0],[0]
"First, we introduce multi-axis modeling to represent the temporal structure of events, based on which we anchor events to different semantic axes; only events from the same axis will then be temporally compared (Sec. 2).",1 Introduction,[0],[0]
"As explained later, those event pairs in Examples 1-3 are difficult because they represent different semantic phenomena and belong to different axes.",1 Introduction,[0],[0]
"Second, while we represent an event pair using two time intervals (say, [t1start, t 1 end] and [t 2 start, t 2 end]), we suggest that comparisons involving end-points (e.g., t1end vs. t2end) are typically more difficult than comparing start-points (i.e., t1start vs. t2start); we attribute this to the ambiguity of expressing and perceiving durations of events (Coll-Florit and Gennari, 2011).",1 Introduction,[0],[0]
"We believe that this is an important consideration, and we propose in Sec. 3 that TempRel annotation should focus on start-points.",1 Introduction,[0],[0]
"Using the proposed annotation scheme, a pilot study done by experts achieved a high IAA of .84 (Cohen’s Kappa) on a subset of TB-Dense, in contrast to the conventional 60’s.
",1 Introduction,[0],[0]
"In addition to the low IAA issue, TempRel annotation is also known to be labor intensive.",1 Introduction,[0],[0]
"Our third contribution is that we facilitate, for the first time, the use of crowdsourcing to collect a new, high quality (under multiple metrics explained later) TempRel dataset.",1 Introduction,[0],[0]
"We explain how the crowdsourcing quality was controlled and how vague relations were handled in Sec. 4, and present some statistics and the quality of the new dataset in Sec. 5.",1 Introduction,[0],[0]
"A baseline system is also shown to achieve much better performance on the new dataset, when compared with system performance in the literature (Sec. 6).",1 Introduction,[0],[0]
"The paper’s results are very encouraging and hopefully, this work would significantly benefit research in this area.",1 Introduction,[0],[0]
"Given a set of events, one important question in designing the TempRel annotation task is: which pairs of events should have a relation?",2 Temporal Structure of Events,[0],[0]
The answer to it depends on the modeling of the overall temporal structure of events.,2 Temporal Structure of Events,[0],[0]
"TimeBank (Pustejovsky et al., 2003b) laid the foundation for many later TempRel corpora, e.g., (Bethard et al., 2007; UzZaman et al., 2013; Cas-
sidy et al., 2014).2",2.1 Motivation,[0],[0]
"In TimeBank, the annotators were allowed to label TempRels between any pairs of events.",2.1 Motivation,[0],[0]
"This setup models the overall structure of events using a general graph, which made annotators inadvertently overlook some pairs, resulting in low IAAs and many false negatives.
",2.1 Motivation,[0],[0]
Example 4: Dense Annotation Scheme.,2.1 Motivation,[0],[0]
Serbian police (e7:tried) to (e8:eliminate) the proindependence Kosovo Liberation Army and (e1:restore) order.,2.1 Motivation,[0],[0]
At least 51 people were (e2:killed) in clashes between Serb police and ethnic Albanians in the troubled region.,2.1 Motivation,[0],[0]
"Given 4 NON-GENERIC events above, the dense scheme presents 6 pairs to annotators one by one: (e7, e8), (e7, e1), (e7, e2), (e8, e1), (e8, e2), and (e1, e2).",2.1 Motivation,[0],[0]
"Apparently, not all pairs are well-defined, e.g., (e8, e2) and (e1, e2), but annotators are forced to label all of them.
",2.1 Motivation,[0],[0]
"To address this issue, Cassidy et al. (2014) proposed a dense annotation scheme, TB-Dense, which annotates all event pairs within a sliding, two-sentence window (see Example 4).",2.1 Motivation,[0],[0]
"It requires all TempRels between GENERIC3 and NON-GENERIC events to be labeled as vague, which conceptually models the overall structure by two disjoint time-axes: one for the NONGENERIC and the other one for the GENERIC.
",2.1 Motivation,[0],[0]
"However, as shown by Examples 1-3 in which the highlighted events are NON-GENERIC, the TempRels may still be ill-defined: In Example 1, Serbian police tried to restore order but ended up with conflicts.",2.1 Motivation,[0],[0]
"It is reasonable to argue that the attempt to e1:restore order happened before the conflict where 51 people were e2:killed; or, 51 people had been killed but order had not been restored yet, so e1:restore is after e2:killed.",2.1 Motivation,[0],[0]
"Similarly, in Example 2, service industries and manufacturers were originally expected to be hardest e4:hit but actually e3:showed gains, so e4:hit is before e3:showed; however, one can also argue that the two areas had showed gains but had not been hit, so e4:hit is after e3:showed.",2.1 Motivation,[0],[0]
"Again, e5:rebuilding is a hypothetical event: “we will act if rebuilding is true”.",2.1 Motivation,[0],[0]
"Readers do not know for sure if “he is already rebuilding weapons but we have no evidence”, or “he will be building weapons in the future”, so annotators may disagree on the relation between e5:rebuilding and e6:responded.",2.1 Motivation,[0],[0]
"Despite, importantly, minimizing missing annota-
2EventTimeCorpus (Reimers et al., 2016) is based on TimeBank, but aims at anchoring events onto explicit time expressions in each document rather than annotating TempRels between events, which can be a good complementary to other TempRel datasets.
",2.1 Motivation,[0],[0]
"3For example, lions eat meat is GENERIC.
tions, the current dense scheme forces annotators to label many such ill-defined pairs, resulting in low IAA.",2.1 Motivation,[0],[0]
"Arguably, an ideal annotator may figure out the above ambiguity by him/herself and mark them as vague, but it is not a feasible requirement for all annotators to stay clear-headed for hours; let alone crowdsourcers.",2.2 Multi-Axis Modeling,[0],[0]
"What makes things worse is that, after annotators spend a long time figuring out these difficult cases, whether they disagree with each other or agree on the vagueness, the final decisions for such cases will still be vague.
",2.2 Multi-Axis Modeling,[0],[0]
"As another way to handle this dilemma, TBDense resorted to a 80% confidence rule: annotators were allowed to choose a label if one is 80% sure that it was the writer’s intent.",2.2 Multi-Axis Modeling,[0],[0]
"However, as pointed out by TB-Dense, annotators are likely to have rather different understandings of 80% confidence and it will still end up with disagreements.
",2.2 Multi-Axis Modeling,[0],[0]
"In contrast to these annotation difficulties, humans can easily grasp the meaning of news articles, implying a potential gap between the difficulty of the annotation task and the one of understanding the actual meaning of the text.",2.2 Multi-Axis Modeling,[0],[0]
"In Examples 1-3, the writers did not intend to explain the TempRels between those pairs, and the original annotators of TimeBank4 did not label relations between those pairs either, which indicates that both writers and readers did not think the TempRels between these pairs were crucial.",2.2 Multi-Axis Modeling,[0],[0]
"Instead, what is crucial in these examples is that “Serbian police tried to restore order but killed 51 people”, that “two areas were expected to be hit but showed gains”, and that “if he rebuilds weapons then we will act.”",2.2 Multi-Axis Modeling,[0],[0]
"To “restore order”, to be “hardest hit”, and “if he was rebuilding” were only the intention of police, the opinion of economists, and the condition to act, respectively, and whether or not they actually happen is not the focus of those writers.
",2.2 Multi-Axis Modeling,[0],[0]
This discussion suggests that a single axis is too restrictive to represent the complex structure of NON-GENERIC events.,2.2 Multi-Axis Modeling,[0],[0]
"Instead, we need a modeling which is more restrictive than a general graph so that annotators can focus on relation annotation (rather than looking for pairs first), but also more flexible than a single axis so that ill-defined
4Recall that they were given the entire article and only salient relations would be annotated.
",2.2 Multi-Axis Modeling,[0],[0]
relations are not forcibly annotated.,2.2 Multi-Axis Modeling,[0],[0]
"Specifically, we need axes for intentions, opinions, hypotheses, etc.",2.2 Multi-Axis Modeling,[0],[0]
in addition to the main axis of an article.,2.2 Multi-Axis Modeling,[0],[0]
"We thus argue for multi-axis modeling, as defined in Table 1.",2.2 Multi-Axis Modeling,[0],[0]
"Following the proposed modeling, Examples 1-3 can be represented as in Fig. 1.",2.2 Multi-Axis Modeling,[0],[0]
"This modeling aims at capturing what the author has explicitly expressed and it only asks annotators to look at comparable pairs, rather than forcing them to make decisions on often vaguely defined pairs.
",2.2 Multi-Axis Modeling,[0],[0]
"In practice, we annotate one axis at a time: we first classify if an event is anchorable onto a given axis (this is also called the anchorability annotation step); then we annotate every pair of anchorable events (i.e., the relation annotation step); finally, we can move to another axis and repeat the two steps above.",2.2 Multi-Axis Modeling,[0],[0]
Note that ruling out cross-axis relations is only a strategy we adopt in this paper to separate well-defined relations from ill-defined relations.,2.2 Multi-Axis Modeling,[0],[0]
"We do not claim that cross-axis relations are unimportant; instead, as shown in Fig. 2, we think that cross-axis relations are a different semantic phenomenon that requires additional investigation.",2.2 Multi-Axis Modeling,[0],[0]
"There have been other proposals of temporal structure modelings (Bramsen et al., 2006; Bethard et al., 2012), but in general, the semantic phenomena handled in our work are very different and complementary to them.",2.3 Comparisons with Existing Work,[0],[0]
"(Bramsen et al., 2006) introduces “temporal segments” (a fragment of text that does not exhibit abrupt changes) in the medical domain.",2.3 Comparisons with Existing Work,[0],[0]
"Similarly, their temporal segments can also be considered as a special temporal structure modeling.",2.3 Comparisons with Existing Work,[0],[0]
"But a key difference is that (Bramsen et al., 2006) only annotates inter-segment relations, ignoring intra-segment ones.",2.3 Comparisons with Existing Work,[0],[0]
"Since those segments are usually large chunks of text, the semantics handled in (Bramsen et al., 2006) is in a very coarse granularity (as pointed out by (Bramsen et al., 2006)) and is thus different from ours.
",2.3 Comparisons with Existing Work,[0],[0]
"(Bethard et al., 2012) proposes a tree structure for children’s stories, which “typically have simpler temporal structures”, as they pointed out.",2.3 Comparisons with Existing Work,[0],[0]
"Moreover, in their annotation, an event can only be linked to a single nearby event, even if multiple nearby events may exist, whereas we do not have such restrictions.
",2.3 Comparisons with Existing Work,[0],[0]
"In addition, some of the semantic phenomena in Table 1 have been discussed in existing work.",2.3 Comparisons with Existing Work,[0],[0]
Here we compare with them for a better positioning of the proposed scheme.,2.3 Comparisons with Existing Work,[0],[0]
TB-Dense handled the incomparability between main-axis events and HYPOTHESIS/NEGATION by treating an event as having occurred if the event is HYPOTHESIS/NEGATION.5,2.3.1 Axis Projection,[0],[0]
"In our multiaxis modeling, the strategy adopted by TB-Dense falls into a more general approach, “axis projection”.",2.3.1 Axis Projection,[0],[0]
"That is, projecting events across different axes to handle the incomparability between any two axes (not limited to HYPOTHESIS/NEGATION).",2.3.1 Axis Projection,[0],[0]
"Axis projection works well for certain event pairs like Asian crisis and e4:hardest hit in Example 2: as in Fig. 1, Asian crisis is before expected, which is again before e4:hardest hit, so Asian crisis is before e4:hardest hit.
",2.3.1 Axis Projection,[0],[0]
"Generally, however, since there is no direct evidence that can guide the projection, annotators may have different projections (imagine projecting e5:rebuilding onto the main axis: is it in the past or in the future?).",2.3.1 Axis Projection,[0],[0]
"As a result, axis projec-
5In the case of Example 3, it is to treat rebuilding as actually happened and then link it to responded.
tion requires many specially designed guidelines or strong external knowledge.",2.3.1 Axis Projection,[0],[0]
"Annotators have to rigidly follow the sometimes counter-intuitive guidelines or “guess” a label instead of looking for evidence in the text.
",2.3.1 Axis Projection,[0],[0]
"When strong external knowledge is involved in axis projection, it becomes a reasoning process and the resulting relations are a different type.",2.3.1 Axis Projection,[0],[0]
"For example, a reader may reason that in Example 3, it is well-known that they did “act again”, implying his e5:rebuilding had happened and is before e6:responded.",2.3.1 Axis Projection,[0],[0]
Another example is in Fig. 2.,2.3.1 Axis Projection,[0],[0]
"It is obvious that relations based on these projections are not the same with and more challenging than those same-axis relations, so in the current stage, we should focus on same-axis relations only.",2.3.1 Axis Projection,[0],[0]
"Another prominent difference to earlier work is the introduction of orthogonal axes, which has not been used in any existing work as we know.",2.3.2 Introduction of the Orthogonal Axes,[0],[0]
"A special property is that the intersection event of two axes can be compared to events from both, which can sometimes bridge events, e.g., in Fig. 1, Asian crisis is seemingly before hardest hit due to their connections to expected.",2.3.2 Introduction of the Orthogonal Axes,[0],[0]
"Since Asian crisis is on the main axis, it seems that e4:hardest hit is on the main axis as well.",2.3.2 Introduction of the Orthogonal Axes,[0],[0]
"However, the “hardest hit” in “Asian crisis before hardest hit” is only a projection of the original e4:hardest hit onto the real axis and is valid only when this OPINION is true.
",2.3.2 Introduction of the Orthogonal Axes,[0],[0]
"Nevertheless, OPINIONS are not always true and INTENTIONS are not always fulfilled.",2.3.2 Introduction of the Orthogonal Axes,[0],[0]
"In Example 5, e9:sponsoring and e10:resolve are the opinions of the West and the speaker, respectively; whether or not they are true depends on the au-
thors’ implications or the readers’ understandings, which is often beyond the scope of TempRel annotation.6 Example 6 demonstrates a similar situation for INTENTIONS: when reading the sentence of e11:report, people are inclined to believe that it is fulfilled.",2.3.2 Introduction of the Orthogonal Axes,[0],[0]
"But if we read the sentence of e12:report, we have reason to believe that it is not.",2.3.2 Introduction of the Orthogonal Axes,[0],[0]
"When it comes to e13:tell, it is unclear if everyone told the truth.",2.3.2 Introduction of the Orthogonal Axes,[0],[0]
"The existence of such examples indicates that orthogonal axes are a better modeling for INTENTIONS and OPINIONS.
",2.3.2 Introduction of the Orthogonal Axes,[0],[0]
Example 5: Opinion events may not always be true.,2.3.2 Introduction of the Orthogonal Axes,[0],[0]
He is ostracized by the West for (e9:sponsoring) terrorism.,2.3.2 Introduction of the Orthogonal Axes,[0],[0]
We need to (e10:resolve) the deep-seated causes that have resulted in these problems.,2.3.2 Introduction of the Orthogonal Axes,[0],[0]
Example 6: Intentions may not always be fulfilled.,2.3.2 Introduction of the Orthogonal Axes,[0],[0]
A passerby called the police to (e11:report) the body.,2.3.2 Introduction of the Orthogonal Axes,[0],[0]
A passerby called the police to (e12:report) the body.,2.3.2 Introduction of the Orthogonal Axes,[0],[0]
"Unfortunately, the line was busy.",2.3.2 Introduction of the Orthogonal Axes,[0],[0]
I asked everyone to (e13:tell) the truth.,2.3.2 Introduction of the Orthogonal Axes,[0],[0]
"Event modality have been discussed in many existing event annotation schemes, e.g., Event Nugget (Mitamura et al., 2015), Rich ERE (Song et al., 2015), and RED.",2.3.3 Differences from Factuality,[0],[0]
"Generally, an event is classified as Actual or Non-Actual, a.k.a. factuality (Saurı́ and Pustejovsky, 2009; Lee et al., 2015).
",2.3.3 Differences from Factuality,[0],[0]
"The main-axis events defined in this paper seem to be very similar to Actual events, but with several important differences: First, future events are Non-Actual because they indeed have not happened, but they may be on the main axis.",2.3.3 Differences from Factuality,[0],[0]
"Second, events that are not on the main axis can also be Actual events, e.g., intentions that are fulfilled, or opinions that are true.",2.3.3 Differences from Factuality,[0],[0]
"Third, as demonstrated by Examples 5-6, identifying anchorability as defined in Table 1 is relatively easy, but judging if an event actually happened is often a high-level understanding task that requires an understanding of the entire document or external knowledge.
",2.3.3 Differences from Factuality,[0],[0]
Interested readers are referred to Appendix B for a detailed analysis of the difference between Anchorable (onto the main axis) and Actual on a subset of RED.,2.3.3 Differences from Factuality,[0],[0]
"All existing annotation schemes adopt the interval representation of events (Allen, 1984) and there
6For instance, there is undoubtedly a causal link between e9:sponsoring and ostracized.
are 13 relations between two intervals (for readers who are not familiar with it, please see Fig. 4 in the appendix).",3 Interval Splitting,[0],[0]
"To reduce the burden of annotators, existing schemes often resort to a reduced set of the 13 relations.",3 Interval Splitting,[0],[0]
"For instance, Verhagen et al. (2007) merged all the overlap relations into a single relation, overlap.",3 Interval Splitting,[0],[0]
Bethard et al. (2007); Do et al. (2012); O’Gorman et al. (2016) all adopted this strategy.,3 Interval Splitting,[0],[0]
"In Cassidy et al. (2014), they further split overlap into includes, included and equal.
",3 Interval Splitting,[0],[0]
"Let [t1start, t1end] and [t 2 start, t 2 end] be the time intervals of two events (with the implicit assumption that tstart  tend).",3 Interval Splitting,[0],[0]
"Instead of reducing the relations between two intervals, we try to explicitly compare the time points (see Fig. 3).",3 Interval Splitting,[0],[0]
"In this way, the label set is simply before, after and equal,7 while the expressivity remains the same.",3 Interval Splitting,[0],[0]
"This interval splitting technique has also been used in (Raghavan et al., 2012).
",3 Interval Splitting,[0],[0]
"In addition to same expressivity, interval splitting can provide even more information when the relation between two events is vague.",3 Interval Splitting,[0],[0]
"In the conventional setting, imagine that the annotators find that the relation between two events can be either before or before and overlap.",3 Interval Splitting,[0],[0]
"Then the resulting annotation will have to be vague, although the annotators actually agree on the relation between t1start and t2start.",3 Interval Splitting,[0],[0]
"Using interval splitting, however, such information can be preserved.
",3 Interval Splitting,[0],[0]
An obvious downside of interval splitting is the increased number of annotations needed (4 point comparisons vs. 1 interval comparison).,3 Interval Splitting,[0],[0]
"In practice, however, it is usually much fewer than 4 comparisons.",3 Interval Splitting,[0],[0]
"For example, when we see t1end < t 2 start (as in Fig. 3), the other three can be skipped because they can all be inferred.",3 Interval Splitting,[0],[0]
"Moreover, although the number of annotations is increased, the work load for human annotators may still be the same, because even in the conventional scheme, they still need to think of the relations between start- and
7We will discuss vague in Sec. 4.
end-points before they can make a decision.",3 Interval Splitting,[0],[0]
"During our pilot annotation, the annotation quality dropped significantly when the annotators needed to reason about relations involving end-points of events.",3.1 Ambiguity of End-Points,[0],[0]
Table 2 shows four metrics of task difficulty when only t1start vs. t2start or t1end vs. t 2 end are annotated.,3.1 Ambiguity of End-Points,[0],[0]
Non-anchorable events were removed for both jobs.,3.1 Ambiguity of End-Points,[0],[0]
"The first two metrics, qualifying pass rate and survival rate are related to the two quality control protocols (see Sec. 4.1 for details).",3.1 Ambiguity of End-Points,[0],[0]
"We can see that when annotating the relations between end-points, only one out of ten crowdsourcers (11%) could successfully pass our qualifying test; and even if they had passed it, half of them (56%) would have been kicked out in the middle of the task.",3.1 Ambiguity of End-Points,[0],[0]
"The third line is the overall accuracy on gold set from all crowdsourcers (excluding those who did not pass the qualifying test), which drops from 67% to 37% when annotating end-end relations.",3.1 Ambiguity of End-Points,[0],[0]
The last line is the average response time per annotation and we can see that it takes much longer to label an end-end TempRel (52s) than a start-start TempRel (33s).,3.1 Ambiguity of End-Points,[0],[0]
"This important discovery indicates that the TempRels between end-points is probably governed by a different linguistic phenomenon.
",3.1 Ambiguity of End-Points,[0],[0]
We hypothesize that the difficulty is a mixture of how durative events are expressed (by authors) and perceived (by readers) in natural language.,3.1 Ambiguity of End-Points,[0],[0]
"In cognitive psychology, Coll-Florit and Gennari (2011) discovered that human readers take longer to perceive durative events than punctual events, e.g., owe 50 bucks vs. lost 50 bucks.",3.1 Ambiguity of End-Points,[0],[0]
"From the writer’s standpoint, durations are usually fuzzy (Schockaert and De Cock, 2008), or assumed to be a prior knowledge of readers (e.g., college takes 4 years and watching an NBA game takes a few hours), and thus not always written explicitly.",3.1 Ambiguity of End-Points,[0],[0]
"Given all these reasons, we ignore the comparison of end-points in this work, although event duration is indeed, another important task.",3.1 Ambiguity of End-Points,[0],[0]
"To summarize, with the proposed multi-axis modeling (Sec. 2) and interval splitting (Sec. 3), our annotation scheme is two-step.",4 Annotation Scheme Design,[0],[0]
"First, we mark every event candidate as being temporally Anchorable or not (based on the time axis we are working on).",4 Annotation Scheme Design,[0],[0]
"Second, we adopt the dense annotation scheme to label TempRels only between Anchorable events.",4 Annotation Scheme Design,[0],[0]
"Note that we only work on verb events in this paper, so non-verb event candidates are also deleted in a preprocessing step.",4 Annotation Scheme Design,[0],[0]
"We design crowdsourcing tasks for both steps and as we show later, high crowdsourcing quality was achieved on both tasks.",4 Annotation Scheme Design,[0],[0]
"In this section, we will discuss some practical issues.",4 Annotation Scheme Design,[0],[0]
We take advantage of the quality control feature in CrowdFlower in our crowdsourcing jobs.,4.1 Quality Control for Crowdsourcing,[0],[0]
"For any job, a set of examples are annotated by experts beforehand, which is considered gold and will serve two purposes.",4.1 Quality Control for Crowdsourcing,[0],[0]
(i) Qualifying test: Any crowdsourcer who wants to work on this job has to pass with 70% accuracy on 10 questions randomly selected from the gold set.,4.1 Quality Control for Crowdsourcing,[0],[0]
"(ii) Surviving test: During the annotation process, questions from the gold set will be randomly given to crowdsourcers without notice, and one has to maintain 70% accuracy on the gold set till the end of the annotation; otherwise, he or she will be forbidden from working on this job anymore and all his/her annotations will be discarded.",4.1 Quality Control for Crowdsourcing,[0],[0]
"At least 5 different annotators are required for every judgement and by default, the majority vote will be the final decision.",4.1 Quality Control for Crowdsourcing,[0],[0]
How to handle vague relations is another issue in temporal annotation.,4.2 Vague Relations,[0],[0]
"In non-dense schemes, annotators usually skip the annotation of a vague pair.",4.2 Vague Relations,[0],[0]
"In dense schemes, a majority agreement rule is applied as a postprocessing step to back off a decision to vague when annotators cannot pass a majority vote (Cassidy et al., 2014), which reminds us that annotators often label a vague relation as non-vague due to lack of thinking.
",4.2 Vague Relations,[0],[0]
We decide to proactively reduce the possibility of such situations.,4.2 Vague Relations,[0],[0]
"As mentioned earlier, our label set for t1start vs. t2start is before, after, equal and vague.",4.2 Vague Relations,[0],[0]
We ask two questions: Q1=Is it possible that t1start is before t2start?,4.2 Vague Relations,[0],[0]
Q2=Is it possible that t2start is before t1start?,4.2 Vague Relations,[0],[0]
"Let the an-
swers be A1 and A2.",4.2 Vague Relations,[0],[0]
"Then we have a oneto-one mapping as follows: A1=A2=yes7!vague, A1=A2=no7!equal, A1=yes, A2=no 7!before, and A1=no, A2=yes 7!after.",4.2 Vague Relations,[0],[0]
"An advantage is that one will be prompted to think about all possibilities, thus reducing the chance of overlook.
",4.2 Vague Relations,[0],[0]
"Finally, the annotation interface we used is shown in Appendix C.",4.2 Vague Relations,[0],[0]
"In this section, we first focus on annotations on the main axis, which is usually the primary storyline and thus has most events.",5 Corpus Statistics and Quality,[0],[0]
"Before launching the crowdsourcing tasks, we checked the IAA between two experts on a subset of TB-Dense (about 100 events and 400 relations).",5 Corpus Statistics and Quality,[0],[0]
A Cohen’s Kappa of .85 was achieved in the first step: anchorability annotation.,5 Corpus Statistics and Quality,[0],[0]
"Only those events that both experts labeled Anchorable were kept before they moved onto the second step: relation annotation, for which the Cohen’s Kappa was .90 for Q1 and .87 for Q2.",5 Corpus Statistics and Quality,[0],[0]
"Table 3 furthermore shows the distribution, Cohen’s Kappa, and F1 of each label.",5 Corpus Statistics and Quality,[0],[0]
"We can see the Kappa and F1 of vague (=.75, F1=.81) are generally lower than those of the other labels, confirming that temporal vagueness is a more difficult semantic phenomenon.",5 Corpus Statistics and Quality,[0],[0]
"Nevertheless, the overall IAA shown in Table 3 is a significant improvement compared to existing datasets.
",5 Corpus Statistics and Quality,[0],[0]
"With the improved IAA confirmed by experts, we sequentially launched the two-step crowdsourcing tasks through CrowdFlower on top of the same 36 documents of TB-Dense.",5 Corpus Statistics and Quality,[0],[0]
"To evaluate how well the crowdsourcers performed on our task, we calculate two quality metrics: accuracy on the gold set and the Worker Agreement with Aggregate (WAWA).",5 Corpus Statistics and Quality,[0],[0]
WAWA indicates the average number of crowdsourcers’ responses agreed with the aggregate answer (we used majority aggregation for each question).,5 Corpus Statistics and Quality,[0],[0]
"For example, if N individual responses were obtained in total, and n of them were correct when compared to the aggregate answer, then WAWA is simply n/N .",5 Corpus Statistics and Quality,[0],[0]
"In the first step,
crowdsourcers labeled 28% of the events as NonAnchorable to the main axis, with an accuracy on the gold of .86 and a WAWA of .79.
With Non-Anchorable events filtered, the relation annotation step was launched as another crowdsourcing task.",5 Corpus Statistics and Quality,[0],[0]
"The label distribution is b=.50, a=.28, e=.03, and v=.19 (consistent with Table 3).",5 Corpus Statistics and Quality,[0],[0]
"In Table 4, we show the annotation quality of this step using accuracy on the gold set and WAWA.",5 Corpus Statistics and Quality,[0],[0]
"We can see that the crowdsourcers achieved a very good performance on the gold set, indicating that they are consistent with the authors who created the gold set; these crowdsourcers also achieved a high-level agreement under the WAWA metric, indicating that they are consistent among themselves.",5 Corpus Statistics and Quality,[0],[0]
"These two metrics indicate that the annotation task is now well-defined and easy to understand even by non-experts.
",5 Corpus Statistics and Quality,[0],[0]
We continued to annotate INTENTION and OPINION which create orthogonal branches on the main axis.,5 Corpus Statistics and Quality,[0],[0]
"In the first step, crowdsourcers achieved an accuracy on gold of .82 and a WAWA of .89.",5 Corpus Statistics and Quality,[0],[0]
"Since only 16% of the events are in this category and these axes are usually very short (e.g., allocate funds to build a museum.), the annotation task is relatively small and two experts took the second step and achieved an agreement of .86 (F1).
",5 Corpus Statistics and Quality,[0],[0]
We name our new dataset MATRES for MultiAxis Temporal RElations for Start-points.,5 Corpus Statistics and Quality,[0],[0]
Each individual judgement cost us $0.01 and MATRES in total cost about $400 for 36 documents.,5 Corpus Statistics and Quality,[0],[0]
"To get another checkpoint of the quality of the new dataset, we compare with the annotations of TBDense.",5.1 Comparison to TB-Dense,[0],[0]
"TB-Dense has 1.1K verb events, between which 3.4K event-event (EE) relations are annotated.",5.1 Comparison to TB-Dense,[0],[0]
"In the new dataset, 72% of the events (0.8K) are anchored onto the main axis, resulting in 1.6K EE relations, and 16% (0.2K) are anchored onto orthogonal axes, resulting in 0.2K EE relations.
",5.1 Comparison to TB-Dense,[0],[0]
The following comparison is based on the 1.8K EE relations in common.,5.1 Comparison to TB-Dense,[0],[0]
"Moreover, since TB-Dense annotations are for intervals instead of start-points only, we converted TB-Dense’s interval relations to start-point relations (e.g., if A includes B, then tAstart is before tBstart).
",5.1 Comparison to TB-Dense,[0],[0]
The confusion matrix is shown in Table 5.,5.1 Comparison to TB-Dense,[0],[0]
"A few remarks about how to understand it: First, when TB-Dense labels before or after, MATRES also has a high-probability of having the same label (b=455/513=.89, a=309/438=.71); when MATRES labels vague, TB-Dense is also very likely to label vague (v=192/312=.62).",5.1 Comparison to TB-Dense,[0],[0]
This indicates the high agreement level between the two datasets if the interval- or point-based annotation difference is ruled out.,5.1 Comparison to TB-Dense,[0],[0]
"Second, many vague relations in TB-Dense are labeled as before, after or equal in MATRES.",5.1 Comparison to TB-Dense,[0],[0]
"This is expected because TB-Dense annotates relations between intervals, while MATRES annotates start-points.",5.1 Comparison to TB-Dense,[0],[0]
"When durative events are involved, the problem usually becomes more difficult and interval-based annotation is more likely to label vague (see earlier discussions in Sec. 3).",5.1 Comparison to TB-Dense,[0],[0]
"Example 7 shows three typical cases, where e14:became, e17:backed, e18:rose and e19:extending can be considered durative.",5.1 Comparison to TB-Dense,[0],[0]
"If only their start-points are considered, the crowdsourcers were correct in labeling e14 before e15, e16 after e17, and e18 equal to e19, although TBDense says vague for all of them.",5.1 Comparison to TB-Dense,[0],[0]
"Third, equal seems to be the relation that the two dataset mostly disagree on, which is probably due to crowdsourcers’ lack of understanding in time granularity and event coreference.",5.1 Comparison to TB-Dense,[0],[0]
"Although equal relations only constitutes a small portion in all relations, it needs further investigation.",5.1 Comparison to TB-Dense,[0],[0]
"We develop a baseline system for TempRel extraction on MATRES, assuming that all the events and axes are given.",6 Baseline System,[0],[0]
"The following commonly-
Example 7: Typical cases that TB-Dense annotated vague but MATRES annotated before, after, and equal, respectively.",6 Baseline System,[0],[0]
"At one point , when it (e14:became) clear controllers could not contact the plane, someone (e15:said) a prayer.",6 Baseline System,[0],[0]
"TB-Dense: vague; MATRES: before The US is bolstering its military presence in the gulf, as President Clinton (e16:discussed) the Iraq crisis with the one ally who has (e17:backed) his threat of force, British prime minister Tony Blair.",6 Baseline System,[0],[0]
TB-Dense: vague; MATRES: after Average hourly earnings of nonsupervisory employees (e18:rose) to $12.51.,6 Baseline System,[0],[0]
"The gain left wages 3.8 percent higher than a year earlier, (e19:extending) a trend that has given back to workers some of the earning power they lost to inflation in the last decade.",6 Baseline System,[0],[0]
"TB-Dense: vague; MATRES: equal
used features for each event pair are used: (i)",6 Baseline System,[0],[0]
The part-of-speech (POS) tags of each individual event and of its neighboring three words.,6 Baseline System,[0],[0]
(ii) The sentence and token distance between the two events.,6 Baseline System,[0],[0]
"(iii) The appearance of any modal verb between the two event mentions in text (i.e., will, would, can, could, may and might).",6 Baseline System,[0],[0]
"(iv) The appearance of any temporal connectives between the two event mentions (e.g., before, after and since).",6 Baseline System,[0],[0]
"(v) Whether the two verbs have a common synonym from their synsets in WordNet (Fellbaum, 1998).",6 Baseline System,[0],[0]
(vi) Whether the input event mentions have a common derivational form derived from WordNet.,6 Baseline System,[0],[0]
(vii),6 Baseline System,[0],[0]
"The head words of the preposition phrases that cover each event, respectively.",6 Baseline System,[0],[0]
"And (viii) event properties such as Aspect, Modality, and Polarity that come with the TimeBank dataset and are commonly used as features.
",6 Baseline System,[0],[0]
The proposed baseline system uses the averaged perceptron algorithm to classify the relation between each event pair into one of the four relation types.,6 Baseline System,[0],[0]
"We adopted the same train/dev/test split of TB-Dense, where there are 22 documents in train, 5 in dev, and 9 in test.",6 Baseline System,[0],[0]
"Parameters were tuned on the train-set to maximize its F1 on the dev-set, after which the classifier was retrained on the union of train and dev.",6 Baseline System,[0],[0]
A detailed analysis of the baseline system is provided in Table 6.,6 Baseline System,[0],[0]
"The performance on equal and vague is lower than on before and after, probably due to shortage in these labels in the training data and the inherent difficulty in event coreference and temporal vagueness.",6 Baseline System,[0],[0]
"We can see, though, that the overall performance on MATRES is much better than those in the literature for TempRel extraction, which used to be in the low 50’s (Chambers et al., 2014; Ning et al., 2017).",6 Baseline System,[0],[0]
"The same system was also retrained
and tested on the original annotations of TB-Dense (Line “Original”), which confirms the significant improvement if the proposed annotation scheme is used.",6 Baseline System,[0],[0]
"Note that we do not mean to say that the proposed baseline system itself is better than other existing algorithms, but rather that the proposed annotation scheme and the resulting dataset lead to better defined machine learning tasks.",6 Baseline System,[0],[0]
"In the future, more data can be collected and used with advanced techniques such as ILP (Do et al., 2012), structured learning (Ning et al., 2017) or multi-sieve (Chambers et al., 2014).",6 Baseline System,[0],[0]
"This paper proposes a new scheme for TempRel annotation between events, simplifying the task by focusing on a single time axis at a time.",7 Conclusion,[0],[0]
"We have also identified that end-points of events is a major source of confusion during annotation due to reasons beyond the scope of TempRel annotation, and proposed to focus on start-points only and handle the end-points issue in further investigation (e.g., in event duration annotation tasks).",7 Conclusion,[0],[0]
"Pilot study by expert annotators shows significant IAA improvements compared to literature values, indicating a better task definition under the proposed scheme.",7 Conclusion,[0],[0]
"This further enables the usage of crowdsourcing to collect a new dataset, MATRES, at a lower time cost.",7 Conclusion,[0],[0]
"Analysis shows that MATRES, albeit crowdsourced, has achieved a reasonably good agreement level, as confirmed by its performance on the gold set (agreement with the authors), the WAWA metric (agreement with the crowdsourcers themselves), and consistency with TB-Dense (agreement with an existing dataset).",7 Conclusion,[0],[0]
"Given the fact that existing schemes suffer from low IAAs and lack of data, we hope that the findings in this work would
provide a good start towards understanding more sophisticated semantic phenomena in this area.",7 Conclusion,[0],[0]
"We thank Martha Palmer, Tim O’Gorman, Mark Sammons and all the anonymous reviewers for providing insightful comments and critique in earlier stages of this work.",Acknowledgements,[0],[0]
"This research is supported in part by a grant from the Allen Institute for Artificial Intelligence (allenai.org); the IBM-ILLINOIS Center for Cognitive Computing Systems Research (C3SR) - a research collaboration as part of the IBM AI Horizons Network; by DARPA under agreement number FA8750-132-0008; and by the Army Research Laboratory (ARL) under agreement W911NF-09-2-0053 (the ARL Network Science CTA).
",Acknowledgements,[0],[0]
The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon.,Acknowledgements,[0],[0]
"The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA, of the Army Research Laboratory or the U.S. Government.",Acknowledgements,[0],[0]
"Any opinions, findings, conclusions or recommendations are those of the authors and do not necessarily reflect the view of the ARL.",Acknowledgements,[0],[0]
"Existing temporal relation (TempRel) annotation schemes often have low interannotator agreements (IAA) even between experts, suggesting that the current annotation task needs a better definition.",abstractText,[0],[0]
This paper proposes a new multi-axis modeling to better capture the temporal structure of events.,abstractText,[0],[0]
"In addition, we identify that event end-points are a major source of confusion in annotation, so we also propose to annotate TempRels based on start-points only.",abstractText,[0],[0]
A pilot expert annotation effort using the proposed scheme shows significant improvement in IAA from the conventional 60’s to 80’s (Cohen’s Kappa).,abstractText,[0],[0]
This better-defined annotation scheme further enables the use of crowdsourcing to alleviate the labor intensity for each annotator.,abstractText,[0],[0]
We hope that this work can foster more interesting studies towards event understanding.1,abstractText,[0],[0]
A Multi-Axis Annotation Scheme for Event Temporal Relations,title,[0],[0]
"Proceedings of NAACL-HLT 2013, pages 673–679, Atlanta, Georgia, 9–14 June 2013. c©2013 Association for Computational Linguistics",text,[0],[0]
"In language, stylistic variation is a reflection of various contextual factors, including the backgrounds of and relationship between the parties involved.",1 Introduction,[0],[0]
"Although in the context of prescriptive linguistics (Strunk and White, 1979), style is often assumed to be a matter of aesthetics, the stylistic intuitions of language users are inextricably linked to the conventions of register and genre (Biber and Conrad, 2009).",1 Introduction,[0],[0]
"Intentional or not, stylistic differences play a role in numerous NLP tasks.",1 Introduction,[0],[0]
"Examples include genre classification (Kessler et al., 1997), author profiling (Garera and Yarowsky, 2009; Rosenthal and McKeown, 2011), social relationship classification (Peterson et al., 2011), sentiment analysis (Wilson et al., 2005), readability classification (Collins-Thompson and Callan, 2005), and text generation (Hovy, 1990; Inkpen and Hirst, 2006).",1 Introduction,[0],[0]
"Following the classic work of Biber (1988), computational modeling of style has often focused on textual statistics and the frequency of function words and syntactic categories.",1 Introduction,[0],[0]
"When content words are considered, they are often limited to manually-constructed lists (Argamon
et al., 2007), or used as individual features for supervised classification, which can be confounded by topic (Petrenz and Webber, 2011) or fail in the face of lexical variety.",1 Introduction,[0],[0]
"Our interest is models that offer broad lexical coverage of human-identifiable stylistic variation.
",1 Introduction,[0],[0]
"Research most similar to ours has focused on classifying the lexicon in terms of individual aspects relevant to style (e.g. formality, specificity, readability) (Brooke et al., 2010; Pan and Yang, 2010; Kidwell et al., 2009) and a large body of research on the induction of polarity lexicons, in particular from large corpora (Turney, 2002; Kaji and Kitsuregawa, 2007; Velikovich et al., 2010).",1 Introduction,[0],[0]
"Our work is the first to represent multiple dimensions of style in a single statistical model, adapting latent Dirichlet allocation (Blei et al., 2003), a Bayesian ‘topic’ model, to our stylistic purposes; as such, our approach also follows on recent interest in the interpretability of topic-model topics (Chang et al., 2009; Newman et al., 2011).",1 Introduction,[0],[0]
"We show that our model can be used for acquisition of stylistic lexicons, and we also evaluate the model relative to theories of register variation and the expected stylistic character of particular genres.",1 Introduction,[0],[0]
"In English manuals of style and other prescriptivist texts (Fowler and Fowler, 1906; Gunning, 1952; Follett, 1966; Strunk and White, 1979; Kane, 1983; Hayakawa, 1994), writers are urged to pay attention to various aspects of lexical style, including elements such as clarity, familiarity, readability, for-
673
mality, fanciness, colloquialness, specificity, concreteness, objectivity, and naturalness; these stylistic categories reflect common aesthetic judgments about language.",2.1 Linguistic foundations,[0],[0]
"In descriptive studies of register, some researchers have posited a few fixed styles (Joos, 1961) or a small, discrete set of situational constraints which determine style and register (Crystal and Davy, 1969; Halliday and Hasan, 1976); by contrast, the applied approach of Biber (1988) and theoretical framework of Leckie-Tarry (1995) offer a more continuous interpretation of register variation.
",2.1 Linguistic foundations,[0],[0]
"In Biber’s approach, functional dimensions such as Involved vs. Informational, Argumentative vs. Non-argumentative, and Abstract vs. non-Abstract are derived in an unsupervised manner from a mixed-genre corpus, with the labels assigned depending on where features (a small set of known indicators of register) and genres fall on each spectrum.",2.1 Linguistic foundations,[0],[0]
"The theory of Leckie-Tarry posits a single main cline of register with one pole (the oral pole) reflecting a full reliance on the context of the linguistic situation, and the other (the literate pole) reflecting a reliance on cultural knowledge.",2.1 Linguistic foundations,[0],[0]
"The more specific elements of register are represented as subclines which are strongly influenced by this main cline, creating probabilistic relationships between related dimensions (Birch, 1995).
",2.1 Linguistic foundations,[0],[0]
"For the present study, we have chosen 3 dimensions (6 styles) which are clearly represented in the lexicon, which are discussed often in the relevant literature, and which fit well into the Leckie-Tarry conception of related subclines: colloquial vs. literary, concrete vs. abstract, and subjective vs. objective.",2.1 Linguistic foundations,[0],[0]
"In addition to a negative correlation between opposing styles, we also expect a positive correlation between stylistic aspects that tend toward the same main pole, situational (i.e. colloquial, concrete, subjective) or cultural (i.e. literary, abstract, objective).",2.1 Linguistic foundations,[0],[0]
These correlations can potentially interfere with accurate lexical acquisition.,2.1 Linguistic foundations,[0],[0]
"Our main model is an adaption of the popular latent Dirichlet allocation topic model (Blei et al., 2003), with each of the 6 styles corresponding to a topic.",2.2 Implementation,[0],[0]
"Briefly, latent Dirichlet allocation (LDA) is a generative Bayesian model: for each document d, a distribution of topics θd is drawn from a Dirichlet prior
(with parameter α).",2.2 Implementation,[0],[0]
"For each topic z, there is a probability distribution βz1 corresponding to the probability of that topic generating any given word in the vocabulary.",2.2 Implementation,[0],[0]
"Words in document d are generated by first selecting a topic z randomly according to θd , and then randomly selecting a word w according to βz.",2.2 Implementation,[0],[0]
"An extension of LDA, the correlated topic model (CTM) (Blei and Lafferty, 2007), supposes a more complex representation of topics: given a matrix Σ representing the covariance between topics and µ representing the means, for each document a topic distribution η (analogous to θ ) is drawn from the logistic normal distribution.",2.2 Implementation,[0],[0]
"Given a corpus, good estimates for the relevant parameters can be derived using Bayesian inference.
",2.2 Implementation,[0],[0]
For both LDA and CTM we use the original variational Bayes implementation of Blei.,2.2 Implementation,[0],[0]
"Variational Bayes (VB) works by approximating the true posterior with a simpler distribution, minimizing the Kullback-Leibler divergence between the two through iterative updates of specially-introduced free variables.",2.2 Implementation,[0],[0]
The mathematical and algorithmic details are omitted here; see Blei et al. (2003; 2007).,2.2 Implementation,[0],[0]
"Our early investigations used an online, batch version of LDA (Hoffman et al., 2010), which is more appropriate for large corpora because it requires only a single iteration over the dataset.",2.2 Implementation,[0],[0]
"We discovered, however, that batch models were markedly inferior to more traditional models for our purposes because the influence of the initial model diminishes too quickly; here, we need particular topics in the model to correspond to particular styles, and we accomplish this by seeding the model with known instances of each style (see Section 3).",2.2 Implementation,[0],[0]
"Specifically, our initial β consists of distributions where the entire probability mass is divided amongst the seeds for each corresponding topic, and a full iteration over the corpus occurs before β is updated.",2.2 Implementation,[0],[0]
"Typically, LDA iterates over the corpus until a convergence requirement is met, but in this case this is neither practical (due to the size of our corpus) nor necessarily desirable; the diminishing effects of the initial seeding means that the model may not stabilize, in terms of its likelihood, until after it has shifted away from our desired stylistic dimensions towards some other
1Some versions of LDA smooth this distribution using a Dirichlet prior; here, though, we use the original formulation from Blei (2003), which does not.
variation in the data.",2.2 Implementation,[0],[0]
"Therefore, we treat the optimal number of iterations as a variable to investigate.
",2.2 Implementation,[0],[0]
"The model is trained on a 1 million text portion of the 2009 version of the ICWSM Spinn3r dataset (Burton et al., 2009), a corpus of blogs we have previously used for formality lexicon induction (Brooke et al., 2010).",2.2 Implementation,[0],[0]
"Since our method relies on cooccurrence, we followed our earlier work in using only texts with at least 100 different word types.",2.2 Implementation,[0],[0]
"All words were tokenized and converted to lower-case, with no further lemmatization.",2.2 Implementation,[0],[0]
"Following Hoffman et al. (2010), we initialized the α of our models to 1/k where k is the number of topics.",2.2 Implementation,[0],[0]
Otherwise we used the default settings; when they overlap they were identical for the LDA and CTM models.,2.2 Implementation,[0],[0]
Our primary evaluation is based on the stylistic induction of held-out seed words.,3 Lexicon Induction,[0],[0]
The words were collected from various sources by the first author and further reviewed by the second; we are both native speakers of English with significant experience in English linguistics.,3 Lexicon Induction,[0],[0]
"Included words had to be clear, extreme members of their stylistic category, with little or no ambiguity with respect to their style.",3 Lexicon Induction,[0],[0]
"The colloquial seeds consist of English slang terms and acronyms, e.g. cuz, gig, asshole, lol.",3 Lexicon Induction,[0],[0]
"The literary seeds were primarily drawn from web sites which explain difficult language in texts such as the Bible and Lord of the Rings; examples include behold, resplendent, amiss, and thine.",3 Lexicon Induction,[0],[0]
"The concrete seeds all denote objects and actions strongly rooted in the physical world, e.g. shove and lamppost, while the abstract seeds all involve concepts which require significant human psychological or cultural knowledge to grasp, for instance patriotism and nonchalant.",3 Lexicon Induction,[0],[0]
"For our subjective seeds, we used an edited list of strongly positive and negative terms from a manually-constructed sentiment lexicon (Taboada et al., 2011), e.g. gorgeous and depraved, and for our objective set we selected words from sets of nearsynonyms where one was clearly an emotionallydistant alternative, e.g. residence (for home), jocular (for funny) and communicable (for contagious).",3 Lexicon Induction,[0],[0]
"We filtered initial lists to 150 of each type, removing words which did not appear in the corpus or which occurred in multiple lists.",3 Lexicon Induction,[0],[0]
"For evaluation we
used stratified 3-fold crossvalidation, averaged over 5 different (3-way) splits of the seeds, with the same splits used for all evaluated conditions.
",3 Lexicon Induction,[0],[0]
"Given two sets of opposing seeds, we follow our earlier work in evaluating our performance in terms of the number of pairings of seeds from each set which have the expected stylistic relationship relative to each other (the guessing baseline is 0.5).",3 Lexicon Induction,[0],[0]
"Given a word w and two opposing styles (topics) p and n, we place w on the PN dimension according to the β of our trained model as follows:
PNw = βpw−βnw βpw",3 Lexicon Induction,[0],[0]
"+βnw
The normalization is important because otherwise more-common words would tend to have higher PN’s, when in fact the opposite is true (rare words tend to be more stylistically prominent).",3 Lexicon Induction,[0],[0]
"We then calculate pairwise accuracy as the percentage of pairs 〈wp,wn〉 (wp ∈ Pseeds and wn ∈ Nseeds) where PNwp > PNwn .",3 Lexicon Induction,[0],[0]
"However, this metric does not address the case where the degree of a word in one stylistic dimension is overestimated because of its status on a parallel dimension.",3 Lexicon Induction,[0],[0]
"Two more-holistic alternatives are total accuracy, the percentage of seeds for which the highest βtw is the topic t for which w is a seed (guessing baseline is 0.17), and the average rank of the correct t as ordered by βtw (in the range 1–6, guessing baseline is 3.5); the latter is more forgiving of near misses.
",3 Lexicon Induction,[0],[0]
We tested a few options which involved straightforward modifications to model training.,3 Lexicon Induction,[0],[0]
"Standard LDA produces all tokens in the document, but when dealing with style rather than topic, the number of times a word appears is much less relevant (Brooke et al., 2010).",3 Lexicon Induction,[0],[0]
"Our binary model assumes an LDA that generates types, not tokens.2 A key comparison
2At the theoretical level, this move is admittedly problematic, since our LDA model is thus being trained under the assumption that texts with multiple instances of the same type can be generated, when of course such texts cannot by definition exist.",3 Lexicon Induction,[0],[0]
"We might address this by moving to Bayesian models with very different generative assumptions, e.g. the spherical topic model (Reisinger et al., 2010), but these methods involve a significant increase of computational complexity and we believe that on a practical level there are no real negatives associated with directly using a binary representation as input to LDA; in fact, we are avoiding what appears to be a much more serious problem, burstiness (Doyle and Elkan, 2009), i.e. the fact that
here is with a combined LDA model (combo), an amalgamation of three independently trained 2-topic models, one for each dimension; this tests our key hypothesis that training dimensions of style together is beneficial.",3 Lexicon Induction,[0],[0]
"Finally, we test against the correlated topic model (CTM), which offers an explicit representation of style correlation, but which has done poorly with respect to interpretability, despite offering better perplexity (Chang et al., 2009).
",3 Lexicon Induction,[0],[0]
The results of the lexicon induction evaluation are in Table 1.,3 Lexicon Induction,[0],[0]
"Since the number of optimal iterations varies, we report the result from the best of the first five iterations, as measured by total accuracy; the best iteration is shown in parenthesis.",3 Lexicon Induction,[0],[0]
"In general, all the results are high enough—we are reliably above 90% for the pairwise task, and above 50% for the 6-way task—for us to conclude with some confidence that our model is capturing a significant amount of stylistic variation.",3 Lexicon Induction,[0],[0]
"As predicted, using words as boolean features had a net positive gain, consistent across all of our metrics, though this effect was not as marked as we have seen previously.",3 Lexicon Induction,[0],[0]
"The model with independent training of each dimension (combo) did noticeably worse, supporting our conclusion that a multidimensional approach is warranted here.",3 Lexicon Induction,[0],[0]
"Particularly striking is the much larger drop in overall accuracy as compared to pairwise accuracy, which suggests that the combo model is capturing the general trends but not distinguishing correlated styles as well.",3 Lexicon Induction,[0],[0]
"However, the most complex model, the CTM, actually does slightly worse than the combo, which was contrary to our expectations but nonetheless consistent with previous work on the interpretability of topic models.",3 Lexicon Induction,[0],[0]
"The performance of the full LDA models benefited from a second itera-
traditional LDA is influenced too much by multiple instances of the same word.
tion, but this was not true of combo LDA or CTM, and the performance of all models dropped after the second iteration.
",3 Lexicon Induction,[0],[0]
"An analysis of individual errors reveals, unsurprisingly, that most of the errors occur across styles on the same pole; by far the largest single common misclassification is objective words to abstract.",3 Lexicon Induction,[0],[0]
"Of the words that consistently show this misclassification across the runs, many of them, e.g. animate, aperture, encircle, and constrain are clearly errors (if anything, these words tend towards concreteness), but in other cases the word in question is arguably also fairly abstract, e.g. categorize and predominant, and might not be labeled an error at all.",3 Lexicon Induction,[0],[0]
"Other signs that our model might be doing better than our total accuracy metric gives it credit for: many of the subjective words that are consistently mislabeled as literary have an exaggerated, literary feel, e.g. jubilant, grievous, and malevolent.",3 Lexicon Induction,[0],[0]
Our secondary analysis involved evaluating the θ ’s of our best configuration (based on average pairwise and total accuracy) on other texts.,4 Text-level Analysis,[0],[0]
"After training, we carried out inference on the BNC corpus, averaging the resulting θ ’s to see which styles are associated with which genres.",4 Text-level Analysis,[0],[0]
Appearances of the seed terms for each model were disregarded during this process; only the induced part of the lexicon was used.,4 Text-level Analysis,[0],[0]
"The average differences relative to the mean across the various stylistic dimensions (as measured by the probabilities in θ ) are given for a selection of genres in Table 2.
",4 Text-level Analysis,[0],[0]
"The most obvious pattern in table 2 is the dominance of the medium: all written genres are positive for our styles on the ‘cultural’ pole and negative for styles on the ‘situational’ pole and the opposite is
true for spoken genres.",4 Text-level Analysis,[0],[0]
"The magnitude of this effect is more difficult to interpret: though it is clear why fiction should sit on the boundary (since it contains spoken dialogue), the appearance of news at the written extreme is odd, though it might be due to the fact that news blogs are the most prevalent formal genre in the training corpus.
",4 Text-level Analysis,[0],[0]
"However, if we ignore magnitude and focus on the relative ratios of the stylistic differences for styles on the same pole, we can identify some individual stylistic effects among genres within the same medium.",4 Text-level Analysis,[0],[0]
"Relative to the other written genres, for instance, fiction is, sensibly, more literary and much less objective, while academic texts are much more abstract and objective; for the other two written genres, the spread is more even, though relative to religious texts, news is more objective.",4 Text-level Analysis,[0],[0]
"At the situational pole, fiction also stands out, being much more colloquial and concrete than other written genres.",4 Text-level Analysis,[0],[0]
"Predictably, if we consider again the ratios across styles, conversation is the most colloquial genre here, though the difference is subtle.
",4 Text-level Analysis,[0],[0]
"We carried out a correlation analysis of the LDAreduced styles of all texts in the BNC and, consistent with the genre results in Table 2, found a strong positive correlation for all styles on the same main pole, averaging 0.83.",4 Text-level Analysis,[0],[0]
"The average negative correlation between opposing poles is even higher, −0.88.",4 Text-level Analysis,[0],[0]
This supports the Leckie-Tarry formulation.,4 Text-level Analysis,[0],[0]
"The independence assumptions of the LDA model did not prevent strong correlations from forming between these distinct yet clearly interrelated dimensions; if anything, the correlations are stronger than we would have predicted.",4 Text-level Analysis,[0],[0]
We have introduced a Bayesian model of stylistic variation.,5 Conclusion,[0],[0]
"Topic models like LDA are often evaluated using information-theoretic measures, but our emphasis has been on interpretibility: at the word level we can use the model to induce stylistic lexicons which correspond to human judgement, and at the text level we can use it distinguish genres in expected ways.",5 Conclusion,[0],[0]
"Another theme has been to offer evidence that indeed a multi-dimensional approach is strongly warranted: importantly, our results indicate that separate unidimensional models of style are inferior for identifying the core stylistic character of each word, and in our secondary analysis we found strong correlations among styles attributable to the situational/cultural dichotomy.",5 Conclusion,[0],[0]
"However, an off-theshelf model that integrates correlation among topics did not outperform basic LDA.
",5 Conclusion,[0],[0]
"One advantage of a Bayesian approach is in the flexibility of the model: there are any number of other interesting possible extensions at both the θ and β levels of the model, including alternative approaches to correlation (Li and McCallum, 2006).",5 Conclusion,[0],[0]
"Beyond Bayesian models, vector space and graphical approaches should be compared.",5 Conclusion,[0],[0]
"More work is clearly needed to improve evaluation: some of our seeds could fall into multiple stylistic categories, so a more detailed annotation would be useful.",5 Conclusion,[0],[0]
This work was financially supported by the Natural Sciences and Engineering Research Council of Canada.,Acknowledgements,[0],[0]
"We adapt the popular LDA topic model (Blei et al., 2003) to the representation of stylistic lexical information, evaluating our model on the basis of human-interpretability at the word and text level.",abstractText,[0],[0]
"We show, in particular, that this model can be applied to the task of inducing stylistic lexicons, and that a multi-dimensional approach is warranted given the correlations among stylistic dimensions.",abstractText,[0],[0]
A Multi-Dimensional Bayesian Approach to Lexical Style,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 799–809 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
799",text,[0],[0]
"When we use supervised learning to solve Natural Language Processing (NLP) problems, we typically train an individual model for each task with task-specific labeled data.",1 Introduction,[0],[0]
"However, our target task may be intrinsically linked to other tasks.",1 Introduction,[0],[0]
"For example, Part-of-speech (POS) tagging and Name Tagging can both be considered as sequence labeling; Machine Translation (MT) and Abstractive Text Summarization both require the ability to understand the source text and generate natural language sentences.",1 Introduction,[0],[0]
"Therefore, it is valuable to transfer knowledge from related tasks to the target task.",1 Introduction,[0],[0]
"Multi-task Learning (MTL) is one of
∗*",1 Introduction,[0],[0]
"Part of this work was done when the first author was on an internship at Facebook.
1The code of our model is available at https://github.",1 Introduction,[0],[0]
"com/limteng-rpi/mlmt
the most effective solutions for knowledge transfer across tasks.",1 Introduction,[0],[0]
"In the context of neural network architectures, we usually perform MTL by sharing parameters across models (Ruder, 2017).
",1 Introduction,[0],[0]
"Previous studies (Collobert and Weston, 2008; Dong et al., 2015; Luong et al., 2016; Liu et al., 2018; Yang et al., 2017) have proven that MTL is an effective approach to boost the performance of related tasks such as MT and parsing.",1 Introduction,[0],[0]
"However, most of these previous efforts focused on tasks and languages which have sufficient labeled data but hit a performance ceiling on each task alone.",1 Introduction,[0],[0]
"Most NLP tasks, including some well-studied ones such as POS tagging, still suffer from the lack of training data for many low-resource languages.",1 Introduction,[0],[0]
"According to Ethnologue2, there are 7, 099 living languages in the world.",1 Introduction,[0],[0]
"It is an unattainable goal to annotate data in all languages, especially for tasks with complicated annotation requirements.",1 Introduction,[0],[0]
"Furthermore, some special applications (e.g., disaster response and recovery) require rapid development of NLP systems for extremely low-resource languages.",1 Introduction,[0],[0]
"Therefore, in this paper, we concentrate on enhancing supervised models in low-resource settings by borrowing knowledge learned from related high-resource languages and tasks.
",1 Introduction,[0],[0]
"In (Yang et al., 2017), the authors simulated a low-resource setting for English and Spanish by downsampling the training data for the target task.",1 Introduction,[0],[0]
"However, for most low-resource languages, the data sparsity problem also lies in related tasks and languages.",1 Introduction,[0],[0]
"Under such circumstances, a single transfer model can only bring limited improvement.",1 Introduction,[0],[0]
"To tackle this issue, we propose a multi-lingual multi-task architecture which combines different transfer models within a unified architecture through two levels of parameter sharing.",1 Introduction,[0],[0]
"In the first level, we share character embeddings,
2https://www.ethnologue.com/guides/ how-many-languages
character-level convolutional neural networks, and word-level long-short term memory layer across all models.",1 Introduction,[0],[0]
These components serve as a basis to connect multiple models and transfer universal knowledge among them.,1 Introduction,[0],[0]
"In the second level, we adopt different sharing strategies for different transfer schemes.",1 Introduction,[0],[0]
"For example, we use the same output layer for all Name Tagging tasks to share task-specific knowledge (e.g., I-PER3 should not be assigned to the first word in a sentence).
",1 Introduction,[0],[0]
"To illustrate our idea, we take sequence labeling as a case study.",1 Introduction,[0],[0]
"In the NLP context, the goal of sequence labeling is to assign a categorical label (e.g., POS tag) to each token in a sentence.",1 Introduction,[0],[0]
"It underlies a range of fundamental NLP tasks, including POS Tagging, Name Tagging, and chunking.
",1 Introduction,[0],[0]
Experiments show that our model can effectively transfer various types of knowledge from different auxiliary tasks and obtains up to 50.5% absolute F-score gains on Name Tagging compared to the mono-lingual single-task baseline.,1 Introduction,[0],[0]
"Additionally, our approach does not rely on a large amount of auxiliary task data to achieve the improvement.",1 Introduction,[0],[0]
"Using merely 1% auxiliary data, we already obtain up to 9.7% absolute gains in Fscore.",1 Introduction,[0],[0]
The goal of sequence labeling is to assign a categorical label to each token in a given sentence.,2.1 Basic Architecture,[0],[0]
"Though traditional methods such as Hidden Markov Models (HMMs) and Conditional Random Fields (CRFs) (Lafferty et al., 2001; Ratinov and Roth, 2009; Passos et al., 2014) achieved high performance on sequence labeling tasks, they typically relied on hand-crafted features, therefore it is difficult to adapt them to new tasks or languages.",2.1 Basic Architecture,[0],[0]
"To avoid task-specific engineering, (Collobert et al., 2011) proposed a feed-forward neural network model that only requires word embeddings trained on a large scale corpus as features.",2.1 Basic Architecture,[0],[0]
"After that, several neural models based on the combination of long-short term memory (LSTM) and CRFs (Ma and Hovy, 2016; Lample et al., 2016; Chiu and Nichols, 2016) were proposed and
3We adopt the BIOES annotation scheme.",2.1 Basic Architecture,[0],[0]
"Prefixes B-, I, E-, and S- represent the beginning of a mention, inside of a mention, the end of a mention and a single-token mention respectively.",2.1 Basic Architecture,[0],[0]
"The O tag is assigned to a word which is not part of any mention.
achieved better performance on sequence labeling tasks.
",2.1 Basic Architecture,[0],[0]
"LSTM-CRFs-based models are well-suited for multi-lingual multi-task learning for three reasons: (1) They learn features from word and character embeddings and therefore require little feature engineering; (2) As the input and output of each layer in a neural network are abstracted as vectors, it is fairly straightforward to share components between neural models; (3) Character embeddings can serve as a bridge to transfer morphological and semantic information between languages with identical or similar scripts, without requiring cross-lingual dictionaries or parallel sentences.
",2.1 Basic Architecture,[0],[0]
"Therefore, we design our multi-task multilingual architecture based on the LSTM-CNNs model proposed in (Chiu and Nichols, 2016).",2.1 Basic Architecture,[0],[0]
The overall framework is illustrated in Figure 1.,2.1 Basic Architecture,[0],[0]
"First, each word wi is represented as the combination xi of two parts, word embedding and character feature vector, which is extracted from character embeddings of the characters in wi using convolutional neural networks (CharCNN).",2.1 Basic Architecture,[0],[0]
"On top of that, a bidirectional LSTM processes the sequence x = {x1, x2, ...} in both directions and encodes each word and its context into a fixed-size vector hi.",2.1 Basic Architecture,[0],[0]
"Next, a linear layer converts hi to a score vector yi, in which each component represents the predicted score of a target tag.",2.1 Basic Architecture,[0],[0]
"In order to model correlations between tags, a CRFs layer is added at the top to generate the best tagging path for the whole sequence.",2.1 Basic Architecture,[0],[0]
"In the CRFs layer, given an input sentence x of length L and the output of the linear layer y, the score of a sequence of tags z is
defined as:
S(x,y, z) = L∑
t=1
(Azt−1,zt + yt,zt),
where A is a transition matrix in which Ap,q represents the binary score of transitioning from tag p to tag q, and yt,z represents the unary score of assigning tag z to the t-th word.",2.1 Basic Architecture,[0],[0]
"Given the ground truth sequence of tags z, we maximize the following objective function during the training phase:
O = logP (z|x)",2.1 Basic Architecture,[0],[0]
"= S(x,y, z)− log ∑ z̃∈Z eS(x,y,z̃),
where Z is the set of all possible tagging paths.",2.1 Basic Architecture,[0],[0]
We emphasize that our actual implementation differs slightly from the LSTM-CNNs model.,2.1 Basic Architecture,[0],[0]
"We do not use additional word- and characterlevel explicit symbolic features (e.g., capitalization and lexicon) as they may require additional language-specific knowledge.",2.1 Basic Architecture,[0],[0]
"Additionally, we transform character feature vectors using highway networks (Srivastava et al., 2015), which is reported to enhance the overall performance by (Kim et al., 2016) and (Liu et al., 2018).",2.1 Basic Architecture,[0],[0]
Highway networks is a type of neural network that can smoothly switch its behavior between transforming and carrying information.,2.1 Basic Architecture,[0],[0]
"MTL can be employed to improve performance on multiple tasks at the same time, such as MT and parsing in (Luong et al., 2016).",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"However, in our scenario, we only focused on enhancing the performance of a low-resource task, which is our target task or main task.",2.2 Multi-task Multi-lingual Architecture,[0],[0]
Our proposed architecture aims to transfer knowledge from a set of auxiliary tasks to the main task.,2.2 Multi-task Multi-lingual Architecture,[0],[0]
"For simplicity, we refer to a model of a main (auxiliary) task as a main (auxiliary) model.
",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"To jointly train multiple models, we perform multi-task learning using parameter sharing.",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"Let Θi be the set of parameters for model mi and Θi,j = Θi ∩Θj be the shared parameters between mi and mj .",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"When optimizing model mi, we update Θi and hence Θi,j .",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"In this way, we can partially train model mj as Θi,j ⊆ Θj .",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"Previously, each MTL model generally uses a single transfer scheme.",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"In order to merge different transfer models into a unified architecture, we employ two levels of parameter sharing as follows.
",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"On the first level, we construct the basis of the architecture by sharing character embeddings, CharCNN and bidirectional LSTM among all models.",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"This level of parameter sharing aims to provide universal word representation and feature extraction capability for all tasks and languages.
",2.2 Multi-task Multi-lingual Architecture,[0],[0]
Character Embeddings and Character-level CNNs.,2.2 Multi-task Multi-lingual Architecture,[0],[0]
"Character features can represent morphological and semantic information; e.g., the English morpheme dis- usually indicates negation and reversal as in “disagree” and “disapproval”.",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"For low-resource languages lacking in data to suffice the training of high-quality word embeddings, character embeddings learned from other languages may provide crucial information for labeling, especially for rare and out-of-vocabulary words.",2.2 Multi-task Multi-lingual Architecture,[0],[0]
Take the English word “overflying” (flying over) as an example.,2.2 Multi-task Multi-lingual Architecture,[0],[0]
"Even if it is rare or absent in the corpus, we can still infer the word meaning from its suffix over- (above), root fly, and prefix -ing (present participle form).",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"In our architecture, we share character embeddings and the CharCNN between languages with identical or similar scripts to enhance word representation for low-resource languages.
",2.2 Multi-task Multi-lingual Architecture,[0],[0]
Bidirectional LSTM.,2.2 Multi-task Multi-lingual Architecture,[0],[0]
"The bidirectional LSTM layer is essential to extract character, word, and contextual information from a sentence.",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"However, with a large number of parameters, it cannot be fully trained only using the low-resource task data.",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"To tackle this issue, we share the bidirectional LSTM layer across all models.",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"Bear in mind that because our architecture does not require aligned cross-lingual word embeddings, sharing this layer across languages may confuse the model as it equally handles embeddings in different spaces.",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"Nevertheless, under low-resource circumstances, data sparsity is the most critical factor that affects the performance.
",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"On top of this basis, we adopt different parameter sharing strategies for different transfer schemes.",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"For cross-task transfer, we use the same word embedding matrix across tasks so that they can mutually enhance word representations.",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"For cross-lingual transfer, we share the linear layer and CRFs layer among languages to transfer taskspecific knowledge, such as the transition score between two tags.
",2.2 Multi-task Multi-lingual Architecture,[0],[0]
Word Embeddings.,2.2 Multi-task Multi-lingual Architecture,[0],[0]
"For most words, in addition to character embeddings, word embeddings are still crucial to represent semantic informa-
tion.",2.2 Multi-task Multi-lingual Architecture,[0],[0]
We use the same word embedding matrix for tasks in the same language.,2.2 Multi-task Multi-lingual Architecture,[0],[0]
The matrix is initialized with pre-trained embeddings and optimized as parameters during training.,2.2 Multi-task Multi-lingual Architecture,[0],[0]
"Thus, task-specific knowledge can be encoded into the word embeddings by one task and subsequently utilized by another one.",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"For a low-resource language even without sufficient raw text, we mix its data with a related high-resource language to train word embeddings.",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"In this way, we merge both corpora and hence their vocabularies.
",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"Recently, Conneau et al. (2017) proposed a domain-adversarial method to align two monolingual word embedding matrices without crosslingual supervision such as a bilingual dictionary.",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"Although cross-lingual word embeddings are not required, we evaluate our framework with aligned embeddings generated using this method.",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"Experiment results show that the incorporation of crosslingual embeddings substantially boosts the performance under low-resource settings.
",2.2 Multi-task Multi-lingual Architecture,[0],[0]
Linear Layer and CRFs.,2.2 Multi-task Multi-lingual Architecture,[0],[0]
"As the tag set varies from task to task, the linear layer and CRFs can only be shared across languages.",2.2 Multi-task Multi-lingual Architecture,[0],[0]
We share these layers to transfer task-specific knowledge to the main model.,2.2 Multi-task Multi-lingual Architecture,[0],[0]
"For example, our model corrects [SPER Charles]",2.2 Multi-task Multi-lingual Architecture,[0],[0]
[S-PER Picqué] to [B-PER Charles],2.2 Multi-task Multi-lingual Architecture,[0],[0]
[E-PER Picqué] because the CRFs layer fully trained on other languages assigns a low score to the rare transition S-PER→S-PER and promotes B-PER→E-PER.,2.2 Multi-task Multi-lingual Architecture,[0],[0]
"In addition to the shared linear layer, we add an unshared language-specific linear layer to allow the model to behave differently
toward some features for different languages.",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"For example, the suffix -ment usually indicates nouns in English whereas indicates adverbs in French.
",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"We combine the output of the shared linear layer yu and the output of the language-specific linear layer ys using:
y = g ⊙ ys",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"+ (1− g)⊙ yu,
where g = σ(W gh + bg).",2.2 Multi-task Multi-lingual Architecture,[0],[0]
W g,2.2 Multi-task Multi-lingual Architecture,[0],[0]
and bg are optimized during training.,2.2 Multi-task Multi-lingual Architecture,[0],[0]
h is the LSTM hidden states.,2.2 Multi-task Multi-lingual Architecture,[0],[0]
"As W g is a square matrix, y, ys, and yu have the same dimension.",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"Although we only focus on sequence labeling in this work, our architecture can be adapted for many NLP tasks with slight modification.",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"For example, for text classification tasks, we can take the last hidden state of the forward LSTM as the sentence representation and replace the CRFs layer with a Softmax layer.
",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"In our model, each task has a separate object function.",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"To optimize multiple tasks within one model, we adopt the alternating training approach in (Luong et al., 2016).",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"At each training step, we sample a task di with probability ri∑
j rj , where ri
is the mixing rate value assigned to di.",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"In our experiments, instead of tuning ri, we estimate it by:
ri = µiζi √ Ni ,
where µi is the task coefficient, ζi is the language coefficient, and Ni is the number of training examples.",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"µi (or ζi) takes the value 1 if the task
(or language) of di is the same as that of the target task; Otherwise it takes the value 0.1.",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"For example, given English Name Tagging as the target task, the task coefficient µ and language coefficient ζ of Spanish Name Tagging are 0.1 and 1 respectively.
",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"While assigning lower mixing rate values to auxiliary tasks, this formula also takes the amount of data into consideration.",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"Thus, auxiliary tasks receive higher probabilities to reduce overfitting when we have a smaller amount of main task data.",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"For Name Tagging, we use the following data sets: Dutch (NLD) and Spanish (ESP) data from the CoNLL 2002 shared task (Tjong Kim Sang, 2002), English (ENG) data from the CoNLL 2003 shared task (Tjong Kim Sang and De Meulder, 2003), Russian (RUS) data from LDC2016E95 (Russian Representative Language Pack), and Chechen (CHE) data from TAC KBP 2017 10-Language EDL Pilot Evaluation Source Corpus4.",3.1 Data Sets,[0],[0]
"We select Chechen as another target language in addition to Dutch and Spanish because it is a truly under-resourced language and its related language, Russian, also lacks NLP resources.
",3.1 Data Sets,[0],[0]
"For POS Tagging, we use English, Dutch, Spanish, and Russian data from the CoNLL 2017 shared task (Zeman et al., 2017; Nivre et al., 2017).",3.1 Data Sets,[0],[0]
"In this data set, each token is annotated with two POS tags, UPOS (universal POS tag) and XPOS (language-specific POS tag).",3.1 Data Sets,[0],[0]
We use UPOS because it is consistent throughout all languages.,3.1 Data Sets,[0],[0]
We use 50-dimensional pre-trained word embeddings and 50-dimensional randomly initialized character embeddings.,3.2 Experimental Setup,[0],[0]
We train word embeddings using the word2vec package5.,3.2 Experimental Setup,[0],[0]
"English, Span-
4https://tac.nist.gov/2017/KBP/data.html 5https://github.com/tmikolov/word2vec
ish, and Dutch embeddings are trained on corresponding Wikipedia articles (2017-12-20 dumps).",3.2 Experimental Setup,[0],[0]
Russian embeddings are trained on documents in LDC2016E95.,3.2 Experimental Setup,[0],[0]
Chechen embeddings are trained on documents in TAC KBP 2017 10-Language EDL Pilot Evaluation Source Corpus.,3.2 Experimental Setup,[0],[0]
"To learn a mapping between mono-lingual word embeddings and obtain cross-lingual embeddings, we use the unsupervised model in the MUSE library6 (Conneau et al., 2017).",3.2 Experimental Setup,[0],[0]
"Although word embeddings are fine-tuned during training, we update the embedding matrix in a sparse way and thus do not have to update a large number of parameters.
",3.2 Experimental Setup,[0],[0]
"We optimize parameters using Stochastic Gradient Descent with momentum, gradient clipping and exponential learning rate decay.",3.2 Experimental Setup,[0],[0]
"At step t, the learning rate αt is updated using αt = α0 ∗ ρt/T , where α0 is the initial learning rate, ρ is the decay rate, and T is the decay step.7 To reduce overfitting, we apply Dropout (Srivastava et al., 2014) to the output of the LSTM layer.
",3.2 Experimental Setup,[0],[0]
"We conduct hyper-parameter optimization by exploring the space of parameters shown in Table 2 using random search (Bergstra and Bengio, 2012).",3.2 Experimental Setup,[0],[0]
"Due to time constraints, we only perform parameter sweeping on the Dutch Name Tagging task with 200 training examples.",3.2 Experimental Setup,[0],[0]
We select the set of parameters that achieves the best performance on the development set and apply it to all models.,3.2 Experimental Setup,[0],[0]
"In Figure 3, 4, and 5, we compare our model with the mono-lingual single-task LSTM-CNNs model (denoted as baseline), cross-task transfer model, and cross-lingual transfer model in low-resource settings with Dutch, Spanish, and Chechen Name Tagging as the main task respectively.",3.3 Comparison of Different Models,[0],[0]
"We use English as the related language for Dutch and Spanish, and use Russian as the related language for
6https://github.com/facebookresearch/MUSE 7Momentum β, gradient clipping threshold, ρ, and T are
set to 0.9, 5.0, 0.9, and 10000 in the experiments.
Chechen.",3.3 Comparison of Different Models,[0],[0]
"For cross-task transfer, we take POS Tagging as the auxiliary task.",3.3 Comparison of Different Models,[0],[0]
"Because the CoNLL 2017 data does not include Chechen, we only use Russian POS Tagging and Russian Name Tagging as auxiliary tasks for Chechen Name Tagging.
",3.3 Comparison of Different Models,[0],[0]
We take Name Tagging as the target task for three reasons: (1) POS Tagging has a much lower requirement for the amount of training data.,3.3 Comparison of Different Models,[0],[0]
"For example, using only 10 training sentences, our baseline model achieves 75.5% and 82.9% prediction accuracy on Dutch and Spanish; (2) Compared to POS Tagging, Name Tagging has been considered as a more challenging task; (3) Existing POS Tagging resources are relatively richer than Name Tagging ones; e.g., the CoNLL 2017 data set provides POS Tagging training data for 45 languages.",3.3 Comparison of Different Models,[0],[0]
"Name Tagging also has a higher annotation cost as its annotation guidelines are usually more complicated.
",3.3 Comparison of Different Models,[0],[0]
We can see that our model substantially outperforms the mono-lingual single-task baseline model and obtains visible gains over single transfer models.,3.3 Comparison of Different Models,[0],[0]
"When trained with less than 50 main tasks training sentences, cross-lingual transfer consistently surpasses cross-task transfer, which is not surprising because in the latter scheme, the linear layer and CRFs layer of the main model are not shared with other models and thus cannot be fully trained with little data.
",3.3 Comparison of Different Models,[0],[0]
"Because there are only 20,400 sentences in Chechen documents, we also experiment with the data augmentation method described in Section 2.2 by training word embeddings on a mixture of Russian and Chechen data.",3.3 Comparison of Different Models,[0],[0]
This method yields additional 3.5%-10.0% absolute F-score gains.,3.3 Comparison of Different Models,[0],[0]
We also experiment with transferring from English to Chechen.,3.3 Comparison of Different Models,[0],[0]
"Because Chechen uses Cyrillic alphabet , we convert its data set to Latin script.",3.3 Comparison of Different Models,[0],[0]
"Surprisingly, although these two languages are not close, we get more improvement by using English as the auxiliary language.
",3.3 Comparison of Different Models,[0],[0]
"In Table 3, we compare our model with state-ofthe-art models using all Dutch or Spanish Name Tagging data.",3.3 Comparison of Different Models,[0],[0]
"Results show that although we design this architecture for low-resource settings, it also achieves good performance in high-resource settings.",3.3 Comparison of Different Models,[0],[0]
"In this experiment, with sufficient training data for the target task, we perform another round of parameter sweeping.",3.3 Comparison of Different Models,[0],[0]
We increase the embedding sizes and LSTM hidden state size to 100 and 225 respectively.,3.3 Comparison of Different Models,[0],[0]
"In Table 4, we compare Name Tagging results from the baseline model and our model, both trained with 100 main task sentences.
",3.4 Qualitative Analysis,[0],[0]
"The first three examples show that shared character-level networks can transfer different levels of morphological and semantic information.
",3.4 Qualitative Analysis,[0],[0]
"In example #1, the baseline model fails to identify “Palestijnen”, an unseen word in the Dutch data, while our model can recognize it because the shared CharCNN represents it in a way similar to its corresponding English word “Palestinians”, which occurs 20 times.",3.4 Qualitative Analysis,[0],[0]
"In addition to mentions, the shared CharCNN can also improve representations of context words, such as “staat” (state) in the example.",3.4 Qualitative Analysis,[0],[0]
"For some words dissimilar to corresponding English words, the CharCNN may enhance their word representations by transferring morpheme-level knowledge.",3.4 Qualitative Analysis,[0],[0]
"For example, in sentence #2, our model is able to identify “Rusland” (Russia) as the suffix -land is usually associated with location names in the English data; e.g., Finland.",3.4 Qualitative Analysis,[0],[0]
"Furthermore, the CharCNN is capable of capturing some word-level patterns, such as capitalized hyphenated compound and acronym as example #3 shows.",3.4 Qualitative Analysis,[0],[0]
"In this sentence, neither “PMScentra” nor “MST” can be found in auxiliary task data, while we observe a number of similar expressions, such as American-style and LDP.
",3.4 Qualitative Analysis,[0],[0]
The transferred knowledge also helps reduce overfitting.,3.4 Qualitative Analysis,[0],[0]
"For example, in sentence #4, the baseline model mistakenly tags “sección” (section) and “consellerı́a” (department) as organizations because their capitalized forms usually appear in Spanish organization names.",3.4 Qualitative Analysis,[0],[0]
"With knowledge learned in auxiliary tasks that a lowercased word is rarely tagged as a proper noun, our model is able to avoid overfitting and correct these errors.",3.4 Qualitative Analysis,[0],[0]
"Sentence #5 shows an opposite situation, where the capitalized word “campesinos” (farm worker) never appears in Spanish names.
",3.4 Qualitative Analysis,[0],[0]
"In Table 5, we show differences between cross-
lingual transfer and cross-task transfer.",3.4 Qualitative Analysis,[0],[0]
"Although the cross-task transfer model recognizes “Ingeborg Marx” missed by the baseline model, it mistakenly assigns an S-PER tag to “Marx”.",3.4 Qualitative Analysis,[0],[0]
"Instead, from English Name Tagging, the cross-lingual transfer model borrows task-specific knowledge through the shared CRFs layer that (1) B-PER→SPER is an invalid transition, and (2) even if we assign S-PER to “Ingeborg”, it is rare to have continuous person names without any conjunction or punctuation.",3.4 Qualitative Analysis,[0],[0]
"Thus, the cross-lingual model promotes the sequence B-PER→E-PER.
",3.4 Qualitative Analysis,[0],[0]
"In Figure 6, we depict the change of tag distribution with the number of training sentences.",3.4 Qualitative Analysis,[0],[0]
"When trained with less than 100 sentences, the baseline model only correctly predicts a few tags dominated by frequent types.",3.4 Qualitative Analysis,[0],[0]
"By contrast, our model has a visibly higher recall and better predicts infrequent tags, which can be attributed to the implicit data augmentation and inductive bias introduced by MTL (Ruder, 2017).",3.4 Qualitative Analysis,[0],[0]
"For example, if all location names in the Dutch training data are single-token ones, the baseline model will inevitably overfit to the tag S-LOC and possibly label “Caldera de Taburiente” as [S-LOC Caldera]",3.4 Qualitative Analysis,[0],[0]
[S-LOC de],3.4 Qualitative Analysis,[0],[0]
"[S-LOC Taburiente], whereas with the shared CRFs layer fully trained on English Name Tagging, our model prefers B-LOC→I-LOC→ELOC, which receives a higher transition score.",3.4 Qualitative Analysis,[0],[0]
"In order to quantify the contributions of individual components, we conduct ablation studies on Dutch Name Tagging with different numbers of training sentences for the target task.",3.5 Ablation Studies,[0],[0]
"For the basic model, we we use separate LSTM layers and
model (B), and result of our model (A).",3.5 Ablation Studies,[0],[0]
"The GREEN ( RED ) highlight indicates a correct (incorrect) tag.
remove the character embeddings, highway networks, language-specific layer, and",3.5 Ablation Studies,[0],[0]
Dropout layer.,3.5 Ablation Studies,[0],[0]
"As Table 6 shows, adding each component usually enhances the performance (F-score, %), while the impact also depends on the size of the target task data.",3.5 Ablation Studies,[0],[0]
"For example, the language-specific layer slightly impairs the performance with only 10 training sentences.",3.5 Ablation Studies,[0],[0]
"However, this is unsurpris-
ing as it introduces additional parameters that are only trained by the target task data.",3.5 Ablation Studies,[0],[0]
"For many low-resource languages, their related languages are also low-resource.",3.6 Effect of the Amount of Auxiliary Task Data,[0],[0]
"To evaluate our model’s sensitivity to the amount of auxiliary task data, we fix the size of main task data and downsample all auxiliary task data with sample rates from 1% to 50%.",3.6 Effect of the Amount of Auxiliary Task Data,[0],[0]
"As Figure 7 shows, the performance goes up when we raise the sample rate from
1% to 20%.",3.6 Effect of the Amount of Auxiliary Task Data,[0],[0]
"However, we do not observe significant improvement when we further increase the sample rate.",3.6 Effect of the Amount of Auxiliary Task Data,[0],[0]
"By comparing scores in Figure 3 and Figure 7, we can see that using only 1% auxiliary data, our model already obtains 3.7%-9.7% absolute F-score gains.",3.6 Effect of the Amount of Auxiliary Task Data,[0],[0]
"Due to space limitations, we only show curves for Dutch Name Tagging, while we observe similar results on other tasks.",3.6 Effect of the Amount of Auxiliary Task Data,[0],[0]
"Therefore, we may conclude that our model does not heavily rely on the amount of auxiliary task data.",3.6 Effect of the Amount of Auxiliary Task Data,[0],[0]
"Multi-task Learning has been applied in different NLP areas, such as machine translation (Luong et al., 2016; Dong et al., 2015; Domhan and Hieber, 2017), text classification (Liu et al., 2017), dependency parsing (Peng et al., 2017), textual entailment (Hashimoto et al., 2017), text summarization (Isonuma et al., 2017) and sequence labeling (Collobert and Weston, 2008; Søgaard and Goldberg, 2016; Rei, 2017; Peng and Dredze, 2017; Yang et al., 2017; von Däniken and Cieliebak, 2017; Aguilar et al., 2017; Liu et al., 2018)
Collobert and Weston (2008) is an early attempt that applies MTL to sequence labeling.",4 Related Work,[0],[0]
"The authors train a CNN model jointly on POS Tagging, Semantic Role Labeling, Name Tagging, chunking, and language modeling using parameter sharing.",4 Related Work,[0],[0]
"Instead of using other sequence labeling tasks, Rei (2017) and Liu et al. (2018) take language modeling as the secondary training objective to extract semantic and syntactic knowledge from large scale raw text without additional supervision.",4 Related Work,[0],[0]
"In (Yang et al., 2017), the authors propose three transfer models for crossdomain, cross-application, and cross-lingual trans-
fer for sequence labeling, and also simulate a lowresource setting by downsampling the training data.",4 Related Work,[0],[0]
"By contrast, we combine cross-task transfer and cross-lingual transfer within a unified architecture to transfer different types of knowledge from multiple auxiliary tasks simultaneously.",4 Related Work,[0],[0]
"In addition, because our model is designed for lowresource settings, we share components among models in a different way (e.g., the LSTM layer is shared across all models).",4 Related Work,[0],[0]
"Differing from most MTL models, which perform supervisions for all tasks on the outermost layer, (Søgaard and Goldberg, 2016) proposes an MTL model which supervised tasks at different levels.",4 Related Work,[0],[0]
It shows that supervising low-level tasks such as POS Tagging at lower layer obtains better performance.,4 Related Work,[0],[0]
We design a multi-lingual multi-task architecture for low-resource settings.,5 Conclusions and Future Work,[0],[0]
We evaluate the model on sequence labeling tasks with three language pairs.,5 Conclusions and Future Work,[0],[0]
Experiments show that our model can effectively transfer different types of knowledge to improve the main model.,5 Conclusions and Future Work,[0],[0]
"It substantially outperforms the mono-lingual single-task baseline model, cross-lingual transfer model, and crosstask transfer model.
",5 Conclusions and Future Work,[0],[0]
"The next step of this research is to apply this architecture to other types of tasks, such as Event Extract and Semantic Role Labeling that involve structure prediction.",5 Conclusions and Future Work,[0],[0]
We also plan to explore the possibility of integrating incremental learning into this architecture to adapt a trained model for new tasks rapidly.,5 Conclusions and Future Work,[0],[0]
This work was supported by the U.S. DARPA LORELEI Program No. HR0011-15-C-0115 and U.S. ARL NS-CTA No.,Acknowledgments,[0],[0]
W911NF-09-2-0053.,Acknowledgments,[0],[0]
"The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Government.",Acknowledgments,[0],[0]
The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on.,Acknowledgments,[0],[0]
We propose a multi-lingual multi-task architecture to develop supervised models with a minimal amount of labeled data for sequence labeling.,abstractText,[0],[0]
"In this new architecture, we combine various transfer models using two layers of parameter sharing.",abstractText,[0],[0]
"On the first layer, we construct the basis of the architecture to provide universal word representation and feature extraction capability for all models.",abstractText,[0],[0]
"On the second level, we adopt different parameter sharing strategies for different transfer schemes.",abstractText,[0],[0]
"This architecture proves to be particularly effective for low-resource settings, when there are less than 200 training sentences for the target task.",abstractText,[0],[0]
"Using Name Tagging as a target task, our approach achieved 4.3%-50.5% absolute Fscore gains compared to the mono-lingual single-task baseline model.",abstractText,[0],[0]
1,abstractText,[0],[0]
A Multi-lingual Multi-task Architecture for Low-resource Sequence Labeling,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 758–763 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
758",text,[0],[0]
"Sentiment classification is an important task of natural language processing (NLP), aiming to classify the sentiment polarity of a given text as positive, negative, or more fine-grained classes.",1 Introduction,[0],[0]
"It has obtained considerable attention due to its broad applications in natural language processing (Hao et al., 2012; Gui et al., 2017).",1 Introduction,[0],[0]
"Most existing studies set up sentiment classifiers using supervised machine learning approaches, such as support vector machine (SVM) (Pang et al., 2002), convolutional neural network (CNN) (Kim, 2014; Bonggun et al., 2017), long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997; Qian et al., 2017), Tree-LSTM (Tai et al., 2015), and attention-based methods (Zhou et al., 2016; Yang et al., 2016; Lin et al., 2017; Du et al., 2017).
",1 Introduction,[0],[0]
"Despite the remarkable progress made by the
previous work, we argue that sentiment analysis still remains a challenge.",1 Introduction,[0],[0]
"Sentiment resources including sentiment lexicon, negation words, intensity words play a crucial role in traditional sentiment classification approaches (Maks and Vossen, 2012; Duyu et al., 2014).",1 Introduction,[0],[0]
"Despite its usefulness, to date, the sentiment linguistic knowledge has been underutilized in most recent deep neural network models (e.g., CNNs and LSTMs).
",1 Introduction,[0],[0]
"In this work, we propose a Multi-sentimentresource Enhanced Attention Network (MEAN) for sentence-level sentiment classification to integrate many kinds of sentiment linguistic knowledge into deep neural networks via multi-path attention mechanism.",1 Introduction,[0],[0]
"Specifically, we first design a coupled word embedding module to model the word representation from character-level and word-level semantics.",1 Introduction,[0],[0]
This can help to capture the morphological information such as prefixes and suffixes of words.,1 Introduction,[0],[0]
"Then, we propose a multisentiment-resource attention module to learn more comprehensive and meaningful sentiment-specific sentence representation by using the three types of sentiment resource words as attention sources attending to the context words respectively.",1 Introduction,[0],[0]
"In this way, we can attend to different sentimentrelevant information from different representation subspaces implied by different types of sentiment sources and capture the overall semantics of the sentiment, negation and intensity words for sentiment prediction.
",1 Introduction,[0],[0]
The main contributions of this paper are summarized as follows.,1 Introduction,[0],[0]
"First, we design a coupled word embedding obtained from character-level embedding and word-level embedding to capture both the character-level morphological information and word-level semantics.",1 Introduction,[0],[0]
"Second, we propose a multi-sentiment-resource attention module to learn more comprehensive sentiment-specific sentence representation from multiply subspaces
implied by three kinds of sentiment resources including sentiment lexicon, intensity words, negation words.",1 Introduction,[0],[0]
"Finally, the experimental results show that MEAN consistently outperforms competitive methods.",1 Introduction,[0],[0]
"Our proposed MEAN model consists of three key components: coupled word embedding module, multi-sentiment-resource attention module, sentence classifier module.",2 Model,[0],[0]
"In the rest of this section, we will elaborate these three parts in details.",2 Model,[0],[0]
"To exploit the sentiment-related morphological information implied by some prefixes and suffixes of words (such as “Non-”, “In-”, “Im-”), we design a coupled word embedding learned from character-level embedding and word-level embedding.",2.1 Coupled Word Embedding,[0],[0]
"We first design a character-level convolution neural network (Char-CNN) to obtain characterlevel embedding (Zhang et al., 2015).",2.1 Coupled Word Embedding,[0],[0]
"Different from (Zhang et al., 2015), the designed CharCNN is a fully convolutional network without max-pooling layer to capture better semantic information in character chunk.",2.1 Coupled Word Embedding,[0],[0]
"Specifically, we first input one-hot-encoding character sequences to a 1 × 1 convolution layer to enhance the semantic nonlinear representation ability of our model (Long et al., 2015), and the output is then fed into a multi-gram (i.e. different window sizes) convolution layer to capture different local character chunk information.",2.1 Coupled Word Embedding,[0],[0]
"For word-level embedding, we use pre-trained word vectors, GloVe (Pennington et al., 2014), to map each word to a lowdimensional vector space.",2.1 Coupled Word Embedding,[0],[0]
"Finally, each word is represented as a concatenation of the characterlevel embedding and word-level embedding.",2.1 Coupled Word Embedding,[0],[0]
"This is performed on the context words and the three types of sentiment resource words 1, resulting in four final coupled word embedding matrices: the W c =",2.1 Coupled Word Embedding,[0],[0]
"[wc1, ..., w c t ] ∈",2.1 Coupled Word Embedding,[0],[0]
"Rd×t for context words, the W s =",2.1 Coupled Word Embedding,[0],[0]
"[ws1, ..., w s m] ∈ Rd×m for sentiment words, the W i =",2.1 Coupled Word Embedding,[0],[0]
"[wi1, ..., w i k] ∈ Rd×k for intensity words, the Wn = [wn1 , ..., w n p ] ∈",2.1 Coupled Word Embedding,[0],[0]
Rd×p for negation words.,2.1 Coupled Word Embedding,[0],[0]
"Here, t,m, k, p are the length of the corresponding items respectively, and d is the embedding dimension.",2.1 Coupled Word Embedding,[0],[0]
"Each W is normalized to better calculate the following word correlation.
",2.1 Coupled Word Embedding,[0],[0]
"1To be precise, sentiment resource words include sentiment words, negation words and intensity words.",2.1 Coupled Word Embedding,[0],[0]
"After obtaining the coupled word embedding, we propose a multi-sentiment-resource attention mechanism to help select the crucial sentimentresource-relevant context words to build the sentiment-specific sentence representation.",2.2 Multi-sentiment-resource Attention Module,[0],[0]
"Concretely, we use the three kinds of sentiment resource words as attention sources to attend to the context words respectively, which is beneficial to capture different sentiment-relevant context words corresponding to different types of sentiment sources.",2.2 Multi-sentiment-resource Attention Module,[0],[0]
"For example, using sentiment words as attention source attending to the context words helps form the sentiment-word-enhanced sentence representation.",2.2 Multi-sentiment-resource Attention Module,[0],[0]
"Then, we combine the three kinds of sentiment-resource-enhanced sentence representations to learn the final sentiment-specific sentence representation.",2.2 Multi-sentiment-resource Attention Module,[0],[0]
"We design three types of attention mechanisms: sentiment attention, intensity attention, negation attention to model the three kinds of sentiment resources, respectively.",2.2 Multi-sentiment-resource Attention Module,[0],[0]
"In the following, we will elaborate the three types of attention mechanisms in details.
",2.2 Multi-sentiment-resource Attention Module,[0],[0]
"First, inspired by (Xiong et al.), we expect to establish the word-level relationship between the context words and different kinds of sentiment resource words.",2.2 Multi-sentiment-resource Attention Module,[0],[0]
"To be specific, we define the dot products among the context words and the three kinds of sentiment resource words as correlation matrices.",2.2 Multi-sentiment-resource Attention Module,[0],[0]
"Mathematically, the detailed formulation is described as follows.
",2.2 Multi-sentiment-resource Attention Module,[0],[0]
M s = (W c)T ·W s ∈ Rt×m (1) M i =,2.2 Multi-sentiment-resource Attention Module,[0],[0]
(W c)T ·W i ∈ Rt×k (2),2.2 Multi-sentiment-resource Attention Module,[0],[0]
"Mn = (W c)T ·Wn ∈ Rt×p (3)
whereM s,M i,",2.2 Multi-sentiment-resource Attention Module,[0],[0]
"Mn are the correlation matrices to measure the relationship among the context words and the three kinds of sentiment resource words, representing the relevance between the context words and the sentiment resource word.
",2.2 Multi-sentiment-resource Attention Module,[0],[0]
"After obtaining the correlation matrices, we can compute the sentiment-resource-relevant context word representations Xcs ,",2.2 Multi-sentiment-resource Attention Module,[0],[0]
"X c i , X c n",2.2 Multi-sentiment-resource Attention Module,[0],[0]
by the dot products among the context words and different types of corresponding correlation matrices.,2.2 Multi-sentiment-resource Attention Module,[0],[0]
"Meanwhile, we can also obtain the context-wordrelevant sentiment word representation matrix Xs by the dot product between the correlation matrix M s and the sentiment words W s, the context-
word-relevant intensity word representation matrix Xi by the dot product between the intensity words W i and the correlation matrix M i, the context-word-relevant negation word representation matrix Xn by the dot product between the negation words Wn and the correlation matrix Mn.",2.2 Multi-sentiment-resource Attention Module,[0],[0]
"The detailed formulas are presented as follows:
Xcs = W cM s, Xs = W s(M",2.2 Multi-sentiment-resource Attention Module,[0],[0]
s)T (4) Xci = W cM,2.2 Multi-sentiment-resource Attention Module,[0],[0]
"i, Xi = W i(M i)T (5) Xcn = W cMn, Xn = Wn(Mn)T (6)
",2.2 Multi-sentiment-resource Attention Module,[0],[0]
"The final enhanced context word representation matrix is computed as:
Xc = Xcs",2.2 Multi-sentiment-resource Attention Module,[0],[0]
"+X c i +X c n. (7)
",2.2 Multi-sentiment-resource Attention Module,[0],[0]
"Next, we employ four independent GRU networks (Chung et al., 2015) to encode hidden states of the context words and the three types of sentiment resource words, respectively.",2.2 Multi-sentiment-resource Attention Module,[0],[0]
"Formally, given the word embedding Xc, Xs, Xi, Xn, the hidden state matrices Hc, Hs, H i, Hn can be obtained as follows:
Hc = GRU(Xc) (8)
Hs = GRU(Xs) (9)
H i = GRU(Xi) (10)
",2.2 Multi-sentiment-resource Attention Module,[0],[0]
"Hn = GRU(Xn) (11)
",2.2 Multi-sentiment-resource Attention Module,[0],[0]
"After obtaining the hidden state matrices, the sentiment-word-enhanced sentence representation o1 can be computed as:
o1 = t∑ i=1",2.2 Multi-sentiment-resource Attention Module,[0],[0]
"αih c i , q s = m∑ i=1",2.2 Multi-sentiment-resource Attention Module,[0],[0]
hsi/m (12) β([hci ; qs]),2.2 Multi-sentiment-resource Attention Module,[0],[0]
= u T s tanh(Ws[h c,2.2 Multi-sentiment-resource Attention Module,[0],[0]
i ; qs]) (13) αi = exp(β([hci ; qs]))∑t i=1,2.2 Multi-sentiment-resource Attention Module,[0],[0]
exp(β([h,2.2 Multi-sentiment-resource Attention Module,[0],[0]
c i ; qs])),2.2 Multi-sentiment-resource Attention Module,[0],[0]
"(14)
where qs denotes the mean-pooling operation towards Hs, β is the attention function that calculates the importance of the i-th word hci in the context and αi indicates the importance of the ith word in the context, us and Ws are learnable parameters.
",2.2 Multi-sentiment-resource Attention Module,[0],[0]
"Similarly, with the hidden states H i and Hn for the intensity words and the negation words as attention sources, we can obtain the intensityword-enhanced sentence representation o2 and the
negation-word-enhanced sentence representation o3.",2.2 Multi-sentiment-resource Attention Module,[0],[0]
"The final comprehensive sentiment-specific sentence representation õ is the composition of the above three sentiment-resource-specific sentence representations o1, o2, o3:
õ",2.2 Multi-sentiment-resource Attention Module,[0],[0]
"= [o1, o2, o3] (15)",2.2 Multi-sentiment-resource Attention Module,[0],[0]
"After obtaining the final sentence representation õ, we feed it to a softmax layer to predict the sentiment label distribution of a sentence:
ŷ = exp(W̃o T õ + b̃o)∑C
i=1",2.3 Sentence Classifier,[0],[0]
exp(W̃o,2.3 Sentence Classifier,[0],[0]
"T õ + b̃o)
(16)
where ŷ is the predicted sentiment distribution of the sentence, C is the number of sentiment labels, W̃o and b̃o are parameters to be learned.
",2.3 Sentence Classifier,[0],[0]
"For model training, our goal is to minimize the cross entropy between the ground truth and predicted results for all sentences.",2.3 Sentence Classifier,[0],[0]
"Meanwhile, in order to avoid overfitting, we use dropout strategy to randomly omit parts of the parameters on each training case.",2.3 Sentence Classifier,[0],[0]
"Inspired by (Lin et al., 2017), we also design a penalization term to ensure the diversity of semantics from different sentiment-resourcespecific sentence representations, which reduces information redundancy from different sentiment resources attention.",2.3 Sentence Classifier,[0],[0]
"Specifically, the final loss function is presented as follows:
L(ŷ, y) =",2.3 Sentence Classifier,[0],[0]
− N∑ i=1,2.3 Sentence Classifier,[0],[0]
C∑,2.3 Sentence Classifier,[0],[0]
j=1,2.3 Sentence Classifier,[0],[0]
yji log(ŷ j i ) + λ,2.3 Sentence Classifier,[0],[0]
"( ∑ θ∈Θ θ2)
(17)
+ µ||ÕÕT",2.3 Sentence Classifier,[0],[0]
"− ψI||2F Õ =[o1; o2; o3] (18)
where yji is the target sentiment distribution of the sentence, ŷji is the prediction probabilities, θ denotes each parameter to be regularized, Θ is parameter set, λ is the coefficient for L2 regularization, µ is a hyper-parameter to balance the three terms, ψ is the weight parameter, I denotes the the identity matrix and ||.||F denotes the Frobenius norm of a matrix.",2.3 Sentence Classifier,[0],[0]
"Here, the first two terms of the loss function are cross-entropy function of the predicted and true distributions and L2 regularization respectively, and the final term is a penalization term to encourage the diversity of sentiment sources.",2.3 Sentence Classifier,[0],[0]
Movie Review (MR)2 and Stanford Sentiment Treebank (SST)3 are used to evaluate our model.,3.1 Datasets and Sentiment Resources,[0],[0]
"MR dataset has 5,331 positive samples and 5,331 negative samples.",3.1 Datasets and Sentiment Resources,[0],[0]
"We adopt the same data split as in (Qian et al., 2017).",3.1 Datasets and Sentiment Resources,[0],[0]
"SST consists of 8,545 training samples, 1,101 validation samples, 2210 test samples.",3.1 Datasets and Sentiment Resources,[0],[0]
"Each sample is marked as very negative, negative, neutral, positive, or very positive.",3.1 Datasets and Sentiment Resources,[0],[0]
"Sentiment lexicon combines the sentiment words from both (Qian et al., 2017) and (Hu and Liu, 2004), resulting in 10,899 sentiment words in total.",3.1 Datasets and Sentiment Resources,[0],[0]
We collect negation and intensity words manually as the number of these words is limited.,3.1 Datasets and Sentiment Resources,[0],[0]
"In order to comprehensively evaluate the performance of our model, we list several baselines for sentence-level sentiment classification.
RNTN:",3.2 Baselines,[0],[0]
"Recursive Tensor Neural Network (Socher et al., 2013) is used to model correlations between different dimensions of child nodes vectors.
",3.2 Baselines,[0],[0]
LSTM/Bi-LSTM:,3.2 Baselines,[0],[0]
"Cho et al. (2014) employs Long Short-Term Memory and the bidirectional variant to capture sequential information.
",3.2 Baselines,[0],[0]
Tree-LSTM:,3.2 Baselines,[0],[0]
"Memory cells was introduced by Tree-Structured Long Short-Term Memory (Tai et al., 2015) and gates into tree-structured neural network, which is beneficial to capture semantic relatedness by parsing syntax trees.
",3.2 Baselines,[0],[0]
"CNN: Convolutional Neural Networks (Kim, 2014) is applied to generate task-specific sentence representation.
",3.2 Baselines,[0],[0]
NCSL:,3.2 Baselines,[0],[0]
"Teng et al. (2016) designs a Neural Context-Sensitive Lexicon (NSCL) to obtain prior sentiment scores of words in the sentence.
",3.2 Baselines,[0],[0]
LR-Bi-LSTM:,3.2 Baselines,[0],[0]
"Qian et al. (2017) imposes linguistic roles into neural networks by applying linguistic regularization on intermediate outputs with KL divergence.
",3.2 Baselines,[0],[0]
"Self-attention: Lin et al. (2017) proposes a selfattention mechanism to learn structured sentence embedding.
2http://www.cs.cornell.edu/people/ pabo/movie-review-data/
3https://nlp.stanford.edu/sentiment/we train the model on both phrases and sentences but only test on sentences
ID-LSTM: (Tianyang et al., 2018) uses reinforcement learning to learn structured sentence representation for sentiment classification.",3.2 Baselines,[0],[0]
"In our experiments, the dimensions of characterlevel embedding and word embedding (GloVe) are both set to 300.",3.3 Implementation Details,[0],[0]
"Kernel sizes of multi-gram convolution for Char-CNN are set to 2, 3, respectively.",3.3 Implementation Details,[0],[0]
"All the weight matrices are initialized as random orthogonal matrices, and we set all the bias vectors as zero vectors.",3.3 Implementation Details,[0],[0]
"We optimize the proposed model with RMSprop algorithm, using mini-batch training.",3.3 Implementation Details,[0],[0]
The size of mini-batch is 60.,3.3 Implementation Details,[0],[0]
"The dropout rate is 0.5, and the coefficient λ of L2 normalization is set to 10−5. µ is set to 10−4. ψ is set to 0.9.",3.3 Implementation Details,[0],[0]
"When there are not sentiment resource words in the sentences, all the context words are treated as sentiment resource words to implement the multi-path self-attention strategy.",3.3 Implementation Details,[0],[0]
"In our experiments, to be consistent with the recent baseline methods, we adopt classification accuracy as evaluation metric.",3.4 Experiment Results,[0],[0]
We summarize the experimental results in Table 1.,3.4 Experiment Results,[0],[0]
Our model has robust superiority over competitors and sets stateof-the-art on MR and SST datasets.,3.4 Experiment Results,[0],[0]
"First, our model brings a substantial improvement over the methods that do not leverage sentiment linguistic knowledge (e.g., RNTN, LSTM, BiLSTM, CNN and ID-LSTM) on both datasets.",3.4 Experiment Results,[0],[0]
This verifies the effectiveness of leveraging sentiment linguistic resource with the deep learning algorithms.,3.4 Experiment Results,[0],[0]
"Second, our model also consistently outperforms LR-Bi-LSTM which integrates linguistic roles of sentiment, negation and intensity words into neural networks via the linguistic regularization.",3.4 Experiment Results,[0],[0]
"For example, our model achieves 2.4% improvements over the MR dataset and 0.8% improvements over the SST dataset compared to LR-Bi-LSTM.",3.4 Experiment Results,[0],[0]
"This is because that MEAN designs attention mechanisms to leverage sentiment resources efficiently, which utilizes the interactive information between context words and sentiment resource words.
",3.4 Experiment Results,[0],[0]
"In order to analyze the effectiveness of each component of MEAN, we also report the ablation test in terms of discarding character-level embedding (denoted as MEAN w/o CharCNN) and sentiment words/negation words/intensity words (denoted as MEAN w/o sentiment words/negation words/intensity words).",3.4 Experiment Results,[0],[0]
"All the tested factors con-
tribute greatly to the improvement of the MEAN.",3.4 Experiment Results,[0],[0]
"In particular, the accuracy decreases sharply when discarding the sentiment words.",3.4 Experiment Results,[0],[0]
This is within our expectation since sentiment words are vital when classifying the polarity of the sentences.,3.4 Experiment Results,[0],[0]
"In this paper, we propose a novel Multi-sentimentresource Enhanced Attention Network (MEAN) to enhance the performance of sentence-level sentiment analysis, which integrates the sentiment linguistic knowledge into the deep neural network.",4 Conclusion,[0],[0]
This work was supported in part by the Research Fund for the development of strategic emerging industries by ShenZhen city (No.JCYJ20160301151844537 and No. JCYJ20160331104524983).,Acknowledgements,[0],[0]
Deep learning approaches for sentiment classification do not fully exploit sentiment linguistic knowledge.,abstractText,[0],[0]
"In this paper, we propose a Multi-sentiment-resource Enhanced Attention Network (MEAN) to alleviate the problem by integrating three kinds of sentiment linguistic knowledge (e.g., sentiment lexicon, negation words, intensity words) into the deep neural network via attention mechanisms.",abstractText,[0],[0]
"By using various types of sentiment resources, MEAN utilizes sentiment-relevant information from different representation subspaces, which makes it more effective to capture the overall semantics of the sentiment, negation and intensity words for sentiment prediction.",abstractText,[0],[0]
The experimental results demonstrate that MEAN has robust superiority over strong competitors.,abstractText,[0],[0]
A Multi-sentiment-resource Enhanced Attention Network for Sentiment Classification,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 884–895 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1082",text,[0],[0]
"What does it mean to be welcoming or standoffish, light-hearted or cynical?",1 Introduction,[0],[0]
"Such interactional styles are performed primarily with language, yet little is known about how linguistic resources are arrayed to create these social impressions.",1 Introduction,[0],[0]
"The sociolinguistic concept of interpersonal stancetaking attempts to answer this question, by providing a conceptual framework that accounts for a range of interpersonal phenomena, subsuming formality, politeness, and subjectivity (Du Bois, 2007).1 This
1Stancetaking is distinct from the notion of stance which corresponds to a position in a debate (Walker et al., 2012).",1 Introduction,[0],[0]
"Similarly, Freeman et al. (2014) correlate phonetic features with the strength of such argumentative stances.
framework has been applied almost exclusively through qualitative methods, using close readings of individual texts or dialogs to uncover how language is used to position individuals with respect to their interlocutors and readers.
",1 Introduction,[0],[0]
We attempt the first large-scale operationalization of stancetaking through computational methods.,1 Introduction,[0],[0]
"Du Bois (2007) formalizes stancetaking as a multi-dimensional construct, reflecting the relationship of discourse participants to (a) the audience or interlocutor; (b) the topic of discourse; (c) the talk or text itself.",1 Introduction,[0],[0]
"However, the multidimensional nature of stancetaking poses problems for traditional computational approaches, in which labeled data is obtained by relying on annotator intuitions about scalar concepts such politeness (Danescu-Niculescu-Mizil et al., 2013) and formality (Pavlick and Tetreault, 2016).
",1 Introduction,[0],[0]
"Instead, our approach is based on a theoretically-guided application of unsupervised learning, in the form of factor analysis, applied to lexical features.",1 Introduction,[0],[0]
"Stancetaking is characterized in large part by an array of linguistic features ranging from discourse markers such as actually to backchannels such as yep (Kiesling, 2009).",1 Introduction,[0],[0]
"We therefore first compile a lexicon of stance markers, combining prior lexicons from Biber and Finegan (1989) and the Switchboard Dialogue Act Corpus (Jurafsky et al., 1998).",1 Introduction,[0],[0]
We then extend this lexicon to the social media domain using word embeddings.,1 Introduction,[0],[0]
"Finally, we apply multi-dimensional analysis of co-occurrence patterns to identify a small set of stance dimensions.
",1 Introduction,[0],[0]
"To measure the internal coherence (construct validity) of the stance dimensions, we use a word
884
intrusion task (Chang et al., 2009) and a set of preregistered hypotheses.",1 Introduction,[0],[0]
"To measure the utility of the stance dimensions, we perform a series of extrinsic evaluations.",1 Introduction,[0],[0]
A predictive evaluation shows that the membership of online communities is determined in part by the interactional stances that predominate in those communities.,1 Introduction,[0],[0]
"Furthermore, the induced stance dimensions are shown to align with annotations of politeness and formality.
",1 Introduction,[0],[0]
"Contributions We operationalize the sociolinguistic concept of stancetaking as a multidimensional framework, making it possible to measure at scale.",1 Introduction,[0],[0]
"Specifically,
• we contribute a lexicon of stance markers based on prior work and adapted to the genre of online interpersonal discourse;
• we group stance markers into latent dimensions; • we show that these stance dimensions are inter-
nally coherent;
• we demonstrate that the stance dimensions predict and correlate with social phenomena.2",1 Introduction,[0],[0]
"From a theoretical perspective, we build on prior work on interactional meaning in language.",2 Related Work,[0],[0]
"Methodologically, our paper relates to prior work on lexicon-based analysis and contrastive studies of social media communities.",2 Related Work,[0],[0]
"In computational sociolinguistics (Nguyen et al., 2016), language variation has been studied primarily in connection with macro-scale social variables, such as age (Argamon et al., 2007; Nguyen et al., 2013), gender (Burger et al., 2011; Bamman et al., 2014), race (Eisenstein et al., 2011; Blodgett et al., 2016), and geography (Eisenstein et al., 2010).",2.1 Linguistic Variation and Social Meaning,[0],[0]
"This parallels what Eckert (2012) has called the “first wave” of language variation studies in sociolinguistics, which also focused on macro-scale variables.
",2.1 Linguistic Variation and Social Meaning,[0],[0]
"More recently, sociolinguists have dedicated increased attention to situational and stylistic variation, and the interactional meaning that such variation can convey (Eckert and Rickford, 2001).",2.1 Linguistic Variation and Social Meaning,[0],[0]
"This linguistic research can be aligned with computational efforts to quantify phenomena such
2Lexicons and stance dimensions are available at https://github.com/umashanthi-research/ multidimensional-stance-lexicon
as subjectivity (Riloff and Wiebe, 2003), sentiment (Wiebe et al., 2005), politeness (DanescuNiculescu-Mizil et al., 2013), formality (Pavlick and Tetreault, 2016), and power dynamics (Prabhakaran et al., 2012).",2.1 Linguistic Variation and Social Meaning,[0],[0]
"While linguistic research on interactional meaning has focused largely on qualitative methodologies such as discourse analysis (e.g., Bucholtz and Hall, 2005), these computational efforts have made use of crowdsourced annotations to build large datasets of, for example, polite and impolite text.",2.1 Linguistic Variation and Social Meaning,[0],[0]
"These annotation efforts draw on the annotators’ intuitions about the meaning of these sociolinguistic constructs.
",2.1 Linguistic Variation and Social Meaning,[0],[0]
"Interpersonal stancetaking represents an attempt to unify concepts such as sentiment, politeness, formality, and subjectivity under a single theoretical framework (Jaffe, 2009; Kiesling, 2009).",2.1 Linguistic Variation and Social Meaning,[0],[0]
"The key idea, as articulated by Du Bois (2007), is that stancetaking captures the speaker’s relationship to (a) the topic of discussion, (b) the interlocutor or audience, and (c) the talk (or writing) itself.",2.1 Linguistic Variation and Social Meaning,[0],[0]
Various configurations of these three legs of the “stance triangle” can account for a range of phenomena.,2.1 Linguistic Variation and Social Meaning,[0],[0]
"For example, epistemic stance relates to the speaker’s certainty about what is being expressed, while affective stance indicates the speaker’s emotional position with respect to the content (Ochs, 1993).
",2.1 Linguistic Variation and Social Meaning,[0],[0]
"The framework of stancetaking has been widely adopted in linguistics, particularly in the discourse analytic tradition, which involves close reading of individual texts or conversations (Kärkkäinen, 2006; Keisanen, 2007; Precht, 2003; White, 2003).",2.1 Linguistic Variation and Social Meaning,[0],[0]
"But despite its strong theoretical foundation, we are aware of no prior efforts to operationalize stancetaking at scale.",2.1 Linguistic Variation and Social Meaning,[0],[0]
Since annotators may not have strong intuitions about stance — in the way that they do about formality and politeness — we cannot rely on the annotation methodologies employed in prior work.,2.1 Linguistic Variation and Social Meaning,[0],[0]
"We take a different approach, performing a multidimensional analysis of the distribution of likely stance markers.",2.1 Linguistic Variation and Social Meaning,[0],[0]
Our operationalization of stancetaking is based on the induction of lexicons of stance markers.,2.2 Lexicon-based Analysis,[0],[0]
"The lexicon-based methodology is related to earlier work from social psychology, such as the General Inquirer (Stone, 1966) and LIWC (Tausczik and Pennebaker, 2010).",2.2 Lexicon-based Analysis,[0],[0]
"In LIWC, the basic categories were identified first, based on psychological
constructs (e.g., positive emotion, cognitive processes, drive to power) and syntactic groupings of words and phrases (e.g., pronouns, prepositions, quantifiers).",2.2 Lexicon-based Analysis,[0],[0]
"The lexicon designers then manually contructed lexicons for each category, augmenting their intuitions by using distributional statistics to suggest words that may have been missed (Pennebaker et al., 2015).",2.2 Lexicon-based Analysis,[0],[0]
"In contrast, we follow the approach of Biber (1991), using multidimensional analysis to identify latent groupings of markers based on co-occurrence statistics.",2.2 Lexicon-based Analysis,[0],[0]
We then use crowdsourcing and extrinsic comparisons to validate the coherence of these dimensions.,2.2 Lexicon-based Analysis,[0],[0]
"Social media platforms such as Reddit, Stack Exchange, and Wikia can be considered multicommunity environments, in that they host multiple subcommunities with distinct social and linguistic properties.",2.3 Multicommunity Studies,[0],[0]
"Such subcommunities can be contrasted in terms of topics (Adamic et al., 2008; Hessel et al., 2014) and social networks (Backstrom et al., 2006).",2.3 Multicommunity Studies,[0],[0]
"Our work focuses on Reddit, emphasizing community-wide differences in norms for interpersonal interaction.",2.3 Multicommunity Studies,[0],[0]
"In the same vein, Tan and Lee (2015) attempt to characterize stylistic differences across subreddits by focusing on very common words and parts-of-speech; Tran and Ostendorf (2016) use language models and topic models to measure similarity across threads within a subreddit.",2.3 Multicommunity Studies,[0],[0]
One distinction of our approach is that the use of multidimensional analysis gives us interpretable dimensions of variation.,2.3 Multicommunity Studies,[0],[0]
This makes it possible to identify the specific interpersonal features that vary across communities.,2.3 Multicommunity Studies,[0],[0]
"Reddit, one of the internet’s largest social media platforms, is a collection of subreddits organized around various topics of interest.",3 Data,[0],[0]
"As of January 2017, there were more than one million subreddits and nearly 250 million users, discussing topics ranging from politics (r/politics) to horror stories (r/nosleep).3 Although Reddit was originally designed for sharing hyperlinks, it also provides the ability to post original textual content, submit comments, and vote on content quality (Gilbert, 2013).",3 Data,[0],[0]
"Reddit’s conversation-like threads are therefore well suited for the study of interpersonal social and linguistic phenomena.
",3 Data,[0],[0]
"3http://redditmetrics.com/
For example, the following are two comments from the subreddit r/malefashionadvice, posted in response to a picture posted by a user asking for fashion advise.
U1: “I think the beard looks pretty good.",3 Data,[0],[0]
Definitely not the goatee.,3 Data,[0],[0]
"Clean shaven is always the safe option.”
U2: “Definitely the beard.",3 Data,[0],[0]
"But keep it trimmed.”
The phrases in bold face are markers of stance, indicating a evaluative stance.",3 Data,[0],[0]
The following example is a part of a thread in the subreddit r/photoshopbattles where users discuss an edited image posted by the original poster OP.,3 Data,[0],[0]
"The phrases in bold face are markers of stance, indicating an involved and interactional stance.
",3 Data,[0],[0]
U3: “Ha ha awesome!”,3 Data,[0],[0]
"U4: ‘‘are those..... furries?”
",3 Data,[0],[0]
"OP: “yes, sir.",3 Data,[0],[0]
They are!”,3 Data,[0],[0]
U4: “Oh cool.,3 Data,[0],[0]
"That makes sense!”
",3 Data,[0],[0]
"We used an archive of 530 million comments posted on Reddit in 2014, retrieved from the public archive of Reddit comments.4",3 Data,[0],[0]
"This dataset consists of each post’s textual content, along with metadata that identifies the subreddit, thread, author, and post creation time.",3 Data,[0],[0]
More statistics about the full dataset are shown in Table 1.,3 Data,[0],[0]
"Interpersonal stancetaking can be characterized in part by an array of linguistic features such as hedges (e.g., might, kind of ), discourse markers (e.g., actually, I mean), and backchannels (e.g., yep, um).",4 Stance Lexicon,[0],[0]
"Our analysis focuses on these markers, which we collect into a lexicon.",4 Stance Lexicon,[0],[0]
"We began with a seed lexicon of stance markers from Biber and Finegan (1989), who compiled an
4https://archive.org/details/2015_",4.1 Seed lexicon,[0],[0]
"reddit_comments_corpus
extensive list by surveying dictionaries, previous studies on stance, and texts in several genres of English.",4.1 Seed lexicon,[0],[0]
"This list includes certainty adverbs (e.g., actually, of course, in fact), affect markers (e.g., amazing, thankful, sadly), and hedges (e.g., kind of, maybe, something like) among other adverbial, adjectival, verbal, and modal markers of stance.",4.1 Seed lexicon,[0],[0]
"In total, this list consists of 448 stance markers.
",4.1 Seed lexicon,[0],[0]
The Biber and Finegan (1989) lexicon is primarily based on written genres from the pre-social media era.,4.1 Seed lexicon,[0],[0]
"Our dataset — like much of the recent work in this domain — consists of online discussions, which differ significantly from printed texts (Eisenstein, 2013).",4.1 Seed lexicon,[0],[0]
"One difference is that online discussions contain a number of dialog act markers that are characteristic of spoken language, such as oh yeah, nah,",4.1 Seed lexicon,[0],[0]
wow.,4.1 Seed lexicon,[0],[0]
"We accounted for this by adding 74 dialog act markers from the Switchboard Dialog Act Corpus (Jurafsky et al., 1998).",4.1 Seed lexicon,[0],[0]
"The final seed lexicon consists of 517 unique markers, from these two sources.",4.1 Seed lexicon,[0],[0]
"Note that the seed lexicon also includes markers that contain multiple tokens (e.g. kind of, I know).",4.1 Seed lexicon,[0],[0]
"Online discussions differ not only from written texts, but also from spoken discussions, due to their use of non-standard vocabulary and spellings.",4.2 Lexicon expansion,[0],[0]
"To measure stance accurately, these genre differences must be accounted for.",4.2 Lexicon expansion,[0],[0]
We therefore expanded the seed lexicon using automated techniques based on distributional statistics.,4.2 Lexicon expansion,[0],[0]
"This is similar to prior work on the expansion of sentiment lexicons (Hatzivassiloglou and McKeown, 1997; Hamilton et al., 2016).
",4.2 Lexicon expansion,[0],[0]
Our lexicon expansion approach used word embeddings to find words that are distributionally similar to those in the seed set.,4.2 Lexicon expansion,[0],[0]
"We trained word embeddings on a corpus of 25 million Reddit comments and a vocabulary of 100K most frequent words on Reddit using the structured skip-gram models of both WORD2VEC (Mikolov et al., 2013) and WANG2VEC (Ling et al., 2015) with default parameters.",4.2 Lexicon expansion,[0],[0]
The WANG2VEC method augments WORD2VEC by accounting for word order information.,4.2 Lexicon expansion,[0],[0]
"We found the similarity judgments obtained from WANG2VEC to be qualitatively more meaningful, so we used these embeddings to construct the expanded lexicon.5
5We used the following default parameters: 100 dimensions, a window size of five, a negative sampling size of ten, five-epoch iterations, and a sub-sampling rate of 10−4.
",4.2 Lexicon expansion,[0],[0]
"To perform lexicon expansion, we constructed a dictionary of candidate terms, consisting of all unigrams that occur with a frequency rate of at least 10−7 in the Reddit comment corpus.",4.2 Lexicon expansion,[0],[0]
"Then, for each single-token marker in the seed lexicon, we identified all terms from the candidate set whose embedding has cosine similarity of at least 0.75 with respect to the seed marker.6 Table 2 shows examples of seed markers and related terms we extracted from word embeddings.",4.2 Lexicon expansion,[0],[0]
"Through this procedure, we identified 228 additional markers based on similarity to items in the seed list from Biber and Finegan (1989), and 112 additional markers based on the seed list of dialog acts.",4.2 Lexicon expansion,[0],[0]
"In total, our stance lexicon contains 812 unique markers.",4.2 Lexicon expansion,[0],[0]
"To summarize the main axes of variation across the lexicon of stance markers, we apply a multidimensional analysis (Biber, 1992) to the distributional statistics of stance markers across subreddit communities.",5 Linguistic Dimensions of Stancetaking,[0],[0]
"Each dimension of variation can then be viewed as a spectrum, characterized by the stance markers and subreddits that are associated with the positive and negative extremes.",5 Linguistic Dimensions of Stancetaking,[0],[0]
"Multidimensional analysis is based on singular value decomposition, which has been applied successfully to a wide range of problems in natural language processing and information retrieval (e.g., Landauer et al., 1998).",5 Linguistic Dimensions of Stancetaking,[0],[0]
"While Bayesian topic models are an appealing alternative, singular value decomposition is fast and deterministic, with a minimal number of tuning parameters.
",5 Linguistic Dimensions of Stancetaking,[0],[0]
"6We tried different thresholds on the similarity value and the corpus frequency, and the reported values were chosen based on the quality of the resulting related terms.",5 Linguistic Dimensions of Stancetaking,[0],[0]
This was done prior to any of the validations or extrinsic analyses described later in the paper.,5 Linguistic Dimensions of Stancetaking,[0],[0]
Our analysis is based on the co-occurrence of stance markers and subreddits.,5.1 Extracting Stance Dimensions,[0],[0]
"This is motivated by our interest in comparisons of the interactional styles of online communities within Reddit, and by the premise that these distributional differences reflect socially meaningful communicative norms.",5.1 Extracting Stance Dimensions,[0],[0]
"A pilot study applied the same technique to the cooccurrence of stance markers and individual authors, and the resulting dimensions appeared to be less stylistically coherent.
",5.1 Extracting Stance Dimensions,[0],[0]
"Singular value decomposition is often used in combination with a transformation of the cooccurrence counts by pointwise mutual information (Bullinaria and Levy, 2007).",5.1 Extracting Stance Dimensions,[0],[0]
This transformation ensures that each cell in the matrix indicates how much more likely a stance marker is to cooccur with a given subreddit than would happen by chance under an independence assumption.,5.1 Extracting Stance Dimensions,[0],[0]
"Because negative PMI values tend to be unreliable, we use positive PMI (PPMI), which involves replacing all negative PMI values with zeros (Niwa and Nitta, 1994).",5.1 Extracting Stance Dimensions,[0],[0]
"Therefore, we obtain stance dimensions by applying singular value decomposition to the matrix constructed as follows:
Xm,s =
( log Pr(marker = m, subreddit = s)
Pr(marker = m) Pr(subreddit = s)
)
+
.
",5.1 Extracting Stance Dimensions,[0],[0]
Truncated singular value decomposition performs the approximate factorization X,5.1 Extracting Stance Dimensions,[0],[0]
"≈ UΣV >, where each row of the matrix U is a k-dimensional description of each stance marker, and each row of V is a k-dimensional description of each subreddit.",5.1 Extracting Stance Dimensions,[0],[0]
"We included the 7,589 subreddits that received at least 1,000 comments in 2014.",5.1 Extracting Stance Dimensions,[0],[0]
"From the SVD analysis, we extracted the six principal latent dimensions that explain the most variation in our dataset.7",5.2 Results: Stance Dimensions,[0],[0]
The decision to include only the first six dimensions was based on the strength of the singular values corresponding to the dimensions.,5.2 Results: Stance Dimensions,[0],[0]
Table 3 shows the top five stance markers for each extreme of the six dimensions.,5.2 Results: Stance Dimensions,[0],[0]
"The stance dimensions convey a range of concepts, such as involved versus informational language, narrative
7Similar to factor analysis, the top few dimensions of SVD explain the most variation, and tend to be most interpretable.",5.2 Results: Stance Dimensions,[0],[0]
"A scree plot (Cattell, 1966) showed that the amount of variation explained dropped after the top six dimensions, and qualitative interpretation showed that the remaining dimension were less interpretable.
versus dialogue-oriented writing, standard versus non-standard variation, and positive versus negative affect.",5.2 Results: Stance Dimensions,[0],[0]
Figure 1 shows the distribution of subreddits along two of these dimensions.,5.2 Results: Stance Dimensions,[0],[0]
Evaluating model output against gold-standard annotations is appropriate when there is some notion of a correct answer.,6 Construct Validity,[0],[0]
"As stancetaking is a multidimensional concept, we have taken an unsupervised approach.",6 Construct Validity,[0],[0]
"Therefore, we use evaluation techniques based on the notion of validity, which is the extent to which the operationalization of a construct truly captures the intended quantity or concept.",6 Construct Validity,[0],[0]
"Validation techniques for unsupervised content analysis are widely found in the social science literature (Weber, 1990; Quinn et al., 2010) and have also been recently used in the NLP and machine learning communities (e.g., Chang et al., 2009; Murphy et al., 2012; Sim et al., 2013).
",6 Construct Validity,[0],[0]
We used several methods to validate the stance dimensions extracted from the corpus of Reddit comments.,6 Construct Validity,[0],[0]
"This section describes intrinsic evaluations, which test whether the extracted stance dimensions are linguistically coherent and mean-
ingful, thereby testing the construct or content validity of the proposed stance dimensions (Quinn et al., 2010).",6 Construct Validity,[0],[0]
Extrinsic evaluations are presented in section 7.,6 Construct Validity,[0],[0]
A word intrusion task is used to measure the coherence and interpretability of a group of words.,6.1 Word Intrusion Task,[0],[0]
"Human raters are presented with a list of terms, all but one of which are selected from a target concept; their task is to identify the intruder.",6.1 Word Intrusion Task,[0],[0]
"If the target concept is internally coherent, human raters should be able to perform this task accurately; if not, their selections should be random.",6.1 Word Intrusion Task,[0],[0]
"Word intrusion tasks have previously been used to validate the interpretability of topic models (Chang et al., 2009) and vector space models (Murphy et al., 2012).
",6.1 Word Intrusion Task,[0],[0]
"We deployed a word intrusion task on Amazon Mechanical Turk (AMT), in which we presented the top four stance markers from one end of a dimension, along with an intruder marker selected from the top four markers of the opposite end of that dimension.",6.1 Word Intrusion Task,[0],[0]
"In this way, we created four word intrusion tasks for each end of each dimension.",6.1 Word Intrusion Task,[0],[0]
The main reason for including only the top four words in each dimension is the expense of conducting crowd-sourced evaluations.,6.1 Word Intrusion Task,[0],[0]
"In the most relevant prior work, Chang et al. (2009) used the top five words from each topic in their evaluation of topic models.
",6.1 Word Intrusion Task,[0],[0]
"Worker selection We required that the AMT workers (“turkers”) have completed a minimum of 1,000 HITs and have at least 95% approval rate
Furthermore, because our task is based on analysis of English language texts, we required the turkers to be native speakers of English living in one of the majority English speaking countries.",6.1 Word Intrusion Task,[0],[0]
"As a further requirement, we required the turkers to obtain a qualification which involves an English comprehension test similar to the questions in standardized English language tests.",6.1 Word Intrusion Task,[0],[0]
"These requirements are based on best practices identified by CallisonBurch and Dredze (2010).
",6.1 Word Intrusion Task,[0],[0]
"Task specification Each AMT human intelligence task (HIT) consists of twelve word intrusion tasks, one for each end of the six dimensions.",6.1 Word Intrusion Task,[0],[0]
"We provided minimal instructions regarding the task, and did not provide any examples, to avoid introducing bias.8 As a further quality control, each HIT included three questions which ask the turkers to pick the best synonym for a given word from a list of five answers, where one answer was clearly correct; Turkers who gave incorrect answers were to be excluded, but this situation did not arise in practice.",6.1 Word Intrusion Task,[0],[0]
"Altogether each HIT consists of 15 questions, and was paid US$1.50.",6.1 Word Intrusion Task,[0],[0]
"Five different turkers performed each HIT.
Results We measured the interrater reliability using Krippendorf’s α (Krippendorff, 2007) and the model precision metric of Chang et al. (2009).",6.1 Word Intrusion Task,[0],[0]
Results on both metrics were encouraging.,6.1 Word Intrusion Task,[0],[0]
"We obtained a value of α = 0.73, on a scale where
8The prompt for the word intrusions task was: “Select the intruder word/phrase: you will be given a list of five English words/phrases and asked to pick the word/phrase that is least similar to the other four words/phrases when used in online discussion forums.”
α = 0 indicates chance agreement and α = 1 indicates perfect agreement.",6.1 Word Intrusion Task,[0],[0]
The model precision was 0.82; chance precision is 0.20.,6.1 Word Intrusion Task,[0],[0]
"To offer a sense of typical values for this metric, Chang et al. (2009) report model precisions in the range 0.7–0.83 in their analysis of topic models.",6.1 Word Intrusion Task,[0],[0]
"Overall, these results indicate that the multi-dimensional analysis has succeeded at identifying dimensions that reflect natural groupings of stance markers.",6.1 Word Intrusion Task,[0],[0]
Content validity was also assessed using a set of pre-registered hypotheses.,6.2 Pre-registered Hypotheses,[0],[0]
The practice of preregistering hypotheses before an analysis and testing the correctness is widely used in the social sciences; it was adopted by Sim et al. (2013) to evaluate the induction of political ideological models from text.,6.2 Pre-registered Hypotheses,[0],[0]
"Before performing the mutidimensional analysis, we identified two groups of hypotheses that are expected to hold with respect to the latent stancetaking dimensions using our prior linguistic knowledge:
",6.2 Pre-registered Hypotheses,[0],[0]
• Hypothesis I: Stance markers that are synonyms should not appear on the opposite ends of a stance dimension.,6.2 Pre-registered Hypotheses,[0],[0]
• Hypothesis II:,6.2 Pre-registered Hypotheses,[0],[0]
"If at least one stance marker
from a predefined stance feature group (defined below) appears on one end of a stance dimension, then other markers from the same feature group will tend not to appear at the opposite end of the same dimension.",6.2 Pre-registered Hypotheses,[0],[0]
"For each marker in our stance lexicon, we extracted synonyms from Wordnet, focusing on markers that appear in only one Wordnet synset, and not including pairs in which one term was an inflection of the other.9 Our final list contains 73 synonym pairs (e.g., eventually/finally, grateful/thankful, yea/yeah).",6.2.1 Synonym Pairs,[0],[0]
"Of these pairs, there were 59 cases in which both terms appeared in either the top or bottom 200 positions of a stance dimension.",6.2.1 Synonym Pairs,[0],[0]
"In 51 of these cases (86%), the two terms appeared on the same side of the dimension.",6.2.1 Synonym Pairs,[0],[0]
"The chance rate would be 50%, so this supports Hypothesis I and
9It is possible that inflections are semantically similar, because by definition they are changes in the form of a word to mark distinctions such as tense, person, or number.",6.2.1 Synonym Pairs,[0],[0]
"However, different inflections of a single word form might be used to mark different stances (e.g., some stances might be associated with the past while others might be associated with the present or future).
",6.2.1 Synonym Pairs,[0],[0]
further validates the stance dimensions.,6.2.1 Synonym Pairs,[0],[0]
More details of the results are shown in Table 4.,6.2.1 Synonym Pairs,[0],[0]
"Note that synonym pairs may differ in aspects such as formality (e.g., said/informed, want/desire), which is one of the main dimensions of stancetaking.",6.2.1 Synonym Pairs,[0],[0]
"Therefore, perfect support for Hypothesis I is not expected.",6.2.1 Synonym Pairs,[0],[0]
"Biber and Finegan (1989) group stance markers into twelve “feature groups”, such as certainty adverbs, doubt adverbs, affect expressions, and hedges.",6.2.2 Stance Feature Groups,[0],[0]
"Ideally, the stance dimensions should preserve these groupings.",6.2.2 Stance Feature Groups,[0],[0]
"To test this, for each of the seven feature groups with at least ten stance markers in the lexicon, we counted the number of terms appearing among the top 200 positions in both ends (high/low) of each dimension.",6.2.2 Stance Feature Groups,[0],[0]
"Under the null hypothesis, the stance dimensions are random with respect to the feature groups, so we would expect roughly an equal number of markers on both ends.",6.2.2 Stance Feature Groups,[0],[0]
"As shown in Table 5, for five of the seven feature groups, it is possible to reject the null hypothesis at p < .007, which is the significance threshold at α = 0.05, after correcting for multiple comparisons using the Bonferroni correction.",6.2.2 Stance Feature Groups,[0],[0]
This indicates that the stance dimensions are aligned with predefined stance feature groups.,6.2.2 Stance Feature Groups,[0],[0]
The evaluations in the previous section test internal validity; we now describe evaluations testing whether the stance dimensions are relevant to external social and interactional phenomena.,7 Extrinsic Evaluations,[0],[0]
"Online communities can be considered as communities of practice (Eckert and McConnell-Ginet, 1992), where members come together to engage in shared linguistic practices.",7.1 Predicting Cross-posting,[0],[0]
"These practices
evolve simultaneously with membership, coalescing into shared norms.",7.1 Predicting Cross-posting,[0],[0]
"The memberships of multiple subreddits on the same topic (e.g., r/science and r/askscience) often do not overlap considerably.",7.1 Predicting Cross-posting,[0],[0]
"Therefore we hypothesize that users of Reddit have preferred interactional styles, and that participation in subreddit communities is governed not only by topic interest, but also by these interactional preferences.",7.1 Predicting Cross-posting,[0],[0]
"The proposed stancetaking dimensions provide a simple measure of interactional style, allowing us to test whether it is predictive of community membership decisions.
",7.1 Predicting Cross-posting,[0],[0]
"Classification task We design a classification task, in which the goal is to determine whether a pair of subreddits is high-crossover or lowcrossover.",7.1 Predicting Cross-posting,[0],[0]
"In high-crossover subreddit pairs, individuals are especially likely to participate in both.",7.1 Predicting Cross-posting,[0],[0]
"For the purpose of this evaluation, individuals are considered to participate in a subreddit if they contribute posts or comments.",7.1 Predicting Cross-posting,[0],[0]
We compute the pointwise mutual information (PMI) with respect to cross-participation among the 100 most popular subreddits.,7.1 Predicting Cross-posting,[0],[0]
"For each subreddit s, we identify the five highest and lowest PMI pairs 〈s, t〉, and add these to the high-crossover and low-crossover sets, respectively.",7.1 Predicting Cross-posting,[0],[0]
Example pairs are shown in Table 6.,7.1 Predicting Cross-posting,[0],[0]
"After eliminating redundant pairs, we identify 437 unique high-crossover pairs, and 465 unique lowcrossover pairs.",7.1 Predicting Cross-posting,[0],[0]
"All evaluations are based on multiple random training/test splits over this dataset.
",7.1 Predicting Cross-posting,[0],[0]
Classification approaches A simple classification approach is to predict that subreddits with similar text will have high crossover.,7.1 Predicting Cross-posting,[0],[0]
"We measure similarity using TF-IDF weighted cosine similarity, using two possible lexicons: the 8,000 most frequent words on reddit (BOW), and the stance lexicon (STANCE MARKERS).",7.1 Predicting Cross-posting,[0],[0]
"The similarity threshold between high-crossover and low-
crossover pairs was estimated on the training data.",7.1 Predicting Cross-posting,[0],[0]
"We also tested the relevance of multi-dimensional analysis, by applying SVD to both lexicons.",7.1 Predicting Cross-posting,[0],[0]
"For each pair of subreddits, we computed a feature set of the absolute difference across the top six latent dimensions, and applied a logistic regression classifier.",7.1 Predicting Cross-posting,[0],[0]
"Regularization was tuned by internal crossvalidation.
",7.1 Predicting Cross-posting,[0],[0]
Results Table 7 shows average accuracies for these models.,7.1 Predicting Cross-posting,[0],[0]
"The stance-based SVD features are considerably more accurate than the BOWbased SVD features, indicating that interactional style does indeed predict cross-posting behavior.10 Both are considerably more accurate than the bagof-words models based on cosine similarity.",7.1 Predicting Cross-posting,[0],[0]
The utility of the induced stance dimensions depends on their correlation with social phenomena of interest.,7.2 Politeness and Formality,[0],[0]
Prior work has used crowdsourcing to annotate texts for politeness and formality.,7.2 Politeness and Formality,[0],[0]
"We now evaluate the stancetaking properties of these annotated texts.
",7.2 Politeness and Formality,[0],[0]
"Data We used the politeness corpus of Wikipedia edit requests from Danescu-NiculescuMizil et al. (2013), which includes the textual content of the edit requests, along with scalar annotations of politeness.",7.2 Politeness and Formality,[0],[0]
"Following the original
10We use BOW+SVD as the most comparable contentbased alternative to our stancetaking dimensions.",7.2 Politeness and Formality,[0],[0]
"While there may be more accurate discriminative approaches, our goal is a direct comparison of stance and content-based features, not an exhaustive comparison of classification approaches.
",7.2 Politeness and Formality,[0],[0]
"authors, we compare the text for the messages ranked in the first and fourth quartiles of politeness scores.",7.2 Politeness and Formality,[0],[0]
"For formality, we used the corpus from Pavlick and Tetreault (2016), focusing on the blogs domain, which is most similar to our domain of Reddit.",7.2 Politeness and Formality,[0],[0]
Each sentence in this corpus was annotated for formality levels from −3 to +3.,7.2 Politeness and Formality,[0],[0]
"We considered only the sentences with mean formality score greater than +1 (more formal) and less than −1 (less formal).
Stance dimensions For each document in the above datasets, we compute the stance properties, as follows: for each dimension, we compute the total frequency of the hundred most positive terms and the hundred most negative terms, and then take the difference.",7.2 Politeness and Formality,[0],[0]
Instances containing no terms from either list are excluded.,7.2 Politeness and Formality,[0],[0]
"We focus on stance dimensions two and five (summarized in Table 3), because they appeared to be most relevant to politeness and formality.",7.2 Politeness and Formality,[0],[0]
Dimension two contrasts informational and argumentative language against emotional and non-standard language.,7.2 Politeness and Formality,[0],[0]
"Dimension five contrasts positive and formal language against non-standard and somewhat negative language.
",7.2 Politeness and Formality,[0],[0]
Results A kernel density plot of the resulting differences is shown in Figure 2.,7.2 Politeness and Formality,[0],[0]
"The effect sizes of the resulting differences are quantified using Cohen’s d statistic (Cohen, 1988).",7.2 Politeness and Formality,[0],[0]
"Effect sizes for all differences are between 0.3 and 0.4, indicating small-to-medium effects — except for the evaluation of formality on dimension five, where the effect size is close to zero.",7.2 Politeness and Formality,[0],[0]
"The relatively modest effect sizes are unsurprising, given the short length of the texts.",7.2 Politeness and Formality,[0],[0]
"However, these differences lend insight to the relationship between formality and politeness, which may seem to be closely related concepts.",7.2 Politeness and Formality,[0],[0]
"On dimension two, it is possible to be polite while using non-standard language such as hehe and awww, so long as the sentiment expressed is positive; however, these markers are not consistent with formality.",7.2 Politeness and Formality,[0],[0]
"On dimension five, we see that positive sentiment terms such as lovely and stunning are consistent with politeness, but not with formality.",7.2 Politeness and Formality,[0],[0]
"Indeed, the distribution of dimension five indicates that both ends of dimension five are consistent only with informal texts.
",7.2 Politeness and Formality,[0],[0]
"Overall, these results indicate that interactional phenomena such as politeness and formality are reflected in our stance dimensions, which are induced in an unsupervised manner.",7.2 Politeness and Formality,[0],[0]
"Future work
may consider the utility of these stance dimensions to predict these social phenomena, particularly in cross-domain settings where lexical classifiers may overfit.",7.2 Politeness and Formality,[0],[0]
Stancetaking provides a general perspective on the various linguistic phenomena that structure social interactions.,8 Conclusion,[0],[0]
"We have identified a set of several hundred stance markers, building on previouslyidentified lexicons by using word embeddings to perform lexicon expansion.",8 Conclusion,[0],[0]
"We then used multidimensional analysis to group these markers into stance dimensions, which we show to be internally coherent and extrinsically useful.",8 Conclusion,[0],[0]
"Our hope is that these stance dimensions will be valuable as a convenient building block for future research on interactional meaning.
",8 Conclusion,[0],[0]
Acknowledgments Thanks to the anonymous reviewers for their useful and constructive feedback on our submission.,8 Conclusion,[0],[0]
"This research was supported by Air Force Office of Scientific Research award FA9550-14-1-0379, by National Institutes of Health award R01-GM112697, and by the National Science Foundation awards 1452443 and 1111142.",8 Conclusion,[0],[0]
"We thank Tyler Schnoebelen for helpful discussions; C.J. Hutto, Tanushree Mitra, and Sandeep Soni for assistance with Mechanical Turk experiments; and Ian Stewart for assistance with creating word embeddings.",8 Conclusion,[0],[0]
"We also thank the Mechanical Turk workers for performing the word intrusion task, and for feedback on a pilot task.",8 Conclusion,[0],[0]
"The sociolinguistic construct of stancetaking describes the activities through which discourse participants create and signal relationships to their interlocutors, to the topic of discussion, and to the talk itself.",abstractText,[0],[0]
"Stancetaking underlies a wide range of interactional phenomena, relating to formality, politeness, affect, and subjectivity.",abstractText,[0],[0]
"We present a computational approach to stancetaking, in which we build a theoretically-motivated lexicon of stance markers, and then use multidimensional analysis to identify a set of underlying stance dimensions.",abstractText,[0],[0]
"We validate these dimensions intrinsically and extrinsically, showing that they are internally coherent, match pre-registered hypotheses, and correlate with social phenomena.",abstractText,[0],[0]
A Multidimensional Lexicon for Interpersonal Stancetaking,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 753–762 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1070",text,[0],[0]
"One of the most successful approaches to grammatical error correction (GEC) is to cast the problem as (monolingual) machine translation (MT), where we translate from possibly ungrammatical English sentences to corrected ones (Brockett et al., 2006; Gao et al., 2010; Junczys-Dowmunt and Grundkiewicz, 2016).",1 Introduction,[0],[0]
"Such systems, which are based on phrasebased MT models that are typically trained on large sets of sentence-correction pairs, can correct global errors such as word order and usage and local errors in spelling and inflection.",1 Introduction,[0],[0]
"The approach has proven superior to systems based on local classifiers that can only fix focused errors in prepositions, determiners, or inflected forms (Rozovskaya and Roth, 2016).
∗This work was conducted while the third author worked at Microsoft Research.
",1 Introduction,[0],[0]
"Recently, neural machine translation (NMT) systems have achieved substantial improvements in translation quality over phrase-based MT systems (Sutskever et al., 2014; Bahdanau et al., 2015).",1 Introduction,[0],[0]
"Thus, there is growing interest in applying neural systems to GEC (Yuan and Briscoe, 2016; Xie et al., 2016).",1 Introduction,[0],[0]
"In this paper, we significantly extend previous work, and explore new neural models to meet the unique challenges of GEC.
",1 Introduction,[0],[0]
The core component of most NMT systems is a sequence-to-sequence (S2S) model which encodes a sequence of source words into a vector and then generates a sequence of target words from the vector.,1 Introduction,[0],[0]
"Unlike the phrase-based MT models, the S2S model can capture long-distance, or even global, word dependencies, which are crucial to correcting global grammatical errors and helping users achieve native speaker fluency (Sakaguchi et al., 2016).",1 Introduction,[0],[0]
"Thus, the S2S model is expected to perform better on GEC than phrase-based models.",1 Introduction,[0],[0]
"However, as we will show in this paper, to achieve the best performance on GEC, we still need to extend the standard S2S model to address several task-specific challenges, which we will describe below.
",1 Introduction,[0],[0]
"First, a GEC model needs to deal with an extremely large vocabulary that consists of a large number of words and their (mis)spelling variations.",1 Introduction,[0],[0]
"Second, the GEC model needs to capture structure at different levels of granularity in order to correct errors of different types.",1 Introduction,[0],[0]
"For example, while correcting spelling and local grammar errors requires only word-level or sub-word level information, e.g., violets→ violates (spelling) or violate→ violates (verb form), correcting errors in word order or usage requires global semantic relationships among phrases and words.
",1 Introduction,[0],[0]
"Standard approaches in neural machine translation, also applied to grammatical error correction by Yuan and Briscoe (2016), address the large vocabulary problem by restricting the vocabulary to a limited number of high-frequency words and re-
753
sorting to standard word translation dictionaries to provide translations for the words that are out of the vocabulary (OOV).",1 Introduction,[0],[0]
"However, this approach often fails to take into account the OOVs in context for making correction decisions, and does not generalize well to correcting words that are unseen in the parallel training data.",1 Introduction,[0],[0]
"An alternative approach, proposed by Xie et al. (2016), applies a character-level sequence to sequence neural model.",1 Introduction,[0],[0]
"Although the model eliminates the OOV issue, it cannot effectively leverage word-level information for GEC, even if it is used together with a separate word-based language model.
",1 Introduction,[0],[0]
"Our solution to the challenges mentioned above is a novel, hybrid neural model with nested attention layers that infuse both word-level and character-level information.",1 Introduction,[0],[0]
The architecture of the model is illustrated in Figure 1.,1 Introduction,[0],[0]
The word-level information is used for correcting global grammar and fluency errors while the character-level information is used for correcting local errors in spelling or inflected forms.,1 Introduction,[0],[0]
Contextual information is crucial for GEC.,1 Introduction,[0],[0]
"Using the proposed model, by combining embedding vectors and attention at both word and character levels, we model all contextual words, including OOVs, in a unified context vector representation.",1 Introduction,[0],[0]
"In particular, as we will discuss in Section 5, the character-level attention layer captures most useful information for correcting local errors that involve small edits in orthography.
",1 Introduction,[0],[0]
Our model differs substantially from the wordlevel S2S model of Yuan and Briscoe (2016) and the character-level S2S model of Xie et al. (2016) in the way we infuse information at both the word level and the character level.,1 Introduction,[0],[0]
"We extend the wordcharacter hybrid model of Luong and Manning (2016), which was originally developed for machine translation, by introducing a character attention layer.",1 Introduction,[0],[0]
"This allows the model to learn substitution patterns at both the character level and the word level in an end-to-end fashion, using sentencecorrection pairs.
",1 Introduction,[0],[0]
"We validate the effectiveness of our model on the CoNLL-14 benchmark dataset (Ng et al., 2014).",1 Introduction,[0],[0]
"Results show that the proposed model outperforms all previous neural models for GEC, including the hybrid model of Luong and Manning (2016), which we apply to GEC for the first time.",1 Introduction,[0],[0]
"When integrated with a large word-based n-gram language model, our GEC system achieves an F0.5 of 45.15 on CoNLL-14, substantially exceeding the previ-
ously reported top performance of 40.56 achieved by using a neural model and an external language model (Xie et al., 2016).",1 Introduction,[0],[0]
A variety of classifier-based and MT-based techniques have been applied to grammatical error correction.,2 Related Work,[0],[0]
The CoNLL-14 shared task overview paper of Ng et al. (2014) provides a comparative evaluation of approaches.,2 Related Work,[0],[0]
"Two notable advances after the shared task have been in the areas of combining classifiers and phrase-based MT (Rozovskaya and Roth, 2016) and adapting phrase-based MT to the GEC task (Junczys-Dowmunt and Grundkiewicz, 2016).",2 Related Work,[0],[0]
The latter work has reported the highest performance to date on the task of 49.5 in F0.5 score on the CoNLL-14 test set.,2 Related Work,[0],[0]
"This method integrates discriminative training toward the task-specific evaluation function, a rich set of features, and multiple large language models.",2 Related Work,[0],[0]
Neural approaches to the task are less explored.,2 Related Work,[0],[0]
"We believe that the advances from Junczys-Dowmunt and Grundkiewicz (2016) are complementary to the ones we propose for neural MT, and could be integrated with neural models to achieve even higher performance.
",2 Related Work,[0],[0]
"Two prior works explored sequence to sequence neural models for GEC (Xie et al., 2016; Yuan and Briscoe, 2016), while Chollampatt et al. (2016) integrated neural features in a phrase-based system for the task.",2 Related Work,[0],[0]
"Neural models were also applied to the related sub-task of grammatical error identification (Schmaltz et al., 2016).",2 Related Work,[0],[0]
"Yuan and Briscoe (2016) demonstrated the promise of neural MT for GEC but did not adapt the basic sequence-to-sequence with attention to its unique challenges, falling back to traditional word-alignment models to address vocabulary coverage with a post-processing heuristic.",2 Related Work,[0],[0]
"Xie et al. (2016) built a character-level sequence
to sequence model, which achieves open vocabulary and character-level modeling, but has difficulty with global word-level decisions.
",2 Related Work,[0],[0]
"The primary focus of our work is integration of character and word-level reasoning in neural models for GEC, to capture global fluency errors and local errors in spelling and closely related morphological variants, while obtaining open vocabulary coverage.",2 Related Work,[0],[0]
This is achieved with the help of character and word-level encoders and decoders with two nested levels of attention.,2 Related Work,[0],[0]
Our model is inspired by advances in sub-word level modeling in neural machine translation.,2 Related Work,[0],[0]
We build mostly on the hybrid model of Luong and Manning (2016) to expand its capability to correct rare words by fine-grained character-level attention.,2 Related Work,[0],[0]
We directly compare our model to the one of Luong and Manning (2016) on the grammar correction task.,2 Related Work,[0],[0]
"Alternative methods for MT include modeling of word pieces to achieve open vocabulary (Sennrich et al., 2016), and more recently, fully character-level modeling (Lee et al., 2017).",2 Related Work,[0],[0]
None of these models integrate two nested levels of attention although an empirical evaluation of these approaches for GEC would also be interesting.,2 Related Work,[0],[0]
"Our model is hybrid, and uses both word-level and character-level representations.",3 Nested Attention Hybrid Model,[0],[0]
"It consists of a word-based sequence-to-sequence model as a backbone, and additional character-level encoder, decoder, and attention components, which focus on words that are outside the word-level model’s vocabulary.",3 Nested Attention Hybrid Model,[0],[0]
The word-based backbone closely follows the basic neural sequence-to-sequence architecture with attention as proposed by Bahdanau et al. (2015) and applied to grammatical error correction by Yuan and Briscoe (2016).,3.1 Word-based sequence-to-sequence model as backbone,[0],[0]
"For completeness, we give a sketch here.",3.1 Word-based sequence-to-sequence model as backbone,[0],[0]
"It uses recurrent neural networks to encode the input sentence and to decode the output sentence.
",3.1 Word-based sequence-to-sequence model as backbone,[0],[0]
"Given a sequence of embedding vectors, corresponding to a sequence of input words x:
x = (x1, . . .",3.1 Word-based sequence-to-sequence model as backbone,[0],[0]
", xT ), (1)
the encoder creates a corresponding context-
specific sequence of hidden state vectors e:
e = (h1, . . .",3.1 Word-based sequence-to-sequence model as backbone,[0],[0]
", hT )
",3.1 Word-based sequence-to-sequence model as backbone,[0],[0]
"The hidden state ht at time t is computed as: ft = GRUencf (ft−1, xt) , bt = GRUencb(bt+1, xt), ht =",3.1 Word-based sequence-to-sequence model as backbone,[0],[0]
"[ft; bt], where GRUencf and GRUencb stand for gated recurrent unit functions as described in Cho et al. (2014).",3.1 Word-based sequence-to-sequence model as backbone,[0],[0]
"We use the symbol GRU with different subscripts to represent GRU functions using different sets of parameters (for example, we used the encf and encb subscripts to denote the parameters of the forward and backward word-level encoder units.)
",3.1 Word-based sequence-to-sequence model as backbone,[0],[0]
"The decoder network is also an RNN using GRU units, and defines a sequence of hidden states d̄1, . . .",3.1 Word-based sequence-to-sequence model as backbone,[0],[0]
", d̄S used to define the probability of an output sequence y1, . . .",3.1 Word-based sequence-to-sequence model as backbone,[0],[0]
", yS as follows:
The context vector cs at time step s is computed as follows:
cs = T∑
j=1
αsjhj (2)
where: αsk =
usk∑T j=1 usj
(3)
usk = φ1(ds) Tφ2(hk) (4)
Here φ1 and φ2 denote feedforward linear transformations followed by a tanh nonlinearity.",3.1 Word-based sequence-to-sequence model as backbone,[0],[0]
"The next hidden state d̄s is then defined as:
ds = GRUdec( ¯ds−1, ys−1),
d̄s = ReLU(W",3.1 Word-based sequence-to-sequence model as backbone,[0],[0]
"[cs; ds])
where ys−1 is the embedding of the output token at time s-1.",3.1 Word-based sequence-to-sequence model as backbone,[0],[0]
"ReLU indicates rectified linear units (Hahnloser et al., 2000).
",3.1 Word-based sequence-to-sequence model as backbone,[0],[0]
"The probability of each target word ys is computed as: p(ys|y<s,x) = softmax(g(d̄s)), where g is a function that maps the decoder state into a vector of size the dimensionality of the target vocabulary.
",3.1 Word-based sequence-to-sequence model as backbone,[0],[0]
"The model is trained by minimizing the crossentropy loss, which for a given (x,y) pair is:
Loss(x,y) =",3.1 Word-based sequence-to-sequence model as backbone,[0],[0]
"− S∑
s=1
log p(ys|y<s,x) (5)
",3.1 Word-based sequence-to-sequence model as backbone,[0],[0]
"For parallel training data C, the loss is:
Loss = − ∑
(x,y)∈C
S∑
s=1
log p(ys|y<s,x)",3.1 Word-based sequence-to-sequence model as backbone,[0],[0]
"The word-level backbone models a limited vocabulary of source and target words, and represents out-of-vocabulary tokens with special UNK symbols.",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"In the standard word-level NMT approach, valuable information is lost for source OOV words and target OOV words are predicted using postprocessing heuristics.
",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"Hybrid encoder
Our hybrid architecture overcomes the loss of source information in the word-level backbone by building up compositional representations of the source OOV words using a character-level recurrent neural network with GRU units.",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"These representations are used in place of the special source UNK embeddings in the backbone, and contribute to the contextual encoding of all source tokens.
",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"For example, a three word input sentence where the last term is out-of-vocabulary will be represented as the following vector of embeddings in the word-level model: x = (x1, x2, x3), where x3 would be the embedding for the UNK symbol.
",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"The hybrid encoder builds up a word embedding for the third word based on its character sequence: xc1, . . .",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
", x c M .",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"The encoder computes a sequence of hidden states ec for this character sequence, by a forward character-level GRU network:
ec = (h c 1, . . .",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
", h c M ), (6)
",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
The last state hcM is used as an embedding of the unknown word.,3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"The sequence of embeddings for our example three-word sequence becomes: x = (x1, x2, h c M ).",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
We use the same dimensionality for word embedding vectors xi and composed character sequence vectors hcM to ensure the two ways to define embeddings are compatible.,3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"Our hybrid source encoder architecture is similar to the one proposed by Luong and Manning (2016).
",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"Nested attention hybrid decoder
In traditional word-based sequence-to-sequence models special target UNK tokens are used to represent outputs that are outside the target vocabulary.",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"A post-processing UNK-replacement method is then used (Cho et al., 2015; Yuan and Briscoe, 2016) to replace these special tokens with target words.",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"The hybrid model of (Luong and Manning, 2016) uses a jointly trained character-level decoder
to generate target words corresponding to UNK tokens, and outperforms the traditional approach in the machine translation task.
",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"However, unlike machine translation, models for grammar correction conduct “translation” in the same language, and often need to apply a small number of local edits to the character sequence of a source word corresponding to the target UNK word.",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"For example, rare but correct words such as entity names need to be copied as is, and local spelling errors or errors in inflection need to be corrected.",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"The architecture of Luong and Manning (2016) does not have direct access to a source character sequence, but only uses a single fixed-dimensionality embedding of source unknown words aggregated with additional contextual information from the source.
",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"To address the needs of the grammatical error correction task, we propose a novel hybrid decoder with two nested levels of attention: word level and character-level.",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"The character-level attention serves to provide the decoder with direct access to the relevant source character sequence.
",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"More specifically, the probability of each target word is defined as follows: For words in the target vocabulary, the probability is defined by the wordlevel backbone.",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"For words outside the vocabulary, the probability of each token is the probability of UNK according to the backbone, multiplied by the probability of the word’s character sequence.
",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
The probability of the target character sequence corresponding to an UNK token at position s in the target is defined using a character-level decoder.,3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"As in Luong and Manning (2016), the “separate path” architecture is used to capture the relevant context and define the initial state for the character-level decoder:
d̂s = ReLU(Ŵ",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"[cs; ds])
where Ŵ are parameters different from W , and d̂s is not used by the word-level model in predicting the subsequent tokens, but is only used to initialize the character-level decoder.
",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"To be able to attend to the relevant source character sequence when generating the target character sequence, we use the concept of hard attention (Xu et al., 2015), but use an arg-max approximation for inference instead of sampling.",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"A similar approach to represent discrete hidden structure in a variety of architectures is used in Kong et al. (2017).
",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"The source index zs corresponding to the target
position s is defined according to the word-level attention model:
zs = arg max k∈0...",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"T−1 αsk
where αsk are the intermediate outputs of the word-level attention model we described in Eq.(3).
",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"The character-level decoder generates a character sequence yc = (yc1, . . .",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
", y c N ), conditioned on the initial vector d̂s and the source index",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
zs.,3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"The characters are generated using a hidden state vector dcn at each time step, via a softmax(gc(dcn)), where gc maps the state to the target character vocabulary space.
",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"If the source word xzs is in the source vocabulary, the model is analogous to the one of Luong and Manning (2016) and does not use characterlevel attention: the source context is available only in aggregated form to initialize the state of the decoder.",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"The state dcn for step n in the characterlevel decoder is defined as follows, where GRUcdec are parameters for the gated recurrent cell of this decoder:
dcn = { GRUcdec(d̂s, ycn−1) n",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
= 0,3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"GRUcdec(dcn−1, ycn−1) n > 0
",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"In contrast, if the corresponding token in the source xzs is also an out-of-vocabulary word, we define a second nested level of character attention and use it in the character-level decoder.",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
The character-level attention focuses on individual characters from the source word xzs .,3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"If ec are the source character hidden vectors computed as in Eq.(6), the recurrence equations for the characterlevel decoder with nested attention are:
¯dcn = ReLU(Wc[ccn; dcn])
dcn = { GRUcdecNested(d̂s, ycn−1) n = 0",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"GRUcdecNested( ¯dcn−1, ycn−1) n > 0
where ccn is the context vector obtained using character-level attention on the sequence ec and the last state of the character-level decoder dcn, computed following equations 2, 3 and 4, but using a different set of parameters.
",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"These equations show that the character-level decoder with nested attention can use both the wordlevel state d̂s, and the character-level context ccn
and hidden state dcn to perform global and local editing operations.
",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"Since we introduced two architectures for the character-level decoder depending on whether the source word xzs is OOV, the combined loss function is defined as follows for end-to-end training:
Losstotal = Lossw + αLossc1 + βLossc2
Here Lossw is the standard word-level loss in Eq.(5); character level losses Lossc1 and Lossc2 are losses for target OOV words corresponding to source known and unknown tokens, respectively.",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"α and β are hyper-parameters to balance the loss terms.
",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"As seen, our proposed nested attention hybrid model uses character-level attention only when both a predicted target word and its corresponding source input word are OOV.",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"While the model can be naturally generalized to integrate characterlevel attention for known words, the original hybrid model proposed by Luong and Manning (2016) does not use any character-level information for known words.",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"Thus for a controlled evaluation of the impact of the addition of character-level attention only, in this paper we limit character-level attention to OOV words, which already use characters as a basis for building their embedding vectors.",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"A thorough investigation of the impact of characterlevel information in the encoder, attention, and decoder for known words as well is an interesting topic for future research.
",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
Decoding for word-level and hybrid models Beam-search is used to decode hypotheses according to the word-level backbone model.,3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"For the hybrid model architecture, word-level beam search is conducted first; for each target UNK token, character-level beam-search is used to generate a corresponding target word.",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
We use standard publicly available datasets for training and evaluation.,4.1 Dataset and Evaluation,[0],[0]
"One data source is the NUS Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013), which is provided as a training set for the CoNLL-13 and CoNLL-14 shared tasks.",4.1 Dataset and Evaluation,[0],[0]
"From the original corpus of size about 60K parallel sentences, we randomly selected close to 5K sentence pairs for use as a validation set, and 45K parallel sentences for use in training.",4.1 Dataset and Evaluation,[0],[0]
"A second data source
Training Validation Development Test #Sent pairs 2,608,679 4,771 1,381 1,312
is the Cambridge Learner Corpus (CLC) (Nicholls, 2003), from which we extracted a substantially larger set of parallel sentences.",4.1 Dataset and Evaluation,[0],[0]
"Finally, we used additional training examples from the Lang-8 Corpus of Learner English v1.0 (Tajiri et al., 2012).",4.1 Dataset and Evaluation,[0],[0]
"As Lang-8 data is crowd-sourced, we used heuristics to filter out noisy examples: we removed sentences longer than 100 words and sentence pairs where the correction was substantially shorter than the input text.",4.1 Dataset and Evaluation,[0],[0]
"Table 2 shows the number of sentence pairs from each source used for training.
",4.1 Dataset and Evaluation,[0],[0]
"We evaluate the performance of the models on the standard sets from the CoNLL-14 shared task (Ng et al., 2014).",4.1 Dataset and Evaluation,[0],[0]
"We report final performance on the CoNLL-14 test set without alternatives, and analyze model performance on the CoNLL-13 development set (Dahlmeier et al., 2013).",4.1 Dataset and Evaluation,[0],[0]
We use the development and validation sets for model selection.,4.1 Dataset and Evaluation,[0],[0]
The sizes of all datasets in number of sentences are shown in Table 1.,4.1 Dataset and Evaluation,[0],[0]
"We report performance in F0.5-measure, as calculated by the m2scorer— the official implementation of the scoring metric in the shared task.",4.1 Dataset and Evaluation,[0],[0]
1,4.1 Dataset and Evaluation,[0],[0]
"Given system outputs and gold-standard edits, m2scorer computes the F0.5 measure of a set of system edits against a set of gold-standard edits.",4.1 Dataset and Evaluation,[0],[0]
"We evaluate our model in comparison to the strong baseline of a word-based neural sequenceto-sequence model with attention, with postprocessing for handling out-of-vocabulary words (Yuan and Briscoe, 2016); we refer to this model as word NMT+UNK replacement.",4.2 Baseline,[0],[0]
"Like Yuan and Briscoe (2016), we use a traditional wordalignment model (GIZA++) to derive a wordcorrection lexicon from the parallel training set.",4.2 Baseline,[0],[0]
"However, in decoding, we don’t use GIZA++ to find the corresponding source word for each tar-
1http://www.comp.nus.edu.sg/˜nlp/sw/ m2scorer.tar.gz
get OOV, but follow Cho et al. (2015), Section 3.3 to use the NMT system’s attention weights instead.",4.2 Baseline,[0],[0]
"The target OOV is then replaced by the most likely correction of the source word from the wordcorrection lexicon, or by the source word itself if there are no available corrections.",4.2 Baseline,[0],[0]
"The embedding size for all word and characterlevel encoders and decoders is set to 1000, and the hidden unit size is also 1000.",4.3 Training Details and Results,[0],[0]
"To reproduce the model of Yuan and Briscoe (2016), we selected the word vocabulary for the baseline by choosing the 30K most frequent words in the source and target respectively to form the source and target vocabularies.",4.3 Training Details and Results,[0],[0]
"In preliminary experiments for the hybrid models, we found that selecting the same vocabulary of 30K words for the source and target based on combined frequency was better (.003 in F0.5) and use that method for vocabulary selection instead.",4.3 Training Details and Results,[0],[0]
"However, there was no gain observed by using such a vocabulary selection method in the baseline.",4.3 Training Details and Results,[0],[0]
"Although the source and target vocabularies in the hybrid models are the same, like in the word-level model, the embedding parameters for source and target words are not shared.
",4.3 Training Details and Results,[0],[0]
The hyper-parameters for the losses in our models are selected based on the development set and set as follows: α = β = 0.5.,4.3 Training Details and Results,[0],[0]
"All models are trained with mini-batch size of 128 (batches are shuffled), initial learning rate of 0.0003 and a 0.95 decay ratio if the cost increases in two consecutive 100 iterations.",4.3 Training Details and Results,[0],[0]
"The gradient is rescaled whenever its norm exceeds 10, and dropout is used with a probability of 0.15.",4.3 Training Details and Results,[0],[0]
"Parameters are uniformly ini-
tialized in [− √
(3)√ 1000 ,
√ (3)√
1000 ].
",4.3 Training Details and Results,[0],[0]
We perform inference on the validation set every 5000 iterations to log word-level cost and characterlevel costs; we save parameter values for the model every 10000 iterations as well as the end of each epoch.,4.3 Training Details and Results,[0],[0]
The stopping point for training is selected based on development set F0.5 among the top 20 parameter sets with best validation set value of the loss function.,4.3 Training Details and Results,[0],[0]
Training of the nested attention hybrid model takes approximately five days on a Tesla k40m GPU.,4.3 Training Details and Results,[0],[0]
"The basic hybrid model trains in around four days and the word-level backbone trains in approximately three days.
",4.3 Training Details and Results,[0],[0]
Table 3 shows the performance of the baseline and our nested attention hybrid model on the development and test sets.,4.3 Training Details and Results,[0],[0]
"In addition to the word-level
baseline, we include the performance of a hybrid model with a single level of attention, which follows the work of Luong and Manning (2016) for machine translation, and is the first application of a hybrid word/character-level model to grammatical error correction.",4.3 Training Details and Results,[0],[0]
"Based on hyper-parameter selection, the character-level component weight of the loss is α = 1 for the basic hybrid model.
",4.3 Training Details and Results,[0],[0]
"As shown in Table 3, our implementation of the word NMT+UNK replacement baseline approaches the performance of the one reported in Yuan and Briscoe (2016) (38.77 versus 39.9).",4.3 Training Details and Results,[0],[0]
We attribute the difference to differences in the training set and the word-alignment methods used.,4.3 Training Details and Results,[0],[0]
Our reimplementation serves to provide a controlled experimental evaluation of the impact of hybrid models and nested attention on the GEC task.,4.3 Training Details and Results,[0],[0]
"As seen, our nested attention hybrid model substantially improves upon the baseline, achieving a gain of close to 3 points on the test set.",4.3 Training Details and Results,[0],[0]
"The hybrid word/character model with a single level of attention brings a large improvement as well, showing the importance of character-level information for this task.",4.3 Training Details and Results,[0],[0]
We delve deeper into the impact of nested attention for the hybrid model in Section 5.,4.3 Training Details and Results,[0],[0]
"The value of large language models for grammatical error correction is well known, and such models have been used in classifier and MT-based systems.",4.4 Integrating a Web-scale Language Model,[0],[0]
"To establish the potential of such models in word-based neural sequence-to-sequence systems, we integrate a web-scale count-based language model.",4.4 Integrating a Web-scale Language Model,[0],[0]
"In particular, we use the modified Kneser-Ney 5-gram language model trained from Common Crawl (Buck et al., 2014), made available for download by Junczys-Dowmunt and Grundkiewicz (2016).
",4.4 Integrating a Web-scale Language Model,[0],[0]
Candidates generated by neural models are reranked using the following linear interpolation of log probabilities: sy|x,4.4 Integrating a Web-scale Language Model,[0],[0]
=,4.4 Integrating a Web-scale Language Model,[0],[0]
logPNN (y|x) + λ,4.4 Integrating a Web-scale Language Model,[0],[0]
logPLM (y).,4.4 Integrating a Web-scale Language Model,[0],[0]
Here λ is a hyper-parameter that balances the weights of the neural network model and the language model.,4.4 Integrating a Web-scale Language Model,[0],[0]
"We tuned λ separately
for each neural model variant, by exploring values in the range [0.0, 2.0] with step size 0.1, and selecting according to development set F0.5.",4.4 Integrating a Web-scale Language Model,[0],[0]
"The selected values of λ are: 1.6 for word NMT + UNK replacement and 1.0 for the nested attention model.
",4.4 Integrating a Web-scale Language Model,[0],[0]
Table 4 shows the impact of the LM when combined with the neural models implemented in this work.,4.4 Integrating a Web-scale Language Model,[0],[0]
"The table also lists the results reported by Xie et al. (2016), for their character-level neural model combined with a large word-level language model.",4.4 Integrating a Web-scale Language Model,[0],[0]
"Our best results exceed the ones reported in the prior work by more than 4 points, although we should note that Xie et al. (2016) used a smaller parallel data set for training.",4.4 Integrating a Web-scale Language Model,[0],[0]
We analyze the impact of sub-word level information and the two nested levels of attention in more detail by looking at the performance of the models on different segments of the data.,5 Analysis,[0],[0]
"In particular, we analyze the performance of the models on sentences containing OOV source words versus ones without OOV words, and corrections to orthographically similar versus dissimilar word forms.",5 Analysis,[0],[0]
We present a comparative performance analysis of models on the CoNLL-13 development set.,5.1 Performance by Segment: OOV versus Non-OOV,[0],[0]
"First, we divide the set into two segments: OOV and NonOOV, based on whether there is at least one OOV word in the given source input.",5.1 Performance by Segment: OOV versus Non-OOV,[0],[0]
Table 5 shows that both hybrid architectures substantially outperform the word-level model in both segments of the data.,5.1 Performance by Segment: OOV versus Non-OOV,[0],[0]
The additional nested character-level attention of our hybrid model brings a sizable improvement over the basic hybrid model in the OOV segment and a small degradation in the non-OOV segment.,5.1 Performance by Segment: OOV versus Non-OOV,[0],[0]
"We should note that in future work characterlevel attention can be added for non-OOV source words in the nested attention model, which could improve performance on this segment as well.
",5.1 Performance by Segment: OOV versus Non-OOV,[0],[0]
"Table 6 shows an example where the nested attention hybrid model successfully corrects a misspelling resulting in an OOV word on the source, whereas the baseline word-level system simply copies the source word without fixing the error (since this particular error is not observed in the parallel training set).",5.1 Performance by Segment: OOV versus Non-OOV,[0],[0]
"To analyze more precisely the impact of the additional character-level attention introduced by our design, we continue to investigate the OOV segment in more detail.
",5.2 Impact of Nested Attention on Different Error Types,[0],[0]
"The concept of edit, which is also used by the official M2 score metric, is defined as a minimal pair of corresponding sub-strings in a source sentence and a correction.",5.2 Impact of Nested Attention on Different Error Types,[0],[0]
"For example, in the sentence fragment pair: “Even though there is a risk of causing harms to someone, people still are prefers to keep their pets without a leash.”",5.2 Impact of Nested Attention on Different Error Types,[0],[0]
"→ “Even though there is a risk of causing harm to someone, people still prefer to keep their pets without a leash.”",5.2 Impact of Nested Attention on Different Error Types,[0],[0]
", the minimal edits are “harms→ harm” and “are prefers→ prefer”.",5.2 Impact of Nested Attention on Different Error Types,[0],[0]
"The F0.5 score is computed using weighted precision and recall of the set of a system’s edits against one or more sets of reference edits.
",5.2 Impact of Nested Attention on Different Error Types,[0],[0]
"For our in-depth analysis, we classify edits in the OOV segment into two types: small changes and large changes, based on whether the source and target phrase of the edit are orthographically similar or not.",5.2 Impact of Nested Attention on Different Error Types,[0],[0]
"More specifically, we say that the target and
source phrases are orthographically similar, iff: the character edit distance is at most 2 and the source or target is at most 8 characters long, or edit ratio < 0.25, where edit ratio = character edit distancemin(len(src),len(tar))+0.1 , len(∗) denotes number of characters in ∗, and src and tgt denote the pairs in the edit.",5.2 Impact of Nested Attention on Different Error Types,[0],[0]
"There are 307 gold edits in the “small changes” portion of the CoNLL-13 OOV segment, and 481 gold edits in the “large changes” portion.
",5.2 Impact of Nested Attention on Different Error Types,[0],[0]
Our hypothesis is that the additional characterlevel attention layer is particularly useful to model edits among orthographically similar words.,5.2 Impact of Nested Attention on Different Error Types,[0],[0]
Table 7 contrasts the impact of character-level attention on the two portions of the data.,5.2 Impact of Nested Attention on Different Error Types,[0],[0]
"We can see that the gains in the “small changes” portion are indeed quite large, indicating that the fine-grained character-level attention empowers the model to more accurately correct confusions among phrases with high character-level similarity.",5.2 Impact of Nested Attention on Different Error Types,[0],[0]
The impact in the “large changes” portion is slightly positive in precision and slightly negative in recall.,5.2 Impact of Nested Attention on Different Error Types,[0],[0]
"Thus most of the benefit of the additional character-level attention stems from improvements in the “small changes” portion.
",5.2 Impact of Nested Attention on Different Error Types,[0],[0]
Table 8 shows an example input which illustrates the precision gain of the nested attention hybrid model.,5.2 Impact of Nested Attention on Different Error Types,[0],[0]
The input sentence has a source OOV word which is correct.,5.2 Impact of Nested Attention on Different Error Types,[0],[0]
"The hybrid model introduces an error in this word, because it uses only a single source context vector, aggregating the characterlevel embedding of the source OOV word together with other source words.",5.2 Impact of Nested Attention on Different Error Types,[0],[0]
"The additional characterlevel attention layer in the nested hybrid model enables the correct copying of this long source OOV word, without employing the heuristic mechanism of the word-level NMT system.",5.2 Impact of Nested Attention on Different Error Types,[0],[0]
We have introduced a novel hybrid neural model with two nested levels of attention: word-level and character-level.,6 Conclusions,[0],[0]
The model addresses the unique challenges of the grammatical error correction task and achieves the best reported results on the CoNLL-14 benchmark among fully neural systems.,6 Conclusions,[0],[0]
"Our nested attention hybrid model deeply combines the strengths of word and character level information in all components of an end-to-end neural model: the encoder, the attention layers, and the decoder.",6 Conclusions,[0],[0]
This enables it to correct both global wordlevel and local character-level errors in a unified way.,6 Conclusions,[0],[0]
The new architecture contributes substantial improvement in correction of confusions among rare or orthographically similar words compared to word-level sequence-to-sequence and non-nested hybrid models.,6 Conclusions,[0],[0]
"We would like to thank the ACL reviewers for their insightful suggestions, Victoria Zayats for her help with reproducing the baseline word-level NMT system and Yu Shi, Daxin Jiang and Michael Zeng for the helpful discussions.",Acknowledgements,[0],[0]
"Grammatical error correction (GEC) systems strive to correct both global errors in word order and usage, and local errors in spelling and inflection.",abstractText,[0],[0]
"Further developing upon recent work on neural machine translation, we propose a new hybrid neural model with nested attention layers for GEC.",abstractText,[0],[0]
"Experiments show that the new model can effectively correct errors of both types by incorporating word and character-level information, and that the model significantly outperforms previous neural models for GEC as measured on the standard CoNLL14 benchmark dataset.",abstractText,[0],[0]
"Further analysis also shows that the superiority of the proposed model can be largely attributed to the use of the nested attention mechanism, which has proven particularly effective in correcting local errors that involve small edits in orthography.",abstractText,[0],[0]
A Nested Attention Neural Hybrid Model for Grammatical Error Correction,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1066–1076 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1066",text,[0],[0]
"The International Classification of Diseases (ICD) is a healthcare classification system maintained by the World Health Organization (Organization et al., 1978).",1 Introduction,[0],[0]
"It provides a hierarchy of diagnostic codes of diseases, disorders, injuries, signs, symptoms, etc.",1 Introduction,[0],[0]
"It is widely used for reporting diseases and health conditions, assisting in medical reimbursement decisions, collecting morbidity and mortality statistics, to name a few.
",1 Introduction,[0],[0]
"While ICD codes are important for making clinical and financial decisions, medical coding
– which assigns proper ICD codes to a patient visit – is time-consuming, error-prone, and expensive.",1 Introduction,[0],[0]
"Medical coders review the diagnosis descriptions written by physicians in the form of textual phrases and sentences, and (if necessary) other information in the electronic health record of a clinical episode, then manually attribute the appropriate ICD codes by following the coding guidelines (O’malley et al., 2005).",1 Introduction,[0],[0]
Several types of errors frequently occur.,1 Introduction,[0],[0]
"First, the ICD codes are organized in a hierarchical structure.",1 Introduction,[0],[0]
"For a node representing a disease C, the children of this node represent the subtypes of C. In many cases, the difference between disease subtypes is very subtle.",1 Introduction,[0],[0]
It is common that human coders select incorrect subtypes.,1 Introduction,[0],[0]
"Second, when writing diagnosis descriptions, physicians often utilize abbreviations and synonyms, which causes ambiguity and imprecision when the coders are matching ICD codes to those descriptions (Sheppard et al., 2008).",1 Introduction,[0],[0]
"Third, in many cases, several diagnosis descriptions are closely related and should be mapped to a single ICD code.",1 Introduction,[0],[0]
"However, unexperienced coders may code each disease separately.",1 Introduction,[0],[0]
Such errors are called unbundling.,1 Introduction,[0],[0]
"The cost incurred by coding errors and the financial investment spent on improving coding quality are estimated to be $25 billion per year in the US (Lang, 2007; Farkas and Szarvas, 2008).
",1 Introduction,[0],[0]
"To reduce coding errors and cost, we aim at building an ICD coding model which automatically and accurately translates the free-text diagnosis descriptions into ICD codes.",1 Introduction,[0],[0]
"To achieve this goal, several technical challenges need to be addressed.",1 Introduction,[0],[0]
"First, there exists a hierarchical structure among the ICD codes.",1 Introduction,[0],[0]
This hierarchy can be leveraged to improve coding accuracy.,1 Introduction,[0],[0]
"On one hand, if code A and B are both children of C, then it is unlikely to simultaneously assign A and B to a patient.",1 Introduction,[0],[0]
"On the other hand, if the distance be-
tween A and B in the code tree is smaller than that between A and C and we know A is the correct code, then B is more likely to be a correct code than C, since codes with smaller distance are more clinically relevant.",1 Introduction,[0],[0]
How to explore this hierarchical structure for better coding is technically demanding.,1 Introduction,[0],[0]
"Second, the diagnosis descriptions and the textual descriptions of ICD codes are written in quite different styles even if they refer to the same disease.",1 Introduction,[0],[0]
"In particular, the textual description of an ICD code is formally and precisely worded, while diagnosis descriptions are usually written by physicians in an informal and ungrammatical way, with telegraphic phrases, abbreviations, and typos.",1 Introduction,[0],[0]
"Third, it is required that the assigned ICD codes are ranked according to their relevance to the patient.",1 Introduction,[0],[0]
How to correctly determine this order is technically nontrivial.,1 Introduction,[0],[0]
"Fourth, as stated earlier, there does not necessarily exist an one-toone mapping between diagnosis descriptions and ICD codes, and human coders should consider the overall health condition when assigning codes.",1 Introduction,[0],[0]
"In many cases, two closely related diagnosis descriptions need to be mapped onto a single combination ICD code.",1 Introduction,[0],[0]
"On the other hand, physicians may write two health conditions into one diagnosis description which should be mapped onto two ICD codes under such circumstances.
",1 Introduction,[0],[0]
"Contributions In this paper, we design a neural architecture to automatically perform ICD coding given the diagnosis descriptions.",1 Introduction,[0],[0]
"Specifically, we make the following contributions:
• We propose a tree-of-sequences LSTM architecture to simultaneously capture the hierarchical relationship among codes and the semantics of each code.
",1 Introduction,[0],[0]
"• We use an adversarial learning approach to reconcile the heterogeneous writing styles of diagnosis descriptions and ICD code descriptions.
",1 Introduction,[0],[0]
"• We use isotonic constraints to preserve the importance order among codes and develop an algorithm based on ADMM and isotonic projection to solve the constrained problem.
",1 Introduction,[0],[0]
"• We use an attentional matching mechanism to perform many-to-one and one-to-many mappings between diagnosis descriptions and codes.
",1 Introduction,[0],[0]
"• On a clinical datasets with 59K patient visits, we demonstrate the effectiveness of the proposed methods.
",1 Introduction,[0],[0]
The rest of the paper is organized as follows.,1 Introduction,[0],[0]
Section 2 introduces related works.,1 Introduction,[0],[0]
Section 3 and 4 present the dataset and methods.,1 Introduction,[0],[0]
Section 5 gives experimental results.,1 Introduction,[0],[0]
Section 6 presents conclusions and discussions.,1 Introduction,[0],[0]
"Larkey and Croft (1996) studied the automatic assignment of ICD-9 codes to dictated inpatient discharge summaries, using a combination of three classifiers: k-nearest neighbors, relevance feedback, and Bayesian independence classifiers.",2 Related Works,[0],[0]
This method assigns a single code to each patient visit.,2 Related Works,[0],[0]
"However, in clinical practice, each patient is usually assigned with multiple codes.",2 Related Works,[0],[0]
Franz et al. (2000) investigated the automated coding of German-language free-text diagnosis phrases.,2 Related Works,[0],[0]
This approach performs one-to-one mapping between diagnosis descriptions and ICD codes.,2 Related Works,[0],[0]
"This is not in accordance with the coding practice where one-to-many and many-to-one mappings widely exist (O’malley et al., 2005).",2 Related Works,[0],[0]
Pestian et al. (2007) studied the assignment of ICD-9 codes to radiology reports.,2 Related Works,[0],[0]
Kavuluru et al. (2013) proposed an unsupervised ensemble approach to automatically perform ICD-9 coding based on textual narratives in electronic health records (EHRs),2 Related Works,[0],[0]
"Kavuluru et al. (2015) developed multi-label classification, feature selection, and learning to rank approaches for ICD-9 code assignment of in-patient visits based on EHRs.",2 Related Works,[0],[0]
Koopman et al. (2015) explored the automatic ICD-10 classification of cancers from free-text death certificates.,2 Related Works,[0],[0]
"These methods did not consider the hierarchical relationship or importance order among codes.
",2 Related Works,[0],[0]
"The tree LSTM network was first proposed by (Tai et al., 2015) to model the constituent or dependency parse trees of sentences.",2 Related Works,[0],[0]
Teng and Zhang (2016) extended the unidirectional tree LSTM to a bidirectional one.,2 Related Works,[0],[0]
Xie and Xing (2017) proposed a sequence-of-trees LSTM network to model a passage.,2 Related Works,[0],[0]
"In this network, a sequential LSTM is used to compose a sequence of tree LSTMs.",2 Related Works,[0],[0]
The tree LSTMs are built on the constituent parse trees of individual sentences and the sequential LSTM is built on the sequence of sentences.,2 Related Works,[0],[0]
Our proposed tree-of-sequences LSTM network differs from the previous works in twofold.,2 Related Works,[0],[0]
"First, it is applied to a code tree to capture the hierarchical relationship among codes.",2 Related Works,[0],[0]
"Second, it uses a tree LSTM to compose a hierarchy
Diagnosis Descriptions 1.",2 Related Works,[0],[0]
Prematurity at 35 4/7 weeks gestation 2.,2 Related Works,[0],[0]
Twin number two of twin gestation 3.,2 Related Works,[0],[0]
"Respiratory distress secondary to transient tachypnea
of the newborn 4.",2 Related Works,[0],[0]
Suspicion for sepsis ruled out Assigned ICD Codes 1.,2 Related Works,[0],[0]
"V31.00 (Twin birth, mate liveborn, born in hospital, delivered without mention of cesarean section) 2.",2 Related Works,[0],[0]
765.18,2 Related Works,[0],[0]
"(Other preterm infants, 2,000-2,499 grams) 3.",2 Related Works,[0],[0]
775.6 (Neonatal hypoglycemia) 4.,2 Related Works,[0],[0]
770.6 (Transitory tachypnea of newborn) 5.,2 Related Works,[0],[0]
V29.0,2 Related Works,[0],[0]
(Observation for suspected infectious condition) 6.,2 Related Works,[0],[0]
"V05.3 (Need for prophylactic vaccination and inoculation
against viral hepatitis)
Table 1:",2 Related Works,[0],[0]
The diagnosis descriptions of a patient visit and the assigned ICD codes.,2 Related Works,[0],[0]
Inside the parentheses are the descriptions of the codes.,2 Related Works,[0],[0]
"The codes are ranked according to descending importance.
of sequential LSTMs.",2 Related Works,[0],[0]
"Adversarial learning (Goodfellow et al., 2014) has been widely applied to image generation (Goodfellow et al., 2014), domain adaption (Ganin and Lempitsky, 2015), feature learning (Donahue et al., 2016), text generation (Yu et al., 2017), to name a few.",2 Related Works,[0],[0]
"In this paper, we use adversarial learning for mitigating the discrepancy among the writing styles of a pair of sentences.
",2 Related Works,[0],[0]
"The attention mechanism was widely used in machine translation (Bahdanau et al., 2014), image captioning (Xu et al., 2015), reading comprehension (Seo et al., 2016), text classification (Yang et al., 2016), etc.",2 Related Works,[0],[0]
"In this work, we compute attention between sentences to perform many-to-one and one-to-many mappings.",2 Related Works,[0],[0]
"We performed the study on the publicly available MIMIC-III dataset (Johnson et al., 2016), which contains de-identified electronic health records (EHRs) of 58,976 patient visits in the Beth Israel Deaconess Medical Center from 2001 to 2012.",3 Dataset and Preprocessing,[0],[0]
"Each EHR has a clinical note called discharge summary, which contains multiple sections of information, such as ‘discharge diagnosis’, ‘past medical history’, etc.",3 Dataset and Preprocessing,[0],[0]
"From the ‘discharge diagnosis’ and ‘final diagnosis’ sections, we extracted the diagnosis descriptions (DDs) written by physicians.",3 Dataset and Preprocessing,[0],[0]
"Each DD is a short phrase or a sentence, articulating a certain disease or condition.",3 Dataset and Preprocessing,[0],[0]
Medical coders perform ICD coding mainly based on DDs.,3 Dataset and Preprocessing,[0],[0]
"Following such a practice, in this paper, we set the inputs of the automated coding model to be
the DDs while acknowledging that other information in the EHRs is also valuable and is referred to by coders for code assignment.",3 Dataset and Preprocessing,[0],[0]
"For simplicity, we leave the incorporation of non-DD information to future study.
",3 Dataset and Preprocessing,[0],[0]
"Each patient visit is assigned with a list of ICD codes, ranked in descending order of importance and relevance.",3 Dataset and Preprocessing,[0],[0]
"For each visit, the number of codes is usually not equal to the number of diagnosis descriptions.",3 Dataset and Preprocessing,[0],[0]
These ground-truth codes serve as the labels to train our coding model.,3 Dataset and Preprocessing,[0],[0]
"The entire dataset contains 6,984 unique codes, each of which has a textual description, describing a disease, symptom, or condition.",3 Dataset and Preprocessing,[0],[0]
The codes are organized into a hierarchy where the top-level codes correspond to general diseases while the bottom-level ones represent specific diseases.,3 Dataset and Preprocessing,[0],[0]
"In the code tree, children of a node represent subtypes of a disease.",3 Dataset and Preprocessing,[0],[0]
Table 1 shows the DDs and codes of an exemplar patient.,3 Dataset and Preprocessing,[0],[0]
"In this section, we present a neural architecture for ICD coding.",4 Methods,[0],[0]
Figure 1 shows the overview of our approach.,4.1 Overview,[0],[0]
The proposed ICD coding model consists of five modules.,4.1 Overview,[0],[0]
The model takes the ICD-code tree and diagnosis descriptions (DDs) of a patient as inputs and assigns a set of ICD codes to the patient.,4.1 Overview,[0],[0]
The encoder of DDs generates a latent representation vector for a DD.,4.1 Overview,[0],[0]
"The encoder of ICD codes is a tree-of-sequences long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) network.",4.1 Overview,[0],[0]
"It takes the textual descriptions of the ICD codes and their hierarchical structure as in-
puts and produces a latent representation for each code.",4.1 Overview,[0],[0]
The representation aims at simultaneously capturing the semantics of each code and the hierarchical relationship among codes.,4.1 Overview,[0],[0]
"By incorporating the code hierarchy, the model can avoid selecting codes that are subtypes of the same disease and promote the selection of codes that are clinically correlated.",4.1 Overview,[0],[0]
"The writing styles of DDs and code descriptions (CDs) are largely different, which makes the matching between a DD and a CD error-prone.",4.1 Overview,[0],[0]
"To address this issue, we develop an adversarial learning approach to reconcile the writing styles.",4.1 Overview,[0],[0]
"On top of the latent representation vectors of the descriptions, we build a discriminative network to distinguish which ones are DDs and which are CDs.",4.1 Overview,[0],[0]
The encoders of DDs and CDs try to make such a discrimination impossible.,4.1 Overview,[0],[0]
"By doing this, the learned representations are independent of the writing styles and facilitate more accurate matching.",4.1 Overview,[0],[0]
The representations of DDs and CDs are fed into an attentional matching module to perform code assignment.,4.1 Overview,[0],[0]
This attentional mechanism allows multiple DDs to be matched to a single code and allows a single DD to be matched to multiple codes.,4.1 Overview,[0],[0]
"During training, we incorporate the order of importance among codes as isotonic constraints.",4.1 Overview,[0],[0]
These constraints regulate the model’s weight parameters so that codes with higher importance are given larger prediction scores.,4.1 Overview,[0],[0]
This section introduces the encoder of ICD codes.,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
Each code has a description (a sequence of words) that tells the semantics of this code.,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"We use a sequential LSTM (SLSTM) (Hochreiter and Schmidhuber, 1997) to encode this description.",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"To capture the hierarchical relationship among codes, we build a tree LSTM (TLSTM) (Tai et al., 2015) along the code tree.",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"At each TLSTM node, the input vector is the latent representation generated
by the SLSTM.",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"Combining these two types of LSTMs together, we obtain a tree-of-sequences LSTM network (Figure 2).
",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"Sequential LSTM A sequential LSTM (SLSTM) (Hochreiter and Schmidhuber, 1997) network is a special type of recurrent neural network that (1) learns the latent representation (which usually reflects certain semantic information) of words, and (2) models the sequential structure among words.",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"In the word sequence, each word t is allocated with an SLSTM unit, which consists of the following components: an input gate it, a forget gate ft, an output gate ot, a memory cell ct, and a hidden state st.",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"These components (vectors) are computed as follows:
it = σ(W",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
(i)st−1 + U (i)xt + b,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
(i)),4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
ft = σ(W (f)st−1 + U (f)xt + b (f)),4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
ot = σ(W (o)st−1 + U (o)xt + b (o)),4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
ct,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
= it tanh(W(c)st−1 + U(c)xt + b(c)),4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"+ft ct−1 st = ot tanh(ct) (1) where xt is the embedding vector of word t. W, U are component-specific weight matrices and b are bias vectors.
",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"Tree-of-sequences LSTM We use a bidirectional tree LSTM (TLSTM) (Tai et al., 2015; Xie and Xing, 2017) to capture the hierarchical relationships among codes.",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
The inputs of this LSTM include the code hierarchy and hidden states of individual codes produced by the SLSTMs.,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"It consists of a bottom-up TLSTM and a top-down TLSTM, which produce two hidden states h↑ and h↓ at each node in the tree.
",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"In the bottom-up TLSTM, an internal node (representing a code C, having M children) is comprised of these components: an input gate i↑, an output gate o↑, a memory cell c↑, a hidden state h↑ and M child-specific forget gates {f (m) ↑ } M m=1 where f (m)↑ corresponds to the m-th child.",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"The transition equations among components are:
i↑ = σ",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
( ∑M m=1,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"W (i,m) ↑",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
h (m) ↑ +,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
U (i)s + b,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
(i) ↑ ),4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"∀m, f (m)↑ = σ(W",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"(f,m) ↑ h",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
(m) ↑ +,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"U (f,m)s + b",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"(f,m) ↑ )
",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
o↑ = σ,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
( ∑M m=1,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"W (o,m) ↑",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
h (m) ↑ +,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"U (o)s + b (o) ↑ )
",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"u↑ = tanh( ∑M m=1 W (u,m) ↑",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
h (m) ↑ +,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
U (u)s + b,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"(u) ↑ )
",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
c↑ = i↑ u↑ + ∑M m=1,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
f (m) ↑,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
c (m) ↑,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
h↑ =,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
o↑ tanh(c↑),4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"(2)
where s is the SLSTM hidden state that encodes the description of code C; {h(m)↑ } M m=1 and {c(m)↑ } M m=1 are the bottom-up TLSTM hidden states and memory cells of the children.",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"W, U, b are component-specific weight matrices and bias vectors.",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"For a leaf node having no children, its only input is the SLSTM hidden state s and no forget gates are needed.
",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"In the top-down TLSTM, for a non-root node, it has such components: an input gate i↓, a forget gate f↓, an output gate o↓, a memory cell c↓ and a hidden state",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
h↓.,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"The transition equations are:
i↓ = σ(W",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
(i) ↓,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
h (p) ↓,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
+ b (i) ↓ ),4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
f↓ = σ(W (f) ↓,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
h (p) ↓,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
+ b,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
(f) ↓ ),4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
o↓,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
= σ(W (o) ↓,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
h (p) ↓,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
+ b (o) ↓ ) u↓ =,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
tanh(W (u) ↓,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
h (p) ↓,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
+ b (u) ↓ ) c↓ = i↓ u↓ + f↓,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"c (p) ↓ h↓ = o↓ tanh(c↓)
(3)
where h(p)↓ and c (p) ↓ are the top-down TLSTM hidden state and memory cell of the parent of this node.",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"For the root node which has no parent, h↓ cannot be computed using the above equations.",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"Instead, we set h↓ to h↑ (the bottom-up TLSTM hidden state generated at the root node).",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"h↑ captures the semantics of all codes in this hierarchy, which is then propagated downwards to each individual code via the top-down TLSTM dynamics.
",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
We concatenate the hidden states of the two directions to obtain the bidirectional TLSTM encoding of each code h =,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
[h↑;h↓].,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"The bottom-up TLSTM composes the semantics of children (representing sub-diseases) and merge them into the current node, which hence captures child-to-parent relationship.",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"The top-down TLSTM makes each node inherit the semantics of its parent, which captures parent-to-child relation.",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"As a result, the hierarchical relationship among codes are encoded in the hidden states.
",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"For the diagnosis descriptions of a patient, we use an SLSTM network to encode each description individually.",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
The weight parameters of this SLSTM are tied with those of the SLSTM used for encoding code descriptions.,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"Next, we introduce how to map the DDs to codes.",4.3 Attentional Matching,[0],[0]
"We denote the hidden representations of DDs and codes as {hm}Mm=1 and {un}Nn=1 respectively, where M is the number of DDs of one patient and
N is the total number of codes in the dataset.",4.3 Attentional Matching,[0],[0]
The mapping from DDs to codes is not one-to-one.,4.3 Attentional Matching,[0],[0]
"In many cases, a code is assigned only when a certain combination of K (1 < K ≤ M ) diseases simultaneously appear within the M DDs and the value of K depends on this code.",4.3 Attentional Matching,[0],[0]
"Among the K diseases, their importance of determining the assignment of this code is different.",4.3 Attentional Matching,[0],[0]
"For the rest M −K DDs, we can consider their importance score to be zero.",4.3 Attentional Matching,[0],[0]
"We use a soft-attention mechanism (Bahdanau et al., 2014) to calculate these importance scores.",4.3 Attentional Matching,[0],[0]
"For a code un, the importance of a DD hm to un is calculated as anm = u>nhm.",4.3 Attentional Matching,[0],[0]
We normalize the scores {anm}Mm=1 of all DDs into a probabilistic simplex using the softmax operation: ãnm = exp(anm)/ ∑M l=1 exp(anl).,4.3 Attentional Matching,[0],[0]
"Given these normalized importance scores {ãnm}Mm=1, we use them to weight the representations of DDs and get a single attentional vector of the M DDs: ĥn = ∑M m=1 ãnmhm.",4.3 Attentional Matching,[0],[0]
"Then we concatenate ĥn and un, and use a linear classifier to predict the probability that code n should be assigned: pn = sigmoid(w>n",4.3 Attentional Matching,[0],[0]
"[ĥn;un] + bn), where the coefficients wn and bias bn are specific to code",4.3 Attentional Matching,[0],[0]
"n.
We train the weight parameters Θ of the proposed model using the data of L patient visits.",4.3 Attentional Matching,[0],[0]
"Θ includes the sequential LSTM weights Ws, tree LSTM weights Wt and weights Wp in the final prediction layer.",4.3 Attentional Matching,[0],[0]
Let c(l) ∈ RN be a binary vector where c(l)n = 1 if the n-th code is assigned to this patient and c(l)n = 0,4.3 Attentional Matching,[0],[0]
if otherwise.,4.3 Attentional Matching,[0],[0]
"Θ can be learned by minimizing the following prediction loss:
minΘ Lpred(Θ) = L∑",4.3 Attentional Matching,[0],[0]
l=1,4.3 Attentional Matching,[0],[0]
"N∑ n=1 CE(p(l)n , c (l) n )",4.3 Attentional Matching,[0],[0]
"(4)
where p(l)n is the predicted probability that code n is assigned to patient visit l and p(l)n is a function of Θ. CE(·, ·) is the cross-entropy loss.",4.3 Attentional Matching,[0],[0]
"We use an adversarial learning (Goodfellow et al., 2014) approach to reconcile the different writing styles of diagnosis descriptions (DDs) and code descriptions (CDs).",4.4 Adversarial Reconciliation of Writing Styles,[0],[0]
"The basic idea is: after encoded, if a description cannot be discerned to be a DD or a CD, then the difference in their writing styles is eliminated.",4.4 Adversarial Reconciliation of Writing Styles,[0],[0]
"We build a discriminative network which takes the encoding vector of a description as input and tries to identify it as a DD
or CD.",4.4 Adversarial Reconciliation of Writing Styles,[0],[0]
The encoders of DDs and CDs adjust their weight parameters so that such a discrimination is difficult to be achieved by the discriminative network.,4.4 Adversarial Reconciliation of Writing Styles,[0],[0]
"Consider all the descriptions {tr, yr}Rr=1 where tr is a description and yr is a binary label.",4.4 Adversarial Reconciliation of Writing Styles,[0],[0]
yr = 1 if tr is a DD and yr = 0,4.4 Adversarial Reconciliation of Writing Styles,[0],[0]
if otherwise.,4.4 Adversarial Reconciliation of Writing Styles,[0],[0]
Let f(tr;Ws) denote the sequential LSTM (SLSTM) encoder parameterized by Ws.,4.4 Adversarial Reconciliation of Writing Styles,[0],[0]
This encoder is shared by the DDs and CDs.,4.4 Adversarial Reconciliation of Writing Styles,[0],[0]
"Note that for CDs, a tree LSTM is further applied on top of the encodings produced by the SLSTM.",4.4 Adversarial Reconciliation of Writing Styles,[0],[0]
We use the SLSTM encoding vectors of CDs as the input of the discriminative network rather than using the TLSTM encodings since the latter are irrelevant to writing styles.,4.4 Adversarial Reconciliation of Writing Styles,[0],[0]
Let g(f(tr;Ws);Wd) denote the discriminative network parameterized by Wd.,4.4 Adversarial Reconciliation of Writing Styles,[0],[0]
It takes the encoding vector f(tr;Ws) as input and produces the probability that tr is a DD.,4.4 Adversarial Reconciliation of Writing Styles,[0],[0]
"Adversarial learning is performed by solving this problem:
",4.4 Adversarial Reconciliation of Writing Styles,[0],[0]
"max Ws min Wd Ladv = R∑ r=1 CE(g(f(tr;Ws);Wd), yr)
(5)",4.4 Adversarial Reconciliation of Writing Styles,[0],[0]
The discriminative network tries to differentiate DDs from CDs by minimizing this classification loss while the encoder maximizes this loss so that DDs and CDs are not distinguishable.,4.4 Adversarial Reconciliation of Writing Styles,[0],[0]
"Next, we incorporate the importance order among ICD codes.",4.5 Isotonic Constraints,[0],[0]
"For the D(l) codes assigned to patient l, without loss of generality, we assume the order is 1 2 · · · D(l) (the order is given by human coders as ground-truth in the MIMIC-III dataset).",4.5 Isotonic Constraints,[0],[0]
"We use the predicted probability pi (1 ≤ i ≤ D(l)) defined in Section 4.3 to characterize the importance of code i. To incorporate the order, we impose an isotonic constraint on the probabilities: p
(l) 1 p (l) 2 · · · p (l) D(l) , and solve the following
problem:
minΘ Lpred(Θ) + maxWd(−λLadv(Ws,Wd))",4.5 Isotonic Constraints,[0],[0]
"s.t. p
(l) 1 p (l) 2 · · · p (l)
D(l)
∀l = 1, · · · , L (6)
where the probabilities p(l)i are functions of Θ and λ is a tradeoff parameter.
",4.5 Isotonic Constraints,[0],[0]
"We develop an algorithm based on the alternating direction method of multiplier (ADMM) (Boyd et al., 2011) to solve the problem defined in Eq.(6).",4.5 Isotonic Constraints,[0],[0]
"Let p(l) be a |D(l)|-dimensional
vector where the i-th element is p(l)i .",4.5 Isotonic Constraints,[0],[0]
"We first write the problem into an equivalent form
minΘ Lpred(Θ)",4.5 Isotonic Constraints,[0],[0]
"+ maxWd(−λLadv(Ws,Wd))",4.5 Isotonic Constraints,[0],[0]
"s.t. p(l) = q(l)
q",4.5 Isotonic Constraints,[0],[0]
"(l) 1 q (l) 2 · · · q (l) |D(l)| ∀l = 1, · · · , L
(7) Then we write down the augmented Lagrangian
min Θ,q,v Lpred(Θ)",4.5 Isotonic Constraints,[0],[0]
"+ maxWd(−λLadv(Ws,Wd))
",4.5 Isotonic Constraints,[0],[0]
+〈p(l),4.5 Isotonic Constraints,[0],[0]
"− q(l),v(l)〉+ ρ2‖p (l)",4.5 Isotonic Constraints,[0],[0]
"− q(l)‖22
s.t. q",4.5 Isotonic Constraints,[0],[0]
"(l) 1 q (l) 2 · · · q (l)
|D(l)| ∀l = 1, · · · , L
(8) We solve this problem by alternating between {p(l)}Ll=1, {q(l)}Ll=1 and {v(l)}Ll=1 The subproblem defined over q(l) is
minq(l) −〈q(l),v(l)〉+ ρ 2‖p (l)",4.5 Isotonic Constraints,[0],[0]
"− q(l)‖22 s.t. q
(l) 1 q (l) 2 · · · q (l) |D(l)| (9)
which is an isotonic projection problem and can be solved via the algorithm proposed in (Yu and Xing, 2016).",4.5 Isotonic Constraints,[0],[0]
"With {q(l)}Ll=1 and {v(l)}Ll=1 fixed, the sub-problem is minΘ Lpred(Θ) + maxWd(−λLadv(Ws,Wd))",4.5 Isotonic Constraints,[0],[0]
which can be solved using stochastic gradient descent (SGD).,4.5 Isotonic Constraints,[0],[0]
The update of v(l) is simple: v(l) = v(l) + ρ(p(l)−q(l)).,4.5 Isotonic Constraints,[0],[0]
"In this section, we present experiment results.",5 Experiments,[0],[0]
"Out of the 6,984 unique codes, we selected 2,833 codes that have the top frequencies to perform the study.",5.1 Experimental Settings,[0],[0]
We split the data into a train/validation/test dataset with 40k/7k/12k patient visits respectively.,5.1 Experimental Settings,[0],[0]
The hyperparameters were tuned on the validation set.,5.1 Experimental Settings,[0],[0]
"The SLSTMs were bidirectional and dropout with 0.5 probability (Srivastava et al., 2014) was used.",5.1 Experimental Settings,[0],[0]
The size of hidden states in all LSTMs was set to 100.,5.1 Experimental Settings,[0],[0]
The word embeddings were trained on the fly and their dimension was set to 200.,5.1 Experimental Settings,[0],[0]
The tradeoff parameter λ was set to 0.1.,5.1 Experimental Settings,[0],[0]
The parameter ρ in the ADMM algorithm was set to 1.,5.1 Experimental Settings,[0],[0]
"In the SGD algorithm for solving minΘ Lpred(Θ)+maxWd(−λLadv(Ws,Wd)), we used the ADAM (Kingma and Ba, 2014) optimizer with an initial learning rate 0.001 and a minibatch size 20.",5.1 Experimental Settings,[0],[0]
"Sensitivity (true positive rate) and
specificity (true negative rate) were used to evaluate the code assignment performance.",5.1 Experimental Settings,[0],[0]
"We calculated these two scores for each individual code on the test set, then took a weighted (proportional to codes’ frequencies) average across all codes.",5.1 Experimental Settings,[0],[0]
"To evaluate the ranking performance of codes, we used normalized discounted cumulative gain (NDCG) (Järvelin and Kekäläinen, 2002).",5.1 Experimental Settings,[0],[0]
We perform ablation study to verify the effectiveness of each module in our model.,5.2 Ablation Study,[0],[0]
"To evaluate module X, we remove it from the model without changing other modules and denote such a baseline by No-X. The comparisons of No-X with the full model are given in Table 2.
",5.2 Ablation Study,[0],[0]
"Tree-of-sequences LSTM To evaluate this module, we compared with the two configurations: (1) No-TLSTM, which removes the tree LSTM and directly uses the hidden states produced by the sequential LSTM as final representations of codes; (2) Bottom-up TLSTM, which removes the hidden states generated by the top-down TLSTM.",5.2 Ablation Study,[0],[0]
"In addition, we compared with four hierarchical classification baselines including (1) hierarchical network (HierNet) (Yan et al., 2015), (2) HybridNet (Hou et al., 2017), (3) branch network (BranchNet) (Zhu and Bain, 2017), (4) label embedding tree (LET) (Bengio et al., 2010), by using them to replace the bidirectional tree LSTM while keeping other modules untouched.",5.2 Ablation Study,[0],[0]
Table 2 shows the average sensitivity and specificity scores achieved by these methods on the test set.,5.2 Ablation Study,[0],[0]
We make the following observations.,5.2 Ablation Study,[0],[0]
"First, removing tree LSTM largely degrades performance: the sensitivity and specificity of No-TLSTM is 0.23 and 0.28 respectively while our full model (which uses bidirectional TLSTM) achieves 0.29 and 0.33 respectively.",5.2 Ablation Study,[0],[0]
The reason is No-TLSTM ignores the hierarchical relationship among codes.,5.2 Ablation Study,[0],[0]
"Second, bottom-up tree LSTM alone performs less well than bidirectional tree LSTM.",5.2 Ablation Study,[0],[0]
"This demonstrates the necessity of the top-down TLSTM, which ensures every two codes are connected by directed paths and can more expressively capture code-relations in the hierarchy.",5.2 Ablation Study,[0],[0]
"Third, our method outperforms the four baselines.",5.2 Ablation Study,[0],[0]
"The possible reason is our method directly builds codes’ hierarchical relationship into their representations while the baselines perform representation-learning and relationship-capturing
separately.
",5.2 Ablation Study,[0],[0]
"Next, we present some qualitative results.",5.2 Ablation Study,[0],[0]
"For a patient (admission ID 147798) having a DD ‘E Coli urinary tract infection’, without using tree LSTM, two sibling codes 585.2 (chronic kidney disease, stage II (mild)) – which is the groundtruth – and 585.4 (chronic kidney disease, stage IV (severe)) are simultaneously assigned possibly because their textual descriptions are very similar (only differ in the level of severity).",5.2 Ablation Study,[0],[0]
This is incorrect because 585.2 and 585.4 are the children of 585 (chronic kidney disease) and the severity level of this disease cannot simultaneously be mild and severe.,5.2 Ablation Study,[0],[0]
"After tree LSTM is added, the false prediction of 585.4 is eliminated, which demonstrates the effectiveness of tree LSTM in incorporating one constraint induced by the code hierarchy: among the nodes sharing the same parent, only one should be selected.
",5.2 Ablation Study,[0],[0]
"For patient 197205, No-TLSTM assigns the following codes: 462 (subacute sclerosing panencephalitis), 790.29 (other abnormal glucose), 799.9 (unspecified viral infection), and 285.21 (anemia in chronic kidney disease).",5.2 Ablation Study,[0],[0]
"Among these codes, the first three are ground-truth and the fourth one is incorrect (the ground-truth is 401.9 (unspecified essential hypertension)).",5.2 Ablation Study,[0],[0]
Adding tree LSTM fixes this error.,5.2 Ablation Study,[0],[0]
The average distance between 401.9 and the rest of ground-truth codes is 6.2.,5.2 Ablation Study,[0],[0]
"For the incorrectly assigned code 285.21, such a distance is 7.9.",5.2 Ablation Study,[0],[0]
"This demonstrates that tree LSTM is able to capture another constraint imposed by the hierarchy: codes with smaller treedistance are more likely to be assigned together.
",5.2 Ablation Study,[0],[0]
"Adversarial learning To evaluate the efficacy of adversarial learning (AL), we remove it from the full model and refer to this baseline as No-AL.",5.2 Ablation Study,[0],[0]
"Specifically, in Eq.(6), the loss term maxWd(−Ladv(Ws,Wd)) is taken away.",5.2 Ablation Study,[0],[0]
"Table 2 shows the results, from which we observe that after AL is removed, the sensitivity and specificity are dropped from 0.29 and 0.33 to 0.26 and 0.31 respectively.",5.2 Ablation Study,[0],[0]
No-AL does not reconcile different writing styles of diagnosis descriptions (DDs) and code descriptions (CDs).,5.2 Ablation Study,[0],[0]
"As a result, a DD and a CD that have similar semantics may be mismatched because their writing styles are different.",5.2 Ablation Study,[0],[0]
"For example, a patient (admission ID 147583) has a DD ‘h/o DVT on anticoagulation’, which contains abbreviation DVT (deep vein thrombosis).",5.2 Ablation Study,[0],[0]
"Due to the presence of this abbreviation, it is difficult to assign a proper code to this DD since the textual descriptions of codes do not contain abbreviations.",5.2 Ablation Study,[0],[0]
"With adversarial learning, our model can correctly map this DD to a ground-truth code: 443.9 (peripheral vascular disease, unspecified).",5.2 Ablation Study,[0],[0]
"Without AL, this code is not selected.",5.2 Ablation Study,[0],[0]
"As another example, a DD ‘coronary artery disease, STEMI, s/p 2 stents placed in RCA’ was given to patient 148532.",5.2 Ablation Study,[0],[0]
"This DD is written informally and ungrammatically, and contains too much detailed information, e.g., ‘s/p 2 stents placed in RCA’.",5.2 Ablation Study,[0],[0]
Such a writing style is quite different from that of CDs.,5.2 Ablation Study,[0],[0]
"With AL, our model successfully matches this DD to a ground-truth code: 414.01 (coronary atherosclerosis of native coronary artery).",5.2 Ablation Study,[0],[0]
"On the contrary, No-AL fails to achieve this.
",5.2 Ablation Study,[0],[0]
"Isotonic constraint (IC) To evaluate this ingredient, we remove the ICs from Eq.(6) during training and denote this baseline as No-IC.",5.2 Ablation Study,[0],[0]
"We use NDCG to measure the ranking performance, which is calculated in the following way.",5.2 Ablation Study,[0],[0]
Consider a testing patient-visit lwhere the ground-truth ICD codes are M(l).,5.2 Ablation Study,[0],[0]
"For any code c, we define the relevance score of c to l as 0 if c /∈ M(l) and as |M(l)| − r(c) if otherwise, where r(c) is the ground-truth rank of c inM(l).",5.2 Ablation Study,[0],[0]
"We rank codes in descending order of their corresponding prediction
probabilities and obtain the predicted rank for each code.",5.2 Ablation Study,[0],[0]
"We calculate the NDCG scores at position 2, 4, 6, 8 based on the relevance scores and predicted ranks, which are shown in Table 3.",5.2 Ablation Study,[0],[0]
"As can be seen, using IC achieves much higher NDCG than NoIC, which demonstrates the effectiveness of IC in capturing the importance order among codes.
",5.2 Ablation Study,[0],[0]
We also evaluate how IC affects the sensitivity and specificity of code assignment.,5.2 Ablation Study,[0],[0]
"As can be seen from Table 2, No-IC degrades the two scores from 0.29 and 0.33 to 0.24 and 0.29 respectively, which indicates that IC is helpful in training a model that can more correctly assign codes.",5.2 Ablation Study,[0],[0]
"This is because IC encourages codes that are highly relevant to the patients to be ranked at top positions, which prevents the selection of irrelevant codes.
",5.2 Ablation Study,[0],[0]
Attentional matching (AM),5.2 Ablation Study,[0],[0]
"In the evaluation of this module, we compare with a baseline – No-AM, which performs an unweighted average of the M DDs:",5.2 Ablation Study,[0],[0]
ĥn = 1M ∑M m=1,5.2 Ablation Study,[0],[0]
"hm, concatenates ĥn with un and feeds the concatenated vector into the final prediction layer.",5.2 Ablation Study,[0],[0]
"From Table 2, we can see our full model (with AM) outperforms No-AM, which demonstrates the effectiveness of attentional matching.",5.2 Ablation Study,[0],[0]
"In determining whether a code should be assigned, different DDs have different importance weights.",5.2 Ablation Study,[0],[0]
"No-AM ignores such weights, therefore performing less well.
",5.2 Ablation Study,[0],[0]
AM can correctly perform many-to-one mapping from multiple DDs to a CD.,5.2 Ablation Study,[0],[0]
"For example, patient 190236 was given two DDs: ‘renal insufficiency’ and ‘acute renal failure’.",5.2 Ablation Study,[0],[0]
"AM maps them to a combined ICD code: 403.91 (hypertensive chronic kidney disease, unspecified, with chronic kidney disease stage V or end stage renal disease), which is in the ground-truth provided by medical coders.",5.2 Ablation Study,[0],[0]
"On the contrary, No-AM fails to assign this code.",5.2 Ablation Study,[0],[0]
"On the other hand, AM is able to correctly map a DD to multiple CDs.",5.2 Ablation Study,[0],[0]
"For example, a DD ‘congestive heart failure, diastolic’ was given to patient 140851.",5.2 Ablation Study,[0],[0]
"AM successfully maps this DD to two codes: (1) 428.0 (congestive heart failure, unspecified); (2) 428.30 (diastolic heart failure, unspecified).",5.2 Ablation Study,[0],[0]
"Without AM, this DD is mapped only to 428.0.",5.2 Ablation Study,[0],[0]
"In addition to evaluating the four modules individually, we also compared our full model with four other baselines proposed by (Larkey and Croft,
1996; Franz et al., 2000; Pestian et al., 2007; Kavuluru et al., 2013, 2015; Koopman et al., 2015) for ICD coding.",5.3 Holistic Comparison with Other Baselines,[0],[0]
Table 2 shows the results.,5.3 Holistic Comparison with Other Baselines,[0],[0]
"As can be seen, our approach achieves much better sensitivity and specificity scores.",5.3 Holistic Comparison with Other Baselines,[0],[0]
The reason that our model works better is two-fold.,5.3 Holistic Comparison with Other Baselines,[0],[0]
"First, our model is based on deep neural network, which has arguably better modeling power than linear methods used in the baselines.",5.3 Holistic Comparison with Other Baselines,[0],[0]
"Second, our model is able to capture the hierarchical relationship and importance order among codes, can alleviate the discrepancy in writing styles and allows flexible many-toone and one-to-many mappings from DDs to CDs.",5.3 Holistic Comparison with Other Baselines,[0],[0]
These merits are not possessed by the baselines.,5.3 Holistic Comparison with Other Baselines,[0],[0]
"In this paper, we build a neural network model for automated ICD coding.",6 Conclusions and Discussions,[0],[0]
Evaluations on the MIMIC-III dataset demonstrate the following.,6 Conclusions and Discussions,[0],[0]
"First, the tree-of-sequences LSTM network effectively discourages the co-selection of sibling codes and promotes the co-assignment of clinicallyrelevant codes.",6 Conclusions and Discussions,[0],[0]
Adversarial learning improves the matching accuracy by alleviating the discrepancy among the writing styles of DDs and CDs.,6 Conclusions and Discussions,[0],[0]
"Third, isotonic constraints promote the correct ranking of codes.",6 Conclusions and Discussions,[0],[0]
"Fourth, the attentional matching mechanism is able to perform many-to-one and one-tomany mappings.
",6 Conclusions and Discussions,[0],[0]
"In the coding practice of human coders, in addition to the diagnosis descriptions, other information contained in nursing notes, lab values, and medical procedures are also leveraged for code assignment.",6 Conclusions and Discussions,[0],[0]
We have initiated preliminary investigation along this line and added two new input sources: (1) the rest of discharge summary and (2) lab values.,6 Conclusions and Discussions,[0],[0]
The sensitivity is improved from 0.29 to 0.32 and the specificity is improved from 0.33 to 0.35.,6 Conclusions and Discussions,[0],[0]
"A full study is ongoing.
",6 Conclusions and Discussions,[0],[0]
"At present, the major limitations of this work include: (1) it does not perform well on infrequent codes; (2) it is less capable of dealing with abbreviations.",6 Conclusions and Discussions,[0],[0]
"We will address these two issues in future by investigating diversity-promoting regularization (Xie et al., 2017) and leveraging an external knowledge base that maps medical abbreviations into their full names.
",6 Conclusions and Discussions,[0],[0]
The proposed methods can be applied to other tasks in NLP.,6 Conclusions and Discussions,[0],[0]
The tree-of-sequences model can be applied for ontology annotation.,6 Conclusions and Discussions,[0],[0]
"It takes the textual descriptions of concepts in the ontology
and their hierarchical structure as inputs and produces a latent representation for each concept.",6 Conclusions and Discussions,[0],[0]
The representations can simultaneously capture the semantics of codes and their relationships.,6 Conclusions and Discussions,[0],[0]
The proposed adversarial reconciliation of writing styles and attentional matching can be applied for knowledge mapping or entity linking.,6 Conclusions and Discussions,[0],[0]
"For example, in tweets, we can use the method to map an informally written mention ‘nbcbightlynews’ to a canonical entity ‘NBC Nightly News’ in the knowledge base.",6 Conclusions and Discussions,[0],[0]
We would like to thank the anonymous reviewers for their very constructive and helpful comments and suggestions.,Acknowledgements,[0],[0]
"Pengtao Xie and Eric P. Xing are supported by National Institutes of Health P30DA035778, Pennsylvania Department of Health BD4BH4100070287, and National Science Foundation IIS1617583.",Acknowledgements,[0],[0]
The International Classification of Diseases (ICD) provides a hierarchy of diagnostic codes for classifying diseases.,abstractText,[0],[0]
Medical coding – which assigns a subset of ICD codes to a patient visit – is a mandatory process that is crucial for patient care and billing.,abstractText,[0],[0]
"Manual coding is time-consuming, expensive, and errorprone.",abstractText,[0],[0]
"In this paper, we build a neural architecture for automated coding.",abstractText,[0],[0]
It takes the diagnosis descriptions (DDs) of a patient as inputs and selects the most relevant ICD codes.,abstractText,[0],[0]
"This architecture contains four major ingredients: (1) tree-ofsequences LSTM encoding of code descriptions (CDs), (2) adversarial learning for reconciling the different writing styles of DDs and CDs, (3) isotonic constraints for incorporating the importance order among the assigned codes, and (4) attentional matching for performing many-toone and one-to-many mappings from DDs to CDs.",abstractText,[0],[0]
We demonstrate the effectiveness of the proposed methods on a clinical datasets with 59K patient visits.,abstractText,[0],[0]
A Neural Architecture for Automated ICD Coding,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 379–389, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
Summarization is an important challenge of natural language understanding.,1 Introduction,[0],[0]
The aim is to produce a condensed representation of an input text that captures the core meaning of the original.,1 Introduction,[0],[0]
Most successful summarization systems utilize extractive approaches that crop out and stitch together portions of the text to produce a condensed version.,1 Introduction,[0],[0]
"In contrast, abstractive summarization attempts to produce a bottom-up summary, aspects of which may not appear as part of the original.
",1 Introduction,[0],[0]
We focus on the task of sentence-level summarization.,1 Introduction,[0],[0]
"While much work on this task has looked at deletion-based sentence compression techniques (Knight and Marcu (2002), among many others), studies of human summarizers show that it is common to apply various other operations while condensing, such as paraphrasing, generalization, and reordering (Jing, 2002).",1 Introduction,[0],[0]
"Past work has modeled this abstractive summarization problem either using linguistically-inspired constraints (Dorr et al., 2003; Zajic et al., 2004) or with syntactic transformations of the input text (Cohn and
Lapata, 2008; Woodsend et al., 2010).",1 Introduction,[0],[0]
"These approaches are described in more detail in Section 6.
",1 Introduction,[0],[0]
We instead explore a fully data-driven approach for generating abstractive summaries.,1 Introduction,[0],[0]
"Inspired by the recent success of neural machine translation, we combine a neural language model with a contextual input encoder.",1 Introduction,[0],[0]
Our encoder is modeled off of the attention-based encoder of Bahdanau et al. (2014) in that it learns a latent soft alignment over the input text to help inform the summary (as shown in Figure 1).,1 Introduction,[0],[0]
Crucially both the encoder and the generation model are trained jointly on the sentence summarization task.,1 Introduction,[0],[0]
The model is described in detail in Section 3.,1 Introduction,[0],[0]
"Our model also incorporates a beam-search decoder as well as additional features to model extractive elements; these aspects are discussed in Sections 4 and 5.
",1 Introduction,[0],[0]
"This approach to summarization, which we call Attention-Based Summarization (ABS), incorporates less linguistic structure than comparable abstractive summarization approaches, but can easily
379
scale to train on a large amount of data.",1 Introduction,[0],[0]
Since our system makes no assumptions about the vocabulary of the generated summary it can be trained directly on any document-summary pair.1,1 Introduction,[0],[0]
"This allows us to train a summarization model for headline-generation on a corpus of article pairs from Gigaword (Graff et al., 2003) consisting of around 4 million articles.",1 Introduction,[0],[0]
"An example of generation is given in Figure 2, and we discuss the details of this task in Section 7.
To test the effectiveness of this approach we run extensive comparisons with multiple abstractive and extractive baselines, including traditional syntax-based systems, integer linear programconstrained systems, information-retrieval style approaches, as well as statistical phrase-based machine translation.",1 Introduction,[0],[0]
Section 8 describes the results of these experiments.,1 Introduction,[0],[0]
Our approach outperforms a machine translation system trained on the same large-scale dataset and yields a large improvement over the highest scoring system in the DUC-2004 competition.,1 Introduction,[0],[0]
We begin by defining the sentence summarization task.,2 Background,[0],[0]
"Given an input sentence, the goal is to produce a condensed summary.",2 Background,[0],[0]
"Let the input consist of a sequence of M words x1, . . .",2 Background,[0],[0]
",xM coming from a fixed vocabulary V of size |V| = V .",2 Background,[0],[0]
"We will represent each word as an indicator vector xi ∈ {0, 1}V for i ∈ {1, . . .",2 Background,[0],[0]
",M}, sentences as a sequence of indicators, and X as the set of possible inputs.",2 Background,[0],[0]
"Furthermore define the notation x[i,j,k] to indicate the sub-sequence of elements i, j, k.
A summarizer takes x as input and outputs a shortened sentence y of length N < M .",2 Background,[0],[0]
"We will assume that the words in the summary also come from the same vocabulary V and that the output is
1In contrast to a large-scale sentence compression systems like Filippova and Altun (2013) which require monotonic aligned compressions.
",2 Background,[0],[0]
"a sequence y1, . .",2 Background,[0],[0]
.,2 Background,[0],[0]
",yN .",2 Background,[0],[0]
"Note that in contrast to related tasks, like machine translation, we will assume that the output length N is fixed, and that the system knows the length of the summary before generation.2
Next consider the problem of generating summaries.",2 Background,[0],[0]
"Define the set Y ⊂ ({0, 1}V , . . .",2 Background,[0],[0]
", {0, 1}V )",2 Background,[0],[0]
"as all possible sentences of length N , i.e. for all i and y ∈ Y , yi is an indicator.",2 Background,[0],[0]
"We say a system is abstractive if it tries to find the optimal sequence from this set Y ,
arg max y∈Y s(x,y), (1)
under a scoring function s : X ×Y 7→ R. Contrast this to a fully extractive sentence summary3 which transfers words from the input:
arg max m∈{1,...M}N s(x,x[m1,...,mN ]), (2)
or to the related problem of sentence compression that concentrates on deleting words from the input:
arg max m∈{1,...",2 Background,[0],[0]
"M}N ,mi−1<mi s(x,x[m1,...,mN ]).",2 Background,[0],[0]
"(3)
While abstractive summarization poses a more difficult generation challenge, the lack of hard constraints gives the system more freedom in generation and allows it to fit with a wider range of training data.
",2 Background,[0],[0]
"In this work we focus on factored scoring functions, s, that take into account a fixed window of previous words:
s(x,y)",2 Background,[0],[0]
"≈ N−1∑ i=0 g(yi+1,x,yc), (4)
2For",2 Background,[0],[0]
"the DUC-2004 evaluation, it is actually the number of bytes of the output that is capped.",2 Background,[0],[0]
"More detail is given in Section 7.
",2 Background,[0],[0]
3Unfortunately the literature is inconsistent on the formal definition of this distinction.,2 Background,[0],[0]
"Some systems self-described as abstractive would be extractive under our definition.
where we define yc , y[i−C+1,...,i] for a window of size C.
In particular consider the conditional logprobability of a summary given the input, s(x,y) = log p(y|x; θ).",2 Background,[0],[0]
"We can write this as:
log p(y|x; θ) ≈ N−1∑ i=0 log p(yi+1|x,yc; θ),
where we make a Markov assumption on the length of the context as size C and assume for i < 1, yi is a special start symbol 〈S〉.
",2 Background,[0],[0]
"With this scoring function in mind, our main focus will be on modelling the local conditional distribution: p(yi+1|x,yc; θ).",2 Background,[0],[0]
"The next section defines a parameterization for this distribution, in Section 4, we return to the question of generation for factored models, and in Section 5 we introduce a modified factored scoring function.",2 Background,[0],[0]
"The distribution of interest, p(yi+1|x,yc; θ), is a conditional language model based on the input sentence",3 Model,[0],[0]
"x. Past work on summarization and compression has used a noisy-channel approach to split and independently estimate a language model and a conditional summarization model (Banko et al., 2000; Knight and Marcu, 2002; Daumé III and Marcu, 2002), i.e.,
arg max y log p(y|x) = arg max y log p(y)p(x|y)
where p(y) and p(x|y) are estimated separately.",3 Model,[0],[0]
Here we instead follow work in neural machine translation and directly parameterize the original distribution as a neural network.,3 Model,[0],[0]
The network contains both a neural probabilistic language model and an encoder which acts as a conditional summarization model.,3 Model,[0],[0]
The core of our parameterization is a language model for estimating the contextual probability of the next word.,3.1 Neural Language Model,[0],[0]
"The language model is adapted from a standard feed-forward neural network language model (NNLM), particularly the class of NNLMs described by Bengio et al. (2003).",3.1 Neural Language Model,[0],[0]
"The full model is:
p(yi+1|yc,x; θ) ∝",3.1 Neural Language Model,[0],[0]
"exp(Vh + Wenc(x,yc)), ỹc =",3.1 Neural Language Model,[0],[0]
"[Eyi−C+1, . . .",3.1 Neural Language Model,[0],[0]
",Eyi], h = tanh(Uỹc).
",3.1 Neural Language Model,[0],[0]
"The parameters are θ = (E,U,V,W) where E ∈ RD×V is a word embedding matrix, U ∈ R(CD)×H , V ∈ RV×H , W ∈ RV×H are weight matrices,4 D is the size of the word embeddings, and h is a hidden layer of size H .",3.1 Neural Language Model,[0],[0]
"The black-box function enc is a contextual encoder term that returns a vector of size H representing the input and current context; we consider several possible variants, described subsequently.",3.1 Neural Language Model,[0],[0]
Figure 3a gives a schematic representation of the decoder architecture.,3.1 Neural Language Model,[0],[0]
Note that without the encoder term this represents a standard language model.,3.2 Encoders,[0],[0]
By incorporating in enc and training the two elements jointly we crucially can incorporate the input text into generation.,3.2 Encoders,[0],[0]
"We discuss next several possible instantiations of the encoder.
",3.2 Encoders,[0],[0]
"Bag-of-Words Encoder Our most basic model simply uses the bag-of-words of the input sentence embedded down to size H , while ignoring properties of the original order or relationships between neighboring words.",3.2 Encoders,[0],[0]
"We write this model as:
enc1(x,yc) = p>x̃, p =",3.2 Encoders,[0],[0]
"[1/M, . . .",3.2 Encoders,[0],[0]
", 1/M ], x̃ =",3.2 Encoders,[0],[0]
"[Fx1, . . .",3.2 Encoders,[0],[0]
",FxM ].
Where the input-side embedding matrix F ∈ RH×V is the only new parameter of the encoder and p ∈",3.2 Encoders,[0],[0]
"[0, 1]M is a uniform distribution over the input words.
",3.2 Encoders,[0],[0]
"4Each of the weight matrices U, V, W also has a corresponding bias term.",3.2 Encoders,[0],[0]
"For readability, we omit these terms throughout the paper.
",3.2 Encoders,[0],[0]
For summarization this model can capture the relative importance of words to distinguish content words from stop words or embellishments.,3.2 Encoders,[0],[0]
"Potentially the model can also learn to combine words; although it is inherently limited in representing contiguous phrases.
",3.2 Encoders,[0],[0]
Convolutional Encoder To address some of the modelling issues with bag-of-words we also consider using a deep convolutional encoder for the input sentence.,3.2 Encoders,[0],[0]
"This architecture improves on the bag-of-words model by allowing local interactions between words while also not requiring the context yc while encoding the input.
",3.2 Encoders,[0],[0]
"We utilize a standard time-delay neural network (TDNN) architecture, alternating between temporal convolution layers and max pooling layers.
∀j, enc2(x,yc)j = max i x̃Li,j , (5) ∀i, l ∈ {1, . . .",3.2 Encoders,[0],[0]
"L}, x̃lj = tanh(max{x̄l2i−1, x̄l2i}), (6) ∀i, l ∈ {1, . . .",3.2 Encoders,[0],[0]
"L}, x̄li = Qlx̃l−1[i−Q,...,i+Q], (7) x̃0 =",3.2 Encoders,[0],[0]
"[Fx1, . . .",3.2 Encoders,[0],[0]
",FxM ]. (8)
Where F is a word embedding matrix and QL×H×2Q+1 consists of a set of filters for each layer {1, . . .",3.2 Encoders,[0],[0]
L}.,3.2 Encoders,[0],[0]
"Eq. 7 is a temporal (1D) convolution layer, Eq. 6 consists of a 2-element temporal max pooling layer and a pointwise non-linearity, and final output Eq. 5 is a max over time.",3.2 Encoders,[0],[0]
At each layer x̃ is one half the size of x̄.,3.2 Encoders,[0],[0]
"For simplicity we assume that the convolution is padded at the boundaries, and that M is greater than 2L so that the dimensions are well-defined.
",3.2 Encoders,[0],[0]
"Attention-Based Encoder While the convolutional encoder has richer capacity than bag-ofwords, it still is required to produce a single representation for the entire input sentence.",3.2 Encoders,[0],[0]
A similar issue in machine translation inspired Bahdanau et al. (2014) to instead utilize an attention-based contextual encoder that constructs a representation based on the generation context.,3.2 Encoders,[0],[0]
"Here we note that if we exploit this context, we can actually use a rather simple model similar to bag-of-words:
enc3(x,yc) = p>x̄, p ∝ exp(x̃Pỹ′c), x̃ =",3.2 Encoders,[0],[0]
"[Fx1, . . .",3.2 Encoders,[0],[0]
",FxM ],
ỹ′c =",3.2 Encoders,[0],[0]
"[Gyi−C+1, . . .",3.2 Encoders,[0],[0]
",Gyi],
∀i x̄i = i+Q∑
q=i−Q x̃i/Q.
Where G ∈ RD×V is an embedding of the context, P ∈ RH×(CD) is a new weight matrix parameter mapping between the context embedding and input embedding, and Q is a smoothing window.",3.2 Encoders,[0],[0]
"The full model is shown in Figure 3b.
",3.2 Encoders,[0],[0]
"Informally we can think of this model as simply replacing the uniform distribution in bag-of-words with a learned soft alignment, P, between the input and the summary.",3.2 Encoders,[0],[0]
Figure 1 shows an example of this distribution p as a summary is generated.,3.2 Encoders,[0],[0]
The soft alignment is then used to weight the smoothed version of the input x̄ when constructing the representation.,3.2 Encoders,[0],[0]
"For instance if the current context aligns well with position i then the words xi−Q, . . .",3.2 Encoders,[0],[0]
",xi+Q are highly weighted by the encoder.",3.2 Encoders,[0],[0]
"Together with the NNLM, this model can be seen as a stripped-down version of the attention-based neural machine translation model.5",3.2 Encoders,[0],[0]
The lack of generation constraints makes it possible to train the model on arbitrary input-output pairs.,3.3 Training,[0],[0]
"Once we have defined the local conditional model, p(yi+1|x,yc; θ), we can estimate the parameters to minimize the negative loglikelihood of a set of summaries.",3.3 Training,[0],[0]
"Define this training set as consisting of J input-summary pairs (x(1),y(1)), . . .",3.3 Training,[0],[0]
", (x(J),y(J)).",3.3 Training,[0],[0]
"The negative loglikelihood conveniently factors6 into a term for each token in the summary:
NLL(θ) =",3.3 Training,[0],[0]
"− J∑
j=1
log p(y(j)|x(j); θ),
=",3.3 Training,[0],[0]
"− J∑
j=1 N−1∑ i=1",3.3 Training,[0],[0]
"log p(y (j) i+1|x(j),yc; θ).
",3.3 Training,[0],[0]
We minimize NLL by using mini-batch stochastic gradient descent.,3.3 Training,[0],[0]
"The details are described further in Section 7.
",3.3 Training,[0],[0]
"5To be explicit, compared to Bahdanau et al. (2014)",3.3 Training,[0],[0]
"our model uses an NNLM instead of a target-side LSTM, source-side windowed averaging instead of a source-side bidirectional RNN, and a weighted dot-product for alignment instead of an alignment MLP.
",3.3 Training,[0],[0]
6This is dependent on using the gold standard contexts yc.,3.3 Training,[0],[0]
An alternative is to use the predicted context within a structured or reenforcement-learning style objective.,3.3 Training,[0],[0]
We now return to the problem of generating summaries.,4 Generating Summaries,[0],[0]
"Recall from Eq. 4 that our goal is to find,
y∗ = arg max y∈Y N−1∑ i=0 g(yi+1,x,yc).
",4 Generating Summaries,[0],[0]
"Unlike phrase-based machine translation where inference is NP-hard, it actually is tractable in theory to compute y∗.",4 Generating Summaries,[0],[0]
"Since there is no explicit hard alignment constraint, Viterbi decoding can be applied and requires O(NV C) time to find an exact solution.",4 Generating Summaries,[0],[0]
"In practice though V is large enough to make this difficult.
",4 Generating Summaries,[0],[0]
An alternative approach is to approximate the arg max with a strictly greedy or deterministic decoder.,4 Generating Summaries,[0],[0]
"While decoders of this form can produce very bad approximations, they have shown to be relatively effective and fast for neural MT models (Sutskever et al., 2014).
",4 Generating Summaries,[0],[0]
A compromise between exact and greedy decoding is to use a beam-search decoder (Algorithm 1) which maintains the full vocabulary V while limiting itself to K potential hypotheses at each position of the summary.,4 Generating Summaries,[0],[0]
"The beam-search algorithm is shown here:
Algorithm 1 Beam Search Input: Parameters θ, beam size K, input x Output: Approx.",4 Generating Summaries,[0],[0]
K-best summaries π[0]← { } S = V if abstractive else {xi | ∀i} for i = 0 to N,4 Generating Summaries,[0],[0]
"− 1 do
.",4 Generating Summaries,[0],[0]
Generate Hypotheses N ←,4 Generating Summaries,[0],[0]
"{[y,yi+1]",4 Generating Summaries,[0],[0]
| y ∈,4 Generating Summaries,[0],[0]
"π[i],yi+1 ∈ S} .",4 Generating Summaries,[0],[0]
"Hypothesis Recombination
H ← { y ∈ N | s(y,x) > s(y′,x) ∀y′",4 Generating Summaries,[0],[0]
∈ N s.t.,4 Generating Summaries,[0],[0]
"yc = y′c }
.",4 Generating Summaries,[0],[0]
"Filter K-Max π[i+ 1]← K-arg max
y∈H",4 Generating Summaries,[0],[0]
"g(yi+1,yc,x) +",4 Generating Summaries,[0],[0]
"s(y,x)
end for return π[N ]
As with Viterbi this beam search algorithm is much simpler than beam search for phrase-based MT.",4 Generating Summaries,[0],[0]
Because there is no explicit constraint that each source word be used exactly once there is no need to maintain a bit set,4 Generating Summaries,[0],[0]
and we can simply move from left-to-right generating words.,4 Generating Summaries,[0],[0]
The beam search algorithm requires O(KNV ) time.,4 Generating Summaries,[0],[0]
"From a computational perspective though, each round of beam search is dominated by computing p(yi|x,yc) for each of the K hypotheses.",4 Generating Summaries,[0],[0]
"These
can be computed as a mini-batch, which in practice greatly reduces the factor of K.",4 Generating Summaries,[0],[0]
"While we will see that the attention-based model is effective at generating summaries, it does miss an important aspect seen in the human-generated references.",5 Extension: Extractive Tuning,[0],[0]
"In particular the abstractive model does not have the capacity to find extractive word matches when necessary, for example transferring unseen proper noun phrases from the input.",5 Extension: Extractive Tuning,[0],[0]
"Similar issues have also been observed in neural translation models particularly in terms of translating rare words (Luong et al., 2014).
",5 Extension: Extractive Tuning,[0],[0]
To address this issue we experiment with tuning a very small set of additional features that tradeoff the abstractive/extractive tendency of the system.,5 Extension: Extractive Tuning,[0],[0]
"We do this by modifying our scoring function to directly estimate the probability of a summary using a log-linear model, as is standard in machine translation:
p(y|x; θ, α) ∝",5 Extension: Extractive Tuning,[0],[0]
"exp(α> N−1∑ i=0 f(yi+1,x,yc)).
",5 Extension: Extractive Tuning,[0],[0]
Where α ∈ R5 is a weight vector and f is a feature function.,5 Extension: Extractive Tuning,[0],[0]
"Finding the best summary under this distribution corresponds to maximizing a factored scoring function s,
s(y,x) = N−1∑ i=0 α>f(yi+1,x,yc).
where g(yi+1,x,yc) , α>f(yi+1,x,yc) to satisfy Eq. 4.",5 Extension: Extractive Tuning,[0],[0]
"The function f is defined to combine the local conditional probability with some additional indicator featrues:
f(yi+1,x,yc) =",5 Extension: Extractive Tuning,[0],[0]
"[ log p(yi+1|x,yc; θ), 1{∃j.",5 Extension: Extractive Tuning,[0],[0]
yi+1,5 Extension: Extractive Tuning,[0],[0]
"= xj }, 1{∃j.",5 Extension: Extractive Tuning,[0],[0]
"yi+1−k = xj−k ∀k ∈ {0, 1}}, 1{∃j.",5 Extension: Extractive Tuning,[0],[0]
"yi+1−k = xj−k ∀k ∈ {0, 1, 2}}, 1{∃k > j. yi = xk,yi+1 = xj} ].
",5 Extension: Extractive Tuning,[0],[0]
"These features correspond to indicators of unigram, bigram, and trigram match with the input as well as reordering of input words.",5 Extension: Extractive Tuning,[0],[0]
"Note that setting α = 〈1, 0, . . .",5 Extension: Extractive Tuning,[0],[0]
", 0〉 gives a model identical to standard ABS.
",5 Extension: Extractive Tuning,[0],[0]
"After training the main neural model, we fix θ and tune the α parameters.",5 Extension: Extractive Tuning,[0],[0]
"We follow the statistical machine translation setup and use minimumerror rate training (MERT) to tune for the summarization metric on tuning data (Och, 2003).",5 Extension: Extractive Tuning,[0],[0]
This tuning step is also identical to the one used for the phrase-based machine translation baseline.,5 Extension: Extractive Tuning,[0],[0]
Abstractive sentence summarization has been traditionally connected to the task of headline generation.,6 Related Work,[0],[0]
Our work is similar to early work of Banko et al. (2000) who developed a statistical machine translation-inspired approach for this task using a corpus of headline-article pairs.,6 Related Work,[0],[0]
"We extend this approach by: (1) using a neural summarization model as opposed to a count-based noisy-channel model, (2) training the model on much larger scale (25K compared to 4 million articles), (3) and allowing fully abstractive decoding.
",6 Related Work,[0],[0]
"This task was standardized around the DUC2003 and DUC-2004 competitions (Over et al., 2007).",6 Related Work,[0],[0]
"The TOPIARY system (Zajic et al., 2004) performed the best in this task, and is described in detail in the next section.",6 Related Work,[0],[0]
"We point interested readers to the DUC web page (http://duc.nist. gov/) for the full list of systems entered in this shared task.
",6 Related Work,[0],[0]
"More recently, Cohn and Lapata (2008) give a compression method which allows for more arbitrary transformations.",6 Related Work,[0],[0]
"They extract tree transduction rules from aligned, parsed texts and learn weights on transfomations using a max-margin learning algorithm.",6 Related Work,[0],[0]
Woodsend et al. (2010) propose a quasi-synchronous grammar approach utilizing both context-free parses and dependency parses to produce legible summaries.,6 Related Work,[0],[0]
Both of these approaches differ from ours in that they directly use the syntax of the input/output sentences.,6 Related Work,[0],[0]
"The latter system is W&L in our results; we attempted to train the former system T3 on this dataset but could not train it at scale.
",6 Related Work,[0],[0]
In addition to Banko et al. (2000) there has been some work using statistical machine translation directly for abstractive summary.,6 Related Work,[0],[0]
"Wubben et al. (2012) utilize MOSES directly as a method for text simplification.
",6 Related Work,[0],[0]
Recently Filippova and Altun (2013) developed a strictly extractive system that is trained on a relatively large corpora (250K sentences) of articletitle pairs.,6 Related Work,[0],[0]
"Because their focus is extractive com-
pression, the sentences are transformed by a series of heuristics such that the words are in monotonic alignment.",6 Related Work,[0],[0]
"Our system does not require this alignment step but instead uses the text directly.
",6 Related Work,[0],[0]
Neural MT,6 Related Work,[0],[0]
This work is closely related to recent work on neural network language models (NNLM) and to work on neural machine translation.,6 Related Work,[0],[0]
"The core of our model is a NNLM based on that of Bengio et al. (2003).
",6 Related Work,[0],[0]
"Recently, there have been several papers about models for machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014).",6 Related Work,[0],[0]
"Of these our model is most closely related to the attention-based model of Bahdanau et al. (2014), which explicitly finds a soft alignment between the current position and the input source.",6 Related Work,[0],[0]
Most of these models utilize recurrent neural networks (RNNs) for generation as opposed to feedforward models.,6 Related Work,[0],[0]
We hope to incorporate an RNNLM in future work.,6 Related Work,[0],[0]
We experiment with our attention-based sentence summarization model on the task of headline generation.,7 Experimental Setup,[0],[0]
"In this section we describe the corpora used for this task, the baseline methods we compare with, and implementation details of our approach.",7 Experimental Setup,[0],[0]
"The standard sentence summarization evaluation set is associated with the DUC-2003 and DUC2004 shared tasks (Over et al., 2007).",7.1 Data Set,[0],[0]
"The data for this task consists of 500 news articles from the New York Times and Associated Press Wire services each paired with 4 different human-generated reference summaries (not actually headlines), capped at 75 bytes.",7.1 Data Set,[0],[0]
"This data set is evaluation-only, although the similarly sized DUC-2003 data set was made available for the task.",7.1 Data Set,[0],[0]
"The expectation is for a summary of roughly 14 words, based on the text of a complete article (although we only make use of the first sentence).",7.1 Data Set,[0],[0]
"The full data set is available by request at http://duc.nist.gov/data.html.
",7.1 Data Set,[0],[0]
"For this shared task, systems were entered and evaluated using several variants of the recalloriented ROUGE metric (Lin, 2004).",7.1 Data Set,[0],[0]
"To make recall-only evaluation unbiased to length, output of all systems is cut-off after 75-characters and no bonus is given for shorter summaries.
",7.1 Data Set,[0],[0]
"Unlike BLEU which interpolates various n-gram matches, there are several versions of ROUGE for different match lengths.",7.1 Data Set,[0],[0]
"The DUC evaluation uses ROUGE-1 (unigrams), ROUGE-2 (bigrams), and ROUGE-L (longest-common substring), all of which we report.
",7.1 Data Set,[0],[0]
"In addition to the standard DUC-2014 evaluation, we also report evaluation on single reference headline-generation using a randomly heldout subset of Gigaword.",7.1 Data Set,[0],[0]
"This evaluation is closer to the task the model is trained for, and it allows us to use a bigger evaluation set, which we will include in our code release.",7.1 Data Set,[0],[0]
"For this evaluation, we tune systems to generate output of the average title length.
",7.1 Data Set,[0],[0]
"For training data for both tasks, we utilize the annotated Gigaword data set (Graff et al., 2003; Napoles et al., 2012), which consists of standard Gigaword, preprocessed with Stanford CoreNLP tools (Manning et al., 2014).",7.1 Data Set,[0],[0]
"Our model only uses annotations for tokenization and sentence separation, although several of the baselines use parsing and tagging as well.",7.1 Data Set,[0],[0]
"Gigaword contains around 9.5 million news articles sourced from various domestic and international news services over the last two decades.
",7.1 Data Set,[0],[0]
"For our training set, we pair the headline of each article with its first sentence to create an inputsummary pair.",7.1 Data Set,[0],[0]
"While the model could in theory be trained on any pair, Gigaword contains many spurious headline-article pairs.",7.1 Data Set,[0],[0]
We therefore prune training based on the following heuristic filters: (1) Are there no non-stop-words in common?,7.1 Data Set,[0],[0]
(2) Does the title contain a byline or other extraneous editing marks?,7.1 Data Set,[0],[0]
(3) Does the title have a question mark or colon?,7.1 Data Set,[0],[0]
"After applying these filters, the training set consists of roughly J = 4 million title-article pairs.",7.1 Data Set,[0],[0]
"We apply a minimal preprocessing step using PTB tokenization, lower-casing, replacing all digit characters with #, and replacing of word types seen less than 5 times with UNK.",7.1 Data Set,[0],[0]
We also remove all articles from the time-period of the DUC evaluation.,7.1 Data Set,[0],[0]
"release.
",7.1 Data Set,[0],[0]
The complete input training vocabulary consists of 119 million word tokens and 110K unique word types with an average sentence size of 31.3 words.,7.1 Data Set,[0],[0]
The headline vocabulary consists of 31 million tokens and 69K word types with the average title of length 8.3 words (note that this is significantly shorter than the DUC summaries).,7.1 Data Set,[0],[0]
"On average there are 4.6 overlapping word types between the
headline and the input; although only 2.6 in the first 75-characters of the input.",7.1 Data Set,[0],[0]
"Due to the variety of approaches to the sentence summarization problem, we report a broad set of headline-generation baselines.
",7.2 Baselines,[0],[0]
From the DUC-2004 task we include the PREFIX baseline that simply returns the first 75- characters of the input as the headline.,7.2 Baselines,[0],[0]
"We also report the winning system on this shared task, TOPIARY (Zajic et al., 2004).",7.2 Baselines,[0],[0]
"TOPIARY merges a compression system using linguisticallymotivated transformations of the input (Dorr et al., 2003) with an unsupervised topic detection (UTD) algorithm that appends key phrases from the full article onto the compressed output.",7.2 Baselines,[0],[0]
"Woodsend et al. (2010) (described above) also report results on the DUC dataset.
",7.2 Baselines,[0],[0]
The DUC task also includes a set of manual summaries performed by 8 human summarizers each summarizing half of the test data sentences (yielding 4 references per sentence).,7.2 Baselines,[0],[0]
We report the average inter-annotater agreement score as REFERENCE.,7.2 Baselines,[0],[0]
"For reference, the best human evaluator scores 31.7 ROUGE-1.
",7.2 Baselines,[0],[0]
We also include several baselines that have access to the same training data as our system.,7.2 Baselines,[0],[0]
"The first is a sentence compression baseline COMPRESS (Clarke and Lapata, 2008).",7.2 Baselines,[0],[0]
This model uses the syntactic structure of the original sentence along with a language model trained on the headline data to produce a compressed output.,7.2 Baselines,[0],[0]
"The syntax and language model are combined with a set of linguistic constraints and decoding is performed with an ILP solver.
",7.2 Baselines,[0],[0]
"To control for memorizing titles from training, we implement an information retrieval baseline, IR.",7.2 Baselines,[0],[0]
"This baseline indexes the training set, and gives the title for the article with highest BM-25 match to the input (see Manning et al. (2008)).
",7.2 Baselines,[0],[0]
"Finally, we use a phrase-based statistical machine translation system trained on Gigaword to produce summaries, MOSES+ (Koehn et al., 2007).",7.2 Baselines,[0],[0]
"To improve the baseline for this task, we augment the phrase table with “deletion” rules mapping each article word to , include an additional deletion feature for these rules, and allow for an infinite distortion limit.",7.2 Baselines,[0],[0]
"We also explicitly tune the model using MERT to target the 75- byte capped ROUGE score as opposed to standard
BLEU-based tuning.",7.2 Baselines,[0],[0]
"Unfortunately, one remaining issue is that it is non-trivial to modify the translation decoder to produce fixed-length outputs, so we tune the system to produce roughly the expected length.",7.2 Baselines,[0],[0]
"For training, we use mini-batch stochastic gradient descent to minimize negative log-likelihood.",7.3 Implementation,[0],[0]
"We use a learning rate of 0.05, and split the learning rate by half if validation log-likelihood does not improve for an epoch.",7.3 Implementation,[0],[0]
Training is performed with shuffled mini-batches of size 64.,7.3 Implementation,[0],[0]
The minibatches are grouped by input length.,7.3 Implementation,[0],[0]
"After each epoch, we renormalize the embedding tables (Hinton et al., 2012).",7.3 Implementation,[0],[0]
"Based on the validation set, we set hyperparameters as D = 200, H = 400, C = 5, L = 3, and Q = 2.
",7.3 Implementation,[0],[0]
Our implementation uses the Torch numerical framework (http://torch.ch/) and will be openly available along with the data pipeline.,7.3 Implementation,[0],[0]
"Crucially, training is performed on GPUs and would be intractable or require approximations otherwise.",7.3 Implementation,[0],[0]
"Processing 1000 mini-batches with D = 200, H = 400 requires 160 seconds.",7.3 Implementation,[0],[0]
"Best validation accuracy is reached after 15 epochs through the data, which requires around 4 days of training.
",7.3 Implementation,[0],[0]
"Additionally, as described in Section 5 we apply a MERT tuning step after training using the DUC2003 data.",7.3 Implementation,[0],[0]
"For this step we use Z-MERT (Zaidan, 2009).",7.3 Implementation,[0],[0]
We refer to the main model as ABS and the tuned model as ABS+.,7.3 Implementation,[0],[0]
Our main results are presented in Table 1.,8 Results,[0],[0]
"We run experiments both using the DUC-2004 evaluation data set (500 sentences, 4 references, 75 bytes) with all systems and a randomly held-out
Gigaword test set (2000 sentences, 1 reference).",8 Results,[0],[0]
"We first note that the baselines COMPRESS and IR do relatively poorly on both datasets, indicating that neither just having article information or language model information alone is sufficient for the task.",8 Results,[0],[0]
"The PREFIX baseline actually performs surprisingly well on ROUGE-1 which makes sense given the earlier observed overlap between article and summary.
",8 Results,[0],[0]
"Both ABS and MOSES+ perform better than TOPIARY, particularly on ROUGE-2 and ROUGE-L in DUC.",8 Results,[0],[0]
"The full model ABS+ scores the best on these tasks, and is significantly better based on the default ROUGE confidence level than TOPIARY on all metrics, and MOSES+ on ROUGE-1 for DUC as well as ROUGE-1 and ROUGE-L for Gigaword.",8 Results,[0],[0]
"Note that the additional extractive features bias the system towards retaining more input words, which is useful for the underlying metric.
",8 Results,[0],[0]
Next we consider ablations to the model and algorithm structure.,8 Results,[0],[0]
Table 2 shows experiments for the model with various encoders.,8 Results,[0],[0]
"For these experiments we look at the perplexity of the system as a language model on validation data, which controls for the variable of inference and tuning.",8 Results,[0],[0]
The NNLM language model with no encoder gives a gain over the standard n-gram language model.,8 Results,[0],[0]
Including even the bag-of-words encoder reduces perplexity number to below 50.,8 Results,[0],[0]
"Both the convolutional encoder and the attention-based encoder further reduce the perplexity, with attention giving a value below 30.
",8 Results,[0],[0]
"We also consider model and decoding ablations on the main summary model, shown in Table 3.",8 Results,[0],[0]
"These experiments compare to the BoW encoding models, compare beam search and greedy decoding, as well as restricting the system to be com-
plete extractive.",8 Results,[0],[0]
"Of these features, the biggest impact is from using a more powerful encoder (attention versus BoW), as well as using beam search to generate summaries.",8 Results,[0],[0]
"The abstractive nature of the system helps, but for ROUGE even using pure extractive generation is effective.
",8 Results,[0],[0]
Finally we consider example summaries shown in Figure 4.,8 Results,[0],[0]
"Despite improving on the baseline scores, this model is far from human performance on this task.",8 Results,[0],[0]
"Generally the models are good at picking out key words from the input, such as names and places.",8 Results,[0],[0]
"However, both models will reorder words in syntactically incorrect ways, for instance in Sentence 7 both models have the wrong subject.",8 Results,[0],[0]
"ABS often uses more interesting re-wording, for instance new nz pm after election in Sentence 4, but this can also lead to attachment mistakes such a russian oil giant chevron in Sentence 11.",8 Results,[0],[0]
"We have presented a neural attention-based model for abstractive summarization, based on recent developments in neural machine translation.",9 Conclusion,[0],[0]
We combine this probabilistic model with a generation algorithm which produces accurate abstractive summaries.,9 Conclusion,[0],[0]
"As a next step we would like to further improve the grammaticality of the summaries in a data-driven way, as well as scale this system to generate paragraph-level summaries.",9 Conclusion,[0],[0]
Both pose additional challenges in terms of efficient alignment and consistency in generation.,9 Conclusion,[0],[0]
"Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build.",abstractText,[0],[0]
"In this work, we propose a fully data-driven approach to abstractive sentence summarization.",abstractText,[0],[0]
Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence.,abstractText,[0],[0]
"While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data.",abstractText,[0],[0]
The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines.,abstractText,[0],[0]
A Neural Attention Model for Sentence Summarization,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 1446–1459 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,[0],[0]
"The task of named entity recognition (NER) involves the extraction from text of names of entities pertaining to semantic types such as person (PER), location (LOC) and geo-political entity (GPE).",1 Introduction,[0],[0]
"NER has drawn the attention of many researchers as the first step towards NLP applications such as entity linking (Gupta et al., 2017), relation extraction (Miwa and Bansal, 2016), event
1Code is available at https://github.com/ meizhiju/layered-bilstm-crf
extraction (Feng et al., 2016) and co-reference resolution (Fragkou, 2017; Stone and Arora, 2017).
",1 Introduction,[0],[0]
"Due to the properties of natural language, many named entities contain nested entities: embedded names which are included in other entities, illustrated in Figure 1.",1 Introduction,[0],[0]
"This phenomenon is quite common in many domains (Alex et al., 2007; Byrne, 2007; Wang, 2009; Màrquez et al., 2007).",1 Introduction,[0],[0]
"However, much of the work on NER copes only with non-nested entities which are also called flat entities and neglects nested entities.",1 Introduction,[0],[0]
"This leads to loss of potentially important information, with negative impacts on subsequent tasks.
",1 Introduction,[0],[0]
"Traditional approaches to NER mainly involve two types of approaches: supervised learning (Ling and Weld, 2012; Marcińczuk, 2015; Leaman and Lu, 2016) and hybrid approaches (Bhasuran et al., 2016; Rocktäschel et al., 2012; Leaman et al., 2015) that combine supervised learning with rules.",1 Introduction,[0],[0]
Such approaches require either domain knowledge or heavy featureengineering.,1 Introduction,[0],[0]
"Recent advances in neural networks enable NER without depending on external knowledge resources through automated learning highlevel and abstract features from text (Lample et al., 2016; Ma and Hovy, 2016; Pahuja et al., 2017; Strubell et al., 2017).
",1 Introduction,[0],[0]
"In this paper, we propose a novel dynamic neural model for nested entity recognition, without relying on any external knowledge resources or linguistics features.",1 Introduction,[0],[0]
"Our model enables sequentially
1446
O B-DNA I-DNA I-DNA I-DNA O
Gold labels
Outer
stacking flat NER layers from bottom to up and identifying entities in an end-to-end manner.",1 Introduction,[0],[0]
"The number of stacked layers depends on the level of entity nesting and dynamically adjusts to the input sequences as the nested level varies from different sequences.
",1 Introduction,[0],[0]
"Given a sequence of words, our model first represents each word using a low-dimensional vector concatenated from its corresponding word and character sequence embeddings.",1 Introduction,[0],[0]
"Taking the sequence of the word representation as input, our flat NER layer enables capturing context representation by a long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) layer.",1 Introduction,[0],[0]
The context representation is then fed to a CRF layer for label prediction.,1 Introduction,[0],[0]
"Subsequently, the context representation from the LSTM layer is merged to build representation for each detected entity, which is used as the input for the next flat NER layer.",1 Introduction,[0],[0]
Our model stops detecting entities if no entities are predicted by the current flat NER layer.,1 Introduction,[0],[0]
"Through stacking flat NER layers in order, we are able to extract entities from inside to outside with sharing parameters among the different LSTM layers and CRF layers.
",1 Introduction,[0],[0]
We gain 3.9 and 9.1 percentage point improvements regarding F-score over the state-of-the-art feature-based model on two nested entity corpora:,1 Introduction,[0],[0]
"GENIA (Kim et al., 2003) and ACE2005 (Walker et al., 2006), and analyze contributions of inner entities to outer entity detection, drawing several key conclusions.
",1 Introduction,[0],[0]
"In addition, experiments are conducted on a flatly annotated corpora JNLPBA (Kim et al.,
2004).",1 Introduction,[0],[0]
"Our model can be a complete NER model as well for flat entities, on the condition that it is trained on annotations that do not account for nested entities.",1 Introduction,[0],[0]
We obtain 75.55% in terms of Fscore that is comparable to the state-of-the-art performance.,1 Introduction,[0],[0]
Our nested NER model is designed based on a sequential stack of flat NER layers that detects nested entities in an end-to-end manner.,2 Neural Layered Model,[0],[0]
Figure 2 provides the overview of our model.,2 Neural Layered Model,[0],[0]
Our flat NER layers are inspired by the state-of-theart model proposed in Lample et al. (2016).,2 Neural Layered Model,[0],[0]
The layer utilizes one single bidirectional LSTM layer to represent word sequences and predict flat entities by putting one single CRF layer on top of the LSTM layer.,2 Neural Layered Model,[0],[0]
"Therefore, we refer to our model as Layered-BiLSTM-CRF model.",2 Neural Layered Model,[0],[0]
"If any entities are predicted, a new flat NER layer is introduced and the word sequence representation of each detected entity by the current flat NER layer is merged to compose a representation for the entity, which is then passed on to the new flat NER layer as its input.",2 Neural Layered Model,[0],[0]
"Otherwise, the model terminates stacking and hence finishes entity detection.
",2 Neural Layered Model,[0],[0]
"In this section, we provide a brief description of the model architecture: the flat NER layers and their stacking, the embedding layer and their training.",2 Neural Layered Model,[0],[0]
A flat NER layer consists of an LSTM layer and a CRF layer.,2.1 Flat NER layer,[0],[0]
"The LSTM layer captures the bidi-
rectional context representation of sequences and subsequently feeds it to the CRF layer to globally decode label sequences.
",2.1 Flat NER layer,[0],[0]
"LSTM is a variant of recurrent neural networks (RNNs) (Goller and Kuchler, 1996) that incorporates a memory cell to remember the past information for a long period of time.",2.1 Flat NER layer,[0],[0]
"This enables capturing long dependencies, thus reducing the gradient vanishing/explosion problem existing in RNNs.",2.1 Flat NER layer,[0],[0]
We employ bidirectional LSTM with no peephole connection.,2.1 Flat NER layer,[0],[0]
"We refer the readers to Hochreiter and Schmidhuber (1997) for more details of LSTM used in our work.
",2.1 Flat NER layer,[0],[0]
CRFs are used to globally predict label sequences for any given sequences.,2.1 Flat NER layer,[0],[0]
"Given an input sequence X = (x1, x2, . . .",2.1 Flat NER layer,[0],[0]
", xn) which is the output from the LSTM layer, we maximize the logprobability during training.",2.1 Flat NER layer,[0],[0]
"In decoding, we set transition costs between illegal transitions, e.g., transition from O to I-PER, as infinite to restrict illegal labels.",2.1 Flat NER layer,[0],[0]
"The expected label sequence y = (y1, y2, . . .",2.1 Flat NER layer,[0],[0]
", yn) is predicted based on maximum scores in decoding.",2.1 Flat NER layer,[0],[0]
"We stack a flat NER layer on the top of the current flat NER layer, aiming to extract outer entities.",2.2 Stacking flat NER layers,[0],[0]
"Concretely, we merge and average current context representation of the regions composed in the detected entities, as described in the following equation:
mi = 1 end− start+",2.2 Stacking flat NER layers,[0],[0]
"1 end∑
i=start
zi, (1)
where zi denotes the representation of the i-th word from the flat NER layer, and mi is the merged representation for an entity.",2.2 Stacking flat NER layers,[0],[0]
The region starts from a position start and ends at a position end of the sequence.,2.2 Stacking flat NER layers,[0],[0]
"This merged representation of detected entities allows us to treat each detected entity as a single token, and hence we are able to make the most of inner entity information to encourage outer entity recognition.",2.2 Stacking flat NER layers,[0],[0]
"If the region is detected as a non-entity, we keep the representation without any processing.",2.2 Stacking flat NER layers,[0],[0]
The processed context representation of the flat NER layer is used as the input for the next flat NER layer.,2.2 Stacking flat NER layers,[0],[0]
"The input for the first NER layer is different from the remaining flat NER layers since the first layer
has no previous layers.",2.3 Embedding layer,[0],[0]
We thus represent each word by concatenating character sequence embeddings and word embeddings for the first flat NER layer.,2.3 Embedding layer,[0],[0]
"Figure 3 describes the architecture of the embedding layer to produce word representation.
",2.3 Embedding layer,[0],[0]
"Following the successes of Ma and Hovy (2016) and Lample et al. (2016) in utilizing character embeddings on the flat NER task, we also represent each word with its character sequence to capture the orthographic and morphological features of the word.",2.3 Embedding layer,[0],[0]
Each character is mapped to a randomly initialized vector through a character lookup table.,2.3 Embedding layer,[0],[0]
"We feed the character vectors comprising a word to a bidirectional LSTM layer and concatenate the forward and backward representation to obtain the word-level embedding.
Differently from the character sequence embeddings, the pretrained word embeddings are used to initialize word embeddings.",2.3 Embedding layer,[0],[0]
"When evaluating or applying the model, words that are outside of the pretrained embeddings and training dataset are mapped to an unknown (UNK) embedding, which is randomly initialized during training.",2.3 Embedding layer,[0],[0]
"To train the UNK embedding, we replace words whose frequency is 1 in the training dataset with the UNK embedding with a probability 0.5.",2.3 Embedding layer,[0],[0]
"We prepare the gold labels based on the conventional BIO (Beginning, Inside, Out of entities) tagging scheme to represent a label attached to each word.
",2.4 Training,[0],[0]
"As our model detects entities from inside to outside, we keep the same order in preparing the gold labels for each word sequence.",2.4 Training,[0],[0]
We call it the detection order rule.,2.4 Training,[0],[0]
"Meantime, we define that each entity region in the sequence can only be tagged once with the same entity type, referred to as the non-duplicate rule.",2.4 Training,[0],[0]
"For instance, in Figure 2, “interleukin-2” is tagged first while “interleukin2 receptor alpha gene” is subsequently tagged following the above two rules.",2.4 Training,[0],[0]
"When assigning the label O to non-entity regions, we only follow the detection order rule.",2.4 Training,[0],[0]
"As a result, two gold label sequences {O, B-Protein, O, O, O, O} and {O, B-DNA, I-DNA, I-DNA, I-DNA, O} are assigned to the given word sequence “Mouse interleukin-2 receptor alpha gene expression” as shown in Figure 2.",2.4 Training,[0],[0]
"With these rules, the number of labels for each word equals the nested level of entities in the given word sequence.
",2.4 Training,[0],[0]
"We employ mini-batch training and update the model parameters using back-propagation through time (BPTT) (Werbos, 1990) with Adam (Kingma and Ba, 2014).",2.4 Training,[0],[0]
"The model parameters include weights, bias, transition costs, and embeddings of characters.",2.4 Training,[0],[0]
"We disable updating the word embeddings.2 During training, early stopping, L2regularization and dropout (Hinton et al., 2012) are used to prevent overfitting.",2.4 Training,[0],[0]
Dropout is employed to the input of each flat NER layer.,2.4 Training,[0],[0]
"Hyperparameters including batch size, number of hidden units in LSTM, character dimensions, dropout rate, Adam learning rate, gradient clipping and weight decay (L2) are all tuned with Bayesian optimization (Snoek et al., 2012).",2.4 Training,[0],[0]
"We employed three datasets for evaluation: GENIA3 (Kim et al., 2003), ACE20054",3 Evaluation Settings,[0],[0]
"(Walker et al., 2006) and JNLPBA5 (Kim et al., 2004).",3 Evaluation Settings,[0],[0]
We briefly explain the data and task settings and then introduce model and experimental settings.,3 Evaluation Settings,[0],[0]
"We performed nested entity extraction experiments on GENIA and ACE2005 while we con-
2We tried updating and disabling updating word embeddings.",3.1 Data and Task Settings,[0],[0]
"The former trial did not work.
",3.1 Data and Task Settings,[0],[0]
"3http://www.geniaproject.org/ genia-corpus/term-corpus
4https://catalog.ldc.upenn.edu/ ldc2006t06
5http://www.nactem.ac.uk/tsujii/GENIA/ ERtask/report.html
ducted flat entity extraction on the JNLPBA dataset.",3.1 Data and Task Settings,[0],[0]
"For the details of data statistics and preprocessing, please refer to the supplementary materials.
",3.1 Data and Task Settings,[0],[0]
"GENIA involves 36 fine-grained entity categories among total 2,000 MEDLINE abstracts.",3.1 Data and Task Settings,[0],[0]
"Following the same task settings as in Finkel and Manning (2009) and Lu and Roth (2015), we collapsed all DNA subcategories as DNA.",3.1 Data and Task Settings,[0],[0]
"The same setting was applied to RNA, protein, cell line and cell type categories.",3.1 Data and Task Settings,[0],[0]
"We used same test portion as Finkel and Manning (2009), Lu and Roth (2015) and Muis and Lu (2017) for the direct comparison.
ACE2005 contains 7 fine-grained entity categories.",3.1 Data and Task Settings,[0],[0]
"We made same modifications described in Lu and Roth (2015) and Muis and Lu (2017) by keeping files from bn, bw, nw and wl and spitting them into training, development and testing datasets at random following same ratio 8:1:1, respectively.
",3.1 Data and Task Settings,[0],[0]
JNLPBA defines both training and testing datasets.,3.1 Data and Task Settings,[0],[0]
"These two datasets are composed of 2,000 and 404 MEDLINE abstracts, respectively.",3.1 Data and Task Settings,[0],[0]
JNLPBA is originally from the GENIA corpus.,3.1 Data and Task Settings,[0],[0]
"However, only flat and topmost entities in JNLPBA are kept while nested and discontinuous entities are removed.",3.1 Data and Task Settings,[0],[0]
"Like our preprocessing on the GENIA corpus, subcategories are collapsed and only 5 entity types are finally reserved.",3.1 Data and Task Settings,[0],[0]
"We randomly chose the 90% sentences of the original training dataset as our training dataset and the remaining as our development dataset.
",3.1 Data and Task Settings,[0],[0]
"Precision (P), recall (R) and F-score (F) were used for the evaluation metrics in our tasks.",3.1 Data and Task Settings,[0],[0]
"We define that if the numbers of gold entities and predictions are all zeros, the evaluation metrics all equal one hundred percent.",3.1 Data and Task Settings,[0],[0]
"Our model was implemented with Chainer6 (Tokui et al., 2015).",3.2 Model and Experimental Settings,[0],[0]
"We initialized word embeddings in GENIA and JNLPBA with the pretrained embeddings trained on MEDLINE abstracts (Chiu et al., 2016).",3.2 Model and Experimental Settings,[0],[0]
"For ACE2005, we initialized each word with the pretrained embeddings which are trained by Miwa and Bansal (2016).",3.2 Model and Experimental Settings,[0],[0]
"Except for the word embeddings, parameters of word embeddings were initialized with a normal distribution.",3.2 Model and Experimental Settings,[0],[0]
"For LSTM, we initialized hidden states, cell state and all the bias terms as 0 except for the forget gate
6https://chainer.org/
bias that was set as 1.",3.2 Model and Experimental Settings,[0],[0]
"For other hyper-parameters, we chose the best hyper-parameters via Bayesian optimization.",3.2 Model and Experimental Settings,[0],[0]
"We refer the readers to the supplemental material for the settings of the hyperparameters of the models and Bayesian optimization.
",3.2 Model and Experimental Settings,[0],[0]
"For ablation tests, we compared with our layered-BiLSTM-CRF model with two models that produce the input for next flat NER layer in different ways.",3.2 Model and Experimental Settings,[0],[0]
The first model is called layeredBiLSTM-CRF w/o layered out-of-entities which uses the input of the current flat NER layer for out-of-entity words.,3.2 Model and Experimental Settings,[0],[0]
We name the second model as layered-BiLSTM-CRF w/o layered LSTM as it skips all intermediate LSTM layers and only uses output of embedding layer to build the input for the next flat NER layer.,3.2 Model and Experimental Settings,[0],[0]
"Please refer to supplemental material for the introduced two models.7
To investigate the effectiveness of our model on different nested levels of entities, we evaluated the model performance on each flat NER layer on GENIA and ACE2005 test datasets.8",3.2 Model and Experimental Settings,[0],[0]
"When calculating precision and recall measurements, we collected the predictions and gold entities from the corresponding flat NER layer.",3.2 Model and Experimental Settings,[0],[0]
"Since predicted entities on a specific flat NER layer might be from other flat NER layers, we defined extended precision (EP), extended recall (ER) and extended Fscore (EF) to measure the performance.",3.2 Model and Experimental Settings,[0],[0]
"We calculated EP by comparing the predicted entities in a specific flat NER layer with all the gold entities, and ER by comparing the gold entities in a specific flat NER layer with all the predicted entities.",3.2 Model and Experimental Settings,[0],[0]
"EF was calculated in the same way with F.
In addition to experiments on nested GENIA and ACE2005 datasets, flat entity recognition was conducted on the JNLPBA dataset.",3.2 Model and Experimental Settings,[0],[0]
We trained our flat model that only kept the first flat NER layer and removed the following stacking layers.,3.2 Model and Experimental Settings,[0],[0]
"We follow the hyper-parameters settings by Lample et al. (2016) for this evaluation.
",3.2 Model and Experimental Settings,[0],[0]
7We examined the contributions of predicted labels of the current flat NER layer to the next flat NER layer.,3.2 Model and Experimental Settings,[0],[0]
"For this, we introduced label embeddings into each test by combining the embedding with context representation.",3.2 Model and Experimental Settings,[0],[0]
"Experiments show that appending label embedding hurts the performance of our model while gain slight improvements in the rest 2 models on development datasets.
8We removed entities which were predicted in previous flat NER layers during evaluation.",3.2 Model and Experimental Settings,[0],[0]
Table 1 presents the comparisons of our model with related work including the state-of-the-art feature-based model by Muis and Lu (2017).,4.1 Nested NER,[0],[0]
"Our model outperforms the state-of-the-art models with 74.7% and 72.2% in terms of F-score, achieving the new state-of-the-art in the nested NER tasks.",4.1 Nested NER,[0],[0]
"For GENIA, our model gained more improvement in terms of recall with enabling extract more nested entities without reducing precision.",4.1 Nested NER,[0],[0]
"On ACE2005, we improved recall with 12.2 percentage points and obtained 5.1% relative error reductions.",4.1 Nested NER,[0],[0]
"Compared with GENIA, our model gained more improvements in ACE2005 in terms of F-score.",4.1 Nested NER,[0],[0]
Two possible reasons account for it.,4.1 Nested NER,[0],[0]
One reason is that ACE2005 contains more deeper nested entities (maximum nested level is 5) than GENIA (maximum nested level is 3) on the test dataset.,4.1 Nested NER,[0],[0]
This allows our model to capture the potentially ‘nested’ relations among nested entities.,4.1 Nested NER,[0],[0]
"The other reason is that ACE2005 has more nested entities (37.45%) compared with GENIA (21.56%).
",4.1 Nested NER,[0],[0]
"Table 2 shows the results of models on the development datasets of GENIA and ACE2005, respectively.",4.1 Nested NER,[0],[0]
"From this table, we can see that our model, which only utilizes context representation for preparation of input for the next flat NER layer, performs better than the rest two models.",4.1 Nested NER,[0],[0]
This demonstrates that introducing input of the current flat NER layer such as skipping either representation for any non-entity or words or all intermediate LSTM layers hurts performance.,4.1 Nested NER,[0],[0]
"Compared with the layered-BiLSTM-CRF model, the drop of the performance in the layered-BiLSTM-CRF w/o layered out-of-entities model reflects the skip of representation for out-of-entity words leads to the decline in performance.",4.1 Nested NER,[0],[0]
This is because the representation of non-entity words didn’t incorporate the current context representation as we used the input rather than the output to represent them.,4.1 Nested NER,[0],[0]
"By analogy, the layered BiLSTM-CRF w/o layer LSTM model skips representation for both entities and non-entity words, resulting in worse performance.",4.1 Nested NER,[0],[0]
"This is because, when skipping all intermediate LSTM layers, input of the first flat NER layer, i.e., word embeddings, is passed to the remaining flat NER layers.",4.1 Nested NER,[0],[0]
"Since word embeddings do not contain context representation, we fail to incorporate the context representation when we use
the word embeddings as the input for the flat NER layers.",4.1 Nested NER,[0],[0]
"Therefore, we have no chance to take advantage of the context representation and instead we only manage to use the word embeddings as the input for flat NER layers in this case.
",4.1 Nested NER,[0],[0]
"Table 3 and Table 4 describe the performance for each entity type in GENIA and ACE2005 test datasets, respectively.",4.1 Nested NER,[0],[0]
"In GENIA, our model performed best in recognizing entities with type RNA.
",4.1 Nested NER,[0],[0]
This is because most of the entities pertaining to RNA mainly end up either with “mRNA” or RNA.,4.1 Nested NER,[0],[0]
These two words are informative indicators of RNA entities.,4.1 Nested NER,[0],[0]
"For entities in rest entity types, their performances are close to the overall performance.",4.1 Nested NER,[0],[0]
One possible reason is that there are many instances to model them.,4.1 Nested NER,[0],[0]
"This also accounts for the high performances of entity types such as PER, GPE in ACE2005.",4.1 Nested NER,[0],[0]
The small amounts of instances of entity types like FAC in ACE2005 is one reason for their under overall performances.,4.1 Nested NER,[0],[0]
"We refer readers to supplemental material for statistics details.
",4.1 Nested NER,[0],[0]
"When evaluating our model on top level which contains only outermost entities, the precision, recall and F-score were 78.19%, 75.17% and 76.65% on GENIA test dataset.",4.1 Nested NER,[0],[0]
"For ACE2005, the corresponding precision, recall and F-score were 68.37%, 68.57% and 68.47%.",4.1 Nested NER,[0],[0]
"Compared with the overall performance listed in Table 1, we obtained higher top level performance on GENIA but lower performance in ACE2005.",4.1 Nested NER,[0],[0]
"We discuss details of this phenomena in the following tables.
",4.1 Nested NER,[0],[0]
Table 5 shows the performances of each flat NER layer in GENIA test dataset.,4.1 Nested NER,[0],[0]
"Among all the stacking flat NER layers, our model resulted in the best performance regarding standard evaluation metrics on the first flat NER layer which contains the predictions for the gold innermost entities.",4.1 Nested NER,[0],[0]
"When the model went to deeper flat NER layers, the performance dropped gradually as the number of gold entities decreased.",4.1 Nested NER,[0],[0]
"However, the performance for predictions on each flat
NER layer was different in terms of extended evaluation metrics.",4.1 Nested NER,[0],[0]
"For the first two flat NER layers, performance of extended evaluation is better than the performance of standard evaluation.",4.1 Nested NER,[0],[0]
It indicates that gold entities correspond to some of the predictions on the specific flat NER layer are from other flat NER layers.,4.1 Nested NER,[0],[0]
This may lead to the zero performances for the last flat NER layer.,4.1 Nested NER,[0],[0]
"In addition, performance on the second flat NER layer was higher than it was on the first flat NER layer in terms of extended F-score.",4.1 Nested NER,[0],[0]
"This demonstrates that our model is able to obtain higher performance on top level of entities than innermost entities.
",4.1 Nested NER,[0],[0]
Table 6 lists the results of each flat NER layer on ACE2005 test dataset.,4.1 Nested NER,[0],[0]
"Similar to GENIA, the first flat NER layer achieved better performance than the rest flat NER layers.",4.1 Nested NER,[0],[0]
Performances decreased in a bottom-to-up manner regarding model architecture.,4.1 Nested NER,[0],[0]
"This phenomena was the same with the extended evaluation performances, which reflects that some of the predictions in a specific flat NER layer were detected in other flat NER layers.",4.1 Nested NER,[0],[0]
"Unlike rising tendency (except last flat NER layer) regarding extend F-score in GENIA, performance in ACE2005 was in downtrend.",4.1 Nested NER,[0],[0]
This accounts for the fact that F-score on top level was lower than it on the fist flat NER layer.,4.1 Nested NER,[0],[0]
"Even though the decline trend in extended F-score, the first flat NER layer contained the largest proportion of predictions for the gold entities, the overall performance on all nested entities showed in Table 1 was still high.",4.1 Nested NER,[0],[0]
"Unlike GENIA, our model in ACE2005 stopped before reaching the maximum nested level of entities.",4.1 Nested NER,[0],[0]
It indicates our model failed to model the appropriate nested levels.,4.1 Nested NER,[0],[0]
"This is one of the reasons that account for the zero predictions on the last
flat NER layer.",4.1 Nested NER,[0],[0]
One reason is that our model The sparse instances on the high nested levels could be another reason that resulted in the zero performances on the last flat NER layer.,4.1 Nested NER,[0],[0]
"Compared with the state-of-the-art work on JNLPBA (Gridach, 2017) which achieved 75.87% in terms of F-score, our model obtained 75.55% in F-score.",4.2 Flat NER,[0],[0]
"Since both the model by Gridach (2017) and our flat model are based on Lample et al. (2016), so it is reasonable that both models were able to get comparable performance.",4.2 Flat NER,[0],[0]
We showed the error types and their statistics both for all nested entities and each flat NER layer on GENIA and ACE2005 test datasets.,4.3 Error analysis,[0],[0]
"From ACE2005 test dataset, 28% of predictions were incorrect in 200 sentences which were selected at random.",4.3 Error analysis,[0],[0]
"Among these errors, 39% of them were because their text spans were assigned with other entity types.",4.3 Error analysis,[0],[0]
We call this type of errors type error.,4.3 Error analysis,[0],[0]
The main reason is that most of them are pronouns and co-refer to other entities which are absent in the sentence.,4.3 Error analysis,[0],[0]
"Taking this sentence “whether that is true now, we can not say” as an example, “we” is annotated as ORG while our model labeled it as PER.",4.3 Error analysis,[0],[0]
Lack of context information such as the absence of co-referent entities leads our model to make the wrong decisions.,4.3 Error analysis,[0],[0]
"In addition, 30% of the errors were caused by that incorrect predictions were predicted as only parts of gold entities with correct entity types.",4.3 Error analysis,[0],[0]
This error type is referred to as partial prediction error.,4.3 Error analysis,[0],[0]
"This might be due to these gold entities tend to clauses or inde-
pendent sentences, thus possibly containing many modifiers.",4.3 Error analysis,[0],[0]
"For example, in this sentence “A man who has been to Baghdad many times and can tell us with great knowledge exactly what it’s going to be like to fight on those avenues in that sprawling city of Baghdad - Judy .”, “A man who has been to Baghdad many times and can tell us with great knowledge exactly what it’s going to be like to fight on those avenues in that sprawling city of Baghdad” is annotated as PER while our model could only extract “A man who has been to Baghdad many times” and predicted it as PER.
",4.3 Error analysis,[0],[0]
"Errors on the first flat NER layer, we got 41% in type error and 11% of partial prediction error, respectively.",4.3 Error analysis,[0],[0]
"Apart from this, our model recognized predictions from other flat NER layers, leading to 5% errors.",4.3 Error analysis,[0],[0]
We define this error type as layer error.,4.3 Error analysis,[0],[0]
"Unlike the first flat NER layer, 26% of errors were caused by layer error.",4.3 Error analysis,[0],[0]
"Additionally, 17% of the errors belong to type error.",4.3 Error analysis,[0],[0]
"In particular, 22% errors were due to the type error.",4.3 Error analysis,[0],[0]
"As for the last flat NER layer, 40% errors were caused by partial prediction error.",4.3 Error analysis,[0],[0]
The rest errors were different from the mentioned error types.,4.3 Error analysis,[0],[0]
One possible reason is that we have less gold entities to train this flat NER layer compared with previous flat NER layers.,4.3 Error analysis,[0],[0]
"Another reason might be the error propagation.
",4.3 Error analysis,[0],[0]
"Similarly, 200 sentences were randomly selected from GENIA test dataset.",4.3 Error analysis,[0],[0]
We got 20% errors of predictions in the subset.,4.3 Error analysis,[0],[0]
"Among these errors, 17% and 24% of errors were separately due to type error and partial prediction error.",4.3 Error analysis,[0],[0]
"In addition, 24% of the predictions on the first flat NER layer were incorrect.",4.3 Error analysis,[0],[0]
"Among them, the top error types were layer error, partial prediction error and type error, accounting for 21%, 18% and 13%, respectively.",4.3 Error analysis,[0],[0]
Errors on the second flat NER layer were mainly caused by type error and the and partial prediction error.,4.3 Error analysis,[0],[0]
"The success of neural networks has boosted the performance of flat named NER in different domains (Lample et al., 2016; Ma and Hovy, 2016; Gridach, 2017; Strubell et al., 2017).",5 Related Work,[0],[0]
"Such models achieved the state of the art without any handcrafted features and external knowledge resources.
",5 Related Work,[0],[0]
"Contrary to flat NER, much fewer attempts have emphasized the nested entity recognition.",5 Related Work,[0],[0]
"Existing approaches to nested NER (Shen et al., 2003; Alex et al., 2007; Finkel and Manning, 2009; Lu
and Roth, 2015; Xu and Jiang, 2016; Muis and Lu, 2017) mainly rely on hand-crafted features.",5 Related Work,[0],[0]
They also failed to take advantage of the dependencies among nested entities.,5 Related Work,[0],[0]
"Our model enables capturing dependencies and automatic learning highlevel abstract features from texts.
",5 Related Work,[0],[0]
Early work regarding nested NER involve mainly hybrid systems that combined rules with supervised learning algorithms.,5 Related Work,[0],[0]
"For example, Shen et al. (2003), Zhou et al. (2004) and Zhang et al. (2004) employed a Hidden Markov Model to GENIA to extract inner entities and then used rule-based methods to obtain the outer entities.",5 Related Work,[0],[0]
"Furthermore, Gu (2006) extracted nested entities based on SVM which were trained separately on both inner entities and outermost entities without putting the hidden relations between nested entities into consideration.",5 Related Work,[0],[0]
All these methods failed to capture the dependencies between nested entities.,5 Related Work,[0],[0]
One trial work is that Alex et al. (2007) separately built a inside-out and outside-in layered CRFs which were able to use the current guesses as the input for next layer.,5 Related Work,[0],[0]
"They also cascaded separate CRFs of each entity type by using output from previous CRFs as features of current CRFs, yielding best performance in their work.",5 Related Work,[0],[0]
"One of the main drawbacks in the cascading approach was that it failed to handle nested entities sharing the same entity type, which were quite common in natural languages.
",5 Related Work,[0],[0]
Finkel and Manning (2009) proposed a discriminative constituency tree to represent each sentence where the root node was used for connection.,5 Related Work,[0],[0]
All entities were treated as phrases and represented as subtrees following the whole tree structure.,5 Related Work,[0],[0]
"Unlike our linguistic features independent model, Finkel and Manning (2009) used a CRFbased approach driven by entity-level features to detect nested entities
Later on, Lu and Roth (2015) built hyper-graphs that allow edges to connect multiple nodes to represent both the nested entities and their references (a.k.a. mentions).",5 Related Work,[0],[0]
"One issue in their approach is the spurious structures of hyper-graphs as they enumerate combinations of nodes, types and boundaries to represent entities.",5 Related Work,[0],[0]
"In addition, they fail to encode the dependencies among embedded entities using hyper-graphs.",5 Related Work,[0],[0]
"In contrast, our model enables nested entity representation by merging representation of multiple tokens composed in the entity and considers it as the longer
entity representation.",5 Related Work,[0],[0]
"This allows us to represent outer entities based on inner entity representation, thus managing to capture the relations between inner and outer entities, and hence overcoming the spurious entity structure problem.
",5 Related Work,[0],[0]
"As an improvement in overcoming spurious structure issue in Lu and Roth (2015), Muis and Lu (2017) further incorporated mention separators along with features to yield better performance on nested entities.",5 Related Work,[0],[0]
Both Lu and Roth (2015) and Muis and Lu (2017) rely on hand-crafted features to extract nested entities without incorporating hidden dependencies in nested entities.,5 Related Work,[0],[0]
"In contrast, we make the most of dependencies of nested entities in our model to encourage outer entity recognition by automatic learning of high-level and abstract features from sequences.
",5 Related Work,[0],[0]
Shared tasks dealing with nested entities like SemEval-2007 Task 99 and GermEval-201410 were held in order to advance the state-of-theart on this issue.,5 Related Work,[0],[0]
"Additionally, as subtasks in KBP 201511 and KBP 201612, one of the aims in tri-lingual Entity Discovery and Linking Track (EDL) track was extracting nested entities from textual documents varying from English, Chinese and Spanish.",5 Related Work,[0],[0]
"Following this task, Xu and Jiang (2016) firstly developed a new tagging scheme which is based on fixed-size ordinally-forgetting encoding (FOFE) method for text fragment representation.",5 Related Work,[0],[0]
All the entities along their contexts were represented using this novel tagging scheme.,5 Related Work,[0],[0]
"Different from the extensively used LSTM-RNNs in sequence labeling task, a feed-forward neural network was used to predict labels on entity level for each fragment in any of given sequences.",5 Related Work,[0],[0]
"Additionally, Li et al. (2017) used the model proposed in Lample et al. (2016) to the extract both flat entities and components composed in nested and discontinuous entities.",5 Related Work,[0],[0]
Another BiLSTM was applied to combine the components to get nested and discontinuous entities.,5 Related Work,[0],[0]
"However, these methods failed to capture and utilize the inner entity representation to facilitate outer entity detection.
9http://nlp.cs.swarthmore.edu/semeval/ tasks/index.php
10https://sites.google.com/site/ germeval2014ner/
11https://tac.nist.gov//2015/KBP/ 12https://tac.nist.gov//2016/KBP/",5 Related Work,[0],[0]
This paper presented a dynamic layered model which takes full advantage of inner entity information to encourage outer entity recognition in an end-to-end manner.,6 Conclusion,[0],[0]
"Our model is based on a flat NER layer consisting of LSTM and CRF, so our model is able to capture context representation of input sequences and globally decode predicted labels at a flat NER layer without relying on feature-engineering.",6 Conclusion,[0],[0]
Our model automatically stacks the flat NER layers with sharing the parameters of LSTM and CRF in the layers.,6 Conclusion,[0],[0]
"The stacking continues until the current flat NER layer predicts sequences as all outside of entities, which enables stopping dynamically stacked flat NER layers.",6 Conclusion,[0],[0]
"Each flat NER layer receives the merged context representation as input for outer entity recognition, based on the predicted entities from the previous flat NER layer.",6 Conclusion,[0],[0]
"With this dynamic endto-end model, our model is able to outperform existing models, achieving the-state-of-art on two nested NER tasks.",6 Conclusion,[0],[0]
"In addition, the model can be flexibly simplified as a flat NER model by removing components cascaded after the first NER layer.
",6 Conclusion,[0],[0]
"Extensive evaluation shows that utilization of inner entities significantly encourages outer entities detection with improvements of 3.9 and 9.1 percentage points in F-score on GENIA and ACE2005, respectively.",6 Conclusion,[0],[0]
"Additionally, utilization of only current context representation contributes to the performance improvement than use of context representation from multi-layers.",6 Conclusion,[0],[0]
We thank the anonymous reviewers for their valuable comments.,Acknowledgments,[0],[0]
The first author is financially supported by the University of Manchesters 2016 Presidents Doctoral Scholar Award.,Acknowledgments,[0],[0]
Sophia Ananiadou acknowledges BBSRC BB/P025684/1 Japan Partnering Award and BB/M006891/1 Empathy.,Acknowledgments,[0],[0]
This research has also been carried out with funding from AIRC/AIST and results obtained from a project commissioned by the New Energy and Industrial Technology Development Organization (NEDO).,Acknowledgments,[0],[0]
"Statistics of GENIA, ACE2005 and JNLPBA are described in Tables 7, 8 and 9, respectively.
",A Data Statistics and Preprocessing,[0],[0]
"We used NERSuite (Cho et al., 2010) for GENIA to perform tokenization while Stanford CoreNLP",A Data Statistics and Preprocessing,[0],[0]
"(Manning et al., 2014) was used for ACE2005.",A Data Statistics and Preprocessing,[0],[0]
"The JNLPBA dataset has already been went through tokenization and sentence splitting, so we did not apply any preprocessing.
",A Data Statistics and Preprocessing,[0],[0]
"For GENIA, we had to manually revolve the following two issues, in addition to the above preprocessing.
",A Data Statistics and Preprocessing,[0],[0]
One of the issues we had in this corpus is the removal of discontinuous entities during parsing.,A Data Statistics and Preprocessing,[0],[0]
"In provided GENIA XML file, each flat entity is annotated with ‘lex’ (lexical) and ‘sem’ (semantics) attributes while discontinuous and nested entities may have none, one or two attributes when these entities embed with each other, making it difficult to extract the strictly nested ones.",A Data Statistics and Preprocessing,[0],[0]
"Taken the text “recombinant human nm23-H1, -H2, mouse nm23-M1, and -M2 proteins” as an example, there are six discontinuous entities, “recombinant human nm23-H1 protein”, “recombinant human
H2 protein”, “recombinant mouse nm23-M1 protein”, “recombinant mouse nm23-M2 protein”, “mouse nm23-M2” and “human nm23-H2”, and two nested entities, “mouse nm23-M1” and “human nm23-H1”.",A Data Statistics and Preprocessing,[0],[0]
"We extract these nested entities based on symbol * appeared ‘lex’ attribute which
is an connection indicator of the separated texts in discontinuous entities.",A Data Statistics and Preprocessing,[0],[0]
"Meanwhile, each of the
separated texts has no ‘sem’ attribute unless itself is an innermost entity.",A Data Statistics and Preprocessing,[0],[0]
"Unfortunately, there are some inconsistent cases such as “c-fos and c-jun transcripts” where symbol * should be in the ‘lex’ attribute as the discontinuous entity “c-fos transcript” is connected by “c-fos” and “transcript” while “c-jun transcript” is connected by “c-jun” and “transcript”.",A Data Statistics and Preprocessing,[0],[0]
These two entities share the same text “transcript”.,A Data Statistics and Preprocessing,[0],[0]
"However, each of them is annotated with two attributes: ‘lex’ and ‘sem’, following the same annotation for flat entities.",A Data Statistics and Preprocessing,[0],[0]
"Although it is possible to ignore the latter entity based on ‘lex’ attribute and its belonging sentence, this rule fails to deal with entity “c-jun gene” in the example of “c-fos and c-jun genes” as the ‘lex’ of “c-jun gene” is mistaken as “c-jun genes”.",A Data Statistics and Preprocessing,[0],[0]
"Therefore, in this case, we ignored “c-fos transcript” and instead kept the “c-jun transcripts” as a flat entity.
",A Data Statistics and Preprocessing,[0],[0]
Another issue is the incomplete tokenization.,A Data Statistics and Preprocessing,[0],[0]
"The label assignment to one word was conducted on the word-level instead of character level, but there are entities that correspond to parts of words.",A Data Statistics and Preprocessing,[0],[0]
"An example is “NF-YA subunit”, which contains two protein entities: “NF-Y” and “A subunit”.",A Data Statistics and Preprocessing,[0],[0]
"To cope with this problem, we treat both two entities as false negative entities in training dataset as there are only 13 such entities in the training data set.",A Data Statistics and Preprocessing,[0],[0]
The hyper-parameters which were tuned for our model are listed in Table 10 and Table 11.,B Bayesian Optimization Setting,[0],[0]
These hyper-parameters are tuned by Bayesian optimization with the hyper parameters listed in Table 12.,B Bayesian Optimization Setting,[0],[0]
"Figure 4 shows the model architecture when we skip all intermediate LSTM layers and only word embeddings are used to produce the input for the next flat NER layer.
",C Model Structure,[0],[0]
Figure 5 describes the model architecture when we skip the representation of non-entity words to prepare the input for the next flat NER layer.,C Model Structure,[0],[0]
"Concretely, we merge and average representation following Equation 1.",C Model Structure,[0],[0]
"For the predicted non-entity words, however, we skip the LSTM layer and directly use their corresponding representation from the input rather than the output context representation.",C Model Structure,[0],[0]
Entity mentions embedded in longer entity mentions are referred to as nested entities.,abstractText,[0],[0]
"Most named entity recognition (NER) systems deal only with the flat entities and ignore the inner nested ones, which fails to capture finer-grained semantic information in underlying texts.",abstractText,[0],[0]
"To address this issue, we propose a novel neural model to identify nested entities by dynamically stacking flat NER layers.",abstractText,[0],[0]
Each flat NER layer is based on the state-ofthe-art flat NER model that captures sequential context representation with bidirectional long short-term memory (LSTM) layer and feeds it to the cascaded CRF layer.,abstractText,[0],[0]
Our model merges the output of the LSTM layer in the current flat NER layer to build new representation for detected entities and subsequently feeds them into the next flat NER layer.,abstractText,[0],[0]
"This allows our model to extract outer entities by taking full advantage of information encoded in their corresponding inner entities, in an inside-to-outside way.",abstractText,[0],[0]
Our model dynamically stacks the flat NER layers until no outer entities are extracted.,abstractText,[0],[0]
"Extensive evaluation shows that our dynamic model outperforms state-ofthe-art feature-based systems on nested NER, achieving 74.7% and 72.2% on GENIA and ACE2005 datasets, respectively, in terms of Fscore.1",abstractText,[0],[0]
A Neural Layered Model for Nested Named Entity Recognition,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4328–4339 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
4328",text,[0],[0]
Coherence is a key factor that distinguishes well-written texts from random collections of sentences.,1 Introduction,[0],[0]
A potential application of coherence models is text quality assessment.,1 Introduction,[0],[0]
"Examples include readability assessment (Pitler and Nenkova, 2008; Li and Hovy, 2014) and essay scoring (Miltsakaki and Kukich, 2004; Burstein et al., 2010).",1 Introduction,[0],[0]
"Here, we address the problem of local coherence modeling, which captures text relatedness at the level of sentence-to-sentence transitions.
",1 Introduction,[0],[0]
Several approaches to local coherence modeling have been proposed.,1 Introduction,[0],[0]
"Entity-based methods principally relate adjacent sentences by means of entities, which are mentioned as noun phrases, NPs, in sentences (Barzilay and Lapata, 2008; Elsner and Charniak, 2011; Guinaudeau and Strube,
∗This author is currently employed by the Ubiquitous Knowledge Processing (UKP) Lab, Technische Universität Darmstadt, https://www.ukp.tu-darmstadt.de.
2013; Tien Nguyen and Joty, 2017).",1 Introduction,[0],[0]
"Lexical models connect sentences based on semantic relations between words in sentences (Beigman Klebanov and Shamir, 2006; Heilman et al., 2007; Mesgar and Strube, 2016).",1 Introduction,[0],[0]
Both of these approaches suffer from different weaknesses.,1 Introduction,[0],[0]
"The entity-based models require an entity detection system, a coreference model, and a syntactic parser.",1 Introduction,[0],[0]
These subsystems need to be perfect to gain the best performance of entity-based coherence models.,1 Introduction,[0],[0]
"The weakness of the lexical models is that they consider words independently, i.e. regardless of context in that words appear.",1 Introduction,[0],[0]
"More concretely, such lexical models take sentences as a bag of words.",1 Introduction,[0],[0]
"Recent deep learning coherence work (Li and Hovy, 2014; Li and Jurafsky, 2017) adopts recursive and recurrent neural networks for computing semantic vectors for sentences.",1 Introduction,[0],[0]
"Coherence models that use recursive neural networks suffer from a severe dependence on external resources, e.g. a syntactic parser to construct their recursion structure.",1 Introduction,[0],[0]
Coherence models that purely rely on the recurrent neural networks process words sequentially within a text.,1 Introduction,[0],[0]
"However, in such models, long-distance dependencies between words cannot be captured effectively due to the limits of the memorization capability of recurrent networks.
",1 Introduction,[0],[0]
Our motivation is to overcome these limitations.,1 Introduction,[0],[0]
"We use the advantages of distributional representations in order to, first, identify and represent salient semantic information that connects sentences, and second, extract patterns of changes in such information as a text progresses.",1 Introduction,[0],[0]
"By representing words of sentences with their pre-trained embeddings, we take lexical semantic relations between words into account.",1 Introduction,[0],[0]
We employ a Recurrent Neural Network (RNN) layer to combine information in word embeddings and actual context information of words in sentences.,1 Introduction,[0],[0]
"Our model encodes salient information that relates two adja-
cent sentences based on the two most similar RNN states in sentences.",1 Introduction,[0],[0]
We accumulate two identified RNN states to represent semantic information that connects two adjacent sentences.,1 Introduction,[0],[0]
We encode pattern of semantic information changes across sentences in a text by a convolutional neural network to represent coherence.,1 Introduction,[0],[0]
Our end-to-end coherence model is superior to previous work because it relates sentences based on two semantic information states in sentences that are highly similar.,1 Introduction,[0],[0]
So it does not need extra tools such as coreference resolution systems.,1 Introduction,[0],[0]
"Furthermore, our model incorporates words in their sentence context and models (roughly) distant relations between words.
",1 Introduction,[0],[0]
We evaluate our model on two tasks: readability assessment and essay scoring.,1 Introduction,[0],[0]
"Both have been frequently used for coherence evaluation (Barzilay and Lapata, 2008; Miltsakaki and Kukich, 2004).",1 Introduction,[0],[0]
Readability assessment is a ranking task where we compare the rankings given by the model against human judgments.,1 Introduction,[0],[0]
"Essay scoring is a regression task, in which we investigate if the combination of coherence vectors produced by our model and other essay scoring features proposed by Phandi et al. (2015) improves the performance of the essay scorer.",1 Introduction,[0],[0]
"The experimental results show that our model achieves the state-of-the-art result for readability assessment on the examined dataset (De Clercq and Hoste, 2016); and the combination of our coherence features with other essay scoring features significantly improves the performance of the examined essay scorer (Phandi et al., 2015).",1 Introduction,[0],[0]
"Early work on coherence captures different types of relations: entity-based (Grosz et al., 1995; Barzilay and Lapata, 2008), lexical-based (Beigman Klebanov and Flor, 2013; Somasundaran et al., 2014; Zhang et al., 2015), etc.",2 Related Work,[0],[0]
"Among these models, the entity-grid model (Barzilay and Lapata, 2005, 2008) has received a lot of attention.",2 Related Work,[0],[0]
"In this model, entities are defined, heuristically, by applying a string match over head nouns of all NPs in a text.",2 Related Work,[0],[0]
"The model, then, defines all possible changes over syntactic roles of entities in adjacent sentences as coherence patterns.",2 Related Work,[0],[0]
"The entity-grid model has been extended both by expanding its entity extraction phase (Elsner and Charniak, 2011; Feng and Hirst, 2012) and by defining other types of patterns (Lin et al., 2011; Louis and Nenkova, 2012; Ji and Eisenstein, 2014; Guinaudeau and
Strube, 2013).",2 Related Work,[0],[0]
"Recently, Tien Nguyen and Joty (2017) fed entity grid representations of texts to a convolutional neural network (CNN) in order to overcome the limitation of predefined coherence patterns and extract patterns automatically.",2 Related Work,[0],[0]
"However, all of these models limit relations between sentences to entities that are shared by sentences.",2 Related Work,[0],[0]
This makes the performance of these models dependent on the performance of other tools like coreference resolution systems and syntactic parsers.,2 Related Work,[0],[0]
"Our coherence model, in contrast, is based on relations between any embedded semantic information in sentences, and does not require entity annotations.",2 Related Work,[0],[0]
A similar approach to ours is proposed by Mesgar and Strube (2016).,2 Related Work,[0],[0]
Their approach encodes lexical relations between sentences in a text via a graph.,2 Related Work,[0],[0]
"Sentences are encoded by nodes, and lexical semantic relations between sentences are represented by edges.",2 Related Work,[0],[0]
Coherence patterns are obtained by applying a subgraph mining method to graph representations of all texts in a corpus.,2 Related Work,[0],[0]
This model involves words individually and independent of their sentence context.,2 Related Work,[0],[0]
Our model uses a RNN layer over words in sentences to incorporate context information.,2 Related Work,[0],[0]
Our approach for extracting coherence patterns also differs from this model as we employ CNNs rather than graph mining.,2 Related Work,[0],[0]
Li and Hovy (2014) model sentences as vectors derived from RNNs and train a feed-forward neural network that takes an input window of sentence vectors and assigns a probability which represents the coherence of the sentences in the window.,2 Related Work,[0],[0]
Text coherence is evaluated by sliding the window over sentences and aggregating their coherence probabilities.,2 Related Work,[0],[0]
"Similarly, Li and Jurafsky (2017) study the same model at a larger scale and use a sequence-to-sequence approach in which the model is trained to generate the next sentence given the current sentence and vice versa.",2 Related Work,[0],[0]
Our approach differs from these methods; we represent coherence by a vector of coherence patterns.,2 Related Work,[0],[0]
"Moreover, our model takes distant relations between words in a text into account by relating two semantic states of sentences that are highly similar.",2 Related Work,[0],[0]
Lai and Tetreault (2018) compare the performance of the aforementioned coherence models on texts from different domains.,2 Related Work,[0],[0]
"They conclude that the neural coherence models, which are explained above, surpass examined nonneural coherence models such as the entity-based models and the lexical-based model.",2 Related Work,[0],[0]
"Unlike their
evaluation method, which predicts the coherence level of a text, we rank two texts with respect to their coherence levels for the readability assessment task.",2 Related Work,[0],[0]
"We also show that integrating our coherence model into an essay scorer improves its performance.
",2 Related Work,[0],[0]
"An important task for evaluating a coherence model is readability assessment (Li and Hovy, 2014; Petersen et al., 2015; Todirascu et al., 2016).",2 Related Work,[0],[0]
"The more coherent a text, the faster to read and easier to understand it is.",2 Related Work,[0],[0]
"Early readability formulas were based on superficial text features such as average word lengths (Kincaid et al., 1975).",2 Related Work,[0],[0]
"These formulas systematically ignore many important factors that affect readability such as discourse coherence (Barzilay and Lapata, 2008).",2 Related Work,[0],[0]
"Schwarm and Ostendorf (2005) and Feng et al. (2010) recast readability assessment as a ranking task, and employ different semantic (e.g. language model perplexity scores) and syntactic (e.g. the average number of NPs) features to solve this task.",2 Related Work,[0],[0]
Pitler and Nenkova (2008) show that discourse coherence features are more informative than other features for ranking texts with respect to their readability.,2 Related Work,[0],[0]
"Following the related work on coherence modeling (Barzilay and Lapata, 2008; Mesgar and Strube, 2015), we evaluate our coherence model on this task.
",2 Related Work,[0],[0]
"Another popular task for evaluating coherence models is essay scoring (Beigman Klebanov and Flor, 2013; Somasundaran et al., 2014).",2 Related Work,[0],[0]
"Miltsakaki and Kukich (2004) employ an essay scoring system to examine whether local coherence features, as defined by a measure of Centering Theory’s Rough-Shift transitions (Grosz et al., 1995), might be a significant contributor to the evaluation of essays.",2 Related Work,[0],[0]
They show that adding such features to their essay scorer improves its performance significantly.,2 Related Work,[0],[0]
"Burstein et al. (2010) specifically focus on the impact of entity transition features, as proposed by the entity-grid model for coherence modeling, on the essay scoring task.",2 Related Work,[0],[0]
"They demonstrate that by combining these features with other features related to grammar errors and word usage, the performance of their automated essay scoring system improves.",2 Related Work,[0],[0]
"Likewise, we combine our coherence vectors with other features that are used by a strong essay scorer (Phandi et al., 2015) and show that our coherence vectors improve the performance of this system significantly.
",2 Related Work,[0],[0]
"e0 e1 e2
E0
e3 e4 e5
E1
e6 e7 e8
E2
e9 e10 e12
E3
e13 e14 e15
E4
LOOKUP LOOKUP",2 Related Work,[0],[0]
LOOKUP,2 Related Work,[0],[0]
LOOKUP,2 Related Work,[0],[0]
"LOOKUP
LSTM LSTM LSTM LSTM LSTM
h00 h 1 0 h 2 0 h 0 1 h 1 1 h 2 1 h 0 2 h 1 2 h 2 2 h 0 3 h 1 3 h 2 3 h 0 4 h 1 4 h 2 4
",2 Related Work,[0],[0]
"H0 H1 H2 H3 H4
�f1 �f2 �f3 �f4
d23
CNN
�p",2 Related Work,[0],[0]
"In this section, we describe details of our model.",3 Coherence Model,[0],[0]
"First, we explain how we encode words in their context (Section 3.1).",3 Coherence Model,[0],[0]
"Then we show how we relate sentences (Section 3.2), and finally we explain how we represent coherence based on sentence relations (Section 3.3).",3 Coherence Model,[0],[0]
"A general formulation of our model is a parametric function, �p = Lθ (d), where d is an input document, θ indicates parameters of neural modules, and �p is a vector representation for the coherence of d. Figure 1 illustrates our model.",3 Coherence Model,[0],[0]
We use a lookup table to associate all words in the vocabulary with word embeddings.,3.1 Word and Context Representations,[0],[0]
"The lookup table is initialized by existing pre-trained word embeddings because they capture lexical semantic re-
lations between words.",3.1 Word and Context Representations,[0],[0]
"For sentence si, the lookup table returns matrix Ei whose rows are embeddings of words in si.",3.1 Word and Context Representations,[0],[0]
"A weakness of former lexical coherence models (Somasundaran et al., 2014; Mesgar and Strube, 2016) is that they only rely on semantic relations between words in sentences, regardless of the current context of words.",3.1 Word and Context Representations,[0],[0]
"In order to overcome this limitation, we use a standard unidirectional1 RNN with Long Short-Term Memory (LSTM) cells to encode the current context of words in sentences.",3.1 Word and Context Representations,[0],[0]
"For embedding matrix Ei:
Hi = LSTM � Ei, h n−1 i−1 � ,
where Hi is a list of LSTM states, and hn−1i−1 is the last LSTM state of sentence si−1.",3.1 Word and Context Representations,[0],[0]
Parameter n is the number of words in a sentence.,3.1 Word and Context Representations,[0],[0]
We take state vector hji ∈,3.1 Word and Context Representations,[0],[0]
"Hi as a representation of its input word embedding, ej , that is combined with its preceding word vectors in sentence si.",3.1 Word and Context Representations,[0],[0]
"For sake of brevity, the details of LSTM formulations are explained in Appendix A.",3.1 Word and Context Representations,[0],[0]
The relation between sentences is encoded by the most similar semantic states of sentences.,3.2 Sentence Relation Representations,[0],[0]
"Given two adjacent sentences, two of their LSTM states that have the highest similarity are selected to connect them.",3.2 Sentence Relation Representations,[0],[0]
Those LSTM states refer to the salient semantic information that is shared between sentences.,3.2 Sentence Relation Representations,[0],[0]
"To model this, we follow attention components in neural language models (Bahdanau et al., 2014; Vaswani et al., 2017) where the similarity between the last LSTM state and each of its preceding states is computed to measure the amount of attention that the model should give to its preceding context for generating the next word.",3.2 Sentence Relation Representations,[0],[0]
"More formally, for two adjacent sentences si and si−1, one LSTM state in Hi and one LSTM state in Hi−1 that have the maximum similarity are selected to represent the relation between the sentences:
(�u,�v) =",3.2 Sentence Relation Representations,[0],[0]
argmax,3.2 Sentence Relation Representations,[0],[0]
(�hm∈Hi) (,3.2 Sentence Relation Representations,[0],[0]
"�hn∈Hi−1)
(sim(�hm,�hn)),
where Hi and Hi−1 are LSTM states corresponding to sentences si and si−1.",3.2 Sentence Relation Representations,[0],[0]
"The similarity function, sim, returns the absolute value of the dot
1We use unidirectional RNN to model the way that an English text is read.
product between input vectors,
sim(�hm,�hn) =",3.2 Sentence Relation Representations,[0],[0]
"|�hm · �hn|, (1)
where the function |.| computes the absolute value of its input2.",3.2 Sentence Relation Representations,[0],[0]
"We use the dot product function because in practice it enables our model to calculate the above equations efficiently in parallel and in matrix-space, i.e., directly on Hi and Hi−1.",3.2 Sentence Relation Representations,[0],[0]
"Since this is the details of implementation, we explain matrix-based equations in Appendix B.",3.2 Sentence Relation Representations,[0],[0]
"The absolute value in the similarity function is used to encode semantic relatedness between associated information with vectors, which is independent of the sign of the similarity function (Manning and Schütze, 1999).
",3.2 Sentence Relation Representations,[0],[0]
We represent semantic information that relates two adjacent sentences by accumulating its selected LSTM states in the corresponding sentences.,3.2 Sentence Relation Representations,[0],[0]
"Since averaging in the vector space is an effective way to accumulate information represented in some vectors (Iyyer et al., 2015; Wieting et al., 2016), we compute the average of two identified vectors among the LSTM states of two adjacent sentences to represent semantic information shared by the sentences.",3.2 Sentence Relation Representations,[0],[0]
"More concretely, the vector representation of what relates sentence si to its immediately preceding sentence is obtained by averaging a vector of Hi and a vector of Hi−1 that are identified as highly similar:
�fi = avg(�u,�v) = �u+ �v
2 ,
where �u and �v are selected vectors.",3.2 Sentence Relation Representations,[0],[0]
�fi is the vector representation of what connects si to its immediately preceding sentence.,3.2 Sentence Relation Representations,[0],[0]
"Since sentences in a coherent text are about similar topics and share some semantic information, we compute semantic similarity between adjacent information states, i.e. �fis, to capture how they are changing through a text.",3.3 Coherence Representations,[0],[0]
"We propose to encode changes by a continuous value between 0 and 1, where 1 shows that there is no change and 0 indicates that there is a big semantic drift in a text.",3.3 Coherence Representations,[0],[0]
Any value in between depicts how far a text is semantically changing.,3.3 Coherence Representations,[0],[0]
"Given two adjacent vectors �fi and �fi+1, the degree of continuity between them is:
2In practice, the absolute function is implemented as g(z)",3.3 Coherence Representations,[0],[0]
"= max(0, z)−min(0, z) to be differentiable.
",3.3 Coherence Representations,[0],[0]
"d = sim(�fi, �fi+1)
l ,
where l is the length of input vectors, which is used to prevent large numbers (Vaswani et al., 2017), and sim is the similarity function (Section 3.2).",3.3 Coherence Representations,[0],[0]
"The task of this layer is to check if the salient information that is shared by two adjacent sentences is salient in the subsequent sentence or not.
",3.3 Coherence Representations,[0],[0]
The last layer of our model is a convolutional layer to automatically extract and represent patterns of semantic changes in a text.,3.3 Coherence Representations,[0],[0]
"CNNs have proven useful for various NLP tasks (Collobert et al., 2011; Kim, 2014; Kalchbrenner et al., 2014; Cheng and Lapata, 2016) because of their effectiveness in identifying patterns in their input (Xu et al., 2015).",3.3 Coherence Representations,[0],[0]
"In the case of coherence, the convolution layer can identify coherence patterns that correlate with final tasks (Tien Nguyen and Joty, 2017).",3.3 Coherence Representations,[0],[0]
We use a temporal narrow convolution by applying a kernel filter k of width h to a window of h adjacent transitions over sentences to produce a new coherence feature.,3.3 Coherence Representations,[0],[0]
"This filter is applied to each possible window of transitions in a text to produce a feature map �p, which is a coherence vector.",3.3 Coherence Representations,[0],[0]
"Since we use a standard convolution layer, we explain details of the CNN formulations in Appendix C.",3.3 Coherence Representations,[0],[0]
"In our experiments, we consider two variants of our model:",3.4 Variants of Our Model,[0],[0]
CohLSTM that is the full version of our model as described above; and CohEmb that is an ablation.,3.4 Variants of Our Model,[0],[0]
"CohEmb has no RNN layer, so the model is built directly on word embeddings.",3.4 Variants of Our Model,[0],[0]
"In this model, relations between sentences are made over only content words by eliminating all stop words.",3.4 Variants of Our Model,[0],[0]
Model configurations.,4 Implementation Details,[0],[0]
Our model is implemented in PyTorch3 with CUDA 8.0 support.,4 Implementation Details,[0],[0]
In preprocessing we apply zero-padding to all sentences and documents to make their length equal.,4 Implementation Details,[0],[0]
The vocabulary is limited to the 4000 most frequent words in the training data and all other words are replaced with the unknown token.,4 Implementation Details,[0],[0]
"We use the pre-trained word embeddings released by Zou et al. (2013), which are employed by
3https://pytorch.org
state-of-the-art essay scoring systems.",4 Implementation Details,[0],[0]
"The dimensions of word embeddings and LSTM cells are 50 and 300, respectively.",4 Implementation Details,[0],[0]
The convolution layer uses one filter with size 4.,4 Implementation Details,[0],[0]
"However, optimizing hyperparameters for each task may lead to better performance.",4 Implementation Details,[0],[0]
"For selecting two vectors with the highest similarity from the LSTM states of two adjacent sentences, we capture the similarity between any pair of LSTM states of the sentences as an element in a vector, and then apply a max-pooling layer to this vector of similarities to identify the pair with maximally similar LSTM states.",4 Implementation Details,[0],[0]
Selected LSTM states are used for representing salient information shared by the sentences.,4 Implementation Details,[0],[0]
"In CohEmb, stop words are removed by the SMART English stop word list (Salton, 1971).
",4 Implementation Details,[0],[0]
Training setup.,4 Implementation Details,[0],[0]
We set the mini-batch size to 32 and train the network for 100 epochs.,4 Implementation Details,[0],[0]
At each epoch we evaluate the model on the validation set and select the one with the best performance for test evaluations.,4 Implementation Details,[0],[0]
"We optimize with Adam, with an initial learning rate of 0.01.",4 Implementation Details,[0],[0]
Word vectors are updated during training.,4 Implementation Details,[0],[0]
The dropout method with rate 0.5 is employed for regularization.,4 Implementation Details,[0],[0]
Loss functions are specifically defined for each task.,4 Implementation Details,[0],[0]
"We evaluate our model on two downstream tasks: readability assessment (Section 5.1), in which coherence representations of documents are mapped to coherence scores, and then documents are ranked based on these scores; and essay scoring (Section 5.2), in which the coherence representation of an essay is combined with other features for essay scoring to quantify the quality of the essay.",5 Experiments,[0],[0]
Readability assessment – How difficult is a text to read and understand? – depends on many factors one of which is coherence.,5.1 Readability Assessment,[0],[0]
Texts that are more coherent are supposed to be faster to read and easier to understand.,5.1 Readability Assessment,[0],[0]
"Following earlier research on local coherence (Barzilay and Lapata, 2008; Pitler and Nenkova, 2008; Guinaudeau and Strube, 2013), we evaluate our coherence model on this task by ranking texts with respect to readability, instead of predicting readability scores.",5.1 Readability Assessment,[0],[0]
"More formally, we approach readability assessment as follows:",5.1 Readability Assessment,[0],[0]
"Given a text-pair, which text is easier to read?
",5.1 Readability Assessment,[0],[0]
Compared models.,5.1 Readability Assessment,[0],[0]
"We compare the two variants of our model as described in Section 3.4 with two following state-of-the-art systems:
Mesgar and Strube (2016).",5.1 Readability Assessment,[0],[0]
"This is a graph-based coherence model, in which nodes of a graph indicate sentences of a text, and an edge between two sentence nodes represents the existence of a lexico-semantic relation between two words in the sentences.",5.1 Readability Assessment,[0],[0]
Semantic relations between words are measured by the absolute value of the cosine function over their corresponding pre-trained word embeddings.,5.1 Readability Assessment,[0],[0]
If the similarity value for two word vectors is below a certain threshold4 then the connection between these two words is omitted.,5.1 Readability Assessment,[0],[0]
"Given the graph representation of a text, its coherence is encoded as a vector whose elements are frequencies of different subgraphs in the graph.",5.1 Readability Assessment,[0],[0]
The size of subgraphs is defined by the number of their nodes and is set to five.,5.1 Readability Assessment,[0],[0]
Subgraphs are extracted by a random sampling approach.,5.1 Readability Assessment,[0],[0]
We choose this model for comparison because its intuition is similar to our model.,5.1 Readability Assessment,[0],[0]
"However, this model suffers from the following limitations: word embeddings are considered independently, not in their current context; and a manual threshold is used for connection filtering.",5.1 Readability Assessment,[0],[0]
"We overcome these two weaknesses using the RNN and CNN layers in our model, respectively.
",5.1 Readability Assessment,[0],[0]
De Clercq and Hoste (2016).,5.1 Readability Assessment,[0],[0]
This is the state-of-the-art readability system on the examined dataset.,5.1 Readability Assessment,[0],[0]
It uses a rich set of readability features ranging from surface to semantic text features.,5.1 Readability Assessment,[0],[0]
The ranking is performed by LibSVM in their model.,5.1 Readability Assessment,[0],[0]
"We report their best performance that is achieved by extensive feature engineering and SVM’s parameter optimization.
",5.1 Readability Assessment,[0],[0]
Experimental setup.,5.1 Readability Assessment,[0],[0]
"In Section 3, we formulated our model as �p = Lθ (d) where θ represents parameters of the neural modules (i.e. the CNN and RNN layers) in our model.",5.1 Readability Assessment,[0],[0]
"For this task, we use an output layer to map coherence vector �p to score s which quantifies the degree of the perceived coherence of document d. Formally, the output layer is sd = �u · �p + b where �u and b are the weight vector and bias, respectively.",5.1 Readability Assessment,[0],[0]
"Let document d be more readable than document d�, then the model should ideally produce sd > s�d. We train the parameters of the model by a pairwise
4Like Mesgar and Strube (2016), we set this threshold to 0.9.
ranking approach and define the loss function as:
loss = max {0, 1− sd + sd�} .",5.1 Readability Assessment,[0],[0]
"The parameters of the model are shared to obtain the scores for texts in pair (d, d�).
",5.1 Readability Assessment,[0],[0]
Data.,5.1 Readability Assessment,[0],[0]
We use the readability dataset proposed by De Clercq et al. (2014).,5.1 Readability Assessment,[0],[0]
"It consists of 105 texts collected from the British National Corpus and Wikipedia in four different genres: administrative (e.g. reports and surveys), informative (e.g. articles of newspapers and Wikipedia entries), instructive (e.g. user manuals and guidelines), and miscellaneous (e.g., very technical texts and children’s literature).",5.1 Readability Assessment,[0],[0]
The average number of sentences is about 12 per text.,5.1 Readability Assessment,[0],[0]
"10, 907 pairs of texts are labeled with five fine-grained categories: {−100,−50, 0, 50, 100} indicating that the first text in a pair is respectively much easier, somewhat easier, equally difficult, somewhat difficult, more difficult to read than the second text in the pair.",5.1 Readability Assessment,[0],[0]
Labels of text-pairs are assigned by human judges.,5.1 Readability Assessment,[0],[0]
"Similar to De Clercq and Hoste (2016), we evaluate on the positive and negative labels as two sets of classes resulting in 6, 290 text-pairs in total.",5.1 Readability Assessment,[0],[0]
The original readability dataset does not provide any standard training/validation/test sets.,5.1 Readability Assessment,[0],[0]
"We apply 5-fold cross-validation over this dataset.
",5.1 Readability Assessment,[0],[0]
Evaluation metric.,5.1 Readability Assessment,[0],[0]
"The quality of a model is measured in terms of accuracy, which is the fraction of pairs that are correctly ranked by a model divided by the total number of document-pairs.",5.1 Readability Assessment,[0],[0]
We report the average accuracy over all runs of cross-validation as the final result.,5.1 Readability Assessment,[0],[0]
"We perform a paired t-test to determine if improvements are statistically significant (p < .05).
Results.",5.1 Readability Assessment,[0],[0]
"Table 1 summarizes the results of different systems for the readability assessment task.
",5.1 Readability Assessment,[0],[0]
"CohEmb significantly outperforms the graph-based coherence model proposed by
Mesgar and Strube (2016) by a large margin (6%), showing that our model captures coherence better than their model.",5.1 Readability Assessment,[0],[0]
"In our model, the CNN layer automatically learns which connections are important to be considered for coherence patterns, whereas this is performed in Mesgar and Strube (2016) by defining a threshold for eliminating connections.
",5.1 Readability Assessment,[0],[0]
"CohLSTM significantly outperforms both the coherence model proposed by Mesgar and Strube (2016) and the CohEmb model by 11% and 5%, respectively, and defines a new state-of-the-art on this dataset.",5.1 Readability Assessment,[0],[0]
"CohLSTM, unlike Mesgar and Strube (2016)’s model and CohEmb, considers words of sentences in their sentence context.",5.1 Readability Assessment,[0],[0]
"This supports our intuition that actual context information of words contributes to the perceived coherence of texts.
CohLSTM, which captures exclusively local coherence, even outperforms the readability system proposed by De Clercq and Hoste (2016), which relies on a wide range of lexical, syntactic and semantic features.",5.1 Readability Assessment,[0],[0]
One part of the student assessment process is essay writing where students are asked to write an essay about a given topic known as a prompt.,5.2 Essay Scoring,[0],[0]
An essay scoring system assigns an essay a score reflecting the quality of the essay.,5.2 Essay Scoring,[0],[0]
The quality of an essay depends on various factors including coherence.,5.2 Essay Scoring,[0],[0]
"Following previous studies (Miltsakaki and Kukich, 2004; Lei et al., 2014; Somasundaran et al., 2014; Zesch et al., 2015), we approach this task by combining the coherence vector produced by our model and the feature vector developed by an open-source essay scorer to represent an essay by a vector.",5.2 Essay Scoring,[0],[0]
"The final vector representation of an essay, �x, is mapped to a score by a simple neural regression method as follows:
s = sigmoid(�u · �x+ b),
where �u and b are the weight vector and the bias, respectively.",5.2 Essay Scoring,[0],[0]
"We exactly define vector �x for different examined systems, where we explain compared models for essay scoring.
",5.2 Essay Scoring,[0],[0]
Compared models.,5.2 Essay Scoring,[0],[0]
"We compare variations of our model (Section 3.4) with the following models:
EASE (BLRR).",5.2 Essay Scoring,[0],[0]
"As a baseline we use an open-source essay scoring system, Enhanced AI
Scoring Engine5 (EASE) (Phandi et al., 2015).",5.2 Essay Scoring,[0],[0]
This system was ranked third among all 154 participating teams in the Automated Student Assessment Prize (ASAP) competition and is the best among all open-source participating systems.,5.2 Essay Scoring,[0],[0]
"It employs Bayesian Linear Ridge Regression (BLRR) as its regression method applied to a set of linguistic features grouped in four categories: (i) Frequency-based features: such as the number of characters, the number of words, the number of commas, etc; (ii) POS-based features: the number of POS n-grams; (iii) Word overlap with prompt; (iv) Bag of n-grams: the number of uni-grams and bi-grams.
EASE.",5.2 Essay Scoring,[0],[0]
The difference between this system and EASE (BLRR) is in the employed regression method.,5.2 Essay Scoring,[0],[0]
This system uses a neural regression method as described above.,5.2 Essay Scoring,[0],[0]
"In order to have a similar experimental settings for this task, here, we use feature vectors generated by Phandi et al. (2015) to train our neural regression system.",5.2 Essay Scoring,[0],[0]
"The input of the neural regression function is a nonlinear transformation of the feature vector produced by EASE, �f .",5.2 Essay Scoring,[0],[0]
"Therefore �x = tanh(�w · �f + b).
EASE & CohEmb.",5.2 Essay Scoring,[0],[0]
"This model combines the feature vector computed by EASE, �f , and the coherence vector produced by CohEmb, �p, to have a more reliable representation of an essay.",5.2 Essay Scoring,[0],[0]
"More concretely, the input to our regression function, �x, is obtained as follows:
�h1 = tanh(�w1 · �f + b1), �h2 = �h1 ⊕ �p, �x = tanh(�w2 · �h2 + b2),
where ⊕ indicates the concatenation operatation.",5.2 Essay Scoring,[0],[0]
EASE & CohLSTM.,5.2 Essay Scoring,[0],[0]
The structure of this model is the same as the EASE & CohEmb structure.,5.2 Essay Scoring,[0],[0]
"But the input coherence vector, �p, is produced by CohLSTM.
",5.2 Essay Scoring,[0],[0]
Dong et al. (2017).,5.2 Essay Scoring,[0],[0]
"It is a sentence-document model, which is especially designed for this task.",5.2 Essay Scoring,[0],[0]
"It first encodes each sentence by a vector, which represents whole sentence meanings, and then use an RNN to embed vectors of sentences into a document vector.
",5.2 Essay Scoring,[0],[0]
Experimental setup.,5.2 Essay Scoring,[0],[0]
"The size of the input vector for the regression method, �x, is fixed to 100 and its output size is fixed to 1.",5.2 Essay Scoring,[0],[0]
"Dimensions of other parameters, �w1 and �w2, are set accordingly.",5.2 Essay Scoring,[0],[0]
"The
5https://github.com/edx/ease
loss function is the Mean Squared Error (MSE) between human scores, �H , and scores predicted by our system, �S:
MSE( �H, �S) = 1
N
�",5.2 Essay Scoring,[0],[0]
"( �H − �S)2.
",5.2 Essay Scoring,[0],[0]
"The models are compared for each prompt by running 5-fold cross-validation (Dong et al., 2017).
",5.2 Essay Scoring,[0],[0]
Data.,5.2 Essay Scoring,[0],[0]
We apply our model to a dataset used in the Automated Student Assessment Prize (ASAP) competition run by Kaggle6.,5.2 Essay Scoring,[0],[0]
The essays are associated with scores given by humans and categorized in eight prompts.,5.2 Essay Scoring,[0],[0]
Each prompt can be interpreted as a different essay topic along with different genres.,5.2 Essay Scoring,[0],[0]
"Table 2 summarizes some properties of this dataset.
",5.2 Essay Scoring,[0],[0]
Evaluation metric.,5.2 Essay Scoring,[0],[0]
ASAP adopted Quadratic Weighted Kappa (QWK) as the official evaluation metric.,5.2 Essay Scoring,[0],[0]
This metric measures the agreement between scores predicted by a system and scores assigned by humans.,5.2 Essay Scoring,[0],[0]
QWK considers chance agreements and penalizes large disagreements more than small agreements.,5.2 Essay Scoring,[0],[0]
We use an implementation of QWK that is described in Taghipour and Ng (2016).,5.2 Essay Scoring,[0],[0]
The formulation of QWK are explained in Appendix D. The final reported QWK is the average over QWKs of all prompts.,5.2 Essay Scoring,[0],[0]
"We perform a paired t-test to determine if improvements are statistically significant (p < .05).
Results.",5.2 Essay Scoring,[0],[0]
"Table 3 shows the results of different systems for the essay scoring task.
",5.2 Essay Scoring,[0],[0]
"Both EASE & CohEmb, and EASE & CohLSTM significantly improve EASE, confirming that our proposed representation for coherence is beneficial for essay scoring and
6https://www.kaggle.com/c/asap-aes/ data
improves the performance of the examined essay scoring system.",5.2 Essay Scoring,[0],[0]
"Our model does not beat the state-of-the-art essay scoring system (Dong et al., 2017), which is especially designed for this task and is tuned on this dataset.",5.2 Essay Scoring,[0],[0]
This model learns a vector representation for an input essay so that the vector performs the best for this regression task.,5.2 Essay Scoring,[0],[0]
"In contrast, the core of our best performing essay scoring system, i.e. EASE & CohLSTM, is the feature vector generated by EASE, which has less modeling capacity than a deep learning model like the model proposed by Dong et al. (2017).",5.2 Essay Scoring,[0],[0]
"The reason that we combine our coherence model with EASE, rather than the model proposed by Dong et al. (2017), is that EASE has no notion of coherence.",5.2 Essay Scoring,[0],[0]
"By combining our coherence model with it, we examine if our coherence vector improves its performance or not.
",5.2 Essay Scoring,[0],[0]
"Surprisingly, EASE & CohLSTM works on par with EASE & CohEmb.",5.2 Essay Scoring,[0],[0]
"To gain a better insight, we ablate EASE feature vectors and compare the performance of the coherence models, i.e., CohLSTM, and CohEmb.",5.2 Essay Scoring,[0],[0]
"Of course, coherence vectors on their own are not sufficient for predicting essay scores but this setup shows how much each variant of our model contributes to this task.",5.2 Essay Scoring,[0],[0]
"The two last rows in Table 3 show the results.
",5.2 Essay Scoring,[0],[0]
"CohLSTM outperforms CohEmb on all prompts, which matches the results for readability assessment.",5.2 Essay Scoring,[0],[0]
"This confirms our intuition that integrating the information of the current context of words contributes to coherence measurement.
",5.2 Essay Scoring,[0],[0]
"In terms of average QWK, CohLSTM works similar to EASE; however they behave differently on different prompts.",5.2 Essay Scoring,[0],[0]
"The largest improvement for CohLSTM, with respect to EASE, is obtained on prompt 7 and 8.",5.2 Essay Scoring,[0],[0]
"These two prompts ask for stories about laughter and patience, so corresponding essays can be categorized in the narrative genre (see Table 2).",5.2 Essay Scoring,[0],[0]
"The guidelines of these two prompts, which are publicly available in the Kaggle data, ask human annotators to assign the highest score to essays that are coherent and hold the attention of readers through an essay.",5.2 Essay Scoring,[0],[0]
"This is what our model captures: the sequence of semantic changes in a text, or coherence.
",5.2 Essay Scoring,[0],[0]
"On prompt 5, in contrast, we see the largest deterioration in performance of CohLSTM in comparison to EASE.",5.2 Essay Scoring,[0],[0]
This prompt asks students to describe the mood created by the author of a memoir.,5.2 Essay Scoring,[0],[0]
"Essays are expected to contain specific infor-
mation from the memoir so that an essay with the highest score has the highest coverage of all relevant and specific information from the memoir.",5.2 Essay Scoring,[0],[0]
"Therefore, mentioning the details of the memoir in essays of prompt 5 is more important than coherence for this prompt.",5.2 Essay Scoring,[0],[0]
"This also shows that our model exclusively captures the coherence of a text, which is the goal of this paper.",5.2 Essay Scoring,[0],[0]
We developed a local coherence model that encodes patterns of changes in what semantically relates adjacent sentences.,6 Conclusions,[0],[0]
The main novelty of our approach is that it defines sentence connections based on any semantic concept in sentences.,6 Conclusions,[0],[0]
"In this sense, our model goes beyond entity-based coherence models, which need extra dependencies such as coreference resolution systems.",6 Conclusions,[0],[0]
"Moreover, in contrast to lexical cohesion models, which take words individually, our model encodes words in their sentence context.",6 Conclusions,[0],[0]
Our model relates sentences by means of distant relations between word representations.,6 Conclusions,[0],[0]
The most similar LSTM states in two adjacent sentences are selected to encode the salient semantic concept that relates the sentences.,6 Conclusions,[0],[0]
"The model finally employs a convolutional layer to extract and represent patterns of topic changes across sentences in a text as a coherence vector.
",6 Conclusions,[0],[0]
We evaluate coherence vectors generated by our model on the readability assessment and essay scoring tasks.,6 Conclusions,[0],[0]
"On the former, our model achieves new state-of-the-art results.",6 Conclusions,[0],[0]
"On the latter, it significantly improves the performance of a strong essay scorer.",6 Conclusions,[0],[0]
"We believe the reason that our system works is that it learns which semantic concepts of sentences should be used to relate sentences, and which information about concepts is required to model sentence-to-sentence transitions.",6 Conclusions,[0],[0]
"In future
work we intend to run qualitative experiments on patterns that are extracted by our model to see if they are also linguistically interpretable.",6 Conclusions,[0],[0]
This work has been supported by the German Research Foundation (DFG) as part of the Research Training Group Adaptive Preparation of Information from Heterogeneous Sources (AIPHES) under grant No.,Acknowledgments,[0],[0]
"GRK 1994/1 and the Klaus Tschira Foundation, Heidelberg, Germany.",Acknowledgments,[0],[0]
"We thank Mohammad Taher Pilehvar, Ines Rehbein, and Mark-Christoph Müller for their valuable feedback on earlier drafts of this paper.",Acknowledgments,[0],[0]
We also thank anonymous reviewers for their useful suggestions for improving the quality of the paper.,Acknowledgments,[0],[0]
We propose a local coherence model that captures the flow of what semantically connects adjacent sentences in a text.,abstractText,[0],[0]
We represent the semantics of a sentence by a vector and capture its state at each word of the sentence.,abstractText,[0],[0]
"We model what relates two adjacent sentences based on the two most similar semantic states, each of which is in one of the sentences.",abstractText,[0],[0]
"We encode the perceived coherence of a text by a vector, which represents patterns of changes in salient information that relates adjacent sentences.",abstractText,[0],[0]
"Our experiments demonstrate that our approach is beneficial for two downstream tasks: Readability assessment, in which our model achieves new state-of-the-art results; and essay scoring, in which the combination of our coherence vectors and other taskdependent features significantly improves the performance of a strong essay scorer.",abstractText,[0],[0]
A Neural Local Coherence Model for Text Quality Assessment,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 209–216 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-2033",text,[0],[0]
"Many services such as web search (Leung et al., 2010), recommender systems (Ho et al., 2012), targeted advertising (Lim and Datta, 2013), and rapid disaster response (Ashktorab et al., 2014) rely on the location of users to personalise information and extract actionable knowledge.",1 Introduction,[0],[0]
"Explicit user geolocation metadata (e.g. GPS tags, WiFi footprint, IP address) is not usually available to third-party consumers, giving rise to the need for geolocation based on profile data, text content, friendship graphs (Jurgens et al., 2015) or some combination of these (Rahimi et al., 2015b,a).",1 Introduction,[0],[0]
"The strong geographical bias, most obviously at the language level (e.g. Finland vs. Japan), and more subtly at the dialect level (e.g. in English used in north-west England vs. north-east USA vs. Texas, USA), clearly reflected in language use in social media services such as Twitter, has been used extensively either for geolocation of users (Eisenstein et al., 2010; Roller et al., 2012; Rout et al., 2013; Han et al., 2014; Wing and Baldridge, 2014) or dialectology (Cook et al., 2014; Eisenstein, 2015).",1 Introduction,[0],[0]
"In these methods, a user is often represented by the concatenation of their tweets, and the geolocation model is trained on a very small percentage of explicitly geotagged
tweets, noting the potential biases implicit in geotagged tweets (Pavalanathan and Eisenstein, 2015).
",1 Introduction,[0],[0]
"Lexical dialectology is (in part) the converse of user geolocation (Eisenstein, 2015): given text associated with a variety of regions, the task is to identify terms that are distinctive of particular regions.",1 Introduction,[0],[0]
"The complexity of the task is two-fold: (1) localised named entities (e.g. sporting team names) are not of interest; and (2) without semantic knowledge it is difficult to detect terms that are in general use but have a special meaning in a region.
",1 Introduction,[0],[0]
In this paper we propose a text-based geolocation method based on neural networks.,1 Introduction,[0],[0]
"Our contributions are as follows: (1) we achieve state-of-the-art results on benchmark Twitter geolocation datasets; (2) we show that the model is less sensitive to the specific location discretisation method; (3) we release the first broad-coverage dataset for evaluation of lexical dialectology models; (4) we incorporate our text-based model into a network-based model (Rahimi et al., 2015a) and improve the performance utilising both network and text; and (5) we use the model’s embeddings for extraction of local terms and show that it outperforms two baselines.",1 Introduction,[0],[0]
Related work on Twitter user geolocation falls into two categories: text-based and network-based methods.,2 Related Work,[0],[0]
"Text-based methods make use of the geographical biases of language use, and networkbased methods rely on the geospatial homophily of user–user interactions.",2 Related Work,[0],[0]
"In both cases, the assumption is that users who live in the same geographic area share similar features (linguistic or interactional).",2 Related Work,[0],[0]
"Three main text-based approaches are: (1) the use of gazetteers (Lieberman et al., 2010; Quercini et al., 2010); (2) unsupervised text clustering based on topic models or similar (Eisenstein et al., 2010; Hong et al., 2012; Ahmed et al.,
209
2013); and (3) supervised classification (Ding et al., 2000; Backstrom et al., 2008; Cheng et al., 2010; Hecht et al., 2011; Kinsella et al., 2011; Wing and Baldridge, 2011; Han et al., 2012;",2 Related Work,[0],[0]
"Rout et al., 2013), which unlike gazetteers can be applied to informal text and compared to topic models, scales better.",2 Related Work,[0],[0]
"The classification models often rely on less than 1% of geotagged tweets for supervision and discretise real-valued coordinates into equalsized grids (Serdyukov et al., 2009), administrative regions (Cheng et al., 2010; Hecht et al., 2011; Kinsella et al., 2011; Han et al., 2012, 2014), or flat (Wing and Baldridge, 2011) or hierarchical k-d tree clusters (Wing and Baldridge, 2014).",2 Related Work,[0],[0]
"Network-based methods also use either real-valued coordinates (Jurgens et al., 2015) or discretised regions (Rahimi et al., 2015a) as labels, and use label propagation over the interaction graph (e.g. @-mentions).",2 Related Work,[0],[0]
"More recent methods have focused on representation learning by using sparse coding (Cha et al., 2015) or neural networks (Liu and Inkpen, 2015), utilising both text and network information (Rahimi et al., 2015a).
",2 Related Work,[0],[0]
"Dialect is a variety of language shared by a group of speakers (Wolfram and Schilling, 2015).",2 Related Work,[0],[0]
Our focus here is on geographical dialects which are spoken (and written in social media) by people from particular areas.,2 Related Work,[0],[0]
"The traditional approach to dialectology is to find the geographical distribution of known lexical alternatives (e.g. you, yall and yinz: (Labov et al., 2005; Nerbonne et al., 2008; Gonçalves and Sánchez, 2014; Doyle, 2014; Huang et al., 2015; Nguyen and Eisenstein, 2016)), the shortcoming of which is that the alternative lexical variables must be known beforehand.",2 Related Work,[0],[0]
"There have also been attempts to automatically identify such words from geotagged documents (Eisenstein et al., 2010; Ahmed et al., 2013; Cook et al., 2014; Eisenstein, 2015).",2 Related Work,[0],[0]
"The main idea is to find lexical variables that are disproportionately distributed in different locations either via model-based or statistical methods (Monroe et al., 2008).",2 Related Work,[0],[0]
"There is a research gap in evaluating the geolocation models in terms of their usability in retrieving dialect terms given a geographic region.
",2 Related Work,[0],[0]
"We use a text-based neural approach trained on geotagged Twitter messages that: (a) given a geographical region, identifies the associated lexical terms; and (b) given a text, predicts its location.",2 Related Work,[0],[0]
"We use three existing Twitter user geolocation datasets: (1) GEOTEXT (Eisenstein et al., 2010), (2) TWITTER-US (Roller et al., 2012), and (3) TWITTER-WORLD (Han et al., 2012).",3 Data,[0],[0]
These datasets have been used widely for training and evaluation of geolocation models.,3 Data,[0],[0]
"They are all prepartitioned into training, development and test sets.",3 Data,[0],[0]
"Each user is represented by the concatenation of their tweets, and labeled with the latitude/longitude of the first collected geotagged tweet in the case of GEOTEXT and TWITTER-US, and the centre of the closest city in the case of TWITTER-WORLD.1 GEOTEXT and TWITTER-US cover the continental US, and TWITTER-WORLD covers the whole world, with 9k, 449k and 1.3m users, respectively as shown in Figure 1.2 DAREDS is a dialect-term dataset novel to this research, created from the Dictionary of American Regional English (DARE) (Cassidy et al., 1985).",3 Data,[0],[0]
"DARE consists of dialect regions, their terms and meaning.3",3 Data,[0],[0]
"It is based on dialectal surveys from different regions of the U.S., which are then postprocessed to identify dialect regions and terms.",3 Data,[0],[0]
"In order to construct a dataset based on DARE, we downloaded the web version of DARE, cleaned it, and removed multiword expressions and highly-frequent words (any word which occurred in the top 50k most frequent words, based on a word frequency list (Norvig, 2009).",3 Data,[0],[0]
"For dialect regions that don’t correspond to a single state or set of cities (e.g. South), we mapped it to the most populous cities within each region.",3 Data,[0],[0]
"For example, within the Pacific Northwest dialect region, we manually extracted the most populous cities (Seattle, Tacoma, Portland, Salem, Eugene) and added those cities to DAREDS as subregions.
",3 Data,[0],[0]
The resulting dataset (DAREDS) consists of around 4.3k dialect terms from 99 U.S. dialect regions.,3 Data,[0],[0]
DAREDS is the largest standardised dialectology dataset.,3 Data,[0],[0]
"We use a multilayer perceptron (MLP) with one hidden layer as our location classifier, where the
1The decision as to how a given user is labeled was made by the creators of the original datasets, and has been preserved in this work, despite misgivings about the representativeness of the label for some users.
2The datasets can be obtained from https://github. com/utcompling/textgrounder
3http://www.daredictionary.com/
input is l2 normalised bag-of-words features for a given user.",4 Methods,[0],[0]
"We exclude @-mentions, words with document frequency less than 10, and stop words.",4 Methods,[0],[0]
"The output is either a k-d tree leaf node or k-means discretisation of real-valued coordinates of training locations, the output of which is visualised for TWITTER-US in Figure 2.",4 Methods,[0],[0]
"The hidden layer output provides word (and phrase, as bags of words) embeddings for dialectal analysis.
",4 Methods,[0],[0]
"The number of regions, regularisation strength, hidden layer and mini-batch size are tuned over development data and set to (32, 10−5, 896, 100), (256, 10−6, 2048, 10000) and (930, 10−6, 3720, 10000) for GEOTEXT, TWITTER-US and TWITTER-WORLD, respectively.",4 Methods,[0],[0]
"The parameters are optimised using Adamx (Kingma and Ba, 2014) using Lasagne/Theano (Theano Development Team, 2016).",4 Methods,[0],[0]
"Following Cheng (2010) and Eisenstein (2010), we evaluated the geolocation model using mean and median error in km (“Mean” and “Median” resp.)",4 Methods,[0],[0]
and accuracy within 161km of the actual location (“Acc@161”).,4 Methods,[0],[0]
"Note that lower numbers are better for Mean and Median, and higher numbers better for Acc@161.
",4 Methods,[0],[0]
"4The results reported in Rahimi et al. (2015b; 2015a) for TWITTER-WORLD were over a superset of the dataset; the results reported here are based on the actual dataset.
",4 Methods,[0],[0]
"While the focus of this paper is text-based user geolocation, state-of-the-art results for the three datasets have been achieved with hybrid text+network-based models, where the predictions of the text-based model are fed into a mention network as “dongle” nodes to each user node, providing a personalised geolocation prior for each user (Rahimi et al., 2015a).",4 Methods,[0],[0]
"Note that it would, of course, be possible to combine text and network information in a joint deep learning model (Yang et al., 2016; Kipf and Welling, 2016), which we leave to future work (noting that scalability will potentially be a major issue for the larger datasets).
",4 Methods,[0],[0]
"To test the applicability of the model’s embeddings in dialectology, we created DAREDS.",4 Methods,[0],[0]
The output of the hidden layer of the model is used as embeddings for both location names and dialect terms.,4 Methods,[0],[0]
"Given a dialect region name, we retrieve its nearest neighbours in the embedding space, and compare them to dialect terms associated with that location.",4 Methods,[0],[0]
"We also compare the quality of the embeddings with pre-trained word2vec embeddings and the embeddings from the output layer of LR (logistic regression) (Rahimi et al., 2015b) as baselines.",4 Methods,[0],[0]
"Regions in DAREDS can be very broad (e.g. SouthWest), meaning that words associated with those locations will be used across a large number
GEOTEXT TWITTER-US TWITTER-WORLD
Acc@161 Mean Median Acc@161 Mean Median Acc@161 Mean Median TEXT-BASED METHODS
Proposed method (MLP + k-d tree) 38 844 389 54 554 120 34 1456 415 Proposed method (MLP + k-means) 40 856 380 55 581 91 36 1417 373
(Rahimi et al., 2015b) (LR) 38 880 397 50 686 159 32 1724 530 (Wing and Baldridge, 2014) (uniform) — — — 49 703 170 32 1714 490 (Wing and Baldridge, 2014) (k-d tree) — — — 48 686 191 31 1669 509 (Melo and Martins, 2015) — — — — 702 208 — 1507 502 (Cha et al., 2015) — 581 425 — — — — — — (Liu and Inkpen, 2015) — — — — 733 377 — — —
NETWORK-BASED METHODS
(Rahimi et al., 2015a)",4 Methods,[0],[0]
"MADCEL-W 58 586 60 54 705 116 45 2525 279
TEXT+NETWORK-BASED METHODS
of cities contained within that region.",4 Methods,[0],[0]
"We generate a region-level embedding by simply taking the city names associated with the region, and feeding them as BoW input for LR and MLP and averaging their embeddings for word2vec.",4 Methods,[0],[0]
"We evaluate the retrieved terms by computing recall of DAREDS terms existing in TWITTER-US (1071 terms) at k ∈ {0.05%, 0.1%, 0.2%, 0.5%, 1%, 2%, 5%} of vocabulary size.",4 Methods,[0],[0]
The code and the DAREDS dataset are available at https://github.,4 Methods,[0],[0]
com/afshinrahimi/acl2017.,4 Methods,[0],[0]
The performance of the text-based MLP model with k-d tree and k-means discretisation over the three datasets is shown in Table 1.,5.1 Geolocation,[0],[0]
"The results are also compared with state-of-the-art text-based methods based on a flat (Rahimi et al., 2015b; Cha et al., 2015) or hierarchical (Wing and Baldridge, 2014; Melo and Martins, 2015; Liu and Inkpen, 2015) geospatial representation.",5.1 Geolocation,[0],[0]
Our method outperforms both the flat and hierarchical text-based models by a large margin.,5.1 Geolocation,[0],[0]
"Comparing the two discretisation strategies, k-means outperforms k-d tree by a reasonable margin.",5.1 Geolocation,[0],[0]
"We also incorporated the MLP predictions into a network-based model based on the method of Rahimi et al. (2015a), and improved upon their work.",5.1 Geolocation,[0],[0]
"We analysed the Median error of MLP (k-d tree) over the development users of TWITTER-US in each of the U.S. states as shown
in Figure 3.",5.1 Geolocation,[0],[0]
"The error is highest in states with lower training coverage (e.g. Maine, Montana, Wisconsin, Iowa and Kansas).",5.1 Geolocation,[0],[0]
We also randomly sampled 50 development samples from the 1000 samples with highest prediction errors to check the biases of the model.,5.1 Geolocation,[0],[0]
Most of the errors are the result of geolocating users from Eastern U.S. in Western U.S. particularly in Los Angeles and San Francisco.,5.1 Geolocation,[0],[0]
"We quantitatively tested the quality of the geographical embeddings by calculating the micro-average recall of the k-nearest dialect terms (in terms of the proportion of retrieved dialect terms) given a dialect region, as shown in Figure 4.",5.2 Dialectology,[0],[0]
"Recall at 0.5% is about 3.6%, meaning that we were able to retrieve 3.6% of the dialect terms given the dialect region name in the geographical embedding space.",5.2 Dialectology,[0],[0]
"The embeddings slightly outperform the output layer of logistic regression (LR) (Rahimi et al., 2015b)
and word2vec pre-trained embeddings, but there is still substantial room for improvement.
",5.2 Dialectology,[0],[0]
"Our model is slightly better than both baselines, and can retrieve 3.6% of the correct dialect terms given the region name at 0.5% of the total vocabulary, noting the significant performance gap left for future research.",5.2 Dialectology,[0],[0]
"It is worth noting that the retrieved terms that are not included in DAREDS are not irrelevant: many of them are toponyms (e.g. city names, rivers, companies, or companies) associated with the given region which are not of interest in dialectology.",5.2 Dialectology,[0],[0]
"Equally, some are terms that don’t exist in the DARE dictionary but might be of interest for dialectologists because language use in social media is so dynamic that they won’t be captured by traditional survey-like approaches.",5.2 Dialectology,[0],[0]
"A major shortcoming of this work is that it doesn’t
incorporate sense distinctions and so can’t recover dialect terms that are uniformly distributed but have an idiomatic usage in a particular region.",5.2 Dialectology,[0],[0]
"We proposed a new text geolocation model based on the multilayer perceptron (MLP), and evaluated it over three benchmark Twitter geolocation datasets.",6 Conclusion and Future Work,[0],[0]
We achieved state-of-the-art text-based results over all datasets.,6 Conclusion and Future Work,[0],[0]
We used the parameters of the hidden layer of the neural network as word and phrase embeddings.,6 Conclusion and Future Work,[0],[0]
"We performed a nearest neighbour search on a sample of city names and dialect terms, and showed that the embeddings can be used both to discover dialect terms from a geographic area and to find the geographic area a dialect term is spoken.",6 Conclusion and Future Work,[0],[0]
"To evaluate the geographical embeddings quantitatively, we created DAREDS, a machine-readable version of the DARE dictionary and compared the performance of dialect term retrieval given dialect region name in terms of recall (Figure 4), and compared the performance to the performance in pre-trained word2vec and LR embeddings.",6 Conclusion and Future Work,[0],[0]
We thank the anonymous reviewers for their insightful comments and valuable suggestions.,Acknowledgments,[0],[0]
"This work was funded in part by the Australian Government Research Training Program Scholarship, and the Australian Research Council.",Acknowledgments,[0],[0]
"We propose a simple yet effective textbased user geolocation model based on a neural network with one hidden layer, which achieves state of the art performance over three Twitter benchmark geolocation datasets, in addition to producing word and phrase embeddings in the hidden layer that we show to be useful for detecting dialectal terms.",abstractText,[0],[0]
"As part of our analysis of dialectal terms, we release DAREDS, a dataset for evaluating dialect term detection methods.",abstractText,[0],[0]
A Neural Model for User Geolocation and Lexical Dialectology,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4704–4710 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
4704",text,[0],[0]
Reading involves the integration of noisy perceptual evidence with probabilistic expectations about the likely contents of the text.,1 Introduction,[0],[0]
"Words that are consistent with these expectations are identified more quickly (Ehrlich and Rayner, 1981; Smith and Levy, 2013).",1 Introduction,[0],[0]
"For the reader’s expectations to be maximally effective, they should not only reflect the reader’s past experience with the language (Hale, 2001; MacDonald and Christiansen, 2002), but should also be adapted to the current context.",1 Introduction,[0],[0]
"Optimal adaptation would reflect properties of the text being read, such as genre, topic and writer identity, as well as the general tendency for recently used words and syntactic structures to be reused with higher probability (Bock, 1986; Church, 2000; Dubey et al., 2006).
",1 Introduction,[0],[0]
"Several studies have suggested that readers do in fact adapt their lexical and syntactic predictions to the current context (Otten and Van Berkum, 2008; Fine et al., 2013; Fine and Jaeger, 2016).1 For example, Fine and Jaeger investigated the processing of “garden path” sentences such as (1):
1Recently, Harrington Stack et al. (2018) questioned the robustness of the results of Fine et al. (2013).
",1 Introduction,[0],[0]
"(1) The experienced soldiers warned about the dangers conducted the midnight raid.
",1 Introduction,[0],[0]
The word warned in (1) is initially ambiguous between a main verb interpretation (the soldiers were doing the warning) and a reduced relative clause interpretation (the soldiers were being warned).,1 Introduction,[0],[0]
"When the word conducted is reached, this ambiguity is resolved in favor of the reduced relative parse.",1 Introduction,[0],[0]
Reduced relatives are infrequent constructions.,1 Introduction,[0],[0]
"This makes the disambiguating word conducted unexpected, causing it to be read more slowly than it would be in a context such as (2), in which the words who were indicate early on that only the relative clause parse is possible:
(2) The experienced soldiers who were warned about the dangers conducted the midnight raid.
",1 Introduction,[0],[0]
Fine,1 Introduction,[0],[0]
and Jaeger included a large proportion of reduced relatives in their experiment.,1 Introduction,[0],[0]
"As the experiment progressed, the cost of disambiguation in favor of the reduced relative interpretation decreased, suggesting that readers had come to expect a construction that is normally infrequent.
",1 Introduction,[0],[0]
"Human syntactic expectations have been successfully modeled with syntax-based language models (Hale, 2001; Levy, 2008; Roark et al., 2009).",1 Introduction,[0],[0]
"Recently, language models (LMs) based on recurrent neural networks (RNNs) have been shown to make adequate syntactic predictions (Linzen et al., 2016; Gulordava et al., 2018), and to make comparable reading time predictions to syntax-based LMs (van Schijndel and Linzen, 2018).",1 Introduction,[0],[0]
"In this paper, we propose a simple way to continuously adapt a neural LM, and test the method’s psycholinguistic plausibility.",1 Introduction,[0],[0]
We show that LM adaptation significantly improves our ability to predict human reading times using the LM.,1 Introduction,[0],[0]
"Follow-up experiments with controlled materials show that the LM adapts not only to specific
vocabulary items but also to abstract syntactic constructions, as humans do.",1 Introduction,[0],[0]
"We use a simple method to adapt our LM: at the end of each new test sentence, we update the parameters of the LM based on its cross-entropy loss when predicting that sentence; the new weights are then used to predict the next test sentence.2",2 Method,[0],[0]
"Our baseline LM is a long short-term memory (LSTM; Hochreiter and Schmidhuber, 1997) language model trained on 90 million words of English Wikipedia by Gulordava et al. (2018) (see Supplementary Materials for details).",2 Method,[0],[0]
"For adaptation, we keep the learning rate of 20 used by Gulordava et al. (the gradient is multiplied by this learning rate during weight updates).",2 Method,[0],[0]
"We examine the effect of this parameter in Section 5.2.
",2 Method,[0],[0]
"We tested the model on the Natural Stories Corpus (Futrell et al., 2018), which has 10 narratives with self-paced reading times from 181 native English speakers.",2 Method,[0],[0]
There are two narrative genres in the corpus: fairy tales (seven texts) and documentary accounts (three texts).,2 Method,[0],[0]
We first measured how well the adaptive model predicted upcoming words.,3 Linguistic accuracy,[0],[0]
"We report the model’s perplexity, a quantity which is lower when the LM assigns higher probabilities to the words that in fact occurred.",3 Linguistic accuracy,[0],[0]
"We adapted the model to the first k sentences of each text, then tested it on sentence k+1, for all k. Adaptation dramatically improved test perplexity compared to the non-adaptive version of the model (86.99 vs. 141.49).
",3 Linguistic accuracy,[0],[0]
We next adapted the model to each genre separately.,3 Linguistic accuracy,[0],[0]
"If the model adapts to stylistic or syntactic patterns, we might expect adaptation to be more helpful in the fairy tale than the documentary genre: the Wikipedia corpus that the LM was originally trained on is likely to be more similar in style to the documentary genre.",3 Linguistic accuracy,[0],[0]
"Consistent with this hypothesis, the documentary texts benefited less from adaptation (99.33 to 73.20) than the fairy tales (160.05 to 86.47), though the fact that both saw improvement from adaptation suggests that text-specific adaptation is beneficial even if the genre is similar to the training genre.
",3 Linguistic accuracy,[0],[0]
"2Our code is publicly available at: https://github. com/vansky/neural-complexity.git
Each genre consists of multiple texts.",3 Linguistic accuracy,[0],[0]
"Does adaptation to a particular text lead to catastrophic forgetting (McCloskey and Cohen, 1989), such that the LM overfits to the text and forgets its more general knowledge acquired from the Wikipedia training corpus?",3 Linguistic accuracy,[0],[0]
"This was not the case; in fact, adapting to the entirety of each genre without reverting to the baseline model after each text led to a very slightly better perplexity (fairytales: 86.47, documentaries: 73.20) compared with a setting in which the LM was reverted after each text (fairytales: 86.61, documentaries: 73.63).",3 Linguistic accuracy,[0],[0]
We next tested whether our adaptive LM matches human expectations better than a non-adaptive model.,4 Modeling human expectations,[0],[0]
"Since each reader saw the texts in a different order, we adapted the LM to each text separately: after each story, we reverted to the initial Wikipedia-trained LM and restarted adaptation on the next text.",4 Modeling human expectations,[0],[0]
"If anything, this likely resulted in a conservative estimate of the benefit of adaptation compared to a model that adapts continuously across multiple stories from the same genre, as humans might do.3
We used surprisal as a linking function between the LM’s predictions and human reading times
3We do not distinguish between priming and adaptation in this paper.",4 Modeling human expectations,[0],[0]
"While it may be tempting to think of the LSTM memory cell as a model of priming and of the weight updates as a model of adaptation, Bock and Griffin (2000) provide evidence that priming cannot simply be a function of residual activation and that priming can be driven by longer-term learning (see Tooley and Traxler (2010) for more discussion on priming vs. adaptation).
",4 Modeling human expectations,[0],[0]
"(Hale, 2001; Smith and Levy, 2013).",4 Modeling human expectations,[0],[0]
"Surprisal quantifies how unpredictable each word (wi) is given the preceding words:
surprisal(wi) = −log",4 Modeling human expectations,[0],[0]
"P(wi | w1...wi−1) (1)
We fit the self-paced reading times in the Natural Stories Corpus with linear mixed effects models (LMEMs), a generalization of linear regression (see Supplementary Materials for details).
",4 Modeling human expectations,[0],[0]
"In line with previous work, non-adaptive surprisal was a significant predictor of reading times (p < 0.001) when the model only included other baseline factors (Table 1, Top).",4 Modeling human expectations,[0],[0]
"Adaptive surprisal was a significant predictor of reading times (p < 0.001) over non-adaptive surprisal and all baseline factors (Table 1, Bottom).",4 Modeling human expectations,[0],[0]
"Crucially, nonadaptive surprisal was no longer a significant predictor of reading times once adaptive surprisal was included.",4 Modeling human expectations,[0],[0]
This indicates that the predictions of the adaptive model subsume the predictions of the non-adaptive one.,4 Modeling human expectations,[0],[0]
We have shown that LM adaptation improves our ability to model human expectations as reflected in a self-paced reading time corpus.,5 Does the model adapt to syntax?,[0],[0]
"How much of this improvement is due to adaptation of the model’s syntactic representations (Bacchiani et al., 2006; Dubey et al., 2006) and how much is simply due to the model assigning a higher probability to words that have recently occurred (Kuhn and de Mori, 1990; Church, 2000)?",5 Does the model adapt to syntax?,[0],[0]
"We address this ques-
tion using two syntactic phenomena: reduced relative clauses and the dative alternation.",5 Does the model adapt to syntax?,[0],[0]
We adapted the model independently to random orderings of the critical and filler stimuli used in Experiment 3 of Fine and Jaeger (2016);4 this experiment (described in the Introduction) contained a much higher proportion of reduced relative clauses than their general distribution in English.,5.1 Reduced relative clauses,[0],[0]
We used surprisal as our proxy for reading times.,5.1 Reduced relative clauses,[0],[0]
"Following Fine and Jaeger, we took the mean surprisal over three words in each ambiguous sentence: the disambiguating word and the following two words (e.g., conducted the midnight in example (1)).",5.1 Reduced relative clauses,[0],[0]
"To estimate the magnitude of the syntactic disambiguation penalty while also controlling for lexical content, we subtracted this quantity from the mean surprisal over the exact same words in the paired unambiguous sentence (2).",5.1 Reduced relative clauses,[0],[0]
"Linear regression showed that the disambiguation penalty decreased as the model was exposed to more critical items (item order coefficient: β̂ = −0.0804, p < 0.001), indicating that the LM was adapting to reduced relatives, a syntactic construction without any lexical content.
",5.1 Reduced relative clauses,[0],[0]
"In order to compare our findings more directly with the results given by Fine and Jaeger (2016) (shown in Figure 1), we mimicked their method of plotting reading times.",5.1 Reduced relative clauses,[0],[0]
"First, we fit a linear model of the mean surprisal of each disambiguating region with the number of trials the model had seen in the experiment thus far to account for a general trend of subjects speeding up over the course
4See details in the Supplementary Materials.
of the experiment.",5.1 Reduced relative clauses,[0],[0]
"Then, we plotted the mean residual model surprisal that was left in the disambiguating region in both the ambiguous and unambiguous conditions as the experiment progressed.",5.1 Reduced relative clauses,[0],[0]
The shape of our model’s adaptation to the reduced relative construction (upper curve in Figure 2) matched the human results reported by Fine and Jaeger.,5.1 Reduced relative clauses,[0],[0]
"Like humans, the model showed an initially large adaptation effect, followed by more gradual adaptation thereafter.",5.1 Reduced relative clauses,[0],[0]
Both humans and our model continued to adapt over all the items rather than just at the beginning of the experiment.,5.1 Reduced relative clauses,[0],[0]
"Also like humans, the model’s response to unambiguous items did not change significantly over the course of the experiment (p = 0.91).",5.1 Reduced relative clauses,[0],[0]
"Dative events can be expressed using two roughly equivalent English constructions:
(3) a. Prepositional object (PO): The boy threw the ball to the dog.
",5.2 The dative alternation,[0],[0]
"b. Double object (DO): The boy threw the dog the ball.
",5.2 The dative alternation,[0],[0]
"Work in psycholinguistics has shown that recent experience with one of these variants increases the probability of producing that variant (Bock, 1986; Kaschak et al., 2006) as well as the likelihood of predicting it in reading (Tooley and Bock, 2014).",5.2 The dative alternation,[0],[0]
"To test whether our adaptation method can reproduce this behavior, we generated 200 pairs of dative sentences similar to (3).",5.2 The dative alternation,[0],[0]
"We shuffled 100 DO sentences into 1000 filler sentences sampled from the Wikitext-2 training corpus (Merity et al., 2016) and adapted the model to these 1100 sentences.",5.2 The dative alternation,[0],[0]
"We then froze the weights of the adapted model and tested its predictions for two types of sentences: the PO counterparts of the DO sentences in the adaptation set, which shared the vocabulary of the adaptation set but differed in syntax; and 100 new DO sentences, which shared syntax but no content words with the adaptation set.5
An additional goal of this experiment was to examine the effect of learning rate on adaptation.",5.2 The dative alternation,[0],[0]
During adaptation the model performs a single parameter update after each sentence and does not train until convergence with gradual reduction of the learning rate as would normally be the case during LM training.,5.2 The dative alternation,[0],[0]
"Consequently, the learning
5For additional details as well as the reverse setting (adaptation to PO), see Supplementary Materials.
rate parameter crucially determines the amount of adaptation the model can undertake after each sentence.",5.2 The dative alternation,[0],[0]
"If the learning rate is very low, adaptation will not have any effect; if it is too high, either the model will overfit after each update and will not generalize well, or the model will forget its trained representation as it overshoots the targeted minima.",5.2 The dative alternation,[0],[0]
The optimal rate may differ between lexical and syntactic adaptation.,5.2 The dative alternation,[0],[0]
"Our experiments thus far all used the same learning rate as our original model (20); here, we varied the learning rate on a logarithmic scale between 0.002 and 200.
",5.2 The dative alternation,[0],[0]
The results of this experiment are shown in Figure 3.,5.2 The dative alternation,[0],[0]
The model successfully adapted to the DO construction as well as to the vocabulary of the adaptation sentences.,5.2 The dative alternation,[0],[0]
"This was the case for all of the learning rates except for 200, which resulted in enormous perplexity on both sentence types.",5.2 The dative alternation,[0],[0]
"Both lexical and syntactic adaptation were most successful when the learning rate was around 2, with perplexity reductions of 94% for lexical adaptation and 84% for syntactic adaptation.
",5.2 The dative alternation,[0],[0]
Syntactic adaption was penalized at higher learning rates more than lexical adaptation (compare learning rates of 2 and 20).,5.2 The dative alternation,[0],[0]
"This fragility of syntactic adaptation likely stems from the fact that the model can directly observe the relevant vocabulary but syntax is latent and must be inferred from multiple similar sentences, a generalization which is impeded by overfitting at higher learning rates.",5.2 The dative alternation,[0],[0]
Our analysis of the Natural Stories corpus did not indicate that the model suffered from catastrophic forgetting.,6 Testing for catastrophic forgetting,[0],[0]
"Yet the Natural Stories corpus contained only two genres; to address the issue of catastrophic forgetting more systematically, we used the premise sentences from the MultiNLI corpus (Williams et al., 2018) — a total of 2000 sentences for each of 10 genres.
",6 Testing for catastrophic forgetting,[0],[0]
"For each genre pair G1 and G2 (omitting cases where G1 = G2), we first adapted the baseline Wikipedia model to 1000 sentences of G1 using a learning rate of 2 (shown to be optimal in Section 5.2).",6 Testing for catastrophic forgetting,[0],[0]
We then adapted the model to 1000 sentences of G2.,6 Testing for catastrophic forgetting,[0],[0]
"Finally, we froze the model’s weights and tested its perplexity on the 1000 heldout sentences from G1.
",6 Testing for catastrophic forgetting,[0],[0]
The results averaged across all pairs of genres are plotted in Figure 4.,6 Testing for catastrophic forgetting,[0],[0]
"Unsurprisingly, the model performed best on G1 immediately after adapting to it (middle bar).",6 Testing for catastrophic forgetting,[0],[0]
"Crucially, even after adapting to 1000 sentences of G2 after its last exposure to G1 (right bar), it still modeledG1 much better than the non-adapted model (left bar).",6 Testing for catastrophic forgetting,[0],[0]
These results suggest that catastrophic forgetting is not a concern even with a relatively large amount of data.,6 Testing for catastrophic forgetting,[0],[0]
"Adaptation greatly improved an RNN LM’s word prediction accuracy, in line with other work on LM adaptation (Kneser and Steinbiss, 1993).",7 Discussion,[0],[0]
"We showed that the adapted model was psycholinguistically plausible, in two senses.",7 Discussion,[0],[0]
"First, it improved the correlation between surprisal derived from the model and human reading times, sug-
gesting that the model generated more human-like expectations.",7 Discussion,[0],[0]
"Second, using materials that teased apart lexical content from syntax, we showed that the model adapted both its lexical and its syntactic predictions, in line with findings from human experiments.",7 Discussion,[0],[0]
"Finally, as in other neural-network based models in psychology (Chang et al., 2006), our gradient-based updates naturally incorporate the error-driven nature of syntactic adaptation; while we did not demonstrate this in the current paper, we hypothesize that our model will reproduce the finding that more surprising words lead to greater adaptation (Jaeger and Snider, 2013).
",7 Discussion,[0],[0]
The simplicity of our adaptation method makes it attractive for use in modeling human expectations.,7 Discussion,[0],[0]
"Since adaptive surprisal is strictly superior to non-adaptive surprisal in modeling reading times, it would be a stronger baseline in analyses that aim to demonstrate the contribution of factors other than predictability.
",7 Discussion,[0],[0]
"We used a simple neural adaptation approach, where we performed continuous gradient updates based on the prediction error on the adaptation sentences (see also Krause et al., 2017).",7 Discussion,[0],[0]
"An alternative approach to neural LM adaptation uses recent RNN states in conjunction with the current state to make word predictions (Grave et al., 2017; Merity et al., 2017); a comparison of the two methods using our paradigms may provide insight into their relative strengths and weaknesses.
",7 Discussion,[0],[0]
"Finally, we reverted to the base model after the end of each text in our experiments, forgetting any text-specific adaptation.",7 Discussion,[0],[0]
This mimics the effect of a participant leaving an experiment that had an unusual distribution of syntactic constructions and reverting to their standard expectations.,7 Discussion,[0],[0]
"In practice, however, humans are able to generalize from prior experience when they begin adapting to a new speaker or text if it is similar in some way to their previous experiences.",7 Discussion,[0],[0]
"For example, the model of Jaech and Ostendorf (2018) adapts to environmental factors, so it could potentially draw on independent experiences with female speakers and with lawyer speech in order to initialize a model of adaptation to a new female lawyer (see also Mikolov and Zweig, 2012; Kleinschmidt, 2018).",7 Discussion,[0],[0]
The psycholinguistic plausibility of these models can be tested in future work.,7 Discussion,[0],[0]
It has been argued that humans rapidly adapt their lexical and syntactic expectations to match the statistics of the current linguistic context.,abstractText,[0],[0]
We provide further support to this claim by showing that the addition of a simple adaptation mechanism to a neural language model improves our predictions of human reading times compared to a non-adaptive model.,abstractText,[0],[0]
We analyze the performance of the model on controlled materials from psycholinguistic experiments and show that it adapts not only to lexical items but also to abstract syntactic structures.,abstractText,[0],[0]
A Neural Model of Adaptation in Reading,title,[0],[0]
"Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 196–205, Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics
We present a novel response generation system that can be trained end to end on large quantities of unstructured Twitter conversations. A neural network architecture is used to address sparsity issues that arise when integrating contextual information into classic statistical models, allowing the system to take into account previous dialog utterances. Our dynamic-context generative models show consistent gains over both context-sensitive and non-context-sensitive Machine Translation and Information Retrieval baselines.",text,[0],[0]
"Until recently, the goal of training open-domain conversational systems that emulate human conversation has seemed elusive.",1 Introduction,[0],[0]
"However, the vast quantities of conversational exchanges now available on social media websites such as Twitter and Reddit raise the prospect of building data-driven models that can begin to communicate conversationally.",1 Introduction,[0],[0]
"The work of Ritter et al. (2011), for example, demonstrates that a response generation system can be constructed from Twitter conversations using statistical machine translation techniques, where a status post by a Twitter user is “translated” into a plausible looking response.
",1 Introduction,[0],[0]
"However, an approach such as that presented in Ritter et al. (2011) does not address the challenge of
*The entirety of this work was conducted while at Microsoft Research.
†Corresponding authors: Alessandro Sordoni (sordonia@iro.umontreal.ca) and Michel Galley (mgalley@microsoft.com).
generating responses that are sensitive to the context of the conversation.",1 Introduction,[0],[0]
"Broadly speaking, context may be linguistic or involve grounding in the physical or virtual world, but we here focus on linguistic context.",1 Introduction,[0],[0]
The ability to take into account previous utterances is key to building dialog systems that can keep conversations active and engaging.,1 Introduction,[0],[0]
Figure 1 illustrates a typical Twitter dialog where the contextual information is crucial: the phrase “good luck” is plainly motivated by the reference to “your game” in the first utterance.,1 Introduction,[0],[0]
"In the MT model, such contextual sensitivity is difficult to capture; moreover, naive injection of context information would entail unmanageable growth of the phrase table at the cost of increased sparsity, and skew towards rarely-seen context pairs.",1 Introduction,[0],[0]
"In most statistical approaches to machine translation, phrase pairs do not share statistical weights regardless of their intrinsic semantic commonality.
",1 Introduction,[0],[0]
We propose to address the challenge of contextsensitive response generation by using continuous representations or embeddings of words and phrases to compactly encode semantic and syntactic similarity.,1 Introduction,[0],[0]
"We argue that embedding-based models af-
196
ford flexibility to model the transitions between consecutive utterances and to capture long-span dependencies in a domain where traditional word and phrase alignment is difficult (Ritter et al., 2011).",1 Introduction,[0],[0]
"To this end, we present two simple, context-sensitive response-generation models utilizing the Recurrent Neural Network Language Model (RLM) architecture of (Mikolov et al., 2010).",1 Introduction,[0],[0]
"These models first encode past information in a hidden continuous representation, which is then decoded by the RLM to promote plausible responses that are simultaneously fluent and contextually relevant.",1 Introduction,[0],[0]
"Unlike typical complex task-oriented multi-modular dialog systems (Young, 2002; Stent and Bangalore, 2014), our architecture is completely data-driven and can easily be trained end-to-end using unstructured data without requiring human annotation, scripting, or automatic parsing.
",1 Introduction,[0],[0]
This paper makes the following contributions.,1 Introduction,[0],[0]
We present a neural network architecture for response generation that is both context-sensitive and datadriven.,1 Introduction,[0],[0]
"As such, it can be trained from end to end on massive amounts of social media data.",1 Introduction,[0],[0]
"To our knowledge, this is the first application of a neural-network model to open-domain response generation, and we believe that the present work will lay groundwork for more complex models to come.",1 Introduction,[0],[0]
We additionally introduce a novel multi-reference extraction technique that shows promise for automated evaluation.,1 Introduction,[0],[0]
"Our work naturally lies in the path opened by Ritter et al. (2011), but we generalize their approach by exploiting information from a larger context.",2 Related Work,[0],[0]
Ritter et al. and our work represent a radical paradigm shift from other work in dialog.,2 Related Work,[0],[0]
"More traditional dialog systems typically tease apart dialog management (Young, 2002) from response generation (Stent and Bangalore, 2014), while our holistic approach can be considered a first attempt to accomplish both tasks jointly.",2 Related Work,[0],[0]
"While there are previous uses of machine learning for response generation (Walker et al., 2003), dialog state tracking (Young et al., 2010), and user modeling (Georgila et al., 2006), many components of typical dialog systems remain hand-coded: in particular, the labels and attributes defining dialog states.",2 Related Work,[0],[0]
"In contrast, the dialog state in our neural network model is completely latent and directly optimized towards end-to-end performance.",2 Related Work,[0],[0]
"In this sense,
we believe the framework of this paper is a significant milestone towards more data-driven and less hand-coded dialog processing.
",2 Related Work,[0],[0]
"Continuous representations of words and phrases estimated by neural network models have been applied on a variety of tasks ranging from Information Retrieval (IR) (Huang et al., 2013; Shen et al., 2014), Online Recommendation (Gao et al., 2014b), Machine Translation (MT) (Auli et al., 2013; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014), and Language Modeling (LM) (Bengio et al., 2003; Collobert and Weston, 2008).",2 Related Work,[0],[0]
"Gao et al. (2014a) successfully use an embedding model to refine the estimation of rare phrase-translation probabilities, which is traditionally affected by sparsity problems.",2 Related Work,[0],[0]
"Robustness to sparsity is a crucial property of our method, as it allows us to capture context information while avoiding unmanageable growth of model parameters.
",2 Related Work,[0],[0]
"Our work extends the Recurrent Neural Network Language Model (RLM) of (Mikolov et al., 2010), which uses continuous representations to estimate a probability function over natural language sentences.",2 Related Work,[0],[0]
"We propose a set of conditional RLMs where contextual information (i.e., past utterances) is encoded in a continuous context vector to help generate the response.",2 Related Work,[0],[0]
Our models differ from most previous work in the way the context vector is constructed.,2 Related Work,[0],[0]
"For example, Mikolov and Zweig (2012) and Auli et al. (2013) use a pre-trained topic model.",2 Related Work,[0],[0]
"In our models, the context vector is learned along with the conditional RLM that generates the response.",2 Related Work,[0],[0]
"Additionally, the learned context encodings do not exclusively capture contentful words.",2 Related Work,[0],[0]
"Indeed, even “stop words” can carry discriminative power in this task; for example, all words in the utterance “how are you?” are commonly characterized as stop words, yet this is a contentful dialog utterance.",2 Related Work,[0],[0]
"We give a brief overview of the Recurrent Language Model (RLM) (Mikolov et al., 2010) architecture that our models extend.",3 Recurrent Language Model,[0],[0]
"A RLM is a generative model of sentences, i.e., given sentence s = s1, . . .",3 Recurrent Language Model,[0],[0]
", sT , it estimates:
p(s) = T∏ t=1 p(st|s1, . . .",3 Recurrent Language Model,[0],[0]
", st−1).",3 Recurrent Language Model,[0],[0]
"(1)
The model architecture is parameterized by three weight matrices, ΘRNN = 〈Win,Wout,Whh〉: an input matrixWin, a recurrent matrixWhh and an output matrix Wout, which are usually initialized randomly.",3 Recurrent Language Model,[0],[0]
The rows of the input matrix Win ∈,3 Recurrent Language Model,[0],[0]
RV×K contain the K-dimensional embeddings for each word in the language vocabulary of size V .,3 Recurrent Language Model,[0],[0]
"Let us denote by st both the vocabulary token and its one-hot representation, i.e., a zero vector of dimensionality V with a 1 corresponding to the index of the st token.",3 Recurrent Language Model,[0],[0]
The embedding for st is then obtained by s>t Win.,3 Recurrent Language Model,[0],[0]
The recurrent matrix Whh ∈ RK×K keeps a history of the subsequence that has already been processed.,3 Recurrent Language Model,[0],[0]
"The output matrix Wout ∈ RK×V projects the hidden state ht into the output layer ot, which has an entry for each word in the vocabulary V .",3 Recurrent Language Model,[0],[0]
This value is used to generate a probability distribution for the next word in the sequence.,3 Recurrent Language Model,[0],[0]
"Specifically, the forward pass proceeds with the following recurrence, for t = 1, . . .",3 Recurrent Language Model,[0],[0]
", T :
ht = σ(s>t Win + h > t−1Whh), ot = h > t",3 Recurrent Language Model,[0],[0]
"Wout (2)
where σ is a non-linear function applied elementwise, in our case the logistic sigmoid.",3 Recurrent Language Model,[0],[0]
"The recurrence is seeded by setting h0 = 0, the zero vector.",3 Recurrent Language Model,[0],[0]
"The probability distribution over the next word given the previous history is obtained by applying the softmax activation function:
P (st = w|s1, . . .",3 Recurrent Language Model,[0],[0]
", st−1) = exp(otw)∑V v=1 exp(otv) .",3 Recurrent Language Model,[0],[0]
"(3)
The RLM is trained to minimize the negative loglikelihood of the training sentence s:
L(s) =",3 Recurrent Language Model,[0],[0]
"− T∑ t=1 logP (st|s1, . . .",3 Recurrent Language Model,[0],[0]
", st−1).",3 Recurrent Language Model,[0],[0]
"(4)
The recurrence is unrolled backwards in time using the back-propagation through time (BPTT) algorithm (Rumelhart et al., 1988), and gradients are accumulated over multiple time-steps.",3 Recurrent Language Model,[0],[0]
"We distinguish three linguistic entities in a conversation between two users A and B: the context1 c,
1In this work, the context is purely linguistic, but future work might integrate further contextual information, e.g., geographical location, time information, or other forms of grounding.
",4 Context-Sensitive Models,[0],[0]
the message m and response r.,4 Context-Sensitive Models,[0],[0]
"The context c represents a sequence of past dialog exchanges of any length; then B emits a message m to which A reacts by formulating its response r (see Figure 1).
",4 Context-Sensitive Models,[0],[0]
"We use three context-based generation models to estimate a generation model of the response r, r = r1, . . .",4 Context-Sensitive Models,[0],[0]
", rT , conditioned on past information c and m:
p(r|c,m) = T∏ t=1 p(rt|r1, . . .",4 Context-Sensitive Models,[0],[0]
", rt−1, c,m).",4 Context-Sensitive Models,[0],[0]
"(5)
These three models differ in the manner in which they compose the context-message pair (c,m).",4 Context-Sensitive Models,[0],[0]
"In our first model, dubbed RLMT, we straightforwardly concatenate each utterance c, m, r into a single sentence s and train the RLM to minimize L(s).",4.1 Tripled Language Model,[0],[0]
"Given c and m, we compute the probability of the response as follows: we perform the forward propagation over the known utterances c andm to obtain a hidden state encoding useful information about previous utterances.",4.1 Tripled Language Model,[0],[0]
"Subsequently, we compute the likelihood of the response from that hidden state.
",4.1 Tripled Language Model,[0],[0]
"An issue with this simple approach is that the concatenated sentence s will be very long on average, especially if the context comprises multiple utterances.",4.1 Tripled Language Model,[0],[0]
"Modelling such long-range dependencies with an RLM is difficult and is still considered an open problem (Pascanu et al., 2013).",4.1 Tripled Language Model,[0],[0]
We will consider RLMT as an additional context-sensitive baseline for the models we present next.,4.1 Tripled Language Model,[0],[0]
The above limitation of RLMT can be addressed by strengthening the context bias.,4.2 Dynamic-Context Generative Model I,[0],[0]
"In our second model (DCGM-I), the context and the message are encoded
DCGM-I DCGM-II
into a fixed-length vector representation the is used by the RLM to decode the response.",4.2 Dynamic-Context Generative Model I,[0],[0]
This is illustrated in Figure 3 (left).,4.2 Dynamic-Context Generative Model I,[0],[0]
"First, we consider c andm as a single sentence and compute a single bag-of-words representation bcm ∈ RV .",4.2 Dynamic-Context Generative Model I,[0],[0]
"Then, bcm is provided as input to a multilayered non-linear forward architecture that produces a fixed-length representation that is used to bias the recurrent state of the decoder RLM.",4.2 Dynamic-Context Generative Model I,[0],[0]
"At training time, both the context encoder and the RLM decoder are learned so as to minimize the negative log-probability of the generated response.
",4.2 Dynamic-Context Generative Model I,[0],[0]
"The parameters of the model are ΘDCGM-I = 〈Win,Whh,Wout, {W `f}L`=1〉, where {W `f}L`=1 are the weights for the L layers of the feed-forward context networks.",4.2 Dynamic-Context Generative Model I,[0],[0]
"The fixed-length context vector kL is obtained by forward propagation of the network:
k1 = b>cmW",4.2 Dynamic-Context Generative Model I,[0],[0]
"1f k` = σ(k>`−1W ` f ) for ` = 2, · · · , L
(6)
",4.2 Dynamic-Context Generative Model I,[0],[0]
The rows of W 1f contain the embeddings of the vocabulary.2 These are different from those employed in the RLM and play a crucial role in promoting the specialization of the context encoder to a distinct task.,4.2 Dynamic-Context Generative Model I,[0],[0]
"The hidden layer of the decoder RLM takes the
2Notice that the first layer of the encoder network is linear.",4.2 Dynamic-Context Generative Model I,[0],[0]
"We found that this helps learning the embedding matrix as it reduces the vanishing gradient effect partially due to stacking of squashing non-linearities (Pascanu et al., 2013).
",4.2 Dynamic-Context Generative Model I,[0],[0]
"following form:
ht = σ(h>t−1Whh + kL + s > t Win) (7a)
",4.2 Dynamic-Context Generative Model I,[0],[0]
ot = h>t,4.2 Dynamic-Context Generative Model I,[0],[0]
"Wout (7b)
p(st+1|s1, . . .",4.2 Dynamic-Context Generative Model I,[0],[0]
", st−1, c,m) = softmax(ot) (7c)",4.2 Dynamic-Context Generative Model I,[0],[0]
This model conditions on the previous utterances via biasing the hidden layer state on the context representation kL.,4.2 Dynamic-Context Generative Model I,[0],[0]
Note that the context representation does not change through time.,4.2 Dynamic-Context Generative Model I,[0],[0]
This is useful because: (a) it forces the context encoder to produce a representation general enough to be useful for generating all words in the response and (b) it helps the RLM decoder to remember context information when generating long responses.,4.2 Dynamic-Context Generative Model I,[0],[0]
"Because DCGM-I does not distinguish between c and m, that model has the propensity to underestimate the strong dependency that holds between m and r.",4.3 Dynamic-Context Generative Model II,[0],[0]
Our third model (DCGM-II) addresses this issue by concatenating the two linear mappings of the bag-ofwords representations bc and bm in the input layer of the feed-forward network representing c and m (see Figure 3 right).,4.3 Dynamic-Context Generative Model II,[0],[0]
"Concatenating continuous representations prior to deep architectures is a common strategy to obtain order-sensitive representations (Bengio et al., 2003; Devlin et al., 2014).
",4.3 Dynamic-Context Generative Model II,[0],[0]
"The forward equations for the context encoder are:
k1 =",4.3 Dynamic-Context Generative Model II,[0],[0]
"[b>c W 1f , b > mW 1 f ],
k` = σ(k>`−1W ` f ) for ` = 2, · · · , L
(8)
where [x, y] denotes the concatenation of x and y vectors.",4.3 Dynamic-Context Generative Model II,[0],[0]
"In DCGM-II, the bias on the recurrent hidden state and the probability distribution over the next token are computed as described in Eq. 7.",4.3 Dynamic-Context Generative Model II,[0],[0]
"For computational efficiency and to alleviate the burden of human evaluators, we restrict the context sequence c to a single sentence.",5.1 Dataset Construction,[0],[0]
"Hence, our dataset is composed of “triples” τ ≡",5.1 Dataset Construction,[0],[0]
"(cτ ,mτ , rτ ) consisting of three sentences.",5.1 Dataset Construction,[0],[0]
"We mined 127M context-messageresponse triples from the Twitter FireHose, covering the 3-month period June 2012 through August 2012.
",5.1 Dataset Construction,[0],[0]
Only those triples where context and response were generated by the same user were extracted.,5.1 Dataset Construction,[0],[0]
"To minimize noise, we selected triples that contained at least one frequent bigram that appeared more than 3 times in the corpus.",5.1 Dataset Construction,[0],[0]
This produced a corpus of 29M Twitter triples.,5.1 Dataset Construction,[0],[0]
"Additionally, we hired crowdsourced raters to evaluate approximately 33K candidate triples.",5.1 Dataset Construction,[0],[0]
Judgments on a 5-point scale were obtained from 3 raters apiece.,5.1 Dataset Construction,[0],[0]
This yielded a set of 4232 triples with a mean score of 4 or better that was then randomly binned into a tuning set of 2118 triples and a test set of 2114 triples3.,5.1 Dataset Construction,[0],[0]
"The mean length of responses in these sets was approximately 11.5 tokens, after cleanup (e.g., stripping of emoticons), including punctuation.",5.1 Dataset Construction,[0],[0]
"We evaluate all systems using BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005), and supplement these results with more targeted human pairwise comparisons in Section 6.3.",5.2 Automatic Evaluation,[0],[0]
A major challenge in using these automated metrics for response generation is that the set of reasonable responses in our task is potentially vast and extremely diverse.,5.2 Automatic Evaluation,[0],[0]
The dataset construction method just described yields only a single reference for each status.,5.2 Automatic Evaluation,[0],[0]
"Accordingly, we extend the set of references using an IR approach to mine potential responses, after which we have human judges rate their appropriateness.",5.2 Automatic Evaluation,[0],[0]
"As we see in Section 6.3, it turns out that by optimizing systems towards BLEU using mined multi-references, BLEU rankings align well with human judgments.",5.2 Automatic Evaluation,[0],[0]
"This lays groundwork for interesting future correlation studies.
",5.2 Automatic Evaluation,[0],[0]
Multi-reference extraction We use the following algorithm to better cover the space of reasonable responses.,5.2 Automatic Evaluation,[0],[0]
Given a test triple τ ≡,5.2 Automatic Evaluation,[0],[0]
"(cτ ,mτ , rτ ), our goal is to mine other responses {rτ̃} that fit the context and message pair (cτ ,mτ ).",5.2 Automatic Evaluation,[0],[0]
"To this end, we first select a set of 15 candidate triples {τ̃} using an IR
3The Twitter ids of the tuning and test sets along with the code for the neural network models may be obtained from http://research.microsoft.com/convo/
system.",5.2 Automatic Evaluation,[0],[0]
The IR system is calibrated in order to select candidate triples τ̃ for which both the message mτ̃ and the response rτ̃ are similar to the original message mτ and response rτ .,5.2 Automatic Evaluation,[0],[0]
"Formally, the score of a candidate triple is:
s(τ̃ , τ) =",5.2 Automatic Evaluation,[0],[0]
"d(mτ̃ ,mτ ) (αd(rτ̃ , rτ )+(1−α) ), (9)
where d is the bag-of-words BM25 similarity function (Robertson et al., 1995), α controls the impact of the similarity between the responses and is a smoothing factor that avoids zero scores for candidate responses that do not share any words with the reference response.",5.2 Automatic Evaluation,[0],[0]
We found that this simple formula provided references that were both diverse and plausible.,5.2 Automatic Evaluation,[0],[0]
"Given a set of candidate triples {τ̃}, human evaluators are asked to rate the quality of the response within the new triples {(cτ ,mτ , rτ̃ )}.",5.2 Automatic Evaluation,[0],[0]
"After human evaluation, we retain the references for which the score is 4 or better on a 5 point scale, resulting in 3.58 references per example on average (Table 1).",5.2 Automatic Evaluation,[0],[0]
The average lengths for the responses in the multi-reference tuning and test sets are 8.75 and 8.13 tokens respectively.,5.2 Automatic Evaluation,[0],[0]
"The response generation systems evaluated in this paper are parameterized as log-linear models in a framework typical of statistical machine translation (Och and Ney, 2004).",5.3 Feature Sets,[0],[0]
"These log-linear models comprise the following feature sets:
MT MT features are derived from a large response generation system built along the lines of Ritter et al. (2011), which is based on a phrase-based MT decoder similar to Moses (Koehn et al., 2007).",5.3 Feature Sets,[0],[0]
"Our MT feature set includes the following features that are common in Moses: forward and backward maximum likelihood “translation” probabilities, word and phrase penalties, linear distortion, and a modified Kneser-Ney language model (Kneser and Ney, 1995) trained on Twitter responses.",5.3 Feature Sets,[0],[0]
"For the translation probabilities, we built a very large phrase table of 160.7 million entries by first filtering out Twitterisms (e.g., long sequences of vowels, hashtags), and then selecting candidate phrase pairs using Fisher’s exact test (Ritter et al., 2011).",5.3 Feature Sets,[0],[0]
"We also included MT decoder features specifically motivated by the response generation task: Jaccard distance between source and
target phrase, Fisher’s exact probability, and a score relating the lengths of source and target phrases.
",5.3 Feature Sets,[0],[0]
"IR We also use an IR feature built from an index of triples, whose implementation roughly matches the IRstatus approach described in Ritter et al. (2011): For a test triple τ , we choose rτ̃",5.3 Feature Sets,[0],[0]
as the candidate response iff,5.3 Feature Sets,[0],[0]
"τ̃ = arg maxτ̃ d(mτ ,mτ̃ ).
",5.3 Feature Sets,[0],[0]
CMM Neither MT nor IR traditionally take into account contextual information.,5.3 Feature Sets,[0],[0]
"Therefore, we take into consideration context and message matches (CMM), i.e., exact matches between c, m and r.",5.3 Feature Sets,[0],[0]
We define 8 features as the [1-4]-gram matches between c and the candidate reply r and the [1-4]-gram matches between m and the candidate reply r.,5.3 Feature Sets,[0],[0]
"These exact matches help capture and promote contextual information in the replies.
",5.3 Feature Sets,[0],[0]
"RLMT, DCGM-I, DCGM-II We consider the RLM trained on the concatenated triples, denoted as RLMT (Section 4.1), to be a context-sensitive RLM baseline.",5.3 Feature Sets,[0],[0]
Each neural network model contributes an additional feature corresponding to the likelihood of the candidate response given context and message.,5.3 Feature Sets,[0],[0]
The proposed models are trained on a 4M subset of the triple data.,5.4 Model Training,[0],[0]
The vocabulary consists of the most frequent V = 50K words.,5.4 Model Training,[0],[0]
"In order to speed up training, we use the Noise-Contrastive Estimation (NCE) loss, which avoids repeated summations over V by approximating the probability of the target word (Gutmann and Hyvärinen, 2010).",5.4 Model Training,[0],[0]
"Parameter optimization is done using Adagrad (Duchi et al., 2011) with a mini-batch size of 100 and a learning rate α = 0.1, which we found to work well on held-out data.",5.4 Model Training,[0],[0]
"In order to stabilize learning, we clip the gradients to a fixed range [−10, 10], as suggested in Mikolov et al. (2010).",5.4 Model Training,[0],[0]
"All the parameters of the neural models are sampled from a normal distribution N (0, 0.01) while the recurrent weight Whh is initialized as a
random orthogonal matrix and scaled by 0.01.",5.4 Model Training,[0],[0]
"To prevent over-fitting, we evaluate performance on a held-out set during training and stop when the objective increases.",5.4 Model Training,[0],[0]
"The size of the RLM hidden layer is set to K = 512, where the context encoder is a 512, 256, 512 multilayer network.",5.4 Model Training,[0],[0]
The bottleneck in the middle compresses context information that leads to similar responses and thus achieves better generalization.,5.4 Model Training,[0],[0]
The last layer embeds the context vector into the hidden space of the decoder RLM.,5.4 Model Training,[0],[0]
We evaluate the proposed models by rescoring the n-best candidate responses obtained using the MT phrase-based decoder and the IR system.,5.5 Rescoring Setup,[0],[0]
"In contrast to MT, the candidate responses provided by IR have been created by humans and are less affected by fluency issues.",5.5 Rescoring Setup,[0],[0]
The different n-best lists will provide a comprehensive testbed for our experiments.,5.5 Rescoring Setup,[0],[0]
"First, we augment the n-best list of the tuning set with the scores of the model of interest.",5.5 Rescoring Setup,[0],[0]
"Then, we run an iteration of MERT (Och, 2003) to estimate the log-linear weights of the new features.",5.5 Rescoring Setup,[0],[0]
"At test time, we rescore the test n-best list with the new weights.",5.5 Rescoring Setup,[0],[0]
Table 2 shows the expected upper and lower bounds for this task as suggested by BLEU scores for human responses and a random response baseline.,6.1 Lower and Upper Bounds,[0],[0]
The RANDOM system comprises responses randomly extracted from the triples corpus.,6.1 Lower and Upper Bounds,[0],[0]
"HUMAN is computed by choosing one reference amongst the multi-reference set for each context-status pair.4 Although the scores are lower than those usually reported in SMT tasks, the ranking of the three systems is unambiguous.",6.1 Lower and Upper Bounds,[0],[0]
"The results of automatic evaluation using BLEU and METEOR are presented in Table 3, where some broad patterns emerge.",6.2 BLEU and METEOR,[0],[0]
"First, both metrics indicate that a phrase-based MT decoder outperforms a purely IR approach.",6.2 BLEU and METEOR,[0],[0]
"Second, adding CMM features
4For the human score, we compute corpus-level BLEU with a sampling scheme that randomly leaves out one reference - the human sentence to score - for each reference set.",6.2 BLEU and METEOR,[0],[0]
"This sampling scheme (repeated with 100 trials) is also applied for the MT and RANDOM system so as to make BLEU scores comparable.
to the baseline systems helps.",6.2 BLEU and METEOR,[0],[0]
"Third, the neural network models contribute measurably to improvement: RLMT and DCGM models outperform baselines, and DCGM models provide more consistent gains than RLMT.
MT vs. IR BLEU and METEOR scores indicate that the phrase-based MT decoder outperforms a purely IR approach, despite the fact that IR proposes fluent human generated responses.",6.2 BLEU and METEOR,[0],[0]
This may be because the IR model only loosely captures important patterns between message and response: It ranks candidate responses solely by the similarity of their message with the message of the test triple (§5.3).,6.2 BLEU and METEOR,[0],[0]
"As a result, the top ranked response is likely to drift from the purpose of the original conversation.",6.2 BLEU and METEOR,[0],[0]
"The MT approach, by contrast, more directly models statistical patterns between message and response.
",6.2 BLEU and METEOR,[0],[0]
"CMM MT+CMM, totaling 17 features (9 from MT + 8 CMM), improves 0.38 BLEU points, a 9.5% relative improvement, over the baseline MT model.",6.2 BLEU and METEOR,[0],[0]
"IR+CMM, with 10 features (IR + word penalty + 8 CMM), benefits even more, attaining 1.8 BLEU points and 1.5 METEOR points over the IR baseline.",6.2 BLEU and METEOR,[0],[0]
Figure 4 (a) and (b) plots the magnitude of the learned CMM feature weights for MT+CMM and IR+CMM.,6.2 BLEU and METEOR,[0],[0]
CMM features help in both these hypothesis spaces and especially on the IR n-best list.,6.2 BLEU and METEOR,[0],[0]
"Figure 4 (b) supports the hypothesis formulated in the previous paragraph: Since IR solely captures intermessage similarities, the matches between message and response are important, while context matches help in providing additional gains.",6.2 BLEU and METEOR,[0],[0]
"The phrase-based statistical patterns captured by the MT system do a
good job in explaining away 1-gram and 2-gram message matches (Figure 4 (a)) and the performance gain mainly comes from context matches.",6.2 BLEU and METEOR,[0],[0]
"On the other hand, we observe that 4-gram matches may be important in selecting appropriate responses.",6.2 BLEU and METEOR,[0],[0]
"Inspection of the tuning set reveals instances where responses contain long subsequences of their corresponding messages, e.g., m = “good night best friend, I love you”, r =",6.2 BLEU and METEOR,[0],[0]
"“I love you too, good night best friend”.",6.2 BLEU and METEOR,[0],[0]
"Although infrequent, such higher-order n-gram matches, when they occur, may provide a more robust signal of the quality of the response than 1- and 2-gram matches, given the highly conversational nature of our dataset.
RLMT and DCGM Both RLMT and DCGM models outperform their respective MT and IR baselines.",6.2 BLEU and METEOR,[0],[0]
"Both models also exhibit similar performance and show improvements over the MT+CMM models, albeit using a lower dimensional feature space.",6.2 BLEU and METEOR,[0],[0]
"We believe that their similar performance is due to the limited diversity of MT n-best list together with gains in fluency stemming from the strong language
model provided by the RLM.",6.2 BLEU and METEOR,[0],[0]
"In the case of IR models, on the other hand, there is more headroom for improvement and fluency is already guaranteed.",6.2 BLEU and METEOR,[0],[0]
Any gains must come from context and message matches.,6.2 BLEU and METEOR,[0],[0]
"Hence, RLMT underperforms with respect to both DCGM and IR+CMM.",6.2 BLEU and METEOR,[0],[0]
"The DCGM models appear to have better capacity to retain contextual information and thus achieve similar performance to IR+CMM despite their lack of exact n-gram match information.
",6.2 BLEU and METEOR,[0],[0]
"In the present experimental setting, no striking performance difference can be observed between the two versions of the DCGM architecture.",6.2 BLEU and METEOR,[0],[0]
"If multiple sequences were used as context, we expect that the DCGM-II model would likely benefit more owing to the separate encoding of message and context.
",6.2 BLEU and METEOR,[0],[0]
DCGM+CMM,6.2 BLEU and METEOR,[0],[0]
We also investigated whether mixing exact CMM n-gram overlap with semantic information encoded by the DCGM models can bring additional gains.,6.2 BLEU and METEOR,[0],[0]
DCGM-{I-II}+CMM systems each totaling 10 features show increases of up to 0.48 BLEU points over MT+CMM and up to 0.88 BLEU over the model based on Ritter et al. (2011).,6.2 BLEU and METEOR,[0],[0]
METEOR improvements similarly align with BLEU improvements both for MT and IR lists.,6.2 BLEU and METEOR,[0],[0]
"We take this as evidence that CMM exact matches and DCGM semantic matches interact positively, a finding that comports with Gao et al. (2014a), who show that semantic relationships mined through phrase embeddings correlate positively with classic co-occurrencebased estimations.",6.2 BLEU and METEOR,[0],[0]
"Analysis of CMM feature weights in Figure 4 (c) and (d) suggests that 1-gram matches are explained away by the DCGM model, but that higher order matches are important.",6.2 BLEU and METEOR,[0],[0]
"It appears that DCGM models might be improved by preserving
word-order information in context and message encodings.",6.2 BLEU and METEOR,[0],[0]
Human evaluation was conducted using crowdsourced annotators.,6.3 Human Evaluation,[0],[0]
Annotators were asked to compare the quality of system output responses pairwise (“Which is better?”) in relation to the context and message strings in the 2114 item test set.,6.3 Human Evaluation,[0],[0]
"Identical strings were held out, so that the annotators only saw those outputs that differed.",6.3 Human Evaluation,[0],[0]
"Paired responses were presented in random order to the annotators, and each pair of responses was judged by 5 annotators.
",6.3 Human Evaluation,[0],[0]
"Table 4 summarizes the results of human evaluation, giving the difference in mean scores (pairwise preference margin) between systems and 95% confidence intervals generated using Welch’s t-test.",6.3 Human Evaluation,[0],[0]
Identical strings not shown to raters are incorporated with an automatically assigned score of 0.5.,6.3 Human Evaluation,[0],[0]
"The pattern in these results is clear and consistent: context-sensitive systems (+CMM) outperform non-context-sensitive systems, with preference gains as high as approximately 5.3% in the case of DCGM-II+CMM versus IR, and about 3.1% in the case of DCGM-II+CMM versus MT.",6.3 Human Evaluation,[0],[0]
"Similarly, context-sensitive DCGM systems outperform non-DCGM context-sensitive systems by 1.5% (MT) and 2.3% (IR).",6.3 Human Evaluation,[0],[0]
These results are consistent with the automated BLEU rankings and confirm that our best performing DCGM models outperform both raw baseline and the context-sensitive baseline using CMM features.,6.3 Human Evaluation,[0],[0]
"Table 5 provides examples of responses generated on the tuning corpus by the MT-based DCGM-II+CMM system, our best system in terms of both BLEU and human evaluation.",6.4 Discussion,[0],[0]
Responses from this system are on average shorter (8.95 tokens) than the original human responses in the tuning set (11.5 tokens).,6.4 Discussion,[0],[0]
"Overall, the outputs tend to be generic or commonplace, but are often reasonably plausible in the context as in examples 1-3, especially where context and message contain common conversational elements.",6.4 Discussion,[0],[0]
Example 2 illustrates the impact of context-sensitivity: the word “book” in the response is not found in the message.,6.4 Discussion,[0],[0]
"Nonetheless, longer generated responses are apt to degrade both syntactically and in terms of content.",6.4 Discussion,[0],[0]
"We notice that longer responses are likely to present
information that conflicts either internally within the response itself, or is at odds with the context, as in examples 4-5.",6.4 Discussion,[0],[0]
"This is not unsurprising, since our model lacks mechanisms both for reflecting agent intent in the response and for maintaining consistency with respect to sentiment polarity.",6.4 Discussion,[0],[0]
"Longer context and message components may also result in responses that wander off-topic or lapse into incoherence as in 6-8, especially when relatively low frequency unigrams (“bass”, “threat”) are echoed in the response.",6.4 Discussion,[0],[0]
"In general, we expect that larger datasets and incorporation of more extensive contexts into the model will help yield more coherent results in these cases.",6.4 Discussion,[0],[0]
"Consistent representation of agent intent is outside the scope of this work, but will likely remain a significant challenge.",6.4 Discussion,[0],[0]
"We have formulated a neural network architecture for data-driven response generation trained from social media conversations, in which generation of responses is conditioned on past dialog utterances that provide contextual information.",7 Conclusion,[0],[0]
We have proposed a novel multi-reference extraction technique allowing for robust automated evaluation using standard SMT metrics such as BLEU and METEOR.,7 Conclusion,[0],[0]
"Our context-sensitive models consistently outperform both context-independent and context-sensitive baselines by up to 11% relative improvement in
BLEU in the MT setting and 24% in the IR setting, albeit using a minimal number of features.",7 Conclusion,[0],[0]
"As our models are completely data-driven and self-contained, they hold the potential to improve fluency and contextual relevance in other types of dialog systems.
",7 Conclusion,[0],[0]
Our work suggests several directions for future research.,7 Conclusion,[0],[0]
We anticipate that there is much room for improvement if we employ more complex neural network models that take into account word order within the message and context utterances.,7 Conclusion,[0],[0]
Direct generation from neural network models is an interesting and potentially promising next step.,7 Conclusion,[0],[0]
Future progress in this area will also greatly benefit from thorough study of automated evaluation metrics.,7 Conclusion,[0],[0]
"We thank Alan Ritter, Ray Mooney, Chris Quirk, Lucy Vanderwende, Susan Hendrich and Mouni Reddy for helpful discussions, as well as the three anonymous reviewers for their comments.",Acknowledgments,[0],[0]
We present a novel response generation system that can be trained end to end on large quantities of unstructured Twitter conversations.,abstractText,[0],[0]
"A neural network architecture is used to address sparsity issues that arise when integrating contextual information into classic statistical models, allowing the system to take into account previous dialog utterances.",abstractText,[0],[0]
Our dynamic-context generative models show consistent gains over both context-sensitive and non-context-sensitive Machine Translation and Information Retrieval baselines.,abstractText,[0],[0]
A Neural Network Approach to Context-Sensitive Generation of Conversational Responses,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 339–348, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
"Dependency parsing is an important task for Natural Language Processing (NLP) with application to text classification (Özgür and Güngör, 2010), relation extraction (Bunescu and Mooney, 2005), question answering (Cui et al., 2005), statistical machine translation (Xu et al., 2009), and sentiment analysis (Socher et al., 2013).",1 Introduction,[0],[0]
"A mature parser normally requires a large treebank for training, yet such resources are rarely available and are costly to build.",1 Introduction,[0],[0]
"Ideally, we would be able to construct a high quality parser with less training data, thereby enabling accurate parsing for lowresource languages.
",1 Introduction,[0],[0]
"In this paper we formalize the dependency parsing task for a low-resource language as a domain adaptation task, in which a target resource-poor language treebank is treated as in-domain, while a much larger treebank in a high-resource language forms the out-of-domain data.",1 Introduction,[0],[0]
"In this way, we can apply well-understood domain adaptation techniques to the dependency parsing task.",1 Introduction,[0],[0]
"However, a crucial requirement for domain adaptation is that the in-domain and out-of-domain data have
compatible representations.",1 Introduction,[0],[0]
"In applying our approach to data from several languages, we must learn such a cross-lingual representation.",1 Introduction,[0],[0]
Here we frame this representation learning as part of a neural network training.,1 Introduction,[0],[0]
The underlying hypothesis for the joint learning is that there are some shared-structures across languages that we can exploit.,1 Introduction,[0],[0]
"This hypothesis is motivated by the excellent results of the cross-lingual application of unlexicalised parsing (McDonald et al., 2011), whereby a delexicalized parser constructed on one language is applied directly to another language.
",1 Introduction,[0],[0]
Our approach works by jointly training a neural network dependency parser to model the syntax in both a source and target language.,1 Introduction,[0],[0]
"Many of the parameters of the source and target language parsers are shared, except for a small handful of language-specific parameters.",1 Introduction,[0],[0]
"In this way, the information can flow back and forth between languages, allowing for the learning of a compatible cross-lingual syntactic representation, while also allowing the parsers to mutually correct one another’s errors.",1 Introduction,[0],[0]
"We include some language-specific components, in order to better model the lexicon of each language and allow learning of the syntactic idiosyncrasies of each language.",1 Introduction,[0],[0]
"Our experiments show that this outperforms a purely supervised setting, on both small and large data conditions, with a gain as high as 10% for small training sets.",1 Introduction,[0],[0]
"Our proposed joint training method also out-performs the conventional cascade approach where the parameters between source and target languages are related together through a regularization term (Duong et al., 2015).
",1 Introduction,[0],[0]
"Our model is flexible, allowing easy incorporation of peripheral information.",1 Introduction,[0],[0]
"For example, assuming the presence of a small bilingual dictionary is befitting of a low-resource setting, as this is prototypically one of the first artifacts generated by field linguists.",1 Introduction,[0],[0]
"We incorporate a bilingual dictionary as a set of soft constraints on the
339
model, such that it learns similar representations for each word and its translation(s).",1 Introduction,[0],[0]
"For example, the representation of house in English should be close to haus in German.",1 Introduction,[0],[0]
"We empirically show that adding a bilingual dictionary improves parser performance, particularly when target data is limited.
",1 Introduction,[0],[0]
The final contribution of the paper concerns the learned word embeddings.,1 Introduction,[0],[0]
"We demonstrate that these encode meaningful syntactic phenomena, both in terms of the observable clusters and through a verb classification task.",1 Introduction,[0],[0]
The code for this paper is published as an open source project.1,1 Introduction,[0],[0]
"This work is motivated by the idea of delexicalized parsing, in which a parser is built without any lexical features and trained on a treebank for a resource-rich source language (Zeman et al., 2008).",2 Related Work,[0],[0]
It is then applied directly to parse sentences in the target resource-poor languages.,2 Related Work,[0],[0]
"Delexicalized parsing relies on the fact that identical part-ofspeech (POS) inventories are highly informative of dependency relations, and that there exists shared dependency structures across languages.
",2 Related Work,[0],[0]
Building a dependency parser for a resourcepoor language usually starts with the delexicalized parser and then uses other resources to refine the model.,2 Related Work,[0],[0]
McDonald et al. (2011) and Ma and Xia (2014) exploited parallel data as the bridge to transfer constraints from the source resourcerich language to the target resource-poor languages. Täckström,2 Related Work,[0],[0]
et al. (2012) also used parallel data to induce cross-lingual word clusters which added as features for their delexicalized parser.,2 Related Work,[0],[0]
Durrett et al. (2012) constructed the set of language-independent features and used a bilingual dictionary as the bridge to transfer these features from source to target language. Täckström,2 Related Work,[0],[0]
"et al. (2013) additionally used high-level linguistic features extracted from the World Atlas of Language Structures (WALS) (Dryer and Haspelmath, 2013).
",2 Related Work,[0],[0]
"For low-resource languages, no large parallel corpus is available.",2 Related Work,[0],[0]
"Some linguists are dependency-annotating small amounts of field data, e.g. for Karuk, a nearly-extinct language of Northwest California (Garrett et al., 2013).",2 Related Work,[0],[0]
"Accordingly, we adopt a different resource require-
1http://github.com/longdt219/ universal_dependency_parser
ment: a small treebank in the target low-resource language.
",2 Related Work,[0],[0]
"Domain adaptation or joint-training is a different branch of research, and falls outside the scope of this paper.",2 Related Work,[0],[0]
"Nevertheless, we would like to contrast our work with Senna (Collobert et al., 2011), a neural network framework to perform a variety of NLP tasks such as part-of-speech (POS) tagging, named entity recognition (NER), chunking, and so forth.",2 Related Work,[0],[0]
Both approaches exploit common linguistic properties of the data through joint learning.,2 Related Work,[0],[0]
"However, Collobert et al’s goal is to find a single input representation that can work well for many tasks.",2 Related Work,[0],[0]
Our goal is different: we allow the joint-training inputs to be different but constrain the parameter weights in the upper layer to be identical.,2 Related Work,[0],[0]
"Consequently, our method applies to the task where inputs are different, possibly from different languages or domains.",2 Related Work,[0],[0]
Their method applies for different tasks in the same language/domain where the inputs are fairly similar.,2 Related Work,[0],[0]
This section describes the monolingual neural network dependency parser structure of Chen and Manning (2014).,2.1 Supervised Neural Network Parser,[0],[0]
"This parser achieves excellent performance, and has a highly flexible formulation allowing auxilliary inputs.",2.1 Supervised Neural Network Parser,[0],[0]
"The model is based on a transition-based dependency parser (Nivre, 2006) formulated as a neural-network classifier to decide which transition to apply to each parsing state configuration.2 That is, for each configuration, the selected list of words, POS tags and labels from the Stack, Queue and Arcs are extracted.",2.1 Supervised Neural Network Parser,[0],[0]
"Each word, POS and label is mapped into a lowdimension vector representation using an embedding matrix, which is then fed into a two-layer neural network classifier to predict the next parsing action.",2.1 Supervised Neural Network Parser,[0],[0]
"The set of parameters for the model is E = {Eword, Epos, Earc} for the embedding layer, W1 for the fully connected cubic hidden layer and W2 for the softmax output layer.",2.1 Supervised Neural Network Parser,[0],[0]
"The model prediction function is
P (Y |X = ~x,W1,W2, E) = softmax ( W2 × cube(W1",2.1 Supervised Neural Network Parser,[0],[0]
"× Φ [~x,E]) )",2.1 Supervised Neural Network Parser,[0],[0]
"(1)
2Our approach is focused on a technique for transfer learning which can be more widely applied to other types of dependency parser (and models, generally) regardless of whether they are transition-based or graph-based.
where cube is a non-linear activation function, Φ is the embedding function that returns a vector representation of parsing state x using an embedding matrix E. We refer the reader to Chen and Manning (2014) for a more detailed description.",2.1 Supervised Neural Network Parser,[0],[0]
"We assume a small treebank in a target resourcepoor language, as well as a larger treebank in the source language.",3 A Joint Interlingual Model,[0],[0]
"Our objective is to learn a model of both languages, subject to the constraint that both models are similar overall, while allowing for some limited language variability.",3 A Joint Interlingual Model,[0],[0]
"Instead of just training two different parsers on source and then on target, we train them jointly, in order to learn an interlingual parser.",3 A Joint Interlingual Model,[0],[0]
"This allows the method to take maximum advantage of the limited treebank data available, resulting in highly accurate predicted parses.
",3 A Joint Interlingual Model,[0],[0]
"Training a monolingual parser as described in section 2.1 requires optimizing the simple cross-entropy learning objective,",3 A Joint Interlingual Model,[0],[0]
"L = −∑|D|i=1 logP (Y = ~y(i)|X = ~x(i)), where P (Y |X) is given by equation 1 and D = {~x(i), ~y(i)}ni=1 is the training data.",3 A Joint Interlingual Model,[0],[0]
"Joint training of a parser over the source and target languages can be achieved by simply adding two such cross-entropy objectives, i.e.,
Ljoint = − |Ds|∑ i=1",3 A Joint Interlingual Model,[0],[0]
logP (Ys = ~y(i)s,3 A Joint Interlingual Model,[0],[0]
"|Xs = ~x(i)s )
",3 A Joint Interlingual Model,[0],[0]
− |Dt|∑ i=1,3 A Joint Interlingual Model,[0],[0]
"logP (Yt = ~y (i) t |Xt = ~x(i)t ) , (2)
where the training data, D = Ds ∪Dt, comprises data in both the source and target language.",3 A Joint Interlingual Model,[0],[0]
However training the model according to equation 2 will result in two independent parsers.,3 A Joint Interlingual Model,[0],[0]
"To enforce similarity between the two parsers, we adopt parameter sharing: the neural network parameters, W1 and W2, are identical in both parsers.",3 A Joint Interlingual Model,[0],[0]
"Thereby
P (Yα|Xα = ~x) = P (Y |X = ~x,W1,W2, Eα) ,
where the subscript α ∈ {s, t} denotes the source or target language.",3 A Joint Interlingual Model,[0],[0]
"We allow the embedding matrix Eα to differ in order to accommodate language-specific features, in terms of the representations of lexical types, Ewords , part-of-speech, E
pos s and dependency arc labels Earcs .",3 A Joint Interlingual Model,[0],[0]
"This reflects
the fact that different languages have different lexicon, parts-of-speech often exhibit different roles, and dependency edges serve different functions, e.g. in Korean a static verb can serve as an adjective (Kim, 2001).",3 A Joint Interlingual Model,[0],[0]
"During training, the languagespecific errors are back propagated through different branches according to the language, guiding learning towards an interlingual representation that informs parsing decisions in both languages.",3 A Joint Interlingual Model,[0],[0]
"The set of parameters for the model is W1,W2, Es, Et where Es, Et are the embedding matrices for the source and target languages.
",3 A Joint Interlingual Model,[0],[0]
"Generally speaking, we can understand the model as building the universal dependency parser that parses the universal language.",3 A Joint Interlingual Model,[0],[0]
"Specifically, the model is the combination of two parts: the universal part (W1,W2) that is shared between the languages, and the conversion part (Es, Et) that maps a language-specific representation into the universal language.",3 A Joint Interlingual Model,[0],[0]
"Naturally, we could stack several non-linear layers in the conversion components such that the model can better transform the input into the universal representation; we leave this exploration for future work.",3 A Joint Interlingual Model,[0],[0]
"Currently, our cross-lingual word embeddings are meaningful for a pair of source and target languages.",3 A Joint Interlingual Model,[0],[0]
"However, our model can easily be used for joint training over k > 2 languages.",3 A Joint Interlingual Model,[0],[0]
"We also leave this avenue of enquiry for future work
One concern from equation 2 is that when the source language treebank",3 A Joint Interlingual Model,[0],[0]
"Ds is much bigger than the target language treebank Dt, it is likely to dominate, and consequently, learning will mainly focus on optimizing the source language parser.",3 A Joint Interlingual Model,[0],[0]
"We adjust for this disparity by balancing the two datasets,Ds andDt, during training.",3 A Joint Interlingual Model,[0],[0]
"When selecting mini-batches for online gradient updates, we select an equal number of classification instances from the source and target languages.",3 A Joint Interlingual Model,[0],[0]
"Thus, for each step |Ds| = |Dt|, effectively reweighting the cross-entropy components in (2) to ensure parity between the languages.
",3 A Joint Interlingual Model,[0],[0]
"The other concern is over-fitting, especially when we only have a small treebank in the target language.",3 A Joint Interlingual Model,[0],[0]
"As suggested by Chen and Manning (2014), we apply drop-out, a form of regularization for both source and target language.",3 A Joint Interlingual Model,[0],[0]
"That is, we randomly drop some of the activation units from both hidden layer and input layer.",3 A Joint Interlingual Model,[0],[0]
"Following Srivastava et al. (2014), we randomly dropout 20% of the input layer and 50% of the hid-
den layer.",3 A Joint Interlingual Model,[0],[0]
"Empirically, we observe a substantial improvement applying dropout to the model over MLE or l2 regularization.",3 A Joint Interlingual Model,[0],[0]
"Our model is flexible, enabling us to freely add additional components.",3.1 Incorporating a Dictionary,[0],[0]
"In this section, we assume the presence of a bilingual dictionary between the source and target language.",3.1 Incorporating a Dictionary,[0],[0]
"We seek to incorporate this dictionary as a part of model learning, to encode the intuition that if two lexical items are translations of one another, the parser should treat them similarly.3 Recall that the mapping layer is the combination of word, pos and arc embeddings, i.e., Eα = {Ewordα , Eposα , Earcα }.",3.1 Incorporating a Dictionary,[0],[0]
"We can easily add bilingual dictionary constraints to the model in the form of regularization to minimize the l2 distance between word representations, i.e.,∑
〈i,j〉∈D ‖Eword(i)s",3.1 Incorporating a Dictionary,[0],[0]
"− Eword(j)t ‖2F , where D comprises translation pairs, word(i) and word(j).
",3.1 Incorporating a Dictionary,[0],[0]
"When the languages share the same POS tagset and arc set,4 we can also add further constraints such as their language-specific embeddings be close together.",3.1 Incorporating a Dictionary,[0],[0]
"This results a regularised training objective,
Ldict = Ljoint−λ",3.1 Incorporating a Dictionary,[0],[0]
"( ∑ 〈i,j〉∈D ‖Eword(i)s −Eword(j)t ‖2F
+ ‖Eposs − Epost ‖2F + ‖Earcs",3.1 Incorporating a Dictionary,[0],[0]
"− Earct ‖2F ) , (3)
where λ ∈",3.1 Incorporating a Dictionary,[0],[0]
"[0,∞] controls to what degree we bind these words or pos tags or arc labels together, with high λ tying the parameters and small λ allowing independent learning.",3.1 Incorporating a Dictionary,[0],[0]
We expect the best value of λ to fall somewhere between these extremes.,3.1 Incorporating a Dictionary,[0],[0]
"Finally, we use a mini-batch size of 1000 instance pairs and adaptive learning rate trainer, adagrad (Duchi et al., 2011) to build our two separate models corresponding to equations 2 and 3.",3.1 Incorporating a Dictionary,[0],[0]
"In this section, we compare our joint training approach with baseline methods of supervised learning in the target language, and cascaded learning of source and target parsers.
",4 Experiments,[0],[0]
"3However, this is not always the case.",4 Experiments,[0],[0]
"For example, modal or auxiliary verbs in English often have no translations in different languages or map to words with different syntactic functions.
",4 Experiments,[0],[0]
4As was the case for our experiments.,4 Experiments,[0],[0]
"We experiment with the Universal Dependency Treebank (UDT) V1.0 (Nivre et al., 2015), simulating low resource settings.5",4.1 Dataset,[0],[0]
This treebank has many desirable properties for our model: the dependency types (arc labels set) and coarse POS tagset are the same across languages.,4.1 Dataset,[0],[0]
This removes the need for mapping the source and target language tagsets to a common tagset.,4.1 Dataset,[0],[0]
"Moreover, the dependency types are also common across languages allowing evaluation of the labelled attachment score (LAS).",4.1 Dataset,[0],[0]
"The treebank covers 10 languages,6 with some languages very highly resourced—Czech, French and Spanish have 400k tokens—and only modest amounts of data for other languages—Hungarian and Irish have only around 25k tokens.",4.1 Dataset,[0],[0]
"Cross-lingual models assume English as the source language, for which we have a large treebank, and only a small treebank of 3k tokens exists in each target language, simulated by subsampling the corpus.",4.1 Dataset,[0],[0]
"We compare our approach to a baseline interlingual model based on the same parsing algorithm as presented in section 2.1, but with cascaded training (Duong et al., 2015).",4.2 Baseline Cascade Model,[0],[0]
"This works by first learning the source language parser, and then training the target language parser using a regularization term to minimise the distance between the parameters of the target parser and the source parser (which is fixed).",4.2 Baseline Cascade Model,[0],[0]
"In this way, some structural information from the source parser can be used in the target parser, however it is likely that the representation will be overly biased towards the source language and consequently may not prove as useful for modelling the target.",4.2 Baseline Cascade Model,[0],[0]
"While the Epos and Earc are randomly initialized, we initialize both the source and target language word embeddings Ewords , E word t of our neural network models with pre-trained embeddings.",4.3 Monolingual Word Embeddings,[0],[0]
"This is an advantage since we can incorporate the monolingual data which is often available, even for
5Evaluating on truly resource-poor languages would be preferable to simulation.",4.3 Monolingual Word Embeddings,[0],[0]
"However for ease of training and evaluation, which requires a small treebank in the target language, we simulate the low-resource setting using a small part of the UDT.
6Czech (cs), English (en), Finnish (fi), French (fr), German (de), Hungarian (hu), Irish (ga), Italian (it), Spanish (es), Swedish (sv).
resource-poor languages.",4.3 Monolingual Word Embeddings,[0],[0]
"We collect monolingual data for each language from the Machine Translation Workshop (WMT) data,7",4.3 Monolingual Word Embeddings,[0],[0]
"Europarl (Koehn, 2005) and EU Bookshop Corpus (Skadiņš et al., 2014).",4.3 Monolingual Word Embeddings,[0],[0]
"The size of monolingual data also varies significantly, with as much as 400 million tokens for English and German, and as few as 4 million tokens for Irish.",4.3 Monolingual Word Embeddings,[0],[0]
"We use the skip-gram model (Mikolov et al., 2013b) to induce 50-dimensional word embeddings.",4.3 Monolingual Word Embeddings,[0],[0]
"For the extended model as described in section 3.1, we also need a bilingual dictionary.",4.4 Bilingual Dictionary,[0],[0]
"We extract dictionaries from PanLex (Kamholz et al., 2014) which currently covers around 1300 language varieties and about 12 million expressions.",4.4 Bilingual Dictionary,[0],[0]
This dataset is growing and aims at covering all languages in the world and up to 350 million expressions.,4.4 Bilingual Dictionary,[0],[0]
"The translations in PanLex come from various sources such as glossaries, dictionaries, automatic inference from other languages, etc.",4.4 Bilingual Dictionary,[0],[0]
"Naturally, the bilingual dictionary size varies greatly among resource-poor and resource-rich languages.",4.4 Bilingual Dictionary,[0],[0]
Joint training with a dictionary (see equation 3) includes a regularization sensitivity parameter λ.,4.5 Regularization Parameter Tuning,[0],[0]
"This parameter controls to what extent we should bind the source words and their target translation, common POS tags and arcs together.",4.5 Regularization Parameter Tuning,[0],[0]
In this section we measure the sensitivity of our approach with respect to this parameter.,4.5 Regularization Parameter Tuning,[0],[0]
"In a real world sce-
7http://www.statmt.org/wmt14/
nario, getting development data to tune this parameter is difficult.",4.5 Regularization Parameter Tuning,[0],[0]
"Thus, we want a parameter that can work well cross-lingually.",4.5 Regularization Parameter Tuning,[0],[0]
"To simulate this, we only tune the parameter on one language and apply it directly to different languages.",4.5 Regularization Parameter Tuning,[0],[0]
"We trained on a small Swedish treebank with 1k tokens, testing several different values of λ.",4.5 Regularization Parameter Tuning,[0],[0]
We evaluated on the Swedish development dataset.,4.5 Regularization Parameter Tuning,[0],[0]
Figure 1 shows the labelled attachment score (LAS) for different λ.,4.5 Regularization Parameter Tuning,[0],[0]
It’s clearly visible that λ = 0.0001 gives the maximum LAS on the development set.,4.5 Regularization Parameter Tuning,[0],[0]
"Thus, we use this value for all the experiments involving a dictionary hereafter.",4.5 Regularization Parameter Tuning,[0],[0]
For our initial experiments we assume that we have only a small target treebank with 3000 tokens (around 200 sentences).,4.6 Results,[0],[0]
Ideally the much larger source language (English) treebank should be able to improve parser performance versus simple supervised learning on such a small collection.,4.6 Results,[0],[0]
"We apply the joint model (equation 2) and joint model with the dictionary constraints (equation 3) for each target language,
The results are reported in Table 1.",4.6 Results,[0],[0]
"The supervised neural network dependency parser performed worst, as expected, and the baseline cascade model consistently outperformed the supervised model on all languages by an average margin of 5.6% (absolute).8 The joint model also consistently out-performed both baselines giving a further 1.9% average improvement over the cascade.",4.6 Results,[0],[0]
"This was despite the fact that the cascaded model had the benefit of tuning for the regularization parameters on a development corpus, while the joint model had no parameter tuning.",4.6 Results,[0],[0]
"Note that the improvement varies substantially across languages, and is largest for Czech but is only minor for Swedish.",4.6 Results,[0],[0]
"The joint model with the bilingual dictionary outperforms the joint model, however, the improvement is modest (0.7%).",4.6 Results,[0],[0]
"Nevertheless, this model gives substantial improvements compared with the cascaded and the supervised model (2.6% and 8.2%).",4.6 Results,[0],[0]
"In section 4.6, we used a 3k token treebank in the target language.",5.1 Learning Curve,[0],[0]
"What if we have more or less
8We use absolute percentage comparisons herein.
target language data?",5.1 Learning Curve,[0],[0]
Figure 2 shows the learning curve with respect to various models on different data sizes averaged over all target languages.,5.1 Learning Curve,[0],[0]
"For small datasets of 1k training tokens, the cascaded model, joint model and joint + dict model performed similarly well, out-performing the supervised model by about 10% (absolute).",5.1 Learning Curve,[0],[0]
"With more training data, we see interesting changes to the relative performance of the different models.",5.1 Learning Curve,[0],[0]
"While the baseline cascade model still outperforms the supervised model, the improvement is diminishing, and by 15k, the difference is only 2.9%.",5.1 Learning Curve,[0],[0]
"On the other hand, compared with the supervised model, the joint and joint + dict models perform consistently well at all sizes, maintaining an 8% lead at 15k.",5.1 Learning Curve,[0],[0]
"This shows the superiority of joint training compared with single language training.
",5.1 Learning Curve,[0],[0]
"To understand this pattern of performance differences for the cascade versus the joint model, one needs to consider the cascade model formulation.",5.1 Learning Curve,[0],[0]
"In this approach, the target language parameters are tied (softly) with the source language
parameters through regularization.",5.1 Learning Curve,[0],[0]
"This is a benefit for small datasets, providing a smoothing function to limit overtraining.",5.1 Learning Curve,[0],[0]
"However, when we have more training data, these constraints limit the capacity of the model to describe the target data.",5.1 Learning Curve,[0],[0]
"This is compounded by the problem that the source representation may not be appropriate for modelling the target language, and there is no way to correct for this.",5.1 Learning Curve,[0],[0]
"In contrast the joint model learns a mutually compatible representation automatically during joint training.
",5.1 Learning Curve,[0],[0]
The performance results for the joint model with and without the dictionary are similar overall.,5.1 Learning Curve,[0],[0]
"Only on small datasets (1k, 3k), is the difference notable.",5.1 Learning Curve,[0],[0]
"From 5k tokens, the bilingual dictionary doesn’t confer additional information, presumably as there is sufficient data for learning syntactic word representations.",5.1 Learning Curve,[0],[0]
"Moreover, translation entries exist between syntactically related word types as well as semantically related pairs, with the latter potentially limiting the beneficial effect of the dictionary.
",5.1 Learning Curve,[0],[0]
"When training on all the target language data, the supervised model does well, surpassing the cascade model.",5.1 Learning Curve,[0],[0]
"Surprisingly, the joint models outperform slightly, yielding a 0.4% improvement.",5.1 Learning Curve,[0],[0]
"This is an interesting observation suggesting that our method has potential for use not only for low resource problems, but also high resource settings.",5.1 Learning Curve,[0],[0]
"In the above experiments, we used the universal POS tagset for all the languages in the corpus.",5.2 Different Tagsets,[0],[0]
"However, for some languages,9 the UDT also provides language specific POS tags.",5.2 Different Tagsets,[0],[0]
We use this data to test the relative performance of the model using a universal tagset cf.,5.2 Different Tagsets,[0],[0]
language specific tagsets.,5.2 Different Tagsets,[0],[0]
"In this experiment, we applied the same joint model (see §3) but with a language specific tagset instead of UPOS for these languages.",5.2 Different Tagsets,[0],[0]
"We expect the joint
9en, cs, fi, ga, it and sv.
model to automatically learn to project the different tagsets into a common space, i.e., implicitly learn a tagset mapping between languages.",5.2 Different Tagsets,[0],[0]
Figure 3 shows the learning curve comparing the joint model with the two types of POS tagsets.,5.2 Different Tagsets,[0],[0]
"For the small dataset, it is clear that the data is insufficient for the model to learn a good tagset mapping, especially for a morphologically rich language like Czech.",5.2 Different Tagsets,[0],[0]
"However, with more data, the model is better able to learn the tagset mapping as part of joint training.",5.2 Different Tagsets,[0],[0]
"Beyond 15k tokens, the joint model using the language specific POS tagset outperforms UPOS.",5.2 Different Tagsets,[0],[0]
"Clearly there is some information lost in the UPOS tagset, although the UPOS mapping simultanously provides implicit linguistic supervision.",5.2 Different Tagsets,[0],[0]
"This explains why the UPOS might be useful in small data scenarios, but detrimental at scale.",5.2 Different Tagsets,[0],[0]
Using all the target data (“All”) the language specific POS provides a 1% (absolute) gain over UPOS.,5.2 Different Tagsets,[0],[0]
"As described in section 3, we can consider our joint model as the combination of two parts: a universal parser and a language-specific embedding Es or Et that converts the source and target language into the universal representation.",5.3 Universal Representation,[0],[0]
We now seek to analyse qualitatively this universal representation through visualization.,5.3 Universal Representation,[0],[0]
"For this purpose we use a joint model of English and French, using all the available French treebank (more than 350k
tokens) as well as a bilingual dictionary.10 Figure 4 shows the t-SNE (Van Der Maaten, 2014) projection of the 50 dimensional word embeddings in both languages.",5.3 Universal Representation,[0],[0]
We can see that English and French are mixed nicely together.,5.3 Universal Representation,[0],[0]
"The colouring denotes the POS tag, showing clearly that the words with similar POS tags are grouped together regardless of languages.",5.3 Universal Representation,[0],[0]
"This is partially understandable since word embeddings for dependency parsing need to convey the dependency context rather than surrounding words, as in most distributional embedding models.",5.3 Universal Representation,[0],[0]
"Words having similar dependency relation should be grouped together as they are treated similarly by the parser.
",5.3 Universal Representation,[0],[0]
"Some of the learned cross-lingual wordembeddings are shown in Table 2, which includes the five nearest neighbours to selected English words according to the monolingual word embedding (section 4.3) and our cross-lingual dependency word embeddings, trained using PanLex.",5.3 Universal Representation,[0],[0]
The monolingual sets appear to be strongly characterised by distributional similarity.,5.3 Universal Representation,[0],[0]
"The crosslingual embeddings display greater semantic similarity, while being more variable morphosyntactically.",5.3 Universal Representation,[0],[0]
"In many cases, the top five words of English and French are translations of each other, but with varying inflectional endings in the French forms.",5.3 Universal Representation,[0],[0]
"For example, “buy” vs “vendez” or “invest” vs “in-
10We also visualized the cross-lingual word embeddings without the dictionary, however the results were rather odd.",5.3 Universal Representation,[0],[0]
"Although we saw coherent POS clusters, the two languages were largely disjoint.",5.3 Universal Representation,[0],[0]
"We speculate that many components of the embeddings are use for only one language, and these outnumber the shared components, and thus more careful projection is needed for meaningful visualisation.
vestir”.",5.3 Universal Representation,[0],[0]
This is a direct consequence of incorporating the bilingual lexicon.,5.3 Universal Representation,[0],[0]
"Moreover, the top five closest words of both English and French mostly have the same part of speech.",5.3 Universal Representation,[0],[0]
"This is consistent with the finding in Figure 4.
",5.3 Universal Representation,[0],[0]
Levin (1993) has shown that there is a strong connection between a verb’s meaning and its syntactic behaviour.,5.3 Universal Representation,[0],[0]
"We compare the English side of our cross-lingual dependency based word embeddings with various other pre-trained monolingual English word embeddings and our monolingual embedding (section 4.3) on Verb-143 dataset (Baker et al., 2014).",5.3 Universal Representation,[0],[0]
This dataset contains 143 pairs of verbs that are manually given score from 1 to 10 according to the meaning similarity.,5.3 Universal Representation,[0],[0]
"Table 3 shows the Pearson correlation
with human judgment for our embeddings and other pre-trained embeddings.",5.3 Universal Representation,[0],[0]
"As expected, our cross-lingual embeddings out-perform others embeddings on this dataset.",5.3 Universal Representation,[0],[0]
"This is partly because the syntactic behaviour is well encoded in our word embeddings through dependency relation.
",5.3 Universal Representation,[0],[0]
"Our embeddings encode not just cross-lingual correspondences, but also capture dependency relations which we expect might be beneficial for other NLP tasks based on dependency parsing, e.g., cross-lingual semantic role labelling where long-distance relationship can be captured by word embedding.",5.3 Universal Representation,[0],[0]
"In this paper, we present a training method for building a dependency parser for a resourcepoor language using a larger treebank in a highresource language.",6 Conclusion,[0],[0]
"Our approach takes advantage of the shared structure among languages to learn a universal parser and language-specific mappings to the lexicon, parts of speech and dependency arcs.",6 Conclusion,[0],[0]
"Compared with supervised learning, our joint model gives a consistent 8-10% improvement over several different datasets in simulation lowresource scenarios.",6 Conclusion,[0],[0]
"Interestingly, some small but consistent gains are still realised by joint crosslingual training even on large complete treebanks.",6 Conclusion,[0],[0]
This suggests that our approach has utility not just in low resource settings.,6 Conclusion,[0],[0]
"Our joint model is flexible, allowing the incorporation of a bilingual dictionary, which results in small improvements particularly for tiny training scenarios.
",6 Conclusion,[0],[0]
"As the side-effect of training our joint model, we obtain cross-lingual word embeddings specialized for dependency parsing.",6 Conclusion,[0],[0]
"We expect these embeddings to be beneficial to other syntatic and se-
mantic tasks.",6 Conclusion,[0],[0]
"In future work, we plan to extend joint training to several languages, and further explore the idea of learning and exploiting crosslingual embeddings.",6 Conclusion,[0],[0]
This work was supported by the University of Melbourne and National ICT Australia (NICTA).,Acknowledgments,[0],[0]
Trevor Cohn is the recipient of an Australian Research Council Future Fellowship (project number FT130101105).,Acknowledgments,[0],[0]
"Accurate dependency parsing requires large treebanks, which are only available for a few languages.",abstractText,[0],[0]
We propose a method that takes advantage of shared structure across languages to build a mature parser using less training data.,abstractText,[0],[0]
"We propose a model for learning a shared “universal” parser that operates over an interlingual continuous representation of language, along with language-specific mapping components.",abstractText,[0],[0]
"Compared with supervised learning, our methods give a consistent 8-10% improvement across several treebanks in low-resource simulations.",abstractText,[0],[0]
A Neural Network Model for Low-Resource Universal Dependency Parsing,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4243–4252 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
4243",text,[0],[0]
Machine comprehension (MC) systems mimic the process of reading comprehension (RC) by answering questions after understanding natural language text.,1 Introduction,[0],[0]
Several datasets and resources have been developed recently.,1 Introduction,[0],[0]
Richardson et al. (2013) developed a small-scale multiple-choice question answering (QA) dataset.,1 Introduction,[0],[0]
Hermann et al. (2015) created a large cloze-style MC dataset based on CNN and Daily Mail news article summaries.,1 Introduction,[0],[0]
"However, Chen et al. (2016) reported that the
task is not challenging enough and hence, advanced models had to be evaluated on more realistic datasets.",1 Introduction,[0],[0]
"Subsequently, SQuAD (Rajpurkar et al., 2016) was released, where, unlike previous datasets, the answers to different questions can vary in length.
",1 Introduction,[0],[0]
"In previous datasets, questions and answers are formulated given text passages.",1 Introduction,[0],[0]
"Hence, a valid answer can always be found in the associated passage for every question created.",1 Introduction,[0],[0]
"Trischler et al. (2017) proposed a more challenging and realistic dataset, NewsQA, where the questions were formed using CNN article summaries without accessing the original full texts.",1 Introduction,[0],[0]
"As such, some questions have no valid answers in the associated passages (referred to as nil questions).
",1 Introduction,[0],[0]
"Recently, several neural models for answer span extraction have been proposed (Wang and Jiang, 2017; Seo et al., 2017; Yang et al., 2017; Xiong et al., 2017; Weissenborn et al., 2017; Wang et al., 2017; Shen et al., 2017b; Chen et al., 2017; Kundu and Ng, 2018).",1 Introduction,[0],[0]
"However, none of the models considered nil questions, although it is crucial for a practical QA system to be able to determine whether a text passage contains a valid answer for a question.",1 Introduction,[0],[0]
"In this paper, we focus on developing QA systems that extract an answer for a question if and only if the associated passage contains a valid answer.",1 Introduction,[0],[0]
"Otherwise, they are expected to return Nil as answer.",1 Introduction,[0],[0]
"We propose a nil-aware answer extraction framework which returns Nil or a span of text as answer, when integrated with end-toend neural MC models.",1 Introduction,[0],[0]
"Our proposed framework is based on evidence decomposition-aggregation, where the evidence vectors derived by a higher level encoding layer are first decomposed into relevant and irrelevant components and later aggregated to infer the existence of a valid answer.",1 Introduction,[0],[0]
"In addition, we develop several baseline models with pipeline and threshold-based approaches.",1 Introduction,[0],[0]
"In a
pipeline model, detection of nil questions is carried out separately before answer span extraction.",1 Introduction,[0],[0]
"In a threshold-based model, the answer span extraction model is entirely trained on questions that have valid answers, and Nil is returned based on a confidence threshold.
",1 Introduction,[0],[0]
"The contributions of this paper are as follows: (1) We propose a nil-aware answer span extraction framework to return Nil or an exact answer span to a question, in a single step, depending on the existence of a valid answer.",1 Introduction,[0],[0]
(2) Our proposed framework can be readily integrated with many recently proposed neural machine comprehension models.,1 Introduction,[0],[0]
"In this paper, we extend four machine comprehension models, namely BiDAF (Seo et al., 2017), RNet (Wang et al., 2017),",1 Introduction,[0],[0]
"DrQA (Chen et al., 2017), and AMANDA (Kundu and Ng, 2018), with our proposed framework, and show that they achieve significantly better results compared to the corresponding pipeline and threshold-based models on the NewsQA dataset.",1 Introduction,[0],[0]
"Given a passage and a question, we propose models that can extract an answer if and only if the passage contains an answer.",2 Task Definition,[0],[0]
"When the passage does not contain any answer, the models return Nil as the answer.",2 Task Definition,[0],[0]
"A valid answer is denoted as two pointers in the passage, representing the start and end tokens of the answer span.",2 Task Definition,[0],[0]
"Let P be a passage with tokens (P1,P2, . . .",2 Task Definition,[0],[0]
",PT ) and Q be a question with tokens (Q1,Q2, . . .",2 Task Definition,[0],[0]
",QU ), where T and U are the length of the passage and question respectively.",2 Task Definition,[0],[0]
"A system needs to determine whether the answer is Nil or comprises two pointers, b and e, such that 1 ≤ b ≤ e ≤ T .",2 Task Definition,[0],[0]
"In this section, we first describe our proposed evidence decomposition-aggregation framework for nil-aware answer extraction.",3 Proposed Framework,[0],[0]
"Then, we provide a detailed description of how we extend a state-ofthe-art model AMANDA (Kundu and Ng, 2018) to NAMANDA1 (nil-aware AMANDA).",3 Proposed Framework,[0],[0]
"We also provide brief descriptions of how we integrate our proposed framework with the other three models.
",3 Proposed Framework,[0],[0]
1Our source code is available at https://github. com/nusnlp/namanda,3 Proposed Framework,[0],[0]
"Decomposition of lexical semantics over sentences has been successfully used in the past for sentence similarity learning (Wang et al., 2016).",3.1 Nil-Aware Answer Extraction,[0],[0]
Most of the recently proposed machine reading comprehension models can be generalized based on a common pattern observed in their network architecture.,3.1 Nil-Aware Answer Extraction,[0],[0]
They have a question-passage joint encoding layer (also known as question-aware passage encoding layer) followed by an evidence encoding layer.,3.1 Nil-Aware Answer Extraction,[0],[0]
"In this work, we decompose the evidence vectors for each passage word obtained from the evidence encoding layer with respect to question-passage joint encoding vectors to derive semantically relevant and irrelevant components.",3.1 Nil-Aware Answer Extraction,[0],[0]
"We decompose the evidence vectors for each passage word, because passage vectors can be partially supported by the corresponding questionpassage joint encoding vectors, and based on the level of support, it either increases or decreases the chance of finding a valid answer.",3.1 Nil-Aware Answer Extraction,[0],[0]
"When we aggregate the orthogonally decomposed evidence vectors, it combines both the supportive and unsupportive pieces of evidence for a particular passage word.",3.1 Nil-Aware Answer Extraction,[0],[0]
"To obtain the most impactful portions, we perform a max-pooling operation over all the aggregated vectors.",3.1 Nil-Aware Answer Extraction,[0],[0]
The resulting vector is denoted as the Nil vector.,3.1 Nil-Aware Answer Extraction,[0],[0]
"As the training set contains both nil questions (with no valid answers) and non-nil questions (with valid answers), the model automatically learns when to pool unsupportive (for nil questions) and supportive (for non-nil questions) portions to construct the Nil vector.",3.1 Nil-Aware Answer Extraction,[0],[0]
"In this way, the model is able to induce a strong bias towards the nil pointer when there is no answer present due to the dominance of unsupportive components in the nil vector.
",3.1 Nil-Aware Answer Extraction,[0],[0]
"The proposed method in Wang et al. (2016) was developed for sentence similarity learning tasks, such as answer sentence selection.",3.1 Nil-Aware Answer Extraction,[0],[0]
They decompose an answer sentence with respect to a question and vice versa.,3.1 Nil-Aware Answer Extraction,[0],[0]
The decomposed vectors are then aggregated to obtain a single vector which is used to derive the similarity score.,3.1 Nil-Aware Answer Extraction,[0],[0]
"Although our proposed method (developed for the more complex task of answer span extraction) is inspired from the idea of lexical decomposition and composition, one major difference is that we decompose the evidence vectors with respect to questionpassage joint encoding vectors.",3.1 Nil-Aware Answer Extraction,[0],[0]
"Another important advance is how it is adopted to return nil or a span
of text from the passage in a single step.",3.1 Nil-Aware Answer Extraction,[0],[0]
The architecture of Nil-aware AMANDA (NAMANDA) is given in Figure 1.,3.2 Nil-Aware AMANDA,[0],[0]
"To obtain the embeddings, we concatenate word and character-level embedding vectors.",3.2.1 Embeddings,[0],[0]
"We use pre-trained vectors from GloVe (Pennington et al., 2014) for word-level embeddings.",3.2.1 Embeddings,[0],[0]
"For character embeddings, a trainable character-based lookup table is used followed by a convolutional neural network (CNN) and max-pooling (Kim, 2014).",3.2.1 Embeddings,[0],[0]
"We use bi-directional LSTM (BiLSTM) (Hochreiter and Schmidhuber, 1997) on the embedding vectors to incorporate contextual information.",3.2.2 Sequence Encoding,[0],[0]
We represent the outputs as D ∈ RT×H and Q ∈ RU×H for passage and question respectively.,3.2.2 Sequence Encoding,[0],[0]
H is the number of hidden units of the BiLSTMs.,3.2.2 Sequence Encoding,[0],[0]
The similarity matrix is obtained by computing the dot product of passage and question sequencelevel encoding vectors.,3.2.3 Similarity Matrix,[0],[0]
"The similarity matrix A ∈ RT×U can be expressed as A = D Q>, where Ai,j is the similarity between the ith passage word and the jth question word.",3.2.3 Similarity Matrix,[0],[0]
"To aggregate the most relevant parts of the question, column-wise maximum values of A are normalized using a softmax function to obtain k ∈ RU .",3.2.4 Question Formulation,[0],[0]
"Then, the question vectors in Q are aggregated by qma = k Q.",3.2.4 Question Formulation,[0],[0]
"The question type information is incorporated via qf ∈ R2H , by concatenating the sequence-level question encoding vectors of the first wh-word qtwh and its following word qtwh+1.",3.2.4 Question Formulation,[0],[0]
It can be given as qf = qtwh ||,3.2.4 Question Formulation,[0],[0]
"qtwh+1, where || denotes the concatenation operation.",3.2.4 Question Formulation,[0],[0]
"The set of wh-words we used is {what, who, how, when, which, where, why}.",3.2.4 Question Formulation,[0],[0]
"The final question representation, q̃ ∈ RH , is formulated by applying a feed-forward neural network on the concatenated representation of qma and qf .",3.2.4 Question Formulation,[0],[0]
"In this step, we jointly encode the passage and question.",3.2.5 Question-Passage Joint Encoding,[0],[0]
We apply a row-wise softmax function on A to obtain R ∈ RT×U .,3.2.5 Question-Passage Joint Encoding,[0],[0]
"Now, for all the passage words, the aggregated question representation G ∈ RT×H is computed by G = R Q.",3.2.5 Question-Passage Joint Encoding,[0],[0]
The aggregated question vectors corresponding to the passage words are then concatenated with the sequence-level passage vectors to obtain S ∈ RT×2H .,3.2.5 Question-Passage Joint Encoding,[0],[0]
We apply another BiLSTM to obtain a combined representation V ∈ RT×H .,3.2.5 Question-Passage Joint Encoding,[0],[0]
"First, multi-factor self-attentive encoding is applied to accumulate evidence from the entire passage.",3.2.6 Evidence Decomposition-Aggregation,[0],[0]
The use of multiple factors while calculating self attention helps to obtain meaningful information from a long context with fine-grained inference.,3.2.6 Evidence Decomposition-Aggregation,[0],[0]
"Ifm represents the number of factors, multifactor attention F[1:m] ∈ RT×m×T is formulated as:
F[1:m] = V W",3.2.6 Evidence Decomposition-Aggregation,[0],[0]
"[1:m] f V > , (1)
where W[1:m]f ∈ R H×m×H is a 3-way tensor.",3.2.6 Evidence Decomposition-Aggregation,[0],[0]
"Now, to refine the evidence, a max-pooling operation is performed on F[1:m] over the number of factors, resulting in the self-attention matrix F ∈ RT×T .",3.2.6 Evidence Decomposition-Aggregation,[0],[0]
"We normalize F by applying a row-wise softmax function, resulting in F̃ ∈ RT×T .",3.2.6 Evidence Decomposition-Aggregation,[0],[0]
"Now the self-attentive encoding M ∈ RT×H can be given as M = F̃ V. The self-attentive encoding vectors are then concatenated with the questiondependent passage word encoding vectors (V), and a feed-forward neural network-based gating is applied to control the overall impact, resulting in Y ∈ RT×2H .
",3.2.6 Evidence Decomposition-Aggregation,[0],[0]
Then we decompose the evidence vector for every passage word with orthogonal decomposition.,3.2.6 Evidence Decomposition-Aggregation,[0],[0]
"Each row of Y, yt ∈",3.2.6 Evidence Decomposition-Aggregation,[0],[0]
"R2H , is decomposed into its parallel and perpendicular components with respect to the corresponding question-passage joint encoding (S) vector, st ∈ R2H .",3.2.6 Evidence Decomposition-Aggregation,[0],[0]
The parallel components represent the relevant parts of the accumulated evidence and the orthogonal components represent the irrelevant counterparts.,3.2.6 Evidence Decomposition-Aggregation,[0],[0]
"If the parallel component of yt is represented as y=t ∈ R2H and the perpendicular component is represented as y⊥t ∈ R2H , then
y=t = yt s
> t
st s>t st (2)
",3.2.6 Evidence Decomposition-Aggregation,[0],[0]
y⊥t = yt,3.2.6 Evidence Decomposition-Aggregation,[0],[0]
"− y=t (3)
Similarly, we derive the parallel and orthogonal vectors for all the passage words.",3.2.6 Evidence Decomposition-Aggregation,[0],[0]
"We denote parallel components with Y= ∈ RT×2H and perpendicular components with Y⊥ ∈ RT×2H .
",3.2.6 Evidence Decomposition-Aggregation,[0],[0]
"In the aggregation step, the parallel and orthogonal components are fed to a feed-forward linear layer.",3.2.6 Evidence Decomposition-Aggregation,[0],[0]
"Ya ∈ RT×H denotes the output of the linear layer and yat ∈ RH is its tth row:
yat = tanh(y = t",3.2.6 Evidence Decomposition-Aggregation,[0],[0]
"Wa + y ⊥ t Wa + ba) , (4)
where Wa ∈ R2H×H and ba ∈ RH are the weight matrix and bias vector respectively.",3.2.6 Evidence Decomposition-Aggregation,[0],[0]
Then we apply a max-pooling operation over all the words to obtain the Nil vector representation denoted as n̂.,3.2.6 Evidence Decomposition-Aggregation,[0],[0]
Now we derive the score for the Nil pointer which will be shared for normalizing the beginning and ending pointers later.,3.2.6 Evidence Decomposition-Aggregation,[0],[0]
"The Nil pointer score is given as:
ns = n̂w > n , (5)
where wn ∈ RH is a learnable weight vector.",3.2.6 Evidence Decomposition-Aggregation,[0],[0]
Two stacked BiLSTMs are used on top of Y to determine the beginning and ending pointers.,3.2.7 Nil-Aware Pointing,[0],[0]
Let the hidden unit representations of these two BiLSTMs be B ∈ RT×H and E ∈ RT×H .,3.2.7 Nil-Aware Pointing,[0],[0]
We measure the similarity scores between the previously derived question vector q̃ and the contextual encoding vectors in B and E.,3.2.7 Nil-Aware Pointing,[0],[0]
"If sb ∈ RT and se ∈ RT are the scores for the beginning and ending pointers, then
sb = q̃ B > , se = q̃ E > (6)
We prepend the nil score ns to sb and se for shared normalization.",3.2.7 Nil-Aware Pointing,[0],[0]
"The updated scores ŝb ∈ RT+1 and ŝe ∈ RT+1 can be represented as:
ŝb =",3.2.7 Nil-Aware Pointing,[0],[0]
"[ns, sb] , ŝe =",3.2.7 Nil-Aware Pointing,[0],[0]
"[ns, se] (7)
The beginning and ending pointer probability distributions for a given passage P and a question Q is given as:
Pr(b | P,Q) = softmax(ŝb) Pr(e | P,Q) = softmax(ŝe) (8)
The joint probability distribution for answer a is given as:
Pr(a | P,Q) =",3.2.7 Nil-Aware Pointing,[0],[0]
"Pr(b | P,Q)",3.2.7 Nil-Aware Pointing,[0],[0]
"Pr(e | P,Q) (9)
",3.2.7 Nil-Aware Pointing,[0],[0]
"For training, we minimize the cross entropy loss summing over all training instances.",3.2.7 Nil-Aware Pointing,[0],[0]
"During prediction, we select the locations in the passage for which the product of Pr(b) and Pr(e) is maximized, where 1 ≤ b ≤ e ≤ T + 1.",3.2.7 Nil-Aware Pointing,[0],[0]
"If the value of b is 1, we assign the answer as Nil.",3.2.7 Nil-Aware Pointing,[0],[0]
"We extend DrQA (Chen et al., 2017) to NDrQA by integrating our proposed nil-aware answer extraction framework.",3.3 Nil-Aware DrQA,[0],[0]
"In DrQA, the embeddings
of passage tokens consist of pretrained word vectors from Glove, several syntactic features, and passage-question joint embedding (aligned question embedding).",3.3 Nil-Aware DrQA,[0],[0]
"The syntactic features include exact match of passage words with question in surface, lowercase, and lemma form.",3.3 Nil-Aware DrQA,[0],[0]
"They also used part-of-speech tags, named entity tags, and term frequency values for each passage word.",3.3 Nil-Aware DrQA,[0],[0]
"Subsequently, a stack of BiLSTMs is used for encoding.",3.3 Nil-Aware DrQA,[0],[0]
The outputs of the stacked BilSTMs are used as evidence vectors to help extract the answer span.,3.3 Nil-Aware DrQA,[0],[0]
We decompose those stacked BiLSTM output vectors with respect to the passage embedding and generate the nil pointer score as given in Eqs (2–5).,3.3 Nil-Aware DrQA,[0],[0]
The question vector formulation in DrQA is performed by applying a stack of BilSTMs on question embedding.,3.3 Nil-Aware DrQA,[0],[0]
The nil-aware pointing mechanism is the same as that given in Section 3.2.7 except an additional bi-linear term is used for each sb and se in Eq (6).,3.3 Nil-Aware DrQA,[0],[0]
"In R-Net (Wang et al., 2017), after embedding and encoding of the passage and question words, a gated recurrent network is used to obtain the question-passage joint representation.",3.4 Nil-Aware R-Net,[0],[0]
"Subsequently, a self-matching attentive encoding is used to accumulate evidence from the entire passage.",3.4 Nil-Aware R-Net,[0],[0]
"In the output layer, an answer recurrent pointer network is used to predict the boundary of an answer span.",3.4 Nil-Aware R-Net,[0],[0]
"To extend R-Net to nil-aware R-Net (NR-Net), we decompose the output vectors of the self-matching layer with respect to the questionpassage joint encoding vectors, and then aggregate them to obtain the nil pointer score as illustrated in Eqs (2–5).",3.4 Nil-Aware R-Net,[0],[0]
"In the output layer, we combine the nil pointer score to the beginning and ending pointer unnormalized scores, and jointly normalize them using softmax function as given in Eqs (7–8).",3.4 Nil-Aware R-Net,[0],[0]
"In BiDAF (Seo et al., 2017), an attention flow layer is used to jointly encode the passage and question.",3.5 Nil-Aware BiDAF,[0],[0]
"Then, a modeling layer is used to capture the interaction among the question-aware passage vectors.",3.5 Nil-Aware BiDAF,[0],[0]
The output of the modeling layer serves as evidence to help extract the answer span in the output layer.,3.5 Nil-Aware BiDAF,[0],[0]
"To extend the BiDAF model to nil-aware BiDAF (NBiDAF), we decompose the output of the modeling layer with respect to the question-passage joint encoding vectors, and then aggregate them to derive the nil pointer score (sim-
ilar to Eqs (2–5).",3.5 Nil-Aware BiDAF,[0],[0]
"Similar to the other nil-aware models, we concatenate the nil pointer score to the start and end pointer unnormalized scores derived in the output layer, and then jointly normalize them.",3.5 Nil-Aware BiDAF,[0],[0]
"For comparison, we propose two types of baseline approaches for nil-aware answer extraction.",4 Baseline Models,[0],[0]
"Here, two models are used in a pipeline: Nil detector:",4.1 Pipeline Approach,[0],[0]
"Given a pair of passage and question, a nil detector model determines whether a valid answer is present in the passage.
",4.1 Pipeline Approach,[0],[0]
Answer span extractor:,4.1 Pipeline Approach,[0],[0]
"If the nil detector model predicts the presence of a valid answer, the answer span extractor then extracts the answer.
",4.1 Pipeline Approach,[0],[0]
"For nil detection, we developed a logistic regression (LR) model with manually defined features and four neural models.",4.1 Pipeline Approach,[0],[0]
"For the LR model, we extract four different features which capture the similarity between a passage and a question.",4.1 Pipeline Approach,[0],[0]
Let P be the passage and Q be the question (consisting of U ′ tokens excluding stop words).,4.1 Pipeline Approach,[0],[0]
"If f(P, Qi) is the frequency of the ith question word in passage P , then the first feature η is defined as:
η = U ′∑ i=1 log(1",4.1 Pipeline Approach,[0],[0]
+,4.1 Pipeline Approach,[0],[0]
"f(P,Qi))",4.1 Pipeline Approach,[0],[0]
"(10)
The second feature is the same as η, except that the lemma form is considered for both passage and question tokens instead of the surface form.",4.1 Pipeline Approach,[0],[0]
"Additionally, we include word overlap count features in both surface and lemma forms.
",4.1 Pipeline Approach,[0],[0]
We also develop several advanced neural network architectures for nil detection.,4.1 Pipeline Approach,[0],[0]
"After embedding (the same as Section 3.2.1), we apply sequence-level encoding with either BiLSTM or CNN.",4.1 Pipeline Approach,[0],[0]
"For CNN, we use equal numbers of unigram, bigram, and trigram filters and the outputs are concatenated to obtain the final encoding.",4.1 Pipeline Approach,[0],[0]
"Next, we apply either global max-pooling (MP) or attentive pooling (AP) over all the sequence vectors to obtain an aggregated vector representation.",4.1 Pipeline Approach,[0],[0]
"Let the sequence encoding of a passage be Pnd ∈ RT×H , and pndt be the tth row of Pnd.",4.1 Pipeline Approach,[0],[0]
"The aggregated vector p̃nd ∈ RH for AP can be
obtained as:
andt ∝ exp(pndt",4.1 Pipeline Approach,[0],[0]
"w>) (11) p̃nd = a ndPnd , (12)
where w ∈ RH is a learnable vector.",4.1 Pipeline Approach,[0],[0]
"Similarly, we derive the aggregated question vector q̃nd.",4.1 Pipeline Approach,[0],[0]
"For nil detection, we compute the similarity score (snd) between the aggregated vectors:
snd = sigmoid(p̃nd q̃>nd) (13)
",4.1 Pipeline Approach,[0],[0]
"We experimented with four state-of-the-art answer span extractor models, namely BiDAF (Seo et al., 2017), R-Net (Wang et al., 2017), DrQA (Chen et al., 2017), and AMANDA (Kundu and Ng, 2018).",4.1 Pipeline Approach,[0],[0]
Note that the answer extraction models are trained entirely on passage-question pairs which always have valid answers.,4.1 Pipeline Approach,[0],[0]
"Here, we do not use any nil questions to train the neural answer span extraction model.",4.2 Threshold-Based Approach,[0],[0]
"This approach assumes that when there is a valid answer, the probability distributions of the beginning and ending pointers will have lower entropy.",4.2 Threshold-Based Approach,[0],[0]
This results in a higher maximum joint probability of the beginning and ending pointers.,4.2 Threshold-Based Approach,[0],[0]
"In contrast, when an answer is not present in the associated passage, the output probability distributions have higher entropy, resulting in a lower value of maximum joint probability.",4.2 Threshold-Based Approach,[0],[0]
We set the maximum joint probability threshold based on the best Nil F1 score on the nil questions in the development set.,4.2 Threshold-Based Approach,[0],[0]
"Now, for a given test passage and question, we first compute the maximum of all the joint probabilities associated with all the answer spans.",4.2 Threshold-Based Approach,[0],[0]
Let aspan be the answer span with highest joint probability pmax.,4.2 Threshold-Based Approach,[0],[0]
"We assign the final answer as follows:
answer = { Nil, if pmax ≤ threshold aspan, otherwise
(14)",4.2 Threshold-Based Approach,[0],[0]
"We use the NewsQA dataset with nil questions (Trischler et al., 2017) in our experiments.",5.1 Experimental Settings,[0],[0]
"Its training, development, and test sets consist of 10,938, 638, and 632 passages respectively and every passage is associated with some questions.",5.1 Experimental Settings,[0],[0]
"In each subset, there are some questions which
have no answers in the corresponding associated passages (i.e., the nil questions).",5.1 Experimental Settings,[0],[0]
"The detailed statistics of the dataset are given in Table 1.
",5.1 Experimental Settings,[0],[0]
We compute exact match (EM) and F1 score for questions with valid answers.,5.1 Experimental Settings,[0],[0]
"For questions without any valid answers, we compute Nil precision, recall, and F1 scores as follows:
Nil precision = #Correctly predicted Nil#predicted",5.1 Experimental Settings,[0],[0]
"Nil (15)
",5.1 Experimental Settings,[0],[0]
"Nil recall = #Correctly predicted Nil#Nil questions (16)
",5.1 Experimental Settings,[0],[0]
"Nil F1 = 2× Nil precision ×Nil recallNil precision+Nil recall (17)
To compute the overall EM and F1 scores, we consider Nil as correct for the questions which do not have any valid answers.",5.1 Experimental Settings,[0],[0]
"All evaluation scores reported in this paper are in %.
",5.1 Experimental Settings,[0],[0]
All the neural network models are implemented in PyTorch2.,5.1 Experimental Settings,[0],[0]
We use the default hyper-parameters for all the answer span extractor models.,5.1 Experimental Settings,[0],[0]
We use the open source implementation of DrQA3.,5.1 Experimental Settings,[0],[0]
We use a third party implementation of R-Net4 whose performance is very similar to the original scores.,5.1 Experimental Settings,[0],[0]
We reimplemented BiDAF5 and AMANDA6 to easily integrate our proposed nil-aware answer extraction framework and make the training faster.,5.1 Experimental Settings,[0],[0]
We integrate the nil-aware answer span extraction framework with each model keeping all the hyperparameters unchanged.,5.1 Experimental Settings,[0],[0]
"For nil-detection models, we use the same settings as (N)AMANDA.",5.1 Experimental Settings,[0],[0]
We use 300 hidden units for BiLSTMs and a total of 300 filters for the CNN-based models.,5.1 Experimental Settings,[0],[0]
"We use dropout (Srivastava et al., 2014) with probability 0.3 for every trainable layer.",5.1 Experimental Settings,[0],[0]
"We use binary crossentropy loss and the Adam optimizer (Kingma and Ba, 2015) for training the nil-detection models.
2http://pytorch.org 3https://github.com/facebookresearch/DrQA 4https://github.com/HKUST-KnowComp/
MnemonicReader/blob/master/r_net.py 5Our implementation gives 3% lower F1 score compared to the reported results in Seo et al. (2017) on the SQuAD development set.
",5.1 Experimental Settings,[0],[0]
6Our implementation gives 0.5% higher F1 score compared to the reported scores in Kundu and Ng (2018) on the NewsQA test set.,5.1 Experimental Settings,[0],[0]
"Tables 2 and 3 compare results of the nil-aware answer span extractor models with several pipeline and threshold-based models, respectively.",5.2 Results,[0],[0]
We also include the results of four standalone answer span extraction models on the test set without nil questions.,5.2 Results,[0],[0]
Table 2 shows that the end-to-end nil-aware models achieve the highest overall EM and F1 scores compared to all the corresponding pipeline systems.,5.2 Results,[0],[0]
Note that the MP-BiLSTM nil detection model achieves higher Nil F1 scores compared to LR and MP-CNN.,5.2 Results,[0],[0]
This is because BiLSTM is able to capture long-range contextual information to infer the existence of valid answers.,5.2 Results,[0],[0]
"Furthermore, AP-based models perform better compared to MP-based models as the attention mechanism used in AP-based models inherently identifies important contextual information.",5.2 Results,[0],[0]
"Due to this, the performance gap between AP-CNN and AP-BiLSTM is lower than the performance gap between MP-CNN and MP-BiLSTM.",5.2 Results,[0],[0]
"In addition to achieving higher Nil F1 score than the strong nil detection baseline systems, nil-aware models
manage to achieve competitive scores compared to the corresponding standalone answer span extractors on the test set where there are no nil questions.
Table 3 shows that the nil-aware models outperform the corresponding threshold-based models.",5.2 Results,[0],[0]
"Note that all four answer span extraction models, when used in a threshold-based approach for nil detection, produce low Nil precision and relatively higher Nil recall.",5.2 Results,[0],[0]
The low precision significantly degrades performance on the test set without nil questions.,5.2 Results,[0],[0]
These models often return Nil since it is critical to find suitable values for the required threshold.,5.2 Results,[0],[0]
"This is because NewsQA passages are often very long and as a result, probability distributions with higher entropy for answer pointer selection lead to irregular maximum joint probability threshold values.
",5.2 Results,[0],[0]
We perform statistical significance tests using paired t-test and bootstrap resampling.,5.2 Results,[0],[0]
Performances of all the nil-aware models (in terms of overall EM and F1) are significantly better (p < 0.01) than the corresponding best pipeline models and threshold-based approaches.,5.2 Results,[0],[0]
"For better understanding, we present further experiments and analysis of one of the proposed models, NAMANDA.
",5.3 Analysis,[0],[0]
"In addition to linear aggregation, we experiment with BiLSTM-based and CNN-based aggregation models.",5.3 Analysis,[0],[0]
"When we use BiLSTM aggregation, Eq.",5.3 Analysis,[0],[0]
"(4) is modified to yat = h = t + h ⊥ t , where
h=t = BiLSTM(y = t ,h = t−1,h = t+1) (18) h⊥t = BiLSTM(y ⊥ t ,h ⊥ t−1,h ⊥ t+1)
",5.3 Analysis,[0],[0]
"We use equal numbers of unigram, bigram, and trigram filters for CNN-based aggregation.",5.3 Analysis,[0],[0]
"Similar to BiLSTM-based aggregation, we add the CNN outputs for Y= and Y⊥. Table 4 shows that linear aggregation achieves the highest overall F1 score despite using the least number of parameters.
",5.3 Analysis,[0],[0]
"Table 5 shows the results of NAMANDA on the NewsQA development set when different components are removed such as character embeddings, question-passage joint encoding, and the second LSTM for the answer-ending pointer.",5.3 Analysis,[0],[0]
"When question-passage joint encoding is removed, selfattentive encoding is formed as well as decomposed with respect to sequence-level passage encoding.",5.3 Analysis,[0],[0]
"When we remove the second LSTM for the answer-ending pointer, a feed-forward network is used instead.",5.3 Analysis,[0],[0]
"It is clear from Table 5 that question-passage joint encoding has the highest impact.
",5.3 Analysis,[0],[0]
Figure 2(a) and Figure 2(b) show the results of NAMANDA on different question (excluding the stop words) and passage lengths respectively on the NewsQA development set.,5.3 Analysis,[0],[0]
"With increasing question length, the Nil F1 score also improves.
",5.3 Analysis,[0],[0]
"This is because with more information in a question, it becomes easier to detect whether the associated passage contains a valid answer.",5.3 Analysis,[0],[0]
Increasing Nil F1 scores also help to improve the overall F1 scores.,5.3 Analysis,[0],[0]
"However, the overall F1 score degrades with increasing length of the associated passage.",5.3 Analysis,[0],[0]
"When the associated passage is long, it is difficult for the answer span extractor to extract an answer for a question which has a valid answer, due to the increasing amount of potentially distracting information.",5.3 Analysis,[0],[0]
"The Nil F1 scores remain similar for passages consisting of not more than 1,200 tokens.",5.3 Analysis,[0],[0]
"Beyond that, the Nil F1 score degrades a little
as it becomes very challenging to infer the existence of a valid answer accurately with increasing amount of potentially distracting information present in the passage.
",5.3 Analysis,[0],[0]
Nil detection is itself a very challenging task.,5.3 Analysis,[0],[0]
Performances of the nil-aware models are worse than the corresponding answer extractor models on the test set without nil questions as Nil precision is lower than 100%.,5.3 Analysis,[0],[0]
We carried out an experiment to evaluate the performance of NAMANDA on development sets with varying number of nil questions.,5.3 Analysis,[0],[0]
"As the proportion of nil questions in a set increases, NAMANDA outperforms AMANDA by a larger margin on overall scores.",5.3 Analysis,[0],[0]
"In some years of the question answering track at the Text Retrieval Conference (TREC)7, some questions were considered as nil questions for which no valid answers could be found in the entire corpus.",6 Related Work,[0],[0]
Participating teams were required to return Nil as answer for those questions.,6 Related Work,[0],[0]
Many teams used threshold-based methods to determine whether any of the retrieved answers for a given question was valid or not.,6 Related Work,[0],[0]
"If none of the answers had high confidence, Nil was returned as answer.",6 Related Work,[0],[0]
"To evaluate the performance on the nil questions, TREC used Nil precision, recall and F1 scores.
",6 Related Work,[0],[0]
"In recent years, research on question answering has witnessed substantial progress with rapid advances in neural network architectures.",6 Related Work,[0],[0]
"For example, on the answer sentence selection task, where a system has to choose the correct answer sentence from a pool of candidate sentences for a given question, the introduction of attention-based neural models has rapidly advanced the state of the art (Tan et al., 2015; Yang et al., 2016; dos Santos et al., 2016; Wang et al., 2016; Bian et al., 2017;
7https://trec.nist.gov/data/qa.html
Shen et al., 2017a).",6 Related Work,[0],[0]
"However, in the answer sentence selection task, the answer is always a full sentence.",6 Related Work,[0],[0]
"Rajpurkar et al. (2016) released a reading comprehensionbased QA dataset SQuAD, where given a passage and a question, a system needs to find the exact answer span rather than a sentence.",6 Related Work,[0],[0]
"Although SQuAD became very popular and served as a good test set to develop advanced end-toend neural network architectures, it does not include any nil questions.",6 Related Work,[0],[0]
"In practical QA, it is critical to decide whether or not a passage contains a valid answer for a given question.",6 Related Work,[0],[0]
"Subsequently, the NewsQA (Trischler et al., 2017) dataset has been released which attempts to overcome this deficiency.",6 Related Work,[0],[0]
"However, all the proposed models for NewsQA so far have excluded nil questions during evaluation.",6 Related Work,[0],[0]
"Contrary to prior work, we focus on developing models for nil-aware answer span extraction.",6 Related Work,[0],[0]
"Very recently, Rajpurkar et al. (2018) released the SQUADRUN dataset by augmenting the SQuAD dataset with unanswerable questions.",6 Related Work,[0],[0]
The unanswerable questions are written adversarially by crowdworkers to look similar to the answerable ones.,6 Related Work,[0],[0]
"In this paper, we have focused on nil-aware answer span extraction for RC-based QA.",7 Conclusion,[0],[0]
A nil-aware QA system only extracts a span of text from the associated passage as an answer to a given question if and only if the passage contains a valid answer.,7 Conclusion,[0],[0]
We have proposed a nil-aware answer span extraction framework based on evidence decomposition and aggregation that can be easily integrated with several recently proposed neural answer span extraction models.,7 Conclusion,[0],[0]
We have also developed several pipeline and threshold-based models using advanced neural architectures for comparison.,7 Conclusion,[0],[0]
"Experiments on the NewsQA dataset show that our proposed framework, when integrated with the answer span extraction models, achieves better performance compared to all the corresponding pipeline and threshold-based models.",7 Conclusion,[0],[0]
Employing such a nil-aware answer span extractor in practical IR-style QA tasks will be interesting future work.,7 Conclusion,[0],[0]
"Recently, there has been a surge of interest in reading comprehension-based (RC) question answering (QA).",abstractText,[0],[0]
"However, current approaches suffer from an impractical assumption that every question has a valid answer in the associated passage.",abstractText,[0],[0]
A practical QA system must possess the ability to determine whether a valid answer exists in a given text passage.,abstractText,[0],[0]
"In this paper, we focus on developing QA systems that can extract an answer for a question if and only if the associated passage contains an answer.",abstractText,[0],[0]
"If the associated passage does not contain any valid answer, the QA system will correctly return Nil.",abstractText,[0],[0]
We propose a nil-aware answer span extraction framework that is capable of returning Nil or a text span from the associated passage as an answer in a single step.,abstractText,[0],[0]
We show that our proposed framework can be easily integrated with several recently proposed QA models developed for reading comprehension and can be trained in an endto-end fashion.,abstractText,[0],[0]
Our proposed nil-aware answer extraction neural network decomposes pieces of evidence into relevant and irrelevant parts and then combines them to infer the existence of any answer.,abstractText,[0],[0]
Experiments on the NewsQA dataset show that the integration of our proposed framework significantly outperforms several strong baseline systems that use pipeline or threshold-based approaches.,abstractText,[0],[0]
A Nil-Aware Answer Extraction Framework for Question Answering,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 170–176 New Orleans, Louisiana, June 1 - 6, 2018. c©2017 Association for Computational Linguistics
As a specialized example of information extraction, part name extraction is an area that presents unique challenges. Part names are typically multiword terms longer than two words. There is little consistency in how terms are described in noisy free text, with variations spawned by typos, ad hoc abbreviations, acronyms, and incomplete names. This makes search and analyses of parts in these data extremely challenging. In this paper, we present our algorithm, PANDA (Part Name Discovery Analytics), based on a unique method that exploits statistical, linguistic and machine learning techniques to discover part names in noisy text such as that in manufacturing quality documentation, supply chain management records, service communication logs, and maintenance reports. Experiments show that PANDA is scalable and outperforms existing techniques significantly.",text,[0],[0]
"Part information plays a key role in manufacturing, maintenance, supplier management and customer support of any large complex system, such as an airplane, which may easily involve over 30,000 types of parts.",1 Introduction,[0],[0]
Parts can be described by part numbers or nomenclature.,1 Introduction,[0],[0]
"Furthermore, a given part serving the same function can often be supplied by multiple suppliers, who may use different part numbers and do not always use the same nomenclature to describe functionally equivalent parts.",1 Introduction,[0],[0]
"In addition, part names are very frequent in the service descriptions and notes written by mechanics and engineers around the world.",1 Introduction,[0],[0]
"Due to time constraints, working conditions (maintenance mechanics do not work in an office environment), time crunch, and job focus (primarily getting the aircraft
ready for on-time takeoff, not writing perfect English), compounded by the fact that many of those involved are not native speakers of English, the data often contains a high percentage of non-standard spellings and ad hoc shorthand notations and typos.",1 Introduction,[0],[0]
"Table 1 exemplifies these issues with real sample maintenance records.
",1 Introduction,[0],[0]
"In order to pinpoint types of issues involved in manufacturing, maintenance support, or supply chain management, it is crucial to identify the specific part involved.",1 Introduction,[0],[0]
"Importantly, a robust and scalable approach for extracting parts from text of the nature described above should never rely on simple matching from a list of predefined part names.",1 Introduction,[0],[0]
"It should also have a way of exploiting abundant free text data amassed over years.
",1 Introduction,[0],[0]
"While information extraction is a well-studied field, typically information extraction focuses on people, organization, time, location, event and their relationship.",1 Introduction,[0],[0]
Part name extraction is much less studied.,1 Introduction,[0],[0]
Part name extraction has the following unique properties.,1 Introduction,[0],[0]
"First, the language of part names as well as their context in these data sources is very domain-specific.",1 Introduction,[0],[0]
"This means that, not only is there nothing analogous to special word lists like people’s first and last names or city, state and country names, but general language resources like WorldNet (Miller, 1995) and Freebase (Freebase, 2018) are also virtually useless.",1 Introduction,[0],[0]
"In addition, part names are often longer than the names of people, organizations, or locations and can be as long as 5-
170
7 words (e.g. variable speed constant frequency cool fan).",1 Introduction,[0],[0]
"Furthermore, it is often the case that a substring Y (e.g. landing gear) of a part name X (e.g. main landing gear) is also a valid part name.",1 Introduction,[0],[0]
This creates a challenge for a system to identify X instead of Y as the part being mentioned.,1 Introduction,[0],[0]
"In addition, as noted above, the free text data containing the part names are often non-professionally authored and contain a high percentage of spelling variants (both ad hoc abbreviations and typos) and domain-specific acronyms.",1 Introduction,[0],[0]
"The spelling challenges do not just occur in part names, but occur throughout the free text, posing challenges for either traditional grammar-based parsing or n-gram approaches.
",1 Introduction,[0],[0]
"In this paper we present PANDA (Part Name Discovery Analytics), a fast and scalable method that exploits statistical, linguistic and supervised machine learning techniques in a unique way such that minimal human supervision is sufficient to discover thousands of part names from noisy text.",1 Introduction,[0],[0]
Basic information extraction methods typically rely on language models and hand-crafted rules.,2 Related Work,[0],[0]
"The n-gram approach derives common strings from a large corpus and, together with hand-crafted rules, makes for an easy-to-implement way of inferring entities (Chandramouli, Subramanian, & Bal, 2013).",2 Related Work,[0],[0]
A more sophisticated approach would define rules based on regular expressions over text content or their parts-of-speech tags to extract information.,2 Related Work,[0],[0]
"Noun-phrase identification is a typical approach under this category (Vilain and Day, 2000).",2 Related Work,[0],[0]
Rule-based and language model-based systems are very effective in cases when the entities of interest follow specific patterns.,2 Related Work,[0],[0]
"However, when the text is very noisy, generating hand-crafted rules and patterns is cost-prohibitive and not feasible.
",2 Related Work,[0],[0]
"To our knowledge, there is only one previously published work specifically focusing on part name extraction (Chandramouli, Subramanian, & Bal, 2013).",2 Related Work,[0],[0]
The authors propose an n-gram based approach which extracts part names from service logs.,2 Related Work,[0],[0]
"Given a list of basic part types (e.g. valve), they generate bigrams and trigrams ending with those part types and consider them as part candidates.",2 Related Work,[0],[0]
The candidates are ranked using a mutual information metric.,2 Related Work,[0],[0]
"Furthermore, the authors found that Part-of-Speech (POS) based filtering improved the quality of prediction.",2 Related Work,[0],[0]
"While this work is unsupervised and easy to implement, it has important
limitations.",2 Related Work,[0],[0]
"First, it cannot predict any new part types, because it relies on predefined part types.",2 Related Work,[0],[0]
"Therefore, any part types which are not already known will be missed.",2 Related Work,[0],[0]
"Secondly, their system cannot extract part names which have more than three tokens.",2 Related Work,[0],[0]
"In our data, part names consisting of more than three tokens occur frequently (i.e. left main landing gear, horizontal stabilizer trim actuator).",2 Related Work,[0],[0]
"Importantly, all n-gram based approaches suffer from the pervasive misspellings and abbreviations in noisy data.",2 Related Work,[0],[0]
"They may not be able to extract outflow vlv, trim act or door switche, as the respective part types valve, actuator and switch are misspelled or written in a non-standard way.
",2 Related Work,[0],[0]
"To enable more flexibility and more power in extracting entities, machine learning methods, especially supervised learning methods, have become a natural choice in modern day information extraction.",2 Related Work,[0],[0]
"Typical machine learning methods considered include Hidden Markov Models (HMM) (Skounakis, Craven and Ray, 2003; Freitag and McCallum, 1999), and Conditional Random Fields (CRFs) (McCallum, 2002).",2 Related Work,[0],[0]
The supervised systems learn a set of rules or models from the supplied hand-tagged samples for the training phase of machine learning.,2 Related Work,[0],[0]
"Once a new model is built based on training data, the model can be applied to new documents to extract entities.",2 Related Work,[0],[0]
"Rules and models learned by supervised techniques are effective for extracting information from the same genre of documents they are trained on, but they may perform poorly when applied to a different genre.",2 Related Work,[0],[0]
"In addition, acquiring the right training examples can be very expensive.",2 Related Work,[0],[0]
"Part information extraction is one such area which requires SME (Subject Matter Expert) knowledge and is thus not suited to crowd sourcing.
",2 Related Work,[0],[0]
"Recent approaches that address the scalability problem in training data associated with supervised machine learning include weakly-supervised methods (Pasca, 2007), bootstrapping techniques (Vilain and Day, 2000; Maedche, 2003), and active learning (Thompson, Callif and Mooney, 1999; Williams et al., 2015).",2 Related Work,[0],[0]
Active learning starts with bootstrap samples creating an initial model and uses that model to select the most informative examples in order to minimize the annotation cost required to generate training examples.,2 Related Work,[0],[0]
A new model is created with the new examples and the process continues until a stopping criteria is met.,2 Related Work,[0],[0]
"However, such iteration still requires SME involvement.",2 Related Work,[0],[0]
"When the free text data contain a high degree of
noise, and the number of parts involved is in the tens of thousands, it is not clear how fast an active learning approach will converge.",2 Related Work,[0],[0]
"Besides, SMEs are very expensive and reducing their efforts in part name extraction process, as do ne by our method PANDA, offers a huge cost saving for a company.
",2 Related Work,[0],[0]
"Studies show that open-ended and domain independent information extraction systems do not work well for domain-specific information extraction (Etzioni et al., 2004).",2 Related Work,[0],[0]
"As such, existing approaches can be expected to perform poorly for part extraction, a domain specific information extraction problem.",2 Related Work,[0],[0]
"To this end, we propose a system that is tuned to extract parts from the target natural language text (e.g. maintenance logs).",2 Related Work,[0],[0]
The proposed system is robust so that it can operate on noisy text and yet scalable as it demands very minimal supervision.,2 Related Work,[0],[0]
The intent of Part Name Discovery Analytics (PANDA) is primarily to extract part information in the domain of aircraft to support vehicle health management by exploiting hundreds of thousands of free text records.,3 Part Name Discovery Analytics,[0],[0]
"However, its use can extend to any large data sets containing part name mentions.",3 Part Name Discovery Analytics,[0],[0]
The primary design philosophy is to utilize machine learning capabilities and at the same time exploit linguistic knowledge of how part names are constructed in English.,3 Part Name Discovery Analytics,[0],[0]
This allows discovery of new parts and at the same time minimizes the expensive training process required for supervised machine learning.,3 Part Name Discovery Analytics,[0],[0]
Dealing with highly noisy data is a key requirement of this domain.,3 Part Name Discovery Analytics,[0],[0]
"Therefore, a non-learning based method would not meet our requirements.",3 Part Name Discovery Analytics,[0],[0]
"As we show later, PANDA learns to infer new part names from the noisy text.
",3 Part Name Discovery Analytics,[0],[0]
"We leverage the linguistic fact that the most important term in a multiword part name is the head noun (the “Head”), and in English, the Head is the last term in a multiword term.",3 Part Name Discovery Analytics,[0],[0]
"These Heads are terms such as panel, valve, switch etc.",3 Part Name Discovery Analytics,[0],[0]
"Although most people who are somewhat familiar with this domain can easily come up with 10-20 examples of these Heads, it is important to note that there is no knowledge base anywhere that contains all of them.",3 Part Name Discovery Analytics,[0],[0]
"By utilizing linguistic knowledge, we can automatically provide the most effective training examples to the machine learning algorithm, as well as greatly minimize SME review in providing crucial feedback for the machine learning process.
",3 Part Name Discovery Analytics,[0],[0]
"At a high level, PANDA cleverly shuttles between Heads and Heads plus modifiers, which are the full part names of interest.",3 Part Name Discovery Analytics,[0],[0]
The requirement for SME’s attention are focused on Heads.,3 Part Name Discovery Analytics,[0],[0]
"Since SME’s need only review Heads, and not the full parts associated with each Head, the training is highly efficient.
",3 Part Name Discovery Analytics,[0],[0]
Fig. 1 shows the architecture of PANDA.,3 Part Name Discovery Analytics,[0],[0]
"It consists of a loop which starts with seed Heads, a small set of basic part names such as gear, panel, switch, etc.",3 Part Name Discovery Analytics,[0],[0]
The collected Heads are used to predict the part names in the Extract Part Names step (Section 3.1).,3 Part Name Discovery Analytics,[0],[0]
The extracted part names are “purified” using several filtering mechanisms (Section 3.2).,3 Part Name Discovery Analytics,[0],[0]
"The purified parts are used to generate training examples (Section 3.3) for a CRF model (Section 3.4) which, in turn, is used to predict new part names in the data set (Section 3.5).",3 Part Name Discovery Analytics,[0],[0]
The predicted part names are again purified (Section 3.2) and new Heads are extracted (Section 3.6).,3 Part Name Discovery Analytics,[0],[0]
The extracted Heads themselves are also purified (Section 3.7).,3 Part Name Discovery Analytics,[0],[0]
"Finally, the purified Heads are added back to the earlier list forming a larger initial set of Heads.",3 Part Name Discovery Analytics,[0],[0]
The loop is repeated until a stopping criteria is met (Section 3.8).,3 Part Name Discovery Analytics,[0],[0]
Parts predicted by CRF and trie in the last run are collected as the final output.,3 Part Name Discovery Analytics,[0],[0]
The purpose of this step is to use part Heads and automatically generate complete part names that we need later to generate training examples for a machine learning model.,3.1 Trie-based Part Name Prediction,[0],[0]
"To do this, we construct a data structure called a trie (Trie, 2018) from a large corpus such that the first level nodes are the given part Heads (e.g. gear) and their descendant nodes are the tokens appearing before them in the data set (see Fig. 2 below).",3.1 Trie-based Part Name Prediction,[0],[0]
"This type of trie is computed by
scanning the tokens in reverse order, and is highly efficient.",3.1 Trie-based Part Name Prediction,[0],[0]
We then traverse the trie in depth-first fashion as long as it satisfies the minimum frequency criteria.,3.1 Trie-based Part Name Prediction,[0],[0]
We collect the potential part name sequence as we traverse.,3.1 Trie-based Part Name Prediction,[0],[0]
"To further ensure better examples are used in the training example generation process, we could have a constraint to only use the Heads having certain minimum frequency.",3.1 Trie-based Part Name Prediction,[0],[0]
"As is the case of all machine learning methods, a significant amount of bad training samples may negatively impact the resulting model.",3.2 Purify Part Names,[0],[0]
"Thus, to further improve the quality of part names predicted in the previous step, this step applies a number of heuristics based on POS features.",3.2 Purify Part Names,[0],[0]
"For instance, a part name must not start with a verb or an article.",3.2 Purify Part Names,[0],[0]
"If it does, we remove them and considered the remaining chunk as part name (e.g. replaced main landing gear becomes main landing gear).",3.2 Purify Part Names,[0],[0]
"Similarly, PANDA requires that all part name tokens must either be nouns or adjectives.",3.2 Purify Part Names,[0],[0]
The goal of this phase is to generate training data for a machine learning model by annotating the data in the corpus with the part names resulting from the previous steps.,3.3 Generate Training Examples,[0],[0]
"Since the goal is to leverage patterns in the part names and their context to discover new part names, additional features need to be provided.",3.3 Generate Training Examples,[0],[0]
PANDA currently employs k-previous and k-next word tokens and their POS tags as well as the POS tags of part names themselves as these features.,3.3 Generate Training Examples,[0],[0]
"The POS features can be generated using a POS Tagger such as Brill Tagger (Brill, 1992).",3.3 Generate Training Examples,[0],[0]
"Fig. 3 shows a sample annotated record with
the part name left main landing gear and corresponding POS-tags.",3.3 Generate Training Examples,[0],[0]
The goal of this phase is to use the annotated corpus as training data to generate a model that identifies part names in the data.,3.4 Train a Sequence Model,[0],[0]
"Any sequence model that extracts sequences of tokens, such as a CRF (Lafferty, McCallum and Pereira, 2001) or Long Short Term Memory network (Gers, Schmidhuber and Cummins, 1999), can be used at this phase.",3.4 Train a Sequence Model,[0],[0]
We use CRF in our experiments.,3.4 Train a Sequence Model,[0],[0]
The goal of this phase is to use the sequence model trained in Section 3.4 on the corpus to extract new potential part names.,3.5 Predict Part Names,[0],[0]
The newly predicted parts are collected and purified using the approach presented in Section 3.2.,3.5 Predict Part Names,[0],[0]
This step is to extract Heads from the newly identified potential part names in the machine learning output.,3.6 Extract Head Nouns,[0],[0]
It extracts the last token of the supplied part and returns that as the Head.,3.6 Extract Head Nouns,[0],[0]
"For instance, it returns cap for oil filter cap.",3.6 Extract Head Nouns,[0],[0]
The goal of this step is for PANDA to validate Heads generated in the previous phase.,3.7 Purify Head Nouns,[0],[0]
The feedback can be done with a human-in-the-loop (a SME).,3.7 Purify Head Nouns,[0],[0]
"The SME will review all generated Heads and classify them into different categories, typically Good (e.g. antenna) or Bad (e.g. inoperable).",3.7 Purify Head Nouns,[0],[0]
"Optionally, an additional category Borderline (e.g. unit) can be used.",3.7 Purify Head Nouns,[0],[0]
"However, only Good Heads are used in the next iteration of the loop to generate additional new part names.",3.7 Purify Head Nouns,[0],[0]
"Borderline heads will not be used to generate new part names for the training purpose, but will be accepted as potentially valid heads at the last run.",3.7 Purify Head Nouns,[0],[0]
PANDA supports various types of stopping criteria.,3.8 Stopping Criteria for the Loop,[0],[0]
"It can be stopped after a certain number of iterations or after a certain number of parts are generated, or after it reaches a certain ratio of bad vs. good new parts generated.",3.8 Stopping Criteria for the Loop,[0],[0]
The parts collected after the final run can be used to extract the part names in new incoming records.,3.9 Part Prediction using PANDA,[0],[0]
"Alternatively, the final CRF model can also be used to predict the parts.",3.9 Part Prediction using PANDA,[0],[0]
Or a combination of both of these can be used.,3.9 Part Prediction using PANDA,[0],[0]
"We conducted experiments using three major key data sources in the aerospace domain: (a) Maintenance Logbook (MLB) includes key maintenance condition, maintenance action and parts involved for issues identified on an aircraft.",4 Experiments and Results,[0],[0]
(b) Schedule Interruption (SI) includes records generated by dozens of major airlines at airports all over the world.,4 Experiments and Results,[0],[0]
"It contains reasons for significant delays in departure or landing, often due to the condition of one or more parts/systems.",4 Experiments and Results,[0],[0]
(c) Communication Systems (CS) includes professional help desk type correspondence between an aircraft manufacturing company and airline operators.,4 Experiments and Results,[0],[0]
MLB and SI are very noisy (as shown in Table 1) compared to CS.,4 Experiments and Results,[0],[0]
"All data sets contained multiple records, had comparable sizes of 1 million tokens each, and were subject to the same preprocessing steps and POS-tagging.
",4 Experiments and Results,[0],[0]
We ran PANDA on SI data set using 36 seed head nouns.,4 Experiments and Results,[0],[0]
"We set PANDA to identify full part names of length up to 5 tokens with minimum frequency threshold of 1, to capture maximum recall, and allowed it to run till no new good Head was generated.",4 Experiments and Results,[0],[0]
The results are presented in Table 2.,4 Experiments and Results,[0],[0]
PANDA stopped after iteration 7 generating 9374 parts.,4 Experiments and Results,[0],[0]
"SME’s feedback to predicted Heads in each iteration as Good, Bad and Borderline heads, defined in Section 3.7, are also presented in the table.",4 Experiments and Results,[0],[0]
"Starting from 36 initial Heads, PANDA was able to extract 382 (= 317 Good + 65 Borderline) new part Heads.",4 Experiments and Results,[0],[0]
This demonstrates PANDA’s ability to infer new part Heads which are not known initially.,4 Experiments and Results,[0],[0]
"This is crucial because all Heads are not known in advance and hundreds of new full parts may be associated with a single new Head.
",4 Experiments and Results,[0],[0]
Table 2 also shows the total number of parts collected up to a given iteration.,4 Experiments and Results,[0],[0]
It extracted 9374 full parts at the end of the final iteration but only required annotations of 780 part Heads.,4 Experiments and Results,[0],[0]
"Since the annotation task only involves annotating the Heads and not the full parts, the annotation is very fast.",4 Experiments and Results,[0],[0]
"As a reference, this whole experiment took less than 2 hours to complete.",4 Experiments and Results,[0],[0]
"This demonstrates the scalability of PANDA in that it requires minimal human
input in the training phase of machine learning.",4 Experiments and Results,[0],[0]
"Since previously annotated Heads can be reused in subsequent experiments, PANDA will run even faster in the later experiments.
",4 Experiments and Results,[0],[0]
Next we sought to evaluate the quality of full parts generated by PANDA.,4 Experiments and Results,[0],[0]
"However, no gold data set is currently available for that purpose.",4 Experiments and Results,[0],[0]
"Also, evaluation in terms of recall by annotating all parts is not feasible, as annotating all 9374 full parts would be very costly.",4 Experiments and Results,[0],[0]
"Therefore, we randomly selected 1000 parts for evaluation.",4 Experiments and Results,[0],[0]
PANDA scored 80.9% accuracy on this evaluation.,4 Experiments and Results,[0],[0]
"This clearly shows that, though a SME only provides feedback on Heads during the training process, PANDA is still able to extract full part names from noisy data with a high degree of accuracy.",4 Experiments and Results,[0],[0]
"As noted in Section 2, the only known algorithm in the literature to extract part names from free text is by Chandramouli, Subramanian, and Bal (2013).",4.1 PANDA VS Baseline,[0],[0]
It considers parts as n-grams ending at provided heads and ranks them by a collocation measure.,4.1 PANDA VS Baseline,[0],[0]
We implemented their best performing algorithm that purifies parts with POS tags as baseline.,4.1 PANDA VS Baseline,[0],[0]
"We ran both the baseline and PANDA to extract full part names of length up to 5 words from the SI data, with a minimum acceptable collocation value of 25 for the baseline.",4.1 PANDA VS Baseline,[0],[0]
"The results are shown in Table 3.
",4.1 PANDA VS Baseline,[0],[0]
"Since the baseline relies on provided Heads - 36 in this case - and has no way of inferring new part Heads and their corresponding parts, it suffers from
low recall.",4.1 PANDA VS Baseline,[0],[0]
"PANDA, on the other hand can easily infer new Heads and associated parts.",4.1 PANDA VS Baseline,[0],[0]
"For instance, although annunciator was not in the initial Head list, PANDA was able to infer it and its variants such as annunc, annuc, and ann.",4.1 PANDA VS Baseline,[0],[0]
"In addition, PANDA extracted 32 types of annunciators such as antiskid annunciator, door warn annunciator, and cabin zone temp annunc.
",4.1 PANDA VS Baseline,[0],[0]
"In addition, PANDA captured the longest part name possible (a very specific part ) while the baseline broke it down to its constituent chunks, creating parts that did not exist in the data set or were incorrect.",4.1 PANDA VS Baseline,[0],[0]
Baseline results contained 186 such over-generated parts.,4.1 PANDA VS Baseline,[0],[0]
"In one example, when access always preceded door panel, PANDA only generated access door panel.",4.1 PANDA VS Baseline,[0],[0]
"In contrast, the baseline generated door panel (a non-existence part) as well as access door panel.",4.1 PANDA VS Baseline,[0],[0]
"Since door panel can easily be derived from access door panel (a specific part), PANDA still is able to identify generic parts, if they exist, in new incoming records without generating the parts that do not exist in the current data set, which may lead to error.",4.1 PANDA VS Baseline,[0],[0]
"For instance, the constituent part one valve that the baseline generates from generator one valve is not by itself a valid part.
",4.1 PANDA VS Baseline,[0],[0]
"Lastly, the baseline generated incorrect parts when there were more than one head in a part name.",4.1 PANDA VS Baseline,[0],[0]
"It extracted temperature control valve, temperature control, and control valve from the record containing “temperature control valve” as they were n-grams ending at known heads control and valve.",4.1 PANDA VS Baseline,[0],[0]
"In fact, out of 979 baseline parts, 466 were common with PANDA and the rest were either over-generated or invalid parts.",4.1 PANDA VS Baseline,[0],[0]
"These facts clearly demonstrate PANDA’s superiority over the baseline model in terms of recall, learning ability for heads and parts, and accuracy of extracted parts.",4.1 PANDA VS Baseline,[0],[0]
"To test the generality of PANDA across different genres of part records, we ran PANDA on MLB, SI and BCS data sets for 5 iterations each.",4.2 PANDA on Diverse Data Sets,[0],[0]
"As noted above, MLB and SI are very noisy compared to BCS.",4.2 PANDA on Diverse Data Sets,[0],[0]
Each of these experiments needed less than 2 hours.,4.2 PANDA on Diverse Data Sets,[0],[0]
We report head annotation counts and total extracted parts in each of these data sets in Table 4.,4.2 PANDA on Diverse Data Sets,[0],[0]
The results show that PANDA can process data sets of different genres with minimal annotations and can extract thousands of complex part names from them.,4.2 PANDA on Diverse Data Sets,[0],[0]
"As expected, fewer parts were discovered in BCS than in SI and MLB since it consisted of email conversations with boilerplate texts.",4.2 PANDA on Diverse Data Sets,[0],[0]
We identified some error types that affected PANDA results.,4.3 Error Analysis,[0],[0]
"First, there are certain parts that PANDA could not correctly extract due to its assumption that the last word of a part is the head of the part.",4.3 Error Analysis,[0],[0]
"From the text “Replaced handle of door”, it could capture handle and door separately but not as door handle or handle of door.",4.3 Error Analysis,[0],[0]
"Such cases, however, were very rare.",4.3 Error Analysis,[0],[0]
"Second, POS-tagging errors affected some of PANDA’s predictions.",4.3 Error Analysis,[0],[0]
It captured report generator drive instead of generator drive due to report being incorrectly tagged as a noun.,4.3 Error Analysis,[0],[0]
"Third, a few parts were only partially captured due to the maximum part length setting.",4.3 Error Analysis,[0],[0]
For instance variable was missed in the 6-word part variable speed constant frequency cool fan.,4.3 Error Analysis,[0],[0]
"We presented PANDA, a novel approach that discovers part names in noisy text.",5 Conclusion and Future Work,[0],[0]
PANDA cleverly exploits the linguistic characteristics of part names in English to automatically generate full part names using basic part names.,5 Conclusion and Future Work,[0],[0]
"This automates the training example generation process, the most expensive step for building a supervised machine learning model.",5 Conclusion and Future Work,[0],[0]
"Experiments demonstrated that:
• PANDA required minimal human input for
training the machine learning model
• PANDA was superior to the existing sys-
tem in that it was able to infer new heads and parts and dramatically improved recall as compared to the existing system
• PANDA extracted high quality full parts • PANDA can scale across diverse data sets With these promising results, PANDA is currently being deployed to extract part names from several data sets for different aircraft models and subsystems.",5 Conclusion and Future Work,[0],[0]
"In the future, we plan to focus on the normalization of heads (e.g. pnl and panal to panel) and parts (e.g. lft valve and left vlv to left valve) from PANDA extracted results.",5 Conclusion and Future Work,[0],[0]
"As a specialized example of information extraction, part name extraction is an area that presents unique challenges.",abstractText,[0],[0]
Part names are typically multiword terms longer than two words.,abstractText,[0],[0]
"There is little consistency in how terms are described in noisy free text, with variations spawned by typos, ad hoc abbreviations, acronyms, and incomplete names.",abstractText,[0],[0]
This makes search and analyses of parts in these data extremely challenging.,abstractText,[0],[0]
"In this paper, we present our algorithm, PANDA (Part Name Discovery Analytics), based on a unique method that exploits statistical, linguistic and machine learning techniques to discover part names in noisy text such as that in manufacturing quality documentation, supply chain management records, service communication logs, and maintenance reports.",abstractText,[0],[0]
Experiments show that PANDA is scalable and outperforms existing techniques significantly.,abstractText,[0],[0]
A Novel Approach to Part Name Discovery in Noisy Text,title,[0],[0]
"Phrase-based translation models (Koehn et al., 2003; Och and Ney, 2004) are widely used in statistical machine translation.",1 Introduction,[0],[0]
"The decoding problem for phrase-based translation models is known to be difficult: the results from Knight (1999) imply that in the general case decoding of phrase-based translation models is NP-complete.
",1 Introduction,[0],[0]
The complexity of phrase-based decoding comes from reordering of phrases.,1 Introduction,[0],[0]
"In practice, however, various constraints on reordering are often imposed in phrase-based translation systems.",1 Introduction,[0],[0]
"A common constraint is a “distortion limit”, which places a hard constraint on how far phrases can move.",1 Introduction,[0],[0]
"The complexity of decoding with such a distortion limit is an open question: the NP-hardness result from Knight
∗On leave from Columbia University.
(1999) applies to a phrase-based model with no distortion limit.
",1 Introduction,[0],[0]
"This paper describes an algorithm for phrasebased decoding with a fixed distortion limit whose runtime is linear in the length of the sentence, and for a fixed distortion limit is polynomial in other factors.",1 Introduction,[0],[0]
"More specifically, for a hard distortion limit d, and sentence length n, the runtime is O(nd!lhd+1), where l is a bound on the number of phrases starting at any point in the sentence, and h is related to the maximum number of translations for any word in the source language sentence.
",1 Introduction,[0],[0]
"The algorithm builds on the insight that decoding with a hard distortion limit is related to the bandwidth-limited traveling salesman problem (BTSP) (Lawler et al., 1985).",1 Introduction,[0],[0]
The algorithm is easily amenable to beam search.,1 Introduction,[0],[0]
"It is quite different from previous methods for decoding of phrase-based models, potentially opening up a very different way of thinking about decoding algorithms for phrasebased models, or more generally for models in statistical NLP that involve reordering.",1 Introduction,[0],[0]
"Knight (1999) proves that decoding of word-to-word translation models is NP-complete, assuming that there is no hard limit on distortion, through a reduction from the traveling salesman problem.",2 Related Work,[0],[0]
"Phrasebased models are more general than word-to-word models, hence this result implies that phrase-based decoding with unlimited distortion is NP-complete.
",2 Related Work,[0],[0]
"Phrase-based systems can make use of both reordering constraints, which give a hard “distortion limit” on how far phrases can move, and reordering models, which give scores for reordering steps, often penalizing phrases that move long distances.",2 Related Work,[0],[0]
"Moses (Koehn et al., 2007b) makes use of a distortion limit, and a decoding algorithm that makes use
59
Transactions of the Association for Computational Linguistics, vol. 5, pp.",2 Related Work,[0],[0]
"59–71, 2017.",2 Related Work,[0],[0]
Action Editor: Holger Schwenk.,2 Related Work,[0],[0]
"Submission batch: 10/2016; Revision batch: 11/2016; Published 2/2017.
",2 Related Work,[0],[0]
c©2017 Association for Computational Linguistics.,2 Related Work,[0],[0]
"Distributed under a CC-BY 4.0 license.
of bit-strings representing which words have been translated.",2 Related Work,[0],[0]
"We show in Section 5.2 of this paper that this can lead to at least 2n/4 bit-strings for an input sentence of length n, hence an exhaustive version of this algorithm has worst-case runtime that is exponential in the sentence length.",2 Related Work,[0],[0]
"The current paper is concerned with decoding phrase-based models with a hard distortion limit.
",2 Related Work,[0],[0]
Various other reordering constraints have been considered.,2 Related Work,[0],[0]
"Zens and Ney (2003) and Zens et al. (2004) consider two types of hard constraints: the IBM constraints, and the ITG (inversion transduction grammar) constraints from the model of Wu (1997).",2 Related Work,[0],[0]
They give polynomial time dynamic programming algorithms for both of these cases.,2 Related Work,[0],[0]
It is important to note that the IBM and ITG constraints are different from the distortion limit constraint considered in the current paper.,2 Related Work,[0],[0]
"Decoding algorithms with ITG constraints are further studied by Feng et al. (2010) and Cherry et al. (2012).
",2 Related Work,[0],[0]
Kumar and Byrne (2005) describe a class of reordering constraints and models that can be encoded in finite state transducers.,2 Related Work,[0],[0]
"Lopez (2009) shows that several translation models can be represented as weighted deduction problems and analyzes their complexities.1 Koehn et al. (2003) describe a beamsearch algorithm for phrase-based decoding that is in widespread use; see Section 5 for discussion.
",2 Related Work,[0],[0]
"A number of reordering models have been proposed, see for example Tillmann (2004), Koehn et al. (2007a) and Galley and Manning (2008).
",2 Related Work,[0],[0]
"DeNero and Klein (2008) consider the phrase alignment problem, that is, the problem of finding an optimal phrase-based alignment for a sourcelanguage/target-language sentence pair.",2 Related Work,[0],[0]
"They show that in the general case, the phrase alignment problem is NP-hard.",2 Related Work,[0],[0]
"It may be possible to extend the techniques in the current paper to the phrasealignment problem with a hard distortion limit.
",2 Related Work,[0],[0]
Various methods for exact decoding of phrasebased translation models have been proposed.,2 Related Work,[0],[0]
"Zaslavskiy et al. (2009) describe the use of travel-
1An earlier version of this paper states the complexity of decoding with a distortion limit as O(I32d) where d is the distortion limit and I is the number of words in the sentence; however (personal communication from Adam Lopez) this runtime is an error, and should be O(2I) i.e., exponential time in the length of the sentence.",2 Related Work,[0],[0]
"A corrected version of the paper corrects this.
",2 Related Work,[0],[0]
ing salesman algorithms for phrase-based decoding.,2 Related Work,[0],[0]
Chang and Collins (2011) describe an exact method based on Lagrangian relaxation.,2 Related Work,[0],[0]
Aziz et al. (2014) describe a coarse-to-fine approach.,2 Related Work,[0],[0]
"These algorithms all have exponential time runtime (in the length of the sentence) in the worst case.
",2 Related Work,[0],[0]
Galley and Manning (2010) describe a decoding algorithm for phrase-based systems where phrases can have discontinuities in both the source and target languages.,2 Related Work,[0],[0]
"The algorithm has some similarities to the algorithm we propose: in particular, it makes use of a state representation that contains a list of disconnected phrases.",2 Related Work,[0],[0]
"However, the algorithms differ in several important ways: Galley and Manning (2010) make use of bit string coverage vectors, giving an exponential number of possible states; in contrast to our approach, the translations are not formed in strictly left-to-right ordering on the source side.",2 Related Work,[0],[0]
"This section first defines the bandwidth-limited traveling salesman problem, then describes a polynomial time dynamic programming algorithm for the traveling salesman path problem on bandwidth limited graphs.",3 Background: The Traveling Salesman Problem on Bandwidth-Limited Graphs,[0],[0]
"This algorithm is the algorithm proposed by Lawler et al. (1985)2 with small modifications to make the goal a path instead of a cycle, and to consider directed rather than undirected graphs.",3 Background: The Traveling Salesman Problem on Bandwidth-Limited Graphs,[0],[0]
"The input to the problem is a directed graph G = (V,E), where V is a set of vertices and E is a set of directed edges.",3.1 Bandwidth-Limited TSPPs,[0],[0]
"We assume that V = {1, 2, . . .",3.1 Bandwidth-Limited TSPPs,[0],[0]
", n}.",3.1 Bandwidth-Limited TSPPs,[0],[0]
"A directed edge is a pair (i, j) where i, j ∈ V , and i 6=",3.1 Bandwidth-Limited TSPPs,[0],[0]
"j. Each edge (i, j) ∈ E has an associated weight wi,j .",3.1 Bandwidth-Limited TSPPs,[0],[0]
"Given an integer k ≥ 1, a graph is bandwidth-limited with bandwidth k if
∀(i, j) ∈ E, |i− j| ≤ k",3.1 Bandwidth-Limited TSPPs,[0],[0]
"The traveling salesman path problem (TSPP) on
the graph G is defined as follows.",3.1 Bandwidth-Limited TSPPs,[0],[0]
We will assume that vertex 1 is the “source” vertex and vertex n is the “sink” vertex.,3.1 Bandwidth-Limited TSPPs,[0],[0]
"The TSPP is to find the minimum cost directed path from vertex 1 to vertex n, which passes through each vertex exactly once.
",3.1 Bandwidth-Limited TSPPs,[0],[0]
2The algorithm is based on the ideas of Monien and Sudborough (1981) and Ratliff and Rosenthal (1983).,3.1 Bandwidth-Limited TSPPs,[0],[0]
"The key idea of the dynamic-programming algorithm for TSPPs is the definition of equivalence classes corresponding to dynamic programming states, and an argument that the number of equivalence classes depends only on the bandwidth k.
The input to our algorithm will be a directed graph G = (V,E), with weights wi,j , and with bandwidth k.",3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
We define a 1-n path to be any path from the source vertex 1 to the sink vertex n that visits each vertex in the graph exactly once.,3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
"A 1-n path is a subgraph (V ′, E′)",3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
"ofG, where V ′ = V andE′ ⊆ E.
",3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
"We will make use of the following definition:
Definition 1.",3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
"For any 1-n path H , define Hj to be the subgraph that H induces on vertices 1, 2, . . .",3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
"j, where 1 ≤ j ≤ n.
",3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
"That is, Hj contains the vertices 1, 2, . . .",3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
"j and the edges in H between these vertices.
",3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
"For a given value for j, we divide the vertices V into three sets Aj , Bj and Cj :
• Aj = {1, 2, . . .",3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
", (j − k)}",3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
(Aj is the empty set if j ≤ k).,3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
•,3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
Bj = {1 . . .,3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
j} \Aj .3,3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
•,3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
"Cj = {j + 1, j + 2, . . .",3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
", n}
(Cj is the empty set if j = n).
",3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
Note that the vertices in subgraph Hj are the union of the sets Aj and Bj .,3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
"Aj is the empty set if j ≤ k, but",3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
"Bj is always non-empty.
",3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
"The following Lemma then applies:
Lemma 1.",3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
"For any 1-n path H in a graph with bandwidth k, for any 1 ≤ j ≤ n, the subgraph Hj has the following properties:
1.",3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
"If vertex 1 is in Aj , then vertex 1 has degree one.
2.",3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
For any vertex v ∈,3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
"Aj with v ≥ 2, vertex v has degree two.
3.",3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
"Hj contains no cycles.
",3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
Proof.,3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
The first and second properties are true because of the bandwidth limit.,3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
"Under the constraint of bandwidth k, any edge (u, v) in H such that
3For sets X and Y we use the notation X \ Y to refer to the set difference: i.e., X \ Y = {x|x ∈ X and x /∈ Y }.
",3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
"u ∈ Aj , must have v ∈ Aj ∪ Bj = Hj .",3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
This follows because if v ∈,3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
"Cj = {j + 1, j + 2, . . .",3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
n},3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
"and u ∈ Aj = {1, 2, . . .",3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
j,3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
"− k}, then |u − v| > k.",3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
"Similarly any edge (u, v) ∈",3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
H such that v ∈ Aj must have u ∈ Aj ∪ Bj = Hj .,3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
"It follows that for any vertex u ∈ Aj , with u > 1, there are edges (u, v) ∈",3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
"Hj and (v′, u) ∈",3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
"Hj , hence vertex u has degree 2.",3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
"For vertex u ∈ Aj with u = 1, there is an edge (u, v) ∈",3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
"Hj , hence vertex u has degree 1.",3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
"The third property (no cycles) is true because Hj is a subgraph of H , which has no cycles.
",3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
"It follows that each connected component of Hj is a directed path, that the start points of these paths are in the set {1} ∪ Bj , and that the end points of these paths are in the set Bj .
",3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
We now define an equivalence relation on subgraphs.,3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
"Two subgraphs Hj and H ′j are in the same equivalence class if the following conditions hold (taken from Lawler et al. (1985)):
1.",3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
For any vertex v ∈,3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
"Bj , the degree of v in Hj and H ′j is the same.",3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
2.,3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
"For each path (connected component) in Hj there is a path in H ′j with the same start and end points, and conversely.
",3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
The significance of this definition is as follows.,3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
"Assume that H∗ is an optimal 1-n path in the graph, and that it induces the subgraph Hj on vertices 1 . . .",3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
j. Assume that H ′j is another subgraph over vertices 1 . . .,3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
"j, which is in the same equivalence class as Hj .",3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
"For any subgraph Hj , define c(Hj) to be the sum of edge weights in Hj :
c(Hj) =",3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
"∑
(u,v)∈Hj wu,v
Then it must be the case",3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
that c(H ′j) ≥ c(Hj).,3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
"Otherwise, we could simply replace Hj by H ′j in H
∗, thereby deriving a new 1-n path with a lower cost, implying that H∗ is not optimal.
",3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
This observation underlies the dynamic programming approach.,3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
Define σ to be a function that maps a subgraph Hj to its equivalence class σ(Hj).,3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
"The equivalence class σ(Hj) is a data structure that stores the degrees of the vertices inBj , together with the start and end points of each connected component in Hj .
",3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
"Next, define ∆ to be a set of 0, 1 or 2 edges between vertex (j + 1) and the vertices in Bj .",3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
"For any subgraph Hj+1 of a 1-n path, there is some ∆, simply found by recording the edges incident to vertex (j + 1).",3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
"For any Hj , define τ(σ(Hj),∆) to be the equivalence class resulting from adding the edges in ∆ to the data structure σ(Hj).",3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
"If adding the edges in ∆ to σ(Hj) results in an ill-formed subgraph—for example, a subgraph that has one or more cycles— then τ(σ(Hj),∆) is undefined.",3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
"The following recurrence then defines the dynamic program (see Eq. 20 of Lawler et al. (1985)):
α(j + 1, S) =",3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
"min ∆,S′:τ(S′,∆)=S
( α(j, S′) + c(∆) )
",3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
Here S is an equivalence class over vertices {1 . . .,3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
"(j+1)}, and α(S, j+1) is the minimum score for any subgraph in equivalence class S.",3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
The min is taken over all equivalence classes S′ over vertices {1 . . .,3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
"j}, together with all possible values for ∆.",3.2 An Algorithm for Bandwidth-Limited TSPPs,[0],[0]
We now describe the dynamic programming algorithm for phrase-based decoding with a fixed distortion limit.,4 A Dynamic Programming Algorithm for Phrase-Based Decoding,[0],[0]
"We first give basic definitions for phrasebased decoding, and then describe the algorithm.",4 A Dynamic Programming Algorithm for Phrase-Based Decoding,[0],[0]
Consider decoding an input sentence consisting of words x1 . . .,4.1 Basic Definitions,[0],[0]
xn,4.1 Basic Definitions,[0],[0]
for some integer n.,4.1 Basic Definitions,[0],[0]
We assume that x1 = <s> and xn = </s> where <s> and </s> are the sentence start and end symbols respectively.,4.1 Basic Definitions,[0],[0]
"A phrase-based lexicon specifies a set of possible translations in the form of phrases p = (s, t, e), where s and t are integers such that 1 ≤ s ≤ t ≤ n, and e is a sequence of m ≥ 1 target-language words e1 . . .",4.1 Basic Definitions,[0],[0]
em.,4.1 Basic Definitions,[0],[0]
This signifies that words xs . . .,4.1 Basic Definitions,[0],[0]
xt,4.1 Basic Definitions,[0],[0]
in the source language have a translation as e1 . . .,4.1 Basic Definitions,[0],[0]
em in the target language.,4.1 Basic Definitions,[0],[0]
"We use s(p), t(p) and e(p) to refer to the three components of a phrase p = (s, t, e), and e1(p) . . .",4.1 Basic Definitions,[0],[0]
em(p) to refer to the words in the targetlanguage string e(p).,4.1 Basic Definitions,[0],[0]
"We assume that (1, 1,<s>) and (n, n,</s>) are the only translation entries with s(p) ≤ 1 and t(p) ≥ n respectively.
",4.1 Basic Definitions,[0],[0]
A derivation is then defined as follows: Definition 2 (Derivations).,4.1 Basic Definitions,[0],[0]
A derivation is a sequence of phrases p1 . . .,4.1 Basic Definitions,[0],[0]
"pL such that
• p1 = (1, 1,<s>) and pL =",4.1 Basic Definitions,[0],[0]
"(n, n,</s>).",4.1 Basic Definitions,[0],[0]
•,4.1 Basic Definitions,[0],[0]
Each source word is translated exactly once.,4.1 Basic Definitions,[0],[0]
•,4.1 Basic Definitions,[0],[0]
"The distortion limit is satisfied for each pair of phrases pi−1, pi, that is:
|t(pi−1)",4.1 Basic Definitions,[0],[0]
+ 1− s(pi)| ≤,4.1 Basic Definitions,[0],[0]
d ∀ i = 2 . . .,4.1 Basic Definitions,[0],[0]
"L.
where d is an integer specifying the distortion limit in the model.
",4.1 Basic Definitions,[0],[0]
Given a derivation p1 . . .,4.1 Basic Definitions,[0],[0]
"pL, a target-language translation can be obtained by concatenating the target-language strings e(p1) . . .",4.1 Basic Definitions,[0],[0]
"e(pL).
",4.1 Basic Definitions,[0],[0]
"The scoring function is defined as follows:
f(p1 . . .",4.1 Basic Definitions,[0],[0]
pL),4.1 Basic Definitions,[0],[0]
= λ(e(p1) . .,4.1 Basic Definitions,[0],[0]
.,4.1 Basic Definitions,[0],[0]
e(pL)),4.1 Basic Definitions,[0],[0]
"+
L∑
i=1
κ(pi)
+ L∑
i=2
η × |t(pi−1)",4.1 Basic Definitions,[0],[0]
"+ 1− s(pi)| (1)
",4.1 Basic Definitions,[0],[0]
"For each phrase p, κ(p) is the translation score for the phrase.",4.1 Basic Definitions,[0],[0]
"The parameter η is the distortion penalty, which is typically a negative constant.",4.1 Basic Definitions,[0],[0]
"λ(e) is a language model score for the string e. We will assume a bigram language model:
λ(e1 . . .",4.1 Basic Definitions,[0],[0]
em),4.1 Basic Definitions,[0],[0]
=,4.1 Basic Definitions,[0],[0]
"m∑
i=2
λ(ei|ei−1).
",4.1 Basic Definitions,[0],[0]
"The generalization of our algorithm to higher-order n-gram language models is straightforward.
",4.1 Basic Definitions,[0],[0]
"The goal of phrase-based decoding is to find y∗ = arg maxy∈Y f(y) where Y is the set of valid derivations for the input sentence.
",4.1 Basic Definitions,[0],[0]
"Remark (gap constraint): Note that a common restriction used in phrase-based decoding (Koehn et al., 2003; Chang and Collins, 2011), is to impose an additional “gap constraint” while decoding.",4.1 Basic Definitions,[0],[0]
See Chang and Collins (2011) for a description.,4.1 Basic Definitions,[0],[0]
"In this case it is impossible to have a dynamicprogramming state where word xi has not been translated, and where word xi+k has been translated, for k > d.",4.1 Basic Definitions,[0],[0]
"This limits distortions further, and it can be shown in this case that the number of possible bitstrings is O(2d) where d is the distortion limit.",4.1 Basic Definitions,[0],[0]
"Without this constraint the algorithm of Koehn et al. (2003) actually fails to produce translations for many input sentences (Chang and Collins, 2011).",4.1 Basic Definitions,[0],[0]
We now describe the dynamic programming algorithm.,4.2 The Algorithm,[0],[0]
Intuitively the algorithm builds a derivation by processing the source-language sentence in strictly left-to-right order.,4.2 The Algorithm,[0],[0]
"This is in contrast with the algorithm of Koehn et al. (2007b), where the targetlanguage sentence is constructed from left to right.
",4.2 The Algorithm,[0],[0]
"Throughout this section we will use π, or πi for some integer i, to refer to a sequence of phrases:
π = 〈 p1 . . .",4.2 The Algorithm,[0],[0]
"pl 〉
where each phrase pi = (s(pi), t(pi), e(pi)), as defined in the previous section.
",4.2 The Algorithm,[0],[0]
"We overload the s, t and e operators, so that if π = 〈 p1 . . .",4.2 The Algorithm,[0],[0]
"pl 〉 , we have s(π) = s(p1), t(π) = t(pl), and e(π) = e(p1) ·e(p2) . . .",4.2 The Algorithm,[0],[0]
"·e(pl), where x ·y is the concatenation of strings x and y.
A derivation H consists of a single phrase sequence π = 〈 p1 . . .",4.2 The Algorithm,[0],[0]
"pL 〉 :
H = π = 〈 p1 . . .",4.2 The Algorithm,[0],[0]
"pL 〉
where the sequence p1 . . .",4.2 The Algorithm,[0],[0]
"pL satisfies the constraints in definition 2.
",4.2 The Algorithm,[0],[0]
"We now give a definition of sub-derivations and complement sub-derivations:
Definition 3 (Sub-derivations and Complement Sub-derivations).",4.2 The Algorithm,[0],[0]
For any H = 〈 p1 . . .,4.2 The Algorithm,[0],[0]
"pL 〉 , for any j ∈ {1 . . .",4.2 The Algorithm,[0],[0]
n} such that ∃ i ∈ {1 . . .,4.2 The Algorithm,[0],[0]
L} s.t. t(pi),4.2 The Algorithm,[0],[0]
"= j, the sub-derivation Hj and the complement subderivation H̄j are defined as
Hj =〈π1 . . .",4.2 The Algorithm,[0],[0]
"πr〉, H̄j = 〈π̄1 . . .",4.2 The Algorithm,[0],[0]
"π̄r〉
where the following properties hold: • r is an integer with r ≥ 1.",4.2 The Algorithm,[0],[0]
•,4.2 The Algorithm,[0],[0]
Each πi for i = 1 . . .,4.2 The Algorithm,[0],[0]
"r is a sequence of one or more phrases, where each phrase p ∈ πi has t(p) ≤",4.2 The Algorithm,[0],[0]
"j.
•",4.2 The Algorithm,[0],[0]
Each π̄i for i = 1 . . .,4.2 The Algorithm,[0],[0]
"(r−1) is a sequence of one or more phrases, where each phrase p ∈ π̄i has s(p)",4.2 The Algorithm,[0],[0]
>,4.2 The Algorithm,[0],[0]
"j.
• π̄r is a sequence of zero or more phrases, where each phrase p ∈ π̄r has s(p)",4.2 The Algorithm,[0],[0]
"> j. We have zero phrases in π̄r iff j = n where n is the length of the sentence.
",4.2 The Algorithm,[0],[0]
"• Finally, π1 · π̄1 · π2 · π̄2 . . .",4.2 The Algorithm,[0],[0]
πr ·,4.2 The Algorithm,[0],[0]
π̄r = p1 . . .,4.2 The Algorithm,[0],[0]
pL,4.2 The Algorithm,[0],[0]
where x · y denotes the concatenation of phrase sequences x and y. Note that for any j ∈ {1 . . .,4.2 The Algorithm,[0],[0]
n} such that @i ∈ {1 . . .,4.2 The Algorithm,[0],[0]
"L} such that t(pi) = j, the sub-derivation Hj and the complement sub-derivation H̄j is not defined.
",4.2 The Algorithm,[0],[0]
"Thus for each integer j such that there is a phrase in H ending at point j, we can divide the phrases in H into two sets: phrases p with t(p) ≤ j, and phrases pwith s(p)",4.2 The Algorithm,[0],[0]
>,4.2 The Algorithm,[0],[0]
j. The sub-derivationHj lists all maximal sub-sequences of phrases with t(p) ≤,4.2 The Algorithm,[0],[0]
j.,4.2 The Algorithm,[0],[0]
The complement sub-derivation H̄j lists all maximal sub-sequences of phrases with s(p),4.2 The Algorithm,[0],[0]
"> j.
Figure 1 gives all sub-derivations Hj for the derivation
H = 〈〈 p1 . . .",4.2 The Algorithm,[0],[0]
"p7 〉〉
= 〈〈 (1, 1,<s>)(2, 3,we must)(4, 4, also)
(8, 8, take)(5, 6, these criticisms) (7, 7, seriously)(9, 9,</s>)",4.2 The Algorithm,[0],[0]
"〉〉
As one example, the sub-derivation H7 = 〈π1, π2〉 induced by H has two phrase sequences:
π1 = 〈 (1, 1,<s>)(2, 3,we must)(4, 4, also) 〉 π2 = 〈 (5, 6, these criticisms)(7, 7, seriously) 〉
Note that the phrase sequences π1 and π2 give translations for all words x1 . . .",4.2 The Algorithm,[0],[0]
x7 in the sentence.,4.2 The Algorithm,[0],[0]
"There
are two disjoint phrase sequences because in the full derivation H , the phrase p = (8, 8, take), with t(p) = 8 > 7, is used to form a longer sequence of phrases π1 p π2.
",4.2 The Algorithm,[0],[0]
"For the above example, the complement subderivation H̄7 is as follows:
π̄1 = 〈 (8, 8, take) 〉 π̄2 = 〈 (9, 9,</s>) 〉
It can be verified that π1 · π̄1 ·π2 · π̄2 = H as required by the definition of sub-derivations and complement sub-derivations.
",4.2 The Algorithm,[0],[0]
We now state the following Lemma: Lemma 2.,4.2 The Algorithm,[0],[0]
For any derivation H = p1 . . .,4.2 The Algorithm,[0],[0]
"pL, for any j such that ∃i such that t(pi)",4.2 The Algorithm,[0],[0]
"= j, the subderivation Hj = 〈π1 . . .",4.2 The Algorithm,[0],[0]
"πr〉 satisfies the following properties:
1. s(π1) = 1 and e1(π1) = <s>.",4.2 The Algorithm,[0],[0]
2.,4.2 The Algorithm,[0],[0]
For all positions i ∈ {1 . . .,4.2 The Algorithm,[0],[0]
"j}, there exists a
phrase p ∈ π, for some phrase sequence π ∈",4.2 The Algorithm,[0],[0]
"Hj , such that s(p) ≤",4.2 The Algorithm,[0],[0]
i ≤ t(p).,4.2 The Algorithm,[0],[0]
3.,4.2 The Algorithm,[0],[0]
For all i = 2 . . .,4.2 The Algorithm,[0],[0]
"r, s(πi) ∈ {(j − d+ 2) . . .",4.2 The Algorithm,[0],[0]
j} 4.,4.2 The Algorithm,[0],[0]
For all i = 1 . . .,4.2 The Algorithm,[0],[0]
"r, t(πi) ∈ {(j − d) . . .",4.2 The Algorithm,[0],[0]
"j}
Here d is again the distortion limit.
",4.2 The Algorithm,[0],[0]
This lemma is a close analogy of Lemma 1.,4.2 The Algorithm,[0],[0]
"The proof is as follows: Proof of Property 1: For all values of j, the phrase p1 = (1, 1,<s>) has t(p1) ≤ j, hence we must have π1 = p1 . . .",4.2 The Algorithm,[0],[0]
pk for some k ∈ {1 . . .,4.2 The Algorithm,[0],[0]
L}.,4.2 The Algorithm,[0],[0]
It follows that s(π1) = 1 and e1(π1) = <s>.,4.2 The Algorithm,[0],[0]
Proof of Property 2: For any position i ∈ {1 . . .,4.2 The Algorithm,[0],[0]
"j}, define the phrase (s, t, e) in the derivation H to be the phrase that covers word i; i.e., the phrase such that s ≤",4.2 The Algorithm,[0],[0]
i ≤ t.,4.2 The Algorithm,[0],[0]
We must have s ∈ {1 . . .,4.2 The Algorithm,[0],[0]
"j}, because s ≤",4.2 The Algorithm,[0],[0]
i,4.2 The Algorithm,[0],[0]
and i ≤ j.,4.2 The Algorithm,[0],[0]
We must also have t ∈ {1 . . .,4.2 The Algorithm,[0],[0]
"j}, because otherwise we have s ≤ j < t, which contradicts the assumption that there is some i ∈ {1 . . .",4.2 The Algorithm,[0],[0]
L} such that t(pi),4.2 The Algorithm,[0],[0]
"= j. It follows that the phrase (s, t, e) has t ≤ j, and from the definition of sub-derivations it follows that the phrase is in one of the phrase sequences π1 . . .",4.2 The Algorithm,[0],[0]
πr.,4.2 The Algorithm,[0],[0]
Proof of Property 3:,4.2 The Algorithm,[0],[0]
This follows from the distortion limit.,4.2 The Algorithm,[0],[0]
Consider the complement sub-derivation H̄j = 〈π̄1 . . .,4.2 The Algorithm,[0],[0]
π̄r〉.,4.2 The Algorithm,[0],[0]
"For the distortion limit to be satisfied, for all i ∈ {2 . . .",4.2 The Algorithm,[0],[0]
"r}, we must have
|t(π̄i−1)",4.2 The Algorithm,[0],[0]
+ 1− s(πi)| ≤,4.2 The Algorithm,[0],[0]
"d
We must also have t(π̄i−1)",4.2 The Algorithm,[0],[0]
"> j, and s(πi) ≤",4.2 The Algorithm,[0],[0]
"j, by the definition of sub-derivations.",4.2 The Algorithm,[0],[0]
It follows that s(πi) ∈ {(j − d+ 2) . . .,4.2 The Algorithm,[0],[0]
j}.,4.2 The Algorithm,[0],[0]
Proof of Property 4:,4.2 The Algorithm,[0],[0]
This follows from the distortion limit.,4.2 The Algorithm,[0],[0]
First consider the case where π̄r is non-empty.,4.2 The Algorithm,[0],[0]
"For the distortion limit to be satisfied, for all i ∈ {1 . . .",4.2 The Algorithm,[0],[0]
"r}, we must have
|t(πi)",4.2 The Algorithm,[0],[0]
"+ 1− s(π̄i)| ≤ d
We must also have t(πi) ≤ j, and s(π̄i)",4.2 The Algorithm,[0],[0]
"> j, by the definition of sub-derivations.",4.2 The Algorithm,[0],[0]
It follows that t(πi) ∈ {(j − d) . . .,4.2 The Algorithm,[0],[0]
"j}.
",4.2 The Algorithm,[0],[0]
Next consider the case where π̄r is empty.,4.2 The Algorithm,[0],[0]
In this case we must have j = n.,4.2 The Algorithm,[0],[0]
"For the distortion limit to be satisfied, for all i ∈ {1 . . .",4.2 The Algorithm,[0],[0]
"(r−1)}, we must have
|t(πi)",4.2 The Algorithm,[0],[0]
"+ 1− s(π̄i)| ≤ d
We must also have t(πi) ≤ j, and s(π̄i)",4.2 The Algorithm,[0],[0]
"> j, by the definition of sub-derivations.",4.2 The Algorithm,[0],[0]
It follows that t(πi) ∈ {(j−d) . . .,4.2 The Algorithm,[0],[0]
j} for i ∈ {1 . . .,4.2 The Algorithm,[0],[0]
(r−1)}.,4.2 The Algorithm,[0],[0]
"For i = r, we must have t(πi) = n, from which it again follows that t(πr)",4.2 The Algorithm,[0],[0]
= n ∈,4.2 The Algorithm,[0],[0]
{,4.2 The Algorithm,[0],[0]
(j − d) . .,4.2 The Algorithm,[0],[0]
.,4.2 The Algorithm,[0],[0]
"j}.
",4.2 The Algorithm,[0],[0]
"We now define an equivalence relation between sub-derivations, which will be central to the dynamic programming algorithm.",4.2 The Algorithm,[0],[0]
We define a function σ that maps a phrase sequence π to its signature.,4.2 The Algorithm,[0],[0]
"The signature is a four-tuple:
σ(π) =",4.2 The Algorithm,[0],[0]
"(s, ws, t, wt).
where s is the start position, ws is the start word, t is the end position and wt is the end word of the phrase sequence.",4.2 The Algorithm,[0],[0]
"We will use s(σ), ws(σ), t(σ), and wt(σ) to refer to each component of a signature σ.
",4.2 The Algorithm,[0],[0]
"For example, given a phrase sequence
π = 〈 (1, 1, <s>) (2, 2, we) (4, 4, also) 〉 ,
its signature is σ(π) = (1, <s>, 4, also).",4.2 The Algorithm,[0],[0]
The signature of a sub-derivation Hj = 〈π1 . . .,4.2 The Algorithm,[0],[0]
"πr〉 is defined to be
σ(Hj) = 〈σ(π1) . . .",4.2 The Algorithm,[0],[0]
"σ(πr)〉.
",4.2 The Algorithm,[0],[0]
"For example, with H7 as defined above, we have σ(H7) = 〈( 1,<s>, 4, also ) , ( 5, these, 7, seriously )〉
Two partial derivationsHj andH ′j are in the same equivalence class iff σ(Hj) = σ(H ′j).
",4.2 The Algorithm,[0],[0]
"We can now state the following Lemma:
Lemma 3.",4.2 The Algorithm,[0],[0]
"Define H∗ to be the optimal derivation for some input sentence, and H∗j to be a subderivation of H∗. Suppose H ′j is another subderivation with j words, such that σ(H ′j) = σ(H ∗ j ).",4.2 The Algorithm,[0],[0]
Then it must be the case that f(H∗j ),4.2 The Algorithm,[0],[0]
"≥ f(H ′j), where f is the function defined in Section 4.1.
",4.2 The Algorithm,[0],[0]
Proof.,4.2 The Algorithm,[0],[0]
"Define the sub-derivation and complement sub-derivation of H∗ as
H∗j = 〈π1 . . .",4.2 The Algorithm,[0],[0]
πr〉 H̄∗j = 〈π̄1 . . .,4.2 The Algorithm,[0],[0]
"π̄r〉
We then have
f(H∗) = f(H∗j )",4.2 The Algorithm,[0],[0]
+ f(H̄ ∗ j ) +,4.2 The Algorithm,[0],[0]
"γ (2)
where f(. . .) is as defined in Eq. 1, and γ takes into account the bigram language modeling scores and the distortion scores for the transitions π1 → π̄1, π̄1 → π2, π2 → π̄2, etc.
",4.2 The Algorithm,[0],[0]
The proof is by contradiction.,4.2 The Algorithm,[0],[0]
"Define
H ′j = π ′ 1 . . .",4.2 The Algorithm,[0],[0]
π ′,4.2 The Algorithm,[0],[0]
"r
and assume that f(H∗j )",4.2 The Algorithm,[0],[0]
< f(H ′ j).,4.2 The Algorithm,[0],[0]
"Now consider
H ′ = π′1π̄1π ′",4.2 The Algorithm,[0],[0]
2π̄2 . . .,4.2 The Algorithm,[0],[0]
π ′,4.2 The Algorithm,[0],[0]
"rπ̄r
This is a valid derivation because the transitions π′1 → π̄1, π̄1 → π′2, π′2 → π̄2 have the same distortion distances as π1 → π̄1, π̄1 → π2, π2 → π̄2, hence they must satisfy the distortion limit.
",4.2 The Algorithm,[0],[0]
"We have
f(H ′) = f(H ′j)",4.2 The Algorithm,[0],[0]
+ f(H̄ ∗ j ) +,4.2 The Algorithm,[0],[0]
"γ (3)
where γ has the same value as in Eq. 2.",4.2 The Algorithm,[0],[0]
"This follows because the scores for the transitions π′1 → π̄1, π̄1 → π′2, π′2 → π̄2 are identical to the scores for the transitions π1 → π̄1, π̄1",4.2 The Algorithm,[0],[0]
"→ π2, π2 → π̄2, because σ(H∗j )",4.2 The Algorithm,[0],[0]
"= σ(H ′ j).
",4.2 The Algorithm,[0],[0]
It follows from Eq. 2 and Eq. 3,4.2 The Algorithm,[0],[0]
"that if f(H ′j) > f(H∗j ), then f(H
′) >",4.2 The Algorithm,[0],[0]
f(H∗).,4.2 The Algorithm,[0],[0]
But this contradicts the assumption that H∗ is optimal.,4.2 The Algorithm,[0],[0]
"It follows that we must have f(H ′j) ≤ f(H∗j ).
",4.2 The Algorithm,[0],[0]
This lemma leads to a dynamic programming algorithm.,4.2 The Algorithm,[0],[0]
Each dynamic programming state consists of an integer j ∈ {1 . . .,4.2 The Algorithm,[0],[0]
n},4.2 The Algorithm,[0],[0]
"and a set of r signatures:
T = (j, {σ1 . . .",4.2 The Algorithm,[0],[0]
"σr})
Figure 2 shows the dynamic programming algorithm.",4.2 The Algorithm,[0],[0]
"It relies on the following functions:
•",4.2 The Algorithm,[0],[0]
"For any state T , δ(T ) is the set of outgoing transitions from state T .
",4.2 The Algorithm,[0],[0]
•,4.2 The Algorithm,[0],[0]
"For any state T , for any transition ∆ ∈ δ(T ), τ(T,∆) is the state reached by transition ∆ from state T .
",4.2 The Algorithm,[0],[0]
•,4.2 The Algorithm,[0],[0]
"For any state T , valid(T ) checks if a resulting state is valid.
",4.2 The Algorithm,[0],[0]
"• For any transition ∆, score(∆) is the score for the transition.
",4.2 The Algorithm,[0],[0]
We next give full definitions of these functions.,4.2 The Algorithm,[0],[0]
"Recall that for any state T , δ(T ) returns the set
of possible transitions from state T .","4.2.1 Definitions of δ(T ) and τ(T,∆)",[0],[0]
"In addition τ(T,∆) returns the state reached when taking transition ∆ ∈ δ(T ).
","4.2.1 Definitions of δ(T ) and τ(T,∆)",[0],[0]
"Given the state T = (j, {σ1 . . .","4.2.1 Definitions of δ(T ) and τ(T,∆)",[0],[0]
"σr}), each transition is of the form ψ1 pψ2 where ψ1, p and ψ2 are defined as follows:
• p is a phrase such that s(p)","4.2.1 Definitions of δ(T ) and τ(T,∆)",[0],[0]
= j + 1.,"4.2.1 Definitions of δ(T ) and τ(T,∆)",[0],[0]
• ψ1 ∈ {σ1 . . .,"4.2.1 Definitions of δ(T ) and τ(T,∆)",[0],[0]
σr} ∪ {φ}.,"4.2.1 Definitions of δ(T ) and τ(T,∆)",[0],[0]
"If ψ1 6= φ, it must be the
case that |t(ψ1)","4.2.1 Definitions of δ(T ) and τ(T,∆)",[0],[0]
+ 1− s(p)| ≤ d and t(ψ1) 6=,"4.2.1 Definitions of δ(T ) and τ(T,∆)",[0],[0]
n. • ψ2 ∈ {σ1 . . .,"4.2.1 Definitions of δ(T ) and τ(T,∆)",[0],[0]
σr} ∪ {φ}.,"4.2.1 Definitions of δ(T ) and τ(T,∆)",[0],[0]
"If ψ2 6= φ, it must be the
case that |t(p) + 1− s(ψ2)| ≤ d and s(ψ2) 6= 1.","4.2.1 Definitions of δ(T ) and τ(T,∆)",[0],[0]
•,"4.2.1 Definitions of δ(T ) and τ(T,∆)",[0],[0]
"If ψ1 6= φ and ψ2 6= φ, then ψ1 6= ψ2.","4.2.1 Definitions of δ(T ) and τ(T,∆)",[0],[0]
"Thus there are four possible types of transition from a state T = (j, {σ1 . . .","4.2.1 Definitions of δ(T ) and τ(T,∆)",[0],[0]
σr}): Case 1: ∆ = φ pφ.,"4.2.1 Definitions of δ(T ) and τ(T,∆)",[0],[0]
In this case the phrase p is incorporated as a stand-alone phrase.,"4.2.1 Definitions of δ(T ) and τ(T,∆)",[0],[0]
"The new state T ′ is equal to (j′, {σ′1 . . .","4.2.1 Definitions of δ(T ) and τ(T,∆)",[0],[0]
σ′r+1}),"4.2.1 Definitions of δ(T ) and τ(T,∆)",[0],[0]
"where j′ = t(p), where σ′i = σi for i = 1 .","4.2.1 Definitions of δ(T ) and τ(T,∆)",[0],[0]
. .,"4.2.1 Definitions of δ(T ) and τ(T,∆)",[0],[0]
"r, and σ ′ r+1 = (s(p), e1(p), t(p), em(p)).
","4.2.1 Definitions of δ(T ) and τ(T,∆)",[0],[0]
Case 2: ∆ = σi p φ for some σi ∈ {σ1 . . .,"4.2.1 Definitions of δ(T ) and τ(T,∆)",[0],[0]
σr}.,"4.2.1 Definitions of δ(T ) and τ(T,∆)",[0],[0]
In this case the phrase p is appended to the signature σi.,"4.2.1 Definitions of δ(T ) and τ(T,∆)",[0],[0]
"The new state T ′ = τ(T,∆) is of the form (j′, σ′1 . . .","4.2.1 Definitions of δ(T ) and τ(T,∆)",[0],[0]
σ ′,"4.2.1 Definitions of δ(T ) and τ(T,∆)",[0],[0]
"r), where j
′ = t(p), where σi is replaced by (s(σi), ws(σi), t(p), em(p)), and where σ′i′ = σi′ for all i′ 6=","4.2.1 Definitions of δ(T ) and τ(T,∆)",[0],[0]
i. Case 3: ∆ = φ,"4.2.1 Definitions of δ(T ) and τ(T,∆)",[0],[0]
pσi for some σi ∈ {σ1 . . .,"4.2.1 Definitions of δ(T ) and τ(T,∆)",[0],[0]
σr}.,"4.2.1 Definitions of δ(T ) and τ(T,∆)",[0],[0]
In this case the phrase p is prepended to the signature σi.,"4.2.1 Definitions of δ(T ) and τ(T,∆)",[0],[0]
"The new state T ′ = τ(T,∆) is of the form (j′, σ′1 . . .","4.2.1 Definitions of δ(T ) and τ(T,∆)",[0],[0]
σ ′,"4.2.1 Definitions of δ(T ) and τ(T,∆)",[0],[0]
"r), where j
′ = t(p), where σi is replaced by (s(p), e1(p), t(σi), wt(σi)), and where σ′i′ = σi′ for all i′ 6=","4.2.1 Definitions of δ(T ) and τ(T,∆)",[0],[0]
"i. Case 4: ∆ = σi p σi′ for some σi, σi′ ∈ {σ1 . . .","4.2.1 Definitions of δ(T ) and τ(T,∆)",[0],[0]
"σr}, with i′ 6= i.","4.2.1 Definitions of δ(T ) and τ(T,∆)",[0],[0]
"In this case phrase p is appended to signature σi, and prepended to signature σi′ , effectively joining the two signatures together.","4.2.1 Definitions of δ(T ) and τ(T,∆)",[0],[0]
"In this case the new state T ′ = τ(T,∆) is of the form (j′, σ′1 . . .","4.2.1 Definitions of δ(T ) and τ(T,∆)",[0],[0]
σ ′,"4.2.1 Definitions of δ(T ) and τ(T,∆)",[0],[0]
"r−1), where signatures σi and σi′ are replaced by a new signature (s(σi), ws(σi), t(σi′), wt(σi′)), and all other signatures are copied across from T to T ′.
Figure 3 gives the dynamic programming states and transitions for the derivation H in Figure 1.","4.2.1 Definitions of δ(T ) and τ(T,∆)",[0],[0]
"For example, the sub-derivation
H7 = 〈〈 (1, 1,<s>)(2, 3,we must)(4, 4, also) 〉 ,
〈 (5, 6, these criticisms)(7, 7, seriously)
","4.2.1 Definitions of δ(T ) and τ(T,∆)",[0],[0]
"〉〉
will be mapped to a state
T = ( 7, σ(H7) )
=","4.2.1 Definitions of δ(T ) and τ(T,∆)",[0],[0]
"( 7, { (1,<s>, 4, also), (5, these, 7, seriously) })
","4.2.1 Definitions of δ(T ) and τ(T,∆)",[0],[0]
"The transition σ1(8, 8, take)σ2 from this state leads to a new state,
T ′","4.2.1 Definitions of δ(T ) and τ(T,∆)",[0],[0]
"= ( 8, { σ1 =","4.2.1 Definitions of δ(T ) and τ(T,∆)",[0],[0]
"(1,<s>, 7, seriously) })","4.2.1 Definitions of δ(T ) and τ(T,∆)",[0],[0]
"Figure 4 gives the definition of score(∆), which incorporates the language model, phrase scores, and distortion penalty implied by the transition ∆.
4.4 Definition of valid(T )
",4.3 Definition of score(∆),[0],[0]
Figure 5 gives the definition of valid(T ).,4.3 Definition of score(∆),[0],[0]
This function checks that the start and end points of each signature are in the set of allowed start and end points given in Lemma 2.,4.3 Definition of score(∆),[0],[0]
We now give a bound on the algorithm’s run time.,4.5 A Bound on the Runtime of the Algorithm,[0],[0]
"This will be the product of terms N and M , where N is an upper bound on the number of states in the dynamic program, and M is an upper bound on the number of outgoing transitions from any state.
",4.5 A Bound on the Runtime of the Algorithm,[0],[0]
For any j ∈ {1 . . .,4.5 A Bound on the Runtime of the Algorithm,[0],[0]
n,4.5 A Bound on the Runtime of the Algorithm,[0],[0]
"}, define first(j) to be the set of target-language words that can begin at position j and last(j) to be the set of target-language
words that can end at position j.
first(j) = {w : ∃ p = (s, t, e) s.t. s = j, e1 = w} last(j) = {w : ∃ p = (s, t, e) s.t. t = j, em = w}
In addition, define singles(j) to be the set of phrases that translate the single word at position j:
singles(j) = {p : s(p) = j and t(p) = j}
Next, define h to be the smallest integer such that for all j, |first(j)| ≤ h, |last(j)| ≤ h, and |singles(j)| ≤ h.",4.5 A Bound on the Runtime of the Algorithm,[0],[0]
"Thus h is a measure of the maximal ambiguity of any word xj in the input.
",4.5 A Bound on the Runtime of the Algorithm,[0],[0]
"Finally, for any position j, define start(j) to be the set of phrases starting at position j:
start(j) = {p : s(p) = j}
and define l to be the smallest integer such that for all j, |start(j)| ≤ l.",4.5 A Bound on the Runtime of the Algorithm,[0],[0]
"Given these definitions we can state the following result:
Theorem 1.",4.5 A Bound on the Runtime of the Algorithm,[0],[0]
"The time complexity of the algorithm is O(nd!lhd+1).
",4.5 A Bound on the Runtime of the Algorithm,[0],[0]
"To prove this we need the following definition:
Definition 4 (p-structures).",4.5 A Bound on the Runtime of the Algorithm,[0],[0]
"For any finite set A of integers with |A| = k, a p-structure is a set of r ordered pairs {(si, ti)}ri=1 that satisfies the following properties: 1) 0",4.5 A Bound on the Runtime of the Algorithm,[0],[0]
≤,4.5 A Bound on the Runtime of the Algorithm,[0],[0]
r ≤ k; 2) for each i ∈ {1 . . .,4.5 A Bound on the Runtime of the Algorithm,[0],[0]
"r}, si ∈",4.5 A Bound on the Runtime of the Algorithm,[0],[0]
A and ti ∈ A (both si = ti and si,4.5 A Bound on the Runtime of the Algorithm,[0],[0]
"6= ti are allowed); 3) for each j ∈ A, there is at most one index i ∈",4.5 A Bound on the Runtime of the Algorithm,[0],[0]
{1 . . .,4.5 A Bound on the Runtime of the Algorithm,[0],[0]
"r} such that (si = j) or (ti = j) or (si = j and ti = j).
",4.5 A Bound on the Runtime of the Algorithm,[0],[0]
"We use g(k) to denote the number of unique pstructures for a set A with |A| = k.
We then have the following Lemmas:
Lemma 4.",4.5 A Bound on the Runtime of the Algorithm,[0],[0]
"The function g(k) satisfies g(0) = 0, g(1) = 2, and the following recurrence for k ≥ 2:
g(k) = 2g(k − 1) + 2(n− 1)g(k",4.5 A Bound on the Runtime of the Algorithm,[0],[0]
"− 2)
Proof.",4.5 A Bound on the Runtime of the Algorithm,[0],[0]
"The proof is in Appendix A.
Lemma 5.",4.5 A Bound on the Runtime of the Algorithm,[0],[0]
Consider the function h(k) = k2×g(k).,4.5 A Bound on the Runtime of the Algorithm,[0],[0]
h(k) is in O((k − 2)!).,4.5 A Bound on the Runtime of the Algorithm,[0],[0]
Proof.,4.5 A Bound on the Runtime of the Algorithm,[0],[0]
"The proof is in Appendix B.
We can now prove the theorem:",4.5 A Bound on the Runtime of the Algorithm,[0],[0]
Proof of Theorem 1: First consider the number of states in the dynamic program.,4.5 A Bound on the Runtime of the Algorithm,[0],[0]
"Each state is of the form (j, {σ1 . . .",4.5 A Bound on the Runtime of the Algorithm,[0],[0]
"σr}) where the set {(s(σi), t(σi))}ri=1 is a p-structure over the set {1}∪ {(j − d) . . .",4.5 A Bound on the Runtime of the Algorithm,[0],[0]
d}.,4.5 A Bound on the Runtime of the Algorithm,[0],[0]
"The number of possible values for {(s(σi), e(σi))}ri=1 is at most g(d + 2).",4.5 A Bound on the Runtime of the Algorithm,[0],[0]
"For a fixed choice of {(s(σi), t(σi))}ri=1 we will argue that there are at most hd+1 possible values for {(ws(σi), wt(σi))}ri=1.",4.5 A Bound on the Runtime of the Algorithm,[0],[0]
This follows because for each k ∈ {(j − d) . . .,4.5 A Bound on the Runtime of the Algorithm,[0],[0]
j} there are at most h possible choices: if there is some i such that s(σi),4.5 A Bound on the Runtime of the Algorithm,[0],[0]
"= k, and t(σi) 6=",4.5 A Bound on the Runtime of the Algorithm,[0],[0]
"k, then the associated word ws(σi) is in the set first(k); alternatively if there is some i such that t(σi)",4.5 A Bound on the Runtime of the Algorithm,[0],[0]
"= k, and s(σi) 6=",4.5 A Bound on the Runtime of the Algorithm,[0],[0]
"k, then the associated word wt(σi) is in the set last(k); alternatively if there is some i such that s(σi) = t(σi)",4.5 A Bound on the Runtime of the Algorithm,[0],[0]
"= k then the associated words ws(σi), wt(σi) must be the first/last word of some phrase in singles(k); alternatively there is no i such that s(σi) = k or t(σi)",4.5 A Bound on the Runtime of the Algorithm,[0],[0]
"= k, in which case there is no choice associated with position k in the sentence.",4.5 A Bound on the Runtime of the Algorithm,[0],[0]
Hence there are at most h choices associated with each position k ∈ {(j − d) . . .,4.5 A Bound on the Runtime of the Algorithm,[0],[0]
"j}, giving hd+1 choices in total.",4.5 A Bound on the Runtime of the Algorithm,[0],[0]
"Combining these results, and noting that there are
n choices of the variable j, implies that there are at most ng(d+ 2)hd+1 states in the dynamic program.
",4.5 A Bound on the Runtime of the Algorithm,[0],[0]
Now consider the number of transitions from any state.,4.5 A Bound on the Runtime of the Algorithm,[0],[0]
A transition is of the form ψ1pψ2 as defined in Section 4.2.1.,4.5 A Bound on the Runtime of the Algorithm,[0],[0]
"For a given state there are at most (d + 2) choices for ψ1 and ψ2, and l choices for p, giving at most (d+ 2)2l choices in total.
",4.5 A Bound on the Runtime of the Algorithm,[0],[0]
Multiplying the upper bounds on the number of states and number of transitions for each state gives an upper bound on the runtime of the algorithm as O(ng(d + 2)hd+1(d + 2)2l).,4.5 A Bound on the Runtime of the Algorithm,[0],[0]
"Hence by Lemma 5 the runtime is O(nd!lhd+1) time.
",4.5 A Bound on the Runtime of the Algorithm,[0],[0]
"The bound g(d + 2) over the number of possible values for {(s(σi), e(σi))}ri=1 is somewhat loose, as the set of p-structures over {1} ∪ {(j − d) . . .",4.5 A Bound on the Runtime of the Algorithm,[0],[0]
"d} includes impossible values {(si, ti)}ri=1 where for example there is no i such that s(σi) = 1.",4.5 A Bound on the Runtime of the Algorithm,[0],[0]
However the bound is tight enough to give the O(d!),4.5 A Bound on the Runtime of the Algorithm,[0],[0]
runtime.,4.5 A Bound on the Runtime of the Algorithm,[0],[0]
We conclude the paper with discussion of some issues.,5 Discussion,[0],[0]
First we describe how the dynamic programming structures we have described can be used in conjunction with beam search.,5 Discussion,[0],[0]
"Second, we give more analysis of the complexity of the widely-used decoding algorithm of Koehn et al. (2003).",5 Discussion,[0],[0]
Beam search is widely used in phrase-based decoding; it can also be applied to our dynamic programming construction.,5.1 Beam Search,[0],[0]
"We can replace the line
for each state T ∈",5.1 Beam Search,[0],[0]
"Tj
in the algorithm in Figure 2 with
for each state T ∈ beam(Tj)
where beam is a function that returns a subset of Tj , most often the highest scoring elements of Tj under some scoring criterion.",5.1 Beam Search,[0],[0]
A key question concerns the choice of scoring function γ(T ) used to rank states.,5.1 Beam Search,[0],[0]
One proposal is to define γ(T ) = α(T ),5.1 Beam Search,[0],[0]
+ β(T ),5.1 Beam Search,[0],[0]
"where α(T ) is the score used in the dynamic program, and β(T )",5.1 Beam Search,[0],[0]
"=
∑ i:ws(σi)6=<s> λu(ws(σi)).
",5.1 Beam Search,[0],[0]
Here λu(w) is the score of word w under a unigram language model.,5.1 Beam Search,[0],[0]
"The β(T ) scores allow different states in Tj , which have different words ws(σi) at
the start of signatures, to be comparable: for example it compensates for the case wherews(σi) is a rare word, which will incur a low probability when the bigram 〈w ws(σi)〉 for some word w is constructed during search.
",5.1 Beam Search,[0],[0]
The β(T ) values play a similar role to “future scores” in the algorithm of Koehn et al. (2003).,5.1 Beam Search,[0],[0]
"However in the Koehn et al. (2003) algorithm, different items in the same beam can translate different subsets of the input sentence, making futurescore estimation more involved.",5.1 Beam Search,[0],[0]
In our case all items in Tj translate all words x1 . . .,5.1 Beam Search,[0],[0]
"xj inclusive, which may make comparison of different hypotheses more straightforward.",5.1 Beam Search,[0],[0]
"A common method for decoding phrase-based models, as described in Koehn et al. (2003), is to use beam search in conjunction with a search algorithm that 1) creates the target language string in strictly left-to-right order; 2) uses a bit string with bits bi ∈ {0, 1} for i = 1 . . .",5.2 Complexity of Decoding with Bit-string Representations,[0],[0]
n,5.2 Complexity of Decoding with Bit-string Representations,[0],[0]
representing at each point whether word i in the input has been translated.,5.2 Complexity of Decoding with Bit-string Representations,[0],[0]
A natural question is whether the number of possible bit strings for a model with a fixed distortion limit d can grow exponentially quickly with respect to the length of the input sentence.,5.2 Complexity of Decoding with Bit-string Representations,[0],[0]
"This section gives an example that shows that this is indeed the case.
",5.2 Complexity of Decoding with Bit-string Representations,[0],[0]
Assume that our sentence length n is such that (n−2)/4 is an integer.,5.2 Complexity of Decoding with Bit-string Representations,[0],[0]
Assume as before x1 = <s> and xn = </s>.,5.2 Complexity of Decoding with Bit-string Representations,[0],[0]
For each k ∈ {0 . . .,5.2 Complexity of Decoding with Bit-string Representations,[0],[0]
"((n − 2)/4 − 1)}, assume we have the following phrases for the words x4k+2 . . .",5.2 Complexity of Decoding with Bit-string Representations,[0],[0]
"x4k+5:
(4k + 2, 4k + 2, uk) (4k + 3, 4k + 3, vk)
(4k + 4, 4k + 4, wk) (4k + 5, 4k + 5, zk)
(4k + 4, 4k + 5, yk)
Note that the only source of ambiguity is for each k whether we use yk to translate the entire phrase x4k+4x4k+5, or whether we use wk and zk to translate x4k+4 and x4k+5 separately.
",5.2 Complexity of Decoding with Bit-string Representations,[0],[0]
"With a distortion limit d ≥ 5, the number of possible bit strings in this example is at least 2(n−2)/4.",5.2 Complexity of Decoding with Bit-string Representations,[0],[0]
"This follows because for any setting of the variables b4k+4 ∈ {0, 1} for k ∈ {0 . . .",5.2 Complexity of Decoding with Bit-string Representations,[0],[0]
"((n − 2)/4 − 1)},
there is a valid derivation p1 . . .",5.2 Complexity of Decoding with Bit-string Representations,[0],[0]
pL such that the prefix p1 . . .,5.2 Complexity of Decoding with Bit-string Representations,[0],[0]
pl,5.2 Complexity of Decoding with Bit-string Representations,[0],[0]
where l = 1 + (n − 2)/4 gives this bit string.,5.2 Complexity of Decoding with Bit-string Representations,[0],[0]
"Simply choose p1 = (1, 1,<s>) and for l′ ∈ {0 . . .",5.2 Complexity of Decoding with Bit-string Representations,[0],[0]
(n− 2)/4− 1} choose pl′+2 =,5.2 Complexity of Decoding with Bit-string Representations,[0],[0]
"(4l′+ 4, 4l′+ 5, yi) if b4k+4 = 1, pl′+2 = (4l′ + 5, 4l′ + 5, zi) otherwise.",5.2 Complexity of Decoding with Bit-string Representations,[0],[0]
It can be verified that p1 . . .,5.2 Complexity of Decoding with Bit-string Representations,[0],[0]
pl is a valid prefix (there is a valid way to give a complete derivation from this prefix).,5.2 Complexity of Decoding with Bit-string Representations,[0],[0]
"As one example, for n = 10, and b4 = 1 and b8 = 0, a valid derivation is
(1, 1,<s>)(4, 5, y1)(9, 9, z2)(7, 7, v2)(3, 3, v1)
(2, 2, u1)(6, 6, u2)(8, 8, w2)(10, 10,</s>)
",5.2 Complexity of Decoding with Bit-string Representations,[0],[0]
"In this case the prefix (1, 1,<s>)(4, 5, y1)(9, 9, z2) gives b4 = 1 and b8 = 0.",5.2 Complexity of Decoding with Bit-string Representations,[0],[0]
"Other values for b4 and b8 can be given by using (5, 5, z1) in place of (4, 5, y1), and (8, 9, y2) in place of (9, 9, z2), with the following phrases modified appropriately.",5.2 Complexity of Decoding with Bit-string Representations,[0],[0]
We have given a polynomial-time dynamic programming algorithm for phrase-based decoding with a fixed distortion limit.,6 Conclusion,[0],[0]
"The algorithm uses a quite different representation of states from previous decoding algorithms, is easily amenable to beam search, and leads to a new perspective on phrase-based decoding.",6 Conclusion,[0],[0]
Future work should investigate the effectiveness of the algorithm in practice.,6 Conclusion,[0],[0]
"Without loss of generality assume A = {1, 2, 3, . . .",A Proof of Lemma 4,[0],[0]
k}.,A Proof of Lemma 4,[0],[0]
"We have g(1) = 2, because in this case the valid p-structures are {(1, 1)} and ∅. To calculate g(k) we can sum over four possibilities:
Case 1: There are g(k − 1) p-structures with si = ti = 1 for some i ∈ {1 . . .",A Proof of Lemma 4,[0],[0]
r}.,A Proof of Lemma 4,[0],[0]
"This follows because once si = ti = 1 for some i, there are g(k − 1) possible p-structures for the integers {2, 3, 4 . . .",A Proof of Lemma 4,[0],[0]
k}.,A Proof of Lemma 4,[0],[0]
Case 2: There are g(k − 1) p-structures such that si 6= 1,A Proof of Lemma 4,[0],[0]
and ti 6= 1 for all i ∈ {1 . . .,A Proof of Lemma 4,[0],[0]
r}.,A Proof of Lemma 4,[0],[0]
"This follows because once si 6= 1 and ti 6= 1 for all i, there are g(k − 1) possible p-structures for the integers {2, 3, 4 . . .",A Proof of Lemma 4,[0],[0]
k}.,A Proof of Lemma 4,[0],[0]
Case 3: There are (k− 1)× g(k− 2) p-structures such that there is some i ∈ {1 . . .,A Proof of Lemma 4,[0],[0]
r} with si = 1 and ti 6= 1.,A Proof of Lemma 4,[0],[0]
"This follows because for the i such that
si = 1, there are (k− 1) choices for the value for ti, and there are then g(k − 2) possible p-structures for the remaining integers in the set {1 . . .",A Proof of Lemma 4,[0],[0]
"k}/{1, ti}.",A Proof of Lemma 4,[0],[0]
Case 4: There are (k− 1)× g(k− 2) p-structures such that there is some i ∈ {1 . . .,A Proof of Lemma 4,[0],[0]
r} with ti = 1 and si 6= 1.,A Proof of Lemma 4,[0],[0]
"This follows because for the i such that ti = 1, there are (k− 1) choices for the value for si, and there are then g(k − 2) possible p-structures for the remaining integers in the set {1 . . .",A Proof of Lemma 4,[0],[0]
"k}/{1, si}.
",A Proof of Lemma 4,[0],[0]
Summing over these possibilities gives the following recurrence: g(k) = 2g(k − 1) + 2(k,A Proof of Lemma 4,[0],[0]
− 1)× g(k − 2),A Proof of Lemma 4,[0],[0]
Recall that h(k) = f(k)× g(k) where f(k) = k2.,B Proof of Lemma 5,[0],[0]
"Define k0 to be the smallest integer such that for all k ≥ k0, 2f(k)
",B Proof of Lemma 5,[0],[0]
f(k − 1) + 2f(k) f(k,B Proof of Lemma 5,[0],[0]
− 2) ·,B Proof of Lemma 5,[0],[0]
k − 1 k,B Proof of Lemma 5,[0],[0]
− 3 ≤ k,B Proof of Lemma 5,[0],[0]
"− 2 (4)
",B Proof of Lemma 5,[0],[0]
For f(k) = k2 we have k0 = 9.,B Proof of Lemma 5,[0],[0]
Now choose a constant c such that for all k ∈ {1 . . .,B Proof of Lemma 5,[0],[0]
"(k0 − 1)}, h(k) ≤",B Proof of Lemma 5,[0],[0]
c,B Proof of Lemma 5,[0],[0]
× (k − 2)!.,B Proof of Lemma 5,[0],[0]
We will prove by induction that under these definitions of k0 and c we have h(k) ≤ c(k,B Proof of Lemma 5,[0],[0]
− 2)!,B Proof of Lemma 5,[0],[0]
"for all integers k, hence h(k) is in O((k − 2)!).
",B Proof of Lemma 5,[0],[0]
"For values k ≥ k0, we have h(k) = f(k)g(k)
= 2f(k)g(k − 1) + 2f(k)(k − 1)g(k",B Proof of Lemma 5,[0],[0]
"− 2) (5)
= 2f(k) f(k − 1)h(k − 1) + 2f(k) f(k − 2)(k − 1)h(k − 2) ≤ ( 2cf(k)
",B Proof of Lemma 5,[0],[0]
f(k − 1) + 2cf(k) f(k,B Proof of Lemma 5,[0],[0]
− 2) ·,B Proof of Lemma 5,[0],[0]
k − 1 k,B Proof of Lemma 5,[0],[0]
"− 3
) (k − 3)!",B Proof of Lemma 5,[0],[0]
"(6)
≤ c(k",B Proof of Lemma 5,[0],[0]
− 2)!,B Proof of Lemma 5,[0],[0]
(7) Eq. 5 follows from g(k) = 2g(k−1)+2(k−1)g(k− 2).,B Proof of Lemma 5,[0],[0]
Eq. 6 follows by the inductive hypothesis that h(k − 1) ≤ c(k,B Proof of Lemma 5,[0],[0]
− 3)!,B Proof of Lemma 5,[0],[0]
and h(k − 2) ≤ c(k,B Proof of Lemma 5,[0],[0]
− 4)!.,B Proof of Lemma 5,[0],[0]
Eq 7 follows because Eq. 4 holds for all k ≥ k0.,B Proof of Lemma 5,[0],[0]
"Decoding of phrase-based translation models in the general case is known to be NPcomplete, by a reduction from the traveling salesman problem (Knight, 1999).",abstractText,[0],[0]
"In practice, phrase-based systems often impose a hard distortion limit that limits the movement of phrases during translation.",abstractText,[0],[0]
"However, the impact on complexity after imposing such a constraint is not well studied.",abstractText,[0],[0]
"In this paper, we describe a dynamic programming algorithm for phrase-based decoding with a fixed distortion limit.",abstractText,[0],[0]
"The runtime of the algorithm is O(nd!lh) where n is the sentence length, d is the distortion limit, l is a bound on the number of phrases starting at any position in the sentence, and h is related to the maximum number of target language translations for any source word.",abstractText,[0],[0]
The algorithm makes use of a novel representation that gives a new perspective on decoding of phrase-based models.,abstractText,[0],[0]
A Polynomial-Time Dynamic Programming Algorithm for Phrase-Based Decoding with a Fixed Distortion Limit,title,[0],[0]
"Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 90–99, Baltimore, Maryland, USA, June 23-25 2014. c©2014 Association for Computational Linguistics",text,[0],[0]
"The research of the last two decades has established empirically that distributional vectors for words obtained from corpus statistics can be used to represent word meaning in a variety of tasks (Turney and Pantel, 2010).",1 Compositional distributional semantics,[0],[0]
"If distributional vectors encode certain aspects of word meaning, it is natural to expect that similar aspects of sentence meaning can also receive vector representations, obtained compositionally from word vectors.",1 Compositional distributional semantics,[0],[0]
"Developing a practical model of compositionality is still an open issue, which we address in this paper.",1 Compositional distributional semantics,[0],[0]
"One approach is to use simple, parameterfree models that perform operations such as pointwise multiplication or summing (Mitchell and Lapata, 2008).",1 Compositional distributional semantics,[0],[0]
"Such models turn out to be surprisingly effective in practice (Blacoe and Lapata, 2012), but they have obvious limitations.",1 Compositional distributional semantics,[0],[0]
"For instance, symmetric operations like vector addition are insensitive to syntactic structure, therefore meaning differences encoded in word order
are lost in composition: pandas eat bamboo is identical to bamboo eats pandas.",1 Compositional distributional semantics,[0],[0]
"Guevara (2010), Mitchell and Lapata (2010), Socher et al. (2011) and Zanzotto et al. (2010) generalize the simple additive model by applying structure-encoding operators to the vectors of two sister nodes before addition, thus breaking the inherent symmetry of the simple additive model.",1 Compositional distributional semantics,[0],[0]
"A related approach (Socher et al., 2012) assumes richer lexical representations where each word is represented with a vector and a matrix that encodes its interaction with its syntactic sister.",1 Compositional distributional semantics,[0],[0]
The training proposed in this model estimates the parameters in a supervised setting.,1 Compositional distributional semantics,[0],[0]
"Despite positive empirical evaluation, this approach is hardly practical for generalpurpose semantic language processing, since it requires computationally expensive approximate parameter optimization techniques, and it assumes task-specific parameter learning whose results are not meant to generalize across tasks.",1 Compositional distributional semantics,[0],[0]
"None of the proposals mentioned above, from simple to elaborate, incorporates in its architecture the intuitive idea (standard in theoretical linguistics) that semantic composition is more than a weighted combination of words.",1.1 The lexical function model,[0],[0]
"Generally one of the components of a phrase, e.g., an adjective, acts as a function affecting the other component (e.g., a noun).",1.1 The lexical function model,[0],[0]
"This underlying intuition, adopted from formal semantics of natural language, motivated the creation of the lexical function model of composition (lf ) (Baroni and Zamparelli, 2010; Coecke et al., 2010).",1.1 The lexical function model,[0],[0]
"The lf model can be seen as a projection of the symbolic Montagovian approach to semantic composition in natural language onto the domain of vector spaces and linear operations on them (Baroni et al., 2013).",1.1 The lexical function model,[0],[0]
"In lf, arguments are vectors and functions taking arguments (e.g., adjectives that combine with nouns) are tensors, with the number of arguments (n) determining the
90
order of tensor (n+1).",1.1 The lexical function model,[0],[0]
"For example, adjectives, as unary functors, are modeled with 2-way tensors, or matrices.",1.1 The lexical function model,[0],[0]
"Tensor by vector multiplication formalizes function application and serves as the general composition method.
",1.1 The lexical function model,[0],[0]
Baroni and Zamparelli (2010) propose a practical and empirically effective way to estimate matrices representing adjectival modifiers of nouns by linear regression from corpus-extracted examples of noun and adjective-noun vectors.,1.1 The lexical function model,[0],[0]
"Unlike the neural network approach of Socher et al. (2011; 2012), the Baroni and Zamparelli method does not require manually labeled data nor costly iterative estimation procedures, as it relies on automatically extracted phrase vectors and on the analytical solution of the least-squares-error problem.
",1.1 The lexical function model,[0],[0]
"The same method was later applied to matrix representations of intransitive verbs and determiners (Bernardi et al., 2013; Dinu et al., 2013), always with good empirical results.
",1.1 The lexical function model,[0],[0]
"The full range of semantic types required for natural language processing, including those of adverbs and transitive verbs, has to include, however, tensors of greater rank.",1.1 The lexical function model,[0],[0]
The estimation method originally proposed by Baroni and Zamparelli has been extended to 3-way tensors representing transitive verbs by Grefenstette et al. (2013) with preliminary success.,1.1 The lexical function model,[0],[0]
Grefenstette et al.’s method works in two steps.,1.1 The lexical function model,[0],[0]
"First, one estimates matrices of verb-object phrases from subject and subject-verb-object vectors; next, transitive verb tensors are estimated from verb-object matrices and object vectors.",1.1 The lexical function model,[0],[0]
"With all the advantages of lf, scaling it up to arbitrary sentences, however, leads to several issues.",1.2 Problems with the extension of the lexical function model to sentences,[0],[0]
"In particular, it is desirable for all practical purposes to limit representation size.",1.2 Problems with the extension of the lexical function model to sentences,[0],[0]
"For example, if noun meanings are encoded in vectors of 300 dimensions, adjectives become matrices of 3002 cells, and transitive verbs are represented as tensors with 3003=27, 000, 000 dimensions.
",1.2 Problems with the extension of the lexical function model to sentences,[0],[0]
Estimating tensors of this size runs into data sparseness issues already for less common transitive verbs.,1.2 Problems with the extension of the lexical function model to sentences,[0],[0]
"Indeed, in order to train a transitive verb tensor (e.g., eat), the method of Grefenstette et al. (2013) requires a sufficient number of distinct verb object phrases with that verb (e.g., eat
cake, eat fruits), each attested in combination with a certain number of subject nouns with sufficient frequency to extract sensible vectors.",1.2 Problems with the extension of the lexical function model to sentences,[0],[0]
"It is not feasible to obtain enough data points for all verbs in such a training design.
",1.2 Problems with the extension of the lexical function model to sentences,[0],[0]
Things get even worse for other categories.,1.2 Problems with the extension of the lexical function model to sentences,[0],[0]
"Adverbs like quickly that modify intransitive verbs have to be represented with 30022 = 8, 100, 000, 000 dimensions.",1.2 Problems with the extension of the lexical function model to sentences,[0],[0]
"Modifiers of transitive verbs would have even greater representation size, which may not be possible to store and learn efficiently.
",1.2 Problems with the extension of the lexical function model to sentences,[0],[0]
Another issue is that the same or similar items that occur in different syntactic contexts are assigned different semantic types with incomparable representations.,1.2 Problems with the extension of the lexical function model to sentences,[0],[0]
"For example, verbs like eat can be used in transitive or intransitive constructions (children eat meat/children eat), or in passive (meat is eaten).",1.2 Problems with the extension of the lexical function model to sentences,[0],[0]
"Since predicate arity is encoded in the order of the corresponding tensor, eat and the like have to be assigned different representations (matrix or tensor) depending on the context.",1.2 Problems with the extension of the lexical function model to sentences,[0],[0]
"Deverbal nouns like demolition, often used without mention of who demolished what, would have to get vector representations while the corresponding verbs (demolish) would become tensors, which makes immediately related verbs and nouns incomparable.",1.2 Problems with the extension of the lexical function model to sentences,[0],[0]
Nouns in general would oscillate between vector and matrix representations depending on argument vs. predicate vs. modifier position (an animal runs vs. this is an animal vs. animal shelter).,1.2 Problems with the extension of the lexical function model to sentences,[0],[0]
"Prepositions are the hardest, as the syntactic positions in which they occur are most diverse (park in the dark vs. play in the dark vs. be in the dark vs. a light glowing in the dark).
",1.2 Problems with the extension of the lexical function model to sentences,[0],[0]
"In all those cases, the same word has to be mapped to tensors of different orders.",1.2 Problems with the extension of the lexical function model to sentences,[0],[0]
"Since each of these tensors must be learned from examples individually, their obvious relation is missed.",1.2 Problems with the extension of the lexical function model to sentences,[0],[0]
"Besides losing the comparability of the semantic contribution of a word across syntactic contexts, we also worsen the data sparseness issues.
",1.2 Problems with the extension of the lexical function model to sentences,[0],[0]
"The last, and related, point is that for the tensor calculus to work, one needs to model, for each word, each of the constructions in the corpus that the word is attested in.",1.2 Problems with the extension of the lexical function model to sentences,[0],[0]
In its pure form lf does not include an emergency backoff strategy when unknown words or constructions are encountered.,1.2 Problems with the extension of the lexical function model to sentences,[0],[0]
"For example, if we only observe transitive usages of to eat in the training corpus, and encounter an intransitive or passive example of it in testing data,
the system would not be able to compose a sentence vector at all.",1.2 Problems with the extension of the lexical function model to sentences,[0],[0]
This issue is unavoidable since we don’t expect to find all words in all possible constructions even in the largest corpus.,1.2 Problems with the extension of the lexical function model to sentences,[0],[0]
"As follows from section 1.2, it would be desirable to have a compositional distributional model that encodes function-argument relations but avoids the troublesome high-order tensor representations of the pure lexical function model, with all the practical problems that come with them.",2 The practical lexical function model,[0],[0]
"We may still want to represent word meanings in different syntactic contexts differently, but at the same time we need to incorporate a formal connection between those representations, e.g., between the transitive and the intransitive instantiations of the verb to eat.",2 The practical lexical function model,[0],[0]
"Last but not least, all items need to include a common aspect of their representation (e.g., a vector) to allow comparison across categories (the case of demolish and demolition).
",2 The practical lexical function model,[0],[0]
"To this end, we propose a new model of composition that maintains the idea of function application, while avoiding the complications and rigidity of lf.",2 The practical lexical function model,[0],[0]
"We call our proposal practical lexical function model, or plf.",2 The practical lexical function model,[0],[0]
"In plf, a functional word is not represented by a single tensor of arity-dependent order, but by a vector plus an ordered set of matrices, with one matrix for each argument the function takes.",2 The practical lexical function model,[0],[0]
"After applying the matrices to the corresponding argument vectors, a single representation is obtained by summing across all resulting vectors.",2 The practical lexical function model,[0],[0]
"In plf, all words are represented by a vector, and functional words, such as predicates and modifiers, are also assigned one or more matrices.",2.1 Word meaning representation,[0],[0]
The general form of a semantic representation for a linguistic unit is an ordered tuple of a vector and n ∈,2.1 Word meaning representation,[0],[0]
N,2.1 Word meaning representation,[0],[0]
"matrices:1〈
~x, 21 x , . .",2.1 Word meaning representation,[0],[0]
.,2.1 Word meaning representation,[0],[0]
", 2n x 〉 The number of matrices in the representation encodes the arity of a linguistic unit, i.e., the number of other units to which it applies as a function.",2.1 Word meaning representation,[0],[0]
"Each matrix corresponds to a function-argument relation, and words have as many matrices as many arguments they take: none for (most) nouns,
1Matrices associated with term x are symbolized 2 x.
one for adjectives and intransitive verbs, two for transitives, etc.",2.1 Word meaning representation,[0],[0]
"The matrices formalize argument slot saturation, operating on an argument vector representation through matrix by vector multiplication, as described in the next section.
",2.1 Word meaning representation,[0],[0]
Modifiers of n-ary functors are represented by n+1-ary structures.,2.1 Word meaning representation,[0],[0]
"For instance, we treat adjectives that modify nouns (0-ary) as unary functions, encoded in a vector-matrix pair.",2.1 Word meaning representation,[0],[0]
Adverbs have different semantic types depending on their syntactic role.,2.1 Word meaning representation,[0],[0]
"Sentential adverbs are unary, while adverbs that modify adjectives (very) or verb phrases (quickly) are encoded as binary functions, represented by a vector and two matrices.",2.1 Word meaning representation,[0],[0]
"The form of semantic representations we are using is shown in
Table 1.2",2.1 Word meaning representation,[0],[0]
"Our system incorporates semantic composition via two composition rules, one for combining structures of different arity and the other for symmetric composition of structures with the same arity.",2.2 Semantic composition,[0],[0]
"These rules incorporate insights of two empirically successful models, lexical function and the simple additive approach, used as the default structure merging strategy.
",2.2 Semantic composition,[0],[0]
"The first rule is function application, illustrated in Figure 1.",2.2 Semantic composition,[0],[0]
Table 2 illustrates simple cases of function application.,2.2 Semantic composition,[0],[0]
For transitive verbs semantic composition applies iteratively as shown in the derivation of Figure 2.,2.2 Semantic composition,[0],[0]
"For ternary predicates such
2To determine the number and ordering of matrices representing the word in the current syntactic context, our plf implementation relies on the syntactic type assigned to the word in the categorial grammar parse of the sentence.
",2.2 Semantic composition,[0],[0]
"as give in a ditransitive construction, the first step in the derivation absorbs the innermost argument by multiplying its vector by the third give matrix, and then composition proceeds like for transitives.
",2.2 Semantic composition,[0],[0]
"The second composition rule, symmetric composition applies when two syntactic sisters are of the same arity (e.g., two vectors, or two vectormatrix pairs).",2.2 Semantic composition,[0],[0]
"Symmetric composition simply sums the objects in the two tuples: vector with vector, n-th matrix with n-th matrix.
",2.2 Semantic composition,[0],[0]
Symmetric composition is reserved for structures in which the function-argument distinction is problematic.,2.2 Semantic composition,[0],[0]
"Some candidates for such treatment are coordination and nominal compounds, although we recognize that the headless analysis is
not the only possible one here.",2.2 Semantic composition,[0],[0]
"See two examples of Symmetric Composition application in Table 3.
",2.2 Semantic composition,[0],[0]
Note that the sing and dance composition in Table 3 skips the conjunction.,2.2 Semantic composition,[0],[0]
"Our current plf implementation treats most grammatical words, including conjunctions, as “empty” elements, that do not project into semantics.",2.2 Semantic composition,[0],[0]
This choice leads to some interesting “serendipitous” treatments of various constructions.,2.2 Semantic composition,[0],[0]
"For example, since the copula is empty, a sentence with a predicative adjective (cars are red) is treated in the same way as a phrase with the same adjective in attributive position (red cars) – although the latter, being a phrase and not a full sentence, will later be embedded as argument in a larger construction.",2.2 Semantic composition,[0],[0]
"Similarly, leaving the relative pronoun empty makes cars that run identical to cars run, although, again, the former will be embedded in a larger construction later in the derivation.
",2.2 Semantic composition,[0],[0]
"We conclude our brief exposition of plf with an alternative intuition for it: the plf model is also a more sophisticated version of the additive approach, where argument words are adapted by matrices that encode the relation to their functors before the sentence vector is derived by summing.",2.2 Semantic composition,[0],[0]
Let us now outline how plf addresses the shortcomings of lf listed in Section 1.2.,2.3 Satisfying the desiderata,[0],[0]
"First, all issues caused by representation size disappear.",2.3 Satisfying the desiderata,[0],[0]
An n-ary predicate is no longer encoded as an n+1way tensor; instead we have a sequence of n matrices.,2.3 Satisfying the desiderata,[0],[0]
"The representation size grows linearly, not exponentially, for higher semantic types, allowing for simpler and more efficient parameter estimation, storage, and computation.
",2.3 Satisfying the desiderata,[0],[0]
"As a consequence of our architecture, we no longer need to perform the complicated step-bystep estimation for elements of higher arity.",2.3 Satisfying the desiderata,[0],[0]
"Indeed, one can estimate each matrix of a complex representation individually using the simple method of Baroni and Zamparelli (2010).",2.3 Satisfying the desiderata,[0],[0]
"For instance, for transitive verbs we estimate the verbsubject combination matrix from subject and verb-
subject vectors, the verb-object combination matrix from object and verb-object vectors.",2.3 Satisfying the desiderata,[0],[0]
"We expect a reasonably large corpus to feature many occurrences of a verb with a variety of subjects and a variety of objects (but not necessarily a variety of subjects with each of the objects as required by Grefenstette et al.’s training), allowing us to avoid the data sparseness issue.
",2.3 Satisfying the desiderata,[0],[0]
"The semantic representations we propose include a semantic vector for constituents of any semantic type, thus enabling semantic comparison for words of different parts of speech (the case of demolition vs. demolish).
",2.3 Satisfying the desiderata,[0],[0]
"Finally, the fact that we represent the predicate interaction with each of its arguments in a separate matrix allows for a natural and intuitive treatment of argument alternations.",2.3 Satisfying the desiderata,[0],[0]
"For instance, as shown in Table 4, one can distinguish the transitive and intransitive usages of the verb to eat by the presence of the object-oriented matrix of the verb while keeping the rest of the representation intact.",2.3 Satisfying the desiderata,[0],[0]
"To model passive usages, we insert the object matrix of the verb only, which will be multiplied by the syntactic subject vector, capturing the similarity between eat meat and meat is eaten.
",2.3 Satisfying the desiderata,[0],[0]
"So keeping the verb’s interaction with subject and object encoded in distinct matrices not only solves the issues of representation size for arbitrary semantic types, but also provides a sensible built-in strategy for handling a word’s occurrence in multiple constructions.",2.3 Satisfying the desiderata,[0],[0]
"Indeed, if we encounter a verb used intransitively which was only attested as transitive in the training corpus, we can simply omit the object matrix to obtain a type-appropriate representation.",2.3 Satisfying the desiderata,[0],[0]
"On the other hand, if the verb occurs with more arguments than usual in testing materials, we can add a default diagonal identity matrix to its representation, signaling agnosticism about how the verb relates to the unexpected argu-
ment.",2.3 Satisfying the desiderata,[0],[0]
"This flexibility makes our model suitable to compute vector representations of sentences without stumbling at unseen syntactic usages of words.
",2.3 Satisfying the desiderata,[0],[0]
"To summarize, plf is an extension of the lexical function model that inherits its strengths and overcomes its weaknesses.",2.3 Satisfying the desiderata,[0],[0]
We still employ a linguistically-motivated notion of semantic composition as function application and use distinct kinds of representations for different semantic types.,2.3 Satisfying the desiderata,[0],[0]
"At the same time, we avoid high order tensor representations, produce semantic vectors for all syntactic constituents, and allow for an elegant and transparent correspondence between different syntactic usages of a lexeme, such as the transitive, the intransitive, and the passive usages of the verb to eat.",2.3 Satisfying the desiderata,[0],[0]
"Last but not least, our implementation is suitable for realistic language processing since it allows to produce vectors for sentences of arbitrary size, including those containing novel syntactic configurations.",2.3 Satisfying the desiderata,[0],[0]
We consider 5 different benchmarks that focus on different aspects of sentence-level semantic composition.,3.1 Evaluation materials,[0],[0]
"The first data set, created by Edward Grefenstette and Mehrnoosh Sadrzadeh and introduced in Kartsaklis et al. (2013), features 200 sentence pairs that were rated for similarity by 43 annotators.",3.1 Evaluation materials,[0],[0]
"In this data set, sentences have fixed adjective-noun-verb-adjective-noun (anvan) structure, and they were built in order to crucially require context-based verb disambiguation (e.g., young woman filed long nails is paired with both young woman smoothed long nails and young woman registered long nails).",3.1 Evaluation materials,[0],[0]
"We also consider a similar data set introduced by Grefenstette (2013), comprising 200 sentence pairs rated by 50 annotators.",3.1 Evaluation materials,[0],[0]
"We will call these benchmarks anvan1 and anvan2, respectively.",3.1 Evaluation materials,[0],[0]
"Evaluation is carried out by computing the Spearman correlation between the annotator similarity ratings for the sentence pairs and the cosines of the vectors produced by the various systems for the same sentence pairs.
",3.1 Evaluation materials,[0],[0]
The benchmark introduced by The Pham et al. (2013) at the TFDS workshop (tfds below) was specifically designed to test compositional methods for their sensitivity to word order and the semantic effect of determiners.,3.1 Evaluation materials,[0],[0]
"The tfds benchmark contains 157 target sentences that are matched with a set of (approximate) paraphrases (8 on av-
erage), and a set of “foils” (17 on average).",3.1 Evaluation materials,[0],[0]
"The foils have high lexical overlap with the targets but very different meanings, due to different determiners and/or word order.",3.1 Evaluation materials,[0],[0]
"For example, the target A man plays an acoustic guitar is matched with paraphrases such as A man plays guitar and The man plays the guitar, and foils such as The man plays no guitar and A guitar plays a man.",3.1 Evaluation materials,[0],[0]
A good system should return higher similarities for the comparison with the paraphrases with respect to that with the foils.,3.1 Evaluation materials,[0],[0]
"Performance is assessed through the t-standardized cross-target average of the difference between mean cosine with paraphrases and mean cosine with foils (Pham and colleagues, equivalently, reported non-standardized average and standard deviations).
",3.1 Evaluation materials,[0],[0]
"The two remaining data sets are larger and more ‘natural’, as they were not constructed by linguists under controlled conditions to focus on specific phenomena.",3.1 Evaluation materials,[0],[0]
They are aimed at evaluating systems on the sort of free-form sentences one encounters in real-life applications.,3.1 Evaluation materials,[0],[0]
"The msrvid data set from the SemEval-2012 Semantic Textual Similarity (STS) task (Agirre et al., 2012) consists of 750 sentence pairs that describe brief videos.",3.1 Evaluation materials,[0],[0]
Sentence pairs were scored for similarity by 5 subjects each.,3.1 Evaluation materials,[0],[0]
"Following standard practice in paraphrase detection studies (e.g., Blacoe and Lapata (2012)), we use cosine similarity between sentence pairs as computed by one of our systems together with two shallow similarity cues: word overlap between the two sentences and difference in sentence length.",3.1 Evaluation materials,[0],[0]
"We obtain a final similarity score by weighted addition of the 3 cues, with the optimal weights determined by linear regression on separate msrvid train data that were also provided by the SemEval task organizers (before combining, we checked that the collinearity between cues was low).",3.1 Evaluation materials,[0],[0]
"System scores are evaluated by their Pearson correlation with the human ratings.
",3.1 Evaluation materials,[0],[0]
"The final set we use is onwn, from the *SEM2013 STS shared task (Agirre et al., 2013).",3.1 Evaluation materials,[0],[0]
"This set contains 561 pairs of glosses (from the WordNet and OntoNotes databases), rated by 5 judges for similarity.",3.1 Evaluation materials,[0],[0]
"Our main interest in this set stems from the fact that glosses are rarely well-formed full sentences (consider, e.g., cause something to pass or lead somewhere; coerce by violence, fill with terror).",3.1 Evaluation materials,[0],[0]
"For this reason, they are very challenging for standard parsers.",3.1 Evaluation materials,[0],[0]
"Indeed, we estimated from a sample of 40 onwn glosses that the C&C
parser (see below) has only 45% accuracy on this set.",3.1 Evaluation materials,[0],[0]
"Since plf needs syntactic information to construct sentence vectors compositionally, we test it on onwn to make sure that it is not overly sensitive to parser noise.",3.1 Evaluation materials,[0],[0]
Evaluation proceeds as with msrvid (cue weights are determined by 10-fold cross-validation).3,3.1 Evaluation materials,[0],[0]
"Our source corpus was given by the concatenation of ukWaC (wacky.sslmit.unibo.it), a mid-2009 dump of the English Wikipedia (en. wikipedia.org) and the British National Corpus (www.natcorp.ox.ac.uk), for a total of about 2.8 billion words.
",3.2 Semantic space construction and composition model implementation,[0],[0]
"We collected a 30K-by-30K matrix by counting co-occurrence of the 30K most frequent content lemmas (nouns, adjectives and verbs) within a 3- word window.",3.2 Semantic space construction and composition model implementation,[0],[0]
The raw count vectors were transformed into positive Pointwise Mutual Information scores and reduced to 300 dimensions by the Singular Value Decomposition.,3.2 Semantic space construction and composition model implementation,[0],[0]
All vectors were normalized to length 1.,3.2 Semantic space construction and composition model implementation,[0],[0]
"This setup was picked without tuning, as we found it effective in previous, unrelated experiments.4
We consider four composition models.",3.2 Semantic space construction and composition model implementation,[0],[0]
The add (additive) model produces the vector of a sentence by summing the vectors of all content words in it.,3.2 Semantic space construction and composition model implementation,[0],[0]
"Similarly, mult uses component-wise multiplication of vectors for composition.",3.2 Semantic space construction and composition model implementation,[0],[0]
"While these models are very simple, a long experimental tradition has proven their effectiveness (Landauer and Dumais, 1997; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Blacoe and Lapata, 2012).
",3.2 Semantic space construction and composition model implementation,[0],[0]
"For the lf (lexical function) model, we construct functional matrix representations of adjectives, determiners and intransitive verbs.",3.2 Semantic space construction and composition model implementation,[0],[0]
"These are trained using Ridge regression with generalized crossvalidation from corpus-extracted vectors of nouns,
3We did not evaluate on other STS benchmarks since they have characteristics, such as high density of named entities, that would require embedding our compositional models into more complex systems, obfuscating their impact on the overall performance.
",3.2 Semantic space construction and composition model implementation,[0],[0]
"4With the multiplicative composition model we also tried Nonnegative Matrix Factorization instead of Singular Value Decomposition, because the negative values produced by SVD are potentially problematic for mult.",3.2 Semantic space construction and composition model implementation,[0],[0]
"In addition, we repeated the evaluation for the multiplicative and additive models without any form of dimensionality reduction.",3.2 Semantic space construction and composition model implementation,[0],[0]
"The overall pattern of results did not change significantly, and thus for consistency we report all models’ performance only for the SVD-reduced space.
as input, and phrases including those nouns as output (e.g., the matrix for red is trained from corpusextracted 〈noun, red-noun〉 vector pairs).",3.2 Semantic space construction and composition model implementation,[0],[0]
Transitive verb tensors are estimated using the two-step regression procedure outlined by Grefenstette et al. (2013).,3.2 Semantic space construction and composition model implementation,[0],[0]
"We did not attempt to train a lf model for the larger and more varied msrvid and onwn data sets, as this would have been extremely time consuming and impractical for all the reasons we discussed in Section 1.2 above.
",3.2 Semantic space construction and composition model implementation,[0],[0]
"Training plf (practical lexical function) proceeds similarly, but we also build preposition matrices (from 〈noun, preposition-noun〉 vector pairs), and for verbs we prepare separate subject and object matrices.
",3.2 Semantic space construction and composition model implementation,[0],[0]
"Since syntax guides lf and plf composition, we supplied all test sentences with categorial grammar parses.",3.2 Semantic space construction and composition model implementation,[0],[0]
Every sentence in the anvan1 and anvan2 datasets has the form (subject) Adjective + Noun + Transitive,3.2 Semantic space construction and composition model implementation,[0],[0]
Verb + (object),3.2 Semantic space construction and composition model implementation,[0],[0]
"Adjective + Noun, so parsing them is trivial.",3.2 Semantic space construction and composition model implementation,[0],[0]
All sentences in tfds have a predictable structure that allows perfect parsing with simple finite state rules.,3.2 Semantic space construction and composition model implementation,[0],[0]
"In all these cases, applying a general-purpose parser to the data would have, at best, had no impact and, at worst, introduced parsing errors.",3.2 Semantic space construction and composition model implementation,[0],[0]
"For msrvid and onwn, we used the output of the C&C parser (Clark and Curran, 2007).",3.2 Semantic space construction and composition model implementation,[0],[0]
"Table 5 summarizes the performance of our models on the chosen tasks, and compares it to the state of the art reported in previous work, as well as to various strong baselines.
",3.3 Results,[0],[0]
"The plf model performs very well on both anvan benchmarks, outperforming not only add and mult, but also the full-fledged lf model.",3.3 Results,[0],[0]
"Given that these data sets contain, systematically, transitive verbs, the major difference between plf and lf lies in their representation of the latter.",3.3 Results,[0],[0]
"Evidently, the separately-trained subject and object matrices of plf, being less affected by data sparseness than the 3-way tensors of lf, are better able to capture how verbs interact with their arguments.",3.3 Results,[0],[0]
"For anvan1, plf is just below the state of the art, which is based on disambiguating the verb vector in context (Kartsaklis and Sadrzadeh, 2013), and lf outperforms the baseline, which consists in using the verb vector only as a proxy to sentence similarity.5 On anvan2, plf outperforms the best model
5We report state of the art from Kartsaklis and Sadrzadeh
reported by Grefenstette (2013) (an implementation of the lexical function ideas along the lines of Grefenstette and Sadrzadeh (2011a; 2011b)).",3.3 Results,[0],[0]
"And lf is, again, the only model, besides plf, that performs better than the baseline.
",3.3 Results,[0],[0]
"In the tfds task, not surprisingly the add and mult models, lacking determiner representations and being order-insensitive, fail to distinguish between true paraphrases and foils (indeed, for the mult model foils are significantly closer to the targets than the paraphrases, probably because the latter have lower content word overlap than the foils, that often differ in word order and determiners only).",3.3 Results,[0],[0]
"Our plf approach is able to handle determiners and word order correctly, as demonstrated by a highly significant (p < 0.01) difference between paraphrase and foil similarity (average difference in cosine .017, standard deviation .077).",3.3 Results,[0],[0]
"In this case, however, the traditional lf model (average difference .044, standard deviation .092) outperforms plf.",3.3 Results,[0],[0]
"Since determiners are handled identically under the two approaches, the culprit must be word order.",3.3 Results,[0],[0]
"We conjecture that the lf 3-way tensor representation of transitive verbs leads to a stronger asymmetry between sentences with in-
(2013) rather than Kartsaklis et al. (2013), since only the former used a source corpus that is comparable to ours.
",3.3 Results,[0],[0]
"verted arguments, and thus makes this model particularly sensitive to word order differences.",3.3 Results,[0],[0]
"Indeed, if we limit evaluation to those foils characterized by word order changes only, lf discriminates between paraphrases and foils even more clearly, whereas the plf difference, while still significant, decreases slightly.
",3.3 Results,[0],[0]
"The state-of-the-art row for tfds reports the lf implementation by The Pham et al. (2013), which outperforms ours.",3.3 Results,[0],[0]
The main difference is that Pham and colleagues do not normalize vectors like we do.,3.3 Results,[0],[0]
"If we don’t normalize, we do get larger differences for our models as well, but consistently lower performance in all other tasks.",3.3 Results,[0],[0]
"More worryingly, the simple word overlap baseline reported in the table sports a larger difference than our best model.",3.3 Results,[0],[0]
"Clearly, this baseline is exploiting the systematic determiner differences in the foils and, indeed, when it is evaluated on foils where only word order changes its performance is no longer significant.
",3.3 Results,[0],[0]
"On msrvid, the plf approach outperforms add and mult, although the difference between the three is not big.",3.3 Results,[0],[0]
"Our result stands in contrast with Blacoe and Lapata (2012), the only study we are aware of that compared a sophisticated composition model (Socher et al.’s 2011 model) to add and mult on realistic sentences, which attained the top performance with the simple models for both figures of merit they used.6 The best 2012 STS system (Bär et al., 2012), obtained 0.87 correlation, but with many more and considerably more complex features than the ones we used here.",3.3 Results,[0],[0]
"Indeed, our simple system would have obtained a respectable 25/89 ranking in the STS 2012 msrvid task.",3.3 Results,[0],[0]
"Still, we must also stress the impressive performance of our baseline, given by the combination of the word overlap and sentence length cues.",3.3 Results,[0],[0]
"This suggests that the msrvid benchmark lacks the lexical and syntactic variety we would like to test our systems on.
",3.3 Results,[0],[0]
Our plf model is again the best on the onwn set (albeit by a small margin over add).,3.3 Results,[0],[0]
"This is a very positive result, in the light of the fact that the parser has very low performance on the onwn glosses, thus suggesting that plf can produce sensible semantic vectors from noisy syntac-
6We refer here to the results reported in the erratum available at http://homepages.inf.ed.ac.",3.3 Results,[0],[0]
uk/s1066731/pdf/emnlp2012erratum.pdf.,3.3 Results,[0],[0]
"The add/mult advantage was even more marked in the original paper.
",3.3 Results,[0],[0]
tic representations.,3.3 Results,[0],[0]
"Here the overlap+length baseline does not perform so well, and again the best STS 2013 system (Han et al., 2013) uses considerably richer knowledge sources and algorithms than ours.",3.3 Results,[0],[0]
"Our plf-based method would have reached a respectable 20/90 rank in the STS 2013 onwn task.
",3.3 Results,[0],[0]
"As a final remark, in all experiments the running time of plf was only slightly larger than for the simpler models, but orders of magnitude smaller than lf, confirming another practical side of our approach.",3.3 Results,[0],[0]
"We introduced an approach to compositional distributional semantics based on a linguisticallymotivated syntax-to-semantics type mapping, but simple and flexible enough that it can produce representations of English sentences of arbitrary size and structure.
",4 Conclusion,[0],[0]
"We showed that our approach is competitive against the more complex lexical function model when evaluated on the simple constructions the latter can be applied to, and it outperforms the additive and multiplicative compositionality models when tested on more realistic benchmarks (where the full-fledged lexical function approach is difficult or impossible to use), even in presence of strong noise in its syntactic input.",4 Conclusion,[0],[0]
"While our results are encouraging, no current benchmark combines large-scale, real-life data with the syntactic variety on which a syntax-driven approach to semantics such as ours could truly prove its worth.",4 Conclusion,[0],[0]
"The recently announced SemEval 2014 Task 17 is filling exactly this gap, and we look forward to apply our method to this new benchmark, as soon as it becomes available.
",4 Conclusion,[0],[0]
One of the strengths of our framework is that it allows for incremental improvement focused on specific constructions.,4 Conclusion,[0],[0]
"For example, one could add representations for different conjunctions (and vs. or), train matrices for verb arguments other than subject and direct object, or include new types of modifiers into the model, etc.
While there is potential for local improvements, our framework, which extends and improves on existing compositional semantic vector models, has demonstrated its ability to account for full sentences in a principled and elegant way.",4 Conclusion,[0],[0]
"Our implementation of the model relies on simple and effi-
7http://alt.qcri.org/semeval2014/ task1/
cient training, works fast, and shows good empirical results.",4 Conclusion,[0],[0]
We thank Roberto Zamparelli and the COMPOSES team for helpful discussions.,Acknowledgements,[0],[0]
This research was supported by the ERC 2011 Starting Independent Research Grant n. 283554 (COMPOSES).,Acknowledgements,[0],[0]
"Distributional semantic methods to approximate word meaning with context vectors have been very successful empirically, and the last years have seen a surge of interest in their compositional extension to phrases and sentences.",abstractText,[0],[0]
"We present here a new model that, like those of Coecke et al. (2010) and Baroni and Zamparelli (2010), closely mimics the standard Montagovian semantic treatment of composition in distributional terms.",abstractText,[0],[0]
"However, our approach avoids a number of issues that have prevented the application of the earlier linguistically-motivated models to full-fledged, real-life sentences.",abstractText,[0],[0]
"We test the model on a variety of empirical tasks, showing that it consistently outperforms a set of competitive rivals.",abstractText,[0],[0]
1 Compositional distributional semantics,abstractText,[0],[0]
"The research of the last two decades has established empirically that distributional vectors for words obtained from corpus statistics can be used to represent word meaning in a variety of tasks (Turney and Pantel, 2010).",abstractText,[0],[0]
"If distributional vectors encode certain aspects of word meaning, it is natural to expect that similar aspects of sentence meaning can also receive vector representations, obtained compositionally from word vectors.",abstractText,[0],[0]
"Developing a practical model of compositionality is still an open issue, which we address in this paper.",abstractText,[0],[0]
"One approach is to use simple, parameterfree models that perform operations such as pointwise multiplication or summing (Mitchell and Lapata, 2008).",abstractText,[0],[0]
"Such models turn out to be surprisingly effective in practice (Blacoe and Lapata, 2012), but they have obvious limitations.",abstractText,[0],[0]
"For instance, symmetric operations like vector addition are insensitive to syntactic structure, therefore meaning differences encoded in word order are lost in composition: pandas eat bamboo is identical to bamboo eats pandas.",abstractText,[0],[0]
"Guevara (2010), Mitchell and Lapata (2010), Socher et al. (2011) and Zanzotto et al. (2010) generalize the simple additive model by applying structure-encoding operators to the vectors of two sister nodes before addition, thus breaking the inherent symmetry of the simple additive model.",abstractText,[0],[0]
"A related approach (Socher et al., 2012) assumes richer lexical representations where each word is represented with a vector and a matrix that encodes its interaction with its syntactic sister.",abstractText,[0],[0]
The training proposed in this model estimates the parameters in a supervised setting.,abstractText,[0],[0]
"Despite positive empirical evaluation, this approach is hardly practical for generalpurpose semantic language processing, since it requires computationally expensive approximate parameter optimization techniques, and it assumes task-specific parameter learning whose results are not meant to generalize across tasks.",abstractText,[0],[0]
1.1,abstractText,[0],[0]
"The lexical function model None of the proposals mentioned above, from simple to elaborate, incorporates in its architecture the intuitive idea (standard in theoretical linguistics) that semantic composition is more than a weighted combination of words.",abstractText,[0],[0]
"Generally one of the components of a phrase, e.g., an adjective, acts as a function affecting the other component (e.g., a noun).",abstractText,[0],[0]
"This underlying intuition, adopted from formal semantics of natural language, motivated the creation of the lexical function model of composition (lf ) (Baroni and Zamparelli, 2010; Coecke et al., 2010).",abstractText,[0],[0]
"The lf model can be seen as a projection of the symbolic Montagovian approach to semantic composition in natural language onto the domain of vector spaces and linear operations on them (Baroni et al., 2013).",abstractText,[0],[0]
"In lf, arguments are vectors and functions taking arguments (e.g., adjectives that combine with nouns) are tensors, with the number of arguments (n) determining the",abstractText,[0],[0]
A practical and linguistically-motivated approach to compositional distributional semantics,title,[0],[0]
"Low-rank matrix recovery has received increasing attention in recent years, due to its wide range of applications including signal processing, computer vision and collaborative filtering (Rennie & Srebro, 2005; Ahmed & Romberg, 2015).",1. Introduction,[0],[0]
The objective is to estimate,1. Introduction,[0],[0]
an unknown rank-r matrix X∗ ∈,1. Introduction,[0],[0]
Rd1×d2 based on partially observed measurements.,1. Introduction,[0],[0]
"More formally, low-rank matrix recovery can be formulated as the following optimization problem
min X∈C
Fn(X) subject to rank(X) ≤",1. Introduction,[0],[0]
"r, (1.1)
*Equal contribution 1Department of Computer Science, University of Virginia, Charlottesville, VA 22904,",1. Introduction,[0],[0]
"USA. 2Department of Computer Science, University of California, Los Angeles, Los Angeles, CA 90095, USA.",1. Introduction,[0],[0]
"Correspondence to: Quanquan Gu <qgu@cs.ucla.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
where Fn : Rd1×d2 → R denotes a general sample loss function with respect to n measurements, and C denotes a constraint set such that X∗",1. Introduction,[0],[0]
∈ C.,1. Introduction,[0],[0]
"For example, C is set to be Rd1×d2 in matrix sensing (Recht et al., 2010; Negahban & Wainwright, 2011), and is chosen to be the set of incoherent matrices in matrix completion (Rohde et al., 2011; Koltchinskii et al., 2011; Negahban & Wainwright, 2012) and one-bit matrix completion (Cai & Zhou, 2013; Davenport et al., 2014).
",1. Introduction,[0],[0]
"Tremendous efforts have been made to efficiently solve (1.1), among which the most popular ones are nuclear norm relaxation based methods (Srebro et al., 2004; Candès & Tao, 2010; Rohde et al., 2011; Recht et al., 2010; Recht, 2011; Negahban & Wainwright, 2011; 2012; Gui & Gu, 2015).",1. Introduction,[0],[0]
"These methods can achieve near optimal sample complexity for recovery (Balcan et al., 2017), but a singular value decomposition (SVD) step, whose time complexity is O(d3)1, is required at each iteration, which is computationally expensive for large-scale datasets.",1. Introduction,[0],[0]
"To avoid using SVD, the most commonly-used technique is based on BurerMonteiro factorization (Burer & Monteiro, 2003), which reparameterizes the low-rank matrix X as the product of two smaller matrices U ∈ Rd1×r and V ∈ Rd2×r such that X = UV>.",1. Introduction,[0],[0]
"Instead of optimizing (1.1) directly, we turn to solve the following nonconvex optimization problem
min U∈C1,V∈C2
Fn(UV>), (1.2)
where C1 ⊆ Rd1×r, C2 ⊆ Rd2×r are some constraint sets induced by C (c.f. Section 2.1).",1. Introduction,[0],[0]
"Note that (1.2) automatically ensures the low-rankness of the estimated matrix.
",1. Introduction,[0],[0]
"A line of research (Bach et al., 2008; Keshavan et al., 2009; Lee et al., 2013; Jain et al., 2013; Bach, 2013; Hardt, 2014; Hardt & Wootters, 2014; Netrapalli et al., 2014; Jain & Netrapalli, 2014; Haeffele et al., 2014; Sun & Luo, 2015; Bhojanapalli et al., 2015; Chen & Wainwright, 2015; Zhao et al., 2015; Tu et al., 2015; Chen & Wainwright, 2015; Zheng & Lafferty, 2015; 2016; Park et al., 2016b; Jin et al., 2016; Gu et al., 2016; Wang et al., 2017a;b; Xu et al., 2017; Zhang et al., 2018) proposed to solve (1.2) based on gradient
1We assume d1 = d2 = d when discussing the sample complexity for simplicity.
descent and/or alternating minimization, and established a locally linear convergence property provided that the initial solution falls into a basin of attraction, i.e., a small neighbourhood around the optimum.",1. Introduction,[0],[0]
"Recently, another line of research (Bhojanapalli et al., 2016; Ge et al., 2016; Park et al., 2016c; Li et al., 2016; Zhu et al., 2017a; Ge et al., 2017) directly characterized the optimization landscape of (1.2) and proved that various low-rank matrix recovery problems, including matrix sensing (Bhojanapalli et al., 2016; Park et al., 2016c; Zhu et al., 2017a), matrix completion (Ge et al., 2016), and robust PCA (Ge et al., 2017), have no spurious local minima, i.e., all local minima are global ones.",1. Introduction,[0],[0]
"Based on existing results on finding local minimum for certain nonconvex problems (Ge et al., 2015; Carmon et al., 2016; Agarwal et al., 2016; Jin et al., 2017), they further showed that (1.2) can be successfully solved by saddle-avoiding algorithms, such as perturbed gradient descent.",1. Introduction,[0],[0]
"However, none of the aforementioned work is generic enough to cover objective functions beyond square loss, e.g., the sample loss function for one-bit matrix completion (Davenport et al., 2014).
",1. Introduction,[0],[0]
"Following the second line of research, we propose a primaldual analysis to characterize the landscape of general objective functions in nonconvex low-rank matrix recovery including both square loss and beyond.",1. Introduction,[0],[0]
"By using restricted strongly convex and smooth conditions (Negahban et al., 2009; Negahban & Wainwright, 2011), we are able to characterize a large family of low-rank problems.",1. Introduction,[0],[0]
"To incorporate the widely-used incoherence constraints for low-rank matrix estimation, we propose to analyze the corresponding Lagrangian function rather than the primal objective function and use the Karush-Kuhn-Tucker (KKT) condition (Nocedal & Wright, 2006) to characterize the local minima of (1.2).",1. Introduction,[0],[0]
"Our analysis shows that the optimization landscape of (1.2) is well-behaved, i.e., there are no spurious local minima.",1. Introduction,[0],[0]
"In addition, we demonstrate empirically that the primal-dual based algorithm proposed in (Nocedal & Wright, 2006) can recover the ground truth matrix successfully.",1. Introduction,[0],[0]
Our major contributions are further highlighted as follows.,1. Introduction,[0],[0]
"• Our general framework can be applied to any loss function that satisfies the restricted strongly convex and smooth conditions (c.f. Section 3), which covers a broad family of loss functions for low-rank problems.",1.1. Contributions,[0],[0]
"All the existing theoretical analyses (Bhojanapalli et al., 2016; Ge et al., 2016; Park et al., 2016c; Li et al., 2016; Zhu et al., 2017a; Ge et al., 2017) are limited to square loss, thus we resolve an open problem raised in Ge et al. (2017) regarding the characterization of global geometry for one-bit matrix completion.
",1.1. Contributions,[0],[0]
"• Our primal-dual analysis is applicable to various noisy
low-rank problems.",1.1. Contributions,[0],[0]
"In particular, our analysis suggests there are no spurious local minima in noisy matrix completion, provided that the number of observations is O(r2d log d).",1.1. Contributions,[0],[0]
"Compared with existing studies (Ge et al., 2016; 2017) whose sample complexity scales to the fourth power with the rank, the sample requirement of our method matches the best-known sample complexity of matrix completion using nonconvex optimization algorithm (Zheng & Lafferty, 2016) under the incoherence condition.
",1.1. Contributions,[0],[0]
•,1.1. Contributions,[0],[0]
"Compared with the seminal work (Ge et al., 2016; 2017) along this line that makes use of ad hoc regularizer to deal with incoherence constraints, our primaldual analytic framework directly characterizes the global geometry of constrained nonconvex optimization problem for low-rank matrix recovery using duality theory.",1.1. Contributions,[0],[0]
"We believe the Lagrangian based proof technique is of independent interest, which can be extended to handle more general inequality constraints in other nonconvex problems.",1.1. Contributions,[0],[0]
Characterizing the landscape of various objective functions has attracted more and more attention in recent years.,1.2. Related Work,[0],[0]
"For instance, Sun et al. (2015) studied the nonconvex geometry of complete dictionary recovery problem, and proved that all local minima are global ones.",1.2. Related Work,[0],[0]
Sun et al. (2016) showed that a nonconvex fourth-order polynomial objective for phase retrieval has no spurious local minimum and all global minima are equivalent.,1.2. Related Work,[0],[0]
"Lee et al. (2016) showed that gradient descent converges to local minimum almost surely, using the stable manifold theorem from dynamical system.",1.2. Related Work,[0],[0]
Ge et al. (2016) proved that the commonly used nonconvex objective function for positive semidefinite matrix completion has no spurious local minimum.,1.2. Related Work,[0],[0]
"In an independent work, Bhojanapalli et al. (2016) proved that positive semidefinite (PSD) matrix sensing, a very related problem to matrix completion, has no spurious local minima under the restricted isometry property (RIP).",1.2. Related Work,[0],[0]
"Later on, Park et al. (2016c) extended the geometric analysis of matrix sensing from PSD matrices to rectangular matrices.",1.2. Related Work,[0],[0]
"Zhu et al. (2017a) provided a unified geometric analysis for objective functions satisfying the restricted strong convexity/smoothness property, but their work cannot deal with the constrained optimization, e.g., matrix completion and one-bit matrix completion.
",1.2. Related Work,[0],[0]
"Most recently, several studies attempted to unify the global geometry analyses for nonconvex low-rank matrix recovery problems.",1.2. Related Work,[0],[0]
"For instance, Li et al. (2016) proposed a general theory to characterize the global geometry of positive semidefinite low-rank matrix factorization problem.",1.2. Related Work,[0],[0]
"Zhu et al. (2017b) further extended the geometric analysis in Li et al. (2016) to rectangular matrix factorization problem.
",1.2. Related Work,[0],[0]
"Nevertheless, both of their analyses require the objective function to be quadratic (i.e., square loss function), which is not applicable to constrained low-rank matrix recovery problems.",1.2. Related Work,[0],[0]
"The most relevant work to ours is Ge et al. (2017), which proposed a general framework to characterize the landscape of nonconvex low-rank matrix recovery problem.",1.2. Related Work,[0],[0]
"More specifically, they incorporated the constraints for matrix completion and robust PCA by a specifically designed regularizer, to make the solution lie in a desired region (e.g., the set of incoherent matrices).",1.2. Related Work,[0],[0]
"However, their framework still requires the loss function to be quadratic, thus unable to analyze low-rank problems with general objective function, such as one-bit matrix completion.",1.2. Related Work,[0],[0]
The remainder of this paper is organized as follows.,1.3. Organization and Notation,[0],[0]
We formally state the general low-rank matrix recovery problem and introduce two specific applications in Section 2.,1.3. Organization and Notation,[0],[0]
"In Section 3, we lay out conditions for the proposed primal-dual based framework and present our main theoretical results.",1.3. Organization and Notation,[0],[0]
"In Section 4, we apply the general results to two specific low-rank problems.",1.3. Organization and Notation,[0],[0]
"The primal-dual based method and the numerical experiments are illustrated in Sections 5 and 6, respectively.",1.3. Organization and Notation,[0],[0]
"We conclude in Section 7 and defer the detailed proofs to the supplementary materials.
",1.3. Organization and Notation,[0],[0]
"We use capital symbols such as A to denote matrices and [d] to denote index set {1, 2, . . .",1.3. Organization and Notation,[0],[0]
", d}.",1.3. Organization and Notation,[0],[0]
"Let the i-th row, jth column and (i, j)-th entry of A be Ai,∗, A∗,j and Aij respectively.",1.3. Organization and Notation,[0],[0]
Denote the i-th standard basis by ei and the `-th largest singular value of A by σ`(A).,1.3. Organization and Notation,[0],[0]
We use vec(A) to denote the vectorization of matrix A.,1.3. Organization and Notation,[0],[0]
"For any vector x, we use ‖x‖2 to denote its `2 norm.",1.3. Organization and Notation,[0],[0]
"Let ‖A‖F , ‖A‖2 be the Frobenius norm and the spectral norm of matrix A, respectively.",1.3. Organization and Notation,[0],[0]
"We define the largest `2 norm of its rows as ‖A‖2,∞ = maxi ‖Ai,∗‖2.",1.3. Organization and Notation,[0],[0]
"For any two sequences {an} and {bn}, if there exists a constant 0 < C < ∞ such that an ≤ Cbn, then we denote an = O(bn).",1.3. Organization and Notation,[0],[0]
"In this section, we introduce our general framework for low-rank matrix recovery, along with several applications.",2. Constrained Nonconvex Optimization for Low-Rank Matrix Recovery,[0],[0]
"Let the singular value decomposition of the unknown lowrank matrix be X∗ = U ∗ ΣV ∗> and U∗, V∗ be the underlying factorized matrices such that U∗ = U ∗ Σ1/2, V∗",2.1. Generic Framework,[0],[0]
= V ∗ Σ1/2.,2.1. Generic Framework,[0],[0]
Denote the sorted singular values of X∗ by σ1 ≥ σ2 ≥ . . .,2.1. Generic Framework,[0],[0]
≥,2.1. Generic Framework,[0],[0]
σr > 0.,2.1. Generic Framework,[0],[0]
Recall that we aim to characterize the global optimality of the general nonconvex optimization problem (1.2).,2.1. Generic Framework,[0],[0]
"Formally, we define the general
constraint sets C1, C2 as follows
C1 = { U ∈ Rd1×r ∣∣ ‖U‖2,∞ ≤ α1}, C2 = { V ∈ Rd1×r
∣∣ ‖V‖2,∞ ≤ α2}, (2.1) where α1, α2 will be specified for different examples.",2.1. Generic Framework,[0],[0]
"To guarantee optimization problem (1.2) is well-defined, we assume U∗ ∈ C1 and V∗ ∈ C2.",2.1. Generic Framework,[0],[0]
It is worth noting that (2.1) can cover a wide range of low-rank matrix recovery problems.,2.1. Generic Framework,[0],[0]
"For instance, in matrix completion, constraint sets in the form of (2.1) are proposed to ensure the estimator X = UV> is incoherent.
",2.1. Generic Framework,[0],[0]
"In addition, following Tu et al. (2015); Zheng & Lafferty (2016); Park et al. (2016a); Wang et al. (2017a), we add an additional regularization term ‖U>U −V>V‖2F to (1.2) such that the solutions U, V are in similar scale.",2.1. Generic Framework,[0],[0]
"Specifically, we propose to analyze the following constrained optimization problem with respect to the stacked matrix Z =",2.1. Generic Framework,[0],[0]
"[U; V] in the lifted space R(d1+d2)×r
min Z∈D
G(Z) = Fn(UV>)",2.1. Generic Framework,[0],[0]
"+ γ
4 ‖U>U−V>V‖2F , (2.2)
",2.1. Generic Framework,[0],[0]
where the constraint set D is defined as D = {Z ∈ R(d1+d2)×r,2.1. Generic Framework,[0],[0]
"| ‖Z‖2,∞ ≤ α}, where α = max{α1, α2}, and γ denotes the regularization parameter.",2.1. Generic Framework,[0],[0]
"We briefly introduce two specific examples: noisy matrix completion and one-bit matrix completion.
",2.2. Specific Examples,[0],[0]
Noisy matrix completion.,2.2. Specific Examples,[0],[0]
"The goal of matrix completion (Rohde et al., 2011; Koltchinskii et al., 2011; Negahban & Wainwright, 2012) is to estimate the unknown low-rank matrix X∗ from its partially observed (noisy) entries.",2.2. Specific Examples,[0],[0]
"More specifically, we consider the uniform observation model
Yjk = { X∗jk + Ejk, for any (j, k) ∈ Ω; ∗, otherwise.",2.2. Specific Examples,[0],[0]
"(2.3)
where Ω ⊆",2.2. Specific Examples,[0],[0]
"[d1] × [d2] denotes the observed index set such that for any (j, k) ∈ Ω, j ∼ uniform([d1]) and k ∼ uniform([d2]).",2.2. Specific Examples,[0],[0]
"Here, Y ∈ Rd1×d2 denotes the observed data matrix, E ∈ Rd1×d2 denotes a random noise matrix such that each entry of E follows i.i.d.",2.2. Specific Examples,[0],[0]
"Gaussian distribution with variance ν2/(d1d2).
",2.2. Specific Examples,[0],[0]
"As discussed in previous work (Gross, 2011; Negahban & Wainwright, 2012), it is impossible to recover the unknown low-rank matrix X∗ if it is too sparse.",2.2. Specific Examples,[0],[0]
"To avoid such issue, we impose the following incoherence condition (Candès & Recht, 2009) on the singular spaces of X∗
‖U∗‖2,∞ ≤",2.2. Specific Examples,[0],[0]
"√ βr
d1 and ‖V∗‖2,∞ ≤
√ βr
d2 , (2.4)
",2.2. Specific Examples,[0],[0]
"where r denotes the rank of X∗, and β denotes the incoherence parameter.",2.2. Specific Examples,[0],[0]
"Note that based on the incoherence condition (2.4), we can derive ‖X∗‖∞,∞ ≤ ‖U
∗‖2,∞ · ‖Σ‖2 · ‖V∗‖2,∞ ≤ βrσ1/",2.2. Specific Examples,[0],[0]
"√ d1d2, which leads to a dimension-free signal-to-noise ratio in observation model (2.3).
",2.2. Specific Examples,[0],[0]
"More specifically, we consider the following constrained optimization problem for matrix completion
min Z∈D
1
2p ∑ (j,k)∈Ω (Uj,∗V > k,∗ − Yjk)2",2.2. Specific Examples,[0],[0]
+,2.2. Specific Examples,[0],[0]
"γ 4 ‖U>U−V>V‖2F ,
(2.5)
where D is defined in Section 2.1 and p = |Ω|/(d1d2) denotes the sampling rate.",2.2. Specific Examples,[0],[0]
In order to guarantee Z∗ =,2.2. Specific Examples,[0],[0]
"[U∗; V∗] ∈ D, we set α = √ βrσ∗1/d, where d1 and d2 are in the same order O(d) for simplicity.
",2.2. Specific Examples,[0],[0]
One-bit matrix completion.,2.2. Specific Examples,[0],[0]
"The objective of one-bit matrix completion (Davenport et al., 2014; Cai & Zhou, 2013; Ni & Gu, 2016) is to recover the unknown low-rank matrix X∗ from a set of binary observations.",2.2. Specific Examples,[0],[0]
"In detail, the dependence of the observed binary matrix Y ∈ Rd1×d2 on X∗ is presented as follows
Yjk = { +1, if X∗jk + Ejk > 0, −1, if X∗jk + Ejk < 0,
(2.6)
where E ∈ Rd1×d2 denotes a random noise matrix.",2.2. Specific Examples,[0],[0]
"Let f be the cumulative distribution function of −Ejk, then the observation model (2.6) can be recast as the following probabilistic model
Yjk = { +1, with probability f(X∗jk), −1, with probability 1− f(X∗jk).
(2.7)
",2.2. Specific Examples,[0],[0]
"In addition, we consider the same uniform sampling model for the observed index set Ω as in matrix completion, and impose the incoherence condition (2.4) on X∗ to avoid the overly sparse matrices.",2.2. Specific Examples,[0],[0]
"Specifically, we aim to solve the following optimization problem for one-bit matrix completion
min Z∈D − 1 |Ω| ∑ (j,k)∈Ω { 1 (Yjk=1) log ( f(Uj∗V > k∗) )
",2.2. Specific Examples,[0],[0]
"+ 1 (Yjk=−1)
log ( 1− f(Uj,∗V>k,∗) )} + γ
4 ‖U>U−V>V‖2F ,
(2.8)
where Ω ⊆",2.2. Specific Examples,[0],[0]
"[d1] × [d2] denotes the observed index set with cardinality |Ω|, and we also set the parameter α =√ βrσ∗1/d in the constraint set D to ensure optimization probelm (2.8) is well-defined.",2.2. Specific Examples,[0],[0]
"Before presenting our main theoretical results, we first lay out the formal definition of local minimizer and the basic
necessary optimality conditions with respect to constrained optimization problem (2.2) .
",3. Results for Generic Framework,[0],[0]
Definition 3.1.,3. Results for Generic Framework,[0],[0]
"Z is a local minimizer of constrained optimization (2.2), if Z satisfies the following conditions: (i) Z ∈ D. (ii)",3. Results for Generic Framework,[0],[0]
"There exists a neighbourhood N of Z such that for any Z̃ ∈ N ∩ D, G(Z̃)",3. Results for Generic Framework,[0],[0]
"≥ G(Z) holds.
",3. Results for Generic Framework,[0],[0]
"For general constrained optimization (2.2), we provide the fundamental first-order necessary condition as follows.
",3. Results for Generic Framework,[0],[0]
Lemma 3.2.,3. Results for Generic Framework,[0],[0]
Suppose Z is a local minimizer of constrained optimization problem (3.1).,3. Results for Generic Framework,[0],[0]
"Then for all feasible directions2 ∆ ∈ R(d1+d2)×r, the following inequality holds:
〈∇G(Z),∆〉 ≥ 0.
",3. Results for Generic Framework,[0],[0]
Recall that the constraint set D is defined as D = {Z ∈ R(d1+d2)×r,3. Results for Generic Framework,[0],[0]
"| ‖Z‖2,∞ ≤ α}.",3. Results for Generic Framework,[0],[0]
"Thus, according to the definition of ‖ · ‖2,∞, we can reformulate (2.2) as the following equivalent standard form
min Z∈R(d1+d2)×r
G(Z) = Fn(UV>) +",3. Results for Generic Framework,[0],[0]
"γ
4 ‖U>U−V>V‖2F
subject to hi(Z) ≤ 0, for all i ∈",3. Results for Generic Framework,[0],[0]
"[d1 + d2], (3.1)
where hi(Z) = ‖e>i Z‖22 − α2, and ei represents the i-th natural basis.",3. Results for Generic Framework,[0],[0]
"Accordingly, the Lagrangian with respect to (3.1) is given as follows
L(Z,λ) = G(Z) + d1+d2∑ i=1 λihi(Z),
where λ =",3. Results for Generic Framework,[0],[0]
"[λ1, λ2, . . .",3. Results for Generic Framework,[0],[0]
λ(d1+d2)],3. Results for Generic Framework,[0],[0]
> denotes the Lagrange multiplier vector.,3. Results for Generic Framework,[0],[0]
"Based on the Lagrangian, we give the following necessary optimality conditions (Nocedal & Wright, 2006) for constrained optimization problem (3.1).
",3. Results for Generic Framework,[0],[0]
Lemma 3.3.,3. Results for Generic Framework,[0],[0]
Let Z be a local minimizer of constrained optimization problem (3.1).,3. Results for Generic Framework,[0],[0]
Define the set of active constraint gradients at Z as A(Z),3. Results for Generic Framework,[0],[0]
= {i ∈,3. Results for Generic Framework,[0],[0]
[d1 + d2] | hi(Z) = 0}.,3. Results for Generic Framework,[0],[0]
Suppose the active set {∇hi(Z) | i ∈ A(Z)} is linearly independent.,3. Results for Generic Framework,[0],[0]
"Then there exists a Lagrange multiplier vector λ such that (Z,λ) satisfies the following Karush-Kuhn-Tucker (KKT) conditions
1. hi(Z)",3. Results for Generic Framework,[0],[0]
"≤ 0, for all i ∈",3. Results for Generic Framework,[0],[0]
"[d1 + d2],
2.",3. Results for Generic Framework,[0],[0]
"λ ≥ 0,
3. λihi(Z)",3. Results for Generic Framework,[0],[0]
"= 0, for all i ∈",3. Results for Generic Framework,[0],[0]
"[d1 + d2],
4.",3. Results for Generic Framework,[0],[0]
"∇ZL(Z,λ)",3. Results for Generic Framework,[0],[0]
= 0.,3. Results for Generic Framework,[0],[0]
"2We say ∆ is a feasible direction for constraint set D at Z, if
there exists t > 0 such that Z + s∆ ∈ D for all s ∈",3. Results for Generic Framework,[0],[0]
"[0, t].
",3. Results for Generic Framework,[0],[0]
Lemma 3.4.,3. Results for Generic Framework,[0],[0]
"Under the same conditions as in Lemma 3.3, let λ be a Lagrange multiplier vector such that (Z,λ) satisfies the KKT condition.",3. Results for Generic Framework,[0],[0]
"For any ∆ ∈ R(d1+d2)×r satisfying the condition that 〈∇hi(Z),∆〉 ≤ 0 holds for all i ∈ A(Z), (Z,λ) also satisfies the following inequality
vec(∆)>∇2ZL(Z,λ)vec(∆) ≥ 0.
",3. Results for Generic Framework,[0],[0]
The aforementioned necessary optimality conditions characterize the properties of local minima with respect to constrained optimization problem.,3. Results for Generic Framework,[0],[0]
"As will be seen in later analysis, these optimality conditions are essential to show that there are no spurious local minima in (3.1).
",3. Results for Generic Framework,[0],[0]
"Next, we lay out several conditions for the general sample loss function Fn.",3. Results for Generic Framework,[0],[0]
"To begin with, we present the restricted strong convexity and smoothness conditions (Negahban et al., 2009; Loh & Wainwright, 2013).
",3. Results for Generic Framework,[0],[0]
Condition 3.5 (Restricted Strong Convexity).,3. Results for Generic Framework,[0],[0]
"The sample loss function Fn is µ-restricted strongly convex, i.e., for all matrices Y,W ∈ Rd1×d2 with rank at most 6r
Fn(Y) ≥ Fn(W) +",3. Results for Generic Framework,[0],[0]
"〈∇Fn(W),Y −W〉+ µ
2 ‖Y −W‖2F .
",3. Results for Generic Framework,[0],[0]
Condition 3.6 (Restricted Strong Smoothness).,3. Results for Generic Framework,[0],[0]
"The sample loss function Fn is L-restricted strongly smooth, i.e., for all matrices Y,W ∈ Rd1×d2 with rank at most 6r
Fn(Y) ≤",3. Results for Generic Framework,[0],[0]
Fn(W) +,3. Results for Generic Framework,[0],[0]
"〈∇Fn(W),Y −W〉+ L
2 ‖Y −W‖2F .
",3. Results for Generic Framework,[0],[0]
"In addition, we assume there is an upper bound for the gradient of the sample loss function ∇Fn with respect to the unknown low-rank matrix X∗.
Condition 3.7.",3. Results for Generic Framework,[0],[0]
Given a fixed sample size n and tolerance parameter δ ∈,3. Results for Generic Framework,[0],[0]
"(0, 1).",3. Results for Generic Framework,[0],[0]
"Let (n, δ) be the smallest scalar such that with probability at least 1− δ, we have
‖∇Fn(X∗)‖2 ≤ (n, δ),
where (n, δ) depends on sample size n and δ, and (n, δ) deceases as n increases.
",3. Results for Generic Framework,[0],[0]
"As will be clear in the next section and in the proofs, Conditions 3.5, 3.6 and 3.7 can be verified for a wide range of sample loss functions, such as the objective functions in matrix completion and one-bit matrix completion.
",3. Results for Generic Framework,[0],[0]
"Finally, we present the main results regarding the global optimality of optimization problem (3.1).",3. Results for Generic Framework,[0],[0]
"In particular, we are going to show that under proper conditions, (3.1) has no spurious local minima.
",3. Results for Generic Framework,[0],[0]
Theorem 3.8.,3. Results for Generic Framework,[0],[0]
"Assume the sample loss functionFn satisfies Conditions 3.5, 3.6 and 3.7.",3. Results for Generic Framework,[0],[0]
"Under condition that L/µ ∈
(1, 18/17), for all local minima Z =",3. Results for Generic Framework,[0],[0]
[U; V] of optimization problem (3.1) with regularization parameter γ satisfying µ − L/2,3. Results for Generic Framework,[0],[0]
≤,3. Results for Generic Framework,[0],[0]
γ < min{(22µ,3. Results for Generic Framework,[0],[0]
"− 19L)/4, (3L − 2µ)/2}, the reconstruction error satisfies
‖UV> −X∗‖2F ≤",3. Results for Generic Framework,[0],[0]
"Γr 2(n, δ), (3.2)
with probability at least 1− δ, where Γ is a constant depending on µ,L and γ.
Remark 3.9.",3. Results for Generic Framework,[0],[0]
Theorem 3.8 suggests that for all local minima Z =,3. Results for Generic Framework,[0],[0]
"[U; V] of (3.1), the reconstructed matrix UV> lies in a small neighbourhood of X∗, and the radius of such neighbourhood decreases as the sample size n increases.",3. Results for Generic Framework,[0],[0]
"While in the noiseless case ( (n, δ) = 0), the right hand side of (3.2) is 0, which suggests that all local minima are global ones, i.e., UV> = X∗. Note that we require the condition number L/µ to be close to 1 in Theorem 3.8.",3. Results for Generic Framework,[0],[0]
"As will be clear in the next section and in the proofs, this assumption can be verified for the specific examples discussed in Section 2.1.",3. Results for Generic Framework,[0],[0]
"Similar assumption has been imposed in existing work on matrix sensing (Bhojanapalli et al., 2016; Ge et al., 2017), in that the restricted isometry property parameter is required to be in a small range around 0.",3. Results for Generic Framework,[0],[0]
"In this section, we illustrate how to apply our general framework to two specific low-rank problems: noisy matrix completion and one-bit matrix completion.",4. Implications for Specific Examples,[0],[0]
"Note that given the general results in Section 3, we only need to verify Conditions 3.5, 3.6, 3.7 and the assumption regarding the restricted condition number L/µ for each specific example.",4. Implications for Specific Examples,[0],[0]
"Recall that for noisy matrix completion, we aim to optimize (2.5) under the uniform sampling model (2.3) and incoherence condition (2.4).",4.1. Results for Noisy Matrix Completion,[0],[0]
"Specifically, we verify Conditions 3.5, 3.6 and 3.7 for Fn in the following corollary to characterize the global optimality of noisy matrix completion.
",4.1. Results for Noisy Matrix Completion,[0],[0]
Corollary 4.1.,4.1. Results for Noisy Matrix Completion,[0],[0]
Consider noisy matrix completion problem (2.5) under the uniform sampling model.,4.1. Results for Noisy Matrix Completion,[0],[0]
Suppose the unknown rank-r matrix X∗ satisfies incoherence condition (2.4) and each entry of the noise matrix E follows i.i.d.,4.1. Results for Noisy Matrix Completion,[0],[0]
Gaussian distribution with variance ν2/(d1d2).,4.1. Results for Noisy Matrix Completion,[0],[0]
Provided the number of observed samples |Ω| ≥ c1r2d,4.1. Results for Noisy Matrix Completion,[0],[0]
"log d, with regularization parameter γ = 1/2, all local minima Z =",4.1. Results for Noisy Matrix Completion,[0],[0]
"[U; V] of optimization problem (2.5) satisfy
‖UV> −X∗‖F ≤",4.1. Results for Noisy Matrix Completion,[0],[0]
"c2 max { ν, √ rβσ1 }√rd log d n ,
with probability at least 1 − c2/d , where c1, c2 are both universal constants.
",4.1. Results for Noisy Matrix Completion,[0],[0]
Remark 4.2.,4.1. Results for Noisy Matrix Completion,[0],[0]
"Due to the existence of noise, it is impossible to exactly recover the unknown low-rank matrix X∗ for noisy matrix completion.",4.1. Results for Noisy Matrix Completion,[0],[0]
"However, we show that the reconstruction UV> from any local minimizer of (2.5) is actually a good estimator of X∗.",4.1. Results for Noisy Matrix Completion,[0],[0]
"The estimation error is in the order of O( √ r2d log d/n), which suggests that the more observations we have, the smaller estimation error we can achieve.",4.1. Results for Noisy Matrix Completion,[0],[0]
"It is worth noting that we only need O(r2d log d) observed entries of X∗ to ensure (3.1) has no spurious local minima, in sharp contrast to existing work (Ge et al., 2016; 2017) whose sample complexity is at least O(r4d log d).",4.1. Results for Noisy Matrix Completion,[0],[0]
Recall that the objective of one-bit matrix completion is to solve optimization problem (2.8).,4.2. Results for One-Bit Matrix Completion,[0],[0]
"We assume the standard regularity condition (Cai & Zhou, 2013) on the cumulative distribution function f in (2.7) as follows
sup |x|≤β′
{ |f ′(x)|/ ( f(x)(1− f(x)) )} ≤ γβ′ , (4.1)
where γβ′ reflects the steepness of the sample loss function, and when f and β′ are given, γβ′ is a fixed constant.",4.2. Results for One-Bit Matrix Completion,[0],[0]
"We note that this condition holds for a large family of distributions, such as Logistic distribution, Gaussian distribution, and Laplacian distribution.",4.2. Results for One-Bit Matrix Completion,[0],[0]
"To apply the results in the unified framework, it suffices to prove Conditions 3.5, 3.6 and 3.7 for one-bit matrix completion, respectively.
",4.2. Results for One-Bit Matrix Completion,[0],[0]
Corollary 4.3.,4.2. Results for One-Bit Matrix Completion,[0],[0]
Assume that the observed matrix Y follows the binary observation model (2.7) generated based on a cumulative distribution function f satisfying (4.1).,4.2. Results for One-Bit Matrix Completion,[0],[0]
Suppose the unknown low-rank matrix X∗ satisfies incoherence condition (2.4) and the observed index set Ω follows the uniform sampling model.,4.2. Results for One-Bit Matrix Completion,[0],[0]
"If the sample complexity |Ω| exceeds c1r2d log d, and the regularization parameter is set as γ = 1/2, then with probability at least 1 − c2/d, all local minima Z =",4.2. Results for One-Bit Matrix Completion,[0],[0]
"[U; V] of optimization problem (2.8) satisfy
‖UV> −X∗‖F ≤ c3 max{γβ′ , √ rβσ1}
√ rd log d
n ,
where c1, c2, c3 are all universal constants.
",4.2. Results for One-Bit Matrix Completion,[0],[0]
Remark 4.4.,4.2. Results for One-Bit Matrix Completion,[0],[0]
Corollary 4.3 shows all local minima Z =,4.2. Results for One-Bit Matrix Completion,[0],[0]
[U; V] of one-bit matrix completion satisfy the condition UV> lies in a close neighborhood around X∗ with radius O( √ r2d log d/n).,4.2. Results for One-Bit Matrix Completion,[0],[0]
"This suggests that as long as the number of observations is sufficient, we can obtain a good estimator for X∗ by solving (2.8).",4.2. Results for One-Bit Matrix Completion,[0],[0]
"To the best of our knowledge, all the existing studies on the characterization of global geometry for low-rank problems require the objective function to be square loss, thus our work is the first that can characterize the global optimality for one-bit matrix completion, which resolves an open problem in Ge et al. (2017).",4.2. Results for One-Bit Matrix Completion,[0],[0]
"So far, we have shown that all local minima of inequality constrained optimization problem (3.1) belong to a close neighbourhood of the ground truth matrix, with applications to two specific low-rank problems.",5. The Primal-Dual Algorithm,[0],[0]
"It remains to find an efficient and effective algorithm that can find a local minimizer of (3.1) successfully.
",5. The Primal-Dual Algorithm,[0],[0]
Recall that our characterization of global optimality with respect to (3.1) is based on the Lagrange function and the duality theory.,5. The Primal-Dual Algorithm,[0],[0]
This motivates us to search for local minima of (3.1) from the primal-dual perspective.,5. The Primal-Dual Algorithm,[0],[0]
It has been proved in Di Pillo et al. (2011) that a particular primal-dual based algorithm can converge to a solution that satisfies the necessary optimality Conditions 3.3 and 3.4 for general nonlinear inequality constrained optimization problems.,5. The Primal-Dual Algorithm,[0],[0]
"It immediately suggests that their algorithm can be directly applied to solve the optimization problem (3.1), and is guaranteed to find a local minimizer.",5. The Primal-Dual Algorithm,[0],[0]
"Nevertheless, the algorithm in Di Pillo et al. (2011) requires to access the Hessian information, which is computationally very expensive for large scale problems.",5. The Primal-Dual Algorithm,[0],[0]
"Thus, a more practical primal-dual algorithm is preferred.
",5. The Primal-Dual Algorithm,[0],[0]
"Witnessing the empirical success of Augmented Lagrangian Method (Nocedal & Wright, 2006), a first-order primal-dual method for general constrained optimization problem, we propose to use the augmented Lagrangian method to solve the inequality constrained optimization problem (3.1), as displayed in Algorithm 1.",5. The Primal-Dual Algorithm,[0],[0]
"More specifically, we introduce a slack variable ξ =",5. The Primal-Dual Algorithm,[0],[0]
"[ξ1, . . .",5. The Primal-Dual Algorithm,[0],[0]
", ξd1+d2 ]
> to transform the inequality constraints in (3.1) into equality ones.",5. The Primal-Dual Algorithm,[0],[0]
"Define the augmented Lagrange function L̃ as follows
L̃(Z, ξ,λ, µ) = G(Z)",5. The Primal-Dual Algorithm,[0],[0]
+,5. The Primal-Dual Algorithm,[0],[0]
d1+d2∑ i=1,5. The Primal-Dual Algorithm,[0],[0]
λici(Z),5. The Primal-Dual Algorithm,[0],[0]
+ µ 2 d1+d2∑ i=1,5. The Primal-Dual Algorithm,[0],[0]
"c2i (Z),
where ci(Z) = hi(Z) +",5. The Primal-Dual Algorithm,[0],[0]
ξ2i .,5. The Primal-Dual Algorithm,[0],[0]
"At each iteration of Algorithm 1, we solve the minimization sub-problem in line 2 based on gradient descent with respect to Z and ξ, and update the dual variable λ and the penalty parameter µ as suggested in Nocedal & Wright (2006).",5. The Primal-Dual Algorithm,[0],[0]
"Here, we let h(Z) =",5. The Primal-Dual Algorithm,[0],[0]
"[h1(Z), . . .",5. The Primal-Dual Algorithm,[0],[0]
", hd1+d2(Z)]",5. The Primal-Dual Algorithm,[0],[0]
"> in line 3.
Algorithm 1 Augmented Lagrangian Method
Input: Augmented Lagrangian function L̃; parameters T, ξ0,λ0, µ0 > 0",5. The Primal-Dual Algorithm,[0],[0]
"and ρ ≥ 1; initial estimator Z0
1: for t = 0, . . .",5. The Primal-Dual Algorithm,[0],[0]
", T do 2:",5. The Primal-Dual Algorithm,[0],[0]
"Solve ( Zt+1, ξt+1 ) =",5. The Primal-Dual Algorithm,[0],[0]
"argminZ,ξ L̃(Z, ξ,λt, µt)
3: λt+1 =",5. The Primal-Dual Algorithm,[0],[0]
"λt + µt ( h(Zt+1) + ξ 2 t+1 ) 4: µt+1 = ρµt 5: end for
Output: ZT+1
As will be seen in the next section, we demonstrate through numerical experiments that the primal-dual based Algorithm 1 can efficiently solve the constrained nonconvex optimization problem (3.1) given enough observations.",5. The Primal-Dual Algorithm,[0],[0]
"In this section, we provide simulation results of the primaldual based method, as discussed in Section 5, for matrix completion and one-bit matrix completion.",6. Experiments,[0],[0]
"Under random initialization, we compare our primal-dual based Algorithm 1 (Primal-Dual) with existing gradient-based methods, including vanilla gradient descent (GD), projected gradient descent (Proj-GD) and perturbed gradient descent (Jin et al., 2017)",6. Experiments,[0],[0]
(Perturb-GD).,6. Experiments,[0],[0]
"We remark that GD and Proj-GD have been proposed in Ma et al. (2017) and Zheng & Lafferty (2016) for matrix completion respectively, but they all require a specially designed initialization procedure.",6. Experiments,[0],[0]
"Here, we are interested in evaluating the performance of all these algorithms with random initialization.",6. Experiments,[0],[0]
"All of the aforementioned algorithms are implemented in Matlab, and all the following experimental results are based on the optimal parameters, which are selected by cross validation and averaged over 20 trials.",6. Experiments,[0],[0]
We generate the observed data matrix according to the uniform observation model (2.3).,6.1. Matrix Completion,[0],[0]
"In particular, the un-
known low-rank matrix X∗ ∈ Rd1×d2 is generated via X∗ = U∗V∗>, where each entry of U∗ ∈ Rd1×r and V∗ ∈ Rd2×r is generated independently from standard Gaussian distribution, and we scale them to ensure max{‖U∗‖2,∞, ‖V∗‖2,∞} ≤ α, where α = 2.",6.1. Matrix Completion,[0],[0]
"The noise matrix E is set as 0 in the noiseless case, while under the noisy setting, we generate each element of the noise matrix E from i.i.d. centered Gaussian distribution with variance σ2 = 0.25.",6.1. Matrix Completion,[0],[0]
"Note that due to random initialization, the initial estimators may not satisfy the incoherence constraint.",6.1. Matrix Completion,[0],[0]
"In the sequel, we are going to evaluate the recovery performance of different algorithms under the noiseless setting, and investigate the statistical rate of our method.
",6.1. Matrix Completion,[0],[0]
"To begin with, we compare the sample complexities required by the aforementioned algorithms under the setting d1 = 120 and d2 = 100 with sample size n and rank r varied.",6.1. Matrix Completion,[0],[0]
"We say the final estimator X̂ successfully recover the ground truth matrix X∗, if the relative error ‖X̂−X∗‖F /‖X∗‖F ≤ 10−3.",6.1. Matrix Completion,[0],[0]
Figures 1(a)-1(d) illustrate the recovery probability of different methods.,6.1. Matrix Completion,[0],[0]
"Here, the white block indicates successful recovery and the black block denotes failure.",6.1. Matrix Completion,[0],[0]
"As for GD, all the cases have phase transition around n = 4rd.",6.1. Matrix Completion,[0],[0]
"While our method has phase transition around n = 2.5rd under all the settings, and similar results are observed in Figure 1(b) for Proj-GD and Figure 1(c) regarding Perturb-GD.",6.1. Matrix Completion,[0],[0]
"These results suggest that the sample complexity for all these methods could be linear in both d and r.
Next, we compare the convergence rate of different methods under the setting d1 = 2000, d2 = 1500, r = 20 with sampling rate p = |Ω|/(d1d2) chosen from {15%, 25%, 35%}.",6.1. Matrix Completion,[0],[0]
The experimental results in terms of the averaged Frobenius norm error ‖X̂−X∗‖F / √ d1d2 versus the number of data passes are demonstrated in Figures 1(e)-1(g).,6.1. Matrix Completion,[0],[0]
"It can be seen that our primal-dual based algorithm achieves lower estimation error than GD after the same number of data passes, especially when the observed sample size is small.",6.1. Matrix Completion,[0],[0]
"On the other hand, compared with Proj-GD and Perturb-GD, our method achieves comparable performance, which verifies the effectiveness of Algorithm 1 for solving nonconvex optimization problem (2.5) to find a local minimum.
",6.1. Matrix Completion,[0],[0]
"Finally, we study the statistical rate of our method under the following settings: (i) d1 = 100, d2 = 80, r = 2; (ii) d1 = 120, d2 = 100, r = 3; (iii) d1 = 140, d2 = 120, r = 4.",6.1. Matrix Completion,[0],[0]
The results are displayed in Figure 1(h).,6.1. Matrix Completion,[0],[0]
"In detail, the vertical axis represents the estimation error ‖X̂−X∗‖2F /(d1d2), and the horizontal axis is the rescaled sample size n/(rd log d).",6.1. Matrix Completion,[0],[0]
"The results show that the estimation error and the rescaled sample size align well under different settings, which suggests that the statistical rate of our method is O(rd log d/n).",6.1. Matrix Completion,[0],[0]
"We generate the data matrix Y based on probability model (2.7) with f(Xij) = Φ(Xij/σ), where Φ is the cumulative distribution function of the standard Gaussian distribution, and we choose σ = 0.5 as the noise level.",6.2. One-Bit Matrix Completion,[0],[0]
"For the unknown low-rank matrix X∗ = U∗V∗> with rank r, we follow the same generative procedure as in Davenport et al. (2014); Bhaskar & Javanmard (2015); Ni & Gu (2016).",6.2. One-Bit Matrix Completion,[0],[0]
"More specifically, U∗ ∈ Rd1×r,V∗ ∈ Rd2×r are randomly generated from a uniform distribution on [−1/2, 1/2], and are scaled properly such that the incoherence constraint max{‖U∗‖2,∞, ‖V∗‖2,∞} ≤ α is satisfied, where we set α = 1.",6.2. One-Bit Matrix Completion,[0],[0]
"In addition, we sample the observed index set Ω according to the uniform sampling model.
",6.2. One-Bit Matrix Completion,[0],[0]
"To demonstrate the effectiveness of Algorithm 1, we com-
pare our method with existing gradient-based algorithms including GD, Proj-GD and Perturb-GD with random initialization.",6.2. One-Bit Matrix Completion,[0],[0]
"In particular, we compute the logarithm of the averaged estimation error ‖X̂",6.2. One-Bit Matrix Completion,[0],[0]
"− X∗‖F / √ d1d2 and plot it with the number of data passes for different methods, which are illustrated in Figures 2(a)-2(c) under the setting d1 = 2000, d2 = 1500, r = 20 with varied sampling rate.",6.2. One-Bit Matrix Completion,[0],[0]
"These results again confirm that with random initialization, the primal-dual based algorithm can recover the unknown low-rank matrix X∗ successfully.",6.2. One-Bit Matrix Completion,[0],[0]
"In addition, Proj-GD demonstrates a sharp decrease in estimation error after the first several data passes, which is due to the simple but effective projection mechanism.
",6.2. One-Bit Matrix Completion,[0],[0]
"In addition, we investigate the statistical rate for one-bit matrix completion based on our algorithm.",6.2. One-Bit Matrix Completion,[0],[0]
Figure 2(d) plots the averaged estimation error ‖X̂,6.2. One-Bit Matrix Completion,[0],[0]
"− X∗‖2F /(d1d2) versus the rescaled sample size n/rd log d under different settings, which suggests that our primal-dual based approach achieves statistical rate with order O(rd log d/n).",6.2. One-Bit Matrix Completion,[0],[0]
"In this paper, we proposed a primal-dual based framework to characterize the global optimality for nonconvex lowrank matrix recovery with incoherence constraints.",7. Conclusions and Future Work,[0],[0]
"Based on duality, we proved that the optimization landscape of such problem is well-behaved.",7. Conclusions and Future Work,[0],[0]
"We further applied a primaldual based algorithm to solve the nonconvex optimization problem and demonstrated its effectiveness via simulations.
",7. Conclusions and Future Work,[0],[0]
There are still some open problems along this line of research.,7. Conclusions and Future Work,[0],[0]
"For example, how to prove the optimization guarantees for the primal-dual based algorithm?",7. Conclusions and Future Work,[0],[0]
Another question is how to generalize our framework to other constrained nonconvex optimization beyond incoherence constraints.,7. Conclusions and Future Work,[0],[0]
We hope this work can act as the first step towards understanding the global geometry of general constrained nonconvex optimization problems.,7. Conclusions and Future Work,[0],[0]
We would like to thank the anonymous reviewers for their helpful comments.,Acknowledgement,[0],[0]
This research was sponsored in part by the National Science Foundation IIS-1618948 and IIS1652539.,Acknowledgement,[0],[0]
The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agencies.,Acknowledgement,[0],[0]
We propose a primal-dual based framework for analyzing the global optimality of nonconvex lowrank matrix recovery.,abstractText,[0],[0]
"Our analysis are based on the restricted strongly convex and smooth conditions, which can be verified for a broad family of loss functions.",abstractText,[0],[0]
"In addition, our analytic framework can directly handle the widely-used incoherence constraints through the lens of duality.",abstractText,[0],[0]
"We illustrate the applicability of the proposed framework to matrix completion and one-bit matrix completion, and prove that all these problems have no spurious local minima.",abstractText,[0],[0]
"Our results not only improve the sample complexity required for characterizing the global optimality of matrix completion, but also resolve an open problem in Ge et al. (2017) regarding one-bit matrix completion.",abstractText,[0],[0]
Numerical experiments show that primaldual based algorithm can successfully recover the global optimum for various low-rank problems.,abstractText,[0],[0]
A Primal-Dual Analysis of Global Optimality in Nonconvex Low-Rank  Matrix Recovery,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 26–31 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-2005",text,[0],[0]
"The task of extractive summarization (ES) can naturally be cast as a discrete optimization problem where the text source is considered as a set of sentences and the summary is created by selecting an optimal subset of the sentences under a length constraint (McDonald, 2007; Lin and Bilmes, 2011).
",1 Introduction,[0],[0]
"In this work, we go one step further and mathematically prove that ES is equivalent to the problem of choosing (i) an objective function θ for scoring system summaries, and (ii) an optimizer O. We use (θ, O) to denote the resulting decomposition of any extractive summarizer.",1 Introduction,[0],[0]
"Our proposed decomposition enables a principled analysis and evaluation of existing summarizers, and addresses a major issue in the current evaluation of ES.
",1 Introduction,[0],[0]
This issue concerns the traditional “intrinsic” evaluation comparing system summaries against human reference summaries.,1 Introduction,[0],[0]
"This kind of evaluation is actually an end-to-end evaluation of summarization systems which is performed after θ has been optimized by O. This is highly problematic
from an evaluation point of view, because first, θ is typically not optimized exactly, and second, there might be side-effects caused by the particular optimization technique O, e.g., a sentence extracted to maximize θ might be suitable because of other properties not included in θ.",1 Introduction,[0],[0]
"Moreover, the commonly used evaluation metric ROUGE yields a noisy surrogate evaluation (despite its good correlation with human judgments) compared to the much more meaningful evaluation based on human judgments.",1 Introduction,[0],[0]
"As a result, the current end-toend evaluation does not provide any insights into the task of automatic summarization.
",1 Introduction,[0],[0]
"The (θ,O) decomposition we propose addresses this issue: it enables a well-defined and principled evaluation of extractive summarizers on the level of their components θ and O.",1 Introduction,[0],[0]
"In this work, we focus on the analysis and evaluation of θ, because θ is a model of the quality indicators of a summary, and thus crucial in order to understand the properties of “good” summaries.",1 Introduction,[0],[0]
"Specifically, we compare θ functions of different summarizers by measuring the correlation of their θ functions with human judgments.
",1 Introduction,[0],[0]
Our goal is to provide an evaluation framework which the research community could build upon in future research to identify the best possible θ and use it in optimization-based systems.,1 Introduction,[0],[0]
"We believe that the identification of such a θ is the central question of summarization, because this optimal θ would represent an optimal definition of summary quality both from an algorithmic point of view and from the human perspective.
",1 Introduction,[0],[0]
"In summary, our contribution is twofold: (i) We present a novel and principled evaluation framework for ES which allows evaluating the objective function and the optimization technique separately and independently.",1 Introduction,[0],[0]
"(ii) We compare wellknown summarization systems regarding their implicit choices of θ by measuring the correlation
26
of their θ functions with human judgments on two datasets from the Text Analysis Conference (TAC).",1 Introduction,[0],[0]
"Our comparative evaluation yields surprising results and shows that extractive summarization is not solved yet.
",1 Introduction,[0],[0]
"The code used in our experiments, including a general evaluation tool is available at github.com/UKPLab/acl2017-theta_",1 Introduction,[0],[0]
evaluation_summarization.,1 Introduction,[0],[0]
Let D = {si} be a document collection considered as a set of sentences.,"2.1 (θ,O) decomposition",[0],[0]
"A summary S is then a subset of D, or we can say that S is an element of P(D), the power set of D. Objective function We define an objective function to be a function that takes a summary of the document collection D and outputs a score:
θ : P(D) → R S 7→ θD(S)","2.1 (θ,O) decomposition",[0],[0]
"(1)
Optimizer Then the task of ES is to select the set of sentences S∗ with maximal θ(S∗) under a length constraint:
S∗ = argmax S θ(S)
len(S) = ∑
s∈S len(s) ≤","2.1 (θ,O) decomposition",[0],[0]
c,"2.1 (θ,O) decomposition",[0],[0]
"(2)
We use O to denote the technique which solves this optimization problem.","2.1 (θ,O) decomposition",[0],[0]
"O is an operator which takes an objective function θ from the set of all objective functions Θ and a document collection D from the set of all document collections D, and outputs a summary S∗:
","2.1 (θ,O) decomposition",[0],[0]
O : Θ×D,"2.1 (θ,O) decomposition",[0],[0]
"→ S (θ,D) 7→ S∗ (3)
Decomposition Theorem","2.1 (θ,O) decomposition",[0],[0]
"Now we show that the problem of ES is equivalent to the problem of choosing a decomposition (θ, O).
","2.1 (θ,O) decomposition",[0],[0]
"We formalize an extractive summarizer σ as a set function which takes a document collection D ∈ D and outputs a summary SD,σ ∈ P(D).","2.1 (θ,O) decomposition",[0],[0]
"With this formalism, it is clear that every (θ,O) tuple forms a summarizer because O(θ, ·) produces a summary from a document collection.
","2.1 (θ,O) decomposition",[0],[0]
"But the other direction is also true: for every extractive summarizer there exists at least one tuple (θ, O) which perfectly describes the summarizer:
Theorem 1 ∀σ, ∃(θ,O) such that: ∀D ∈ D, σ(D) = O(θ,D)
","2.1 (θ,O) decomposition",[0],[0]
"This theorem is quite intuitive, especially since it is common to use a similar decomposition in optimization-based summarization systems.","2.1 (θ,O) decomposition",[0],[0]
"In the next section we illustrate the theorem by way of several examples, and provide a rigorous proof of the existence in the supplemental material.","2.1 (θ,O) decomposition",[0],[0]
We analyze a range of different summarizers regarding their (mostly implicit) θ.,2.2 Examples of θ,[0],[0]
"ICSI (Gillick and Favre, 2009) is a global linear optimization that extracts a summary by solving a maximum coverage problem considering the most frequent bigrams in the source documents.",2.2 Examples of θ,[0],[0]
"ICSI has been among the best systems in a classical ROUGE evaluation (Hong et al., 2014).",2.2 Examples of θ,[0],[0]
"For ICSI, the identification of θ is trivial because it was formulated as an optimization task.",2.2 Examples of θ,[0],[0]
"If ci is the i-th bigram selected in the summary and wi its weight computed from D, then:
θICSI(S) = ∑
ci∈S ci ∗ wi (4)
LexRank (Erkan and Radev, 2004) is a wellknown graph-based approach.",2.2 Examples of θ,[0],[0]
"A similarity graph G(V,E) is constructed where V is the set of sentences and an edge eij is drawn between sentences vi and vj if and only if the cosine similarity between them is above a given threshold.",2.2 Examples of θ,[0],[0]
"Sentences are scored according to their PageRank score inG. We observe that θLexRank is given by:
θLexRank(S) = ∑
s∈S PRG(s) (5)
where PR is the PageRank score of sentence s. KL-Greedy (Haghighi and Vanderwende, 2009) minimizes the Kullback Leibler (KL) divergence between the word distributions in the summary and D (i.e θKL = −KL).",2.2 Examples of θ,[0],[0]
"Recently, Peyrard and Eckle-Kohler (2016) optimized KL and Jensen Shannon (JS) divergence with a genetic algorithm.",2.2 Examples of θ,[0],[0]
"In this work, we use KL and JS for both unigram and bigram distributions.",2.2 Examples of θ,[0],[0]
"LSA (Steinberger and Jezek, 2004) is an approach involving a dimensionality reduction of the termdocument matrix via Singular Value Decomposition (SVD).",2.2 Examples of θ,[0],[0]
"The sentences extracted should cover the most important latent topics:
θLSA = ∑
t∈S",2.2 Examples of θ,[0],[0]
"λt (6)
where t is a latent topic identified by SVD on the term-document matrix and λt the associated singular value.",2.2 Examples of θ,[0],[0]
"Edmundson (Edmundson, 1969) is an older heuristic method which scores sentences according to cue-phrases, overlap with title, term frequency and sentence position.",2.2 Examples of θ,[0],[0]
θEdmundson is simply a weighted sum of these heuristics.,2.2 Examples of θ,[0],[0]
"TF?IDF (Luhn, 1958) scores sentences with the TF*IDF of their terms.",2.2 Examples of θ,[0],[0]
The best sentences are then greedily extracted.,2.2 Examples of θ,[0],[0]
We use both the unigram and bigram versions in our experiments.,2.2 Examples of θ,[0],[0]
"Now we compare the summarizers analyzed above by measuring the correlation of their θ functions with human judgments.
",3 Experiments,[0],[0]
Datasets We use two multi-document summarization datasets from the Text Analysis Conference (TAC) shared task: TAC-2008,3 Experiments,[0],[0]
and TAC2009.1 TAC-2008,3 Experiments,[0],[0]
"and TAC-2009 contain 48 and 44 topics, respectively.",3 Experiments,[0],[0]
Each topic consists of 10 news articles to be summarized in a maximum of 100 words.,3 Experiments,[0],[0]
"We use only the so-called initial summaries (A summaries), but not the update part.
",3 Experiments,[0],[0]
"For each topic, there are 4 human reference summaries along with a manually created Pyramid set.",3 Experiments,[0],[0]
"In both editions, all system summaries and the 4 reference summaries were manually evaluated by NIST assessors for readability, content selection (with Pyramid) and overall responsiveness.",3 Experiments,[0],[0]
"At the time of the shared tasks, 57 systems were submitted to TAC-2008 and 55 to TAC-2009.",3 Experiments,[0],[0]
"For our experiments, we use the Pyramid and the responsiveness annotations.
",3 Experiments,[0],[0]
"System Comparison For each θ, we compute the scores of all system and all manual summaries for any given topic.",3 Experiments,[0],[0]
These scores are compared with the human scores.,3 Experiments,[0],[0]
We include the manual summaries in our computation because this yields a more diverse set of summaries with a wider range of scores.,3 Experiments,[0],[0]
"Since an ideal summarizer would create summaries as well as humans, an ideal θ would also be able to correctly score human summaries with high scores.
",3 Experiments,[0],[0]
"For comparison, we also report the correlation between pyramid and responsiveness.
",3 Experiments,[0],[0]
"Correlations are measured with 3 metrics: Pear1http://tac.nist.gov/2009/ Summarization/, http://tac.nist.gov/2008/ Summarization/
son’s r, Spearman’s ρ and Normalized Discounted Cumulative Gain (Ndcg).",3 Experiments,[0],[0]
Pearson’s r is a value correlation metric which depicts linear relationships between the scores produced by θ and the human judgments.,3 Experiments,[0],[0]
Spearman’s ρ is a rank correlation metric which compares the ordering of systems induced by θ and the ordering of systems induced by human judgments.,3 Experiments,[0],[0]
Ndcg is a metric that compares ranked lists and puts more emphasis on the top elements by logarithmic decay weighting.,3 Experiments,[0],[0]
"Intuitively, it captures how well θ can recognize the best summaries.",3 Experiments,[0],[0]
"The optimization scenario benefits from high Ndcg scores because only summaries with high θ scores are extracted.
",3 Experiments,[0],[0]
"Previous work on correlation analysis averaged scores over topics for each system and then computed the correlation between averaged scores (Louis and Nenkova, 2013; Nenkova et al., 2007).",3 Experiments,[0],[0]
An alternative and more natural option which we use here is to compute the correlation for each topic and average these correlations over topics (CORRELATION-AVERAGE).,3 Experiments,[0],[0]
"Since we want to estimate how well θ functions measure the quality of summaries, we find the summary level averaging more meaningful.
",3 Experiments,[0],[0]
"Analysis The results of our correlation analysis are presented in Table 1.
",3 Experiments,[0],[0]
"In our (θ,O) formulation, the end-to-end approach maps a set of documents to exactly one summary selected by the system.",3 Experiments,[0],[0]
We call the (classical and well known) evaluation of this single summary end-to-end evaluation because it measures the end product of the system.,3 Experiments,[0],[0]
This is in contrast to our proposed evaluation of the assumption made by individual summarizers shown in Table 1.,3 Experiments,[0],[0]
"A system summary was extracted by a given system because it was high scoring using its θ, but we ask the question whether optimizing this θ made sense in the first place.
",3 Experiments,[0],[0]
We first observe that scores are relatively low.,3 Experiments,[0],[0]
Summarization is not a solved problem and the systems we investigated can not identify correctly what makes a good summary.,3 Experiments,[0],[0]
This is in contrast to the picture in the classical end-to-end evaluation with ROUGE where state-of-the-art systems score relatively high.,3 Experiments,[0],[0]
Some Ndcg scores are higher (for TAC-2008) which explains why these systems can extract relatively good summaries in the end-toend evaluation.,3 Experiments,[0],[0]
"In this classical evaluation, only the single best summary is evaluated, which means that a system does not need to be able to rank all
possible summaries correctly.",3 Experiments,[0],[0]
We see that systems with high end-to-end ROUGE scores (according to Hong et al. (2014)) do not necessarily have a good model of summary quality.,3 Experiments,[0],[0]
"Indeed, the best performing θ functions are not part of the systems performing best with ROUGE.",3 Experiments,[0],[0]
"For example, ICSI is the best system according to ROUGE, but it is not clear that it has the best model of summary quality.",3 Experiments,[0],[0]
"In TAC-2009, LexRank, LSA and the heuristic Edmundson have better correlations with human judgments.",3 Experiments,[0],[0]
"The difference with end-to-end evaluation might stem from the fact that ICSI solves the optimization problem exactly, while LexRank and Edmundson use greedy optimizers.",3 Experiments,[0],[0]
"There might also be some side-effects from which ICSI profits: extracting sentences to improve θ might lead to accidentally selecting suitable sentences, because θ can merely correlate well with properties of good summaries, while not modeling these properties itself.
",3 Experiments,[0],[0]
It is worth noting that systems perform differently on TAC2009 and TAC2008.,3 Experiments,[0],[0]
"There are several differences between TAC2008 and TAC2009 like redundancy level or guidelines for annotations; for example, responsiveness is scored out of 5 in 2008 and out of 10 in 2009.",3 Experiments,[0],[0]
The LSA summarizer ranks among the best systems in TAC2009 with pearson’s r but is closer to the worst systems in TAC2008.,3 Experiments,[0],[0]
While this is difficult to explain we hypothesize that the model of summary quality from LSA is sensitive to the slight variations and therefore not robust.,3 Experiments,[0],[0]
"In general, any system which claims to have a better θ than previous works should indeed report results on several datasets to ensure robustness and generality.
",3 Experiments,[0],[0]
"Interestingly, we observe that the correlation between Pyramid and responsiveness is better than in
any system, but still not particularly high.",3 Experiments,[0],[0]
Responsiveness is an overall annotation while Pyramid is a manual measure of content only.,3 Experiments,[0],[0]
These results confirm the intuition that humans take into account much more aspects when evaluating summaries.,3 Experiments,[0],[0]
"While correlation analyses on human judgment data have been performed in the context of validating automatic summary evaluation metrics (Louis and Nenkova, 2013; Nenkova et al., 2007; Lin, 2004), there is no prior work which uses these data for a principled comparison of summarizers.
",4 Related Work and Discussion,[0],[0]
"Much previous work focused on efficient optimizers O, such as ILP, which impose constraints on the θ function.",4 Related Work and Discussion,[0],[0]
"Linear (Gillick and Favre, 2009) and submodular (Lin and Bilmes, 2011) θ functions are widespread in the summarization community because they can be optimized efficiently and effectively via ILP (Schrijver, 1986) and the greedy algorithm for submodularity (Fujishige, 2005).",4 Related Work and Discussion,[0],[0]
"A greedy approach is often used when θ does not have convenient properties that can be leveraged by a classical optimizer (Haghighi and Vanderwende, 2009).
",4 Related Work and Discussion,[0],[0]
Such interdependencies of O and θ limit the expressiveness of θ.,4 Related Work and Discussion,[0],[0]
"However, realistic θ functions are unlikely to be linear or submodular, and in the well-studied field of optimization there exist a range of different techniques developed to tackle difficult combinatorial problems (Schrijver, 2003; Blum and Roli, 2003).
",4 Related Work and Discussion,[0],[0]
"A recent example of such a technique adapted to extractive summarization are meta-heuristics used to optimize non-linear, non-submodular objective functions (Peyrard and Eckle-Kohler, 2016).
",4 Related Work and Discussion,[0],[0]
"Other methods like Markov Chain Monte Carlo (Metropolis et al., 1953) or Monte-Carlo Tree Search (Suttner and Ertel, 1991; Silver et al., 2016) could also be adapted to summarization and thus become realistic choices for O. General purpose optimization techniques are especially appealing, because they offer a decoupling of θ and O and allow investigating complex θ functions without making any assumption on their mathematical properties.",4 Related Work and Discussion,[0],[0]
"In particular, this supports future work on identifying an “optimal” θ as a model of relevant quality aspects of a summary.",4 Related Work and Discussion,[0],[0]
We presented a novel evaluation framework for ES which is based on the proof that ES is equivalent to the problem of choosing an objective function θ and an optimizer O. This principled and welldefined framework allows evaluating θ and O of any extractive summarizer – separately and independently.,5 Conclusion,[0],[0]
"We believe that our framework can serve as a basis for future work on identifying an “optimal” θ function, which would provide an answer to the central question of what are the properties of a “good” summary.",5 Conclusion,[0],[0]
This work has been supported by the German Research Foundation (DFG) as part of the Research Training Group “Adaptive Preparation of Information from Heterogeneous Sources” (AIPHES) under grant No.,Acknowledgments,[0],[0]
"GRK 1994/1,",Acknowledgments,[0],[0]
"and via the GermanIsraeli Project Cooperation (DIP, grant No. GU 798/17-1).",Acknowledgments,[0],[0]
"We present a new framework for evaluating extractive summarizers, which is based on a principled representation as optimization problem.",abstractText,[0],[0]
We prove that every extractive summarizer can be decomposed into an objective function and an optimization technique.,abstractText,[0],[0]
We perform a comparative analysis and evaluation of several objective functions embedded in wellknown summarizers regarding their correlation with human judgments.,abstractText,[0],[0]
Our comparison of these correlations across two datasets yields surprising insights into the role and performance of objective functions in the different summarizers.,abstractText,[0],[0]
A Principled Framework for Evaluating Summarizers: Comparing Models of Summary Quality against Human Judgments,title,[0],[0]
"Similarity (or distance) functions play a key role in many machine learning algorithms for problems ranging from classification (e.g., k-nearest neighbors) and clustering (e.g., k-means) to dimensionality reduction (van der Maaten & Hinton, 2008) and ranking (Chechik et al., 2010).",1. Introduction,[0],[0]
"The success of such methods are heavily dependent on the relevance
1Télécom ParisTech, Paris, France 2IDEMIA, Colombes, France 3INRIA, France.",1. Introduction,[0],[0]
"Correspondence to: Robin Vogel <robin.vogel@telecom-paristech.fr>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
of the similarity function to the task and dataset of interest.,1. Introduction,[0],[0]
"This has motivated the research in similarity and distance metric learning (Bellet et al., 2015), a line of work which consists in automatically learning a similarity function from data.",1. Introduction,[0],[0]
"This training data often comes in the form of pairwise similarity judgments derived from labels, such as positive (resp. negative)",1. Introduction,[0],[0]
pairs composed of two instances with same (resp. different) label.,1. Introduction,[0],[0]
"Most existing algorithms can then be framed as unconstrained optimization problems where the objective is to minimize some average loss function over the set of similarity judgments (see for instance Goldberger et al., 2004; Weinberger & Saul, 2009; Bellet et al., 2012, for methods tailored to classification).",1. Introduction,[0],[0]
"Some generalization bounds for this class of methods have been derived, accounting for the specific dependence structure found in the training similarity judgments (Jin et al., 2009; Bellet & Habrard, 2015; Cao et al., 2016; Jain et al., 2017; Verma & Branson, 2015).",1. Introduction,[0],[0]
"We refer to Kulis (2012) and Bellet et al. (2015) for detailed surveys on similarity and metric learning.
",1. Introduction,[0],[0]
"In this paper, we study similarity learning from the perspective of pairwise bipartite ranking, where the goal is to rank the elements of a database by decreasing order of the probability that they share the same label with some query data point.",1. Introduction,[0],[0]
"This problem is motivated by many concrete applications: for instance, biometric identification aims to check the claimed identity of an individual by matching her biometric information (e.g., a photo taken at an airport) with a large reference database of authorized people (e.g., of passport photos) (Jain et al., 2011).",1. Introduction,[0],[0]
"Given a similarity function and a threshold, the database elements are ranked in decreasing order of similarity score with the query, and the matching elements are those whose score is above the threshold.",1. Introduction,[0],[0]
"In this context, performance criteria are related to the ROC curve associated with the similarity function, i.e., the relation between the false positive rate and the true positive rate.",1. Introduction,[0],[0]
"Previous approaches have empirically tried to optimize the Area under the ROC curve (AUC) of the similarity function (McFee & Lanckriet, 2010; Huo et al., 2018), without establishing any generalization guarantees.",1. Introduction,[0],[0]
The AUC is a global summary of the ROC curve which penalizes pairwise ranking errors regardless of the positions in the list.,1. Introduction,[0],[0]
"More local versions of the AUC (e.g., focusing
on the top of the list) are difficult to optimize in practice and lead to complex nonconvex formulations (Clémençon & Vayatis, 2007; Huo et al., 2018).",1. Introduction,[0],[0]
"In contrast, the performance criterion we consider in this work is pointwise ROC optimization, which aims at maximizing the true positive rate under a fixed false positive rate.",1. Introduction,[0],[0]
"This objective, formulated as a constrained optimization problem, naturally expresses the operational constraints present in many practical scenarios.",1. Introduction,[0],[0]
"For instance, in biometric applications such as the one outlined above, the verification system is typically set to keep the proportion of people falsely considered a match below a predefined acceptable threshold (see e.g., Jain et al., 2000; 2004).
",1. Introduction,[0],[0]
"In addition to proposing an appropriate probabilistic framework to study this novel perspective on similarity learning, we make the following key contributions:
Universal and fast learning rates.",1. Introduction,[0],[0]
"We derive statistical guarantees for the approach of solving the constrained optimization problem corresponding to the empirical version of our theoretical objective, based on a dataset of n labeled data points.",1. Introduction,[0],[0]
As the empirical quantities involved are not i.i.d.,1. Introduction,[0],[0]
"averages but rather in the form of Ustatistics (Lee, 1990), our results rely on concentration bounds developed for U -processes (Clémençon et al., 2008).",1. Introduction,[0],[0]
We first derive a learning rate of order O(1/ √ n) which holds without any assumption on the data distribution.,1. Introduction,[0],[0]
"We then show that one can obtain faster rates under a low-noise assumption on the data distribution, which has the form of a margin criterion involving the conditional quantile.",1. Introduction,[0],[0]
We are unaware of previous results of this kind for constrained similarity/distance metric learning.,1. Introduction,[0],[0]
"Interestingly, we are able to illustrate the faster rates empirically through numerical simulations, which is rarely found in the literature on fast learning rates.
",1. Introduction,[0],[0]
Scalability by sampling.,1. Introduction,[0],[0]
We address scalability issues that arise from the very large number of negative pairs when the dataset and the number of classes are large.,1. Introduction,[0],[0]
"In particular, we show that using an approximation of the pairwise negative risk consisting of O(n) randomly sampled terms, known as an incomplete U -statistic (see Blom, 1976; Lee, 1990), is sufficient to maintain the universal learning rate of O(1/ √ n).",1. Introduction,[0],[0]
We analyze two different choices of sampling strategies and discuss properties of the data distribution which can make one more accurate than the other.,1. Introduction,[0],[0]
"We further provide numerical experiments to illustrate the practical benefits of this strategy.
",1. Introduction,[0],[0]
The rest of this paper is organized as follows.,1. Introduction,[0],[0]
Section 2 introduces the proposed probabilistic framework for similarity learning and draws connections to existing approaches.,1. Introduction,[0],[0]
"In Section 3, we derive universal and fast learning rates for the minimizer of the empirical version of our problem.",1. Introduction,[0],[0]
"Section 4 addresses scalability issues through random sampling, and Section 5 presents some numerical experiments.
",1. Introduction,[0],[0]
Detailed proofs can be found in the supplementary material.,1. Introduction,[0],[0]
"In this section, we introduce the main notations and concepts involved in the subsequent analysis.",2. Background and Preliminaries,[0],[0]
"We formulate the supervised similarity learning problem from the perspective of pairwise bipartite ranking, and highlight connections with some popular metric and similarity learning algorithms of the literature.",2. Background and Preliminaries,[0],[0]
"Here and throughout, the indicator function of any event E is denoted by I{E}, the Dirac mass at any point x by δx, and the pseudo-inverse of any cdf F (u) on R by F−1(t) = inf{v ∈ R : F (v) ≥ t}.",2. Background and Preliminaries,[0],[0]
We consider the (multi-class) classification setting.,2.1. Probabilistic Framework for Similarity Learning,[0],[0]
"The random variable Y denotes the output label with values in the discrete set {1, . . .",2.1. Probabilistic Framework for Similarity Learning,[0],[0]
", K}withK ≥ 1, andX is the input random variable, taking its values in a feature spaceX ⊂ Rd with d ≥ 1 and modeling some information hopefully useful to predict Y .",2.1. Probabilistic Framework for Similarity Learning,[0],[0]
We denote by µ(dx) the marginal distribution of X and by η(x) =,2.1. Probabilistic Framework for Similarity Learning,[0],[0]
"(η1(x), . .",2.1. Probabilistic Framework for Similarity Learning,[0],[0]
". , ηK(x))",2.1. Probabilistic Framework for Similarity Learning,[0],[0]
"the posterior probability, where ηk(x) =",2.1. Probabilistic Framework for Similarity Learning,[0],[0]
"P{Y = k | X = x} for x ∈ X and k ∈ {1, . . .",2.1. Probabilistic Framework for Similarity Learning,[0],[0]
", K}.",2.1. Probabilistic Framework for Similarity Learning,[0],[0]
"The distribution of the random pair (X,Y ) is entirely characterized by P = (µ, η).",2.1. Probabilistic Framework for Similarity Learning,[0],[0]
"The probability of occurrence of an observation with label k ∈ {1, . . .",2.1. Probabilistic Framework for Similarity Learning,[0],[0]
", K} is assumed to be strictly positive and denoted by pk = P{Y = k}, and the conditional distribution of X given Y = k is denoted by µk(dx).",2.1. Probabilistic Framework for Similarity Learning,[0],[0]
"Equipped with these notations, we have µ = ∑K k=1 pkµk.
",2.1. Probabilistic Framework for Similarity Learning,[0],[0]
Optimal similarity measures.,2.1. Probabilistic Framework for Similarity Learning,[0],[0]
"The objective of similarity learning can be informally formulated as follows: the goal is to learn, from a training sample Dn = {(X1, Y1), . . .",2.1. Probabilistic Framework for Similarity Learning,[0],[0]
", (Xn, Yn)} composed of n ≥ 1 independent copies of (X,Y ), a (measurable) similarity measure S :",2.1. Probabilistic Framework for Similarity Learning,[0],[0]
"X × X → R+ such that given two independent pairs (X,Y ) and (X ′, Y ′) drawn from P , the larger the similarity S(X,X ′) between two observations, the more likely they are to share the same label.",2.1. Probabilistic Framework for Similarity Learning,[0],[0]
"The set of all similarity measures is denoted by S. The class S∗ of optimal similarity rules naturally corresponds to the set of strictly increasing transforms T of the pairwise posterior probability η(x, x′) =",2.1. Probabilistic Framework for Similarity Learning,[0],[0]
P{Y = Y ′,2.1. Probabilistic Framework for Similarity Learning,[0],[0]
"| (X,X ′) =",2.1. Probabilistic Framework for Similarity Learning,[0],[0]
"(x, x′)}, where (X ′, Y ′) denotes an independent copy of (X,Y ):
{T ◦ η |T : Im(η)→ R+ borelian, strictly increasing}, and where Im(η) denotes the support of η(X,X ′)’s distribution.",2.1. Probabilistic Framework for Similarity Learning,[0],[0]
"With the notations previously introduced, we have η(x, x′) = ∑K k=1 ηk(x)ηk(x
′) for all (x, x′) ∈ X 2.",2.1. Probabilistic Framework for Similarity Learning,[0],[0]
"A similarity rule S∗ ∈ S∗ defines the optimal preorder1 ∗ on
1A preorder on a set X is any reflexive and transitive binary relationship on X .",2.1. Probabilistic Framework for Similarity Learning,[0],[0]
"A preorder is an order if, in addition, it is
the product space X × X : for any (x1, x2, x3, x4) ∈ X 4, x1 and x2 are more similar to each other than x3 and x4 iff η(x1, x2)",2.1. Probabilistic Framework for Similarity Learning,[0],[0]
"≥ η(x3, x4), and one writes (x3, x4) ∗",2.1. Probabilistic Framework for Similarity Learning,[0],[0]
"(x1, x2) in this case.",2.1. Probabilistic Framework for Similarity Learning,[0],[0]
"For any x ∈ X , S∗ also defines a preorder ∗x on the input space X , permitting to rank optimally all possible observations by increasing degree of similarity to x: for all (x1, x2) ∈ X 2, x1 is more similar to x than x2 (one writes x2 ∗x x1) iff (x, x2) ∗",2.1. Probabilistic Framework for Similarity Learning,[0],[0]
"(x, x1), meaning that η(x, x2) ≤ η(x, x1).",2.1. Probabilistic Framework for Similarity Learning,[0],[0]
"We point out that, despite its simplicity, this framework covers a wide variety of applications, such as the biometric identification problem mentioned earlier in the introduction.
",2.1. Probabilistic Framework for Similarity Learning,[0],[0]
Similarity learning as pairwise bipartite ranking.,2.1. Probabilistic Framework for Similarity Learning,[0],[0]
"In view of the objective formulated above, similarity learning can be seen as a bipartite ranking problem on the product space X × X where, given two independent realizations (X,Y ) and (X ′, Y ′) of P , the input r.v. is the pair (X,X ′) and the binary label is Z = 2I{Y = Y ′} − 1.",2.1. Probabilistic Framework for Similarity Learning,[0],[0]
One may refer to e.g. Clémençon & Vayatis (2009) and the references therein for a statistical learning view of bipartite ranking.,2.1. Probabilistic Framework for Similarity Learning,[0],[0]
"ROC analysis is the gold standard to evaluate the performance of a similarity measure S in this context, i.e. to measure how close the preorder induced by S is to ∗.",2.1. Probabilistic Framework for Similarity Learning,[0],[0]
"The ROC curve of S is the PP-plot t ∈ R+ 7→ (FS,−(t), FS,+(t)), where, for all t ≥ 0,
FS,−(t) = P{S(X,X ′)",2.1. Probabilistic Framework for Similarity Learning,[0],[0]
"> t | Z = −1}, FS,+(t) = P{S(X,X ′) >",2.1. Probabilistic Framework for Similarity Learning,[0],[0]
"t | Z = +1},
where possible jumps are connected by line segments.",2.1. Probabilistic Framework for Similarity Learning,[0],[0]
"Hence, it can be viewed as the graph of a continuous function α ∈ (0, 1) 7→ ROCS(α), where ROCS(α) =",2.1. Probabilistic Framework for Similarity Learning,[0],[0]
"FS,+ ◦ F−1S,−(α) at any point α ∈ (0, 1) such that FS,−◦F−1S,−(α) = α.",2.1. Probabilistic Framework for Similarity Learning,[0],[0]
"The curve ROCS reflects the ability of S to discriminate between pairs with same labels and pairs with different labels: the stochastically smaller than FS,− the distribution FS,+ is, the higher the associated ROC curve.",2.1. Probabilistic Framework for Similarity Learning,[0],[0]
"Note that it corresponds to the type I error vs power plot of the statistical test I{S(X,X ′) > t} when the null hypothesis stipulates that X and X ′ have different marginal distribution (i.e., Y 6= Y ′).",2.1. Probabilistic Framework for Similarity Learning,[0],[0]
"A similarity measure S1 is said to be more accurate than another similarity S2 when ROCS2(α) ≤ ROCS1(α) for any α ∈ (0, 1).",2.1. Probabilistic Framework for Similarity Learning,[0],[0]
"A straightforward Neyman-Pearson argument shows that S∗ is the set of optimal elements regarding this partial order on S: ∀(S, S∗) ∈ S×S∗, ROCS(α) ≤ ROCS∗(α) = ROCη(α) for all α ∈ (0, 1).",2.1. Probabilistic Framework for Similarity Learning,[0],[0]
"For simplicity, we will assume that the conditional cdf of η(X,X ′) given Z = −1 is invertible.
",2.1. Probabilistic Framework for Similarity Learning,[0],[0]
Pointwise ROC optimization.,2.1. Probabilistic Framework for Similarity Learning,[0],[0]
"In many applications, one is interested in finding a similarity function which optimizes the ROC curve at a particular point α ∈ (0, 1).",2.1. Probabilistic Framework for Similarity Learning,[0],[0]
"The superantisymmetrical.
level sets of similarity functions in S∗ define the solutions of pointwise ROC optimization problems in this context.",2.1. Probabilistic Framework for Similarity Learning,[0],[0]
"In the above framework, it indeed follows from Neyman Pearson’s lemma that the test statistic of type I error less than α with maximum power is the indicator function of the set R∗α = {(x, x′) ∈ X 2 : η(x, x′) ≥",2.1. Probabilistic Framework for Similarity Learning,[0],[0]
"Q∗α}, where Q∗α is the conditional quantile of the r.v. η(X,X ′) given Z = −1",2.1. Probabilistic Framework for Similarity Learning,[0],[0]
at level 1− α.,2.1. Probabilistic Framework for Similarity Learning,[0],[0]
"Restricting our attention to similarity functions bounded by 1, this corresponds to the unique solution of the following problem:
max S:X 2→[0,1], borelian
R+(S) subject to R−(S)",2.1. Probabilistic Framework for Similarity Learning,[0],[0]
"≤ α, (1)
where R+(S)",2.1. Probabilistic Framework for Similarity Learning,[0],[0]
"= E[S(X,X ′)",2.1. Probabilistic Framework for Similarity Learning,[0],[0]
| Z = +1] is referred to as positive risk and R−(S) =,2.1. Probabilistic Framework for Similarity Learning,[0],[0]
"E[S(X,X ′)",2.1. Probabilistic Framework for Similarity Learning,[0],[0]
"| Z = −1] as the negative risk.
",2.1. Probabilistic Framework for Similarity Learning,[0],[0]
Remark 1.,2.1. Probabilistic Framework for Similarity Learning,[0],[0]
(UNCONSTRAINED FORMULATION),2.1. Probabilistic Framework for Similarity Learning,[0],[0]
"The superlevel setR∗α of the pairwise posterior probability η(x, x′) is the measurable subset R of X 2 that minimizes the costsensitive classification risk:
p(1−Q∗α)P {(X,X ′) /∈",2.1. Probabilistic Framework for Similarity Learning,[0],[0]
"R | Z = +1}+ (1− p)Q∗αP {(X,X ′) ∈",2.1. Probabilistic Framework for Similarity Learning,[0],[0]
"R | Z = −1} ,
where p = P{Z = +1} = ∑Kk=1 p2k.",2.1. Probabilistic Framework for Similarity Learning,[0],[0]
"Notice however that the asymmetry factor, namely the quantile Q∗α, is unknown in practice, just like the r.v. η(X,X ′).",2.1. Probabilistic Framework for Similarity Learning,[0],[0]
"For this reason, one typically considers the problem of maximizing
R+(S)− λR−(S), (2)
for different values of the constant λ > 0.",2.1. Probabilistic Framework for Similarity Learning,[0],[0]
"The performance in terms of ROC curve can only be analyzed a posteriori, and the value λ thus needs to be tuned empirically by model selection techniques.",2.1. Probabilistic Framework for Similarity Learning,[0],[0]
We point out that the similarity learning framework described above can be equivalently described in terms of learning a dissimilarity measure (or pseudo distance metric) D : X × X → R+.,2.2. Connections to Existing Similarity and Metric Learning Approaches,[0],[0]
"In this case, the pointwise ROC optimization problem (1) translates into:
min D:X 2→[0,1]
",2.2. Connections to Existing Similarity and Metric Learning Approaches,[0],[0]
"E [D(X,X ′)",2.2. Connections to Existing Similarity and Metric Learning Approaches,[0],[0]
"| Z = +1]
subject to E [D(X,X ′) | Z = −1] ≥ 1− α.",2.2. Connections to Existing Similarity and Metric Learning Approaches,[0],[0]
"(3)
A large variety of practical similarity and distance metric learning algorithms have been proposed in the literature, all revolving around the same idea that a good similarity function should output large scores for pairs of points in the same class, and small scores for pairs with different label.",2.2. Connections to Existing Similarity and Metric Learning Approaches,[0],[0]
"They
differ from one another by the class of metric/similarity functions considered, and by the kind of objective function they optimize (see Bellet et al., 2015, for a comprehensive review).",2.2. Connections to Existing Similarity and Metric Learning Approaches,[0],[0]
"In any case, ROC curves are commonly used to evaluate metric learning algorithms when the number of classes is large (see for instance Guillaumin et al., 2009; Kstinger et al., 2012; Shen et al., 2012), which makes our framework very relevant in practice.",2.2. Connections to Existing Similarity and Metric Learning Approaches,[0],[0]
"Several popular algorithms optimize an empirical version of Problems (1)-(3), often in their unconstrained version as in (2) (Liu et al., 2010; Xie & Xing, 2015).",2.2. Connections to Existing Similarity and Metric Learning Approaches,[0],[0]
"We argue here in favor of the constrained version as the parameter α has a direct correspondence with the point ROCS(α) of the ROC curve, unlike the unconstrained case (see Remark 1).",2.2. Connections to Existing Similarity and Metric Learning Approaches,[0],[0]
"This will be illustrated in our numerical experiments of Section 5.
",2.2. Connections to Existing Similarity and Metric Learning Approaches,[0],[0]
"Interestingly, our framework sheds light on MMC, the seminal metric learning algorithm of Xing et al. (2002) originally designed for clustering with side information.",2.2. Connections to Existing Similarity and Metric Learning Approaches,[0],[0]
MMC solves the empirical version of (3) with α fixed to 0.,2.2. Connections to Existing Similarity and Metric Learning Approaches,[0],[0]
"This is because MMC optimizes over a class of distance functions with unbounded values, hence modifying α does not change the solution (up to a scaling factor).",2.2. Connections to Existing Similarity and Metric Learning Approaches,[0],[0]
"We note that by choosing a bounded family of distance functions, one can use the same formulation to optimize the pointwise ROC curve.",2.2. Connections to Existing Similarity and Metric Learning Approaches,[0],[0]
Pointwise ROC optimization problems have been investigated from a statistical learning perspective by Scott & Nowak (2005) and Clémençon & Vayatis (2010) in the context of binary classification.,3. Statistical Guarantees for Generalization,[0],[0]
"The major difference with the present framework lies in the pairwise nature of the quantities appearing in Problem (1) and, consequently, in the complexity of its empirical version.",3. Statistical Guarantees for Generalization,[0],[0]
"In particular, natural statistical estimates for the positive risk R+(S) and the negative risk R−(S) (1) computed on the training sample Dn = {(X1, Y1), . . .",3. Statistical Guarantees for Generalization,[0],[0]
", (Xn, Yn)} are given by:
R̂+n (S) = 1
n+ ∑ 1≤i<j≤n S(Xi, Xj) ·",3. Statistical Guarantees for Generalization,[0],[0]
"I{Yi = Yj}, (4)
R̂−n (S) = 1
n− ∑ 1≤i<j≤n S(Xi, Xj) ·",3. Statistical Guarantees for Generalization,[0],[0]
I{Yi 6=,3. Statistical Guarantees for Generalization,[0],[0]
"Yj}, (5)
where n+ = ∑
1≤i<j≤n I{Yi = Yj} = n(n − 1)/2",3. Statistical Guarantees for Generalization,[0],[0]
"− n−. It is important to note that these quantities are not i.i.d. averages, since several pairs involve each i.i.d. sample.",3. Statistical Guarantees for Generalization,[0],[0]
"This breaks the analysis carried out by Clémençon & Vayatis (2010, Section 5 therein) for the case of binary classification.
",3. Statistical Guarantees for Generalization,[0],[0]
We can however observe that U+n (S) = 2n+/(n(n − 1))R̂+n (S) and U − n,3. Statistical Guarantees for Generalization,[0],[0]
(S) = 2n−/(n(n,3. Statistical Guarantees for Generalization,[0],[0]
"− 1))R̂−n (S) are U - statistics of degree two with respective symmetric kernels h+((x, y), (x′, y′))",3. Statistical Guarantees for Generalization,[0],[0]
"= S(x, x′) · I{y = y′} and
h−((x, y), (x ′, y′))",3. Statistical Guarantees for Generalization,[0],[0]
"= S(x, x′) · I{y 6=",3. Statistical Guarantees for Generalization,[0],[0]
"y′}.2 We will therefore be able to use existing representation tricks to derive concentration bounds for U -processes (collections of U -statistics indexed by classes of kernel functions), under appropriate complexity conditions, see e.g. (Dudley, 1999).
",3. Statistical Guarantees for Generalization,[0],[0]
"We thus investigate the generalization ability of solutions obtained by solving the empirical version of Problem (1), where we also restrict the domain to a subset S0 ⊂ S of similarity functions bounded by 1, and we assume S0 has controlled complexity (e.g. finite VC dimension).",3. Statistical Guarantees for Generalization,[0],[0]
"Finally, we replace the target level α by α + Φ, where Φ is some tolerance parameter that should be of the same order as the maximal deviation supS∈S0 |R̂−n (S)−R−(S)|.",3. Statistical Guarantees for Generalization,[0],[0]
"This leads to the following empirical problem:
max S∈S0
R̂+n (S) subject to R̂ − n (S) ≤ α+ Φ. (6)
",3. Statistical Guarantees for Generalization,[0],[0]
"Following Clémençon et al. (2008), we have the following lemma.
",3. Statistical Guarantees for Generalization,[0],[0]
Lemma 1.,3. Statistical Guarantees for Generalization,[0],[0]
"(Clémençon et al., 2008, Corollary 3) Assume that S0 is a VC-major class of functions with finite VC dimension V < +∞.",3. Statistical Guarantees for Generalization,[0],[0]
"We have with probability larger than 1− δ: ∀n > 1,
sup S∈S0 ∣∣∣Û+n (S)− E[Û+n (S)]∣∣∣ ≤ 2C√Vn + 2 √ log(1/δ)
n− 1 , (7)
where C is a universal constant, explicited in Bousquet et al. (2004, page 198 therein).
",3. Statistical Guarantees for Generalization,[0],[0]
A similar result holds for the U -process {Û−n (S) − U−(S)}S∈S0 .,3. Statistical Guarantees for Generalization,[0],[0]
"We are now ready to state our universal learning rate, describing the generalization capacity of solutions of the constrained optimization program (6) under specific conditions for the class S0 of similarity functions and a suitable choice of the tolerance parameter Φ. This result can be established by combining Lemma 1 with the derivations of Clémençon & Vayatis (2010, Theorem 10 therein).",3. Statistical Guarantees for Generalization,[0],[0]
"Details can be found in the supplementary material.
",3. Statistical Guarantees for Generalization,[0],[0]
Theorem 1.,3. Statistical Guarantees for Generalization,[0],[0]
"Suppose that the assumptions of Lemma 1 are fulfilled and that S(x, x′) ≤ 1 for all S ∈ S0 and any (x, x′) ∈ X 2.",3. Statistical Guarantees for Generalization,[0],[0]
"Assume also that there exists a constant κ ∈ (0, 1) such that κ ≤",3. Statistical Guarantees for Generalization,[0],[0]
∑k=1 p2k ≤ 1,3. Statistical Guarantees for Generalization,[0],[0]
− κ.,3. Statistical Guarantees for Generalization,[0],[0]
"For all δ ∈ (0, 1) and n > 1, set
Φn,δ = 2Cκ −1",3. Statistical Guarantees for Generalization,[0],[0]
"√ V
n + 2κ−1(1 + κ−1)
√ log(3/δ)
",3. Statistical Guarantees for Generalization,[0],[0]
"n− 1 ,
and consider a solution Ŝn of the contrained minimization problem (6) with Φ = Φn,δ/2.",3. Statistical Guarantees for Generalization,[0],[0]
"Then, for any δ ∈ (0, 1),
2We give the definition of U -statistics in the supplementary material for completeness.
",3. Statistical Guarantees for Generalization,[0],[0]
"we have simultaneously with probability at least 1 − δ: ∀n ≥ 1 + 4κ−2 log(3/δ),
R+(Ŝn) ≥",3. Statistical Guarantees for Generalization,[0],[0]
"ROCS∗(α)− Φn,δ/2 − {
ROCS∗(α)− sup S∈S0: R−(S)≤α
R+(S) } , (8)
and R−(Ŝn) ≤ α+ Φn,δ/2.",3. Statistical Guarantees for Generalization,[0],[0]
"(9)
Remark 2.",3. Statistical Guarantees for Generalization,[0],[0]
(ON BIAS AND MODEL SELECTION),3. Statistical Guarantees for Generalization,[0],[0]
"We point out that the last term on the right hand side of (8) should be interpreted as the bias of the statistical learning problem (6), which depends on the richness of class S0.",3. Statistical Guarantees for Generalization,[0],[0]
"This term vanishes when I{(x, x′) ∈ R∗α} belongs to S0.",3. Statistical Guarantees for Generalization,[0],[0]
"Choosing a class yielding a similarity rule of highest true positive rate with large probability can be tackled by means of classical model selection techniques, based on resampling methods or complexity penalization (note that oracle inequalities can be straightforwardly derived from the same analysis).
",3. Statistical Guarantees for Generalization,[0],[0]
"Except for the minor condition stipulating that the probability of occurrence of “positive pairs” ∑K k=1 p 2 k stays bounded away from 0 and 1, the generalization bound stated in Theorem 1 holds whatever the probability distribution of (X,Y ).",3. Statistical Guarantees for Generalization,[0],[0]
"Beyond such universal results, we investigate situations where rates faster than O(1/ √ n) can be achieved by solutions of (6).",3. Statistical Guarantees for Generalization,[0],[0]
"Such fast rates results exist for binary classification under the so-called Mammen-Tsybakov noise condition, see e.g. Bousquet et al. (2004) for details.",3. Statistical Guarantees for Generalization,[0],[0]
"By means of a variant of the Bernstein inequality for U -statistics, we can establish fast rate bounds under the following condition on the data distribution.",3. Statistical Guarantees for Generalization,[0],[0]
Noise assumption (NA).,3. Statistical Guarantees for Generalization,[0],[0]
There exist a constant c and a ∈,3. Statistical Guarantees for Generalization,[0],[0]
"[0, 1] such that, almost surely,
EX′ [ |η(X,X ′)−Q∗α| −a] ≤ c.",3. Statistical Guarantees for Generalization,[0],[0]
"This noise condition is similar to that introduced by Mammen & Tsybakov (1995) for the binary classification framework, except that the threshold 1/2 is replaced here by the conditional quantile Q∗α.",3. Statistical Guarantees for Generalization,[0],[0]
It characterizes “nice” distributions for the problem of ROC optimization at point α: it essentially ensures that the pairwise posterior probability is bounded away from Q∗α with high probability.,3. Statistical Guarantees for Generalization,[0],[0]
"Under the assumption, we can derive the following fast learning rates.",3. Statistical Guarantees for Generalization,[0],[0]
Theorem 2.,3. Statistical Guarantees for Generalization,[0],[0]
"Suppose that the assumptions of Theorem 1 are satisfied, that condition NA holds true and that the optimal similarity rule S∗α(x, x
′) = I{(x, x′) ∈ R∗α} belongs to S0.",3. Statistical Guarantees for Generalization,[0],[0]
Fix δ > 0.,3. Statistical Guarantees for Generalization,[0],[0]
"Then, there exists a constant C ′, depending on δ, κ, Q∗α, a, c and V such that, with probability at least 1− δ,
ROCS∗(α)−R+(Ŝn) ≤ C ′n−(2+a)/4, and R−(Ŝn) ≤ α+ 2Φn,δ/2.
",3. Statistical Guarantees for Generalization,[0],[0]
Remark 3.,3. Statistical Guarantees for Generalization,[0],[0]
(ON THE NA CONDITION),3. Statistical Guarantees for Generalization,[0],[0]
"The noise condition is automatically fulfilled for any a ∈ (0, 1) when, for almost every point x with respect to the measure induced by X , η(x,X ′) has an absolutely continuous distribution and bounded density.",3. Statistical Guarantees for Generalization,[0],[0]
"This assumption means that the problem of ranking by similarity to an instance x is not too hard for any value of x, see supplementary material for more details.
",3. Statistical Guarantees for Generalization,[0],[0]
"The proof is based on the same argument as that of Clémençon & Vayatis (2010, Theorem 12 therein), except that it involves a sharp control of the fluctuations of the U -statistic estimates of the true positive rate excess ROCS∗(α)−R+(S) over the class S0.",3. Statistical Guarantees for Generalization,[0],[0]
"The reduced variance property of U -statistics plays a crucial role in the analysis, which essentially relies on the Hoeffding decomposition (see Hoeffding, 1948).",3. Statistical Guarantees for Generalization,[0],[0]
Technical details can be found in the supplementary material.,3. Statistical Guarantees for Generalization,[0],[0]
"In the previous section, we analyzed the learning rates achieved by a minimizer of the empirical problem (6).",4. Scalability by Sampling Approximations,[0],[0]
"In the large-scale setting, solving this problem can be computationally costly due to the very large number of training pairs.",4. Scalability by Sampling Approximations,[0],[0]
"In particular, the positive and negative empirical risks R̂+n (S) and R̂−n (S) are sums over respectively ∑K k=1 nk(nk−1)/2
and ∑ k<l nknl pairs.",4. Scalability by Sampling Approximations,[0],[0]
"We focus here more specifically on the setting where we have a large number of (rather balanced) classes, as in our biometric identification motivating example where a class corresponds to an identity.",4. Scalability by Sampling Approximations,[0],[0]
"In this regime, we are facing a highly imbalanced problem since the number of negative pairs becomes overwhelmingly large compared to the number of positive pairs.",4. Scalability by Sampling Approximations,[0],[0]
"For instance, even for the MNIST dataset where the number of classes is only K = 10 and nk = 6000, there are already 10 times more negative pairs than positive pairs.
",4. Scalability by Sampling Approximations,[0],[0]
"A natural strategy, often used by metric learning practitioners (see e.g., Babenko et al., 2009; Wu et al., 2013; Xie & Xing, 2015), is to drastically subsample the negative pairs while keeping all positive pairs.",4. Scalability by Sampling Approximations,[0],[0]
"In this section, we shed light on this popular practice by analyzing the effect of subsampling (conditionally upon the data) the negative pairs onto the generalization performance.
",4. Scalability by Sampling Approximations,[0],[0]
"A simple approach consists in replacing the empirical negative risk R̂−n (S) by the following approximation:
R̄−B(S)",4. Scalability by Sampling Approximations,[0],[0]
":= 1
B ∑ (i,j)∈PB S(Xi, Xj),
where PB is a set of cardinality B built by sampling with replacement in the set of negative training pairs ΛP = {(i, j) |",4. Scalability by Sampling Approximations,[0],[0]
"i, j ∈ {1, . . .",4. Scalability by Sampling Approximations,[0],[0]
", n};Yi 6= Yj}.",4. Scalability by Sampling Approximations,[0],[0]
"Conditioned upon the
nk’s, R̄−B(S) can be viewed as an incomplete version of the U -statistic R̂−n (S) consisting of B pairs (Blom, 1976; Lee, 1990).
",4. Scalability by Sampling Approximations,[0],[0]
"Despite the simplicity of the above approximation, we also consider an alternative sampling strategy, which consists in sampling a number B of K-tuples containing one random sample of each class.",4. Scalability by Sampling Approximations,[0],[0]
"Formally, this corresponds to the following approximation:
R̃−B(S)",4. Scalability by Sampling Approximations,[0],[0]
":= 1
B ∑ (i1,...iK)∈TB hS(Xi1 , . .",4. Scalability by Sampling Approximations,[0],[0]
.,4. Scalability by Sampling Approximations,[0],[0]
", XiK ),
where hS(X1, . . .",4. Scalability by Sampling Approximations,[0],[0]
", XK) = 1n− ∑ k<l nknlS(Xk, Xl) and TB is a set of cardinality B built by sampling with replacement in the set of K-tuples ΛT = {(i1, . . .",4. Scalability by Sampling Approximations,[0],[0]
", iK) | ik ∈ {1, . . .",4. Scalability by Sampling Approximations,[0],[0]
", nk}; k = 1, . . .",4. Scalability by Sampling Approximations,[0],[0]
",K}. R̃−B(S) is also an incomplete version of R̂−n (S), with the alternative view of R̂−n (S) as a generalized K-sample U -statistic",4. Scalability by Sampling Approximations,[0],[0]
"(Lee, 1990) of degree (1, . . .",4. Scalability by Sampling Approximations,[0],[0]
", 1) and kernel hS , see supplementary material for a full definition.",4. Scalability by Sampling Approximations,[0],[0]
"Note that R̃−B(S) contains BK(K − 1)/2 pairs, balanced across all class pairs.",4. Scalability by Sampling Approximations,[0],[0]
R̄−B(S) and R̃ − B(S) are both unbiased estimates of R̂,4. Scalability by Sampling Approximations,[0],[0]
"− n (S),",4. Scalability by Sampling Approximations,[0],[0]
but their variances are different and one approximation might be better than the other in some regimes.,4. Scalability by Sampling Approximations,[0],[0]
"The following result provides expressions for the variances of both incomplete estimators for a fixed budget of B0 sampled pairs, under a standard asymptotic framework.
",4. Scalability by Sampling Approximations,[0],[0]
Proposition 1.,4. Scalability by Sampling Approximations,[0],[0]
"Let B0 be the number of pairs sampled in both schemes, and denote Vn = Var(R̂−n (S)).",4. Scalability by Sampling Approximations,[0],[0]
"When B0/n→ 0, n→∞ and for all k ∈ {1, . . .",4. Scalability by Sampling Approximations,[0],[0]
",K}, nk/n→ pk > 0, we have:
Var(R̃−B(S))−",4. Scalability by Sampling Approximations,[0],[0]
"Vn ∼ K(K − 1)
2B0 Var(hS(X(1), . . .",4. Scalability by Sampling Approximations,[0],[0]
", X(k))),
",4. Scalability by Sampling Approximations,[0],[0]
"Var(R̄−B(S))− Vn ∼ B−10 Var(S(X,X ′) |Y 6= Y ′),
where X(k) denotes X |Y = k for all k ∈ {1, . . .",4. Scalability by Sampling Approximations,[0],[0]
",K}.
",4. Scalability by Sampling Approximations,[0],[0]
"Proposition 1 states that if the variance of similarity scores on the negative pairs is high compared to the variance of a weighted average of similarity scores on all types (k, l) of negative pairs, then one should prefer tuple-based sampling (otherwise pair-based sampling is better).",4. Scalability by Sampling Approximations,[0],[0]
"As an example, consider the case where the similarity scores on the negative pairs constructed from classes (k0, l0) are consistently higher than for other negative pairs.",4. Scalability by Sampling Approximations,[0],[0]
"These high similarity pairs will not be sampled very often by the pair-based sampling method, in contrast to the tuple-based approach.",4. Scalability by Sampling Approximations,[0],[0]
"In that scenario, the variance of S(X,X ′) |Y 6=",4. Scalability by Sampling Approximations,[0],[0]
"Y ′ is high while the variance of hS ( X(1), . . .",4. Scalability by Sampling Approximations,[0],[0]
", X(k) ) is low, and the tuple-based method should be preferred.",4. Scalability by Sampling Approximations,[0],[0]
"In practice, the
properties of the data should guide the choice of the sampling approach.
",4. Scalability by Sampling Approximations,[0],[0]
We now analyze the effect of sampling on the performance of the empirical risk minimizer.,4. Scalability by Sampling Approximations,[0],[0]
We consider tuple-based sampling (results of the same order can be obtained for pairbased sampling).,4. Scalability by Sampling Approximations,[0],[0]
"Let S̃B be the minimizer of the following simpler empirical problem:
arg max S∈S0
R̂+n (S) subject to R̃ − B(S) ≤",4. Scalability by Sampling Approximations,[0],[0]
"α+ Φn,δ,B .",4. Scalability by Sampling Approximations,[0],[0]
"(10)
We have the following theorem, based on combining Theorem 1 with a result bounding the maximal deviation between R̂−n (S) and its incomplete version R̃ − B(S), see Clémençon et al. (2016).
",4. Scalability by Sampling Approximations,[0],[0]
Theorem 3.,4. Scalability by Sampling Approximations,[0],[0]
"Let N = min1≤k≤K nk and α ∈ (0, 1), assume that S∗ ∈ S0 and that S0 is a VC-major class of dimension V .",4. Scalability by Sampling Approximations,[0],[0]
"For all (δ, n,B) ∈ (0, 1)× N∗ × N∗, set
Φn,δ,B = 4
√ V log(1 +N)
N +
√ log(2/δ)
N
+ √ 2 V log(1 + ∏K k=1 nk) + log(4/δ)
B .
",4. Scalability by Sampling Approximations,[0],[0]
"Then we have simultaneously with probability at least 1− δ,
R+(S̃B) ≥ R+∗",4. Scalability by Sampling Approximations,[0],[0]
"− 2Φn,δ,B and R−(S̃B) ≤ α+ 2Φn,δ,B .
",4. Scalability by Sampling Approximations,[0],[0]
"This result is very similar to Theorem 1, with an additive error term in O( √ log n/B).",4. Scalability by Sampling Approximations,[0],[0]
"Remarkably, this implies that it is sufficient to sample B = O(n) tuples (hence only O(nK2) pairs) to preserve the O( √ log n/n) learning rate achieved when using all negative pairs.",4. Scalability by Sampling Approximations,[0],[0]
"This will be confirmed empirically in our numerical experiments.
",4. Scalability by Sampling Approximations,[0],[0]
Remark 4 (Approximating the positive risk).,4. Scalability by Sampling Approximations,[0],[0]
"When needed, sampling-based techniques can also be used to approximate the empirical positive risk R̂+n (S), with generalization results analogous to Theorem 3.",4. Scalability by Sampling Approximations,[0],[0]
Details are left to the reader.,4. Scalability by Sampling Approximations,[0],[0]
"In this section, we present some experiments to illustrate our main results.",5. Illustrative Experiments,[0],[0]
We first illustrate how solving instances of Problem (6) allows to optimize for specific points of the ROC curve.,5. Illustrative Experiments,[0],[0]
We then provide some numerical evidence of the fast rates of Theorem 2.,5. Illustrative Experiments,[0],[0]
"Finally, we illustrate our scalability results of Section 4 by showing that dramatically subsampling the negative empirical risk leads to negligible loss in generalization performance.",5. Illustrative Experiments,[0],[0]
"We illustrate on synthetic data that solving (6) for different values of α can optimize for different regions of the
ROC curve.",5.1. Pointwise ROC Optimization,[0],[0]
"Let X ⊂ Rd, and let S0 be the set of bilinear similarities with norm-constrained matrices
S0 =",5.1. Pointwise ROC Optimization,[0],[0]
"{ SA : (x, x
′) 7→ 1 2
( 1 + x>Ax′ ) ∣∣",5.1. Pointwise ROC Optimization,[0],[0]
"‖A‖2F ≤ 1} , where ‖A‖2F = ∑d",5.1. Pointwise ROC Optimization,[0],[0]
"i,j=1 a 2 ij .",5.1. Pointwise ROC Optimization,[0],[0]
"Note that when data is scaled (‖x‖ = 1 for all x ∈ X ), we have SA(x, x′) ∈",5.1. Pointwise ROC Optimization,[0],[0]
"[0, 1] for all x, x′ ∈ X and all SA ∈ S0.",5.1. Pointwise ROC Optimization,[0],[0]
"In our simple experiment, we have K = 3 classes and observations belong to the sphere in R3.",5.1. Pointwise ROC Optimization,[0],[0]
"Denoting by θx,ci the angle between the element x and the centroid ci of class",5.1. Pointwise ROC Optimization,[0],[0]
"i, we set for all i ∈ {1, 2, 3},
µi(x) ∝",5.1. Pointwise ROC Optimization,[0],[0]
"I { θx,ci < π
4
} , pi = 1
3
and c1 = (cos(π/3), sin(π/3), 0), c2 = e2, c3 = e3 with ei vectors of the standard basis of R3.",5.1. Pointwise ROC Optimization,[0],[0]
"See Figure 1(a) for a graphical representation of the data.
",5.1. Pointwise ROC Optimization,[0],[0]
The solutions of the problem can be expressed in closed form using Lagrangian duality.,5.1. Pointwise ROC Optimization,[0],[0]
"In particular, when the constraints are saturated, the solution SAα is an increasing transformation of sP−λαN with
P = 1
2n+ ∑ 1≤i<j≤n",5.1. Pointwise ROC Optimization,[0],[0]
"I {Yi = Yj} · ( XiX > j +XjX > i ) ,
N = 1
2n− ∑ 1≤i<j≤n I {Yi 6=",5.1. Pointwise ROC Optimization,[0],[0]
"Yj} · ( XiX > j +XjX > i ) ,
and λα is a positive Lagrange multiplier decreasing in α, see supplementary material for details.",5.1. Pointwise ROC Optimization,[0],[0]
"By varying α, we tradeoff between the information contained in the positive pairs (α large, λα close to zero) and in the negative pairs (α small, λα large), which indeed results in optimizing different areas of the ROC curve, see Figure 1(b).",5.1. Pointwise ROC Optimization,[0],[0]
"Theorem 2 shows that when the noise assumption NA is verified, faster rates of generalization can be achieved.",5.2. Fast Rates,[0],[0]
"Showing
the existence of fast rates experimentally requires us to design a problem for which the η satisfies NA, which is not trivial due to the pairwise nature of the involved quantities.",5.2. Fast Rates,[0],[0]
"We emphasize that such empirical evidence of fast rates is rarely found in the literature.
",5.2. Fast Rates,[0],[0]
We put ourselves in a simple scenario where X =,5.2. Fast Rates,[0],[0]
"[0, 1], µ = 1, K = 2 and p1 = p2 = 1/2.",5.2. Fast Rates,[0],[0]
"In that context, characterizing µ1(dx) is sufficient to have a fully defined problem.",5.2. Fast Rates,[0],[0]
"With m ∈ (0, 12 ), a ∈ (0, 1) and C ∈ (0, 12 ), we set
µ1(x) =",5.2. Fast Rates,[0],[0]
{ 2C if x ∈,5.2. Fast Rates,[0],[0]
"[0,m], 1− |2x− 1|(1−a)/a if x ∈ (m, 1/2],
where C is chosen so that Q∗α = 1/2 and m is fixed in advance.",5.2. Fast Rates,[0],[0]
"Since ∫ µ1(dx) = 1, we chose µ1 symmetric in (1/2, 1) to satisfy that constraint.",5.2. Fast Rates,[0],[0]
"Figure 2 shows example distributions.
",5.2. Fast Rates,[0],[0]
"Given that µ = 1, the noise assumption with a close to 1 requires that there are sharp variations of η close to Q∗α.",5.2. Fast Rates,[0],[0]
"To induce the form of the function more easily, we fixed Q∗α = 1/2, which requires us to choose µ1 such that the value of the integral of η is controlled while η has the expected local property around 1/2.",5.2. Fast Rates,[0],[0]
More details about the design of the experiment can be found in the supplementary material.,5.2. Fast Rates,[0],[0]
"When t is small enough, P (|η(X,X ′)−Q∗α| ≤ t)
is of order −t a1−a log(t).",5.2. Fast Rates,[0],[0]
"Due to the logarithm term in the noise condition, we expect that the generalization speeds to be slightly worse than O(n−(2+a)/4).
",5.2. Fast Rates,[0],[0]
"The family S0 is composed of indicators of sets, which are parameterized by t ∈ (0, 1) (see supplementary material for a graphical representation).",5.2. Fast Rates,[0],[0]
"Each set contains the pairs (x, x′) such that one of the supremum distances between (x, x′) and (0, 0) or (1, 1) is smaller than t, which writes
{x, x′ ∈ X | min(max(1− x, 1− x′),max(x, x′))",5.2. Fast Rates,[0],[0]
"< t} .
",5.2. Fast Rates,[0],[0]
"The optimal set can thus always be identified, and R+(S) and R−(S) can be expressed analytically for some S ∈ S0.",5.2. Fast Rates,[0],[0]
The empirical problem Eq.,5.2. Fast Rates,[0],[0]
"(6) is always solved neglecting the tolerance parameter Φ, i.e. setting Φ = 0.
",5.2. Fast Rates,[0],[0]
"Figure 3 shows experiments for the case α = 0.26, m = 0.35 and a ∈",5.2. Fast Rates,[0],[0]
"[0.1, 0.9].",5.2. Fast Rates,[0],[0]
"For some a, the empirical 90- quantile of ROCS∗(α)−R+(Ŝn) is computed for different values of n on 1000 experiments and its logarithm is fitted to Ca× log(n)",5.2. Fast Rates,[0],[0]
+Da to get the empirical generalization speed Ca.,5.2. Fast Rates,[0],[0]
"There is a clear downward trend when a increases, illustrating the fast rates in practice.",5.2. Fast Rates,[0],[0]
"We illustrate the results of Section 4 on MMC (Xing et al., 2002), a popular metric learning algorithm whose formulation is very close to the one we consider.",5.3. Scalability by Sampling,[0],[0]
"We introduce the set of Mahalanobis distances dA indexed by a positive
semidefinite matrix A:
dA(x, x ′)",5.3. Scalability by Sampling,[0],[0]
"= √ (x− x′)>A(x− x′).
",5.3. Scalability by Sampling,[0],[0]
"MMC solves the following problem (using projected gradient ascent):
max A
1
n− ∑ 1≤i<j≤n I{Yi 6=",5.3. Scalability by Sampling,[0],[0]
"Yj} · dA(Xi, Xj)
s.t. 1
n+ ∑ 1≤i<j≤n I{Yi = Yj} · d2A(Xi, Xj) ≤ 1
A 0
We use MNIST dataset, composed of 70, 000 images representing the 0-9 handwritten digits, with classes roughly equally distributed.",5.3. Scalability by Sampling,[0],[0]
"We randomly split it into a training set and a test set of 10, 000 instances.",5.3. Scalability by Sampling,[0],[0]
"As done in previous work, the dimension of the features is reduced using PCA to keep 90% of the explained variance.",5.3. Scalability by Sampling,[0],[0]
"We approximate the average over negative pairs by sampling K-tuples with B terms, as proposed in Section 4 (pair-based sampling performs similarly on this dataset).",5.3. Scalability by Sampling,[0],[0]
We aim to show that optimizing the criterion on the resulting smaller set of pairs does not significantly impact the learning rate (yet greatly reduces training time).,5.3. Scalability by Sampling,[0],[0]
"We solve MMC on the training set for a varying number of training instances n and of K-tuples B, and report the objective and constraint values on the test set.",5.3. Scalability by Sampling,[0],[0]
"The results, summarized in Figure 4, confirm the small performance loss due to subsampling, for a huge improvement in terms of computing time.",5.3. Scalability by Sampling,[0],[0]
"Indeed, when n = 60, 000, the total number of negative pairs is almost 2 billions while B = 0.15n corresponds to sampling only 400, 000 pairs.",5.3. Scalability by Sampling,[0],[0]
We have introduced a rigorous probability framework to study similarity learning from the novel perspective of pairwise bipartite ranking and pointwise ROC optimization.,6. Conclusion,[0],[0]
"We derived statistical guarantees for generalization in this context, and analyzed the impact of using sampling-based approximations.",6. Conclusion,[0],[0]
Our results are illustrated on a series of numerical experiments.,6. Conclusion,[0],[0]
Our study opens promising directions of future work.,6. Conclusion,[0],[0]
"We are especially interested in extending our results to allow the rejection of queries from unseen classes (e.g., unknown identities) at test time (see for instance Bendale & Boult, 2015).",6. Conclusion,[0],[0]
"This could be achieved by incorporating a loss function to encourage the score of all positive pairs to be above some fixed threshold, below which we would reject the query.",6. Conclusion,[0],[0]
This work was supported by IDEMIA.,Acknowledgments,[0],[0]
"We would like to thank Anne Sabourin for her substantial feedback that has greatly improved this work, as well as the ICML reviewers for their constructive input.",Acknowledgments,[0],[0]
The performance of many machine learning techniques depends on the choice of an appropriate similarity or distance measure on the input space.,abstractText,[0],[0]
Similarity learning (or metric learning) aims at building such a measure from training data so that observations with the same (resp.,abstractText,[0],[0]
different) label are as close (resp. far) as possible.,abstractText,[0],[0]
"In this paper, similarity learning is investigated from the perspective of pairwise bipartite ranking, where the goal is to rank the elements of a database by decreasing order of the probability that they share the same label with some query data point, based on the similarity scores.",abstractText,[0],[0]
A natural performance criterion in this setting is pointwise ROC optimization: maximize the true positive rate under a fixed false positive rate.,abstractText,[0],[0]
We study this novel perspective on similarity learning through a rigorous probabilistic framework.,abstractText,[0],[0]
"The empirical version of the problem gives rise to a constrained optimization formulation involving U -statistics, for which we derive universal learning rates as well as faster rates under a noise assumption on the data distribution.",abstractText,[0],[0]
We also address the large-scale setting by analyzing the effect of sampling-based approximations.,abstractText,[0],[0]
Our theoretical results are supported by illustrative numerical experiments.,abstractText,[0],[0]
A Probabilistic Theory of Supervised Similarity Learning for Pointwise ROC Curve Optimization,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1996–2001, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
"Gender has been an important research topic in the social sciences, with studies conducted on the effect of gender on various aspects of human perception and expression (Benshoff and Griffin, 2011) as well as investigations of the societal (BehmMorawitz and Mastro, 2008) and career implications of gender and possible underlying biases.",1 Introduction,[0],[0]
"Previous studies report significant implications of gender on career progress in medicine (Sidhu et al., 2009), information technology (Cohoon and Aspray, 2006), politics (Niven, 2006) and showbusiness (Smith, 2010).
",1 Introduction,[0],[0]
"In this paper we investigate the depictions of the genders in feature films, through the analysis of their respective dialogues.",1 Introduction,[0],[0]
"The differences in depiction are a contentious subject, since aspects of these can be viewed as the result of stereotyping or gender bias, with the relative presence of women being a well investigated subject (Bielby and Bielby, 1996; Lincoln and Allen, 2004).",1 Introduction,[0],[0]
"We are interested in the existing gender depictions, re-
gardless of relative frequencies, as well as any factors that may affect them.",1 Introduction,[0],[0]
"While popular tools such as the Bechdel test provide a test for detecting female presence in the movies, we hope to identify more subtle forms of gender differences across character gender from the dialogues.",1 Introduction,[0],[0]
"Our aim is to devise a non-binary metric that can be used to compare or rank movies, characters and perhaps individual utterances.
",1 Introduction,[0],[0]
"To analyze the dialogues we propose using a metric of language gender ladenness, a number representing a normative rating of the “perceived feminine or masculine association” (Paivio et al., 1968) of language.",1 Introduction,[0],[0]
"The metric, as originally defined, is meant to provide an indication of gender-specificity of individual words, with extreme values assigned to highly stereotypical concepts.",1 Introduction,[0],[0]
"Generating this rating for male and female character dialogues and comparing the character gender with this rating of “language gender” should allow us to observe stereotypical behavior.
",1 Introduction,[0],[0]
"Word based ratings such as the gender ladenness are referred to as linguistic norms (or psycholinguistic norms when corresponding to psychological constructs) and are popular in cognitive psychology (Clark and Paivio, 2004) and some computational disciplines, such as sentiment analysis (Nielsen, 2011) and opinion mining.",1 Introduction,[0],[0]
"To utilize gender ladenness, we follow an approach similar to simple sentiment analysis, with word-level norms automatically generated based on a small starting set of manually annotated norms and sentence (and higher) level norms estimated through word-level norm statistics.",1 Introduction,[0],[0]
"The resulting algorithm allows us to estimate gender ladenness at any arbitrary granularity.
",1 Introduction,[0],[0]
"We use these ratings of dialogue language to quantify the depictions of male and female characters and attempt to relate the observed gender ladenness with objective factors.
",1 Introduction,[0],[0]
"In section 2 and 3 we describe the data corpus
1996
and the feature extraction process respectively.",1 Introduction,[0],[0]
We detail the experimental procedure in section 4 and analysis in section 5 and conclude with future extensions in section 6.,1 Introduction,[0],[0]
"Gender Ladenness, as defined in (Clark and Paivio, 2004) represents the degree of perceived “feminine or masculine association” on a numerical scale ranging from very masculine to very feminine.",2 Estimating Gender Ladenness,[0],[0]
"It is important to note that there was no restriction to what “association” may mean: while it is reasonable to assume that associations of the form “A is B” or “A has B” would dominate annotator perception, that does not preclude other forms of association.",2 Estimating Gender Ladenness,[0],[0]
"Because of that, referring to the norms as indicators of how masculine or feminine the words are is not entirely accurate, though it is a reasonable approximation.",2 Estimating Gender Ladenness,[0],[0]
"The original ratings were re-scaled to [−1, 1] for our purposes, with lower values indicating a masculine association and high values indicating a feminine association.",2 Estimating Gender Ladenness,[0],[0]
"Some sample words, utterances and their corresponding ratings are presented in Table 1 and Table 3.",2 Estimating Gender Ladenness,[0],[0]
Figure 1 shows the average gender ladenness across all utterances for the major characters of a few movies.,2 Estimating Gender Ladenness,[0],[0]
"The annotations as a whole are reflective of stereotypical views of gender roles, e.g., words related to war and violence have a strong masculine association, whereas words related to family or positive emotions carry strong feminine associations.
",2 Estimating Gender Ladenness,[0],[0]
"The manual annotations from (Clark and Paivio, 2004) contain ratings for only 925 words, which are not enough to provide sufficient coverage.
",2 Estimating Gender Ladenness,[0],[0]
"Therefore we use a lexicon expansion method, inspired by the work of (Malandrakis et al., 2013) to estimate the gender ladenness ĝ(wi) of word wi using the semantic similarities s() between wi and reference words or concepts cj , as
ĝ(wi) = θ0 + N∑
j=1
θjs(wi, cj), (1)
where the terms θi are trained model parameters.",2 Estimating Gender Ladenness,[0],[0]
"Given a manually annotated lexicon and a set of reference words, this equation can be used to create a linear system.",2 Estimating Gender Ladenness,[0],[0]
"Solving the system via Least Squares Estimation (LSE) gives us the parameters θ and an equation that can be used to generate gender ladenness for any new set of words.
",2 Estimating Gender Ladenness,[0],[0]
"Gender ladenness for larger lexical units is generated via simple statistics, as the average of word gender ladenness over all content words (adjectives, nouns, verbs and adverbs).",2 Estimating Gender Ladenness,[0],[0]
"Our primary data source is the Movie DiC corpus (Banchs, 2012) which includes 619 movie scripts parsed from The Internet Movie Script Database (IMSDb, 2015).",3 Data,[0],[0]
The xml formatted scripts contain transcripts with speaker information as well as some structural information.,3 Data,[0],[0]
"Additional metadata for each movie were collected from the Internet Movie Database (IMDb, 2015).
",3 Data,[0],[0]
"Since our goal was to analyze gender depictions, we had to annotate each script utterance with a gender label.",3 Data,[0],[0]
"The process was complicated by inconsistencies between the information contained in the IMDb and Movie DiC corpora, like mismatched names, particularly for minor characters.",3 Data,[0],[0]
"Initially the script character names were cleaned using simple heuristics, such as the removal of all instances of the possessive “’s”.",3 Data,[0],[0]
"The IMDb api (IMDbPY, 2015) was used to recover candidate movies matching the script movie name and, in the case of multiple candidates, the best candidate was selected based on the number of character names matching the script.",3 Data,[0],[0]
"Character names were compared using the Jaro-Winkler distance (Winkler, 1990).",3 Data,[0],[0]
"Having achieved a one to one mapping between IMDb and Movie DiC, we assigned a gender label to each matched character, using the gender predictor (NamSor Applied Onomastics, 2015).",3 Data,[0],[0]
"To make these predictions, we first use the name of the corresponding actor portraying that role; if there was no character match, we use the name of the character.",3 Data,[0],[0]
"Finally, we calculate a confidence score of our gender assignment per utterance for each movie, equal to the percentage of utterances with perfectly matched character name and a high confidence by the gender predictor.",3 Data,[0],[0]
"For the movies for which the confidence scores are not satisfactory, we manually match the script characters with IMDb’s characters and annotate genders.",3 Data,[0],[0]
"In our experiments, we did this manual annotation with roughly 75 movies.
",3 Data,[0],[0]
"Having a mapping of scripts to IMDb entries, we collected more information about the movie such as the list of genres it belongs to and the members of the production team (producers, scriptwriters, directors), and followed a similar process as described above to assign genders to all persons.",3 Data,[0],[0]
"While movies may be created by multiple scriptwriters and directors, we retain only the first name, the primary credit, in each category.",3 Data,[0],[0]
We removed infrequent genres and movies which belonged only to the removed genres.,3 Data,[0],[0]
"We also filtered out utterances with missing or incorrect character information and the utterances corresponding to characters for which the gender predictor fails to make confident predictions.
",3 Data,[0],[0]
"Movies with missing fields were also removed, leaving us with a total of 568 movies after the aforementioned pre-processing steps.",3 Data,[0],[0]
"Table 2 lists some descriptive statistics of the processed movie
corpus.",3 Data,[0],[0]
"At least in terms of raw frequencies, the gender ratio is clearly skewed towards male, particularly in the case of directors and with the exception of casting directors.
",3 Data,[0],[0]
"The norm generating equation (1) requires a semantic similarity estimate s(), which for the purposes of this paper is the cosine of context vectors generated over a large corpus of raw web text.",3 Data,[0],[0]
The corpus was created by posing a query to the Yahoo! search engine for every word in the English version of the aspell spell-checker and collecting the top 500 result previews.,3 Data,[0],[0]
"Each preview is composed of a title and a sample of the content, each being a single sentence.",3 Data,[0],[0]
Overall the collected corpus contains approximately 117 million sentences.,3 Data,[0],[0]
"The descriptive feature in this method is gender ladenness, so we extracted an estimate for each utterance of every movie.",4 Experimental Procedure,[0],[0]
"Initially, all utterances were part-of-speech tagged and non-content words were removed.",4 Experimental Procedure,[0],[0]
"Then, word-level gender landenness norms were generated for every remaining word.
",4 Experimental Procedure,[0],[0]
"To generate word-level norms, we used equation (1) with the intermediate seed words wi being the top 10000 most frequent words in our corpus of web text with length longer that 3 characters.",4 Experimental Procedure,[0],[0]
"For each word in our corpus, we generated a binary weighted context vector (of window size 1) of size ∼ 125000.",4 Experimental Procedure,[0],[0]
"Then, for each word
of interest we calculated a 10000 place similarity vector, containing the cosine similarity scores between the context vector of said word and the context vectors of the 10000 intermediate seeds.",4 Experimental Procedure,[0],[0]
"Using the training set we generated a K × 10000 matrix of similarities to the seed words and applied dimensionality reduction via Principal Component Analysis (PCA), keeping the firstN = 300 components.",4 Experimental Procedure,[0],[0]
These transformed similarities became the similarity terms s() of equation (1) and were used to train the model.,4 Experimental Procedure,[0],[0]
"For any word in the scripts, a 10000 place similarity vector is generated and transformed using the pre-calculated PCA matrix, then equation (1) is used to create the gender ladenness estimate.
",4 Experimental Procedure,[0],[0]
"Ratings were generated at the utterance level, and collective ratings (per character, gender or movie) were calculated as utterance rating averages.",4 Experimental Procedure,[0],[0]
"To evaluate the word norm generation algorithm, we performed a 10-fold cross-validation experiment on the 925 manually annotated norms in (Paivio et al., 1968).",5 Results,[0],[0]
The generated norms were evaluated against the ground truth and the method achieved a 0.801 Pearson correlation to the ground truth.,5 Results,[0],[0]
"While there is no comparable result in literature, the resulting performance appears sufficiently high.
",5 Results,[0],[0]
"We first investigated the overall gender ladenness of movies, represented as the average of all utterance level scores, with respect to the genre(s) the movie belongs to.",5 Results,[0],[0]
"The independent variables for this analysis were nine binary indicator variables, one for each of the most frequent genre labels in our movie corpus, with values of zero if the movie does not belong to the specific genre and one if it does.",5 Results,[0],[0]
The particular representation of genres as separate variables was chosen because each movie can belong to multiple genres.,5 Results,[0],[0]
Interaction terms were included.,5 Results,[0],[0]
"Running nway ANOVA with the aggregate gender ladenness across both character genders as the dependent variable revealed significant differences between genres, with Action movies leaning towards the masculine (p = 0.013) compared to Non-Action movies, a not surprising result.
",5 Results,[0],[0]
A few significant interactions between genres are shown in figure 2.,5 Results,[0],[0]
Fig.,5 Results,[0],[0]
"2a indicates that among non-drama movies, romantic movies tend to in-
Drama Others −12
−10
−8
−6
−4
−2 x 10
−3
Rom Oth
Crime Others −0.014
−0.012
−0.01
−0.008
Thr Oth
(a) Drama v/s Romance (b) Crime v/s Thriller
clude more feminine language compared to nonromantic movies.",5 Results,[0],[0]
"However, if a movie belongs to the genre drama, its mean gender ladenness scores remain fairly constant, irrespective of its other genres.",5 Results,[0],[0]
"Similar interpretations can be drawn from the other plots in figure 2.
",5 Results,[0],[0]
"To analyze the effect of character gender on the gender ladenness scores, we next ran ANOVA with the character gender and the movie writer’s gender as additional independent variables.",5 Results,[0],[0]
"The dependent variable in this case was the aggregate gender ladenness score across all utterances for male and female characters, so two scores per movie.",5 Results,[0],[0]
The interaction of character gender and movie genre is shown in figure 3.,5 Results,[0],[0]
"The scores of male and female characters are correlated, which can be attributed to the underlying concepts in the utterances from these movies.",5 Results,[0],[0]
"The difference between genders is significant (p = 0.034), with male characters consistently using significantly more masculine language than their female counterparts, a finding that lends some credence to the metric used.",5 Results,[0],[0]
"Looking at the binary genre variables revealed that
Action movies contained significantly more masculine language than Non-Action movies (p < 10−5) and the same holds for Crime movies (p < 10−5).",5 Results,[0],[0]
"Conversely, Romantic movies leaned towards the more feminine language than nonRomantic movies (p < 10−5) and similarly for Comedy movies compared to non-Comedy movies (p = 0.02).",5 Results,[0],[0]
"The male - female character gender
ladenness distance however is not affected in any significant way by the genre.
",5 Results,[0],[0]
"We include only the screenplay writer’s gender in our analysis, though both the directors and screenplay writers influence the dialog lines (utterances), since the writers are more likely to directly influence the actual language used.",5 Results,[0],[0]
"In addition, the very small number of female directors in the data, as seen in table 2, leads to a violation of ANOVA’s homoscedasticity assumption.",5 Results,[0],[0]
"Though the writer gender itself was not a significant factor, the interaction of writer’s gender with the Action genre was significant (p = 0.005).",5 Results,[0],[0]
The plot illustrating this interaction is shown in figure 4.,5 Results,[0],[0]
"It appears that female script writers write more masculine utterances compared to their male colleagues, at least for Action movies.",5 Results,[0],[0]
"We also investigated interactions between the writer and character gender, but none proved significant.",5 Results,[0],[0]
We used regression to extrapolate manually annotated psycholinguistic normatives to movie utterances and investigated the use of these metrics to describe gender depictions.,6 Conclusions and Future Work,[0],[0]
"The metric proved successful, showing significant differences between the genders and predictable patterns with respect to movie genres.
",6 Conclusions and Future Work,[0],[0]
"Future work will include the use of further metrics, with those describing emotions being the first candidates.",6 Conclusions and Future Work,[0],[0]
We also intend to collect more movie and character level metadata to be used in analysis.,6 Conclusions and Future Work,[0],[0]
"Finally, it is worth remembering that language provides only a partial description of de-
picted characters, so we should aim to augment with aural/visual information.",6 Conclusions and Future Work,[0],[0]
"The authors gratefully acknowledge support from NSF, Geena Davis Institute on Gender in Media and Google.org.",7 Acknowledgements,[0],[0]
Direct content analysis reveals important details about movies including those of gender representations and potential biases.,abstractText,[0],[0]
"We investigate the differences between male and female character depictions in movies, based on patterns of language used.",abstractText,[0],[0]
"Specifically, we use an automatically generated lexicon of linguistic norms characterizing gender ladenness.",abstractText,[0],[0]
We use multivariate analysis to investigate gender depictions and correlate them with elements of movie production.,abstractText,[0],[0]
The proposed metric differentiates between male and female utterances and exhibits some interesting interactions with movie genres and the screenplay writer gender.,abstractText,[0],[0]
A quantitative analysis of gender differences in movies using psycholinguistic normatives,title,[0],[0]
"With the rapid growth of social network platforms, more and more people tend to share their experiences and emotions online.",1 Introduction,[0],[0]
Emotion analysis of online text becomes a new challenge in Natural Language Processing (NLP).,1 Introduction,[0],[0]
"In recent years, studies in emotion analysis largely focus on emotion classification including detection of writers’ emotions (Gao et al., 2013) as well as readers’ emotions (Chang et al., 2015).",1 Introduction,[0],[0]
"There are also some information extraction tasks defined in emotion analysis (Chen et al., 2016; Balahur et al., 2011), such as extracting the feeler of an emotion (Das and Bandyopadhyay, 2010).",1 Introduction,[0],[0]
"These methods
†Corresponding",1 Introduction,[0],[0]
"Author: xuruifeng@hit.edu.cn
assume that emotion expressions are already observed.",1 Introduction,[0],[0]
"Sometimes, however, we care more about the stimuli, or the cause of an emotion.",1 Introduction,[0],[0]
"For instance, Samsung wants to know why people love or hate Note 7 rather than the distribution of different emotions.",1 Introduction,[0],[0]
Ex.1我的手机昨天丢了，我现在很难过。,1 Introduction,[0],[0]
"Ex.1 Because I lost my phone yesterday, I feel sad now.
",1 Introduction,[0],[0]
"In an example shown above, “sad” is an emotion word, and the cause of “sad” is “I lost my phone”.",1 Introduction,[0],[0]
The emotion cause extraction task aims to identify the reason behind an emotion expression.,1 Introduction,[0],[0]
"It is a more difficult task compared to emotion classification since it requires a deep understanding of the text that conveys an emotions.
",1 Introduction,[0],[0]
"Existing approaches to emotion cause extraction mostly rely on methods typically used in information extraction, such as rule based template matching, sequence labeling and classification based methods.",1 Introduction,[0],[0]
"Most of them use linguistic rules or lexicon features, but do not consider the semantic information and ignore the relation between the emotion word and emotion cause.
",1 Introduction,[0],[0]
"In this paper, we present a new method for emotion cause extraction.",1 Introduction,[0],[0]
We consider emotion cause extraction as a question answering (QA) task.,1 Introduction,[0],[0]
"Given a text containing the description of an event which may or may not cause a certain emotion, we take an emotion word in context, such as “sad”, as a query.",1 Introduction,[0],[0]
The question to the QA system is: “Does the described event cause the emotion of sadness?”.,1 Introduction,[0],[0]
The expected answer is either “yes” or “no”.,1 Introduction,[0],[0]
(see Figure 1).,1 Introduction,[0],[0]
We build our QA system based on a deep memory network.,1 Introduction,[0],[0]
"The memory network has two inputs: a piece of text, referred to as a story in QA systems, and a query.",1 Introduction,[0],[0]
"The story is represented using a sequence of word embeddings.
",1 Introduction,[0],[0]
A recurrent structure is implemented to mine the deep relation between a query and a text.,1 Introduction,[0],[0]
"It
ar X
iv :1
70 8.
05 48
2v 2
[ cs
.C",1 Introduction,[0],[0]
"L
] 2
4 Se
p 20
17
measures the importance of each word in the text by an attention mechanism.",1 Introduction,[0],[0]
"Based on the learned attention result, the network maps the text into a low dimensional vector space.",1 Introduction,[0],[0]
This vector is then used to generate an answer.,1 Introduction,[0],[0]
Existing memory network based approaches to QA use weighted sum of attentions to jointly consider short text segments stored in memory.,1 Introduction,[0],[0]
"However, they do not explicitly model sequential information in the context.",1 Introduction,[0],[0]
"In this paper, we propose a new deep memory network architecture to model the context of each word simultaneously by multiple memory slots which capture sequential information using convolutional operations (Kim, 2014), and achieves the state-of-the-art performance compared to existing methods which use manual rules, common sense knowledge bases or other machine learning models.
",1 Introduction,[0],[0]
The rest of the paper is organized as follows.,1 Introduction,[0],[0]
Section 2 gives a review of related works on emotion analysis.,1 Introduction,[0],[0]
Section 3 presents our proposed deep memory network based model for emotion cause extraction.,1 Introduction,[0],[0]
Section 4 discusses evaluation results.,1 Introduction,[0],[0]
"Finally, Section 5 concludes the work and outlines the future directions.",1 Introduction,[0],[0]
"Identifying emotion categories in text is one of the key tasks in NLP (Liu, 2015).",2 Related Work,[0],[0]
"Going one step further, emotion cause extraction can reveal important information about what causes a certain emotion and why there is an emotion change.",2 Related Work,[0],[0]
"In this section, we introduce related work on emotion analysis including emotion cause extraction.
",2 Related Work,[0],[0]
"In emotion analysis, we first need to determine the taxonomy of emotions.",2 Related Work,[0],[0]
"Researchers have proposed a list of primary emotions (Plutchik, 1980; Ekman, 1984; Turner, 2000).",2 Related Work,[0],[0]
"In this study, we
adopt Ekman’s emotion classification scheme (Ekman, 1984), which identifies six primary emotions, namely happiness, sadness, fear, anger, disgust and surprise, known as the “Big6” scheme in the W3C Emotion Markup Language.",2 Related Work,[0],[0]
"This emotion classification scheme is agreed upon by most previous works in Chinese emotion analysis.
",2 Related Work,[0],[0]
"Existing work in emotion analysis mostly focuses on emotion classification (Li et al., 2013; Zhou et al., 2016) and emotion information extraction (Balahur et al., 2013).",2 Related Work,[0],[0]
Xu et al. (2012) used a coarse to fine method to classify emotions in Chinese blogs.,2 Related Work,[0],[0]
Gao et al. (2013) proposed a joint model to co-train a polarity classifier and an emotion classifier.,2 Related Work,[0],[0]
Beck et al. (2014) proposed a Multi-task Gaussian-process based method for emotion classification.,2 Related Work,[0],[0]
Chang et al. (2015) used linguistic templates to predict reader’s emotions.,2 Related Work,[0],[0]
Das and Bandyopadhyay (2010) used an unsupervised method to extract emotion feelers from Bengali blogs.,2 Related Work,[0],[0]
"There are other studies which focused on joint learning of sentiments (Luo et al., 2015; Mohtarami et al., 2013) or emotions in tweets or blogs (Quan and Ren, 2009; Liu et al., 2013; Hasegawa et al., 2013; Qadir and Riloff, 2014; Ou et al., 2014), and emotion lexicon construction (Mohammad and Turney, 2013; Yang et al., 2014; Staiano and Guerini, 2014).",2 Related Work,[0],[0]
"However, the aforementioned work all focused on analysis of emotion expressions rather than emotion causes.
",2 Related Work,[0],[0]
Lee et al. (2010) first proposed a task on emotion cause extraction.,2 Related Work,[0],[0]
They manually constructed a corpus from the Academia Sinica Balanced Chinese Corpus.,2 Related Work,[0],[0]
"Based on this corpus, Chen et al. (2010) proposed a rule based method to detect emotion causes based on manually define linguistic rules.",2 Related Work,[0],[0]
"Some studies (Gui et al., 2014; Li and Xu, 2014; Gao et al., 2015) extended the rule based method to informal text in Weibo text (Chinese tweets).
",2 Related Work,[0],[0]
"Other than rule based methods, Russo et al. (2011) proposed a crowdsourcing method to construct a common-sense knowledge base which is related to emotion causes.",2 Related Work,[0],[0]
But it is challenging to extend the common-sense knowledge base automatically.,2 Related Work,[0],[0]
Ghazi et al. (2015) used Conditional Random Fields (CRFs) to extract emotion causes.,2 Related Work,[0],[0]
"However, it requires emotion cause and emotion keywords to be in the same sentence.",2 Related Work,[0],[0]
"More recently, Gui et al. (2016) proposed a multi-kernel based method to extract emotion causes through
learning from a manually annotated emotion cause dataset.
",2 Related Work,[0],[0]
"Most existing work does not consider the relation between an emotion word and the cause of such an emotion, or they simply use the emotion word as a feature in their model learning.",2 Related Work,[0],[0]
"Since emotion cause extraction requires an understanding of a given piece of text in order to correctly identify the relation between the description of an event which causes an emotion and the expression of that emotion, it can essentially be considered as a QA task.",2 Related Work,[0],[0]
"In our work, we choose the memory network, which is designed to model the relation between a story and a query for QA systems (Weston et al., 2014; Sukhbaatar et al., 2015).",2 Related Work,[0],[0]
"Apart from its application in QA, memory network has also achieved great successes in other NLP tasks, such as machine translation (Luong et al., 2015), sentiment analysis (Tang et al., 2016) or summarization (M. Rush et al., 2015).",2 Related Work,[0],[0]
"To the best of our knowledge, this is the first work which uses memory network for emotion cause extraction.",2 Related Work,[0],[0]
"In this section, we will first define our task.",3 Our Approach,[0],[0]
"Then, a brief introduction of memory network will be given, including its basic learning structure of memory network and deep architecture.",3 Our Approach,[0],[0]
"Last, our modified deep memory network for emotion cause extraction will be presented.",3 Our Approach,[0],[0]
"The formal definition of emotion cause extraction is given in (Gui et al., 2016).",3.1 Task Definition,[0],[0]
"In this task, a given document, which is a passage about an emotion event, contains an emotion word E and the cause of the event.",3.1 Task Definition,[0],[0]
The document is manually segmented in the clause level.,3.1 Task Definition,[0],[0]
"For each clause c = {w1, w2, ...wk} consisting of k words, the goal is to identify which clause contains the emotion cause.",3.1 Task Definition,[0],[0]
"For data representation, we can map each word into a low dimensional embedding space, a.k.a word vector (Mikolov et al., 2013).",3.1 Task Definition,[0],[0]
All the word vectors are stacked in a word embedding matrix L ∈,3.1 Task Definition,[0],[0]
"Rd×‖V ‖, where d is the dimension of word vector and V is the vocabulary size.
",3.1 Task Definition,[0],[0]
"For example, the sentence, “I lost my phone yesterday, I feel so sad now.”",3.1 Task Definition,[0],[0]
"shown in Figure 1, consists of two clauses.",3.1 Task Definition,[0],[0]
"The first clause contains the emotion cause while the second clause ex-
presses the emotion of sadness.",3.1 Task Definition,[0],[0]
Current methods to emotion cause extraction cannot handle complex sentence structures where the expression of an emotion and its cause are not adjacent.,3.1 Task Definition,[0],[0]
We envision that the memory network can better model the relation between a emotion word and its emotion causes in such complex sentence structures.,3.1 Task Definition,[0],[0]
"In our approach, we only select the clause with the highest probability to be the emotion cause in each document.",3.1 Task Definition,[0],[0]
We first present a basic memory network model for emotion cause extraction (shown in Figure 2).,3.2 Memory Network,[0],[0]
"Given a clause c = {w1, w2, ..., wk}, and an emotion word, we first obtain the emotion word’s representation in an embedding space, denoted by E. For the clause, let the embedding representations of the words be denoted by e1, e2, ..., ek.",3.2 Memory Network,[0],[0]
"Here, both ei and E are defined in Rd.",3.2 Memory Network,[0],[0]
"Then, we use the inner product to evaluate the correlation between each word i in a clause and the emotion word, denoted as mi:
mi = ei · E. (1) We then normalize the value of mi to [0, 1] us-
ing a softmax function, denoted by αi as:
αi = exp (mi)∑k j=1 exp (mj) , (2)
where k is the length of the clause.",3.2 Memory Network,[0],[0]
k also serves as the size of the memory.,3.2 Memory Network,[0],[0]
"Obviously, αi ∈",3.2 Memory Network,[0],[0]
"[0, 1] and ∑k i=1",3.2 Memory Network,[0],[0]
αi,3.2 Memory Network,[0],[0]
= 1.,3.2 Memory Network,[0],[0]
"αi can serve as an attention weight to measure the importance of each word in our model.
",3.2 Memory Network,[0],[0]
"Then, a sum over the word embedding ei, weighted by the attention vector form the output of the memory network for the prediction of o:
o = k∑ i=1",3.2 Memory Network,[0],[0]
"ei · αi + E. (3)
",3.2 Memory Network,[0],[0]
"The final prediction is an output from a softmax function, denoted as ô:
ô = softmax ( W T o ) .",3.2 Memory Network,[0],[0]
"(4)
Usually, W is a d × d weight matrix and T is the transposition.",3.2 Memory Network,[0],[0]
"Since the answer in our task is a simple “yes” or “no”, we use a d × 1 matrix for W .",3.2 Memory Network,[0],[0]
"As the distance between a clause and an emotion words is a very important feature according to (Gui et al., 2016), we simply add this distance into the softmax function as an additional feature in our work.
",3.2 Memory Network,[0],[0]
The basic model can be extended to deep architecture consisting of multiple layers to handle L hop operations.,3.2 Memory Network,[0],[0]
"The network is stacked as follows:
• For hop 1, the query is E and the prediction vector is o1;
• For hop i, the query is the prediction vector of the previous hop and the prediction vector is oi;
•",3.2 Memory Network,[0],[0]
The output vector is at the top of the network.,3.2 Memory Network,[0],[0]
"It is a softmax function on the prediction vector from hop L: ô = softmax ( W T oL ) .
",3.2 Memory Network,[0],[0]
The illustration of a deep memory network with three layers is shown in Figure 3.,3.2 Memory Network,[0],[0]
"Since a memory network models the emotion cause at a fine-grained level, each word has a corresponding weight to measure its importance in this task.",3.2 Memory Network,[0],[0]
"Comparing to previous approaches in emotion cause extraction which are mostly based on manually defined rules or linguistic features, a memory network is a more principled way to identify
the emotion cause from text.",3.2 Memory Network,[0],[0]
"However, the basic memory network model does not capture the sequential information in context which is important in emotion cause extraction.",3.2 Memory Network,[0],[0]
"It is often the case that the meaning of a word is determined by its context, such as the previous word and the following word.",3.3 Convolutional Multiple-Slot Deep Memory Network,[0],[0]
"Also, negations and emotion transitions are context sensitive.",3.3 Convolutional Multiple-Slot Deep Memory Network,[0],[0]
"However, the memory network described in Section 3.2 has only one memory slot with size d × k to represent a clause, where d is the dimension of a word embedding and k is the length of a clause.",3.3 Convolutional Multiple-Slot Deep Memory Network,[0],[0]
"It means that when the memory network models a clause, it only considers each word separately.
",3.3 Convolutional Multiple-Slot Deep Memory Network,[0],[0]
"In order to capture context information for clauses, we propose a new architecture which contains more memory slot to model the context with a convolutional operation.",3.3 Convolutional Multiple-Slot Deep Memory Network,[0],[0]
"The basic architecture of Convolutional Multiple-Slot Memory Network (in short: ConvMS-Memnet) is shown in Figure 4.
",3.3 Convolutional Multiple-Slot Deep Memory Network,[0],[0]
"Considering the text length is usually short in the dataset used here for emotion cause extraction, we set the size of the convolutional kernel to 3.",3.3 Convolutional Multiple-Slot Deep Memory Network,[0],[0]
"That is, the weight of word wi in the i-th position considers both the previous wordwi−1 and the following word wi+1 by a convolutional operation:
m′i = 3∑ j=1 ei−2+j · E (5)
",3.3 Convolutional Multiple-Slot Deep Memory Network,[0],[0]
"For the first and the last word in a clause, we use zero padding, w0 = wk+1 = ~0, where k is the
length of a clause.",3.3 Convolutional Multiple-Slot Deep Memory Network,[0],[0]
"Then, the attention weight for each word position in the clause is now defined as:
α′i = exp (m′i)∑k j=1 exp ( m′j ) (6) Note that we obtain the attention for each position rather than each word.",3.3 Convolutional Multiple-Slot Deep Memory Network,[0],[0]
It means that the corresponding attention for the i-th word in the previous convolutional slot should be αi+1.,3.3 Convolutional Multiple-Slot Deep Memory Network,[0],[0]
"Hence, there are three prediction output vectors, namely, oprevious, ocurrent, ofollowing:
oprevious = k∑ i=1",3.3 Convolutional Multiple-Slot Deep Memory Network,[0],[0]
"ei−1 · α′i + E (7)
ocurrent = k∑
i=1
ei · α′i + E (8)
ofollowing = k∑
i=1
ei+1 ·",3.3 Convolutional Multiple-Slot Deep Memory Network,[0],[0]
"α′i + E (9)
",3.3 Convolutional Multiple-Slot Deep Memory Network,[0],[0]
"At last, we concatenate the three vectors as o = oprevious ⊕ ocurrent ⊕ ofollowing for the prediction by a softmax function:
ô = softmax ( W Tmo ) (10)
Here, the size of Wm is (3 · d) ×",3.3 Convolutional Multiple-Slot Deep Memory Network,[0],[0]
d. Since the prediction vector is a concatenation of three outputs.,3.3 Convolutional Multiple-Slot Deep Memory Network,[0],[0]
We implement a concatenation operation rather than averaging or other operations because the parameters in different memory slots can be updated by back propagation.,3.3 Convolutional Multiple-Slot Deep Memory Network,[0],[0]
The concatenation of three output vectors forms a sequence-level feature which can be used in the training.,3.3 Convolutional Multiple-Slot Deep Memory Network,[0],[0]
"Such a feature is important especially when the size of annotated training data is small.
",3.3 Convolutional Multiple-Slot Deep Memory Network,[0],[0]
"For deep architecture with multiple layer training, the network is more complex (shown in Figure 5).
",3.3 Convolutional Multiple-Slot Deep Memory Network,[0],[0]
"• For the first layer, the query is an embedding of the emotion word, E.
•",3.3 Convolutional Multiple-Slot Deep Memory Network,[0],[0]
"In the next layer, there are three input queries since the previous layer has three outputs: o1previous, o 1 current, o 1 following.",3.3 Convolutional Multiple-Slot Deep Memory Network,[0],[0]
"So, for the j-th
layer (j 6= 1), we need to re-define the weight function (5) as:
m′i = ei−1·o j−1 previous+ei·o j−1 current+ei+1·o j−1 following
(11)
",3.3 Convolutional Multiple-Slot Deep Memory Network,[0],[0]
•,3.3 Convolutional Multiple-Slot Deep Memory Network,[0],[0]
"In the last layer, the concatenation of the three prediction vectors form the final prediction vector to generate the answer.
",3.3 Convolutional Multiple-Slot Deep Memory Network,[0],[0]
"For model training, we use stochastic gradient descent and back propagation to optimize the loss function.",3.3 Convolutional Multiple-Slot Deep Memory Network,[0],[0]
Word embeddings are learned using a skip-gram model.,3.3 Convolutional Multiple-Slot Deep Memory Network,[0],[0]
The size of the word embedding is 20 since the vocabulary size in our dataset is small.,3.3 Convolutional Multiple-Slot Deep Memory Network,[0],[0]
The dropout is set to 0.4.,3.3 Convolutional Multiple-Slot Deep Memory Network,[0],[0]
We first presents the experimental settings and then report the results in this section.,4 Experiments and Evaluation,[0],[0]
"We conduct experiments on a simplified Chinese emotion cause corpus (Gui et al., 2016)∗, the only publicly available dataset on this task to the best of our knowledge.",4.1 Experimental Setup and Dataset,[0],[0]
"The corpus contains 2,105 documents from SINA city news†. Each document has only one emotion word and one or more emotion causes.",4.1 Experimental Setup and Dataset,[0],[0]
The documents are segmented into clauses manually.,4.1 Experimental Setup and Dataset,[0],[0]
"The main task is to identify which clause contains the emotion cause.
",4.1 Experimental Setup and Dataset,[0],[0]
Details of the corpus are shown in Table 1.,4.1 Experimental Setup and Dataset,[0],[0]
The metrics we used in evaluation follows Lee et al. (2010).,4.1 Experimental Setup and Dataset,[0],[0]
It is commonly accepted so that we can compare our results with others.,4.1 Experimental Setup and Dataset,[0],[0]
"If a proposed emotion cause clause covers the annotated answer, the word sequence is considered correct.",4.1 Experimental Setup and Dataset,[0],[0]
"The precision, recall, and F-measure are defined by ∗Available at: http://hlt.hitsz.edu.cn/?page id=694 †http://news.sina.com.cn/society/
P = ∑ correct causes 1∑
proposed causes 1 ,
",4.1 Experimental Setup and Dataset,[0],[0]
"R = ∑ correct causes 1∑
annotated causes 1 ,
F = 2× P ×R P +R .
",4.1 Experimental Setup and Dataset,[0],[0]
"In the experiments, we randomly select 90% of the dataset as training data and 10% as testing data.",4.1 Experimental Setup and Dataset,[0],[0]
"In order to obtain statistically credible results, we evaluate our method and baseline methods 25 times with different train/test splits.",4.1 Experimental Setup and Dataset,[0],[0]
"We compare with the following baseline methods:
• RB (Rule based method): The rule based method proposed in (Lee et al., 2010).
",4.2 Evaluation and Comparison,[0],[0]
"• CB (Common-sense based method): This is the knowledge based method proposed by (Russo et al., 2011).",4.2 Evaluation and Comparison,[0],[0]
"We use the Chinese Emotion Cognition Lexicon (Xu et al., 2013) as the common-sense knowledge base.",4.2 Evaluation and Comparison,[0],[0]
"The lexicon contains more than 5,000 kinds of emotion stimulation and their corresponding reflection words.
",4.2 Evaluation and Comparison,[0],[0]
• RB+CB+ML,4.2 Evaluation and Comparison,[0],[0]
(Machine learning method trained from rule-based features and facts from a common-sense knowledge base):,4.2 Evaluation and Comparison,[0],[0]
"This methods was previously proposed for emotion cause classification in (Chen et al., 2010).",4.2 Evaluation and Comparison,[0],[0]
It takes rules and facts in a knowledge base as features for classifier training.,4.2 Evaluation and Comparison,[0],[0]
"We train a SVM using features extracted from the rules defined in (Lee et al., 2010) and the Chinese Emotion Cognition Lexicon (Xu et al., 2013).
",4.2 Evaluation and Comparison,[0],[0]
• SVM:,4.2 Evaluation and Comparison,[0],[0]
"This is a SVM classifier using the unigram, bigram and trigram features.",4.2 Evaluation and Comparison,[0],[0]
"It is a baseline previously used in (Li and Xu, 2014; Gui et al., 2016)
•",4.2 Evaluation and Comparison,[0],[0]
Word2vec:,4.2 Evaluation and Comparison,[0],[0]
"This is a SVM classifier using word representations learned by Word2vec (Mikolov et al., 2013) as features.
",4.2 Evaluation and Comparison,[0],[0]
"• Multi-kernel: This is the state-of-the-art method using the multi-kernel method (Gui et al., 2016) to identify the emotion cause.",4.2 Evaluation and Comparison,[0],[0]
"We use the best performance reported in their paper.
",4.2 Evaluation and Comparison,[0],[0]
"• CNN: The convolutional neural network for sentence classification (Kim, 2014).
",4.2 Evaluation and Comparison,[0],[0]
• Memnet:,4.2 Evaluation and Comparison,[0],[0]
The deep memory network described in Section 3.2.,4.2 Evaluation and Comparison,[0],[0]
Word embeddings are pre-trained by skip-grams.,4.2 Evaluation and Comparison,[0],[0]
"The number of hops is set to 3.
",4.2 Evaluation and Comparison,[0],[0]
• ConvMS-Memnet: The convolutional multiple-slot deep memory network we proposed in Section 3.3.,4.2 Evaluation and Comparison,[0],[0]
Word embeddings are pre-trained by skip-grams.,4.2 Evaluation and Comparison,[0],[0]
"The number of hops is 3 in our experiments.
",4.2 Evaluation and Comparison,[0],[0]
Table 2 shows the evaluation results.,4.2 Evaluation and Comparison,[0],[0]
The rule based RB gives fairly high precision but with low recall.,4.2 Evaluation and Comparison,[0],[0]
"CB, the common-sense based method, achieves the highest recall.",4.2 Evaluation and Comparison,[0],[0]
"Yet, its precision is the worst.",4.2 Evaluation and Comparison,[0],[0]
"RB+CB,",4.2 Evaluation and Comparison,[0],[0]
"the combination of RB and CB gives higher the F-measure But, the improvement of 1.27% is only marginal compared to RB.
",4.2 Evaluation and Comparison,[0],[0]
"For machine learning methods, RB+CB+ML uses both rules and common-sense knowledge as features to train a machine learning classifier.",4.2 Evaluation and Comparison,[0],[0]
"It achieves F-measure of 0.5597, outperforming RB+CB.",4.2 Evaluation and Comparison,[0],[0]
Both SVM and word2vec are word feature based methods and they have similar performance.,4.2 Evaluation and Comparison,[0],[0]
"For word2vec, even though word representations are obtained from the SINA news raw corpus, it still performs worse than SVM trained using n-gram features only.",4.2 Evaluation and Comparison,[0],[0]
"The multi-kernel method (Gui et al., 2016) is the best performer
among the baselines because it considers context information in a structured way.",4.2 Evaluation and Comparison,[0],[0]
It models text by its syntactic tree and also considers an emotion lexicon.,4.2 Evaluation and Comparison,[0],[0]
"Their work shows that the structure information is important for the emotion cause extraction task.
",4.2 Evaluation and Comparison,[0],[0]
Naively applying the original deep memory network or convolutional network for emotion cause extraction outperforms all the baselines except the convolutional multi-kernel method.,4.2 Evaluation and Comparison,[0],[0]
"However, using our proposed ConvMS-Memnet architecture, we manage to boost the performance by 11.54% in precision, 4.84% in recall and 8.24% in Fmeasure respectively when compared to Memnet.",4.2 Evaluation and Comparison,[0],[0]
The improvement is very significant with p-value less than 0.01 in t-test.,4.2 Evaluation and Comparison,[0],[0]
"The ConvMS-Memnet also outperforms the previous best-performing method, multi-kernel, by 3.01% in F-measure.",4.2 Evaluation and Comparison,[0],[0]
"It shows that by effectively capturing context information, ConvMS-Memnet is able to identify the emotion cause better compared to other methods.",4.2 Evaluation and Comparison,[0],[0]
"To gain better insights into our proposed ConvMSMemnet, we conduct further experiments to understand the impact on performance by using: 1) pre-trained or randomly initialized word embedding; 2) multiple hops; 3) attention visualizations; 4) more training epochs.",4.3 More Insights into the ConvMS-Memnet,[0],[0]
"In our ConvMS-Memnet, we use pre-trained word embedding as the input.",4.3.1 Pre-trained Word Embeddings,[0],[0]
The embedding maps each word into a lower dimensional real-value vector as its representation.,4.3.1 Pre-trained Word Embeddings,[0],[0]
"Words sharing simi-
lar meanings should have similar representations.",4.3.1 Pre-trained Word Embeddings,[0],[0]
"It enables our model to deal with synonyms more effectively.
",4.3.1 Pre-trained Word Embeddings,[0],[0]
"The question is, “can we train the network without using pre-trained word embeddings?”.",4.3.1 Pre-trained Word Embeddings,[0],[0]
"We initialize word vectors randomly, and use an embedding matrix to update the word vectors in the training of the network simultaneously.",4.3.1 Pre-trained Word Embeddings,[0],[0]
Comparison results are shown in Table 3.,4.3.1 Pre-trained Word Embeddings,[0],[0]
It can be observed that pre-trained word embedding gives 2.59% higher F-measure compared to random initialization.,4.3.1 Pre-trained Word Embeddings,[0],[0]
This is partly due to the limited size of our training data.,4.3.1 Pre-trained Word Embeddings,[0],[0]
Hence using word embedding trained from other much larger corpus gives better results.,4.3.1 Pre-trained Word Embeddings,[0],[0]
It is widely acknowledged that computational models using deep architecture with multiple layers have better ability to learn data representations with multiple levels of abstractions.,4.3.2 Multiple Hops,[0],[0]
"In this section, we evaluate the power of multiple hops in this task.",4.3.2 Multiple Hops,[0],[0]
We set the number of hops from 1 to 9 with 1 standing for the simplest single layer network shown in Figure 4.,4.3.2 Multiple Hops,[0],[0]
"The more hops are stacked, the more complicated the model is.",4.3.2 Multiple Hops,[0],[0]
Results are shown in Table 4.,4.3.2 Multiple Hops,[0],[0]
The single layer network has achieved a competitive performance.,4.3.2 Multiple Hops,[0],[0]
"With the increasing number of hops, the performance improves.",4.3.2 Multiple Hops,[0],[0]
"However, when the number of hops is larger than 3, the performance decreases due to overfitting.",4.3.2 Multiple Hops,[0],[0]
"Since the dataset for this task is small, more parameters will lead to overfitting.",4.3.2 Multiple Hops,[0],[0]
"As such, we choose 3 hops in our final model since it gives the best performance in our experiments.",4.3.2 Multiple Hops,[0],[0]
"Essentially, memory network aims to measure the weight of each word in the clause with respect to the emotion word.",4.3.3 Word-Level Attention Weights,[0],[0]
"The question is, will the model really focus on the words which describe the emotion cause?",4.3.3 Word-Level Attention Weights,[0],[0]
"We choose one example to show the attention results in Table 5: Ex.2 家人/family 的/’s 坚持/insistence 更/more 让/makes人/people感动/touched
In this example, the cause of the emotion “touched” is “insistence”.",4.3.3 Word-Level Attention Weights,[0],[0]
We show in Table 5 the distribution of word-level attention weights in different hops of memory network training.,4.3.3 Word-Level Attention Weights,[0],[0]
"We can observe that in the first two hops, the highest attention weights centered on the word “more”.",4.3.3 Word-Level Attention Weights,[0],[0]
"However, from the third hop onwards, the highest atten-
tion weight moves to the word sub-sequence centred on the word “insistence”.",4.3.3 Word-Level Attention Weights,[0],[0]
This shows that our model is effective in identifying the most important keyword relating to the emotion cause.,4.3.3 Word-Level Attention Weights,[0],[0]
"Also, better results are obtained using deep memory network trained with at least 3 hops.",4.3.3 Word-Level Attention Weights,[0],[0]
"This is consistent with what we observed in Section 4.3.2.
",4.3.3 Word-Level Attention Weights,[0],[0]
"In order to evaluate the quality of keywords extracted by memory networks, we define a new metric on the keyword level of emotion cause extraction.",4.3.3 Word-Level Attention Weights,[0],[0]
The keyword is defined as the word which obtains the highest attention weight in the identified clause.,4.3.3 Word-Level Attention Weights,[0],[0]
"If the keywords extracted by our algorithm is located within the boundary of annotation, it is treated as correct.",4.3.3 Word-Level Attention Weights,[0],[0]
"Thus, we can obtain the precision, recall, and F-measure by comparing the proposed keywords with the correct keywords by:
P = ∑ correct keywords 1∑
proposed keywords 1 ,
R = ∑ correct keywords 1∑
annotated keywords 1 ,
F = 2× P ×R P +R .
",4.3.3 Word-Level Attention Weights,[0],[0]
"Since the reference methods do not focus on the keywords level, we only compare the performance of Memnet and ConvMS-Memnet in Table 6.",4.3.3 Word-Level Attention Weights,[0],[0]
It can be observed that our proposed ConvMS-Memnet outperforms Memnet by 5.6% in F-measure.,4.3.3 Word-Level Attention Weights,[0],[0]
"It shows that by capturing context features, ConvMS-Memnet is able to identify the word level emotion cause better compare to Memnet.",4.3.3 Word-Level Attention Weights,[0],[0]
"In our model, the training epochs are set to 20.",4.3.4 Training Epochs,[0],[0]
"In this section, we examine the testing error using a case study.",4.3.4 Training Epochs,[0],[0]
"Due to the page length limit, we only choose one example from the corpus.",4.3.4 Training Epochs,[0],[0]
The text below has four clauses: Ex.3 45天，对于失去儿子的他们是多么的漫 长，宝贝回家了，这个春节是多么幸福。,4.3.4 Training Epochs,[0],[0]
"Ex.3 45 days, it is long time for the parents who lost their baby.",4.3.4 Training Epochs,[0],[0]
"If the baby comes back home, they would become so happy in this Spring Festival.
",4.3.4 Training Epochs,[0],[0]
"In this example, the cause of emotion “happy” is described in the third clause.
",4.3.4 Training Epochs,[0],[0]
We show in Table 7 the probability of each clause containing an emotion cause in different training epochs.,4.3.4 Training Epochs,[0],[0]
It is interesting to see that our model is able to detect the correct clause with only 5 epochs.,4.3.4 Training Epochs,[0],[0]
"With the increasing number of training epochs, the probability associated with the correct clause increases further while the probabilities of incorrect clauses decrease generally.",4.3.4 Training Epochs,[0],[0]
We have shown in Section 4.3.4 a simple example consisting of only four clauses from which our model can identify the clause containing the emotion cause correctly.,4.4 Limitations,[0],[0]
"We notice that for some complex text passages which contain long distance dependency relations, negations or emotion transitions, our model may have a difficulty in detecting the correct clause containing the emotion causes.",4.4 Limitations,[0],[0]
It is a challenging task to properly model the discourse relations among clauses.,4.4 Limitations,[0],[0]
"In the future, we will explore different network architecture with consideration of various discourse relations possibly through transfer learning of larger annotated data available for other tasks.
",4.4 Limitations,[0],[0]
"Another shortcoming of our model is that, the answer generated from our model is simply “yes” or “no”.",4.4 Limitations,[0],[0]
The main reason is that the size of the annotated corpus is too small to train a model which can output natural language answers in full sentences.,4.4 Limitations,[0],[0]
"Ideally, we would like to develop a model which can directly give the cause of an emotion
expressed in text.",4.4 Limitations,[0],[0]
"However, since the manual annotation of data is too expensive for this task, we need to explore feasible ways to automatically collect annotate data for emotion cause detection.",4.4 Limitations,[0],[0]
We also need to study effective evaluation mechanisms for such QA systems.,4.4 Limitations,[0],[0]
"In this work, we treat emotion cause extraction as a QA task and propose a new model based on deep memory networks for identifying the emotion causes for an emotion expressed in text.",5 Conclusions,[0],[0]
The key property of this approach is the use of context information in the learning process which is ignored in the original memory network.,5 Conclusions,[0],[0]
Our new memory network architecture is able to store context in different memory slots to capture context information in proper sequence by convolutional operation.,5 Conclusions,[0],[0]
Our model achieves the state-of-the-art performance on a dataset for emotion cause detection when compared to a number of competitive baselines.,5 Conclusions,[0],[0]
"In the future, we will explore effective ways to model discourse relations among clauses and develop a QA system which can directly output the cause of emotions as answers.",5 Conclusions,[0],[0]
"This work was supported by the National Natural Science Foundation of China 61370165, U1636103, 61632011, 61528302, National 863 Program of China 2015AA015405, Shenzhen Foundational Research Funding JCYJ20150625142543470, JCYJ20170307150024907 and Guangdong Provincial Engineering Technology Research Center for Data Science 2016KF09.",Acknowledgments,[0],[0]
Emotion cause extraction aims to identify the reasons behind a certain emotion expressed in text.,abstractText,[0],[0]
It is a much more difficult task compared to emotion classification.,abstractText,[0],[0]
"Inspired by recent advances in using deep memory networks for question answering (QA), we propose a new approach which considers emotion cause identification as a reading comprehension task in QA.",abstractText,[0],[0]
"Inspired by convolutional neural networks, we propose a new mechanism to store relevant context in different memory slots to model context information.",abstractText,[0],[0]
Our proposed approach can extract both word level sequence features and lexical features.,abstractText,[0],[0]
"Performance evaluation shows that our method achieves the state-of-the-art performance on a recently released emotion cause dataset, outperforming a number of competitive baselines by at least 3.01% in F-measure.",abstractText,[0],[0]
A Question Answering Approach to Emotion Cause Extraction,title,[0],[0]
"Over the past few years, the media have paid considerable attention to machine learning systems and their ability to inadvertently discriminate against minorities, historically disadvantaged populations, and other protected groups when allocating resources (e.g., loans) or opportunities (e.g., jobs).",1. Introduction,[0],[0]
"In response to this scrutiny—and driven by ongoing debates and collaborations with lawyers, policy-makers, social scientists, and others (e.g., Barocas & Selbst, 2016)—machine learning researchers have begun to turn their attention to the topic of “fairness in machine learning,” and, in particular, to the design of fair classification and regression algorithms.
",1. Introduction,[0],[0]
"In this paper we study the task of binary classification subject to fairness constraints with respect to a pre-defined protected attribute, such as race or sex.",1. Introduction,[0],[0]
"Previous work in this area can be divided into two broad groups of approaches.
",1. Introduction,[0],[0]
"The first group of approaches incorporate specific quantitative definitions of fairness into existing machine learning
1Microsoft Research, New York 2Yahoo!",1. Introduction,[0],[0]
"Research, New York.",1. Introduction,[0],[0]
Correspondence to: A. Agarwal,1. Introduction,[0],[0]
<alekha@microsoft.com,1. Introduction,[0],[0]
">, A. Beygelzimer <beygel@gmail.com>, M. Dudı́k <mdudik@microsoft.com>, J. Langford <jcl@microsoft.com>, H. Wallach <wallach@microsoft.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
methods, often by relaxing the desired definitions of fairness, and only enforcing weaker constraints, such as lack of correlation (e.g., Woodworth et al., 2017; Zafar et al., 2017; Johnson et al., 2016; Kamishima et al., 2011; Donini et al., 2018).",1. Introduction,[0],[0]
"The resulting fairness guarantees typically only hold under strong distributional assumptions, and the approaches are tied to specific families of classifiers, such as SVMs.
",1. Introduction,[0],[0]
"The second group of approaches eliminate the restriction to specific classifier families and treat the underlying classification method as a “black box,” while implementing a wrapper that either works by pre-processing the data or post-processing the classifier’s predictions (e.g., Kamiran & Calders, 2012; Feldman et al., 2015; Hardt et al., 2016; Calmon et al., 2017).",1. Introduction,[0],[0]
"Existing pre-processing approaches are specific to particular definitions of fairness and typically seek to come up with a single transformed data set that will work across all learning algorithms, which, in practice, leads to classifiers that still exhibit substantial unfairness (see our evaluation in Section 4).",1. Introduction,[0],[0]
"In contrast, post-processing allows a wider range of fairness definitions and results in provable fairness guarantees.",1. Introduction,[0],[0]
"However, it is not guaranteed to find the most accurate fair classifier, and requires test-time access to the protected attribute, which might not be available.
",1. Introduction,[0],[0]
"We present a general-purpose approach that has the key advantage of this second group of approaches—i.e., the underlying classification method is treated as a black box—but without the noted disadvantages.",1. Introduction,[0],[0]
"Our approach encompasses a wide range of fairness definitions, is guaranteed to yield the most accurate fair classifier, and does not require test-time access to the protected attribute.",1. Introduction,[0],[0]
"Specifically, our approach allows any definition of fairness that can be formalized via linear inequalities on conditional moments, such as demographic parity or equalized odds (see Section 2.1).",1. Introduction,[0],[0]
We show how binary classification subject to these constraints can be reduced to a sequence of cost-sensitive classification problems.,1. Introduction,[0],[0]
"We require only black-box access to a cost-sensitive classification algorithm, which does not need to have any knowledge of the desired definition of fairness or protected attribute.",1. Introduction,[0],[0]
"We show that the solutions to our sequence of cost-sensitive classification problems yield a randomized classifier with the lowest (empirical) error subject to the desired fairness constraints.
",1. Introduction,[0],[0]
"Corbett-Davies et al. (2017) and Menon & Williamson
(2018) begin with a similar goal to ours, but they analyze the Bayes optimal classifier under fairness constraints in the limit of infinite data.",1. Introduction,[0],[0]
"In contrast, our focus is algorithmic, our approach applies to any classifier family, and we obtain finite-sample guarantees.",1. Introduction,[0],[0]
Dwork et al. (2018) also begin with a similar goal to ours.,1. Introduction,[0],[0]
Their approach partitions the training examples into subsets according to protected attribute values and then leverages transfer learning to jointly learn from these separate data sets.,1. Introduction,[0],[0]
"Our approach avoids partitioning the data and assumes access only to a classification algorithm rather than a transfer learning algorithm.
",1. Introduction,[0],[0]
"A preliminary version of this paper appeared at the FAT/ML workshop (Agarwal et al., 2017), and led to extensions with more general optimization objectives (Alabi et al., 2018) and combinatorial protected attributes (Kearns et al., 2018).
",1. Introduction,[0],[0]
"In the next section, we formalize our problem.",1. Introduction,[0],[0]
"While we focus on two well-known quantitative definitions of fairness, our approach also encompasses many other previously studied definitions of fairness as special cases.",1. Introduction,[0],[0]
"In Section 3, we describe our reductions approach to fair classification and its guarantees in detail.",1. Introduction,[0],[0]
"The experimental study in Section 4 shows that our reductions compare favorably to three baselines, while overcoming some of their disadvantages and also offering the flexibility of picking a suitable accuracy– fairness tradeoff.",1. Introduction,[0],[0]
Our results demonstrate the utility of having a general-purpose approach for combining machine learning methods and quantitative fairness definitions.,1. Introduction,[0],[0]
"We consider a binary classification setting where the training examples consist of triples (X,A, Y ), whereX ∈ X is a feature vector,",2. Problem Formulation,[0],[0]
"A ∈ A is a protected attribute, and Y ∈ {0, 1} is a label.",2. Problem Formulation,[0],[0]
The feature vector X can either contain the protected attribute A as one of the features or contain other features that are arbitrarily indicative of A.,2. Problem Formulation,[0],[0]
"For example, if the classification task is to predict whether or not someone will default on a loan, each training example might correspond to a person, whereX represents their demographics, income level, past payment history, and loan amount; A represents their race; and Y represents whether or not they defaulted on that loan.",2. Problem Formulation,[0],[0]
"Note that X might contain their race as one of the features or, for example, contain their zipcode—a feature that is often correlated with race.",2. Problem Formulation,[0],[0]
"Our goal is to learn an accurate classifier h : X→ {0, 1} from some set (i.e., family) of classifiers H, such as linear threshold rules, decision trees, or neural nets, while satisfying some definition of fairness.",2. Problem Formulation,[0],[0]
Note that the classifiers in H do not explicitly depend on A.,2. Problem Formulation,[0],[0]
"We focus on two well-known quantitative definitions of fairness that have been considered in previous work on
fair classification; however, our approach also encompasses many other previously studied definitions of fairness as special cases, as we explain at the end of this section.
",2.1. Fairness Definitions,[0],[0]
"The first definition—demographic (or statistical) parity— can be thought of as a stronger version of the US Equal Employment Opportunity Commission’s “four-fifths rule,” which requires that the “selection rate for any race, sex, or ethnic group [must be at least] four-fifths (4/5) (or eighty percent) of the rate for the group with the highest rate.”1
Definition 1 (Demographic parity—DP).",2.1. Fairness Definitions,[0],[0]
"A classifier h satisfies demographic parity under a distribution over (X,A, Y ) if its prediction h(X) is statistically independent of the protected attribute A—that is, if P[h(X) = ŷ",2.1. Fairness Definitions,[0],[0]
"| A = a] = P[h(X) = ŷ] for all a, ŷ.",2.1. Fairness Definitions,[0],[0]
"Because ŷ ∈ {0, 1}, this is equivalent to E[h(X) |A",2.1. Fairness Definitions,[0],[0]
"= a] = E[h(X)] for all a.
The second definition—equalized odds—was recently proposed by Hardt et al. (2016) to remedy two previously noted flaws with demographic parity (Dwork et al., 2012).",2.1. Fairness Definitions,[0],[0]
"First, demographic parity permits a classifier which accurately classifies data points with one value A = a, such as the value a with the most data, but makes random predictions for data points with A 6= a as long as the probabilities of h(X) = 1 match.",2.1. Fairness Definitions,[0],[0]
"Second, demographic parity rules out perfect classifiers whenever Y is correlated with A.",2.1. Fairness Definitions,[0],[0]
"In contrast, equalized odds suffers from neither of these flaws.
",2.1. Fairness Definitions,[0],[0]
Definition 2 (Equalized odds—EO).,2.1. Fairness Definitions,[0],[0]
"A classifier h satisfies equalized odds under a distribution over (X,A, Y ) if its prediction h(X) is conditionally independent of the protected attribute A given the label Y—that is, if P[h(X) = ŷ",2.1. Fairness Definitions,[0],[0]
"| A = a, Y = y] = P[h(X) = ŷ",2.1. Fairness Definitions,[0],[0]
"| Y = y] for all a, y, and ŷ. Because ŷ ∈ {0, 1}, this is equivalent to E[h(X) |A",2.1. Fairness Definitions,[0],[0]
"= a, Y = y] = E[h(X)",2.1. Fairness Definitions,[0],[0]
"| Y = y] for all a, y.
We now show how each definition can be viewed as a special case of a general set of linear constraints of the form
Mµ(h) ≤",2.1. Fairness Definitions,[0],[0]
"c, (1)
where matrix M ∈ R|K|×|J| and vector c ∈",2.1. Fairness Definitions,[0],[0]
"R|K| describe the linear constraints, each indexed by k ∈ K, and µ(h) ∈ R|J| is a vector of conditional moments of the form
µj(h) = E [ gj(X,A, Y, h(X)) ∣∣",2.1. Fairness Definitions,[0],[0]
"Ej ] for j ∈ J, where gj : X×A× {0, 1} × {0, 1} → [0, 1] and Ej is an event defined with respect to (X,A, Y ).",2.1. Fairness Definitions,[0],[0]
"Crucially, gj depends on h, while Ej cannot depend on h in any way.
",2.1. Fairness Definitions,[0],[0]
Example 1 (DP).,2.1. Fairness Definitions,[0],[0]
"In a binary classification setting, demographic parity can be expressed as a set of |A| equality constraints, each of the form E[h(X) |A = a] = E[h(X)].",2.1. Fairness Definitions,[0],[0]
"Letting J = A ∪ {?}, gj(X,A, Y, h(X))",2.1. Fairness Definitions,[0],[0]
"= h(X) for all j,
1See the Uniform Guidelines on Employment Selection Procedures, 29 C.F.R. §1607.4(D) (2015).
",2.1. Fairness Definitions,[0],[0]
"Ea = {A = a}, and E? = {True}, where {True} refers to the event encompassing all points in the sample space, each equality constraint can be expressed as µa(h) = µ?(h).2 Finally, because each such constraint can be equivalently expressed as a pair of inequality constraints of the form
µa(h)− µ?(h) ≤ 0 −µa(h) + µ?(h)",2.1. Fairness Definitions,[0],[0]
"≤ 0,
demographic parity can be expressed as equation (1), where K = A×{+,−},M(a,+),a′ =",2.1. Fairness Definitions,[0],[0]
"1{a′ = a},M(a,+),?",2.1. Fairness Definitions,[0],[0]
"= −1, M(a,−),a′ = −1{a′ = a}, M(a,−),?",2.1. Fairness Definitions,[0],[0]
"= 1, and c = 0.",2.1. Fairness Definitions,[0],[0]
Expressing each equality constraint as a pair of inequality constraints allows us to control the extent to which each constraint is enforced by positing ck > 0,2.1. Fairness Definitions,[0],[0]
"for some (or all) k.
Example 2 (EO).",2.1. Fairness Definitions,[0],[0]
"In a binary classification setting, equalized odds can be expressed as a set of 2 |A| equality constraints, each of the form E[h(X) |",2.1. Fairness Definitions,[0],[0]
"A = a, Y = y] = E[h(X) | Y = y].",2.1. Fairness Definitions,[0],[0]
Letting J =,2.1. Fairness Definitions,[0],[0]
(A ∪ {?}),2.1. Fairness Definitions,[0],[0]
"× {0, 1}, gj(X,A, Y, h(X))",2.1. Fairness Definitions,[0],[0]
"= h(X) for all j, E(a,y) =",2.1. Fairness Definitions,[0],[0]
"{A = a, Y = y}, and E(?,y) = {Y = y}, each equality constraint can be equivalently expressed as
µ(a,y)(h)− µ(?,y)(h)",2.1. Fairness Definitions,[0],[0]
≤ 0,2.1. Fairness Definitions,[0],[0]
"−µ(a,y)(h)",2.1. Fairness Definitions,[0],[0]
"+ µ(?,y)(h)",2.1. Fairness Definitions,[0],[0]
"≤ 0.
",2.1. Fairness Definitions,[0],[0]
"As a result, equalized odds can be expressed as equation (1), where K = A× Y× {+,−}, M(a,y,+),(a′,y′) = 1{a′= a, y′= y}, M(a,y,+),(?,y′) = −1, M(a,y,−),(a′,y′) = −1{a′= a, y′= y}, M(a,y,−),(?,y′) = 1, and c = 0.",2.1. Fairness Definitions,[0],[0]
"Again, we can posit ck > 0",2.1. Fairness Definitions,[0],[0]
"for some (or all) k to allow small violations of some (or all) of the constraints.
",2.1. Fairness Definitions,[0],[0]
"Although we omit the details, we note that many other previously studied definitions of fairness can also be expressed as equation (1).",2.1. Fairness Definitions,[0],[0]
"For example, equality of opportunity (Hardt et al., 2016) (also known as balance for the positive class; Kleinberg et al., 2017), balance for the negative class (Kleinberg et al., 2017), error-rate balance (Chouldechova, 2017), overall accuracy equality (Berk et al., 2017), and treatment equality (Berk et al., 2017) can all be expressed as equation (1); in contrast, calibration (Kleinberg et al., 2017) and predictive parity (Chouldechova, 2017) cannot because to do so would require the event Ej to depend on h.",2.1. Fairness Definitions,[0],[0]
"We note that our approach can also be used to satisfy multiple definitions of fairness, though if these definitions are mutually contradictory, e.g., as described by Kleinberg et al. (2017), then our guarantees become vacuous.",2.1. Fairness Definitions,[0],[0]
"In a standard (binary) classification setting, the goal is to learn the classifier h ∈ H with the minimum classification
2Note that µ?(h) = E[h(X) |",2.2. Fair Classification,[0],[0]
"True] = E[h(X)].
error: err(h) := P[h(X) 6=",2.2. Fair Classification,[0],[0]
Y ].,2.2. Fair Classification,[0],[0]
"However, because our goal is to learn the most accurate classifier while satisfying fairness constraints, as formalized above, we instead seek to find the solution to the constrained optimization problem3
min h∈H
err(h) subject to Mµ(h) ≤ c. (2)
",2.2. Fair Classification,[0],[0]
"Furthermore, rather than just considering classifiers in the set H, we can enlarge the space of possible classifiers by considering randomized classifiers that can be obtained via a distribution over H. By considering randomized classifiers, we can achieve better accuracy–fairness tradeoffs than would otherwise be possible.",2.2. Fair Classification,[0],[0]
A randomized classifier Q makes a prediction by first sampling a classifier h ∈ H from Q and then using h to make the prediction.,2.2. Fair Classification,[0],[0]
"The resulting classification error is err(Q) = ∑ h∈HQ(h) err(h) and
the conditional moments are µ(Q) = ∑ h∈HQ(h)µ(h) (see Appendix A for the derivation).",2.2. Fair Classification,[0],[0]
"Thus we seek to solve
min Q∈∆
err(Q) subject to Mµ(Q) ≤",2.2. Fair Classification,[0],[0]
"c, (3)
where ∆ is the set of all distributions over H.
In practice, we do not know the true distribution over (X,A, Y ) and only have access to a data set of training examples {(Xi, Ai, Yi)}ni=1.",2.2. Fair Classification,[0],[0]
We therefore replace err(Q) and µ(Q) in equation (3) with their empirical versions êrr(Q) and µ̂(Q).,2.2. Fair Classification,[0],[0]
"Because of the sampling error in µ̂(Q), we also allow errors in satisfying the constraints by setting ĉk = ck",2.2. Fair Classification,[0],[0]
"+ εk for all k, where εk ≥ 0.",2.2. Fair Classification,[0],[0]
"After these modifications, we need to solve the empirical version of equation (3):
min Q∈∆
êrr(Q) subject to Mµ̂(Q) ≤ ĉ. (4)",2.2. Fair Classification,[0],[0]
We now show how the problem (4) can be reduced to a sequence of cost-sensitive classification problems.,3. Reductions Approach,[0],[0]
We further show that the solutions to our sequence of cost-sensitive classification problems yield a randomized classifier with the lowest (empirical) error subject to the desired constraints.,3. Reductions Approach,[0],[0]
"We assume access to a cost-sensitive classification algorithm for the set H. The input to such an algorithm is a data set of training examples {(Xi, C0i , C1i )}ni=1, where C0i and C1i denote the losses—costs in this setting—for predicting the labels 0 or 1, respectively, for Xi.",3.1. Cost-sensitive Classification,[0],[0]
"The algorithm outputs
arg min h∈H n∑ i=1",3.1. Cost-sensitive Classification,[0],[0]
h(Xi)C 1,3.1. Cost-sensitive Classification,[0],[0]
i + (1− h(Xi))C0i .,3.1. Cost-sensitive Classification,[0],[0]
"(5)
3We consider misclassification error for concreteness, but all the results in this paper apply to any error of the form err(h)",3.1. Cost-sensitive Classification,[0],[0]
"= E[gerr(X,A, Y, h(X))",3.1. Cost-sensitive Classification,[0],[0]
"],",3.1. Cost-sensitive Classification,[0],[0]
"where gerr(·, ·, ·, ·) ∈",3.1. Cost-sensitive Classification,[0],[0]
"[0, 1].
",3.1. Cost-sensitive Classification,[0],[0]
"This abstraction allows us to specify different costs for different training examples, which is essential for incorporating fairness constraints.",3.1. Cost-sensitive Classification,[0],[0]
"Moreover, efficient cost-sensitive classification algorithms are readily available for several common classifier representations (e.g., Beygelzimer et al., 2005; Langford & Beygelzimer, 2005; Fan et al., 1999).",3.1. Cost-sensitive Classification,[0],[0]
"In particular, equation (5) is equivalent to a weighted classification problem, where the input consists of labeled examples {(Xi, Yi,Wi)}ni=1 with Yi ∈ {0, 1} and Wi ≥ 0, and the goal is to minimize the weighted classification error ∑n i=1Wi 1{h(Xi) 6=",3.1. Cost-sensitive Classification,[0],[0]
Yi}.,3.1. Cost-sensitive Classification,[0],[0]
This is equivalent to equation (5) if we set Wi = |C0i − C1i | and Yi = 1{C0i ≥,3.1. Cost-sensitive Classification,[0],[0]
C1i }.,3.1. Cost-sensitive Classification,[0],[0]
"To derive our fair classification algorithm, we rewrite equation (4) as a saddle point problem.",3.2. Reduction,[0],[0]
"We begin by introducing a Lagrange multiplier λk ≥ 0 for each of the |K| constraints, summarized as λ ∈ R|K|+ , and form the Lagrangian
L(Q,λ) =",3.2. Reduction,[0],[0]
êrr(Q),3.2. Reduction,[0],[0]
"+ λ> ( Mµ̂(Q)− ĉ ) .
",3.2. Reduction,[0],[0]
"Thus, equation (4) is equivalent to
min Q∈∆ max λ∈R|K|+ L(Q,λ).",3.2. Reduction,[0],[0]
"(6)
For computational and statistical reasons, we impose an additional constraint on the `1 norm of λ and seek to simultaneously find the solution to the constrained version of (6) as well as its dual, obtained by switching min and max:
min Q∈∆ max λ∈R|K|+ , ‖λ‖1≤B L(Q,λ),",3.2. Reduction,[0],[0]
"(P)
max λ∈R|K|+ , ‖λ‖1≤B min Q∈∆ L(Q,λ).",3.2. Reduction,[0],[0]
"(D)
Because L is linear in Q and λ and the domains of Q and λ are convex and compact, both problems have solutions (which we denote by Q† and λ†) and the minimum value of (P) and the maximum value of (D) are equal and coincide with L(Q†,λ†).",3.2. Reduction,[0],[0]
"Thus, (Q†,λ†) is the saddle point of L (Corollary 37.6.2 and Lemma 36.2 of Rockafellar, 1970).
",3.2. Reduction,[0],[0]
"We find the saddle point by using the standard scheme of Freund & Schapire (1996), developed for the equivalent problem of solving for an equilibrium in a zero-sum game.",3.2. Reduction,[0],[0]
"From game-theoretic perspective, the saddle point can be viewed as an equilibrium of a game between two players: the Q-player choosing Q and the λ-player choosing λ.",3.2. Reduction,[0],[0]
"The LagrangianL(Q,λ) specifies how much theQ-player has to pay to the λ-player after they make their choices.",3.2. Reduction,[0],[0]
"At the saddle point, neither player wants to deviate from their choice.
",3.2. Reduction,[0],[0]
"Our algorithm finds an approximate equilibrium in which neither player can gain more than ν by changing their choice
Algorithm 1 Exp. gradient reduction for fair classification Input: training examples {(Xi, Yi, Ai)}ni=1
fairness constraints specified by gj , Ej , M, ĉ bound B, accuracy ν, learning rate η
Set θ1 = 0 ∈",3.2. Reduction,[0],[0]
"R|K| for t = 1, 2, . . .",3.2. Reduction,[0],[0]
"do
Set λt,k = B exp{θk} 1+ ∑ k′∈K exp{θk′} for all k ∈ K ht ← BESTh(λt) Q̂t ← 1t ∑t t′=1 ht′ ,",3.2. Reduction,[0],[0]
"L← L ( Q̂t,BESTλ(Q̂t)
) λ̂t ← 1t ∑t t′=1 λt′ , L← L ( BESTh(λ̂t), λ̂t
) νt",3.2. Reduction,[0],[0]
"← max { L(Q̂t, λ̂t)− L, L− L(Q̂t, λ̂t)
}",3.2. Reduction,[0],[0]
"if νt ≤ ν then
Return (Q̂t, λ̂t) end if Set θt+1 = θt + η",3.2. Reduction,[0],[0]
"(Mµ̂(ht)− ĉ)
end for
(where ν > 0 is an input to the algorithm).",3.2. Reduction,[0],[0]
"Such an approximate equilibrium corresponds to a ν-approximate saddle point of the Lagrangian, which is a pair (Q̂, λ̂), where
L(Q̂, λ̂) ≤",3.2. Reduction,[0],[0]
"L(Q, λ̂)",3.2. Reduction,[0],[0]
"+ ν for all Q ∈ ∆,
L(Q̂, λ̂) ≥ L(Q̂,λ)− ν for all λ ∈ R|K|+ , ‖λ‖1 ≤",3.2. Reduction,[0],[0]
"B.
We proceed iteratively by running a no-regret algorithm for the λ-player, while executing the best response of the Qplayer.",3.2. Reduction,[0],[0]
"Following Freund & Schapire (1996), the average play of both players converges to the saddle point.",3.2. Reduction,[0],[0]
"We run the exponentiated gradient algorithm (Kivinen & Warmuth, 1997) for the λ-player and terminate as soon as the suboptimality of the average play falls below the pre-specified accuracy ν.",3.2. Reduction,[0],[0]
"The best response of the Q-player can always be chosen to put all of the mass on one of the candidate classifiers h ∈ H, and can be implemented by a single call to a cost-sensitive classification algorithm for the set H.
Algorithm 1 fully implements this scheme, except for the functions BESTλ and BESTh, which correspond to the bestresponse algorithms of the two players.",3.2. Reduction,[0],[0]
(We need the best response of the λ-player to evaluate whether the suboptimality of the current average play has fallen below ν.),3.2. Reduction,[0],[0]
"The two best response functions can be calculated as follows.
BESTλ(Q): the best response of the λ-player.",3.2. Reduction,[0],[0]
"The best response of the λ-player for a given Q is any maximizer of L(Q,λ) over all valid λs.",3.2. Reduction,[0],[0]
"In our setting, it can always be chosen to be either 0 or put all of the mass on the most violated constraint.",3.2. Reduction,[0],[0]
Letting γ̂(Q),3.2. Reduction,[0],[0]
":= Mµ̂(Q) and letting ek denote the kth vector of the standard basis, BESTλ(Q) returns{
0 if γ̂(Q) ≤ ĉ, Bek∗ otherwise, where k∗ = arg maxk[γ̂k(Q)− ĉk].
BESTh(λ): the best response of theQ-player.",3.2. Reduction,[0],[0]
"Here, the best response minimizes L(Q,λ) over all Qs in the simplex.",3.2. Reduction,[0],[0]
"Because L is linear in Q, the minimizer can always be chosen to put all of the mass on a single classifier h.",3.2. Reduction,[0],[0]
We show how to obtain the classifier constituting the best response via a reduction to cost-sensitive classification.,3.2. Reduction,[0],[0]
"Letting pj := P̂[Ej ] be the empirical event probabilities, the Lagrangian for Q which puts all of the mass on a single h is then
L(h,λ) = êrr(h)",3.2. Reduction,[0],[0]
+ λ> ( Mµ̂(h)− ĉ ),3.2. Reduction,[0],[0]
= Ê [ 1{h(X) 6=,3.2. Reduction,[0],[0]
Y } ],3.2. Reduction,[0],[0]
"− λ>ĉ +
∑ k,j Mk,jλkµ̂j(h)
= −λ>ĉ + Ê [ 1{h(X) 6=",3.2. Reduction,[0],[0]
"Y } ] + ∑ k,j Mk,jλk pj Ê [ gj ( X,A,Y,h(X) )",3.2. Reduction,[0],[0]
"1{(X,A,Y ) ∈",3.2. Reduction,[0],[0]
"Ej} ] .
",3.2. Reduction,[0],[0]
"Assuming a data set of training examples {(Xi, Ai, Yi)}ni=1, the minimization of L(h,λ) over h then corresponds to costsensitive classification on {(Xi, C0i , C1i )}ni=1 with costs4
C0i = 1{Yi 6= 0} + ∑ k,j Mk,jλk pj gj(Xi,Ai,Yi, 0)1{(Xi,Ai,Yi) ∈",3.2. Reduction,[0],[0]
"Ej}
C1i = 1{Yi 6= 1} + ∑ k,j Mk,jλk pj gj(Xi,Ai,Yi, 1)1{(Xi,Ai,Yi) ∈",3.2. Reduction,[0],[0]
"Ej}.
",3.2. Reduction,[0],[0]
Theorem 1.,3.2. Reduction,[0],[0]
"Letting ρ := maxh‖Mµ̂(h) − ĉ‖∞, Algorithm 1 satisfies the inequality
νt ≤ B log(|K|+ 1)
ηt + ηρ2B.
Thus, for η = ν2ρ2B , Algorithm 1 will return a ν-approximate saddle point of L in at most 4ρ 2B2 log(|K|+1)
ν2 iterations.
",3.2. Reduction,[0],[0]
"This theorem, proved in Appendix B, bounds the suboptimality νt of the average play (Q̂t, λ̂t), which is equal to its suboptimality as a saddle point.",3.2. Reduction,[0],[0]
The right-hand side of the bound is optimized by η = √ log(|K|+ 1) /,3.2. Reduction,[0],[0]
"(ρ √ t), lead-
ing to the bound νt ≤ 2ρB √
log(|K|+ 1) /",3.2. Reduction,[0],[0]
t.,3.2. Reduction,[0],[0]
This bound decreases with the number of iterations t and grows very slowly with the number of constraints |K|.,3.2. Reduction,[0],[0]
The quantity ρ is a problem-specific constant that bounds how much any single classifier h ∈ H can violate the desired set of fairness constraints.,3.2. Reduction,[0],[0]
"Finally, B is the bound on the `1-norm of λ, which we introduced to enable this specific algorithmic scheme.",3.2. Reduction,[0],[0]
"In general, larger values of B will bring the problem (P) closer to (6), and thus also to (4), but at the cost of
4For general error, err(h) =",3.2. Reduction,[0],[0]
"E[gerr(X,A, Y, h(X))",3.2. Reduction,[0],[0]
"], the costs C0i and C 1 i contain, respectively, the terms gerr(Xi, Ai, Yi, 0) and gerr(Xi, Ai, Yi, 1) instead of 1{Yi 6= 0} and 1{Yi 6= 1}.
needing more iterations to reach any given suboptimality.",3.2. Reduction,[0],[0]
"In particular, as we derive in the theorem, achieving suboptimality ν may need up to 4ρ2B2 log(|K|+ 1) / ν2 iterations.",3.2. Reduction,[0],[0]
Example 3 (DP).,3.2. Reduction,[0],[0]
"Using the matrix M for demographic parity as described in Section 2, the cost-sensitive reduction for a vector of Lagrange multipliers λ uses costs
C0i = 1{Yi 6= 0}, C1i = 1{Yi 6=",3.2. Reduction,[0],[0]
"1}+ λAi pAi − ∑ a∈A λa,
where pa := P̂[A = a] and λa := λ(a,+)",3.2. Reduction,[0],[0]
"− λ(a,−), effectively replacing two non-negative Lagrange multipliers by a single multiplier, which can be either positive or negative.",3.2. Reduction,[0],[0]
"Because ck = 0 for all k, ĉk = εk.",3.2. Reduction,[0],[0]
"Furthermore, because all empirical moments are bounded in [0, 1], we can assume εk ≤ 1, which yields the bound ρ ≤ 2.",3.2. Reduction,[0],[0]
"Thus, Algorithm 1 terminates in at most 16B2 log(2 |A|+ 1) / ν2 iterations.",3.2. Reduction,[0],[0]
Example 4 (EO).,3.2. Reduction,[0],[0]
"For equalized odds, the cost-sensitive reduction for a vector of Lagrange multipliers λ uses costs
C0i = 1{Yi 6= 0},
C1i = 1{Yi 6=",3.2. Reduction,[0],[0]
"1}+ λ(Ai,Yi) p(Ai,Yi)",3.2. Reduction,[0],[0]
"− ∑ a∈A λ(a,Yi) p(?,Yi) ,
where p(a,y) := P̂[A = a, Y = y], p(?,y) := P̂[Y = y], and λ(a,y) := λ(a,y,+)",3.2. Reduction,[0],[0]
"− λ(a,y,−).",3.2. Reduction,[0],[0]
"If we again assume εk ≤ 1, then we obtain the bound ρ ≤ 2.",3.2. Reduction,[0],[0]
"Thus, Algorithm 1 terminates in at most 16B2 log(4 |A|+ 1) / ν2 iterations.",3.2. Reduction,[0],[0]
"Our ultimate goal, as formalized in equation (3), is to minimize the classification error while satisfying fairness constraints under a true but unknown distribution over (X,A, Y ).",3.3. Error Analysis,[0],[0]
"In the process of deriving Algorithm 1, we introduced three different sources of error.",3.3. Error Analysis,[0],[0]
"First, we replaced the true classification error and true moments with their empirical versions.",3.3. Error Analysis,[0],[0]
"Second, we introduced a bound B on the magnitude of λ.",3.3. Error Analysis,[0],[0]
"Finally, we only run the optimization algorithm for a fixed number of iterations, until it reaches suboptimality level ν.",3.3. Error Analysis,[0],[0]
"The first source of error, due to the use of empirical rather than true quantities, is unavoidable and constitutes the underlying statistical error.",3.3. Error Analysis,[0],[0]
"The other two sources of error, the bound B and the suboptimality level ν, stem from the optimization algorithm and can be driven arbitrarily small at the cost of additional iterations.",3.3. Error Analysis,[0],[0]
"In this section, we show how the statistical error and the optimization error affect the true accuracy and the fairness of the randomized classifier returned by Algorithm 1—in other words, how well Algorithm 1 solves our original problem (3).
",3.3. Error Analysis,[0],[0]
"To bound the statistical error, we use the Rademacher complexity of the classifier family H, which we denote by Rn(H), where n is the number of training examples.",3.3. Error Analysis,[0],[0]
"We assume that Rn(H) ≤ Cn−α for some C ≥ 0 and
α ≤ 1/2.",3.3. Error Analysis,[0],[0]
"We note that α = 1/2 in the vast majority of classifier families, including norm-bounded linear functions (see Theorem 1 of Kakade et al., 2009), neural networks (see Theorem 18 of Bartlett & Mendelson, 2002), and classifier families with bounded VC dimension (see Lemma 4 and Theorem 6 of Bartlett & Mendelson, 2002).
",3.3. Error Analysis,[0],[0]
"Recall that in our empirical optimization problem we assume that ĉk = ck + εk, where εk ≥ 0 are error bounds that account for the discrepancy between µ(Q) and µ̂(Q).",3.3. Error Analysis,[0],[0]
"In our analysis, we assume that these error bounds have been set in accordance with the Rademacher complexity of H.
Assumption 1.",3.3. Error Analysis,[0],[0]
"There exists C,C ′",3.3. Error Analysis,[0],[0]
≥ 0 and α ≤ 1/2 such that Rn(H) ≤,3.3. Error Analysis,[0],[0]
Cn−α and εk = C ′,3.3. Error Analysis,[0],[0]
∑,3.3. Error Analysis,[0],[0]
"j∈J|Mk,",3.3. Error Analysis,[0],[0]
"j |n −α j , where nj is the number of data points that fall in Ej ,
nj := ∣∣{i : (Xi, Ai, Yi) ∈",3.3. Error Analysis,[0],[0]
"Ej}∣∣.
",3.3. Error Analysis,[0],[0]
The optimization error can be bounded via a careful analysis of the Lagrangian and the optimality conditions of (P) and (D).,3.3. Error Analysis,[0],[0]
"Combining the three different sources of error yields the following bound, which we prove in Appendix C.
Theorem 2.",3.3. Error Analysis,[0],[0]
Let Assumption 1 hold for C ′,3.3. Error Analysis,[0],[0]
"≥ 2C + 2 + √ ln(4/δ) / 2, where δ > 0.",3.3. Error Analysis,[0],[0]
"Let (Q̂, λ̂) be any νapproximate saddle point of L, let Q? minimize err(Q) subject to Mµ(Q) ≤ c, and let p?j = P[Ej ].",3.3. Error Analysis,[0],[0]
"Then, with probability at least 1− (|J|+ 1)δ, the distribution Q̂ satisfies
err(Q̂) ≤ err(Q?)",3.3. Error Analysis,[0],[0]
"+ 2ν + Õ(n−α),
γk(Q̂) ≤ ck + 1+2ν B + ∑ j∈J |Mk,j | Õ(n−αj ) for all k,
where Õ(·) suppresses polynomial dependence on ln(1/δ).",3.3. Error Analysis,[0],[0]
"If np?j ≥ 8 log(2/δ) for all j, then, for all k,
γk(Q̂)",3.3. Error Analysis,[0],[0]
≤,3.3. Error Analysis,[0],[0]
ck,3.3. Error Analysis,[0],[0]
"+ 1+2ν B + ∑ j∈J |Mk,j | Õ",3.3. Error Analysis,[0],[0]
"( (np?j ) −α ) .
",3.3. Error Analysis,[0],[0]
"In other words, the solution returned by Algorithm 1 achieves the lowest feasible classification error on the true distribution up to the optimization error, which grows linearly with ν, and the statistical error, which grows as n−α.",3.3. Error Analysis,[0],[0]
"Therefore, if we want to guarantee that the optimization error does not dominate the statistical error, we should set ν ∝",3.3. Error Analysis,[0],[0]
n−α.,3.3. Error Analysis,[0],[0]
The fairness constraints on the true distribution are satisfied up to the optimization error (1 + 2ν) /B,3.3. Error Analysis,[0],[0]
and up to the statistical error.,3.3. Error Analysis,[0],[0]
"Because the statistical error depends on the moments, and the error in estimating the moments grows as n−αj ≥ n−α, we can setB ∝ nα to guarantee that the optimization error does not dominate the statistical error.",3.3. Error Analysis,[0],[0]
"Combining this reasoning with the learning rate setting of Theorem 1 yields the following theorem (proved in Appendix C).
",3.3. Error Analysis,[0],[0]
Theorem 3.,3.3. Error Analysis,[0],[0]
Let ρ := maxh‖Mµ̂(h)− ĉ‖∞. Let Assumption 1 hold for C ′,3.3. Error Analysis,[0],[0]
"≥ 2C + 2 + √ ln(4/δ) / 2, where δ > 0.
",3.3. Error Analysis,[0],[0]
Let Q? minimize err(Q) subject to Mµ(Q) ≤,3.3. Error Analysis,[0],[0]
c.,3.3. Error Analysis,[0],[0]
Then Algorithm 1 with ν ∝,3.3. Error Analysis,[0],[0]
"n−α, B ∝ nα and η ∝ ρ−2n−2α terminates in O(ρ2n4α ln |K|) iterations and returns Q̂, which with probability at least 1− (|J|+ 1)δ satisfies
err(Q̂) ≤ err(Q?) + Õ(n−α), γk(Q̂) ≤",3.3. Error Analysis,[0],[0]
ck,3.3. Error Analysis,[0],[0]
"+ ∑ j∈J |Mk,j | Õ(n−αj ) for all k.
Example 5 (DP).",3.3. Error Analysis,[0],[0]
"If na denotes the number of training examples with Ai = a, then Assumption 1 states that we should set ε(a,+) = ε(a,−) = C ′(n−αa + n
−α) and Theorem 3 then shows that for a suitable setting of C ′, ν, B, and η, Algorithm 1 will return a randomized classifier Q̂ with the lowest feasible classification error up to Õ(n−α) while also approximately satisfying the fairness constraints∣∣∣E[h(X) |A = a]− E[h(X)]∣∣∣ ≤ Õ(n−αa ) for all a, where E is with respect to (X,A, Y ) as well as h ∼ Q̂. Example 6 (EO).",3.3. Error Analysis,[0],[0]
"Similarly, if n(a,y) denotes the number of examples with Ai = a and Yi = y and n(?,y) denotes the number of examples with Yi = y, then Assumption 1 states that we should set ε(a,y,+) = ε(a,y,−)",3.3. Error Analysis,[0],[0]
"= C ′(n −α (a,y) +n −α (?,y)) and Theorem 3 then shows that for a suitable setting ofC ′, ν, B, and η, Algorithm 1 will return a randomized classifier Q̂ with the lowest feasible classification error up to Õ(n−α) while also approximately satisfying the fairness constraints∣∣∣E[h(X) |A = a, Y = y]−E[h(X) | Y = y]∣∣∣ ≤",3.3. Error Analysis,[0],[0]
"Õ(n−α(a,y))",3.3. Error Analysis,[0],[0]
"for all a, y. Again, E includes randomness under the true distribution over (X,A, Y ) as well as h ∼ Q̂.",3.3. Error Analysis,[0],[0]
"In some situations, it is preferable to select a deterministic classifier, even if that means a lower accuracy or a modest violation of the fairness constraints.",3.4. Grid Search,[0],[0]
"A set of candidate classifiers can be obtained from the saddle point (Q†,λ†).",3.4. Grid Search,[0],[0]
"Specifically, because Q† is a minimizer of L(Q,λ†) and L is linear in Q, the distribution Q† puts non-zero mass only on classifiers that are theQ-player’s best responses to λ†. If we knew λ†, we could retrieve one such best response via the reduction to cost-sensitive learning introduced in Section 3.2.
",3.4. Grid Search,[0],[0]
"We can compute λ† using Algorithm 1, but when the number of constraints is very small, as is the case for demographic parity or equalized odds with a binary protected attribute, it is also reasonable to consider a grid of values λ, calculate the best response for each value, and then select the value with the desired tradeoff between accuracy and fairness.
",3.4. Grid Search,[0],[0]
Example 7 (DP).,3.4. Grid Search,[0],[0]
"When the protected attribute is binary, e.g., A ∈ {a, a′}, then the grid search can in fact be conducted in a single dimension.",3.4. Grid Search,[0],[0]
"The reduction formally takes
two real-valued arguments λa and λa′ , and then adjusts the costs for predicting h(Xi) = 1 by the amounts
δa = λa pa − λa − λa′ and δa′ = λa′ pa′ − λa − λa′ ,
respectively, on the training examples with Ai = a and Ai = a
′. These adjustments satisfy paδa + pa′δa′ = 0, so instead of searching over λa and λa′ , we can carry out the grid search over δa alone and apply the adjustment δa′ = −paδa/pa′ to the protected attribute value a′.
With three attribute values, e.g., A ∈ {a, a′, a′′}, we similarly have paδa + pa′δa′ + pa′′δa′′ = 0, so it suffices to conduct grid search in two dimensions rather than three.
",3.4. Grid Search,[0],[0]
Example 8 (EO).,3.4. Grid Search,[0],[0]
"If A ∈ {a, a′}, we obtain the adjustment
δ(a,y) = λ(a,y)",3.4. Grid Search,[0],[0]
"p(a,y) − λ(a,y) + λ(a′,y) p(?,y)
for an example with protected attribute value a and label y, and similarly for protected attribute value a′.",3.4. Grid Search,[0],[0]
"In this case, separately for each y, the adjustments satisfy
p(a,y)δ(a,y) + p(a′,y)δ(a′,y) = 0,
so it suffices to do the grid search over δ(a,0) and δ(a,1) and set the parameters for a′ to δ(a′,y) =",3.4. Grid Search,[0],[0]
"−p(a,y)δ(a,y)/p(a′,y).",3.4. Grid Search,[0],[0]
We now examine how our exponentiated-gradient reduction5 performs at the task of binary classification subject to either demographic parity or equalized odds.,4. Experimental Results,[0],[0]
"We provide an evaluation of our grid-search reduction in Appendix D.
We compared our reduction with the score-based postprocessing algorithm of Hardt et al. (2016), which takes as its input any classifier, (i.e., a standard classifier without any fairness constraints) and derives a monotone transformation of the classifier’s output to remove any disparity with respect to the training examples.",4. Experimental Results,[0],[0]
"This post-processing algorithm works with both demographic parity and equalized odds, as well as with binary and non-binary protected attributes.
",4. Experimental Results,[0],[0]
"For demographic parity, we also compared our reduction with the reweighting and relabeling approaches of Kamiran & Calders (2012).",4. Experimental Results,[0],[0]
"Reweighting can be applied to both binary and non-binary protected attributes and operates by changing importance weights on each example with the goal of removing any statistical dependence between the protected attribute and label.6 Relabeling was developed for
5https://github.com/Microsoft/fairlearn 6Although reweighting was developed for demographic parity, the weights that it induces are achievable by our grid search, albeit the grid search for equalized odds rather than demographic parity.
binary protected attributes.",4. Experimental Results,[0],[0]
"First, a classifier is trained on the original data (without considering fairness).",4. Experimental Results,[0],[0]
The training examples close to the decision boundary are then relabeled to remove all disparity while minimally affecting accuracy.,4. Experimental Results,[0],[0]
"The final classifier is then trained on the relabeled data.
",4. Experimental Results,[0],[0]
"As the base classifiers for our reductions, we used the weighted classification implementations of logistic regression and gradient-boosted decision trees in scikit-learn (Pedregosa et al., 2011).",4. Experimental Results,[0],[0]
"In addition to the three baselines described above, we also compared our reductions to the “unconstrained” classifiers trained to optimize accuracy only.
",4. Experimental Results,[0],[0]
"We used four data sets, randomly splitting each one into training examples (75%) and test examples (25%):
• The adult income data set (Lichman, 2013) (48,842 examples).",4. Experimental Results,[0],[0]
"Here the task is to predict whether someone makes more than $50k per year, with gender as the protected attribute.",4. Experimental Results,[0],[0]
"To examine the performance for non-binary protected attributes, we also conducted another experiment with the same data, using both gender and race (binarized into white and non-white) as the protected attribute.",4. Experimental Results,[0],[0]
"Relabeling, which requires binary protected attributes, was therefore not applicable here.",4. Experimental Results,[0],[0]
"• ProPublica’s COMPAS recidivism data (7,918 examples).",4. Experimental Results,[0],[0]
"The task is to predict recidivism from someone’s criminal history, jail and prison time, demographics, and COMPAS risk scores, with race as the protected attribute (restricted to white and black defendants).",4. Experimental Results,[0],[0]
"• Law School Admissions Council’s National Longitudinal Bar Passage Study (Wightman, 1998) (20,649 examples).",4. Experimental Results,[0],[0]
"Here the task is to predict someone’s eventual passage of the bar exam, with race (restricted to white and black only) as the protected attribute.",4. Experimental Results,[0],[0]
•,4. Experimental Results,[0],[0]
"The Dutch census data set (Dutch Central Bureau for
Statistics, 2001) (60,420 examples).",4. Experimental Results,[0],[0]
"Here the task is to predict whether or not someone has a prestigious occupation, with gender as the protected attribute.
",4. Experimental Results,[0],[0]
"While all the evaluated algorithms require access to the protected attribute A at training time, only the post-processing algorithm requires access to A at test time.",4. Experimental Results,[0],[0]
"For a fair comparison, we included A in the feature vector X , so all algorithms had access to it at both the training time and test time.
",4. Experimental Results,[0],[0]
"We used the test examples to measure the classification error for each approach, as well as the violation of the desired fairness constraints, i.e., maxa ∣∣E[h(X) |A = a]− E[h(X)]∣∣ and maxa,y
∣∣E[h(X) |",4. Experimental Results,[0],[0]
"A = a, Y = y]− E[h(X)",4. Experimental Results,[0],[0]
"| Y = y]∣∣ for demographic parity and equalized odds, respectively.
",4. Experimental Results,[0],[0]
We ran our reduction across a wide range of tradeoffs between the classification error and fairness constraints.,4. Experimental Results,[0],[0]
"We considered ε ∈ {0.001, . . .",4. Experimental Results,[0],[0]
", 0.1} and for each value ran Algorithm 1 with ĉk = ε across all k. As expected, the returned randomized classifiers tracked the training Pareto
frontier (see Figure 2 in Appendix D).",4. Experimental Results,[0],[0]
"In Figure 1, we evaluate these classifiers alongside the baselines on the test data.
",4. Experimental Results,[0],[0]
"For all the data sets, the range of classification errors is much smaller than the range of constraint violations.",4. Experimental Results,[0],[0]
Almost all the approaches were able to substantially reduce or remove disparity without much impact on classifier accuracy.,4. Experimental Results,[0],[0]
"One exception was the Dutch census data set, where the classification error increased the most in relative terms.
",4. Experimental Results,[0],[0]
Our reduction generally dominated or matched the baselines.,4. Experimental Results,[0],[0]
The relabeling approach frequently yielded solutions that were not Pareto optimal.,4. Experimental Results,[0],[0]
"Reweighting yielded solutions on the Pareto frontier, but often with substantial disparity.",4. Experimental Results,[0],[0]
"As expected, post-processing yielded disparities that were statistically indistinguishable from zero, but the resulting classification error was sometimes higher than achieved by our reduction under a statistically indistinguishable disparity.",4. Experimental Results,[0],[0]
"In addition, and unlike the post-processing algorithm, our reduction can achieve any desired accuracy–fairness tradeoff, allows a wider range of fairness definitions, and does not require access to the protected attribute at test time.
",4. Experimental Results,[0],[0]
"Our grid-search reduction, evaluated in Appendix D, sometimes failed to achieve the lowest disparities on
the training data, but its performance on the test data very closely matched that of our exponentiated-gradient reduction.",4. Experimental Results,[0],[0]
"However, if the protected attribute is non-binary, then grid search is not feasible.",4. Experimental Results,[0],[0]
"For instance, for the version of the adult income data set where the protected attribute takes on four values, the grid search would need to span three dimensions for demographic parity and six dimensions for equalized odds, both of which are prohibitively costly.",4. Experimental Results,[0],[0]
We presented two reductions for achieving fairness in a binary classification setting.,5. Conclusion,[0],[0]
"Our reductions work for any classifier representation, encompass many definitions of fairness, satisfy provable guarantees, and work well in practice.
",5. Conclusion,[0],[0]
Our reductions optimize the tradeoff between accuracy and any (single) definition of fairness given training-time access to protected attributes.,5. Conclusion,[0],[0]
"Achieving fairness when trainingtime access to protected attributes is unavailable remains an open problem for future research, as does the navigation of tradeoffs between accuracy and multiple fairness definitions.",5. Conclusion,[0],[0]
"We would like to thank Aaron Roth, Sam Corbett-Davies, and Emma Pierson for helpful discussions.",Acknowledgements,[0],[0]
We present a systematic approach for achieving fairness in a binary classification setting.,abstractText,[0],[0]
"While we focus on two well-known quantitative definitions of fairness, our approach encompasses many other previously studied definitions as special cases.",abstractText,[0],[0]
"The key idea is to reduce fair classification to a sequence of cost-sensitive classification problems, whose solutions yield a randomized classifier with the lowest (empirical) error subject to the desired constraints.",abstractText,[0],[0]
"We introduce two reductions that work for any representation of the cost-sensitive classifier and compare favorably to prior baselines on a variety of data sets, while overcoming several of their disadvantages.",abstractText,[0],[0]
A Reductions Approach to Fair Classification,title,[0],[0]
"1The University of Iowa, Iowa City, IA 52242, USA 2National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China. Correspondence to: Tianbao Yang <tianbao-yang@uiowa.edu>.
This is the long version of our paper appearing in the Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).",text,[0],[0]
"In this paper, we aim at solving the following convex constrained optimization problem:
min x∈Rd f(x), s.t. c(x) ≤ 0, (1)
where f(x) is a smooth or non-smooth convex function and c(x) is a lower-semicontinuous and convex function.",1. Introduction,[0],[0]
"The problem can find applications in machine learning, signal processing, statistics, marketing optimization, and etc.",1. Introduction,[0],[0]
"For example, in distance metric learning one needs to learn a positive semi-definite (PSD) matrix such that similar examples are close to each other and dissimilar examples are far from each other (Weinberger et al., 2006; Xing et al., 2003), where the positive semi-definite constraint can be cast into a convex inequality constraint.",1. Introduction,[0],[0]
"Another example arising in compressive sensing is to minimize the `1 norm of high-dimensional vector subject to a measurement constraint (Candès & Wakin, 2008).",1. Introduction,[0],[0]
"Although general interiorpoint methods can be applied to solve the problem with linear convergence, they suffer from exceedingly high computational cost per-iteration.",1. Introduction,[0],[0]
"Another solution is to employ the projected gradient (PG) method (Nesterov, 2004) or the conditional gradient (CG) method (Frank & Wolfe, 1956), where the PG method needs to compute the projection into the constrained domain at each iteration and CG needs to solve a linear optimization problem under the constraint.",1. Introduction,[0],[0]
"However, for many constraints (e.g., PSD, quadratic constraints) both projection into the constrained domain and the linear optimization under the constraint are time-consuming, which restrict their capabilities to solving these problems.
",1. Introduction,[0],[0]
"Recently, there emerges a new direction towards addressing the challenge of expensive projection that is to reduce the number of projections.",1. Introduction,[0],[0]
"In the seminal paper (Mahdavi et al., 2012), the authors have proposed two algorithms with only one projection at the end of iterations for non-smooth convex and strongly convex optimization, respectively.",1. Introduction,[0],[0]
The idea of both algorithms is to move the constraint function into the objective function and to control the violation of constraint for intermediate solutions.,1. Introduction,[0],[0]
"While their developed algorithms enjoy an optimal convergence rate for nonsmooth optimization (i.e., O(1/ 2) iteration complexity)
and a close-to-optimal convergence rate for strongly convex optimization (i.e., Õ(1/ ) 1), there still lack of theory and algorithms with reduced projections and faster rates for smooth convex optimization and for convex optimization without strong convexity assumptions.
",1. Introduction,[0],[0]
"In this paper, we make significant contributions by developing a richer theory of convex constrained optimization with reduced projections and faster rates.",1. Introduction,[0],[0]
"To be specific,
• we develop a general framework and theory of optimization with only one projection, where any favorable smooth or non-smooth convex optimization algorithms can be employed to solve the intermediate augmented unconstrained objective function.",1. Introduction,[0],[0]
"We discuss in full details the applicability of the proposed algorithms to problems with polyhedral, quadratic or PSD constraints.
",1. Introduction,[0],[0]
• Applying the general theory to smooth convex optimization 2 with Nesterov’s accelerated gradient methods yields an iteration complexity of O(1/ ) with only one projection.,1. Introduction,[0],[0]
"In addition, when equipped with an optimal algorithm for strongly convex optimization the general theory implies the optimal iteration complexity of O(1/ ) for strongly convex optimization with only one projection.",1. Introduction,[0],[0]
"For smooth and strongly convex optimization, the general theory implies an iteration complexity of O(1/ β) where β ∈ (1/2, 1) with only one projection and a sufficiently large number of iterations.
",1. Introduction,[0],[0]
"• Building on the general framework and theory, we further develop an improved theory with faster convergence rates for non-strongly convex optimization at the price of a logarithmic number of projections.",1. Introduction,[0],[0]
"In particular, we show that under a mild local error bound condition, the iteration complexities can be reduced to Õ(1/ 2(1−θ)) for non-smooth optimization and Õ(1/ 1−θ) for smooth optimization, where θ ∈ (0, 1] is a constant in the local error bound condition that characterizes the local growth rate of functional values.",1. Introduction,[0],[0]
"To our knowledge, these are the best convergence results with only a logarithmic number of projections for non-strongly convex optimization.",1. Introduction,[0],[0]
We also demonstrate their effectiveness for solving compressive sensing and distance metric learning problems.,1. Introduction,[0],[0]
The issue of high projection cost in projected gradient descent has received increasing attention in recent years.,2. Related Work,[0],[0]
"Most studies are based on the Frank-Wolfe technique that eschews the projection in favor of a linear optimization over the constrained domain (Jaggi, 2013; Hazan & Kale, 2012; Lacoste-Julien et al., 2013; Garber & Hazan, 2015).",2. Related Work,[0],[0]
"It happens that for many bounded domains (e.g., bounded
1where Õ() suppresses a logarithmic factor.",2. Related Work,[0],[0]
"2where the constraint function is assumed to be smooth.
balls for vectors and matrices, a PSD constraint with a bounded trace norm)",2. Related Work,[0],[0]
"the linear optimization over the constrained domain is much cheaper than projection into the constrained domain (Jaggi, 2013).",2. Related Work,[0],[0]
"However, there still exist many constraints that render both projection into the constrained domain and linear optimization under the constraint are comparably expensive.",2. Related Work,[0],[0]
"Examples include polyhedral constraints, quadratic constraints and a PSD constraint 3.
",2. Related Work,[0],[0]
"To tackle these complex constraints, the idea of optimization with a reduced number of projections was explored in several studies since (Mahdavi et al., 2012).",2. Related Work,[0],[0]
"In a recent paper (Chen et al., 2016), the authors show that for stochastic strongly convex optimization, the optimal convergence rate can be achieved using a logarithmic number of projections.",2. Related Work,[0],[0]
"In contrast, the developed theory in this paper implies that only one projection is sufficient to achieve the optimal convergence rate for strongly convex optimization, and a logarithmic number of projections can be used to accelerate convergence rates for non-strongly convex optimization.",2. Related Work,[0],[0]
"Cotter et al. (2016) proposed a stochastic algorithm for solving heavily constrained problems with many constraint functions by extending the work of (Mahdavi et al., 2012).",2. Related Work,[0],[0]
"Nonetheless, their focus is not to improve the convergence rates.",2. Related Work,[0],[0]
"Zhang et al. (2013) studied the smooth and strongly convex optimization and they proposed a stochastic algorithm withO(κ log(T )) projections and proved anO(1/T ) convergence rate, where κ is the condition number and T is the total number of iterations.",2. Related Work,[0],[0]
"Nonetheless, if the condition number is high the number of projections could be very large.",2. Related Work,[0],[0]
"In addition, their algorithm utilizes the minibatch to avoid frequent projections in stochastic optimization, which is different from the present paper.
",2. Related Work,[0],[0]
"We note that several recent works also exploit different forms of error bound conditions to improve the convergence (Wang & Lin, 2014; So, 2013; Hou et al., 2013; Zhou et al., 2015; Yang & Lin, 2016; Xu et al., 2016).",2. Related Work,[0],[0]
"Most notably, the technique used in our work is closely related to (Yang & Lin, 2016).",2. Related Work,[0],[0]
"However, for constrained optimization problems the methods in (Yang & Lin, 2016) still need to conduct projections at each iteration.
",2. Related Work,[0],[0]
"Finally, we comment on the differences between the proposed methods and the classical penalty methods that also move the constraint into the objective using a penalty function (Bertsekas, 1996).",2. Related Work,[0],[0]
"The major differences are that (i) the classical penalty methods typically require solving each subproblem exactly while our methods do not require that; and (ii) the classical penalty methods typically guarantee asymptotic convergence while our methods have explicit convergence rates.
",2. Related Work,[0],[0]
"3Indeed, a linear optimization over a PSD constraint is illposed because the PSD domain is unbounded.",2. Related Work,[0],[0]
Let Ω = {x ∈,3. Preliminaries,[0],[0]
"Rd : c(x) ≤ 0} denote the constrained domain, Ω∗ denote the optimal solution set and f∗ denote the optimal objective value.",3. Preliminaries,[0],[0]
"We denote by ∇f(x) the gradient and by ∂f(x) the subgradient of a smooth or non-smooth function, respectively.",3. Preliminaries,[0],[0]
"When f(x) is a non-smooth function, we consider the problem as non-smooth constrained optimization.",3. Preliminaries,[0],[0]
"When both f(x) and c(x) are smooth, we consider the problem as smooth constrained optimization.",3. Preliminaries,[0],[0]
"A function f(x) is L-smooth if it has a Lipschitz continuous gradient, i.e., ‖∇f(x)−∇f(y)‖ ≤",3. Preliminaries,[0],[0]
"L‖x− y‖, where ‖ · ‖ denotes the Euclidean norm.",3. Preliminaries,[0],[0]
A function f(x) is µstrongly convex if it satisfies f(x) ≥ f(y) + ∂f(y)>(x− y) +,3. Preliminaries,[0],[0]
"µ2 ‖x− y‖ 2.
",3. Preliminaries,[0],[0]
"In the sequel, dist(x,Ω) denotes the distance of x to a set Ω, i.e., dist(x,Ω) =",3. Preliminaries,[0],[0]
minu∈Ω ‖x−u‖.,3. Preliminaries,[0],[0]
"Let [s]+ be a hinge operator that is defined as [s]+ = s if s ≥ 0, and [s]+ = 0",3. Preliminaries,[0],[0]
"if s < 0.
",3. Preliminaries,[0],[0]
"Throughout the paper, we make the the following assumptions to facilitate the development of our algorithms and theory.
",3. Preliminaries,[0],[0]
Assumption 1.,3. Preliminaries,[0],[0]
"For a convex minimization problem (1), we assume (i) there exists a positive value ρ > 0",3. Preliminaries,[0],[0]
"such that
min c(x)=0
v∈∂c(x),v 6=0
‖v‖ ≥ ρ, (2)
or more generally there exists a constant ρ > 0",3. Preliminaries,[0],[0]
"for any x ∈ Rd, such that x\ = arg minu∈Rd,",3. Preliminaries,[0],[0]
c(u)≤0 ‖u,3. Preliminaries,[0],[0]
"− x‖2 satisfies
‖x\ − x‖ ≤",3. Preliminaries,[0],[0]
[c(x)]+/ρ.,3. Preliminaries,[0],[0]
"(3)
(ii) there exists a strictly feasible solution such that c(x) < 0; (iii) both f(x) and c(x) are defined everywhere and are Lipschitz continuous with their Lipschitz constants denoted by G and Gc, respectively.",3. Preliminaries,[0],[0]
We make several remarks about the assumptions.,3. Preliminaries,[0],[0]
"The inequality in (2) is introduced in (Mahdavi et al., 2012), which is to ensure the distance from the final solution before projection to constrained domain Ω is not too large.",3. Preliminaries,[0],[0]
"Note that the inequality in (3) is a more general condition than (2) as seen from the following lemma.
",3. Preliminaries,[0],[0]
Lemma 1.,3. Preliminaries,[0],[0]
"For any x ∈ Rd, let x\ = arg minc(u)≤0 ‖u− x‖2.",3. Preliminaries,[0],[0]
"If (2) holds, then (3) holds.
",3. Preliminaries,[0],[0]
"The above lemma is implicit in the proof of (Mahdavi et al., 2012).",3. Preliminaries,[0],[0]
"We will provide more discussions about Assumption 1(i) - the key assumption, and exhibit the value of ρ for a number of commonly seen constraints (e.g., polyhedral, quadratic and PSD constraints).",3. Preliminaries,[0],[0]
"To make the presentation more fluent, we postpone these discussions to Section 6.",3. Preliminaries,[0],[0]
"The strict feasibility assumption (ii) allows us to explore the KKT condition of the projection problem shown below.
",3. Preliminaries,[0],[0]
"Assumption (iii) imposes mild Lipschitz continuity conditions on both f(x) and c(x).
",3. Preliminaries,[0],[0]
Traditional projected gradient descent methods need to solve the following projection at each iteration ΠΩ[x] = arg minc(u)≤0 ‖u,3. Preliminaries,[0],[0]
− x‖2.,3. Preliminaries,[0],[0]
Conditional gradient methods (a.k.a.,3. Preliminaries,[0],[0]
"the Frank-Wolfe technique) need to solve the following linear optimization at each iteration minu∈Rd,c(u)≤0 u
>∇f(x).",3. Preliminaries,[0],[0]
"For many constraint functions (see Section 6), solving the projection problem and the linear optimization could be very expensive.",3. Preliminaries,[0],[0]
"In this section, we extend the idea of only one projection proposed in (Mahdavi et al., 2012) to a general theory, and then present optimization algorithms with only one projection for non-smooth and smooth optimization, respectively.",4. A General Theory of Optimization with only one projection,[0],[0]
"To tackle the constraint, we introduce a penalty function hγ(x) parameterized by γ, which obeys the following certificate: there exist constants C ≥ 0 and λ > G/ρ such that
hγ(x) ≥",4. A General Theory of Optimization with only one projection,[0],[0]
"λ[c(x)]+,∀x hγ(x) ≤ Cγ, ∀x such that c(x) ≤ 0.
(4)
",4. A General Theory of Optimization with only one projection,[0],[0]
"From the above condition, it is clear that γ ≥ 0.",4. A General Theory of Optimization with only one projection,[0],[0]
"It is notable that the penalty function hγ(x) will also depend on λ; however it will be set to a constant value, thus the dependence on λ is omitted.",4. A General Theory of Optimization with only one projection,[0],[0]
We will construct such a penalty function hγ(x) for non-smooth and smooth optimization in next two subsections.,4. A General Theory of Optimization with only one projection,[0],[0]
"We propose to optimize the following augmented objective function
min x∈Rd Fγ(x) = f(x) + hγ(x).",4. A General Theory of Optimization with only one projection,[0],[0]
"(5)
We can employ any applicable optimization algorithms to optimize Fγ(x) pretending that there is no constraint, and finally obtain a solution x̂T that is not necessarily feasible.",4. A General Theory of Optimization with only one projection,[0],[0]
"In order to obtain a feasible solution, we perform one projection to get x̃T = ΠΩ(x̂T ).",4. A General Theory of Optimization with only one projection,[0],[0]
"The following theorem allows us to convert the convergence of x̂T for Fγ(x) to that of x̃T for f(x).
",4. A General Theory of Optimization with only one projection,[0],[0]
Theorem 1.,4. A General Theory of Optimization with only one projection,[0],[0]
"LetA be any iterative optimization algorithm applied to minx Fγ(x) with T iterations, which starts with x1 and returns x̂T as the final solution, such that the following convergence of x̂T holds for any x ∈ Rd
Fγ(x̂T )− Fγ(x) ≤ BT (γ;x,x1), (6)
",4. A General Theory of Optimization with only one projection,[0],[0]
"where BT (γ;x,x1) → 0 when T → ∞. Suppose that Assumption 1 hold, then
f(x̃T )",4. A General Theory of Optimization with only one projection,[0],[0]
− f(x∗) ≤,4. A General Theory of Optimization with only one projection,[0],[0]
"λρ
λρ−G",4. A General Theory of Optimization with only one projection,[0],[0]
"(Cγ +BT (γ;x∗,x1)), (7)
where x̃T = ΠΩ[x̂T ] and x∗ is an optimal solution to (1).
",4. A General Theory of Optimization with only one projection,[0],[0]
"Remark: It is worth mentioning that we omit some constant factors in the convergence bound BT (γ;x,x1) that are irrelevant to our discussions.",4. A General Theory of Optimization with only one projection,[0],[0]
"The notationBT (γ;x,x1) emphasizes that it is a function of γ and depends on x1 and a target solution x and it will be referred to as BT .",4. A General Theory of Optimization with only one projection,[0],[0]
"In the next several subsections, we will see that by carefully choosing the penalty function hγ(x)",4. A General Theory of Optimization with only one projection,[0],[0]
we are able to provide nice convergence for smooth and non-smooth optimization with only one projection.,4. A General Theory of Optimization with only one projection,[0],[0]
"In the above theorem, we assume the optimization algorithm A is deterministic.",4. A General Theory of Optimization with only one projection,[0],[0]
"However, a similar result can be easily extended to a stochastic optimization algorithm A.
Proof.",4. A General Theory of Optimization with only one projection,[0],[0]
"First, we consider c(x̂T ) ≤ 0, which implies that x̂T = x̃T .",4. A General Theory of Optimization with only one projection,[0],[0]
"Due to the certificate of hγ(x), Fγ(x̃T ) ≥ f(x̃T ) and Fγ(x∗) ≤ f(x∗) + Cγ.",4. A General Theory of Optimization with only one projection,[0],[0]
"Hence f(x̃T ) ≤ Fγ(x̂T ) ≤ Fγ(x∗) + BT (γ;x1,x∗) ≤ f(x∗) + Cγ + BT (γ;x1,x∗).",4. A General Theory of Optimization with only one projection,[0],[0]
Then (7) follows due to λρ/(λρ−G) ≥ 1.,4. A General Theory of Optimization with only one projection,[0],[0]
"Next, we assume c(x̂T ) >",4. A General Theory of Optimization with only one projection,[0],[0]
0.,4. A General Theory of Optimization with only one projection,[0],[0]
"Inequality (6) implies that
f(x̂T ) +λ[c(x̂T )]",4. A General Theory of Optimization with only one projection,[0],[0]
+ ≤ f(x∗),4. A General Theory of Optimization with only one projection,[0],[0]
"+Cγ+BT (γ;x∗,x1).",4. A General Theory of Optimization with only one projection,[0],[0]
"(8)
By Assumption 1(i), we have [c(x̂T )]",4. A General Theory of Optimization with only one projection,[0],[0]
+ ≥ ρ‖x̂T − x̃T ‖.,4. A General Theory of Optimization with only one projection,[0],[0]
"Combined with (8) we have
λρ‖x̂T − x̃T ‖ ≤ f(x∗)− f(x̂T ) +",4. A General Theory of Optimization with only one projection,[0],[0]
"Cγ +BT (γ;x∗,x1) ≤ G‖x̂T",4. A General Theory of Optimization with only one projection,[0],[0]
− x̃T ‖+,4. A General Theory of Optimization with only one projection,[0],[0]
"Cγ +BT (γ;x∗,x1),
where the last inequality follows that fact f(x∗)−f(x̂T ) ≤ f(x∗)−f(x̃T )",4. A General Theory of Optimization with only one projection,[0],[0]
+f(x̃T )−f(x̂T ),4. A General Theory of Optimization with only one projection,[0],[0]
≤ G‖x̂T,4. A General Theory of Optimization with only one projection,[0],[0]
− x̃T ‖,4. A General Theory of Optimization with only one projection,[0],[0]
because the Lipschitz property and f(x∗) ≤ f(x̃T ),4. A General Theory of Optimization with only one projection,[0],[0]
.,4. A General Theory of Optimization with only one projection,[0],[0]
"Therefore we have
‖x̂T − x̃T ‖ ≤",4. A General Theory of Optimization with only one projection,[0],[0]
"Cγ +BT (γ;x∗,x1, )
λρ−G .
",4. A General Theory of Optimization with only one projection,[0],[0]
"Finally, we obtain
f(x̃T )",4. A General Theory of Optimization with only one projection,[0],[0]
− f(x∗) ≤ f(x̃T ),4. A General Theory of Optimization with only one projection,[0],[0]
− f(x̂T ) + f(x̂T ),4. A General Theory of Optimization with only one projection,[0],[0]
− f(x∗) ≤ G‖x̂T,4. A General Theory of Optimization with only one projection,[0],[0]
− x̃T ‖+,4. A General Theory of Optimization with only one projection,[0],[0]
"Cγ +BT (γ;x∗,x1)
≤",4. A General Theory of Optimization with only one projection,[0],[0]
λρ λρ−G,4. A General Theory of Optimization with only one projection,[0],[0]
"(Cγ +BT (γ;x∗,x1)).",4. A General Theory of Optimization with only one projection,[0],[0]
"Since an optimal convergence rate for general non-smooth optimization with only one projection has been attained in (Mahdavi et al., 2012), in this subsection we present an optimal convergence result for strongly convex problems.",4.1. Non-smooth Optimization,[0],[0]
"For non-smooth optimization, we can choose
h(x) = λ[c(x)]+,
and hence γ = 0.",4.1. Non-smooth Optimization,[0],[0]
"We will use deterministic subgradient descent as an example to demonstrate the convergence for f(x), though many other optimization algorithms designed for non-smooth optimization are applicable (e.g.,
the stochastic subgradient method).",4.1. Non-smooth Optimization,[0],[0]
"The update of subgradient descent method is given by the following
xt+1 = xt",4.1. Non-smooth Optimization,[0],[0]
"− ηt∂F (xt), t = 1, . . .",4.1. Non-smooth Optimization,[0],[0]
", T, (9)
where ηt is an appropriate step size.",4.1. Non-smooth Optimization,[0],[0]
"If f(x) is µ-strongly convex, the step size can be set as ηt = 1/(µt) and the final solution can be computed by the α-suffix averaging x̂T = 1αT ∑T t=(1−α)T+1 xt with α > 0",4.1. Non-smooth Optimization,[0],[0]
"(Rakhlin et al., 2012), or by the polynomial decay averaging with x̂t = (1− s+1s+t )x̂t−1 + s+1 s+txt and s ≥ 1 (Shamir & Zhang, 2013).",4.1. Non-smooth Optimization,[0],[0]
Both schemes can attain BT = O(1/(µT )) for the convergence of F (x) when f(x) is µ-strongly convex.,4.1. Non-smooth Optimization,[0],[0]
"Combining this with Theorem 1, we have the following convergence result with the proof omitted due to its simplicity.
",4.1. Non-smooth Optimization,[0],[0]
Corollary 2.,4.1. Non-smooth Optimization,[0],[0]
Suppose that Assumption 1 holds and f(x) is µ-strongly convex.,4.1. Non-smooth Optimization,[0],[0]
Set F (x) = f(x) + λ[c(x)]+ with λ ≥ G/ρ.,4.1. Non-smooth Optimization,[0],[0]
Let (9) run for T iterations with ηt = 1/(µt).,4.1. Non-smooth Optimization,[0],[0]
Let x̂T be computed by α-suffix averaging or the polynomial decay averaging.,4.1. Non-smooth Optimization,[0],[0]
"Then with only one projection x̃T = ΠΩ(x̂T ), we achieve
f(x̃T )",4.1. Non-smooth Optimization,[0],[0]
− f∗ ≤ λρ λρ−G,4.1. Non-smooth Optimization,[0],[0]
"(G+ λGc)
2O(1)
µT .
",4.1. Non-smooth Optimization,[0],[0]
"Remark: We note that the O(1/(µT )) is also achieved for strongly convex optimization in (Zhang et al., 2013; Chen et al., 2016) but with a logarithmic number of projections.",4.1. Non-smooth Optimization,[0],[0]
"In contrast, Corollary 2 implies only one projection is sufficient to achieve the optimal convergence for strongly convex optimization.",4.1. Non-smooth Optimization,[0],[0]
"For smooth optimization, we consider both f(x) and c(x) to be smooth 4.",4.2. Smooth Optimization,[0],[0]
"Let the smoothness parameter of f(x) and c(x) be Lf and Lc, respectively.",4.2. Smooth Optimization,[0],[0]
"In order to ensure the augmented function Fγ(x) to be still a smooth function, we construct the following penalty function
hγ(x) =",4.2. Smooth Optimization,[0],[0]
γ ln (1 + exp (λc(x)/γ)) .,4.2. Smooth Optimization,[0],[0]
"(10)
The following proposition shows that hγ(x) is a smooth function and obeys the condition in (4).
",4.2. Smooth Optimization,[0],[0]
Proposition 1.,4.2. Smooth Optimization,[0],[0]
Suppose c(x) is Lc-smooth and,4.2. Smooth Optimization,[0],[0]
GcLipschitz continuous.,4.2. Smooth Optimization,[0],[0]
The penalty function in (10) is a (λLc + λ2G2c 4γ )-smooth function and satisfies (i) hγ(x),4.2. Smooth Optimization,[0],[0]
≥ λ[c(x)]+ and (ii) hγ(x) ≤,4.2. Smooth Optimization,[0],[0]
"γ ln 2, ∀x such that c(x) ≤ 0.
",4.2. Smooth Optimization,[0],[0]
"Then Fγ(x) is a smooth function and its smoothness parameter is given by LF = Lf +λLc+ λ2G2c
4γ .",4.2. Smooth Optimization,[0],[0]
"Next, we will
4it can be extended to when f(x) is non-smooth but its proximal mapping can be easily solved.
establish the convergence for f(x) using Nesterov’s optimal accelerated gradient (NAG) methods.",4.2. Smooth Optimization,[0],[0]
"The update of one variant of NAG can be written as follows
xt+1 =",4.2. Smooth Optimization,[0],[0]
yt,4.2. Smooth Optimization,[0],[0]
−∇Fγ(yt)/LF yt+1 = xt+1,4.2. Smooth Optimization,[0],[0]
+ βt+1(xt+1,4.2. Smooth Optimization,[0],[0]
"− xt),
(11)
where the value of βt can be set to different values depending on whether f(x) is strongly convex or not (see Corollary 3).",4.2. Smooth Optimization,[0],[0]
"Previous work have established the convergence of x̂T = xT for Fγ(x), in particular BT = O(LFT 2 ) for smooth non-strongly convex optimization and BT = O ( LF exp(−T √ µ LF ) )
for smooth and strongly convex optimization.",4.2. Smooth Optimization,[0],[0]
"By combining these results with Theorem 1 and appropriately setting γ, we can achieve the following convergence of x̃T for f(x).",4.2. Smooth Optimization,[0],[0]
Corollary 3.,4.2. Smooth Optimization,[0],[0]
"Suppose that Assumption 1 holds, dist(y0,Ω∗) ≤ D, f(x) is Lf -smooth and c(x) is Lc-smooth.",4.2. Smooth Optimization,[0],[0]
Set Fγ(x) = f(x) + hγ(x) with λ > G/ρ and hγ(x) being (10).,4.2. Smooth Optimization,[0],[0]
"Let (11) run for T iterations and x̃T = ΠΩ(xT ).
",4.2. Smooth Optimization,[0],[0]
"• If f(x) is convex, we can set γ = λGcD (T+1) √ 2 ln 2 , βt =
τt−1−1 τt
, where τt = 1+ √ 1+4τ2t−1 2 with τ0 = 1, and achieve
f(x̃T )",4.2. Smooth Optimization,[0],[0]
"−f∗ ≤ λρ
λρ−G
[ λGcD √ 2 ln 2
T + 1 +
(Lf + λLc)D 2
(T + 1)2 ]",4.2. Smooth Optimization,[0],[0]
"• If f(x) is µ-strongly convex, we can set γ = 1T 2α with α ∈ (1/2, 1) and βt = √ LF− √ µ√
LF+",4.2. Smooth Optimization,[0],[0]
"√ µ , and achieve f(x̃T )",4.2. Smooth Optimization,[0],[0]
"− f∗ ≤ O ( 1
T 2α +
1
T 4α
) ,
as long as T ≥ ( Lf+λLc+λ
2G2c/4 µ
) 1 2(1−α)
(4α lnT ) 1 1−α .
",4.2. Smooth Optimization,[0],[0]
"Remark: The convergence results above indicate an O(1/ ) iteration complexity for smooth optimization and O(1/ 1/(2α)) with α ∈ (1/2, 1) for smooth and strongly convex optimization with only one projection.",4.2. Smooth Optimization,[0],[0]
"All omitted proofs can be found in (Yang et al., 2017).",4.2. Smooth Optimization,[0],[0]
"In this section, we will develop improved convergence for non-strongly convex optimization at a price of a logarithmic number of projections by considering an additional condition on the target problem.",5. Improved Convergence for Non-strongly Convex Optimization,[0],[0]
"To facilitate the presentation, we first introduce some notations.",5. Improved Convergence for Non-strongly Convex Optimization,[0],[0]
"The -sublevel set S and -level set L of the problem (1) are denoted by S = {x ∈ Ω : f(x) ≤ f∗ + }, and L = {x ∈ Ω : f(x) = f∗ + }, respectively.",5. Improved Convergence for Non-strongly Convex Optimization,[0],[0]
"Let x† denote the closest point in the -sublevel set S to x ∈ Ω, i.e.,
x† = arg min u∈Ω ‖u− x‖2, s.t. f(u) ≤ f∗ + .",5. Improved Convergence for Non-strongly Convex Optimization,[0],[0]
"(12)
Let x∗ denote the closest optimal solution in Ω∗ to x, i.e., x∗ = arg minu∈Ω∗ ‖u− x‖2.
",5. Improved Convergence for Non-strongly Convex Optimization,[0],[0]
"In this section, we will make the following additional assumption about the problem (1).",5. Improved Convergence for Non-strongly Convex Optimization,[0],[0]
Assumption 2.,5. Improved Convergence for Non-strongly Convex Optimization,[0],[0]
"For a convex minimization problem (1), we assume (i) there exist x0 ∈ Ω and 0",5. Improved Convergence for Non-strongly Convex Optimization,[0],[0]
≥ 0,5. Improved Convergence for Non-strongly Convex Optimization,[0],[0]
"such that f(x0)− minx∈Ω f(x) ≤ 0; (ii) Ω∗ is a non-empty convex compact set; (iii) the optimization problem (1) satisfies a local error bound condition, i.e., there exist θ ∈ (0, 1] and σ > 0 such that for any x ∈ S we have dist(x,Ω∗) ≤ σ(f(x) − f∗)
θ where Ω∗ denotes the optimal set and f∗ denotes the optimal value.",5. Improved Convergence for Non-strongly Convex Optimization,[0],[0]
Remark: we would like to remark that the new assumption only imposes mild conditions on the problem.,5. Improved Convergence for Non-strongly Convex Optimization,[0],[0]
"In particular, Assumption 2 (i) supposes there is a lower bound of the optimal value f∗, which usually holds in machine learning applications where the objective function if non-negative; Assumption 2 (ii) ensures that S is also bounded (Rockafellar, 1970), therefore the σ in the local error bound is finite, which can be easily satisfied for a norm regularized or constraint problems; the local error bound condition holds for a broad family of functions (e.g., semi-algebraic functions or real subanalytic functions (Jerome Bolte, 2015; Yang & Lin, 2016)).",5. Improved Convergence for Non-strongly Convex Optimization,[0],[0]
"In Section 7, we will also demonstrate several applications of the improved algorithms proposed in this section by establishing the local error bound condition.
",5. Improved Convergence for Non-strongly Convex Optimization,[0],[0]
"Although the local error bound condition is much weaker than the strong convexity assumption, below we will propose novel algorithms leveraging this condition with faster convergence and only a logarithmic number of projections.",5. Improved Convergence for Non-strongly Convex Optimization,[0],[0]
"To establish an improved convergence for non-smooth optimization, we develop a new algorithm shown in Algorithm 1 based on subgradient descent (GD) method, to which we refer as LoPGD.",5.1. Non-smooth Optimization,[0],[0]
The algorithm runs for K epochs and each epoch employs GD for minimizing F (x) = f(x) + λ[c(x)]+ with a feasible solution xk−1 ∈ Ω as a starting point and t iterations of updates.,5.1. Non-smooth Optimization,[0],[0]
"At the end of each epoch, the averaged solution x̂k is projected into the constrained domain Ω and the solution xk will be used as the starting point for next epoch.",5.1. Non-smooth Optimization,[0],[0]
The step size ηk is decreased by half every epoch starting from a given value η1.,5.1. Non-smooth Optimization,[0],[0]
"The theorem below establishes the iteration complexity of LoPGD and also exhibits the values of K, t and η1.",5.1. Non-smooth Optimization,[0],[0]
"To simplify notations, we let p = λρλρ−G and Ḡ = G+ λGc.",5.1. Non-smooth Optimization,[0],[0]
Theorem 4.,5.1. Non-smooth Optimization,[0],[0]
Suppose Assumptions 1 and 2 hold.,5.1. Non-smooth Optimization,[0],[0]
"Let η1 = 0 2pḠ2 , K = dlog2( 0/ )e",5.1. Non-smooth Optimization,[0],[0]
"and t = 4σ2p2Ḡ2 2(1−θ) in Algorithm 1, where θ and σ are constants appearing in the local error bound condition.",5.1. Non-smooth Optimization,[0],[0]
Then f(xK)− f∗ ≤ 2 .,5.1. Non-smooth Optimization,[0],[0]
"Remark: Since the projection is only conducted at the end of each epoch and the total number of epochs is at
Algorithm 1 LoPGD 1",5.1. Non-smooth Optimization,[0],[0]
": INPUT: K ∈ N+ , t ∈ N+, η1 2: Initialization: x0 ∈ Ω, 0 3: for k = 1, 2, . . .",5.1. Non-smooth Optimization,[0],[0]
",K do 4: Let xk1 = xk−1 5: for s = 1, 2, . . .",5.1. Non-smooth Optimization,[0],[0]
", t− 1 do 6: Update xks+1 = x k s − ηk∂F (xks)
7: end for 8: Let x̂k = ∑t s=1",5.1. Non-smooth Optimization,[0],[0]
"x k s/t
9: Let xk = ΠΩ[x̂k] and ηk+1 = ηk/2 10: end for
Algorithm 2 LoPNAG 1:",5.1. Non-smooth Optimization,[0],[0]
"INPUT: K ∈ N+ , t1, . . .",5.1. Non-smooth Optimization,[0],[0]
", tK ∈ N+, γ1 2: Initialization: x0 ∈ Ω, 0 3: for k = 1, 2, . . .",5.1. Non-smooth Optimization,[0],[0]
",K do 4: Let yk0 = xk−1 5: for s = 0, 1, 2, . . .",5.1. Non-smooth Optimization,[0],[0]
", tk",5.1. Non-smooth Optimization,[0],[0]
"− 1 do 6: Update xks+1 = y k s − 1Lk∇Fγk(x k s)
7: Update yks+1 = x k s+1 + βs+1(x k s+1 − xks) 8: end for 9: Let x̂k = xktk , xk = ΠΩ[x̂k] and γk+1 = γk/2
10: end for
most K = dlog2( 0/ )",5.1. Non-smooth Optimization,[0],[0]
"e, so the total number of projections is only a logarithmic number K.",5.1. Non-smooth Optimization,[0],[0]
The iteration complexity in Theorem 4 is Õ(1/ 2(1−θ)),5.1. Non-smooth Optimization,[0],[0]
that improves the standard result of O(1/ 2) without strong convexity.,5.1. Non-smooth Optimization,[0],[0]
"With θ = 1/2, we can achieve Õ(1/ ) iteration complexity with only O(log(1/ )) projections.",5.1. Non-smooth Optimization,[0],[0]
"Similar to non-smooth optimization, we also develop a new algorithm based on NAG shown in Algorithm 2, where Fγ(x) is defined using hγ(x) in (10), Lk = LFγk is the smoothness parameter of Fγk and βs = τs−1−1 τs
, s = 1, . . .",5.2. Smooth Optimization,[0],[0]
", is a sequence with τs updated as in Corollary 3.",5.2. Smooth Optimization,[0],[0]
We refer to this algorithm as LoPNAG.,5.2. Smooth Optimization,[0],[0]
"The key idea is to use to a sequence of reducing values for γk instead of using a small value as in Corollary 3, and solve each augmented unconstrained problem Fγk(x) approximately with one projection.",5.2. Smooth Optimization,[0],[0]
"The theorem below exhibits the iteration complexity of LoPNAG and reveals the values ofK, γ1 and t1, . . .",5.2. Smooth Optimization,[0],[0]
", tK .",5.2. Smooth Optimization,[0],[0]
"To simplify notations, we let L̄ =",5.2. Smooth Optimization,[0],[0]
"Lf + λLc.
Theorem 5.",5.2. Smooth Optimization,[0],[0]
Suppose Assumptions 1 and 2 hold and f(x) is Lf -smooth and c(x) is Lc-smooth.,5.2. Smooth Optimization,[0],[0]
Let γ1 = 06p,5.2. Smooth Optimization,[0],[0]
"ln 2 , K = dlog2( 0/ )e and tk = σ
1−θ max{λGcp",5.2. Smooth Optimization,[0],[0]
"√ 18 ln 2, √ 12(Lf + λLc) 0/2k−1}
in Algorithm 2, where θ and σ are constants appearing in the local error bound condition.",5.2. Smooth Optimization,[0],[0]
Then f(xK)− f∗ ≤ 2 .,5.2. Smooth Optimization,[0],[0]
"Remark: It is not difficult to show that the total number of iterations is bounded by Õ(1/ 1−θ), which improves the one in Corollary 3 without strong convexity.",5.2. Smooth Optimization,[0],[0]
"If f(x) is a
simple non-smooth function whose proximal mapping can be easily computed (e.g., `1 norm), we can replace step 6 in Algorithm 2 by a proximal mapping to handle f(x), which gives the same convergence result in Theorem 5.",5.2. Smooth Optimization,[0],[0]
An example is presented in Section 7 for compressive sensing with θ = 1/2.,5.2. Smooth Optimization,[0],[0]
One might note that a key condition for developing the theory with reduced projections is Assumption 1 (i).,6. Discussion of Assumption 1 (i),[0],[0]
"Although Mahdavi et al. (2012) has briefly mentioned that the condition can be satisfied for a PSD cone or a Polytope (a bounded polyhedron), their discussion lacks of details in particular on the value of ρ in (2) or (3).",6. Discussion of Assumption 1 (i),[0],[0]
"Below, we discuss the condition in details about three types of constraints.
",6. Discussion of Assumption 1 (i),[0],[0]
Polyhedral constraints.,6. Discussion of Assumption 1 (i),[0],[0]
"First, we show that when c(x) is a polyhedral function, i.e., its epigraph is a polyhedron (not necessarily bounded), the inequality (3) is satisfied.",6. Discussion of Assumption 1 (i),[0],[0]
"To this end, we explore the polyhedral error bound (PEB) condition (Gilpin et al., 2012; Yang & Lin, 2016).",6. Discussion of Assumption 1 (i),[0],[0]
"In particular, if we consider an optimization problem, minx∈Rd h(x), where the epigraph of h(x) is polyhedron.",6. Discussion of Assumption 1 (i),[0],[0]
Let H∗ denote the optimal set and h∗ denote the optimal value of the problem above.,6. Discussion of Assumption 1 (i),[0],[0]
The PEB says that there exists ρ > 0,6. Discussion of Assumption 1 (i),[0],[0]
"such that for any x ∈ Rd
dist(x,H∗) ≤ (h(x)− h∗)/ρ.",6. Discussion of Assumption 1 (i),[0],[0]
"(13)
To show that the inequality (3) holds for a polyhedral function c(·), we can consider the optimization problem minx∈Rd",6. Discussion of Assumption 1 (i),[0],[0]
[c(x)]+.,6. Discussion of Assumption 1 (i),[0],[0]
The optimal set of the above problem is given by H∗ = {x ∈,6. Discussion of Assumption 1 (i),[0],[0]
Rd : c(x) ≤ 0}.,6. Discussion of Assumption 1 (i),[0],[0]
"For any x such that c(x) > 0, let x\ = arg minc(u)≤0 ‖u",6. Discussion of Assumption 1 (i),[0],[0]
− x‖2 be the closest point in the optimal set to x.,6. Discussion of Assumption 1 (i),[0],[0]
"Therefore if c(·) is a polyhedral function so does [c(x)]+, by the PEB condition (13) there exists a ρ > 0 such that ‖x − x\‖ ≤",6. Discussion of Assumption 1 (i),[0],[0]
([c(x)]+ − minx[c(x)]+)/ρ =,6. Discussion of Assumption 1 (i),[0],[0]
[c(x)]+/ρ.,6. Discussion of Assumption 1 (i),[0],[0]
"Let us consider a concrete example, where the problem has a set of affine inequalities",6. Discussion of Assumption 1 (i),[0],[0]
"c>i x − bi ≤ 0, i = 1, . . .",6. Discussion of Assumption 1 (i),[0],[0]
",m. There are two methods to encode this into a single constraint function c(x) ≤ 0.",6. Discussion of Assumption 1 (i),[0],[0]
The first method is to use c(x) = max1≤i≤m,6. Discussion of Assumption 1 (i),[0],[0]
c,6. Discussion of Assumption 1 (i),[0],[0]
>,6. Discussion of Assumption 1 (i),[0],[0]
i x,6. Discussion of Assumption 1 (i),[0],[0]
"− bi, which is a polyhedral function and therefore satisfies (3).",6. Discussion of Assumption 1 (i),[0],[0]
The second method is to use c(x) =,6. Discussion of Assumption 1 (i),[0],[0]
"‖[Cx − b]+‖, where [a]+ = max(0,a) and C = (c1, . . .",6. Discussion of Assumption 1 (i),[0],[0]
", cm)
",6. Discussion of Assumption 1 (i),[0],[0]
>.,6. Discussion of Assumption 1 (i),[0],[0]
"Thus [c(x)]+ = ‖[Cx − b]+‖. The inequality (3) is then guaranteed by Hoffman’s bound and the parameter ρ is given by the minimum non-zero eigenvalue of C>C (Wang & Lin, 2014).",6. Discussion of Assumption 1 (i),[0],[0]
"Note that the projection onto a polyhedron is a linear constrained quadratic programming problem, and the linear optimization over a polyhedron is a linear programming problem.",6. Discussion of Assumption 1 (i),[0],[0]
"Both have polynomial time complexity that would be high if m and d are large (Karmarkar, 1984; Kozlov et al., 1980).
",6. Discussion of Assumption 1 (i),[0],[0]
Quadratic constraint.,6. Discussion of Assumption 1 (i),[0],[0]
"A quadratic constraint can take the form of ‖Ax − y‖2 ≤ τ , where A ∈ Rm×d and y ∈ Rm.",6. Discussion of Assumption 1 (i),[0],[0]
"Such a constraint appears in compressive sensing (Candès & Wakin, 2008)5, where the goal is to reconstruct a sparse high-dimensional vector x from a small number of noisy measurements y =",6. Discussion of Assumption 1 (i),[0],[0]
Ax + ε ∈,6. Discussion of Assumption 1 (i),[0],[0]
"Rm with m d. The corresponding optimization problem is
minx∈Rd ‖x‖1, s.t. ‖Ax− y‖2 ≤",6. Discussion of Assumption 1 (i),[0],[0]
τ.,6. Discussion of Assumption 1 (i),[0],[0]
"(14)
where τ ≥",6. Discussion of Assumption 1 (i),[0],[0]
‖ε‖2 is an upper bound on the magnitude of the noise.,6. Discussion of Assumption 1 (i),[0],[0]
"To check the Assumption 1(i), we note that c(x) = ‖Ax−y‖2−τ and∇c(x) = A>(Ax−y).",6. Discussion of Assumption 1 (i),[0],[0]
Let us consider that A has a full row rank 6 and denote by v =,6. Discussion of Assumption 1 (i),[0],[0]
"Ax − y, then on the boundary c(x) = 0",6. Discussion of Assumption 1 (i),[0],[0]
"we have ‖v‖ = √ τ and
‖A>v‖ ≥ √ τλmin(AA>), where λmin(AA>)",6. Discussion of Assumption 1 (i),[0],[0]
> 0 is the minimum eigenvalue of AA> ∈ Rm×m.,6. Discussion of Assumption 1 (i),[0],[0]
Therefore the Assumption 1(i) is satisfied with ρ = √ τλmin(AA>).,6. Discussion of Assumption 1 (i),[0],[0]
"It is notable that the projection and the linear optimization under the quadratic constraint require solving a quadratic programming problem and therefore could be expensive.
PSD constraint.",6. Discussion of Assumption 1 (i),[0],[0]
A PSD constraintX 0,6. Discussion of Assumption 1 (i),[0],[0]
forX ∈,6. Discussion of Assumption 1 (i),[0],[0]
"Rd×d can be written as an inequality constraint −λmin(X) ≤ 0, where λmin(X) denotes the minimum eigen-value of X .",6. Discussion of Assumption 1 (i),[0],[0]
"The subgradient of c(X) = −λmin(X) when λmin(X) = 0 is given by Conv{−uu>|‖u‖ = 1, Xu = 0}, i.e., the convex hull of the outer products of normalized vectors in the null space of the matrix X .",6. Discussion of Assumption 1 (i),[0],[0]
"In (Yang et al., 2017), we show that if the dimension of the null space of X is r with 1 ≤ r ≤ d, the norm of the subgradient of c(X) on the boundary c(X) = 0 is lower bounded by ρ = 1√
r ≥ 1√ d .
",6. Discussion of Assumption 1 (i),[0],[0]
"Finally, we note that computing a subgradient of [c(X)]+ only needs to compute one eigen-vector corresponding to the smallest eigen-value.",6. Discussion of Assumption 1 (i),[0],[0]
"In contrast, both projection and linear optimization under a PSD constraint could be very expensive for high-dimensional problems.",6. Discussion of Assumption 1 (i),[0],[0]
"In particular, the projection onto a PSD domain needs to conduct a singular value decomposition.",6. Discussion of Assumption 1 (i),[0],[0]
The linear optimization over a PSD cone is ill-posed due to that PSD cone is not compact (the solution is either 0 or infinity).,6. Discussion of Assumption 1 (i),[0],[0]
One may add an artificial constraint on the upper bound of the eigen-values.,6. Discussion of Assumption 1 (i),[0],[0]
"According to (Jaggi, 2013), the time complexity for solving this linear optimization problem approximately up to an accuracy level ′ is O(Nd1.5/ ′2.5) with N being the number of non-zeros in the gradient and ′ decreasing iteratively required in the Frank-Wolfe method, which could be much more expensive especially for high-dimensional problems and in later iterations than computing the first eigen-pairs at each iteration in our methods.
",6. Discussion of Assumption 1 (i),[0],[0]
"5Here we use the square constraint to make it a smooth function so that the proposed algorithms for smooth optimization are applicable by using proximal gradient mapping to handle the `1 norm.
",6. Discussion of Assumption 1 (i),[0],[0]
6which is reasonable because m d.,6. Discussion of Assumption 1 (i),[0],[0]
We first consider a compressive sensing problem in (14).,7.1. Compressive Sensing,[0],[0]
"Becker et al. (2011) proposed an optimization algorithm based on the Nesterov’s smoothing and the Nesterov’s optimal method for the smoothed problem, known as NESTA.",7.1. Compressive Sensing,[0],[0]
It needs to perform the projection into the domain ‖Ax − y‖2 ≤ τ,7.1. Compressive Sensing,[0],[0]
at every iteration and has an iteration complexity of O(1/ ).,7.1. Compressive Sensing,[0],[0]
"In contrast, the presented algorithm with only one projection in Section 4.2 using Nesterov’s accelerated proximal gradient method (Beck & Teboulle, 2009) to solve the unconstrained problem enjoys an iteration complexity ofO(1/ ).",7.1. Compressive Sensing,[0],[0]
"Moreover, we present a theorem below showing that the problem (14) satisfies the local error bound condition with θ = 1/2, and hence the presented LoPNAG enjoys an Õ(1/ √ ) iteration complexity with only a logarithmic number of projections.
",7.1. Compressive Sensing,[0],[0]
Theorem 6.,7.1. Compressive Sensing,[0],[0]
"Let f(x) = ‖x‖1, c(x) =",7.1. Compressive Sensing,[0],[0]
"‖Ax − y‖2 − τ , Ω∗ denote the optimal set and f∗ be the optimal solution to (14).",7.1. Compressive Sensing,[0],[0]
"Assume that there exists x0 such that ‖Ax0−y‖2 < τ and 0 6∈ Ω∗. Then for any > 0, x ∈ Rd such that c(x) ≤ 0 and f(x) ≤ f∗ + , there exists 0",7.1. Compressive Sensing,[0],[0]
< σ,7.1. Compressive Sensing,[0],[0]
< ∞,7.1. Compressive Sensing,[0],[0]
"such that dist(x,Ω∗) ≤ σ(f(x) − f∗)1/2.",7.1. Compressive Sensing,[0],[0]
"Hence, LoPNAG can have an iteration complexity of Õ(1/ √ ) with only O(log(1/ )) projections.
",7.1. Compressive Sensing,[0],[0]
"Next, we demonstrate the effectiveness of the LoPNAG for solving the compressive sensing problem in (14) by comparing with NESTA.",7.1. Compressive Sensing,[0],[0]
We generate a synthetic data for testing.,7.1. Compressive Sensing,[0],[0]
"In particular, we generate a random measurement matrix A ∈ Rm×d with m = 1000 and d = 5000.",7.1. Compressive Sensing,[0],[0]
"The entries of the matrix A are generated independently with the uniform distribution over the interval [−1,+1].",7.1. Compressive Sensing,[0],[0]
The vector x∗ ∈ Rd is generated with the same distribution at 100 randomly chosen coordinates.,7.1. Compressive Sensing,[0],[0]
The noise ε ∈,7.1. Compressive Sensing,[0],[0]
Rm is a dense vector with independent random entries with the uniform distribution over the interval,7.1. Compressive Sensing,[0],[0]
"[−ζ, ζ], where ζ is the noise magnitude and is set to 0.01.",7.1. Compressive Sensing,[0],[0]
"Finally the vector y was obtained as y = Ax∗ + ε.
",7.1. Compressive Sensing,[0],[0]
We use the Matlab package of NESTA 7.,7.1. Compressive Sensing,[0],[0]
"For fair comparison, we also use the projection code in the NESTA package for conducting projection.",7.1. Compressive Sensing,[0],[0]
"To handle the unknown smoothness parameter in the proposed algorithm, we use the backtracking technique (Beck & Teboulle, 2009).",7.1. Compressive Sensing,[0],[0]
The parameter γ is initially set to 0.001 and decreased by half every 5000 iterations after a projection and the target smoothing parameter in NESTA is set to 10−5.,7.1. Compressive Sensing,[0],[0]
"For the value of λ in LoPNAG, we tune it from its theoretical value to several smaller values and choose the one that yields the fastest convergence.",7.1. Compressive Sensing,[0],[0]
"We report the results in Table 7.1, which include different number of iterations, the corresponding
7http://statweb.stanford.edu/˜candes/ nesta/
number of projections, the recovery error of the found solution compared to the underlying true sparse solution, the objective value (i.e., the `1 norm of the found solution) and the running time.",7.1. Compressive Sensing,[0],[0]
Note that each iteration of NESTA requires two projections because it maintains two extra sequence of solutions.,7.1. Compressive Sensing,[0],[0]
"From the results, we can see that LoPNAG converges significantly faster than NESTA.",7.1. Compressive Sensing,[0],[0]
"Even with only one projection, we are able to obtain a better solution than that of NESTA after running 10000 iterations.",7.1. Compressive Sensing,[0],[0]
"Consider the following distance metric learning problem:
min A 0
1 2|E| ∑
(i,j)∈E
(1−yij−‖xi−xj‖2A)2 + τ‖A‖off1 , (15)
where E denotes all pairs of training examples, yij = 1 indicates xi,xj belong to the same class and yij = −1 indicates they belong to different classes, ‖z‖2A = z>Az and ‖A‖off1 = ∑ i 6=j |Aij |.",7.2. High-dimensional Distance Metric Learning,[0],[0]
We note that such a formulation is useful for high dimensional problems due to the `1 regularizer.,7.2. High-dimensional Distance Metric Learning,[0],[0]
"A similar formulation with different forms of loss function has been adopted in literature (Qi et al., 2009).",7.2. High-dimensional Distance Metric Learning,[0],[0]
We consider the square loss because it gives us faster convergence with a logarithmic number of projections by LoPGD.,7.2. High-dimensional Distance Metric Learning,[0],[0]
"Due to the presence of the non-smooth PSD constraint and the `1 regularizer, Nesterov’s accelerated proximal gradient methods can not be applied efficiently to solving (15) and the augmented unconstrained problem.",7.2. High-dimensional Distance Metric Learning,[0],[0]
"Nevertheless, we can apply the proposed LoPGD method for solving the problem with a logarithmic number of projections.",7.2. High-dimensional Distance Metric Learning,[0],[0]
"Regarding the constant θ in the local error bound condition for (15), it still remains an open problem.",7.2. High-dimensional Distance Metric Learning,[0],[0]
"Nonetheless, a local error bound condition with θ = 0.5 might be established under certain regularity condition of the problem (Zhou & So, 2015; Cui et al., 2017).",7.2. High-dimensional Distance Metric Learning,[0],[0]
"For example, Cui et al. (2017) provided a direct analysis of a local error bound condition with θ = 0.5 for a class of constrained convex symmetric matrix optimization problems
regularized by nonsmooth spectral functions (including the indicator function of a PSD constraint).",7.2. High-dimensional Distance Metric Learning,[0],[0]
"They established sufficient conditions (Theorem 16) for a local error bound condition with θ = 0.5 to hold, which reduces to a regularity condition for (15) depending on the optimal solutions of the problem.",7.2. High-dimensional Distance Metric Learning,[0],[0]
"A thorough analysis of the regularity condition is much more involved and left as an open problem.
",7.2. High-dimensional Distance Metric Learning,[0],[0]
"Next, we demonstrate the empirical performance of LoPGD for solving (15).",7.2. High-dimensional Distance Metric Learning,[0],[0]
"We use the colon-cancer data available on libsvm web portal, which has 2000 features and 62 examples.",7.2. High-dimensional Distance Metric Learning,[0],[0]
Fourty examples are used as training examples to generate 780 pairs to learn the distance metric.,7.2. High-dimensional Distance Metric Learning,[0],[0]
The regularization parameter is set to τ = 0.001.,7.2. High-dimensional Distance Metric Learning,[0],[0]
"We compare LoPGD, gradient descent method with only one projection (referred to as OPGD), and standard projected GD (referred to PGD).",7.2. High-dimensional Distance Metric Learning,[0],[0]
"The step size in PGD and OPGD is set to η0/ √ t, where t is the iteration index.",7.2. High-dimensional Distance Metric Learning,[0],[0]
We use the same tuned initial step size for all algorithms.,7.2. High-dimensional Distance Metric Learning,[0],[0]
The number of iterations per-epoch in LoPGD is set to 1000.,7.2. High-dimensional Distance Metric Learning,[0],[0]
The penalization parameter λ in both OPGD and LoPGD is tuned and set to 10.,7.2. High-dimensional Distance Metric Learning,[0],[0]
"In Table 2, we report the objective values, the #of iterations/projections, and running time across the first 8000 iterations.",7.2. High-dimensional Distance Metric Learning,[0],[0]
We can see that LoPGD converges dramatically faster than PGD and also much faster than OPGD.,7.2. High-dimensional Distance Metric Learning,[0],[0]
We have developed a general theory of optimization with only one projection for a family of inequality constrained convex optimization problems.,8. Conclusion,[0],[0]
It yields an improved iteration complexity for smooth optimization compared with non-smooth optimization.,8. Conclusion,[0],[0]
"By exploring the local error bound condition, we further develop new algorithms with a logarithmic number of projections and achieve better convergence for both smooth and non-smooth optimization without strong convexity assumption.",8. Conclusion,[0],[0]
Applications in compressive sensing and distance metric learning demonstrate the effectiveness of the proposed improved algorithms.,8. Conclusion,[0],[0]
We are grateful to all anonymous reviewers for their helpful comments.,Acknowledgements,[0],[0]
"T. Yang is partially supported by National Science Foundation (IIS-1463988, IIS-1545995).",Acknowledgements,[0],[0]
L. Zhang thanks the support from NSFC (61603177) and JiangsuSF (BK20160658).,Acknowledgements,[0],[0]
"This paper focuses on convex constrained optimization problems, where the solution is subject to a convex inequality constraint.",abstractText,[0],[0]
"In particular, we aim at challenging problems for which both projection into the constrained domain and a linear optimization under the inequality constraint are time-consuming, which render both projected gradient methods and conditional gradient methods (a.k.a.",abstractText,[0],[0]
the Frank-Wolfe algorithm) expensive.,abstractText,[0],[0]
"In this paper, we develop projection reduced optimization algorithms for both smooth and non-smooth optimization with improved convergence rates under a certain regularity condition of the constraint function.",abstractText,[0],[0]
We first present a general theory of optimization with only one projection.,abstractText,[0],[0]
"Its application to smooth optimization with only one projection yields O(1/ ) iteration complexity, which improves over the O(1/ ) iteration complexity established before for nonsmooth optimization and can be further reduced under strong convexity.",abstractText,[0],[0]
Then we introduce a local error bound condition and develop faster algorithms for non-strongly convex optimization at the price of a logarithmic number of projections.,abstractText,[0],[0]
"In particular, we achieve an iteration complexity of Õ(1/ 2(1−θ)) for non-smooth optimization and Õ(1/ 1−θ) for smooth optimization, where θ ∈ (0, 1] appearing the local error bound condition characterizes the functional local growth rate around the optimal solutions.",abstractText,[0],[0]
Novel applications in solving the constrained `1 minimization problem and a positive semi-definite constrained distance metric learning problem demonstrate that the proposed algorithms achieve significant speed-up compared with previous algorithms.,abstractText,[0],[0]
"The University of Iowa, Iowa City, IA 52242, USA National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China.",abstractText,[0],[0]
Correspondence to: Tianbao Yang,abstractText,[0],[0]
<tianbao-yang@uiowa.edu>.,abstractText,[0],[0]
"This is the long version of our paper appearing in the Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",abstractText,[0],[0]
Copyright 2017 by the author(s).,abstractText,[0],[0]
A Richer Theory of Convex Constrained Optimization with Reduced Projections and Improved Rates,title,[0],[0]
"ar X
iv :1
91 2.
01 70
6v 2
[ cs
.L G
] 3
M ar
2 02
0
In this paper, we reproduce the experiments of Artetxe et al. (2018b) regarding the robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings. We show that the reproduction of their method is indeed feasible with some minor assumptions. We further investigate the robustness of their model by introducing four new languages that are less similar to English than the ones proposed by the original paper. In order to assess the stability of their model, we also conduct a grid search over sensible hyperparameters. We then propose key recommendations that apply to any research project in order to deliver fully reproducible research. Keywords: Machine Learning, Unsupervised Learning, Word Alignment, Cross-Lingual Word Embeddings, Reproducibility",text,[0],[0]
"The cross-lingual mapping of word embeddings is a prob-
lem that has been studied more thoroughly with the rise of distributed representations induced from neural network architectures (Mikolov et al., 2013b).",1. Introduction,[0],[0]
"The goal of this task is to induce automatically a word-to-word translation dictionary D ∈ R|Vs|×|Vt| where D[i, j] = 1 means that the i-th word from the source vocabulary Vs is a translation of the j-th word in the target vocabulary Vt.",1. Introduction,[0],[0]
The data used to learn the mapping from two different languages is two sets of word embeddings,1. Introduction,[0],[0]
Xs and Xt corresponding to the vectors of the source and the target language respectively.,1. Introduction,[0],[0]
"The mapping from one language space to the other is usually done with a projection matrix Wt which projects the embeddings of the target language in the same space as the source language (or vice versa), i.e. Xs ≈ XtWt.",1. Introduction,[0],[0]
There are several methods available to achieve such a mapping depending on the resources in hand.,1. Introduction,[0],[0]
"Given a dataset of parallel word-aligned data, supervised mapping-based approaches are amongst the most popular to date (Mikolov et al., 2013a; Dinu and Baroni, 2014; Gouws and Søgaard, 2015).",1. Introduction,[0],[0]
"Several unsupervised methods (Yang et al., 2018; Conneau et al., 2017) based on Generative Adversarial Neural Networks (GANs) (Goodfellow et al., 2014) have also been proposed in the case where no seed dictionary is available.",1. Introduction,[0],[0]
The method that we reproduce is the work of Artetxe et al. (2018b) (referenced as ”the authors”).,1. Introduction,[0],[0]
It also falls in the unsupervised setting but is based on distances between nearest neighbors to build the initial seed dictionary.,1. Introduction,[0],[0]
"We refer the reader to the survey of Ruder et al. (2019) for more details, which provides an extensive overview of the different methods for learning cross-lingual mappings between two different word embedding spaces.",1. Introduction,[0],[0]
"On another hand, delivering reproducible research is too often an underestimated concern.",1. Introduction,[0],[0]
"The fast pace of the research community makes the verification of every submit-
†Authors contributed equally to this work.
",1. Introduction,[0],[0]
"ted paper barely possible, especially in a boiling community such as natural language processing.",1. Introduction,[0],[0]
"Fortunately, we see the rise of different challenges1,2,3 that emphasize the importance of supporting the proposed results with an official code implementation as well as the corresponding dataset.",1. Introduction,[0],[0]
It has even been mandatory to provide the source code and the detailed procedure to obtain the same results like the ones claimed in the paper in the NeurIPS Reproducibility Challenge4.,1. Introduction,[0],[0]
"In light of this quest for reproducibility, we hereby propose to reproduce the paper from Artetxe et al. (2018b) in the context of REPROLANG 2020 by also providing the stammering of a methodology for delivering reproducible experiments.",1. Introduction,[0],[0]
"Even though this algorithm relies on a stochastic component, we can reproduce the results issued from this algorithm and further analyze its behavior.",1. Introduction,[0],[0]
"It is therefore important to provide a clean, readable codebase that supports a clear and concise paper.",1. Introduction,[0],[0]
We begin in section 2.,1. Introduction,[0],[0]
with the problem statement and section 3.,1. Introduction,[0],[0]
with the presentation of the analyzed algorithm.,1. Introduction,[0],[0]
We then present what we reproduced from the original paper’s results as well as how we obtained our results in section 4.,1. Introduction,[0],[0]
We provide recommendations regarding the techniques used to obtain these results and their applicability to other research projects in section 5.,1. Introduction,[0],[0]
and close the analysis with an assessment of the algorithm’s robustness in section 6.,1. Introduction,[0],[0]
We also publish our code online as required 5.,1. Introduction,[0],[0]
"Word vectors, often called word embeddings, are distributed representations derived from a textual corpus (Mikolov et al., 2013b; Pennington et al., 2014).",2. Problem Statement,[0],[0]
"The dimension d of these representations often spans from 100 to
1 https://reproducibility-challenge.github.io/iclr_2019/ 2 https://aaai.org/Conferences/AAAI-19/ws19workshops/#ws16 3 http://rescience.github.io/ 4 https://reproducibility-challenge.github.io/neurips2019/ 5 https://gitlab.com/nicolasgarneau/vecmap/
using this commit: b1abbd26
1,000.",2. Problem Statement,[0],[0]
One key outcome of learning these word representations is that it associates words in a vocabulary V with a similar meaning (appearing in a similar context) with similar distribution vectors.,2. Problem Statement,[0],[0]
"This set of representations, often called the embedding matrix X ∈ R|V|×d serves as the input for many models in natural language understanding applications such as text classification and machine translation.",2. Problem Statement,[0],[0]
The paper on which we conduct our reproducibility experiment tackles the task of word vectors space alignment.,2. Problem Statement,[0],[0]
Given two sets of source and target embedding matrices,2. Problem Statement,[0],[0]
"Xs ∈ R|Vs|×d and Xt ∈ R|Vt|×d induced from two different textual corpora, one tries to find the mappings Ws ∈ Rd×d and Wt ∈ Rd×d such that XsWs ≈ XtWt.",2. Problem Statement,[0],[0]
The vocabularies Vs and Vt can be of different sizes.,2. Problem Statement,[0],[0]
"Usually, only a subset of the n most frequent words is used for the alignment.",2. Problem Statement,[0],[0]
"The work of the authors strictly focuses on the task of unsupervised bilingual dictionary induction, hence aligning the word vector space of a source language with one of a target language without word-aligned data.",2. Problem Statement,[0],[0]
"Essentially, the original paper’s approach tries to find a good initial solution D0 by aligning word vectors from the source and the target language that have a similar distribution.",2. Problem Statement,[0],[0]
"Their motivation, referred to in the literature as the isometry assumption, is that monolingual word embedding spaces are approximately isomorphic (Vulić et al., 2019).",2. Problem Statement,[0],[0]
"They demonstrated for example that the vector of the word “two” in English has a similar distribution as the word vector “due” in Italian and will be different from the distribution of the word “cane”, also in Italian.",2. Problem Statement,[0],[0]
"Once the initial dictionary D0 is induced from the unsupervised procedure, a self-supervised iteration loop is invoked to refine the mapping from the source to the target language.",2. Problem Statement,[0],[0]
The whole algorithm is further analyzed in the following section.,2. Problem Statement,[0],[0]
"In this section, we detail the four different steps of the algorithm proposed by Artetxe et al. (2018b).",3. The Proposed Algorithm,[0],[0]
"To directly quote the original paper, the first step of the proposed method is to length normalize the word embeddings Xs and Xt, then mean center each dimension and finally, length normalize again.",3.1. Step 1: Embedding Normalization,[0],[0]
The next step is a component introduced by the authors in the original paper: the unsupervised seed dictionary initialization.,3.2. Step 2: Fully Unsupervised Initialization,[0],[0]
"To build the initial dictionary D0, we begin by applying multiple transformations to both Xs and Xt, described as follows.",3.2. Step 2: Fully Unsupervised Initialization,[0],[0]
"Given one language’s embedding matrix X, we start by computing its Singular Value Decomposition, namely USVT = X. We then compute√ M = USUT where M = XXT = US2UT corresponds to the similarity matrix for the given language’s embedding matrix.",3.2. Step 2: Fully Unsupervised Initialization,[0],[0]
Each row (word embedding) of the yielded matrices √ Ms and √ Mt is then sorted independently of other rows and we apply the embedding normalization described in subsection 3.1.,3.2. Step 2: Fully Unsupervised Initialization,[0],[0]
A similarity matrix between the two sets of languages is computed K = √ Ms √ Mt T .,3.2. Step 2: Fully Unsupervised Initialization,[0],[0]
"It is
important to note that before the above steps, a vocabulary cutoff of n = 4, 000 is applied, yielding Xs,Xt ∈ Rn×d.",3.2. Step 2: Fully Unsupervised Initialization,[0],[0]
"Finally, D0 is built by applying Cross-domain Similarity Local Scaling (CSLS) (Conneau et al., 2017) retrieval onK and bidirectional dictionary induction D0 = DXs→Xt + DXs←Xt .",3.2. Step 2: Fully Unsupervised Initialization,[0],[0]
Further details on both CSLS and the bidirectional dictionary induction can be found in ??,3.2. Step 2: Fully Unsupervised Initialization,[0],[0]
The initial dictionary D0 is rarely a good solution in itself.,3.3. Step 3: Robust Self-Learning,[0],[0]
"To overcome this, the authors proposed a self-learning algorithm that iteratively refines the previously induced dictionary.",3.3. Step 3: Robust Self-Learning,[0],[0]
"Hartmann et al. (2019) specifically demonstrated that without this algorithm, the unsupervised dictionary induction is worse than vanilla GAN methods such as Conneau et al. (2017).",3.3. Step 3: Robust Self-Learning,[0],[0]
The algorithm comprises two main steps done iteratively until convergence.,3.3. Step 3: Robust Self-Learning,[0],[0]
The first step is to compute the optimal orthogonal mapping maximizing the objective function,3.3. Step 3: Robust Self-Learning,[0],[0]
"6
argmax Ws,Wt
∑
i
∑
j
Dij
( (Xs[i, ∗]·Ws)·(Xt[j, ∗]·Wt) )",3.3. Step 3: Robust Self-Learning,[0],[0]
"(1)
for the current dictionary Dt at iteration t.",3.3. Step 3: Robust Self-Learning,[0],[0]
The second step is then to apply nearest neighbor retrieval over the similarity matrix of the mapped embeddings XsWsW T t Xt to yield the next seed dictionary Dt+1 for the next iteration.,3.3. Step 3: Robust Self-Learning,[0],[0]
"In order to make the self-learning more robust and achieve better performance, the authors also propose four improvements to the above algorithm: stochasticity in the dictionary induction, a frequency-based vocabulary cutoff, usage of the CSLS instead of the nearest neighbor to compute the optimal dictionary and usage of a bidirectional approach in the dictionary induction.",3.3. Step 3: Robust Self-Learning,[0],[0]
"The frequency-based vocabulary cutoff only retains the top n = 20, 000 most frequent words from both embedding matrices.",3.3. Step 3: Robust Self-Learning,[0],[0]
"Done after the unsupervised seed dictionary initialization and before the first self-learning step, the objective of this step is to increase the computing efficiency and reduce the complexity of the optimization problem.",3.3. Step 3: Robust Self-Learning,[0],[0]
"The proposed value of n was given without much explanation, only saying that it was working well in practice.",3.3. Step 3: Robust Self-Learning,[0],[0]
We chose to further analyze the impact of different values of n in subsection 6.2.,3.3. Step 3: Robust Self-Learning,[0],[0]
The authors proposed a stochastic feature that may be vital for the convergence of the algorithm with some languages.,3.3. Step 3: Robust Self-Learning,[0],[0]
They randomly keep some elements in the similarity matrix yielded at the end of each iteration with a probability of p while the others are ignored.,3.3. Step 3: Robust Self-Learning,[0],[0]
This encourages the exploration of the search space and allows the dictionary to greatly vary between two iterations when p is small.,3.3. Step 3: Robust Self-Learning,[0],[0]
"As the algorithm starts to converge, the value of p gradually grows to the maximum value of 1.",3.3. Step 3: Robust Self-Learning,[0],[0]
The initial value of p is set to p0 = 0.1 and it is multiplied by pfactor = 2 every time the objective function (1) didn’t improve of more than a delta value of ǫ = 10−6 in the last 50 iterations.,3.3. Step 3: Robust Self-Learning,[0],[0]
"We also chose to further analyze the impact of the (p0, pfactor) combination in subsection 6.2.
",3.3. Step 3: Robust Self-Learning,[0],[0]
"6Here X[i, ∗] denotes the i-th row of the matrix X
Typically, to compute the optimal dictionary over the mapped embeddings, we use the nearest neighbor retrieval from the source language into the target language but Dinu and Baroni (2014) showed that this approach suffers from the hubness problem.",3.3. Step 3: Robust Self-Learning,[0],[0]
"This phenomenon, where many points are universal neighbours to many other points, is an intrinsic problem of high-dimensional spaces (Radovanović et al., 2010).",3.3. Step 3: Robust Self-Learning,[0],[0]
The authors adopted the CSLS introduced by Conneau et al. (2017) which specifically tackles this problem.,3.3. Step 3: Robust Self-Learning,[0],[0]
This approach’s idea is to penalize universal neighbors by subtracting each word’s average cosine similarity to its k nearest neighbors in the other language from the cosine similarity result between words from the target and source languages.,3.3. Step 3: Robust Self-Learning,[0],[0]
"Since the value used by the authors is k = 10, as per Conneau et al. (2017)’s recommendation, we again chose to further analyze the impact of different values of k in subsection 6.2.",3.3. Step 3: Robust Self-Learning,[0],[0]
"to grasp a better understanding of its impact.
",3.3. Step 3: Robust Self-Learning,[0],[0]
The authors also proposed to use a bidirectional approach for the dictionary induction.,3.3. Step 3: Robust Self-Learning,[0],[0]
"This improvement is based on the intuition that, when the dictionary is induced from the source into the target language, some of the words may not be present or some may occur numerous times.",3.3. Step 3: Robust Self-Learning,[0],[0]
"The authors claim that those target words occurring multiple times may cause a problem of local optima since they may act as an aggregation hub, making it much more difficult to escape from that undesired solution.",3.3. Step 3: Robust Self-Learning,[0],[0]
"The bidirectional approach thus uses the concatenation of both mappings, source to target and target to source as the dictionary D, namely D = DXs→Xt +DXs←Xt .",3.3. Step 3: Robust Self-Learning,[0],[0]
"As explained in Artetxe et al. (2018a), re-weighting the parameters of the target language’s embeddings according to their cross-correlation is beneficial and greatly improved the quality of the induced dictionary.",3.4. Step 4: Symmetric Re-Weighting,[0],[0]
"They also showed that using re-weighting and self-learning didn’t seem to work well together since it provokes an accentuation of the local optima problem and discourages the exploration of other possible better regions, which is most of the problem addressed by the four improvements proposed by the authors in the self-learning step.",3.4. Step 4: Symmetric Re-Weighting,[0],[0]
"As a result, this step is done only once after the self-learning loop converged.",3.4. Step 4: Symmetric Re-Weighting,[0],[0]
"However, unlike Artetxe et al. (2018a) which applied the re-weighting on either the source or the target language, the authors applied the re-weighting to both languages.",3.4. Step 4: Symmetric Re-Weighting,[0],[0]
"Using the symmetric approach improves the performance of the system, but it’s not clear why they chose to use a symmetric reweighting instead of a target only re-weighting as proposed in Artetxe et al. (2018a).",3.4. Step 4: Symmetric Re-Weighting,[0],[0]
"We focused on reproducing the results of the entire ablation study of Table 4, as well as the “Proposed Method” line from Tables 2 and 3 of the original paper of Artetxe et al. (2018b).",4. Reproducing the Results,[0],[0]
"The results comprise four different languages, Deutsch (DE), Spanish (ES), Finnish (FI) and Italian (IT).",4. Reproducing the Results,[0],[0]
"Since we did not have access to the dataset of Zhang et al. (2017), we could not reproduce the results of
Table 1.",4. Reproducing the Results,[0],[0]
We thus discuss in this section the results we obtained for the proposed method and the ablation study with some issues we faced.,4. Reproducing the Results,[0],[0]
"To reproduce the results of the original paper, we directly used their publicly available codebase7, instead of completely reimplementing their algorithm on our side.",4.1. Original Results,[0],[0]
"As reported in Table 1, we were able to reproduce the original results with their codebase within a negligible difference, most likely due to the stochastic nature of the dictionary induction of the algorithm.",4.1. Original Results,[0],[0]
"Like in the original paper, we provide the best and average (avg) accuracy for every language pair as well as its average runtime (t).",4.1. Original Results,[0],[0]
"We performed 25 runs per target language and, instead of listing the number of successful runs (where accuracy > 5 %), we present the success rate (s).",4.1. Original Results,[0],[0]
"We can see that, as expected, we have a success rate of 1.0.",4.1. Original Results,[0],[0]
The execution time highly differs from the original paper.,4.1. Original Results,[0],[0]
"It is important to note that even by using the same hardware as the authors (Nvidia Titan Xp GPU), the average runtime for each language is 2 to 4 times longer than the actual runtime reported in the paper.",4.1. Original Results,[0],[0]
It is an important factor when comes the time to reproduce the results if we have a limited amount of resources at hand.,4.1. Original Results,[0],[0]
Another thing to keep in mind when using this algorithm is that the frequency-based vocabulary cutoff assumes that the word vectors have been saved in the embedding text file ordered by their frequency in the training corpus.,4.1. Original Results,[0],[0]
"While this is the default behavior of the Fasttext library (Mikolov et al., 2018), it may not always be the case.",4.1. Original Results,[0],[0]
Our reproduction results of the ablation study in the original paper are reported in Table 2.,4.2. Ablation Study,[0],[0]
"Amongst other things, we note that the accuracy results we obtained are all within the 95 % confidence interval given by our 25 runs, with the only exceptions being the unsupervised initialization ablation and the runtimes.",4.2. Ablation Study,[0],[0]
"Regarding the unsupervised initialization ablation, we initially faced the challenge of having to reproduce the random seed dictionary initialization that was mentioned in the paper yet missing in the code.",4.2. Ablation Study,[0],[0]
We therefore explored two very plausible approaches to random initializations: one where each word of the smallest language is randomly assigned a word from the biggest language (referred to as ‘Random Complete’) and one where a cutoff is done on both languages before the random pairing (referred to as ‘Random Cutoff ’).,4.2. Ablation Study,[0],[0]
"The first thing to note is that both our tested random initializations reach convergence between 10 and 30 % of the time, in contrast with the authors’ 0 % success rate.",4.2. Ablation Study,[0],[0]
"Also, when runs beginning with random initialization are successful, the final performance of the algorithm is the same as the one with the full system.",4.2. Ablation Study,[0],[0]
"This hints that the initial seed dictionary used, whether obtained by unsupervised or random initialization, only affects the difficulty of the optimization problem but not the retrieved solution.",4.2. Ablation Study,[0],[0]
"Our results also showed great differences in the algorithm’s runtimes, even though we used the same graphics card as
7https://github.com/artetxem/vecmap
the original paper.",4.2. Ablation Study,[0],[0]
"We also point out that not all ablation study configurations can be run with the same compute power, i.e. when removing the vocabulary cutoff parameter, the matrices no longer fit inside a GPU’s memory and we have to prepare additional RAM space and CPU resources to run the script.",4.2. Ablation Study,[0],[0]
"However, when we attempted to reproduce the frequency-based vocabulary cutoff ablation where n is set to 100k, we were unable to obtain a single run to complete even after 3 days of computations.",4.2. Ablation Study,[0],[0]
This is why we left this line dashed out in Table 2.,4.2. Ablation Study,[0],[0]
"While it may seem trivial to use an official implementation to reproduce the results of a paper, the reality is that it often requires a good amount of human effort to run a complete reproduction of the results.",4.3. Reproduction Issues,[0],[0]
The latter is what happened with us when following the given instructions to obtain the reported results.,4.3. Reproduction Issues,[0],[0]
"While the code did execute and complete when using the ACL 2018 setting, we had accuracies below the ones expected for each language pair (5 to 7 % below).",4.3. Reproduction Issues,[0],[0]
"It is only after the further analysis that we found that the provided setting did not include the CSLS procedure, explaining the different results.",4.3. Reproduction Issues,[0],[0]
"After eventually managing to reproduce the reported full algorithm results, we hit another breaking point: the ablation study was not included in the source code.",4.3. Reproduction Issues,[0],[0]
"While almost all ablations (except the random initialization, as per subsection 4.2.) could be run from the provided implementation, no script was given to sequentially execute all ablation tests and report the results.",4.3. Reproduction Issues,[0],[0]
We propose some key recommendations on how to address these issues and facilitate reproducibility and reusability when providing an official implementation with a paper in the section 5.,4.3. Reproduction Issues,[0],[0]
Reproducing the model and the results of an original paper can be quite a hassle.,5. Recommendations,[0],[0]
"In this section, we provide a general framework applicable to any Machine Learning project that will help researchers deliver highly reproducible experiments.",5. Recommendations,[0],[0]
We begin with minor recommendations regarding the source code provided by the authors.,5. Recommendations,[0],[0]
We then propose a way to host the dataset and a tool that handles the download and the upload of it.,5. Recommendations,[0],[0]
"Since another very important thing to consider when running experiments is to log them all, we hereby propose to automate the logging of the experimentations as well as the gathering of the results.",5. Recommendations,[0],[0]
These steps considerably facilitate the automatic generation of tables and graphs as was required for this challenge.,5. Recommendations,[0],[0]
"Finally, we recommend to use a 100 % reproducible environment to
run the experiments, hence to Dockerize the whole project (Cito and Gall, 2016; Hartmann et al., 2019).",5. Recommendations,[0],[0]
One thing that every research codebase should have is a list of the external libraries needed to execute the code.,5.1. Codebase Recommendations,[0],[0]
"In the case of a Python project, it should have a requirements.txt file.",5.1. Codebase Recommendations,[0],[0]
This file holds all the python dependencies needed to run the project’s main script.,5.1. Codebase Recommendations,[0],[0]
"We thus prepared such a file in our codebase since the original codebase was missing one.
",5.1. Codebase Recommendations,[0],[0]
"When running experiments, it is important to reduce the number of actions a human needs to perform in order to obtain the final results.",5.1. Codebase Recommendations,[0],[0]
"We then made the training and the evaluation of the algorithm, originally in two separate files, into one single script.",5.1. Codebase Recommendations,[0],[0]
"This also removes the writing of the mapped embeddings on disk which vastly reduces the amount of disk space needed.
",5.1. Codebase Recommendations,[0],[0]
"In the same line of ideas, the default hyperparameters of the algorithm should be the ones that reproduce the main results of the paper (Table 2 in Artetxe et al. (2018b)).",5.1. Codebase Recommendations,[0],[0]
This is why we proposed to explicitly code not only the full algorithm but also every ablation configuration as Experiment classes within our codebase.,5.1. Codebase Recommendations,[0],[0]
"This abstraction in our source code enables us to easily provide scripts that reproduce our ablation study as well as the hyperparameter grid search conducted in subsection 6.2.
",5.1. Codebase Recommendations,[0],[0]
Coordination between a paper’s key sections and its official implementation is also a concern we wish to raise.,5.1. Codebase Recommendations,[0],[0]
"When reading a scientific paper, if one wishes to have a closer look at the implementation of a particular algorithm step or section, one should be able to do so without having to understand the entire codebase.",5.1. Codebase Recommendations,[0],[0]
This is why we created an exact correspondence between step names in the original paper like ’CSLS Retrieval’ and ’Robust Self-Learning’ and function names in our source code.,5.1. Codebase Recommendations,[0],[0]
We argue this name mapping should be easy to implement at the end of the delivery of a research project and that it contributes significantly towards easier reusability of the delivered implementation.,5.1. Codebase Recommendations,[0],[0]
Properly handling the benchmark dataset is often an underestimated point.,5.2. Dataset Handling,[0],[0]
"In an iterative and collaborative setting, it is important to efficiently host (when possible) and version the data.",5.2. Dataset Handling,[0],[0]
We thus recommend a tool designed to handle those two elements flawlessly; Data Version Control (DVC).,5.2. Dataset Handling,[0],[0]
"Similar to standard Version Control Systems (VCS)
like Git8, DVC tracks the different state of the dataset during development as well as in between the processing steps before obtaining the final results of the model.",5.2. Dataset Handling,[0],[0]
"While there is no need in our particular reproducibility challenge context to track the different states of the dataset over time, it definitely requires an efficient collaboration environment, hence our choice to use the Python DVC library 9 with Amazon S3 as the remote repository.",5.2. Dataset Handling,[0],[0]
"DVC was designed with large data files in mind, meaning gigabytes or even hundreds of gigabytes in file size.",5.2. Dataset Handling,[0],[0]
"In our case, the original dataset takes up to 6 Gigabytes.",5.2. Dataset Handling,[0],[0]
The previous way of retrieving the dataset over the network with a standard 20 Mbits/sec internet connexion took up to an hour to complete (including uncompressing the data).,5.2. Dataset Handling,[0],[0]
Using DVC reduced the retrieval time of the dataset to 3 minutes over the network with the same internet connexion.,5.2. Dataset Handling,[0],[0]
"While retrieving the dataset may seem like a one-time effort during the development of the model, when comes the time to distribute the computation over several machines, one can save valuable time.",5.2. Dataset Handling,[0],[0]
We also made the dataset available as a public archive10 since it was required by the challenge.,5.2. Dataset Handling,[0],[0]
"When doing research, it is easy to enter the experiment’s hurry loop; as soon as we have an idea, we code it and we launch our main script without committing the modifications.",5.3. Automatic Experiment Logging,[0],[0]
"Grossly keeping track of an architecture and its corresponding results in our head or a spreadsheet is good for nothing when it comes to the time to retrieve and analyze
8 https://git-scm.com/
9https://dvc.org 10https://vecmap-submission.s3.amazonaws.com/dataset.tar.gz
past experiments.",5.3. Automatic Experiment Logging,[0],[0]
We thus propose to automate the process of logging as well as retrieving the results of every experiment in order to reduce the risk of losing experiment information.,5.3. Automatic Experiment Logging,[0],[0]
"To this end, we used a flexible yet emergent tool that beautifully solves this problem; MLflow11,12. MLflow provides a model agnostic Python API that lets you track not only the results of a given configuration but also the associated source code, the dataset used, and much more.",5.3. Automatic Experiment Logging,[0],[0]
It has been of great use for the automatic generation of tables and graphs in this actual paper as it is required by the challenge.,5.3. Automatic Experiment Logging,[0],[0]
"We also believe it is vital to use such a framework for any scientific team doing serious research to reduce the overhead and stress of manually logging and keeping the information about experimentations, especially considering the low effort it requires to setup.",5.3. Automatic Experiment Logging,[0],[0]
"Docker13 is a software that provides an abstraction of the system libraries, tools, and runtime.",5.4. Dockerization,[0],[0]
A Docker container is essentially a lightweight executable package that can run on every14 environment.,5.4. Dockerization,[0],[0]
"In this project, we did face a dependencies problem between the Cupy python library and its associated CUDA drivers.",5.4. Dockerization,[0],[0]
"In fact, even with a Docker container and the nvidia-docker15 library, we had to make
11https://mlflow.org 12One can find numerous alternatives such as Sacred,
Comet.ml or Weights and Biases for example.",5.4. Dockerization,[0],[0]
"13 https://www.docker.com/
14As long as the environment provides the necessary hardware
specifications.",5.4. Dockerization,[0],[0]
"15ht ps:// ithub.com/NVIDIA/nvidia-docker
sure that the Cupy compiled library matched the actual host’s CUDA drivers.",5.4. Dockerization,[0],[0]
This issue brings the reproducibility of the project at stake when the hardware of the host’s machine differs from the original one.,5.4. Dockerization,[0],[0]
We thus assume that the host machine running our codebase within our provided docker image16 has the requirements such as the CUDA drivers to fulfill the experiments.,5.4. Dockerization,[0],[0]
Vulić et al. (2019) showed that completely unsupervised word translation approaches tend to fail when language pairs are distant.,6. Assessing the Algorithm’s Robustness,[0],[0]
They however identify Artetxe et al. (2018b)’s algorithm as the current most robust among completely unsupervised approaches.,6. Assessing the Algorithm’s Robustness,[0],[0]
"Hence, in order to assess ourselves the algorithm’s robustness, we decided to apply it on other languages that have fewer similarities with the English language.",6. Assessing the Algorithm’s Robustness,[0],[0]
We also conduct a grid search over key hyperparameters which enlightens us on the stability of the whole procedure.,6. Assessing the Algorithm’s Robustness,[0],[0]
We carefully selected four new languages that are characterized by very different roots than the one used in the original paper.,6.1. More Languages,[0],[0]
"We used Estonian (ET) which is a language that gets its root from Finno-Ugric, the same as Finnish.",6.1. More Languages,[0],[0]
"We also selected Persian (FA), Latvian (LV) and Vietnamese (VI).",6.1. More Languages,[0],[0]
We can see in Table 3 that the results on Estonian corroborate the results from the initial paper where the stochastic dictionary induction step is crucial for proper convergence.,6.1. More Languages,[0],[0]
We observed similar behavior for the Persian language.,6.1. More Languages,[0],[0]
"Interestingly, even with the full system, the algorithm poorly performs on Latvian and Vietnamese languages.",6.1. More Languages,[0],[0]
We conducted the same ablation study as with the original languages.,6.1. More Languages,[0],[0]
We can see that the algorithm does not converge without an unsupervised initialization and without the stochastic procedure.,6.1. More Languages,[0],[0]
It also struggles to converge on three languages out of four when the CSLS component is turned off.,6.1. More Languages,[0],[0]
These results clearly show that the proposed method may become brittle when the target language shares fewer commonalities with the source language.,6.1. More Languages,[0],[0]
"In order to correctly assess the algorithm’s robustness to variations in one key hyperparameter’s values, we conducted experiments where we fixed all of the parameter values to the default ones and only varied the tested hyperparameter, ensuring adequate conclusions could be drawn.",6.2. Robustness to Hyperparameters,[0],[0]
"The key parameters we chose to examine are (1) the number of considered neighbors in the CSLS procedure, (2) the number of retained words in the frequency-based vocabulary cutoff and (3) the initial value of p and its growing factor in the stochastic dictionary induction.",6.2. Robustness to Hyperparameters,[0],[0]
"We then assess each hyperparameter’s impact on both the performance and the execution time (in terms of the number of iterations and/or iteration duration) of the algorithm in order to provide well-informed recommendations.
",6.2. Robustness to Hyperparameters,[0],[0]
"16registry.gitlab.com/nicolasgarneau/vecmap
CSLS",6.2. Robustness to Hyperparameters,[0],[0]
"We conducted experiments where we varied the k number of neighbors considered in the CSLS procedure from 1 to 20, with results reported on Figure 1.",6.2. Robustness to Hyperparameters,[0],[0]
"For all language pairs, we denote a variation of approximately 1 % between the highest and lowest accuracy obtained over the evaluated range.",6.2. Robustness to Hyperparameters,[0],[0]
"These variations are however well in between the 95 % confidence interval region for most of the tested values, suggesting the correlation between the performance and the number of neighbors considered in CSLS is loose.",6.2. Robustness to Hyperparameters,[0],[0]
"Furthermore, when taking into account that iteration duration only slightly grows with the growth of k, the author’s suggested universal value of k = 10 neighbors considered in the CSLS retrieval procedure appears like a legitimate compromise.
",6.2. Robustness to Hyperparameters,[0],[0]
"Frequency-based vocabulary cutoff For this experiment, we only retained the n most frequent words of both languages before launching the self-learning iterative procedure (subsection 3.3.), with values of n ranging from 10k to 30k, with increments of 1k.",6.2. Robustness to Hyperparameters,[0],[0]
"When increasing the value of n, our results show the system’s accuracy decreasing on Spanish, increasing on both Finnish and Deutsch and attaining a stable range for Italian.",6.2. Robustness to Hyperparameters,[0],[0]
"While the accuracy difference is very different from one target language to another, the variation on all the tested range is between 1 and 2 %.",6.2. Robustness to Hyperparameters,[0],[0]
"Regarding the iteration duration’s correlation with the number of retained words, our experiments show a quadratic growth of an iteration’s duration as well as an overall increase in the number of iterations before convergence when increasing n, in line with the original paper’s conclusion.",6.2. Robustness to Hyperparameters,[0],[0]
"Pairing the highly languagedependent impact of this hyperparameter on the algorithm’s performance with its major impact on its execution time, we conclude that the number of most frequent words retained before the self-learning procedure should be the target of careful finetuning for each target language.
",6.2. Robustness to Hyperparameters,[0],[0]
"Stochastic dictionary induction For the tests on the stochastic dictionary induction, we considered a linear space of 5 values between 0.05 and 0.3 for the initial keep probability (p0) and a linear space of 4 values between 1.5 and 3 for p’s growth factor (pfactor) and ran tests for each of the 20 total combinations.",6.2. Robustness to Hyperparameters,[0],[0]
"Our results only show a slight performance difference between all tested value pairs, with all language pairs only varying for less than 1 % and three of the four language pairs varying for less than 0.5 %.",6.2. Robustness to Hyperparameters,[0],[0]
One important to note however is that making the algorithm greedier (with a higher value of p0) does not lead to any performance loss: the best performances are rather found when using those high p0 values.,6.2. Robustness to Hyperparameters,[0],[0]
"Considering the number of iterations only decreases when p0 grows, it appears the original paper’s p0 = 0.1 value only increases the number of iterations without a significant impact on performance.",6.2. Robustness to Hyperparameters,[0],[0]
"No such conclusion can be drawn for the pfactor hyperparameter, which appears very weakly correlated to overall performance.",6.2. Robustness to Hyperparameters,[0],[0]
"In this paper, we studied the reproducibility of the model proposed by Artetxe et al. (2018b).",7. Conclusion,[0],[0]
"We found out that their
English-Deutsch
English-Spanish
English-Finnish
English-Italian
English-Deutsch
English-Spanish
English-Finnish
English-Italian
method of mapping embeddings between two languages is robust when the languages share commonalities.",7. Conclusion,[0],[0]
"Otherwise, the approach struggles to learn proper mapping.",7. Conclusion,[0],[0]
"We also assessed the robustness of the hyperparameters of the algorithm in many languages.
",7. Conclusion,[0],[0]
We introduced several recommendations regarding the guidelines every researcher should follow in order to deliver fully reproducible research.,7. Conclusion,[0],[0]
It is often said that replicability (reproducing the results of a model from a new implementation) is more complicated than reproducibility (reproducing the results from an existing implementation).,7. Conclusion,[0],[0]
"However, we found out that reproducing the results may become an issue if there are hardware or time constraints as we faced during our experimentations.",7. Conclusion,[0],[0]
"Indeed, we were able to perform a grid-search on the hyperparameter and
validate the robustness of the algorithm thanks to the 64 GPUs we had in hand, otherwise, it would have taken months to run.",7. Conclusion,[0],[0]
"That being said, reproducibility is an issue when hardware and time constraints come into play.",7. Conclusion,[0],[0]
This research was enabled in part by support provided by Calcul Québec (https://www.calculquebec.ca/) and Compute Canada (www.computecanada.ca).,8. Acknowledgements,[0],[0]
We also acknowledge the support of the Natural Sciences and Engineering Research Council of Canada (NSERC).,8. Acknowledgements,[0],[0]
"Finally, we wish to thank Anders Søgaard for his precious advice and the reviewers for their insightful comments regarding our work and methodology.
",8. Acknowledgements,[0],[0]
"English-Deutsch
English-Spanish",8. Acknowledgements,[0],[0]
"Artetxe, M., Labaka, G., and Agirre, E. (2018a).",9. References,[0],[0]
"Gener-
",9. References,[0],[0]
alizing and improving bilingual word embedding mappings with a multi-step framework of linear transformations.,9. References,[0],[0]
"In AAAI Conference on Artificial Intelligence, pages 5012–5019.
",9. References,[0],[0]
"Artetxe, M., Labaka, G., and Agirre, E. (2018b).",9. References,[0],[0]
"A robust
self-learning method for fully unsupervised cross-lingual mappings of word embeddings.",9. References,[0],[0]
"In ACL, pages 789–798.
",9. References,[0],[0]
"Cito, J. and Gall, H. C. (2016).",9. References,[0],[0]
"Using docker containers
to improve reproducibility in software engineering research.",9. References,[0],[0]
"In IEEE/ACM, pages 906–907.
",9. References,[0],[0]
"Conneau, A., Lample, G., Ranzato, M., Denoyer, L., and
Jégou, H. (2017).",9. References,[0],[0]
Word translation without parallel data.,9. References,[0],[0]
"ArXiv, 1710.04087.
Dinu, G. and Baroni, M. (2014).",9. References,[0],[0]
"Improving zero-shot
learning by mitigating the hubness problem.",9. References,[0],[0]
"ArXiv, 1412.6568.
Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B.,
Warde-Farley, D., Ozair, S., Courville, A. C., and Ben-
gio, Y. (2014).",9. References,[0],[0]
Generative adversarial networks.,9. References,[0],[0]
"ArXiv, 1406.2661.
",9. References,[0],[0]
"Gouws, S. and Søgaard, A. (2015).",9. References,[0],[0]
"Simple task-specific
bilingual word embeddings.",9. References,[0],[0]
"In HLT-NAACL.
Hartmann, M., Kementchedjhieva, Y., and Søgaard, A.
(2019).",9. References,[0],[0]
Comparing unsupervised word translation methods step by step.,9. References,[0],[0]
"In NeurIPS.
",9. References,[0],[0]
"Mikolov, T., Le, Q. V., and Sutskever, I. (2013a).",9. References,[0],[0]
"Exploit-
",9. References,[0],[0]
ing similarities among languages for machine translation.,9. References,[0],[0]
"ArXiv, 1309.4168.
",9. References,[0],[0]
"Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and
Dean, J. (2013b).",9. References,[0],[0]
Distributed representations of words and phrases and their compositionality.,9. References,[0],[0]
"In NIPS.
Mikolov, T., Grave, E., Bojanowski, P., Puhrsch, C., and
Joulin, A. (2018).",9. References,[0],[0]
Advances in pre-training distributed word representations.,9. References,[0],[0]
"In LREC.
Pennington, J., Socher, R., and Manning, C. D. (2014).
",9. References,[0],[0]
Glove: Global vectors for word representation.,9. References,[0],[0]
"In EMNLP.
Radovanović, M., Nanopoulos, A., and Ivanović, M.
(2010).",9. References,[0],[0]
Hubs in space: Popular nearest neighbors in high-dimensional data.,9. References,[0],[0]
"JMLR, 11:2487–2531, December.",9. References,[0],[0]
"Ruder, S., Vuli’c, I., and Sogaard, A. (2019).",9. References,[0],[0]
"A survey of
cross-lingual word embedding models.",9. References,[0],[0]
"Journal of Artificial Intelligence Research, 65:569–631, Aug. Vulić, I., Glavaš, G., Reichart, R., and Korhonen, A.
(2019).",9. References,[0],[0]
Do we really need fully unsupervised crosslingual embeddings?,9. References,[0],[0]
"In EMNLP-IJCNLP, pages 4398– 4409.",9. References,[0],[0]
"Yang, P., Luo, F., Wu, S., Xu, J., Zhang, D., and Sun, X.
(2018).",9. References,[0],[0]
Learning unsupervised word mapping by maximizing mean discrepancy.,9. References,[0],[0]
"ArXiv, 1811.00275.",9. References,[0],[0]
"Zhang, M., Liu, Y., Luan, H., and Sun, M. (2017).",9. References,[0],[0]
"Adver-
",9. References,[0],[0]
sarial training for unsupervised bilingual lexicon induction.,9. References,[0],[0]
"In ACL, pages 1959–1970.",9. References,[0],[0]
"In this paper, we reproduce the experiments of Artetxe et al. (2018b) regarding the robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings.",abstractText,[0],[0]
We show that the reproduction of their method is indeed feasible with some minor assumptions.,abstractText,[0],[0]
We further investigate the robustness of their model by introducing four new languages that are less similar to English than the ones proposed by the original paper.,abstractText,[0],[0]
"In order to assess the stability of their model, we also conduct a grid search over sensible hyperparameters.",abstractText,[0],[0]
We then propose key recommendations that apply to any research project in order to deliver fully reproducible research.,abstractText,[0],[0]
A Robust Self-Learning Method for Fully Unsupervised Cross-Lingual Mappings of Word Embeddings: Making the Method Robustly Reproducible as Well,title,[0],[0]
