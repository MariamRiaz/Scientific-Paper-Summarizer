0	13	Autonomous agents can learn how to maximise future expected rewards by choosing how to act based on incoming sensory observations via reinforcement learning (RL).
1	13	Early RL approaches did not scale well to environments with large state spaces and high-dimensional raw observations (Sutton & Barto, 1998).
3	20	Recently, the advent of deep learning and its successful combination with RL has enabled end-to-end learning of such embeddings directly from raw inputs, sparking success in a wide variety of previously challenging RL domains (Mnih et al., 2015; 2016; Jaderberg et al., 2017).
13	49	In many scenarios, such as robotics, this reliance on target domain information can be problematic, as the data may be expensive or difficult to obtain (Finn et al., 2017; Rusu et al., 2016).
15	22	On the other hand, policies learnt exclusively on the source domain using existing deep RL approaches that have few constraints on the nature of the learnt representations often overfit to the source input distribution, resulting in poor domain adaptation performance (Lake et al., 2016; Rusu et al., 2016).
16	33	We propose tackling both of these issues by focusing instead on learning representations which capture an underlying low-dimensional factorised representation of the world and are therefore not task or domain specific.
21	33	Disentangled representations are defined as interpretable, factorised latent representations where either a single latent or a group of latent units are sensitive to changes in single ground truth factors of variation used to generate the visual world, while being invariant to changes in other factors (Bengio et al., 2013).
34	26	Each domain corresponds to an MDP defined as a tuple DS ⌘ (SS ,AS , TS , RS) or DT ⌘ (ST ,AT , TT , RT ) (we assume a shared fixed discount factor ), each with its own state space S , action space A, transition function T and reward function R.1 In domain adaptation scenarios the states S of the source and the target domains can be quite different, while the action spaces A are shared and the transitions T and reward functions R have structural similarity.
38	17	Finally, the source and target domain transition and reward functions share structural similarity (TS ⇡ TT and RS ⇡ RT ), since in both domains transitions between states are governed by the physics of the world and the performance on the task depends on the relative position of the arm’s end effectors (i.e. fingertips) with respect to an object of interest.
42	32	We now introduce notation for two state space variables that may in principle be used interchangeably within the source and target domain MDPs DS and DT – the agent observation state space So, and the agent’s internal latent state space Sz .2 Soi in Di consists of raw (pixel) observations soi generated by the true world simulator from a sampled set of data generative factors ŝi, i.e. soi ⇠ Sim(̂si).
44	23	Using the newly introduced notation, domain adaptation scenarios can be described as having different sampling processes GS and GT such that ŝS ⇠ GS( ˆS) and ŝT ⇠ GT ( ˆS) for the source and target domains respectively, and then using these to generate different agent observation states soS ⇠ Sim(̂sS) and soT ⇠ Sim(̂sT).
45	38	Intuitively, consider a source domain where oranges appear in blue rooms and apples appear in red rooms, and a target domain where the object/room conjunctions are reversed and oranges appear in red rooms and apples appear in blue rooms.
64	37	Since S z Ŝ is a disentangled representation of object and room attributes, the source policy ⇡S can learn a decision boundary that ignores the irrelevant room attributes: oranges!
65	25	Such a policy would then generalise well to the target domain out of the box, since ⇡S(a| ˆF(soS); ✓) = ⇡T (a| ˆF(soT ); ✓) = ⇡T (a|szŜ ; ✓).
68	13	1 for a graphical representation of these steps): 1) Learn to see (unsupervised learning of FU ) – the task of inferring a factorised set of generative factors SzŜ = ˆS from observations So is the goal of the extensive disentangled factor learning literature (e.g. Chen et al., 2016; Higgins et al., 2017).
105	15	We use the Jaco arm with a matching MuJoCo simulation environment (Todorov et al., 2012) in two domain adaptation scenarios: simulation to simulation (sim2sim) and simulation to reality (sim2real).
119	23	In both domains cans and balloons were the rewarded objects.
120	34	1) Learn to see: we used -VAEDAE to learn the disentangled latent state representation sz that includes both the room and the object generative factors of variation within DeepMind Lab.
121	61	We had to use the high-level feature space of a pre-trained DAE within the -VAEDAE framework (see Section 2.3.1), instead of the pixel space of vanilla - VAE , because we found that objects failed to reconstruct when using the values of necessary to disentangle the generative factors of variation within DeepMind Lab (see Fig.
122	27	-VAEDAE was trained on observations soU collected by an RL agent with a simple wall-avoiding policy ⇡U (otherwise the training data was dominated by close up images of walls).
123	32	In order to enable the model to learn F(soU ) ⇡ ˆS , it is important to expose the agent to at least a minimal set of environments that span the range of values for each factor, and where no extraneous correlations are added between different factors 3 (see Fig.
126	97	Pre-trained -VAEDAE from stage one was used as the ‘vision’ part of various RL algorithms (DQN, A3C and Episodic Control: Mnih et al., 2015; 2016; Blundell et al., 2016) to learn a source policy ⇡S that picks up balloons and avoids cakes in both the green and the pink rooms, and picks up cans and avoids hats in the green rooms.
127	13	See Section A.3.1 in Supplementary Materials for more details of the various versions of DARLA we have tried, each based on a different base RL algorithm.
128	22	3) Transfer: we tested the ability of DARLA to transfer the seek-avoid policy ⇡S it had learnt on the source domain in stage two using the domain adaptation condition DT illustrated in Figure 2A (red).
129	22	The agent had to continue picking up cans and avoid hats in the pink room, even though these objects had only been seen in the green room during source policy training.
154	33	On each task we compared DARLA’s performance to that of various baselines.
156	18	In order to do this, we ran the DARLA pipeline with different vision models: the encoders of a disentangled -VAE 4 (the original DARLA), an entangled -VAE (DARLA ENT ), and a denoising autoencoder (DARLA DAE ).
166	18	It can be seen that DARLA is able to zero-shot-generalise significantly better than DARLA ENT or DARLA DAE , highlighting the importance of learning a disentangled representation sz = szŜ during the unsupervised stage one of the DARLA pipeline.
177	27	Figure: Correlation between zero-shot performance transfer performance on the DeepMind Lab task obtained by EC based DARLA and the level of disentanglement as measured by the transfer/disentanglement score (r = 0.6, p < 0.001) pacity of the latent channel.
179	88	Furthermore, we examined what happens if DARLA’s vision (i.e. the encoder of the disentangled -VAE ) is allowed to be fine-tuned via gradient updates while learning the source policy during stage two of the pipeline.
