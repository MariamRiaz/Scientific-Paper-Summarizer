2	20	Currently, NMT models are usually trained with the word-level loss (i.e., cross-entropy) under the teacher forcing algorithm (Williams and Zipser, 1989), which forces the model to generate translation strictly matching the ground-truth at the word level.
3	20	However, in practice it is impossible to generate translation totally the same as ground truth.
5	17	In addition, the teacher forcing algorithm suffers from the exposure bias (Ranzato et al., 2015) as it uses different inputs at training and inference, that is ground-truth words for the training and previously predicted words for the inference.
6	36	Kim and Rush (2016) proposed a method of sequence-level knowledge distillation, which use teacher outputs to direct the training of student model, but the student model still have no access to its own predicted words.
7	14	Scheduled sampling(SS) (Bengio et al., 2015; Venkatraman et al., 2015) attempts to alleviate the exposure bias problem through mixing ground-truth words and previously predicted words as inputs during training.
8	14	However, the sequence generated by SS may not be aligned with the target sequence, which is inconsistent with the word-level loss.
9	21	In contrast, sequence-level objectives, such as BLEU (Papineni et al., 2002), GLEU (Wu et al., 2016), TER (Snover et al., 2006), and NIST (Doddington, 2002), evaluate translation at the sentence or n-gram level and allow for greater flexibility, and thus can mitigate the above problems of the word-level loss.
10	11	However, due to the nondifferentiable of sequence-level objectives, previous works on sequence-level training (Ranzato et al., 2015; Shen et al., 2016; Bahdanau et al., 2016; Wu et al., 2016; He et al., 2016; Wu et al., 2017; Yang et al., 2017) mainly rely on reinforcement learning algorithms (Williams, 1992; Sutton et al., 2000) to find an unbiased gradient estimator for the gradient update.
11	14	Sparse rewards in this situation often cause the high variance of gradient estimation, which consequently leads to unstable training and limited improvements.
12	7	Lamb et al. (2016); Gu et al. (2017); Ma et al. (2018) respectively use the discriminator, critic and bag-of-words target as sequence-level training objectives, all of which are directly connected to the generation model and hence enable direct gradient update.
15	15	Our method introduces probabilistic ngram matching which makes sequence-level objectives (e.g., BLEU, GLEU) differentiable.
16	42	During training, it abandons teacher forcing and performs greedy search instead to take into consideration the predicted words.
17	7	Experiment results show that our method significantly outperforms word-level training with the cross-entropy loss and sequence-level training under the reinforcement framework.
21	4	In the above model, the probability of each target word p(ŷmj |ŷm<j ,xm, θ) is conditioned on the previous target words.
23	2	This discrepancy is called exposure bias.
24	8	Many automatic evaluation metrics of machine translation, such as BLEU, GLEU and NIST, are based on the n-gram matching.
26	6	, gn) in sentence y is calculated as Cy(g) = T−n∑ t=0 n∏ i=1 1{gi = yt+i}, (3) where 1{·} is the indicator function.
27	4	The matching count of the n-gram g between ŷ and y is given by Cŷy(g) = min (Cy(g),Cŷ(g)).
30	1	In contrast, GLEU is the minimum of recall and precision of 1-4 grams where 1-4 grams are counted together: GLEU = min(p1-4, r1-4).
31	64	In the output sentence y, the prediction probability varies among words.
35	19	To give a more precise description of n-gram counts which considers the variety of prediction probabilities, we use the prediction probability p(yj |y<j ,x, θ) as the count of word yj , and correspondingly the count of an n-gram is the product of these probabilistic counts of all the words in the n-gram, not one anymore.
36	14	Then the probabilistic count of g = (g1, .
37	41	, gn) is calculated by summing over the output sentence y as C̃y(g) = T−n∑ t=0 n∏ i=1 1{gi = yt+i} · p(yt+i|y<t+i,x, θ).
40	70	From this purpose, the matching count of n-gram g in Eq.
42	56	(11) Finally, the probabilistic BLEU (P-BLEU) is defined as P-BLEU = BP · exp( N∑ n=1 wn log p̃n), (12) Probabilistic GLEU (P-GLEU) can be defined in a similar way.
43	4	Specifically, we denote the probabilistic precision of n-grams as P-Pn.
45	4	(11) plays a normalization role, so we modify the definition in Eq.
50	118	In this paper we employ greedy search rather than teacher forcing so as to use the previously predicted words as context and alleviate the exposure bias problem.
