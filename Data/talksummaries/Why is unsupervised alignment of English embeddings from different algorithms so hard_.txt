0	17	This paper brings together two fascinating research topics in natural language processing (NLP), namely understanding the properties of word embeddings (Mikolov et al., 2013; Mitchell and Steedman, 2015; Mimno and Thompson, 2017) and unsupervised bilingual dictionary induction (Conneau et al., 2018; Zhang et al., 2017; Søgaard et al., 2018).
1	29	In an effort to better understand when unsupervised bilingual dictionary induction is possible, we factored out linguistic differences between languages, and studied EnglishEnglish alignability (by learning to align English embeddings trained on different samples of the English Wikipedia), when we came across a puzzling phenomena: English-English can be aligned with almost 100% precision, if you use the same embedding algorithms for the two samples, but not at all (0% precision), if you use different embedding algorithms.
2	22	This results suggest that the properties of word embeddings induced by different algorithms challenge unsupervised bilingual dictionary algorithms.
3	19	Understanding why will enable us to develop more stable adversarial learning algorithms and give us a better understanding of how embedding algorithms differ.
4	33	Contributions We are, to the best of our knowledge, the first to study unsupervised alignability of pairs of English word embeddings.
5	16	We show that unsupervised alignment – specifically the MUSE system (Conneau et al., 2018) – fails when the algorithms used to induce the two embeddings differ, and that this is not because there is no linear transformation between the two spaces.
7	31	Finally, we present an experiment showing what the minimal corpus size is for unsupervised alignment to succeed, in the absence of linguistic differences.
8	25	MUSE (Conneau et al., 2018) uses a vanilla generative adversarial network (GAN) with a linear generator to learn alignments between embedding spaces without supervision.
11	36	The parameters of a GAN with a linear generator are (Ω, w).
16	15	Our generator, which learns a linear transform Ω, has very limited capacity, for example, and we are updating Ω rather than pg.
21	13	Here, the optimal alignment between two embedding spaces is computed using singular value decomposition of the aligned embeddings in a seed dictionary.
23	6	In our supervised experiments, we use 5000 seed words as supervision for learning the alignment between embeddings.
25	10	We discuss five embedding algorithms: SVD on positive PMI matrices (Hyperwords-SVD) (Levy et al., 2015), skip-gram negative sampling applied to co-occurrence matrices (Hyperwords-SGNS) (Levy et al., 2015), continuous bag-of-words (CBOW) (Mikolov et al., 2013a), GloVe (Pennington et al., 2014), and FastText (Bojanowski et al., 2017).
31	10	The vectors trained by GloVe show a clear relationship with word frequency, with low-frequency words opposing the frequency-balanced mean vector.
35	25	The differences are the results of the inductive biases of the different embedding algorithms.
38	24	We train 300-dimensional word embeddings using the algorithms’ recommended hyperparameter settings, listed in the following:3 For HyperwordsSGNS, the window size is set to 2 and the subsampling of frequent words and smoothing of the context distribution are disabled.
44	22	We train word embeddings using the different embedding algorithms listed in §3.2 on two non-overlapping 10% samples of the English Wikipedia dump (the samples contain 463,576 and 528,556 distinct words, with an overlap in vocabulary of 351,858 words).
45	36	We learn unsupervised and supervised alignments for embeddings (as described in §2) trained by different algorithms on the same datasplits, and for embeddings trained by the same algorithm on the two different datasplits.
46	11	For the unsupervised alignments, we use the html 3We also ran experiments with one of the embedding algorithms (FastText) to check if our results were robust across hyper-parameter settings default parameters of the MUSE system for the adversarial training, i.e. a discriminator with 2 fully connected layers of 2048 units trained over 5 epochs, 1,000,000 iterations per epoch with 5 discriminator steps per iteration and a batch size of 32.
50	27	(b) MUSE cannot learn alignments for embeddings learned by different algorithms on the same data splits, even if there exists a linear transformation aligning both sets of embeddings (the supervised algorithm learns perfect alignments).
54	11	§4 discusses potential answers to why MUSE fails when embeddings are induced using different algorithms.
64	30	To verify this holds in general, i.e., that results are not affected by normalization in general, we also ran experiments with the remaining 14 embedding pairs, normalizing and/or centering both embeddings.
65	5	Results stayed the same: Precision at 1 scores of 0.
66	11	MUSE perfectly aligns independently induced word embeddings induced by the same algorithm.
69	4	English-English alignment is an interesting control experiment for unsupervised bilingual dictionary induction, abstracting away from linguistic differences, and we ran a series of experiments to see how small samples MUSE can align in the absence of linguistic differences.
72	27	The only explanation left seems to be that the inductive biases of the different algorithms lead to a loss landscape so riddled with local optima that MUSE cannot possible escape them.
73	50	To support this hypothesis, compare the loss curves for the MUSE runs aligning embeddings induced with the same algorithms (black curves) to the runs aligning embeddings induced with dif- ferent algorithms, in Figure 2.
74	60	When the embeddings are induced by the same algorithm, we clearly see the contours of a min-max game, suggesting that the generator and discriminator challenge each other, both contributing to a good alignment.
75	42	When the embeddings are induced by different algorithms, however, the discriminator quickly drops, with the generator unable to push the discriminator out of a local optimum.
76	79	Understanding when biases induce highly non-convex landscapes, and how to make adversarial training less sensitive to such scenarios, remains an open problem, which we think will be key to the success of unsupervised machine translation and related tasks.
