4	29	Rumor is commonly defined as information that emerge and spread among people whose truth value is unverified or intentionally false (DiFonzo and Bordia, 2007; Qazvinian et al., 2011).
9	37	For automating rumor detection, most of the previous studies focused on text mining from sequential microblog streams using supervised models based on feature engineering (Castillo et al., 2011; Kwon et al., 2013; Liu et al., 2015; Ma et al., 2015), and more recently deep neural models (Ma et al., 2016; Chen et al., 2017; Ruchansky et al., 2017).
10	61	These methods largely ignore or oversimplify the structural information associated with message propagation which however has been shown conducive to provide useful clues for identifying rumors.
13	25	In this paper, we present a neural rumor detection approach based on recursive neural networks (RvNN) to bridge the content semantics and propagation clues.
14	34	RvNN and its variants were originally used to compose phrase or sentence representation for syntactic and semantic parsing (Socher et al., 2011, 2012).
30	21	Our contributions are summarized as follows in three folds: • This is the first study that deeply integrates both structure and content semantics based on tree-structured recursive neural networks for detecting rumors from microblog posts.
31	70	• We propose two variants of RvNN models based on bottom-up and top-down tree structures to generate better integrated representations for a claim by capturing both structural and textural properties signaling rumors.
51	23	We define a Twitter rumor detection dataset as a set of claims C = {C1, C2, · · · , C|C|}, where each claim Ci corresponds to a source tweet ri which consists of ideally all its relevant responsive tweets in chronological order, i.e., Ci = {ri, xi1, xi2, · · · , xim}where each xi∗ is a responsive tweet of the root ri.
52	36	Note that although the tweets are notated sequentially, there are connections among them based on their reply or repost relationships, which can form a propagation tree structure (Wu et al., 2015; Ma et al., 2017) with ri being the root node.
58	21	This structure reverses bottomup tree and simulates how information cas- cades from a source tweet, i.e., the root, to all its receivers, i.e., the decedents, which is similar as (Wu et al., 2015; Ma et al., 2017).
70	23	The core idea of bottom-up model is to generate a feature vector for each subtree by recursively visiting every node from the leaves at the bottom to the root at the top.
72	25	And thus such local rumor indicative features are aggregated along different branches into some global representation of the whole tree.
80	21	The transition equations of node j in the bottom-up model are formulated as follows: x̃j = xjE hS = ∑ s∈S(j) hs rj = σ (Wrx̃j + UrhS) zj = σ (Wzx̃j + UzhS) h̃j = tanh (Whx̃j + Uh(hS rj)) hj = (1− zj) hS + zj h̃j (1) where xj is the original input vector of node j, E denotes the parameter matrix for transforming this input post, x̃j is the transformed representation of j, [W∗, U∗] are the weight connections inside GRU, and hj and hs refer to the hidden state of j and its s-th child.
86	21	This model is designed to leverage the structure of top-down tree to capture complex propagation patterns for classifying rumorous claims, which is shown in Figure 3(c).
89	21	For example, if current post agree with its parent’s stance which denies the source post, the denial stance from the root node down to the current node on this path should be reinforced.
92	23	The representation of each node is computed by combining its own input and its parent node instead of its children nodes.
100	40	Based on the pooling result, we finally use a softmax function in the output layer to predict the label of the tree: ŷ = Softmax(Vh∞ + b) (4) where h∞ is the pooling vector over all leaf nodes, V and b are parameters in the output layer.
102	20	The hypothesis is that in the bottom-up case the final output relies on the representation of single root, and its information loss can be larger than the top-down one since in the top-down case the representations embedded into all leaf nodes along different propagation paths can be incorporated via pooling holistically.
118	29	- SVM-TK and SVM-HK: SVM classifier uses a Tree Kernel (Ma et al., 2017) and that uses a Hybrid Kernel (Wu et al., 2015), respectively, both of which model propagation structures with kernels.
129	30	There are two reasons: 1) SVM-HK was originally proposed and experimented on Sina Weibo (Wu et al., 2015), which may not be generalize well on Twitter.
143	20	This can be explained by the fact that these baselines are trained with additional features such as user information (e.g., profile, verification status, etc) which may contain clues for differentiating non-rumors from rumors.
145	22	Detecting rumors at early state of propagation is important so that interventions can be made in a timely manner.
149	22	Although all the methods are getting to their best per- formance in the end, TD-RvNN and BU-RvNN only need around 8 hours or about 90 tweets to achieve the comparable performance of the best baseline model, i.e., SVM-TK, which needs about 36 hours or around 300 posts, indicating superior early detection performance of our method.
151	158	We can see that this false rumor demonstrates typical patterns in subtrees and propagation paths indicative of the falsehood, where a set of responses supporting the parent posts that deny or question the source post are captured by our bottom-up model.
152	122	Similarly, some patterns of propagation from the root to leaf nodes like “support→deny→support” are also seized by our top-down model.
153	60	In comparison, sequential models may be confused because the supportive key terms such as “be right”, “yeah”, “exactly!” dominate the responses, and the SVM-TK may miss similar subtrees by just comparing the surface words.
155	82	The inher- ent nature of recursive models allows them using propagation tree to guide the learning of representations from tweets content, such as embedding various indicative signals hidden in the structure, for better identifying rumors.
156	37	Results on two public Twitter datasets show that our method improves rumor detection performance in very large margins as compared to state-of-the-art baselines.
157	68	In our future work, we plan to integrate other types of information such as user properties into the structured neural models to further enhance representation learning and detect rumor spreaders at the same time.
158	43	We also plan to use unsupervised models for the task by exploiting structural information.
