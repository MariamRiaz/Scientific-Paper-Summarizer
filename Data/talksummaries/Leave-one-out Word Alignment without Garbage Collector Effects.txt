0	45	Unsupervised word alignment (WA) on bilingual sentence pairs serves as an essential foundation for building most statistical machine translation (SMT) systems.
7	50	However, the EM algorithm for WA is wellknown for introducing “garbage collector effects.” Rare words have a tendency to collect garbage, that is they have a tendency to be erroneously aligned to untranslated words (Brown et al., 1993a; Moore, 2004; Ganchev et al., 2008; V Graça et al., 2010).
17	17	In this way, these “garbage collector effects” are a form of over-fitting.
20	35	Fig- ure 1(c) shows the effect of using our technique on the example.
33	95	In order to maximize the likelihood of the alignment modelθ given the dataS, the following two steps are conducted iteratively (Brown et al., 1993b; Och and Ney, 2000; Och and Ney, 2003), Expectation Step (E step): calculating the conditional probability of alignments for each sentence pair, P (a|s, θ) = ∏Jj=1 θali(aj |aj−1, I)θlex(fj |eaj ),(1) whereθali(i|i′, I) is the alignment probability and θlex(f |e) is the translation probability.
35	39	Maximization step (M step): re-estimating the probability models, θali(i|i′, I) ← ∑ s Ni|i′,I(s)∑ s Ni′,I(s) (2) θlex(f |e) ← ∑ s Nf |e(s)∑ s ne(s) (3) whereNi′,I(s) is the marginal number of timesei′ is aligned to some foreign word if the length ofe is I, or0 otherwise;Ni|i′,I(s) is the marginal number of times the next alignment position afteri′ is i in a if the length ofe is I, or0 otherwise;ne(s) is the count ofe in e; Nf |e(s,a) is the marginal number of timese is aligned tof .
40	100	The framework of the standard EM for IBM Model 4 is similar with the one for IBM Models 1, 2 and HMM Model, but the calculation of alignment probability is more complicated.
41	19	E step: calculating the conditional probability through the reverted alignment (Och and Ney, 2003), P (a|s, θ) = P (B0|B1, .
45	22	M step: re-estimating the probability models, θfer(φ|e) ← ∑ s Nφ|e(s)∑ s ∑ φ′ Nφ′|e(s) (9) θhea(∆i|E) ← ∑ s N hea ∆i|E(s)∑ s ∑ ∆i′ N hea ∆i′|E(s) (10) θoth(∆i) ← ∑ s N oth ∆i (s)∑ s ∑ ∆i′ N oth ∆i′(s) , (11) where∆i is a difference of the indexes of two foreign words.
46	21	The leave-one-out treatment were applied to the three component probability modelsθfer, θhea and θoth of IBM model 4.
50	23	Singletons are the words that occur only once in corpora.
51	16	Singletons cause problems when applying leave-one-out to lexicalized models such as the translation modelθs̄lex and the fertility modelθ s̄ fer.
54	75	In that case, the alignments are primarily determined by the rest of the sentence.
85	24	The IBM model 1, HMM model, IBM model 3 and IBM model 4 were run with 5, 5, 3 and 3 iterations.
88	20	They were run with 5, 5 and 6 iterations.
92	40	In all experiments, WA was performed independently in two directions: from foreign languages to English, and from English to foreign languages.
96	33	The improvement in F1, precision and recall based on IBM Model 4 is in the range 8.3% to 9.1% compared with the GIZA++ baseline, and in the range 5.0% to 17.2% compared with our own baseline.
102	31	Each experiment was conducted three times to mitigate the variance in the results due to MERT.
106	17	We think this is because that hierarchical systems are more sensitive to word alignment quality than phrase-based systems.
120	16	On word alignment accuracy, the proposed method achieved improvements ofF1 from 0.041 to 0.090 under the different training corpora (Table 5.
121	18	The maximum improvement compared with GIZA++ is 0.069 when the training corpus has 4,000 sentence pairs.
127	46	In addition, the BLEUs achieved by the proposed method is close to the ones achieved by gold WA annotations.
129	22	4.5 Comparison to l0-Normalization and Kneser-Ney Smoothing Methods The proposed leave-one-word word alignment method was empirically compared to l0-normalized GIZA++ (Vaswani et al., 2012)11 and Kneser-Ney smoothed GIZA++ (Zhang and Chiang, 2014)12. l0-normalization and KneserNey smoothing methods are established methods to overcome the sparse problem.
138	16	The leave-one-out method outperformed related methods in terms of precision, recall and F1 when evaluated on all words.
144	16	The explanation might be that the leave-one-out method punishes rare words more than the Kneser-Ney smoothing method, by totally removing the derived expected counts of current sentence pair from the alignment models.
145	19	This leads to rare words being passively aligned.
156	36	In a Chinese-English parallel training corpus of 18,057 sentence pairs, the manual WA annotation outperformed the unsupervised WA results produced by standard EM algorithms.
159	16	It is a interesting question why GIZA++ achieved competitive BLEU scores though its alignment accuracy measured by F1 was substantially lower.
160	128	The answer to this question which may reveal essence of good word alignment for MT and eventually help to improve MT.
161	22	In addition, we plan to improve the proposed method by integrating Kneser-Ney smoothing.
