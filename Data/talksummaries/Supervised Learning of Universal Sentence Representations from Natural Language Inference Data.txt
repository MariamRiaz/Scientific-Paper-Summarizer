2	17	That is, how to capture the relationships among multiple words and phrases in a single vector remains an question to be solved.
3	77	In this paper, we study the task of learning universal representations of sentences, i.e., a sentence encoder model that is trained on a large corpus and subsequently transferred to other tasks.
5	55	Following existing work on learning word embeddings, most current approaches consider learning sentence encoders in an unsupervised manner like SkipThought (Kiros et al., 2015) or FastSent (Hill et al., 2016).
7	112	We compare sentence embeddings trained on various supervised tasks, and show that sentence embeddings generated from models trained on a natural language inference (NLI) task reach the best results in terms of transfer accuracy.
8	90	We hypothesize that the suitability of NLI as a training task is caused by the fact that it is a high-level understanding task that involves reasoning about the semantic relationships within sentences.
9	45	Unlike in computer vision, where convolutional neural networks are predominant, there are multiple ways to encode a sentence using neural networks.
11	89	Our experiments show that an encoder based on a bi-directional LSTM architecture with max pooling, trained on the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015), yields state-of-the-art sentence embeddings com- 670 pared to all existing alternative unsupervised approaches like SkipThought or FastSent, while being much faster to train.
49	16	, wT ) (or using GRU units instead).
51	30	We also consider a model BiGRU-last that concatenates the last hidden state of a forward GRU, and the last hidden state of a backward GRU to have the same architecture as for SkipThought vectors.
62	36	Following Lin et al. (2017) we use a selfattentive network with multiple views of the input sentence, so that the model can learn which part of the sentence is important for the given task.
63	16	Concretely, we have 4 context vectors u1w, u 2 w, u 3 w, u 4 w which generate 4 representations that are then concatenated to obtain the sentence representation u.
80	37	Binary and multi-class classification We use a set of binary classification tasks (see Table 1) that covers various types of sentence classification, including sentiment analysis (MR, SST), question-type (TREC), product reviews (CR), subjectivity/objectivity (SUBJ) and opinion polarity (MPQA).
90	48	These tasks evaluate how the cosine distance between two sentences correlate with a human-labeled similarity score through Pearson and Spearman correlations.
91	15	Paraphrase detection The Microsoft Research Paraphrase Corpus is composed of pairs of sentences which have been extracted from news sources on the Web.
105	24	Looking at the micro and macro averages, we see that it performs significantly better than the other models LSTM, GRU, BiGRU-last, BiLSTM-Mean, inner-attention and the hierarchical-ConvNet.
106	17	Table 3 also shows that better performance on the training task does not necessarily translate in better results on the transfer tasks like when comparing inner-attention and BiLSTM-Mean for instance.
107	108	We hypothesize that some models are likely to over-specialize and adapt too well to the biases of a dataset without capturing general-purpose information of the input sentence.
108	20	For example, the inner-attention model has the ability to focus only on certain parts of a sentence that are useful for the SNLI task, but not necessarily for the transfer tasks.
112	54	Embedding size Figure 5 compares the overall performance of different architectures, showing the evolution of micro averaged performance with Model MR CR SUBJ MPQA SST TREC MRPC SICK-R SICK-E STS14 Unsupervised representation training (unordered sentences) Unigram-TFIDF 73.7 79.2 90.3 82.4 - 85.0 73.6/81.7 - - .58/.57 ParagraphVec (DBOW) 60.2 66.9 76.3 70.7 - 59.4 72.9/81.1 - - .42/.43 SDAE 74.6 78.0 90.8 86.9 - 78.4 73.7/80.7 - - .37/.38 SIF (GloVe + WR) - - - - 82.2 - - - 84.6 .69/ - word2vec BOW† 77.7 79.8 90.9 88.3 79.7 83.6 72.5/81.4 0.803 78.7 .65/.64 fastText BOW† 76.5 78.9 91.6 87.4 78.8 81.8 72.4/81.2 0.800 77.9 .63/.62 GloVe BOW† 78.7 78.5 91.6 87.6 79.8 83.6 72.1/80.9 0.800 78.6 .54/.56 GloVe Positional Encoding† 78.3 77.4 91.1 87.1 80.6 83.3 72.5/81.2 0.799 77.9 .51/.54 BiLSTM-Max (untrained)† 77.5 81.3 89.6 88.7 80.7 85.8 73.2/81.6 0.860 83.4 .39/.48 Unsupervised representation training (ordered sentences) FastSent 70.8 78.4 88.7 80.6 - 76.8 72.2/80.3 - - .63/.64 FastSent+AE 71.8 76.7 88.8 81.5 - 80.4 71.2/79.1 - - .62/.62 SkipThought 76.5 80.1 93.6 87.1 82.0 92.2 73.0/82.0 0.858 82.3 .29/.35 SkipThought-LN 79.4 83.1 93.7 89.3 82.9 88.4 - 0.858 79.5 .44/.45 Supervised representation training CaptionRep (bow) 61.9 69.3 77.4 70.8 - 72.2 - - - .46/.42 DictRep (bow) 76.7 78.7 90.7 87.2 - 81.0 68.4/76.8 - - .67/.70 NMT En-to-Fr 64.7 70.1 84.9 81.5 - 82.8 - - .43/.42 Paragram-phrase - - - - 79.7 - - 0.849 83.1 .71/ - BiLSTM-Max (on SST)† (*) 83.7 90.2 89.5 (*) 86.0 72.7/80.9 0.863 83.1 .55/.54 BiLSTM-Max (on SNLI)† 79.9 84.6 92.1 89.8 83.3 88.7 75.1/82.3 0.885 86.3 .68/.65 BiLSTM-Max (on AllNLI)† 81.1 86.3 92.4 90.2 84.6 88.2 76.2/83.1 0.884 86.3 .70/.67 Supervised methods (directly trained for each task – no transfer) Naive Bayes - SVM 79.4 81.8 93.2 86.3 83.1 - - - - - AdaSent 83.1 86.3 95.5 93.3 - 92.4 - - - - TF-KLD - - - - - - 80.4/85.9 - - - Illinois-LH - - - - - - - - 84.5 - Dependency Tree-LSTM - - - - - - - 0.868 - - Table 4: Transfer test results for various architectures trained in different ways.
118	23	However, this is particularly true for some models (BiLSTM-Max, HConvNet, inner-att), which demonstrate unequal abilities to incorporate more information as the size grows.
122	28	The first group corresponds to models trained with unsupervised unordered sentences.
126	27	We report results from models trained on supervised data in the third group, and also report some results of supervised methods trained directly on each task for comparison with transfer learning approaches.
131	46	Except for the SUBJ dataset, it also performs better than SkipThought-LN on MR, CR and MPQA.
137	81	One explanation is that the natural language inference task constrains the model to encode the semantic information of the input sentence, and that the information required to perform NLI is generally discriminative and informative.
149	22	With 433K sentence pairs, MultiNLI improves upon SNLI in its coverage: it contains ten distinct genres of written and spoken English, covering most of the complexity of the language.
155	14	This paper studies the effects of training sentence embeddings with supervised data by testing on 12 different transfer tasks.
156	95	We showed that models learned on NLI can perform better than models trained in unsupervised conditions or on other supervised tasks.
157	189	By exploring various architectures, we showed that a BiLSTM network with max pooling makes the best current universal sentence encoding methods, outperforming existing approaches like SkipThought vectors.
158	186	We believe that this work only scratches the surface of possible combinations of models and tasks for learning generic sentence embeddings.
159	105	Larger datasets that rely on natural language understanding for sentences could bring sentence embedding quality to the next level.
