2	47	In this work, we concentrate on automatic poetry generation.
3	17	Besides the long-term goal of building artificial intelligence, research on this task could become the auxiliary tool to better analyse poetry and understand the internal mechanism of human writing.
5	14	In recent years, neural networks have proven to be powerful on poetry generation.
7	18	However, existing models are all based on maximum likelihood estimation (MLE), which brings two substantial problems.
19	43	To tackle these problems, we directly model the four aforementioned human evaluation criteria and use them as explicit rewards to guide gradient update by reinforcement learning.
53	19	We apply our method to a basic poetry generation model, which is pre-trained with MLE.
72	21	We use a neural language model to measure fluency.
85	49	The basic model tends to generate some common and meaningless words, such as bu zhi (don’t know), he chu (where), and wu ren (no one).
95	15	In fact, human experts will also focus on discourselevel to judge the overall quality of a poem, ignoring some minor defects.
96	42	We train a neural classifier to classify a given poem (in terms of the concatenation of all lines) into three classes: computer-generated poetry (class 1), ordinary human-authored poetry (class 2) and masterpiece (class 3).
110	49	As discussed in Section 1 & 2, to further improve the performance, we mimic the mutual writing learning activity by simultaneously training two generators defined as Pg(θ1) and Pg(θ2).
112	15	From the perspective of machine learning, one generator may not explore the policy space sufficiently and thus is easy to get stuck in the local minima.
118	17	For the same input, suppose P1, P2 are generated by Pg(θ1) and Pg(θ2) respectively.
120	32	That is, if a learner creates a significantly better poem, then the other learner will learn it.
121	61	This process gives a generator more high-reward instances and allows it to explore larger space along a more proper direction so as to escape from the local minima.
169	17	Poetry is a kind of literature text with high requirements on diversity and innovation.
185	15	As we can see, Base generates poems with lower TF-IDF compared to GT, while MRL pulls the distribution towards that of GT, making the model generate more meaningful words and hence benefiting innovation and diversity.
206	23	As we can see, for SRL, the adequately pre-trained generator 2 al- ways gets higher rewards than the other one during the DRL training process.
209	29	For MRL, generator 2 gets higher rewards at the beginning, but it is exceeded by generator 1 since generator 1 learns from it and keeps chasing.
212	21	The Base model generates two words, ‘sunset’ and ‘moon’ in poem (1), which appear together and thus cause the conflict of time.
222	14	Poem (6) talks about an old poet, with the description of cheap wine, poem and dream, expressing something about life and time.
225	16	In this work, we address two substantial problems in automatic poetry generation: lack of diversity, and loss-evaluation mismatch, which are caused by MLE-based neural models.
228	27	Furthermore, inspired by writing theories, we propose a novel mutual learning schema to further improve the performance.
229	32	Mimicking the poetry learning activity, we simultaneously train two generators, which will not only be taught by the rewarders but also learn from each other.
231	85	Can we better model the meaningfulness of a whole poem?
232	23	Can we quantify some other intractable criteria, e.g, poeticness?
233	28	Besides, we only tried two learners in this work.
234	63	Would the collaboration of more learners lead to better results?
235	74	How to design the methods of communication among many generators?
236	31	We will explore these questions in the future.
