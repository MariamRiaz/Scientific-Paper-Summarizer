3	47	When summarizing meeting recordings, sport videos and movies, such videos consist of synchronized voice, visual and captions.
6	19	In this paper, as shown in Figure 1, we propose an approach to a generate textual summary from a set of asynchronous documents, images, audios and videos on the same topic.
7	35	Since multimedia data are heterogeneous and contain more complex information than pure text does, MMS faces a great challenge in addressing the semantic gap between different modalities.
9	21	For the audio information contained in videos, we obtain speech transcriptions through Automatic Speech Recognition (ASR) and design a method to use these transcriptions selectively.
15	51	For example, when a transcription provides similar information to a sentence in documents, we should prefer the sentence to the transcription presented in the summary.
16	51	(4) Coverage for the visual information: images that appear in documents and videos often capture event highlights that are usually very important.
19	22	Our main contributions are as follows: • We design an MMS method that can automatically generate a textual summary from a set of asynchronous documents, images, audios and videos related to a specific topic.
20	72	• To select the representative sentences, we consider four criteria that are jointly optimized by the budgeted maximization of submodular functions.
58	38	In this way, we can guarantee the coverage of generated summary for the visual information.
60	29	LexRank first constructs a graph based on the text units and their relationship and then conducts an iteratively random walk to calculate the salience score of the text unit, sa(ti), until convergence using the following equation: Sa(ti) = µ ∑ j Sa(tj) ·Mji + 1− µ N (2) where µ is the damping factor that is set to 0.85.
65	61	The random walk process can be understood as a recommendation: Mji in Equation 2 denotes that tj will recommend ti to the degree of Mji.
69	71	To this end, the process of a random walk should be guided to control the recommendation direction: when a document sentence is related to a speech transcription, the symmetric weighted edge between them should be transformed into a unidirectional edge, in which we invalidate the direction from document sentence to the transcribed one.
70	32	In this way, speech transcriptions will not be recommended by the corresponding document sentences.
74	34	It is a publicly avail- able paraphrase corpus that consists of 5801 pairs of sentences, of which 3900 pairs are semantically equivalent.
83	22	Before measuring the coverage for images, we should train the model to bridge the gap between text and image, i.e., to match the text and image.
102	22	http://wwwnlpir.nist.gov/projects/trecvid/ Note that the images in Flickr30K are similar to our task.
105	46	To solve this problem, we simplify each sentence and speech transcription based on semantic role labelling (Gildea and Jurafsky, 2002), in which each predicate indicates an event and the arguments express the relevant information of this event.
111	40	We model the salience of a summary S as the sum of salience scores Sa(ti)5 of the sentence ti in the summary, combining a λ-weighted redundancy penalty term: Fs(S) = ∑ ti∈S Sa(ti)− λs|S| ∑ ti,tj∈S sim(ti, tj) (8) We model the summary S coverage for the image set I as the weighted sum of image covered by the summary: Fc(S) = ∑ pi∈I Im(pi)bi (9) where the weight Im(pi) for the image pi is the length ratio between the shot pi and the whole videos.
113	42	Finally, considering all the modalities, the objective function is defined as follows: Fm(S) = 1 Ms ∑ ti∈S Sa(ti) + 1 Mc ∑ pi∈I Im(pi)bi − λm|S| ∑ i,j∈S sim(ti, tj) (10) where Ms is the summary score obtained by Equation 8 and Mc is the summary score obtained by Equation 9.
122	18	We select 50 news topics in the most recent five years, 25 in English and 25 in Chinese.
127	20	We employ 10 graduate students to write reference summaries after reading documents and watching videos on the same topic.
133	22	This model generates summaries using the text in documents and the speech transcriptions but without guidance strategies.
137	26	The image is first captioned using the model of Vinyals et al. (2016) which achieved first place in the 2015 MSCOCO Image Captioning Challenge.
140	21	This model uses generated image captions to match the text; i.e., if the similarity between a generated image caption and a sentence exceeds the threshold Ttext, the image and the sentence match.
159	62	The readability and informativeness for summaries are difficult to evaluate formally.
160	51	We ask five graduate students to measure the quality of summaries generated by different methods.
166	50	We give two instances of readability guidance that arise between document text (DT) and speech transcriptions (ST) in Table 6.
173	59	An image and the corresponding texts obtained using different methods are given in Figure 4 an d Figure 5.
175	59	The image alignment introduces more noise because it is possible that the whole text in documents or the speech transcriptions in shot are aligned to the document images or the key-frames, respectively.
182	34	Adding audio and video does not seem to improve dramatically over text only model, which indicates that better models are needed to capture the interactions between text and other modalities, especially for visual.
