0	21	The advent of Neural Machine Translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2014) has led to remarkable improvements in machine translation quality (Bentivogli et al., 2016) but has also produced models that are much less interpretable.
1	23	In particular, the role played by linguistic features in the process of understanding the source text and rendering it in the target language remains hard to gauge.
2	63	Acquiring this knowledge is important to inform future research in NMT, especially regarding the usefulness of injecting linguistic information into the NMT model, e.g. by using supervised annotation (Sennrich and Haddow, 2016).
4	39	More recent work (Shi et al., 2016) has shown that source sentence representations produced by NMT encoders contain a great deal of syntactic information.
6	34	The latter study found that source-side morphology is captured slightly better by the first recurrent layer than by the word embedding and the final recurrent layer.
7	24	Another, somewhat surprising finding was that source-side morphology is learned better when translating into an ‘easier’ target language than into a related one, even if the ’easier’ language is morphologically poor.
8	10	In this paper, we also focus on source-side morphology but perform a finer-grained analysis of how morphological features are captured by different components of the NMT encoder while varying the target language.
11	11	• Are morphological features captured as a word type property (i.e. at the word embedding level) or are they mostly computed in context (i.e. at the recurrent state level)?
13	6	More specifically, we look at whether the NMT encoder only learns those morphological features that can be directly transferred to the target words (such as number) or whether it also learns features that are not directly transferable but can still be useful to correctly parse and infer the meaning of a sentence (such as gender).
16	5	We train NMT systems on the following language pairs: French-Italian (FRIT ), French-German (FRDE), and French-English (FREN ).
18	7	Grammatical gender is especially interesting as it is marked in French, Italian and German, but not in English (except for a few pronouns).
19	47	The gender of Italian nouns often corresponds to that of French because of their common language ancestor, whereas German gender is mostly unrelated from French gender (see example in Fig.
20	10	The continuous word representations produced by the three NMT systems while encoding a corpus of French sentences are used to build and evaluate several specialized classifiers: one per morphological feature.
22	9	While this methodology is similar to that of previous work (Köhn, 2015; Belinkov et al., 2017a,b; Dalvi et al., 2017) we make sure that our results are not affected by overfitting by eliminat- ing any vocabulary overlap between the classifier’s training and test sets.
23	8	We find this step crucial to ensure that the redundancy in this type of data does not lead to over-optimistic conclusions.
25	15	For a fair comparison among target languages, we extract the intersection of the Europarl corpus (Koehn, 2005) in our three language pairs so that the source side data is identical for all NMT systems.
29	21	The models have 3 stacked LSTM layers and are trained for 15 epochs.
30	5	Embedding and hidden state sizes are set to 1000.
34	20	As we are mostly interested in the impact of context on word representations, we compare the word embeddings against the final layer of the stacked LSTMs (corresponding to layers 0 and 3 in Belinkov et al. (2017a)’s terms) while disregarding the intermediate layers.
35	16	The continuous word representations are used to train a logistic regression classifier2 for each morphological feature: gender and number for noun and adjectives; tense for verbs (with labels: present, future, imperfect, or simple past).
60	9	In both absolute and rela- tive terms, the best performing feature is number.
66	7	Differently from Belinkov et al. (2017a) we do not find that source-side morphology is captured better when translating into the ‘easiest’ language, which in our case is English, both in terms of morphological complexity and BLEU performance.
72	12	To find that out, we experiment with a modified Italian target language without gender marking, i.e. all gender-marked words are replaced by their masculine form (FRIT ∗).
73	23	This language pair achieves a slightly higher BLEU score than FRIT (33.2 vs 32.6), which can be attributed to the smaller target vocabulary.
74	36	However its source gender accuracy is much worse (see Fig.
75	12	3), which indicates that the high performance of the FRIT encoder is mostly due to the ubiquitous gender marking in the target language, rather than to language relatedness.
76	32	All this suggests that source morphological features contribute to sentence understanding to some degree, but the incentive to learn them mostly depends on how directly they can be transferred to the target sentence.
80	21	3 (right).4 Note that, while word embeddings are identical for the three language pairs, recurrent states change according to the language tag.
81	10	In this setup the target language impact is less visible and gender accuracy at the LSTM state level is overall much higher than that of the mono-target systems (0.77 vs 0.68 on average) whereas BLEU scores are slightly lower (−0.9% on average).
82	13	While this is only an initial exploration of multilingual NMT systems, our results suggest that this kind of multi-task objective pushes the model to learn linguistic features in a more consistent way (Bjerva, 2017; Enguehard et al., 2017).
