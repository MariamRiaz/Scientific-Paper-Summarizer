16	20	We present a novel neural dialog model adapted from conditional variational autoencoders (CVAE) (Yan et al., 2015; Sohn et al., 2015), which introduces a latent variable that can capture discourse-level variations as described above 2.
24	59	To tackle this problem, one line of research has focused on augmenting the input of encoder-decoder models with richer context information, in order to generate more spe- cific responses.
26	17	Xing et al., (2016) maintain topic encoding based on Latent Dirichlet Allocation (LDA) (Blei et al., 2003) of the conversation to encourage the model to output more topic coherent responses.
34	16	Thus, they initialized a encoderdecoder model with MLE objective and leveraged reinforcement learning to fine tune the model by optimizing three heuristic rewards functions: informativity, coherence, and ease of answering.
47	26	To improve upon the past models, we firstly introduce a novel mechanism to leverage linguistic knowledge in training end-to-end neural dialog models, and we also propose a novel training technique that mitigates the vanishing latent variable problem.
48	15	Each dyadic conversation can be represented via three random variables: the dialog context c (context window size k − 1), the response utterance x (the kth utterance) and a latent variable z, which is used to capture the latent distribution over the valid responses.
50	17	We then define the conditional distribution p(x, z|c) = p(x|z, c)p(z|c) and our goal is to use deep neural networks (parametrized by θ) to approximate p(z|c) and p(x|z, c).
51	19	We refer to pθ(z|c) as the prior network and pθ(x, |z, c) as the response decoder.
55	33	CVAE is trained to maximize the conditional log likelihood of x given c, which involves an intractable marginalization over the latent variable z.
62	58	Since we assume z follows isotropic Gaussian distribution, the recognition network qφ(z|x, c) ∼ N (µ, σ2I) and the prior network pθ(z|c) ∼ N (µ′, σ′2I), and then we have: [ µ log(σ2) ] =Wr [ x c ] + br (2) [ µ′ log(σ′2) ] = MLPp(c) (3) We then use the reparametrization trick (Kingma and Welling, 2013) to obtain samples of z either from N (z;µ, σ2I) predicted by the recognition network (training) or N (z;µ′, σ′2I) predicted by the prior network (testing).
74	29	KgCVAE model is trained by maximizing: L(θ, φ;x, c, y) = −KL(qφ(z|x, c, y)‖Pθ(z|c)) +Eqφ(z|c,x,y)[log p(x|z, c, y)] +Eqφ(z|c,x,y)[log p(y|z, c)] (4) Since now the reconstruction of y is a part of the loss function, kgCVAE can more efficiently encode y-related information into z than discovering it only based on the surface-level x and c. Another advantage of kgCVAE is that it can output a highlevel label (e.g. dialog act) along with the wordlevel responses, which allows easier interpretation of the model’s outputs.
75	33	A straightforward VAE with RNN decoder fails to encode meaningful information in z due to the vanishing latent variable problem (Bowman et al., 2015).
76	16	Bowman et al., (2015) proposed two solutions: (1) KL annealing: gradually increasing the weight of the KL term from 0 to 1 during training; (2) word drop decoding: setting a certain percentage of the target words to 0.
121	44	The generalized responselevel precision/recall for a given dialog context is: precision(c) = ∑N i=1maxj∈[1,Mc]d(rj , hi) N recall(c) = ∑Mc j=1maxi∈[1,N ]d(rj , hi)) Mc where d(rj , hi) is a distance function which lies between 0 to 1 and measures the similarities between rj and hi.
127	23	We used Glove embedding described in Section 4 and denote the average method as A-bow and extrema method as E-bow.
149	22	Further, we notice that the generated text exhibits similar dialog acts compared to the ones predicted separately by the model, implying the consistency of natural language generation based on y.
150	32	On the contrary, the responses from the baseline model are limited to local n-gram variations and share a similar prefix, i.e. ”I’m”.
154	19	The kgCVAE model is also able to generate various ways of back-channeling.
159	29	We found that the learned latent space is highly correlated with the dialog act and length of responses, which confirms our assumption.
161	19	To compare with past work (Bowman et al., 2015), we conducted the same language modelling (LM) task on Penn Treebank using VAE.
174	55	The model with BOW loss, however, consistently converges to a non-trivial KL cost even without KLA, which confirms the importance of BOW loss for training latent variable models with the RNN decoder.
175	34	Last but not least, our experiments showed that the conclusions drawn from LM using VAE also apply to training CVAE/kgCVAE, so we used BOW loss together with KLA for all previous experiments.
178	74	In turn, the output of this novel neural dialog model will be easier to explain and control by humans.
179	32	In addition to dialog acts, we plan to apply our kgCVAE model to capture other different linguistic phenomena including sentiment, named entities,etc.
180	29	Last but not least, the recognition network in our model will serve as the foundation for designing a datadriven dialog manager, which automatically discovers useful high-level intents.
181	43	All of the above suggest a promising research direction.
182	21	Variational Lower Bound for kgCVAE We assume that even with the presence of linguistic feature y regarding x, the prediction of xbow still only depends on the z and c. Therefore, we have: L(θ, φ;x, c, y) = −KL(qφ(z|x, c, y)‖Pθ(z|c)) +Eqφ(z|c,x,y)[log p(x|z, c, y)] +Eqφ(z|c,x,y)[log p(y|z, c)] +Eqφ(z|c,x,y)[log p(xbow|z, c)] (7) Collection of Multiple Reference Responses We collected multiple reference responses for each dialog context in the test set by information retrieval techniques combining with traditional a machine learning method.
184	123	Then we computed the similarity d(ci, cj) between two dialog contexts using: d(ci, cj) = 1(ti = tj)1(ti = tj) hi · hj ||hi||||hj || (8) Unlike past work (Sordoni et al., 2015), this similarity function only cares about the distance in the context and imposes no constraints on the response, therefore is suitbale for finding diverse responses regarding to the same dialog context.
185	32	Secondly, for each dialog context in the test set, we retrieved the 10 nearest neighbors from the training set and treated the responses from the training set as candidate reference responses.
186	49	Thirdly, we further sampled 240 context-responses pairs from 5481 pairs in the total test set and post-processed the selected candidate responses by two human computational linguistic experts who were told to give a binary label for each candidate response about whether the response is appropriate regarding its dialog context.
