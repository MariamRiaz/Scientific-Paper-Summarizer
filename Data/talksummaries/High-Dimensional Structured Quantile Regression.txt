33	22	In Section 2, we discuss the problem formulation along with assumptions, review the general framework for analyzing regularized estimation problems and discuss the three atomic norms used as examples throughout the paper.
39	15	We consider data is generated as y = Xθ + ω, X ∈ Rn×p is the design matrix, θ ∈ Rp and ω ∈ Rn is the noise.
43	20	We do not make any assumptions on the noise vector ω ∈ Rn.
46	58	We consider a parametric quantile regression model where the τ th conditional quantile function of the response vari- able yi given any xi ∈ Rp is given by, F−1yi|xi(τ |xi) = 〈xi, θ ∗ τ 〉, θ∗τ ∈ Rp, τ ∈ (0, 1) , (2) where F−1yi|xi is the inverse of the conditional distribution function of yi given xi.
48	54	The goal is to estimate parameter θ̂τ close to θ∗τ using n observations of the data when n < p. The estimator in this paper belongs to the family of regularized estimators and is of the form: θ̂λn,τ := argminθ∈Rp Lτ (θ) + λnR(θ) , (3) where Lτ (θ) = 1n ∑n i=1 ρτ (yi−〈xi, θ〉), ρτ (·) is the quantile loss function and R(·) is any atomic norm.
49	13	Examples of atomic norms we consider in this paper are the l1, l1/l2 nonoverlapping group sparse norm and the k-support norm.
50	66	We present all results assuming any single τ ∈ (0, 1) and going forward drop the subscripts from θ∗τ and θ̂λn,τ .
66	17	Let ‖θ‖A denote the gauge of A, R(θ) = ‖θ‖A = inf{t > 0 : θ ∈ tconv(A)} (11) = inf{ ∑ a∈A ca : θ = ∑ a∈A caa, ca ≥ 0,∀a ∈ A} .
69	37	Am similar to the setting considered in Chen & Banerjee (2015).
74	15	Let θNG denote a vector with coordinates θiNG = θ i if i ∈ GNG , else θiNG = 0.
98	35	(14) For the regularized quantile regression problem penalized with the atomic norm R(θ) = ‖θ‖A, θ̂ = arg min θ∈Rp Lτ (θ)+λR(θ) = arg min θ∈Rp 1 n n∑ i=1 ρτ (θ)+λR(θ) , (15) let C := { u|R(θ∗ + u) ≤ R(θ∗) + 12R(u) } denote the error set, let λ ≥ R∗(∇Lτ (θ∗)) and let n ≥ (c1s + c2Ψ 2(C)w(A)) where Ψ(C) = sup u∈C ‖u‖A ‖u‖2 is the norm compatibility constant in the error set, w(A) is the Gaussian width of the unit norm ball and c1 and c2 are some constants.
99	53	Then with probability atleast 1 − exp(−c2k1 log(em))−2 exp(−ηw2(C)) the number of in- terpolated samples, ν = sup u∈C |{i : yi = 〈xi, θ∗+u〉, u ∈ C}| ≤ cΨ2(C)w2(A) , (16) where c is a constant.
100	75	To understand the intuition consider the case of the l1 norm.
111	13	For any group i, let θi denote the vector constructed from θ such that it has component k, θk = 0 if k /∈ i.
134	25	A major difference to the least squares loss setting, is the independence of the regularization parameter to assumptions on the noise vector (see for example Theorem 3 and Theorem 4 in Banerjee et al. (2014) where the noise is explicitly assumed to be subGaussian and homoscedastic and the noise enters the analysis through properties of ‖ω‖2).
136	12	Indeed the most interesting applications of quantile regression arise in such settings.
148	14	In general, as shown in Section 3, ν gets determined by the structure.
152	14	The first line follows from the definition of the cumulative distribution function, the second line by a simple Taylor series expansion, the last line by the assumption that f ≤ fi(ξi),∀i and (1/n)‖Xu‖22 ≥ κ, where κ is the restricted eigenvalue (RE) constant.
154	17	More generally RSC is a condition on the minimum eigenvalue of the Jacobian matrix 1n ∑n i=1 fi(ξi)〈xi, u〉2 restricted to the error set C. This quantity has also been considered in prior literature (see Section 4.2 in Koenker (2005) and the proof in page 121, also see condition D.1 in Belloni & Chernozhukov (2011)).
156	11	Theorem 4.2 ConsiderX ∈ Rn×p has subGaussian rows.
157	41	Let 0 < f < fi(〈xi, θ∗〉) be a uniform lower bound on the conditional density for all xi in the support of x.
170	22	The two norm of the error depends on the two terms√ τ(1− τ) and f .
172	12	But typically this is dominated by the lower bound on the density f term which makes the estimate less precise in regions of low density.
176	15	All results make no assumptions on the noise apart from an assumption on the lower bound of the noise density.
179	15	We perform simulations with synthetic data.
184	24	For each n we generate 100 datasets with the probability of success defined as the fraction of times we are able to faithfully estimate the true parameter.
192	19	For heavy-tailed noise we consider the student t-distribution with different degrees of freedom, with lower degrees of freedom corresponding to heavier tailed data.
194	15	We vary the proportion of contamination from 2.5% to 15%.
196	66	Again for both exercises, we run 100 simulations and plot the mean and standard deviation of the estimation error ‖θ̂ − θ∗‖2.
204	16	We also prove that NIPS is of the order of square of the Gaussian width of the error set for many atomic norms - which is the same order as that for regularized least squares regression and match results from previous work for the l1 norm (Belloni & Chernozhukov, 2011).
