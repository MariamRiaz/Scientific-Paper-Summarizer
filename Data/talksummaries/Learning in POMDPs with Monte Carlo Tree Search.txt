0	35	The Partially Observable Markov Decision Processes (POMDP) (Kaelbling et al., 1998) is a general model for sequential decision making in stochastic and partially observable environments, which are ubiquitous in real-world problems.
1	17	A key shortcoming of POMDP methods is the assumption that the dynamics of the environment are known a priori.
2	85	In real-world applications, however, it may be impossible to obtain a complete and accurate description of the system.
3	18	Instead, we may have uncertain prior knowledge about the model.
4	21	When lacking a model, a prior can be incorporated into the POMDP problem in a principled way, as demonstrated by the Bayes-Adaptive POMDP framework (Ross et al., 2011).
6	37	This method casts the Bayesian reinforcement learning problem into a POMDP planning problem where the hidden model of the environment is part of the state space.
11	13	In particular, we improve the sampling approach by exploiting the structure of the BA-POMDP resulting in root sampling and expected models methods.
12	16	We also present an approach for more efficient state representation, which we call linking states.
14	23	As a result, we present methods that significantly improve the scalability of learning in BA-POMDPs, making them practical for larger problems.
15	22	First, we discuss POMDPs and BA-POMDPs in Sections 2.1 and 2.2.
16	21	Formally, a POMDP is described by a tuple (S, A, Z, D, R, γ, h), where S is the set of states of the environment; A is the set of actions; Z is the set of observations; D is the ‘dynamics function’ that describes the dynamics of the system in the form of transition probabilities D(s′,z|s,a);1 1 This formulation generalizes the typical formulation with separate transition T and observation functions O: D = 〈T,O〉.
18	28	R is the immediate reward function R(s, a) that describes the reward of selecting a in s; γ ∈ (0,1) is the discount factor; and h is the horizon of an episode in the system.
20	114	The agent has no direct access to the system’s state, so it can only rely on the action-observation history ht = 〈a0,z1, .
21	16	,at−1,zt〉 up to the current step t. It can use this history to maintain a probability distribution over the state, also called a belief, b(s).
23	14	Solution methods aim to find an optimal policy, a mapping from a belief to an action with the highest possible expected return.
24	16	The agent maintains its belief during execution through belief updates.
26	17	This operation is infeasible for large spaces because it enumerates over the entire state space.
29	44	Each particle represents a probability of 1K ; if a specific state x occurs n times in a particle filter, then P (x) = nK .
31	49	Given some action a, the agent repeatedly samples a state s from particle filter, then simulates the execution of a on s through D, and receives a (simulated) new state s′sim and observation zsim.
32	21	s′ is added to the new belief only when zsim == z, and rejected otherwise.
33	25	This process repeats until the new belief contains K particles.
34	58	Partially Observable Monte-Carlo Planning (POMCP) (Silver & Veness, 2010), is a scalable method which extends Monte Carlo tree search (MCTS) to solve POMDPs.
35	105	POMCP is one of the leading algorithms for solving general POMDPs.
36	32	At each time step, the algorithm performs online planning by incrementally building a lookahead tree that contains (statistics that represent) Q(h,a), where h is the action-observation history h to reach that node.
37	71	It samples hidden states s at the root node (called ‘root sampling’) and uses that state to sample a trajectory that first traverses the lookahead tree and then performs a (random) rollout.
38	18	The return of this trajectory is used to update the statistics for all visited nodes.
39	48	These statistics include the number of times an action has been taken at a history (N(h,a)) and estimated value of being in that node (Q(h,a)), based on an average over the returns.
42	35	At the end of each simulation, the discounted accumulated return is used to update the estimated value of all the nodes in the tree that have been visited during that simulation.
44	25	The agent then picks the action with the highest estimated value (maxaQ(b,a)).
51	79	A fundamental RL problem is the difficulty of deciding whether to select actions in order to learn a better model of the environment, or to exploit the current knowledge about the rewards and effects of actions.
