0,1,label2,summary_sentences
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2094–2099, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
"Neural network translation models, which learn mappings over real-valued vector representations in high-dimensional space, have recently achieved large gains in translation accuracy (Hu et al., 2014; Devlin et al., 2014; Sundermeyer et al., 2014; Auli et al., 2013; Schwenk, 2012; Sutskever et al., 2014; Bahdanau et al., 2015).
",1 Introduction,[0],[0]
"Notably, Devlin et al. (2014) proposed a neural network joint model (NNJM), which augments the n-gram neural network language model (NNLM) with an m-word source context window, as shown in Figure 1a.",1 Introduction,[0],[0]
"While this model is effective, the computation cost of using it in a large-vocabulary SMT task is quite expensive, as probabilities need to be normalized over the entire vocabulary.",1 Introduction,[0],[0]
"To solve this problem, Devlin et al. (2014) presented a technique to train the NNJM to be selfnormalized and avoided the expensive normalization cost during decoding.",1 Introduction,[0],[0]
"However, they also
note that this self-normalization technique sacrifices neural network accuracy, and the training process for the self-normalized neural network is very slow, as with standard maximum likelihood estimation (MLE).
",1 Introduction,[0],[0]
"To remedy the problem of long training times in the context of NNLMs, Vaswani et al. (2013) used a method called noise contrastive estimation (NCE).",1 Introduction,[0],[0]
"Compared with MLE, NCE does not require repeated summations over the whole vocabulary and performs nonlinear logistic regression to discriminate between the observed data and artificially generated noise.
",1 Introduction,[0],[0]
"This paper proposes an alternative framework of binarized NNJMs (BNNJM), which are similar to the NNJM, but use the current target word not as the output, but as the input of the neural network, estimating whether the target word under examination is correct or not, as shown in Figure 1b.",1 Introduction,[0],[0]
"Because the BNNJM uses the current target word as input, the information about the current target word can be combined with the context word information and processed in the hidden layers.
",1 Introduction,[0],[0]
"The BNNJM learns a simple binary classifier, given the context and target words, therefore it can be trained by MLE very efficiently.",1 Introduction,[0],[0]
"“Incorrect” target words for the BNNJM can be generated in the same way as NCE generates noise
2094
for the NNJM.",1 Introduction,[0],[0]
We present a novel noise distribution based on translation probabilities to train the NNJM and the BNNJM efficiently.,1 Introduction,[0],[0]
Let T = t|T |1 be a translation of S = s |S| 1 .,2 Neural Network Joint Model,[0],[0]
"The NNJM (Devlin et al., 2014) defines the following probability,
P (T |S) = ∏|T | i=1",2 Neural Network Joint Model,[0],[0]
"P ( ti|sai+(m−1)/2ai−(m−1)/2, t i−1 i−n+1 ) (1) where target word ti is affiliated with source word sai .",2 Neural Network Joint Model,[0],[0]
Affiliation ai is derived from the word alignments using heuristics1.,2 Neural Network Joint Model,[0],[0]
"To estimate these probabilities, the NNJM uses m source context words and n− 1 target history words as input to a neural network and performs estimation of unnormalized probabilities p (ti|C) before normalizing over all words in the target vocabulary V ,
P (ti|C) = p(ti|C)Z(C) Z (C) = ∑ ti′∈V p (ti′|C) (2)
where C stands for source and target context words as in Equation 1.
",2 Neural Network Joint Model,[0],[0]
"The NNJM can be trained on a word-aligned parallel corpus using standard MLE, but the cost of normalizing over the entire vocabulary to calculate the denominator in Equation 2 is quite large.",2 Neural Network Joint Model,[0],[0]
"Devlin et al. (2014)’s self-normalization technique can avoid normalization cost during decoding, but not during training.
",2 Neural Network Joint Model,[0],[0]
"NCE can be used to train NNLM-style models (Vaswani et al., 2013) to reduce training times.",2 Neural Network Joint Model,[0],[0]
NCE creates a noise distribution q,2 Neural Network Joint Model,[0],[0]
"(ti), selects k noise samples ti1, ..., tik for each ti and introduces a random variable v which is 1 for training examples and 0 for noise samples,
P (v = 1, ti|C)",2 Neural Network Joint Model,[0],[0]
"= 11+k · p(ti|C)Z(C) P (v = 0, ti|C) =",2 Neural Network Joint Model,[0],[0]
"k1+k · q (ti) .
",2 Neural Network Joint Model,[0],[0]
"NCE trains the model to distinguish training data from noise by maximize the conditional likelihood,
L = log P (v = 1|C, ti) + k∑
j=1
log P (v = 0|C, tik).
",2 Neural Network Joint Model,[0],[0]
"The normalization cost can be avoided by using p (ti|C) as an approximation of P (ti|C).2
1If ti aligns to exactly one source word, ai is the index of this source word; If ti aligns to multiple source words, ai is the index of the aligned word in the middle; If ti is unaligned, they inherit its affiliation from the closest aligned word.
",2 Neural Network Joint Model,[0],[0]
"2The theoretical properties of self-normalization techniques, including NCE and Devlin et al. (2014)’s method, are investigated by Andreas and Klein (2015).",2 Neural Network Joint Model,[0],[0]
"In this paper, we propose a new framework of the binarized NNJM (BNNJM), which is similar to the NNJM but learns not to predict the next word given the context, but solves a binary classification problem by adding a variable v ∈ {0, 1} that stands for whether the current target word ti is correctly/wrongly produced in terms of source context words sai+(m−1)/2ai−(m−1)/2 and target history words ti−1i−n+1 ,
P ( v|sai+(m−1)/2ai−(m−1)/2, t i−1 i−n+1, ti ) .
",3 Binarized NNJM,[0],[0]
"The BNNJM is learned by a feedforward neural network with m + n inputs{ s ai+(m−1)/2 ai−(m−1)/2, t i−1 i−n+1, ti } and two outputs for v = 1/0.",3 Binarized NNJM,[0],[0]
"Because the BNNJM uses the current target word as input, the information about the current target word can be combined with the context word information and processed in the hidden layers.",3 Binarized NNJM,[0],[0]
"Thus, the hidden layers can be used to learn the difference between correct target words and noise in the BNNJM, while in the NNJM the hidden layers just contain information about context words and only the output layer can be used to discriminate between the training data and noise, giving the BNNJM more power to learn this classification problem.
",3 Binarized NNJM,[0],[0]
"We can use the BNNJM probability in translation as an approximation for the NNJM as below,
P ( ti|sai+(m−1)/2ai−(m−1)/2, t i−1 i−n+1 )",3 Binarized NNJM,[0],[0]
"≈ P ( v = 1|sai+(m−1)/2ai−(m−1)/2, t i−1 i−n+1, ti ) .
",3 Binarized NNJM,[0],[0]
"As a binary classifier, the gradient for a single example in the BNNJM can be calculated efficiently by MLE without it being necessary to calculate the softmax over the full vocabulary.",3 Binarized NNJM,[0],[0]
"On the other hand, we need to create “positive” and “negative” examples for the classifier.",3 Binarized NNJM,[0],[0]
"Positive examples can be extracted directly from the word-aligned parallel corpus as〈 s ai+(m−1)/2 ai−(m−1)/2, t i−1 i−n+1, ti 〉 ; Negative examples can be generated for each positive example in the same way that NCE generates noise data as〈 s ai+(m−1)/2 ai−(m−1)/2, t i−1 i−n+1, ti ′ 〉 , where ti′ ∈ V \ {ti}.",3 Binarized NNJM,[0],[0]
"Vaswani et al. (2013) adopted the unigram probability distribution (UPD) to sample noise for train-
ing NNLMs with NCE,
q (ti′) = occur(ti ′)∑ ti ′′∈V occur(ti′′)
where occur (ti′) stands for how many times ti′ occurs in the training corpus.",4.1 Unigram Noise,[0],[0]
"In this paper, we propose a noise distribution specialized for translation models, such as the NNJM or BNNJM.
",4.2 Translation Model Noise,[0],[0]
Figure 2 gives a Chinese-to-English parallel sentence pair with word alignments to demonstrate the intuition behind our method.,4.2 Translation Model Noise,[0],[0]
"Focusing on sai=“安排”, this is translated into ti =“arrange”.",4.2 Translation Model Noise,[0],[0]
"For this positive example, UPD is allowed to sample any arbitrary noise, such as ti′ = “banana”.",4.2 Translation Model Noise,[0],[0]
"However, in this case, noise ti′ = “banana” is not useful for model training, as constraints on possible translations given by the phrase table ensure that “安排” will never be translated into “banana”.",4.2 Translation Model Noise,[0],[0]
"On the other hand, noise ti′ = “arranges” and “arrangement” are both possible translations of “安排” and therefore useful training data, that we would like our model to penalize.
",4.2 Translation Model Noise,[0],[0]
"Based on this intuition, we propose the use of another noise distribution that only uses ti′ that are possible translations of sai , i.e., ti ′ ∈",4.2 Translation Model Noise,[0],[0]
"U (sai) \ {ti}, where U (sai) contains all target words aligned to sai in the parallel corpus.
",4.2 Translation Model Noise,[0],[0]
"Because U (sai) may be quite large and contain many wrong translations caused by wrong alignments, “banana” may actually be included in U (“安排”).",4.2 Translation Model Noise,[0],[0]
"To mitigate the effect of uncommon examples, we use a translation probability distribution (TPD) to sample noise ti′ from U (sai)",4.2 Translation Model Noise,[0],[0]
\,4.2 Translation Model Noise,[0],[0]
"{ti} as follows,
q (ti′|sai) = align(sai ,ti′)∑
ti ′′∈U(sai )
align(sai ,ti′′)
where align (sai , ti ′) is how many times ti′ is aligned to sai in the parallel corpus.",4.2 Translation Model Noise,[0],[0]
"Note that ti could be unaligned, in which case we assume that it is aligned to a special null word.",4.2 Translation Model Noise,[0],[0]
Noise for unaligned words is sampled according to the TPD of the null word.,4.2 Translation Model Noise,[0],[0]
"If several target/source words are aligned to one source/target word, we
choose to combine these target/source words as a new target/source word.3",4.2 Translation Model Noise,[0],[0]
"We evaluated the effectiveness of the proposed approach for Chinese-to-English (CE), Japanese-toEnglish (JE) and French-to-English (FE) translation tasks.",5.1 Setting,[0],[0]
"The datasets officially provided for the patent machine translation task at NTCIR-9 (Goto et al., 2011) were used for the CE and JE tasks.",5.1 Setting,[0],[0]
The development and test sets were both provided for the CE task while only the test set was provided for the JE task.,5.1 Setting,[0],[0]
"Therefore, we used the sentences from the NTCIR-8 JE test set as the development set.",5.1 Setting,[0],[0]
"Word segmentation was done by BaseSeg (Zhao et al., 2006) for Chinese and Mecab4 for Japanese.",5.1 Setting,[0],[0]
"For the FE language pair, we used standard data for the WMT 2014 translation task.",5.1 Setting,[0],[0]
"The training sets for CE, JE and FE tasks contain 1M, 3M and 2M sentence pairs, respectively.
",5.1 Setting,[0],[0]
"For each translation task, a recent version of Moses HPB decoder (Koehn et al., 2007) with the training scripts was used as the baseline (Base).",5.1 Setting,[0],[0]
"We used the default parameters for Moses, and a 5-gram language model was trained on the target side of the training corpus using the IRSTLM Toolkit5 with improved Kneser-Ney smoothing.",5.1 Setting,[0],[0]
"Feature weights were tuned by MERT (Och, 2003).
",5.1 Setting,[0],[0]
"The word-aligned training set was used to learn the NNJM and the BNNJM.6 For both NNJM and BNNJM, we set m = 7 and n = 5.",5.1 Setting,[0],[0]
The NNJM was trained by NCE using UPD and TPD as noise distributions.,5.1 Setting,[0],[0]
"The BNNJM was trained by standard MLE using UPD and TPD to generate negative examples.
",5.1 Setting,[0],[0]
The number of noise samples for NCE was set to be 100.,5.1 Setting,[0],[0]
"For the BNNJM, we used only one negative example for each positive example in each training epoch, as the BNNJM needs to calculate
3The processing for multiple alignments helps sample more useful negative examples for TPD, and had little effect on the translation performance when UPD was used as the noise distribution for the NNJM and the BNNJM in our preliminary experiments.
",5.1 Setting,[0],[0]
"4http://sourceforge.net/projects/mecab/files/ 5http://hlt.fbk.eu/en/irstlm 6Both the NNJM and the BNNJM had one hidden layer, 100 hidden nodes, input embedding dimension 50, output embedding dimension 50.",5.1 Setting,[0],[0]
A small set of training data was used as validation data.,5.1 Setting,[0],[0]
"The training process was stopped when validation likelihood stopped increasing.
",5.1 Setting,[0],[0]
the whole neural network (not just the output layer like the NNJM) for each noise sample and thus noise computation is more expensive.,5.1 Setting,[0],[0]
"However, for different epochs, we resampled the negative example for each positive example, so the BNNJM can make use of different negative examples.",5.1 Setting,[0],[0]
"Table 1 shows how many epochs these two models needed and the training time for each epoch on a 10-core 3.47GHz Xeon X5690 machine.7 Translation results are shown in Table 2.
",5.2 Results and Discussion,[0],[0]
"We can see that using TPD instead of UPD as a noise distribution for the NNJM trained by NCE can speed up the training process significantly, with a small improvement in performance.",5.2 Results and Discussion,[0],[0]
"But for the BNNJM, using different noise distributions affects translation performance significantly.",5.2 Results and Discussion,[0],[0]
"The BNNJM with UPD does not improve over the baseline system, likely due to the small number of noise samples used in training the BNNJM, while the BNNJM with TPD achieves good performance, even better than the NNJM with TPD on the Chinese-to-English and French-to-English translation tasks.
",5.2 Results and Discussion,[0],[0]
"From Table 2, the NNJM does not improve translation performance significantly on the FE task.",5.2 Results and Discussion,[0],[0]
"Note that the baseline BLEU for the FE
7The decoding time for the NNJM and the BNNJM were similar, since the NNJM trained by NCE uses p (ti|C) as an approximation of P (ti|C) without normalization and the BNNJM only needs to be normalized over two output neurons.
task is lower than CE and JE tasks, indicating that learning is harder for the FE task than CE and JE tasks.",5.2 Results and Discussion,[0],[0]
"The validation perplexities of the NNJM with UPD for CE, JE and FE tasks are 4.03, 3.49 and 8.37.",5.2 Results and Discussion,[0],[0]
"Despite these difficult learning circumstances and lack of large gains for the NNJM, the BNNJM improves translations significantly for the FE task, suggesting that the BNNJM is more robust to difficult translation tasks that are hard for the NNJM.
",5.2 Results and Discussion,[0],[0]
Table 3 gives Chinese-to-English translation examples to demonstrate how the BNNJM (with TPD) helps to improve translations over the NNJM (with TPD).,5.2 Results and Discussion,[0],[0]
"In this case, the BNNJM helps to translate the phrase “该 移动 持续 到” better.",5.2 Results and Discussion,[0],[0]
Table 4 gives translation scores for these two translations calculated by the NNJM and the BNNJM.,5.2 Results and Discussion,[0],[0]
"Context words are used for predictions but not shown in the table.
",5.2 Results and Discussion,[0],[0]
"As can be seen, the BNNJM prefers T2 while the NNJM prefers T1.",5.2 Results and Discussion,[0],[0]
"Among these predictions, the NNJM and the BNNJM predict the translation for “到” most differently.",5.2 Results and Discussion,[0],[0]
"The NNJM clearly predicts that in this case “到” should be translated into “to” more than “until”, likely because this example rarely occurs in the training corpus.",5.2 Results and Discussion,[0],[0]
"However, the BNNJM prefers “until” more than “to”, which
demonstrates the BNNJM’s robustness to less frequent examples.",5.2 Results and Discussion,[0],[0]
"Finally, we examine the translation results to explore why the BNNJM with TPD did not outperform the NNJM with TPD for the JE translation task, as it did for the other translation tasks.",5.3 Analysis for JE Translation Results,[0],[0]
"We found that using the BNNJM instead of the NNJM on the JE task did improve translation quality significantly for infrequent words, but not for frequent words.
",5.3 Analysis for JE Translation Results,[0],[0]
"First, we describe how we estimate translation quality for infrequent words.",5.3 Analysis for JE Translation Results,[0],[0]
"Suppose we have a test set S, a reference set R and a translation set T with I sentences, Si (1 ≤",5.3 Analysis for JE Translation Results,[0],[0]
i ≤,5.3 Analysis for JE Translation Results,[0],[0]
"I) , Ri (1 ≤",5.3 Analysis for JE Translation Results,[0],[0]
i ≤,5.3 Analysis for JE Translation Results,[0],[0]
"I) , Ti (1 ≤",5.3 Analysis for JE Translation Results,[0],[0]
i ≤,5.3 Analysis for JE Translation Results,[0],[0]
I),5.3 Analysis for JE Translation Results,[0],[0]
"Ti contains J individual words, Wij ∈Words (Ti) To (Wij) is how many times Wij occurs in Ti and Ro (Wij) is how many times Wij occurs in Ri.
",5.3 Analysis for JE Translation Results,[0],[0]
"The general 1-gram translation accuracy (Papineni et al., 2002) is calculated as,
Pg =
I∑ i=1 J∑ j=1 min(To(Wij),Ro(Wij))
I∑ i=1 J∑ j=1 To(Wij)
",5.3 Analysis for JE Translation Results,[0],[0]
"This general 1-gram translation accuracy does not distinguish word frequency.
",5.3 Analysis for JE Translation Results,[0],[0]
"We use a modified 1-gram translation accuracy that weights infrequent words more heavily,
Pc =
I∑ i=1 J∑ j=1 min(To(Wij),Ro(Wij))· 1 Occur(Wij)
I∑ i=1 J∑ j=1 To(Wij)
where Occur (Wij) is how many times Wij occurs in the whole reference set.",5.3 Analysis for JE Translation Results,[0],[0]
"Note Pc will not be 1 even in the case of completely accurate translations, but it can approximately reflect infrequent word translation accuracy, since correct frequent word translations contribute less to Pc.
Table 5 shows Pg and Pc for different translation tasks.",5.3 Analysis for JE Translation Results,[0],[0]
"It can be seen that the BNNJM improves infrequent word translation quality similarly for all translation tasks, but improves general translation quality less for the JE task than the other translation tasks.",5.3 Analysis for JE Translation Results,[0],[0]
"We conjecture that the reason why the BNNJM is less useful for frequent word translations on the JE task is the fact that the JE parallel corpus has less accurate function word alignments than other language pairs, as the
grammatical features of Japanese and English are quite different.8 Wrong function word alignments will make noise sampling less effective and therefore lower the BNNJM performance for function word translations.",5.3 Analysis for JE Translation Results,[0],[0]
"Although wrong word alignments will also make noise sampling less effective for the NNJM, the BNNJM only uses one noise sample for each positive example, so wrong word alignments affect the BNNJM more than the NNJM.",5.3 Analysis for JE Translation Results,[0],[0]
Xu et al. (2011) proposed a method to use binary classifiers to learn NNLMs.,6 Related Work,[0],[0]
"But they also used the current target word in the output, similarly to NCE.",6 Related Work,[0],[0]
"The BNNJM uses the current target word as input, so the information about the current target word can be combined with the context word information and processed in hidden layers.
",6 Related Work,[0],[0]
Mauser et al. (2009) presented discriminative lexicon models to predict target words.,6 Related Work,[0],[0]
"They train a separate classifier for each target word, as these lexicon models use discrete representations of words and different classifiers do not share features.",6 Related Work,[0],[0]
"In contrast, the BNNJM uses real-valued vector representations of words and shares features, so we train one classifier and can use the similarity information between words.",6 Related Work,[0],[0]
"This paper proposes an alternative to the NNJM, the BNNJM, which learns a binary classifier that takes both the context and target words as input and combines all useful information in the hidden layers.",7 Conclusion,[0],[0]
We also present a novel noise distribution based on translation probabilities to train the BNNJM efficiently.,7 Conclusion,[0],[0]
"With the improved noise sampling method, the BNNJM can achieve comparable performance with the NNJM and even improve the translation results over the NNJM on Chineseto-English and French-to-English translations.
",7 Conclusion,[0],[0]
8Infrequent words are usually content words and frequent words are usually function words.,7 Conclusion,[0],[0]
"The neural network joint model (NNJM), which augments the neural network language model (NNLM) with an m-word source context window, has achieved large gains in machine translation accuracy, but also has problems with high normalization cost when using large vocabularies.",abstractText,[0],[0]
"Training the NNJM with noise-contrastive estimation (NCE), instead of standard maximum likelihood estimation (MLE), can reduce computation cost.",abstractText,[0],[0]
"In this paper, we propose an alternative to NCE, the binarized NNJM (BNNJM), which learns a binary classifier that takes both the context and target words as input, and can be efficiently trained using MLE.",abstractText,[0],[0]
We compare the BNNJM and NNJM trained by NCE on various translation tasks.,abstractText,[0],[0]
A Binarized Neural Network Joint Model for Machine Translation,title,[0],[0]
"We are interested in time series settings where we observe data {Yt ∈ Y : t = 1, . . .",1. Introduction - Problem Statement,[0],[0]
", L}.",1. Introduction - Problem Statement,[0],[0]
We consider problems where the observations are explained by a latent structure which assigns objects to features and this feature allocation changes over time.,1. Introduction - Problem Statement,[0],[0]
"For instance, consider the topics covered by a number of newspapers over time; some topics “die” while new ones are “born”.",1. Introduction - Problem Statement,[0],[0]
"The topic coverage of each paper is its latent feature allocation which could be modelled with an Indian buffet process (Griffiths & Ghahramani, 2011, IBP).",1. Introduction - Problem Statement,[0],[0]
"While static feature allocation models are well studied, these are not able to handle the time series nature of many datasets.",1. Introduction - Problem Statement,[0],[0]
"We propose a process that extends the IBP by allowing the feature allocation to evolve over the covariate as a result of “birth” and “death” of features.
1University of Oxford, Oxford, UK 2Stanford University, California, USA 3University of Cambridge, Cambridge, UK 4Uber AI Labs, SF, California, USA.",1. Introduction - Problem Statement,[0],[0]
"Correspondence to: Konstantina Palla <konstantina.palla@gmail.com>.
",1. Introduction - Problem Statement,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction - Problem Statement,[0],[0]
Copyright 2017 by the author(s).,1. Introduction - Problem Statement,[0],[0]
"We target problems where the data depends on a covariate, such as time or space, and is explained by a latent structure, in particular a (multi-membership) clustering of the data points.",2. Related Work,[0],[0]
The observations are result of the underlying partitioning and its evolution over the covariate.,2. Related Work,[0],[0]
Typical models fall in two main categories: clustering and feature allocation.,2. Related Work,[0],[0]
"The former allow each data point to belong to one and only one class (cluster), while the latter let each data point belong to multiple groups (features).",2. Related Work,[0],[0]
"Bayesian nonparametric approaches are primarily based on the Chinese restaurant process (CRP, Aldous, 1983) or the Indian buffet process (IBP, Griffiths & Ghahramani, 2005) corresponding to the two categories.",2. Related Work,[0],[0]
"In particular, a sample from a CRP is an assignment of data points to disjoint classes (a clustering), while a sample from an IBP is an allocation of the data points to (possibly) overlapping classes (a feature allocation).",2. Related Work,[0],[0]
"Dependent nonparametric processes extend distributions over partitions to distributions over collections of partitions indexed by locations in some covariate space, such as R+ (e.g. continuous time), Z (e.g. discrete time), or Rd (e.g. geographical location).",2. Related Work,[0],[0]
"Teh et al. (2013) define such a process based on the duality between Kingman’s coalescent (Kingman, 1982) and the Dirichlet diffusion tree (Neal, 2003).",2. Related Work,[0],[0]
In the resulting “Fragmentation-Coagulation” process (FCP) a partitioning of the data points evolves over the covariate undergoing fragmentation and coagulation events while maintaining CRP marginals.,2. Related Work,[0],[0]
"More recently, Palla et al. (2013) derived a dependent partition-valued process (DPVP) on an arbitrary covariate space which, like the FCP, is exchangeable and has CRP distributed marginals.",2. Related Work,[0],[0]
"In the setting of feature allocations, Williamson et al. (2010) propose a nonparametric process, the dependent IBP (dIBP), with IBP distributed marginals and in which the feature allocations are coupled over the covariate space using a Gaussian process (GP, Rasmussen & Williams, 2006).",2. Related Work,[0],[0]
"In a similar vein, Van Gael et al. (iFHMM, 2009) define the Markov Indian Buffet process (mIBP), a probability distribution over a potentially infinite number of binary Markov chains evolving in discrete time.",2. Related Work,[0],[0]
"They use the mIBP to extend the factorial hidden Markov model (FHMM, Ghahramani & Jordan, 1997) to the infinite FHMM (iFHMM).
",2. Related Work,[0],[0]
"In this paper, we address the problem of dependence for
binary latent feature models.",2. Related Work,[0],[0]
"We propose a process that extends the IBP by allowing features to be “born” and “die” at times learnt by the model, while maintaining the essential mathematical properties of the IBP.",2. Related Work,[0],[0]
The process is a Markov Jump process (MJP) where the events are the birth or the death of a feature.,2. Related Work,[0],[0]
The idea is closely related to the FCP where the events are either a fragmentation of a cluster or a coagulation of two clusters.,2. Related Work,[0],[0]
"The partitions at each location in the FCP are marginally a sample from a Chinese restaurant process, while the feature allocations in the BDFP are marginally samples from an IBP.",2. Related Work,[0],[0]
"Compared to the dIBP, both processes model feature allocations evolving over the covariate.",2. Related Work,[0],[0]
"However, while in the dIBP the assignment of data points to a feature might change over the covariate, in our process, it remains the same until the feature dies.",2. Related Work,[0],[0]
"In the case of the iFHMM, the authors model the dependence of a feature allocation on a discrete time variable as opposed to our process where continuous covariate space is assumed.",2. Related Work,[0],[0]
"Moreover, in the iFHMM, the marginal distribution of a feature allocation is analogous but not equal to an IBP.",2. Related Work,[0],[0]
We call the proposed process the birth-death feature allocation process (BDFP).,2. Related Work,[0],[0]
"The BDFP is exchangeable, projective, stationary and reversible, and its equilibrium distribution is given by the Indian buffet process.",2. Related Work,[0],[0]
Consider a dataset with N data points indexed by integers,3. Feature Allocations and the Indian Buffet Process,[0],[0]
"[N ] := {1, 2, . . .",3. Feature Allocations and the Indian Buffet Process,[0],[0]
", N} (allowing N → ∞).",3. Feature Allocations and the Indian Buffet Process,[0],[0]
Each datapoint n is associated with a binary vector Zn of length K that defines its feature allocation; Znk = 1 if datapoint n has feature k and Znk = 0 otherwise.,3. Feature Allocations and the Indian Buffet Process,[0],[0]
The potential total number of features K may be infinite.,3. Feature Allocations and the Indian Buffet Process,[0],[0]
The binary matrix Z[N ] =,3. Feature Allocations and the Indian Buffet Process,[0],[0]
"[ZT1 ,Z T 2 , . . .",3. Feature Allocations and the Indian Buffet Process,[0],[0]
",Z T N ]",3. Feature Allocations and the Indian Buffet Process,[0],[0]
"T specifies a random feature allocation of [N ], while ZN denotes the space of all feature allocations of [N ], i.e. Z[N ] ∈ ZN .",3. Feature Allocations and the Indian Buffet Process,[0],[0]
"We define mk as the number of datapoints that possess feature k, K+ = ∑2N−1 h=1",3. Feature Allocations and the Indian Buffet Process,[0],[0]
Kh as the number of features for which mk > 0,3. Feature Allocations and the Indian Buffet Process,[0],[0]
"and Kh as the multiplicity of feature h, that is the number of times the same binary column h appears in Z[N ].",3. Feature Allocations and the Indian Buffet Process,[0],[0]
"Under the IBP (Griffiths & Ghahramani, 2011), the probability of a matrix Z[N ] is
g([Z[N ]];α) = αK+∏H h=1Kh!",3. Feature Allocations and the Indian Buffet Process,[0],[0]
exp(−αHN ) K+∏ k=1 (N −mk)!(mk,3. Feature Allocations and the Indian Buffet Process,[0],[0]
"− 1) N !
(1)
where α > 0 is the concentration parameter, HN =∑N j=1 1 j is the N th harmonic number and H ≤ 2
N",3. Feature Allocations and the Indian Buffet Process,[0],[0]
"− 1 is the number of distinct nonzero features in the allocation.
",3. Feature Allocations and the Indian Buffet Process,[0],[0]
"Thibaux & Jordan (2007) showed one can construct the Indian buffet process from a Beta-Bernoulli process using the
following two stage sampling process for n = 1, . . .",3. Feature Allocations and the Indian Buffet Process,[0],[0]
", N :
B|c, µ0 ∼BP(c, µ0) Zn|B ∼ BeP(B) (2) where B = ∑∞",3. Feature Allocations and the Indian Buffet Process,[0],[0]
k=1 ωkδθk and Z = ∑∞,3. Feature Allocations and the Indian Buffet Process,[0],[0]
k=1 fkδθk .,3. Feature Allocations and the Indian Buffet Process,[0],[0]
"First a draw B is sampled from the Beta process BP(cµ0) (Hjort, 1990) with µ0 as the base distribution.",3. Feature Allocations and the Indian Buffet Process,[0],[0]
"B is a set of pairs (ωk, θk) sampled from a Poisson process on the product space",3. Feature Allocations and the Indian Buffet Process,[0],[0]
"[0, 1] × Θ with Lévy intensity ν(dω,dθ) = cω−1(1 − ω)c−1dωµ0(dθ).",3. Feature Allocations and the Indian Buffet Process,[0],[0]
"Then, B is used as the atomic hazard measure for a Bernoulli process BeP(B).",3. Feature Allocations and the Indian Buffet Process,[0],[0]
"Each Zn is a draw from the Bernoulli process and constitutes a collection of atoms of unit mass on Θ. Then, Zn is a binary vector containing the {fk}∞k=1 values resulting from tossing a countably infinite sequence of (conditionally independent) coins with success probabilities ωk, i.e. fk|ωk ∼ Bernoulli(ωk).",3. Feature Allocations and the Indian Buffet Process,[0],[0]
"This construction allows the use of de Finetti’s theorem (de Finetti, 1931) that lets the joint distribution of the rows to be written as
P (Z1, . . .",3. Feature Allocations and the Indian Buffet Process,[0],[0]
",ZN ) = ∫ [ N∏ n=1 P (Zn|B) ] dP",3. Feature Allocations and the Indian Buffet Process,[0],[0]
"(B) (3)
where B is the random measure that renders the variables Zn conditionally independent.",3. Feature Allocations and the Indian Buffet Process,[0],[0]
"Equation (3) shows the exchangeability of the rows of Zn, since they can be described as a mixture of Bernoulli processes.",3. Feature Allocations and the Indian Buffet Process,[0],[0]
We consider a continuous-time Markov process (Z(t))t≥0 in which each Z(t) is a random feature allocation taking values in the discrete space ZN .,4. Birth-Death Process for Feature Allocation,[0],[0]
"The state space is countably infinite; it is determined by all the possible feature allocations defined by N datapoints and K features, where K → ∞. The Markov process (Z(t)) evolves over time jumping to different states (feature allocations).",4. Birth-Death Process for Feature Allocation,[0],[0]
"Let {t1, . . .",4. Birth-Death Process for Feature Allocation,[0],[0]
", tJ ∈ R : J ∈ N} denote the times when the chain jumps such that tj = inf{τ ≥ tj−1 : Z(τ) 6= Z(tj−1)} and Z(tj) ∈ ZN .",4. Birth-Death Process for Feature Allocation,[0],[0]
These jumps are a result of a birth or a death of a feature.,4. Birth-Death Process for Feature Allocation,[0],[0]
"The process (Z(t)) can only jump to neighbouring states, i.e. if the chain is currently at state Z(tj) = s, then at time tj+1 it transitions to Z(tj+1) = s′ where a new feature is created or an existing feature is deleted after a birth or a death event respectively.",4. Birth-Death Process for Feature Allocation,[0],[0]
Let ZsN ⊂ ZN be the discrete space of neighboring states to state s.,4. Birth-Death Process for Feature Allocation,[0],[0]
The process is time homogeneous with transition probabilities P(Z(t + y) = s′|Z(y) = s) = P(Z(t) = s′|Z(0),4. Birth-Death Process for Feature Allocation,[0],[0]
"= s) = pss′(t) for all t, y, where s, s′ ∈ ZN .",4. Birth-Death Process for Feature Allocation,[0],[0]
"At time tj+1 the process jumps to the next state Z(tj+1) = s′ with rate determined by the current state Z(t) = s and the corresponding event, i.e birth or death.",4. Birth-Death Process for Feature Allocation,[0],[0]
"More specifically,
• Birth: Suppose s ∈ ZN is a feature allocation with Ks nonzero features and s′ ∈ ZsN is another feature
allocation that differs from s in having one additional feature of size |a| so that K ′s =",4. Birth-Death Process for Feature Allocation,[0],[0]
Ks + 1.,4. Birth-Death Process for Feature Allocation,[0],[0]
"We choose the transition rate from s to s′ as
qss′ =",4. Birth-Death Process for Feature Allocation,[0],[0]
"R (|a| − 1)!(N − |a|)!
N !",4. Birth-Death Process for Feature Allocation,[0],[0]
"(4)
where R > 0 is a parameter governing the birth rate.",4. Birth-Death Process for Feature Allocation,[0],[0]
The new feature a is a binary column of length N .,4. Birth-Death Process for Feature Allocation,[0],[0]
"There are ( N |a| ) binary formulations for this fea-
ture and 2N",4. Birth-Death Process for Feature Allocation,[0],[0]
− 1 = ∑N n=1 ( N n ),4. Birth-Death Process for Feature Allocation,[0],[0]
"for all possible feature births and thus, the total birth rate from s is∑N n=1 ( N n )",4. Birth-Death Process for Feature Allocation,[0],[0]
R (n−1)!(N−n)!N !,4. Birth-Death Process for Feature Allocation,[0],[0]
= R ∑N n=1 1 n =,4. Birth-Death Process for Feature Allocation,[0],[0]
"R ·HN
where HN = ∑N n=1 1/n is the N -th harmonic num-
ber and n = |a| .",4. Birth-Death Process for Feature Allocation,[0],[0]
•,4. Birth-Death Process for Feature Allocation,[0],[0]
"Death: The rate of transitioning from s′ to s is
qs′s =",4. Birth-Death Process for Feature Allocation,[0],[0]
"Rr
α (5)
",4. Birth-Death Process for Feature Allocation,[0],[0]
where D = Ra is a parameter governing the death rate and r is the multiplicity of the feature in s′ that dies.,4. Birth-Death Process for Feature Allocation,[0],[0]
The multiplicity r is the combinatorial factor that accounts for all the possible ways of obtaining the same equivalence class as defined in Griffiths & Ghahramani (2011) .,4. Birth-Death Process for Feature Allocation,[0],[0]
"There are Ks′ features (including repetitions of the same feature) in s′ that might “die”, thus the total death rate from s′ is RKs′α .
",4. Birth-Death Process for Feature Allocation,[0],[0]
"The total rate of transition out of state s ∈ ZN is the sum of the total birth and death rates, qs = RHN + RKsα =
R ( HN + Ks α ) .",4. Birth-Death Process for Feature Allocation,[0],[0]
"We call (Z(t))t>=0 a birth-death feature allocation process with birth rate R and death rate Rα and write BDFP(α,R).
",4. Birth-Death Process for Feature Allocation,[0],[0]
Theorem 1.,4. Birth-Death Process for Feature Allocation,[0],[0]
The Markov process (Z(t))t≥0 is irreducible and has stationary distribution IBP(α).,4. Birth-Death Process for Feature Allocation,[0],[0]
"Furthermore, it is reversible.
",4. Birth-Death Process for Feature Allocation,[0],[0]
Proof.,4. Birth-Death Process for Feature Allocation,[0],[0]
A continuous time Markov chain is irreducible if it is possible to eventually get from every state to every other state with positive probability.,4. Birth-Death Process for Feature Allocation,[0],[0]
"It is reversible if detailed balance holds, i.e. there is a probability distribution π on ZN such that πsqss′ = πs′qs′s for all s, s′ ∈ ZN .",4. Birth-Death Process for Feature Allocation,[0],[0]
Then π is also the invariant (equilibrium) distribution of the Markov chain.,4. Birth-Death Process for Feature Allocation,[0],[0]
"The chain in BDFP is irreducible, because for any T > 0 and any two distinct feature allocations γ, ρ ∈ ZN , there is a positive probability that if it starts at γ ∈ ZN , it will end at ρ ∈ ZN .",4. Birth-Death Process for Feature Allocation,[0],[0]
Reversibility and the equilibrium distribution can be demonstrated by detailed balance.,4. Birth-Death Process for Feature Allocation,[0],[0]
"Suppose γ, ρ are feature allocations such that γ, ρ ∈ ZN and ρ differs from γ in that it has one additional feature a of size |a|.",4. Birth-Death Process for Feature Allocation,[0],[0]
"The number of (nonzero) features in ρ isKρ = Kγ+1.
",4. Birth-Death Process for Feature Allocation,[0],[0]
"Then,
g(γ;α)qγρ = αKγ
Π",4. Birth-Death Process for Feature Allocation,[0],[0]
Hγ h=1Kh!,4. Birth-Death Process for Feature Allocation,[0],[0]
exp (−αHN ) Kγ∏ k=1 (N −mk)!(mk,4. Birth-Death Process for Feature Allocation,[0],[0]
− 1)!,4. Birth-Death Process for Feature Allocation,[0],[0]
N !,4. Birth-Death Process for Feature Allocation,[0],[0]
R (|a| − 1)!(N − |a|)!,4. Birth-Death Process for Feature Allocation,[0],[0]
"N !
",4. Birth-Death Process for Feature Allocation,[0],[0]
"mKγ+1=|α| = αKγ+1
αΠ",4. Birth-Death Process for Feature Allocation,[0],[0]
Hγ h=1Kh!,4. Birth-Death Process for Feature Allocation,[0],[0]
exp (−αHN ) Kγ+1∏ k=1 (N −mk)!(mk,4. Birth-Death Process for Feature Allocation,[0],[0]
− 1)!,4. Birth-Death Process for Feature Allocation,[0],[0]
N !,4. Birth-Death Process for Feature Allocation,[0],[0]
"R
= αKρ
rαΠ",4. Birth-Death Process for Feature Allocation,[0],[0]
Hγ h=1Kh!,4. Birth-Death Process for Feature Allocation,[0],[0]
exp (−αHN ) Kρ∏,4. Birth-Death Process for Feature Allocation,[0],[0]
k=1,4. Birth-Death Process for Feature Allocation,[0],[0]
(N −mk)!(mk,4. Birth-Death Process for Feature Allocation,[0],[0]
− 1)!,4. Birth-Death Process for Feature Allocation,[0],[0]
N !,4. Birth-Death Process for Feature Allocation,[0],[0]
R α,4. Birth-Death Process for Feature Allocation,[0],[0]
"ra
rα=Kα= αKρ
Π",4. Birth-Death Process for Feature Allocation,[0],[0]
Hρ h=1Kh!,4. Birth-Death Process for Feature Allocation,[0],[0]
exp (−αHN ) Kρ∏,4. Birth-Death Process for Feature Allocation,[0],[0]
k=1,4. Birth-Death Process for Feature Allocation,[0],[0]
(N −mk)!(mk,4. Birth-Death Process for Feature Allocation,[0],[0]
− 1)!,4. Birth-Death Process for Feature Allocation,[0],[0]
N !,4. Birth-Death Process for Feature Allocation,[0],[0]
R α,4. Birth-Death Process for Feature Allocation,[0],[0]
"ra
= g(ρ;α)qργ (6)
where g(γ;α) is the probability of a feature allocation γ under the IBP as defined in Equation (1), qγρ is the transition rate from state γ to state ρ,",4. Birth-Death Process for Feature Allocation,[0],[0]
"Hγ , Hρ are the number of distinct features in states γ and ρ respectively and ra is the multiplicity (the times the feature is present at the current feature allocation) of feature a that dies.",4. Birth-Death Process for Feature Allocation,[0],[0]
"Detailed balance holds, and as such the process is reversible and the equilibrium distribution is IBP[N ](α).
",4. Birth-Death Process for Feature Allocation,[0],[0]
Assume that (z(t)) is a realization of the BDFP (Z(t)) over the finite interval,4. Birth-Death Process for Feature Allocation,[0],[0]
"[0, T ], T > 0",4. Birth-Death Process for Feature Allocation,[0],[0]
and we write (z(t))0≤t≤T .,4. Birth-Death Process for Feature Allocation,[0],[0]
"With probability one the sample path (z(t))0≤t≤T will only contain a finite number of jump events, each of which is either a birth or a death event.",4. Birth-Death Process for Feature Allocation,[0],[0]
"We write B and Q to denote the set of the features created or turned off by birth or death events respectively.
",4. Birth-Death Process for Feature Allocation,[0],[0]
Proposition 1.,4. Birth-Death Process for Feature Allocation,[0],[0]
"Writing q(t) = qz(t) to denote the total transition rate out of state z(t), the probability of a realization (z(t)) under the law of the BDFP is:
R|B|+|Q| αA−|B|−|Q|∏A∗−|B∗| h=1",4. Birth-Death Process for Feature Allocation,[0],[0]
"Kh!
exp (−αHN ) exp ( − ∫ T
0
q(t)dt )",4. Birth-Death Process for Feature Allocation,[0],[0]
× . .,4. Birth-Death Process for Feature Allocation,[0],[0]
".
",4. Birth-Death Process for Feature Allocation,[0],[0]
∏ b∈B∪{z(t=0)} (|b| − 1)!(N − |b|)!,4. Birth-Death Process for Feature Allocation,[0],[0]
N !,4. Birth-Death Process for Feature Allocation,[0],[0]
∏,4. Birth-Death Process for Feature Allocation,[0],[0]
"d∈D rd
(7)
where A = K0 + |B| = KT + |Q|, A∗ = H0 + |B∗| = HT + |Q∗|. B∗, Q∗ are the sets of features with zero multiplicity at their creation time or with multiplicity of one at their death time respectively, and {z(t)} denotes the set of features at time t.",4. Birth-Death Process for Feature Allocation,[0],[0]
The BDFP process can be constructed using a nonhomogenous Poisson process Π. Consider the Lévy measure ν(dωdxdtbdtω) on a product space,4.1. Dependent Beta Process Construction,[0],[0]
"[0, 1] ⊗ X ⊗ R ⊗",4.1. Dependent Beta Process Construction,[0],[0]
"[0,∞).",4.1. Dependent Beta Process Construction,[0],[0]
"A sample corresponds to set of points Π = {ωk, xk, tkb , tkω}k where the range of k is countably infinite.",4.1. Dependent Beta Process Construction,[0],[0]
Each atom corresponds to a feature and is associated with a weight ωk ∈,4.1. Dependent Beta Process Construction,[0],[0]
"[0, 1], a location xk, a birth time tkb ∈ R and a life-span tkω ∈",4.1. Dependent Beta Process Construction,[0],[0]
"[0,∞) (Figure 1).",4.1. Dependent Beta Process Construction,[0],[0]
"The Lévy measure is of the form ν(dωdxdtbdtω) = ρ(dω)µ(dxdtbdtω) and
corresponds to a Beta process on the combined space",4.1. Dependent Beta Process Construction,[0],[0]
Θ = X⊗ R⊗,4.1. Dependent Beta Process Construction,[0],[0]
"[0,∞) with ρ(dω)",4.1. Dependent Beta Process Construction,[0],[0]
= αω−1(1− ω)α−1 and base measure µ(dθ) = µ(dxdtbdtw).,4.1. Dependent Beta Process Construction,[0],[0]
"Setting g(dtb) = dtb and β(dtω) = D exp
−Dtω dtω , the base measure is µ(dθ) = µ0(dx)g(dtb)β(dtω) = µ0(dx)dtbD exp
−Dtω dtω , where D is the death rate.",4.1. Dependent Beta Process Construction,[0],[0]
"The constant measure g(dtb) over the real line R is infinite but σ-finite, that is the total measure g(R)",4.1. Dependent Beta Process Construction,[0],[0]
"= ∞, but there is a measurable partition (Ek) of R with each g(Ek)",4.1. Dependent Beta Process Construction,[0],[0]
< ∞. Since ν(dωdθ) integrates to infinity but satisfies ∫,4.1. Dependent Beta Process Construction,[0],[0]
"[0,1] ∫ Θ
(1 ∧ |ω|)ν(dωdθ) < ∞, a countably infinite number of i.i.d. random points {(ωk,θk)}∞k=1 are obtained from the Poisson process and ∑∞",4.1. Dependent Beta Process Construction,[0],[0]
k=1 ωk is finite with probability one.,4.1. Dependent Beta Process Construction,[0],[0]
"A Beta process is a completely random measure (Kingman, 1967) and, as such, a sample can be expressed as B = ∑∞",4.1. Dependent Beta Process Construction,[0],[0]
"k=1 ωkδθk |α, µ ∼ BP(αµ), where the atoms θk = {xk, tkb , tkω} ∈ Θ and weights",4.1. Dependent Beta Process Construction,[0],[0]
ωk ∈,4.1. Dependent Beta Process Construction,[0],[0]
"[0, 1].
Having drawn a sample B we can construct the feature allocations over an index space R as follows:
B = ∞∑",4.1. Dependent Beta Process Construction,[0],[0]
"k=1 ωkδθk |α, µ ∼ BP(αµ)
",4.1. Dependent Beta Process Construction,[0],[0]
"Sn: = ∞∑ k=1 bnkδθ |B ∼ BeP(ωk) Znk(t) = SnkI(tkb < t < tkb + tkω) (8)
with bnk|ωk ∼ Bernoulli(ωk) and n = 1, . . .",4.1. Dependent Beta Process Construction,[0],[0]
", N .",4.1. Dependent Beta Process Construction,[0],[0]
"The binary matrix S of dimension N ×K, is a feature potential matrix.",4.1. Dependent Beta Process Construction,[0],[0]
Each binary element Snk indicates whether object n possesses feature fk.,4.1. Dependent Beta Process Construction,[0],[0]
S is a global variable and doesn’t depend on time t.,4.1. Dependent Beta Process Construction,[0],[0]
"At any time t, the feature allocation matrix Z(t) is a deterministic function of the current features present at t, that is {fk : tkb < t < tkb + tkw, k = 1, . . .",4.1. Dependent Beta Process Construction,[0],[0]
",∞}
and the feature potential matrix S, i.e. Znk(t) = 1 iff Snk = 1 and tkb < t",4.1. Dependent Beta Process Construction,[0],[0]
<,4.1. Dependent Beta Process Construction,[0],[0]
"t k b + t k ω .
",4.1. Dependent Beta Process Construction,[0],[0]
"The resulting feature allocation process (zn(t))T is equivalent to the following: every time a new feature fk is created, each object n joins with probability ωk, i.e. znk(tkb )|ωk ∼ Bernoulli(ωk).",4.1. Dependent Beta Process Construction,[0],[0]
"If znk(tkb ) = 1, object n will possess feature fk until tkb + t k ω .",4.1. Dependent Beta Process Construction,[0],[0]
Repeat this process for all objects.,4.1. Dependent Beta Process Construction,[0],[0]
Proposition 2.,4.1. Dependent Beta Process Construction,[0],[0]
"The BDFP is exchangeable and the Beta process BP(αµ) on X⊗R⊗[0,∞) describes its underlying mixing measure.
",4.1. Dependent Beta Process Construction,[0],[0]
Proof.,4.1. Dependent Beta Process Construction,[0],[0]
"Consider a sequence of variables (zn(t))T with n = 1, 2, . . .",4.1. Dependent Beta Process Construction,[0],[0]
", N such that each (zn(t))T is the feature allocation evolution of object n over the index space T. These variables are not independent since each (zn(t))T depends on the Z|[n−1](t) =",4.1. Dependent Beta Process Construction,[0],[0]
"(z1:(n−1)(t))T. However, given a sample from the B ∼ BP(αµ) described in Section 4.1, each variable (zn(t))T becomes conditionally independent and the following holds
P ((z1(t))T, (z2(t))T, . . .",4.1. Dependent Beta Process Construction,[0],[0]
", (zN (t))T) = ∫ N∏ n=1 P ((zn(t))T|B)φ(dB)
(9)
where φ = BP(αµ).
",4.1. Dependent Beta Process Construction,[0],[0]
Equation (9) is the de Finetti representation of the BDFP and as such the BDFP is exchangeable and the BP on Θ = X ⊗ R ⊗,4.1. Dependent Beta Process Construction,[0],[0]
"[0,∞) is its underlying mixing measure.",4.1. Dependent Beta Process Construction,[0],[0]
"Restricting our focus on each index t, the overall Beta process BP(αµ) on X⊗ R⊗ [0,∞) results in a set of dependent random measures over X, oneBt for each t ∈ T, such that each Bt is marginally a Beta process.",4.1. Dependent Beta Process Construction,[0],[0]
"Consider a fixed time point t ∈ T and the space [0, 1] ⊗ X (the red vertical plane in Figure 1).",4.1. Dependent Beta Process Construction,[0],[0]
"The point process on this plane (where blue lines intersect the plane) corresponds to features alive at time t, i.e. t ∈",4.1. Dependent Beta Process Construction,[0],[0]
"[tb, tb + tω].",4.1. Dependent Beta Process Construction,[0],[0]
"The Lévy measure on this plane, is calculated by projecting the overall Lévy measure onto the plane,
νt(dωdx) = ∫",4.1. Dependent Beta Process Construction,[0],[0]
"∞ 0 ∫ t t−tω ν(dωdxdtbdtω)
= αω−1(1− ω)α−1µ0(dx) D
(10)
where νt is a measure over [0, 1] ⊗ X for a specific t ∈ T. More specifically, it is the Lévy measure of a Beta process on X with ρ(dω) = αω−1(1 − ω)α−1 and base measure µt(dx)",4.1. Dependent Beta Process Construction,[0],[0]
"=
µ0(dx) D .",4.1. Dependent Beta Process Construction,[0],[0]
"Thus we have that marginally Bt|α, µt ∼ BP(αµt), ∀t ∈ T. (11)
",4.1. Dependent Beta Process Construction,[0],[0]
The restricted and projected measure at any index t ∈ T defines a Beta process.,4.1. Dependent Beta Process Construction,[0],[0]
"Two draws, Bt and Bs, with t, s ∈ T, will be dependent with the amount of dependence decreasing as |s− t| increases.",4.1. Dependent Beta Process Construction,[0],[0]
Proposition 3.,4.1. Dependent Beta Process Construction,[0],[0]
"The dependent Beta process construction presented has IBP marginals at any t.
Proof.",4.1. Dependent Beta Process Construction,[0],[0]
"At any t ∈ T, Bt|α, µt ∼ BP(αµt).",4.1. Dependent Beta Process Construction,[0],[0]
"It is straightforward to see that, marginally, the feature allocation matrix Zt obtained using the generative process in Equation (8) is equivalent to Zt|Bt ∼ BeP(Bt) and therefore Zt ∼ IBP(α), ∀t ∈ T. Corollary 1.",4.1. Dependent Beta Process Construction,[0],[0]
"At any t ∈ T, the feature allocation matrix Zt can be generated by the following generative model as K →∞:
ωk|α ∼ Beta ( R
K
) , Znk|ωk ∼ Bernoulli(ωk) (12)
for k = 1, . . .",4.1. Dependent Beta Process Construction,[0],[0]
",K and n = 1, . . .",4.1. Dependent Beta Process Construction,[0],[0]
", N
The proof of the corollary in included in the supplementary material.",4.1. Dependent Beta Process Construction,[0],[0]
"Note that the above is true only marginally, i.e. at time t ∈ T and it doesn’t generste dependence structure between Zt’s.
",4.1. Dependent Beta Process Construction,[0],[0]
"We underline the dependence of Zs and Zt when |s− t| → 0, ∀s, t ∈ T.",4.1. Dependent Beta Process Construction,[0],[0]
"The closer s, t are, the more the atoms (features)",4.1. Dependent Beta Process Construction,[0],[0]
Bs and Bt share.,4.1. Dependent Beta Process Construction,[0],[0]
"If we independently sampled Zs|Bs ∼ BeP(Bs) and Zt|Bt ∼ BeP(Bt) then Zs, Zt would be dependent, but not equal, even as |s − t| → 0.",4.1. Dependent Beta Process Construction,[0],[0]
"However, in the BDFP the presence of the same features results in the same (not just similar) allocation as |s−t| → 0.",4.1. Dependent Beta Process Construction,[0],[0]
"In both cases, the marginal distribution of the feature allocation matrix at any t ∈ T is Zt|Bt ∼ BeP(Bt) and Zt|α ∼ IBP(α).",4.1. Dependent Beta Process Construction,[0],[0]
"The BDFP results in a continuous evolution of the Z(t) over T: formally Zt d→ Zs as t→ s.
This construction of the BDFP resembles the spatial normalised Gamma process (SNΓP) by (Rao & Teh, 2009).",4.1. Dependent Beta Process Construction,[0],[0]
"The main difference lies in the marginal distribution; the SNΓP admits DP marginals as opposed to the Beta process marginals of the dependent Beta process as shown in Equation (11).
",4.1. Dependent Beta Process Construction,[0],[0]
Proposition 4.,4.1. Dependent Beta Process Construction,[0],[0]
"The feature allocation process described by Equation (8) with B ∼ BP(αµ), has the same birth and death rates as the BDF process.",4.1. Dependent Beta Process Construction,[0],[0]
"For the BDFP, the inference simplifies considerably if we consider a finite approximation which gives the countably infinite model in the limit.",5. Finite Model,[0],[0]
Consider the space S =,5. Finite Model,[0],[0]
"[0, 1] ⊗ X ⊗",5. Finite Model,[0],[0]
"[0, T ] ⊗",5. Finite Model,[0],[0]
"[0,∞), where we restrict the space of tb to be [0, T ] instead of the whole real line R. This accounts for typical applications of the model where we observe data at distinct times over a finite time range.",5. Finite Model,[0],[0]
"Consider the Lévy measure ν(dωdxdtbdtω) on the space S. Then, under the dependent Beta process representation (see section 4.1), the expected number of atoms present in S is
∫ S ν(dωdxdtbdtω) =∫ 1
0 ρ(dω) ∫",5. Finite Model,[0],[0]
X µ0(dx) ∫ T 0,5. Finite Model,[0],[0]
g(dtb) ∫∞ 0 β(dtω),5. Finite Model,[0],[0]
"= KT , where
K → ∞ since ∫ 1
0 ρ(dω) =",5. Finite Model,[0],[0]
"∞. By considering finite K
we allow inference on a finite model which approximates the infinite case with increasing fidelity as K →∞.
The process is depicted in Figure 2 and the infinite case can be derived as the limit K →∞ of the following:
• Consider a time range",5. Finite Model,[0],[0]
"[0, T ] and a set of features F , such that |F| ∼ Poisson(KT ).",5. Finite Model,[0],[0]
"Assign to each feature fk ∈ F , k = 1, . . .",5. Finite Model,[0],[0]
|F|,5. Finite Model,[0],[0]
"a weight ω, such that ωk",5. Finite Model,[0],[0]
"∼ Beta ( R K , 1 ) and Ω =",5. Finite Model,[0],[0]
"[ω1, ω2 . . .",5. Finite Model,[0],[0]
ω|F|].,5. Finite Model,[0],[0]
"• Associate each feature fk ∈ F , k = 1, . . .",5. Finite Model,[0],[0]
|F|,5. Finite Model,[0],[0]
"with a birth time tkb uniformly sampled in [0, T ]; t k b ∼
U(0, T ) and tb",5. Finite Model,[0],[0]
=,5. Finite Model,[0],[0]
[t1b . . .,5. Finite Model,[0],[0]
t |F|,5. Finite Model,[0],[0]
b,5. Finite Model,[0],[0]
"].
",5. Finite Model,[0],[0]
"• For each fk ∈ F , sample its life span tkw ∼ Exponential(D), where D is the death rate.",5. Finite Model,[0],[0]
Define the time of death tkd as t k d = t k b,5. Finite Model,[0],[0]
+,5. Finite Model,[0],[0]
t k w,5. Finite Model,[0],[0]
and tw,5. Finite Model,[0],[0]
"=
[t1w . . .",5. Finite Model,[0],[0]
t |F|,5. Finite Model,[0],[0]
"w ].
",5. Finite Model,[0],[0]
We call the sequence of the above steps Beta Event Process (BEP).,5. Finite Model,[0],[0]
"Putting everything together, generate a sample B = {F ,Ω, tb, tw} ∼ BEP(α,R,K, T ) as follows:
|F| ∼ Poisson(KT )",5. Finite Model,[0],[0]
"ωk ∼ Beta ( R
K , 1
) , tkb ∼ U(0, T ), tkω ∼ Exponential(D)
(13)
for k = 1, . . .",5. Finite Model,[0],[0]
", |F|.",5. Finite Model,[0],[0]
"Having drawn a sample B from the BEP, we can construct the feature allocations over time as follows
Snk|ωk ∼ Bernoulli(ωk) Znk(t) =",5. Finite Model,[0],[0]
"SnkI(tkb < t < tkb + tkω) (14)
",5. Finite Model,[0],[0]
"where n = 1, .",5. Finite Model,[0],[0]
. .,5. Finite Model,[0],[0]
", N .",5. Finite Model,[0],[0]
The feature potential matrix (as defined in section 4.1) has now N × |F| dimensions.,5. Finite Model,[0],[0]
"Moreover, each Z(t) for t ∈ T is a matrix of dimensions N × F (t) and F (t) ≤ |F|.",5. Finite Model,[0],[0]
"Figure 3(a) show the graphical model for the BEP.
",5. Finite Model,[0],[0]
Proposition 5.,5. Finite Model,[0],[0]
"In the finite model, the expected number of features present at any t ∈ T is E[Nf ]",5. Finite Model,[0],[0]
= KD and for D = Rα we have E[Nf ],5. Finite Model,[0],[0]
"= Kα R .
Hyperpriors.",5. Finite Model,[0],[0]
"We put gamma priors on α and R.
Likelihood models.",5. Finite Model,[0],[0]
"We consider two different likelihood models: linear-Gaussian for real data and logistic for binary network data.
",5. Finite Model,[0],[0]
"For the linear-Gaussian likelihood model, consider a sequence of observations {Yt ∈ Y : t = 1, . . .",5. Finite Model,[0],[0]
", L} generated as
Yt = ZtA + t (15)
where Yt is a N ×M observation matrix at each time t = 1, . . .",5. Finite Model,[0],[0]
", L, A is a factor loading matrix of dimension |F|",5. Finite Model,[0],[0]
"× M shared across time and t ∼ N ( 0, σ2 ) is Gaussian white noise.",5. Finite Model,[0],[0]
"We choose a Gaussian prior over A, i.e Afm ∼ N (0, 1).
",5. Finite Model,[0],[0]
"In the case of dynamic binary network data we extend the latent feature relational model (LFRM) proposed by (Miller et al., 2009).",5. Finite Model,[0],[0]
"Let Yt be the N × N binary matrix that contains links, i.e. ytij = Yt(i, j) = 1 iff we observe a link from entity i to entity j at time t. We assume that the matrices Yt are symmetric and ignore diagonal elements (self-links).",5. Finite Model,[0],[0]
The probability of a link from one entity to another is determined by the combined effect of all pairwise feature interactions.,5. Finite Model,[0],[0]
Let Wt be a |F| × |F|,5. Finite Model,[0],[0]
"real-valued weight matrix where Wt(k, k′) is the weight that affects the probability of there being a link from entity i to entity j if entity i has feature k on, i.e. Ztik = Zt(i, k) = 1 and entity j has feature k′ on, i.e. Ztjk′ = Zt(j, k′)",5. Finite Model,[0],[0]
= 1.,5. Finite Model,[0],[0]
"The links are independent conditioned on Zt and Wt, and only the features that are on for the entities i and j at time t influence the probability of a link between those entities at that time (see Figure 3(b)).",5. Finite Model,[0],[0]
"Formally,
P (ytij = 1|Zt,Wt) = σ",5. Finite Model,[0],[0]
"(∑
kl
ZtikZtjlWtkl + s ) (16)
for k, l = 1, .",5. Finite Model,[0],[0]
. .,5. Finite Model,[0],[0]
", |F|, where s is a bias term and σ(x) =",5. Finite Model,[0],[0]
(1 + e−x)−1 is the sigmoid function.,5. Finite Model,[0],[0]
"For completeness, we assume the priors wt(k, l) ∼ N ( µw, σ 2 w ) and s ∼
N ( µs, σ 2 s ) .",5. Finite Model,[0],[0]
"As with many other Bayesian models, exact inference is intractable so we employ Markov Chain Monte Carlo (MCMC) for posterior inference over the latent variables of the finite model.",5.1. Inference,[0],[0]
A detailed description is provided in the supplementary material.,5.1. Inference,[0],[0]
We experimentally evaluate the BEP model on real-world genomics and social network data.,6. Experiments,[0],[0]
"To evaluate the model fit, we compared the BEP model to independent models at each time point.",6. Experiments,[0],[0]
"Here we used a subset of the gene expression data from Piechota et al. (2010), including N = 500 genes in D = 4 different conditions (exposure to different drugs) over L = 24 time intervals.",6.1. Circadian Rhythm Dataset,[0],[0]
The measurements indicate how active a gene is at different times.,6.1. Circadian Rhythm Dataset,[0],[0]
"We created 7 train-test splits holding out 20% of the data, and ran 700 MCMC iterations.",6.1. Circadian Rhythm Dataset,[0],[0]
We see that in terms of predictive performance the BEP outperforms independent IBP models (Table 1).,6.1. Circadian Rhythm Dataset,[0],[0]
The genes belonging to each factor show enrichment for different known biological pathways (Figure 4).,6.1. Circadian Rhythm Dataset,[0],[0]
"Of particular note are the tryptophan metabolism genes enriched in factor 2, given tryptophan’s suspected effects on drowsiness;
the vasopressin regulated water reabsorption, given this hormone’s known circadian regulation (Earnest & Sladek, 1986; Yamaguchi et al., 2013); and the regulation of insulin producing beta cells, another hormone with circadian variation (Shi et al., 2013).",6.1. Circadian Rhythm Dataset,[0],[0]
"For this experiment we used ChIP-seq (chromatin immunoprecipitation sequencing) data downloaded from the ENCODE project (Consortium, 2007), representing histone modifications and transcription factor binding in human neural crest cell lines (see (Park, 2009) for a nice review).
",6.2. ChIP-seq Epigenetic Marks,[0],[0]
The observations involve counts associated with N = 14 (human) cell lines and D = 10 proteins.,6.2. ChIP-seq Epigenetic Marks,[0],[0]
"The counts indicate what proteins, with what chemical modifications, are bound to DNA along the genome.",6.2. ChIP-seq Epigenetic Marks,[0],[0]
"The measurements are stored in N × D matrix of counts Yt: for each cell line, how many reads for each of the 10 proteins mapped to bin t (100 base pair (bp) region of the genome).",6.2. ChIP-seq Epigenetic Marks,[0],[0]
"t = 1, . . .",6.2. ChIP-seq Epigenetic Marks,[0],[0]
", 500 bins were considered at the start of chromosome 1 (50K bp in total).",6.2. ChIP-seq Epigenetic Marks,[0],[0]
In Figure 5(a) each subfigure corresponds to one of the 10 proteins and in each subfigure the counts for the N = 14 cell lines are plotted over the genome section of length 50Kbp.,6.2. ChIP-seq Epigenetic Marks,[0],[0]
"Before inference, the raw counts were square-root transformed (a standard variance stabilizing transform for Poisson data) to make the Gaussian likelihood appropriate.",6.2. ChIP-seq Epigenetic Marks,[0],[0]
"We ran 7 different held-out tests, holding out a different 20% of the data each time.",6.2. ChIP-seq Epigenetic Marks,[0],[0]
"Results, using 700 MCMC iterations, are presented in Table 2.",6.2. ChIP-seq Epigenetic Marks,[0],[0]
"The
BEP outperforms the independent IBP model in both test likelihood and error with a statistically significant difference.",6.2. ChIP-seq Epigenetic Marks,[0],[0]
"The independent IBP appears to have better results in train error and likelihood, again suggesting overfitting.",6.2. ChIP-seq Epigenetic Marks,[0],[0]
"Comparing the plots of the true measurements to the learnt ones by the BEP and independent IBP model in Figure 5 we see that both models successfully reproduce the data but the BEP reconstructions provide a cleaned up picture of the meaningful signal.
",6.2. ChIP-seq Epigenetic Marks,[0],[0]
The features found by the model in the different genome locations correspond to different states associated with the specific genome location.,6.2. ChIP-seq Epigenetic Marks,[0],[0]
"Genes and regulatory DNA elements such as enhancers, silencers and insulators are embedded in genomes.",6.2. ChIP-seq Epigenetic Marks,[0],[0]
"These genomic elements on the DNA have footprints for the transacting proteins involved in transcription, either for the positioning or regulation of the transcriptional machinery.",6.2. ChIP-seq Epigenetic Marks,[0],[0]
"For instance, promoters are regions of DNA which recruit proteins required to initiate transcription of a particular gene and located near the transcription start sites.",6.2. ChIP-seq Epigenetic Marks,[0],[0]
Enhancers are regions of DNA that can be bound by proteins which activate transcription of a distal gene.,6.2. ChIP-seq Epigenetic Marks,[0],[0]
"So a cell line, at specific genome location (recall that here each location corresponds to 100 base pairs), will have underlying feature membership (some promoters and some enhancer for example) that determines whether particular protein are found there using ChIP-seq.
Genomic annotations, from ChromHMM (Ernst et al., 2011), are shown in Figure 8 in the supplementary document for the region we model.",6.2. ChIP-seq Epigenetic Marks,[0],[0]
Different levels of the marks in these different regions are much easier to see in the reconstructed signal using BEP in Figure 5(b).,6.2. ChIP-seq Epigenetic Marks,[0],[0]
"In van de Bunt et al. (1999), 32 university freshman students in a given discipline at a Dutch university were surveyed at seven time points about who in their class they considered as friends.",7. van de Bunt’s Dataset,[0],[0]
"Initially, i.e. t1, most of the students were unknown to each other.",7. van de Bunt’s Dataset,[0],[0]
"The first four time points are three weeks apart, whereas the last three time points are six weeks apart as showin in Figure 11 in the supplementary matrial.",7. van de Bunt’s Dataset,[0],[0]
We symmetrise the matrix by assuming friendship if either individual reported it.,7. van de Bunt’s Dataset,[0],[0]
"We test the performance of BEP using the sigmoid likelihood model as in Equation
(16) by holding out 10% of all links across all time points.",7. van de Bunt’s Dataset,[0],[0]
We ran each model for 1000 MCMC iterations.,7. van de Bunt’s Dataset,[0],[0]
The results are shown in Table 3.,7. van de Bunt’s Dataset,[0],[0]
The independent network LFR models outperform BEP in the train setting and the test error while BEP outperforms in the test likelihood.,7. van de Bunt’s Dataset,[0],[0]
"However, here the results are comparable.",7. van de Bunt’s Dataset,[0],[0]
"Looking at Figure 6, both models provide the same picture of the allocation.",7. van de Bunt’s Dataset,[0],[0]
It is possible the stationary assumption hurts the BEP: in the VDB dataset the number of links almost exclusively increases over time.,7. van de Bunt’s Dataset,[0],[0]
"Many modern machine learning and statistics tasks involve multidimensional data positioned along some linear covariate: we have shown functional genomics data where the covariate is position in the genome, and network data where links change over time.",8. Discussion,[0],[0]
"To model such data we need priors that utilize the dependencies through time, while handling high dimensionality.",8. Discussion,[0],[0]
The BDFP is an expressive new Bayesian non-parametric prior that fulfills these criteria.,8. Discussion,[0],[0]
"It outputs time-evolving feature allocations, which can then be effectively used to model high-dimensional time-series data.",8. Discussion,[0],[0]
"Since the number of latent features is unbounded, like other Bayesian non-parametric methods, the model can adapt its complexity to the data.",8. Discussion,[0],[0]
"While the combinatorial BDFP may seem like a complex object to handle computationally, our theoretical results showing that the de Finetti measure underlying the BDFP is a specific beta process, which can be well approximated by a finite K model, the BEP.",8. Discussion,[0],[0]
"Our experimental results, compared to independent feature allocations, provides evidence that effectively modeling dependency in the feature allocation through the birth-death mechanism is appropriate for a wide range of statistical applications.",8. Discussion,[0],[0]
"Moreover, the BEP provides an interpretable structure using parameters not found, to the best of our knowledge, in existing models, i.e. birth and death rate of features.",8. Discussion,[0],[0]
"We are interested in scaling inference under the BEP to larger datasets, for example using (stochastic) variational inference methods that have been successful for the IBP (Doshi et al., 2009).
",8. Discussion,[0],[0]
Acknowledgements Konstantina’s research leading to these results has received funding from the European Research Council under the European Union’s Seventh Framework Programme (FP7/2007-2013) ERC grant agreement no. 617411.,8. Discussion,[0],[0]
"We propose a Bayesian nonparametric prior over feature allocations for sequential data, the birthdeath feature allocation process (BDFP).",abstractText,[0],[0]
The BDFP models the evolution of the feature allocation of a set of N objects across a covariate (e.g. time) by creating and deleting features.,abstractText,[0],[0]
"A BDFP is exchangeable, projective, stationary and reversible, and its equilibrium distribution is given by the Indian buffet process (IBP).",abstractText,[0],[0]
We show that the Beta process on an extended space is the de Finetti mixing distribution underlying the BDFP.,abstractText,[0],[0]
"Finally, we present the finite approximation of the BDFP, the Beta Event Process (BEP), that permits simplified inference.",abstractText,[0],[0]
The utility of the BDFP as a prior is demonstrated on real world dynamic genomics and social network data.,abstractText,[0],[0]
A Birth-Death Process for Feature Allocation,title,[0],[0]
Replicating results in deep learning research is often hard.,1. Introduction,[0],[0]
"This harms their usefulness to industry, leads to a waste of effort by other researchers, and limits the scientific value of such results.
",1. Introduction,[0],[0]
One reason is that many papers provide information insufficient for replication.,1. Introduction,[0],[0]
"Details of the experimental setup can significantly influence the results (Henderson et al., 2018; Fokkens et al., 2013; Raeder et al., 2010), so the details should be provided at least in appendices, ideally alongside the source code, as was strongly emphasized e.g. by Ince et al. (2012).
",1. Introduction,[0],[0]
"However, an important second factor hinders replicability: most deep learning training methods are inherently stochastic.",1. Introduction,[0],[0]
"This randomness usually comes from random data ordering in stochastic gradient descent and from random parameter initialization, though there can be additional sources of randomness such as dropout or gradient noise.",1. Introduction,[0],[0]
"Consequently, even if we fix the model architecture and the experimental setup (including the hyper-parameters), we obtain a different result each time we run an experiment.",1. Introduction,[0],[0]
Statistical techniques are needed to handle this variability.,1. Introduction,[0],[0]
"However,
1 IBM Watson, Prague AI Research & Development Lab.",1. Introduction,[0],[0]
RK has since moved to Deepmind.,1. Introduction,[0],[0]
Correspondence to: Ondrej Bajgar < OBajgar@cz.ibm.com,1. Introduction,[0],[0]
">.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
in deep learning research, they are heavily underused.",1. Introduction,[0],[0]
"What is usually done instead?
",1. Introduction,[0],[0]
Most empirical deep learning papers simply report the performance of the best single model (we will later show this is the case at least for some sub-domains).,1. Introduction,[0],[0]
"Given the result stochasticity, such method is statistically flawed.",1. Introduction,[0],[0]
"The best model performance is not robust under experiment replication, and its expected value improves with an increasing number of experiments performed, among other problems.",1. Introduction,[0],[0]
"Since many deep learning publications largely ignore these issues, we dedicate the first part of this article to explaining in some detail, and later run experiments to quantify them.
",1. Introduction,[0],[0]
Appropriate statistical techniques are hence necessary for evaluating (and comparing) the performance of machine learning (ML) architectures.,1. Introduction,[0],[0]
Some well-developed methods exist for such comparisons (a great introduction is given for instance by Cohen (1995)).,1. Introduction,[0],[0]
"However, most existing methods focus on comparing the mean performance.",1. Introduction,[0],[0]
"This may be one of the reasons why statistical methods are being underused, since mean may be unattractive to researchers in certain situations.
",1. Introduction,[0],[0]
There are multiple possible reasons for this.,1. Introduction,[0],[0]
"The one that we do consider sound1 is that when deploying models in practice, it is often natural to train multiple instances of a model and then deploy the best one to production based on a validation set evaluation.2 Underperforming models can be discarded, so the final deployed model does come from the higher tier of the model performance population, and
1Other reasons why researchers resort to the best performance as opposed to the mean may come from the current highly competitive atmosphere in the field with (possibly excessive) focus on performance on standard datasets (see Church (2017) or Sculley et al. (2018) for further discussion), which may motivate researchers to publish only their best results.",1. Introduction,[0],[0]
"Also, statistically sound estimation of performance does require repeatedly re-running experiments, which does incur additional cost, which researchers may prefer to invest in additional model tuning, especially in the present situation where reviewers seem not to require statistically sound evaluation of models and on the other hand may favour high-performing models.",1. Introduction,[0],[0]
"Of course, these motives should instead give place to effort to do good science, as opposed to a race on standard benchmarks.
",1. Introduction,[0],[0]
"2In some applications there is focus on speed of training and on reducing computational costs – there it does make sense to focus on the performance of the typical model as opposed to the best out of n, so the use of mean or median is appropriate.
",1. Introduction,[0],[0]
"the use of mean may be inappropriate.
",1. Introduction,[0],[0]
"Hence, rather than to completely abandon reporting the performance of the best model, we propose a way to fix its flaws.",1. Introduction,[0],[0]
"We do this by estimating the expected best-out-of-n (Boon) performance by running more than n experiments, which gives the estimate statistical validity if a sufficient number of experiments are run.",1. Introduction,[0],[0]
"We discuss how this measure relates to the performance distribution of the model, and we also give a method to empirically estimate Boon.
",1. Introduction,[0],[0]
"The paper proceeds as follows: First, we give a high-level explanation of why reporting performance of the best single model is problematic.",1. Introduction,[0],[0]
"We also give some evidence that it is widely used in the deep learning community, which is why this explanation may be needed.",1. Introduction,[0],[0]
We proceed by presenting Boon as a way to fix the above problems.,1. Introduction,[0],[0]
We then give some experimental evidence for the flaws of best-singlemodel reporting and show that Boon does not suffer from them.,1. Introduction,[0],[0]
We wrap up by discussing the place of Boon in a ML researcher’s toolbox alongside traditional measures such as mean or median.,1. Introduction,[0],[0]
"In articles presenting new deep learning architectures, the performance is often reported as the score of the “best single model” or simply “single model”.",2. Best Single Model Performance,[0],[0]
"In practice, this usually means that the researchers train multiple instances of the proposed architecture (often with different sets of hyperparameters), evaluate these instances on some validation set, and select the best-performing model.",2. Best Single Model Performance,[0],[0]
"This best model is evaluated on a test set, and the resulting test score is then reported as the metric characterizing the architecture and used for comparing it to previous models.",2. Best Single Model Performance,[0],[0]
"If the score is better than those reported in previous work, the architecture is presented as superior.",2. Best Single Model Performance,[0],[0]
"This practice results in several issues:
Population variance Since results of experiments are stochastic, the performance of a single model is just a single instance drawn from a possibly disparate population.",2. Best Single Model Performance,[0],[0]
"If others train the model on their own, they get another sample from the architecture’s performance distribution, which may substantially differ from the one listed in the original paper.",2. Best Single Model Performance,[0],[0]
"Such paper thus gives insufficient information about what to expect from the new architecture, which should be one of the article’s main points.
",2. Best Single Model Performance,[0],[0]
One may object that the result published in the paper is not chosen from the population at random – it is selected using a validation result.,2. Best Single Model Performance,[0],[0]
"However, the correlation between the validation and test results is generally imperfect; in fact, in some of our experiments, it is almost zero, as we show in Section 4.",2. Best Single Model Performance,[0],[0]
"Furthermore, if we indeed do have a strong
correlation, we get another problem:
Expectation of best result increases with the number of experiments Simply put, the more samples from a population we get, the more extreme the best among them is likely to be.",2. Best Single Model Performance,[0],[0]
"In other words, the expected value of the best result depends on the number of experiments that the researchers run.",2. Best Single Model Performance,[0],[0]
There are three closely related problems with this:,2. Best Single Model Performance,[0],[0]
"Firstly, this makes the number of experiments run an important explanatory variable; however, this variable is usually unreported, which is a severe methodological flaw in itself.",2. Best Single Model Performance,[0],[0]
"It also leads to the second problem: since each research team runs a different number of experiments, the results are not directly comparable.",2. Best Single Model Performance,[0],[0]
"Thirdly, this motivates researchers to run more experiments and gives an advantage to those who are able to do so.",2. Best Single Model Performance,[0],[0]
"This pushes publishing quantitative results towards a race in computational power rather than a fair comparison of architectures themselves.
",2. Best Single Model Performance,[0],[0]
"Best model performance is not a meaningful characteristic of the performance distribution Even if we knew the underlying theoretical performance distribution – that is, if we had perfect information about the architecture’s performance – it would not be clear what we would mean by ""best model performance"" without specifying the size of the pool from which we are choosing the best model.",2. Best Single Model Performance,[0],[0]
Imagine some architecture having a Gaussian performance distribution.,2. Best Single Model Performance,[0],[0]
"Asking what is the best possible performance does not make sense in such a case, since the support of the distribution is unbounded.",2. Best Single Model Performance,[0],[0]
"Even for capped metrics such as accuracy, where the performance distribution necessarily has bounded support, the best (possible) model3 may be so unlikely, that it would be of no practical importance.",2. Best Single Model Performance,[0],[0]
"Hence, best model performance is not a meaningful characteristic of the performance distribution.
",2. Best Single Model Performance,[0],[0]
"Generality / Falsifiability Finally, there is the question of what the authors are trying to express.",2. Best Single Model Performance,[0],[0]
"Using “best single model performance”, they are essentially claiming: “There once existed an instance of our model that once achieved a result X on dataset Y”.",2. Best Single Model Performance,[0],[0]
"Such fact is not of that much interest to the scientific community, which would rather need to know how the architecture behaves generally.",2. Best Single Model Performance,[0],[0]
"Relatedly, a frequently given characteristic of science is falsifiability of theories (Popper, 1959).",2. Best Single Model Performance,[0],[0]
"A theory claiming that there are invisible unicorns running among us is not science, since we cannot think of any potential empirical evidence that could prove the theory false.",2. Best Single Model Performance,[0],[0]
"Similarly, any number of replication experiments that produce substantially worse results cannot prove the above performance claim wrong.",2. Best Single Model Performance,[0],[0]
"If, for instance, a confidence interval were given, replications could
3In the sense of validation performance being at or near the supremum of the validation performance distribution’s support.
",2. Best Single Model Performance,[0],[0]
"very quickly show the published result at least extremely implausible, if not false.
",2. Best Single Model Performance,[0],[0]
We will quantify the former two problems for two concrete architectures in Section 4.,2. Best Single Model Performance,[0],[0]
"Despite all these problems, reporting the performance of the best model is still the main way of reporting results in some areas of ML, especially in empirically oriented deep learning papers, and, alarmingly, such practice seems to be tolerated by reviewers even at prime conferences.",2.1. Prevalence,[0],[0]
"For instance, what concerns models published on the popular Children’s Book Test dataset for Reading Comprehension (on which we run experiments later), none of the (more than ten) papers used any form of statistical testing or confidence intervals, and most reported only performance of the best single model without even mentioning the total number of experiments run.",2.1. Prevalence,[0],[0]
"These include papers published at NIPS (Hermann et al., 2015), ICLR (Hill et al., 2016; Munkhdalai & Yu, 2017), ACL (Chen et al., 2016; Dhingra et al., 2017; Cui et al., 2017), or EMNLP (Trischler et al., 2016).
",2.1. Prevalence,[0],[0]
"The same is true for the recently popular SQuAD dataset: for instance, none of the four papers (Yang et al., 2017; Wang & Jiang, 2017; Seo et al., 2017; Xiong et al., 2017) that published results on this dataset at ICLR 2017 has used any statistical testing or confidence intervals nor published mean (or otherwise aggregated) results across multiple runs.
",2.1. Prevalence,[0],[0]
"Let us look more generally at the example of ICLR 2017 (chosen as a deep-learning-focused conference featuring many empirical results – as a rough guide, 174 out of 194 ICLR papers have ""experiment"" in a (sub-)section heading).",2.1. Prevalence,[0],[0]
"Only 11 papers mention terms related to hypothesis testing4 and 11 contain the string ""confidence interval"".",2.1. Prevalence,[0],[0]
"Further details can be found in Appendix B.
While this is a rough and limited survey, it does suggest that while deep learning research is to a large extent an empirical science, statistical methods are often underused.
3.",2.1. Prevalence,[0],[0]
"Expected Best-out-of-n (Boon) Performance
The issues outlined above point to desiderata for a more suitable method of reporting an architecture’s performance.",2.1. Prevalence,[0],[0]
"It should provide information about general behaviour of the architecture under specified conditions, well characterizing
4This was checked by searching for any of the following strings: ""hypothesis test"", ""p-value"", ""t-test"" ""confidence level"", ""significance level"", ""ANOVA"", ""analysis of variance"", ""Wilcoxon"", ""sign test"".
",2.1. Prevalence,[0],[0]
the associated random performance distribution.,2.1. Prevalence,[0],[0]
"It should also be invariant under the number of experiments run and other similar variables.
",2.1. Prevalence,[0],[0]
"Given these requirements, traditional statistical measures, such as mean or median, probably come to mind of many readers.",2.1. Prevalence,[0],[0]
"They do indeed fix the above issues; still, they express only the performance of a typical member of the population.",2.1. Prevalence,[0],[0]
"However, in many ML applications, it may be the best model from a pool that is of interest.",2.1. Prevalence,[0],[0]
"When practitioners are choosing a model for deployment, they train multiple models and deploy the best-performing one 5.",2.1. Prevalence,[0],[0]
This gives some justification to reporting the performance of the best model and gives us a reason to attempt to fix its problems rather than completely dismiss it.,2.1. Prevalence,[0],[0]
"Such corrected best-model measure would be more informative than mean or median in these outlined situations.
",2.1. Prevalence,[0],[0]
"A natural way to improve comparability between models, each evaluated in a different number of experiments, is to normalize the results to the expected result if the number of experiments were the same, say n, which can be easily estimated if we run m experiments, m ≥",2.1. Prevalence,[0],[0]
n.,2.1. Prevalence,[0],[0]
"The greater the number of experiments m, the more robust the estimate of the expected best, which also helps us eliminate the problem of statistical robustness.",2.1. Prevalence,[0],[0]
"We are proposing the expected best-out-of-n performance, Boon, to be used where the performance of the best model from a pool seems as an appropriate measure.
",2.1. Prevalence,[0],[0]
"Let us first examine how the expected best-out-of-n (Boon) performance relates to the (theoretical) performance distribution we are trying to characterize; we will then proceed with empirical estimation, which is of value in practice.",2.1. Prevalence,[0],[0]
"The calculations are not particularly innovative from statistical point of view and are close to many standard results from the field of Order Statistics (see for instance Arnold et al. (2008) for more context).
3.1.",2.1. Prevalence,[0],[0]
"Boon of a probability distribution
Showing how to calculate Boon of a known theoretical probability distribution will serve two purposes: Firstly, since we are proposing Boon as a way to characterize the performance distribution, this will make the relation between Boon and the performance distribution explicit.",2.1. Prevalence,[0],[0]
"Secondly, in some cases we may be able to make an assumption about the family to which the theoretical distribution belongs (e.g.
5This would usually be the case when a model is trained once and then deployed for longer-term usage, which may be the case for instance for Machine Translation systems.",2.1. Prevalence,[0],[0]
"In other cases, when it is practical to train only as single model instance due to hardware constraints (either because training is extremely costly, or because it needs to be done repeatedly, e.g. for individual customers), we may indeed be interested in a typical model and hence in mean or median performance.
",2.1. Prevalence,[0],[0]
we could assume it is approximately Gaussian).,2.1. Prevalence,[0],[0]
"The analytic calculation below will allow us to leverage this information when empirically estimating Boon by deducing a parametric estimator, which may be especially useful when our sample size m is small.",2.1. Prevalence,[0],[0]
"Let us first look at the simpler case of validation performance (that is, the case where we are choosing the best model with respect to the metric we are reporting) as it is easier to grasp: How do we calculate an expected best Boon(P)6 of independent identically distributed (i.i.d.)",3.1.1. SINGLE EVALUATION,[0],[0]
"random variables X1, ..., Xn with probability distribution P (the performance distribution of an architecture) with a probability density function (p.d.f.)",3.1.1. SINGLE EVALUATION,[0],[0]
f and a cumulative distribution function (c.d.f.),3.1.1. SINGLE EVALUATION,[0],[0]
F ?,3.1.1. SINGLE EVALUATION,[0],[0]
"In the case where best means maximal (minimum can be calculated similarly), the maximum max{X1, ..., Xn} has a c.d.f. equal to
Fmax(x) = P[max{X1, ..., Xn} ≤ x] = P[X1 ≤ x, ...,Xn ≤ x] = Fn(x) (1)
using the independence of the Xis in the last step.
",3.1.1. SINGLE EVALUATION,[0],[0]
"In the case of a continuous distribution, we can obtain the p.d.f.",3.1.1. SINGLE EVALUATION,[0],[0]
"of the maximum by simply differentiating with respect to x:
fmax(x) = d
dx Fmax(x) = nf(x)F
n−1(x)
",3.1.1. SINGLE EVALUATION,[0],[0]
"Using the p.d.f., we can now calculate the expected value of the maximum as Boon(P) = ∫ ∞ −∞ xfmax(x)dx = ∫ ∞ −∞ xnf(x)Fn−1(x)dx
(2)
We can get a precise numerical estimate of the above integrals in any major numerical computation package such as numpy.",3.1.1. SINGLE EVALUATION,[0],[0]
"For illustration, for the standard normal distribution we have Boo5 (N (0, 1))",3.1.1. SINGLE EVALUATION,[0],[0]
"≈ 1.163, Boo10 (N (0, 1))",3.1.1. SINGLE EVALUATION,[0],[0]
≈ 1.539.,3.1.1. SINGLE EVALUATION,[0],[0]
"More generally, Boon ( N (µ, σ2) ) can then be expressed as µ + σBoon (N (0, 1)).",3.1.1. SINGLE EVALUATION,[0],[0]
"Thanks to this form we can get numerical estimates of Boon ( N (µ, σ2) )",3.1.1. SINGLE EVALUATION,[0],[0]
"just by estimating the two usual parameters of the Gaussian, Boon (N (0, 1)) becoming just a constant coefficient if we fix n. The full details of calculation for the Gaussian distribution can be found in Appendix A.
",3.1.1. SINGLE EVALUATION,[0],[0]
"In the case of a discrete performance distribution, which will be useful for empirical estimation below, we get a prob-
6Using the overline to distinguish from the double validationtest evaluation case later.
",3.1.1. SINGLE EVALUATION,[0],[0]
"ability mass function
P[max{X1, . . .",3.1.1. SINGLE EVALUATION,[0],[0]
", Xn} = m] = P[max{X1, . . .",3.1.1. SINGLE EVALUATION,[0],[0]
", Xn} ≤ m]−P[max{X1, . . .",3.1.1. SINGLE EVALUATION,[0],[0]
", Xn} < m]
(3)
so if pj is the probability weight associated with value xj , i.e. P[Xi = xj ] = pj for all i, this gives us Boon(P) = ∑ i  ∑ j: xj≤xi pj n",3.1.1. SINGLE EVALUATION,[0],[0]
−  ∑ j: xj<xi pj nxi .,3.1.1. SINGLE EVALUATION,[0],[0]
(4),3.1.1. SINGLE EVALUATION,[0],[0]
"In the previous part, we were choosing the best model with respect to the metric whose expectation we were calculating.",3.2. Validation-test evaluation,[0],[0]
"Hence, that method can be used to calculate the expected best validation performance of n models.",3.2. Validation-test evaluation,[0],[0]
"In practice, the best model is usually chosen with respect to the validation performance, while the primary interest is in the corresponding test performance.",3.2. Validation-test evaluation,[0],[0]
"To calculate the expected test performance of the best-validation model, we need to substitute the direct value of x in Equations 2 and 4, with the expectation of the test performance Xtest conditional on the validation performance xval,
Etv (xval)",3.2. Validation-test evaluation,[0],[0]
":= E [Xtest|Xval = xval]
yielding an expression for the expected test performance of the best-validation model chosen from a pool of size n
Boon(P) = ∫ Etv(xval)dPval(xval) =∫ ∞
−∞ Etv(xval)nfval(xval)F
",3.2. Validation-test evaluation,[0],[0]
"n−1 val (xval)dxval (5)
where Pval is the marginal probability distribution of the validation performance.",3.2. Validation-test evaluation,[0],[0]
"Similar simple substitution can be done in the discrete case.
",3.2. Validation-test evaluation,[0],[0]
"Expanding the expression for the bivariate Gaussian distribution with marginal test performance with mean µtest, standard deviation σtest, and test-validation correlation ρ as in Appendix A.2 yields a convenient expression
Boon = µtest + ρ σtest Boon (N (0, 1)) , (6)
which can again be used for parametric estimation.",3.2. Validation-test evaluation,[0],[0]
We usually do not know the exact performance distribution of the model; we only have samples from this distribution – the results of our experiments.,3.3. Empirical estimation,[0],[0]
"In such case, we can estimate the expected maximum empirically, and in fact it is the
empirical estimates that are likely to be used in practice to compare models.
",3.3. Empirical estimation,[0],[0]
"To get a non-parametric estimator, for which we do not make any assumption about the family of the performance distribution, we take the empirical distribution arising from our sample as an approximation of the architecture’s true performance distribution, similarly to Bootstrap methods.",3.3. Empirical estimation,[0],[0]
The empirical performance distribution P̂ assigns a probability weight of 1m to each of our m samples.,3.3. Empirical estimation,[0],[0]
We approximate Boon of the true performance distribution by Boon of this empirical distribution.,3.3. Empirical estimation,[0],[0]
"For the uniform empirical distribution, all the pi in Equation 4 are equal to 1m .",3.3. Empirical estimation,[0],[0]
"Hence, if we rank our samples from worst-validation to best-validation as (xvalidi1 , x test i1 ), . . .",3.3. Empirical estimation,[0],[0]
", (xvalidim , x test im ), we get
B̂oon ( (xvalid1 , x test 1 ), . . .",3.3. Empirical estimation,[0],[0]
", (x valid m , x test m ) )
",3.3. Empirical estimation,[0],[0]
= m∑ j=1 (( j m )n,3.3. Empirical estimation,[0],[0]
− ( j − 1 m )n),3.3. Empirical estimation,[0],[0]
xtestij .,3.3. Empirical estimation,[0],[0]
"(7)
This estimator does not make any assumption about the performance distribution from which our observations are drawn.",3.3. Empirical estimation,[0],[0]
"If we do use such an assumption (e.g. we know that the performance distribution of our architecture usually belongs to a certain family, e.g. Gaussian), we can add information to our estimator and possibly get an even better estimate (i.e. one with lower sampling error).",3.3. Empirical estimation,[0],[0]
"For the Gaussian distribution, we can use the standard estimators of the parameters in Equation 6 to get a parametric estimator
µ̂test + ρ̂ σ̂test",3.3. Empirical estimation,[0],[0]
"Boon (N (0, 1))
where µ̂test, ρ̂ and σ̂test are standard estimators of mean, correlation, and standard deviation respectively.",3.3. Empirical estimation,[0],[0]
A similar parametric estimator could be calculated for other distributions.,3.3. Empirical estimation,[0],[0]
"Boon eliminates the problem of dependence on the number of experiments run, m. However, we still need to choose n, the number of experiments to which we normalize.",3.4. Choice of n,[0],[0]
"This is similar to the choice one is facing when using a quantile – should one use the 75% one, the 95% one, or some other?
",3.4. Choice of n,[0],[0]
The choice of n most useful to a reader is when n is the number of candidate models that a practitioner would train before choosing the best one for some target application.,3.4. Choice of n,[0],[0]
Such number will differ domain to domain and will heavily depend on the computational cost of training the specific architecture on the specific domain.,3.4. Choice of n,[0],[0]
"The n of interest may differ for each reader – ideally researchers should characterize the architecture’s performance distribution as fully as possible and the readers may be able to deduce the value of Boon for whichever n they choose (up to a limit).
Leaving an additional degree of freedom in the choice of metric creates a risk of cherry picking.",3.4. Choice of n,[0],[0]
"However, in many areas of machine learning, there already are many available metrics.",3.4. Choice of n,[0],[0]
"Still, the main reporting metric seems to quickly converge on each task.",3.4. Choice of n,[0],[0]
The first published paper makes a choice; the subsequent ones usually follow suit (else they risk a suspicion that the architecture is not competitive on the previous metric).,3.4. Choice of n,[0],[0]
"We believe similar convergence is likely for Boon on each task.
",3.4. Choice of n,[0],[0]
"In our experiments, we decided to use n = 5 – the AS Reader model which we use for our experiments takes about 2 hours to train on a single GPU, so someone replicating the Boo5 performance could expect to achieve it overnight, which seems a to be a reasonable requirement.",3.4. Choice of n,[0],[0]
Even Boon is just a single number whose estimate can be noisy.,3.5. Accounting for estimator uncertainty,[0],[0]
"Hence, with Boon, as well as with mean and other ways of aggregating results of a wider population, we should always use appropriate statistical methods when trying to compare the quantitative performance of a new model against a baseline.",3.5. Accounting for estimator uncertainty,[0],[0]
"This can be done using significance testing (such as the t-test), or with the help of confidence intervals, which seems to be the method preferred by a significant part of the scientific community (e.g. Gardner & Altman (1986) or Berrar & Lozano (2013)), since it allows us to disentangle the effect size from the uncertainty associated with noise and sample size.
",3.5. Accounting for estimator uncertainty,[0],[0]
"For some theoretical distributions, there exist ways to calculate the hypothesis test or confidence interval analytically (e.g. using the t-test or standard normal quantiles for the Gaussian).",3.5. Accounting for estimator uncertainty,[0],[0]
"However, in cases where the family of the performance distribution or of the estimator is not known, we need to resort to computational methods - usually Monte Carlo (if we do know at least the family of the performance distribution) or the Bootstrap (Efron, 1979) (if we do not).",3.5. Accounting for estimator uncertainty,[0],[0]
A brief description of how to calculate confidence intervals using the Bootstrap is provided in the Appendix.,3.5. Accounting for estimator uncertainty,[0],[0]
"Note: The data and code for their analysis can be found at http://gitlab.com/obajgar/boon, along with Python functions you can use to calculate Boon.
We have run several experiments to quantify the scope of the problems outlined in Section 2.",4. Experiment Results,[0],[0]
"We just briefly summarize the main results here for illustration; a more detailed description of the experiments and analysis in the form of an iPython notebook can be found in the Gitlab repository.
",4. Experiment Results,[0],[0]
"Performance variation To estimate the random variation of results, we repeatedly 7 trained models from two domains of deep learning: the ResNet (He et al., 2016) on the CIFAR-100 dataset (Krizhevsky & Hinton, 2009) to represent Image Recognition and the Attention Sum Reader (AS Reader) (Kadlec et al., 2016) on the Children’s Book Test Common Nouns (CBT CN) (Hill et al., 2016) to represent Reading Comprehension.",4. Experiment Results,[0],[0]
Each of these trainings generated a pair of a validation and test performances.,4. Experiment Results,[0],[0]
The resulting empirical performance distributions are illustrated in Figure 1.,4. Experiment Results,[0],[0]
"If we fix all hyper-parameters, the interquartile ranges of the models’ accuracies are 0.98% and 1.20% (absolute).",4. Experiment Results,[0],[0]
Compare this to the median differences between published results on these datasets: 0.86% and 1.15% respectively8.,4. Experiment Results,[0],[0]
"Hence, random variation in performance cannot be considered negligible as is now often done.",4. Experiment Results,[0],[0]
"Furthermore, if we allow the hyper-parameters to vary (in our case by random search), the result variance further increases, which further amplifies the outlined effects.",4. Experiment Results,[0],[0]
In the case of the AS Reader the interquartile range increased to 2.9% when we randomly picked hyper-parameters from a range applicable to training the model on a single GPU.,4. Experiment Results,[0],[0]
"However, note that the problem of result incomensurability due to hyper-parameter optimization is not the focus of this work.",4. Experiment Results,[0],[0]
"The method that we present here is still applicable to the problem in the case of random hyper-parameter sampling for which we include results, however we aim to compensate mainly for randomness due to parameter initialization and data shuf-
7Specifically, 74 times for Resnet, 370 times for the AS Reader with fixed hyperparameters, and 197 times for the AS Reader with random hyperparameters.
",4. Experiment Results,[0],[0]
"8We looked at results of successive architectures evaluated on the two tasks as listed in (Huang et al., 2017; Munkhdalai & Yu, 2017).",4. Experiment Results,[0],[0]
We sorted the results with respect to the test performance and then calculated the differences between successive models.,4. Experiment Results,[0],[0]
From these we calculated the median.,4. Experiment Results,[0],[0]
"Full details can be found in the Gitlab repository.
fling – which is however significant in itself, as we have just demonstrated.
",4. Experiment Results,[0],[0]
"Several other articles confirm significant variation in model performance due to different random seeds: e.g van den Berg et al. (2016) in Speech Recognition, Henderson et al. (2018) in Deep Reinforcement Learning, or Reimers & Gurevych (2017) in Named Entity Recognition.",4. Experiment Results,[0],[0]
"They all agree that reporting performance scores of single models is insufficient to characterize architecture performance.
",4. Experiment Results,[0],[0]
Estimator variance Figure 2 shows the 95% confidence intervals of best single model results compared to the Boo5 performance for a range of result-pool sizes m. This is shown for the cases of both strong and weak test-validation correlation.,4. Experiment Results,[0],[0]
In both cases Boo5 is significantly less noisy than the best-single-model result.,4. Experiment Results,[0],[0]
"In fact in the case of random hyper-parameter search, Boon shows even smaller variation than the mean (due to the negative skew of the performance distribution).
",4. Experiment Results,[0],[0]
"Best-model performance improves with the number of experiments We also mentioned that if only the performance of the best model is reported, the more experiments are run, the better the expected result.",4. Experiment Results,[0],[0]
"Figure 2b illustrates that this effect can indeed be fairly strong, if the validation performance is a good predictor of the test performance, as is the case of the AS Reader with random hyper-parameter search, where the expectation of the best single model performance increases from 61.3% if we train it once, to 63.3% if we train it 5 times, to 63.5% for 20 times.",4. Experiment Results,[0],[0]
This effect is further explained e.g. by Jensen & Cohen (2000).,4. Experiment Results,[0],[0]
"It gives a further argument for refraining from using this method and certainly also for publishing the number of experiments run, which is often not done.",4. Experiment Results,[0],[0]
"Boon is not subject to this effect.
",4. Experiment Results,[0],[0]
"10 20 30 40 50 # experiments
0.675
0.680
0.685
0.690
0.695
te st
a cc
ur ac
y
best single boo5 mean
(a) Resnet (fixed hyperparameters; low test-validation correlation)
0 20 40 60 80 100 # experiments
0.58
0.60
0.62
0.64
0.66
te st
a cc
ur ac
y
best single boo5 mean
(b) AS Reader (random hyperparameter sampling; high testvalidation correlation)
",4. Experiment Results,[0],[0]
Figure 2: Averages and 95% confidence intervals of test performance for three ways of aggregating results of multiple experiments for various numbers of experiments run.,4. Experiment Results,[0],[0]
Each confidence interval was constructed using smoothed9 Bootstrap sampling from our pool of 75 for Resnet and 197 experiments for the AS Reader with fixed and random hyperparameters respectively.,4. Experiment Results,[0],[0]
"Since we strongly encourage researchers to provide confidence intervals for their results, we provide and overview of how to construct them using the Bootstrap in Appendix D.1 .
",4. Experiment Results,[0],[0]
"Validation-test correlation However, note that the assumption that validation performance is a good predictor of test performance is sometimes not true.",4. Experiment Results,[0],[0]
"In the two cases with fixed hyper-parameters that we looked at, the Spearman correlation between validation and test results was only 0.10 and 0.18 respectively for the two models.",4. Experiment Results,[0],[0]
The correlation significantly increases if we allow the hyper-parameters to vary – to 0.83 for the AS Reader.,4. Experiment Results,[0],[0]
These results are also illustrated in Figure 1.,4. Experiment Results,[0],[0]
"Larger validation sets are also likely to improve this correlation, which can be understood as the degree of generalization from validation to test.",4. Experiment Results,[0],[0]
Note that the problem of increasing expected performance mentioned above is relevant only in the case of higher correlation between validation and test results.,4. Experiment Results,[0],[0]
"The effect becomes very strong in the case where the performance we are reporting is also used for choosing the best model, which emphasizes the need for honest separation of validation and test data.",4. Experiment Results,[0],[0]
Boon does fix the main flaws of reporting the best single model performance.,5. Discussion,[0],[0]
"However, let us have a look at some of its limitations.
",5. Discussion,[0],[0]
"Hyperparameter tuning This work does not fully compensate for improved expected results due to hyperparameter
9While Boon and mean could be sampled using vanilla Bootstrap, best-validation result is influenced only by a single value from the sample and hence uses only few values from the upper tier of our result pool, which makes our pool size insufficient.",5. Discussion,[0],[0]
"Hence we use Gaussian kernel smoothing (Scott, 1992) to expand our result pool.
tuning, nor was it its primary aim.",5. Discussion,[0],[0]
"Boon is appropriate in the case of random hyperparameter sampling, where the performances in different runs are independent.",5. Discussion,[0],[0]
"However, this is not the case for more advanced hyperparameter optimization methods.",5. Discussion,[0],[0]
"The primary focus of this work was on tackling variability due to random initialization, data shuffling, and similar sources, which we have shown to be significant in itself.",5. Discussion,[0],[0]
"Compensation for more advanced hyperparameter tuning (and ensuring the comparability of models in that case) is certainly a worthwhile area for future research.
",5. Discussion,[0],[0]
"Mean, median, and other alternatives We do not claim our method to be strictly superior to traditional ways of aggregating results, such as mean or quantiles.",5. Discussion,[0],[0]
"However, we have outlined a case where using Boon is justified – situations where a final model to be deployed can be chosen from a pool of trained candidates.",5. Discussion,[0],[0]
"In such case, Boon is easily interpretable and more informative than a performance of a typical model, expressed by mean or median.",5. Discussion,[0],[0]
"Hence, we think Boon is a useful addition to the methodological toolbox along existing methods.
",5. Discussion,[0],[0]
Just a single number Boon is still just a single number whose ability to characterize the performance distribution is limited by its single dimension.,5. Discussion,[0],[0]
"Paper authors should try to characterise the performance distribution as fully as possible, which may involve a histogram, mean, standard deviation, ideally along a dataset containing the results of all experiments, from which an interested reader may be able to deduce whichever characteristic she finds interesting.
",5. Discussion,[0],[0]
"Unfortunately, such characterization is usually lacking.
",5. Discussion,[0],[0]
"However, alongside this detailed characterization, describing an architecture’s performance by a single number still has its appeal, especially for the purpose of comparison among architectures and choosing the best one according to some criterion (in fact, each quantitative score can be understood as a proxy for ordering architectures with respect to such criterion of interest, such as expected performance of the best model out of n).",5. Discussion,[0],[0]
"We have explained why, in some cases, Boon may be useful for such purpose.
",5. Discussion,[0],[0]
"Computational cost Some may deem Boon impractical due to its requirement to train architectures many times, which may be very expensive in some cases.",5. Discussion,[0],[0]
"However, result stochasticity needs to be addressed to produce reliable results, and it is hard to imagine a general method to do so without repeated evaluation10.",5. Discussion,[0],[0]
Researchers can simply focus on architectures which they can evaluate properly given their resources.,5. Discussion,[0],[0]
"However, the main target of our criticism is not projects whose resources are stretched by a single training; it is projects that do have the necessary resources for multiple evaluations but use them to produce better-looking results rather than results that are more informative and robust.",5. Discussion,[0],[0]
Reporting just the best single model performance is not statistically sound.,6. Conclusion,[0],[0]
This practice in machine learning research needs to change if the research is to have lasting value.,6. Conclusion,[0],[0]
"Reviewers can play an important role in bringing this change.
",6. Conclusion,[0],[0]
"Still, asking for the performance of a best model out of n can have valid reasons.",6. Conclusion,[0],[0]
"For the situations where the best-model performance is indeed a good metric, we are suggesting Boon as a way to evaluate it properly.",6. Conclusion,[0],[0]
A. Boon of the Gaussian Distribution In this section we will calculate Boon of the Gaussian distribution.,Appendix,[0],[0]
"This can serve as a basis for a parametric estimator of Boon, when we assume a performance distribution to
be (approximately) Gaussian, which was the case of some of the performance distributions we have examined, for instance the AS Reader with fixed hyper-parameters.
A.1.",Appendix,[0],[0]
"Single evaluation dataset
In the simpler case in which the best model is chosen with respect to the same dataset on which the performance is then reported, we can substitute the p.d.f.",Appendix,[0],[0]
and c.d.f.,Appendix,[0],[0]
of the Gaussian distribution,Appendix,[0],[0]
"into Equation 2 to get
Boon(N (µ, σ2)) =∫ ∞ −∞ xn 1√ 2πσ2 e (x−µ)2 2σ2 Φn−1 ( x− µ σ ) dx (8)
where Φ is the c.d.f. of a standard normal random variable",Appendix,[0],[0]
.,Appendix,[0],[0]
"Substituting z = x−µσ , dx = σdz, yields∫ ∞
−∞ n(µ+ σz) 1√ 2πσ2 e z2 2 Φn−1 (z)σdz =
= µ ∫∞ −∞ n 1√ 2π e z2 2 Φn−1 (z) dz+
+σ ∫ ∞ −∞ nz 1√ 2π e z2 2 Φn−1 (z) dz = µ+σBoon",Appendix,[0],[0]
"(N (0, 1))
(9)
(the first integrand has the form of the p.d.f. found above and hence integrates to one) so the expected maximum is neatly expressed in terms of a maximum of a standard normal and is linearly proportional to both the mean and the standard deviation.",Appendix,[0],[0]
"Once n is fixed for comparison purposes, Boon (N (0, 1)) is just a constant, e.g. Boo5 (N (0, 1))",Appendix,[0],[0]
"≈ 1.163, Boo10 (N (0, 1))",Appendix,[0],[0]
≈ 1.539.,Appendix,[0],[0]
Let us turn to the case of reporting a test set performance of a best-validation model.,A.2. Test-validation evaluation,[0],[0]
"If we model the validation and test performances by a Bivariate Normal Distribution with validtest correlation ρ, means µval, µtest, and variances σ2val, σ 2 test, then given a validation performance xval, the test performance is distributed normally with conditional expectation
Etv(xval) = µtest + ρ σtest σval (xval − µval)
which gives
Boon(N (µ, σ2))",A.2. Test-validation evaluation,[0],[0]
=∫ ∞ −∞ ( µtest + ρ σtest σval (xval − µval) ),A.2. Test-validation evaluation,[0],[0]
n,A.2. Test-validation evaluation,[0],[0]
"·
· 1√ 2πσ2val e (x−µval) 2 2σ2val",A.2. Test-validation evaluation,[0],[0]
Φn−1 ( x− µval σval ),A.2. Test-validation evaluation,[0],[0]
dx .,A.2. Test-validation evaluation,[0],[0]
"(10)
Using the same two tricks as above, this can be simplified to
Boon(N (µ, σ2)) = µtest +ρ σtest Boon (N (0, 1))",A.2. Test-validation evaluation,[0],[0]
"(11)
where Boon (N (0, 1)) is the single-evaluation expected maximum of the standard normal distribution as defined above.",A.2. Test-validation evaluation,[0],[0]
"We downloaded the pdfs of all papers accepted to ICLR 201711, extracted text from them using the OpenSource Xpdf package12 and then searched the resulting text documents using the grep command as follows.
",B. Survey of ICLR 2017 Papers: Method,[0],[0]
"Firstly, to roughly estimate the usage of experiments in the papers, we searched for the capitalized string ""EXPERIMENT"" in the documents, since all (sub-)section headings are capitalized in the ICLR format.",B. Survey of ICLR 2017 Papers: Method,[0],[0]
This was matched in 174 documents.,B. Survey of ICLR 2017 Papers: Method,[0],[0]
"Further 6 contained the string ""EVALUATION"" yielding a total of 180 out of 194 papers containing one of the two strings, which suggests that many ICLR papers indeed have an empirical component, though our rough method is only very approximate.
",B. Survey of ICLR 2017 Papers: Method,[0],[0]
"We then searched for the string ""confidence interval"", which was matched in only 11 papers, and further 11 documents matched one of expressions related to hypothesis testing (curiously, a set completely disjoint from the ""confidence interval"" set).",B. Survey of ICLR 2017 Papers: Method,[0],[0]
"These terms were: ""hypothesis test"", ""p-value"", ""t-test"" ""confidence level"", ""significance level"", ""ANOVA"", ""analysis of variance"", ""Wilcoxon"", and ""sign test"".",B. Survey of ICLR 2017 Papers: Method,[0],[0]
This may actually be only an upper bound since mentioning the term somewhere in the paper does not necessarily mean that the method was employed in the experimental procedure.,B. Survey of ICLR 2017 Papers: Method,[0],[0]
"Note: The data and code for their analysis will be available at http://gitlab.com/obajgar/boon.
",C. Experiments: Details,[0],[0]
Here we provide further details of our experiments quantifying the extent of result stochasticity and the resulting effects.,C. Experiments: Details,[0],[0]
"To run our experiments we have chosen Open Source implementations13 of models from two popular domains of deep learning, namely ResNet (He et al., 2016) on the CIFAR-100 dataset (Krizhevsky & Hinton, 2009) for Image Classifica-
11Downloaded from https://openreview.net/group?id=ICLR.cc/ /2017/conference from sections ""Paper decision: Accept (Oral)"" and ""Paper decision:",C.1. Models,[0],[0]
"Accept (Poster)"".
12http://www.xpdfreader.com/; we used version 4.00.01 on Debian Linux 9.2.
13The source code for Resnet can be found at https://github.com/tensorflow/models/tree/ master/research/resnet; the code for the AS Reader at https://github.com/rkadlec/asreader.
tion and the AS Reader (Kadlec et al., 2016) on the CBT CN dataset (Hill et al., 2016) for Reading Comprehension.",C.1. Models,[0],[0]
"We believe these two models are representative of models in their respective areas – Resnet is based on a deep convolutional network architecture as most recent models in machine vision, while the AS Reader is based on a bidirectional GRU network with attention, as is the case for many models in Natural Language Processing.",C.1. Models,[0],[0]
"To collect the data for our experiments, we repeatedly trained the two models.",C.2. Data collection,[0],[0]
Each training instance had a different random parameter initialization and random data shuffling.,C.2. Data collection,[0],[0]
We evaluated the model on the validation and test datasets at least once per epoch.,C.2. Data collection,[0],[0]
"We then took the validation and test performance at the best-validation epoch as a data point for our further analyses.
",C.2. Data collection,[0],[0]
"All training was done on Ubuntu 14.04 on a single GPU per training, either Nvidia Tesla K80 or GTX 1080.
",C.2. Data collection,[0],[0]
C.2.1.,C.2. Data collection,[0],[0]
"RESNET
Resnet was trained with a single set of hyperparameters, the default ones for the above Open Source implementation.",C.2. Data collection,[0],[0]
That means 5 residual units resulting in a 32-layer Resnet.,C.2. Data collection,[0],[0]
"The model was trained using the 0.9 momentum optimizer, with batch size 128, initial learning rate of 0.1 lowered to 0.01 after 40,000 steps and to 0.001 after 60,000 steps.",C.2. Data collection,[0],[0]
"Data augmentation included padding to 36x36 and then random cropping, horizontal flipping and per-image whitening.",C.2. Data collection,[0],[0]
"L2 regularization weight was set 0.002.
",C.2. Data collection,[0],[0]
Training was done using Tensorflow 1.3.,C.2. Data collection,[0],[0]
The AS Reader was trained in two different settings.,C.3. AS Reader,[0],[0]
"Firstly 370 times with hyper-parameters fixed to embedding dimension of 128 and 384 hidden dimensions in the GRU units, with all other hyper-parameters as used in the original AS Reader paper (Kadlec et al., 2016).
",C.3. AS Reader,[0],[0]
"In the second setting, the hyper-parameters for each training instance were chosen randomly from the following ranges:",C.3. AS Reader,[0],[0]
"The batch size was chosen from the range [16, 128], and the embedding size and hidden state size were each chosen from the range",C.3. AS Reader,[0],[0]
"[16, 512] with the log2 value of the parameter being distributed uniformly in the interval.",C.3. AS Reader,[0],[0]
"Parameters from these ranges worked reasonably well in our preliminary experiments.
",C.3. AS Reader,[0],[0]
"Training was done using Theano 0.9.0 and Blocks 0.2.0.
C.4.",C.3. AS Reader,[0],[0]
"Performance distribution results
Figure 1 plots the histograms of test performances of the evaluated models.",C.3. AS Reader,[0],[0]
"The mean test accuracy for Resnet was 68.41% with standard deviation of 0.67% (absolute), the range was 67.31% − 69.41%.",C.3. AS Reader,[0],[0]
For AS reader with fixed hyperparameters the mean was 63.16% with standard deviation 0.94% and range of 61.52% − 64.60%.,C.3. AS Reader,[0],[0]
"In the case of random hyper-parameter search the mean was 61.26%, standard deviation 2.48%, and values ranged from 56.61% to 64.01%.
",C.3. AS Reader,[0],[0]
"In both cases with fixed hyper-parameters the collected results are consistent with coming from a Gaussian distribution according to the Anderson-Darling test14 (Anderson & Darling, 1954); the histograms also make it appear plausible that the performance distribution is approximately Gaussian.",C.3. AS Reader,[0],[0]
"This is not the case for the random hyper-parameter search where the distribution has a clear negative skew.
",C.3. AS Reader,[0],[0]
"To put the above numbers into context, we also examined the margin of improvement of successive architectures published on the corresponding datasets, as listed in (Munkhdalai & Yu, 2017; Huang et al., 2017).",C.3. AS Reader,[0],[0]
We sorted the results with respect to the test performance and then calculated the differences between successive models.,C.3. AS Reader,[0],[0]
"The median difference was for 0.86% for CIFAR-100 and 1.15% for CBT CN.
",C.3. AS Reader,[0],[0]
Note that the median differences are smaller than two standard deviations for each model.,C.3. AS Reader,[0],[0]
Two standard deviations from the mean approximately give the 95% confidence interval for a Gaussian distribution – hence we could typically fit three successive published results within the width of one such confidence interval.,C.3. AS Reader,[0],[0]
"The magnitude of the performance variation due to random initialization and data shuffling is therefore not negligible compared to the improvements in performance, which often hold an important place within articles in which they are presented.",C.3. AS Reader,[0],[0]
"We hence think it is inappropriate to completely ignore this random variation in evaluation protocols, which is currently the usual practice.",C.3. AS Reader,[0],[0]
The best model is usually selected using validation performance 15.,C.5. Test-validation correlation,[0],[0]
This practice is based on the assumption that the validation accuracy is a reasonably good predictor of test accuracy.,C.5. Test-validation correlation,[0],[0]
"The results of our experiments, illustrated also in Figure 1, suggest that this assumption holds for performance variation due to hyper-parameter choice.",C.5. Test-validation correlation,[0],[0]
"However, if we fix the hyper-parameters, the correlation almost disappears.",C.5. Test-validation correlation,[0],[0]
"To some extent, this implies that selecting the best validation
14That is, despite the relatively large sample sizes, gaussianity cannot be ruled out at 0.05 significance level based on collected evidence.
15Or at least should be.
model means we are picking randomly with respect to the test performance.",C.5. Test-validation correlation,[0],[0]
"Since we are picking from a random test performance distribution, this further calls for better characterization of the distribution than a single instance drawn from it.
",C.5. Test-validation correlation,[0],[0]
"On the other hand if the correlation is strong, as seems to be the case if we do perform hyper-parameter search, we face the second problem with reporting the best-validation performance:
C.6.",C.5. Test-validation correlation,[0],[0]
"Effect of the number of experiments
If the validation performance is a good predictor of the test performance, then the more models we train the better the best-validation model is likely to be even on the test set since we are able to select models high up the right tail of the performance distribution.",C.5. Test-validation correlation,[0],[0]
"This effect has been described in more detail in (Jensen & Cohen, 2000), though with focus on induction algorithms; here we present an estimate of its effect in the case of Resnet and AS Reader.
To test this effect we took the pool of trained models.",C.5. Test-validation correlation,[0],[0]
"For eachm in the range from 1 to 50 (or 100 for the AS Reader), we randomly sampled 100, 000 samples of size m from the pool, and selected the best-validation model from each sample.",C.5. Test-validation correlation,[0],[0]
"The mean test performance across the 100, 000 samples for each m is plotted in Figure 2.
",C.5. Test-validation correlation,[0],[0]
"The results show that when there is suitable correlation between validation and test performances, increasing the number of experiments does increase the expected performance of the best-validation model.",C.5. Test-validation correlation,[0],[0]
"This makes the number of experiments an important explanatory variable, which however usually goes unreported.",C.5. Test-validation correlation,[0],[0]
"Furthermore, it makes results reported by different research teams not directly comparable.",C.5. Test-validation correlation,[0],[0]
"Finally, it gives an advantage to those that can run more experiments.",C.5. Test-validation correlation,[0],[0]
We believe that this again makes the practice of reporting the performance of the best single model unsuitable.,C.5. Test-validation correlation,[0],[0]
"If an estimator characterizing a performance distribution, say B̂oon or average, is calculated from experimental observations, it is subject to random variation, so if another research team tries to reproduce the experiments, they generally get a different estimate.",D.1. Confidence intervals,[0],[0]
"The more observations are collected, the more precise the estimate generally is.",D.1. Confidence intervals,[0],[0]
Confidence intervals provide a natural way to express this uncertainty.,D.1. Confidence intervals,[0],[0]
"Their usage also gives a sense whether the number of performed experiments was sufficient to reduce the uncertainly to a reasonable level, which is again not frequently addressed in ML papers.
",D.1. Confidence intervals,[0],[0]
The construction of the confidence interval would be trivial if we knew the distribution from which our estimate was drawn (as opposed to the distribution of the performance!),D.1. Confidence intervals,[0],[0]
"– it is simply the interval between the appropriate quantiles, e.g. the 2.5th and 97.5th quantiles in the case of the 95% confidence interval.",D.1. Confidence intervals,[0],[0]
Such distribution has been studied extensively for instance in the case of a mean of Gaussian random variables.,D.1. Confidence intervals,[0],[0]
"However, in other cases, it is not known.",D.1. Confidence intervals,[0],[0]
"If we know at least the distribution from which the individual observations were drawn, we can use Monte Carlo methods to precisely estimate the confidence interval; however, if we are not able to make an assumption about the underlying distribution, we need to use only what we have: our samples from the distribution.",D.1. Confidence intervals,[0],[0]
"In such case the variability of our estimator can be approximated using the Bootstrap (Efron, 1979) or similar methods.
",D.1. Confidence intervals,[0],[0]
"The Bootstrap consists of repeatedly sampling with replacement m random observations from our pool of m observations, say B times.",D.1. Confidence intervals,[0],[0]
"Each such sample is then used to calculate an estimate of our quantity of interest, say Boon or mean.",D.1. Confidence intervals,[0],[0]
This creates a sample of B values of the estimator.,D.1. Confidence intervals,[0],[0]
"The confidence interval can then be easily estimated taking the appropriate quantiles from this resulting Bootstrap distribution of the estimator, which should be approximating the unknown underlying sampling distribution.",D.1. Confidence intervals,[0],[0]
"The Bootstrap distribution has been shown to converge to the true underlying performance distribution.
",D.1. Confidence intervals,[0],[0]
"If we know the underlying distribution (up to some parameters), we can estimate its parameters and then generate a simulated Monte Carlo sample from the distribution, which can be used to calculate a sample of the estimator and the corresponding confidence interval in a similar way as above
with the advantage of the distribution being smoother.
",D.1. Confidence intervals,[0],[0]
"Beside estimating the confidence interval for the value of Boon or mean itself, either re-sampling method can be used to construct a confidence interval for the relative improvement of the newly proposed architecture compared to a baseline.",D.1. Confidence intervals,[0],[0]
The improvement can then be considered significant if zero is not included in the confidence interval.,D.1. Confidence intervals,[0],[0]
"More details on constructing Bootstrap confidence intervals can be found in many standard texts on computational statistics, for instance in (Efron, 1987).
",D.1. Confidence intervals,[0],[0]
"For illustration, we calculated the Bootstrap confidence interval for several sample sizes m for Resnet and the AS Reader.",D.1. Confidence intervals,[0],[0]
"Each was constructed using B = 100, 000.",D.1. Confidence intervals,[0],[0]
The results are plotted in Figure 2.,D.1. Confidence intervals,[0],[0]
"Figure 3 shows the comparison of the non-parametric and Gaussian parametric estimators of Boon, both introduced in Section 3.3, in terms of their variance for various sample sizes.",E. Comparison of estimators,[0],[0]
The parametric estimator shows a somewhat lower variance.,E. Comparison of estimators,[0],[0]
"This is an advantage if the performance distribution is indeed approximately Gaussian, which is the case for both cases with fixed hyperparameters that we tested in our experiments.",E. Comparison of estimators,[0],[0]
"However, this can introduce bias if the true performance distribution differs from the theoretical distribution assumed by a parametric estimator, so one should be prudent to use it.",E. Comparison of estimators,[0],[0]
"We point out important problems with the common practice of using the best single model performance for comparing deep learning architectures, and we propose a method that corrects these flaws.",abstractText,[0],[0]
"Each time a model is trained, one gets a different result due to random factors in the training process, which include random parameter initialization and random data shuffling.",abstractText,[0],[0]
Reporting the best single model performance does not appropriately address this stochasticity.,abstractText,[0],[0]
We propose a normalized expected best-out-of-n performance (Boon) as a way to correct these problems.,abstractText,[0],[0]
A Boo(n) for Evaluating Architecture Performance,title,[0],[0]
"Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 934–944, Baltimore, Maryland, USA, June 23-25 2014. c©2014 Association for Computational Linguistics
Following the works of Carletta (1996) and Artstein and Poesio (2008), there is an increasing consensus within the field that in order to properly gauge the reliability of an annotation effort, chance-corrected measures of inter-annotator agreement should be used. With this in mind, it is striking that virtually all evaluations of syntactic annotation efforts use uncorrected parser evaluation metrics such as bracket F1 (for phrase structure) and accuracy scores (for dependencies).
In this work we present a chance-corrected metric based on Krippendorff’s α, adapted to the structure of syntactic annotations and applicable both to phrase structure and dependency annotation without any modifications. To evaluate our metric we first present a number of synthetic experiments to better control the sources of noise and gauge the metric’s responses, before finally contrasting the behaviour of our chance-corrected metric with that of uncorrected parser evaluation metrics on real corpora.1",text,[0],[0]
It is a truth universally acknowledged that an annotation task in good standing be in possession of a measure of inter-annotator agreement (IAA).,1 Introduction,[0],[0]
"However, no such measure is in widespread use for the task of syntactic annotation.",1 Introduction,[0],[0]
"This is due to a mismatch between the formulation of the agreement measures, which assumes that the annotations have no or relatively little internal structure,
1The code used to produce the data in this paper, and some of the datasets used, are available to download at https://github.com/arnsholt/syn-agreement/
and syntactic annotation where structure is the entire point of the annotation.",1 Introduction,[0],[0]
For this reason efforts to gauge the quality of syntactic annotation are hampered by the need to fall back to simple accuracy measures.,1 Introduction,[0],[0]
"As shown in Artstein and Poesio (2008), such measures are biased in favour of annotation schemes with fewer categories and do not account for skewed distributions between classes, which can give high observed agreement, even if the annotations are inconsistent.
",1 Introduction,[0],[0]
"In this article we propose a family of chancecorrected measures of agreement, applicable to both dependency- and constituency-based syntactic annotation, based on Krippendorff’s α and tree edit distance.",1 Introduction,[0],[0]
"First we give an overview of traditional agreement measures and why they are insufficient for syntax, before presenting our proposed metrics.",1 Introduction,[0],[0]
"Next, we present a number of synthetic experiments performed in order to find the best distance function for this kind of annotation; finally we contrast our new metric and simple accuracy scores as applied to real-world corpora before concluding and presenting some potential avenues for future work.",1 Introduction,[0],[0]
"The definitive reference for agreement measures in computational linguistics is Artstein and Poesio (2008), who argue forcefully in favour of the use of chance-corrected measures of agreement over simple accuracy measures.",1.1 Previous work,[0],[0]
"However, most evaluations of syntactic treebanks use simple accuracy measures such as bracket F1 scores for constituent trees (NEGRA, Brants, 2000; TIGER, Brants and Hansen, 2002; Cat3LB, Civit et al., 2003; The Arabic Treebank, Maamouri et al., 2008) or labelled or unlabelled attachment scores for dependency syntax (PDT, Hajič, 2004; PCEDT Mikulová and Štěpánek, 2010; Norwegian Dependency Treebank, Skjærholt, 2013).",1.1 Previous work,[0],[0]
"The only work we know of using chance-corrected metrics
934
is Ragheb and Dickinson (2013), who use MASI (Passonneau, 2006) to measure agreement on dependency relations and head selection in multiheaded dependency syntax, and Bhat and Sharma (2012), who compute Cohen’s κ (Cohen, 1960) on dependency relations in single-headed dependency syntax.",1.1 Previous work,[0],[0]
"A limitation of the first approach is that token ID becomes the relevant category for the purposes of agreement, while the second approach only computes agreements on relations, not on structure.
",1.1 Previous work,[0],[0]
"In grammar-driven treebanking (or parsebanking), the problems encountered are slightly different.",1.1 Previous work,[0],[0]
In HPSG and LFG treebanking annotators do not annotate structure directly.,1.1 Previous work,[0],[0]
"Instead, the grammar parses the input sentences, and the annotator selects the correct parse (or rejects all the candidates) based on discriminants2 of the parse forest.",1.1 Previous work,[0],[0]
"In this context, de Castro (2011) developed a variant of κ that measures agreement over discriminant selection.",1.1 Previous work,[0],[0]
"This is different from our approach in that agreement is computed on annotator decisions rather than on the treebanked analyses, and is only applicable to grammar-based approaches such as HPSG and LFG treebanking.
",1.1 Previous work,[0],[0]
The idea of using edit distance as the basis for an inter-annotator agreement metric has previously been explored by Fournier (2013).,1.1 Previous work,[0],[0]
However that work used a boundary edit distance as the basis of a metric for the task of text segmentation.,1.1 Previous work,[0],[0]
"In this paper, we mostly follow the notation and terminology of Artstein and Poesio (2008), with some additions.",1.2 Notation,[0],[0]
"The key components in an agreement study are the items annotated, the coders who make judgements on individual items, and the annotations created for the items.",1.2 Notation,[0],[0]
"We denote these as follows:
•",1.2 Notation,[0],[0]
"The set of items I = {i1, i2, . . . }",1.2 Notation,[0],[0]
"• The set of coders C = {c1, c2, . . . }",1.2 Notation,[0],[0]
• The set of annotations X is a set of sets X = {Xi|i ∈,1.2 Notation,[0],[0]
I} where each set Xi = {xic|c ∈ C} contains the annotations for each item.,1.2 Notation,[0],[0]
"If not all coders annotate all items, the different Xi will be of different sizes.
",1.2 Notation,[0],[0]
"2A discriminant is an attribute of the analyses produced by the grammar where some of the analyses differ, e.g. is the word jump a noun or a verb, or does a PP attach to a VP or the VP’s object NP.
",1.2 Notation,[0],[0]
In the case of nominal categorisation we will also use the set K of possible categories.,1.2 Notation,[0],[0]
"The most common metrics used in computational linguistics are the metrics κ (Cohen, 1960, introduced to computational linguistics by Carletta, 1996) and π (Scott, 1955).",2 The metric,[0],[0]
"These metrics express agreement on a nominal coding task as the ratio κ, π = Ao−Ae/1−Ae where Ao is the observed agreement andAe",2 The metric,[0],[0]
the expected agreement according to some model of “random” annotation.,2 The metric,[0],[0]
"Both metrics have essentially the same model of expected agreement:
Ae = ∑ k∈K P (k|c1)P (k|c2) (1)
differing only in how they estimate the probabilities: κ assigns separate probability distributions to each coder based on their observed behaviour, while π uses the same distribution for both coders based on their aggregate behaviour.
",2 The metric,[0],[0]
"Now, if we want to perform this same kind of evaluation on syntactic annotation it is not possible to use κ or π directly.",2 The metric,[0],[0]
"In the case of dependencybased syntax we could conceivably use a variant of these metrics by considering the ID of a token’s head as a categorical variable (the approach taken in Ragheb and Dickinson, 2013), but we argue that this is not satisfactory.",2 The metric,[0],[0]
"This use of the metrics would consider agreement on categories such as “tokens whose head is token number 24”, which is obviously not a linguistically informative category.",2 The metric,[0],[0]
Thus we have to reject this way of assessing the reliability of dependency syntax annotation.,2 The metric,[0],[0]
"Also, this approach is not directly generalisable to constituency-based syntax.
",2 The metric,[0],[0]
For dependency syntax we could generalise these metrics similarly to how κ is generalised to κw to handle partial credit for overlapping annotations.,2 The metric,[0],[0]
"Let the function LAS(t1, t2) be the number of tokens with the same head and label in the two trees t1 and t2, T (i) the set of trees possible for an item",2 The metric,[0],[0]
i ∈,2 The metric,[0],[0]
"I , and tokens the number of tokens in the corpus.",2 The metric,[0],[0]
"Then we can compute an expected agreement as follows:
Ae = 1
tokens ∑ i∈I ∑ t1,t2∈T (i)2 LASe(t1, t2) (2)
LASe(t1, t2) = P (t1|c1)P (t2|c2)LAS(t1, t2)
",2 The metric,[0],[0]
We see three problems with this approach.,2 The metric,[0],[0]
"First of all the number of possible trees for a sentence grows exponentially with sentence length, which means that explicitly iterating over all possible such pairs is computationally intractable, nor have we been able to easily derive an algorithm for this particular problem from standard algorithms.
",2 The metric,[0],[0]
"Second, the question of which model to use for P (t|c) is not straightforward.",2 The metric,[0],[0]
"It is possible to use generative parsing models such as PCFGs or the generative dependency models of Eisner (1996), but agreement metrics require a model of random annotation, and as such using models designed for parsing runs the risk of over-estimating Ae, resulting in artificially low agreement scores.
",2 The metric,[0],[0]
"Finally, it may be hard to establish a consensus in the field of which particular metric to use.",2 The metric,[0],[0]
"As shown by the existence of three different metrics (κ, π and S (Bennett et al., 1954)) for the relatively simple task of nominal coding, the choice of model for P (t|c) will not be obvious, and thus differing choices of generative model as well as different choices for parameters such as smoothing will result in subtly different agreement metrics.",2 The metric,[0],[0]
"The results of these different metrics will not be directly comparable, which will make the results of groups using different metrics unnecessarily hard to compare.
",2 The metric,[0],[0]
"Instead, we propose to use an agreement measure based on Krippendorff’s α (Krippendorff, 1970; Krippendorff, 2004) and tree edit distance.",2 The metric,[0],[0]
"In this approach we compare tree structures directly, which is extremely parsimonious in terms of assumptions, and furthermore sidesteps the problem of probabilistically modelling annotators’ behaviour entirely.",2 The metric,[0],[0]
"Krippendorff’sα is not as commonly used as κ and π, but it has the advantage of being expressed in terms of an arbitrary distance
function δ.",2 The metric,[0],[0]
"A full derivation of α is beyond the scope of this article, and we will simply state the formula used to compute the agreement.",2 The metric,[0],[0]
"Krippendorff’s α is normally expressed in terms of the ratio of observed and expected disagreements: α = 1 − Do/De, where Do is the mean squared distance between annotations of the same item and De the mean squared distance between all pairs of annotations:
Do = ∑ i∈I 1 |Xi| − 1 ∑ c∈C ∑ c′∈C δ(xic, xic′)2 De = 1∑
i∈I |Xi|",2 The metric,[0],[0]
"− 1 ∑ i∈I ∑ c∈C ∑ i′∈I ∑ c′∈C δ(xic, xi′c′)2
Note that in the expression for De, we are computing the difference between annotations for different items; thus, our distance function for syntactic trees needs to be able to compute the difference between arbitrary trees for completely unrelated sentences.",2 The metric,[0],[0]
"The function δ can be any function as long as it is a metric; that is, it must be (1) non-negative, (2) symmetric, (3) zero only for identical inputs, and (4) it must obey the triangle inequality:
1. ∀x, y : δ(x, y) ≥ 0 2. ∀x, y : δ(x, y) = δ(x, y) 3. ∀x, y : δ(x, y) = 0⇔",2 The metric,[0],[0]
"x = y 4. ∀x, y, z : δ(x, y) + δ(y, z) ≥",2 The metric,[0],[0]
"δ(x, z)",2 The metric,[0],[0]
"This immediately excludes metrics like ParsEval (Black et al., 1991) and Leaf-Ancestor (Sampson and Babarczy, 2003), since they assume that the trees being compared are parses of the same sentence.",2 The metric,[0],[0]
"Instead, we base our work on tree edit distance.",2 The metric,[0],[0]
The tree edit distance (TED) problem is defined analogously to the more familiar problem of string edit distance: what is the minimum number of edit operations required to transform one tree into the other?,2 The metric,[0],[0]
See Bille (2005) for a thorough introduction to the tree edit distance problem and other related problems.,2 The metric,[0],[0]
"For this work, we used the algorithm of Zhang and Shasha (1989).",2 The metric,[0],[0]
"Tree edit distance has previously been used in the TEDEVAL software (Tsarfaty et al., 2011; Tsarfaty et al., 2012) for parser evaluation agnostic to both annotation scheme and theoretical framework, but this by itself is still an
uncorrected accuracy measure and thus unsuitable for our purposes.3
When comparing syntactic trees, we only want to compare dependency relations or non-terminal categories.",2 The metric,[0],[0]
"Therefore we remove the leaf nodes in the case of phrase structure trees, and in the case of dependency trees we compare trees whose edges are unlabelled and nodes are labelled with the dependency relation between that word and its head; the root node receives the label .",2 The metric,[0],[0]
"An example of this latter transformation is shown in Figure 1.
",2 The metric,[0],[0]
"We propose three different distance functions for the agreement computation: the unmodified tree edit distance function, denoted δplain, a second function δdiff (x, y) = TED(x, y)−abs(|x|− |y|), the edit distance minus the difference in length between the two sentences, and finally δnorm(x, y) = TED(x,y)/|x|+|y|, the edit distance normalised to the range [0, 1].4
The plain TED is the simplest in terms of parsimony assumptions, however it may overestimate the difference between sentences, we intuitively find to be syntactically similar.",2 The metric,[0],[0]
"For example the only difference between the two leftmost trees in Figure 2 is a modifier, but δplain gives them distance 4 and δdiff 0.",2 The metric,[0],[0]
"On the other hand, δdiff might underestimate some distances as well; for exam-
3While it is quite different from other parser evaluation schemes, TEDEVAL does not correct for chance agreement and is thus an uncorrected metric.",2 The metric,[0],[0]
"It could of course form the basis for a corrected metric, given a suitable measure of expected agreement.
",2 The metric,[0],[0]
4We can easily show that |x|,2 The metric,[0],[0]
"+ |y| is an upper bound on the TED, corresponding to deleting all nodes in the source tree and inserting all the nodes in the target.
",2 The metric,[0],[0]
"ple the leftmost and rightmost trees also have distance zero using δdiff , despite our syntactic intuition that the difference between a transitive and an intransitive should be taken account of.
",2 The metric,[0],[0]
"The third distance function, δnorm, takes into account a slightly different concern; namely that when comparing a long sentence and a short sentence, the distance has to be quite large simply to account for the difference in number of nodes, unlike comparing two short or two long sentences.",2 The metric,[0],[0]
"Normalising to the range [0, 1] puts all pairs on an equal footing.
",2 The metric,[0],[0]
"However, we cannot a priori say which of the three functions is the optimal choice of distance functions.",2 The metric,[0],[0]
"The different functions have different properties, and different advantages and drawbacks, and the nature of their strengths and weaknesses differ.",2 The metric,[0],[0]
"We will therefore perform a number of synthetic experiments to investigate their properties in a controlled environment, before applying them to real-world data.",2 The metric,[0],[0]
"In the previous section, we proposed three different agreement metrics αplain, αdiff and αnorm, each involving different trade-offs.",3 Synthetic experiments,[0],[0]
Deciding which of these metrics is the best one for our purposes of judging the consistency of syntactic annotation poses a bit of a conundrum.,3 Synthetic experiments,[0],[0]
"We could at this point apply our metrics to various real corpora and compare the results, but since the consistency of the corpora is unknown, it’s impossible to say whether the best metric is the one resulting in the highest scores, the lowest scores or somewhere in the middle.",3 Synthetic experiments,[0],[0]
"To properly settle this question, we first performed a number of synthetic experiments to gauge how the different metrics respond to disagreement.
",3 Synthetic experiments,[0],[0]
"The general approach we take is based on that used by Mathet et al. (2012), adapted to dependency trees.",3 Synthetic experiments,[0],[0]
"An already annotated corpus, in our case 100 randomly selected sentences from the Norwegian Dependency Treebank (Solberg et al., 2014), are taken as correct and then permuted to produce “annotations” of different quality.",3 Synthetic experiments,[0],[0]
"For dependency trees, the input corpus is permuted as follows:
1.",3 Synthetic experiments,[0],[0]
"Each token has a probability prelabel of being assigned a different label uniformly at random from the set of labels used in the corpus.
2.",3 Synthetic experiments,[0],[0]
"Each token has a probability preattach of being assigned a new head uniformly at random from the set of tokens not dominated by the token.
",3 Synthetic experiments,[0],[0]
"The second permutation process is dependent on the order the tokens are processed, and we consider the tokens in the post-order5 as dictated by the original tree.",3 Synthetic experiments,[0],[0]
This way tokens close to the root have a fair chance of having candidate heads if they are selected.,3 Synthetic experiments,[0],[0]
"A pre-order traversal would result in tokens close to the root having few options, and in particular if the root has a single child, that node has no possible new heads unless one of its children has been assigned the root as its new head first.",3 Synthetic experiments,[0],[0]
"For example in the trees in figure 2, assigning any other head than the root to the PRED nodes directly dominated by the root will result in invalid (cyclic and unconnected) dependency trees.",3 Synthetic experiments,[0],[0]
"Traversing the tokens in the linear order dictated by the sentence has similar issues for tokens close to the root and close to the start of the sentence.
",3 Synthetic experiments,[0],[0]
"For our first set of experiments, we set prelabel = preattach and evaluated the different agreement metrics for 10 evenly spaced p-values between 0.1 and 1.0.",3 Synthetic experiments,[0],[0]
"Initial exploration of the data showed that the mean follows the median very closely regardless of metric and perturbation level, and therefore we only report the mean scores across runs in this paper.",3 Synthetic experiments,[0],[0]
"The results of these experiments are shown in Figure 3, with the labelled attachment score6 (LAS) for comparison.
",3 Synthetic experiments,[0],[0]
"5That is, the child nodes of a node are all processed before the node itself.",3 Synthetic experiments,[0],[0]
"Nodes on the same level are traversed from left to right.
6The de facto standard parser evaluation metric in depen-
The αdiff metric is clearly extremely sensitive to noise, with p = 0.1 yielding mean αdiff = 15.8%, while αnorm is more lenient than both LAS and αplain, with mean αnorm = 14.5% at p = 1, quite high compared to LAS = 0.9%, αplain = −6.8% and αdiff = −246%.",3 Synthetic experiments,[0],[0]
"To further study the sensitivity of the metrics to the two kinds of noise, we performed an additional set of experiments, setting one p = 0 while varying the other over the same range as in the previous experiment, the results of which are shown in Figures 4 and 5.
",3 Synthetic experiments,[0],[0]
"The LAS curves are mostly unremarkable, with one exception: Mean LAS at preattach = 1 of Figure 5 is 23.9%, clearly much higher than we would expect if the trees were completely random.",3 Synthetic experiments,[0],[0]
"In comparison, mean LAS when only labels are perturbed is 4.1%, and since the sample space of trees of size n is clearly much larger than that of relabellings, a uniform random selection of tree would yield a LAS much closer to 0.",3 Synthetic experiments,[0],[0]
"This shows that our tree shuffling algorithm has a non-uniform distribution over the sample space.
",3 Synthetic experiments,[0],[0]
"While the behaviour of our alphas and LAS are relatively similar in Figure 3, Figures 4 and 5 show that they do in fact have important differences.",3 Synthetic experiments,[0],[0]
"Whereas LAS responds linearly to perturbation of both labels and structure, with its parabolic behaviour in Figure 3 being simply the product of these two linear responses, the α metrics respond differently to structural noise and label noise, with label disagreements being penalised less harshly
dency parsing: the percentage of tokens that receive the correct head and dependency relation.
than structural disagreements.",3 Synthetic experiments,[0],[0]
The reason for the strictness of the αdiff metric and the laxity of αnorm is the effects the modified distance functions have on the distribution of distances.,3 Synthetic experiments,[0],[0]
"The δdiff function causes an extreme shift of the distances towards 0; more than 30% of the sentence pairs have distance 0, 1, or 2, which causes Ddiffe to be extremely low and thus gives disproportionally large weight to nonzero distances in Ddiffo .",3 Synthetic experiments,[0],[0]
"On the other hand δnorm causes a rightward shift of the distances, which results in a highDnorme and thus individual disagreements having less weight.",3 Synthetic experiments,[0],[0]
"Synthetic experiments do not always fully reflect real-world behaviour, however.",4 Real-world corpora,[0],[0]
Therefore we will also evaluate our metrics on real-world interannotator agreement data sets.,4 Real-world corpora,[0],[0]
"In our evaluation, we will contrast labelled accuracy, the standard parser evaluation metric, and our three α metrics.",4 Real-world corpora,[0],[0]
"In particular, we are interested in the correlation (or lack thereof) between LAS and the alphas, and whether the results of our synthetic experiments correspond well with the results on realworld IAA sets.",4 Real-world corpora,[0],[0]
"Finally, we also evaluate the metric on both dependency and phrase structure data.",4 Real-world corpora,[0],[0]
We obtained7 data from four different corpora.,4.1 The corpora,[0],[0]
"Three of the data sets are dependency treebanks
7We contacted a number of treebank projects, among them the Penn Treebank and the Prague Dependency Treebank, but not all of them had data available.
",4.1 The corpora,[0],[0]
"(NDT, CDT, PCEDT) and one phrase structure treebank (SSD), and of the dependency treebanks the PCEDT contains semantic dependencies, while the other two have traditional syntactic dependencies.",4.1 The corpora,[0],[0]
"The number of annotators and sizes of the different data sets are summarised in Table 1.
",4.1 The corpora,[0],[0]
"NDT The Norwegian Dependency Treebank (Solberg et al., 2014) is a dependency treebank constructed at the National Library of Norway.",4.1 The corpora,[0],[0]
"The data studied in this work has previously been used by Skjærholt (2013) to study agreement, but using simple accuracy measures (UAS, LAS) rather than chance-corrected measures.",4.1 The corpora,[0],[0]
"The IAA data set is divided into three parts, corresponding to different parsers used to preprocess the data before annotation; what we term NDT 1 through 3 correspond to what Skjærholt (2013) labels Danish, Swedish and Norwegian, respectively.
",4.1 The corpora,[0],[0]
"CDT The Copenhagen Dependency Treebanks (Buch-Kromann et al., 2009; Buch-Kromann and Korzen, 2010) is a collection of parallel dependency treebanks, containing data from the Danish PAROLE corpus (Keson, 1998b; Keson, 1998a) in the original Danish and translated into English, Italian and Spanish.
",4.1 The corpora,[0],[0]
PCEDT The Prague Czech-English Dependency Treebank 2.0 Hajič,4.1 The corpora,[0],[0]
"et al. (2012) is a parallel corpus of English and Czech, consisting of English data from the Wall Street Journal Section of the Penn Treebank (Marcus et al., 1993) and
Czech translations of the English data.",4.1 The corpora,[0],[0]
"The syntactic annotations are layered and consist of an analytical layer similar to the annotations in most other dependency treebanks, and a more semantic tectogrammatical layer.
",4.1 The corpora,[0],[0]
"Our data set consists of a common set of analytical annotations shared by all the annotators, and the tectogrammatical analyses built on top of this common foundation.",4.1 The corpora,[0],[0]
"A distinguishing feature of the tectogrammatical analyses, vis a vis the other treebanks we are using, is that semantically empty words only take part in the analytical annotation layer and nodes are inserted at the tectogrammatical layer to represent covert elements of the sentence not present in the surface syntax of the analytical layer.",4.1 The corpora,[0],[0]
"Thus, inserting and deleting nodes is a central part of the task of tectogrammatical annotation, unlike the more surface-oriented annotation of our other treebanks, where the tokenisation is fixed before the text is annotated.
",4.1 The corpora,[0],[0]
"SSD The Star-Sem Data is a portion of the dataset released for the *SEM 2012 shared task (Morante and Blanco, 2012), parsed using the LinGO English Resource Grammar (ERG, Flickinger, 2000) and the resulting parse forest disambiguated based on discriminants.",4.1 The corpora,[0],[0]
"The ERG is an HPSG-based grammar, and as such its analyses are attribute-value matrices (AVMs); an AVM is not a tree but a directed acyclic graph however, and for this reason we compute agreement not on the AVM but the so-called derivation tree.",4.1 The corpora,[0],[0]
This tree describes the types of the lexical items in the sentence and the bottom-up ordering of rule applications used to produce the final analysis and can be handled by our procedure like any phrasestructure tree.,4.1 The corpora,[0],[0]
"To evaluate our corpora, we compute the three α variants described in the previous two sections, and compare these with labelled accuracy scores.
",4.2 Agreement results,[0],[0]
"When there are more than two annotators, we generalise the metric to be the average pairwise LAS for each sentence, weighted by the length of the sentence.",4.2 Agreement results,[0],[0]
"Let LAS(t1, t2) be the fraction of tokens with identical head and label in the trees t1 and t2; the pairwise labelled accuracy LASp(X) of a set of annotations X as described in section
1.2 is:
LASp(X) =",4.2 Agreement results,[0],[0]
"1∑ i |xi1| ∑ |xi1|Λ(Xi) |Xi|(|Xi|−1)/2 (3)
Λ(Xi) = |C|∑ c=1 |C|∑",4.2 Agreement results,[0],[0]
"c′=c+1 LAS(xic, xic′)
",4.2 Agreement results,[0],[0]
"This is equivalent to the traditional metric in the case where there are only two annotators.
",4.2 Agreement results,[0],[0]
"As our uncorrected metric for comparing two phrase structure trees we do not use the traditional bracket F1 as it does not generalise well to more than two annotators, but rather Jaccard similarity.",4.2 Agreement results,[0],[0]
"The Jaccard similarity of two sets A and B is the ratio of the size of their intersection to the size of their union: J(A,B) = |A∩B|/|A∪B|, and we use the Jaccard similarity of the sets of labelled bracketings of two trees as our uncorrected measure.",4.2 Agreement results,[0],[0]
"To compute the similarity for a complete set of annotations we use the mean pairwise Jaccard similarity weighted by sentence length; that is, the same procedure as in 3, but using Jaccard similarity rather than LAS.
",4.2 Agreement results,[0],[0]
"Since LAS assumes that both of the sentences compared have identical sets of tokens, we had to exclude a number of sentences from the LAS computation in the cases of the English and Italian CDT corpora, and especially the PCEDT.",4.2 Agreement results,[0],[0]
"The large number of sentences excluded in the PCEDT is due to the fact that in the tectogrammatical analysis of the PCEDT, inserting and deleting nodes is an important part of the annotation task.
",4.2 Agreement results,[0],[0]
"Looking at the results in Table 2, we observe
two things.",4.2 Agreement results,[0],[0]
"Most obvious, is the extremely large gap between the LAS and α metrics for the PCEDT data.",4.2 Agreement results,[0],[0]
"However, there is a more subtle point; the orderings of the corpora by the different metrics are not the same.",4.2 Agreement results,[0],[0]
"LAS order the corpora NDT 3, 2, 1, CDT da, en, it, es, PCEDT, whereas αdiff and αnorm gives the order NDT 2, 1, 3, PCEDT, CDT da, en, it, es, and αplain gives the same order as the other alphas but with CDT es and it changing places.",4.2 Agreement results,[0],[0]
"Furthermore, as the scatterplot in Figure 6 shows, there is a clear correlation between the α metrics and LAS, if we disregard the PCEDT results.
",4.2 Agreement results,[0],[0]
"The reason the PCEDT gets such low LAS is essentially the same as the reason many sentences had to be excluded from the computation in the first place; since inserting and deleting nodes is an integral part of the tectogrammatical annotation task, the assumption implicit in the LAS computation that sentences with the same number of nodes have the same nodes in the same order is obviously false, resulting in a very low LAS.
",4.2 Agreement results,[0],[0]
"The corpus that scores the highest for all three metrics is the SSD corpus; the reason for this is uncertain, as our corpora differ along many dimensions, but the fact that the annotation was done by professional linguists who are very familiar with the grammar used to parse the data is likely a contributing factor.",4.2 Agreement results,[0],[0]
"The difference between the α metrics and the Jaccard similarity is larger than the difference between α and LAS for our dependency corpora, however the two similarity metrics are not comparable, and it is well known that for phrase structures single disagreements such as a PP-attachment disagreement can result in multiple
disagreeing bracketings.",4.2 Agreement results,[0],[0]
The most important conclusion we draw from this work is the most appropriate agreement metric for syntactic annotation.,5 Conclusion,[0],[0]
"First of all, we disqualify the LAS metric, primarily due to the methodological inadequacies of using an uncorrected measure.",5 Conclusion,[0],[0]
"While our experiments did not reveal any serious shortcomings (unlike those of Mathet et al., 2012 who in the case of categorisation showed that for large p the uncorrected measure can be increasing), the methodological problems of uncorrected metrics makes us wary of LAS as an agreement metric.",5 Conclusion,[0],[0]
"Next, of the three α metrics, αplain is clearly the best; αdiff is extremely sensitive to even moderate amounts of disagreement, while αnorm is overly lenient.
",5 Conclusion,[0],[0]
"Looking solely at Figure 3, one might be led to believe that LAS and αplain are interchangeable, but this is not the case.",5 Conclusion,[0],[0]
"As shown by Figures 4 and 5, the paraboloid shape of the LAS curve in Figure 3 is simply the combination of the metric’s linear responses to both label and structural perturbations.",5 Conclusion,[0],[0]
"The behaviour of α on the other hand is more complex, with structural noise being penalised harder than perturbations of the labels.",5 Conclusion,[0],[0]
"Thus, the similarity of LAS and αplain is not at all assured when the amounts of structural and labelling disagreements differ.",5 Conclusion,[0],[0]
"Additionally, we consider this imbalanced weighting of structural and labelling disagreements a benefit, as structure is the larger part of syntactic annotation compared to the labelling of the dependencies/bracketings.",5 Conclusion,[0],[0]
"Finally our experiments show that α is a single metric that is applicable to both dependencies and phrase structure trees.
",5 Conclusion,[0],[0]
"Furthermore, α metrics are far more flexible than simple accuracy metrics.",5 Conclusion,[0],[0]
"The use of a distance function to define the metric means that more fine-grained distinctions can be made; for example, if the set of labels on the structures is highly structured, partial credit can be given for differing annotations that overlap.",5 Conclusion,[0],[0]
"For example, if different types of adverbials (temporal, negation, etc.) receive different relations, as is the case in the Swedish Talbanken05 (Nivre et al., 2006) corpus, confusion of different adverbial types can be given less weight than confusion between subject and object.",5 Conclusion,[0],[0]
"The α-based metrics are also far easier to apply to a more complex annotation task such
as the tectogrammatical annotation of the PCEDT.",5 Conclusion,[0],[0]
"In this task inserting and deleting nodes is an integral part of the annotation, and if two annotators insert or delete different nodes the all-or-nothing requirement of identical yield of the LAS metric makes it impossible as an evaluation metric in this setting.",5 Conclusion,[0],[0]
"In future work, we would like to investigate the use of other distance functions, in particular the use of approximate tree edit distance functions such as the pq-gram algorithm (Augsten et al., 2005).",5.1 Future work,[0],[0]
"For large data sets such as the PCEDT set used in this work, computing α with tree edit distance as the distance measure can take a very long time.8 This is due to the fact that α requires O(n2) comparisons to be made, each of which is O(n2) using our current approach.",5.1 Future work,[0],[0]
"The problem of directed graph edit distance is NP-hard, which means that to apply our method to HPSG analyses directly approximate algorithms are a requirement.
",5.1 Future work,[0],[0]
Another avenue for future work is improved synthetic experiments.,5.1 Future work,[0],[0]
"As we saw, our implementation of tree perturbations was biased towards trees similar in shape to the source tree, and an improved permutation algorithm may reveal interesting edge-case behaviour in the metrics.",5.1 Future work,[0],[0]
"A method for perturbing phrase structure trees would also be interesting, as this would allow us to repeat the synthetic experiments performed here using phrase structure corpora to compare the behaviour of the metrics on the two types of corpus.
",5.1 Future work,[0],[0]
"Finally, annotator modelling techniques like that presented in Passonneau and Carpenter (2013) has obvious advantages over agreement coefficients such as α.",5.1 Future work,[0],[0]
"These techniques are interpreted more easily than agreement coefficients, and they allow us to assess the quality of individual annotators, a crucial property in crowd-sourcing settings and something that’s impossible using agreement coefficients.",5.1 Future work,[0],[0]
"I would like to thank Jan Štěpánek at Charles University for data from the PCEDT and help with the conversion process, the CDT project for publishing their agreement data, Per Erik Solberg at
8The Python implementation used in this work, using NumPy and the PyPy compiler, took seven and a half hours compute a single α for the PCEDT data set on an Intel Core i7 2.9 GHz computer.",Acknowledgements,[0],[0]
"The program is single-threaded.
",Acknowledgements,[0],[0]
"the Norwegian National Library for data from the NDT, and Emily Bender at the University of Washington for the SSD data.",Acknowledgements,[0],[0]
"Following the works of Carletta (1996) and Artstein and Poesio (2008), there is an increasing consensus within the field that in order to properly gauge the reliability of an annotation effort, chance-corrected measures of inter-annotator agreement should be used.",abstractText,[0],[0]
"With this in mind, it is striking that virtually all evaluations of syntactic annotation efforts use uncorrected parser evaluation metrics such as bracket F1 (for phrase structure) and accuracy scores (for dependencies).",abstractText,[0],[0]
"In this work we present a chance-corrected metric based on Krippendorff’s α, adapted to the structure of syntactic annotations and applicable both to phrase structure and dependency annotation without any modifications.",abstractText,[0],[0]
"To evaluate our metric we first present a number of synthetic experiments to better control the sources of noise and gauge the metric’s responses, before finally contrasting the behaviour of our chance-corrected metric with that of uncorrected parser evaluation metrics on real corpora.1",abstractText,[0],[0]
A chance-corrected measure of inter-annotator agreement for syntax,title,[0],[0]
"The traditional view of generalization holds that a model with sufficient capacity (e.g. more parameters than training examples) will be able to “memorize” each example, overfitting the training set and yielding poor generalization to validation and test sets (Goodfellow et al., 2016).",1. Introduction,[0],[0]
Yet deep neural networks (DNNs) often achieve excellent generalization performance with massively over-parameterized models.,1. Introduction,[0],[0]
"This phenomenon is not well-understood.
",1. Introduction,[0],[0]
"*Equal contribution 1Montréal Institute for Learning Algorithms, Canada 2Université de Montréal, Canada 3Jagiellonian University, Krakow, Poland 4McGill University, Canada 5University of California, Berkeley, USA 6Polytechnique Montréal, Canada 7University of Bonn, Bonn, Germany 8CIFAR Fellow 9CIFAR Senior Fellow.",1. Introduction,[0],[0]
Correspondence to: <,1. Introduction,[0],[0]
"david.krueger@umontreal.ca>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"From a representation learning perspective, the generalization capabilities of DNNs are believed to stem from their incorporation of good generic priors (see, e.g., Bengio et al. (2009)).",1. Introduction,[0],[0]
Lin & Tegmark (2016) further suggest that the priors of deep learning are well suited to the physical world.,1. Introduction,[0],[0]
"But while the priors of deep learning may help explain why DNNs learn to efficiently represent complex real-world functions, they are not restrictive enough to rule out memorization.
",1. Introduction,[0],[0]
"On the contrary, deep nets are known to be universal approximators, capable of representing arbitrarily complex functions given sufficient capacity (Cybenko, 1989; Hornik et al., 1989).",1. Introduction,[0],[0]
"Furthermore, recent work has shown that the expressiveness of DNNs grows exponentially with depth (Montufar et al., 2014; Poole et al., 2016).",1. Introduction,[0],[0]
"These works, however, only examine the representational capacity, that is, the set of hypotheses a model is capable of expressing via some value of its parameters.
",1. Introduction,[0],[0]
"Because DNN optimization is not well-understood, it is unclear which of these hypotheses can actually be reached by gradient-based training (Bottou, 1998).",1. Introduction,[0],[0]
"In this sense, optimization and generalization are entwined in DNNs.",1. Introduction,[0],[0]
"To account for this, we formalize a notion of the effective capacity (EC) of a learning algorithm A (defined by specifying both the model and the training procedure, e.g.,“train the LeNet architecture (LeCun et al., 1998) for 100 epochs using stochastic gradient descent (SGD) with a learning rate of 0.01”) as the set of hypotheses which can be reached by applying that learning algorithm on some dataset.",1. Introduction,[0],[0]
"Formally, using set-builder notation:
EC(A) = {h | ∃D such that h ∈ A(D)} ,
where A(D) represents the set of hypotheses that is reachable by A on a dataset D1.
",1. Introduction,[0],[0]
One might suspect that DNNs effective capacity is sufficiently limited by gradient-based training and early stopping to resolve the apparent paradox between DNNs’ excellent generalization and their high representational capacity.,1. Introduction,[0],[0]
"However, the experiments of Zhang et al. (2017) suggest that this is not the case.",1. Introduction,[0],[0]
"They demonstrate that DNNs are
1 Since A can be stochastic, A(D) is a set.
able to fit pure noise without even needing substantially longer training time.",1. Introduction,[0],[0]
"Thus even the effective capacity of DNNs may be too large, from the point of view of traditional learning theory.
",1. Introduction,[0],[0]
"By demonstrating the ability of DNNs to “memorize” random noise, Zhang et al. (2017) also raise the question whether deep networks use similar memorization tactics on real datasets.",1. Introduction,[0],[0]
"Intuitively, a brute-force memorization approach to fitting data does not capitalize on patterns shared between training examples or features; the content of what is memorized is irrelevant.",1. Introduction,[0],[0]
"A paradigmatic example of a memorization algorithm is k-nearest neighbors (Fix & Hodges Jr, 1951).",1. Introduction,[0],[0]
"Like Zhang et al. (2017), we do not formally define memorization; rather, we investigate this intuitive notion of memorization by training DNNs to fit random data.
",1. Introduction,[0],[0]
"Main Contributions
We operationalize the definition of “memorization” as the behavior exhibited by DNNs trained on noise, and conduct a series of experiments that contrast the learning dynamics of DNNs on real vs. noise data.",1. Introduction,[0],[0]
"Thus, our analysis builds on the work of Zhang et al. (2017) and further investigates the role of memorization in DNNs.
",1. Introduction,[0],[0]
"Our findings are summarized as follows:
1.",1. Introduction,[0],[0]
There are qualitative differences in DNN optimization behavior on real data vs. noise.,1. Introduction,[0],[0]
"In other words, DNNs do not just memorize real data (Section 3).
2.",1. Introduction,[0],[0]
"DNNs learn simple patterns first, before memorizing (Section 4).",1. Introduction,[0],[0]
"In other words, DNN optimization is content-aware, taking advantage of patterns shared by multiple training examples.
",1. Introduction,[0],[0]
3.,1. Introduction,[0],[0]
Regularization techniques can differentially hinder memorization in DNNs while preserving their ability to learn about real data (Section 5).,1. Introduction,[0],[0]
"We perform experiments on MNIST (LeCun et al., 1998) and CIFAR10 (Krizhevsky et al.) datasets.",2. Experiment Details,[0],[0]
We investigate two classes of models: 2-layer multi-layer perceptrons (MLPs) with rectifier linear units (ReLUs) on MNIST and convolutional neural networks (CNNs) on CIFAR10.,2. Experiment Details,[0],[0]
"If not stated otherwise, the MLPs have 4096 hidden units per layer and are trained for 1000 epochs with SGD and learning rate 0.01.",2. Experiment Details,[0],[0]
"The CNNs are a small Alexnet-style CNN2 (as in Zhang et al. (2017)), and are trained using
2Input → Crop(2,2)",2. Experiment Details,[0],[0]
"→ Conv(200,5,5) → BN → ReLU → MaxPooling(3,3) → Conv(200,5,5) →",2. Experiment Details,[0],[0]
"BN→ ReLU→ MaxPool-
SGD with momentum=0.9 and learning rate of 0.01, scheduled to drop by half every 15 epochs.
",2. Experiment Details,[0],[0]
"Following Zhang et al. (2017), in many of our experiments we replace either (some portion of) the labels (with random labels), or the inputs (with i.i.d.",2. Experiment Details,[0],[0]
Gaussian noise matching the real dataset’s mean and variance) for some fraction of the training set.,2. Experiment Details,[0],[0]
"We use randX and randY to denote datasets with (100%, unless specified) noisy inputs and labels (respectively).",2. Experiment Details,[0],[0]
"Zhang et al. (2017) empirically demonstrated that DNNs are capable of fitting random data, which implicitly necessitates some high degree of memorization.",3. Qualitative Differences of DNNs Trained on Random vs. Real Data,[0],[0]
"In this section, we investigate whether DNNs employ similar memorization strategy when trained on real data.",3. Qualitative Differences of DNNs Trained on Random vs. Real Data,[0],[0]
"In particular, our experiments highlight some qualitative differences between DNNs trained on real data vs. random data, supporting the fact that DNNs do not use brute-force memorization to fit real datasets.",3. Qualitative Differences of DNNs Trained on Random vs. Real Data,[0],[0]
A brute-force memorization approach to fitting data should apply equally well to different training examples.,3.1. Easy Examples as Evidence of Patterns in Real Data,[0],[0]
"However, if a network is learning based on patterns in the data, some examples may fit these patterns better than others.",3.1. Easy Examples as Evidence of Patterns in Real Data,[0],[0]
"We show that such “easy examples” (as well as correspondingly “hard examples”) are common in real, but not in random, datasets.",3.1. Easy Examples as Evidence of Patterns in Real Data,[0],[0]
"Specifically, for each setting (real data, randX, randY), we train an MLP for a single epoch starting from 100 different random initializations and shufflings of the data.",3.1. Easy Examples as Evidence of Patterns in Real Data,[0],[0]
"We find that, for real data, many examples are consistently classified (in)correctly after a single epoch, suggesting that different examples are significantly easier or harder in this sense.",3.1. Easy Examples as Evidence of Patterns in Real Data,[0],[0]
"For noise data, the difference between examples is much less, indicating that these examples are fit (more) independently.",3.1. Easy Examples as Evidence of Patterns in Real Data,[0],[0]
"Results are presented in Figure 1.
",3.1. Easy Examples as Evidence of Patterns in Real Data,[0],[0]
"For randX, apparent differences in difficulty are well modeled as random Binomial noise.",3.1. Easy Examples as Evidence of Patterns in Real Data,[0],[0]
"For randY, this is not the case, indicating some use of shared patterns.",3.1. Easy Examples as Evidence of Patterns in Real Data,[0],[0]
"Visualizing first-level features learned by a CNN supports this hypothesis (Figure 2).
ing(3,3)",3.1. Easy Examples as Evidence of Patterns in Real Data,[0],[0]
→ Dense(384) → BN → ReLU → Dense(192) → BN → ReLU → Dense(#classes),3.1. Easy Examples as Evidence of Patterns in Real Data,[0],[0]
→ Softmax.,3.1. Easy Examples as Evidence of Patterns in Real Data,[0],[0]
Here,3.1. Easy Examples as Evidence of Patterns in Real Data,[0],[0]
"Crop(. , .)",3.1. Easy Examples as Evidence of Patterns in Real Data,[0],[0]
crops height and width from both sides with respective values.,3.1. Easy Examples as Evidence of Patterns in Real Data,[0],[0]
"To further investigate the difference between real and fully random inputs, we propose a proxy measure of memorization via gradients.",3.2. Loss-Sensitivity in Real vs. Random Data,[0],[0]
"Since we cannot measure quantitatively how much each training sample x is memorized, we instead measure the effect of each sample on the average loss.",3.2. Loss-Sensitivity in Real vs. Random Data,[0],[0]
"That is, we measure the norm of the loss gradient with respect to a previous example x after t SGD updates.",3.2. Loss-Sensitivity in Real vs. Random Data,[0],[0]
"Let Lt be the loss after t updates; then the sensitivity measure is given by
gtx = ‖∂Lt/∂x‖1 .
",3.2. Loss-Sensitivity in Real vs. Random Data,[0],[0]
The parameter update from training on x influences all future Lt indirectly by changing the subsequent updates on different training examples.,3.2. Loss-Sensitivity in Real vs. Random Data,[0],[0]
"We denote the average over gtx after T steps as ḡx, and refer to it as loss-sensitivity.",3.2. Loss-Sensitivity in Real vs. Random Data,[0],[0]
"Note that we only report `1-norm results, but that results stay very similar using `2-norm and infinity norm.
",3.2. Loss-Sensitivity in Real vs. Random Data,[0],[0]
"We compute gtx by unrolling t SGD steps and applying backpropagation over the unrolled computation graph, as done by Maclaurin et al. (2015).",3.2. Loss-Sensitivity in Real vs. Random Data,[0],[0]
"Unlike Maclaurin et al. (2015), we only use this procedure to compute gtx, and do not modify the training procedure in any way.
",3.2. Loss-Sensitivity in Real vs. Random Data,[0],[0]
"We find that for real data, only a subset of the training set has high ḡx, while for random data, ḡx is high for virtually all examples.",3.2. Loss-Sensitivity in Real vs. Random Data,[0],[0]
"We also find a different behavior when each example is given a unique class; in this scenario, the network has to learn to identify each example uniquely, yet still behaves differently when given real data than when given random data as input.
",3.2. Loss-Sensitivity in Real vs. Random Data,[0],[0]
We visualize (Figure 3) the spread of ḡx as training progresses by computing the Gini coefficient over x’s.,3.2. Loss-Sensitivity in Real vs. Random Data,[0],[0]
"The Gini coefficient (Gini, 1913) is a measure of the inequality among values of a frequency distribution; a coefficient of 0 means exact equality (i.e., all values are the same), while a coefficient of 1 means maximal inequality among values.",3.2. Loss-Sensitivity in Real vs. Random Data,[0],[0]
"We observe that, when trained on real data, the network has a high ḡx for a few examples, while on random data the network is sensitive to most examples.",3.2. Loss-Sensitivity in Real vs. Random Data,[0],[0]
"The difference between the random data scenario, where we know the neural network needs to do memorization, and the real data scenario, where we’re trying to understand what happens, leads us to believe that this measure is indeed sensitive to memorization.",3.2. Loss-Sensitivity in Real vs. Random Data,[0],[0]
"Additionally, these results suggest that when being trained on real data, the neural network probably does not memorize, or at least not in the same manner it needs to for random data.
",3.2. Loss-Sensitivity in Real vs. Random Data,[0],[0]
"In addition to the different behaviors for real and random data described above, we also consider a class specific losssensitivity: ḡi,j = E(x,y)1/",3.2. Loss-Sensitivity in Real vs. Random Data,[0],[0]
"T ∑T t |∂Lt(y = i)/∂xy=j |, where Lt(y = i) is the term in the crossentropy sum corresponding to class i.",3.2. Loss-Sensitivity in Real vs. Random Data,[0],[0]
We observe that the loss-sensitivity w.r.t.,3.2. Loss-Sensitivity in Real vs. Random Data,[0],[0]
"class i for training examples of class j is higher when i = j, but more spread out for real data (see Figure 4).",3.2. Loss-Sensitivity in Real vs. Random Data,[0],[0]
"An interpretation of this is that for real data there are more interesting cross-category patterns that can be learned than for random data.
",3.2. Loss-Sensitivity in Real vs. Random Data,[0],[0]
Figure 3 and 4 were obtained by training a fully-connected network with 2 layers of 16 units on 1000 downscaled 14× 14 MNIST digits using SGD.,3.2. Loss-Sensitivity in Real vs. Random Data,[0],[0]
"In this section, we investigate the impact of capacity and effective capacity on learning of datasets having different amounts of random input data or random labels.",3.3. Capacity and Effective Capacity,[0],[0]
"In a first experiment, we study how overall model capacity impacts the validation performances for datasets with different amounts of noise.",3.3.1. EFFECTS OF CAPACITY AND DATASET SIZE ON VALIDATION PERFORMANCES,[0],[0]
"On MNIST, we found that the optimal validation performance requires a higher capacity model in the presence of noise examples (see Figure 5).",3.3.1. EFFECTS OF CAPACITY AND DATASET SIZE ON VALIDATION PERFORMANCES,[0],[0]
"This trend was consistent for noise inputs on CIFAR10, but
we did not notice any relationship between capacity and validation performance on random labels on CIFAR10.
",3.3.1. EFFECTS OF CAPACITY AND DATASET SIZE ON VALIDATION PERFORMANCES,[0],[0]
"This result contradicts the intuitions of traditional learning theory, which suggest that capacity should be restricted, in order to enforce the learning of (only) the most regular patterns.",3.3.1. EFFECTS OF CAPACITY AND DATASET SIZE ON VALIDATION PERFORMANCES,[0],[0]
"Given that DNNs can perfectly fit the training set in any case, we hypothesize that that higher capacity allows the network to fit the noise examples in a way that does not interfere with learning the real data.",3.3.1. EFFECTS OF CAPACITY AND DATASET SIZE ON VALIDATION PERFORMANCES,[0],[0]
"In contrast, if we were simply to remove noise examples, yielding a smaller (clean) dataset, a lower capacity model would be able to achieve optimal performance.",3.3.1. EFFECTS OF CAPACITY AND DATASET SIZE ON VALIDATION PERFORMANCES,[0],[0]
"Our next experiment measures time-to-convergence, i.e. how many epochs it takes to reach 100% training accuracy.",3.3.2. EFFECTS OF CAPACITY AND DATASET SIZE ON TRAINING TIME,[0],[0]
"Reducing the capacity or increasing the size of the dataset slows down training as well for real as for noise
data3.",3.3.2. EFFECTS OF CAPACITY AND DATASET SIZE ON TRAINING TIME,[0],[0]
"However, the effect is more severe for datasets containing noise, as our experiments in this section show (see Figure 6).
",3.3.2. EFFECTS OF CAPACITY AND DATASET SIZE ON TRAINING TIME,[0],[0]
Effective capacity of a DNN can be increased by increasing the representational capacity (e.g. adding more hidden units) or training for longer.,3.3.2. EFFECTS OF CAPACITY AND DATASET SIZE ON TRAINING TIME,[0],[0]
"Thus, increasing the number of hidden units decreases the number of training iterations needed to fit the data, up to some limit.",3.3.2. EFFECTS OF CAPACITY AND DATASET SIZE ON TRAINING TIME,[0],[0]
"We observe stronger diminishing returns from increasing representational capacity for real data, indicating that this limit is lower, and a smaller representational capacity is sufficient, for real datasets.
",3.3.2. EFFECTS OF CAPACITY AND DATASET SIZE ON TRAINING TIME,[0],[0]
Increasing the number of examples (keeping representational capacity fixed) also increases the time needed to memorize the training set.,3.3.2. EFFECTS OF CAPACITY AND DATASET SIZE ON TRAINING TIME,[0],[0]
"In the limit, the representational capacity is simply insufficient, and memorization is not feasible.",3.3.2. EFFECTS OF CAPACITY AND DATASET SIZE ON TRAINING TIME,[0],[0]
"On the other hand, when the relationship between inputs and outputs is meaningful, new examples sim-
3 Regularization can also increase time-to-convergence; see section 5.
ply give more (possibly redundant) clues as to what the input → output mapping is.",3.3.2. EFFECTS OF CAPACITY AND DATASET SIZE ON TRAINING TIME,[0],[0]
"Thus, in the limit, an idealized learner should be able to predict unseen examples perfectly, absent noise.",3.3.2. EFFECTS OF CAPACITY AND DATASET SIZE ON TRAINING TIME,[0],[0]
"Our experiments demonstrate that time-toconvergence is not only longer on noise data (as noted by Zhang et al. (2017)), but also, increases substantially as a function of dataset size, relative to real data.",3.3.2. EFFECTS OF CAPACITY AND DATASET SIZE ON TRAINING TIME,[0],[0]
"Following the reasoning above, this suggests that our networks are learning to extract patterns in the data, rather than memorizing.",3.3.2. EFFECTS OF CAPACITY AND DATASET SIZE ON TRAINING TIME,[0],[0]
This section aims at studying how the complexity of the hypotheses learned by DNNs evolve during training for real data vs. noise data.,4. DNNs Learn Patterns First,[0],[0]
"To achieve this goal, we build on the intuition that the number of different decision regions into which an input space is partitioned reflects the complexity of the learned hypothesis (Sokolic et al., 2016).",4. DNNs Learn Patterns First,[0],[0]
"This notion is similar in spirit to the degree to which a function can scatter random labels: a higher density of decision boundaries in the data space allows more samples to be scattered.
",4. DNNs Learn Patterns First,[0],[0]
"Therefore, we estimate the complexity by measuring how densely points on the data manifold are present around the model’s decision boundaries.",4. DNNs Learn Patterns First,[0],[0]
"Intuitively, if we were to randomly sample points from the data distribution, a smaller fraction of points in the proximity of a decision boundary suggests that the learned hypothesis is simpler.",4. DNNs Learn Patterns First,[0],[0]
"Here we introduce the notion of a critical sample, which we use to estimate the density of decision boundaries as discussed above.",4.1. Critical Sample Ratio (CSR),[0],[0]
"Critical samples are a subset of a dataset such that for each such sample x, there exists at least one adversarial example x̂ in the proximity of x. Specifically, consider a classification network’s output vector f(x) =
(f1(x), . . .",4.1. Critical Sample Ratio (CSR),[0],[0]
", fk(x)) ∈ Rk for a given input sample x ∈",4.1. Critical Sample Ratio (CSR),[0],[0]
Rn from the data manifold.,4.1. Critical Sample Ratio (CSR),[0],[0]
"Formally we call a dataset sample x a critical sample if there exists a point x̂ such that,
arg max i fi(x) 6=",4.1. Critical Sample Ratio (CSR),[0],[0]
"arg max j fj(x̂) (1)
s.t.",4.1. Critical Sample Ratio (CSR),[0],[0]
‖x− x̂‖∞ ≤,4.1. Critical Sample Ratio (CSR),[0],[0]
"r
where r is a fixed box size.",4.1. Critical Sample Ratio (CSR),[0],[0]
"As in recent work on adversarial examples (Kurakin et al., 2016)",4.1. Critical Sample Ratio (CSR),[0],[0]
"the above definition depends only on the predicted label arg maxi fi(x) of x, and not the true label (as in earlier work on adversarial examples, such as Szegedy et al. (2013); Goodfellow et al. (2014)).
",4.1. Critical Sample Ratio (CSR),[0],[0]
"Following the above argument relating complexity to decision boundaries, a higher number of critical samples indicates a more complex hypothesis.",4.1. Critical Sample Ratio (CSR),[0],[0]
"Thus, we measure complexity as the critical sample ratio (CSR), that is, the fraction of data-points in a set |D| for which we can find a critical sample: #critical samples|D| .
",4.1. Critical Sample Ratio (CSR),[0],[0]
"To identify whether a given data point x is a critical samples, we search for an adversarial sample x̂ within a box of radius r. To perform this search, we propose using Langevin dynamics applied to the fast gradient sign method (FGSM, Goodfellow et al. (2014)) as shown in algorithm 14.",4.1. Critical Sample Ratio (CSR),[0],[0]
We refer to this method as Langevin adversarial sample search (LASS).,4.1. Critical Sample Ratio (CSR),[0],[0]
"While the FGSM search algorithm can get stuck at a points with zero gradient, LASS explores the box more thoroughly.",4.1. Critical Sample Ratio (CSR),[0],[0]
"Specifically, a problem with first order gradient search methods (like FGSM) is that there might exist training points where the gradient is 0, but with a large 2nd derivative corresponding to a large change in prediction in the neighborhood.",4.1. Critical Sample Ratio (CSR),[0],[0]
"The noise added by the LASS algorithm during the search enables escaping from such points.
",4.1. Critical Sample Ratio (CSR),[0],[0]
"4In our experiments, we set α = 0.25, β = 0.2 and η is samples from standard normal distribution.
",4.1. Critical Sample Ratio (CSR),[0],[0]
Algorithm 1 Langevin Adversarial Sample Search (LASS),4.1. Critical Sample Ratio (CSR),[0],[0]
Require: x,4.1. Critical Sample Ratio (CSR),[0],[0]
"∈ Rn, α, β, r, noise process",4.1. Critical Sample Ratio (CSR),[0],[0]
"η Ensure: x̂
1: converged = FALSE 2: x̃← x; x̂← ∅ 3: while not converged or max iter reached do 4: ∆ = α · sign(∂fk(x)∂x ) +",4.1. Critical Sample Ratio (CSR),[0],[0]
β · η 5: x̃← x̃ + ∆ 6: for i ∈,4.1. Critical Sample Ratio (CSR),[0],[0]
"[n] do
7: x̃i ← {
xi + r · sign(x̃i",4.1. Critical Sample Ratio (CSR),[0],[0]
− xi) if |x̃i,4.1. Critical Sample Ratio (CSR),[0],[0]
"− xi| > r x̃i otherwise
8: end for 9: if arg maxi f(x) 6=",4.1. Critical Sample Ratio (CSR),[0],[0]
"arg maxi f(x̃) then
10: converged = TRUE 11: x̂← x̃ 12: end if 13: end while",4.1. Critical Sample Ratio (CSR),[0],[0]
"We now show that the number of critical samples is much higher for a deep network (specifically, a CNN) trained on noise data compared with real data.",4.2. Critical Samples Throughout Training,[0],[0]
"To do so, we mea-
sure the number of critical samples in the validation set5, throughout training6.",4.2. Critical Samples Throughout Training,[0],[0]
Results are shown in Figure 9.,4.2. Critical Samples Throughout Training,[0],[0]
"A
5 We also measure the number of critical samples in the training sets.",4.2. Critical Samples Throughout Training,[0],[0]
"Since we train our models using log loss, training points are pushed away from the decision boundary even after the network learns to classify them correctly.",4.2. Critical Samples Throughout Training,[0],[0]
"This leads to an initial rise and then fall of the number of critical samples in the training sets.
",4.2. Critical Samples Throughout Training,[0],[0]
"6We use a box size of 0.3, which is small enough in a 0-255 pixel scale to be unnoticeable by a human evaluator.",4.2. Critical Samples Throughout Training,[0],[0]
"Different values for r were tested but did not change results qualitatively
higher number of critical samples for models trained on noise data compared with those trained on real data suggests that the learned decision surface is more complex for noise data (randX and randY).",4.2. Critical Samples Throughout Training,[0],[0]
We also observe that the CSR increases gradually with increasing number of epochs and then stabilizes.,4.2. Critical Samples Throughout Training,[0],[0]
"This suggests that the networks learn gradually more complex hypotheses during training for all three datasets.
",4.2. Critical Samples Throughout Training,[0],[0]
"In our next experiment, we evaluate the performance and critical sample ratio of datasets with 20% to 80% of the training data replaced with either input or label noise.",4.2. Critical Samples Throughout Training,[0],[0]
"Results for MNIST and CIFAR-10 are shown in Figures 7 and 8, respectively.",4.2. Critical Samples Throughout Training,[0],[0]
"For both randX and randY datasets, the CSR is higher for noisier datasets, reflecting the higher level of complexity of the learned prediction function.",4.2. Critical Samples Throughout Training,[0],[0]
"The final and maximum validation accuracies are also both lower for noisier datasets, indicating that the noise examples interfere somewhat with the networks ability to learn about the real data.
",4.2. Critical Samples Throughout Training,[0],[0]
"More significantly, for randY datasets (Figures 7(b) and 8(b)), the network achieves maximum accuracy on the validation set before achieving high accuracy on the training set.",4.2. Critical Samples Throughout Training,[0],[0]
Thus the model first learns the simple and general patterns of the real data before fitting the noise (which results in decreasing validation accuracy).,4.2. Critical Samples Throughout Training,[0],[0]
"Furthermore, as the model moves from fitting real data to fitting noise, the CSR greatly increases, indicating the need for more complex hypotheses to explain the noise.",4.2. Critical Samples Throughout Training,[0],[0]
"Combining this result with our results from Section 3.1, we conclude that real data examples are easier to fit than noise.",4.2. Critical Samples Throughout Training,[0],[0]
"Here we demonstrate the ability of regularization to degrade training performance on data with random labels, while maintaining generalization performance on real data.",5. Effect of Regularization on Learning,[0],[0]
"Zhang et al. (2017) argue that explicit regularizations are not the main explanation of good generalization performance, rather SGD based optimization is largely responsible for it.",5. Effect of Regularization on Learning,[0],[0]
"Our findings extend their claim and indicate that explicit regularizations can substantially limit the speed of memorization of noise data without significantly impacting learning on real data.
",5. Effect of Regularization on Learning,[0],[0]
"We compare the performance of CNNs trained on CIFAR10 and randY with the following regularizers: dropout (with dropout rates in range 0-0.9), input dropout (range 0- 0.9), input Gaussian noise (with standard deviation in range 0-5), hidden Gaussian noise (range 0-0.3), weight decay (range 0-1) and additionally dropout with adversarial training (with weighting factor in range 0.2-0.7 and dropout in
and lead to the same conclusions
rate range 0.03-0.5).7",5. Effect of Regularization on Learning,[0],[0]
"We train a separate model for every combination of dataset, regularization technique, and regularization parameter.
",5. Effect of Regularization on Learning,[0],[0]
The results are summarized in Figure 10.,5. Effect of Regularization on Learning,[0],[0]
"For each combination of dataset and regularization technique, the final training accuracy on randY (x-axis) is plotted against the best validation accuracy on CIFAR-10 from amongst the models trained with different regularization parameters (yaxis).",5. Effect of Regularization on Learning,[0],[0]
"Flat curves indicate that the corresponding regularization technique can reduce memorization when applied on random labeling, while resulting in the same validation accuracy on the clean validation set.",5. Effect of Regularization on Learning,[0],[0]
Our results show that different regularizers target memorization behavior to different extent – dropout being the most effective.,5. Effect of Regularization on Learning,[0],[0]
"We find that dropout, especially coupled with adversarial training, is best at hindering memorization without reducing the model’s ability to learn.",5. Effect of Regularization on Learning,[0],[0]
Figure 11 additionally shows this effect for selected experiments (i.e. selected hyperparameter values) in terms of train loss.,5. Effect of Regularization on Learning,[0],[0]
Our work builds on the experiments and challenges the interpretations of Zhang et al. (2017).,6. Related Work,[0],[0]
We make heavy use of their methodology of studying DNN training in the context of noise datasets.,6. Related Work,[0],[0]
"Zhang et al. (2017) show that DNNs can perfectly fit noise and thus that their generalization ability cannot be explained through traditional statistical learning theory (e.g., see (Vapnik & Vapnik, 1998; Bartlett et al., 2005)).",6. Related Work,[0],[0]
"We agree with this finding, but show in addition that the degree of memorization and generalization in DNNs depends not only on the architecture and training
7We perform adversarial training using critical samples found by LASS algorithm with default parameters.
procedure (including explicit regularizations), but also on the training data itself 8.
",6. Related Work,[0],[0]
Another direction we investigate is the relationship between regularization and memorization.,6. Related Work,[0],[0]
Zhang et al. (2017) argue that explicit and implicit regularizers (including SGD) might not explain or limit shattering of random data.,6. Related Work,[0],[0]
In this work we show that regularizers (especially dropout) do control the speed at which DNNs memorize.,6. Related Work,[0],[0]
"This is interesting since dropout is also known to prevent catastrophic forgetting (Goodfellow et al., 2013) and thus in general it seems to help DNNs retain patterns.
",6. Related Work,[0],[0]
"A number of arguments support the idea that SGD-based learning imparts a regularization effect, especially with a small batch size (Wilson & Martinez, 2003) or a small number of epochs (Hardt et al., 2015).",6. Related Work,[0],[0]
Previous work also suggests that SGD prioritizes the learning of simple hypothesis first.,6. Related Work,[0],[0]
"Sjoberg et al. (1995) showed that, for linear models, SGD first learns models with small `2 parameter norm.",6. Related Work,[0],[0]
"More generally, the efficacy of early stopping shows that SGD first learns simpler models (Yao et al., 2007).",6. Related Work,[0],[0]
"We extend these results, showing that DNNs trained with SGD learn patterns before memorizing, even in the presence of noise examples.
",6. Related Work,[0],[0]
Various previous works have analyzed explanations for the generalization power of DNNs.,6. Related Work,[0],[0]
"Montavon et al. (2011) use kernel methods to analyze the complexity of deep learning architectures, and find that network priors (e.g. implemented by the network structure of a CNN or MLP) control the speed of learning at each layer.",6. Related Work,[0],[0]
"Neyshabur et al. (2014) note that the number of parameters does not control the effective capacity of a DNN, and that the reason for DNNs’ generalization is unknown.",6. Related Work,[0],[0]
We supplement this result by showing how the impact of representational capacity changes with varying noise levels.,6. Related Work,[0],[0]
"While exploring
8We conclude the latter part based on experimental findings in sections 3 and 4.2
the effect of noise samples on learning dynamics has a long tradition (Bishop, 1995; An, 1996), we are the first to examine relationships between the fraction of noise samples and other attributes of the learning algorithm, namely: capacity, training time and dataset size.
",6. Related Work,[0],[0]
"Multiple techniques for analyzing the training of DNNs have been proposed before, including looking at generalization error, trajectory length evolution (Raghu et al., 2016), analyzing Jacobians associated to different layers (Wang; Saxe et al., 2013), or the shape of the loss minima found by SGD (Im et al., 2016; Chaudhari et al., 2016; Keskar et al., 2016).",6. Related Work,[0],[0]
"Instead of measuring the sharpness of the loss for the learned hypothesis, we investigate the complexity of the learned hypothesis throughout training and across different datasets and regularizers, as measured by the critical sample ratio.",6. Related Work,[0],[0]
"Critical samples refer to real data-points that have adversarial examples (Szegedy et al., 2013; Goodfellow et al., 2014) nearby.",6. Related Work,[0],[0]
Adversarial examples originally referred to imperceptibly perturbed datapoints that are confidently misclassified.,6. Related Work,[0],[0]
"(Miyato et al., 2015) define virtual adversarial examples via changes in the predictive distribution instead, thus extending the definition to unlabeled data-points.",6. Related Work,[0],[0]
"Kurakin et al. (2016) recommend using this definition when training on adversarial examples, and it is the definition we use.
",6. Related Work,[0],[0]
Two contemporary works perform in-depth explorations of topics related to our work.,6. Related Work,[0],[0]
"Bojanowski & Joulin (2017) show that predicting random noise targets can yield state of the art results in unsupervised learning, corroborating our findings in Section 3.1, especially Figure 2.",6. Related Work,[0],[0]
"Koh & Liang (2017) use influence functions to measure the impact on parameter changes during training, as in our Section 3.2.",6. Related Work,[0],[0]
"They explore several promising applications for this technique, including generation of adversarial training examples.",6. Related Work,[0],[0]
"Our empirical exploration demonstrates qualitative differences in DNN optimization on noise vs. real data, all of which support the claim that DNNs trained with SGDvariants first use patterns, not brute force memorization, to fit real data.",7. Conclusion,[0],[0]
"However, since DNNs have the demonstrated ability to fit noise, it is unclear why they find generalizable solutions on real data; we believe that the deep learning priors including distributed and hierarchical representations likely play an important role.",7. Conclusion,[0],[0]
"Our analysis suggests that memorization and generalization in DNNs depend on network architecture and optimization procedure, but also on the data itself.",7. Conclusion,[0],[0]
"We hope to encourage future research on how properties of datasets influence the behavior of deep learning algorithms, and suggest a data-dependent understanding of DNN capacity as a research goal.",7. Conclusion,[0],[0]
"We thank Akram Erraqabi, Jason Jo and Ian Goodfellow for helpful discussions.",ACKNOWLEDGMENTS,[0],[0]
SJ was supported by Grant No.,ACKNOWLEDGMENTS,[0],[0]
"DI 2014/016644 from Ministry of Science and Higher Education, Poland.",ACKNOWLEDGMENTS,[0],[0]
"DA was supported by IVADO, CIFAR and NSERC.",ACKNOWLEDGMENTS,[0],[0]
EB was financially supported by the Samsung Advanced Institute of Technology (SAIT).,ACKNOWLEDGMENTS,[0],[0]
MSK and SJ were supported by MILA during the course of this work.,ACKNOWLEDGMENTS,[0],[0]
We acknowledge the computing resources provided by ComputeCanada and CalculQuebec.,ACKNOWLEDGMENTS,[0],[0]
"Experiments were carried out using Theano (Theano Development Team, 2016) and Keras (Chollet et al., 2015).",ACKNOWLEDGMENTS,[0],[0]
"We examine the role of memorization in deep learning, drawing connections to capacity, generalization, and adversarial robustness.",abstractText,[0],[0]
"While deep networks are capable of memorizing noise data, our results suggest that they tend to prioritize learning simple patterns first.",abstractText,[0],[0]
"In our experiments, we expose qualitative differences in gradient-based optimization of deep neural networks (DNNs) on noise vs. real data.",abstractText,[0],[0]
"We also demonstrate that for appropriately tuned explicit regularization (e.g., dropout) we can degrade DNN training performance on noise datasets without compromising generalization on real data.",abstractText,[0],[0]
Our analysis suggests that the notions of effective capacity which are dataset independent are unlikely to explain the generalization performance of deep networks when trained with gradient based methods because training data itself plays an important role in determining the degree of memorization.,abstractText,[0],[0]
A Closer Look at Memorization in Deep Networks,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 746–751 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
746",text,[0],[0]
"Enabling machines to understand natural language text is arguably the ultimate goal of natural language processing, and the task of machine reading comprehension is an intermediate step towards this ultimate goal (Richardson et al., 2013; Hermann et al., 2015; Hill et al., 2015; Rajpurkar et al., 2016; Nguyen et al., 2016).",1 Introduction,[0],[0]
"Recently, Lai et al. (2017) released a new multi-choice machine comprehension dataset called RACE that was extracted from middle and high school English examinations in China.",1 Introduction,[0],[0]
Figure 1 shows an example passage and two related questions from RACE.,1 Introduction,[0],[0]
"The key difference between RACE and previously released machine comprehension datasets (e.g., the CNN/Daily Mail dataset (Hermann et al., 2015) and SQuAD (Rajpurkar et al., 2016))",1 Introduction,[0],[0]
"is that the answers in RACE often cannot be directly extracted from the given passages, as illustrated by the two example questions (Q1 & Q2) in Figure 1.",1 Introduction,[0],[0]
"Thus, answering these questions is more challenging and requires more inferences.
",1 Introduction,[0],[0]
"Previous approaches to machine comprehension are usually based on pairwise sequence matching, where either the passage is matched against the sequence that concatenates both the
question and a candidate answer (Yin et al., 2016), or the passage is matched against the question alone followed by a second step of selecting an answer using the matching result of the first step (Lai et al., 2017; Zhou et al., 2018).",1 Introduction,[0],[0]
"However, these approaches may not be suitable for multi-choice reading comprehension since questions and answers are often equally important.",1 Introduction,[0],[0]
"Matching the passage only against the question may not be meaningful and may lead to loss of information from the original passage, as we can see from the first example question in Figure 1.",1 Introduction,[0],[0]
"On the other hand, concatenating the question and the answer into a single sequence for matching may not work, either, due to the loss of interaction information between a question and an answer.",1 Introduction,[0],[0]
"As illustrated by Q2 in Figure 1, the model may need to recognize what “he” and “it” in candidate answer (c) refer to in the question, in order to select (c) as the correct answer.",1 Introduction,[0],[0]
"This observation of the RACE dataset shows that we face a new challenge of matching sequence triplets (i.e., passage, question and answer) instead of pairwise matching.
",1 Introduction,[0],[0]
"In this paper, we propose a new model to match a question-answer pair to a given passage.",1 Introduction,[0],[0]
Our comatching approach explicitly treats the question and the candidate answer as two sequences and jointly matches them to the given passage.,1 Introduction,[0],[0]
"Specifically, for each position in the passage, we compute two attention-weighted vectors, where one is from the question and the other from the candidate answer.",1 Introduction,[0],[0]
"Then, two matching representations are constructed: the first one matches the passage with the question while the second one matches the passage with the candidate answer.",1 Introduction,[0],[0]
These two newly constructed matching representations together form a co-matching state.,1 Introduction,[0],[0]
"Intuitively, it encodes the locational information of the question and the candidate answer matched to a specific context of the passage.",1 Introduction,[0],[0]
"Finally, we apply a hierar-
chical LSTM (Tang et al., 2015) over the sequence of co-matching states at different positions of the passage.",1 Introduction,[0],[0]
Information is aggregated from wordlevel to sentence-level and then from sentencelevel to document-level.,1 Introduction,[0],[0]
"In this way, our model can better deal with the questions that require evidence scattered in different sentences in the passage.",1 Introduction,[0],[0]
Our model improves the state-of-the-art model by 3 percentage on the RACE dataset.,1 Introduction,[0],[0]
Our code will be released under https://github.,1 Introduction,[0],[0]
com/shuohangwang/comatch.,1 Introduction,[0],[0]
"For the task of multi-choice reading comprehension, the machine is given a passage, a question and a set of candidate answers.",2 Model,[0],[0]
The goal is to select the correct answer from the candidates.,2 Model,[0],[0]
"Let us use P ∈ Rd×P , Q ∈ Rd×Q",2 Model,[0],[0]
"and A ∈ Rd×A to represent the passage, the question and a candidate answer, respectively, where each word in each sequence is represented by an embedding vector.",2 Model,[0],[0]
"d is the dimensionality of the embeddings, and P , Q, and A are the lengths of these sequences.
",2 Model,[0],[0]
Overall our model works as follows.,2 Model,[0],[0]
"For each candidate answer, our model constructs a vector that represents the matching of P with both Q and A.",2 Model,[0],[0]
The vectors of all candidate answers are then used for answer selection.,2 Model,[0],[0]
"Because we simultaneously match P with Q and A, we call this a comatching model.",2 Model,[0],[0]
In Section 2.1 we introduce the word-level co-matching mechanism.,2 Model,[0],[0]
"Then in Section 2.2 we introduce a hierarchical aggregation
process.",2 Model,[0],[0]
Finally in Section 2.3 we present the objective function.,2 Model,[0],[0]
An overview of our co-matching model is shown in Figure 2.,2 Model,[0],[0]
The co-matching part of our model aims to match the passage with the question and the candidate answer at the word-level.,2.1 Co-matching,[0],[0]
"Inspired by some previous work (Wang and Jiang, 2016; Trischler et al., 2016), we first use bi-directional LSTMs (Hochreiter and Schmidhuber, 1997) to pre-process the sequences as follows:
Hp = Bi-LSTM(P),Hq = Bi-LSTM(Q),
Ha = Bi-LSTM(A), (1)
where Hp ∈ Rl×P , Hq ∈ Rl×Q and Ha ∈ Rl×A are the sequences of hidden states generated by the bi-directional LSTMs.",2.1 Co-matching,[0],[0]
We then make use of the attention mechanism to match each state in the passage to an aggregated representation of the question and the candidate answer.,2.1 Co-matching,[0],[0]
"The attention vectors are computed as follows:
Gq = SoftMax ( (WgHq + bg ⊗ eQ)THp ) ,
Ga = SoftMax ( (WgHa + bg ⊗ eQ)THp ) , H q = HqGq, H a = HaGa, (2)
where Wg ∈ Rl×l and",2.1 Co-matching,[0],[0]
bg ∈ Rl are the parameters to learn.,2.1 Co-matching,[0],[0]
eQ ∈ RQ is a vector of all 1s and it is used to repeat the bias vector into the matrix.,2.1 Co-matching,[0],[0]
Gq ∈ RQ×P,2.1 Co-matching,[0],[0]
"and Ga ∈ RA×P are the attention
weights assigned to the different hidden states in the question and the candidate answer sequences, respectively.",2.1 Co-matching,[0],[0]
Hq ∈ Rl×P is the weighted sum of all the question hidden states and it represents how the question can be aligned to each hidden state in the passage.,2.1 Co-matching,[0],[0]
So is Ha ∈ Rl×P .,2.1 Co-matching,[0],[0]
"Finally we can co-match the passage states with the question and the candidate answer as follows:
Mq = ReLU",2.1 Co-matching,[0],[0]
"( Wm [ H
q Hp H q ⊗Hp
] + bm ) ,
Ma = ReLU (",2.1 Co-matching,[0],[0]
"Wm [ H
a Hp H a ⊗Hp
] + bm ) ,
C =
[ Mq
Ma
] , (3)
where Wg ∈ Rl×2l and bg ∈",2.1 Co-matching,[0],[0]
Rl are the parameters to learn.,2.1 Co-matching,[0],[0]
"[ · · ] is the column-wise concatenation of two matrices, and · · and ·⊗ · are the elementwise subtraction and multiplication between two matrices, which are used to build better matching representations (Tai et al., 2015; Wang and Jiang, 2017).",2.1 Co-matching,[0],[0]
Mq ∈ Rl×P represents the matching between the hidden states of the passage and the corresponding attention-weighted representations of the question.,2.1 Co-matching,[0],[0]
"Similarly, we match the passage with the candidate answer and represent the matching results using Ma ∈ Rl×P .",2.1 Co-matching,[0],[0]
Finally C ∈ R2l×P is the concatenation of Mq ∈,2.1 Co-matching,[0],[0]
"Rl×P
and Ma ∈ Rl×P and represents how each passage state can be matched with the question and the candidate answer.",2.1 Co-matching,[0],[0]
"We refer to c ∈ R2l, which is a single column of C, as a co-matching state that concurrently matches a passage state with both the question and the candidate answer.",2.1 Co-matching,[0],[0]
"In order to capture the sentence structure of the passage, we further modify the model presented earlier and build a hierarchical LSTM (Tang et al., 2015) on top of the co-matching states.",2.2 Hierarchical Aggregation,[0],[0]
"Specifically, we first split the passage into sentences and we use P1,P2, . . .",2.2 Hierarchical Aggregation,[0],[0]
",PN to represent these sentences, where N is the number of sentences in the passage.",2.2 Hierarchical Aggregation,[0],[0]
"For each triplet {Pn,Q,A}, n ∈",2.2 Hierarchical Aggregation,[0],[0]
"[1, N ], we can get the co-matching states Cn through Eqn.",2.2 Hierarchical Aggregation,[0],[0]
(1-3).,2.2 Hierarchical Aggregation,[0],[0]
"Then we build a bi-directional LSTM followed by max pooling on top of the comatching states of each sentence as follows:
hsn = MaxPooling",2.2 Hierarchical Aggregation,[0],[0]
"(Bi-LSTM (Cn)) , (4)
where the function MaxPooling(·) is the row-wise max pooling operation.",2.2 Hierarchical Aggregation,[0],[0]
hsn ∈,2.2 Hierarchical Aggregation,[0],[0]
"Rl, n ∈",2.2 Hierarchical Aggregation,[0],[0]
"[1, N ] is the sentence-level aggregation of the co-matching states.",2.2 Hierarchical Aggregation,[0],[0]
"All these representations will be further integrated by another Bi-LSTM to get the final triplet matching representation.
",2.2 Hierarchical Aggregation,[0],[0]
Hs =,2.2 Hierarchical Aggregation,[0],[0]
[hs1;h s 2; . . .,2.2 Hierarchical Aggregation,[0],[0]
";h s N ],
ht = MaxPooling (Bi-LSTM (Hs)) , (5)
where Hs ∈ Rl×N is the concatenation of all the sentence-level representations and it is the input of a higher level LSTM.",2.2 Hierarchical Aggregation,[0],[0]
ht ∈,2.2 Hierarchical Aggregation,[0],[0]
"Rl is the final output of the matching between the sequences of the passage, the question and the candidate answer.",2.2 Hierarchical Aggregation,[0],[0]
"For each candidate answer Ai, we can build its matching representation hti ∈",2.3 Objective function,[0],[0]
Rl with the question and the passage through Eqn. (5).,2.3 Objective function,[0],[0]
"Our loss function is computed as follows:
L(Ai|P,Q) =",2.3 Objective function,[0],[0]
"− log exp(wThti)∑4 j=1 exp(w Thtj) , (6)
where w ∈ Rl is a parameter to learn.",2.3 Objective function,[0],[0]
"To evaluate the effectiveness of our hierarchical co-matching model, we use the RACE dataset (Lai et al., 2017), which consists of two subsets: RACE-M comes from middle school examinations while RACE-H comes from high school examinations.",3 Experiment,[0],[0]
"RACE is the combination of the two.
",3 Experiment,[0],[0]
We compare our model with a number of baseline models.,3 Experiment,[0],[0]
"We also compare with two variants of our model for an ablation study.
",3 Experiment,[0],[0]
Comparison with Baselines We compare our model with the following baselines:,3 Experiment,[0],[0]
"• Sliding Window based method (Richardson et al., 2013) computes the matching score based on the sum of the tf-idf values of the matched words between the question-answer pair and each subpassage with a fixed a window size.
",3 Experiment,[0],[0]
"• Stanford Attentive Reader (AR) (Chen et al., 2016) first builds a question-related passage representation through attention mechanism and then compares it with each candidate answer representation to get the answer probabilities.",3 Experiment,[0],[0]
• GA,3 Experiment,[0],[0]
"(Dhingra et al., 2017) uses gated attention mechanism with multiple hops to extract the question-related information of the passage and compares it with candidate answers.",3 Experiment,[0],[0]
"• ElimiNet (Soham et al., 2017) tries to first eliminate the most irrelevant choices and then select the best answer.",3 Experiment,[0],[0]
"• HAF (Zhou et al., 2018) considers not only the matching between the three sequences, namely, passage, question and candidate answer, but also the matching between the candidate answers.",3 Experiment,[0],[0]
"• MUSIC (Xu et al., 2017) integrates different sequence matching strategies into the model and also adds a unit of multi-step reasoning for selecting the answer.
",3 Experiment,[0],[0]
"Besides, we also report the following two results as reference points:",3 Experiment,[0],[0]
Turkers is the performance of Amazon Turkers on a randomly sampled subset of the RACE test set.,3 Experiment,[0],[0]
"Ceiling is the percentage of the unambiguous questions with a correct answer in a subset of the test set.
",3 Experiment,[0],[0]
The performance of our model together with the baselines are shown in Table 2.,3 Experiment,[0],[0]
"We can see that our proposed complete model, Hier-CoMatching, achieved the best performance among all the public results.",3 Experiment,[0],[0]
"Still, there is a huge gap between the best machine reading performance and the human performance, showing the great potential for further research.
",3 Experiment,[0],[0]
"Ablation Study Moreover, we conduct an ablation study of our model architecture.",3 Experiment,[0],[0]
"In this study, we are mainly interested in the contribution of each component introduced in this work to our final results.",3 Experiment,[0],[0]
We studied two key factors: (1) the comatching module and (2) the hierarchical aggregation approach.,3 Experiment,[0],[0]
"We observed a 4 percentage performance decrease by replacing the co-matching module with a single matching state (i.e., only Ma in Eqn (3)) by directly concatenating the question with each candidate answer (Yin et al., 2016).",3 Experiment,[0],[0]
"We also observe about 2 percentage decrease when we treat the passage as a plain sequence, and run a two-layer LSTM (to ensure the numbers of parameters are comparable) over the whole passage instead of the hierarchical LSTM.
",3 Experiment,[0],[0]
Question Type Analysis We also conducted an analysis on what types of questions our model can handle better.,3 Experiment,[0],[0]
"We find that our model obtains similar performance on the “wh” questions such as “why,” “what,” “when” and “where” questions, on which the performance is usually around 50%.",3 Experiment,[0],[0]
"We also check statement-justification questions with the keyword “true” (e.g., “Which of the following statements is true”), negation questions with the keyword “not” (e.g., “which of the following is not true”), and summarization questions with the keyword “title” (e.g., “what is the best title for the passage?”), and their performance is 51%, 52% and 48%, respectively.",3 Experiment,[0],[0]
We can see that the performance of our model on different types of questions in the RACE dataset is quite similar.,3 Experiment,[0],[0]
"However, our model is only based on wordlevel matching and may not have the ability of reasoning.",3 Experiment,[0],[0]
"In order to answer questions that require summarization, inference or reasoning, we still need to further explore the dataset and improve the model.",3 Experiment,[0],[0]
"Finally, we further compared our model to the baseline, which concatenates the question with each candidate answer, and our model can achieve better performance on different types of questions.",3 Experiment,[0],[0]
"For example, on the subset of the questions with pronouns, our model can achieve better accuracy of 49.8% than 47.9%.",3 Experiment,[0],[0]
"Similarly, on statement-justification questions with the keyword “true”, our model could achieve better accuracy of 51% than 47%.",3 Experiment,[0],[0]
"In this paper, we proposed a co-matching model for multi-choice reading comprehension.",4 Conclusions,[0],[0]
The model consists of a co-matching component and a hierarchical aggregation component.,4 Conclusions,[0],[0]
We showed that our model could achieve state-of-the-art performance on the RACE dataset.,4 Conclusions,[0],[0]
"In the future, we will adapt the idea of co-matching and hierarchical aggregation to the standard open-domain QA setting for answer candidate reranking (Wang et al., 2017).",4 Conclusions,[0],[0]
We will also further study how to explicitly model inference and reasoning on the RACE dataset.,4 Conclusions,[0],[0]
This work was partially supported by DSO grant DSOCL15223.,5 Acknowledgement,[0],[0]
"Multi-choice reading comprehension is a challenging task, which involves the matching between a passage and a question-answer pair.",abstractText,[0],[0]
"This paper proposes a new co-matching approach to this problem, which jointly models whether a passage can match both a question and a candidate answer.",abstractText,[0],[0]
Experimental results on the RACE dataset demonstrate that our approach achieves state-of-the-art performance.,abstractText,[0],[0]
A Co-Matching Model for Multi-choice Reading Comprehension,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1401–1411, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.
We propose a conversion of bilingual sentence pairs and the corresponding word alignments into novel linear sequences. These are joint translation and reordering (JTR) uniquely defined sequences, combining interdepending lexical and alignment dependencies on the word level into a single framework. They are constructed in a simple manner while capturing multiple alignments and empty words. JTR sequences can be used to train a variety of models. We investigate the performances of ngram models with modified Kneser-Ney smoothing, feed-forward and recurrent neural network architectures when estimated on JTR sequences, and compare them to the operation sequence model (Durrani et al., 2013b). Evaluations on the IWSLT German→English, WMT German→English and BOLT Chinese→English tasks show that JTR models improve state-of-the-art phrasebased systems by up to 2.2 BLEU.",text,[0],[0]
"Standard phrase-based machine translation (Och et al., 1999; Zens et al., 2002; Koehn et al., 2003) uses relative frequencies of phrase pairs to estimate a translation model.",1 Introduction,[0],[0]
"The phrase table is extracted from a bilingual text aligned on the word level, using e.g. GIZA++",1 Introduction,[0],[0]
"(Och and Ney, 2003).",1 Introduction,[0],[0]
"Although the phrase pairs capture internal dependencies between the source and target phrases aligned to each other, they fail to model dependencies that extend beyond phrase boundaries.",1 Introduction,[0],[0]
Phrase-based decoding involves concatenating target phrases.,1 Introduction,[0],[0]
"The burden of ensuring that the result is linguistically consistent falls on the language model (LM).
",1 Introduction,[0],[0]
This work proposes word-based translation models that are potentially capable of capturing long-range dependencies.,1 Introduction,[0],[0]
"We do this in two steps: First, given bilingual sentence pairs and the associated word alignments, we convert the information into uniquely defined linear sequences.",1 Introduction,[0],[0]
These sequenecs encode both word reordering and translation information.,1 Introduction,[0],[0]
"Thus, they are referred to as joint translation and reordering (JTR) sequences.",1 Introduction,[0],[0]
"Second, we train an n-gram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998) on the resulting JTR sequences.",1 Introduction,[0],[0]
"This yields a model that fuses interdepending reordering and translation dependencies into a single framework.
",1 Introduction,[0],[0]
"Although JTR n-gram models are closely related to the operation sequence model (OSM) (Durrani et al., 2013b), there are three main differences.",1 Introduction,[0],[0]
"To begin with, the OSM employs minimal translation units (MTUs), which are essentially atomic phrases.",1 Introduction,[0],[0]
"As the MTUs are extracted sentence-wise, a word can potentially appear in multiple MTUs.",1 Introduction,[0],[0]
"In order to avoid overlapping translation units, we define the JTR sequences on the level of words.",1 Introduction,[0],[0]
"Consequently, JTR sequences have smaller vocabulary sizes than OSM sequences and lead to models with less sparsity.",1 Introduction,[0],[0]
"Moreover, we argue that JTR sequences offer a simpler reordering approach than operation sequences, as they handle reorderings without the need to predict gaps.",1 Introduction,[0],[0]
"Finally, when used as an additional model in the log-linear framework of phrase-based decoding, an n-gram model trained on JTR sequences introduces only one single feature to be tuned, whereas the OSM additionally uses 4 supportive features (Durrani et al., 2013b).",1 Introduction,[0],[0]
"Experimental results confirm that this simplification does not make JTR models less expressive, as their performance is on par with the OSM.
",1 Introduction,[0],[0]
"Due to data sparsity, increasing the n-gram order of count-based models beyond a certain point becomes useless.",1 Introduction,[0],[0]
"To address this, we resort to neu-
1401
ral networks (NNs), as they have been successfully applied to machine translation recently (Sundermeyer et al., 2014; Devlin et al., 2014).",1 Introduction,[0],[0]
They are able to score any word combination without requiring additional smoothing techniques.,1 Introduction,[0],[0]
"We experiment with feed-forward and recurrent translation networks, benefiting from their smoothing capabilities.",1 Introduction,[0],[0]
"To this end, we split the linear sequence into two sequences for the neural translation models to operate on.",1 Introduction,[0],[0]
This is possible due to the simplicity of the JTR sequence.,1 Introduction,[0],[0]
"We show that the count and NN models perform well on their own, and that combining them yields even better results.
",1 Introduction,[0],[0]
"In this work, we apply n-gram models with modified Kneser-Ney smoothing during phrasebased decoding and neural JTR models in rescoring.",1 Introduction,[0],[0]
"However, using a phrase-based system is not required by the model, but only the initial step to demonstrate the strength of JTR models, which can be applied independently of the underlying decoding framework.",1 Introduction,[0],[0]
"While the focus of this work is on the development and comparison of the models, the long-term goal is to decode using JTR models without the limitations introduced by phrases, in order to exploit the full potential of JTR models.",1 Introduction,[0],[0]
"The JTR models are estimated on word alignments, which we obtain using GIZA++ in this paper.",1 Introduction,[0],[0]
"The future aim is to also generate improved word alignments by a joint optimization of both the alignments and the models, similar to the training of IBM models (Brown et al., 1990; Brown et al., 1993).",1 Introduction,[0],[0]
"In the long run, we intend to achieve a consistency between decoding and training using the introduced JTR models.",1 Introduction,[0],[0]
"In order to address the downsides of the phrase translation model, various approaches have been taken.",2 Previous Work,[0],[0]
"Mariño et al. (2006) proposed a bilingual language model (BILM) that operates on bilingual n-grams, with an own n-gram decoder requiring monotone alignments.",2 Previous Work,[0],[0]
"The lexical reordering model introduced in (Tillmann, 2004) was integrated into phrase-based decoding.",2 Previous Work,[0],[0]
Crego and Yvon (2010) adapted the approach to BILMs.,2 Previous Work,[0],[0]
"The bilingual n-grams are further advanced in (Niehues et al., 2011), where they operate on nonmonotone alignments within a phrase-based translation framework.",2 Previous Work,[0],[0]
"Compared to our JTR models, their BILMs treat jointly aligned source words as minimal translation units, ignore unaligned source
words and do not include reordering information.",2 Previous Work,[0],[0]
Durrani et al. (2011) developed the OSM which combined dependencies on bilingual word pairs and reordering information into a single framework.,2 Previous Work,[0],[0]
It used an own decoder that was based on ngrams of MTUs and predicted single translation or reordering operations.,2 Previous Work,[0],[0]
"This was further advanced in (Durrani et al., 2013a) by a decoder that was capable of predicting whole sequences of MTUs, similar to a phrase-based decoder.",2 Previous Work,[0],[0]
"In (Durrani et al., 2013b), a slightly enhanced version of OSM was integrated into the log-linear framework of the Moses system (Koehn et al., 2007).",2 Previous Work,[0],[0]
"Both the BILM (Stewart et al., 2014) and the OSM (Durrani et al., 2014) can be smoothed using word classes.
",2 Previous Work,[0],[0]
"Guta et al. (2015) introduced the extended translation model (ETM), which operates on the word level and augments the IBM models by an additional bilingual word pair and a reordering operation.",2 Previous Work,[0],[0]
"It is implemented into the log-linear framework of a phrase-based decoder and shown to be competitive with a 7-gram OSM.
",2 Previous Work,[0],[0]
The JTR n-gram models proposed within this work can be seen as an extension of the ETM.,2 Previous Work,[0],[0]
"Nevertheless, JTR models utilize linear sequences of dependencies and combine the translation of bilingual word pairs and reoderings into a single model.",2 Previous Work,[0],[0]
"The ETM, however, features separate models for the translation of individual words and reorderings and provides an explicit treatment of multiple alignments.",2 Previous Work,[0],[0]
"As they operate on linear sequences, JTR count models can be implemented using existing toolkits for n-gram language models, e.g. the KenLM toolkit (Heafield et al., 2013).
",2 Previous Work,[0],[0]
"An HMM approach for word-to-phrase alignments was presented in (Deng and Byrne, 2005), showing performance similar to IBM Model 4 on the task of bitext alignment.",2 Previous Work,[0],[0]
Feng et al. (2013) propose several models which rely only on the information provided by the source side and predict reorderings.,2 Previous Work,[0],[0]
"Contrastingly, JTR models incorporate target information as well and predict both translations and reorderings jointly in a single framework.
",2 Previous Work,[0],[0]
Zhang et al. (2013) explore different Markov chain orderings for an n-gram model on MTUs in rescoring.,2 Previous Work,[0],[0]
"Feng and Cohn (2013) present another generative word-based Markov chain translation model which exploits a hierarchical PitmanYor process for smoothing, but it is only applied to induce word alignments.",2 Previous Work,[0],[0]
"Their follow-up work (Feng et al., 2014) introduces a Markov-model on
MTUs, similar to the OSM described above.",2 Previous Work,[0],[0]
"Recently, neural machine translation has emerged as an alternative to phrase-based decoding, where NNs are used as standalone models to decode source input.",2 Previous Work,[0],[0]
"In (Sutskever et al., 2014), a recurrent NN was used to encode a source sequence, and output a target sentence once the source sentence was fully encoded in the network.",2 Previous Work,[0],[0]
The network did not have any explicit treatment of alignments.,2 Previous Work,[0],[0]
Bahdanau et al. (2015) introduced soft alignments as part of the network architecture.,2 Previous Work,[0],[0]
"In this work, we make use of hard alignments instead, where we encode the alignments in the source and target sequences, requiring no modifications of existing feed-forward and recurrent NN architectures.",2 Previous Work,[0],[0]
"Our feed-forward models are based on the architectures proposed in (Devlin et al., 2014), while the recurrent models are based on (Sundermeyer et al., 2014).",2 Previous Work,[0],[0]
"Further recent research on applying NN models for extended context was carried out in (Le et al., 2012; Auli et al., 2013; Hu et al., 2014).",2 Previous Work,[0],[0]
All of these works focus on lexical context and ignore the reordering aspect covered in our work.,2 Previous Work,[0],[0]
The core idea of this work is the interpretation of a bilingual sentence pair and its word alignment as a linear sequence of K joint translation and reordering (JTR) tokens gK1 .,3 JTR Sequences,[0],[0]
"Formally, the sequence gK1 ( f J 1 ,e I 1,b",3 JTR Sequences,[0],[0]
"I 1) is a uniquely defined interpretation of a given source sentence f J1 , its translation e",3 JTR Sequences,[0],[0]
"I 1 and the inverted alignment bI1, where bi denotes the ordered sequence of source positions j aligned to target position i. We drop the explicit mention of ( f J1 ,e I 1,b I 1) to allow for a better readability.",3 JTR Sequences,[0],[0]
"Each JTR token is either an aligned bilingual word pair 〈 f ,e〉 or a reordering class ∆ j′",3 JTR Sequences,[0],[0]
"j.
Unaligned words on the source and target side are processed as if they were aligned to the empty word ε .",3 JTR Sequences,[0],[0]
"Hence, an unaligned source word f generates the token 〈 f ,ε〉, and an unaligned target word e the token 〈ε,e〉.
",3 JTR Sequences,[0],[0]
Each word of the source and target sentences is to appear in the corresponding JTR sequence exactly once.,3 JTR Sequences,[0],[0]
"For multiply-aligned target words e, the first source word f that is aligned to e generates the token 〈 f ,e〉.",3 JTR Sequences,[0],[0]
"All other source words f ′, that are also aligned to e, are processed as if they were aligned to the artificial word σ .",3 JTR Sequences,[0],[0]
"Thus, each of these f ′ generates a token 〈 f ′,σ〉.",3 JTR Sequences,[0],[0]
"The same approach is applied to multiply-aligned source
Algorithm 1 JTR Conversion Algorithm 1: procedure JTRCONVERSION( f J1 , e I 1, b I 1)
2: gK1← /0",3 JTR Sequences,[0],[0]
"3: // last translated source position j′ 4: j′← 0 5: for i← 1 to I do 6: if ei is unaligned then 7: // align ei to the empty word ε 8: APPEND(gK1 , 〈ε,ei〉) 9: continue
10: // ei is aligned to at least one source word 11: j← first source position in bi 12: if j = j′ then 13: // ei is aligned to the same f j as ei−1 14: APPEND(gK1 , 〈σ ,ei〉) 15: continue 16: if j 6= j′+1 then 17: // alignment step is non-monotone 18: REORDERINGS( f J1 , b I 1, g K 1 , j
′, j) 19: // 1-to-1 translation: f j is aligned to ei 20: APPEND(gK1 , 〈 f j,ei〉) 21: j′← j 22: // generate all other f j that are also 23: // aligned to the current target word ei 24: for all remaining j in bi do 25: APPEND(gK1 , 〈 f j,σ〉) 26: j′← j 27: // check last alignment step at sentence end 28:",3 JTR Sequences,[0],[0]
if j′ 6=,3 JTR Sequences,[0],[0]
"J then 29: // last alignment step is non-monotone 30: REORDERINGS( f J1 , b I 1, g K 1 , j
′, J +1) 31: return gK1 32: 33: // called when a reordering class is appended 34: procedure REORDERINGS( f J1 , b I 1, g K 1 , j
′, j) 35: // check if the predecessor is unaligned 36: if f j−1 is unaligned then 37: //",3 JTR Sequences,[0],[0]
"get unaligned predecessors 38: f j−1j0 ← unaligned predecessors of f j 39: // check if the alignment step to the first 40: // unaligned predecessor is monotone 41: if j0 6= j′+1 then 42: // non-monotone: add reordering class 43: APPEND(gK1 , ∆ j′, j0 ) 44: // translate unaligned predecessors by ε 45: for f ← f j0 to f j−1 do 46:",3 JTR Sequences,[0],[0]
"APPEND(gK1 , 〈 f ,ε〉) 47: else 48: // non-monotone: add reordering class 49: APPEND(gK1 , ∆ j′, j)
words.",3 JTR Sequences,[0],[0]
"Similar to Feng and Cohn (2013), we classify the reordered source positions j′ and j by ∆ j′",3 JTR Sequences,[0],[0]
"j:
∆ j′",3 JTR Sequences,[0],[0]
"j =  step backward (←), j = j′−1 jump forward (y), j > j′+1 jump backward (x), j < j′−1.
",3 JTR Sequences,[0],[0]
The reordering classes are illustrated in Figure 1.,3 JTR Sequences,[0],[0]
Algorithm 1 presents the formal conversion of a bilingual sentence pair and its alignment into the corresponding JTR sequence gK1 .,3.1 Sequence Conversion,[0],[0]
"At first, g K 1 is initialized by an empty sequence (line 2).",3.1 Sequence Conversion,[0],[0]
"For each target position i = 1, . . .",3.1 Sequence Conversion,[0],[0]
", I it is extended by at least one token.",3.1 Sequence Conversion,[0],[0]
"During the generation process, we store the last visited source position j′ (line 4).",3.1 Sequence Conversion,[0],[0]
"If a target word ei is
• unaligned, we align it to the empty word ε and append 〈ε,ei〉 to the current gK1 (line 8), • if it is aligned to the same f j as ei−1, we only
add 〈σ ,ei〉 (line 14), • otherwise we append 〈 f j,ei〉 (line 20) and • in case there are more source words aligned
to ei, we additionally append 〈 f j,σ〉 for each of these (line 24).
",3.1 Sequence Conversion,[0],[0]
"Before a token 〈 f j,ei〉 is generated, we have to check whether the alignment step from j′ to j is monotone (line 16).",3.1 Sequence Conversion,[0],[0]
"In case it is not, we have to deal with reorderings (line 34).",3.1 Sequence Conversion,[0],[0]
"We define that a token 〈 f j−1,ε〉 is to be generated right before the generation of the token containing",3.1 Sequence Conversion,[0],[0]
"f j. Thus, if f j−1 is not aligned, we first determine the contiguous sequence of unaligned predecessors f j−1j0 (line 38).",3.1 Sequence Conversion,[0],[0]
"Next, if the step from j′ to j0 is not monotone, we add the corresponding reordering class (line 43).",3.1 Sequence Conversion,[0],[0]
"Afterwards we append all 〈 f j0 ,ε〉 to 〈 f j−1,ε〉.",3.1 Sequence Conversion,[0],[0]
"If f j−1 is aligned, we do not have to process unaligned source words and only append the corresponding reordering class (line 49).
",3.1 Sequence Conversion,[0],[0]
"Figure 2 illustrates the generation steps of a JTR sequence, whose result is presented in Table 1.",3.1 Sequence Conversion,[0],[0]
The alignment steps are denoted by the arrows connecting the alignment points.,3.1 Sequence Conversion,[0],[0]
"The first dashed alignment point indicates the 〈ε, ,〉 token that is generated right after the 〈Feld,field〉 token.",3.1 Sequence Conversion,[0],[0]
"The second dashed alignment point indicates the 〈ein,ε〉 token, which corresponds to the unaligned source word ein.",3.1 Sequence Conversion,[0],[0]
"Note, that the 〈ein,ε〉
token has to be generated right before 〈., .",3.1 Sequence Conversion,[0],[0]
〉 is generated.,3.1 Sequence Conversion,[0],[0]
"Therefore, there is no forward jump from 〈Code,code〉 to 〈., .",3.1 Sequence Conversion,[0],[0]
"〉, but a monotone step to 〈ein,ε〉 followed by 〈., .",3.1 Sequence Conversion,[0],[0]
〉.,3.1 Sequence Conversion,[0],[0]
"As the JTR sequence gK1 is a unique interpretation of a bilingual sentence pair and its alignment, the probability p( f J1 ,e I 1,b",3.2 Training of Count Models,[0],[0]
"I 1) can be computed as:
p( f J1 ,e I 1,b",3.2 Training of Count Models,[0],[0]
I 1) = p(g K 1 ).,3.2 Training of Count Models,[0],[0]
"(1)
The probability of gK1 can be factorized and approximated by an n-gram model.
p(gK1 )",3.2 Training of Count Models,[0],[0]
"= K
∏ k=1 p(gk|gk−1k−n+1) (2)",3.2 Training of Count Models,[0],[0]
"Within this work, we first estimate the Viterbi alignment for the bilingual training data using GIZA++",3.2 Training of Count Models,[0],[0]
"(Och and Ney, 2003).",3.2 Training of Count Models,[0],[0]
"Secondly, the conversion presented in Algorithm 1 is applied to obtain the JTR sequences, on which we estimate an n-gram model with modified Kneser-Ney smoothing as described in (Chen and Goodman, 1998) using the KenLM toolkit1 (Heafield et al., 2013).
1https://kheafield.com/code/kenlm/",3.2 Training of Count Models,[0],[0]
"Basically, each phrase table entry is annotated with both the word alignment information, which also allows to identify unaligned source words, and the corresponding JTR sequence.",3.3 Integration into Phrase-based Decoding,[0],[0]
The JTR model is added to the log-linear framework as an additional n-gram model.,3.3 Integration into Phrase-based Decoding,[0],[0]
"Within the phrase-based decoder, we extend each search state such that it additionally stores the JTR model history.
",3.3 Integration into Phrase-based Decoding,[0],[0]
"In comparison to the OSM, the JTR model does not predict gaps.",3.3 Integration into Phrase-based Decoding,[0],[0]
Local reorderings within phrases are handled implicitly.,3.3 Integration into Phrase-based Decoding,[0],[0]
"On the other hand, we represent long-range reorderings between phrases by the coverage vector and limit them by reordering constraints.
",3.3 Integration into Phrase-based Decoding,[0],[0]
Phrase-pairs ending with unaligned source words at their right boundary prove to be a problem during decoding.,3.3 Integration into Phrase-based Decoding,[0],[0]
"As shown in Subsection 3.1, the conversion from word alignments to JTR sequences assumes that each token corresponding to an unaligned source word is generated immediately before the token corresponding to the closest aligned source position to its right.",3.3 Integration into Phrase-based Decoding,[0],[0]
"However, if a phrase ends with an unaligned f j as its rightmost source word, the generation of the 〈 f j,ε〉 token has to be postponed until the next word f j+1 is to be translated or, even worse, f j+1 has already been translated before.
",3.3 Integration into Phrase-based Decoding,[0],[0]
"To address this issue, we constrained the phrase table extraction to discard entries with unaligned source tokens at the right boundary.",3.3 Integration into Phrase-based Decoding,[0],[0]
"For IWSLT
De→En, this led to a baseline weaker by 0.2 BLEU than the one described in Section 5.",3.3 Integration into Phrase-based Decoding,[0],[0]
"In order to have an unconstrained and fair baseline, we thereafter removed this constraint and forced such deletion tokens to be generated at the end of the sequence.",3.3 Integration into Phrase-based Decoding,[0],[0]
"Hence, we accept that the JTR model might compute the wrong score in these special cases.",3.3 Integration into Phrase-based Decoding,[0],[0]
"Usually, smoothing techniques are applied to count-based models to handle unseen events.",4 Neural Networks,[0],[0]
"A neural network does not suffer from this, as it is able to score unseen events without additional smoothing techniques.",4 Neural Networks,[0],[0]
"In the following, we will describe how to adapt JTR sequences to be used with feed-forward and recurrent NNs.
",4 Neural Networks,[0],[0]
"The first thing to notice is the vocabulary size, mainly determined by the number of bilingual word pairs, which constituted atomic units in the count-based models.",4 Neural Networks,[0],[0]
NNs that compute probability values at the output layer evaluate a softmax function that produces normalized scores that sum up to unity.,4 Neural Networks,[0],[0]
"The softmax function is given by:
p(ei|ei−11 ) =",4 Neural Networks,[0],[0]
eoei,4 Neural Networks,[0],[0]
"(e
i−1 1 )
∑|V |w=1 eow(e i−1 1 )
(3)
where oei and ow are the raw unnormalized output layer values for the words ei and w, respectively, and |V",4 Neural Networks,[0],[0]
| is the vocabulary size.,4 Neural Networks,[0],[0]
The output layer is a function of the context ei−11 .,4 Neural Networks,[0],[0]
"Computing the denominator is expensive for large vocabularies, as it requires computing the output for all words.",4 Neural Networks,[0],[0]
"Therefore, we split JTR tokens gk and use individual words as input and output units, such that the NN receives jumps, source and target words as input and outputs target words and jumps.",4 Neural Networks,[0],[0]
"Hence, the resulting neural model is not a LM, but a translation model with different input and output vocabularies.",4 Neural Networks,[0],[0]
A JTR sequence gK1 is split into its source and target parts sK1 and t K 1 .,4 Neural Networks,[0],[0]
"The construction of the JTR source sequence sK1 proceeds as follows: Whenever a bilingual pair is encountered, the source word is kept and the target word is discarded.",4 Neural Networks,[0],[0]
"In addition, all jump classes are replaced by a special token δ .",4 Neural Networks,[0],[0]
"The JTR target sequence tK1 is constructed similarly by keeping the target words and dropping source words, and the jump classes are also kept.",4 Neural Networks,[0],[0]
"Table 1 shows the JTR source and target sequences corresponding to JTR sequence of Figure 2.
",4 Neural Networks,[0],[0]
"Due to the design of the JTR sequence, producing the source and target JTR sequences is straightforward.",4 Neural Networks,[0],[0]
"The resulting sequences can then be used with existing NN architectures, without further modifications to the design of the networks.",4 Neural Networks,[0],[0]
This results in powerful models that require little effort to implement.,4 Neural Networks,[0],[0]
"First, we will apply a feed-forward NN (FFNN) to the JTR sequence.",4.1 Feed-forward Neural JTR,[0],[0]
"FFNN models resemble countbased models in using a predefined limited context size, but they do not encounter the same smoothing problems.",4.1 Feed-forward Neural JTR,[0],[0]
"In this work, we use a FFNN similar to that proposed in (Devlin et al., 2014), defined as:
p(tK1 |sK1 )",4.1 Feed-forward Neural JTR,[0],[0]
"≈ K
∏ k=1
p(tk|tk−1k−n ,skk−n).",4.1 Feed-forward Neural JTR,[0],[0]
"(4)
It scores the JTR target word tk at position k using the current source word sk, and the history of n JTR source words.",4.1 Feed-forward Neural JTR,[0],[0]
"In addition, the n JTR target words preceding tk are used as context.",4.1 Feed-forward Neural JTR,[0],[0]
"The FFNN computes the score by looking up the vector embeddings of the source and target context words, concatenating them, then evaluating the rest of the network.",4.1 Feed-forward Neural JTR,[0],[0]
"We reduce the output layer to a shortlist of the most frequent words, and compute word class probabilities for the remaining words.",4.1 Feed-forward Neural JTR,[0],[0]
"Unlike feed-forward NNs, recurrent NNs (RNNs) enable the use of unbounded context.",4.2 Recurrent Neural JTR,[0],[0]
"Following (Sundermeyer et al., 2014), we use bidirectional recurrent NNs (BRNNs) to capture the full JTR source side.",4.2 Recurrent Neural JTR,[0],[0]
"The BRNN uses the JTR target side as well as the full JTR source side as context, and it is given by:
p(tK1 |sK1 )",4.2 Recurrent Neural JTR,[0],[0]
"= K
∏ k=1
p(tk|tk−11 ,sK1 ) (5)
",4.2 Recurrent Neural JTR,[0],[0]
This equation is realized by a network that uses forward and backward recurrent layers to capture the complete source sentence.,4.2 Recurrent Neural JTR,[0],[0]
"By a forward layer we imply a recurrent hidden layer that processes a given sequence from left to right, while a backward layer does the processing backwards, from right to left.",4.2 Recurrent Neural JTR,[0],[0]
"The source sentence is basically split at a given position k, then past and future representations of the sentence are recursively computed by the forward and backward layers, respectively.",4.2 Recurrent Neural JTR,[0],[0]
"To include the target side, we provide the forward
layer with the target input tk−1 as well, that is, we aggregate the embeddings of the input source word sk and the input target word tk−1 before they are fed into the forward layer.",4.2 Recurrent Neural JTR,[0],[0]
"Due to recurrency, the forward layer encodes the parts (tk−11 ,s k 1), and the backward layer encodes sKk , and together they encode (tk−11 ,s K 1 ), which is used to score the output target word tk.",4.2 Recurrent Neural JTR,[0],[0]
"For the sake of comparison to FFNN and count models, we also experiment with a recurrent model that does not include future source information, this is obtained by replacing the term sK1 with s k 1 in Eq. 5.",4.2 Recurrent Neural JTR,[0],[0]
"It will be referred to as the unidirectional recurrent neural network (URNN) model in the experiments.
",4.2 Recurrent Neural JTR,[0],[0]
"Note that the JTR source and target sides include jump information, therefore, the RNN model described above explicitly models reordering.",4.2 Recurrent Neural JTR,[0],[0]
"In contrast, the models proposed in (Sundermeyer et al., 2014) do not include any jumps, and hence do not provide an explicit way of including word reordering.",4.2 Recurrent Neural JTR,[0],[0]
"In addition, the JTR RNN models do not require the use of IBM-1 lexica to resolve multiply-aligned words.",4.2 Recurrent Neural JTR,[0],[0]
"As discussed in Section 3, these cases are resolved by aligning the multiply-aligned word to the first word on the opposite side.
",4.2 Recurrent Neural JTR,[0],[0]
"The integration of the NNs into the decoder is not trivial, due to the dependence on the target context.",4.2 Recurrent Neural JTR,[0],[0]
"In the case of RNNs, the context is unbounded, which would affect state recombination, and lead to less variety in the beam used to prune the search space.",4.2 Recurrent Neural JTR,[0],[0]
"Therefore, the RNN scores are computed using approximations instead (Auli et al., 2013; Alkhouli et al., 2015).",4.2 Recurrent Neural JTR,[0],[0]
"In (Alkhouli et al., 2015), it is shown that approximate RNN integration into the phrase-based decoder has a slight advantage over n-best rescoring.",4.2 Recurrent Neural JTR,[0],[0]
"Therefore, we apply RNNs in rescoring in this work, and to allow for a direct comparison between FFNNs and RNNs, we apply FFNNs in rescoring as well.",4.2 Recurrent Neural JTR,[0],[0]
"We perform experiments on the largescale IWSLT 20132 (Cettolo et al., 2014) German→English, WMT 20153 German→English and the DARPA BOLT Chinese→English tasks.",5 Evaluation,[0],[0]
The statistics for the bilingual corpora are shown in Table 2.,5 Evaluation,[0],[0]
"Word alignments are generated with the GIZA++ toolkit
2http://www.iwslt2013.org 3http://www.statmt.org/wmt15/
(Och and Ney, 2003).",5 Evaluation,[0],[0]
"We use a standard phrasebased translation system (Koehn et al., 2003).",5 Evaluation,[0],[0]
The decoding process is implemented as a beam search.,5 Evaluation,[0],[0]
"All baselines contain phrasal and lexical smoothing models for both directions, word and phrase penalties, a distance-based reordering model, enhanced low frequency features (Chen et al., 2011), a hierarchical reordering model (HRM) (Galley and Manning, 2008), a word class LM (Wuebker et al., 2013) and an n-gram LM.",5 Evaluation,[0],[0]
The lexical and phrase translation models of all baseline systems are trained on all provided bilingual data.,5 Evaluation,[0],[0]
"The log-linear feature weights are tuned with minimum error rate training (MERT) (Och, 2003) on BLEU (Papineni et al., 2001).",5 Evaluation,[0],[0]
"All systems are evaluated with MultEval (Clark et al., 2011).",5 Evaluation,[0],[0]
"The reported BLEU scores are averaged over three MERT optimization runs.
",5 Evaluation,[0],[0]
"All LMs, OSMs and count-based JTR models are estimated with the KenLM toolkit (Heafield et al., 2013).",5 Evaluation,[0],[0]
The OSM and the count-based JTR model are implemented in the phrasal decoder.,5 Evaluation,[0],[0]
NNs are used only in rescoring.,5 Evaluation,[0],[0]
The 9-gram FFNNs are trained with two hidden layers.,5 Evaluation,[0],[0]
"The short lists contain the 10k most frequent words, and all remaining words are clusterd into 1000 word classes.",5 Evaluation,[0],[0]
"The projecton layer has 17× 100 nodes, the first hidden layer 1000 and the second 500.",5 Evaluation,[0],[0]
The RNNs have LSTM architectures.,5 Evaluation,[0],[0]
"The URNN has 2 hidden layers while the BRNN has one forward, one backward and one additional hidden layer.",5 Evaluation,[0],[0]
"All layers have 200 nodes, while the output layer is class-factored using 2000 classes.",5 Evaluation,[0],[0]
For the count-based JTR model and OSM we tuned the n-gram size on the tuning set of each task.,5 Evaluation,[0],[0]
"For the full data, 7-grams were used for the IWSLT and WMT tasks, and 8-grams for BOLT.",5 Evaluation,[0],[0]
"When using in-domain data, smaller n-gram sizes were used.",5 Evaluation,[0],[0]
All rescoring experiments used 1000- best lists without duplicates.,5 Evaluation,[0],[0]
The domain of IWSLT consists of lecture-type talks presented at TED conferences which are also available online4.,5.1 Tasks description,[0],[0]
"All systems are optimized on the dev2010 corpus, named dev here.",5.1 Tasks description,[0],[0]
Some of the OSM and JTR systems are trained on the TED portions of the data containing 138K sentences.,5.1 Tasks description,[0],[0]
"To estimate the 4-gram LM, we additionally make use of parts of the Shuffled News, LDC English Gigaword and 109-French-English corpora, selected by a cross-entropy difference criterion (Moore and Lewis, 2010).",5.1 Tasks description,[0],[0]
"In total, 1.7 billion running words are taken for LM training.",5.1 Tasks description,[0],[0]
The BOLT Chinese→English task is evaluated on the “discussion forum” domain.,5.1 Tasks description,[0],[0]
The 5-gram LM is trained on 2.9 billion running words in total.,5.1 Tasks description,[0],[0]
The in-domain data consists of a subset of 67.8K sentences and we used a set of 1845 sentences for tuning.,5.1 Tasks description,[0],[0]
The evaluation set test1 contains 1844 and test2 1124 sentences.,5.1 Tasks description,[0],[0]
"For the WMT task, we used the target side of the bilingual data and all monolingual data to train a pruned 5-gram LM on a total of 4.4 billion running words.",5.1 Tasks description,[0],[0]
We concatenated the newstest2011 and newstest2012 corpora for tuning the systems.,5.1 Tasks description,[0],[0]
"We start with the IWSLT 2013 German→ English task, where we compare between the different JTR and OSM models.",5.2 Results,[0],[0]
The results are shown in Table 3.,5.2 Results,[0],[0]
"When comparing the in-domain n-gram JTR model trained using Kneser-Ney smoothing (KN) to OSM, we observe that the n-gram KN JTR model improves the baseline by 1.4 BLEU on both test and eval11.",5.2 Results,[0],[0]
"The OSM model performs similarly, with a slight disadvantage on eval11.",5.2 Results,[0],[0]
"In comparison, the FFNN of Eq. (4) improves the baseline by 0.7–0.9 BLEU, compared to the slightly better 0.8–1.1 BLEU achieved by the URNN.",5.2 Results,[0],[0]
"The difference between the FFNN and the
4http://www.ted.com/
URNN is that the latter captures the unbounded source and target history that extends until the beginning of the sentences, giving it an advantage over the FFNN.",5.2 Results,[0],[0]
"The performance of the URNN can be improved by including the future part of the source sentence, as described in Eq.",5.2 Results,[0],[0]
"(5), resulting in the BRNN model.",5.2 Results,[0],[0]
"Next, we explore whether the models are additive.",5.2 Results,[0],[0]
"When rescoring the n-gram KN JTR output with the BRNN, an additional improvement of 0.6 BLEU is obtained.",5.2 Results,[0],[0]
"There are two reasons for this: The BRNN includes the future
part of the source input when scoring target words.",5.2 Results,[0],[0]
This information is not used by the KN model.,5.2 Results,[0],[0]
"Moreover, the BRNN is able to score word combinations unseen in training, while the KN model uses backing off to score unseen events.
",5.2 Results,[0],[0]
"When training the KN, FFNN, and OSM models on the full data, we observe less gains in comparison to in-domain data training.",5.2 Results,[0],[0]
"However, combining the KN models trained on in-domain and full data gives additional gains, which suggests that although the in-domain model is more adapted to the task, it still can gain from out-of-domain data.",5.2 Results,[0],[0]
Adding the FFNN on top improves the combination.,5.2 Results,[0],[0]
"Note here that the FFNN sees the same information as the KN model, but the difference is that the NN operates on the word level rather than the word-pair level.",5.2 Results,[0],[0]
"Second, the FFNN is able to handle unseen sequences by design, without the need for the backing off workaround.",5.2 Results,[0],[0]
"The BRNN improves the combination more than the FFNN, as the model captures an unbounded source and target history in addition to an unbounded future source context.",5.2 Results,[0],[0]
"Combining the KN, FFNN and BRNN JTR models leads to an overall gain of 2.2 BLEU on both dev and test.
",5.2 Results,[0],[0]
"Next, we present the BOLT Chinese→English results, shown in Table 4.",5.2 Results,[0],[0]
"Comparing n-gram KN JTR and OSM trained on the in-domain data shows they perform equally well on test1, improving the baseline by 0.7 BLEU, with a slight advantage for the JTR model on test2.",5.2 Results,[0],[0]
The feedforward and the recurrent in-domain networks yield the same results in comparison to each other.,5.2 Results,[0],[0]
Training the OSM and JTR models on the full data yields slightly worse results than in-domain training.,5.2 Results,[0],[0]
"However, combining the two types of training improves the results.",5.2 Results,[0],[0]
"This is shown when adding the in-domain KN JTR model on top of the model trained on full data, improving it by up to 0.4 BLEU.",5.2 Results,[0],[0]
"Rescoring with the feed-forward and the recurrent network improves this even further, supporting the previous observation that the n-gram KN JTR and NNs complement each other.",5.2 Results,[0],[0]
"The combination of the 4 models yields an overall improvement of 1.2–1.4 BLEU.
",5.2 Results,[0],[0]
"Finally, we compare KN JTR and OSM models on the WMT German→English task in Table 5.",5.2 Results,[0],[0]
The two models perform almost similar to each other.,5.2 Results,[0],[0]
The JTR model improves the baseline by up to 0.7 BLEU.,5.2 Results,[0],[0]
Rescoring the KN JTR with the FFNN improves it by up to 0.3 BLEU leading to an overall improvement between 0.5 and 1.0 BLEU.,5.2 Results,[0],[0]
"To investigate the effect of including jump information in the JTR sequence, we trained a BRNN using jump classes and another excluding them.",5.3 Analysis,[0],[0]
The BRNNs were used in rescoring.,5.3 Analysis,[0],[0]
"Below, we demonstrate the difference between the systems:
source: wir kommen später noch auf diese Leute zurück .",5.3 Analysis,[0],[0]
reference: We’ll come back to these people later .,5.3 Analysis,[0],[0]
Hypothesis 1: JTR source: wir kommen δ zurück δ,5.3 Analysis,[0],[0]
später noch auf diese Leute δ .,5.3 Analysis,[0],[0]
JTR target: we come y back x later σ to these people y .,5.3 Analysis,[0],[0]
Hypothesis 2: JTR source: wir kommen später noch auf diese Leute zurück .,5.3 Analysis,[0],[0]
"JTR target: we come later σ on these guys back .
",5.3 Analysis,[0],[0]
"Note the German verb “zurückkommen”, which is split into “kommen” and “zurück”.",5.3 Analysis,[0],[0]
German places “kommen” at the second position and “zurück” towards the end of the sentence.,5.3 Analysis,[0],[0]
"Unlike German, the corresponding English phrase “come back” has the words adjacent to each other.",5.3 Analysis,[0],[0]
"We found that the system including jumps prefers the correct translation of the verb, as shown in Hypothesis 1 above.",5.3 Analysis,[0],[0]
"The system translates “kommen” to “come”, jumps forward to “zurück”, translates it to “back”, then jumps back to continue translating the word “später”.",5.3 Analysis,[0],[0]
"In contrast, the system that excludes jump classes is blind to this separation of words.",5.3 Analysis,[0],[0]
It favors Hypothesis 2 which is a strictly monotone translation of the German sentence.,5.3 Analysis,[0],[0]
"This is also reflected by the BLEU scores, where we found the system including jump classes outperforming the one without by up to 0.8 BLEU.",5.3 Analysis,[0],[0]
We introduced a method that converts bilingual sentence pairs and their word alignments into joint translation and reordering (JTR) sequences.,6 Conclusion,[0],[0]
They combine interdepending lexical and alignment dependencies into a single framework.,6 Conclusion,[0],[0]
A main advantage of JTR sequences is that a variety of models can be trained on them.,6 Conclusion,[0],[0]
"Here, we have estimated n-gram models with modified Kneser-Ney smoothing, FFNN and RNN architectures on JTR sequences.
",6 Conclusion,[0],[0]
"We compared our count-based JTR model to the OSM, both used in phrase-based decoding, and showed that the JTR model performed at least as good as OSM, with a slight advantage for JTR.",6 Conclusion,[0],[0]
"In comparison to the OSM, the JTR model operates on words, leading to a smaller vocabulary size.",6 Conclusion,[0],[0]
"Moreover, it utilizes simpler reordering structures without gaps and only requires one log-linear feature to be tuned, whereas the OSM needs 5.",6 Conclusion,[0],[0]
"Due to the flexibility of JTR sequences, we can apply them also to FFNNs and RNNs.",6 Conclusion,[0],[0]
"Utilizing two count models and applying both networks in rescoring gains the overall highest improvement over the phrase-based system by up to 2.2 BLEU, on the German→English IWSLT task.",6 Conclusion,[0],[0]
"The combination outperforms OSM by up to 1.2 BLEU on the BOLT Chinese→English tasks.
",6 Conclusion,[0],[0]
"The JTR models are not dependent on the phrase-based framework, and one of the longterm goals is to perform standalone decoding with the JTR models independently of phrase-based systems.",6 Conclusion,[0],[0]
"Without the limitations introduced by phrases, we believe that JTR models could perform even better.",6 Conclusion,[0],[0]
"In addition, we aim to use JTR models to obtain the alignment, which would then be used to train the JTR models in an iterative manner, achieving consistency and hoping for improved models.",6 Conclusion,[0],[0]
This work has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreement no 645452 (QT21).,Acknowledgements,[0],[0]
This material is partially based upon work supported by the DARPA BOLT project under Contract No. HR0011- 12-C-0015.,Acknowledgements,[0],[0]
"Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of DARPA.",Acknowledgements,[0],[0]
We propose a conversion of bilingual sentence pairs and the corresponding word alignments into novel linear sequences.,abstractText,[0],[0]
"These are joint translation and reordering (JTR) uniquely defined sequences, combining interdepending lexical and alignment dependencies on the word level into a single framework.",abstractText,[0],[0]
They are constructed in a simple manner while capturing multiple alignments and empty words.,abstractText,[0],[0]
JTR sequences can be used to train a variety of models.,abstractText,[0],[0]
"We investigate the performances of ngram models with modified Kneser-Ney smoothing, feed-forward and recurrent neural network architectures when estimated on JTR sequences, and compare them to the operation sequence model (Durrani et al., 2013b).",abstractText,[0],[0]
"Evaluations on the IWSLT German→English, WMT German→English and BOLT Chinese→English tasks show that JTR models improve state-of-the-art phrasebased systems by up to 2.2 BLEU.",abstractText,[0],[0]
A Comparison between Count and Neural Network Models Based on Joint Translation and Reordering Sequences,title,[0],[0]
"Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 990–994, Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics",text,[0],[0]
"Vectorial representations derived from large current events datasets such as Google News have been shown to perform well on word similarity tasks (Mikolov, 2013; Levy & Goldberg, 2014).",1 Introduction,[0],[0]
This paper shows vectorial representations derived from substantially smaller explanatory text datasets such as English Wikipedia and Simple English Wikipedia preserve enough lexical semantic information to make these kinds of category judgments with equal or better accuracy.,1 Introduction,[0],[0]
Analysis shows these results may be driven by a prevalence of commonsense facts in explanatory text.,1 Introduction,[0],[0]
These positive results for relatively small datasets suggest vectors derived from slower but more accurate analyses of these resources may be practical for lexical semantic applications.,1 Introduction,[0],[0]
"Wikipedia is a free Internet encyclopedia website and the largest general reference work over the
Internet.1 As of December 2014, Wikipedia contained over 4.6 million articles2 and 1.6 billion words.",2.1 Wikipedia,[0],[0]
Wikipedia as a corpus has been heavily used to train various NLP models.,2.1 Wikipedia,[0],[0]
"Features of Wikipedia are well exploited in research like semantic web (Lehmann et al, 2014) and topic modeling (Dumais, 1988; Gabrilovich, 2007), but more importantly Wikipedia has been a reliable source for word embedding training because of its sheer size and coverage (Qiu, 2014), as recent word embedding models (Mikolov et al, 2013; Pennington et al, 2014) all use Wikipedia as an important corpus to build and evaluate their algorithms for word embedding creation.",2.1 Wikipedia,[0],[0]
Simple English Wikipedia3 is a Wikipedia database where all articles are written using simple English words and grammar.,2.2 Simple English Wikipedia,[0],[0]
It is created to help adults and children who are learning English to look for encyclopedic information.,2.2 Simple English Wikipedia,[0],[0]
"Compared with full English Wikipedia, Simple English Wikipedia is much smaller.",2.2 Simple English Wikipedia,[0],[0]
"It contains around 120,000 articles and 20 million words, which is almost one fortieth the number of articles and one eightieth the number of words compared to full English Wikipedia, so the average length of articles is also shorter.",2.2 Simple English Wikipedia,[0],[0]
"Simple English Wikipedia is often used in simplification research (Coster, 2011; Napoles, 2010) where sentences from full English Wikipedia are matched to sentences from Simple English Wikipedia to explore techniques to simplify sentences.",2.2 Simple English Wikipedia,[0],[0]
"It would be
1",2.2 Simple English Wikipedia,[0],[0]
See https://en.wikipedia.org/wiki/Wikipedia 2,2.2 Simple English Wikipedia,[0],[0]
See http://en.wikipedia.org/wiki/Wikipedia:Size_of_,2.2 Simple English Wikipedia,[0],[0]
Wikipedia 3,2.2 Simple English Wikipedia,[0],[0]
"See http://simple.wikipedia.org/wiki/Main_Page
990
reasonable to expect that the small vocabulary size of Simple English Wikipedia may be disadvantageous when trying to create word embeddings using co-occurrence information, but it may also be true that despite the much smaller vocabulary size and overall size, because of the explanatory nature of its text, Simple English Wikipedia would still preserve enough information to allow the performance of models trained with Simple English Wikipedia to be comparable to models trained on full Wikipedia, and perform equally well or better than non-explanatory texts like the Google News corpus.",2.2 Simple English Wikipedia,[0],[0]
"The distributed representation of words, or word embeddings, has gained significant attention in the research community, and one of the more discussed works is Mikolov’s (2013) word representation estimation research.",2.3 Word2Vec,[0],[0]
Mikolov proposed two neusral network based models for word representation: Continuous Bag-of-Words (CBOW) and Skip-gram.,2.3 Word2Vec,[0],[0]
"CBOW takes advantage of context words surrounding a given word to predict the word by summing all the context word vectors together to represent the word; whereas Skip-gram uses the word to predict the context word vectors for skip-gram positions, therefore making the model sensitive to positions of context words.",2.3 Word2Vec,[0],[0]
"Both of the models scale well to large quantities of training data, however it is noted by Mikolov that Skipgram works well with small amounts of training data and provides good representations for rare words, and CBOW would perform better and have higher accuracy for frequent words if trained on larger corpora.",2.3 Word2Vec,[0],[0]
"The purpose of this paper is not to compare the models, but to use the models to compare training corpora to see how different arrangement of information may impact the quality of the word embeddings.",2.3 Word2Vec,[0],[0]
"To evaluate the effectiveness of full English Wikipedia and Simple English Wikipedia as training corpora for word embeddings, the word similarityrelatedness task described by Levy & Goldberg (2014) is used.",3 Task Description,[0],[0]
"As pointed out by Agirre et al (2009) and Levy & Goldberg (2014), relatedness may actually be measuring topical similarity and be better predicted by a bag-of-words model, and similarity may be measuring functional or syntactic
similarity and be better predicted by a contextwindow model.",3 Task Description,[0],[0]
"However, when the models are constant, the semantic information of the test words in the training corpora is crucial to allowing the model to build semantic representations for the words.",3 Task Description,[0],[0]
"It may be argued that when the corpus is explanatory, more semantic information about the target words is present; whereas when the corpus is non-explanatory, information around the words is merely related to the words.",3 Task Description,[0],[0]
"The WordSim353 (Agirre, 2009) dataset is used as the test dataset.",3 Task Description,[0],[0]
"This dataset contains pairs of words that are decided by human annotators to be either similar or related, and a similarity or relatedness gold standard score is also given to every pair of words.",3 Task Description,[0],[0]
"There are 100 similar word pairs, 149 related pairs and 104 pairs of words with very weak or no relation.",3 Task Description,[0],[0]
"In the evaluation task, the unrelated word pairs are discarded from the dataset.
",3 Task Description,[0],[0]
The objective of the task is to rank the similar word pairs higher than related ones.,3 Task Description,[0],[0]
The retrieval/ranking procedure is as follows.,3 Task Description,[0],[0]
"First, the cosine similarity scores are calculated using word embeddings from a certain model; then the scores are sorted from the highest to the lowest.",3 Task Description,[0],[0]
The retrieval step is then carried out by locating the last pair of the first n% of the pairs of similar words in the sorted list of scores and determining the percentage of similar word pairs in the sub-list delimited by the last pair of similar words.,3 Task Description,[0],[0]
"In other words, the procedure treats similar word pairs as successful retrievals and determines the accuracy rate when the recall rate is n%.",3 Task Description,[0],[0]
"Because the accuracy rate would always fall to the percentage of similar word pairs in all word pairs, it is expected that the later and more suddenly it falls, the better the model is performing in this task.",3 Task Description,[0],[0]
"The word2vec python implementation provided by gensim (Rehurek et al, 2010) package is used to train all the word2vec models.",4 Models,[0],[0]
"For Skip-gram and CBOW, a 5-word window size is used to allow them to get the same amount of raw information, also words appearing 5 times or fewer are filtered out.",4 Models,[0],[0]
The dimensions of the word embeddings from Skip-gram and CBOW are all 300.,4 Models,[0],[0]
"Both full English Wikipedia and Simple English Wikipedia are used as training corpora with minimal preprocessing procedures: XML tags are removed and
infoboxes are filtered out, thus yielding four models: Full English Wikipedia – CBOW(FW-CBOW), Full English Wikipedia – Skip-gram(FW-SG), Simple English Wikipedia – CBOW(SW-CBOW) and Simple English Wikipedia – Skip-gram(SWSG).",4 Models,[0],[0]
The pre-trained Google News skip-gram model with 300-dimensional vectors (GN-SG) is also downloaded from the Google word2vec website for comparison.,4 Models,[0],[0]
"This model is trained on the Google News dataset with 100 billion words, which is 30 times as large as the full English Wikipedia and 240 times as large as Simple English Wikipedia.",4 Models,[0],[0]
"Table 1 shows the accuracy rate at every recall rate point, with the sum of all the accuracy rates as the cumulative score.",5 Results,[0],[0]
"It is shown that GN-SG, although not far behind, is not giving the best performance despite being trained on the largest dataset.",5 Results,[0],[0]
"In fact, it is clear that it never excels at any given recall rate point.",5 Results,[0],[0]
"It outperforms various models at certain recall rate points by a small margin, but there is no obvious advantage gained from training using a much larger corpus even when compared with the models trained on Simple English Wikipedia, despite the greater risk of sparse data problems on this smaller data set.",5 Results,[0],[0]
"For models trained on Simple English Wikipedia and full English Wikipedia, it is also interesting to see that the models almost perform equally well.",5 Results,[0],[0]
"The FW-CBOW trained on full English Wikipedia performs the best among the models overall, but for the first few recall rate points, it performs equally well or slightly worse than either SWCBOW or SW-SG trained on Simple English Wikipedia.",5 Results,[0],[0]
"At the later points, it is also clear that although FW-CBOW is generally better than all the other models most of the time, the margin could be considered narrow and furthermore it is equally as good as SW-CBOW at the first two recall points.
",5 Results,[0],[0]
"Comparing FW-SG with SW-SG and SWCBOW, there is almost no sign of performance gain from training using full Wikipedia instead of the much smaller Simple Wikipedia.",5 Results,[0],[0]
"FW-SG performs equally well or often slightly worse than both Simple Wikipedia models.
",5 Results,[0],[0]
"The main observation in this paper is that Google News is not out-performing other systems substantially and that full Wikipedia systems are not out-performing Simple Wikipedia substantially (that is, comparing the CBOW models to one another and the Skip-gram models to one another).",5 Results,[0],[0]
"The main result from the table is not that smaller training datasets yield better systems, but that systems trained using significantly smaller training datasets of explanatory text have very close performances in this task compared with systems trained on very large datasets, despite the big training data size difference.",5 Results,[0],[0]
"As mentioned previously, similarity may be better predicted by a context-window model because it measures functional or syntactic similarity.",6 Analysis,[0],[0]
"However, it is not clear in these models that the syntactic information is a major component in the word embeddings.",6 Analysis,[0],[0]
"Instead, it may be that the main factor for the performance level of the models is the general explanatory content of the Wikipedia articles, as opposed to the current events content of Google News.",6 Analysis,[0],[0]
"For similar words such as synonyms or hyponyms, the crucial information making them similar is shared general semantic features of the words.",6 Analysis,[0],[0]
"For example, for the word pair physics : chemistry, the shared semantic features might be that they are both academic subjects, both studied in institutions and both composed of different subfields, as shown in Table 2.",6 Analysis,[0],[0]
The ‘@’ sign in table 2 connects a context word with its position relative to the word in the center of the window.,6 Analysis,[0],[0]
"These shared properties
Model   10%   Recall   Rate   20%   Recall   Rate   30%   Recall   Rate   40%   Recall   Rate   50%   Recall   Rate   60%   Recall   Rate   70%   Recall   Rate   80%   Recall   Rate   90%   Recall   Rate   100%   Recall   Rate   Cumulative   Score   FW-‐CBOW   0.91   0.95   0.89   0.83   0.72   0.74   0.61   0.51   0.46   0.40   7.03   SW-‐CBOW   0.91   0.95   0.78   0.75   0.72   0.70   0.56   0.50   0.46   0.40   6.74   FW-‐SG   0.91   0.95   0.79   0.75   0.63   0.61   0.53   0.49   0.43   0.40   6.50   SW-‐SG   0.91   0.95   0.91   0.70   0.62   0.57   0.54   0.45   0.42   0.40   6.47   GN-‐SG   0.85   0.84   0.82   0.79   0.70   0.64   0.57   0.48   0.43   0.40   6.51  
Table 1: Performance of Different Models at Different Recall Rate Points
of the core semantic identities for these words may contribute greatly to the similarity judgments for humans and machines alike, and these shared properties may be considered general knowledge about the words.",6 Analysis,[0],[0]
"For the related words, for example computer : keyboard, it may be difficult to pinpoint the semantic overlap between the components which build up the core semantic identities of these words, and none is observed in the data.",6 Analysis,[0],[0]
"General knowledge of a certain word may be found in explanatory texts about the word like dictionaries or encyclopedias, but rarely found in texts other than that.",6 Analysis,[0],[0]
"It would be assumed by the writers of informative non-explanatory texts like news articles that the readers are well acquainted with all the basic semantic information about the words, therefore repetition of such information would be unnecessary.",6 Analysis,[0],[0]
"For a similarity/relatedness judgment task where basic and compositional semantic information may prove to be useful, using a corpus like Google News, where information or context for a particular word assumes one is already conversant with it, would not be as effective as using a corpus like Wikipedia where general knowledge about a word may be available and repeated.",6 Analysis,[0],[0]
"Also, the smaller vocabulary size of Wikipedia compared with Google News would suggest that general knowledge may be conveyed more efficiently with less data sparsity.",6 Analysis,[0],[0]
"In the Simple Wikipedia vs. full Wikipedia case, both corpora are explanatory texts.",6 Analysis,[0],[0]
"Despite the much smaller size, the general semantic overlap between each pair of similar words seems as evident in Simple Wikipedia as in full Wikipedia.",6 Analysis,[0],[0]
"For measurements like cosine similarity where large values in the same dimensions are favored, the basic semantic components which contribute to the similarity judgments for the words are the same comparatively across two different corpora.",6 Analysis,[0],[0]
"This may not be surprising because although more information may be present in full Wikipedia, because of its explanatory nature, the core semantic components which make a concept distinct still dominate over new and sparser information added to it.",6 Analysis,[0],[0]
"In Simple Wikipedia, the size of the articles
Word Pair COAST SHORE PHYSICS CHEMISTRY Simple Wikipedia
east@-1 164 west@-1 137 south@-1 75 north@-1 64 Africa@2 63 Sea@4 55 Atlantic@-1 53 western@-1 52 northern@-1 52 eastern@-1 50 North@2 46 Australia@2 43 southern@-1 40 Pacific@-1 37 America@3 33 city@-4 33 island@3 30
Lake@2 39 eastern@-1 25 north@-1 17 south@-1 17 Sea@4 14 western@-1 14 northern@-1 12 southern@-1 11 lake@3 10 River@4 8 close@-2 7 Michigan@3 6 washed@-3 5 west@-1 5 island@3 5 sea@-1 5",6 Analysis,[0],[0]
"Texas@-1 4
particle@-1 34 chemistry@2 29 quantum@-1 28 nuclear@-1 23 theoretical@-1 21 University@3 21 laws@-2 21 mathematical@-1 16 chemistry@-2 16 professor@-2 14 mathematics@2 13 mathematics@-2 13 classical@-1 13 atomic@-1 13",6 Analysis,[0],[0]
modern@-1 11 Nobel@-3 10,6 Analysis,[0],[0]
"physics@3 10
organic@-1 86 physics@-2 29 physical@-1 21 used@-3 20 supramolecular@-1 18 chemistry@3 17 chemistry@-3 17 theoretical@-1 16 physics@2 16 placed@3 14 biology@2 14 analytical@-1 12 University@3 12 quantum@-1 12 Organic@-1 11 computational@-1 11",6 Analysis,[0],[0]
"professor@-2 11
Full Wikipedia west@-1 16279 east@-1 13662 south@-1 4574",6 Analysis,[0],[0]
Atlantic@-1 3741 north@-1 3497 Pacific@-1 3383 western@-1 2802 southern@-1 2783 eastern@-1 2771,6 Analysis,[0],[0]
Sea@-1 2463 America@3 2446 northern@-1 2383 Island@3 2333 North@2 2280 Africa@2 2254 located@-4,6 Analysis,[0],[0]
"2177 island@3 1966
Lake@2 3700 north@-1 2718 eastern@-1 2567 along@-3 2229 western@-1 2163 located@-4 1955 south@-1 1908 southern@-1 1810 Lake@3 1645",6 Analysis,[0],[0]
northern@-1 1628,6 Analysis,[0],[0]
batteries@1,6 Analysis,[0],[0]
1162 lake@3 1121 Bay@3 1050 River@4 875 east@-1 800 west@-1 785,6 Analysis,[0],[0]
"bombardment@1 664
particle@-1 2898 theoretical@-1 2366 University@3 2053 mathematics@-2 1929 chemistry@2 1864 nuclear@-1 1745 laws@-2 1686 quantum@-1 1443 chemistry@-2 1192 professor@-2 1192 mathematical@-1 1136",6 Analysis,[0],[0]
"mathematics@2 1032 matter@-1 786 degree@-2 741 state@-1 737 University@4 706 studied@-1 679
organic@-1 2733 physics@-2 1864 University@3 1267 physics@2 1192 physical@-1 1080 professor@-2 977 biology@-2 886 biology@2 756 studied@-1 667 analytical@-1 633 inorganic@-1 575 degree@-2 559 quantum@-1 554 University@4 517 chemistry@3 418 chemistry@-3 418 computational@-1 396
Table 2:",6 Analysis,[0],[0]
"Top 17 Context Words that Co-occur with the Sample Similar Word Pairs
and vocabulary may restrict it to be basic and precise to explain a certain concept with fewer presumptions of what the readers already know, and it is suggested by the analysis that such style is also reflected in full Wikipedia, leading to the domination of general knowledge over specific facts.",6 Analysis,[0],[0]
This paper has shown vectorial representations derived from substantially smaller explanatory text datasets such as Wikipedia and Simple Wikipedia preserve enough lexical semantic information to make these kinds of category judgments with equal or better accuracy than news corpora.,7 Conclusion,[0],[0]
Analysis shows these results may be driven by a prevalence of commonsense facts in explanatory text.,7 Conclusion,[0],[0]
"These positive results for small datasets suggest vectors derived from slower but more accurate analysis of these resources may be practical for lexical semantic applications, and we hope by providing this result, future researchers may be more aware of the viability of smaller-scale resources like Simple English Wikipedia (or presumably Wikipedia in other languages which are substantially smaller in size than English Wikipedia), that can still produce high quality vectors despite a much smaller size.",7 Conclusion,[0],[0]
Vectorial representations of words derived from large current events datasets have been shown to perform well on word similarity tasks.,abstractText,[0],[0]
This paper shows vectorial representations derived from substantially smaller explanatory text datasets such as English Wikipedia and Simple English Wikipedia preserve enough lexical semantic information to make these kinds of category judgments with equal or better accuracy.,abstractText,[0],[0]
A Comparison of Word Similarity Performance Using Explanatory and Non-explanatory Texts,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1795–1804, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
Learning word meanings is a challenging early step in child language acquisition.,1 Introduction,[0],[0]
"Imagine a child hears the word dax for the first time while observing a white rabbit jumping around – dax might mean WHITE RABBIT, RABBIT, ANIMAL, CUTE, LOOK, etc.",1 Introduction,[0],[0]
"(Quine, 1960).",1 Introduction,[0],[0]
How does the child learn the correct meaning of a word from a large pool of potential meanings?,1 Introduction,[0],[0]
"A possible explanation is that children infer a word’s meaning by identifying the commonalities across the situations in which the word occurs (Pinker, 1989).",1 Introduction,[0],[0]
"One mechanism for achieving this is crosssituational learning (e.g., Siskind, 1996; Frank et al., 2007; Fazly et al., 2010; Kachergis et al., 2012).",1 Introduction,[0],[0]
"Recent word learning experiments confirm that both adults and children infer the correct word-meaning mappings by keeping track of cross-situational statistics across individually ambiguous learning trials (Yu and Smith, 2007; Smith and Yu, 2008; Yurovsky et al., 2014).
",1 Introduction,[0],[0]
"Although cross-situational learning is a general mechanism for narrowing down the meaning of a word, it does not explain how children overcome an interesting challenge in word learning: determining the correct level of a hierarchical taxonomy that a word refers to.",1 Introduction,[0],[0]
"For example, children learn that the word dog refers to all kinds of dogs, and not to a specific breed, such as Dalmatians, or to a more general category, such as animals – even though some of these choices (e.g., animals) are compatible with all the cross-situational evidence available for dog (because all dogs are also animals).",1 Introduction,[0],[0]
"We use the term “word generalization” to refer to this problem of associating a word with the meaning at an appropriate category level, given some sample of experiences with the word.
",1 Introduction,[0],[0]
"Previous research has argued that children use a specific bias or constraint – the basic-level assumption – to focus their word generalizations appropriately (Markman, 1991; Golinkoff et al., 1994).",1 Introduction,[0],[0]
"According to this bias, children prefer to associate a word to a set of objects that form a basic-level category, such as dogs or trucks, and that share a significant number of attributes.",1 Introduction,[0],[0]
"It is less preferred to associate a new word to much more specific subordinate categories, such as Dalmatians or bulldozers, or to more general superordinate ones, like animals or vehicles, whose members share fewer attributes (Rosch, 1973; Rosch et al., 1976).",1 Introduction,[0],[0]
"It remains an important open question of whether a word learner requires such a bias to acquire appropriate mappings.
",1 Introduction,[0],[0]
Xu and Tenenbaum (2007) (X&T henceforth) studied the word generalization problem in a set of experiments in which children and adults were asked to determine which level of a taxonomy a novel word referred to.,1 Introduction,[0],[0]
X&T further examined this behavioral data through computational modelling.,1 Introduction,[0],[0]
"They proposed a Bayesian model that, given a few exemplars of a novel word, matches human behaviour in how it maps the word to its
1795
meanings in a taxonomic category.",1 Introduction,[0],[0]
The Bayesian model of X&T is important in providing insight into how people might reason about samples of data that exemplify categories.,1 Introduction,[0],[0]
"However, it relies on having complete, built-in knowledge about the taxonomic hierarchy, including both the detailed composition of categories and the values for between-object similarities, drawn from adult similarity judgments.",1 Introduction,[0],[0]
"Furthermore, the X&T model does not address the issue of word generalization in the broader context of word learning: While their model reasons over samples of data associated with a word label, it does not develop a meaning representation of the word over time, as a child must do.",1 Introduction,[0],[0]
"It is important to understand how word generalization occurs when embedded in the natural process of learning a word meaning and in the context of more limited category knowledge.
",1 Introduction,[0],[0]
We address these issues by providing a unified account of word learning and word generalization within a computational model of crosssituational learning.,1 Introduction,[0],[0]
"Unlike the X&T model, our model is an incremental learner that gradually acquires the meaning of words, and uses these developing meanings in determining the appropriate extension of a word to elements of a taxonomy.",1 Introduction,[0],[0]
Our model has general knowledge of category structure without having an elaborated taxonomy encoding known object similarities.,1 Introduction,[0],[0]
"Moreover, in the absence of any bias toward generalization to particular kinds of categories, the model exhibits the observed “basic-level bias” due to general mechanisms of productivity that have been proposed to apply to many aspects of linguistic knowledge (e.g., Bybee, 1985; Croft and Cruse, 2004).1
In what follows, we first describe the human experiments of X&T, and then present our computational model and the experiments that simulate the X&T data.",1 Introduction,[0],[0]
"X&T perform a set of empirical studies to investigate how children and adults generalize novel words learned from a few examples to the appro-
1Computational cognitive models are often categorized with respect to Marr’s levels of analysis, i.e., their degree of abstraction (Marr, 1982).",2 Novel Word Generalization in People,[0],[0]
"The model of X&T is at the computational level, providing a Bayesian framework for the problem of word generalization.",2 Novel Word Generalization in People,[0],[0]
"In contrast, our model investigates more detailed mechanisms and thus lies between the algorithmic and computational levels of analysis.
priate level of meaning in a taxonomy.",2 Novel Word Generalization in People,[0],[0]
"In each training trial of an experiment, participants hear a novel word (such as fep) and observe one or more instances exemplifying the word (in the form of pictures for adults and toy objects for children).",2 Novel Word Generalization in People,[0],[0]
"The conditions vary in that the make-up of the set of training instances is representative of different levels of a taxonomy (e.g., all Dalmatians vs. various kinds of dogs vs. various kinds of animals).",2 Novel Word Generalization in People,[0],[0]
"In the testing phase, participants are asked to select all objects that they think are feps from a set of test items.",2 Novel Word Generalization in People,[0],[0]
"Both children and adults make various inferences about what a fep is depending on the levels of the taxonomy from which the training instances are drawn.
",2 Novel Word Generalization in People,[0],[0]
"Specifically, X&T use a taxonomy with animals, vehicles, and vegetables, from which instances are drawn to produce the training conditions in Fig. 1(a).",2 Novel Word Generalization in People,[0],[0]
"For example, in one training condition, participants are shown a Dalmatian, a poodle, and a beagle in three consecutive trials, hearing the word fep to refer to each object.",2 Novel Word Generalization in People,[0],[0]
"After training, participants are asked to select all feps from the set of test objects, which includes items from all 3 superordinate categories.",2 Novel Word Generalization in People,[0],[0]
"As illustrated in Fig. 1(b), each test object is assessed as one of the following types of match to the training data:
• a subordinate match: an object of the same subordinate category as a training object (e.g., Dalmatians in Fig. 1)
• a basic-level match: an object of the same basic-level category as a training object (e.g., a dog, but not the same breed as one in training [which would be a subordinate match])
",2 Novel Word Generalization in People,[0],[0]
"• a superordinate match: an object of the same superordinate category as the training objects (e.g., another kind of animal, but not one seen in training [which would be a subordinate or basic-level match])
",2 Novel Word Generalization in People,[0],[0]
X&T report the percentage of test objects of each type of match that are selected by participants within each training condition; see Fig. 2.,2 Novel Word Generalization in People,[0],[0]
"(For example, the reported value for “super. match” would be 75% if participants on average chose 3 of the 4 superordinate matches in the test set.)
",2 Novel Word Generalization in People,[0],[0]
Consider first the data from adults.,2 Novel Word Generalization in People,[0],[0]
"After seeing a single object (1-example condition – e.g., a Dalmatian), adults show a strong basic-level bias – i.e., they tend to generalize the word fep to refer to both Dalmatians (subordinate matches) and to
other dogs (basic-level matches), but not to other animals (superordinate matches).",2 Novel Word Generalization in People,[0],[0]
"But with 3 instances of a Dalmatian (3-subordinate condition), this behaviour is attenuated – the number of basiclevel matches is much lower.",2 Novel Word Generalization in People,[0],[0]
"For the 3-basic-level and 3-superordinate conditions, the adults show generalization up to categories consistent with the evidence – i.e., at the basic and superordinate levels, respectively.
",2 Novel Word Generalization in People,[0],[0]
"Interestingly, children also show a basic-level bias, but differ from adults in that it is less pronounced – e.g., they are less likely than adults to select basic-level matches (other dogs) having seen a single Dalmatian or having seen 3 Dalmatians.",2 Novel Word Generalization in People,[0],[0]
"In the other conditions, children’s behaviour is similar to adults, but shows somewhat less generalization to unseen types of objects (e.g., other kinds of dogs/animals than those in training).",2 Novel Word Generalization in People,[0],[0]
(a) Adult data:,2 Novel Word Generalization in People,[0],[0]
"Our computational model is based on the crosssituational word learner of Fazly et al. (2010) (henceforth, FAS), which accounts for a range of observed patterns in child and adult vocabulary ac-
quisition.",3 The Word Learning Framework,[0],[0]
"Here we give an overview of the FAS model; the next section explains extensions to handle the novel word generalization task.
",3 The Word Learning Framework,[0],[0]
A naturalistic language learning scenario consists of both linguistic data (what a child a hears) and non-linguistic data (what a child perceives).,3 The Word Learning Framework,[0],[0]
"This input is modeled as a sequence of utterance– scene (U–S) pairs, where an utterance is a group of words and a scene is a set of semantic features representing the meaning of those words:
U : { look, a, fep, . . .",3 The Word Learning Framework,[0],[0]
"} S: { PERCEPTION, LOOK, . . .",3 The Word Learning Framework,[0],[0]
", DALMATIAN, DOG, . . .",3 The Word Learning Framework,[0],[0]
"}
Given such input, for each word w, the model of FAS learns a probability distribution over all semantic features, Pt(.|w), which represents the word’s meaning at time t. Initially, at time t = 0, P0(.|w) is a uniform distribution.",3 The Word Learning Framework,[0],[0]
"The word meanings are incrementally learned using an algorithm that implements cross-situational learning: for each pair of a wordw and a semantic feature f , the model learns Pt(f |w) from co-occurrences of w and f across all the utterance–scene pairs seen up to time t, as follows.
",3 The Word Learning Framework,[0],[0]
"Given an utterance–scene pair U–S at time t, and drawing on its learned knowledge of word meanings up to time t−1, the model of FAS calculates an alignment probability for each wj–fi pair.",3 The Word Learning Framework,[0],[0]
"This probability reflects how strongly the feature fi is associated with wj compared to its association with other words in U :
Pt(aij |U, fi) =",3 The Word Learning Framework,[0],[0]
"Pt−1(fi|wj)∑ w′∈U Pt−1(fi|w′) (1)
where aij indicates the mapping between the word wj and the semantic feature fi.
",3 The Word Learning Framework,[0],[0]
"These probabilities are incrementally accumulated for each wj–fi pair, capturing the overall strength of association of wj and fi at time t:
assoct(fi, wj) = assoct−1(fi, wj) + Pt(aij |U, fi)
(2)
",3 The Word Learning Framework,[0],[0]
"The (normalized) association scores then serve as the basis for the incremental adjustment of the meaning probabilities of all features fi for each word wj seen in the input at time t:
Pt(fi|wj) = assoct(fi, wj) + γ∑ fm∈M assoct(fm, wj) + k",3 The Word Learning Framework,[0],[0]
"γ (3)
HereM is the group of all features that the model has observed, k is the expected number of such features, and γ is a small smoothing parameter, which determines the prior probability of observing a new feature.
",3 The Word Learning Framework,[0],[0]
"Smoothing entails that features previously unseen with a word (all fi such that assoct(fi, wj) = 0) have a small but non-zero probability.",3 The Word Learning Framework,[0],[0]
"That is, when fi is unseen with wj , Eqn. (3) reduces to:
P ut (fi|wj) = γ∑
fm∈M assoct(fm, wj) + k",3 The Word Learning Framework,[0],[0]
"γ
(4)
",3 The Word Learning Framework,[0],[0]
"This unseen probability, P ut , reflects the learner’s “openness” to the word being associated with new features (Nematzadeh et al., 2011): a higher or lower P ut (fi|wj) will affect how strongly a previously unseen fi can be associated with wj in the alignment process (Eqn. (1)).",3 The Word Learning Framework,[0],[0]
"We return to this property of the model below, as it relates to the behaviour of our model in making generalizations.",3 The Word Learning Framework,[0],[0]
We assume that the representation of meaning can be abstracted to features that correspond to different levels of categorization.,4 Extensions to the Model,[0],[0]
"For example, a Dalmatian in an input scene is represented as {DALMATIAN, DOG, ANIMAL} and a Bulldog as {BULLDOG, DOG, ANIMAL}, where we use FEATURENAME to refer to all the features that are specific to that level of object category.",4 Extensions to the Model,[0],[0]
"(Note that we could replace each of these features with the appropriate “true” set of features, but use the more compact representation for simplicity.)",4 Extensions to the Model,[0],[0]
"To acquire the meaning of the word Dalmatian, the model must learn a probability distribution in which P (f |Dalmatian) is relatively high for the features DALMATIAN, DOG, and ANIMAL, and low for features such as BULLDOG, CAT, and VEGETABLE.
",4 Extensions to the Model,[0],[0]
Introducing Feature Groups.,4 Extensions to the Model,[0],[0]
"In the FAS model, all the features for a word are dependent: increasing the probability of any feature results in decreasing the probability of others.",4 Extensions to the Model,[0],[0]
"However, this
interaction is not always desirable, as many features regularly co-occur in the world.",4 Extensions to the Model,[0],[0]
"This is especially an issue for features from a category hierarchy, where features of a subordinate category should not compete with features of the parent.",4 Extensions to the Model,[0],[0]
"That is, while a higher probability of DALMATIAN features (e.g., black spotted coat) may lessen the likelihood of BULLDOG features (e.g., wrinkles), it should not decrease the probability of DOG features (e.g., having 4 legs).
",4 Extensions to the Model,[0],[0]
"To address this, we extend the model by using feature groups that collect together sets of features that sensibly compete.",4 Extensions to the Model,[0],[0]
"Each feature group is comprised of all features at the same level of specificity in the category hierarchy, which are therefore mutually exclusive, such as DOG, CAT, and BIRD (i.e., different kinds of animals).",4 Extensions to the Model,[0],[0]
"Instead of learning a single probability distribution over all features as the meaning of a word, the extended model learns a set of probability distributions for a word, one for each feature group (i.e., one per level of the hierarchy).",4 Extensions to the Model,[0],[0]
"Features within a group thereby compete for the probability mass associated with a word, but those from across groups (e.g., DALMATIAN and DOG) can freely co-occur without competing.
",4 Extensions to the Model,[0],[0]
"The model does not know a priori all the features in a group, but when presented with a newly observed feature, it can identify the appropriate group for it.",4 Extensions to the Model,[0],[0]
"In taking this approach, we assume the learner can distinguish the level of specificity of features perceived in the scene.",4 Extensions to the Model,[0],[0]
"For example, in the scene representations {DALMATIAN, DOG, ANIMAL} and {SIAMESE, CAT, ANIMAL}, the learner can recognize that DOG and CAT are at the same level of the hierarchy (kinds of animals) and that DALMATIAN and SIAMESE are at the same, more specific level in the hierarchy (finer-grained breeds of animals).",4 Extensions to the Model,[0],[0]
"Our assumption is that children (at this stage in their development) can identify a degree of similarity among concepts that enables them to recognize that Dalmatians and Siamese are distinguished by similar properties (such as fur color), which differ from more distinguishing properties at higher taxonomic levels (such as number of legs).",4 Extensions to the Model,[0],[0]
The model has no other prior knowledge of the category structure.,4 Extensions to the Model,[0],[0]
"For example, it is not built into the model that DALMATIAN is a type of DOG, only that it is more specific than DOG; any association between them would be learned from their pattern of co-occurrence with a word over time.
",4 Extensions to the Model,[0],[0]
"Note that, in contrast to the model of X&T, our model does not start with a full taxonomy (it does not know, for example, that Dalmatians and poodles are hyponyms of dogs) and it does not have built-in knowledge of similarities among concepts.",4 Extensions to the Model,[0],[0]
"Still, it encodes some taxonomic knowledge in the feature groups, and an important future direction will be to show that this knowledge is learnable from the input.
",4 Extensions to the Model,[0],[0]
Calculating Feature Group Probabilities.,4 Extensions to the Model,[0],[0]
"To appropriately split the probability mass within a feature group G (but not across feature groups), we use a new formulation of Eqn.",4 Extensions to the Model,[0],[0]
"(3) to update the meaning probabilities for fi ∈ G as follows:
Pt(fi|wj) = assoct(fi, wj) + γG∑ fm∈G assoct(fm, wj) + kGγG (5)
where kG is the expected number of features in G, and the smoothing factor γG reflects the prior belief in observing a feature f in G.2
With this new formulation, the probability of a feature fi previously unseen with wordwj now reduces to (cf. Eqn.",4 Extensions to the Model,[0],[0]
"(4)):
P ut (fi|wj) = γG∑
fm∈G assoct(fm, wj) + kGγG
(6)
for fi ∈ G.",4 Extensions to the Model,[0],[0]
"Note that the smoothing factor γG depends on G, and thus the openness of the word to be associated with new (previously unseen) features can vary depending on the feature group.
",4 Extensions to the Model,[0],[0]
This unseen probability is very important to the model’s generalization behaviour.,4 Extensions to the Model,[0],[0]
"Generalization involves the model accepting that a learned word can refer to objects not seen with it before: e.g., in the experiments here, we would expect that the learned meaning for fep after seeing three animals such as a dog, a penguin, and a sheep could also accommodate the meaning of a different animal such as a cat.",4 Extensions to the Model,[0],[0]
"This ability of the model to associate new meaning features with a word depends precisely on the unseen probability formulation: the higher the unseen probability for a feature and a word, the more the feature will be acceptable as a generalization of the word.
",4 Extensions to the Model,[0],[0]
Type-Token Effects on Generalization.,4 Extensions to the Model,[0],[0]
"The unseen probability is sensitive to how many instances of features from a group have already been
2Each feature group forms a Categorical distribution with kG categories (Cat(θ1, ..., θkG )), where the θi are drawn from a prior Dirichlet distribution Dir(γ1, ..., γ kG ) at time t = 0, and the θi are updated at time t to be the expected value of the posterior Dirichlet distribution, given in Eqn.",4 Extensions to the Model,[0],[0]
"(5) or Eqn. (6).
seen with a word wj :",4 Extensions to the Model,[0],[0]
"As the model observes more instances (tokens) of features from G with wj , the corresponding assoct score(s) increase, thereby increasing the denominator in Eqn.",4 Extensions to the Model,[0],[0]
(6) and decreasing P ut .,4 Extensions to the Model,[0],[0]
"Thus the tendency to generalize wj to more features in G – i.e., to accept additional features as part of the meaning of wj – will decrease as the model has more evidence of (observed) features in that group occurring with wj .
",4 Extensions to the Model,[0],[0]
"Generalization of a category to include new kinds of items is typically a function of both token and type frequency (e.g., Bybee, 1985; Croft and Cruse, 2004): a category with more diverse types is more easily extended to new cases.",4 Extensions to the Model,[0],[0]
"While the evolving association scores capture the effect of observing more feature tokens, our model as given does not distinguish the number of different types of features seen within a group (e.g., two DOGs vs. one DOG and one CAT).
",4 Extensions to the Model,[0],[0]
"We address this issue by having γG depend on the number of observed types of features in the group:
γ tG =",4 Extensions to the Model,[0],[0]
"γ 0 G × type(G, t)2 (7)
where type(G, t) is the number of different kinds of features seen in that group (e.g., DOG and CAT are two different feature types from the same group) up through time t. In this way, the P ut of a feature that occurs in a group with more observed feature types is higher than the P ut in a group with fewer observed types.
",4 Extensions to the Model,[0],[0]
Thus both the type frequency of features in G and their token frequency of co-occurrence with word wj will influence – the first positively and the second negatively – how readily wj can refer to objects with previously unseen features from G.,4 Extensions to the Model,[0],[0]
We model X&T’s behavioural experiments with our computational word learner as extended above.3,5 Experimental Set-up,[0],[0]
"Following X&T, we use a three-tiered category hierarchy, and the four training conditions and assessment of three types of test matches as described in Figure 1.
Training the model.",5 Experimental Set-up,[0],[0]
"In each condition, the model processes a sequence of 3 utterance-scene pairs, and updates Pt(fi|wj) after each pair using Eqns.",5 Experimental Set-up,[0],[0]
(5) and (6).,5 Experimental Set-up,[0],[0]
"The utterance-scene pair in each trial consists of the novel word coupled with the scene representation of a training object from the
3Link to our code/data: github.com/eringrant/ word_learning/tree/hypothesis-space.
category hierarchy.",5 Experimental Set-up,[0],[0]
"The object’s scene representation is given as a set of four features, each taken from one of four feature groups: one feature corresponding to each of the subordinate, basic, and superordinate levels of the hierarchy, and a unique “instance” feature, as shown in Table 1.",5 Experimental Set-up,[0],[0]
"(The “instance” feature is added to simulate the variations in the different objects of the same subordinate category in the X&T experiments.)
",5 Experimental Set-up,[0],[0]
Testing the model.,5 Experimental Set-up,[0],[0]
"After training on a novel word, in order to assess its level of generalization within the category hierarchy, we compare the model’s learned meaning of the word to test objects that constitute various types of matches to the training conditions: i.e., subordinate matches, basic-level matches, and superordinate matches.",5 Experimental Set-up,[0],[0]
"Table 2 gives an example of each type of match:
To assess whether the model generalizes the learned meaning of a word w to the various types of test matches, we first consider the probability of a test object Y at time t given the learned meaning of w:
Pt(Y |w) = ∏
yi∈Y Pt(yi|w) (8)
where yi are the features in Y , and Pt(yi|w) is calculated using Eqn.",5 Experimental Set-up,[0],[0]
"(5) for features yi observed with w during training, and using Eqn.",5 Experimental Set-up,[0],[0]
(6) for yi not observed with w. (Recall that Eqn.,5 Experimental Set-up,[0],[0]
(5) reduces to Eqn.,5 Experimental Set-up,[0],[0]
(6) when a feature has not been seen with the word.),5 Experimental Set-up,[0],[0]
"From Pt(Y |w), we subtract the predictive probability of the test object before the model has observed any data, P0(Y |w), which gives us its increase in preference attributable to the word
learning trials.4
Calculating Pt(Y |w)−P0(Y |w) is informative about one test object, but we need to measure generalization of the learned word to all the objects of a certain type of match – i.e., subordinate, basiclevel, or superordinate.",5 Experimental Set-up,[0],[0]
"We formulate the probability of generalization to a type of test match as the relative average increase in preference for test items of that type of match, using the ShepardLuce choice rule (Shepard, 1958; Luce, 1959):
Pgen(m|w) = avgY",5 Experimental Set-up,[0],[0]
∈m,5 Experimental Set-up,[0],[0]
[Pt(Y |w)-P0(Y |w)]∑ m′,5 Experimental Set-up,[0],[0]
avgY,5 Experimental Set-up,[0],[0]
"′∈m′ [Pt(Y ′|w)-P0(Y ′|w)]
where m is the set of test objects at a certain level of match, andm′ ranges over subordinate matches, basic-level matches, and superordinate matches.
",5 Experimental Set-up,[0],[0]
Using Pgen(m|w) to communicate our models results has the advantage of using the learned word meanings in a very direct way to assess the preference for the various types of test matches in the X&T experiments.,5 Experimental Set-up,[0],[0]
"However, the disadvantage is that this measure is not directly comparable to the reported figures from the human data, which are the percentage of test objects selected of a particular type of match.",5 Experimental Set-up,[0],[0]
"Hence, in presenting our results below, we focus on the general patterns of preferences indicated by the different measures.",5 Experimental Set-up,[0],[0]
Parameters.,5 Experimental Set-up,[0],[0]
"To model children, whom we assume to have no bias towards generalization to specific category levels, we equate all parameters k G and γ G across all feature groups, reflecting that all category levels are treated equivalently.",5 Experimental Set-up,[0],[0]
Here we use values of k G = 100 and initial values of γ G = 0.5 for all G as the “child” parameter settings.5,5 Experimental Set-up,[0],[0]
"In contrast, we assume that adults, through word learning experience, have accumulated biases that reflect observed differences in feature groups.",5 Experimental Set-up,[0],[0]
"More specifically, we assume that the probability of observing a new feature for a group G depends on the degree of specificity of that group: That is, over time, it is less likely to observe a completely new kind of animal, e.g., than a new breed of dog.",5 Experimental Set-up,[0],[0]
"We simulate these biases by us-
4P0(Y |w) = ∏G 1kG is the prior probability of any object instance, given parameters drawn from the Dirichlet prior, because Eqn. (6) yields the value 1
kG when all assoct scores are
0 – i.e., no features from G have been observed with the word.",5 Experimental Set-up,[0],[0]
"5To determine the parameters for the “child” learner, we examined a number of settings with equal parameter values for all the feature groups, and observed similar results in these settings.",5 Experimental Set-up,[0],[0]
"(We did not perform an exhaustive search over the parameter space.)
",5 Experimental Set-up,[0],[0]
"ing various values for the parameter γ G, which determines the prior probability of a word being observed with new (previously unseen) features in G (cf. Eqn.",5 Experimental Set-up,[0],[0]
(6)).,5 Experimental Set-up,[0],[0]
We assume that the expected number of features (k G) is the same across groups.,5 Experimental Set-up,[0],[0]
We perform a non-exhaustive search on the parameter space of γ G to select a set of values that yield the patterns of X&T’s adult experiments.,5 Experimental Set-up,[0],[0]
The “adult” parameter values are given in Table 3:6,5 Experimental Set-up,[0],[0]
We present results of the model using both child settings (Figure 3b) and adult settings (Figure 3a).,6 Experimental Results,[0],[0]
"Recall that these values do not correspond to the percentages reported in the human data; to evaluate the patterns of generalization, we look at the relative preference for the various types of test match.",6 Experimental Results,[0],[0]
"Note also that since the generalization probabilities sum to 1.0 within each of the 4 training conditions, we can only compare the pattern of generalization across conditions (and not the actual value of the probabilities).
",6 Experimental Results,[0],[0]
We discuss each of the child and adult sets of results in detail below.,6 Experimental Results,[0],[0]
"Recall that in the simulations of a child, we use equal values across all feature groups for the k G and initial γ G parameter settings, to reflect that the learner has no bias towards generalization to specific category levels.
",6.1 The Child Learner,[0],[0]
"Looking at the results in Figure 3b, we can see that the child learner generally replicates the patterns of results observed in X&T’s experiment on children (cf.",6.1 The Child Learner,[0],[0]
Figure 2b).,6.1 The Child Learner,[0],[0]
"Given multiple training items (the 3-subord., 3-basic, and 3-super. conditions), the model, like children, generalizes to the lowest level category in the hierarchy that is consistent with the training items, roughly equally preferring items from that category or lower, with slight preference for the lower categories.",6.1 The Child Learner,[0],[0]
"In contrast, after seeing a single training example (the
6For a certain range of such parameter settings – i.e., with gradually decreasing γ G , which determines the prior probability of a word being observed with new (previously unseen) features in G (cf. Eqn. (6)).",6.1 The Child Learner,[0],[0]
"for feature groups at successively higher levels in the hierarchy — the model produces similar results to the presented adult learner.
",6.1 The Child Learner,[0],[0]
"(a) Adult data:
1-ex. condition)",6.1 The Child Learner,[0],[0]
", the model shows some tendency to generalize to the basic-level, demonstrating a small but notable basic-level bias — e.g., the tendency to consider the word as referring to any dogs (but less so to other animals) after seeing just a single example of a particular kind of dog.",6.1 The Child Learner,[0],[0]
"As in children, the difference in the model between the preference for subordinate vs. basic-level matches is much smaller when trained on 1 instance as opposed to 3 subordinates.",6.1 The Child Learner,[0],[0]
"(In Figure 3b, compare the difference between the 1st bar [subord. match] and 2nd bar [basic match] of the 1-ex. training condition to that of the 3-subord.",6.1 The Child Learner,[0],[0]
"training condition.)
",6.1 The Child Learner,[0],[0]
"Interestingly, our child learner exhibits the observed basic-level bias in the absence of any difference in the model in how it treats different category levels.",6.1 The Child Learner,[0],[0]
"The observed pattern arises from a type/token frequency interaction of the kind often noted to influence generalization of linguistic categories (e.g., Bybee, 1985; Croft and Cruse, 2004): here, the interaction between the token frequency of word–feature pairs in the input and the type frequency of different features within a group of dependent features.",6.1 The Child Learner,[0],[0]
"For example, having seen 3 types of animals (“3 super.” condition), the model can readily accommodate that fep refers to another kind of animal, in contrast to the “3 basic” condition, where it has seen the same number of tokens but only a single feature type from the feature
group at that level (3 dogs).",6.1 The Child Learner,[0],[0]
"We can also clearly see the inverse impact of token frequencies on generalization: the more examples of a single subordinate type are seen, the less the model accepts that fep refers to a different kind of subordinate (the “3-subord.”",6.1 The Child Learner,[0],[0]
vs. “1-ex.” conditions).,6.1 The Child Learner,[0],[0]
"That is, with only 1 token of DALMATIAN, the model can generalize to other types of dogs more readily than when it has seen 3 tokens of DALMATIAN.
",6.1 The Child Learner,[0],[0]
"In general, interactions between the type and token frequencies of the different feature groups interact to yield the observed patterns in the model.",6.1 The Child Learner,[0],[0]
"These results indicate that properties of the input data coupled with the model’s handling of feature groups can account for children’s word generalization behaviour, without the need for an explicit basic-level bias.",6.1 The Child Learner,[0],[0]
"Adult participants in X&T exhibited a stronger tendency than children to generalize to the basiclevel category, especially after seeing a single exemplar.",6.2 The Adult Learner,[0],[0]
We explore whether the model can simulate an adult learner as well.,6.2 The Adult Learner,[0],[0]
"As discussed in Section 5, by varying the parameters γ G , we can incorporate biases towards different category levels that we assume an adult has learned.",6.2 The Adult Learner,[0],[0]
"More specifically, we set γ G to successively larger values for more specific feature groups G, to ensure successively greater generalization in lower levels of the hierarchy (see Table 3).",6.2 The Adult Learner,[0],[0]
"As shown in Figure 3a, our model (using such settings of the parameters) replicates the patterns of X&T’s adult experiments (cf.",6.2 The Adult Learner,[0],[0]
"Figure 2a), including a stronger basic-level bias than that shown by children.",6.2 The Adult Learner,[0],[0]
"That is, in the 1-ex. and 3-subord.",6.2 The Adult Learner,[0],[0]
"conditions, the difference between the 1st bar [subord.",6.2 The Adult Learner,[0],[0]
"match] and 2nd bar [basic match] is smaller for the adult settings of the model (Figure 3a) than for the child settings (Figure 3b), mimicking the stronger basic-level bias found in adults.",6.2 The Adult Learner,[0],[0]
Research shows that people’s degree of basic-level generalization depends on the overall category of the objects.,6.3 Variations in Basic-level Generalization,[0],[0]
"Specifically, Abbott et al. (2012) perform the same set of experiments as X&T on adults, exploring three additional superordinate categories (clothing, containers, and seats).",6.3 Variations in Basic-level Generalization,[0],[0]
"Their results are shown in Figure 4; for space reasons, we focus here on the training conditions with 1- example or 3-subordinates, which are the locus
of the basic-level effect.",6.3 Variations in Basic-level Generalization,[0],[0]
"The results show that people exhibit no basic-level generalization for containers, moderate generalization for clothing, and strong generalization for seats (compare Figures 4a, 4b and 4c).
",6.3 Variations in Basic-level Generalization,[0],[0]
"Interestingly, the computational experiments of Abbott et al. (2012) also reveal that the Bayesian model of X&T mimics varying levels of basiclevel generalization in the 1-example cases, but does not capture the differences that people exhibit across the categories in the 3-subordinate condition (compare “3 subord.”",6.3 Variations in Basic-level Generalization,[0],[0]
"in Figures 4 and 5): unlike people, here the X&T model does not exhibit basic-level generalization for any of the categories.
",6.3 Variations in Basic-level Generalization,[0],[0]
Abbott et al. (2012) note that a domain like containers may not follow a “natural taxonomy” in having a clear basic-level category.,6.3 Variations in Basic-level Generalization,[0],[0]
This suggestion is compatible with our view that a basic-level bias arises in response to the particular pattern of co-occurrence of features across the category hierarchy.,6.3 Variations in Basic-level Generalization,[0],[0]
"We looked more closely at the training stimuli of their experiment, and observe that the examples of the category “containers” (with the least basic-level generalization) vary greatly, while those of “clothing” and “seats” are less differentiated.",6.3 Variations in Basic-level Generalization,[0],[0]
"Examples from “containers” include a cigar box, trash can, and mailbox, whereas “seats” are restricted to different types of chair (such as a dining chair and an armchair; see Table 1 in Abbott et al. (2012)).
",6.3 Variations in Basic-level Generalization,[0],[0]
"Based on this observation, we hypothesize that people generalize less to a basic-level category when their mental representations for that category’s instances have more distinguishing features.",6.3 Variations in Basic-level Generalization,[0],[0]
"Specifically, we assume that people differentiate the given instances of the category “containers” more than those for “clothing” and “seats”.",6.3 Variations in Basic-level Generalization,[0],[0]
We model this difference in the granularity of representations by varying the number of feature groups used in representing an object.,6.3 Variations in Basic-level Generalization,[0],[0]
"Recall that in our earlier experiments, each object was represented as a set of features drawn from 4 different feature groups.",6.3 Variations in Basic-level Generalization,[0],[0]
We take this representation as the least fine-grained representation and use it for the category “seats”.,6.3 Variations in Basic-level Generalization,[0],[0]
"We assume that the objects from the categories “clothing” and “containers” (that exhibit less basic-level generalization) are represented with more feature groups (8 and 12, respectively).
",6.3 Variations in Basic-level Generalization,[0],[0]
"Figure 6 shows the results of running our model
on these three categories using the “adult” parameter settings.",6.3 Variations in Basic-level Generalization,[0],[0]
"As expected, the generalization to the basic-level category is high for the least distinguished category “seats”, moderate for the category “clothing”, and low for the most distinguished category “containers”.7
Our results suggest that the observed variation across categories in basic-level generalization could arise from differences in the granularity of representations of categories.",6.3 Variations in Basic-level Generalization,[0],[0]
"This is particularly interesting since the model of X&T, despite encoding an elaborated taxonomy, does not capture the observed behaviour across all training conditions.",6.3 Variations in Basic-level Generalization,[0],[0]
"A key challenge faced by children in vocabulary acquisition is learning which of many possible meanings is appropriate for a word, based largely on ambiguous situational evidence.",7 Conclusions,[0],[0]
"One aspect of this is what we term the “word generalization” problem, which refers to how children associate a word such as dog with a meaning at the appropriate category level in a taxonomy of objects, such as Dalmatians, dogs, or animals.
",7 Conclusions,[0],[0]
We present extensions to a cross-situational learner that enable the first computational study of word generalization that is integrated within a word learning model.,7 Conclusions,[0],[0]
"The model mimics child behavior found by Xu and Tenenbaum (2007): it shows a “basic-level” bias – a preference for word meanings that refer to basic-level objects (like dogs), in contrast to higher-level (animals) or lower-level (Dalmatians) categories – and does so
7Similar results obtain using “child” parameter settings, but (as expected) the basic-level generalization is lower.
under parameter settings that treat all levels of category the same in the model (i.e., with no built-in basic-level bias).",7 Conclusions,[0],[0]
"Other (unequal) parameter settings, which could reflect learned knowledge leading to differential treatment of categories, yield behavior that mimics that of adults, who show a stronger basic-level bias.",7 Conclusions,[0],[0]
"Moreover, similarly to people (Abbott et al., 2012), our model exhibits variations in generalization to the basic level for different types of objects, a behavior that the model of Xu and Tenenbaum (2007) does not fully replicate.
",7 Conclusions,[0],[0]
"Overall, the results of our model arise from the interaction of type and token frequencies of features in the input data, which impact the model’s evolving word representations.",7 Conclusions,[0],[0]
"This mechanism in the model captures the type-token influence often observed to underlie people’s generalization of linguistic categories – i.e., their linguistic productivity (e.g., Bybee, 1985; Croft and Cruse, 2004).
",7 Conclusions,[0],[0]
One shortcoming of the current model is its built-in ability to “detect” in the input that DOG and CAT features are more specific than ANIMAL features.,7 Conclusions,[0],[0]
"The next step is to consider how the model might learn these relationships from its evolving knowledge of co-occurring features.
",7 Conclusions,[0],[0]
"Finally, a similar problem to that of word generalization in humans arises in computational linguistics: how to appropriately generalize a set of concepts to an overarching concept that subsumes the set.",7 Conclusions,[0],[0]
"For example, this problem underlies one way to determine the selectional preferences of a verb: extract the set of nouns that occur as objects of the verb, map them to the concept nodes in a hierarchy such as WordNet, and then determine the best overarching WordNet category for capturing the salient properties of the object nouns overall (e.g., Li and Abe, 1998; Clark and Weir, 2001).",7 Conclusions,[0],[0]
An interesting future direction is to explore how an extension of our work can be applied to such problems in computational linguistics.,7 Conclusions,[0],[0]
We would like to thank Jackie Chi Kit Cheung and the anonymous reviewers for their valuable feedback.,8 Acknowledgements,[0],[0]
"We gratefully acknowledge the support of NSERC of Canada, and of an Ontario Graduate Scholarship to the first author.",8 Acknowledgements,[0],[0]
A key challenge in vocabulary acquisition is learning which of the many possible meanings is appropriate for a word.,abstractText,[0],[0]
"The word generalization problem refers to how children associate a word such as dog with a meaning at the appropriate category level in a taxonomy of objects, such as Dalmatians, dogs, or animals.",abstractText,[0],[0]
We present the first computational study of word generalization integrated within a word-learning model.,abstractText,[0],[0]
The model simulates child and adult patterns of word generalization in a word-learning task.,abstractText,[0],[0]
"These patterns arise due to the interaction of type and token frequencies in the input data, an influence often observed in people’s generalization of linguistic categories.",abstractText,[0],[0]
A Computational Cognitive Model of Novel Word Generalization,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 123–135 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1012",text,[0],[0]
"Neural machine translation (NMT) is an end-to-end approach to machine translation (Sutskever et al., 2014).",1 Introduction,[0],[0]
"The most successful approach to date encodes the source sentence with a bi-directional recurrent neural network (RNN) into a variable length representation and then generates the translation left-to-right with another RNN where both components interface via a soft-attention mechanism (Bahdanau et al., 2015; Luong et al., 2015a; Bradbury and Socher, 2016; Sennrich et al., 2016a).",1 Introduction,[0],[0]
"Recurrent networks are typically parameterized as long short term memory networks (LSTM; Hochreiter et al. 1997) or gated recurrent units (GRU; Cho et al. 2014), often with residual or skip connections (Wu et al., 2016; Zhou et al., 2016) to enable stacking of several layers (§2).
",1 Introduction,[0],[0]
"There have been several attempts to use convolutional encoder models for neural machine trans-
1The source code will be availabe at https://github.",1 Introduction,[0],[0]
"com/facebookresearch/fairseq
lation in the past",1 Introduction,[0],[0]
"but they were either only applied to rescoring n-best lists of classical systems (Kalchbrenner and Blunsom, 2013) or were not competitive to recurrent alternatives (Cho et al., 2014a).",1 Introduction,[0],[0]
This is despite several attractive properties of convolutional networks.,1 Introduction,[0],[0]
"For example, convolutional networks operate over a fixed-size window of the input sequence which enables the simultaneous computation of all features for a source sentence.",1 Introduction,[0],[0]
"This contrasts to RNNs which maintain a hidden state of the entire past that prevents parallel computation within a sequence.
",1 Introduction,[0],[0]
A succession of convolutional layers provides a shorter path to capture relationships between elements of a sequence compared to RNNs.2,1 Introduction,[0],[0]
This also eases learning because the resulting tree-structure applies a fixed number of non-linearities compared to a recurrent neural network for which the number of non-linearities vary depending on the time-step.,1 Introduction,[0],[0]
"Because processing is bottom-up, all words undergo the same number of transformations, whereas for RNNs the first word is over-processed and the last word is transformed only once.
",1 Introduction,[0],[0]
In this paper we show that an architecture based on convolutional layers is very competitive to recurrent encoders.,1 Introduction,[0],[0]
"We investigate simple average pooling as well as parameterized convolutions as an alternative to recurrent encoders and enable very deep convolutional encoders by using residual connections (He et al., 2015; §3).
",1 Introduction,[0],[0]
We experiment on several standard datasets and compare our approach to variants of recurrent encoders such as uni-directional and bi-directional LSTMs.,1 Introduction,[0],[0]
On WMT’16 English-Romanian translation we achieve accuracy that is very competitive to the current state-of-the-art result.,1 Introduction,[0],[0]
"We perform competitively on WMT’15 English-German, and nearly match the performance of the best WMT’14 English-French system based on a deep LSTM setup when comparing on a commonly used subset
2For kernel width k and sequence length n",1 Introduction,[0],[0]
"we require max ( 1, ⌈
n−1 k−1
⌉) forwards on a succession of stacked convo-
lutional layers compared to n forwards with an RNN.
123
of the training data (Zhou et al. 2016; §4, §5).",1 Introduction,[0],[0]
"The general architecture of the models in this work follows the encoder-decoder approach with soft attention first introduced in (Bahdanau et al., 2015).",2 Recurrent Neural Machine Translation,[0],[0]
"A source sentence x = (x1, . . .",2 Recurrent Neural Machine Translation,[0],[0]
", xm) of m words is processed by an encoder which outputs a sequence of states z = (z1. . . .",2 Recurrent Neural Machine Translation,[0],[0]
", zm).
",2 Recurrent Neural Machine Translation,[0],[0]
"The decoder is an RNN network that computes a new hidden state si+1 based on the previous state si, an embedding gi of the previous target language word yi, as well as a conditional input ci derived from the encoder output z. We use LSTMs (Hochreiter and Schmidhuber, 1997) for all decoder networks whose state si comprises of a cell vector and a hidden vector hi which is output by the LSTM at each time step.",2 Recurrent Neural Machine Translation,[0],[0]
"We input ci into the LSTM by concatenating it to gi.
",2 Recurrent Neural Machine Translation,[0],[0]
The translation model computes a distribution over the V possible target words yi+1 by transforming the LSTM output hi via a linear layer with weights,2 Recurrent Neural Machine Translation,[0],[0]
"Wo and bias bo:
p(yi+1|y1, . .",2 Recurrent Neural Machine Translation,[0],[0]
.,2 Recurrent Neural Machine Translation,[0],[0]
", yi,x) =",2 Recurrent Neural Machine Translation,[0],[0]
"softmax(Wohi+1 + bo)
",2 Recurrent Neural Machine Translation,[0],[0]
"The conditional input ci at time i is computed via a simple dot-product style attention mechanism (Luong et al., 2015a).",2 Recurrent Neural Machine Translation,[0],[0]
"Specifically, we transform the decoder hidden state hi by a linear layer with weights Wd and bd to match the size of the embedding of the previous target word gi and then sum the two representations to yield di.",2 Recurrent Neural Machine Translation,[0],[0]
Conditional input ci is a weighted sum of attention scores ai ∈ Rm and encoder outputs,2 Recurrent Neural Machine Translation,[0],[0]
z.,2 Recurrent Neural Machine Translation,[0],[0]
"The attention scores ai are determined by a dot product between hi with each zj , followed by a softmax over the source sequence:
di = Wdhi + bd + gi,
aij = exp
( dTi zj )",2 Recurrent Neural Machine Translation,[0],[0]
"∑m
t=1 exp ( dTi zt
) , ci = m∑
j=1
aijzj
In preliminary experiments, we did not find the MLP attention of (Bahdanau et al., 2015) to perform significantly better in terms of BLEU nor perplexity.",2 Recurrent Neural Machine Translation,[0],[0]
"However, we found the dot-product attention to be more favorable in terms of training and evaluation speed.
",2 Recurrent Neural Machine Translation,[0],[0]
"We use bi-directional LSTMs to implement recurrent encoders similar to (Zhou et al., 2016) which achieved some of the best WMT14 EnglishFrench results reported to date.",2 Recurrent Neural Machine Translation,[0],[0]
"First, each word
of the input sequence x is embedded in distributional space resulting in e = (e1, . . .",2 Recurrent Neural Machine Translation,[0],[0]
", em).",2 Recurrent Neural Machine Translation,[0],[0]
The embeddings are input to two stacks of uni-directional RNNs where the output of each layer is reversed before being fed into the next layer.,2 Recurrent Neural Machine Translation,[0],[0]
The first stack takes the original sequence while the second takes the reversed input sequence; the output of the second stack is reversed so that the final outputs of the stacks align.,2 Recurrent Neural Machine Translation,[0],[0]
"Finally, the top-level hidden states of the two stacks are concatenated and fed into a linear layer to yield z.",2 Recurrent Neural Machine Translation,[0],[0]
We denote this encoder architecture as BiLSTM.,2 Recurrent Neural Machine Translation,[0],[0]
"A simple baseline for non-recurrent encoders is the pooling model described in (Ranzato et al., 2015) which simply averages the embeddings of k consecutive words.",3.1 Pooling Encoder,[0],[0]
Averaging word embeddings does not convey positional information besides that the words in the input are somewhat close to each other.,3.1 Pooling Encoder,[0],[0]
"As a remedy, we add position embeddings to encode the absolute position of each source word within a sentence.",3.1 Pooling Encoder,[0],[0]
Each source embedding ej therefore contains a position embedding lj as well as the word embedding wj .,3.1 Pooling Encoder,[0],[0]
"Position embeddings have also been found helpful in memory networks for question-answering and language modeling (Sukhbaatar et al., 2015).",3.1 Pooling Encoder,[0],[0]
"Similar to the recurrent encoder (§2), the attention scores aij are computed from the pooled representations zj , however, the conditional input ci is a weighted sum of the embeddings ej , not zj , i.e.,
ej = wj",3.1 Pooling Encoder,[0],[0]
+ lj,3.1 Pooling Encoder,[0],[0]
", zj = 1
k
bk/2c∑
t=−bk/2c ej+t,
ci =
m∑
j=1
aijej
The input sequence is padded prior to pooling such that the encoder output matches the input length |z| = |x|.",3.1 Pooling Encoder,[0],[0]
"We set k to 5 in all experiments as (Ranzato et al., 2015).",3.1 Pooling Encoder,[0],[0]
A straightforward extension of pooling is to learn the kernel in a convolutional neural network (CNN).,3.2 Convolutional Encoder,[0],[0]
The encoder output zj contains information about a fixed-sized context depending on the kernel width k but the desired context width may vary.,3.2 Convolutional Encoder,[0],[0]
"This can
be addressed by stacking several layers of convolutions followed by non-linearities: additional layers increase the total context size while non-linearities can modulate the effective size of the context as needed.",3.2 Convolutional Encoder,[0],[0]
"For instance, stacking 5 convolutions with kernel width k = 3 results in an input field of 11 words, i.e., each output depends on 11 input words, and the non-linearities allow the encoder to exploit the full input field, or to concentrate on fewer words as needed.
To ease learning for deep encoders, we add residual connections from the input of each convolution to the output and then apply the non-linear activation function to the output (tanh; He et al., 2015); the non-linearities are therefore not ’bypassed’.",3.2 Convolutional Encoder,[0],[0]
Multi-layer CNNs are constructed by stacking several blocks on top of each other.,3.2 Convolutional Encoder,[0],[0]
"The CNNs do not contain pooling layers which are commonly used for down-sampling, i.e., the full source sequence length will be retained after the network has been applied.",3.2 Convolutional Encoder,[0],[0]
"Similar to the pooling model, the convolutional encoder uses position embeddings.
",3.2 Convolutional Encoder,[0],[0]
"The final encoder consists of two stacked convolutional networks (Figure 1): CNN-a produces the encoder output zj to compute the attention scores ai, while the conditional input ci to the decoder is computed by summing the outputs of CNN-c,
zj = CNN-a(e)j , ci = m∑
j=1
aij CNN-c(e)j .
",3.2 Convolutional Encoder,[0],[0]
"In practice, we found that two different CNNs resulted in better perplexity as well as BLEU compared to using a single one (§5.3).",3.2 Convolutional Encoder,[0],[0]
We also found this to perform better than directly summing the ei without transformation as for the pooling model.,3.2 Convolutional Encoder,[0],[0]
"There are several past attempts to use convolutional encoders for neural machine translation, however, to our knowledge none of them were able to match the performance of recurrent encoders.",3.3 Related Work,[0],[0]
"(Kalchbrenner and Blunsom, 2013) introduce a convolutional sentence encoder in which a multi-layer CNN generates a fixed sized embedding for a source sentence, or an n-gram representation followed by transposed convolutions for directly generating a per-token decoder input.",3.3 Related Work,[0],[0]
The latter requires the length of the translation prior to generation and both models were evaluated by rescoring the output of an existing translation system.,3.3 Related Work,[0],[0]
"(Cho et al., 2014a) propose a gated recursive CNN which is repeatedly applied until a fixed-size representation is ob-
tained but the recurrent encoder achieves higher accuracy.",3.3 Related Work,[0],[0]
"In follow-up work, the authors improved the model via a soft-attention mechanism but did not reconsider convolutional encoder models (Bahdanau et al., 2015).
",3.3 Related Work,[0],[0]
"Concurrently to our work, (Kalchbrenner et al., 2016) have introduced convolutional translation models without an explicit attention mechanism but their approach does not yet result in state-ofthe-art accuracy.",3.3 Related Work,[0],[0]
"(Lamb and Xie, 2016) also proposed a multi-layer CNN to generate a fixed-size encoder representation but their work lacks quantitative evaluation in terms of BLEU.",3.3 Related Work,[0],[0]
"Meng et al. (2015) and (Tu et al., 2015) applied convolutional models to score phrase-pairs of traditional phrasebased and dependency-based translation models.",3.3 Related Work,[0],[0]
"Convolutional architectures have also been successful in language modeling but so far failed to outperform LSTMs (Pham et al., 2016).",3.3 Related Work,[0],[0]
"We evaluate different encoders and ablate architectural choices on a small dataset from the GermanEnglish machine translation track of IWSLT 2014 (Cettolo et al., 2014) with a similar setting to (Ranzato et al., 2015).",4.1 Datasets,[0],[0]
"Unless otherwise stated, we restrict training sentences to have no more than 175 words; test sentences are not filtered.",4.1 Datasets,[0],[0]
This is a higher threshold compared to other publications but ensures proper training of the position embeddings for non-recurrent encoders; the length threshold did not significantly effect recurrent encoders.,4.1 Datasets,[0],[0]
"Length filtering results in 167K sentence pairs and we test on the concatenation of tst2010, tst2011, tst2012, tst2013 and dev2010 comprising 6948 sentence",4.1 Datasets,[0],[0]
pairs.3 Our final results are on three major WMT tasks: WMT’16 English-Romanian.,4.1 Datasets,[0],[0]
"We use the same data and pre-processing as (Sennrich et al., 2016a) and train on 2.8M sentence pairs.4",4.1 Datasets,[0],[0]
"Our model is word-based instead of relying on byte-pair encoding (Sennrich et al., 2016b).",4.1 Datasets,[0],[0]
We evaluate on newstest2016.,4.1 Datasets,[0],[0]
WMT’15 English-German.,4.1 Datasets,[0],[0]
"We use all available parallel training data, namely Europarl v7, Com-
3Different to the other datasets, we lowercase the training data and evaluate with case-insensitive BLEU.
",4.1 Datasets,[0],[0]
"4We followed the pre-processing of https: //github.com/rsennrich/wmt16-scripts/ blob/master/sample/preprocess.sh and added the back-translated data from http://data.statmt.org/ rsennrich/wmt16_backtranslations/en-ro.
mon Crawl and News Commentary v10 and apply the standard Moses tokenization to obtain 3.9M sentence pairs (Koehn et al., 2007).",4.1 Datasets,[0],[0]
We report results on newstest2015.,4.1 Datasets,[0],[0]
WMT’14 English-French.,4.1 Datasets,[0],[0]
"We use a commonly used subset of 12M sentence pairs (Schwenk, 2014), and remove sentences longer than 150 words.",4.1 Datasets,[0],[0]
This results in 10.7M sentence-pairs for training.,4.1 Datasets,[0],[0]
"Results are reported on ntst14.
",4.1 Datasets,[0],[0]
A small subset of the training data serves as validation set (5% for IWSLT’14 and 1% for WMT) for early stopping and learning rate annealing (§4.3).,4.1 Datasets,[0],[0]
"For IWSLT’14, we replace words that occur fewer than 3 times with a <unk> symbol, which results in a vocabulary of 24158 English and 35882 German word types.",4.1 Datasets,[0],[0]
"For WMT datasets, we retain 200K source and 80K target words.",4.1 Datasets,[0],[0]
"For English-French only, we set the target vocabulary to 30K types to be comparable with previous work.",4.1 Datasets,[0],[0]
We use 512 hidden units for both recurrent encoders and decoders.,4.2 Model parameters,[0],[0]
We reset the decoder hidden states to zero between sentences.,4.2 Model parameters,[0],[0]
"For the convolutional encoder, 512 hidden units are used for each layer in CNN-a, while layers in CNN-c contain 256 units each.",4.2 Model parameters,[0],[0]
"All embeddings, including the output produced by the decoder before the final linear layer, are of 256 dimensions.",4.2 Model parameters,[0],[0]
"On the WMT corpora, we find that we can improve the performance of the bidirectional LSTM models (BiLSTM) by using 512- dimensional word embeddings.
",4.2 Model parameters,[0],[0]
"Model weights are initialized from a uniform distribution within [−0.05, 0.05].",4.2 Model parameters,[0],[0]
"For convolutional layers, we use a uniform distribution of[ −kd−0.5, kd−0.5 ] , where k is the kernel width (we use 3 throughout this work) and d is the input size
for the first layer and the number of hidden units for subsequent layers (Collobert et al., 2011b).",4.2 Model parameters,[0],[0]
"For CNN-c, we transform the input and output with a linear layer each to match the smaller embedding size.",4.2 Model parameters,[0],[0]
The model parameters were tuned on IWSLT’14 and cross-validated on the larger WMT corpora.,4.2 Model parameters,[0],[0]
Recurrent models are trained with Adam as we found them to benefit from aggressive optimization.,4.3 Optimization,[0],[0]
"We use a step width of 3.125 · 10−4 and early stopping based on validation perplexity (Kingma and Ba, 2014).",4.3 Optimization,[0],[0]
"For non-recurrent encoders, we obtain best results with stochastic gradient descent (SGD) and annealing: we use a learning rate of 0.1 and once the validation perplexity stops improving, we reduce the learning rate by an order of magnitude each epoch until it falls below 10−4.
",4.3 Optimization,[0],[0]
"For all models, we use mini-batches of 32 sentences for IWSLT’14 and 64 for WMT.",4.3 Optimization,[0],[0]
We use truncated back-propagation through time to limit the length of target sequences per mini-batch to 25 words.,4.3 Optimization,[0],[0]
Gradients are normalized by the mini-batch size.,4.3 Optimization,[0],[0]
"We re-normalize the gradients if their norm exceeds 25 (Pascanu et al., 2013).",4.3 Optimization,[0],[0]
"Gradients of convolutional layers are scaled by sqrt(dim(input))−1 similar to (Collobert et al., 2011b).",4.3 Optimization,[0],[0]
"We use dropout on the embeddings and decoder outputs hi with a rate of 0.2 for IWSLT’14 and 0.1 for WMT (Srivastava et al., 2014).",4.3 Optimization,[0],[0]
"All models are implemented in Torch (Collobert et al., 2011a) and trained on a single GPU.",4.3 Optimization,[0],[0]
"We report accuracy of single systems by training several identical models with different ran-
dom seeds (5 for IWSLT’14, 3 for WMT) and pick the one with the best validation perplexity for final BLEU evaluation.",4.4 Evaluation,[0],[0]
Translations are generated by a beam search and we normalize log-likelihood scores by sentence length.,4.4 Evaluation,[0],[0]
"On IWSLT’14 we use a beam width of 10 and for WMT models we tune beam width and word penalty on a separate test set, that is newsdev2016 for WMT’16 English-Romanian, newstest2014 for WMT’15 English-German and ntst1213 for WMT’14 English-French.5",4.4 Evaluation,[0],[0]
"The word penalty adds a constant factor to log-likelihoods, except for the end-of-sentence token.
",4.4 Evaluation,[0],[0]
"Prior to scoring the generated translations against the respective references, we perform unknown word replacement based on attention scores (Jean et al., 2015).",4.4 Evaluation,[0],[0]
Unknown words are replaced by looking up the source word with the maximum attention score in a pre-computed dictionary.,4.4 Evaluation,[0],[0]
"If the dictionary contains no translation, then we simply copy the source word.",4.4 Evaluation,[0],[0]
"Dictionaries were extracted from the aligned training data that was aligned with fast align (Dyer et al., 2013).",4.4 Evaluation,[0],[0]
"Each source word is mapped to the target word it is most frequently aligned to.
",4.4 Evaluation,[0],[0]
For convolutional encoders with stacked CNN-c layers we noticed for some models that the attention maxima were consistently shifted by one word.,4.4 Evaluation,[0],[0]
We determine this per-model offset on the abovementioned development sets and correct for it.,4.4 Evaluation,[0],[0]
"Finally, we compute case-sensitive tokenized BLEU, except for WMT’16 English-Romanian where we use detokenized BLEU to be comparable with Sennrich et al. (2016a).6",4.4 Evaluation,[0],[0]
"We first compare recurrent and non-recurrent encoders in terms of perplexity and BLEU on IWSLT’14 with and without position embeddings (§3.1) and include a phrase-based system (Koehn et al., 2007).",5.1 Recurrent vs. Non-recurrent Encoders,[0],[0]
Table 1 shows that a single-layer convolutional model with position embeddings (Convolutional) can outperform both a uni-directional LSTM encoder (LSTM) as well as a bi-directional LSTM encoder (BiLSTM).,5.1 Recurrent vs. Non-recurrent Encoders,[0],[0]
"Next, we increase the depth of the convolutional encoder.",5.1 Recurrent vs. Non-recurrent Encoders,[0],[0]
"We choose a
5Specifically, we select a beam from {5, 10} and a word penalty from {0,−0.5,−1,−1.5}
6https://github.com/moses-smt/ mosesdecoder/blob/617e8c8ed1630fb1d1/ scripts/generic/{multi-bleu.perl, mteval-v13a.pl}
good setting by independently varying the number of layers in CNN-a and CNN-c between 1 and 10 and obtained best validation set perplexity with six layers for CNN-a and three layers for CNN-c. This configuration outperforms BiLSTM by 0.7 BLEU (Deep Convolutional 6/3).",5.1 Recurrent vs. Non-recurrent Encoders,[0],[0]
"We investigate depth in the convolutional encoder more in §5.3.
",5.1 Recurrent vs. Non-recurrent Encoders,[0],[0]
"Among recurrent encoders, the BiLSTM is 2.3 BLEU better than the uni-directional version.",5.1 Recurrent vs. Non-recurrent Encoders,[0],[0]
The simple pooling encoder which does not contain any parameters is only 1.3 BLEU lower than a unidirectional LSTM encoder and 3.6 BLEU lower than BiLSTM.,5.1 Recurrent vs. Non-recurrent Encoders,[0],[0]
The results without position embeddings (words) show that position information is crucial for convolutional encoders.,5.1 Recurrent vs. Non-recurrent Encoders,[0],[0]
"In particular for shallow models (Pooling and Convolutional), whereas deeper models are less effected.",5.1 Recurrent vs. Non-recurrent Encoders,[0],[0]
"Recurrent encoders do not benefit from explicit position information because this information can be naturally extracted through the sequential computation.
",5.1 Recurrent vs. Non-recurrent Encoders,[0],[0]
"When tuning model settings, we generally observe good correlation between perplexity and BLEU.",5.1 Recurrent vs. Non-recurrent Encoders,[0],[0]
"However, for convolutional encoders perplexity gains translate to smaller BLEU improvements compared to recurrent counterparts (Table 1).",5.1 Recurrent vs. Non-recurrent Encoders,[0],[0]
We observe a similar trend on larger datasets.,5.1 Recurrent vs. Non-recurrent Encoders,[0],[0]
"Next, we evaluate the BiLSTM encoder and the convolutional encoder architecture on three larger tasks and compare against previously published results.",5.2 Evaluation on WMT Corpora,[0],[0]
"On WMT’16 English-Romanian translation we compare to (Sennrich et al., 2016a), the winning single system entry for this language pair.",5.2 Evaluation on WMT Corpora,[0],[0]
"Their model consists of a bi-directional GRU encoder, a GRU decoder and MLP-based attention.
",5.2 Evaluation on WMT Corpora,[0],[0]
"They use byte pair encoding (BPE) to achieve openvocabulary translation and dropout in all components of the neural network to achieve 28.1 BLEU; we use the same pre-processing but no BPE (§4).
",5.2 Evaluation on WMT Corpora,[0],[0]
"The results (Table 2) show that a deep convolutional encoder can perform competitively to the state of the art on this dataset (Sennrich et al., 2016a).",5.2 Evaluation on WMT Corpora,[0],[0]
Our bi-directional LSTM encoder baseline is 0.6 BLEU lower than the state of the art but uses only 512 hidden units compared to 1024.,5.2 Evaluation on WMT Corpora,[0],[0]
A singlelayer convolutional encoder with embedding size 256 performs at 27.1 BLEU.,5.2 Evaluation on WMT Corpora,[0],[0]
"Increasing the number of convolutional layers to 8 in CNN-a and 4 in CNN-c achieves 27.8 BLEU which outperforms our baseline and is competitive to the state of the art.
",5.2 Evaluation on WMT Corpora,[0],[0]
"On WMT’15 English to German, we compare to a BiLSTM baseline and prior work: (Jean et al., 2015) introduce a large output vocabulary; the decoder of (Chung et al., 2016) operates on the character-level; (Yang et al., 2016) uses LSTMs instead of GRUs and feeds the conditional input to the output layer as well as to the decoder.
",5.2 Evaluation on WMT Corpora,[0],[0]
Our single-layer BiLSTM baseline is competitive to prior work and a two-layer BiLSTM encoder performs 0.6 BLEU better at 24.1 BLEU.,5.2 Evaluation on WMT Corpora,[0],[0]
"Previous work also used multi-layer setups, e.g., (Chung
et al., 2016) has two layers both in the encoder and the decoder with 1024 hidden units, and (Yang et al., 2016) use 1000 hidden units per LSTM.",5.2 Evaluation on WMT Corpora,[0],[0]
We use 512 hidden units for both LSTM and convolutional encoders.,5.2 Evaluation on WMT Corpora,[0],[0]
"Our convolutional model with either 8 or 15 layers in CNN-a outperform the BiLSTM encoder with both a single decoder layer or two decoder layers.
",5.2 Evaluation on WMT Corpora,[0],[0]
"Finally, we evaluate on the larger WMT’14 English-French corpus.",5.2 Evaluation on WMT Corpora,[0],[0]
On this dataset the recurrent architectures benefit from an additional layer both in the encoder and the decoder.,5.2 Evaluation on WMT Corpora,[0],[0]
"For a singlelayer decoder, a deep convolutional encoder outperforms the BiLSTM accuracy by 0.3 BLEU and for a two-layer decoder, our very deep convolutional encoder with up to 20 layers outperforms the BiLSTM by 0.4 BLEU.",5.2 Evaluation on WMT Corpora,[0],[0]
It has 40% fewer parameters than the BiLSTM due to the smaller embedding sizes.,5.2 Evaluation on WMT Corpora,[0],[0]
"We also outperform several previous systems, including the very deep encoder-decoder model proposed by (Luong et al., 2015a).",5.2 Evaluation on WMT Corpora,[0],[0]
"Our best result is just 0.2 BLEU below (Zhou et al., 2016) who use a very deep LSTM setup with a 9-layer encoder, a 7-layer decoder, shortcut connections and extensive regularization with dropout and L2 regularization.",5.2 Evaluation on WMT Corpora,[0],[0]
We next motivate our design of the convolutional encoder (§3.2).,5.3 Convolutional Encoder Architecture Details,[0],[0]
We use the smaller IWSLT’14 German-English setup without unknown word replacement to enable fast experimental turn-around.,5.3 Convolutional Encoder Architecture Details,[0],[0]
"BLEU results are averaged over three training runs initialized with different seeds.
",5.3 Convolutional Encoder Architecture Details,[0],[0]
Figure 2 shows accuracy for a different number of layers of both CNNs with and without residual connections.,5.3 Convolutional Encoder Architecture Details,[0],[0]
Our first observation is that computing the conditional input ci directly over embeddings e (line ”without CNN-c”) is already working well at 28.3 BLEU with a single CNN-a layer and at 29.1 BLEU for CNN-a with 7 layers (Figure 2a).,5.3 Convolutional Encoder Architecture Details,[0],[0]
Increasing the number of CNN-c layers is beneficial up to three layers and beyond this we did not observe further improvements.,5.3 Convolutional Encoder Architecture Details,[0],[0]
"Similarly, increasing the number of layers in CNN-a beyond six does not increase accuracy on this relatively small dataset.",5.3 Convolutional Encoder Architecture Details,[0],[0]
"In general, choosing two to three times as many layers in CNN-a as in CNN-c is a good rule of thumb.",5.3 Convolutional Encoder Architecture Details,[0],[0]
"Without residual connections, the model fails to utilize the increase in modeling power from additional layers, and performance drops significantly for deeper encoders (Figure 2b).
",5.3 Convolutional Encoder Architecture Details,[0],[0]
"Our convolutional architecture relies on two sets of networks, CNN-a for attention score computation ai and CNN-c for the conditional input ci to be fed to the decoder.",5.3 Convolutional Encoder Architecture Details,[0],[0]
"We found that using the same network for both tasks, similar to recurrent encoders, resulted in poor accuracy of 22.9 BLEU.",5.3 Convolutional Encoder Architecture Details,[0],[0]
"This compares to 28.5 BLEU for separate singlelayer networks, or 28.3 BLEU when aggregating embeddings for ci.",5.3 Convolutional Encoder Architecture Details,[0],[0]
Increasing the number of layers in the single network setup did not help.,5.3 Convolutional Encoder Architecture Details,[0],[0]
Figure 2(a) suggests that the attention weights (CNN-a) need to integrate information from a wide context which can be done with a deep stack.,5.3 Convolutional Encoder Architecture Details,[0],[0]
"At the same time, the vectors which are averaged (CNN-c) seem to benefit from a shallower, more local representation closer to the input words.",5.3 Convolutional Encoder Architecture Details,[0],[0]
"Two stacks are an easy way to achieve these contradicting requirements.
",5.3 Convolutional Encoder Architecture Details,[0],[0]
"In Appendix A we visualize attention scores and find that alignments for CNN encoders are less sharp compared to BiLSTMs, however, this does not affect the effectiveness of unknown word replacement once we adjust for shifted maxima.",5.3 Convolutional Encoder Architecture Details,[0],[0]
In Appendix B we investigate whether deep convolutional encoders are required for translating long sentences and observe that even relatively shallow encoders perform well on long sentences.,5.3 Convolutional Encoder Architecture Details,[0],[0]
"For training, we use the fast CuDNN LSTM implementation for layers without attention and experiment on IWSLT’14 with batch size 32.",5.4 Training and Generation Speed,[0],[0]
"The single-layer BiLSTM model trains at 4300 target words/second, while the 6/3 deep convolutional encoder compares at 6400 words/second on an NVidia Tesla M40 GPU.",5.4 Training and Generation Speed,[0],[0]
"We do not observe shorter overall training time since SGD converges slower than Adam which we use for BiLSTM models.
",5.4 Training and Generation Speed,[0],[0]
We measure generation speed on an Intel Haswell CPU clocked at 2.50GHz with a single thread for BLAS operations.,5.4 Training and Generation Speed,[0],[0]
"We use vocabulary selection which can speed up generation by up to a factor of ten at no cost in accuracy via making the time to compute the final output layer negligible (Mi et al., 2016; L’Hostis et al., 2016).",5.4 Training and Generation Speed,[0],[0]
This shifts the focus from the efficiency of the encoder to the efficiency of the decoder.,5.4 Training and Generation Speed,[0],[0]
On IWSLT’14 (Table 3a) the convolutional encoder increases the speed of the overall model by a factor of 1.35 compared to the BiLSTM encoder while improving accuracy by 0.7 BLEU.,5.4 Training and Generation Speed,[0],[0]
"In this setup both encoders models have the same hidden layer and embedding sizes.
",5.4 Training and Generation Speed,[0],[0]
On the larger WMT’15 English-German task (Table 3b) the convolutional encoder speeds up generation by 2.1 times compared to a two-layer BiLSTM.,5.4 Training and Generation Speed,[0],[0]
This corresponds to 231 source words/second with beam size 5.,5.4 Training and Generation Speed,[0],[0]
Our best model on this dataset generates 203 words/second but at slightly lower accuracy compared to the full vocabulary setting in Table 2.,5.4 Training and Generation Speed,[0],[0]
"The recurrent encoder uses larger embeddings than the convolutional encoder which were required for the models to match in accuracy.
",5.4 Training and Generation Speed,[0],[0]
The smaller embedding size is not the only reason for the speed-up.,5.4 Training and Generation Speed,[0],[0]
"In Table 3a (a), we compare a Conv 6/3 encoder and a BiLSTM with equal embedding sizes.",5.4 Training and Generation Speed,[0],[0]
The convolutional encoder is still 1.34x faster (at 0.7 higher BLEU),5.4 Training and Generation Speed,[0],[0]
although it requires roughly 1.6x as many FLOPs.,5.4 Training and Generation Speed,[0],[0]
"We believe that this is likely due to better cache locality for convolutional layers on CPUs: an LSTM with fused gates7 requires two big matrix multiplications with different weights as well as additions, multiplications and non-linearities for each source word, while the output of each convolutional layer can be computed as whole with a single matrix multiply.
",5.4 Training and Generation Speed,[0],[0]
"For comparison, the quantized deep LSTM7Our bi-directional LSTM implementation is based on torch rnnlib which uses fused LSTM gates (https://github.com/facebookresearch/ torch-rnnlib/) and which we consider an efficient implementation.
based model in (Wu et al., 2016) processes 106.4 words/second for English-French on a CPU with 88 cores and 358.8 words/second on a custom TPU chip.",5.4 Training and Generation Speed,[0],[0]
"The optimized RNNsearch model and C++ decoder described by (Junczys-Dowmunt et al., 2016) translates 265.3 words/s on a CPU with a similar vocabulary selection technique, computing 16 sentences in parallel, i.e., 16.6 words/s on a single core.",5.4 Training and Generation Speed,[0],[0]
We introduced a simple encoder model for neural machine translation based on convolutional networks.,6 Conclusion,[0],[0]
This approach is more parallelizable than recurrent networks and provides a shorter path to capture long-range dependencies in the source.,6 Conclusion,[0],[0]
"We find it essential to use source position embeddings as well as different CNNs for attention score computation and conditional input aggregation.
",6 Conclusion,[0],[0]
Our experiments show that convolutional encoders perform on par or better than baselines based on bi-directional LSTM encoders.,6 Conclusion,[0],[0]
"In comparison to other recent work, our deep convolutional encoder is competitive to the best published results to date (WMT’16 English-Romanian) which are obtained with significantly more complex models (WMT’14 English-French) or stem from improvements that are orthogonal to our work (WMT’15 English-German).",6 Conclusion,[0],[0]
"Our architecture also leads to
large generation speed improvements: translation models with our convolutional encoder can translate twice as fast as strong baselines with bi-directional recurrent encoders.
",6 Conclusion,[0],[0]
Future work includes better training to enable faster convergence with the convolutional encoder to better leverage the higher processing speed.,6 Conclusion,[0],[0]
Our fast architecture is interesting for character level encoders where the input is significantly longer than for words.,6 Conclusion,[0],[0]
"Also, we plan to investigate the effectiveness of our architecture on other sequence-tosequence tasks, e.g. summarization, constituency parsing, dialog modeling.",6 Conclusion,[0],[0]
"In Figure 4 and Figure 5, we plot attention scores for a sample WMT’15 English-German and WMT’14 English-French translation with BiLSTM and deep convolutional encoders.",A Alignment Visualization,[0],[0]
"The translation is on the x-axis and the source sentence on the y-axis.
",A Alignment Visualization,[0],[0]
The attention scores of the BiLSTM output are sharp but do not necessarily represent a correct alignment.,A Alignment Visualization,[0],[0]
"For CNN encoders the scores are less focused but still indicate an approximate source location, e.g., in Figure 4b, when moving the clause ”over 1,000 people were taken hostage” to the back of the translation.",A Alignment Visualization,[0],[0]
"For some models, attention maxima are consistently shifted by one token as both in Figure 4b and Figure 5b.
",A Alignment Visualization,[0],[0]
"Interestingly, convolutional encoders tend to focus on the last token (Figure 4b) or both the first and last tokens (Figure 5b).",A Alignment Visualization,[0],[0]
"Motivated by the hypothesis that the this may be due to the decoder depending on the length of the source sentence (which it cannot determine without position embeddings), we explicitly provided a distributed representation of the input length to the decoder and attention module.",A Alignment Visualization,[0],[0]
"However, this did not cause a change in attention patterns nor did it improve translation accuracy.
",A Alignment Visualization,[0],[0]
"B Performance by Sentence Length
One characteristic of our convolutional encoder architecture is that the context over which outputs are computed depends on the number of layers.",A Alignment Visualization,[0],[0]
"With bi-directional RNNs, every encoder output depends on the entire source sentence.",A Alignment Visualization,[0],[0]
"In Figure 3, we evaluate whether limited context affects the translation quality on longer sentences of WMT’15 English-German which often requires moving verbs over long distances.",A Alignment Visualization,[0],[0]
"We sort the newstest2015 test set by source length, partition it into 15 equallysized buckets, and compare the BLEU scores of models listed in Table 2 on a per-bucket basis.
",A Alignment Visualization,[0],[0]
There is no clear evidence for sub-par translations on sentences that are longer than the observable context per encoder output.,A Alignment Visualization,[0],[0]
We include a small encoder with a 6-layer CNN-c and a 3-layer CNN-a in the comparison which performs worse than a 2- layer BiLSTM (23.3 BLEU vs. 24.1).,A Alignment Visualization,[0],[0]
"With 6 convolutional layers at kernel width 3, each encoder output contains information of 13 adjacent source words.",A Alignment Visualization,[0],[0]
"Looking at the accuracy for sentences with 15 words or more, this relatively shallow CNN is either on par or better than the BiLSTM for 5 out of 10 buckets; the BiLSTM has access to the entire source context.",A Alignment Visualization,[0],[0]
Similar observations can be made for the deeper convolutional encoders.,A Alignment Visualization,[0],[0]
The prevalent approach to neural machine translation relies on bi-directional LSTMs to encode the source sentence.,abstractText,[0],[0]
We present a faster and simpler architecture based on a succession of convolutional layers.,abstractText,[0],[0]
This allows to encode the source sentence simultaneously compared to recurrent networks for which computation is constrained by temporal dependencies.,abstractText,[0],[0]
On WMT’16 EnglishRomanian translation we achieve competitive accuracy to the state-of-the-art and on WMT’15 English-German we outperform several recently published results.,abstractText,[0],[0]
Our models obtain almost the same accuracy as a very deep LSTM setup on WMT’14 English-French translation.,abstractText,[0],[0]
We speed up CPU decoding by more than two times at the same or higher accuracy as a strong bidirectional LSTM.1,abstractText,[0],[0]
A Convolutional Encoder Model for Neural Machine Translation,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 217–223 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-2034",text,[0],[0]
Understanding complex compositional language in context is a challenge shared by many tasks.,1 Introduction,[0],[0]
"Visual question answering and robot instruction systems require reasoning about sets of objects, quantities, comparisons, and spatial relations; for example, when instructing home assistance or assembly-line robots to manipulate objects in cluttered environments.",1 Introduction,[0],[0]
"This reasoning requires robust language understanding, and is only partially addressed by existing datasets.",1 Introduction,[0],[0]
"VQA (Antol et al., 2015), while lexically and visually diverse, includes relatively short sentences with limited coverage of such phenomena.",1 Introduction,[0],[0]
"CLEVR (Johnson et al., 2016) and SHAPES (Andreas et al., 2016b), in contrast, display complex compositional structure, but include only synthetic language.
",1 Introduction,[0],[0]
"In this paper, we introduce the Cornell Natural Language Visual Reasoning (NLVR) corpus and task.",1 Introduction,[0],[0]
"We define the binary prediction task of judging if a statement is true for an image or not, and introduce a corpus of annotated pairs of natural language statements and synthetic images.
",1 Introduction,[0],[0]
Collecting this kind of language presents two challenges.,1 Introduction,[0],[0]
"First, we must design environments to
support such descriptions.",1 Introduction,[0],[0]
We use simple visual environments displaying objects with complex visual relations between them.,1 Introduction,[0],[0]
Figure 1 shows two generated images.,1 Introduction,[0],[0]
The second challenge is eliciting complex descriptions displaying a range of syntactic and semantic phenomena.,1 Introduction,[0],[0]
We use a twostage crowdsourcing process.,1 Introduction,[0],[0]
"In the first stage, we present sets of images and ask workers to write descriptive statements that distinguish them.",1 Introduction,[0],[0]
"Using synthetic images with abstract shapes allows us to control the potential distinctions between them; for example, by discouraging simple statements about object existence.",1 Introduction,[0],[0]
"In the second stage, we ask workers to label the truth value for the sentences and images generated in the first stage.
",1 Introduction,[0],[0]
"Our data includes 92,244 sentence-image pairs with 3,962 unique sentences.",1 Introduction,[0],[0]
We include both images and the structured representation used to generate them to support research using both raw visual information and structured data.,1 Introduction,[0],[0]
Figure 1 shows two examples.,1 Introduction,[0],[0]
"To assess the difficulty of NLVR, we experiment with multiple baselines.",1 Introduction,[0],[0]
"The best model using images achieves an accuracy of 66.12, demonstrating remaining challenges
217
in the data.",1 Introduction,[0],[0]
"We also analyze the language in our data for presence of certain linguistic phenomena, and compare this analysis with related datasets.",1 Introduction,[0],[0]
The data and leaderboard are available at http://lic.nlp.cornell.edu/nlvr.,1 Introduction,[0],[0]
Several datasets have been created to study visual reasoning and language.,2 Related Work and Datasets,[0],[0]
"VQA (Antol et al., 2015; Zitnick and Parikh, 2013) includes crowdsourced questions and answers for photographs and abstract scenes, and has been studied extensively (e.g., Lu et al., 2016; Xu and Saenko, 2016; Zhou et al., 2015; Chen et al., 2015a; Andreas et al., 2016b,a; Ray et al., 2016).",2 Related Work and Datasets,[0],[0]
"In contrast to VQA, we use synthetic images and emphasize representing a broad range of language phenomena.",2 Related Work and Datasets,[0],[0]
"Our motivation is similar to that of SHAPES (Andreas et al., 2016b) and CLEVR (Johnson et al., 2016).",2 Related Work and Datasets,[0],[0]
Both datasets also use synthetic images and emphasize representing diverse spatial language.,2 Related Work and Datasets,[0],[0]
"However, unlike our approach, they include only automatically generated language.
",2 Related Work and Datasets,[0],[0]
"Visual reasoning has also been addressed in instructional language corpora (e.g., MacMahon et al., 2006; Chen and Mooney, 2011; Bisk et al., 2016), where executable instructions are grounded in manipulable environments.",2 Related Work and Datasets,[0],[0]
"The language we observe is similar to the type of language studied for understanding and generation of referential expressions (Mitchell et al., 2010; Matuszek et al., 2012; FitzGerald et al., 2013).
",2 Related Work and Datasets,[0],[0]
"Our task is related to caption generation, which has been studied extensively (e.g., Pedersoli et al., 2016; Carrara et al., 2016; Chen et al., 2016) with MSCOCO (Chen et al., 2015b) and Flickr30K (Young et al., 2014; Plummer et al., 2015).",2 Related Work and Datasets,[0],[0]
"In contrast to caption generation, our task does not require approximate metrics like BLEU.
",2 Related Work and Datasets,[0],[0]
"Several existing datasets focus on natural language querying of structured representations, including GeoQuery (Zelle, 1995) and WikiTables (Pasupat and Liang, 2015).",2 Related Work and Datasets,[0],[0]
Our work is complementary to these resources.,2 Related Work and Datasets,[0],[0]
"While our corpus was collected using images, we also provide structured representations.",2 Related Work and Datasets,[0],[0]
"When used with these representations, our corpus is similar to WikiTables, where questions are paired with small web tables.",2 Related Work and Datasets,[0],[0]
"Instead of web tables, we use object sets and focus on visual language.",2 Related Work and Datasets,[0],[0]
Statements in our data are grounded in synthetic images rendered from structured representations.,3 Task,[0],[0]
"Given an example, the task is to determine whether a statement is true or false for the image or structured representation.",3 Task,[0],[0]
"While we describe the image, the structured representation is equivalent.",3 Task,[0],[0]
We provide examples of the structured representation in the supplementary material.,3 Task,[0],[0]
Images are divided into three boxes.,3 Task,[0],[0]
Figure 1 shows two images.,3 Task,[0],[0]
Each box contains 1-8 objects.,3 Task,[0],[0]
"Each object has four properties: position (x/y coordinates), color (black, blue, yellow), shape (triangle, square, circle), and size (small, medium, large).",3 Task,[0],[0]
Objects within a box cannot overlap and must be contained entirely in the box.,3 Task,[0],[0]
We distinguish between images containing scattered objects and images containing only squares arranged in towers up to four blocks tall.,3 Task,[0],[0]
"The top image in Figure 1 is a tower example; the bottom is a scatter example.
",3 Task,[0],[0]
This design encourages compositional language with complex visual reasoning.,3 Task,[0],[0]
We divide the image into boxes to encourage set theoretic reasoning within and between boxes.,3 Task,[0],[0]
We also use a relatively limited number of values for each property.,3 Task,[0],[0]
"While a large number of properties provides a more diverse image, it is likely to result in descriptions that refer to property differences.",3 Task,[0],[0]
We find that the limited number of properties elicits descriptions with rich compositional structure.,3 Task,[0],[0]
"We generate images following the structure described in Section 3, and collect grounded natural language descriptions.",4 Data Collection,[0],[0]
Data is collected in two phases: sentence writing and validation.,4 Data Collection,[0],[0]
"During sentence writing, workers are asked to write contrasting descriptions about a set of images.",4 Data Collection,[0],[0]
"To validate sentences, the description is paired with each of the images.",4 Data Collection,[0],[0]
"We execute the collection process four times to collect training, development, and two test sets (Test-P and Test-U).",4 Data Collection,[0],[0]
We retain one test set as unreleased (Test-U).,4 Data Collection,[0],[0]
Generating Images We generate images by rendering a randomly sampled structured representation.,4 Data Collection,[0],[0]
The number of objects in each box and their properties are sampled uniformly.,4 Data Collection,[0],[0]
We generate an equal number of scatter and tower images.,4 Data Collection,[0],[0]
"To generate the sets of images presented to annotators, we generate two images independently, a third image by using the set of objects in the first im-
age and randomly re-shuffling them between the boxes, and a fourth image by re-shuffling the objects in the second image.",4 Data Collection,[0],[0]
"For images with towers, we constrain the re-shuffling to form towers.
",4 Data Collection,[0],[0]
Phase 1 – Sentence Writing Each writing task presents an annotator with four images.,4 Data Collection,[0],[0]
"Figure 2 shows the sentence writing prompt, including the set of constraints, which is shown for all writing tasks.",4 Data Collection,[0],[0]
"The constraints force the worker to contrast two pairs by referring to similarities and differences between the images, but not to refer to the position of the image in the prompt, or of each box in each image.",4 Data Collection,[0],[0]
"These constraints are placed to elicit more set-theoretic language, and to allow us to divide the result of each task into four examples, pairing the annotator’s sentence with each of the four images it was presented with.
",4 Data Collection,[0],[0]
"Phase 2 – Validation In the second phase, we pair each sentence with the four images used to generate it.",4 Data Collection,[0],[0]
"We re-label all sentence-image pairs as true or false, correcting for any violations of the constraints in the first phase.",4 Data Collection,[0],[0]
We do not use the original position of the image as any part of the final label to neutralize any ordering effect.,4 Data Collection,[0],[0]
"In practice, 8.2% of examples had a different label than inferred from their original position in
the first phase.",4 Data Collection,[0],[0]
"During validation, boxes are randomly permuted to ensure the last constraint was followed.",4 Data Collection,[0],[0]
"We allow workers to annotate a sentence as nonsensical with regard to the image, and instruct annotators to ignore grammar errors.",4 Data Collection,[0],[0]
Post-processing We prune pairs when their majority class is nonsensical.,4 Data Collection,[0],[0]
"When collecting multiple annotations for a pair, we prune pairs if the gap between the classes is less than two votes.",4 Data Collection,[0],[0]
"We use the crowdsourcing platform Upwork,1 and select ten annotators using a small set of example questions.",5 Data Statistics and Analysis,[0],[0]
"We collect 3,974 task instances and 28,723 total validation judgments at a total cost of $5,526.",5 Data Statistics and Analysis,[0],[0]
"From these 3,974 task instances we extract 15,896 sentence-image pairs.",5 Data Statistics and Analysis,[0],[0]
We prune 522 pairs in post-processing.,5 Data Statistics and Analysis,[0],[0]
For the training set we collect a single validation annotation for each sentence-image pair; for the rest of the data we collect five annotations each.,5 Data Statistics and Analysis,[0],[0]
"Finally, we generate six sentence-image pairs from each sample by permuting the boxes.",5 Data Statistics and Analysis,[0],[0]
The validation step ensures this permutation does not change the label.,5 Data Statistics and Analysis,[0],[0]
"Table 1 shows the number of sentences and pairs, including permutations, for each split.
",5 Data Statistics and Analysis,[0],[0]
We merge the development and test splits to calculate agreement statistics.,5 Data Statistics and Analysis,[0],[0]
"We calculate Krippendorf’s α and Fleiss’ κ (Cocos et al., 2015) on both the full and pruned datasets.",5 Data Statistics and Analysis,[0],[0]
"To calculate Fleiss’ κ, we randomly permute the five annotations to be assigned to five “raters” and compute average kappa from 100 iterations.",5 Data Statistics and Analysis,[0],[0]
"Before pruning, we observe α = 0.768 and κ = 0.709, indicating substantial agreement (Landis and Koch, 1977).",5 Data Statistics and Analysis,[0],[0]
"Pruning improves agreement to α = 0.831 (indicating almost-perfect agreement) and κ = 0.808.
",5 Data Statistics and Analysis,[0],[0]
We analyze 200 development sentences to identify the distribution of semantic phenomena and syntactic ambiguity (Table 2).,5 Data Statistics and Analysis,[0],[0]
"For comparison, we apply this analysis to 200 abstract-image and 200 real-image sentences from VQA (Antol et al., 2015).",5 Data Statistics and Analysis,[0],[0]
The difference in the distribution illustrates the complexity of our data.,5 Data Statistics and Analysis,[0],[0]
"The mean sentence
1http://upwork.com
length in our data is 11.22 tokens and the vocabulary size is 262.",5 Data Statistics and Analysis,[0],[0]
"In Figure 3, we compare sentence length distribution to VQA, MSCOCO (Chen et al., 2015b), and CLEVR (Johnson et al., 2016).",5 Data Statistics and Analysis,[0],[0]
Our sentences are generally longer than VQA and more similar in length to MSCOCO.,5 Data Statistics and Analysis,[0],[0]
"However, our task is more similar to VQA, where context is used to understand language, rather than to generate.",5 Data Statistics and Analysis,[0],[0]
We evaluate multiple methods on the rendered images and structured representations.,6 Methods,[0],[0]
"Hyperparameters and initialization details are described in the supplementary material.
",6 Methods,[0],[0]
2We say a statement or question uses presupposition when it assumes the truth value of some proposition in order for its entire truth value to be defined.,6 Methods,[0],[0]
"In this example, an image which does not have three black items will have no defined truth value for this statement.",6 Methods,[0],[0]
We use image- and text-only models to measure how well biases in our data can be used to solve the task.,6.1 Majority Class and Single Modality,[0],[0]
"If the model is able to do well on the text- or image-only baselines, this implies our data does not require the two modalities.",6.1 Majority Class and Single Modality,[0],[0]
"Antol et al. (2015) performed a similar analysis of VQA with the questions only to gauge how and if background knowledge of the domain could aid performance.
",6.1 Majority Class and Single Modality,[0],[0]
"Majority Assign the most common label (true) to all examples.
",6.1 Majority Class and Single Modality,[0],[0]
"Text Only Encode the sentence with a recurrent neural network (RNN; Elman, 1990) with long short-term memory units (LSTM; Hochreiter and Schmidhuber, 1997) and a binary softmax computed from the final output.
",6.1 Majority Class and Single Modality,[0],[0]
Image Only Encode the image with a convolutional neural network (CNN) with three layers.,6.1 Majority Class and Single Modality,[0],[0]
The CNN output is used by a three-layer perceptron with a softmax on the final layer.3,6.1 Majority Class and Single Modality,[0],[0]
"We use the structured representations described in Sections 3 and 4.
3We also experimented using the ImageNet-trained Inception v4 model (Szegedy et al., 2017), but found it did not improve performance, possibly due to the difference between our images and ImageNet.
",6.2 Structured Representation,[0],[0]
MaxEnt Train a MaxEnt classifier.,6.2 Structured Representation,[0],[0]
We use the text and structured representation to compute property- and count-based features.,6.2 Structured Representation,[0],[0]
"Propertybased features trigger when some property (e.g., an object is touching a wall) is true in the structure.",6.2 Structured Representation,[0],[0]
"We create features by crossing triggered properties with each n-grams from the sentence, up to n = 6.",6.2 Structured Representation,[0],[0]
"Count-based features trigger when a count we observe in the image (e.g., the number of black triangles) is present in the sentence.",6.2 Structured Representation,[0],[0]
"We generate features combining the type of item counted (e.g., black triangles) with the n-grams surrounding the count in the sentence, up to n = 6.",6.2 Structured Representation,[0],[0]
We provide details in the supplementary material.,6.2 Structured Representation,[0],[0]
MLP Train a single-layer perceptron with a softmax layer.,6.2 Structured Representation,[0],[0]
The input to the perceptron is the mean of the feature embeddings.,6.2 Structured Representation,[0],[0]
We use the same feature set as the MaxEnt model.,6.2 Structured Representation,[0],[0]
"Image Features+RNN Compute features from the structure representation only, and encode the text with an LSTM RNN.",6.2 Structured Representation,[0],[0]
"The two representations are concatenated, and used as input to a two-layer perceptron and a softmax layer.",6.2 Structured Representation,[0],[0]
CNN+RNN Concatenate the CNN and RNN representations (Section 6.1) and apply a multilayer perceptron with a softmax.,6.3 Image Representation,[0],[0]
NMN,6.3 Image Representation,[0],[0]
The neural module networks approach of Andreas et al. (2016b).,6.3 Image Representation,[0],[0]
"We experiment with the default maximum leaves of two, and with allowing for more expressive representations with a maximum leaves of five.",6.3 Image Representation,[0],[0]
"We observe higher development accuracy with the trees using maximum leaves of five (63.06% vs. 62.4% with the default of two), which we use in our experiments.",6.3 Image Representation,[0],[0]
We run each experiment ten times and report mean accuracy as well as standard deviation for randomly initialized models.,7 Results,[0],[0]
"Table 3 shows our re-
sults.",7 Results,[0],[0]
NMN is the best performing model using images.,7 Results,[0],[0]
Table 2 shows the NMN accuracy for each category in our qualitative analysis sample.,7 Results,[0],[0]
"While the number of sentences in some categories is relatively small, we observe a higher number of failures in sentences that include negations and coordinations.",7 Results,[0],[0]
"For models using the structured representation, the MaxEnt model provides the best performance.",7 Results,[0],[0]
"When ablating count-based features from the MaxEnt model, development accuracy decreases from 68.04 to 57.7.",7 Results,[0],[0]
This indicates counting is an important aspect of the problem.,7 Results,[0],[0]
We introduce the Cornell Natural Language Visual Reasoning dataset and task.,8 Discussion,[0],[0]
The data includes complex compositional language grounded in images and structured representations.,8 Discussion,[0],[0]
The task requires addressing challenges in visual and set-theoretic reasoning.,8 Discussion,[0],[0]
"We experiment with multiple systems and, in general, observe relatively low performance.",8 Discussion,[0],[0]
"Together with our qualitative analysis, this exemplifies the complexity of the data.",8 Discussion,[0],[0]
"We release our annotated training and development sets, and create two test sets.",8 Discussion,[0],[0]
The public test set will be released along with its annotation.,8 Discussion,[0],[0]
Computing results on the unreleased test data will require submitting trained models.,8 Discussion,[0],[0]
Procedures for submitting models and the task leader board are available at http://lic.nlp.cornell.edu/nlvr.,8 Discussion,[0],[0]
"This research was supported by a Microsoft Research Women’s Fellowship, a Google Faculty Award, and an Amazon Web Services Cloud Credits for Research Grant.",Acknowledgments,[0],[0]
We thank the Cornell and University of Washington NLP groups for their support and helpful comments.,Acknowledgments,[0],[0]
We thank the anonymous reviewers for their feedback.,Acknowledgments,[0],[0]
"We present a new visual reasoning language dataset, containing 92,244 pairs of examples of natural statements grounded in synthetic images with 3,962 unique sentences.",abstractText,[0],[0]
"We describe a method of crowdsourcing linguistically-diverse data, and present an analysis of our data.",abstractText,[0],[0]
"The data demonstrates a broad set of linguistic phenomena, requiring visual and set-theoretic reasoning.",abstractText,[0],[0]
"We experiment with various models, and show the data presents a strong challenge for future research.",abstractText,[0],[0]
A Corpus of Natural Language for Visual Reasoning,title,[0],[0]
"Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 403–408, Baltimore, Maryland, USA, June 23-25 2014. c©2014 Association for Computational Linguistics",text,[0],[0]
It is important for authors and speakers to find the appropriate “pitch” to convey a desired message to the public.,1 Introduction,[0],[0]
"Indeed, sometimes heated debates can arise around the choice of statement strength.",1 Introduction,[0],[0]
"For instance, on March 1, 2014, an attack at Kunming’s railway station left 29 people dead and more than 140 others injured.1 In the aftermath, Chinese media accused Western media of “softpedaling the attack and failing to state clearly that it was an act of terrorism”.2",1 Introduction,[0],[0]
"In particular, regarding the statement by the US embassy that referred to this incident as the “terrible and senseless act of violence in Kunming”, a Weibo user posted “If you say that the Kunming attack is a ‘terrible and
1http://en.wikipedia.org/wiki/2014_ Kunming_attack
2http://sinosphere.blogs.nytimes.",1 Introduction,[0],[0]
com/2014/03/03,1 Introduction,[0],[0]
"/u-n-security-councilcondemns-terrorist-attack-in-kunming/
senseless act of violence’, then the 9/11 attack can be called a ‘regrettable traffic incident”’.3
",1 Introduction,[0],[0]
"This example is striking but not an isolated case, for settings in which one party is trying to convince another are pervasive; scenarios range from court trials to conference submissions.",1 Introduction,[0],[0]
"Since the strength and scope of an argument can be a crucial factor in its success, it is important to understand the effects of statement strength in communication.
",1 Introduction,[0],[0]
A first step towards addressing this question is to be able to distinguish between strong and weak statements.,1 Introduction,[0],[0]
"As strength is inherently relative, it is natural to look at revisions that change statement strength, which we refer to as “strength changes”.",1 Introduction,[0],[0]
"Though careful and repeated revisions are presumably ubiquitous in politics, legal systems, and journalism, it is not clear how to collect them; on the other hand, revisions to research papers may be more accessible, and many researchers spend significant time on editing to convey the right message regarding the strength of a project’s contributions, novelty, and limitations.",1 Introduction,[0],[0]
"Indeed, statement strength in science communication matters to writers: understating contributions can affect whether people recognize the true importance of the work; at the same time, overclaiming can cause papers to be rejected.
",1 Introduction,[0],[0]
"With the increasing popularity of e-print services such as the arXiv4, strength changes in scientific papers are becoming more readily available.",1 Introduction,[0],[0]
"Since the arXiv started in 1991, it has become “the standard repository for new papers in mathematics, physics, statistics, computer science, biology, and other disciplines” (Krantz, 2007).",1 Introduction,[0],[0]
An intriguing observation is that many researchers submit multiple versions of the same paper on arXiv.,1 Introduction,[0],[0]
"For instance, among the 70K papers submitted in
3http://www.huffingtonpost.co.uk/2014/ 03/03/china-kunming-911_n_4888748.html
4http://arxiv.org/
403
2011, almost 40% (27.7K) have multiple versions.",1 Introduction,[0],[0]
"Many differences between these versions constitute a source of valid and motivated strength differences, as can be seen from the sentential revisions in Table 1.",1 Introduction,[0],[0]
Pair 1 makes the contribution seem more impressive by replacing “studied” with “proposed”.,1 Introduction,[0],[0]
Pair 2 downgrades “human communication activity” to “mobile phone communication”.,1 Introduction,[0],[0]
Pair 3 removes “significantly” and the emphasis on “same privacy guarantees”.,1 Introduction,[0],[0]
"Pair 4 shows an insertion of hedging, a relatively well-known type of strength reduction.",1 Introduction,[0],[0]
"Pair 5 is an interesting case that shows the complexity of this problem: on the one hand, S2 claims that something is “inefficient”, which is an absolute statement, compared to “efficiency loss” in S1, where the possibility of efficiency still exists; on the other hand, S1 employs an active tone that emphasizes a causal relationship.
",1 Introduction,[0],[0]
The main contribution of this work is to provide the first large-scale corpus of sentence-level revisions for studying a broad range of variations in statement strength.,1 Introduction,[0],[0]
We collected labels for a subset of these revisions.,1 Introduction,[0],[0]
"Given the possibility of all kinds of disagreement, the fair level of agreement (Fleiss’ Kappa) among our annotators was decent.",1 Introduction,[0],[0]
"But in some cases, the labels differed from our expectations, indicating that the general public can interpret the strength of scientific statements differently from researchers.",1 Introduction,[0],[0]
The participants’ comments may further shed light on science communication and point to better ways to define and understand strength differences.,1 Introduction,[0],[0]
"Hedging, which can lead to strength differences, has received some attention in the study of science
communication (Salager-Meyer, 2011; Lewin, 1998; Hyland, 1998; Myers, 1990).",2 Related Work and Data,[0],[0]
"The CoNLL 2010 Shared Task was devoted to hedge detection (Farkas et al., 2010).",2 Related Work and Data,[0],[0]
"Hedge detection was also used to understand scientific framing in debates over genetically-modified organisms in food (Choi et al., 2012).
",2 Related Work and Data,[0],[0]
"Revisions on Wikipedia have been shown useful for various applications, including spelling correction (Zesch, 2012), sentence compression (Yamangil and Nelken, 2008), text simplification (Yatskar et al., 2010), paraphrasing (Max and Wisniewski, 2010), and textual entailment (Zanzotto and Pennacchiotti, 2010).",2 Related Work and Data,[0],[0]
"But none of the categories of Wikipedia revisions previously examined (Daxenberger and Gurevych, 2013; Bronner and Monz, 2012; Mola-Velasco, 2011; Potthast et al., 2008; Daxenberger and Gurevych, 2012) relate to statement strength.",2 Related Work and Data,[0],[0]
"After all, the objective of editing on Wikipedia is to present neutral and objective articles.
",2 Related Work and Data,[0],[0]
"Public datasets of science communication are available, such as the ACL Anthology,5 collections of NIPS papers,6 and so on.",2 Related Work and Data,[0],[0]
These datasets are useful for understanding the progress of disciplines or the evolution of topics.,2 Related Work and Data,[0],[0]
But the lack of edit histories or revisions makes them not immediately suitable for studying strength differences.,2 Related Work and Data,[0],[0]
"Recently, there have been experiments with open peer review.7",2 Related Work and Data,[0],[0]
"Records from open reviewing can provide additional insights into the revision process once enough data is collected.
",2 Related Work and Data,[0],[0]
5http://aclweb.org/anthology/ 6http://nips.djvuzone.org/txt.html 7http://openreview.net,2 Related Work and Data,[0],[0]
Our main dataset was constructed from all papers submitted in 2011 on the arXiv.,3 Dataset Description,[0],[0]
We first extracted the textual content from papers that have multiple versions of tex source files.,3 Dataset Description,[0],[0]
All mathematical environments were ignored.,3 Dataset Description,[0],[0]
"Section titles were not included in the final texts but are used in alignment.
",3 Dataset Description,[0],[0]
"In order to align the first version and the final version of the same paper, we first did macro alignment of paper sections based on section titles.",3 Dataset Description,[0],[0]
"Then, for micro alignment of sentences, we employed a dynamic programming algorithm similar to that of Barzilay and Elhadad (2003).",3 Dataset Description,[0],[0]
"Instead of cosine similarity, we used an idf-weighted longestcommon-subsequence algorithm to define the similarity between two sentences, because changes in word ordering can also be interesting.",3 Dataset Description,[0],[0]
"Formally, the similarity score between sentence i and sentence j is defined as
Simpi, jq “ Weighted-LCSpSi, Sjq maxp ř
wPSi idfpwq, ř wPSj idfpwqq ,
where Si and Sj refer to sentence i and sentence j. Since it is likely that a new version adds or deletes a large sequence of sentences, we did not impose a skip penalty.",3 Dataset Description,[0],[0]
"We set the mismatch penalty to 0.1.8
In the end, there are 23K papers where the first version was different from the last version.9",3 Dataset Description,[0],[0]
"We
8We did not allow cross matching (",3 Dataset Description,[0],[0]
"i.e., i Ñ j´1, i´1 Ñ j), since we thought matching this case as pi ´ 1",3 Dataset Description,[0],[0]
", iq Ñ j or i Ñ pj, j ´ 1q can provide context for annotation purposes.",3 Dataset Description,[0],[0]
"But in the end, we focused on labeling very similar pairs.",3 Dataset Description,[0],[0]
"This decision had little effect.
9",3 Dataset Description,[0],[0]
"This differs from the number in Section 1 because articles may not have the tex source available, or the differences between versions may be in non-textual content.
categorize sentential revisions into the following three types:
• Deletion: we cannot find a match in the final version.
",3 Dataset Description,[0],[0]
"• Typo: all sequences in a pair of matched sentences are typos, where a sequence-level typo is one where the edit distance between the matched sequences is less than three.
",3 Dataset Description,[0],[0]
• Rewrite: matched sentences that are not typos.,3 Dataset Description,[0],[0]
"This type is the focus of this study.
",3 Dataset Description,[0],[0]
What kinds of changes are being made?,3 Dataset Description,[0],[0]
"One might initially think that typo fixes represent a large proportion of revisions, but this is not correct, as shown in Figure 1a.",3 Dataset Description,[0],[0]
"Deletions represent a substantial fraction, especially in the middle section of a paper.",3 Dataset Description,[0],[0]
"But it is clear that the majority of changes are rewrites; thus revisions on the arXiv indeed provide a great source for potential strength differences.
",3 Dataset Description,[0],[0]
Who makes changes?,3 Dataset Description,[0],[0]
Figure 1b shows that the Math subarchive makes the largest number of changes.,3 Dataset Description,[0],[0]
This is consistent with the mathematics community’s custom of using the arXiv to get findings out early.,3 Dataset Description,[0],[0]
"In terms of changes per sentence (Figure 1c), statistics and quantitative studies are the top subareas.
",3 Dataset Description,[0],[0]
"Further, Figure 2 shows the effect of the number of authors.",3 Dataset Description,[0],[0]
"It is interesting that both in terms of sheer number and percentage, single-authored papers have the most changes.",3 Dataset Description,[0],[0]
"This could be because a single author enjoys greater freedom and has stronger motivation to make changes, or because multiple authors tend to submit a more polished initial version.",3 Dataset Description,[0],[0]
"This echoes the finding in Posner
and Baecker (1992) that the collaborative writing process differs considerably from individual writing.",3 Dataset Description,[0],[0]
"Also, more than 25% of the first versions are changed, which again shows that substantive edits are being made in these resubmissions.",3 Dataset Description,[0],[0]
"In order to study statement strength, reliable strength-difference labels are needed.",4 Annotating Strength Differences,[0],[0]
"In this section, we describe how we tried to define strength differences, compiled labeling instructions, and gathered labels using Amazon Mechanical Turk.
Label definition and collection procedure.",4 Annotating Strength Differences,[0],[0]
We focused on matched sentences from abstracts and introductions to maximize the proportion of strength differences (as opposed to factual/no strength changes).,4 Annotating Strength Differences,[0],[0]
We required pairs to have similarity score larger than 0.5 in our labeling task to make pairs more comparable.,4 Annotating Strength Differences,[0],[0]
"We also replaced
all math environments with “[MATH]”.10 We obtained 108K pairs that satisfy the above conditions, available at http://chenhaot.com/ pages/statement-strength.html.",4 Annotating Strength Differences,[0],[0]
"To create the pool of pairs for labeling, we randomly sampled 1000 pairs and then removed pairs that we thought were processing errors.
",4 Annotating Strength Differences,[0],[0]
We used Amazon Mechanical Turk.,4 Annotating Strength Differences,[0],[0]
It may initially seem surprising to have annotations of technical statements not done by domain experts; we did this intentionally because it is common to communicate unfamiliar topics to the public in political and science communication (we comment on non-expert rationales later).,4 Annotating Strength Differences,[0],[0]
"We use the following set of labels: Stronger, Weaker, No Strength Change, I can’t tell.",4 Annotating Strength Differences,[0],[0]
Table 2 gives our definitions.,4 Annotating Strength Differences,[0],[0]
The instructions included 8 pairs as examples and 10 pairs to label as a training exercise.,4 Annotating Strength Differences,[0],[0]
Participants were then asked to choose labels and write mandatory comments for 50 pairs.,4 Annotating Strength Differences,[0],[0]
"According to the comments written by participants, we believe that they did the labeling in good faith.
",4 Annotating Strength Differences,[0],[0]
Quantitative overview.,4 Annotating Strength Differences,[0],[0]
We collected 9 labels each for 500 pairs.,4 Annotating Strength Differences,[0],[0]
"Among the 500 pairs, Fleiss’ Kappa was 0.242, which indicates fair agreement (Landis and Koch, 1977).",4 Annotating Strength Differences,[0],[0]
"We took a conservative approach and only considered pairs with an absolute majority label, i.e., at least 5 of 9 labelers chose the same label.",4 Annotating Strength Differences,[0],[0]
"There are 386 pairs that satisfy this requirement (93 weaker, 194 stronger, 99 no change).",4 Annotating Strength Differences,[0],[0]
"On this subset of pairs, Fleiss’ Kappa is 0.322, and 74.4% of pairs were strength changes.",4 Annotating Strength Differences,[0],[0]
"Considering all the possible disagreement, this result was acceptable.
",4 Annotating Strength Differences,[0],[0]
Qualitative observations.,4 Annotating Strength Differences,[0],[0]
"We were excited about the labels from these participants: despite
10These decisions were made based on the results and feedback that we got from graduate students in an initial labeling.
",4 Annotating Strength Differences,[0],[0]
"the apparent difficulty of the task, we found that many labels for the 386 pairs were reasonable.",4 Annotating Strength Differences,[0],[0]
"However, in some cases, the labels were counterintuitive.",4 Annotating Strength Differences,[0],[0]
"Table 3 shows some representative examples.
",4 Annotating Strength Differences,[0],[0]
"First, participants tend to take details as evidence even when these details are not germane to the statement.",4 Annotating Strength Differences,[0],[0]
"For pair 1, while one turker pointed out the decline in number of experiments, most turkers simply labeled it as stronger because it was more specific.",4 Annotating Strength Differences,[0],[0]
"“Specific” turned out to be a common reason used in the comments, even though we said in the instructions that only additional justification and evidence matter.",4 Annotating Strength Differences,[0],[0]
"This echoes the finding in Bell and Loftus (1989) that even unrelated details influenced judgments of guilt.
",4 Annotating Strength Differences,[0],[0]
"Second, participants interpret constraints/conditions not in strictly logical ways, seeming to care little about scope at times.",4 Annotating Strength Differences,[0],[0]
"For instance, the majority labeled pair 2 as “stronger”.",4 Annotating Strength Differences,[0],[0]
"But in S2 for that pair, the result holds for strictly fewer possible worlds.",4 Annotating Strength Differences,[0],[0]
"But it should be said that there are cases that labelers interpreted logically, e.g., “compelling evidence” subsumes “compelling experimental evidence”.
",4 Annotating Strength Differences,[0],[0]
Both of the above cases share the property that they seem to be correlated with a tendency to judge lengthier statements as stronger.,4 Annotating Strength Differences,[0],[0]
Another interesting case that does not share this characteristic is that participants can have a different understanding of domain-specific terms.,4 Annotating Strength Differences,[0],[0]
"For pair 3, the majority thought that “vectors” sounds more impressive than “images”; for pair 4, the majority considered “adapt” stronger than “discover”.",4 Annotating Strength Differences,[0],[0]
"This issue is common when communicating new topics to the public not only in science commu-
nication but also in politics and other scenarios.",4 Annotating Strength Differences,[0],[0]
It may partly explain miscommunications and misinterpretations of scientific studies in journalism.11,4 Annotating Strength Differences,[0],[0]
"Our observations regarding the annotation results raise questions regarding what is a generalizable way to define strength differences, how to use the labels that we collected, and how to collect labels in the future.",5 Looking ahead,[0],[0]
"We believe that this corpus of sentence-level revisions, together with the labels and comments from participants, can provide insights into better ways to approach this problem and help further understand strength of statements.
",5 Looking ahead,[0],[0]
One interesting direction that this enables is a potentially new kind of learning problem.,5 Looking ahead,[0],[0]
The comments indicate features that humans think salient.,5 Looking ahead,[0],[0]
"Is it possible to automatically learn new features from the comments?
",5 Looking ahead,[0],[0]
"The ultimate goal of our study is to understand the effects of statement strength on the public, which can lead to various applications in public communication.",5 Looking ahead,[0],[0]
"We thank J. Baldridge, J. Boyd-Graber, C. Callison-Burch, and the reviewers for helpful comments; P. Ginsparg for providing data; and S. Chen, E. Kozyri, M. Lee, I. Lenz, M. Ott, J. Park, K. Raman, M. Reitblatt, S. Roy, A. Sharma, R. Sipos, A. Swaminathan, L. Wang, W. Xie, B. Yang and the anonymous annotators for all their labeling help.",Acknowledgments,[0],[0]
"This work was supported in part by NSF grant IIS-0910664 and a Google Research Grant.
11http://www.phdcomics.com/comics/ archive.php?comicid=1174",Acknowledgments,[0],[0]
The strength with which a statement is made can have a significant impact on the audience.,abstractText,[0],[0]
"For example, international relations can be strained by how the media in one country describes an event in another; and papers can be rejected because they overstate or understate their findings.",abstractText,[0],[0]
It is thus important to understand the effects of statement strength.,abstractText,[0],[0]
A first step is to be able to distinguish between strong and weak statements.,abstractText,[0],[0]
"However, even this problem is understudied, partly due to a lack of data.",abstractText,[0],[0]
"Since strength is inherently relative, revisions of texts that make claims are a natural source of data on strength differences.",abstractText,[0],[0]
"In this paper, we introduce a corpus of sentence-level revisions from academic writing.",abstractText,[0],[0]
We also describe insights gained from our annotation efforts for this task.,abstractText,[0],[0]
A Corpus of Sentence-level Revisions in Academic Writing: A Step towards Understanding Statement Strength in Communication,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 197–207 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
197",text,[0],[0]
"In 2015 alone, about 100 manuscripts describing randomized controlled trials (RCTs) for medical interventions were published every day.",1 Introduction,[0],[0]
"It is thus practically impossible for physicians to know which is the best medical intervention for a given patient group and condition (Borah et al., 2017; Fraser and Dunstan, 2010; Bastian et al., 2010).",1 Introduction,[0],[0]
"This inability to easily search and organize the published literature impedes the aims of evidence based medicine (EBM), which aspires to inform patient care using the totality of relevant evidence.
∗* now at Google Inc.
Computational methods could expedite biomedical evidence synthesis (Tsafnat et al., 2013; Wallace et al., 2013) and natural language processing (NLP) in particular can play a key role in the task.
",1 Introduction,[0],[0]
"Prior work has explored the use of NLP methods to automate biomedical evidence extraction and synthesis (Boudin et al., 2010; Marshall et al., 2017; Ferracane et al., 2016; Verbeke et al.,",1 Introduction,[0],[0]
"2012).1 But the area has attracted less attention than it might from the NLP community, due primarily to a dearth of publicly available, annotated corpora with which to train and evaluate models.
",1 Introduction,[0],[0]
"Here we address this gap by introducing EBMNLP, a new corpus to power NLP models in support of EBM.",1 Introduction,[0],[0]
"The corpus, accompanying documentation, baseline model implementations for the proposed tasks, and all code are publicly available.2 EBM-NLP comprises ∼5,000 medical abstracts describing clinical trials, multiply annotated in detail with respect to characteristics of the underlying trial Populations (e.g., diabetics), Interventions (insulin), Comparators (placebo) and Outcomes (blood glucose levels).",1 Introduction,[0],[0]
"Collectively, these key informational pieces are referred to as PICO elements; they form the basis for wellformed clinical questions (Huang et al., 2006).
",1 Introduction,[0],[0]
"We adopt a hybrid crowdsourced labeling strategy using heterogeneous annotators with varying expertise and cost, from laypersons to MDs.",1 Introduction,[0],[0]
Annotators were first tasked with marking text spans that described the respective PICO elements.,1 Introduction,[0],[0]
"Identified spans were subsequently anno-
1There is even, perhaps inevitably, a systematic review of such approaches (Jonnalagadda et al., 2015).
2http://www.ccs.neu.edu/home/bennye/",1 Introduction,[0],[0]
"EBM-NLP
tated in greater detail: this entailed finer-grained labeling of PICO elements and mapping these onto a normalized vocabulary, and indicating redundancy in the mentions of PICO elements.
",1 Introduction,[0],[0]
"In addition, we outline several NLP tasks that would directly support the practice of EBM and that may be explored using the introduced resource.",1 Introduction,[0],[0]
We present baseline models and associated results for these tasks.,1 Introduction,[0],[0]
"We briefly review two lines of research relevant to the current effort: work on NLP to facilitate EBM, and research in crowdsourcing for NLP.",2 Related Work,[0],[0]
"Prior work on NLP for EBM has been limited by the availability of only small corpora, which have typically provided on the order of a couple hundred annotated abstracts or articles for very complex information extraction tasks.",2.1 NLP for EBM,[0],[0]
"For example, the ExaCT system (Kiritchenko et al., 2010) applies rules to extract 21 aspects of the reported trial.",2.1 NLP for EBM,[0],[0]
It was developed and validated on a dataset of 182 marked full-text articles.,2.1 NLP for EBM,[0],[0]
"The ACRES system (Summerscales et al., 2011) produces summaries of several trial characteristic, and was trained on 263 annotated abstracts.",2.1 NLP for EBM,[0],[0]
"Hinting at more challenging tasks that can build upon foundational information extraction, Alamri and Stevenson (2015) developed methods for detecting contradictory claims in biomedical papers.",2.1 NLP for EBM,[0],[0]
"Their corpus of annotated claims contains 259 sentences (Alamri and Stevenson, 2016).
",2.1 NLP for EBM,[0],[0]
Larger corpora for EBM tasks have been derived using (noisy) automated annotation approaches.,2.1 NLP for EBM,[0],[0]
"This approach has been used to build, e.g., datasets to facilitate work on Information Retrieval (IR) models for biomedical texts (Scells et al., 2017; Chung, 2009; Boudin et al., 2010).",2.1 NLP for EBM,[0],[0]
"Similar approaches have been used to ‘distantly supervise’ annotation of full-text articles describing clinical trials (Wallace et al., 2016).",2.1 NLP for EBM,[0],[0]
"In contrast to the corpora discussed above, these automatically derived datasets tend to be relatively large, but they include only shallow annotations.
",2.1 NLP for EBM,[0],[0]
"Other work attempts to bypass basic extraction tasks and address more complex biomedical QA and (multi-document) summarization problems to support EBM (Demner-Fushman and Lin, 2007; Mollá and Santiago-Martinez, 2011; Abacha and
Zweigenbaum, 2015).",2.1 NLP for EBM,[0],[0]
Such systems would directly benefit from more accurate extraction of the types codified in the corpus we present here.,2.1 NLP for EBM,[0],[0]
"Crowdsourcing, which we here define operationally as the use of distributed lay annotators, has shown encouraging results in NLP (Novotney and Callison-Burch, 2010; Sabou et al., 2012).",2.2 Crowdsourcing,[0],[0]
"Such annotations are typically imperfect, but methods that aggregate redundant annotations can mitigate this problem (Dalvi et al., 2013; Hovy et al., 2014; Nguyen et al., 2017).
",2.2 Crowdsourcing,[0],[0]
"Medical articles contain relatively technical content, which intuitively may be difficult for persons without domain expertise to annotate.",2.2 Crowdsourcing,[0],[0]
"However, recent promising preliminary work has found that crowdsourced approaches can yield surprisingly high-quality annotations in the domain of EBM specifically (Mortensen et al., 2017; Thomas et al., 2017; Wallace et al., 2017).",2.2 Crowdsourcing,[0],[0]
"PubMed provides access to the MEDLINE database3 which indexes titles, abstracts and metadata for articles from selected medical journals dating back to the 1970s.",3 Data Collection,[0],[0]
"MEDLINE indexes over 24 million abstracts; the majority of these have been manually assigned metadata which we used to retrieved a set of 5,000 articles describing RCTs with an emphasis on cardiovascular diseases, cancer, and autism.",3 Data Collection,[0],[0]
"These particular topics were selected to cover a range of common conditions.
",3 Data Collection,[0],[0]
"We decomposed the annotation process into two steps, performed in sequence.",3 Data Collection,[0],[0]
"First, we acquired labels demarcating spans in the text describing the clinically salient abstract elements mentioned above: the trial Population, the Interventions and Comparators studied, and the Outcomes measured.",3 Data Collection,[0],[0]
We collapse Interventions and Comparators into a single category (I).,3 Data Collection,[0],[0]
"In the second annotation step, we tasked workers with providing more granular (sub-span) annotations on these spans.
",3 Data Collection,[0],[0]
"For each PIO element, all abstracts were annotated with the following four types of information.
1.",3 Data Collection,[0],[0]
"Spans exhaustive marking of text spans containing information relevant to the respective PIO categories (Stage 1 annotation).
3https://www.nlm.nih.gov/bsd/ pmresources.html
2.",3 Data Collection,[0],[0]
"Hierarchical labels assignment of more specific labels to subsequences comprising the marked relevant spans (Stage 2 annotation).
",3 Data Collection,[0],[0]
3.,3 Data Collection,[0],[0]
"Repetition grouping of labeled tokens to indicate repeated occurrences of the same information (Stage 2 annotation).
",3 Data Collection,[0],[0]
4.,3 Data Collection,[0],[0]
MeSH terms assignment of the metadata MeSH terms associated with the abstract to labeled subsequences (Stage 2,3 Data Collection,[0],[0]
"annotation).4
We collected annotations for each P, I and O element individually to avoid the cognitive load imposed by switching between label sets, and to reduce the amount of instruction required to begin the task.",3 Data Collection,[0],[0]
"All annotation was performed using a modified version of the Brat Rapid Annotation Tool (BRAT) (Stenetorp et al., 2012).",3 Data Collection,[0],[0]
We include all annotation instructions provided to workers for all tasks in the Appendix.,3 Data Collection,[0],[0]
"For large scale crowdsourcing via recruitment of layperson annotators, we used Amazon Mechanical Turk (AMT).",3.1 Non-Expert (Layperson) Workers,[0],[0]
All workers were required to have an overall job approval rate of at least 90%.,3.1 Non-Expert (Layperson) Workers,[0],[0]
Each job presented to the workers required the annotation of three randomly selected abstracts from our pool of documents.,3.1 Non-Expert (Layperson) Workers,[0],[0]
"As we received initial results, we blocked workers who were clearly not following instructions, and we actively recruited the best workers to continue working on our task at a higher pay rate.
4MeSH is a controlled, structured medical vocabulary maintained by the National Library of Medicine.
",3.1 Non-Expert (Layperson) Workers,[0],[0]
"We began by collecting the least technical annotations, moving on to more difficult tasks only after restricting our pool of workers to those with a demonstrated aptitude for the jobs.",3.1 Non-Expert (Layperson) Workers,[0],[0]
"We obtained annotations from ≥ 3 different workers for each of the 5,000 abstracts to enable robust inference of reliable labels from noisy data.",3.1 Non-Expert (Layperson) Workers,[0],[0]
"After performing filtering passes to remove non-RCT documents or those missing relevant data for the second annotation task, we are left with between 4,000 and 5,000 sets of annotations for each PIO element after the second phase of annotation.",3.1 Non-Expert (Layperson) Workers,[0],[0]
"To supplement our larger-scale data collection via AMT, we collected annotations for 200 abstracts for each PIO element from workers with advanced medical training.",3.2 Expert Workers,[0],[0]
"The idea is for these to serve as reference annotations, i.e., a test set with which to evaluate developed NLP systems.",3.2 Expert Workers,[0],[0]
"We plan to enlarge this test set in the near future, at which point we will update the website accordingly.
",3.2 Expert Workers,[0],[0]
"For the initial span labeling task, two medical students from the University of Pennsylvania and Drexel University provided the reference labels.",3.2 Expert Workers,[0],[0]
"In addition, for both stages of annotation and for the detailed subspan annotation in Stage 2, we hired three medical professionals via Upwork,5 an online platform for hiring skilled freelancers.",3.2 Expert Workers,[0],[0]
"After reviewing several dozen suggested profiles, we selected three workers that had the following characteristics: Advanced medical training (the majority of hired workers were Medical Doc-
5http://www.upwork.com
tors, the one exception being a fourth-year medical student); Strong technical reading and writing skills; And an interest in medical research.",3.2 Expert Workers,[0],[0]
"In addition to providing high-quality annotations, individuals hired via Upwork also provided feedback regarding the instructions to help make the task as clear as possible for the AMT workers.",3.2 Expert Workers,[0],[0]
"We now present corpus details, paying special attention to worker performance and agreement.",4 The Corpus,[0],[0]
"We discuss and present statistics for acquired annotations on spans, tokens, repetition and MeSH terms in Sections 4.1, 4.2, 4.3, and 4.4, respectively.",4 The Corpus,[0],[0]
"For each P, I and O element, workers were asked to read the abstract and highlight all spans of text including any pertinent information.",4.1 Spans,[0],[0]
"Annotations for 5,000 articles were collected from a total of 579 AMT workers across the three annotation types, and expert annotations were collected for 200 articles from two medical students.
",4.1 Spans,[0],[0]
We first evaluate the quality of the annotations by calculating token-wise label agreement between the expert annotators; this is reported in Table 2.,4.1 Spans,[0],[0]
"Due to the difficulty and technicality of the material, agreement between even well-trained domain experts is imperfect.",4.1 Spans,[0],[0]
"The effect is magnified by the unreliability of AMT workers, motivating our strategy of collecting several noisy annotations and aggregating over them to produce a single cleaner annotation.",4.1 Spans,[0],[0]
"We tested three different aggregation strategies: a simple majority vote, the Dawid-Skene model (Dawid and Skene, 1979) which estimates worker reliability, and HMMCrowd, a recent extension to Dawid-Skene that includes a HMM component, thus explicitly leveraging the sequential structure of contiguous spans of words (Nguyen et al., 2017).
",4.1 Spans,[0],[0]
"For each aggregation strategy, we compute the token-wise precision and recall of the output labels against the unioned expert labels.",4.1 Spans,[0],[0]
"As shown in Table 3, the HMMCrowd model afforded modest improvement in F-1 scores over the standard Dawid-Skene model, and was thus used to generate the inputs for the second annotation phase.
",4.1 Spans,[0],[0]
"The limited overlap in the document subsets annotated by any given pair of workers, and wide variation in the number of annotations per worker make interpretation of standard agreement statis-
tics tricky.",4.1 Spans,[0],[0]
"We quantify the centrality of the AMT span annotations by calculating token-wise precision and recall for each annotation against the aggregated version of the labels (Table 4).
",4.1 Spans,[0],[0]
"When comparing the average precision and recall for individual crowdworkers against the aggregated labels in Table 4, scores are poor showing very low agreement between the workers.",4.1 Spans,[0],[0]
"Despite this, the aggregated labels compare favorably against the expert labels.",4.1 Spans,[0],[0]
"This further supports the intuition that it is feasible to collect multiple lowquality annotations for a document and synthesize them to extract the signal from the noise.
",4.1 Spans,[0],[0]
"On the dataset website, we provide a variant of the corpus that includes all individual worker span annotations (e.g., for researchers interested in crowd annotation aggregated methods), and also a version with pre-aggregated annotations for convenience.",4.1 Spans,[0],[0]
"For each P, I, and O category we developed a hierarchy of labels intended to capture important sub categories within these.",4.2 Hierarchical Labels,[0],[0]
"Our labels are aligned to (and thus compatible with) the concepts codified by the Medical Subject Headings (MeSH) vocabulary of medical terms maintained by the National Library of Medicine (NLM).6 In consulta-
6https://www.nlm.nih.gov/mesh/
tion with domain experts, we selected subsets of MeSH terms for each PIO category that captured relatively precise information without being overwhelming.",4.2 Hierarchical Labels,[0],[0]
"For illustration, we show the outcomes label hierarchy we used in Figure 2.",4.2 Hierarchical Labels,[0],[0]
"We reproduce the label hierarchies used for all PIO categories in the Appendix.
",4.2 Hierarchical Labels,[0],[0]
"At this stage, workers were presented with abstracts in which relevant spans were highlighted, based on the annotations collected in the first annotation phase (and aggregated via the HMM-
Crowd model).",4.2 Hierarchical Labels,[0],[0]
"This two-step approach served dual purposes: (i) increasing the rate at which workers could complete tasks, and (ii) improving recall by directing workers to all areas in abstracts where they might find the structured information of interest.",4.2 Hierarchical Labels,[0],[0]
"Our choice of a high recall aggregation strategy for the starting spans ensured that the large majority of relevant sections of the article were available as inputs to this task.
",4.2 Hierarchical Labels,[0],[0]
The three trained medical personnel hired via Upwork each annotated 200 documents and reported that spans sufficiently captured the target information.,4.2 Hierarchical Labels,[0],[0]
"These domain experts received feedback and additional training after labeling an initial round of documents, and all annotations were reviewed for compliance.",4.2 Hierarchical Labels,[0],[0]
"The average inter-
annotator agreement is reported in Table 6.
",4.2 Hierarchical Labels,[0],[0]
"With respect to crowdsourcing on AMT, the task for Participants was published first, allowing us to target higher quality workers for the more technical Interventions and Outcomes annotations.",4.2 Hierarchical Labels,[0],[0]
"We retained labels from 118 workers for Participants, the top 67 of whom were invited to continue on to the following tasks.",4.2 Hierarchical Labels,[0],[0]
"Of these, 37 continued to contribute to the project.",4.2 Hierarchical Labels,[0],[0]
"Several workers provided ≥ 1,000 annotations and continued to work on the task over a period of several months.
",4.2 Hierarchical Labels,[0],[0]
"To produce final per-token labels, we again turned to aggregation.",4.2 Hierarchical Labels,[0],[0]
"The subspans annotated in this second pass were by construction shorter than the starting spans, and (perhaps as a result) informal experiments revealed little benefit from HMMCrowd’s sequential modeling aspect.",4.2 Hierarchical Labels,[0],[0]
"The introduction of many label types significantly increased the complexity of the task, resulting in both lower expert inter-annotator agreement (Table 6 and decreased performance when comparing the crowdsourced labels against those of the experts (Table 7.
",4.2 Hierarchical Labels,[0],[0]
"Most observed token-level disagreements (and errors, with respect to reference annotations) involve differences in the span lengths demarcated by individuals.",4.2 Hierarchical Labels,[0],[0]
"For example, many abstracts contain an information-dense description of the patient population, focusing on their medical condition but also including information about their sex and/or age.",4.2 Hierarchical Labels,[0],[0]
"Workers would also sometimes fail
to capture repeated mentions of the same information, producing Type 2 errors more frequently than Type 1.",4.2 Hierarchical Labels,[0],[0]
"This tendency can be seen in the overall token-level confusion matrix for AMT workers on the Participants task, shown in Figure 3.
",4.2 Hierarchical Labels,[0],[0]
"In a similar though more benign category of error, workers differed in the amount of context they included surrounding each subspan.",4.2 Hierarchical Labels,[0],[0]
"Although the instructions asked workers to highlight minimal subspans, there was variance in what workers considered relevant.
",4.2 Hierarchical Labels,[0],[0]
"For the same reasons mentioned above (little pairwise overlap in annotations, high variance with respect to annotations per worker), quantifying agreement between AMT workers is again difficult using traditional measures.",4.2 Hierarchical Labels,[0],[0]
"We thus again take as a measure of agreement the precision, recall, and F-1 of the individual annotations against the aggregated labels and present the results in Table 8.",4.2 Hierarchical Labels,[0],[0]
Medical abstracts often mention the same information in multiple places.,4.3 Repetition,[0],[0]
"In particular, interventions and outcomes are typically described at the beginning of an abstract when introducing the purpose of the underlying study, and then again when discussing methods and results.",4.3 Repetition,[0],[0]
"It is important to
be able to differentiate between novel and reiterated information, especially in cases such as complex interventions, distinct measured outcomes, or multi-armed trials.",4.3 Repetition,[0],[0]
"Merely identifying all occurrences of, for example, a pharmacological intervention leaves ambiguity as to how many distinct interventions were applied.
",4.3 Repetition,[0],[0]
Workers identified repeated information as follows.,4.3 Repetition,[0],[0]
"After completing detailed labeling of abstract spans, they were asked to group together subspans that were instances of the same information (for example, redundant mentions of a particular drug evaluated as one of the interventions in the trial).",4.3 Repetition,[0],[0]
This process produces labels for repetition between short spans of tokens.,4.3 Repetition,[0],[0]
"Due to the differences in the lengths of annotated subspans discussed in the preceding section, the labels are not naturally comparable between workers without directly modeling the entities contained in each subspan.",4.3 Repetition,[0],[0]
"The labels assigned by workers produce repetition labels between sets of tokens but a more sophisticated notion of co-reference is required to identify which tokens correctly represent the entity contained in the span, and which tokens are superfluous noise.
",4.3 Repetition,[0],[0]
"As a proxy for formally enumerating these entities, we observe that a large majority of start-
ing spans only contain a single target relevant to the subspan labeling task, and so identifying repetition between the starting spans is sufficient.",4.3 Repetition,[0],[0]
"For example, consider the starting intervention span ”underwent conventional total knee arthroplasty”; there is only one intervention in the span but some annotators assigned the SURGICAL label to all five tokens while others opted for only ”total knee arthroplasty.”",4.3 Repetition,[0],[0]
"By analyzing repetition at the level of the starting spans, we can compute agreement without concern for the confounds of slight misalignments or differences in length of the subspans.
",4.3 Repetition,[0],[0]
"Overall agreement between AMT workers for span-level repetition, measured by computing precision and recall against the majority vote for each pair of spans, is reported in Table 10.",4.3 Repetition,[0],[0]
The National Library of Medicine maintains an extensive hierarchical ontology of medical concepts called Medical Subject Headings (MeSH terms); this is part of the overarching Metathesaurus of the Unified Medical Language System (UMLS).,4.4 MeSH Terms,[0],[0]
"Personnel at the NLM manually assign citations (article titles, abstracts and meta-data) indexed in MEDLINE relevant MeSH terms.",4.4 MeSH Terms,[0],[0]
"These terms have been used extensively to evaluate the content of articles, and are frequently used to facilitate document retrieval (Lu et al., 2009; Lowe and Barnett, 1994).
",4.4 MeSH Terms,[0],[0]
"In the case of randomized controlled trials, MeSH terms provide structured information regarding key aspects of the underlying studies, ranging from participant demographics to methodologies to co-morbidities.",4.4 MeSH Terms,[0],[0]
"A drawback to these annotations, however, is that they are applied at the document (rather than snippet or token) level.",4.4 MeSH Terms,[0],[0]
"To capture where MeSH terms are instantiated within a given abstract text, we provided a list of all terms associated with said article and instructed workers to select the subset of these that applied to each set of token labels that they annotated.
MeSH terms are domain specific and many re-
quire a medical background to understand, thus rendering this facet of the annotation process particularly difficult for untrained (lay) workers.",4.4 MeSH Terms,[0],[0]
"Perhaps surprisingly, several AMT workers voluntarily mentioned relevant background training; our pool of workers included (self-identified) nurses and other trained medical professionals.",4.4 MeSH Terms,[0],[0]
"A few workers with such training stated this background as a reason for their interest in our tasks.
",4.4 MeSH Terms,[0],[0]
The technical specificity of the more obscure MeSH terms is also exacerbated by their sparsity.,4.4 MeSH Terms,[0],[0]
"Of the 6,963 unique MeSH terms occurring in our set of abstracts, 87% of them are only found in 10 documents or fewer and only 2.0% occur in at least 1% of the total documents.",4.4 MeSH Terms,[0],[0]
"The full distribution of document frequency for MeSH terms is show in Figure 4.
",4.4 MeSH Terms,[0],[0]
To evaluate how often salient MeSH terms were instantiated in the text by annotators we consider only the 135 MeSH terms that occur in at least 1% of abstracts (we list these in the supplementary material).,4.4 MeSH Terms,[0],[0]
"For each term, we calculate its ”instantiation frequency” as the percentage of abstracts containing the term in which at least one annotator assigned it to a span of text.",4.4 MeSH Terms,[0],[0]
The total numbers of MeSH terms with an instantiation rate above different thresholds for the respective PIO elements are shown in Table 11.,4.4 MeSH Terms,[0],[0]
"We outline a few NLP tasks that are central to the aim of processing medical literature generally and
to aiding practitioners of EBM specifically.",5 Tasks & Baselines,[0],[0]
"First, we consider the task of identifying spans in abstracts that describe the respective PICO elements (Section 5.1).",5 Tasks & Baselines,[0],[0]
"This would, e.g., improve medical literature search and retrieval systems.",5 Tasks & Baselines,[0],[0]
"Next, we outline the problem of extracting structured information from abstracts (Section 5.2).",5 Tasks & Baselines,[0],[0]
"Such models would further aid search, and might eventually facilitate automated knowledge-base construction for the clinical trials literature.",5 Tasks & Baselines,[0],[0]
"Furthermore, automatic extraction of structured data would enable automation of the manual evidence synthesis process (Marshall et al., 2017).
",5 Tasks & Baselines,[0],[0]
"Finally, we consider the challenging task of identifying redundant mentions of the same PICO element (Section 5.3).",5 Tasks & Baselines,[0],[0]
"This happens, e.g., when an intervention is mentioned by the authors repeatedly in an abstract, potentially with different terms.",5 Tasks & Baselines,[0],[0]
"Achieving such disambiguation is important for systems aiming to induce structured representations of trials and their results, as this would require recognizing and normalizing the unique interventions and outcomes studied in a trial.
",5 Tasks & Baselines,[0],[0]
For each of these tasks we present baseline models and corresponding results.,5 Tasks & Baselines,[0],[0]
"Note that we have pre-defined train, development and test sets across PIO elements for this corpus, comprising 4300, 500 and 200 abstracts, respectively.",5 Tasks & Baselines,[0],[0]
"The latter set is annotated by domain experts (i.e., persons with medical training).",5 Tasks & Baselines,[0],[0]
"These splits will, of course, be distributed along with the dataset to facilitate model comparisons.",5 Tasks & Baselines,[0],[0]
"We consider two baseline models: a linear Conditional Random Field (CRF) (Lafferty et al., 2001) and a Long Short-Term Memory (LSTM) neural tagging model, an LSTM-CRF (Lample et al., 2016; Ma and Hovy, 2016).","5.1 Identifying P, I and O Spans",[0],[0]
"In both models, we treat tokens as being either Inside (I) or Outside (O) of spans.
","5.1 Identifying P, I and O Spans",[0],[0]
"For the CRF, features include: indicators for the current, previous and next words; part of speech tags inferred using the Stanford CoreNLP tagger (Manning et al., 2014); and character information, e.g., whether a token contains digits, uppercase letters, symbols and so on.
","5.1 Identifying P, I and O Spans",[0],[0]
"For the neural model, the model induces features via a bi-directional LSTM that consumes distributed vector representations of input tokens sequentially.","5.1 Identifying P, I and O Spans",[0],[0]
"The bi-LSTM yields a hidden vector at
each token index, which is then passed to a CRF layer for prediction.","5.1 Identifying P, I and O Spans",[0],[0]
"We also exploit characterlevel information by passing a bi-LSTM over the characters comprising each word (Lample et al., 2016); these are appended to the word embedding representations before being passed through the bi-LSTM.","5.1 Identifying P, I and O Spans",[0],[0]
"Beyond identifying the spans of text containing information pertinent to each of the PIO elements, we consider the task of predicting which of the detailed labels occur in each span, and where they are located.",5.2 Extracting Structured Information,[0],[0]
"Specifically, we begin with the starting spans and predict a single label from the corresponding PIO hierarchy for each token, evaluating against the test set of 200 documents.",5.2 Extracting Structured Information,[0],[0]
"Initial experiments with neural models proved unfruitful but bear further investigation.
",5.2 Extracting Structured Information,[0],[0]
"For the CRF model we include the same features as in the previous model, supplemented with additional features encoding if the adjacent tokens include any parenthesis or mathematical operators (specifically: %,+,−).",5.2 Extracting Structured Information,[0],[0]
"For the logistic regression model, we use a one-vs-rest approach.",5.2 Extracting Structured Information,[0],[0]
"Features include token n-grams, part of speech indicators, and the same character-level information as in the CRF model.",5.2 Extracting Structured Information,[0],[0]
"To formalize repetition, we consider every pair of starting PIO spans from each abstract, and assign
binary labels that indicate whether they share at least one instance of the same information.",5.3 Detecting Repetition,[0],[0]
"Although this makes prediction easier for long and information-dense spans, a large enough majority of the spans contain only a single instance of relevant information that the task serves as a reasonable baseline.",5.3 Detecting Repetition,[0],[0]
"Again, the model is trained on the aggregated labels collected from AMT and evaluated against the high-quality test set.
",5.3 Detecting Repetition,[0],[0]
"We train a logistic regression model that operates over standard features, including bag-ofwords representations and sentence-level features such as length and position in the document.",5.3 Detecting Repetition,[0],[0]
All baseline model implementations are available on the corpus website.,5.3 Detecting Repetition,[0],[0]
"We have presented EBM-NLP: a new, publicly available corpus comprising 5,000 richly annotated abstracts of articles describing clinical randomized controlled trials.",6 Conclusions,[0],[0]
"This dataset fills a need for larger scale corpora to facilitate research on NLP methods for processing the biomedical literature, which have the potential to aid the conduct of EBM.",6 Conclusions,[0],[0]
"The need for such technologies will only become more pressing as the literature continues its torrential growth.
",6 Conclusions,[0],[0]
"The EBM-NLP corpus, accompanying documentation, code for working with the data, and baseline models presented in this work are all publicly available at: http://www.ccs.neu.",6 Conclusions,[0],[0]
edu/home/bennye/EBM-NLP.,6 Conclusions,[0],[0]
"This work was supported in part by the National Cancer Institute (NCI) of the National Institutes of Health (NIH), award number UH2CA203711.",7 Acknowledgements,[0],[0]
"We present a corpus of 5,000 richly annotated abstracts of medical articles describing clinical randomized controlled trials.",abstractText,[0],[0]
"Annotations include demarcations of text spans that describe the Patient population enrolled, the Interventions studied and to what they were Compared, and the Outcomes measured (the ‘PICO’ elements).",abstractText,[0],[0]
"These spans are further annotated at a more granular level, e.g., individual interventions within them are marked and mapped onto a structured medical vocabulary.",abstractText,[0],[0]
We acquired annotations from a diverse set of workers with varying levels of expertise and cost.,abstractText,[0],[0]
We describe our data collection process and the corpus itself in detail.,abstractText,[0],[0]
We then outline a set of challenging NLP tasks that would aid searching of the medical literature and the practice of evidence-based medicine.,abstractText,[0],[0]
"A Corpus with Multi-Level Annotations of Patients, Interventions and Outcomes to Support Language Processing for Medical Literature",title,[0],[0]
"Proceedings of NAACL-HLT 2013, pages 703–708, Atlanta, Georgia, 9–14 June 2013. c©2013 Association for Computational Linguistics",text,[0],[0]
Speech disfluencies are common phenomena in spontaneous speech.,1 Introduction,[0],[0]
"They consist of spoken words and phrases that represent self-correction, hesitation, and floor-grabbing behaviors, but do not add semantic information; removing them yields the intended, fluent utterance.",1 Introduction,[0],[0]
The presence of disfluencies in conversational speech data can cause problems for both downstream processing (parsing and other natural language processing tasks) and human readability of speech transcripts.,1 Introduction,[0],[0]
"There has been much research effort on automatic disfluency detection in recent years (Shriberg and Stolcke, 1997; Snover et al., 2004; Liu et al., 2006; Lin and Lee, 2009; Schuler et al., 2010; Georgila et al., 2010; Zwarts and Johnson, 2011), particularly from the DARPA EARS (Effective, Affordable, Reusable Speech-toText) MDE (MetaData Extraction) (DARPA Information Processing Technology Office, 2003) program, which focused on the automatic transcription
of sizable amounts of speech data and rendering such transcripts in readable form, for both conversational telephone speech (CTS) and broadcast news (BN).
",1 Introduction,[0],[0]
"However, the EARS MDE effort was focused on English only, and there hasn’t been much research on the effectiveness of similar automatic disfluency detection approaches for multiple languages.",1 Introduction,[0],[0]
This paper presents three main innovations.,1 Introduction,[0],[0]
"First, we extend the EARS MDE-style disfluency detection approach combining lexical and prosodic features using a Conditional Random Field (CRF) model, which was employed for detecting disfluency on English conversational speech data (Liu et al., 2005), to Mandarin conversational speech, as presented in Section 2.",1 Introduction,[0],[0]
"Second, we implement an automatic filled pause detection approach through constrained speech recognition, as presented in Section 3.",1 Introduction,[0],[0]
"Third, for both disfluency detection systems, we compare side-by-side contributions of different knowledge sources to detection performance for two languages, English and Mandarin, as presented in Section 4.",1 Introduction,[0],[0]
Conclusions appear in Section 5.,1 Introduction,[0],[0]
"We focus on two types of disfluencies, Fillers and Edit disfluencies, following the EARS MDE disfluency types modeled in (Liu et al., 2006).",2 EARS MDE Style Automatic Disfluency Detection,[0],[0]
"Fillers include filled pauses (FP), discourse markers (DM), and explicit editing terms (ET).",2 EARS MDE Style Automatic Disfluency Detection,[0],[0]
FPs are words used by the speakers as floor holders to maintain control of a conversation.,2 EARS MDE Style Automatic Disfluency Detection,[0],[0]
They can also indicate hesitations of the speaker.,2 EARS MDE Style Automatic Disfluency Detection,[0],[0]
"In this work, English FPs
703
comprise uh and um, based on English CTS corpora.",2 EARS MDE Style Automatic Disfluency Detection,[0],[0]
"For Mandarin, Zhao and Jurafsky found that Mandarin speakers intensively used both demonstratives zhege (literally ‘this’) and nage (literally ‘that’) and uh/mm as FPs based on a large speech corpus of Mandarin telephone conversation (Zhao and Jurafsky, 2005).",2 EARS MDE Style Automatic Disfluency Detection,[0],[0]
We study the same set of Chinese FPs in this study.,2 EARS MDE Style Automatic Disfluency Detection,[0],[0]
"DMs are words or phrases related to the structure of the discourse and help taking or keeping a turn, or serving as acknowledgment, for example, I mean, you know.",2 EARS MDE Style Automatic Disfluency Detection,[0],[0]
An explicit ET is an editing term in an edit disfluency that is not an FP or a DM.,2 EARS MDE Style Automatic Disfluency Detection,[0],[0]
"For example, we have two action items sorry three action items from the meeting, where sorry is an explicit ET.
",2 EARS MDE Style Automatic Disfluency Detection,[0],[0]
"Edit disfluencies involve syntactically relevant content that is either repeated, revised, or abandoned.",2 EARS MDE Style Automatic Disfluency Detection,[0],[0]
The basic pattern for edit disfluencies has the form (reparandum) <editing term> correction.,2 EARS MDE Style Automatic Disfluency Detection,[0],[0]
The reparandum is the portion of the utterance that is corrected or abandoned entirely (in the case of restarts).,2 EARS MDE Style Automatic Disfluency Detection,[0],[0]
"An interruption point (IP), marked with ‘ ’ in the pattern, is the point at which the speaker breaks off the original utterance and then repeats, revises, or restarts the utterance.",2 EARS MDE Style Automatic Disfluency Detection,[0],[0]
The editing term is optional and consists of one or more filler words.,2 EARS MDE Style Automatic Disfluency Detection,[0],[0]
The correction is the portion of the utterance that corrects the original reparandum.,2 EARS MDE Style Automatic Disfluency Detection,[0],[0]
"Revisions denote the cases when a speaker modifies the original utterance with a similar syntactic structure, e.g., we have two action items sorry three action items from the meeting.",2 EARS MDE Style Automatic Disfluency Detection,[0],[0]
"Restarts denote the cases when a speaker abandons an utterance or a constituent and restarts all over again, e.g., He I like this idea.
",2 EARS MDE Style Automatic Disfluency Detection,[0],[0]
"We used a CRF model to combine lexical features, shallow syntactic features, and prosodic features for joint detection of edit words and IP words.",2 EARS MDE Style Automatic Disfluency Detection,[0],[0]
"A CRF defines a global log-linear distribution of the state (or label) sequence E conditioned on an observation sequence, in our case including the word sequenceW and the features F , and optimized globally over the entire sequence considering the context event information for making decisions at each point.",2 EARS MDE Style Automatic Disfluency Detection,[0],[0]
"We used the Mallet package (McCallum, 2002) to implement the CRF model.",2 EARS MDE Style Automatic Disfluency Detection,[0],[0]
We used a first-order model that includes only two sequential events in the feature set.,2 EARS MDE Style Automatic Disfluency Detection,[0],[0]
"The CRF model is trained to maximize the conditional log-likelihood of a given training
set P (EjW;F ).",2 EARS MDE Style Automatic Disfluency Detection,[0],[0]
"During testing, the most likely sequence E is found using the Viterbi algorithm.",2 EARS MDE Style Automatic Disfluency Detection,[0],[0]
"To avoid over-fitting, a zero-mean Gaussian prior (McCallum and Li, 2003) was applied to the parameters, where the variance of the prior was optimized on the development test set.",2 EARS MDE Style Automatic Disfluency Detection,[0],[0]
"Each word is associated with a class label, representing whether it is an edit word or not.",2 EARS MDE Style Automatic Disfluency Detection,[0],[0]
"We included IP in the target classes and used five states, as outside edit (O), begin edit with an IP (B-E+IP), begin edit (B-E), inside edit with an IP (I-E+IP), and inside edit (IE) (Liu et al., 2006).",2 EARS MDE Style Automatic Disfluency Detection,[0],[0]
"State transitions are also the same as in (Liu et al., 2006).",2 EARS MDE Style Automatic Disfluency Detection,[0],[0]
We built a Hidden Markov Model (HMM) based part-of-speech (POS) taggers for English conversational speech and Mandarin broadcast conversation data.,2 EARS MDE Style Automatic Disfluency Detection,[0],[0]
"After employing the co-training approach described in (Wang et al., 2007), we achieved 94% POS tagging accuracy for both data sets.",2 EARS MDE Style Automatic Disfluency Detection,[0],[0]
"The features for CRF modeling include: n-grams from words and automatically generated POS tags, speaker turns, whether there is a repeated word sequence ending at a word boundary, whether a word is a fragment, whether there is a predefined filler phrase after the word boundary, and the prosody model posterior probabilities from a decision tree model (Shriberg and Stolcke, 1997) and discretized by cumulative binning (Liu et al., 2006).",2 EARS MDE Style Automatic Disfluency Detection,[0],[0]
The prosodic features were computed for each interword boundary from words and phonetic alignments of the manual transcriptions.,2 EARS MDE Style Automatic Disfluency Detection,[0],[0]
"We extracted the same set of prosodic features for English and Mandarin data, based on duration, fundamental frequency (f0), energy, and pause information, and nonprosodic information such as speaker gender and speaker change, for training and applying the decision-tree-based prosody model (Liu et al., 2006).
",2 EARS MDE Style Automatic Disfluency Detection,[0],[0]
We implemented a rule-based system for filler word detection.,2 EARS MDE Style Automatic Disfluency Detection,[0],[0]
"We defined a list of possible Chinese and English filler words, including filled pauses and discourse markers.",2 EARS MDE Style Automatic Disfluency Detection,[0],[0]
The rules also explore POS tags assigned by our Chinese and English POS taggers.,2 EARS MDE Style Automatic Disfluency Detection,[0],[0]
We also propose an alternative approach for automatic detection of FPs given speech transcripts that omit FPs but are otherwise accurate.,3 Constrained Speech Recognition for Filled Pause Detection,[0],[0]
"This approach is motivated by situations where only an edited, “cleaned-up” transcript is available, but where an accurate verbatim transcript is to be recovered automatically.",3 Constrained Speech Recognition for Filled Pause Detection,[0],[0]
"We treat this task as a constrained speech recognition problem, and investigate how effectively it is solved by a state-of-the-art large vocabulary continuous speech recognition (LVCSR) system.",3 Constrained Speech Recognition for Filled Pause Detection,[0],[0]
"Hence, this approach can be considered as combining LVCSR acoustic model (AM) and language model (LM) knowledge sources in a search framework for FP detection.",3 Constrained Speech Recognition for Filled Pause Detection,[0],[0]
"Compared to the FP detection component in the disfluency detection systems described in Section 2, this alternative approach explores different knowledge sources.",3 Constrained Speech Recognition for Filled Pause Detection,[0],[0]
"In particular, the AMs explore different front-end features compared to the lexical and prosodic features explored in those disfluency detection systems presented in Section 2.",3 Constrained Speech Recognition for Filled Pause Detection,[0],[0]
"Details of the front-end features are illustrated below.
",3 Constrained Speech Recognition for Filled Pause Detection,[0],[0]
We evaluated this approach on both English and Mandarin conversational speech.,3 Constrained Speech Recognition for Filled Pause Detection,[0],[0]
"For detecting FPs in English conversational speech, we used a modified and simplified form of the recognition system developed for the 2004 NIST Rich Transcription Conversational Telephone Speech (CTS) evaluations, described in (Stolcke et al., 2006).",3 Constrained Speech Recognition for Filled Pause Detection,[0],[0]
"The first pass of the recognizer uses a within-word MFCC+MLP model (i.e, trained on Mel-frequency cepstral coefficient (MFCC) features augmented with Multi-Layer Perceptron (MLP) based phoneposterior features), while the second pass uses a cross-word model trained on Perceptual Linear Prediction (PLP) features adapted (by speaker) to the output of the first pass.",3 Constrained Speech Recognition for Filled Pause Detection,[0],[0]
"For purposes of FP detection, the recognition is constrained to a word lattice formed by the manually transcribed non-FP reference words, with optional FP words inserted between any two words and at the beginning and end of each utterance.",3 Constrained Speech Recognition for Filled Pause Detection,[0],[0]
Both first and second pass decoding was constrained by the optional-FP lattices.,3 Constrained Speech Recognition for Filled Pause Detection,[0],[0]
"In the second pass, HTK lattices were generated with bigram LM probabilities and rescored with a
4-gram LM.",3 Constrained Speech Recognition for Filled Pause Detection,[0],[0]
The consensus decoding output from the rescored lattices was used for scoring FP detection.,3 Constrained Speech Recognition for Filled Pause Detection,[0],[0]
The system thus evaluates the posterior probability of an FP at every word boundary using both acoustic model (AM) and language model (LM) evidence.,3 Constrained Speech Recognition for Filled Pause Detection,[0],[0]
The acoustic model for the English recognition system was trained on about 2300 hours of CTS data.,3 Constrained Speech Recognition for Filled Pause Detection,[0],[0]
The language models (which models FP like any other word) are bigram and 4-gram statistical word,3 Constrained Speech Recognition for Filled Pause Detection,[0],[0]
"n-gram LMs estimated from the same data plus additional non-CTS data and web data.
",3 Constrained Speech Recognition for Filled Pause Detection,[0],[0]
"For detecting FPs in Mandarin broadcast conversation speech, we used a modified form of the recognition system developed for the 2008 DARPA GALE (Global Autonomous Language Exploitation) Speech-to-Text evaluation, described in (Lei et al., 2009).",3 Constrained Speech Recognition for Filled Pause Detection,[0],[0]
"The system conducted a constrained decoding on the optional-FP lattices, using a speaker-independent within-word triphone MPEtrained MFCC+pitch+MLP model and a pruned trigram LM.",3 Constrained Speech Recognition for Filled Pause Detection,[0],[0]
"For the Mandarin ASR system, the MFCC+MLP front-end features were augmented with 3-dimension smoothed pitch features (Lei et al., 2006).",3 Constrained Speech Recognition for Filled Pause Detection,[0],[0]
HTK lattices were generated with probabilities from the pruned trigram LM and rescored by the full trigram LM.,3 Constrained Speech Recognition for Filled Pause Detection,[0],[0]
The consensus decoding output from the rescored lattices was used for scoring FP detection.,3 Constrained Speech Recognition for Filled Pause Detection,[0],[0]
The AMs for this system were trained on 1642 hours of Mandarin broadcast news and conversation speech data and the LMs were trained on 1.4 billion words comprising a variety of resources.,3 Constrained Speech Recognition for Filled Pause Detection,[0],[0]
"Details of training data and system development were illustrated in (Lei et al., 2009).
",3 Constrained Speech Recognition for Filled Pause Detection,[0],[0]
"This procedure is similar to forced aligning the word lattices to the audio data (Finke and Waibel, 1997).",3 Constrained Speech Recognition for Filled Pause Detection,[0],[0]
"Both Finke et al.’s approach (Finke and Waibel, 1997) and our approach built a lattice from each transcription sentence (in our approach, optional filled pauses are inserted between any two words and at the beginning and end of each utterance).",3 Constrained Speech Recognition for Filled Pause Detection,[0],[0]
"Then Finke et al. force-aligned the lattice with utterance; whereas, we used multi-pass constrained decoding with within-word and cross-word models, MLLR adaptation of the acoustic models, and rescoring with a higher-order n-gram LM, so the performance will be better than just flexible alignment to the lattices.",3 Constrained Speech Recognition for Filled Pause Detection,[0],[0]
"Note that when constructing the word lattices with optional FP words, for En-
glish, the optional FP words are a choice between uh and um.",3 Constrained Speech Recognition for Filled Pause Detection,[0],[0]
"For Mandarin, the optional FP words are a choice between uh, mm, zhege, and nage.",3 Constrained Speech Recognition for Filled Pause Detection,[0],[0]
We assigned equal weights to FP words.,3 Constrained Speech Recognition for Filled Pause Detection,[0],[0]
"Scoring of EARS MDE-style automatic disfluency detection output is done using the NIST tools 1, computing the error rate as the average number of misclassified words per reference event word.",4 Experimental Results,[0],[0]
"For English, the training and evaluation data were from the 40 hours CTS data in the NIST RT-04F MDE training data including speech, their transcriptions and disfluency annotations by LDC.",4 Experimental Results,[0],[0]
"We randomly held out two 3-hour subsets from this training data set for evaluation and parameter tuning respectively, and used the remaining data for training.",4 Experimental Results,[0],[0]
"Note that for Mandarin, there is no LDC released Mandarin MDE training data.",4 Experimental Results,[0],[0]
"We adapted the English MDE annotation guidelines for Mandarin and manually annotated the manual transcripts of 92 Mandarin broadcast conversation (BC) shows released by LDC under the DARPA GALE program, for edit disfluencies and filler words.",4 Experimental Results,[0],[0]
"We randomly held out two 3-hour subsets from the 92 shows for evaluation and parameter tuning respectively, and manually corrected disfluency annotation errors on the evaluation set.
",4 Experimental Results,[0],[0]
"Table 1 shows the results in NIST error rate (%) for edit word, IP, and filler word detection.",4 Experimental Results,[0],[0]
"We observe that adding POS features improves edit word, edit IP, and filler word detection for both languages, and adding a prosody model produced further improvement (note that filler word detection systems did not employ prosodic features).",4 Experimental Results,[0],[0]
"The gains from combining the word, POS, and prosody model over the word n-gram baseline are statistically significant for both languages (confidence level p < 0:05 using matched pair test).",4 Experimental Results,[0],[0]
"Also, adding the prosody model over word+POS yielded a larger relative gain in edit word+IP detection performance for Mandarin than for English data.",4 Experimental Results,[0],[0]
A preliminary study of these results has shown that the prosody model contributes differently for different types of disfluencies for English and Mandarin conversational speech and we will continue this study in future work.,4 Experimental Results,[0],[0]
"We also plan
1www.itl.nist.gov/iad/mig/tests/rt/2004-fall/index.html
to investigate the prosodic features considering the special characteristics of edited disfluencies in Mandarin studied in (Lin and Lee, 2009).
",4 Experimental Results,[0],[0]
"For evaluating constrained speech recognition for FP detection, the English test set of conversational speech data and word transcripts is derived from the CTS subset of the NIST 2002 Rich Transcription evaluation.",4 Experimental Results,[0],[0]
"The waveforms were segmented according to utterance boundaries given by the humangenerated transcripts, resulting in 6554 utterance segments with a total duration of 6.8 hours.",4 Experimental Results,[0],[0]
"We then excluded turns that have fewer than five tokens or have two or more FPs in a row (such as ‘uh um’ and ‘uh, uh’), resulting in 3359 segments.",4 Experimental Results,[0],[0]
This yields the test set from which we computed English FP detection scores.,4 Experimental Results,[0],[0]
"The transcripts of this test set contain 54511 non-FP words and 1394 FPs, transcribed as either uh or um.",4 Experimental Results,[0],[0]
"When evaluating FP detection performance, these two orthographical forms were mapped to a single token type, so recognizing one form as the other is not penalized.",4 Experimental Results,[0],[0]
The Mandarin test set is the DARPA GALE 2008 Mandarin speechto-text development test set of 1 hour duration.,4 Experimental Results,[0],[0]
"The transcripts of this test set contain 9820 non-FP words and 370 FP words, transcribed as uh, mm, zhege, and nage.",4 Experimental Results,[0],[0]
We collapsed them to a single token type for FP scoring.,4 Experimental Results,[0],[0]
"We evaluated FP detection performance in terms of both false alarm (incorrect detection) and miss (failed detection) rates, shown in Table 2.",4 Experimental Results,[0],[0]
We observed that adding pronunciation scores didn’t change the Pfa and Pmiss.,4 Experimental Results,[0],[0]
"On the English
test set, adding LM scores degraded Pmiss but improved Pfa.",4 Experimental Results,[0],[0]
"However, on the Mandarin test set, increasing LM weight improved both Pmiss and Pfa, suggesting that for the Mandarin LVCSR system in this study, the LM could provide complementary information to the AM to discriminate FP and non-FP words.",4 Experimental Results,[0],[0]
"In conclusion, we have presented two automatic disfluency detection systems, one combining various lexical and prosodic features, and the other combining LVCSR acoustic and language model knowledge sources.",5 Conclusion,[0],[0]
"We observed significant improvements in combining lexical and prosodic features over just employing word n-gram features, for both languages.",5 Conclusion,[0],[0]
"When combining AM and LM knowledge sources for FP detection in constrained speech recognition, we found increasing LM weight improved both false alarm and miss rates for Mandarin but degraded the miss rate for English.",5 Conclusion,[0],[0]
The authors thank all the anonymous reviewers of this paper for valuable suggestions.,Acknowledgments,[0],[0]
This work is supported in part by NSF grant IIS-0964556.,Acknowledgments,[0],[0]
We investigate two systems for automatic disfluency detection on English and Mandarin conversational speech data.,abstractText,[0],[0]
The first system combines various lexical and prosodic features in a Conditional Random Field model for detecting edit disfluencies.,abstractText,[0],[0]
The second system combines acoustic and language model scores for detecting filled pauses through constrained speech recognition.,abstractText,[0],[0]
We compare the contributions of different knowledge sources to detection performance between these two languages.,abstractText,[0],[0]
A Cross-language Study on Automatic Speech Disfluency Detection,title,[0],[0]
"Proceedings of the SIGDIAL 2017 Conference, pages 395–405, Saarbrücken, Germany, 15-17 August 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
Conversational agents tailored for communication with language learners are studied in the area of Communicative Intelligent Computer-Assisted Language Learning (CommICALL).,1 Introduction,[0],[0]
"Starting with the idea of creating a machine that behaves like a language expert in an informal chat, specific interactional practices need to be described where linguistic identities of interaction participants become visible.",1 Introduction,[0],[0]
"Such practices include repair with linguistic trouble source where non-native speakers address troubles in comprehension or production (Danilava et al., 2013).
",1 Introduction,[0],[0]
Repair is a building block of conversation that helps to deal with troubles in understanding and production of talk.,1 Introduction,[0],[0]
Depending on who produced a trouble source and who initiates a repair we distinguish between self-initiated and other-initiated repair.,1 Introduction,[0],[0]
"A repair can be carried out by the same speaker who produced the trouble source or by the other speaker (self-repair and other-repair).
",1 Introduction,[0],[0]
"Because there is a preference for self-repair, other-initiated self-repair is the most frequent repair type.",1 Introduction,[0],[0]
"It may become even more frequent in
conversations where one of the speakers is more knowledgeable in some matters than the other, for instance in mastering professional terminology or communication in a second language not yet fully mastered.",1 Introduction,[0],[0]
"Therefore it is crucial for conversational agents acting in such environments to recognize and to handle repair initiations properly.
",1 Introduction,[0],[0]
Repair sequences where the machine is the trouble-speaker are in focus of this article.,1 Introduction,[0],[0]
"The learner initiates a repair in response to something not (fully) understood, and the machine explains.",1 Introduction,[0],[0]
"This type of repair corresponds to other-initiated self-repair with a linguistic trouble source where the language learner is the recipient of the trouble talk (OISRL).
",1 Introduction,[0],[0]
"CommICALL research is mainly grounded in Second Language Acquisition (SLA) theory (Petersen, 2010; Wilske, 2014).",1 Introduction,[0],[0]
"The model of explanation sequences, so called negotiations of meaning introduced by (Varonis and Gass, 1985) received a lot of attention and was highly re-used in subsequent CALL research (Fredriksson, 2012; Satomi Kawaguchi, 2012).",1 Introduction,[0],[0]
"The model includes a trigger, an indicator, a response and a reaction to response.",1 Introduction,[0],[0]
"However, this model has been criticized for its view on repair as something ""marring the flow"" of a conversation and for being inapplicable to non-institutional settings (Markee, 2000).",1 Introduction,[0],[0]
"Although repair in native/non-native speaker talk has been intensively studied in Conversation Analysis (CA) (Markee, 2000; Gardner and Wagner, 2004; Hosoda, 2006), the results have not been operationalized for an implementation in a CommICALL system.",1 Introduction,[0],[0]
"Therefore, this article has two objectives:
1.",1 Introduction,[0],[0]
"Identify typical interactional resources employed for initiation and carry-out of repair using methods of Conversation Analysis.
2.",1 Introduction,[0],[0]
"Create a computation models of the repair
395
of the type OISRL to be implemented in a CommICALL application.
",1 Introduction,[0],[0]
"We use a dataset of German native/non-native instant messaging conversations (Höhn, 2015) to analyze practices of repair in native/non-native speaker informal chat.",1 Introduction,[0],[0]
All repair sequences have been annotated.,1 Introduction,[0],[0]
Collections of similar cases have been built.,1 Introduction,[0],[0]
Interactional resources used by language learners for repair initiations have been analyzed.,1 Introduction,[0],[0]
Patterns of repair initiations have been obtained through generalization.,1 Introduction,[0],[0]
"In this way, rules for recognition of repair initiations have been created.",1 Introduction,[0],[0]
An implementation case study was set up to validate the resulting computational models in an AIML-based chatbot.,1 Introduction,[0],[0]
Non-native speakers are usually not considered as the main user group of general-purpose dialogue systems.,2 Repair in Conversational Agents,[0],[0]
The assumption dominates that human users understand everything what an agent may say.,2 Repair in Conversational Agents,[0],[0]
"This assumption is reflected in the two main problems addressed by research on repair for conversational agents: dealing with user’s selfcorrections which may make speech recognition difficult and managing system’s lack of information in order to satisfy user’s request.
",2 Repair in Conversational Agents,[0],[0]
"These two research areas may be found under keywords self-repairs, sometimes speech repairs (Zwarts et al., 2010) or disfluencies (Shriberg, 1994; Martin and Jurafsky, 2009), and clarification dialogues or clarification requests, CRs in AI and NLP publications.",2 Repair in Conversational Agents,[0],[0]
"What is referred to by the term self-repair in speech recognition domain corresponds to user’s self-initiated self-repair in CA terminology.
",2 Repair in Conversational Agents,[0],[0]
Shriberg (1994) uses the term reparandum to refer to what is called trouble source in CA.,2 Repair in Conversational Agents,[0],[0]
The model considers pauses (moment of interruption) and lexicalised means to focus on the replacement (editing terms).,2 Repair in Conversational Agents,[0],[0]
"These are interactional recourses used by speakers to signal trouble in production and to pre-announce a coming replacement.
",2 Repair in Conversational Agents,[0],[0]
"The term clarification dialogues is mostly used to describe repairs dealing with insufficient information available for a system after speech recognition and language understanding (Kruijff et al., 2008; Jian et al., 2010; Buß and Schlangen, 2011).",2 Repair in Conversational Agents,[0],[0]
"The term miscommunication was introduced to distinguish between non-understandings (the system could not match user’s input to a representa-
tion) and misunderstandings (the system matched user’s input to a wrong representation)",2 Repair in Conversational Agents,[0],[0]
"(Dzikovska et al., 2009; Meena et al., 2015).",2 Repair in Conversational Agents,[0],[0]
"These repair types correspond to other-initiated self-repair when the user is the trouble-speaker.
",2 Repair in Conversational Agents,[0],[0]
"Clarification requests in AI and NLP publications should not be confused with clarification requests in SLA publications where this term is used to refer to only a particular form of corrective feedback (Lyster et al., 2013), or to a dialogue move in meaning negotiations (Varonis and Gass, 1985).
",2 Repair in Conversational Agents,[0],[0]
"Emphasising the importance of correct recognition of user’s clarification requests, Purver (2004) provides a study of various types of clarification requests, see also follow-up publications (Purver, 2006; Ginzburg et al., 2007; Ginzburg, 2012).",2 Repair in Conversational Agents,[0],[0]
Purver (2004) uses the HPSG framework to cover the main classes of the identified classification scheme.,2 Repair in Conversational Agents,[0],[0]
"Because different functions might be expressed by a clarification request of the same form, Purver (2004) analyses the clarification readings to cover the correspondence between the form and the meaning of the repair initiations.",2 Repair in Conversational Agents,[0],[0]
"However, several points for critiques arise.",2 Repair in Conversational Agents,[0],[0]
"For instance, some utterances may be formatted as repair initiations but have a different interactional function, such as expressing surprise and topicalization (not listed as possible readings).",2 Repair in Conversational Agents,[0],[0]
"In addition, repair initiations designed to deal with troubles in understanding are put together with strategies for dealing with troubles in production (e.g. gap fillers).",2 Repair in Conversational Agents,[0],[0]
"From the CA perspective, Purver (2004)’s gap fillers correspond to self-initiated other-repair, thus are sequentially completely different.",2 Repair in Conversational Agents,[0],[0]
"Therefore, modifications in the classification proposed by (Purver, 2004) are needed in order to better comply with studies in CA, and therefore better reflect the state-of-the-art in CA-informed dialogue research.
",2 Repair in Conversational Agents,[0],[0]
Example 2.1.,2 Repair in Conversational Agents,[0],[0]
"Different types of causes for clarification used in (Schlangen, 2004, Ex. (12)).
",2 Repair in Conversational Agents,[0],[0]
a. A I ate a Pizza with chopsticks the other day B A Pizza with chopsticks on it?,2 Repair in Conversational Agents,[0],[0]
b.,2 Repair in Conversational Agents,[0],[0]
A Please give me a double torx.,2 Repair in Conversational Agents,[0],[0]
B,2 Repair in Conversational Agents,[0],[0]
What’s a torx?,2 Repair in Conversational Agents,[0],[0]
c. A,2 Repair in Conversational Agents,[0],[0]
Please give me a double torx.,2 Repair in Conversational Agents,[0],[0]
B Which one?,2 Repair in Conversational Agents,[0],[0]
d.,2 Repair in Conversational Agents,[0],[0]
"A Every wire has to be connected to a power source.
",2 Repair in Conversational Agents,[0],[0]
"B Each to a different one, or can it be the same for every wire?
",2 Repair in Conversational Agents,[0],[0]
"Schlangen (2004) analyses communication problems leading to clarification requests focusing on
trouble source types (what caused the communication problem).",2 Repair in Conversational Agents,[0],[0]
"Schlangen (2004) makes clear that a more fine-grained classification of causes for requesting clarification in dialogue may be needed, specifically, a model distinguishing between different cases in Example 2.1.
",2 Repair in Conversational Agents,[0],[0]
"From the CA perspective, speakers’ linguistic and professional identities and preferences play a role in speaker’s selection of a specific format of a repair initiation.",2 Repair in Conversational Agents,[0],[0]
Speaker B in Example 2.1.b.,2 Repair in Conversational Agents,[0],[0]
"positions herself as a novice in torx matters with her repair initiation, while speakers B in Examples 2.1.c. positions herself as knowledgeable in torx matters.",2 Repair in Conversational Agents,[0],[0]
"In addition, utterances may be designed as repair initiations, but may in fact have a different function.",2 Repair in Conversational Agents,[0],[0]
"For instance, the repair initiation produced by B in Example 2.1.a. may be analysed as a joke not requiring any explanation.
",2 Repair in Conversational Agents,[0],[0]
"Other-initiated self-repair when the machine is the trouble-speaker is explored in (Gehle et al., 2014).",2 Repair in Conversational Agents,[0],[0]
"Based on a corpus of video-recorded human-robot-interactions in a museum, the authors analyse interactional resources used by museum visitors to signal troubles in understanding robot’s talk and dealing with misunderstandings.",2 Repair in Conversational Agents,[0],[0]
"It was observed that people deal with different sorts of trouble similarly.
",2 Repair in Conversational Agents,[0],[0]
The potential user of a CommICALL system is a language learner who may have troubles in comprehension.,2 Repair in Conversational Agents,[0],[0]
"While user-initiated repair has been subject of research of studies in human-robot interaction and general dialogue systems, not much attention has been paid to it in CommICALL.",2 Repair in Conversational Agents,[0],[0]
This article seeks to contribute to the research on repair in CommICALL by a microanalytic study of sequences of other-initiated self-repair when the native speaker is the trouble-speaker.,2 Repair in Conversational Agents,[0],[0]
"Based on the results of the empirical study, the problem of computational modeling of system’s reaction to the learner’s repair initiation will be approached.",2 Repair in Conversational Agents,[0],[0]
"The machine will need to recognize repair initiations, to extract the trouble source and to deliver an appropriate response.",2 Repair in Conversational Agents,[0],[0]
The the study contributes to language understanding for dialogue systems targeting language learners and has implications for user and expert models for CommICALL.,2 Repair in Conversational Agents,[0],[0]
"This section analyses interactional resources used by the non-native speakers in chat in order to other-initiate repair with a linguistic trouble
source, that is to signal trouble and to reference the trouble source.",3 Practices of repair in chat,[0],[0]
Turn formats are specifically important for the future recognition of repair initiations by chatbots.,3 Practices of repair in chat,[0],[0]
"Two abstract types of repair other-initiations were identified in the dataset: statements of nonunderstanding where a part of partner’s utterance is marked as unclear, and candidate understandings where the own version of understanding of the problematic unit is provided.",3.1 Repair initiations,[0],[0]
"Nonunderstandings require an explanation of the trouble source in the repair while candidate understandings require a yes/no answer.
",3.1 Repair initiations,[0],[0]
Repair other-initiations were found at two distinct types of position: immediate and delayed.,3.1 Repair initiations,[0],[0]
The first type comes immediately after the trouble source turn.,3.1 Repair initiations,[0],[0]
The second type comes later than the adjacent turn.,3.1 Repair initiations,[0],[0]
"Sequentially, both correspond to the next-turn repair initiation or second position repair described in CA literature as the first structurally specified place for other-initiated repair (Schegloff, 2000; Liddicoat, 2011).",3.1 Repair initiations,[0],[0]
Delayed repair initiations occur because speakers in chat can produce turns simultaneously and follow distinct interleaved conversation threads.,3.1 Repair initiations,[0],[0]
There is a dependency between the position of the repair initiation and the interactional recourses for repair initiation.,3.1 Repair initiations,[0],[0]
"Some resources are used exclusively in the immediate position.
",3.1 Repair initiations,[0],[0]
Example 3.1.,3.1 Repair initiations,[0],[0]
"Open class repair initiation
615 L08 danke.",3.1 Repair initiations,[0],[0]
good night),3.1 Repair initiations,[0],[0]
thank you.,3.1 Repair initiations,[0],[0]
good night 617 N04 gn8 :-),3.1 Repair initiations,[0],[0]
618 L08 ??? ???,3.1 Repair initiations,[0],[0]
"[repair initiation] 619 N04 gn8 ist ein zusammengeschrumpftes ""gute
Nacht"" (lies: ""g"" = ""gut"" und ""n8"" = ""N-Acht"") gn8 is an abbreviation of ""good night"" (read: ""g""=""good"" and ""n8"" = ""n-ight"")
620 N04 oder englisch, g=good, n-eight or English, g=good, n-eight 621 L08 aach sooo))",3.1 Repair initiations,[0],[0]
"I see
In Example 3.1, the learner initiates a repair by posting three question marks directly after the trouble source turn.",3.1 Repair initiations,[0],[0]
"The native speaker N04 is able to locate the trouble source, which is the abbreviation.",3.1 Repair initiations,[0],[0]
"In Example 3.1, the reference to the trouble source is realised by the immediate adjacent position, and signaling trouble with comprehension is realised by the questions marks.
",3.1 Repair initiations,[0],[0]
"Candidate understanding is another possibility
to mark a unit of an utterance as not (completely) clear.",3.1 Repair initiations,[0],[0]
Example 3.2 shows a fragment of a chat where the native speaker N04 uses the word überfülltes to describe an event in Munich (turn 222).,3.1 Repair initiations,[0],[0]
The learner L08 checks her understanding of this term in turn 223 by copying the trouble source and providing her own understanding of the word.,3.1 Repair initiations,[0],[0]
The trouble source is referenced through its repetition in the repair initiation.,3.1 Repair initiations,[0],[0]
"Signalling trouble is realised through the comparison token, the candidate understanding and the question mark.
",3.1 Repair initiations,[0],[0]
Example 3.2.,3.1 Repair initiations,[0],[0]
"Many many people
221 L08 ja ich habe über Oktoberfest gehört, etwas lustiges und buntes))",3.1 Repair initiations,[0],[0]
"yes I have heard about Oktoberfest, something funny and colourful 222 N04 ja, und teures und überfülltes ;-) yes, and expensive and overfilled 223 L08 ))",3.1 Repair initiations,[0],[0]
"überfülltes bedeutet ""viele viele Leute""?",3.1 Repair initiations,[0],[0]
"overfilled means ""many many people""?",3.1 Repair initiations,[0],[0]
"224 N04 genau exactly
The repair initiations produced by the learners in the dataset always try to resolve problems with the meaning, none of them was concerned with the form by itself.",3.1 Repair initiations,[0],[0]
"Repair carry-out strategies depend on the type of the trouble source and the repair initiation format and include confirmations / disconfirmations, definition work and paraphrasing of the trouble source.",3.2 Repair carry-out,[0],[0]
"Direct definition work can be replaced or extended by a hyperlink to an example or a demonstration of an instance of the trouble source.
",3.2 Repair carry-out,[0],[0]
"If the trouble source is an abbreviation, the definition work contained a full spelling of the abbreviated words and their explanation.",3.2 Repair carry-out,[0],[0]
"For chat abbreviations, a full reading of the abbreviation was normally provided and enough for explanation, as Example 3.1 demonstrates.",3.2 Repair carry-out,[0],[0]
"Problematic abbreviation were always repeated in the dataset, followed by the full spelling or reading.
",3.2 Repair carry-out,[0],[0]
"If the trouble source is one semantic unit (one word or an idiomatic expression), a dictionary-like definition (synonyms + examples) is often selected to provide a repair.",3.2 Repair carry-out,[0],[0]
"For longer messages or longer parts of longer messages, a strategy of splitting the message into smaller semantic units and a separate explanation of each unit can be chosen.",3.2 Repair carry-out,[0],[0]
"Paraphrasing is also one of the strategies used by the native speakers to explain longer messages.
",3.2 Repair carry-out,[0],[0]
"Example 3.3 shows how a machine translation
service can be used for definition work.",3.2 Repair carry-out,[0],[0]
"Turn 376 contains an expression that the learner does not (fully) understand: ""in sachen essen"".",3.2 Repair carry-out,[0],[0]
This expression is being formally made to a trouble source in the repair initiation in turns 377 and 378.,3.2 Repair carry-out,[0],[0]
Turn 377 locates the trouble source and marks the expression as unclear.,3.2 Repair carry-out,[0],[0]
"Turn 378 contains an instruction of what kind of explanation is desired.
",3.2 Repair carry-out,[0],[0]
Example 3.3.,3.2 Repair carry-out,[0],[0]
"In Sachen Essen: repair is carried out with the help of machine translation.
",3.2 Repair carry-out,[0],[0]
376 N03 gibt es irgendwas moskau typisches in sachen essen?,3.2 Repair carry-out,[0],[0]
is there something of food which is typical for moscow?,3.2 Repair carry-out,[0],[0]
377 L07 in sachen essen???,3.2 Repair carry-out,[0],[0]
in things food???,3.2 Repair carry-out,[0],[0]
378 L07 übersetze bitte))) translate please [smile] 379 N03 какая пища является типичным Москве?,3.2 Repair carry-out,[0],[0]
which food is typical for Moscow?,3.2 Repair carry-out,[0],[0]
"Regarding repair initiations, it was found that: (1) Questioning is the practice to initiate repair in chat, confirming the results in the academic literature for oral interaction (Dingemanse et al., 2014).",4 Empirical findings,[0],[0]
"Other practices are declarations of lack of understanding such as unklar and ich verstehe nicht.
",4 Empirical findings,[0],[0]
"(2) Devices for signalling are question marks, dashes, explicit statements of non-understanding and presenting candidate understandings.
",4 Empirical findings,[0],[0]
"(3) References to trouble sources may be realised through the adjacent position, demonstrative expressions and full or partial repeats.
",4 Empirical findings,[0],[0]
"(4) Though all repair initiations were secondposition initiations, they were not all immediate.",4 Empirical findings,[0],[0]
"Delayed repair initiation require more specific referencing to trouble source, open-class repair initiations cannot be used in a delayed second position.
",4 Empirical findings,[0],[0]
(5) Repetition-based repair initiations may contain repetitions of one specific unit from the previous turn and contain a copy of the preceding turn regardless the unit boundaries.,4 Empirical findings,[0],[0]
The latter may be placed between open class and restricted class repair initiations.,4 Empirical findings,[0],[0]
"Such types of repetitions have not been previously described in the academic literature and may be typical for non-native speakers.
",4 Empirical findings,[0],[0]
(6) The communication medium influences repair initiation types and formats.,4 Empirical findings,[0],[0]
"In particular, repair initiations eliciting a repetition of the trouble source are uncommon in chat.",4 Empirical findings,[0],[0]
"Misreadings are
possible, but they are made visible through misproductions in repetition-based repair initiations.
",4 Empirical findings,[0],[0]
"(7) The non-native speakers’ identity influences the format of candidate understandings which differ from those in native speaker talk.
",4 Empirical findings,[0],[0]
(8) Repair initiation is one option to deal with trouble in comprehension.,4 Empirical findings,[0],[0]
"Other options include dictionary look-up and the ""let-it-pass"" strategy.
",4 Empirical findings,[0],[0]
"Regarding repair carry-outs, it was found that: (1) Explanations of the meaning through synonyms or paraphrases, translations and demonstrations are common forms of repair carry-outs.
(2) Repair design is linked to expectation of what is known to the repair recipient.",4 Empirical findings,[0],[0]
"Consequently, repairs are designed for the language learners targeting difficulties in linguistic matters.
",4 Empirical findings,[0],[0]
(3) Repair carry-outs may be immediate and delayed.,4 Empirical findings,[0],[0]
"Consequently, references to trouble source may be realised by the same resources as for repair initiations.",4 Empirical findings,[0],[0]
"However, there are dependencies between types of trouble source and participants’ selection of resources for referencing the trouble source.",4 Empirical findings,[0],[0]
"For instance, abbreviations are usually repeated.
",4 Empirical findings,[0],[0]
(4) Split-repeat is a type of a reference to the trouble source which did not appear in repair other-initiations but was found in the corresponding self-repair carry-outs.,4 Empirical findings,[0],[0]
This way of referencing corresponds to self-repairs where native speakers only explained a few words from a longer turn or longer part of a turn marked as a trouble source.,4 Empirical findings,[0],[0]
"The trouble source was split in tokens, and only tokens that were supposed to cause the trouble were explained.
",4 Empirical findings,[0],[0]
"Repair carry-out is the preferred and the most frequent response to a repair initiation but other forms of responses are also possible, for instance a new repair initiation to deal with difficulties in identification of the trouble and responses which do not address the trouble.",4 Empirical findings,[0],[0]
"Finally, repair initiation and carry-out formats need to be ""translated"" into patterns and then into computational models of repair to make the findings applicable for computational purposes.
",4 Empirical findings,[0],[0]
"5 Computational model of OISRL
",4 Empirical findings,[0],[0]
"In order to ""serve computational interests"" (Schegloff, 1996), the following needs to be taken into account for the purpose of modelling.",4 Empirical findings,[0],[0]
"Because repair initiations may occur everywhere, each user’s utterance may be a repair initiation.",4 Empirical findings,[0],[0]
"Therefore, a
repair initiation recognition routine needs to be activated after every user’s turn.",4 Empirical findings,[0],[0]
"Two essential problems must be solved by a computer program in order to react to a repair initiation properly: (1) Recognition of a repair initiation, (2) Extraction of the trouble source.
",4 Empirical findings,[0],[0]
A repair proper needs to be generated after that.,4 Empirical findings,[0],[0]
Each class of repair initiations implies a specific form of referencing the trouble source.,5.1 Recognition of repair initiations,[0],[0]
"We consider the following types of referencing for modelling of the OISRL-sequences:
1.",5.1 Recognition of repair initiations,[0],[0]
Repeat-based initiations:,5.1 Recognition of repair initiations,[0],[0]
"reuse (a 1:1-copy of the trouble source), recycle the trouble source (rewriting it in a slightly different way),
2.",5.1 Recognition of repair initiations,[0],[0]
"Demonstratives-based initiations: using demonstrative determiners and pronouns.
",5.1 Recognition of repair initiations,[0],[0]
3.,5.1 Recognition of repair initiations,[0],[0]
Open-class initiations: referencing by a statement of non-understanding in the immediate position.,5.1 Recognition of repair initiations,[0],[0]
The adjacent position of the repair initiation references the whole preceding turn as a trouble turn.,5.1 Recognition of repair initiations,[0],[0]
"Therefore we refer to this type of referencing as reference by position.
",5.1 Recognition of repair initiations,[0],[0]
Each class of repair initiations references trouble of a particular size: either it is the whole preceding message (open-class and demonstratives-based repair initiations) or it is only a part of it (repeatbased and recycle-based initiations).,5.1 Recognition of repair initiations,[0],[0]
"Therefore, we consider three cases of trouble sources: single word (part of a longer message or a one-word message), part of a message (PoM) of two or more words and a whole message consisting of two or more words.
",5.1 Recognition of repair initiations,[0],[0]
Signalling trouble involves symbolic and/or lexicalised means and a specific format designed either to mark something as unclear or to compare the trouble source with the own version of understanding.,5.1 Recognition of repair initiations,[0],[0]
"We call this signalling format.
",5.1 Recognition of repair initiations,[0],[0]
The architecture of the repair initiation (RI) for OISRL can be formalised as follows.,5.1 Recognition of repair initiations,[0],[0]
"Depending on the time, different formats for the repair initiation may be used:
RI = TIME ×RIFormat Time may be immediate or delayed: TIME = {immediate, delayed}.",5.1 Recognition of repair initiations,[0],[0]
"A repair initiation format is a combination of a reference to the trouble source and a selected signalling format:
RIFormat = REF × SignalFormat
The referencing types are repeat-based repeat(x), based on demonstratives Dem and reference by position AP .",5.1 Recognition of repair initiations,[0],[0]
"Signalling format may mark something in the trouble-turn as unclear unclear(x) or present a candidate understanding equals(x, y).",5.1 Recognition of repair initiations,[0],[0]
"The trouble source x and the candidate understanding y may be a single word, an idiomatic expression, part of a message or a complete turn (utterance).
",5.1 Recognition of repair initiations,[0],[0]
"REF = {repeat(x), AP, Dem} SignalFormat = {unclear(x), equals(x, y)} x, y ∈ {word, idiom, PoM, utterance}
This repair recognition procedure is also expected to differentiate between ordinary questions related to the subject of the ongoing talk and repair initiations.",5.1 Recognition of repair initiations,[0],[0]
"It works because ordinary questions are not formatted as unclear(x) or equals(x, y).
",5.1 Recognition of repair initiations,[0],[0]
"If a complete turn is recognised as a trouble source and this turn is a longer message, further filters may be applied to identify more precisely, which of the parts of the longer message may cause a problem with comprehension.",5.1 Recognition of repair initiations,[0],[0]
"This may be influenced by the learner model, but also by the system’s capabilities to generate a repair proper.",5.1 Recognition of repair initiations,[0],[0]
Section 5.3 will address this problem and provide examples of possible filters.,5.1 Recognition of repair initiations,[0],[0]
"Repair carry-outs can contain a lexical reference to the trouble source, such as repeat-based and demonstratives-based references, or point to it just by the adjacent position to the repair initiation.
",5.2 Generation of a repair carry-out,[0],[0]
"A confirmation or a disconfirmation is an appropriate type of self-repair carry-out after a repair other-initiation presenting candidate understandings equals(x, y).",5.2 Generation of a repair carry-out,[0],[0]
All other self-repair carry-outs are expected to provide an explanation of the unit that is marked as problematic explain(x).,5.2 Generation of a repair carry-out,[0],[0]
"Because different options are available for referencing trouble source in immediate and delayed repair carry-outs, time needs to be taken into account in the abstract description:
RCO = TIME ×RCOFormat TIME = {immediate, delayed}.
",5.2 Generation of a repair carry-out,[0],[0]
"A self-repair carry-out is a product of a reference to the trouble source and the function RCOF , which it is expected to perform: confirming/disconfirming answer or an explanation.
",5.2 Generation of a repair carry-out,[0],[0]
"RCOFormat = REF ×RCOF REF = {repeat(x), AP, Det, splitRepeat(x)} RCOF = {explain(x), conf(equals(x, y))}
Delayed self-repairs need to update the focus of the talk, and therefore, a repeat-based reference makes more sense than other types of referencing.
",5.2 Generation of a repair carry-out,[0],[0]
"In practice, the function explain(x) needs to be implemented differently for different types of trouble source.",5.2 Generation of a repair carry-out,[0],[0]
The quality of the response is highly dependent on the linguistic resources available for the generation of the explanations.,5.2 Generation of a repair carry-out,[0],[0]
We discuss various practical issues in the next section.,5.2 Generation of a repair carry-out,[0],[0]
The purpose of this section is to validate the practical applicability of the abstract model described in the preceding section.,5.3 Model validation,[0],[0]
"Because language understanding and generation capabilities of each dialogue system determines the possibilities for implementation of the OISRL model, we took the simplest form of such a system, namely an AIMLbased chatbot (Bush, 2006).",5.3 Model validation,[0],[0]
"AIML (Artificial Intelligence Markup Language) covers the language understanding and generation task (Droßmann, 2005) in form of pattern-template pairs shown below.",5.3 Model validation,[0],[0]
"If the chatbot finds an input that matches to WIE GEHTS, the utterance stored in the template tag will be delivered to the user as a response.",5.3 Model validation,[0],[0]
"<category>
<pattern>WIE GEHTS</pattern> <template>Gut, und selbst?",5.3 Model validation,[0],[0]
"Alles paletti?</template>
</category>
Example 5.1 illustrates how a chatbot can benefit from patterns extracted from the dataset to come closer to the behaviour of a language expert.",5.3 Model validation,[0],[0]
Example 5.1.,5.3 Model validation,[0],[0]
A sub,5.3 Model validation,[0],[0]
"-dialogue with the chatbot: other-initiated self-repair where the chatbot is the trouble-speaker.
",5.3 Model validation,[0],[0]
1 User wie gehts?,5.3 Model validation,[0],[0]
how are you?,5.3 Model validation,[0],[0]
"2 Bot Gut, und selbst?",5.3 Model validation,[0],[0]
Alles paletti?,5.3 Model validation,[0],[0]
"I’m fine, and you?",5.3 Model validation,[0],[0]
Everything okay?,5.3 Model validation,[0],[0]
3 User paletti?,5.3 Model validation,[0],[0]
"4 Bot umgangssprachlich alles gut, alles in Ordnung,
alles okay.",5.3 Model validation,[0],[0]
"colloquial everything good, everything fine, everything okay.
",5.3 Model validation,[0],[0]
The bot uses a colloquial expression in turn 2 which is not clear for the user.,5.3 Model validation,[0],[0]
The user initiates the repair in turn 3.,5.3 Model validation,[0],[0]
The bot recognises turn 3 as a repair initiation and extracts the trouble source: the repeated word paletti and the corresponding idiomatic expression alles paletti.,5.3 Model validation,[0],[0]
"Bot’s response in turn 4 is a repair carry-out generated from a linguistic database.
",5.3 Model validation,[0],[0]
The work of the repair manager is organised in two steps determined by the model.,5.3 Model validation,[0],[0]
"Every user’s
input that requires an explanation of a single entity (word, idiom) is redirected to the category that implements this function.",5.3 Model validation,[0],[0]
The implementation of ProgramD includes so called processors to process specific AIML tags.,5.3 Model validation,[0],[0]
A new AIML tag has been introduced for the purpose of this work: <explanation>.,5.3 Model validation,[0],[0]
"An additional processor named explanation processor has been implemented to generate a response.
",5.3 Model validation,[0],[0]
The model for the recognition of repair initiations described in Section 5.1 is used for the implementation in form of the rules describing repair initiation formats.,5.3 Model validation,[0],[0]
"For instance, to recognise the repair initiation from Example 5.1, the chatbot matches the rule:
RI = immediate, repeat(x), unclear(x) because the user repeats a part of bot’s utterance placing a question mark after the repeated token and it happens immediately after the bot’s turn.
",5.3 Model validation,[0],[0]
"In Example 5.1, the repair initiation contains only a part of an idiomatic expression and only the entire expression can be found in the linguistic database.",5.3 Model validation,[0],[0]
"Because all chatbot’s utterances are known beforehand in AIML-based chatbots, it is possible to list all idioms to make their recognition easier.",5.3 Model validation,[0],[0]
"For this test implementation, a short list of idiomatic expressions and their parts was created.",5.3 Model validation,[0],[0]
"The explanation processor would first check, if the trouble source may be an idiom (comparing with the list and own preceding turns).",5.3 Model validation,[0],[0]
"If so, the entire expression will be set as the trouble source.
",5.3 Model validation,[0],[0]
AIML provides a possibility to forward inputs with the same or similar meanings to a particular category handling responses to this meaning.,5.3 Model validation,[0],[0]
"Int this way, all recognised repair initiations with the meaning unclear(x) are redirected to the category with the pattern:
<pattern>ICH VERSTEHE * NICHT</pattern>
where * is the matching token for the trouble source",5.3 Model validation,[0],[0]
"x.
",5.3 Model validation,[0],[0]
The following template is responsible for the generation of repair carry-outs for all such trouble sources.,5.3 Model validation,[0],[0]
The <think> tag allows processing of an input without without immediate output.,5.3 Model validation,[0],[0]
"The explanation processor searches for the trouble source in the linguistic database which contains only meanings, examples and notes about usage for German nouns, verbs, adjectives and adverbs.",5.3 Model validation,[0],[0]
The database was automatically generated from Wiktionary.,5.3 Model validation,[0],[0]
"If the trouble source cannot be found in the linguistic database, the ex-
planation processor returns <NOENTITY> and the pre-stored Response-1 is sent to the user.",5.3 Model validation,[0],[0]
"If the trouble source is found but its meaning is not stored in the database, the explanation processor returns <ENTITY NOMEANING>.",5.3 Model validation,[0],[0]
A predefined Response-2 is then sent to the user.,5.3 Model validation,[0],[0]
"Finally, if the explanation processor finds the trouble source in the database and at least one meaning of it is described, an explanation will be rendered.",5.3 Model validation,[0],[0]
"Five additional categories not shown here are responsible for rendering of the explanation and process meanings, examples and notes.",5.3 Model validation,[0],[0]
"<template> <think> <set name=""explanation-tmp""> <explanation><star/></explanation>
</set> </think",5.3 Model validation,[0],[0]
"> <condition name=""explanation-tmp""> <li value=""NOENTITY"">Response-1</li> <li value=""ENTITY NOMEANING"">
Response-2</li> <",5.3 Model validation,[0],[0]
"li><srai>GETFIRSTMEANING <get name=""explanation-tmp""/></srai>
</li",5.3 Model validation,[0],[0]
>,5.3 Model validation,[0],[0]
"</condition> </template>
Every user’s input that corresponds to an inquiry ""does x mean y?"" is redirected to the AIML category implementing meaning checks.",5.3 Model validation,[0],[0]
An additional tag <meaningcheck> has been added to carry out the repair of this type.,5.3 Model validation,[0],[0]
The handling of the meaning checks works in a similar way as the explanations described above.,5.3 Model validation,[0],[0]
The program has been extended by a meaning check processor to process this tag in the following way.,5.3 Model validation,[0],[0]
"To generate a response to a candidate understanding, the chatbot needs to answer the question if x means the same as y?",5.3 Model validation,[0],[0]
This is an instance of the textual entailment problem.,5.3 Model validation,[0],[0]
"If x is a single word, an idiom, a collocation or a proverb, the system can check the list of the synonyms of the corresponding entry in the linguistic database.",5.3 Model validation,[0],[0]
"If x and y are listed as synonyms, a confirming answer will be generated.",5.3 Model validation,[0],[0]
"Otherwise, the system will explain the meaning of x.
Only simple versions for each of paraphrasing and word-by-word explanation (split-reuse) were implemented.",5.3 Model validation,[0],[0]
A word-by-word explanation only makes sense for words that could be difficult for the learner.,5.3 Model validation,[0],[0]
We use a list of 100 and 1000 most frequently used German words1 to filter those words that are supposed to be well known to everybody.,5.3 Model validation,[0],[0]
"The remaining words are explained separately.
1http://wortschatz.uni-leipzig.de/html/wliste.html",5.3 Model validation,[0],[0]
The new model of other-initiated self-repair when the machine is the trouble-speaker allows recognising learner repair initiations and extracting the trouble source based on a description of languagespecific and medium-specific resources for repair initiation.,6 Results,[0],[0]
The model is created on a necessary level of abstraction to be applicable for text chat interaction in languages other than German.,6 Results,[0],[0]
"This assumption builds on (Dingemanse et al., 2014)’s finding that similar repair initiation formats exist across languages.",6 Results,[0],[0]
"Therefore, when provided a set of language-specific devices for repair initiation, it can be implemented for other languages.",6 Results,[0],[0]
The extraction of the trouble source is based on abstract features like repetition of parts of the trouble-turn and adjacent position.,6 Results,[0],[0]
"These features are language independent.
",6 Results,[0],[0]
"The problem of the trouble source extraction is related to referring expression recognition or reference resolution described in NLP textbooks (Martin and Jurafsky, 2009, Ch. 21), which is addressed in a large number of scientific publications (Dahan et al., 2002; Iida et al., 2010).",6 Results,[0],[0]
Usually only noun phrases or their pronominalised alternatives are considered for reference resolution in NLP.,6 Results,[0],[0]
"These are usually definite and indefinite noun phrases, pronouns, demonstratives and names.",6 Results,[0],[0]
The analysis of repair initiations shows that verbs or parts of utterances may be used to refer to the trouble source.,6 Results,[0],[0]
"The presented model implicitly includes a local discourse model which ""contains representations of entities which have been referred to in the discourse"" (Martin and Jurafsky, 2009, p. 730).",6 Results,[0],[0]
"The local discourse model in repair sequences only conserns possible representations of the trouble source.
",6 Results,[0],[0]
"Compared to the model of clarification requests proposed in (Purver, 2004), the model introduced in this work has the following advantages.",6 Results,[0],[0]
"First, the inconsistencies form CA perspective found in (Purver, 2004)’s classification do not exist in the model presented in this work because of a close cross-disciplinary connection with CA.",6 Results,[0],[0]
The model for repair initiations presented here strictly differentiates next-turn repair other-initiations from all other types of repair and describes only these repair initiations.,6 Results,[0],[0]
"Second, (Purver, 2004) introduced the model for clarification requests in a strong connection to the HPSG formalism.",6 Results,[0],[0]
"In contrast, the model presented in this work is already imple-
mentable with a simple language understanding technology.",6 Results,[0],[0]
"The separation between resources for signalling trouble and resources for referencing trouble source allows creating a rule-based grammar which can be implemented in dialogue systems with different levels of complexity.
",6 Results,[0],[0]
"With regard to the analysis of causes of troubles in understanding introduced in (Schlangen, 2004), mainly problems on the level of meaning and understanding were subject of learner’s repair initiations.",6 Results,[0],[0]
"Consequently, the modelling was approached in this work with the assumption that the required kind of clarification is mainly determined by the user model targeting language learners.",6 Results,[0],[0]
"Similarly to the (Schlangen, 2004)’s approach to map the variance in form to a small number of readings, repair initiations in this work are mapped either to a content question What does X mean?",6 Results,[0],[0]
or to a polar question Does X mean Y?,6 Results,[0],[0]
where X is the trouble source and Y is the candidate understanding.,6 Results,[0],[0]
"In this way, the two approaches to modelling repair initiations are similar.
",6 Results,[0],[0]
"Models of repair covering repair initiations proposed in (Purver, 2004) and (Schlangen, 2004) and extended in follow-up work (Purver, 2006; Ginzburg et al., 2007; Ginzburg, 2012) were motivated by Conversation Analysis research.",6 Results,[0],[0]
"However, other approaches for modelling were preferred because of the insufficient operationalisation of CA findings for computational modelling.",6 Results,[0],[0]
"As an implication, the factors influencing the interaction that have been identified as important in CA studies and building a system did not become part of the baseline models in (Purver, 2004) and (Schlangen, 2004).",6 Results,[0],[0]
"Such factors include repair, turn taking, membership categorisation, adjacency pairs and preference organisation.",6 Results,[0],[0]
"In contrast to the previous models of repair (Purver, 2004; Schlangen, 2004) this work analyses repair initiations in a system of interconnected factors in conversation.",6 Results,[0],[0]
"More specifically, the proposed model of repair initiations takes turn taking and sequential organisation of interaction explicitly into account by distinguishing between immediate and delayed repair initiations and respective options for trouble source extraction.",6 Results,[0],[0]
"In addition, the new model takes virtual adjacency in chat into account.",6 Results,[0],[0]
It explicitly differentiates repair initiated by the user from repair initiated by the system taking the sequential organisation into account.,6 Results,[0],[0]
"Finally, the preference organisation and recipient design were
taken into account by the user model.",6 Results,[0],[0]
"Based on the empirical findings, the user model assumes that language learners will request a special kind of clarification.
",6 Results,[0],[0]
"While recognition of repair initiations and trouble source extraction can be implemented using the simplest type of language understanding, namely, pattern-based language understanding, most repair carry-outs require more sophisticated linguistic capabilities.
",6 Results,[0],[0]
Definitions provide an explanation of the trouble source.,6 Results,[0],[0]
Existing online dictionaries such as Wiktionary or Wikipedia may be used to create linguistic knowledge bases.,6 Results,[0],[0]
"Because one term may have multiple meanings, a linking to the correct meaning may be required.",6 Results,[0],[0]
"This problem is related to lexical ambiguity resolution also known as meaning resolution (Small et al., 1987) and is part of a larger area of computational lexical semantics (Martin and Jurafsky, 2009, Ch. 20).
",6 Results,[0],[0]
Paraphrases provide a reformulation of the trouble source.,6 Results,[0],[0]
A lot of efforts have been put in automatic paraphrase generation and recognition.,6 Results,[0],[0]
"Several recent publications are (Metzler et al., 2011; Regneri and Wang, 2012; Marton, 2013).
",6 Results,[0],[0]
Synonyms provide usually a short reformulation of the trouble source.,6 Results,[0],[0]
"Existing language resources such as WordNet (Fellbaum, 2010) and GermaNet (Hamp et al., 1997) can be used for finding synonyms.",6 Results,[0],[0]
"Multiple meanings of a word may need to be resolved.
",6 Results,[0],[0]
"Translations may be generated by using existing machine translation systems (Avramidis et al., 2015; Burchardt et al., 2014).",6 Results,[0],[0]
Open source statistical machine translation systems such as Moses2 make experimental implementations feasible.,6 Results,[0],[0]
"Commercial machine translation API can be integrated into the dialogue manager, for instance Google Translate API3.
Demonstrations include hyperlinks to websites containing relevant information examples of an object referenced by the trouble source.",6 Results,[0],[0]
"For semi-automatically created databases of linguistic knowledge, such information may be included into examples.",6 Results,[0],[0]
"Wikipedia articles sometimes also contain links to example websites and pictures, which may be used as examples of concepts described in the article.
",6 Results,[0],[0]
"Explicit handling of repairs targeted for lan-
2http://www.statmt.org/moses/ 3https://cloud.google.com/translate/docs
guage learners allows an implementation in a CommICALL system that helps to practice conversation.",6 Results,[0],[0]
"In this way, this research advances state-of-the-art in ICALL and strengthens multidisciplinary connections to related disciplines, such as Conversation Analysis and NLP.",6 Results,[0],[0]
Other types of tutorial dialogues where a clarification of the terminology may be necessary would also benefit from the presented model.,6 Results,[0],[0]
This article describes typical interactional resources employed for repair in native/non-native speaker chat with the purpose of computation modelling of repair for a conversational agent in a CommICALL application.,7 Conclusions,[0],[0]
The study shows that CA methods provide a valuable set of tools for computational modelling of rare phenomena in talk from a small number of examples.,7 Conclusions,[0],[0]
"To be successful, such approaches require datasets replicating the speech exchange systems that are envisioned in the communication with the agent.",7 Conclusions,[0],[0]
"In particular, this research showed that native/nonnative speaker chat data can be used for computational models of dialogues in a CommICALL application.",7 Conclusions,[0],[0]
"This research has been carried out at the University of Luxembourg with the great help and support of my scientific advisors Stephan Busemann (DFKI GmbH), Christoph Schommer, Gudrun Ziegler (MultiLearn Institute), Leon van der Torre and Charles Max.",Acknowledgements,[0],[0]
This article describes a model of otherinitiated self-repair for a chatbot that helps to practice conversation in a foreign language.,abstractText,[0],[0]
The model was developed using a corpus of instant messaging conversations between German native and non-native speakers.,abstractText,[0],[0]
Conversation Analysis helped to create computational models from a small number of examples.,abstractText,[0],[0]
The model has been validated in an AIML-based chatbot.,abstractText,[0],[0]
"Unlike typical retrieval-based dialogue systems, the explanations are generated at run-time from a linguistic database.",abstractText,[0],[0]
A data-driven model of explanations for a chatbot that helps to practice conversation in a foreign language,title,[0],[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 340–350, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics
We introduce a manually-created, multireference dataset for abstractive sentence and short paragraph compression. First, we examine the impact of single- and multi-sentence level editing operations on human compression quality as found in this corpus. We observe that substitution and rephrasing operations are more meaning preserving than other operations, and that compressing in context improves quality. Second, we systematically explore the correlations between automatic evaluation metrics and human judgments of meaning preservation and grammaticality in the compression task, and analyze the impact of the linguistic units used and precision versus recall measures on the quality of the metrics. Multi-reference evaluation metrics are shown to offer significant advantage over single reference-based metrics.",text,[0],[0]
"Automated sentence compression condenses a sentence or paragraph to its most important content in order to enhance writing quality, meet document length constraints, and build more accurate document summarization systems (Berg-Kirkpatrick et al., 2011; Vanderwende et al., 2007).",1 Introduction,[0],[0]
"Though word deletion is extensively used (e.g., (Clarke and Lapata, 2008)), state-of-the-art compression models (Cohn and Lapata, 2008; Rush et al., 2015) benefit crucially from data that can represent complex abstractive compression operations, including substitution of words and phrases and reordering.
",1 Introduction,[0],[0]
"∗This research was conducted during the author’s internship at Microsoft Research.
",1 Introduction,[0],[0]
This paper has two parts.,1 Introduction,[0],[0]
"In the first half, we introduce a manually-created multi-reference dataset for abstractive compression of sentences and short paragraphs, with the following features:
•",1 Introduction,[0],[0]
"It contains approximately 6,000 source texts with multiple compressions (about 26,000 pairs of source and compressed texts), representing business letters, newswire, journals, and technical documents sampled from the Open American National Corpus (OANC1).
",1 Introduction,[0],[0]
• Each source text is accompanied by up to five crowd-sourced rewrites constrained to a preset compression ratio and annotated with quality judgments.,1 Introduction,[0],[0]
"Multiple rewrites permit study of the impact of operations on human compression quality and facilitate automatic evaluation.
",1 Introduction,[0],[0]
"• This dataset is the first to provide compressions at the multi-sentence (two-sentence paragraph) level, which may present a stepping stone to whole document summarization.",1 Introduction,[0],[0]
"Many of these two-sentence paragraphs are compressed both as paragraphs and separately sentence-bysentence, offering data that may yield insights into the impact of multi-sentence operations on human compression quality.
",1 Introduction,[0],[0]
"• A detailed edit history is provided that may allow fine-grained alignment of original and compressed texts and measurement of the cognitive load of different rewrite operations.
",1 Introduction,[0],[0]
"Our analysis of this dataset reveals that abstraction has a significant positive impact on meaning preservation, and that application of trans-sentential
1http://www.anc.org/data/oanc
340
context has a significant positive impact on both meaning preservation and grammaticality.
",1 Introduction,[0],[0]
"In the second part, we provide a systematic empirical study of eighty automatic evaluation metrics for text compression using this dataset, correlating them with human judgments of meaning and grammar.",1 Introduction,[0],[0]
"Our study shows strong correlation of the best metrics with human judgments of meaning, but weaker correlations with judgments of grammar.",1 Introduction,[0],[0]
We demonstrate significant gains from multiple references.,1 Introduction,[0],[0]
"We also provide analyses of the impact of the linguistics units used (surface n-grams of different sizes versus parse-based triples), and the use of precision versus recall-based measures.",1 Introduction,[0],[0]
"Prior studies of human compression: Clarke (2008) studied the properties of manually-collected deletion-based compressions in the news genre, comparing them with automatically-mined data from the Ziff-Davis corpus in terms of compression rate, length of deleted spans, and deletion probability by syntactic constituent type.",2 Related Work,[0],[0]
"Jing and McKeown (1999) identified abstractive operations (other than word deletion) employed by professional writers, including paraphrasing and re-ordering of phrases, and merging and reordering sentences, but did not quantify their impact on compression quality.
",2 Related Work,[0],[0]
Deletion-based compression corpora:,2 Related Work,[0],[0]
Currently available automatically-mined deletion corpora are single-reference and have varying (uncontrolled) compression rates.,2 Related Work,[0],[0]
"Knight and Marcu (2002) automatically mined a small parallel corpus (1,035 training and 32 test sentences) by aligning abstracts to sentences in articles.",2 Related Work,[0],[0]
"Filippova and Altun (2013) extracted deletion-based compressions by aligning news headlines to first sentences, yielding a corpus of 250,000 parallel sentences.",2 Related Work,[0],[0]
The same approach was used by Filippova et al. (2015) to create a set of 2M sentence pairs.,2 Related Work,[0],[0]
"Only a subset of 10,000 parallel sentences from the latter has been publicly released.",2 Related Work,[0],[0]
"Clarke and Lapata (2006) and Clarke and Lapata (2008) provide two manually-created two-reference corpora for deletion-based compression:2 their sizes are 1,370 and 1,433 sentences, respectively.
2http://jamesclarke.net/research/ resources
Abstractive compression corpora:",2 Related Work,[0],[0]
"Rush et al. (2015) have mined 4 million compression pairs from news articles and released their code to extract data from the Annotated Gigaword (Napoles et al., 2012).",2 Related Work,[0],[0]
"A news-domain parallel sentence corpus containing 1,496 parallel examples has been culled from multireference Chinese-English translations by Ganitkevitch et al. (2011).",2 Related Work,[0],[0]
"The only publicly-available manually-created abstractive compression corpus is that described by Cohn and Lapata (2008), which comprises 575 single-reference sentence pairs.
",2 Related Work,[0],[0]
"Automatic metrics: Early automatic metrics for evaluation of compressions include success rate (Jing, 2000), defined as accuracy of individual word or constituent deletion decisions; Simple String Accuracy (string edit distance), introduced by Bangalore et al. (2000) for natural language generation tasks; and Word Accuracy (Chiori and Furui, 2004), which generalizes Bangalore et al. (2000) to multiple references.",2 Related Work,[0],[0]
Riezler et al. (2003) introduced the use of F-measure over grammatical relations.,2 Related Work,[0],[0]
"Word unigram and word-bigram F-measure have also been used (Unno et al., 2006; Filippova et al., 2015).",2 Related Work,[0],[0]
"Variants of ROUGE (Lin, 2004), used for summarization evaluation, have also been applied to sentence compressions (Rush et al., 2015).
",2 Related Work,[0],[0]
Riezler et al. (2003) show that F-measure over grammatical relations agrees with human ratings on the relative ranking of three systems at the corpus level.,2 Related Work,[0],[0]
Clarke and Lapata (2006) evaluate two deletion-based automatic compression systems against a deletion-based gold-standard on sets of 20 sentences.,2 Related Work,[0],[0]
"Parse-based F-1 was shown to have high sentence-level Pearson’s ρ correlation with human judgments of overall quality, and to have higher ρ than Simple String Accuracy.
",2 Related Work,[0],[0]
Napoles et al. (2011) have pointed to the need of multiple references and studies of evaluation metrics.,2 Related Work,[0],[0]
"For the related tasks of document and multidocument summarization, Graham (2015) provides a fine-grained comparison of automated evaluation methods.",2 Related Work,[0],[0]
"However, to the best of our knowledge, no studies of automatic evaluation metrics exist for abstractive compression of shorter texts.",2 Related Work,[0],[0]
"We sampled single sentences and two-sentence paragraphs from several genres in the written text section of the Manually Annotated Sub-Corpus (MASC) (Ide et al., 2008; Ide et al., 2010) of the Open American National Corpus (OANC), supplemented by additional data from the written section of OANC.",3 Dataset: Annotation and Properties,[0],[0]
Two-sentence paragraphs account for approximately 23% of multi-sentence paragraphs in the OANC.,3 Dataset: Annotation and Properties,[0],[0]
The two-sentence paragraphs we sampled contain at least 25 words.,3 Dataset: Annotation and Properties,[0],[0]
Table 2 breaks the sampled texts down by genre.,3 Dataset: Annotation and Properties,[0],[0]
Non-news genres are better represented in our sample than the newswire typically used in compression tasks.,3 Dataset: Annotation and Properties,[0],[0]
The Letters examples are expected to be useful for learning to compress emails.,3 Dataset: Annotation and Properties,[0],[0]
The Journal texts are likely to be challenging as their purpose is often more than to convey information.,3 Dataset: Annotation and Properties,[0],[0]
"The Non-Fiction collection includes material from technical academic publications, such as PLoS Medicine, an open access journal.3",3 Dataset: Annotation and Properties,[0],[0]
"Compressions were created using UHRS, an inhouse crowd-sourcing system similar to Amazon’s Mechanical Turk, in two annotation rounds, one for shortening and a second to rate compression quality.
",3.1 Annotation,[0],[0]
"Generating compressions: In the first round, we asked five workers (editors) to abridge each source text by at least 25%, while remaining grammatical and fluent, and retaining the meaning of the original.",3.1 Annotation,[0],[0]
"This requirement was enforced programmat-
3http://journals.plos.org/plosmedicine/
ically on the basis of character count.",3.1 Annotation,[0],[0]
"The 25% rate is intended to reflect practical editing scenarios (e.g., shrink 8 pages to 6).",3.1 Annotation,[0],[0]
"To facilitate meeting this requirement, the minimum source text length presented to editors was 15 words.",3.1 Annotation,[0],[0]
"For a subset of paragraphs, we collected compressions both as independent rewrites of their component sentences, and of the paragraph as a whole.",3.1 Annotation,[0],[0]
"Table 1 show compression examples and strategies.
",3.1 Annotation,[0],[0]
"Evaluating compression quality: In the second round, we asked 3-5 judges (raters) to evaluate the grammaticality of each compression on a scale from 1 (major errors, disfluent) through 3 (fluent), and again analogously for meaning preservation on a scale from 1 (orthogonal) through 3 (most important meaning-preserving).4 We later used the same process to evaluate compressions produced by automatic systems.",3.1 Annotation,[0],[0]
"The full guidelines for the editors and raters are available with the data release.
",3.1 Annotation,[0],[0]
"Quality controls: All editors and raters were based in the US, and the raters were required to pass a qualification test which asked them to rate the meaning and grammaticality for a set of examples with known answers.",3.1 Annotation,[0],[0]
"To further improve the quality of the data, we removed low-quality compressions.",3.1 Annotation,[0],[0]
We computed the quality of each compression as the average of the grammar and meaning quality as judged by the raters.,3.1 Annotation,[0],[0]
"We then computed the mean quality for each editor, and removed compressions authored by the bottom 10% of editors.",3.1 Annotation,[0],[0]
"We did the same for the bottom 10% of the raters.5
4Pilot studies suggested that a scale of 1-3 offered better inter-annotator agreement than the standard 5-point Likert-type scale, at the cost of granularity.
",3.1 Annotation,[0],[0]
"5This was motivated by the observation that the quality of work produced by judges is relatively constant (Gao et al., 2015).
",3.1 Annotation,[0],[0]
"Table 3 shows the number of compressions in the cleaned dataset, as well as the average number of compressions per source text (CPS) and the average meaning and grammar scores.",3.1 Annotation,[0],[0]
"Meaning quality and grammaticality scores are relatively good, averaging 2.78 and 2.82 respectively.",3.1 Annotation,[0],[0]
"The filtered crowdsourced compressions were most frequently judged to retain the most important meaning (80% of the time), or much of the meaning (17% of the time), with the lowest rating of 1 appearing only 3% of the time.",3.1 Annotation,[0],[0]
"This distribution is quite different from that of automatic compression systems in Section 4.
",3.1 Annotation,[0],[0]
"We provide a standard split of the data into training, development and test sets.6",3.1 Annotation,[0],[0]
"There are 4,936 source texts in the training, 448 in the development, and 785 in the test set.",3.1 Annotation,[0],[0]
"Crowd Workers: Since a different set of judges performs each task, large sets of inputs judged by the same two raters are unavailable.",3.2 Inter-Annotator Agreement,[0],[0]
"To simulate two raters, we follow Pavlick and Tetrault (2016): for each sentence, we randomly choose one annotator’s output as the category for annotator A, and select the rounded average ranking for the remaining annotators as the category for annotator B. We then compute quadratic weighted κ (Cohen, 1968) for this pair over the whole corpus.",3.2 Inter-Annotator Agreement,[0],[0]
We repeat the process 1000 times to compute the mean and variance of κ.,3.2 Inter-Annotator Agreement,[0],[0]
"The first row of the Table 4 reports the absolute agreement and κ, where the absolute agreement measures the fraction of times that A is equal to B. The 95% confidence intervals for κ are narrow, with width at most .01.
6The dataset can be downloaded from the project’s website https://www.microsoft.com/en-us/research/ project/intelligent-editing/.
Expert Raters: A small sample of 116 sentence pairs was rated by two expert judges.",3.2 Inter-Annotator Agreement,[0],[0]
"We used quadratic weighted κ directly, without sampling.",3.2 Inter-Annotator Agreement,[0],[0]
"To assess agreement between experts and non-experts, we computed weighted κ between the (rounded) average of the expert judgments and the (rounded) average of the crowd judgments, using 25,000 bootstrap replications each.",3.2 Inter-Annotator Agreement,[0],[0]
The results are shown in the last two rows of Table 4.,3.2 Inter-Annotator Agreement,[0],[0]
"The confidence intervals for κ are wide due to the small sample size, and span values up to .17 away from the mean.",3.2 Inter-Annotator Agreement,[0],[0]
"Overall, agreement of experts with the average crowdsourced ratings is moderate (approaching substantial) for meaning, and fair for grammar.",3.2 Inter-Annotator Agreement,[0],[0]
"Frequency analysis: To analyze the editing operations used, we applied the state-of-the-art monolingual aligner Jacana (Yao et al., 2013) to align input to compressed texts.",3.3 Analysis of Editing Operations,[0],[0]
"Out of the 26,423 compressions collected, 25.2% contained only token deletions.",3.3 Analysis of Editing Operations,[0],[0]
"Those containing deletion and reordering amounted to a mere 9.1%, while those that also contain substitution or rephrasing (abstractive compressions) is 65.6%.",3.3 Analysis of Editing Operations,[0],[0]
"Although abstraction is present in the large majority of compressions, these statistics do not indicate that paraphrasing is more prevalent than copying at the token level.",3.3 Analysis of Editing Operations,[0],[0]
"The word alignments for target compression words indicate that 7.1% of target tokens were inserted, 75.4% were copied and 17.3% were paraphrased.",3.3 Analysis of Editing Operations,[0],[0]
"From the alignments for source text words, we see that 31% of source words were deleted.",3.3 Analysis of Editing Operations,[0],[0]
"The fraction of inserted and deleted words is probably overestimated by this approach, as it is likely that sequences of source words were abstracted as shorter sequences of target words in many-to-one or many-to-many alignment patterns that are difficult to detect automatically.
",3.3 Analysis of Editing Operations,[0],[0]
"For the subset of examples where the input text
contained more than one sentence, we computed the frequency of sentence-merging and sentence deletion when compressing.",3.3 Analysis of Editing Operations,[0],[0]
"Of the compressions for two-sentence paragraphs, 72.4% had two sentences in the output, 0.4% had one sentence deleted, and 27.3% had the two source sentences merged.
",3.3 Analysis of Editing Operations,[0],[0]
"Impact of operations: Because the dataset contains multiple compressions of the same sources, we are able to estimate the impact of different editing operations.",3.3 Analysis of Editing Operations,[0],[0]
These were classified using the Jacana word alignment tool.,3.3 Analysis of Editing Operations,[0],[0]
Table 5 presents the average judgment scores for meaning preservation and grammaticality for four operations.,3.3 Analysis of Editing Operations,[0],[0]
"The upper two rows apply to all texts, the lower two to two-sentence paragraphs only.",3.3 Analysis of Editing Operations,[0],[0]
The statistical significance of their impact was tested using the Wilcoxon signed-rank test on paired observations.,3.3 Analysis of Editing Operations,[0],[0]
"It appears that raters view compressions that involve substitutions as significantly more meaning-preserving than those that do not (p < 0.0001), but judge their grammaticality to be lower than that of deletion-based compressions.",3.3 Analysis of Editing Operations,[0],[0]
"Note that the reduced grammaticality may be due to typographical errors that have been introduced during rephrasing, which could have been avoided had a more powerful word processor been used as an editing platform.",3.3 Analysis of Editing Operations,[0],[0]
"Reordering has no significant impact on meaning, but leads to substantial degradation in grammatically.",3.3 Analysis of Editing Operations,[0],[0]
"Conversely, abridgments that merge or delete sentences are rated as significantly less meaning preserving, but score higher for grammaticality, possibly reflecting greater skill on the part of those editors..
Impact of sentence context: Table 6 shows that the context provided by 2-sentence sources yields significantly improved scores for both meaning and grammaticality.",3.3 Analysis of Editing Operations,[0],[0]
Here we used the matched pairs design to compare the average quality of two-sentence paragraph compressions with the average quality of the compressions of the same paragraphs produced by separately compressing the two sentences.,3.3 Analysis of Editing Operations,[0],[0]
Progress in automated text compression is standardly measured by comparing model outputs at the corpus level.,4 Evaluating Evaluation Metrics,[0],[0]
"To train models discriminatively and to perform fine-grained system comparisons, however, it is also necessary to have evaluation of system outputs at the individual input level.",4 Evaluating Evaluation Metrics,[0],[0]
"Below, we examine automated metric correlation with human judgments at both levels of granularity.",4 Evaluating Evaluation Metrics,[0],[0]
"The goal of this analysis is to develop an understanding of the performance of automatic evaluation metrics for text compression, and the factors contributing to their performance.",4.1 Automatic Metrics,[0],[0]
"To this end, we group automatic metrics according to three criteria.",4.1 Automatic Metrics,[0],[0]
The first is the linguistic units used to compare system and reference compressions.,4.1 Automatic Metrics,[0],[0]
"Prior work on compression evaluation has indicated that a parsebased metric is superior to one based on surface substrings (Clarke and Lapata, 2006), but the contribution of the linguistic units has not been isolated, and surface n-gram units have otherwise been successfully used for evaluation in related tasks (Graham, 2015).",4.1 Automatic Metrics,[0],[0]
"Accordingly, we empirically compare metrics based on surface uni-grams (LR-1), bi-grams (LR-2), tri-grams (LR-3), and four-grams (LR-4), as well skip bi-grams (with a maximum of four intervening words as in ROUGE-S4) (SKIP-2), and dependency tree triples obtained from collapsed dependencies output from the Stanford parser (PARSE2).7 The second criterion is the scoring measure used to evaluate the match between two sets of linguistic units corresponding to a system output and a reference compression.",4.1 Automatic Metrics,[0],[0]
"We compare Precision, Recall, F-measure, and Precision+Brevity penalty (as
7Clarke and Lapata (2006) used the RASP parser (Briscoe and Carroll, 2002), but we expect that the Stanford parser is similarly robust and would lead to similar correlations.
in BLEU).",4.1 Automatic Metrics,[0],[0]
"The third criterion is whether multiple references or a single reference is used, and in the case of multiple references, the method used to aggregate information from multiple references.",4.1 Automatic Metrics,[0],[0]
"We investigate two previously applied methods and introduce a novel approach that often outperforms the standard methods.
",4.1 Automatic Metrics,[0],[0]
"To illustrate, we introduce some notation and use a simple example.",4.1 Automatic Metrics,[0],[0]
"Consider a sub-phrase of one of the sentences in Table 1, think about your household, as an input text to compress.",4.1 Automatic Metrics,[0],[0]
"Let us assume that we have two reference compressions, R1: imagine your household, and R2: your household.",4.1 Automatic Metrics,[0],[0]
"Each metric m is a function from a pair of a system output o and a list of references r1, r2, . . .",4.1 Automatic Metrics,[0],[0]
", rk to the reals.",4.1 Automatic Metrics,[0],[0]
"To compute most metrics, we first compute a linguistic unit feature vector for each reference Φ(rj), as well as for the set of references Φ(r1, r2, . . .",4.1 Automatic Metrics,[0],[0]
", rk).",4.1 Automatic Metrics,[0],[0]
"Similarly, we compute a linguistic unit vector for the output Φ(o) and measure the overlap between the system and reference vectors.",4.1 Automatic Metrics,[0],[0]
"The vectors of the example references, if we use surface bigram units, would be, for R1, {imagine your:1, your household:1}, and for R2, {your household:1}.",4.1 Automatic Metrics,[0],[0]
"The weights of all n-grams in individual references and system outputs are equal to 1.8 If we use dependencyparse triples instead, the vector of R2 would be {nmod:poss(household, your):1}.
",4.1 Automatic Metrics,[0],[0]
"The precision of a system output against a reference is defined as the match Φ(r)TΦ(o) divided by the number of units in the vector of o; the latter can be expressed as the L1 norm of Φ(o) because all weights are positive: Precision(o, r) = Φ(r)
T Φ(o) |Φ(o)|1 .
",4.1 Automatic Metrics,[0],[0]
"The recall against a single reference can be similarly defined as the match divided by the number of units in the reference: Recall(o, r) = Φ(r)
T Φ(o) |Φ(r)|1 .
",4.1 Automatic Metrics,[0],[0]
"We distinguish three methods for aggregating information from multiple references: MULT-MAX which uses the single reference out of a set that results in the highest single-reference score, and two further methods, MULT-ALL and MULT-PROB, that construct an aggregate linguistic unit vector Φ(r1, . . .",4.1 Automatic Metrics,[0],[0]
", rk) before matching.",4.1 Automatic Metrics,[0],[0]
"MULT-ALL is the standard method used in multi-reference BLEU,
8We handle repeating n-grams by assigning each subsequent n-gram of the same type a distinct type, so that the i-th the of a system output can match the i-th the of a reference.
where the vector for a set of references is defined as the union of the features of the set.",4.1 Automatic Metrics,[0],[0]
"For our example, the combined vector of R1 and R2 is equal to the vector of R1, because R2 adds no new bigrams.",4.1 Automatic Metrics,[0],[0]
"MULT-PROB, a new method that we propose here, is motivated by the observation that although judgments of importance of content are subjective, the more annotators assert some information is important, the more this information should contribute to the matching score.9 In MULT-PROB we define the weight of a linguistic unit in the combined reference vector as the proportion of references that include the unit.",4.1 Automatic Metrics,[0],[0]
"For our example, ΦMULT-PROB(R1, R2) is {imagine your:.5, your household:1}.",4.1 Automatic Metrics,[0],[0]
"For the purpose of analysis, we trained and evaluated four compression systems.",4.2 Models for Text Compression,[0],[0]
"These include both deletion-based and abstractive models: (1) ILP, an integer linear programing approach for deletionbased compression (Clarke and Lapata, 2008), (2) T3, a tree transducer-based model for abstractive compression (Cohn and Lapata, 2008), (3) Seq2seq, a neural network model for deletion-based compression (Filippova et al., 2015), and (4) NAMAS, a neural model for abstractive compression and summarization (Rush et al., 2015).",4.2 Models for Text Compression,[0],[0]
We are not concerned with the relative performance of these models so much as we are concerned with evaluating the automatic evaluation metrics themselves.,4.2 Models for Text Compression,[0],[0]
"We have sought to make the models competitive, but have not required that all systems use identical training data.
",4.2 Models for Text Compression,[0],[0]
All of the models are evaluated on the test set portion of our dataset.,4.2 Models for Text Compression,[0],[0]
"All models use the training portion of the data for training, and two models (Seq2Seq and NAMAS10) additionally use external training data.",4.2 Models for Text Compression,[0],[0]
The external data is summarized in Table 7.,4.2 Models for Text Compression,[0],[0]
"The Gigaword set was extracted from the Annotated Gigaword (Napoles et al., 2012), using the implementation provided by Rush et al. (2015).",4.2 Models for Text Compression,[0],[0]
"The Headline data was extracted in similar fashion using an in-house news collection.
",4.2 Models for Text Compression,[0],[0]
"9A similar insight was used in one of the component metrics of the SARI evaluation metric used for text simplification evaluation (Xu et al., 2016).
",4.2 Models for Text Compression,[0],[0]
"10The original works introducing these models employed much larger training corpora, believed to be key to improving the accuracy of neutral network models with large parameter spaces.
",4.2 Models for Text Compression,[0],[0]
"ILP: We use an open-source implementation11 of the semi-supervised ILP model described in (Clarke and Lapata, 2008).",4.2 Models for Text Compression,[0],[0]
The model uses a trigram language model trained on a 9 million token subset of the OANC corpus.,4.2 Models for Text Compression,[0],[0]
"The ILP model requires parsed sentences coupled with deletion-based compressions for training, so we filtered and preprocessed our dataset to satisfy these constraints.",4.2 Models for Text Compression,[0],[0]
"We used all single sentence inputs with their corresponding deletion-based compressions, and additionally used two-sentence paragraph input/output pairs split into sentences by heuristically aligning source to target sentences in the paragraphs.",4.2 Models for Text Compression,[0],[0]
T3:,4.2 Models for Text Compression,[0],[0]
We use the authors’ implementation of the tree transducer system described in Cohn and Lapata (2008).,4.2 Models for Text Compression,[0],[0]
"T3 similarly requires sentence-level input/output pairs, but can also learn from abstractive compressions.",4.2 Models for Text Compression,[0],[0]
"We thus used a larger set of approximately 28,000 examples (single sentences with abstractive compressions taken directly from the data or as a result of heuristic sentence-level alignment of two-sentence paragraphs).",4.2 Models for Text Compression,[0],[0]
"We obtained parse trees using the Stanford parser (Klein and Manning, 2003), and used Jacana (Yao et al., 2013) for word alignment.",4.2 Models for Text Compression,[0],[0]
"The performance obtained by T3 in our experiments is substantially weaker (relative to ILP) than that reported in prior work (Cohn and Lapata, 2008).",4.2 Models for Text Compression,[0],[0]
We therefore interpret this system output solely as data for evaluating automatic metrics.,4.2 Models for Text Compression,[0],[0]
NAMAS:,4.2 Models for Text Compression,[0],[0]
We run the publicly available implementation of NAMAS12 with the settings described by Rush et al. (2015).,4.2 Models for Text Compression,[0],[0]
"We modified the beam search algorithm to produce output with a compression ratio similar to that of the human references, since this ratio is a large factor in compression quality (Napoles et al., 2011), and systems generally perform better if allowed to produce longer output, up to the maximum length limit.",4.2 Models for Text Compression,[0],[0]
"We enforced output length be-
11https://github.com/cnap/ sentence-compression
12https://github.com/facebook/NAMAS
tween 50% and 75% of input length, which resulted in improved performance.",4.2 Models for Text Compression,[0],[0]
Seq2seq:,4.2 Models for Text Compression,[0],[0]
We implemented the sequence-tosequence model13 described in Filippova et al. (2015).,4.2 Models for Text Compression,[0],[0]
"A deletion-based model, it uses the deletionbased subset of our training dataset and the deletionbased subset from the external data in Table 7.",4.2 Models for Text Compression,[0],[0]
"The encoder and decoder have three stacked LSTM layers, the hidden dimension size is 512, and the vocabulary size is 30,000.",4.2 Models for Text Compression,[0],[0]
"The compression rate was controlled in the same range as for the NAMAS model.
",4.2 Models for Text Compression,[0],[0]
All models produce output on all inputs in the test set.,4.2 Models for Text Compression,[0],[0]
"For all models, we generated outputs for multisentence inputs by concatenating outputs for each individual sentence.14",4.2 Models for Text Compression,[0],[0]
"Overall, we consider 80 metric variants, consisting of combinations of six types of linguistic units, combined with three scoring methods (Precision, Recall, and F-measure) and four settings of single reference SINGLE-REF or three ways of scoring against multiple references MULT-ALL,MULT-MAX,MULT-PROB.",4.3 Results,[0],[0]
"Additionally, we include the standard single and multi-reference versions of BLEU-2,BLEU-3,BLEU4, and ROUGE-L.
We compare automatic metrics to human judgements at the level of individual outputs or groups of outputs (the whole corpus).",4.3 Results,[0],[0]
"For a single output o, the human quality judgment is defined as the average assigned by up to five human raters.",4.3 Results,[0],[0]
"We denote the meaning, grammar, and combined quality values by M(o), G(o), and C(o) = .5M(o)",4.3 Results,[0],[0]
"+ .5G(o), respectively.",4.3 Results,[0],[0]
We define the quality for a group of outputs as the arithmetic mean of judgments over the outputs in the group.,4.3 Results,[0],[0]
"We use the arithmetic mean of automating metrics at the individual output level to define automatic corpus quality metrics as well.15 To compare different metrics and establish statistical significance of the difference between two metrics, we use Williams test of the significance of the difference
13https://github.com/ketranm/tardis 14In small scale preliminary manual evaluation, we found that, although some models are theoretically able to make use of context beyond the sentence boundary, they performed better if they compressed each sentence in a sequence independently.
",4.3 Results,[0],[0]
"15This method has been standard for ROUGE, but has not for BLEU.",4.3 Results,[0],[0]
"We find that averaging sentence-level metrics is also advantageous for BLEU .
between dependent Pearson correlations with human judgments (Williams, 1959) as recommended for summarization evaluation (Graham, 2015) and other NLP tasks (e.g. (Yannakoudakis et al., 2011)).",4.3 Results,[0],[0]
"Table 8 shows the average human ratings of the four systems, separately in meaning and grammar, as well as the combined measure (an arithmetic mean of meaning and grammar judgments).",4.3.1 Corpus-level metrics,[0],[0]
"Even though the performance of some systems is similar, the differences between all pairs of systems in meaning and grammar are significant p < 0.0001 according to a paired t-test.",4.3.1 Corpus-level metrics,[0],[0]
It is interesting to note that ILP outperforms the more recently developed neural network systems Seq2Seq and NAMAS.,4.3.1 Corpus-level metrics,[0],[0]
"This might seem to contradict recent results showing that the new models are superior to traditional baselines, such as ILP.",4.3.1 Corpus-level metrics,[0],[0]
"We note however that performance on the test corpus in our study might not substantially improve through the use of large automatically mined data-sets of headlines and corresponding news article sentences, due to differences in genre and domain.",4.3.1 Corpus-level metrics,[0],[0]
"Using such data-sets for effective training of neural network models for nonnewswire domains remains an open problem.
",4.3.1 Corpus-level metrics,[0],[0]
"For each of the 80 metrics, we compared the ranking of the four systems with the ranking according to average human quality.",4.3.1 Corpus-level metrics,[0],[0]
Fifty three of the metrics achieved perfect Spearman ρ and Kendall τB,4.3.1 Corpus-level metrics,[0],[0]
correlation with human judgments of combined meaning and grammar quality.,4.3.1 Corpus-level metrics,[0],[0]
"Due to the small sample size (four systems), we are unable to find statistically significant differences among metrics at the corpus level.",4.3.1 Corpus-level metrics,[0],[0]
We only note that precision-based metrics involving large linguistic units (four-grams) had negative correlations with human judgments.,4.3.1 Corpus-level metrics,[0],[0]
"We can conclude, however, that evaluation at the corpus level is robust for a wide variety of standard metrics using linguistic units of size three or smaller.",4.3.1 Corpus-level metrics,[0],[0]
We can garner greater insight into the difference of metric performance when we compare metrics at the single input level.,4.3.2 Single input-level pairwise system comparisons,[0],[0]
"To gauge the ability of metrics to comparatively evaluate the quality of two systems, we compute single input-level correlations of automatic metrics with human judgments following the protocol of Galley et al. (2015).",4.3.2 Single input-level pairwise system comparisons,[0],[0]
"Each system A produces a sequence of outputs o1A, . . .",4.3.2 Single input-level pairwise system comparisons,[0],[0]
", onA, corresponding to inputs x1, . . .",4.3.2 Single input-level pairwise system comparisons,[0],[0]
", xn.",4.3.2 Single input-level pairwise system comparisons,[0],[0]
"For each system output, we use Q(a) to denote a generic human quality metric, varying over meaning, grammar, and their combination.",4.3.2 Single input-level pairwise system comparisons,[0],[0]
"For each pair of systems A and B, and each metric m, we compute the difference in quality for corresponding system outputs for each input xi: m(oiA) −m(oiB) and the difference in quality according to human judgments: Q(oiA)",4.3.2 Single input-level pairwise system comparisons,[0],[0]
"− Q(oiB), and compute the correlation between these two sequences.",4.3.2 Single input-level pairwise system comparisons,[0],[0]
"We can thus compute the single input-level correlation between m and Q for each pair of systems A and B, resulting in a total of six correlation values (for the six pairs of systems) for each metric.",4.3.2 Single input-level pairwise system comparisons,[0],[0]
"For each pair of metrics m1 and m2, and for each pair of systems A and B, we compute the statistical significance of the difference between the Pearson correlations of these metrics with human judgements.",4.3.2 Single input-level pairwise system comparisons,[0],[0]
We say that m1 is significantly better than m2 on the A vs. B comparison if its Pearson correlation with human quality Q is significantly better (according to the Williams test of the difference in dependent correlations) than that of m2 with a pvalue less than .05.,4.3.2 Single input-level pairwise system comparisons,[0],[0]
"We say that m1 dominates m2 overall if it is significantly better than m2 on at least 80% of the pair-wise system comparisons.
",4.3.2 Single input-level pairwise system comparisons,[0],[0]
Table 9 shows the main correlation results at the level of individual inputs.,4.3.2 Single input-level pairwise system comparisons,[0],[0]
"We report correlations with meaning, grammar, and combined quality separately.",4.3.2 Single input-level pairwise system comparisons,[0],[0]
"For each human quality metric, we see the top automatic metrics in the first group of rows.",4.3.2 Single input-level pairwise system comparisons,[0],[0]
"The top metrics are ones that, for at least 80% of the system comparisons, are not significantly dominated by any other metric.",4.3.2 Single input-level pairwise system comparisons,[0],[0]
"In addition, we show the impact of each of the three criteria: linguistic units, scoring measure, and multiple references, in corresponding groups of rows.",4.3.2 Single input-level pairwise system comparisons,[0],[0]
"For each linguistic unit type, we show the best-performing metric that uses units of
this type.",4.3.2 Single input-level pairwise system comparisons,[0],[0]
"Similarly, for the other criteria, we show the best performing metric for each value of the criterion.",4.3.2 Single input-level pairwise system comparisons,[0],[0]
Metrics with a ∗ suffix in each group significantly dominate metrics with a − suffix.,4.3.2 Single input-level pairwise system comparisons,[0],[0]
"Metrics with a − suffix in a group are dominated by at least one other metric, possibly outside of the group.",4.3.2 Single input-level pairwise system comparisons,[0],[0]
"The lowest group of rows in each main column presents the performance of other metrics that cannot be classified directly based on the three criteria.
",4.3.2 Single input-level pairwise system comparisons,[0],[0]
A high-level observation that can be made is that the correlations with meaning are much higher than the correlations with grammar.,4.3.2 Single input-level pairwise system comparisons,[0],[0]
"The best correlations in meaning can be classified as “strong”, whereas the best correlations in grammar are in the “medium” range.",4.3.2 Single input-level pairwise system comparisons,[0],[0]
Unigrams are heavily dominated by higher order n-grams in all settings.,4.3.2 Single input-level pairwise system comparisons,[0],[0]
Fourgrams are also weaker that other units in measuring meaning preservation.,4.3.2 Single input-level pairwise system comparisons,[0],[0]
"Dependency triple (parsebased) metrics are strong, in particular in measuring grammaticality, but do not significantly dominate skip bi-grams or contiguous bi-grams.",4.3.2 Single input-level pairwise system comparisons,[0],[0]
The scoring measure used has a strong impact.,4.3.2 Single input-level pairwise system comparisons,[0],[0]
"We see that precision-based metrics are substantially dominated by metrics that incorporate recall, except for grammar evaluation.",4.3.2 Single input-level pairwise system comparisons,[0],[0]
"Importantly, we see that multiple
references contribute substantially to metric quality, as all methods that use multiple references outperform single-reference metrics.",4.3.2 Single input-level pairwise system comparisons,[0],[0]
"In both meaning and combined evaluation, this difference was statistically significant.",4.3.2 Single input-level pairwise system comparisons,[0],[0]
"Finally, we observe that standard BLEU metrics and ROUGE-L were not competitive.",4.3.2 Single input-level pairwise system comparisons,[0],[0]
"We have introduced a large manually collected multi-reference abstractive dataset and quantified the impact of editing operations and context on human compression quality, showing that substitution and rephrasing operations are more meaning preserving than other operations, and that compression in context improves quality.",5 Conclusion,[0],[0]
"Further, in the first systematic study of automatic evaluation metrics for text compression, we have demonstrated the importance of utilizing multiple references and suitable linguistic units, and incorporating recall.",5 Conclusion,[0],[0]
"We are grateful to Jaime Teevan, Shamsi Iqbal, Dan Liebling, Bill Dolan, Michel Galley, and Wei Xu, together with the three anonymous reviewers for their helpful advice and suggestions.",Acknowledgments,[0],[0]
"We introduce a manually-created, multireference dataset for abstractive sentence and short paragraph compression.",abstractText,[0],[0]
"First, we examine the impact of singleand multi-sentence level editing operations on human compression quality as found in this corpus.",abstractText,[0],[0]
"We observe that substitution and rephrasing operations are more meaning preserving than other operations, and that compressing in context improves quality.",abstractText,[0],[0]
"Second, we systematically explore the correlations between automatic evaluation metrics and human judgments of meaning preservation and grammaticality in the compression task, and analyze the impact of the linguistic units used and precision versus recall measures on the quality of the metrics.",abstractText,[0],[0]
Multi-reference evaluation metrics are shown to offer significant advantage over single reference-based metrics.,abstractText,[0],[0]
A Dataset and Evaluation Metrics for Abstractive Compression of Sentences and Short Paragraphs,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 968–974 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
968",text,[0],[0]
"Telling stories about what we experience is a central part of human communication (Mateas and Sengers, 2003).",1 Introduction,[0],[0]
"Increasingly, stories about our experiences are captured in the form of videos and then shared on social media platforms.",1 Introduction,[0],[0]
"One goal of automatically understanding and describing such videos with natural language is to generate multi-sentence descriptions which convey the story, making them accessible to situationally (e.g. bandwidth) or physically (“blind”) disabled people.",1 Introduction,[0],[0]
"However, it is still a challenge for vision and language models to automatically encode and describe temporal content in videos with multi-sentence descriptions (Rohrbach et al., 2014; Zhou et al., 2018b).",1 Introduction,[0],[0]
To better understand the stories shared on social media we collect and annotate a novel dataset consisting of videos from a social media platform.,1 Introduction,[0],[0]
"Importantly, we collect descriptions containing multiple sentences,
∗*Work done while SG was intern at Facebook AI Research.
",1 Introduction,[0],[0]
"as single sentences would typically not be able to capture the narration and plot of the video.
",1 Introduction,[0],[0]
We introduce a large-scale multi-sentence description dataset for videos.,1 Introduction,[0],[0]
"To build a dataset of high quality, diverse and narratively interesting videos, we choose videos that had high engagement on a social media platform.",1 Introduction,[0],[0]
"Existing video captioning datasets, such as ActivityNet Captions (Krishna et al., 2017) or cooking video datasets (Regneri et al., 2013; Zhou et al., 2018a), have focused on sets of pre-selected human activities, whereas social media videos contain a great diversity of topics.",1 Introduction,[0],[0]
"Videos with high engagement tend to be narratively interesting, because humans find very predictable videos less enjoyable, meaning that captioning of the videos accurately requires integrating information from the entire video to describe a sequence of events (see Figure 1).",1 Introduction,[0],[0]
"Together, this creates a diverse and challenging new benchmark for video and language understanding.
",1 Introduction,[0],[0]
"We present a thorough analysis of the new benchmark, demonstrating that linguistic and video context is crucial to accurate captioning and that the captions have a temporal consistency.",1 Introduction,[0],[0]
We also show baseline results using state-of-the-art models.,1 Introduction,[0],[0]
In Table 1 we summarize existing video description datasets; most provide only single-sentence descriptions or are restricted to narrow domains.,2 Multi-Sentence VideoStory Dataset,[0],[0]
"Other multi-sentence description datasets are proposed for story narration of sets of images taken from a Flickr album (Huang et al., 2016; Krause et al., 2017).",2 Multi-Sentence VideoStory Dataset,[0],[0]
"Other related work includes visual summarization of Flickr photo albums (Sigurdsson et al., 2016a) or videos (De Avila et al., 2011; Zhang et al., 2016) where the idea is to pick the key images or frames that summarize the visual content.
",2 Multi-Sentence VideoStory Dataset,[0],[0]
"We select videos posted on a social media platform to create our dataset because of the variability in topics, length, viewpoints, and quality.",2 Multi-Sentence VideoStory Dataset,[0],[0]
They also tend to represent a good distribution of stories communicated by humans.,2 Multi-Sentence VideoStory Dataset,[0],[0]
We select videos from social media that are public and popular with a large number of comments and shares that triggered interactions between people.,2 Multi-Sentence VideoStory Dataset,[0],[0]
"In total, our dataset consists of 20k videos with duration ranging from 20s-180s and spanning across diverse topics that are observed on social media platforms.",2 Multi-Sentence VideoStory Dataset,[0],[0]
"We follow Krishna et al. (2017) to create temporally annotated sentences where each task is divided into two steps: (i) describing the video in multiple sentences, covering objects, situations and important details of the video; (ii) aligning each sentence in the paragraph with the corresponding timestamps in the video.",2 Multi-Sentence VideoStory Dataset,[0],[0]
We refer to these as video segments.,2 Multi-Sentence VideoStory Dataset,[0],[0]
"In Figure 1, we present two example annotated videos describing (i) a scene where two girls are playing with horses; (ii) a wedding with a bride walking down the aisle.",2 Multi-Sentence VideoStory Dataset,[0],[0]
We summarize the statistics of our dataset in Table 2 and compare it to prior work in Table 1.,2 Multi-Sentence VideoStory Dataset,[0],[0]
Each of the 20k videos in our VideoStory dataset is annotated with a paragraph which has on average 4.67 temporally localized sentences.,2 Multi-Sentence VideoStory Dataset,[0],[0]
"As we have three paragraphs per video for validation and test set, we have a total of 26,245 paragraphs with a total of 123k sentences.",2 Multi-Sentence VideoStory Dataset,[0],[0]
"Each sentence in the dataset
has an average length of 13.32 words, and each video has the average paragraph length of 62.23 words.",2 Multi-Sentence VideoStory Dataset,[0],[0]
Each sentence is aligned to a clip of on average 18.33 seconds which covers on average 26.04% of the full video.,2 Multi-Sentence VideoStory Dataset,[0],[0]
"However, the entire paragraph for each video on average describes 96.7% of the whole video, demonstrating that each paragraph annotation covers the majority of the video.",2 Multi-Sentence VideoStory Dataset,[0],[0]
"Furthermore, we found that 22% of the temporal descriptions overlap, showing that our annotation allows co-occurring or simultaneous events.",2 Multi-Sentence VideoStory Dataset,[0],[0]
"We divide our dataset in training (17098 videos), validation (999), test (1011) and blind test splits (1039).",2 Multi-Sentence VideoStory Dataset,[0],[0]
"Each video in the training set has a single annotation, but videos in validation, test, and blind test splits have three temporally localized paragraph annotations, for evaluation.",2 Multi-Sentence VideoStory Dataset,[0],[0]
"While the test set can be used to compare model variants in a paper, only the best model per paper should be evaluated on the blind test set annotations, which will only be possible on an evaluation server.",2 Multi-Sentence VideoStory Dataset,[0],[0]
"Annotations for the blind test set will not be released.
",2 Multi-Sentence VideoStory Dataset,[0],[0]
To explore the different domains in our dataset vs. ActivityNet captions we use the normalized pointwise mutual information to identify the words most closely associated with each dataset.,2 Multi-Sentence VideoStory Dataset,[0],[0]
"Highest ranked words for ActivityNet are almost exclusively sports related, whereas in our dataset they include animals, baby, and words related to social events such as weddings.",2 Multi-Sentence VideoStory Dataset,[0],[0]
"Most dominant actions in ActivityNet are either sports or household activity related whereas actions in our dataset are related to social activities such as laughing, waving, cheering etc.",2 Multi-Sentence VideoStory Dataset,[0],[0]
"Our analysis of the distribution of POS categories show that nouns are the most dominant category observed in the VideoStory captions dataset with 24% of the total tokens followed by verbs (18.5%), determiners (15.9%), adjectives (4.36%), adverbs (5.16%) and propositions (5.04%).",2 Multi-Sentence VideoStory Dataset,[0],[0]
"We
also observe the similar distribution of POS categories in ActivityNet captions.
",2 Multi-Sentence VideoStory Dataset,[0],[0]
We also find that ActivityNet has 50% of the videos where at least one segment in the video describes more than half of the video duration whereas in our dataset only 30% of videos have that phenomenon.,2 Multi-Sentence VideoStory Dataset,[0],[0]
"In Figure 2, we show the distribution of sentence/segment annotations in time.",2 Multi-Sentence VideoStory Dataset,[0],[0]
"The average number of (temporally localized) sentences is 4.67 compared to 3.65 in ActivityNet, despite having shorter videos, indicating the high information content of our videos.
",2 Multi-Sentence VideoStory Dataset,[0],[0]
In Table 3 we present all three paragraph annotations for a video showing a wedding ceremony.,2 Multi-Sentence VideoStory Dataset,[0],[0]
"Out of 3 annotations, Annotation 2 is more descriptive compared to 1 and 3.",2 Multi-Sentence VideoStory Dataset,[0],[0]
"However, it misses details about the presence of the photographer and taking the pictures.
",2 Multi-Sentence VideoStory Dataset,[0],[0]
Temporal Analysis.,2 Multi-Sentence VideoStory Dataset,[0],[0]
High quality video descriptions are more than bags of single-sentence captions; they should tell a coherent story.,2 Multi-Sentence VideoStory Dataset,[0],[0]
"To identify the importance of sentence ordering or temporal coherence in our video paragraphs, we train a neural language model (Merity et al., 2017) on the training paragraphs of the VideoStory dataset and report perplexity on the correct order of sentences vs. randomly shuffled order of sentences in the descriptions created to understand the importance of temporal coherence in the video descriptions of our dataset.",2 Multi-Sentence VideoStory Dataset,[0],[0]
"Results in Table 2 show that shuffled sentences have higher perplexity scores, demonstrating that order of sentences in the paragraphs are important for the coherence in the story.",2 Multi-Sentence VideoStory Dataset,[0],[0]
"We explore learning to caption the videos using ground truth video segments.
",3 Baseline Captioning Models,[0],[0]
Image Captioning Models.,3 Baseline Captioning Models,[0],[0]
"To understand if the temporal component of the video is contributing
to the description, we trained image captioning models on a frame sampled from the middle of the each segment of a video.",3 Baseline Captioning Models,[0],[0]
"We use the Show and Tell (Vinyals et al., 2015) image captioning architecture to generate captions.",3 Baseline Captioning Models,[0],[0]
Video Captioning Models.,3 Baseline Captioning Models,[0],[0]
We study various video captioning models.,3 Baseline Captioning Models,[0],[0]
"First, we use sequence to sequence (seq-seq) recurrent neural network (RNN) model which has a two-layer encoder RNN to encode video features and a decoder RNN to generate descriptions.",3 Baseline Captioning Models,[0],[0]
"In the seq-seq approach we treat each description/segment individually and use an RNN decoder to describe each segment of the video, similar to Venugopalan et al. (2015), but using Gated Recurrent Units, GRUs, (Cho et al., 2014) for both the encoder and decoder.
",3 Baseline Captioning Models,[0],[0]
"In most videos, events are correlated with previous and future events.",3 Baseline Captioning Models,[0],[0]
"For example, for the first video description shown in Figure 1 once the girl is thrown into the water, she gets hold of herself, and the horse shakes off water on her.",3 Baseline Captioning Models,[0],[0]
"To capture such contextual correlations, we incorporate context from previous segment description into the captioning module.",3 Baseline Captioning Models,[0],[0]
We build a model (seq-seq + context) which takes current segment video features and hidden representation of previous segment’s sentence generation RNN at every timestamp in the decoder.,3 Baseline Captioning Models,[0],[0]
"For a given video segment, with hidden encoded video representation hvi and hidden representation of previous segment hsi−1, the concatenation of (hvi , h s i−1) is fed as input to the decoder that describes the segment (shown in Figure 3).",3 Baseline Captioning Models,[0],[0]
"Prior work has shown using previous video context has improved generated captions (Krishna et al., 2017).",3 Baseline Captioning Models,[0],[0]
Visual representation.,3 Baseline Captioning Models,[0],[0]
"For the image caption-
ing models, we used features extracted from pretrained ResNet-152 on ImageNet (He et al., 2016).",3 Baseline Captioning Models,[0],[0]
"For video captioning models we extract features from pre-trained 3D convolution ResNext-101 architecture trained on Kinetics (Kay et al., 2017), denoted as R3D, which achieved state-of-the-art results on various activity recognition tasks (Hara et al., 2018).",3 Baseline Captioning Models,[0],[0]
"Since a significant percentage of our videos has objects other than humans (e.g., animals) we also experiment with image-video fusion features(denoted by RNEXT, R3D) i.e., concatenation of ResNext-101 features extracted from pre-trained ImageNet with R3D features described above.",3 Baseline Captioning Models,[0],[0]
We extract image features from the same frames which were used to extract R3D features.,3 Baseline Captioning Models,[0],[0]
"For every segment, we set the maximum number of the sequence of features to 120 (i.e., 16X120 frames from the video) and maximum sentence length to 30.",4 Experiments and Results,[0],[0]
We trained using Adam optimizer with learning rate 0.0001.,4 Experiments and Results,[0],[0]
We use GRU as recurrent architecture to encode frames and decode captions with 512 dimensional hidden representation.,4 Experiments and Results,[0],[0]
"We measure the captioning performance with most
commonly-used evaluation metrics: BLEU{3,4}, METEOR, ROUGE-L, and CIDEr following previous works of image and video captioning (Papineni et al., 2002; Lin, 2004; Banerjee and Lavie, 2005; Vedantam et al., 2015).
",4 Experiments and Results,[0],[0]
"In Table 5, we present the performance of our baseline models on VideoStory test dataset.",4 Experiments and Results,[0],[0]
"We observe that models that consider context (seqseq+context) from the previously generated sentence have better performance than the corresponding models without context (seq-seq), with both 3D convolution based features (R3D) as well as image-video fusion features (RNEXT,R3D).",4 Experiments and Results,[0],[0]
"This indicates that our model benefited from contextual information, and that sentences in our stories are contextual, rather than independent.
",4 Experiments and Results,[0],[0]
"To validate the strength of our baseline model, we train our best performing model on ActivityNet Captions.",4 Experiments and Results,[0],[0]
"It achieves 10.92 (METEOR) and 43.42 (CIDEr) on the val set, close to state-of-the-art results of 11.06 and 44.71 by Zhou et al. (2018b), indicating that it is a strong baseline.",4 Experiments and Results,[0],[0]
"However, when evaluating our ActivityNet model on our VideoStory dataset (Table 5, last row), we see significantly lower performance compared to a model trained on our dataset, highlighting the complementary nature of our dataset.
",4 Experiments and Results,[0],[0]
Our image only (single frame) model has the lowest scores across all metrics suggesting that a single image is not enough to generate contextual descriptions.,4 Experiments and Results,[0],[0]
"We observed that our fusion models consistently outperform models with video-only R3D features, indicating features extracted using pre-trained ImageNet complement activity based R3D features.",4 Experiments and Results,[0],[0]
We show qualitative results from the variants of our models in Table 4.,4 Experiments and Results,[0],[0]
We observe that single frame models tend to repeat same captions and seq-seq model without context repeats phrases in the descriptions.,4 Experiments and Results,[0],[0]
This paper introduces a dataset which we sourced from videos on social media and annotated with multi-sentence descriptions.,5 Conclusions,[0],[0]
"We benchmark strong baseline approaches on the dataset, and our evaluations show that our dataset is complementary from prior work due to more diverse topics and the selection of engaging videos which tell a story.",5 Conclusions,[0],[0]
Our VideoStory dataset can serve as a good benchmark to build models for story understanding and multi-sentence video description.,5 Conclusions,[0],[0]
We would like to thank Ranjay Krishna for providing the annotation interface used in Krishna et al. (2017) which we adapted to collect our dataset.,Acknowledgements,[0],[0]
"We would also like to thank Haoqi Fan, Boris Vassilev, Jamie Ray, Sasha Sheng, Nikhila Ravi, and Evan Numbers for their help collecting the dataset, Devi Parikh for feedback on the annotation interface and Anna Rohrbach for useful feedback on drafts of this paper.",Acknowledgements,[0],[0]
"Video content on social media platforms constitutes a major part of the communication between people, as it allows everyone to share their stories.",abstractText,[0],[0]
"However, if someone is unable to consume video, either due to a disability or network bandwidth, this severely limits their participation and communication.",abstractText,[0],[0]
Automatically telling the stories using multi-sentence descriptions of videos would allow bridging this gap.,abstractText,[0],[0]
"To learn and evaluate such models, we introduce VideoStory, a new large-scale dataset for video description as a new challenge for multisentence video description.",abstractText,[0],[0]
"Our VideoStory captions dataset is complementary to prior work and contains 20k videos posted publicly on a social media platform amounting to 396 hours of video with 123k sentences, temporally aligned to the video.",abstractText,[0],[0]
A Dataset for Telling the Stories of Social Media Videos,title,[0],[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2249–2255, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics",text,[0],[0]
Natural language inference (NLI) refers to the problem of determining entailment and contradiction relationships between a premise and a hypothesis.,1 Introduction,[0],[0]
"NLI is a central problem in language understanding (Katz, 1972; Bos and Markert, 2005; van Benthem, 2008; MacCartney and Manning, 2009) and recently the large SNLI corpus of 570K sentence pairs was created for this task (Bowman et al., 2015).",1 Introduction,[0],[0]
"We present a new model for NLI and leverage this corpus for comparison with prior work.
",1 Introduction,[0],[0]
"A large body of work based on neural networks for text similarity tasks including NLI has been published in recent years (Hu et al., 2014; Rocktäschel et al., 2016; Wang and Jiang, 2016; Yin et al., 2016, inter alia).",1 Introduction,[0],[0]
"The dominating trend in these models is to build complex, deep text representation models, for example, with convolutional networks (LeCun et al., 1990, CNNs henceforth) or long short-term memory networks (Hochreiter and Schmidhuber, 1997,
LSTMs henceforth) with the goal of deeper sentence comprehension.",1 Introduction,[0],[0]
"While these approaches have yielded impressive results, they are often computationally very expensive, and result in models having millions of parameters (excluding embeddings).
",1 Introduction,[0],[0]
"Here, we take a different approach, arguing that for natural language inference it can often suffice to simply align bits of local text substructure and then aggregate this information.",1 Introduction,[0],[0]
"For example, consider the following sentences:
• Bob is in his room, but because of the thunder and lightning outside, he cannot sleep.
",1 Introduction,[0],[0]
• Bob is awake.,1 Introduction,[0],[0]
"• It is sunny outside.
",1 Introduction,[0],[0]
The first sentence is complex in structure and it is challenging to construct a compact representation that expresses its entire meaning.,1 Introduction,[0],[0]
"However, it is fairly easy to conclude that the second sentence follows from the first one, by simply aligning Bob with Bob and cannot sleep with awake and recognizing that these are synonyms.",1 Introduction,[0],[0]
"Similarly, one can conclude that It is sunny outside contradicts the first sentence, by aligning thunder and lightning with sunny and recognizing that these are most likely incompatible.
",1 Introduction,[0],[0]
"We leverage this intuition to build a simpler and more lightweight approach to NLI within a neural framework; with considerably fewer parameters, our model outperforms more complex existing neural architectures.",1 Introduction,[0],[0]
"In contrast to existing approaches, our approach only relies on alignment and is fully computationally decomposable with respect to the input text.",1 Introduction,[0],[0]
An overview of our approach is given in Figure 1.,1 Introduction,[0],[0]
"Given two sentences, where each word is repre-
2249
sented by an embedding vector, we first create a soft alignment matrix using neural attention (Bahdanau et al., 2015).",1 Introduction,[0],[0]
We then use the (soft) alignment to decompose the task into subproblems that are solved separately.,1 Introduction,[0],[0]
"Finally, the results of these subproblems are merged to produce the final classification.",1 Introduction,[0],[0]
"In addition, we optionally apply intra-sentence attention (Cheng et al., 2016) to endow the model with a richer encoding of substructures prior to the alignment step.
",1 Introduction,[0],[0]
"Asymptotically our approach does the same total work as a vanilla LSTM encoder, while being trivially parallelizable across sentence length, which can allow for considerable speedups in low-latency settings.",1 Introduction,[0],[0]
"Empirical results on the SNLI corpus show that our approach achieves state-of-the-art results, while using almost an order of magnitude fewer parameters compared to complex LSTM-based approaches.",1 Introduction,[0],[0]
"Our method is motivated by the central role played by alignment in machine translation (Koehn, 2009) and previous approaches to sentence similarity modeling (Haghighi et al., 2005; Das and Smith, 2009; Chang et al., 2010; Fader et al., 2013), natural language inference (Marsi and Krahmer, 2005; MacCartney et al., 2006; Hickl and Bensley, 2007; MacCartney et al., 2008), and semantic parsing (Andreas et al., 2013).",2 Related Work,[0],[0]
"The neural counterpart to alignment, attention (Bahdanau et al., 2015), which is a key part of our approach, was originally proposed and has been predominantly used in conjunction with LSTMs (Rocktäschel et al., 2016; Wang and Jiang, 2016) and to a lesser extent with CNNs (Yin et al., 2016).",2 Related Work,[0],[0]
"In contrast, our use of attention is purely based on word embeddings and our method essentially consists of feed-forward networks that operate largely independently of word order.",2 Related Work,[0],[0]
"Let a = (a1, . . .",3 Approach,[0],[0]
", a`a) and b = (b1, . . .",3 Approach,[0],[0]
", b`b) be the two input sentences of length `a and `b, respectively.",3 Approach,[0],[0]
"We assume that each ai, bj ∈",3 Approach,[0],[0]
Rd is a word embedding vector of dimension d and that each sentence is prepended with a “NULL” token.,3 Approach,[0],[0]
"Our training data comes in the form of labeled pairs {a(n),b(n),y(n)}Nn=1, where y(n) =",3 Approach,[0],[0]
"(y
(n) 1 , . . .",3 Approach,[0],[0]
", y (n) C ) is an indicator vector encoding the label and C is the number of output classes.",3 Approach,[0],[0]
"At test
H ( )+ +…+=ŷ
in the
park alice plays
so m
eo ne
pl ay
in g
m us
ic
ou ts
id e
flute a solo G ( , )
G ( , )
park outside
alice someone
flute+ solo music
…
G ( , )=
=
= flute music F ( , )
",3 Approach,[0],[0]
"Figure 1: Pictoral overview of the approach, showing the Attend (left), Compare (center) and Aggregate (right) steps.
time, we receive a pair of sentences (a,b) and our goal is to predict the correct label y.
Input representation.",3 Approach,[0],[0]
"Let ā = (ā1, . . .",3 Approach,[0],[0]
", ā`a)",3 Approach,[0],[0]
"and b̄ = (b̄1, . . .",3 Approach,[0],[0]
", b̄`b) denote the input representation of each fragment that is fed to subsequent steps of the algorithm.",3 Approach,[0],[0]
The vanilla version of our model simply defines ā := a and b̄ := b.,3 Approach,[0],[0]
"With this input representation, our model does not make use of word order.",3 Approach,[0],[0]
"However, we discuss an extension using intrasentence attention in Section 3.4 that uses a minimal amount of sequence information.
",3 Approach,[0],[0]
"The core model consists of the following three components (see Figure 1), which are trained jointly:
Attend.",3 Approach,[0],[0]
"First, soft-align the elements of ā and b̄ using a variant of neural attention (Bahdanau et al., 2015) and decompose the problem into the comparison of aligned subphrases.
",3 Approach,[0],[0]
Compare.,3 Approach,[0],[0]
"Second, separately compare each aligned subphrase to produce a set of vectors {v1,i}`ai=1 for a and {v2,j}`bj=1 for b.",3 Approach,[0],[0]
"Each v1,i is a nonlinear combination of ai and its (softly) aligned subphrase in b (and analogously for v2,j).
",3 Approach,[0],[0]
Aggregate.,3 Approach,[0],[0]
"Finally, aggregate the sets {v1,i}`ai=1 and {v2,j}`bj=1 from the previous step and use the result to predict the label ŷ.",3 Approach,[0],[0]
"We first obtain unnormalized attention weights eij , computed by a function F ′, which decomposes as:
eij := F ′(āi, b̄j) := F (āi)TF (b̄j) .",3.1 Attend,[0],[0]
"(1)
This decomposition avoids the quadratic complexity that would be associated with separately applying F ′",3.1 Attend,[0],[0]
`a × `b times.,3.1 Attend,[0],[0]
"Instead, only `a + `b applications of F are needed.",3.1 Attend,[0],[0]
"We take F to be a feed-forward neural network with ReLU activations (Glorot et al., 2011).
",3.1 Attend,[0],[0]
"These attention weights are normalized as follows:
βi :=
`b∑
j=1 exp(eij)∑`b k=1 exp(eik) b̄j ,
αj :=
`a∑
i=1",3.1 Attend,[0],[0]
exp(eij)∑`a k=1 exp(ekj),3.1 Attend,[0],[0]
āi .,3.1 Attend,[0],[0]
"(2)
Here βi is the subphrase in b̄ that is (softly) aligned to āi and vice versa for αj .",3.1 Attend,[0],[0]
"Next, we separately compare the aligned phrases {(āi, βi)}`ai=1 and {(b̄j , αj)}`bj=1 using a function G, which in this work is again a feed-forward network:
v1,i := G([āi, βi]) ∀i ∈",3.2 Compare,[0],[0]
"[1, . . .",3.2 Compare,[0],[0]
", `a] , v2,j := G([b̄j , αj ]) ∀j ∈",3.2 Compare,[0],[0]
"[1, . . .",3.2 Compare,[0],[0]
", `b] .",3.2 Compare,[0],[0]
"(3)
where the brackets [·, ·] denote concatenation.",3.2 Compare,[0],[0]
"Note that since there are only a linear number of terms in this case, we do not need to apply a decomposition as was done in the previous step.",3.2 Compare,[0],[0]
"Thus G can jointly take into account both āi, and βi.",3.2 Compare,[0],[0]
"We now have two sets of comparison vectors {v1,i}`ai=1 and {v2,j}`bj=1.",3.3 Aggregate,[0],[0]
"We first aggregate over each set by summation:
v1 =
`a∑
i=1
v1,i , v2 =
`b∑
j=1
v2,j .",3.3 Aggregate,[0],[0]
"(4)
and feed the result through a final classifier H , that is a feed forward network followed by a linear layer:
ŷ = H([v1,v2]) , (5)
where ŷ ∈ RC represents the predicted (unnormalized) scores for each class and consequently the predicted class is given by ŷ = argmaxiŷi.
",3.3 Aggregate,[0],[0]
"For training, we use multi-class cross-entropy loss with dropout regularization (Srivastava et al., 2014):
L(θF , θG, θH) = 1
N
N∑
n=1
C∑
c=1
y(n)c log exp(ŷc)∑C
c′=1 exp(ŷc′) .
",3.3 Aggregate,[0],[0]
"Here θF , θG, θH denote the learnable parameters of the functions F, G and H, respectively.",3.3 Aggregate,[0],[0]
"In the above model, the input representations are simple word embeddings.",3.4 Intra-Sentence Attention (Optional),[0],[0]
"However, we can augment this input representation with intra-sentence attention to encode compositional relationships between words within each sentence, as proposed by Cheng et al. (2016).",3.4 Intra-Sentence Attention (Optional),[0],[0]
"Similar to Eqs. 1 and 2, we define
fij",3.4 Intra-Sentence Attention (Optional),[0],[0]
":= Fintra(ai) TFintra(aj) , (6)
where Fintra is a feed-forward network.",3.4 Intra-Sentence Attention (Optional),[0],[0]
"We then create the self-aligned phrases
a′i := `a∑
j=1 exp(fij + di−j)∑`a",3.4 Intra-Sentence Attention (Optional),[0],[0]
k=1 exp(fik + di−k) aj .,3.4 Intra-Sentence Attention (Optional),[0],[0]
"(7)
The distance-sensitive bias terms di−j ∈ R provides the model with a minimal amount of sequence information, while remaining parallelizable.",3.4 Intra-Sentence Attention (Optional),[0],[0]
These terms are bucketed such that all distances greater than 10 words share the same bias.,3.4 Intra-Sentence Attention (Optional),[0],[0]
"The input representation for subsequent steps is then defined as āi := [ai, a′i] and analogously b̄i :=",3.4 Intra-Sentence Attention (Optional),[0],[0]
"[bi, b′i].",3.4 Intra-Sentence Attention (Optional),[0],[0]
We now discuss the asymptotic complexity of our approach and how it offers a higher degree of parallelism than LSTM-based approaches.,4 Computational Complexity,[0],[0]
Recall that d denotes embedding dimension and ` means sentence length.,4 Computational Complexity,[0],[0]
"For simplicity we assume that all hidden dimensions are d and that the complexity of matrix(d× d)-vector(d× 1) multiplication is O(d2).
",4 Computational Complexity,[0],[0]
"A key assumption of our analysis is that ` < d, which we believe is reasonable and is true of the SNLI dataset (Bowman et al., 2015) where ` < 80, whereas recent LSTM-based approaches have used d ≥ 300.",4 Computational Complexity,[0],[0]
"This assumption allows us to bound the complexity of computing the `2 attention weights.
",4 Computational Complexity,[0],[0]
Complexity of LSTMs.,4 Computational Complexity,[0],[0]
"The complexity of an LSTM cell is O(d2), resulting in a complexity of O(`d2) to encode the sentence.",4 Computational Complexity,[0],[0]
"Adding attention as in Rocktäschel et al. (2016) increases this complexity to O(`d2 + `2d).
",4 Computational Complexity,[0],[0]
Complexity of our Approach.,4 Computational Complexity,[0],[0]
Application of a feed-forward network requires O(d2) steps.,4 Computational Complexity,[0],[0]
"Thus, the Compare and Aggregate steps have complexity O(`d2) and O(d2) respectively.",4 Computational Complexity,[0],[0]
"For the Attend step,
F is evaluated O(`) times, giving a complexity of O(`d2).",4 Computational Complexity,[0],[0]
"Each attention weight eij requires one dot product, resulting in a complexity of O(`2d).
",4 Computational Complexity,[0],[0]
"Thus the total complexity of the model is O(`d2 + `2d), which is equal to that of an LSTM with attention.",4 Computational Complexity,[0],[0]
"However, note that with the assumption that ` < d, this becomes O(`d2) which is the same complexity as a regular LSTM.",4 Computational Complexity,[0],[0]
"Moreover, unlike the LSTM, our approach has the advantage of being parallelizable over `, which can be useful at test time.",4 Computational Complexity,[0],[0]
"We evaluate our approach on the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015).",5 Experiments,[0],[0]
"Given a sentences pair (a,b), the task is to predict whether b is entailed by a, b contradicts a, or whether their relationship is neutral.",5 Experiments,[0],[0]
"The method was implemented in TensorFlow (Abadi et al., 2015).
",5.1 Implementation Details,[0],[0]
Data preprocessing:,5.1 Implementation Details,[0],[0]
"Following Bowman et al. (2015), we remove examples labeled “–” (no gold label) from the dataset, which leaves 549,367 pairs
for training, 9,842 for development, and 9,824 for testing.",5.1 Implementation Details,[0],[0]
We use the tokenized sentences from the non-binary parse provided in the dataset and prepend each sentence with a “NULL” token.,5.1 Implementation Details,[0],[0]
"During training, each sentence was padded up to the maximum length of the batch for efficient training (the padding was explicitly masked out so as not to affect the objective/gradients).",5.1 Implementation Details,[0],[0]
"For efficient batching in TensorFlow, we semi-sorted the training data to first contain examples where both sentences had length less than 20, followed by those with length less than 50, and then the rest.",5.1 Implementation Details,[0],[0]
"This ensured that most training batches contained examples of similar length.
",5.1 Implementation Details,[0],[0]
"Embeddings: We use 300 dimensional GloVe embeddings (Pennington et al., 2014) to represent words.",5.1 Implementation Details,[0],[0]
"Each embedding vector was normalized to have `2 norm of 1 and projected down to 200 dimensions, a number determined via hyperparameter tuning.",5.1 Implementation Details,[0],[0]
Out-of-vocabulary (OOV) words are hashed to one of 100 random embeddings each initialized to mean 0 and standard deviation 1.,5.1 Implementation Details,[0],[0]
"All embeddings remain fixed during training, but the projection matrix is trained.",5.1 Implementation Details,[0],[0]
"All other parameter weights (hidden layers etc.) were initialized from random Gaussians with mean 0 and standard deviation 0.01.
",5.1 Implementation Details,[0],[0]
"Each hyperparameter setting was run on a single machine with 10 asynchronous gradient-update threads, using Adagrad (Duchi et al., 2011) for optimization with the default initial accumulator value of 0.1.",5.1 Implementation Details,[0],[0]
"Dropout regularization (Srivastava et al., 2014) was used for all ReLU layers, but not for the final linear layer.",5.1 Implementation Details,[0],[0]
"We additionally tuned the following hyperparameters and present their chosen values in
parentheses: network size (2-layers, each with 200 neurons), batch size (4), 1 dropout ratio (0.2) and learning rate (0.05–vanilla, 0.025–intra-attention).",5.1 Implementation Details,[0],[0]
All settings were run for 50 million steps (each step indicates one batch) but model parameters were saved frequently as training progressed and we chose the model that did best on the development set.,5.1 Implementation Details,[0],[0]
Results in terms of 3-class accuracy are shown in Table 1.,5.2 Results,[0],[0]
Our vanilla approach achieves state-of-theart results with almost an order of magnitude fewer parameters than the LSTMN of Cheng et al. (2016).,5.2 Results,[0],[0]
Adding intra-sentence attention gives a considerable improvement of 0.5 percentage points over the existing state of the art.,5.2 Results,[0],[0]
"Table 2 gives a breakdown of accuracy on the development set showing that most of our gains stem from neutral, while most losses come from contradiction pairs.
",5.2 Results,[0],[0]
Table 3 shows some wins and losses.,5.2 Results,[0],[0]
"Examples AC are cases where both variants of our approach are correct while both SPINN-PI (Bowman et al., 2016) and the mLSTM (Wang and Jiang, 2016) are incorrect.",5.2 Results,[0],[0]
"In the first two cases, both sentences contain phrases that are either identical or highly lexically related (e.g. “Two kids” and “ocean / beach”) and our approach correctly favors neutral in these cases.",5.2 Results,[0],[0]
"In Example C, it is possible that relying on word-order may confuse SPINN-PI and the mLSTM due to how “fountain” is the object of a preposition in the first sentence but the subject of the second.
",5.2 Results,[0],[0]
"The second set of examples (D-F) are cases where
116 or 32 also work well and are a bit more stable.
",5.2 Results,[0],[0]
our vanilla approach is incorrect but mLSTM and SPINN-PI are correct.,5.2 Results,[0],[0]
Example F requires sequential information and neither variant of our approach can predict the correct class.,5.2 Results,[0],[0]
"Examples D-E are interesting however, since they don’t require word order information, yet intra-attention seems to help.",5.2 Results,[0],[0]
"We suspect this may be because the word embeddings are not fine-grained enough for the algorithm to conclude that “play/watch” is a contradiction, but intra-attention, by adding an extra layer of composition/nonlinearity to incorporate context, compensates for this.
",5.2 Results,[0],[0]
"Finally, Examples G-I are cases that all methods get wrong.",5.2 Results,[0],[0]
The first is actually representative of many examples in this category where there is one critical word that separates the two sentences (close vs open in this case) and goes unnoticed by the algorithms.,5.2 Results,[0],[0]
Examples H requires inference about numbers and Example I needs sequence information.,5.2 Results,[0],[0]
We presented a simple attention-based approach to natural language inference that is trivially parallelizable.,6 Conclusion,[0],[0]
The approach outperforms considerably more complex neural methods aiming for text understanding.,6 Conclusion,[0],[0]
"Our results suggest that, at least for this task, pairwise comparisons are relatively more important than global sentence-level representations.",6 Conclusion,[0],[0]
"We thank Slav Petrov, Tom Kwiatkowski, Yoon Kim, Erick Fonseca, Mark Neumann for useful discussion and Sam Bowman and Shuohang Wang for providing us their model outputs for error analysis.",Acknowledgements,[0],[0]
We propose a simple neural architecture for natural language inference.,abstractText,[0],[0]
"Our approach uses attention to decompose the problem into subproblems that can be solved separately, thus making it trivially parallelizable.",abstractText,[0],[0]
"On the Stanford Natural Language Inference (SNLI) dataset, we obtain state-of-the-art results with almost an order of magnitude fewer parameters than previous work and without relying on any word-order information.",abstractText,[0],[0]
Adding intra-sentence attention that takes a minimum amount of order into account yields further improvements.,abstractText,[0],[0]
A Decomposable Attention Model for Natural Language Inference,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 37–46 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,[0],[0]
"Human languages are far from arbitrary; crosslinguistically, they exhibit surprising similarity in many respects and many properties appear to be universally true.",1 Introduction,[0],[0]
"The field of linguistic typology seeks to investigate, describe and quantify the axes along which languages vary.",1 Introduction,[0],[0]
"One facet of language that has been the subject of heavy investigation is the nature of vowel inventories, i.e., which vowels a language contains.",1 Introduction,[0],[0]
"It is a cross-linguistic universal that all spoken languages have vowels (Gordon, 2016), and the underlying principles guiding vowel selection are understood: vowels must be both easily recognizable and well-dispersed (Schwartz et al., 2005).",1 Introduction,[0],[0]
"In this work, we offer a more formal treatment of the subject, deriving a generative probability model of vowel inventory typology.",1 Introduction,[0],[0]
"Our work builds on (Cotterell and Eisner, 2017) by investigating not just discrete IPA inventories but the cross-linguistic variation in acoustic formants.
",1 Introduction,[0],[0]
"The philosophy behind our approach is that linguistic typology should be treated probabilistically
and its goal should be the construction of a universal prior over potential languages.",1 Introduction,[0],[0]
"A probabilistic approach does not rule out linguistic systems completely (as long as one’s theoretical formalism can describe them at all), but it can position phenomena on a scale from very common to very improbable.",1 Introduction,[0],[0]
Probabilistic modeling also provides a discipline for drawing conclusions from sparse data.,1 Introduction,[0],[0]
"While we know of over 7000 human languages, we have some sort of linguistic analysis for only 2300 of them (Comrie et al., 2013), and the dataset used in this paper (Becker-Kristal, 2010) provides simple vowel data for fewer than 250 languages.
",1 Introduction,[0],[0]
Formants are the resonant frequencies of the human vocal tract during the production of speech sounds.,1 Introduction,[0],[0]
"We propose a Bayesian generative model of vowel inventories, where each language’s inventory is a finite subset of acoustic vowels represented as points (F1, F2) ∈ R2.",1 Introduction,[0],[0]
We deploy tools from the neural-network and point-process literatures and experiment on a dataset with 233 distinct languages.,1 Introduction,[0],[0]
We show that our most complicated model outperforms simpler models.,1 Introduction,[0],[0]
Much of human communication takes place through speech: one conversant emits a sound wave to be comprehended by a second.,2 Acoustic Phonetics and Formants,[0],[0]
"In this work, we consider the nature of the portions of such sound waves that correspond to vowels.",2 Acoustic Phonetics and Formants,[0],[0]
"We briefly review the relevant bits of acoustic phonetics so as to give an overview of the data we are actually modeling and develop our notation.
",2 Acoustic Phonetics and Formants,[0],[0]
The anatomy of a sound wave.,2 Acoustic Phonetics and Formants,[0],[0]
"The sound wave that carries spoken language is a function from time to amplitude, describing sound pressure variation in the air.",2 Acoustic Phonetics and Formants,[0],[0]
"To distinguish vowels, it is helpful to transform this function into a spectrogram (Fig. 1) by using a short-time Fourier transform
37
0",2 Acoustic Phonetics and Formants,[0],[0]
"Hz
1000 Hz
2000 Hz
3000 Hz
4000 Hz
5000 Hz /i/ /u/ /ɑ/
",2 Acoustic Phonetics and Formants,[0],[0]
"Figure 1: Example spectrogram of the three English vowels: /i/, /u/ and /A/.",2 Acoustic Phonetics and Formants,[0],[0]
The x-axis is time and y-axis is frequency.,2 Acoustic Phonetics and Formants,[0],[0]
The first two formants F1 and F2 are marked in with arrows for each vowel.,2 Acoustic Phonetics and Formants,[0],[0]
"The figure was made with Praat (Boersma et al., 2002).
",2 Acoustic Phonetics and Formants,[0],[0]
"(Deng and O’Shaughnessy, 2003, Chapter 1) to decompose each short interval of the wave function into a weighted sum of sinusoidal waves of different frequencies (measured in Hz).",2 Acoustic Phonetics and Formants,[0],[0]
"At each interval, the variable darkness of the spectrogram indicates the weights of the different frequencies.",2 Acoustic Phonetics and Formants,[0],[0]
"In phonetic analysis, a common quantity to consider is a formant—a local maximum of the (smoothed) frequency spectrum.",2 Acoustic Phonetics and Formants,[0],[0]
The fundamental frequency F0 determines the pitch of the sound.,2 Acoustic Phonetics and Formants,[0],[0]
"The formants F1 and F2 determine the quality of the vowel.
",2 Acoustic Phonetics and Formants,[0],[0]
Two is all you need (and what we left out).,2 Acoustic Phonetics and Formants,[0],[0]
"In terms of vowel recognition, it is widely speculated that humans rely almost exclusively on the first two formants of the sound wave (Ladefoged, 2001, Chapter 5).",2 Acoustic Phonetics and Formants,[0],[0]
"The two-formant assumption breaks down in edge cases: e.g., the third formant F3 helps to distinguish the roundness of the vowel (Ladefoged, 2001, Chapter 5).",2 Acoustic Phonetics and Formants,[0],[0]
Other non-formant features may also play a role.,2 Acoustic Phonetics and Formants,[0],[0]
"For example, in tonal languages, the same vowel may be realized with different tones (which are signaled using F0): Mandarin Chinese makes a distinction between mǎ (horse) and má (hemp) without modifying the quality of the vowel /a/.",2 Acoustic Phonetics and Formants,[0],[0]
"Other features, such as creaky voice, can play a role in distinguishing phonemes.",2 Acoustic Phonetics and Formants,[0],[0]
"We do not explicitly model any of these aspects of vowel space, limiting ourselves to (F1, F2) as in previous work (Liljencrants and Lindblom, 1972).",2 Acoustic Phonetics and Formants,[0],[0]
"However, it would be easy to extend all the models we will propose here to incorporate such information, given appropriate datasets.",2 Acoustic Phonetics and Formants,[0],[0]
The vowel inventories of the world’s languages display clear structure and appear to obey several underlying principles.,3 The Phonology of Vowel Systems,[0],[0]
"The most prevalent of these
principles are focalization and dispersion.
",3 The Phonology of Vowel Systems,[0],[0]
Focalization.,3 The Phonology of Vowel Systems,[0],[0]
"The notion of focalization grew out of quantal vowel theory (Stevens, 1989).",3 The Phonology of Vowel Systems,[0],[0]
Quantal vowels are those that are phonetically “better” than others.,3 The Phonology of Vowel Systems,[0],[0]
"They tend to display certain properties, e.g., the formants tend to be closer together (Stevens, 1987).",3 The Phonology of Vowel Systems,[0],[0]
"Cross-linguistically, quantal vowels are the most frequently attested vowels, e.g., the cross-linguistically common vowel /i/ is considered quantal, but less common /y/ is not.
Dispersion.",3 The Phonology of Vowel Systems,[0],[0]
The second core principle of vowel system organization is known as dispersion.,3 The Phonology of Vowel Systems,[0],[0]
"As the name would imply, the principle states that the vowels in “good” vowel systems tend to be spread out.",3 The Phonology of Vowel Systems,[0],[0]
The motivation for such a principle is clear—a well-dispersed set of vowels reduces a listener’s potential confusion over which vowel is being pronounced.,3 The Phonology of Vowel Systems,[0],[0]
"See Schwartz et al. (1997) for a review of dispersion in vowel system typology and its interaction with focalization, which has led to the joint dispersion-focalization theory.
",3 The Phonology of Vowel Systems,[0],[0]
Notation.,3 The Phonology of Vowel Systems,[0],[0]
We will denote the universal set of international phonetic alphabet (IPA) symbols as V .,3 The Phonology of Vowel Systems,[0],[0]
"The observed vowel inventory for language ` has size n` and is denoted V ` = {(v`1,v`1), . . .",3 The Phonology of Vowel Systems,[0],[0]
", (v`n` ,v`n`)} ⊆ V × Rd, where for each k ∈",3 The Phonology of Vowel Systems,[0],[0]
"[1, n`], v`k ∈ V is an IPA symbol assigned by a linguist and v`k ∈ Rd is a vector of d measurable phonetic quantities.",3 The Phonology of Vowel Systems,[0],[0]
"In short, the IPA symbol v`k was assigned as a label for a phoneme with pronunciation v`k.",3 The Phonology of Vowel Systems,[0],[0]
"The ordering of the elements within V ` is arbitrary.
Goals.",3 The Phonology of Vowel Systems,[0],[0]
"This framework recognizes that the same IPA symbol v (such as /u/) may represent a slightly different sound v in one language than in another, although they are transcribed identically.",3 The Phonology of Vowel Systems,[0],[0]
We are specifically interested in how the vowels in a language influence one another’s fine-grained pronunciation in Rd.,3 The Phonology of Vowel Systems,[0],[0]
"In general, there is no reason to suspect that speakers of two languages, whose phonological systems contain the same IPA symbol, should produce that vowel with identical formants.
Data.",3 The Phonology of Vowel Systems,[0],[0]
"For the remainder of the paper, we will take d = 2 so that each v = (F1, F2) ∈ R2, the vector consisting of the first two formant values, as compiled from the field literature by BeckerKristal (2006).",3 The Phonology of Vowel Systems,[0],[0]
This dataset provides inventories V ` in the form above.,3 The Phonology of Vowel Systems,[0],[0]
"Thus, we do not consider further variation of the vowel pronunciation that
may occur within the language (between speakers, between tokens of the vowel, or between earlier and later intervals within a token).",3 The Phonology of Vowel Systems,[0],[0]
"Previous work (Cotterell and Eisner, 2017) has placed a distribution over discrete phonemes, ignoring the variation across languages in the pronunciation of each phoneme.",4 Phonemes versus Phones,[0],[0]
"In this paper, we crack open the phoneme abstraction, moving to a learned set of finer-grained phones.
",4 Phonemes versus Phones,[0],[0]
Cotterell and Eisner (2017) proposed (among other options) using a determinantal point process (DPP) over a universal inventory V of 53 symbolic (IPA) vowels.,4 Phonemes versus Phones,[0],[0]
"A draw from such a DPP is a language-specific inventory of vowel phonemes, V ⊆ V .",4 Phonemes versus Phones,[0],[0]
"In this paper, we say that a language instead draws its inventory from a larger set V̄ , again using a DPP.",4 Phonemes versus Phones,[0],[0]
"In both cases, the reason to use a DPP is that it prefers relatively diverse inventories whose individual elements are relatively quantal.
",4 Phonemes versus Phones,[0],[0]
"While we could in principle identify V̄ with Rd, for convenience we still take it to be a (large) discrete finite set V̄ = {v̄1, . . .",4 Phonemes versus Phones,[0],[0]
", v̄N}, whose elements we call phones.",4 Phonemes versus Phones,[0],[0]
"V̄ is a learned cross-linguistic parameter of our model; thus, its elements—the “universal phones”—may or may not correspond to phonetic categories traditionally used by linguists.
",4 Phonemes versus Phones,[0],[0]
"We presume that language ` draws from the DPP a subset V̄ ` ⊆ V̄ , whose size we call n`.",4 Phonemes versus Phones,[0],[0]
"For each universal phone v̄i that appears in this inventory V̄ `, the language then draws an observable languagespecific pronunciation v`i ∼ N",4 Phonemes versus Phones,[0],[0]
"( µi, σ 2I )
from a distribution associated cross-linguistically with the universal phone v̄i.",4 Phonemes versus Phones,[0],[0]
"We now have an inventory of pronunciations.
",4 Phonemes versus Phones,[0],[0]
"As a final step in generating the vowel inventory, we could model IPA labels.",4 Phonemes versus Phones,[0],[0]
For each v̄i ∈ V̄,4 Phonemes versus Phones,[0],[0]
"`, a field linguist presumably draws the IPA label v`i conditioned on all the pronunciations {v`i ∈",4 Phonemes versus Phones,[0],[0]
Rd : v̄i ∈ V̄,4 Phonemes versus Phones,[0],[0]
`} in the inventory (and perhaps also on their underlying phones v̄i ∈ V̄,4 Phonemes versus Phones,[0],[0]
`).,4 Phonemes versus Phones,[0],[0]
This labeling process may be complex.,4 Phonemes versus Phones,[0],[0]
"While each pronunciation in Rd (or each underlying phone in V̄) may have a preference for certain IPA labels in V , the n` labels must be drawn jointly because the linguist will take care not to use the same label for two phones, and also because the linguist may like to describe the inventory using a small number of distinct IPA features, which will tend to favor factorial grids of symbols.",4 Phonemes versus Phones,[0],[0]
"The linguist’s use of IPA
features may also be informed by phonological and phonetic processes in the language.",4 Phonemes versus Phones,[0],[0]
"We leave modeling of this step to future work; so our current likelihood term ignores the evidence contributed by the IPA labels in the dataset, considering only the pronunciations in Rd.
",4 Phonemes versus Phones,[0],[0]
"The overall idea is that human languages ` draw their inventories from some universal prior, which we are attempting to reconstruct.",4 Phonemes versus Phones,[0],[0]
"A caveat is that we will train our method by maximum-likelihood, which does not quantify our uncertainty about the reconstructed parameters.",4 Phonemes versus Phones,[0],[0]
"An additional caveat is that some languages in our dataset are related to one another, which belies the idea that they were drawn independently.",4 Phonemes versus Phones,[0],[0]
"Ideally, one ought to capture these relationships using hierarchical or evolutionary modeling techniques.",4 Phonemes versus Phones,[0],[0]
"Before delving into our generative model, we briefly review technical background used by Cotterell and Eisner (2017).",5 Determinantal Point Processes,[0],[0]
"A DPP is a probability distribution over the subsets of a fixed ground set of size N—in our case, the set of phones V̄ .",5 Determinantal Point Processes,[0],[0]
"The DPP is usually given as an L-ensemble (Borodin and Rains, 2005), meaning that it is parameterized by a positive semi-definite matrix L ∈ RN×N .",5 Determinantal Point Processes,[0],[0]
"Given a discrete base set V̄ of phones, the probability of a subset V̄ ⊆ V̄ is given by
p(V̄ ) ∝",5 Determinantal Point Processes,[0],[0]
"det (LV̄ ) , (1)
where LV̄ is the submatrix of L corresponding to the rows and columns associated with the subset V̄ ⊆ V̄ .",5 Determinantal Point Processes,[0],[0]
"The entry Lij , where i 6= j, has the effect of describing the similarity between the elements v̄i and v̄j (both in V̄)—an ingredient needed to model dispersion.",5 Determinantal Point Processes,[0],[0]
"And, the entry Lii describes the quality—focalization—of the vowel v̄i, i.e., how much the model wants to have v̄i in a sampled set independent of the other members.",5 Determinantal Point Processes,[0],[0]
"In this work, each phone v̄i ∈ V̄ is associated with a probability density over the space of possible pronunciations R2.",5.1 Probability Kernel,[0],[0]
Our measure of phone similarity will consider the “overlap” between the densities associated with two phones.,5.1 Probability Kernel,[0],[0]
This works as follows:,5.1 Probability Kernel,[0],[0]
"Given two densities f(x, y) and f ′(x, y) over R2, we define the kernel (Jebara et al., 2004) as
K(f, f ′; ρ) = ∫
x
∫
y f(x, y)ρf ′(x, y)ρdx dy, (3)
with inverse temperature parameter ρ.",5.1 Probability Kernel,[0],[0]
"In our setting, f, f ′ will both be Gaussian distributions with means µ and µ′ that share a fixed spherical covariance matrix σ2I .",5.1 Probability Kernel,[0],[0]
"Then eq. (3) and indeed its generalization to any Rd has a closedform solution (Jebara et al., 2004, §3.1):
K(f,f ′; ρ) = (4)
(2ρ) d 2 ( 2πσ2 ) (1−2ρ)d 2 exp ( −ρ||µ− µ
′||2 4σ2
) .
",5.1 Probability Kernel,[0],[0]
"Notice that making ρ small (i.e., high temperature) has an effect on (4) similar to scaling the variance σ2 by the temperature, but it also results in changing the scale of K, which affects the balance between dispersion and focalization in (6) below.",5.1 Probability Kernel,[0],[0]
The probability kernel given in eq.,5.2 Focalization Score,[0],[0]
(3) naturally handles the linguistic notion of dispersion.,5.2 Focalization Score,[0],[0]
What about focalization?,5.2 Focalization Score,[0],[0]
"We say that a phone is focal to the extent that it has a high score
F (µ) = exp (U2 tanh(U1µ + b1) + b2) > 0",5.2 Focalization Score,[0],[0]
"(5)
where µ is the mean of its density.",5.2 Focalization Score,[0],[0]
To learn the parameters of this neural network from data is to learn which phones are focal.,5.2 Focalization Score,[0],[0]
We use a neural network since the focal regions of R2 are distributed in a complex way.,5.2 Focalization Score,[0],[0]
"If fi = N (µi, σ2I) is the density associated with the phone v̄i, we may populate an N × N real
Algorithm 1 Generative Process 1: N ∼ Poisson (λ) (∈ N) 1 2: for i = 1 to N : 3: µi ∼ N (0, I) (∈ R2) 2 4: define L ∈ RN×N via (6) 5: for ` = 1 to M : 6: V̄ ` ∼ DPP (L) (⊆ [1, N ]); let n` = |V̄",5.3 The L Matrix,[0],[0]
`| 3 7: for i ∈ V̄,5.3 The L Matrix,[0],[0]
"` : 8: ṽ`i ∼ N ( µi, σ 2I ) 4
9: v`i = νθ ( ṽ`i )
4
matrix L where
Lij = { K(fi, fj ; ρ) if i 6= j K(fi, fj ; ρ) + F (µi) if i = j
(6)
Since L is the sum of two positive definite matrices (the first specializes a known kernel and the second is diagonal and positive), it is also positive definite.",5.3 The L Matrix,[0],[0]
"As a result, it can be used to parameterize a DPP over V̄ .",5.3 The L Matrix,[0],[0]
"Indeed, since L is positive definite and not merely positive semidefinite, it will assign positive probability to any subset of V̄ .
",5.3 The L Matrix,[0],[0]
"As previously noted, this DPP does not define a distribution over an infinite set, e.g., the powerset of R2, as does recent work on continuous DPPs (Affandi et al., 2013).",5.3 The L Matrix,[0],[0]
"Rather, it defines a distribution over the powerset of a set of densities with finite cardinality.",5.3 The L Matrix,[0],[0]
"Once we have sampled a subset of densities, a real-valued quantity may be additionally sampled from each sampled density.",5.3 The L Matrix,[0],[0]
We are now in a position to expound our generative model of continuous-space vowel typology.,6 A Deep Generative Model,[0],[0]
"We
generate a set of formant pairs for M languages in a four step process.",6 A Deep Generative Model,[0],[0]
"Note that throughout this exposition, language-specific quantities with be superscripted with an integral language marker `, whereas universal quantities are left unsuperscripted.",6 A Deep Generative Model,[0],[0]
The generative process is written in algorithmic form in Alg. 1.,6 A Deep Generative Model,[0],[0]
"Note that each step is numbered and color-coded for ease of comparison with the full joint likelihood in Fig. 2.
",6 A Deep Generative Model,[0],[0]
Step 1 : p(N).,6 A Deep Generative Model,[0],[0]
"We sample the size N of the universal phone inventory V̄ from a Poisson distribution with a rate parameter λ, i.e.,
N ∼ Poisson (λ) .",6 A Deep Generative Model,[0],[0]
"(7)
That is, we do not presuppose a certain number of phones in the model.
",6 A Deep Generative Model,[0],[0]
"Step 2 : p(µ1, . . .",6 A Deep Generative Model,[0],[0]
",µN ).",6 A Deep Generative Model,[0],[0]
"Next, we sample the means µi of the Gaussian phones.",6 A Deep Generative Model,[0],[0]
"In the model presented here, we assume that each phone is generated independently, so p(µ1, . . .",6 A Deep Generative Model,[0],[0]
",µN ) =∏N i=1",6 A Deep Generative Model,[0],[0]
p(µi).,6 A Deep Generative Model,[0],[0]
"Also, we assume a standard Gaussian prior over the means, µi ∼ N (0, I).",6 A Deep Generative Model,[0],[0]
"The sampled means define our N Gaussian phones N ( µi, σ 2I ) : we are assuming for simplicity that all phones share a single spherical covariance matrix, defined by the hyperparameter σ2.",6 A Deep Generative Model,[0],[0]
"The dispersion and focalization of these phones define the matrix L according to equations (4)–(6), where ρ in (4) and the weights of the focalization neural net (5) are also hyperparameters.
",6 A Deep Generative Model,[0],[0]
"Step 3 : p(V̄ ` | µ1, . . .",6 A Deep Generative Model,[0],[0]
",µN ).",6 A Deep Generative Model,[0],[0]
"Next, for each language ` ∈",6 A Deep Generative Model,[0],[0]
"[1, . . .",6 A Deep Generative Model,[0],[0]
",M ], we sample a diverse subset of the N phones, via a single draw from a DPP parameterized by matrix L:
V̄ ` ∼ DPP(L), (8)
where V̄ ` ⊆ [1, N ].",6 A Deep Generative Model,[0],[0]
"Thus, i ∈ V̄ ` means that language ` contains phone v̄i.",6 A Deep Generative Model,[0],[0]
"Note that even the size of the inventory, n` = |V̄",6 A Deep Generative Model,[0],[0]
"`|, was chosen by the DPP.",6 A Deep Generative Model,[0],[0]
"In general, we have n` N .",6 A Deep Generative Model,[0],[0]
Step 4 : ∏ i∈V̄ ` p(v ` i | µi) The final step in our generative process is that the phones v̄i in language ` must generate the pronunciations v`i ∈ R2 (formant vectors) that are actually observed in language `.,6 A Deep Generative Model,[0],[0]
Each vector takes two steps.,6 A Deep Generative Model,[0],[0]
For each i ∈ V̄,6 A Deep Generative Model,[0],[0]
"`, we generate an underlying ṽi ∈ R2 from the corresponding Gaussian phone.",6 A Deep Generative Model,[0],[0]
"Then, we run
this vector through a feed-forward neural network νθ with parameters θ.",6 A Deep Generative Model,[0],[0]
"In short:
ṽ`i ∼ N (µi, σ2I) (9) v`i = νθ(ṽ ` i), (10)
where the second step is deterministic.",6 A Deep Generative Model,[0],[0]
"We can fuse these two steps into a single step p(vi | µi), whose closed-form density is given in eq. (12) below.",6 A Deep Generative Model,[0],[0]
"In effect, step 4 takes a Gaussian phone as input and produces the observed formant vector with an underlying formant vector in the middle.
",6 A Deep Generative Model,[0],[0]
This completes our generative process.,6 A Deep Generative Model,[0],[0]
"We do not observe all the steps, but only the final collection of pronunciations v`i for each language, where the subscripts i that indicate phone identity have been lost.",6 A Deep Generative Model,[0],[0]
"The probability of this incomplete dataset involves summing over possible phones for each pronunciation, and is presented in Fig. 2.",6 A Deep Generative Model,[0],[0]
A crucial bit of our model is running a sample from a Gaussian through a neural network.,6.1 A Neural Transformation of a Gaussian,[0],[0]
"Under certain restrictions, we can find a closed form for the resulting density; we discuss these below.",6.1 A Neural Transformation of a Gaussian,[0],[0]
"Let νθ be a depth-2 multi-layer perceptron
νθ(ṽi) = W2 tanh (W1ṽi + b1) + b2.",6.1 A Neural Transformation of a Gaussian,[0],[0]
"(11)
In order to find a closed-form solution, we require that (5) be a diffeomorphism, i.e., an invertible mapping from R2 → R2 where both νθ and its inverse ν−1θ are differentiable.",6.1 A Neural Transformation of a Gaussian,[0],[0]
"This will be true as long asW1,W2 ∈ R2×2 are square matrices of fullrank and we choose a smooth, invertible activation function, such as tanh.",6.1 A Neural Transformation of a Gaussian,[0],[0]
"Under those conditions, we may apply the standard theorem for transforming a random variable (see Stark and Woods, 2011):
p(vi | µi) = p(ν−1θ (vi) | µi) det Jν−1θ (vi) = p(ṽi",6.1 A Neural Transformation of a Gaussian,[0],[0]
"| µi) det Jν−1θ (vi) (12)
where Jν−1θ (x) is the Jacobian of the inverse of the neural network at the point x. Recall that p(ṽi | µi) is Gaussian-distributed.",6.1 A Neural Transformation of a Gaussian,[0],[0]
Imbued in our generative story are a number of assumptions about the linguistic processes behind vowel inventories.,7 Modeling Assumptions,[0],[0]
"We briefly draw connections between our theory and the linguistics literature.
",7 Modeling Assumptions,[0],[0]
Why underlying phones?,7 Modeling Assumptions,[0],[0]
A technical assumption of our model is the existence of a universal set of underlying phones.,7 Modeling Assumptions,[0],[0]
"Each phone is equipped with a probability distribution over reported acoustic measurements (pronunciations), to allow for a single phone to account for multiple slightly different pronunciations in different languages (though never in the same language).",7 Modeling Assumptions,[0],[0]
"This distribution can capture both actual interlingual variation and also random noise in the measurement process.
",7 Modeling Assumptions,[0],[0]
"While our universal phones may seem to resemble the universal IPA symbols used in phonological transcription, they lack the rich featural specifications of such phonemes.",7 Modeling Assumptions,[0],[0]
"A phone in our model has no features other than its mean position, which wholly determines its behavior.",7 Modeling Assumptions,[0],[0]
"Our universal phones are not a substantive linguistic hypothesis, but are essentially just a way of partitioning R2 into finitely many small regions whose similarity and focalization can be precomputed.",7 Modeling Assumptions,[0],[0]
"This technical trick allows us to use a discrete rather than a continuous DPP over the R2 space.1
Why a neural network?",7 Modeling Assumptions,[0],[0]
"Our phones are Gaussians of spherical variance σ2, presumed to be scattered with variance 1 about a two-dimensional latent vowel space.",7 Modeling Assumptions,[0],[0]
"Distances in this latent space are used to compute the dissimilarity of phones for modeling dispersion, and also to describe the phone’s ability to vary across languages.",7 Modeling Assumptions,[0],[0]
"That is, two phones that are distant in the latent space can appear in the same inventory—presumably they are easy to discriminate in both perception and articulation—and it is easy to choose which one better explains an acoustic measurement, thereby affecting the other measurements that may appear in the inventory.
",7 Modeling Assumptions,[0],[0]
We relate this latent space to measurable acoustic space by a learned diffeomorphism,7 Modeling Assumptions,[0],[0]
νθ,7 Modeling Assumptions,[0],[0]
"(Cotterell and Eisner, 2017).",7 Modeling Assumptions,[0],[0]
ν−1θ can be regarded as warping the acoustic distances into perceptual/articulatory distances.,7 Modeling Assumptions,[0],[0]
"In some “high-resolution” regions of acoustic space, phones with fairly similar (F1, F2) values might yet be far apart in the latent space.",7 Modeling Assumptions,[0],[0]
"Conversely, in other regions, relatively large acous-
1Indeed, we could have simply taken our universal phone set to be a huge set of tiny, regularly spaced overlapping Gaussians that “covered” (say) the unit circle.",7 Modeling Assumptions,[0],[0]
"As a computational matter, we instead opted to use a smaller set of Gaussians, giving the learner the freedom to infer their positions and tune their variance σ2.",7 Modeling Assumptions,[0],[0]
"Because of this freedom, this set should not be too large, or a MAP learner may overfit the training data with zero-variance Gaussians and be unable to explain the test languages—similar to overfitting a Gaussian mixture model.
",7 Modeling Assumptions,[0],[0]
tic changes in some direction might not prevent two phones from acting as similar or two pronunciations from being attributed to the same phone.,7 Modeling Assumptions,[0],[0]
"In general, a unit circle of radius σ in latent space may be mapped by νθ to an oddly shaped connected region in acoustic space, and a Gaussian in latent space may be mapped to a multimodal distribution.",7 Modeling Assumptions,[0],[0]
"We fit our model via MAP-EM (Dempster et al., 1977).",8 Inference and Learning,[0],[0]
The E-step involves deciding which phones each language has.,8 Inference and Learning,[0],[0]
"To achieve this, we fashion a Gibbs sampler (Geman and Geman, 1984), yielding a Markov-Chain Monte Carlo E-step (Levine and Casella, 2001).",8 Inference and Learning,[0],[0]
"Inference in our model is intractable even when the phones µ1, . . .",8.1 Inference: MCMC E-Step,[0],[0]
",µN are fixed.",8.1 Inference: MCMC E-Step,[0],[0]
"Given a language with n vowels, we have to determine which subset of the N phones best explains those vowels.",8.1 Inference: MCMC E-Step,[0],[0]
"As discussed above, the alignment a between the n vowels and n of the N phones represents a latent variable.",8.1 Inference: MCMC E-Step,[0],[0]
"Marginalizing it out is #P-hard, as we can see that it is equivalent to summing over all bipartite matchings in a weighted graph, which, in turn, is as costly as computing the permanent of a matrix (Valiant, 1979).",8.1 Inference: MCMC E-Step,[0],[0]
Our sampler2 is an approximation algorithm for the task.,8.1 Inference: MCMC E-Step,[0],[0]
"We are interested in sampling a, the labeling of observed vowels with universal phones.",8.1 Inference: MCMC E-Step,[0],[0]
"Note that this implicitly samples the language’s phone inventory V̄ (a), which is fully determined by a.
Specifically, we employ an MCMC method closely related to Gibbs sampling.",8.1 Inference: MCMC E-Step,[0],[0]
"At each step of the sampler, we update our vowel-phone alignment a` as follows.",8.1 Inference: MCMC E-Step,[0],[0]
Choose a language ` and a vowel index k ∈,8.1 Inference: MCMC E-Step,[0],[0]
"[1, n`], and let i = a`k (that is, pronunciation v`,k is currently labeled with universal phone v̄i).",8.1 Inference: MCMC E-Step,[0],[0]
"We will consider changing a`k to j, where j is drawn from the (N − n`) phones that do not appear in V̄ (a`), heuristically choosing j in proportion to the likelihood p(v`,k | µj).",8.1 Inference: MCMC E-Step,[0],[0]
We then stochastically decide whether to keep a`k = i or set a`k = j in proportion to the resulting values of the product 4 · 3 in eq.,8.1 Inference: MCMC E-Step,[0],[0]
"(2).
",8.1 Inference: MCMC E-Step,[0],[0]
"For a single E-step, the Gibbs sampler “warmstarts” with the labeling from the end of the previous iteration’s E-step.",8.1 Inference: MCMC E-Step,[0],[0]
"It sweeps S = 5 times
2Taken from Volkovs and Zemel (2012, 3.1).
through all vowels for all languages, and returns S sampled labelings, one from the end of each sweep.
",8.1 Inference: MCMC E-Step,[0],[0]
"We are also interested in automatically choosing the number of phones N , for which we take the Poisson’s rate parameter λ = 100.",8.1 Inference: MCMC E-Step,[0],[0]
"To this end, we employ reversible-jump MCMC (Green, 1995), resampling N at the start of every E-step.",8.1 Inference: MCMC E-Step,[0],[0]
"Given the set of sampled alignments provided by the E-step, our M-step consists of optimizing the log-likelihood of the now-complete training data using the inferred latent variables.",8.2 Learning: M-Step,[0],[0]
"We achieved this through SGD training of the diffeomorphism parameters θ, the means µi of the Gaussian phones, and the parameters of the focalization kernel F .",8.2 Learning: M-Step,[0],[0]
"Our data is taken from the Becker-Kristal corpus (Becker-Kristal, 2006), which is a compilation of various phonetic studies and forms the largest multilingual phonetic database.",9.1 Data,[0],[0]
Each entry in the corpus corresponds to a linguist’s phonetic description of a language’s vowel system: an inventory consisting of IPA symbols where each symbol is associated with two or more formant values.,9.1 Data,[0],[0]
The corpus contains data from 233 distinct languages.,9.1 Data,[0],[0]
"When multiple inventories were available for the same language (due to various studies in the literature), we selected one at random and discarded the others.",9.1 Data,[0],[0]
Baseline #1: Removing dispersion.,9.2 Baselines,[0],[0]
The key technical innovation in our work lies in the incorporation of a DPP into a generative model of vowel formants—a continuous-valued quantity.,9.2 Baselines,[0],[0]
"The role of the DPP was to model the linguistic principle of dispersion—we may cripple this portion of our model, e.g., by forcing K to be a diagonal kernel, i.e., Kij = 0 for i 6=",9.2 Baselines,[0],[0]
j. In this case the DPP becomes a Bernoulli Point Process (BPP)—a special case of the DPP.,9.2 Baselines,[0],[0]
"Since dispersion is widely accepted to be an important principle governing naturally occurring vowel systems, we expect a system trained without such knowledge to perform worse.
",9.2 Baselines,[0],[0]
Baseline #2: Removing the neural network νθ.,9.2 Baselines,[0],[0]
Another question we may ask of our formulation is whether we actually need a fancy neural mapping νθ to model our typological data well.,9.2 Baselines,[0],[0]
"The human
perceptual system is known to perform a non-linear transformation on acoustic signals, starting with the non-linear cochlear transform that is physically performed in the ear.",9.2 Baselines,[0],[0]
"While ν−1θ is intended as loosely analogous, we determine its benefit by removing eq.",9.2 Baselines,[0],[0]
"(10) from our generative story, i.e., we take the observed formants vk to arise directly from the Gaussian phones.
",9.2 Baselines,[0],[0]
Baseline #3: Supervised phones and alignments.,9.2 Baselines,[0],[0]
A final baseline we consider is supervised phones.,9.2 Baselines,[0],[0]
Linguists standardly employ a finite set of phones— symbols from the international phonetic alphabet (IPA).,9.2 Baselines,[0],[0]
"In phonetic annotation, it is common to map each sound in a language back to this universal discrete alphabet.",9.2 Baselines,[0],[0]
"Under such an annotation scheme, it is easy to discern, cross-linguistically, which vowels originate from the same phoneme: an /I/ in German may be roughly equated with an /I/ in English.",9.2 Baselines,[0],[0]
"However, it is not clear how consistent this annotation truly is.",9.2 Baselines,[0],[0]
There are several reasons to expect high-variance in the cross-linguistic acoustic signal.,9.2 Baselines,[0],[0]
"First, IPA symbols are primarily useful for interlinked phonological distinctions, i.e., one applies the symbol /I/",9.2 Baselines,[0],[0]
"to distinguish it from /i/ in the given language, rather than to associate it with the sound bearing the same symbol in a second language.",9.2 Baselines,[0],[0]
"Second, field linguists often resort to the closest common IPA symbol, rather than an exact match: if a language makes no distinction between /i/ and /I/, it is more common to denote the sound with a /i/.",9.2 Baselines,[0],[0]
"Thus, IPA may not be as universal as hoped.",9.2 Baselines,[0],[0]
Our dataset contains 50 IPA symbols so this baseline is only reported for N = 50.,9.2 Baselines,[0],[0]
Evaluation in our setting is tricky.,9.3 Evaluation,[0],[0]
"The scientific goal of our work is to place a bit of linguistic theory on a firm probabilistic footing, rather than a downstream engineering-task, whose performance we could measure.",9.3 Evaluation,[0],[0]
"We consider three metrics.
",9.3 Evaluation,[0],[0]
Cross-Entropy.,9.3 Evaluation,[0],[0]
"Our first evaluation metric is cross-entropy: the average negative log-probability of the vowel systems in held-out test data, given the universal inventory ofN phones that we trained through EM.",9.3 Evaluation,[0],[0]
"We find this to be the cleanest method for scientific evaluation—it is the metric of optimization and has a clear interpretation: how surprised was the model to see the vowel systems of held-out, but attested, languages?",9.3 Evaluation,[0],[0]
The cross-entropy is the negative log of the∏[ · · · ] expression in eq.,9.3 Evaluation,[0],[0]
"(2), with ` now rang-
ing over held-out languages.3 Wallach et al. (2009) give several methods for estimating the intractable sum in language `.",9.3 Evaluation,[0],[0]
"We use the simple harmonic mean estimator, based on 50 samples of a` drawn with our Gibbs sampler (warm-started from the final E-step of training).
",9.3 Evaluation,[0],[0]
Cloze Evaluation.,9.3 Evaluation,[0],[0]
"In addition, following Cotterell and Eisner (2017), we evaluate our trained model’s ability to perform a cloze task (Taylor, 1953).",9.3 Evaluation,[0],[0]
"Given n`−1 or n`−2 of the vowels in heldout language `, can we predict the pronunciations vk of the remaining 1 or 2?",9.3 Evaluation,[0],[0]
We predict vk to be νθ(µi) where i = a ` k is the phone inferred by the sampler.,9.3 Evaluation,[0],[0]
Note that the sampler’s inference here is based only on the observed vowels (the likelihood) and the focalization-dispersion preferences of the DPP (the prior).,9.3 Evaluation,[0],[0]
"We report the expected error of such a prediction—where error is quantified by Euclidean distance in (F1, F2) formant space—over the same 50 samples of a`.
",9.3 Evaluation,[0],[0]
"For instance, consider a previously unseen vowel system with formant values {(499, 2199), (861, 1420), (571, 1079)}.",9.3 Evaluation,[0],[0]
"A “cloze1” evaluation would aim to predict {(499, 2199)} as the missing
3Since that expression is the product of both probability distributions and probability densities, our “cross-entropy” metric is actually the sum of both entropy terms and (potentially negative) differential entropy terms.",9.3 Evaluation,[0],[0]
"Thus, a value of 0 has no special significance.
vowel, given {(861, 1420), (571, 1079)}, and the fact that n` = 3.",9.3 Evaluation,[0],[0]
A “cloze12” evaluation would aim to predict two missing vowels.,9.3 Evaluation,[0],[0]
"Here, we report experimental details and the hyperparameters that we use to achieve the results reported.",9.4 Experimental Details,[0],[0]
We consider a neural network νθ with k ∈,9.4 Experimental Details,[0],[0]
"[1, 4] layers and find k = 1 the best performer on development data.",9.4 Experimental Details,[0],[0]
"Recall that our diffeomorphism constraint requires that each layer have exactly two hidden units, the same as the number of observed formants.",9.4 Experimental Details,[0],[0]
"We consider N ∈ {15, 25, 50, 100} phones as well as letting N fluctuate with reversible-jump MCMC (see footnote 1).",9.4 Experimental Details,[0],[0]
"We train for 100 iterations of EM, taking S = 5 samples at each E-step.",9.4 Experimental Details,[0],[0]
"At each M-step, we run 50 iterations of SGD for the focalization NN and also for the diffeomorphism NN.",9.4 Experimental Details,[0],[0]
"For each N , we selected (σ2, ρ) by minimizing cross-entropy on a held-out development set.",9.4 Experimental Details,[0],[0]
"We considered (σ2, ρ) ∈ {10k}5k=1 × {ρk}5k=1.",9.4 Experimental Details,[0],[0]
We report results in Tab. 1.,9.5 Results and Error Analysis,[0],[0]
We find that our DPP model improves over the baselines.,9.5 Results and Error Analysis,[0],[0]
The results support two claims: (i) dispersion plays an important role in the structure of vowel systems and (ii) learning a non-linear transformation of a Gaussian improves our ability to model sets of formant-pairs.,9.5 Results and Error Analysis,[0],[0]
"Also, we observe that as we increase the number of phones, the role of the DPP becomes more important.",9.5 Results and Error Analysis,[0],[0]
"We visualize a sample of the trained alignment in Fig. 3.
",9.5 Results and Error Analysis,[0],[0]
Frequency Encodes Dispersion.,9.5 Results and Error Analysis,[0],[0]
Why does dispersion not always help?,9.5 Results and Error Analysis,[0],[0]
The models with fewer phones do not reap the benefits that the models with more phones do.,9.5 Results and Error Analysis,[0],[0]
The reason lies in the fact that the most common vowel formants are already dispersed.,9.5 Results and Error Analysis,[0],[0]
"This indicates that we still have not quite modeled the mechanisms that select for good vowel formants, despite our work at the phonetic level; further research is needed.",9.5 Results and Error Analysis,[0],[0]
"We would prefer a model that explains the evolutionary motivation of sound systems as communication systems.
",9.5 Results and Error Analysis,[0],[0]
Number of Induced Phones.,9.5 Results and Error Analysis,[0],[0]
What is most salient in the number of induced phones is that it is close to the number of IPA phonemes in the data.,9.5 Results and Error Analysis,[0],[0]
"However, the performance of the phonemesupervised system is much worse, indicating that, perhaps, while the linguists have the right idea about the number of universal symbols, they did not specify the correct IPA symbol in all cases.",9.5 Results and Error Analysis,[0],[0]
Our data analysis indicates that this is often due to pragmatic concerns in linguistic field analysis.,9.5 Results and Error Analysis,[0],[0]
"For example, even if /I/ is the proper IPA symbol for the sound, if there is no other sound in the vicinity the annotator may prefer to use more common /i/.",9.5 Results and Error Analysis,[0],[0]
"Most closely related to our work is the classic study of Liljencrants and Lindblom (1972), who provide a simulation-based account of vowel systems.",10 Related Work,[0],[0]
"They argued that minima of a certain objective that encodes dispersion should correspond to canonical vowel systems of a given size n. Our tack is different in that we construct a generative probability model, whose parameters we learn from data.",10 Related Work,[0],[0]
"However, the essence of modeling is the same in that we explain formant values, rather than discrete IPA symbols.",10 Related Work,[0],[0]
"By extension, our work is also closely related to extensions of this theory (Schwartz et al., 1997; Roark, 2001) that focused on incorporating the notion of focalization into the experiments.
",10 Related Work,[0],[0]
"Our present paper can also be regarded as a continuation of Cotterell and Eisner (2017), in which we used DPPs to model vowel inventories as sets of discrete IPA symbols.",10 Related Work,[0],[0]
"That paper pretended that each IPA symbol had a single cross-linguistic (F1, F2) pair, an idealization that we remove in this paper by discarding the IPA symbols and modeling formant values directly.",10 Related Work,[0],[0]
Our model combines existing techniques of probabilistic modeling and inference to attempt to fit the actual distribution of the world’s vowel systems.,11 Conclusion,[0],[0]
"We presented a generative probability model of sets of measured (F1, F2) pairs.",11 Conclusion,[0],[0]
We view this as a necessary step in the development of generative probability models that can explain the distribution of the world’s languages.,11 Conclusion,[0],[0]
"Previous work on generating vowel inventories has focused on how those inventories were transcribed into IPA by field linguists, whereas we focus on the field linguists’ acoustic measurements of how the vowels are actually pronounced.",11 Conclusion,[0],[0]
"We would like to acknowledge Tim Vieira, Katharina Kann, Sebastian Mielke and Chu-Cheng Lin for reading many early drafts.",Acknowledgments,[0],[0]
The first author would like to acknowledge an NDSEG grant and a Facebook PhD fellowship.,Acknowledgments,[0],[0]
This material is also based upon work supported by the National Science Foundation under Grant No. 1718846 to the last author.,Acknowledgments,[0],[0]
What makes some types of languages more probable than others?,abstractText,[0],[0]
"For instance, we know that almost all spoken languages contain the vowel phoneme /i/; why should that be?",abstractText,[0],[0]
"The field of linguistic typology seeks to answer these questions and, thereby, divine the mechanisms that underlie human language.",abstractText,[0],[0]
"In our work, we tackle the problem of vowel system typology, i.e., we propose a generative probability model of which vowels a language contains.",abstractText,[0],[0]
"In contrast to previous work, we work directly with the acoustic information—the first two formant values—rather than modeling discrete sets of phonemic symbols (IPA).",abstractText,[0],[0]
We develop a novel generative probability model and report results based on a corpus of 233 languages.,abstractText,[0],[0]
A Deep Generative Model of Vowel Formant Typology,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 900–904 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
900",text,[0],[0]
Artificial neural networks (ANN) and especially Deep Neural Networks (DNN) give state-of-the art results for sentence classification tasks.,1 Introduction,[0],[0]
"Usually, sentences are treated as separate instances for the task.",1 Introduction,[0],[0]
"However, in many situations the sentence that is the focus of classification appears in a context that can provide additional information.",1 Introduction,[0],[0]
"For example, in the below sentences from the IEMOCAP dataset, it is difficult to classify M02 as showing excitement, without the prior context: • M01: I got it.",1 Introduction,[0],[0]
I got accepted to U.S.C..,1 Introduction,[0],[0]
•,1 Introduction,[0],[0]
F01:,1 Introduction,[0],[0]
"Oh, for real?",1 Introduction,[0],[0]
• M02:,1 Introduction,[0],[0]
Yes!,1 Introduction,[0],[0]
I just found out today.,1 Introduction,[0],[0]
I just got the letter.,1 Introduction,[0],[0]
"Our work is motivated by sentence classification in the text of medical records, in which complex judgements may be made across several sentences, each adding weight and nuance to a point.",1 Introduction,[0],[0]
"We believe, however, that the techniqe is more widely applicable.",1 Introduction,[0],[0]
"In order to test generalisability and to allow reproducibility, we therefore present an evaluation of the method with publicy available, non-medical corpora.
",1 Introduction,[0],[0]
"Previous work on using context for sentence classification used LSTM and CNN network layers to encode the surrounding context, giving an improvement in classification accuracy (Lee and Dernoncourt, 2016).",1 Introduction,[0],[0]
"However, the use of CNN and LSTM layers imposes a significant computational cost when training the network, especially if the size of the context is large.",1 Introduction,[0],[0]
"For this reason, the approach presented in (Lee and Dernoncourt, 2016) is explicitly intended for sequential, shorttext classification.
",1 Introduction,[0],[0]
"In many cases, however, the context available is of significant size.",1 Introduction,[0],[0]
"We therefore introduce a new method, Context-LSTM-CNN1, which is based on the computationally efficient FOFE (Fixed Size Ordinally Forgetting) method (Zhang et al., 2015), and an architecture that combines an LSTM and CNN for the focus sentence.",1 Introduction,[0],[0]
"The method consistently improves over results obtained from either LSTM alone, CNN alone, or these two combined, with little increase in training time.
",1 Introduction,[0],[0]
"This paper makes three contributions: 1) a demonstration of the importance of context in some sentence classification tasks; 2) an adaptation of existing datasets for such sentence classification tasks, in order to support reproducibility of evaluations; 3) a neural architecture for sentence classification that outperforms previous methods, and can include context of arbitrary size without incurring a large computational cost.",1 Introduction,[0],[0]
"Since their introduction (Collobert et al., 2011), CNNs with word embedding language models have become common for text classification tasks (Kim, 2014; Conneau et al., 2017).",2 Related work,[0],[0]
"One limitation of the original CNN approach is the loss
1The code is publicly available at https://github.com/deansong/contextLSTMCNN
of long distance dependencies.",2 Related work,[0],[0]
"In order to deal with this in image and speech recognition tasks, Xu et al. (2015); Sainath et al. (2015) combined CNNs with a Recurrent Neural Network (RNN) layer.",2 Related work,[0],[0]
Zhou et al. (2015) subsequently applied this to text classification.,2 Related work,[0],[0]
"However, the CNN-RNN approach was originally devised for sequence labelling, is biased towards later words in the sequence, and does not perform better than CNN alone.",2 Related work,[0],[0]
Huynh et al. (2016) suggested reversing the architecture to first apply the RNN followed by a CNN with pooling to obtain global features.,2 Related work,[0],[0]
"This gave results that improved over CNN-RNN, but not over CNN alone.",2 Related work,[0],[0]
"In this paper, we build on Huynh et al. (2016)’s approach by replacing the GRU-based RNN (Cho et al., 2014) with an LSTM (Hochreiter and Schmidhuber, 1997) and by using multiple kernel sizes and more features in the subsequent CNN layer.
",2 Related work,[0],[0]
"Lee and Dernoncourt (2016) showed that when classifying short texts, accuracy can be boosted by adding a CNN or LSTM derived vector representation of the surrounding context.",2 Related work,[0],[0]
"For long contexts (such as patient records which may include well over 100 sentences), however, this will incur a significant additional computational cost.",2 Related work,[0],[0]
"In this paper, we therefore apply an adaptation of the FOFE encoding (Zhang et al., 2015) to encode context.",2 Related work,[0],[0]
The Context-LSTM-CNN model is shown in Figure 1.,3 Model,[0],[0]
"It is based on the following components:
1.",3 Model,[0],[0]
Input layer using word embeddings to encode the words of the focus sentence.,3 Model,[0],[0]
2.,3 Model,[0],[0]
Bi-directional LSTM applied to the word embeddings of the focus sentence.,3 Model,[0],[0]
3.,3 Model,[0],[0]
CNN on the outputs of the LSTM.,3 Model,[0],[0]
4.,3 Model,[0],[0]
"FOFE applied to word embeddings of both
left and right context.",3 Model,[0],[0]
5.,3 Model,[0],[0]
"A final output layer.
",3 Model,[0],[0]
"In brief, an LSTM layer is used to encode the focus sentence.",3 Model,[0],[0]
This is followed by convolutional layers with small-size kernels and max-pooling to extract local features at specific points from the LSTM outputs.,3 Model,[0],[0]
"In addition to processing the focus sentence, we also encode the full left and right contexts using an adaptation of FOFE applied to our embeddings.",3 Model,[0],[0]
"This encodes any variable length context into a fixed length embedding, thus allowing us to include large contexts without rapidly in-
creasing the computational cost.",3 Model,[0],[0]
"The output of the FOFE layers are then each passed through separate fully connected layers, before being concatenated and connected to output layer.
",3 Model,[0],[0]
"In detail, the full network takes three inputs.",3 Model,[0],[0]
"The first is the sequence of words X = (x1, x2, ...xT ), where T is the length of the sentence to be classified, and where each xi is a word embedding for the respective word in this sentence.",3 Model,[0],[0]
"Embeddings are pre-trained by Word2Vec (Mikolov et al., 2013) on the corpus used for the respective experiment.",3 Model,[0],[0]
"The embeddings are not updated during the training of our network.
",3 Model,[0],[0]
"The second and third inputs are the left and right context, which will connect to the FOFE encoders.",3 Model,[0],[0]
"Each context is a sequence of sentences XC = (s1, s2, ...sN ), where each sentence is a sequence of word embeddings sn = (x1, x2, ...xU ) from the same embedding space as X .
",3 Model,[0],[0]
"The first component of the inputs, derived from the focus sentence, is processed by a bi-directional LSTM with one layer, in order to capture longdistance dependencies within the sentence.",3 Model,[0],[0]
"Since LSTMs impose a significant computational cost for very long sequences we only use this layer for the input representing the focus sentence, and not for the left and right contexts.
",3 Model,[0],[0]
"The LSTM generates outputs hlstm = (h1, h2..., hT ) which are passed on to the convolutional layer (CNN) in order to learn local features for different kernel sizes l from the history-aware outputs of the LSTM.",3 Model,[0],[0]
"For each of several kernel sizes, we generate f different features, to give CNN outputs clcnn = (c1, c2, · · · , cT−l+1).",3 Model,[0],[0]
"For each CNN output clcnn, we use max-overtime
pooling to extract the most significant feature, and dropout to make the learned features more robust.
",3 Model,[0],[0]
We use an adapted version of FOFE to provide information about the left and right contexts of the focus.,3 Model,[0],[0]
"Instead of the original 1 of k FOFE representation, we apply FOFE encoding to word2vec embeddings.",3 Model,[0],[0]
"This gives a weighted sum of the context word embeddings, with weights decreasing exponentially with distance from the focus.
",3 Model,[0],[0]
"The embedding z for a sentence (x1, x2, ...xU ) is initialised to z1 = x1, and then calculated recursively for u ∈ 2 · · ·U as zu = α · zu−1 + xu.",3 Model,[0],[0]
"The parameter α is the forgetting factor, which controls how fast the weights used for words farther away from the start of the sentence diminish.",3 Model,[0],[0]
"This method is fast and compactly encodes the words of a sentence in a single embedding vector.
",3 Model,[0],[0]
"For our use of FOFE, we encode all sentences in the document to left and right of the focus sentence, in two hierarchical steps.",3 Model,[0],[0]
"First we encode each context sentence into a FOFE embedding zsent, with a slowly-decreasing αsent.",3 Model,[0],[0]
"Following this, the left context FOFE encodings are themselves encoded into a single context embedding using a rapidly decreasing αcont.",3 Model,[0],[0]
This is calculated starting with zcont1 = z sent 1 and is calculated for m ∈ 2 · · · |Cleft| as zcontm = αcont · zcontm−1 + z sent m .,3 Model,[0],[0]
"The right context FOFE encodings are encoded in the same way, starting with zcont|Cright| = z sent |Cright| and recursively applying the same formula for m ∈ |Cright| · · · 2.",3 Model,[0],[0]
"This gives a heavy bias towards sentences more local to the focus sentence, but only slightly decreases the importance of words within each sentence.",3 Model,[0],[0]
"The final FOFE embeddings for the left and right contexts are then put through a dense linear layer to obtain the hidden layer outputs, which are combined with the LSTM-CNN outputs.",3 Model,[0],[0]
The concatenated outputs from the dense FOFE layers and from the CNN layer for all kernel sizes are then used as input to a final softmax output layer.,3 Model,[0],[0]
We compare the performance of four different network architectures: 1) CNN only; 2) LSTM only; 3) LSTM-CNN;,4 Experiments,[0],[0]
"4) LSTM context encoded LSTM-CNN (L-LSTM-CNN), in which the one left and right context sentence are encoded by LSTM; and 5) Context-LSTM-CNN (C-LSTMCNN).",4 Experiments,[0],[0]
"We use the following two datasets for evaluation:
Interactive Emotional Dyadic Motion Capture Database (Busso et al., 2008)2 (IEMOCAP).",4 Experiments,[0],[0]
"Originally created for the analysis of human emotions based on speech and video, a transcript of the speech component is available for NLP research.",4 Experiments,[0],[0]
Each sentence in the dialogue is annotated with one of 10 types of emotion.,4 Experiments,[0],[0]
"There is a class imbalance in the labelled data, and so we follow the approach of (Chernykh et al., 2017), and only use sentences classified with one of four labels (‘Anger’, ‘Excitement’, ‘Neutral’ and ‘Sadness’).",4 Experiments,[0],[0]
"For this dataset, instead of using left and right contexts, we assign all sentences from one person to one context and all sentences from the other person to the other context.",4 Experiments,[0],[0]
"While only the sentences with the four classes of interest are used for classification, all sentences of the dialog are used as the context.",4 Experiments,[0],[0]
"This results in a set of 4936 labelled sentences with average sentence length 14, and average document length is 986.
",4 Experiments,[0],[0]
"Drug-related Adverse Effects (Gurulingappa et al., 2012)3 (ADE).",4 Experiments,[0],[0]
This dataset contains sentences sampled from the abstracts of medical case reports.,4 Experiments,[0],[0]
"For each sentence, the annotation indicates whether adverse effects of a drug are being described (‘Positive’) or not (‘Negative’).",4 Experiments,[0],[0]
"The original release of the data does not contain the document context, which we reconstructed from PubMed4.",4 Experiments,[0],[0]
"Sentences for which the full abstract could not be found were removed, resulting in 20,040 labelled sentences, with average sentence length 21 and average document length 129.
",4 Experiments,[0],[0]
"In all experiments, five-fold cross validation was used for evaluation (for comparison with (Huynh et al., 2016)).",4 Experiments,[0],[0]
"For each fold, 50 epochs were run for training using a minibatch size of 64 for each fold, and the Adamax optimization algo-
2http://sail.usc.edu/iemocap/iemocap_",4 Experiments,[0],[0]
"release.htm
3https://sites.google.com/site/ adecorpus/home/document
4https://www.ncbi.nlm.nih.gov/pubmed/
rithm.",4 Experiments,[0],[0]
"To deal with label imbalance in the data, class weights wi for class i were set proportional to max(fi)/fi where fi is the frequency of class i.
We used word2vec embeddings with 50 dimensions (suggesed as sufficient by (Lai et al., 2016)).",4 Experiments,[0],[0]
"For the LSTM, 64 hidden units were used.",4 Experiments,[0],[0]
"For the CNN, layers for kernel sizes 2 to 6 were included in the network, and 64 features were used for each.",4 Experiments,[0],[0]
We examined the effect of the two context encoder hyperparameters: αcont (context level forgetting factor) and αw (sentence level forgetting factor) on classification performance over the IEMOCAP dataset.,4.1 Effect of Forgetting Factors,[0],[0]
We tested both in the range of 0.1 to 1 with an incremental step of 0.1.,4.1 Effect of Forgetting Factors,[0],[0]
Results are shown in Figure 2.,4.1 Effect of Forgetting Factors,[0],[0]
"Accuracy improves as αcont increases, but drops at αcont = 1, at which point all context sentence are given equal weight.",4.1 Effect of Forgetting Factors,[0],[0]
This may be because context closest to the focus sentence is more important than distant context.,4.1 Effect of Forgetting Factors,[0],[0]
"Therefore, we select αcont = 0.9 in all experiments.
",4.1 Effect of Forgetting Factors,[0],[0]
"For αsent, performance always increases as αsent increases, with best results at αsent = 1, at which point all words in the sentence contribute equally in the context code.",4.1 Effect of Forgetting Factors,[0],[0]
"This implies that for individual sentences in the context, it is more preferable to lose word order, than to down weight any individual word.",4.1 Effect of Forgetting Factors,[0],[0]
"In all experiments, we therefore set the sentence level forgetting fac-
tor to αsent = 1",4.1 Effect of Forgetting Factors,[0],[0]
"Table 1 shows the mean and standard deviations for accuracy over the cross validation folds, and training time, for both data sets.",4.2 Evaluation Results,[0],[0]
CNN alone performs better than LSTM alone in both tasks.,4.2 Evaluation Results,[0],[0]
The combined LSTM-CNN network consistently improves performance beyond both CNN alone and LSTM alone.,4.2 Evaluation Results,[0],[0]
"Both context based models (LLSTM-CNN and C-LSTM-CNN) perform better than non context based models, but note that LLSTM-CNN increases training time by approximately 1.5x, whereas C-LSTM-CNN shows only a marginal increase in training time, with a large increase in accuracy on the IEMOCAP corpus.
Table 2 shows the F1-measure for each class in the two datasets.",4.2 Evaluation Results,[0],[0]
"Again, Context-LSTM-CNN outperforms the other models on all classes for all data sets.",4.2 Evaluation Results,[0],[0]
"C-LSTM-CNN improves on average by 6.28 over L-LSTM-CNN, 10.16 over LSTMCNN, 11.4 over CNN and 13.29 over LSTM.
",4.2 Evaluation Results,[0],[0]
We conducted a t-test between L-LSTM-CNN and C-LSTM-CNN.,4.2 Evaluation Results,[0],[0]
"On IEMOCAP, C-LSTMCNN is significantly better than L-LSTM-CNN (p = 0.002).",4.2 Evaluation Results,[0],[0]
"On ADE, C-LSTM-CNN is not significantly better than L-LSTM-CNN (p = 0.128).",4.2 Evaluation Results,[0],[0]
This may because ADE sentences are less context dependent.,4.2 Evaluation Results,[0],[0]
"Alternatively, as the ADE task is relatively easy, with all models able to achieve about 90% accuracy, a context based approach might not be able to further improve the accuracy.",4.2 Evaluation Results,[0],[0]
"In this paper we introduced a new ANN model, Context-LSTM-CNN, that combines the strength of LSTM and CNN with the lightweight context encoding algorithm, FOFE.",5 Conclusion,[0],[0]
"Our model shows a consistent improvement over either a non-context based model and a LSTM context encoded model, for the sentence classification task.",5 Conclusion,[0],[0]
This work was partially supported by the European Union under grant agreement,Acknowledgements,[0],[0]
No. 654024 SoBigData.,Acknowledgements,[0],[0]
"In the sentence classification task, context formed from sentences adjacent to the sentence being classified can provide important information for classification.",abstractText,[0],[0]
"This context is, however, often ignored.",abstractText,[0],[0]
"Where methods do make use of context, only small amounts are considered, making it difficult to scale.",abstractText,[0],[0]
"We present a new method for sentence classification, Context-LSTM-CNN, that makes use of potentially large contexts.",abstractText,[0],[0]
"The method also utilizes long-range dependencies within the sentence being classified, using an LSTM, and short-span features, using a stacked CNN.",abstractText,[0],[0]
Our experiments demonstrate that this approach consistently improves over previous methods on two different datasets.,abstractText,[0],[0]
A Deep Neural Network Sentence Level Classification Method with Context Information,title,[0],[0]
"Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1426–1436, Baltimore, Maryland, USA, June 23-25 2014. c©2014 Association for Computational Linguistics
A Discriminative Graph-Based Parser for the Abstract Meaning Representation
Jeffrey Flanigan Sam Thomson Jaime Carbonell Chris Dyer Noah A. Smith Language Technologies Institute
Carnegie Mellon University Pittsburgh, PA 15213, USA
{jflanigan,sthomson,jgc,cdyer,nasmith}@cs.cmu.edu
Abstract
Abstract Meaning Representation (AMR) is a semantic formalism for which a growing set of annotated examples is available. We introduce the first approach to parse sentences into this representation, providing a strong baseline for future improvement. The method is based on a novel algorithm for finding a maximum spanning, connected subgraph, embedded within a Lagrangian relaxation of an optimization problem that imposes linguistically inspired constraints. Our approach is described in the general framework of structured prediction, allowing future incorporation of additional features and constraints, and may extend to other formalisms as well. Our open-source system, JAMR, is available at: http://github.com/jflanigan/jamr",text,[0],[0]
Semantic parsing is the problem of mapping natural language strings into meaning representations.,1 Introduction,[0],[0]
"Abstract Meaning Representation (AMR) (Banarescu et al., 2013; Dorr et al., 1998) is a semantic formalism in which the meaning of a sentence is encoded as a rooted, directed, acyclic graph.",1 Introduction,[1.0],"['Abstract Meaning Representation (AMR) (Banarescu et al., 2013; Dorr et al., 1998) is a semantic formalism in which the meaning of a sentence is encoded as a rooted, directed, acyclic graph.']"
"Nodes represent concepts, and labeled directed edges represent the relationships between them–see Figure 1 for an example AMR graph.",1 Introduction,[0],[0]
"The formalism is based on propositional logic and neo-Davidsonian event representations (Parsons, 1990; Davidson, 1967).",1 Introduction,[0],[0]
"Although it does not encode quantifiers, tense, or modality, the set of semantic phenomena included in AMR were selected with natural language applications—in particular, machine translation—in mind.
",1 Introduction,[0],[0]
"In this paper we introduce JAMR, the first published system for automatic AMR parsing.",1 Introduction,[0],[0]
"The
system is based on a statistical model whose parameters are trained discriminatively using annotated sentences in the AMR Bank corpus (Banarescu et al., 2013).",1 Introduction,[0],[0]
"We evaluate using the Smatch score (Cai and Knight, 2013), establishing a baseline for future work.
",1 Introduction,[0],[0]
"The core of JAMR is a two-part algorithm that first identifies concepts using a semi-Markov model and then identifies the relations that obtain between these by searching for the maximum spanning connected subgraph (MSCG) from an edge-labeled, directed graph representing all possible relations between the identified concepts.",1 Introduction,[0],[0]
"To solve the latter problem, we introduce an apparently novel O(|V |2 log |V |) algorithm that is similar to the maximum spanning tree (MST) algorithms that are widely used for dependency parsing (McDonald et al., 2005).",1 Introduction,[0],[0]
Our MSCG algorithm returns the connected subgraph with maximal sum of its edge weights from among all connected subgraphs of the input graph.,1 Introduction,[0],[0]
"Since AMR imposes additional constraints to ensure semantic well-formedness, we use Lagrangian relaxation (Geoffrion, 1974; Fisher, 2004) to augment the MSCG algorithm, yielding a tractable iterative algorithm that finds the optimal solution subject to these constraints.",1 Introduction,[0],[0]
"In our experiments, we have found this algorithm to converge 100% of the time for the constraint set we use.
",1 Introduction,[0],[0]
"The approach can be understood as an alternative to parsing approaches using graph transducers such as (synchronous) hyperedge replacement grammars (Chiang et al., 2013; Jones et al., 2012; Drewes et al., 1997), in much the same way that spanning tree algorithms are an alternative to using shift-reduce and dynamic programming algorithms for dependency parsing.1 While a detailed
1To date, a graph transducer-based semantic parser has not been published, although the Bolinas toolkit (http://www.isi.edu/publications/ licensed-sw/bolinas/) contains much of the necessary infrastructure.
",1 Introduction,[0],[0]
"1426
comparison of these two approaches is beyond the scope of this paper, we emphasize that—as has been observed with dependency parsing—a diversity of approaches can shed light on complex problems such as semantic parsing.",1 Introduction,[0],[0]
"Our approach to AMR parsing represents an AMR parse as a graph G = 〈V,E〉; vertices and edges are given labels from sets LV and LE , respectively.",2 Notation and Overview,[0],[0]
G is constructed in two stages.,2 Notation and Overview,[0],[0]
"The first stage identifies the concepts evoked by words and phrases in an input sentence w = 〈w1, . . .",2 Notation and Overview,[0],[0]
", wn〉, each wi a member of vocabulary W .",2 Notation and Overview,[0],[0]
"The second stage connects the concepts by adding LE-labeled edges capturing the relations between concepts, and selects a root in G corresponding to the focus of the sentence w.
Concept identification (§3) involves segmenting w into contiguous spans and assigning to each span a graph fragment corresponding to a concept from a concept set denoted F (or to ∅ for words that evoke no concept).",2 Notation and Overview,[0],[0]
In §5 we describe how F is constructed.,2 Notation and Overview,[0],[0]
"In our formulation, spans are contiguous subsequences of w.",2 Notation and Overview,[0],[0]
"For example, the
words “New York City” can evoke the fragment represented by
(c / city :name (n / name
:op1 ""New"" :op2 ""York"" :op3 ""City""))))
",2 Notation and Overview,[0],[0]
"We use a sequence labeling algorithm to identify concepts.
",2 Notation and Overview,[0],[0]
The relation identification stage (§4) is similar to a graph-based dependency parser.,2 Notation and Overview,[1.0],['The relation identification stage (§4) is similar to a graph-based dependency parser.']
"Instead of finding the maximum-scoring tree over words, it finds the maximum-scoring connected subgraph that preserves concept fragments from the first stage, links each pair of vertices by at most one edge, and is deterministic2 with respect to a special set of edge labels L∗E ⊂ LE .",2 Notation and Overview,[0],[0]
"The set L∗E consists of the labels ARG0–ARG5, and does not include labels such as MOD or MANNER, for example.",2 Notation and Overview,[0],[0]
"Linguistically, the determinism constraint enforces that predicates have at most one semantic argument of each type; this is discussed in more detail in §4.
",2 Notation and Overview,[0],[0]
"To train the parser, spans of words must be labeled with the concept fragments they evoke.",2 Notation and Overview,[0],[0]
"Although AMR Bank does not label concepts with the words that evoke them, it is possible to build an automatic aligner (§5).",2 Notation and Overview,[0],[0]
The alignments are used to construct the concept lexicon and to train the concept identification and relation identification stages of the parser (§6).,2 Notation and Overview,[0],[0]
"Each stage is a discriminatively-trained linear structured predictor with rich features that make use of part-ofspeech tagging, named entity tagging, and dependency parsing.
",2 Notation and Overview,[0],[0]
"In §7, we evaluate the parser against goldstandard annotated sentences from the AMR Bank corpus (Banarescu et al., 2013) under the Smatch score (Cai and Knight, 2013), presenting the first published results on automatic AMR parsing.",2 Notation and Overview,[1.0],"['In §7, we evaluate the parser against goldstandard annotated sentences from the AMR Bank corpus (Banarescu et al., 2013) under the Smatch score (Cai and Knight, 2013), presenting the first published results on automatic AMR parsing.']"
"The concept identification stage maps spans of words in the input sentence w to concept graph fragments from F , or to the empty graph fragment ∅.",3 Concept Identification,[0],[0]
"These graph fragments often consist of just one labeled concept node, but in some cases they are larger graphs with multiple nodes and edges.3
2By this we mean that, at each node, there is at most one outgoing edge with that label type.
",3 Concept Identification,[0],[0]
"3About 20% of invoked concept fragments are multiconcept fragments.
",3 Concept Identification,[0],[0]
"Concept identification is illustrated in Figure 2 using our running example, “The boy wants to visit New York City.”
Let the concept lexicon be a mapping clex :",3 Concept Identification,[0],[0]
W ∗ → 2F that provides candidate graph fragments for sequences of words.,3 Concept Identification,[0],[0]
(The construction of F and clex is discussed below.),3 Concept Identification,[0],[0]
"Formally, a concept labeling is (i) a segmentation of w into contiguous spans represented by boundaries b, giving spans 〈wb0:b1 ,wb1:b2 , . .",3 Concept Identification,[0],[0]
.wbk−1,3 Concept Identification,[0],[0]
":bk〉, with b0 = 0",3 Concept Identification,[0],[0]
"and bk = n, and (ii) an assignment of each phrase wbi−1:bi to a concept graph fragment ci ∈ clex (wbi−1:bi) ∪ ∅.
",3 Concept Identification,[0],[0]
"Our approach scores a sequence of spans b and a sequence of concept graph fragments c, both of arbitrary length k, using the following locally decomposed, linearly parameterized function:
score(b, c;θ) = ∑k
i=1",3 Concept Identification,[0],[0]
"θ >f(wbi−1:bi , bi−1, bi, ci)
(1) where f is a feature vector representation of a span and one of its concept graph fragments in context.",3 Concept Identification,[0],[0]
"The features are:
• Fragment given words: Relative frequency estimates of the probability of a concept graph fragment given the sequence of words in the span.",3 Concept Identification,[0],[0]
This is calculated from the concept-word alignments in the training corpus (§5).,3 Concept Identification,[0],[0]
•,3 Concept Identification,[0],[0]
"Length of the matching span (number of to-
kens).",3 Concept Identification,[0],[0]
"• NER: 1 if the named entity tagger marked the
span as an entity, 0 otherwise.",3 Concept Identification,[0],[0]
"• Bias: 1 for any concept graph fragment from F
and 0 for ∅.",3 Concept Identification,[0],[0]
"Our approach finds the highest-scoring b and c using a dynamic programming algorithm: the zeroth-order case of inference under a semiMarkov model (Janssen and Limnios, 1999).",3 Concept Identification,[0],[0]
"Let S(i) denote the score of the best labeling of the first i words of the sentence, w0:i; it can be calculated using the recurrence:
S(0) = 0
S(i) = max",3 Concept Identification,[0],[0]
"j:0≤j<i,
c∈clex(wj:i)∪∅
{ S(j) + θ>f(wj:i, j, i, c) }
The best score will be S(n), and the best scoring concept labeling can be recovered using backpointers, as in typical implementations of the Viterbi algorithm.",3 Concept Identification,[0],[0]
"Runtime is O(n2).
clex is implemented as follows.",3 Concept Identification,[0],[0]
"When clex is called with a sequence of words, it looks up the sequence in a table that contains, for every word sequence that was labeled with a concept fragment in the training data, the set of concept fragments it was labeled with.",3 Concept Identification,[1.0],"['When clex is called with a sequence of words, it looks up the sequence in a table that contains, for every word sequence that was labeled with a concept fragment in the training data, the set of concept fragments it was labeled with.']"
clex also has a set of rules for generating concept fragments for named entities and time expressions.,3 Concept Identification,[0],[0]
"It generates a concept fragment for any entity recognized by the named entity tagger, as well as for any word sequence matching a regular expression for a time expression.",3 Concept Identification,[0],[0]
clex returns the union of all these concept fragments.,3 Concept Identification,[0],[0]
"The relation identification stage adds edges among the concept subgraph fragments identified in the first stage (§3), creating a graph.",4 Relation Identification,[1.0],"['The relation identification stage adds edges among the concept subgraph fragments identified in the first stage (§3), creating a graph.']"
"We frame the task as a constrained combinatorial optimization problem.
",4 Relation Identification,[0],[0]
"Consider the fully dense labeled multigraph D = 〈VD, ED〉 that includes the union of all labeled vertices and labeled edges in the concept graph fragments, as well as every possible labeled edge u `−→ v, for all u, v ∈ VD and every ` ∈ LE .4
We require a subgraph G = 〈VG, EG〉 that respects the following constraints:
1.",4 Relation Identification,[0.999999983235248],"['Consider the fully dense labeled multigraph D = 〈VD, ED〉 that includes the union of all labeled vertices and labeled edges in the concept graph fragments, as well as every possible labeled edge u `−→ v, for all u, v ∈ VD and every ` ∈ LE .4 We require a subgraph G = 〈VG, EG〉 that respects the following constraints: 1.']"
"Preserving: all graph fragments (including labels) from the concept identification phase are subgraphs of G.
2.",4 Relation Identification,[0],[0]
"Simple: for any two vertices u and v ∈ VG,EG includes at most one edge between u and v. This constraint forbids a small number of perfectly valid graphs, for example for sentences such as “John hurt himself”; however, we see that< 1% of training instances violate the constraint.",4 Relation Identification,[0],[0]
"We found in preliminary experiments that including the constraint increases overall performance.5
3.",4 Relation Identification,[0],[0]
"Connected: G must be weakly connected (every vertex reachable from every other vertex, ignoring the direction of edges).",4 Relation Identification,[0],[0]
"This constraint follows from the formal definition of AMR and is never violated in the training data.
4.",4 Relation Identification,[0],[0]
"Deterministic: For each node u ∈ VG, and for each label ` ∈ L∗E , there is at most one outgoing edge in EG from u with label `.",4 Relation Identification,[1.0],"['Deterministic: For each node u ∈ VG, and for each label ` ∈ L∗E , there is at most one outgoing edge in EG from u with label `.']"
"As discussed in §2, this constraint is linguistically motivated.",4 Relation Identification,[0],[0]
"4To handle numbered OP labels, we pre",4 Relation Identification,[0],[0]
"-process the training data to convert OPN to OP, and post-process the output by numbering the OP labels sequentially.
",4 Relation Identification,[0],[0]
"5In future work it might be treated as a soft constraint, or the constraint might be refined to specific cases.
",4 Relation Identification,[0],[0]
"One constraint we do not include is acyclicity, which follows from the definition of AMR.",4 Relation Identification,[0],[0]
"In practice, graphs with cycles are rarely produced by JAMR.",4 Relation Identification,[0],[0]
"In fact, none of the graphs produced on the test set violate acyclicity.
",4 Relation Identification,[0],[0]
"Given the constraints, we seek the maximumscoring subgraph.",4 Relation Identification,[0],[0]
"We define the score to decompose by edges, and with a linear parameterization:
score(EG;ψ) = ∑ e∈EG ψ >g(e) (2)
The features are shown in Table 1.",4 Relation Identification,[0],[0]
"Our solution to maximizing the score in Eq. 2, subject to the constraints, makes use of (i) an algorithm that ignores constraint 4 but respects the others (§4.1); and (ii) a Lagrangian relaxation that iteratively adjusts the edge scores supplied to (i) so as to enforce constraint 4 (§4.2).",4 Relation Identification,[0],[0]
"The steps for constructing a maximum preserving, simple, spanning, connected (but not necessarily deterministic) subgraph are as follows.","4.1 Maximum Preserving, Simple, Spanning, Connected Subgraph Algorithm",[1.0],"['The steps for constructing a maximum preserving, simple, spanning, connected (but not necessarily deterministic) subgraph are as follows.']"
"These steps ensure the resulting graph G satisfies the constraints: the initialization step ensures the preserving constraint is satisfied, the pre-processing step ensures the graph is simple, and the core algorithm ensures the graph is connected.
1.","4.1 Maximum Preserving, Simple, Spanning, Connected Subgraph Algorithm",[0.9966048349887608],"['These steps ensure the resulting graph G satisfies the constraints: the initialization step ensures the preserving constraint is satisfied, the pre-processing step ensures the graph is simple, and the core algorithm ensures the graph is connected.']"
"(Initialization) Let E(0) be the union of the concept graph fragments’ weighted, labeled, directed edges.","4.1 Maximum Preserving, Simple, Spanning, Connected Subgraph Algorithm",[0],[0]
Let V denote its set of vertices.,"4.1 Maximum Preserving, Simple, Spanning, Connected Subgraph Algorithm",[0],[0]
"Note that 〈V,E(0)〉 is preserving (constraint 4), as is any graph that contains it.","4.1 Maximum Preserving, Simple, Spanning, Connected Subgraph Algorithm",[0],[0]
"It is also simple (constraint 4), assuming each concept graph fragment is simple.
","4.1 Maximum Preserving, Simple, Spanning, Connected Subgraph Algorithm",[0],[0]
2.,"4.1 Maximum Preserving, Simple, Spanning, Connected Subgraph Algorithm",[0],[0]
(Pre-processing),"4.1 Maximum Preserving, Simple, Spanning, Connected Subgraph Algorithm",[0],[0]
"We form the edge set E by including just one edge from ED between each pair of nodes:
• For any edge e = u `−→ v in E(0), include e in E, omitting all other edges between u and v.
• For any two nodes u and v, include only the highest scoring edge between u and v.
Note that without the deterministic constraint, we have no constraints that depend on the label of an edge, nor its direction.","4.1 Maximum Preserving, Simple, Spanning, Connected Subgraph Algorithm",[0.9947495829819569],"['(Pre-processing) We form the edge set E by including just one edge from ED between each pair of nodes: • For any edge e = u `−→ v in E(0), include e in E, omitting all other edges between u and v. • For any two nodes u and v, include only the highest scoring edge between u and v. Note that without the deterministic constraint, we have no constraints that depend on the label of an edge, nor its direction.']"
"So it is clear that the edges omitted in this step could not be part of the maximum-scoring solution, as they could be replaced by a higher scoring edge without violating any constraints.
","4.1 Maximum Preserving, Simple, Spanning, Connected Subgraph Algorithm",[0.9999999893383665],"['So it is clear that the edges omitted in this step could not be part of the maximum-scoring solution, as they could be replaced by a higher scoring edge without violating any constraints.']"
"Note also that because we have kept exactly one edge between every pair of nodes, 〈V,E〉 is simple and connected.
3.","4.1 Maximum Preserving, Simple, Spanning, Connected Subgraph Algorithm",[0],[0]
(Core algorithm),"4.1 Maximum Preserving, Simple, Spanning, Connected Subgraph Algorithm",[0],[0]
"Run Algorithm 1, MSCG, on 〈V,E〉 and E(0).","4.1 Maximum Preserving, Simple, Spanning, Connected Subgraph Algorithm",[0],[0]
This algorithm is a (to our knowledge novel) modification of the minimum spanning tree algorithm of Kruskal (1956).,"4.1 Maximum Preserving, Simple, Spanning, Connected Subgraph Algorithm",[0],[0]
"Note that the directions of edges do not matter for MSCG.
","4.1 Maximum Preserving, Simple, Spanning, Connected Subgraph Algorithm",[0],[0]
"Steps 1–2 can be accomplished in one pass through the edges, with runtime O(|V |2).","4.1 Maximum Preserving, Simple, Spanning, Connected Subgraph Algorithm",[0],[0]
"MSCG can be implemented efficiently in O(|V |2 log |V |) time, similarly to Kruskal’s algorithm, using a disjoint-set data structure to keep track of connected components.6","4.1 Maximum Preserving, Simple, Spanning, Connected Subgraph Algorithm",[0],[0]
"The total asymptotic runtime complexity is O(|V |2 log |V |).
","4.1 Maximum Preserving, Simple, Spanning, Connected Subgraph Algorithm",[0],[0]
The details of MSCG are given in Algorithm 1.,"4.1 Maximum Preserving, Simple, Spanning, Connected Subgraph Algorithm",[0],[0]
"In a nutshell, MSCG first adds all positive edges to the graph, and then connects the graph by greedily adding the least negative edge that connects two previously unconnected components.","4.1 Maximum Preserving, Simple, Spanning, Connected Subgraph Algorithm",[1.0],"['In a nutshell, MSCG first adds all positive edges to the graph, and then connects the graph by greedily adding the least negative edge that connects two previously unconnected components.']"
Theorem 1.,"4.1 Maximum Preserving, Simple, Spanning, Connected Subgraph Algorithm",[0],[0]
"MSCG finds a maximum spanning, connected subgraph of 〈V,E〉 Proof.","4.1 Maximum Preserving, Simple, Spanning, Connected Subgraph Algorithm",[1.0],"['MSCG finds a maximum spanning, connected subgraph of 〈V,E〉 Proof.']"
We closely follow the original proof of correctness of Kruskal’s algorithm.,"4.1 Maximum Preserving, Simple, Spanning, Connected Subgraph Algorithm",[0],[0]
"We first show by induction that, at every iteration of MSCG, there exists some maximum spanning, connected subgraph that contains G(i) = 〈V,E(i)〉:
6For dense graphs, Prim’s algorithm (Prim, 1957) is asymptotically faster (O(|V |2)).","4.1 Maximum Preserving, Simple, Spanning, Connected Subgraph Algorithm",[0],[0]
"We conjecture that using Prim’s algorithm instead of Kruskall’s to connect the graph could improve the runtime of MSCG.
input : weighted, connected graph 〈V,E〉 and set of edges E(0) ⊆ E to be preserved output: maximum spanning, connected subgraph of 〈V,E〉 that preserves E(0) let E(1) = E(0) ∪ {e ∈ E | ψ>g(e)","4.1 Maximum Preserving, Simple, Spanning, Connected Subgraph Algorithm",[0],[0]
"> 0}; create a priority queue Q containing {e ∈ E | ψ>g(e) ≤ 0} prioritized by scores; i = 1; while Q nonempty and 〈V,E(i)〉 is not yet spanning and connected do
i = i+ 1; E(i) = E(i−1); e = arg maxe′∈Qψ>g(e′); remove e from Q; if e connects two previously unconnected components of 〈V,E(i)〉 then
add e to E(i)
end end return G = 〈V,E(i)〉;
Algorithm 1: MSCG algorithm.
","4.1 Maximum Preserving, Simple, Spanning, Connected Subgraph Algorithm",[0],[0]
"Base case: ConsiderG(1), the subgraph containing E(0) and every positive edge.","4.1 Maximum Preserving, Simple, Spanning, Connected Subgraph Algorithm",[0],[0]
"Take any maximum preserving spanning connected subgraph M of 〈V,E〉.","4.1 Maximum Preserving, Simple, Spanning, Connected Subgraph Algorithm",[0],[0]
"We know that such an M exists because 〈V,E〉 itself is a preserving spanning connected subgraph.","4.1 Maximum Preserving, Simple, Spanning, Connected Subgraph Algorithm",[0],[0]
"Adding a positive edge to M would strictly increase M ’s score without disconnecting M , which would contradict the fact that M is maximal.","4.1 Maximum Preserving, Simple, Spanning, Connected Subgraph Algorithm",[0],[0]
"Thus M must contain G(1).
","4.1 Maximum Preserving, Simple, Spanning, Connected Subgraph Algorithm",[0],[0]
"Induction step: By the inductive hypothesis, there exists some maximum spanning connected
subgraph M = 〈V,EM 〉 that contains G(i).","4.1 Maximum Preserving, Simple, Spanning, Connected Subgraph Algorithm",[0],[0]
Let e be the next edge added to E(i) by MSCG.,"4.1 Maximum Preserving, Simple, Spanning, Connected Subgraph Algorithm",[0],[0]
"If e is in EM , then E(i+1) = E(i) ∪ {e} ⊆ EM , and the hypothesis still holds.","4.1 Maximum Preserving, Simple, Spanning, Connected Subgraph Algorithm",[0],[0]
"Otherwise, since M is connected and does not contain e, EM ∪ {e} must have a cycle containing e.","4.1 Maximum Preserving, Simple, Spanning, Connected Subgraph Algorithm",[0],[0]
"In addition, that cycle must have some edge e′","4.1 Maximum Preserving, Simple, Spanning, Connected Subgraph Algorithm",[0],[0]
that is not in E(i).,"4.1 Maximum Preserving, Simple, Spanning, Connected Subgraph Algorithm",[0],[0]
"Otherwise, E(i) ∪ {e} would contain a cycle, and e would not connect two unconnected components of G(i), contradicting the fact that e was chosen by MSCG.
","4.1 Maximum Preserving, Simple, Spanning, Connected Subgraph Algorithm",[0],[0]
"Since e′ is in a cycle in EM ∪ {e}, removing it will not disconnect the subgraph, i.e. (EM∪{e})\ {e′} is still connected and spanning.","4.1 Maximum Preserving, Simple, Spanning, Connected Subgraph Algorithm",[0],[0]
"The score of e is greater than or equal to the score of e′, otherwise MSCG would have chosen e′ instead of","4.1 Maximum Preserving, Simple, Spanning, Connected Subgraph Algorithm",[0],[0]
"e. Thus, 〈V, (EM ∪{e})","4.1 Maximum Preserving, Simple, Spanning, Connected Subgraph Algorithm",[0],[0]
"\ {e′}〉 is a maximum spanning connected subgraph that containsE(i+1), and the hypothesis still holds.
","4.1 Maximum Preserving, Simple, Spanning, Connected Subgraph Algorithm",[0],[0]
"When the algorithm completes, G = 〈V,E(i)〉 is a spanning connected subgraph.","4.1 Maximum Preserving, Simple, Spanning, Connected Subgraph Algorithm",[0],[0]
"The maximum spanning connected subgraph M that contains it cannot have a higher score, because G contains every positive edge.","4.1 Maximum Preserving, Simple, Spanning, Connected Subgraph Algorithm",[1.0],"['The maximum spanning connected subgraph M that contains it cannot have a higher score, because G contains every positive edge.']"
Hence G is maximal.,"4.1 Maximum Preserving, Simple, Spanning, Connected Subgraph Algorithm",[0],[0]
If the subgraph resulting from MSCG satisfies constraint 4 (deterministic) then we are done.,4.2 Lagrangian Relaxation,[1.0],['If the subgraph resulting from MSCG satisfies constraint 4 (deterministic) then we are done.']
Otherwise we resort to Lagrangian relaxation (LR).,4.2 Lagrangian Relaxation,[0],[0]
"Here we describe the technique as it applies to our task, referring the interested reader to Rush and Collins (2012) for a more general introduction to Lagrangian relaxation in the context of structured prediction problems.
",4.2 Lagrangian Relaxation,[0],[0]
"In our case, we begin by encoding a graph G = 〈VG, EG〉 as a binary vector.",4.2 Lagrangian Relaxation,[1.0],"['In our case, we begin by encoding a graph G = 〈VG, EG〉 as a binary vector.']"
"For each edge e in the fully dense multigraph D, we associate a bi-
nary variable ze = 1{e ∈ EG}, where 1{P} is the indicator function, taking value 1 if the proposition P is true, 0 otherwise.",4.2 Lagrangian Relaxation,[0],[0]
"The collection of ze form a vector z ∈ {0, 1}|ED|.
",4.2 Lagrangian Relaxation,[0],[0]
Determinism constraints can be encoded as a set of linear inequalities.,4.2 Lagrangian Relaxation,[0],[0]
"For example, the constraint that vertex u has no more than one outgoing ARG0 can be encoded with the inequality:∑ v∈V 1{u ARG0−−−→ v ∈ EG} = ∑ v∈V z u ARG0−−−→v ≤",4.2 Lagrangian Relaxation,[0],[0]
"1.
",4.2 Lagrangian Relaxation,[0],[0]
"All of the determinism constraints can collectively be encoded as one system of inequalities:
Az ≤ b, with each row Ai inA and its corresponding entry bi in b together encoding one constraint.",4.2 Lagrangian Relaxation,[1.0000000413186243],"['All of the determinism constraints can collectively be encoded as one system of inequalities: Az ≤ b, with each row Ai inA and its corresponding entry bi in b together encoding one constraint.']"
"For the previous example we have a row Ai that has 1s in the columns corresponding to edges outgoing from u with label ARG0 and 0’s elsewhere, and a corresponding element bi = 1 in b.
The score of graph G (encoded as z) can be written as the objective function φ>z, where φe = ψ>g(e).",4.2 Lagrangian Relaxation,[0],[0]
"To handle the constraint Az ≤ b, we introduce multipliers µ ≥ 0",4.2 Lagrangian Relaxation,[0],[0]
"to get the Lagrangian relaxation of the objective function:
Lµ(z) = maxz (φ>z + µ>(b−Az)), z∗µ = arg maxz Lµ(z).
",4.2 Lagrangian Relaxation,[0],[0]
"And the dual objective:
L(z) = min µ≥0 Lµ(z),
z∗ = arg maxz L(z).
",4.2 Lagrangian Relaxation,[0.9999999632485949],"['And the dual objective: L(z) = min µ≥0 Lµ(z), z∗ = arg maxz L(z).']"
"Conveniently, Lµ(z) decomposes over edges:
Lµ(z) = maxz (φ>z + µ>(b−Az))",4.2 Lagrangian Relaxation,[0],[0]
"= maxz (φ>z− µ>Az) = maxz ((φ−A>µ)>z).
",4.2 Lagrangian Relaxation,[0],[0]
"So for any µ, we can find z∗µ by assigning edges the new Lagrangian adjusted weights φ − A>µ and reapplying the algorithm described in §4.1.",4.2 Lagrangian Relaxation,[1.0],"['So for any µ, we can find z∗µ by assigning edges the new Lagrangian adjusted weights φ − A>µ and reapplying the algorithm described in §4.1.']"
"We can find z∗ by projected subgradient descent, by starting with µ = 0, and taking steps in the direction:
−∂Lµ ∂µ (z∗µ) =",4.2 Lagrangian Relaxation,[0],[0]
"Az ∗ µ.
",4.2 Lagrangian Relaxation,[0],[0]
"If any components of µ are negative after taking a step, they are set to zero.
",4.2 Lagrangian Relaxation,[0],[0]
"L(z) is an upper bound on the unrelaxed objective function φ>z, and is equal to it if and only if the constraints Az ≤ b are satisfied.",4.2 Lagrangian Relaxation,[1.0],"['L(z) is an upper bound on the unrelaxed objective function φ>z, and is equal to it if and only if the constraints Az ≤ b are satisfied.']"
"If L(z∗) = φ>z∗, then z∗ is also the optimal solution to the constrained solution.",4.2 Lagrangian Relaxation,[1.0],"['If L(z∗) = φ>z∗, then z∗ is also the optimal solution to the constrained solution.']"
"Otherwise, there exists a duality gap, and Lagrangian relaxation has failed.",4.2 Lagrangian Relaxation,[0],[0]
"In that case we still return the subgraph encoded by z∗, even though it might violate one or more constraints.",4.2 Lagrangian Relaxation,[1.0],"['In that case we still return the subgraph encoded by z∗, even though it might violate one or more constraints.']"
"Techniques from integer programming such as branch-and-bound or cutting-planes methods could be used to find an optimal solution when LR fails (Das et al., 2012), but we do not use these techniques here.",4.2 Lagrangian Relaxation,[0],[0]
"In our experiments, with a stepsize of 1 and max number of steps as 500, Lagrangian relaxation succeeds 100% of the time in our data.",4.2 Lagrangian Relaxation,[1.0],"['In our experiments, with a stepsize of 1 and max number of steps as 500, Lagrangian relaxation succeeds 100% of the time in our data.']"
"In AMR, one node must be marked as the focus of the sentence.",4.3 Focus Identification,[0],[0]
"We notice this can be accomplished within the relation identification step: we add a special concept node root to the dense graph D, and add an edge from root to every other node, giving each of these edges the label FOCUS.",4.3 Focus Identification,[0],[0]
We require that root have at most one outgoing FOCUS edge.,4.3 Focus Identification,[0],[0]
"Our system has two feature types for this edge: the concept it points to, and the shortest dependency path from a word in the span to the root of the dependency tree.",4.3 Focus Identification,[1.0],"['Our system has two feature types for this edge: the concept it points to, and the shortest dependency path from a word in the span to the root of the dependency tree.']"
"In order to train the parser, we need alignments between sentences in the training data and their annotated AMR graphs.",5 Automatic Alignments,[0],[0]
"More specifically, we need to know which spans of words invoke which concept fragments in the graph.",5 Automatic Alignments,[0],[0]
"To do this, we built an automatic aligner and tested its performance on a small set of alignments we annotated by hand.
",5 Automatic Alignments,[0],[0]
The automatic aligner uses a set of rules to greedily align concepts to spans.,5 Automatic Alignments,[0],[0]
The list of rules is given in Table 2.,5 Automatic Alignments,[0],[0]
"The aligner proceeds down the list, first aligning named-entities exactly, then fuzzy matching named-entities, then date-entities, etc.",5 Automatic Alignments,[0],[0]
"For each rule, an entire pass through the AMR graph is done.",5 Automatic Alignments,[0],[0]
The pass considers every concept in the graph and attempts to align a concept fragment rooted at that concept if the rule can apply.,5 Automatic Alignments,[0],[0]
"Some rules only apply to a particular type of concept fragment, while others can apply to any concept.",5 Automatic Alignments,[0],[0]
"For example, rule 1 can apply to any NAME concept and its OP children.",5 Automatic Alignments,[0],[0]
"It searches the sentence
for a sequence of words that exactly matches its OP children and aligns them to the NAME and OP children fragment.
",5 Automatic Alignments,[0],[0]
"Concepts are considered for alignment in the order they are listed in the AMR annotation (left to right, top to bottom).",5 Automatic Alignments,[0],[0]
Concepts that are not aligned in a particular pass may be aligned in subsequent passes.,5 Automatic Alignments,[0],[0]
"Concepts are aligned to the first matching span, and alignments are mutually exclusive.",5 Automatic Alignments,[0],[0]
"Once aligned, a concept in a fragment is never realigned.7",5 Automatic Alignments,[0],[0]
"However, more concepts can be attached to the fragment by rules 8–14.
",5 Automatic Alignments,[0],[0]
"We use WordNet to generate candidate lemmas, and we also use a fuzzy match of a concept, defined to be a word in the sentence that has the longest string prefix match with that concept’s label, if the match length is ≥ 4.",5 Automatic Alignments,[0],[0]
"If the match length is < 4, then the concept has no fuzzy match.",5 Automatic Alignments,[0],[0]
For example the fuzzy match for ACCUSE-01 could be “accusations” if it is the best match in the sentence.,5 Automatic Alignments,[0],[0]
WordNet lemmas and fuzzy matches are only used if the rule explicitly uses them.,5 Automatic Alignments,[0],[0]
"All tokens and concepts are lowercased before matches or fuzzy matches are done.
",5 Automatic Alignments,[0],[0]
"On the 200 sentences of training data we aligned by hand, the aligner achieves 92% precision, 89% recall, and 90% F1 for the alignments.",5 Automatic Alignments,[0],[0]
We now describe how to train the two stages of the parser.,6 Training,[0],[0]
"The training data for the concept identification stage consists of (X,Y ) pairs:
• Input: X , a sentence annotated with named entities (person, organization, location, misciscellaneous) from the Illinois Named Entity Tagger (Ratinov and Roth, 2009), and part-ofspeech tags and basic dependencies from the Stanford Parser (Klein and Manning, 2003; de Marneffe et al., 2006).",6 Training,[0],[0]
•,6 Training,[0],[0]
"Output: Y , the sentence labeled with concept
subgraph fragments.
",6 Training,[0],[0]
"The training data for the relation identification stage consists of (X,Y ) pairs:
7As an example, if “North Korea” shows up twice in the AMR graph and twice in the input sentence, then the first “North Korea” concept fragment listed in the AMR gets aligned to the first “North Korea” mention in the sentence, and the second fragment to the second mention (because the first span is already aligned when the second “North Korea” concept fragment is considered, so it is aligned to the second matching span).
",6 Training,[0],[0]
"• Input: X , the sentence labeled with graph fragments, as well as named enties, POS tags, and basic dependencies as in concept identification.",6 Training,[0],[0]
•,6 Training,[0],[0]
"Output: Y , the sentence with a full AMR
parse.8
Alignments are used to induce the concept labeling for the sentences, so no annotation beyond the automatic alignments is necessary.
",6 Training,[0],[0]
"We train the parameters of the stages separately using AdaGrad (Duchi et al., 2011) with the perceptron loss function (Rosenblatt, 1957; Collins, 2002).",6 Training,[0],[0]
"We give equations for concept identification parameters θ and features f(X,Y ).",6 Training,[0],[0]
"For a sentence of length k, and spans b labeled with a sequence of concept fragments c, the features are:
f(X,Y ) =",6 Training,[0],[0]
"∑k
i=1",6 Training,[0],[0]
f(wbi−1,6 Training,[0],[0]
":bi , bi−1, bi, ci)
To train with AdaGrad, we process examples in the training data ((X1, Y 1), . . .",6 Training,[0],[0]
", (XN , Y N ))",6 Training,[0],[0]
one at a time.,6 Training,[0],[0]
"At time t, we decode (§3) to get Ŷ t and compute the subgradient:
st = f(Xt, Ŷ t)− f(Xt, Y t)
",6 Training,[0],[0]
We then update the parameters and go to the next example.,6 Training,[0],[0]
"Each component i of the parameter vector gets updated like so:
θt+1i =",6 Training,[0],[0]
θ t i,6 Training,[0],[0]
"− η√∑t t′=1 s t′ i sti
η is the learning rate which we set to 1.",6 Training,[0],[0]
"For relation identification training, we replace θ and f(X,Y ) in the above equations with ψ and
g(X,Y ) =",6 Training,[0],[0]
"∑
e∈EG g(e).
",6 Training,[0],[0]
"We ran AdaGrad for ten iterations for concept identification, and five iterations for relation identification.",6 Training,[0],[0]
The number of iterations was chosen by early stopping on the development set.,6 Training,[0],[0]
We evaluate our parser on the newswire section of LDC2013E117 (deft-amr-release-r3-proxy.txt).,7 Experiments,[0],[0]
"Statistics about this corpus and our train/dev./test splits are given in Table 3.
",7 Experiments,[0],[0]
"8Because the alignments are automatic, some concepts may not be aligned, so we cannot compute their features.",7 Experiments,[0],[0]
We remove the unaligned concepts and their edges from the full AMR graph for training.,7 Experiments,[0],[0]
"Thus some graphs used for training may in fact be disconnected.
",7 Experiments,[0],[0]
"For the performance of concept identification, we report precision, recall, and F1 of labeled spans using the induced labels on the training and test data as a gold standard (Table 4).",7 Experiments,[0],[0]
Our concept identifier achieves 84% F1 on the test data.,7 Experiments,[0],[0]
"Precision is roughly the same between train and test, but recall is worse on test, implicating unseen concepts as a significant source of errors on test data.
",7 Experiments,[0],[0]
"We evaluate the performance of the full parser using Smatch v1.0 (Cai and Knight, 2013), which counts the precision, recall and F1 of the concepts and relations together.",7 Experiments,[0],[0]
"Using the full pipeline (concept identification and relation identification stages), our parser achieves 58% F1 on the test data (Table 5).",7 Experiments,[0],[0]
Using gold concepts with the relation identification stage yields a much higher Smatch score of 80% F1.,7 Experiments,[0],[0]
"As a comparison, AMR Bank annotators have a consensus inter-annotator agreement Smatch score of 83% F1.",7 Experiments,[0],[0]
"The runtime of our system is given in Figure 3.
",7 Experiments,[0],[0]
The large drop in performance of 22% F1 when moving from gold concepts to system concepts suggests that joint inference and training for the two stages might be helpful.,7 Experiments,[0],[0]
Our approach to relation identification is inspired by graph-based techniques for non-projective syntactic dependency parsing.,8 Related Work,[0],[0]
"Minimum spanning tree algorithms—specifically, the optimum branching algorithm of Chu and Liu (1965) and Edmonds (1967)—were first used for dependency parsing by McDonald et al. (2005).",8 Related Work,[0],[0]
"Later ex-
tensions allow for higher-order (non–edge-local) features, often making use of relaxations to solve the NP-hard optimization problem.",8 Related Work,[0],[0]
"Mcdonald and Pereira (2006) incorporated second-order features, but resorted to an approximate algorithm.",8 Related Work,[0],[0]
"Others have formulated the problem as an integer linear program (Riedel and Clarke, 2006; Martins et al., 2009).",8 Related Work,[0],[0]
"TurboParser (Martins et al., 2013) uses AD3 (Martins et al., 2011), a type of augmented Lagrangian relaxation, to integrate third-order features into a CLE backbone.",8 Related Work,[0],[0]
"Future work might extend JAMR to incorporate additional linguistically motivated constraints and higher-order features.
",8 Related Work,[0],[0]
"The task of concept identification is similar in form to the problem of Chinese word segmentation, for which semi-Markov models have successfully been used to incorporate features based on entire spans (Andrew, 2006).
",8 Related Work,[0],[0]
"While all semantic parsers aim to transform natural language text to a formal representation of its meaning, there is wide variation in the meaning representations and parsing techniques used.",8 Related Work,[0],[0]
"Space does not permit a complete survey, but we note some connections on both fronts.
",8 Related Work,[0],[0]
"Interlinguas (Carbonell et al., 1992) are an important precursor to AMR.",8 Related Work,[0],[0]
"Both formalisms are intended for use in machine translation, but AMR has an admitted bias toward the English language.
",8 Related Work,[0],[0]
"First-order logic representations (and extensions using, e.g., the λ-calculus) allow variable quantification, and are therefore more powerful.",8 Related Work,[0],[0]
"In recent research, they are often associated with combinatory categorial grammar (Steedman, 1996).",8 Related Work,[0],[0]
"There has been much work on statistical models for CCG parsing (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010, inter alia), usually using
chart-based dynamic programming for inference.",8 Related Work,[0],[0]
"Natural language interfaces for querying databases have served as another driving application (Zelle and Mooney, 1996; Kate et al., 2005; Liang et al., 2011, inter alia).",8 Related Work,[0],[0]
"The formalisms used here are richer in logical expressiveness than AMR, but typically use a smaller set of concept types—only those found in the database.
",8 Related Work,[0],[0]
"In contrast, semantic dependency parsing—in which the vertices in the graph correspond to the words in the sentence—is meant to make semantic parsing feasible for broader textual domains.",8 Related Work,[0],[0]
Alshawi,8 Related Work,[0],[0]
"et al. (2011), for example, use shift-reduce parsing to map sentences to natural logical form.
",8 Related Work,[0],[0]
"AMR parsing also shares much in common with tasks like semantic role labeling and framesemantic parsing (Gildea and Jurafsky, 2002; Punyakanok et al., 2008; Das et al., 2014, inter alia).",8 Related Work,[0],[0]
"In these tasks, predicates are often disambiguated to a canonical word sense, and roles are filled by spans (usually syntactic constituents).",8 Related Work,[0],[0]
"They consider each predicate separately, and produce a disconnected set of shallow predicate-argument structures.",8 Related Work,[0],[0]
"AMR, on the other hand, canonicalizes both predicates and arguments to a common concept label space.",8 Related Work,[0],[0]
JAMR reasons about all concepts jointly to produce a unified representation of the meaning of an entire sentence.,8 Related Work,[0],[0]
"We have presented the first published system for automatic AMR parsing, and shown that it provides a strong baseline based on the Smatch evaluation metric.",9 Conclusion,[0],[0]
"We also present an algorithm for finding the maximum, spanning, connected subgraph and show how to incorporate extra constraints with Lagrangian relaxation.",9 Conclusion,[0],[0]
Our featurebased learning setup allows the system to be easily extended by incorporating new feature sources.,9 Conclusion,[0],[0]
"The authors gratefully acknowledge helpful correspondence from Kevin Knight, Ulf Hermjakob, and André",Acknowledgments,[0],[0]
"Martins, and helpful feedback from Nathan Schneider, Brendan O’Connor, Waleed Ammar, and the anonymous reviewers.",Acknowledgments,[0],[0]
This work was sponsored by the U. S. Army Research Laboratory and the U. S. Army Research Office under contract/grant number W911NF-10-1-0533 and DARPA grant FA8750-12-2-0342 funded under the DEFT program.,Acknowledgments,[0],[0]
Abstract Meaning Representation (AMR) is a semantic formalism for which a growing set of annotated examples is available.,abstractText,[0],[0]
"We introduce the first approach to parse sentences into this representation, providing a strong baseline for future improvement.",abstractText,[0],[0]
"The method is based on a novel algorithm for finding a maximum spanning, connected subgraph, embedded within a Lagrangian relaxation of an optimization problem that imposes linguistically inspired constraints.",abstractText,[0],[0]
"Our approach is described in the general framework of structured prediction, allowing future incorporation of additional features and constraints, and may extend to other formalisms as well.",abstractText,[0],[0]
"Our open-source system, JAMR, is available at:Meaning Representation (AMR) is a semantic formalism for which a growing set of annotated examples is available.",abstractText,[0],[0]
"We introduce the first approach to parse sentences into this representation, providing a strong baseline for future improvement.",abstractText,[0],[0]
"The method is based on a novel algorithm for finding a maximum spanning, connected subgraph, embedded within a Lagrangian relaxation of an optimization problem that imposes linguistically inspired constraints.",abstractText,[0],[0]
"Our approach is described in the general framework of structured prediction, allowing future incorporation of additional features and constraints, and may extend to other formalisms as well.",abstractText,[0],[0]
"Our open-source system, JAMR, is available at: http://github.com/jflanigan/jamr",abstractText,[0],[0]
A Discriminative Graph-Based Parser for the Abstract Meaning Representation,title,[0],[0]
