1	28	Although events could be spatial and/or high-dimensional, in this paper we exclusively focus on event modeling in the temporal setup due to its dominance in real-world applications.
2	32	The Poisson process is a de facto standard for its simplicity in mathematical analysis and flexibility in representing the intensity function (i.e., the event occurring rate) λ(t).
5	69	Another is to regard λ(t) as a random process, known as the Cox process (Cox, 1955), which is useful for accounting for uncertainty in the intensity function.
6	67	In this paper, we are particularly interested in the Cox process where two most popular ones are: the Markov modulated Poisson process (MMPP) and the Gaussian process modulated Cox process (GPCox).
7	22	Popular in statistics, the MMPP considers λ(t) as a random sample (trajectory) from a continuous-time Markov chain.
8	27	The model has a finite set of intensity levels where the latent state at each time determines which intensity level is used at that moment.
9	22	The GPCox is a nonparametric Bayesian model formed by placing a Gaussian process (GP) prior on λ(t).
11	53	The MMPP is good at modeling highly different intensity phases: bursty events for some intervals and rare events for others.
23	34	The MMPP makes λ(t) fully stationary (i.e., time independent) since the CTMC is stationary and the intensity under a given state is a constant, invariant of t. On the other hand, the GPCox builds a fully non-stationary (time-variant) λ(t) on top of the kernel function defined over t. Our MMGCP somehow aims to model a so-called semi-stationary intensity function in that the macro-scale intensity regime change is governed by the stationary CTMC dynamics, while within each regime, the intensity is modeled as a smooth time-variant function.
24	59	In this sense, an ideal scenario for our model is as follows: there are r underlying candidate intensity functions {λi(t)}ri=1 where at a given time t, which of these candidates is active is determined by the stationary r-state Markov process X(t), that is, λX(t)(t).
40	18	Instead, the Cox process further regards λ(t) as a random process.
51	18	The GPCox model has a latent function f(t) distributed by a Gaussian process a priori, which determines the intensity function as λ(t) = ρ(f(t)) where ρ(·) is a non-negative link function, for instance, sigmoid, exponential or square function.
65	26	, r. To determine which of these r functions is responsible for the intensity at each time, we introduce a latent Markov process X(t), similarly as the MMPP, generated from a r-state CTMC (Q, π).
67	23	(4) The full joint distribution of the model can be written as: P (D, X(·), f(·)|Θ,Ω) = (5) P (f(·)|Θ)× P (X(·)|Ω)× P (D|X(·), f(·)), where Θ = {θm,θk} is the parameters of the mean and covariance functions of the prior GP (e.g., θk = {θik}ri=1 with θik denoting the parameters of the covariance function ki(·, ·) for f i(·)).
70	25	To formally derive these likelihoods, it is convenient to partition the horizon [0, T ] according to a realized state trajectory X(·).
99	24	We define a tractable form of the variational density q(X, f), and optimize it to approximate the true posterior (8) as much as possible.
103	27	, r. Furthermore, we force the conditional density q(f |fZ) to coincide with the prior P (f |fZ) exactly, which is crucial to have some difficult terms canceled out in the KL divergence objective, making the inference scalable (Titsias, 2009; Lloyd et al., 2015).
110	21	Hence, maximizing the ELBO wrt all the parameters can achieve both variational inference (i.e., q(·) optimization) and model selection (i.e, learning prior model parameters) simultaneously.
117	21	We describe how to compute Eq(X)[P (X)] analytically (Eq(X)[q(X)] done similarly).
119	32	With X(·) fixed, let nij be the number of transitions from state i to j where j 6= i, and ∆i be the sojourn time at state i, that is, ∆i = ∑ l:sl=i ∆ul.
123	22	Using q(X(t) = i) = [αetC ]i from the CTMC theorems (see (4) in Appendix A), we have: Eq(X)[∆i] = [αJC ]i, (16) where JC = ∫ T 0 etCdt, is the (r × r) matrix by integrating the matrix exponential over [0, T ], and [v]i indicates the i-th element of the vector v. As the number of transitions nij = ∫ T 0 I{X(t)=i AND X(t+dt)=j}, and using q(X(t) = i,X(t+ dt) = j) = [αetC ]iCijdt ((5) in Appendix A), Eq(X)[nij ] = [αJC ]iCij .
130	24	The technical details are described in Appendix B and Appendix C.2.
146	16	This must not incur much computational overhead since it is univariate sampling.
169	43	As a baseline, we also consider: i) the classical kernel smoothing (KS) approach (Diggle, 1985), specifically the Gaussian kernel density estimator, and ii) the log Gaussian Cox process (LGCP) (Rathbun & Cressie, 1994; Møller et al., 1998), which approximates the problem as a standard GP inference with Poisson-likelihood iid data via event counting through discretization of the time horizon.
171	23	The first setup simulates a fully stationary scenario (denoted by Full-Stn), where we generate data from a r = 3- state MMPP model with highly different intensity levels {1.0, 4.0, 8.0}.
176	31	The average test log-likelihoods are shown in Table 1(A).
177	25	As expected, the MMPP model attains the best performance since the model structure exactly matches that of the data generating one.
187	16	The GPCox and our MMGCP perform comparably well.
208	20	There are 9074 football games as a whole collected from major European leagues for 5 years (from 2011/12 season to 2016/17).
220	24	The dataset7 is obtained by real-time collections of the earthquake events from the Italian Earthquakes National Center, which contains earthquake records of various magnitudes that hit the center of Italy for three months, from August to November in 2016.
