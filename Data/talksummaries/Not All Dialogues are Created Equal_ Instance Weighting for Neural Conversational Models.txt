5	19	They are therefore often trained on conversations collected from various online resources, such as Twitter discussions (Ritter et al., 2010) online chat logs (Lowe et al., 2017), movie scripts (DanescuNiculescu-Mizil and Lee, 2011) and movie and TV subtitles (Lison and Tiedemann, 2016).
6	24	Although these corpora are undeniably useful, they also face some limitations from a dialogue modelling perspective.
8	38	In other words, we do not know whether two consecutive sentences are part of the same dialogue turn or were uttered by different speakers.
10	20	Furthermore, these dialogues contain multiple references to named entities (in particular, person names such as fictional characters) that are specific to the dialogue in question.
11	18	These named entities should ideally not be part of the conversation model, since they often draw on an external context that is absent from the inputs provided to the conversation model.
13	44	Finally, a substantial portion of the utterances observed in these corpora is made of neutral, commonplace responses (“Perhaps”, “I 384 don’t know”, “Err”, ...) that can be used in most conversational situations but fall short of creating meaningful and engaging conversations with human users (Li et al., 2016a).
15	67	The purpose of this model is to associate each 〈context, response〉 example pair to a numerical weight that reflects the intrinsic “quality” of each example.
50	18	The quality of a particular 〈context, response〉 pair is difficult to determine using handcrafted rules – for instance, the probability of a turn bound- ary may depend on multiple factors such as the presence of turn-yielding cues or the time gap between the utterances (Lison and Meena, 2016).
52	37	What constitutes a high-quality response depends in practice on the specific criteria we wish to uphold in the conversation model – for instance, favouring responses that are likely to form a new dialogue turn (rather than a continuation of the current turn), avoiding the use of dull, commonplace responses, or disfavouring the selection of responses that contain unresolved references to person names.
57	37	The fixed-size vectors for the context and response are then fed to a regular densely-connected layer, and finally to the final weight value through a sigmoid activation function.
58	23	Additional features can also be included whenever available – for instance, timing information for movie and TV subtitles (such as the duration gap between the context and its response, in milliseconds), or document-level features such as the dialogue genre or the total duration of the dialogue.
65	18	For retrieval models, this minimisation is expressed as: θ∗ = minθ n∑ 1 wi L(yi, f(ci, ri; θ)) (1) where L is a loss function (for instance, the cross-entropy loss), and yi is set to either 1 if ri is the response to ci, and 0 otherwise (when ri is a negative example).
66	37	For generative models, the minimisation is similarly expressed as: θ∗ = minθ n∑ 1 wi L(ri, f(ci; θ)) (2) In both cases, the loss computed from each example pair is multiplied by the weight value determined by the weight model.
72	29	A Dual Encoder model combined with the weighting model from Section 3.1.
73	17	TF-IDF model The TF-IDF (Term Frequency - Inverse Document Frequency) model computes the similarity between the context and its response using methods from information retrieval (Ramos, 2003).
77	18	The matching score between the context and its response is then determined as the cosine similarity between the two vectors: similarity = vc · vr ‖vc‖2 ‖vr‖2 (3) where vc and vr respectively denote the TF-IDFweighted vectors for the context and response.
81	35	The inner product of the predicted and actual responses is then calculated and normalised, yielding a similarity score.
123	18	Training details The dialogue contexts were limited to the last 10 utterances preceding the response and a maximum of 60 tokens.
133	31	Let {〈ci, ri〉, 1 ≤ i ≤ n} be the list of m context-response pairs from the test set.
134	33	For each context ci, we create a set ofm alternative responses, one response being the actual response ri, and them−1 other responses being sampled at random from the same corpus.
139	18	Our hypothesis is that the weighting model biases the responses selected by the conversation model towards more cohesive adjacency pairs between context and response2.
151	19	These inconclusive results are probably due to the very low agreement between the evaluation participants (Krippendorff’s α for continuous variable = 0.36).
153	59	We hypothesise that the nature of the corpus, which is heavily dependent on an external context (the movie scenes), makes it particularly difficult to assess the consistency of the responses.
154	81	Some examples of responses produced by the two Dual Encoder models illustrate the improvements brought by the weighting model.
159	246	⇒ Response of Dual Encoder + weighting: – Yes, sir.
160	51	(2) Context of conversation: – Let me finish dinner before you eat it... Chop the peppers... – Are you all right?
161	22	⇒ Response of Dual Encoder: – No thanks, not hungry.
164	22	Some of the issues raised in previous papers are the absence of turn segmentation in subtitling corpus (Vinyals and Le, 2015; Serban and Pineau, 2015; Lison and Meena, 2016), the lack of long-term consistency and “personality” in the generated responses (Li et al., 2016b), and the ubiquity of dull, commonplace responses when training generative models (Li et al., 2016a).
171	44	Instead of this two-pass procedure, an alternative approach is to directly learn a conversation model on the subset of example pairs that are known to be of high-quality.
177	77	As generative models are more computationally intensive to train than retrieval models, the presented approach may bring another important benefit, namely the ability to filter out part of the training data to concentrate the training time on “interesting” examples with a high cohesion between the context and its response.
