49	1	(iii) Finally, we compare the performance of our algorithms with several state of the art algorithms (Grill et al., 2015; Huang et al., 2006b; Jones et al., 1998; Kandasamy et al., 2016b;b; Srinivas et al., 2009) for black-box optimization in the multi-fidelity setting, on real and synthetic data-sets.
69	1	It is also assumed that a single query at fidelity z incurs a cost (z), where : Z !
72	1	The optimal fidelity z is assumed to have zero bias i.e. ⇣(1) = 0.
90	1	For any depth h 0, the cells {Ph,i}1iIh denote a partitioning of the space X , where Ih is the number of cells at depth h. At depth 0 there is a single cell P0,1 = X .
94	2	As an illustrative example let us consider a hierarchical black-box optimization problem over the domain X = [0, 1] ⇥ [0, 1].
95	1	Let us consider a hierarchical partition of this domain where the cells are of the form {x 2 X : b1,1  x1 < b1,2, b2,1  x2 < b2,2}.
117	1	Then in Section 4.2, we provide Algorithm 2 that searches for the optimal smoothness by spawning O(log⇤) instances of Algorithm 1 with a carefully designed sequence of smoothness parameters (⌫, ⇢) as arguments.
134	1	In Section 5, we show that Algorithm 2 does almost as well as Algorithm 1 without requiring the optimal parameters as input.
135	1	Algorithm 2 MFPDOO: Multi-Fidelity Parallel Deterministic Optimistic Optimization 1: Arguments: (⌫max, ⇢max), ⇣(z), (z), P , ⇤ 2: Let N = (1/2)Dmax log(⇤/ log(⇤)) where Dmax = logK/ log(1/⇢max) 3: for i = 0 to N 1 do 4: Spawn MFDOO (Algorithm 1) with parameters (⌫max, ⇢ N/(N i) max ) with budget (⇤ N (1))/N 5: end for 6: Let x(i)⇤ be the point returned by the i th MFDOO instance for i 2 {0, .., N 1}.
137	1	We will show in Theorem 2 that at least one of the MFDOO instances will have a performance comparable to Algorithm 1 supplied with parameters (⌫⇤, ⇢⇤) with a budget of O(⇤/N).
148	1	Then we refine these guarantees under some natural conditions on the bias and cost functions.
153	1	We defer the proof of Theorem 1 to Section A in the appendix.
158	1	Consider training a learning algorithm with a particular hyper-parameter that involves optimizing a strongly convex and smooth function with gradient descent.
159	1	Let the fidelity denote a rescaled version of the number of steps in gradient descent n. We assume that at the optimal fidelity (N steps) we reach the optimal value of the function up to an error of ✏⇤.
171	1	(ii) Under Assumption 3: R⇤  2 ⌫ ⇢ ⇣ 2C(⌫,⇢)K ⇤( 1⇢ d(⌫,⇢) 1) ⌘ 1 d(⌫,⇢)+1 .
194	1	If a cell is queried at two different fidelities z1 and z2 and the function values obtained are f1 and f2, then we update c to 2c whenever c|z1 z2| < |f1 f2|.
195	1	The above update is only performed if |z1 z2| is greater than a specified threshold (0.0001 in our experiments).
198	1	Our implementation can be found at https://github.com/rajatsen91/MFTREE DET.
199	1	We evaluate all the algorithms on standard benchmark functions used in global optimization.
204	1	The approximations for these methods are obtained at z = 0.333 and z = 0.667.
208	1	In our algorithm we set the number of MFDOO instances spawned to be N = 0.1Dmax log(⇤/ (1)), given a total budget ⇤.
216	1	In this section we describe our experiments that involve tuning hyper-parameters for text classification.
217	1	For this purpose we use a subset of the 20 news group dataset (Joachims, 1996).
219	1	For our experiments, we use the scikit-learn implementation of SVM classifier and also the inbuilt KFold function for crossvalidation.
221	1	We use a one-dimensional fidelity space, where the fidelity denotes the number of samples used to obtain 5-fold cross-validation accuracy.
232	1	1f for all the candidate algorithms.
246	5	For instance, in the hyper-parameter tuning one can choose to use less samples or train for lesser iterations.
247	26	It is an interesting research direction to incorporate a multi-dimensional fidelity space with tree-search based algorithms.
248	164	Finally, in this work we work in the noise-less setting where the function and the approximations are deterministic.
249	163	We believe it is possible to extend our results to a setting where zero-mean noise is added to the function and its approximations.
