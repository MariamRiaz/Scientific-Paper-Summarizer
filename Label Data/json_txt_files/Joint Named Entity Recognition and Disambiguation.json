{"sections": [{"text": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 879\u2013888, Lisbon, Portugal, 17-21 September 2015. c\u00a92015 Association for Computational Linguistics."}, {"heading": "1 Introduction", "text": "In applications of complex Natural Language Processing tasks, such as automatic knowledge base construction, entity summarization, and question answering systems, it is essential to first have high quality systems for lower level tasks, such as partof-speech (POS) tagging, chunking, named entity recognition (NER), entity linking, and parsing among others. These lower level tasks are usually decoupled and optimized separately to keep the system tractable. The disadvantage of the decoupled approach is that each lower level task is not\naware of other tasks and thus not able to leverage information provided by others to improve performance. What is more, there is no guarantee that their outputs will be consistent.\nThis paper addresses the problem by building a joint model for Entity Recognition and Disambiguation (ERD). The goal of ERD is to extract named entities in text and link extracted names to a knowledge base, usually Wikipedia or Freebase. ERD is closely related to NER and linking tasks. NER aims to identify named entities in text and classify mentions into predefined categories such as persons, organizations, locations, etc. Given a mention and context as input, entity linking connects the mention to a referent entity in a knowledge base.\nExisting ERD systems typically run a NER to extract entity mentions first, then run an entity linking model to link mentions to a knowledge base. Such a decoupled approach makes the system tractable, and both NER and linking models can be optimized separately. The disadvantages are also obvious: 1) errors caused by NER will be propagated to linking and are not recoverable 2) NER can not benefit from information available used in entity linking; 3) NER and linking may create inconsistent outputs.\nWe argue that there is strong mutual dependency between NER and linking tasks. Consider the following two examples:"}, {"heading": "1. The New York Times (NYT) is an American daily newspaper.", "text": ""}, {"heading": "2. Clinton plans to have more news conferences in 2nd term. WASHINGTON 1996-12-06", "text": "Example 1 is the first sentence from the Wikipedia article about \u201cThe New York Times\u201d. It is reasonable but incorrect for NER to identify \u201cNew York Times\u201d without \u201cThe\u201d as a named entity, while entity linking has no trouble connecting \u201cThe New York Times\u201d to the correct entity.\n879\nExample 2 is a news title where our NER classifies \u201cWASHINGTON\u201d as a location, since a location followed by a date is a frequent pattern in news articles it learned, while the entity linking prefers linking this mention to the U.S. president \u201cGeorge Washington\u201d since another president\u2019s name \u201cClinton\u201d is mentioned in the context. Both the entity boundaries and entity types predicted by NER are correlated to the knowledge of entities linked by entity linking. Modeling such mutual dependency is helpful in resolving inconsistency and improving performance for both NER and linking.\nWe propose JERL, Joint Entity Recognition and Linking, to jointly model NER and linking tasks and capture the mutual dependency between them. It allows the information from each task to improve the performance of the other. If NER is highly confident on its outputs of entity boundaries and types, it will encourage entity linking to link an entity which is consistent with NER\u2019s outputs, and vice versa. In other words, JERL is able to model how consistent NER and linking\u2019s outputs are, and predict coherent outputs. According to our experiments, this approach does improve the end to end performance. To the best of our knowledge, JERL is the first model to jointly optimize NER and linking tasks together completely .\nSil (2013) also proposes jointly conducting NER and linking tasks. They leverage existing NER/chunking systems and Freebase to over generate mention candidates and leave the linking algorithm to make final decisions, which is a reranking model. Their model captures the dependency between entity linking decisions and mention boundary decisions with impressive results. The difference between our model and theirs is that our model jointly models NER and linking tasks from the training phrase, while their model is a combined one which depends on an existing state-of-art NER system. Our model is more powerful in capturing mutual dependency by considering entity type and confidences information, while in their model the confidence of outputs is lost in the linking phrase. Furthermore, in our model NER can naturally benefit from entity linking\u2019s decision since both decisions are made together, while in their model, it is not clear how the linking decision can help the NER decision in return.\nJoint optimization is costly. It increases the problem complexity, is usually inefficient, and\nrequires the careful consideration of features of multiple tasks and mutual dependency, making proper assumptions and approximations to enable tractable training and inference. However, we believe that joint optimization is a promising direction for improving performance for NLP tasks since it is closer to how human beings process text information. Experiment result indicates that our joint model does a better job at both NER and linking tasks than separate models with the same features, and outperforms state-of-art systems on a widely used data set. We found improvements of 0.4% absolute F1 for NER on CoNLL\u201903 and 0.36% absolute precision@1 for linking on AIDA. NER is a widely studied problem, and we believe our improvement is significant.\nThe contributions of this paper are as follows: 1. We identify the mutual dependency between NER and linking tasks, and argue that NER and linking should be conducted together to improve the end to end performance. 2. We propose the first completely joint NER and linking model, JERL, to train and inference the two tasks together. Efficient training and inference algorithms are also presented. 3. The JERL outperforms the best NER record on the CoNLL\u201903 data set, which demonstrates how NER could be improved further by leveraging knowledge base and linking techniques.\nThe remainder of this paper is organized as follows: the next section discusses related works on NER, entity linking, and joint optimization; section 3 presents our Joint Entity Recognition and Linking model in detail; section 4 describes experiments, results, and analysis; and section 5 concludes."}, {"heading": "2 Related Work", "text": "The NER problem has been widely addressed by symbolic, statistical, as well as hybrid approaches. It has been encouraged by several editions of evaluation campaigns such as MUC (Chinchor and Marsh, 1998), the CoNLL 2003 NER shared task (Tjong Kim Sang and De Meulder, 2003) and ACE (Doddington et al., 2004). Along with the improvement of Machine Learning techniques, statistical approaches have become a major direction for research on NER, especially after Conditional Random Field is proposed by Lafferty et al. (2001). The well known state-of-art NER systems are Stanford NER (Finkel et al., 2005) and UIUC\nNER (Ratinov and Roth, 2009). Liang (2005) compares the performance of the 2nd order linear chain CRF and Semi-CRF (Sarawagi and Cohen, 2004) in his thesis. Lin and Wu (2009) cluster tens of millions of phrases and use the resulting clusters as features in NER reporting the best performance on the CoNLL\u201903 English NER data set. Recent works on NER have started to focus on multi-lingual named entity recognition or NER on short text, e.g. Twitter.\nEntity linking was initiated with Wikipediabased works on entity disambiguation (Bunescu and Pasca, 2006; Cucerzan, 2007). This task is encouraged by the TAC 2009 KB population task1 first and receives more and more attention from the research community (Hoffart et al., 2011; Ratinov et al., 2011; Han and Sun, 2011). Linking usually takes mentions detected by NER as its input. Stern et al. (2012) and Wang et al. (2012) present joint NER and linking systems and evaluate their systems on French and Chinese data sets. Sil and Yates (2013) take a re-ranking based approach and achieve the best result on the AIDA data set. In 2014, Microsoft and Google jointly hosted \u201cEntity Recognition and Disambiguation Challenge\u201d which focused on the end to end performance of linking system 2.\nJoint optimization models have been studied at great length. E.g. Dynamic CRF (McCallum et al., 2003) has been proposed to conduct Partof-Speech Tagging and Chunking tasks together. Finkel and Manning (2009) show how to model parsing and named entity recognition together. Yu et al. (2011) work on jointly entity identification and relation extraction from Wikipedia. Sil\u2019s (2013) work on jointly NER and linking is described in the introduction section of this paper. It is worth noting that joint optimization does not always work. The CoNLL 2008 shared task (Surdeanu et al., 2008) was intended to encourage jointly optimize parsing and semantic role labeling, but the top performing systems decoupled the two tasks."}, {"heading": "3 Joint Entity Recognition and Linking", "text": "Named entity recognition is usually formalized as a sequence labeling task, in which each word is classified to not-an-entity or entity labels. Conditional Random Fields (CRFs) is one of the popu-\n1http://www.nist.gov/tac/2014/KBP/ 2http://web-ngram.research.microsoft.com/ERD2014/\nlar models used. Most features used in NER are word-level (e.g. a word sequence appears at position i or whether a word contains exactly four digits). It is hard, if not impossible, to encode entity-level features (such as \u201dentity length\u201d and \u201dcorrelation to known entities\u201d) in traditional CRF. Entity linking is typically formalized as a ranking task. Features used for entity linking are at entitylevel inherently (such as entity prior probability; whether there are any related entity names or discriminative keywords occurring in the context).\nThe main challenges of joint optimization between NER and linking are: how to combine a sequence labeling model and a ranking model; and how to incorporate word-level and entity-level features. In a linear chain CRF model, each word\u2019s label is assumed to depend on the observations and the label of its previous word. Semi-CRF carefully relaxes the Markov assumption between words in CRF, and models the distribution of segmentation boundaries directly. We further extend Semi-CRF to model entity distribution and mutual dependency over segmentations, and name it Joint Entity Recognition and Linking (JERL). The model is described below."}, {"heading": "3.1 JERL", "text": "Let x = {xi} be a word sequence containing |x| words. Let s = {sj} be a segmentation assignment over x, where segment sj = (uj , vj) consist of a start position uj and an end position vj . All segments have a positive length and are adjacent to each other, so every (uj , vj) always satisfies 1 \u2264 uj \u2264 vj \u2264 |x| and uj+1 = vj + 1. Let y = {yj} be labels in a fixed label alphabet Y over a segmentation assignment s. Here Y is the set of types NER to predict. xsj = (xuj . . . xvj )\nis the corresponding word sequence to sj , and Esj = {ej,k} is a set of entities in the knowledge base (KB), which may be referred by word sequence xsj in the entity linking task. Each entity ej,k is associated with a label yej,k \u2208 {0, 1}. Label yej,k takes 1 iff xsj referring to entity ej,k, and 0 otherwise. If xsj does not refer to any entity in the KB, yej,0 takes 1, which is analogous to the NIL3 identifier in entity linking.\nBased on the preliminaries and notations, Figure 1 shows the factor graph (Kschischang et al., 2001) of JERL. There are similar factor nodes for every (uj , vj , yej,k), we only show the first one (uj , vj , yej,0) for clarity.\nGiven x, let a = (s,y,ye) be a joint assignment, and g(x, j,a) be local functions for xsj , namely features, each of which maps an assignment a to a measurement gk(x, j, a) \u2208 <. Then G(x,a) = \u2211|s| j=1 g(x, j,a) is the factor graph defining a probability distribution of assignment a conditioned on word sequence x.\nThen JERL, conditional probability of a over x, is defined as:\nP (a|x,w) = 1 Z(x) ew\u00b7G(x,a) (1)\nwhere w is the weight vector corresponding to G will be learned later, and Z(x) is the normalization factor Z(x) = \u2211 a\u2208A e\nw\u00b7G(x,a), in which A is the union of all possible assignments over x.\nJERL is a probabilistic graphical model. More specificly, as shown in Figure 1, there are three groups of local functions and one constrain introduced. Each of them take a different role in JERL, as described below:\nFeatures defined on x, sj , yj , yj\u22121 are written as gner(x, sj , yj , yj\u22121). These functions model segmentation and entity types\u2019 distribution over x. Actually, every local features used in NER can be formulated in this way, and thus can be included in JERL. We thus refer to them as \u201cNER features\u201d.\nFeatures defined on x, sj , yej,k are written as gel(x, sj , yej,k) and are called \u201clinking features\u201d. These features model joint probabilities of word sequence xsj and linking decisions y k j,k = 1(0 \u2264 k \u2264 |Esj |) given context x. JERL incorporates all linking features in this way.\nFeatures defined on yj , yej,k are written as gcr(yj , yej,k). These features model \u201cmutual de-\n3In the entity linking task, if a given mention refers to an entity which is not in the knowledge base, linking system should return a special identifier \u201cNIL\u201d.\npendency\u201d between NER and linking\u2019s outputs. For each entity ej,k, there is additional information available in the knowledge base, e.g. categories information, popularity and relationship to other entities. These features encourage predicting coherent outputs for NER and linking.\nThere is one constrain for each yej that the corresponding xsj can refer to only one entity ej,k \u2208 Esj or NIL. This is equivalent to \u2211|Esj | k=0 y e j,k = 1.\nBased on the above description, G(x,a) in equation 1 is the sum of conjunction (gner, gel, gcr) over s, and can be rewritten as,\nG(x,a) = \u2211|s|\nj=1( g ner(x, sj , yj , yj\u22121) , \u2211|Ej |\nk=0 g el(x, sj , yej,k) , \u2211|Ej |\nk=0 g cr(x, yj , yej,k) )\nIn summary, JERL jointly models the NER and linking, and leverages mutual dependency between them to predict coherent outputs. Previous works (Cucerzan, 2007; Ratinov et al., 2011; Sil and Yates, 2013) on linking argued that entity linking systems often suffer because of errors involved in mention detection phrase, especially false negative errors, and try to mitigate it via overgenerating mention candidates. From the mention generation perspective, JERL actually considers every possible assignment and is able to find the optimal a."}, {"heading": "3.2 Parameter Estimation", "text": "We describe how to conduct parameter estimation for JERL in this section. Given independent and identically distributed (i.i.d.) training data T = {(xt,at)}Nt=1, the goal of parameter estimation is to find optimal w\u2217 to maximize the joint probability of the assignments {at} over {xt}.\nw\u2217 = argmaxw\u2208<|G| N\u220f t=1 P (at|xt,w)\nWe use conditional log likelihood with `2 norm as the objective function in training,\nL(T ,w) = \u2211t logP (at|xt,w)\u2212 12\u03c32 ||w||22 The above function is concave, adding regularization to ensure that it has exactly one global optimum. We adopt a limited-memory quasi-Newton method (Liu and Nocedal, 1989) to solve the optimization problem.\nThe gradient of L(T ,w) is derived as,\n\u2202L \u2202w = \u2211 t (G(xt,at)\n\u2212 \u2211 a\u2032 G(xt,a\u2032)P (a\u2032|xt,w))\u2212 w \u03c32 (2)\nAs shown in Figure 1, our model\u2019s factor graph is a tree, which means the calculation of the gradient is tractable.\nInspired by the forward backward algorithm (Sha and Pereira, 2003) and Semi-CRF (Sarawagi and Cohen, 2004), we leverage dynamic programming techniques to compute the normalization factor Zw and marginal probability P (a\u2032j|xt,w) when w is given.(Sutton and McCallum, 2006) The parameter estimation algorithm is abstracted in Algorithm 1.\nAlgorithm 1: JERL parameter estimation input : training data T = {(xt,at)}Nt=1 output: the optimal w\nw \u2190 0; while weight w is not converged do\nZ \u2190 0; w\u2032 \u2190 0; for t\u2190 1 to N do\ncalculate \u03b1t,\u03b2t according to eq.3; calculate Zt according to eq.4 calculate w\u2032t according to eq.2, 5; Z \u2190 Z + Zt; w\u2032 \u2190 w\u2032 +w\u2032t;\nend update w to maximize log likelihood L(T ,w) under (Z,w\u2032) via L-BFGS;\nend\nLet \u03b1i,y (i \u2208 [0, |x|], y \u2208 Y) be the sum of potential functions of all possible assignments over (x1 . . . xi) whose last segmentation\u2019s labels are y. Then \u03b1i,y can be calculated recursively from i = 0 to i = |x| as below.\nWe first define base cases as \u03b10,y = 1|{y\u2208Y}. When i \u2208 (0, |x|]:\n\u03b1i,y = L\u2211 d=1 \u2211 y\u2032\u2208Y \u03b1i\u2212d,y\u2032\u03c8neri\u2212d+1,i,y,y\u2032\n( \u2211\nyej\u2208Y e\u2217j \u03c8el.cri\u2212d+1,i,y,yej )\n(3)\nwhere L is the max segmentation length in SemiCRF, and Y e\u2217j is all valid assignments for y e j\nwhich satisfies \u2211|Esj |\nk=0 y e j,k = 1. The \u03c8 ner uj ,vj ,yj ,yj\u22121\nand \u03c8el.cruj ,vj ,yj ,yej are precomputed ahead as below,\n\u03c8neruj ,vj ,yj ,yj\u22121 = e wner\u00b7gner(x,sj ,yj ,yj\u22121) \u03c8el.cruj ,vj ,yj ,yej = |Ej |\u220f k=0 ew elgel(x,sj ,y e j,k)+w crgcr(yj ,y e j,k)\nwherewner,wel andwcr are weights for gner, gel and gcr in w accordingly. The value of Zw can then be written as\nZw(x) = \u2211 y \u03b1|x|,y (4)\nDefine \u03b2i,y (i \u2208 [0, |x|], y \u2208 Y) as the sum of potential functions of all possible assignments over (xi+1 . . . x|x|) whose first segmentation\u2019s labels are y. \u03b2i,y is calculated in a similar way, except they are calculated from i = |x| to left i = 0.\nOnce we get {\u03b1i,j} and {\u03b2i,j}, the marginal probability of arbitrary assignment aj = (sj , yj ,yej ), where sj = (uj , vj), can be calculated as below:\nP (sj , yj |x,w) = ( \u2211\ny\u2032\u2208Y \u03b1uj\u22121,y\u2032\u03c8 ner uj ,vj ,yj ,y\u2032)\u03b2vj ,yj\nZw(x)\nand\nP (aj |xt,w) =\nP (sj , yj |x,w) \u03c8el.cruj ,vj ,yj ,yej\u2211\nye\u2032 \u2208Y e\u2217j \u03c8 el.cr uj ,vj ,yj ,ye\u2032\n(5)"}, {"heading": "3.3 Inference", "text": "Given a new word sequence x and model weights w trained on a training set, the goal of inference is to find the best assignment, a\u2217 = argmaxaP (a|x,w) for x. We extend the Viterbi algorithm to exactly infer the best assignment. The inference algorithm is shown in Algorithm 2.\nLet \u03c6(uj , vj , yj , yj\u22121) be the product of potentials depending on (sj , yj , yj\u22121) as,\n\u03c6(uj , vj , yj , yj\u22121) = \u03c8neruj ,vj ,yj ,yj\u22121( \u2211\nyej\u2208Y e\u2217j \u03c8el.cruj ,vj ,yj ,yej ) (6)\nAlgorithm 2: JERL inference input : one word sequence x and weights w output: the best assignment a over x\n// shrink JERL graph to a Semi-CRF graph; for u\u2190 1 to |x| do\nfor v \u2190 u+ 1 to |x| do for (y, y\u2032) \u2208 Y \u00d7 Y do\ncalculate \u03c6u,v,y,y\u2032 // see eq.6; end\nend end // infer the best assignment of (s\u2217, y\u2217); for i\u2190 1 to |x| do\nfor y \u2208 Y do calculate Vi,y // see eq.7;\nend end (s\u2217, y\u2217)\u2190 argmax(Vi,y); // infer the best assignment of {yej }; for j \u2190 1 to |s\u2217| do\nyej \u2190 argmax(P (|x,w, s\u2217j ,y\u2217j )) end a\u2217 \u2190 (s\u2217,y\u2217,ye\u2217);\nand let V (i, y) denotes the largest value of (w \u00b7 G(x,a\u2032)) where a\u2032 could be any possible partial assignment starting from x1 to xi. The best (s\u2217,y\u2217) are derived during the following recursive calculation,\nVi,y = maxy\u2032\u2208Y,d\u2208[1,L] (Vi\u2212d,y\u2032 + \u03c6(i\u2212d+1, i, y, y\u2032)) i > 0 0 i = 0 \u2212\u221e i < 0\n(7)\nwhere L is the maximum segmentation length for Semi-CRF.\nOnce (s\u2217,y\u2217) are found, the corresponding ye\u2217j = argmax{ye\u2032 \u2208Y e\u2217j }(\u03c8 el.cr u\u2217j ,v \u2217 j ,y \u2217 j ,y e \u2032 ) is also the optimal one. Then a\u2217 = (s\u2217,y\u2217,ye\u2217) is the best assignment for the given x and w."}, {"heading": "4 Experiments", "text": "In our experiments, we first construct two baseline models JERLner and JERLel, which use exact NER and EL feature sets used in JERL. Then evaluate JERL and the two baseline models against several state-of-art NER and linking systems. After that, we evaluate JERL under different feature\nsettings to analysis the contributions of each features set, and show some examples we find. We also compare the training speed under different settings."}, {"heading": "4.1 Data set", "text": "We take the CoNLL\u201903/AIDA English data set to evaluate the performance of NER and linking systems. CoNLL\u201903 is extensively used in prior work on NER evaluation (Tjong Kim Sang and De Meulder, 2003). The English data is taken from Reuters news articles published between August 1966 and August 1997. Four types of entities persons (PER), organizations (ORG), locations (LOC), and miscellaneous names (MISC) are annotated. Hoffart et al. (2011) hand-annotated all proper nouns with corresponding entities wiht YAGO2, Freebase and Wikipedia IDs. This data is referenced as AIDA here. To the best of our knowledge, this data set is the biggest data set which has been labeled for both NER and linking tasks. It becomes a really good starting point for our work. Table 1 contains of an overview of the CoNLL\u201903/AIDA data set.\nFor entity linking, we take Wikipedia as the referent knowledge base. We use a Wikipedia snapshot dumped in May 2013, which contains around 4.8 million articles. We also align our Wikipedia dump with additional knowledge bases, Freebase and Satori (a Microsoft internal knowledge base), to enrich the information of these entities."}, {"heading": "4.2 Evaluation Metrics", "text": "We follow the CoNLL\u201903 metrics to evaluate NER performance by precision, recall, and F1 scores, and follow Hoffart\u2019s (2011) experiment setting to evaluate linking performance by micro precision@1. Since the linking labels of CONLL\u201903 were annotated in 2011, it is not completely consistent with the Wikipedia dump we used in the case. We only consider mention entity pairs where the ground truth are known, and ignore around 20% of NIL mentions in the ground truth."}, {"heading": "4.3 JERL Implementation", "text": "Table 2 shows features used in our models. JERL uses all features in the three categories, while JERLner and JERLel use only one corresponding category. All three models are trained on the train and development set, and evaluated on the test set of CoNLL\u201903/AIDA."}, {"heading": "4.3.1 NER", "text": "Features in the NER category are relevant to NER. We considered the most commonly used features in literatures (Finkel et al., 2005; Liang, 2005; Ratinov and Roth, 2009). We collect several known name lists, like popular English first/last names for people, organization lists and so on from Wikipedia and Freebase. UIUC NER\u2019s lists are also included. In addition, we extract entity name lists from the knowledge base we used for entity linking, and construct 655 more lists. Although those lists are noisy, we find that statistically they do improve the performance of our NER baseline by a significant amount."}, {"heading": "4.3.2 Linking", "text": "Features in linking category are relevant to entity linking. An entity can be referred by its canonical name, nick names, alias, and first/last names. Those names are defined as alternative names for this entity. We collect all alternative names for all\nknown entities and build a name to entity index. This index is used to select entity candidates for any word sequence, also known as surface form. Following previous work by Han and Sun (2011), we calculate entity priors and entity name priors from Wikipedia. Context scores are calculated based on discriminative keywords. Geo distance and related entities capture the relatedness among entities in the given context."}, {"heading": "4.3.3 Mutual", "text": "Features in this category capture the mutual dependency between NER and linking\u2019s outputs. For each entity in a knowledge base, there is category information available. We aggregate around 1000 distinct categories from multiple sources. One entity can have multiple categories. For example, London is connected to 29 categories. We use all combinations between NER types and categories as features in JERL, and let the model learn the correlation of each combination. This encourages coherent NER and EL decisions, which is one of the key contributions of our work."}, {"heading": "4.3.4 Non-local features", "text": "Features capturing long distance dependency between hidden labels are classified as non-local features. Those features are very helpful in improving NER system performance but are costly. Since this is not the focus of this paper, we take a simple approach to incorporate non-local features. We cache history results of previous sentences in a 1000 words window, and adopt several heuristic rules for personal names. This approach contributes 0.2 points to the final NER F1 score. Non-local features are also considered in linking (Ratinov et al., 2011; Han et al., 2011). We try several features, which has been proved to be helpful in TAC data set. However, the gain on CoNLL\u201903/AIDA data set is not obvious, we do not optimize linking globally.\nLastly, based on preliminary studies and experiments, we set the maximum segmentation length to 6 and max candidate count per segmentation to 5 for efficient training and inference."}, {"heading": "4.4 State-of-Art systems", "text": "We take three state-of-art NER systems: NereL (Sil and Yates, 2013), UIUC NER (Ratinov and Roth, 2009) and Stanford NER (Finkel et al., 2005). NereL firstly over generates mentions and decomposes them to sets of connected compo-\nnents, then trains a maximum-entropy model to re-rank different assignments. UIUC NER uses a regularized averaged perceptron model and external gazetteers to achieve strong performance. In Addition, NereL also uses UIUC NER to generate mentions. Stanford NER uses Conditional Random Fields and Gibbs sampling to incorporate non-local features into its model.\nFor entity linking systems, NereL, Kul09 (Kulkarni et al., 2009) and Hof11 (Hoffart et al., 2011) are compared with our models. NereL achieves the best precision@1. Kul09 formulates the local compatibility and global coherence in entity linking, and optimizes the overall entity assignment for all entities in a document via a local hill-climbing approach. Hof11 unifies the prior probability of an entity being mentioned, the similarity between context and entity, and the coherence between entity candidates among all mentions in a dense graph."}, {"heading": "4.5 Results", "text": "Table 3 shows the performance of different NER systems on the CoNLL\u201903 testb data set. We refer the numbers of state-of-art systems reported by Sil and Yates (2013). Stanford NER achieves the best precision, but its recall is low. UIUC reports the (almost) best recorded F1. JERLner considers features only in the NER category, which could be treated as a pure NER system implemented in Semi-CRF. Actually CRF-based implementation with a similar feature set has comparable performance. Our baseline JERLner is strong enough. We argue that that it is mainly because of the additional dictionaries derived from the knowledge base. JERL further pushes the F1 to 91.2, which outperforms UIUC by 0.4 points in F1 score. To the best of our knowledge, it is the best F1 on CoNLL\u201903 since 2009. The reason our model can outperform state-of-art systems is that, it has more knowledge about entities via incorporate entity linking techniques. If an entity can be linked to a well known entity via entity linking in high\nconfidence, its mention boundary and entity type are confirmed implicitly.\nTable 4 shows the performance of different entity linking systems on the AIDA test set. Kul09 and Hof11 use only the correct mentions detected by the Stanford NER as input, and thus their recall is bound by the recall of NER. NereL uses its overgeneration techniques to generate mention candidates, and outperforms Hoff11 in both precision and recall. Our baseline model JERLel is also evaluated on Stanford NER generated mentions, which has comparable performance with Kul09 and Hof11. JERL achieves precision@1 84.58 which is better than NereL.\nWe run 15 trials for both NER and linking\u2019s experiments and report the average numbers above. The standard deviations are 0.11% and 0.08% for NER and linking separately, which pass the standard t-test with confidence level 5%, demonstrating the significance of our results.\nIn order to investigate how different features contribute to the overall gain. We compare JERLner with four different feature sets. Table 5 summaries the results. In the trial \u201c+candidate\u201d, JERL expands every possible segmentation with corresponding entity list and builds its factor graph without any linking and mutual features. This version\u2019s F1 drops to 88.7 which indicates the created structure is quite noisy. In the \u201c+candidate +linking\u201d trial, only linking features are enabled and the F1 is comparable to the baseline. On the other side, in the \u201c+candidate +mutual\u201d trial when mutual features are enabled the F1 increases to 90.6. If we combine both linking and mutual features,\nJERL achieves the reported performance. The result indicates that mutual features are the determining factor to the performance gain.\nTable 6 shows weights of learned mutual dependency of three categories \u201dpeople.person\u201d, \u201dlocation.city\u201d, and \u201dsports.team\u201d. The bigger a weight is, the more consistent this combination would be. From the weights, we find several interesting things. If an entity belongs to any of the three categories, it is less likely to be predicted as non-an-entity by NER. If an entity belongs to the category of \u201dpeople.person\u201d, it more likely to be predicted as PER. When an entity belongs to the category \u201dlocation.city\u201d or \u201dsports.team\u201d, NER may predict it as ORG or LOC. This is because in the CoNLL\u201903/AIDA data set, there are many sports teams mentioned by their city/country names. JERL successfully models such unexpected mutual dependency.\nTable 7 compares the performance and training time under different settings of max segmentation length (MSL) and max referent count (MRC). We use machines with Intel Xeon E5620 @ 2.4GHz CPU (8 cores / 16 logical processors) and 48GB memory. We run every setting 10 times and report the averages. As MSL and MRC increasing, the performance is slightly better, but the training time increased a lot. MSL has linear impact on training time, while MRC affects training time more."}, {"heading": "5 Conclusion and Future Work", "text": "In this paper, we address the problem of joint optimization of named entity recognition and linking. We propose a novel model, JERL, to jointly train\nand infer for NER and linking tasks. To the best of our knowledge, this is the first model which trains two tasks at the same time. The joint model is able to leverage mutual dependency of the two tasks, and predict coherent outputs. JERL outperforms the state-of-art systems on both NER and linking tasks on the CoNLL\u201903/AIDA data set.\nFor future works, we would like to study how to leverage existing partial labeled data, either for NER or for linking only, in joint optimization, and incorporate more NLP tasks together for multitasks joint optimization."}, {"heading": "Acknowledgments", "text": "This work was performed when the first author was working at Microsoft Research. The first author is sponsored by Microsoft Bing Core Relevance team. Thanks Shuming Shi, Bin Gao, and Yohn Cao for their helpful guidance and valuable discussions. Additionally, we would like to thank the three anonymous reviewers for their insightful suggestions and detailed comments."}], "year": 2015, "references": [{"title": "Using encyclopedic knowledge for named entity disambiguation", "authors": ["Razvan C Bunescu", "Marius Pasca."], "venue": "EACL, volume 6, pages 9\u201316.", "year": 2006}, {"title": "Muc-7 information extraction task definition", "authors": ["Nancy Chinchor", "Elaine Marsh."], "venue": "Proceeding of the seventh message understanding conference (MUC-7), Appendices, pages 359\u2013367.", "year": 1998}, {"title": "Large-scale named entity disambiguation based on wikipedia data", "authors": ["Silviu Cucerzan."], "venue": "EMNLPCoNLL, volume 7, pages 708\u2013716.", "year": 2007}, {"title": "The automatic content extraction (ace) program-tasks, data, and evaluation", "authors": ["George R Doddington", "Alexis Mitchell", "Mark A Przybocki", "Lance A Ramshaw", "Stephanie Strassel", "Ralph M Weischedel."], "venue": "LREC.", "year": 2004}, {"title": "Joint parsing and named entity recognition", "authors": ["Jenny Rose Finkel", "Christopher D Manning."], "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Lin-", "year": 2009}, {"title": "Incorporating non-local information into information extraction systems by gibbs sampling", "authors": ["Jenny Rose Finkel", "Trond Grenager", "Christopher Manning."], "venue": "Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,", "year": 2005}, {"title": "A generative entitymention model for linking entities with knowledge base", "authors": ["Xianpei Han", "Le Sun."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 945\u2013", "year": 2011}, {"title": "Collective entity linking in web text: a graph-based method", "authors": ["Xianpei Han", "Le Sun", "Jun Zhao."], "venue": "Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval, pages 765\u2013774. ACM.", "year": 2011}, {"title": "Robust disambiguation of named entities in text", "authors": ["Johannes Hoffart", "Mohamed Amir Yosef", "Ilaria Bordino", "Hagen F\u00fcrstenau", "Manfred Pinkal", "Marc Spaniol", "Bilyana Taneva", "Stefan Thater", "Gerhard Weikum."], "venue": "Proceedings of the Conference on", "year": 2011}, {"title": "Factor graphs and the sum-product algorithm", "authors": ["Frank R Kschischang", "Brendan J Frey", "H-A Loeliger."], "venue": "Information Theory, IEEE Transactions on, 47(2):498\u2013519.", "year": 2001}, {"title": "Collective annotation of wikipedia entities in web text", "authors": ["Sayali Kulkarni", "Amit Singh", "Ganesh Ramakrishnan", "Soumen Chakrabarti."], "venue": "Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining,", "year": 2009}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "authors": ["John Lafferty", "Andrew McCallum", "Fernando CN Pereira"], "year": 2001}, {"title": "Semi-supervised learning for natural language", "authors": ["Percy Liang."], "venue": "Ph.D. thesis, Massachusetts Institute of Technology.", "year": 2005}, {"title": "Phrase clustering for discriminative learning", "authors": ["Dekang Lin", "Xiaoyun Wu."], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Vol-", "year": 2009}, {"title": "On the limited memory bfgs method for large scale optimization", "authors": ["Dong C Liu", "Jorge Nocedal."], "venue": "Mathematical programming, 45(1-3):503\u2013528.", "year": 1989}, {"title": "Dynamic conditional random fields for jointly labeling multiple sequences", "authors": ["Andrew McCallum", "Khashayar Rohanimanesh", "Charles Sutton."], "venue": "nips workshop on syntax, semantics and statistics.", "year": 2003}, {"title": "Design challenges and misconceptions in named entity recognition", "authors": ["Lev Ratinov", "Dan Roth."], "venue": "Proceedings of the Thirteenth Conference on Computational Natural Language Learning, pages 147\u2013 155. Association for Computational Linguistics.", "year": 2009}, {"title": "Local and global algorithms for disambiguation to wikipedia", "authors": ["Lev Ratinov", "Dan Roth", "Doug Downey", "Mike Anderson."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language", "year": 2011}, {"title": "Semimarkov conditional random fields for information extraction", "authors": ["Sunita Sarawagi", "William W Cohen."], "venue": "Advances in Neural Information Processing Systems, pages 1185\u20131192.", "year": 2004}, {"title": "Shallow parsing with conditional random fields", "authors": ["Fei Sha", "Fernando Pereira."], "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1, pages 134\u2013", "year": 2003}, {"title": "Re-ranking for joint named-entity recognition and linking", "authors": ["Avirup Sil", "Alexander Yates."], "venue": "Proceedings of the 22nd ACM international conference on Conference on information & knowledge management, pages 2369\u20132374. ACM.", "year": 2013}, {"title": "A joint named entity recognition and entity linking system", "authors": ["Rosa Stern", "Beno\u0131\u0302t Sagot", "Fr\u00e9d\u00e9ric B\u00e9chet"], "venue": "In Proceedings of the Workshop on Innovative Hybrid Approaches to the Processing of Textual Data,", "year": 2012}, {"title": "The conll-2008 shared task on joint parsing of syntactic and semantic dependencies", "authors": ["Mihai Surdeanu", "Richard Johansson", "Adam Meyers", "Llu\u0131\u0301s M\u00e0rquez", "Joakim Nivre"], "venue": "In Proceedings of the Twelfth Conference on Computational Natural", "year": 2008}, {"title": "An introduction to conditional random fields for relational learning", "authors": ["Charles Sutton", "Andrew McCallum."], "venue": "Introduction to statistical relational learning, pages 93\u2013128.", "year": 2006}, {"title": "Introduction to the conll-2003 shared task: Language-independent named entity recognition", "authors": ["Erik F Tjong Kim Sang", "Fien De Meulder."], "venue": "Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003-Volume 4,", "year": 2003}, {"title": "A joint chinese named entity recognition and disambiguation system", "authors": ["Longyue Wang", "Shuo Li", "Derek F Wong", "Lidia S Chao."], "venue": "CLP 2012, page 146.", "year": 2012}, {"title": "Towards a top-down and bottom-up bidirectional approach to joint information extraction", "authors": ["Xiaofeng Yu", "Irwin King", "Michael R Lyu."], "venue": "Proceedings of the 20th ACM international conference on Information and knowledge management, pages 847\u2013", "year": 2011}], "id": "SP:e1b11add582bed8cc77ac667139f56feb87f4a7e", "authors": [{"name": "Gang Luo", "affiliations": []}, {"name": "Xiaojiang Huang", "affiliations": []}, {"name": "Chin-Yew Lin", "affiliations": []}, {"name": "Zaiqing Nie", "affiliations": []}], "abstractText": "Extracting named entities in text and linking extracted names to a given knowledge base are fundamental tasks in applications for text understanding. Existing systems typically run a named entity recognition (NER) model to extract entity names first, then run an entity linking model to link extracted names to a knowledge base. NER and linking models are usually trained separately, and the mutual dependency between the two tasks is ignored. We propose JERL, Joint Entity Recognition and Linking, to jointly model NER and linking tasks and capture the mutual dependency between them. It allows the information from each task to improve the performance of the other. To the best of our knowledge, JERL is the first model to jointly optimize NER and linking tasks together completely. In experiments on the CoNLL\u201903/AIDA data set, JERL outperforms state-of-art NER and linking systems, and we find improvements of 0.4% absolute F1 for NER on CoNLL\u201903, and 0.36% absolute precision@1 for linking on AIDA.", "title": "Joint Named Entity Recognition and Disambiguation"}