1	37	Therefore, co-occurring emotional expressions have been used for distant supervision in social media sentiment analysis and related tasks to make the models learn useful text representations before modeling these tasks directly.
3	28	Similarly, hashtags such as #anger, #joy, #happytweet, #ugh, #yuck and #fml have in previous research been mapped into emotional categories for emotion analysis (Mohammad, 2012).
4	32	Distant supervision on noisy labels often enables a model to obtain better performance on the target task.
6	23	We show that the learned representation of a single pretrained model generalizes across 5 domains.
9	24	I love mom's cooking 49.1% 8.8% 3.1% 3.0% 2.9% I love how you never reply back.. 14.0% 8.3% 6.3% 5.4% 5.1% I love cruising with my homies 34.0% 6.6% 5.7% 4.1% 3.8% I love messing with yo mind!!
10	42	17.2% 11.8% 8.0% 6.4% 5.3% I love you and now you're just gone.. 39.1% 11.0% 7.3% 5.3% 4.5% This is shit 7.0% 6.4% 6.0% 6.0% 5.8% This is the shit 10.9% 9.7% 6.5% 5.7% 4.8% Emojis are not always a direct labeling of emotional content.
11	45	For instance, a positive emoji may serve to disambiguate an ambiguous sentence or to complement an otherwise relatively negative text.
13	19	Nevertheless, our work shows that emojis can be used to classify the emotional content of texts accurately in many cases.
14	46	For instance, our DeepMoji model captures varied usages of the word ‘love’ as well as slang such as ‘this is the shit’ being a positive statement (see Table 1).
59	20	A hyperbolic tangent activation function is used to enforce a constraint of each embedding dimension being within [−1, 1].
69	27	We find that adding the attention mechanism and skipconnections improves the model’s capabilities for transfer learning (see §5.2 for more details).
74	93	One common approach is to use the network as a feature extractor (Donahue et al., 2014), where all layers in the model are frozen when fine-tuning on the target task except the last layer (hereafter referred to as the ‘last’ approach).
76	69	We propose a new simple transfer learning approach, ‘chain-thaw’, that sequentially unfreezes and fine-tunes a single layer at a time.
95	41	To evaluate performance on the pretraining task a validation set and a test set both containing 640K tweets (10K of each emoji type) are used.
142	84	On all datasets are our results statistically significantly better than the state of the art with p < 0.001.
148	36	For instance, both Deriu et al. (2016) and Tang et al. (2014) only used positive and negative emoticons as noisy labels.
152	34	We train our DeepMoji model to predict whether the tweets contain a positive or negative emoji and evaluate this pretrained model across the benchmark datasets.
157	53	Through hierarchical clustering on the correlation matrix of the DeepMoji model’s predictions on the test set we can see that the model captures many similarities that one would intuitively expect (see Figure 3).
164	29	The two architectures performed equally on the pretraining task, suggesting that while the DeepMoji model architecture is indeed better for transfer learning, it may not necessarily be better for single supervised classification task with ample available data.
169	24	These two effects help regularize the model by preventing overfitting (see the supplementary details for an visualization of the effect of this regularization).
170	67	There are numerous ways to express a specific sentiment, emotion or sarcastic comment.
176	20	Note that word coverage can be a misleading metric in this context as for many of these small datasets a word will often occur only once in the training set.
183	23	One concept that the LSTM layers likely learn is negation, which is known to be important for sentiment analysis (Wiegand et al., 2010).
188	59	Tweets where more than half of the evaluators chose ‘Do not know’ were removed (98 tweets).
189	27	For each tweet, we select a MTurk rating random to be the ‘human evaluation’, and average over the remaining nine MTurk ratings are averaged to form the ground truth.
195	41	Table 8 shows that the agreement of the random MTurk rater is 76.1%, meaning that the randomly selected rater will agree with the average of the nine other MTurk-ratings of the tweet’s polarity 76.1% of the time.
196	69	Our DeepMoji model achieves 82.4% agreement, which means it is better at capturing the average human sentiment-rating than a single MTurk rater.
197	56	We have shown how the millions of texts on social media with emojis can be used for pretraining models, thereby allowing them to learn representations of emotional content in texts.
198	77	Through comparison with an identical model pretrained on a subset of emojis, we find that the diversity of our emoji set is important for the performance of our method.
199	277	We release our pretrained DeepMoji model with the hope that other researchers will find good use of them for various emotion-related NLP tasks4.
