28	47	We follow in §3 by illustrating how these results can be directly used for tasks that require to output time series: averaging, clustering and prediction of time series.
30	50	We consider in what follows multivariate discrete time series of varying length taking values inΩ ⊂ Rp.
33	12	For an integer n we write JnK for the set {1, .
34	66	Given two series’ lengths n and m, we write An,m ⊂ {0, 1} n×m for the set of (binary) alignment matrices, that is paths on a n×mmatrix that connect the upper-left (1, 1) matrix entry to the lower-right (n,m) one using only ↓,→,ց moves.
36	62	We propose in this section a unified formulation for the original DTW discrepancy (Sakoe & Chiba, 1978) and the Global Alignment kernel (GAK) (Cuturi et al., 2007), which can be both used to compare two time series x = (x1, .
37	63	Given the cost matrix ∆(x,y) := [ δ(xi, yj) ] ij ∈ Rn×m, the inner product 〈A,∆(x,y) 〉 of that matrix with an alignment matrix A in An,m gives the score of A, as illustrated in Figure 2.
38	42	Both DTW and GAK consider the costs of all possible alignment matrices, yet do so differently: DTW(x,y) := min A∈An,m 〈A,∆(x,y) 〉, kγGA(x,y) := ∑ A∈An,m e−〈A,∆(x,y) 〉/γ .
49	15	Most importantly, and in either case, dtwγ can be computed using Algorithm 1, which requires (nm) operations and (nm) storage cost as well .
53	72	When considering dtw0, that change can be efficiently monitored only when the optimal alignment matrix A⋆ that arises when computing dtw0(x,y) in Eq.
56	42	, n do 5: ri,j = δ(xi, yj) +min γ{ri−1,j−1, ri−1,j , ri,j−1} 6: end for 7: end for 8: Output: (rn,m, R) average time series under the DTW metric (Petitjean et al., 2011; Schultz & Jain, 2017).
58	36	x, we only need to apply the chain rule, thanks to the differentiability of the cost function: ∇xdtw0(x,y) = ( ∂∆(x,y) ∂x )T A⋆, (2) where ∂∆(x,y)/∂x is the Jacobian of ∆ w.r.t.
59	17	When δ is the squared Euclidean distance, the transpose of that Jacobian applied to a matrix B ∈ Rn×m is (◦ being the elementwise product): (∂∆(x,y)/∂x)TB = 2 ( (1p1 T mB T ) ◦ x− yBT ) .
83	41	Algorithm 2 Backward recursion to compute∇x dtwγ(x,y) 1: Inputs: x,y, smoothing γ ≥ 0, distance function δ 2: (·, R) = dtwγ(x,y), ∆ = [δ(xi, yj)]i,j 3: δi,m+1 = δn+1,j = 0, i ∈ JnK, j ∈ JmK 4: ei,m+1 = en+1,j = 0, i ∈ JnK, j ∈ JmK 5: ri,m+1 = rn+1,j = −∞, i ∈ JnK, j ∈ JmK 6: δn+1,m+1 = 0, en+1,m+1 = 1, rn+1,m+1 = rn,m 7: for j = m, .
99	68	Therefore dtwγ will only be concave if δ is concave, or become instead a (non-convex) (soft) minimum of convex functions if δ is convex.
100	42	When δ is a squared-Euclidean distance, dtw0 is a piecewise quadratic function of x, as is also the case with the k-means energy (see for instance Figure 2 in Schultz & Jain 2017).
104	43	Indeed, notice that dtwγ converges to the sum of all costs as γ → ∞.
105	29	Therefore, if δ is convex, dtwγ will gradually become convex as γ grows.
106	31	For smaller values of γ, one can intuitively foresee that using minγ instead of a minimum will smooth out local minima and therefore provide a better (although slightly different from dtw0) optimization landscape.
107	36	We believe this is why our approach recovers better results, even when measured in the original dtw0 discrepancy, than subgradient or alternating minimization approaches such as DBA (Petitjean et al., 2011), which can, on the contrary, get more easily stuck in local minima.
108	21	Evidence for this statement is presented in the experimental section.
111	18	,xk that minimize the following energy: min x1,...,xk∈Rp×n N∑ i=1 1 mi min j∈[[k]] dtwγ(xj ,yi).
119	55	Although very simple, this method was shown to be competitive with k-NN, while requiring much lower computational cost at prediction time (Petitjean et al., 2014).
120	32	Soft-DTW can naturally be used in a nearest centroid classifier, in order to compute the barycenter of each class at train time, and to compute the discrepancy between barycenters and time series, at prediction time.
121	30	Soft-DTW is ideally suited as a loss function for any task that requires time series outputs.
122	99	As an example of such a task, we consider the problem of, given the first 1, .
124	61	Let xt,t ′ ∈ Rp×(t ′−t+1) be the submatrix of x ∈ Rp×n of all columns with indices between t and t′, where 1 ≤ t < t′ < n. Learning to predict the segment of a time series can be cast as the problem min θ∈Θ N∑ i=1 dtwγ ( fθ(x 1,t i ),x t+1,n i ) , where {fθ} is a set of parameterized function that take as input a time series and outputs a time series.
127	16	We use a subset containing 79 datasets encompassing a wide variety of fields (astronomy, geology, medical imaging) and lengths.
132	11	For each dataset, we choose a class at random, pick 10 time series in that class and compute their barycenter.
135	29	To minimize the proposed soft-DTW barycenter objective, Eq.
137	58	Figure 5 shows barycenters obtained using random initialization on the ECG200 dataset.
