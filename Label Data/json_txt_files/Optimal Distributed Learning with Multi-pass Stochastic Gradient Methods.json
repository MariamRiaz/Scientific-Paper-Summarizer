{"sections": [{"heading": "1. Introduction", "text": "In statistical learning theory, a set of N input-output pairs from an unknown distribution is observed. The aim is to learn a function which can be used to predict future outputs given the corresponding inputs. The quality of a predictor is often measured in terms of the mean-squared error. In this case, the conditional mean, which is called as the regression function, is optimal among all the measurable functions (Cucker & Zhou, 2007; Steinwart & Christmann, 2008). In nonparametric regression problems, the properties of the function to be estimated are not known a priori. Nonparametric approaches, which can adapt their complexity to the problem at hand, are key to good results. Kernel methods is one of the most common nonparametric approaches to learning (Scho\u0308lkopf & Smola, 2002; Shawe-Taylor & Cristianini, 2004). It is based on choosing a RKHS as the hypothesis space in the design of learning algorithms. With an appropriate reproducing kernel, RKHS can be used to approximate any smooth function.\n1 Laboratory for Information and Inference Systems, E\u0301cole Polytechnique Fe\u0301de\u0301rale de Lausanne, Lausanne, Switzerland. Correspondence to: Junhong Lin <junhong.lin@epfl.ch>, Volkan Cevher <volkan.cevher@epfl.ch>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nThe classical algorithms to perform learning task are regularized algorithms, such as KRR, kernel principal component regression (KPCR), and more generally, spectral regularization algorithms (SRA). From the point of view of inverse problems, such approaches amount to solving an empirical, linear operator equation with the empirical covariance operator replaced by a regularized one (Engl et al., 1996; Bauer et al., 2007; Gerfo et al., 2008). Here, the regularization term is used for controlling the complexity of the solution to against over-fitting and for ensuring best generalization ability. Statistical results on generalization error had been developed in (Smale & Zhou, 2007; Caponnetto & De Vito, 2007) for KRR and in (Caponnetto, 2006; Bauer et al., 2007) for SRA. Another type of algorithms to perform learning tasks is based on iterative procedure (Engl et al., 1996). In this kind of algorithms, an empirical objective function is optimized in an iterative way with no explicit constraint or penalization, and the regularization against overfitting is realized by early-stopping the empirical procedure. Statistical results on generalization error and the regularization roles of the number of iterations/passes have been investigated in (Zhang & Yu, 2005; Yao et al., 2007) for gradient methods (GM, also known as Landweber algorithm in inverse problems), in (Caponnetto, 2006; Bauer et al., 2007) for accelerated gradient methods (AGM, known as \u03bd-methods in inverse problems) in (Blanchard & Kra\u0308mer, 2010) for conjugate gradient methods (CGM), and in (Lin & Rosasco, 2017b) for (multi-pass) SGM. Statistical results have been well studied for these algorithms; however, these algorithms suffer from computational burdens at least of order O(N2) due to the nonlinearity of kernel methods, where N is the sample size. Indeed, a standard execution of KRR requiresO(N2) in space andO(N3) in time, while SGM after T -iterations requires O(N) in space and O(NT ) (or T 2) in time. Such approaches would be prohibitive when dealing with large-scale learning problems, especially in the case where data cannot be stored on a single machine. These thus motivate one to study distributed learning algorithms (Mcdonald et al., 2009; Zhang et al., 2012). The basic idea of distributed learning is very simple: randomly divide a dataset of size N into m subsets of equal size, compute an independent estimator using a fixed algorithm on each subset, and then average the local solutions into a global predictor. Interestingly, distributed\nlearning technique has been successfully combined with KRR (Zhang et al., 2015; Lin et al., 2017) and more generally, SRA (Guo et al., 2017; Blanchard & Mu\u0308cke, 2016), and it has been shown that statistical results on generalization error can be retained provided that the number of partitioned subsets is not too large. Moreover, it was highlighted (Zhang et al., 2015) that distributed KRR not only allows one to handle large datasets that restored on multiple machines, but also leads to a substantial reduction in computational complexity versus the standard approach of performing KRR on all N samples. In this paper, we study distributed SGM, with multi-passes over the data and mini-batches. The algorithm is a combination of distributed learning technique and (multi-pass) SGM: it randomly partitions a dataset of size N into m subsets of equal size, computes an independent estimator by SGM for each subset, and then averages the local solutions into a global predictor. It has several free parameters: step-size, mini-batch size, total number of iterations and partition level m . We show that with appropriate choices of algorithmic parameters, optimal generalization error bounds can be achieved provided that the partition level m is not too large. The proposed configuration has certain advantages on computational complexity. For example, without considering any benign properties of the studied problem such as the regularity of the regression function (Smale & Zhou, 2007; Caponnetto & De Vito, 2007) and a capacity assumption on the RKHS (Zhang, 2005; Caponnetto & De Vito, 2007), even implementing on a single machine, distributed SGM has an optimal convergence rate of order O(N\u22121/2), with a computational complexity O(N) in space and O(N3/2) in time, compared with O(N) in space and O(N2) in time of classic SGM performing on all N samples, or O(N3/2) in space andO(N2) in time of distributed KRR. Moreover, the approach dovetails naturally with parallel and distributed computation: we are guaranteed a superlinear speedup with m parallel processors (though we must still communicate the function estimates from each processor). The proof of the main results is based on a similar error decomposition from (Lin & Rosasco, 2017b), which decomposes the excess risk into three terms: bias, sample and computational variance. The error decomposition allows one to study distributed GM and distributed SGM simultaneously. Different to those in (Lin & Rosasco, 2017b) which rely heavily on the intrinsic relationship of GM with the square loss, in this paper, an integral operator approach (Smale & Zhou, 2007; Caponnetto & De Vito, 2007) is used, combining with some novel and refined analysis. As a byproduct, we derive optimal statistical results on generalization error for nondistributed SGM, which improve on the results in (Lin & Rosasco, 2017b). Note also that we can extend our analysis to distributed SRA, and get better statistical results than those from (Zhang et al., 2015; Guo et al., 2017). We report\nthese results in a longer version of this paper (Lin & Cevher, 2018). The remainder of the paper is organized as follows. Section 2 introduces the supervised learning setting. Section 3 describes distributed SGM and its numerical realization, and then presents theoretical results on generalization error for distributed SGM, following with simple comments and discussions. Section 4 discusses and compares our results with related work. Proofs for distributed SGM and auxiliary lemmas are provided in the appendix."}, {"heading": "2. Supervised Learning Problems", "text": "We consider a supervised learning problem. Let \u03c1 be a probability measure on a measure space Z = X \u00d7Y, where X is the input space and Y \u2286 R is the output space. Here, \u03c1 is fixed but unknown, with its marginal distribution on X denoted by \u03c1X and its conditional distribution on Y given x \u2208 X denoted by \u03c1(\u00b7|x). Its information can be only known through a set of samples z\u0304 = {zi = (xi, yi)}Ni=1 of N \u2208 N points, which we assume to be i.i.d.. The quality of a predictor f : X \u2192 Y can be measured in terms of the expected risk with a square loss defined as\nE(f) = \u222b Z (f(x)\u2212 y)2d\u03c1(z). (1)\nIn this case, the function minimizing the expected risk over all measurable functions is the regression function given by\nf\u03c1(x) = \u222b Y yd\u03c1(y|x), x \u2208 X. (2)\nThe performance of an estimator f \u2208 L2\u03c1X can be measured in terms of generalization error (excess risk), i.e., E(f) \u2212 E(f\u03c1). It is easy to prove that\nE(f)\u2212 E(f\u03c1) = \u2016f \u2212 f\u03c1\u20162\u03c1. (3)\nHere, L2\u03c1X is the Hilbert space of square integral functions with respect to \u03c1X , with its induced norm given by \u2016f\u2016\u03c1 = \u2016f\u2016L2\u03c1X = (\u222b X |f(x)|2d\u03c1X )1/2 . For any t \u2208 N+, the set {1, \u00b7 \u00b7 \u00b7 , t} is denoted by [t]. Kernel methods are based on choosing the hypothesis space as a RKHS. Recall that a reproducing kernel K is a symmetric function K : X \u00d7 X \u2192 R such that (K(ui, uj)) ` i,j=1 is positive semidefinite for any finite set of points {ui}`i=1 in X . The reproducing kernel K defines a RKHS (H, \u2016 \u00b7 \u2016H) as the completion of the linear span of the set {Kx(\u00b7) := K(x, \u00b7) : x \u2208 X} with respect to the inner product \u3008Kx,Ku\u3009H := K(x, u). Given only the samples z\u0304, the goal is to learn the regression function f\u03c1 through efficient learning algorithms."}, {"heading": "3. Distributed Learning with Stochastic Gradient Methods", "text": "In this section, we first state distributed SGM and discuss its numerical realization. We then present theoretical results on generalization properties for distributed SGM and nondistributed SGM, following with simple discussions."}, {"heading": "3.1. Distributed SGM and Numerical Realization", "text": "Throughout this paper, as that in (Zhang et al., 2015), we assume that1 the sample size N = mn for some positive integers n,m, and we randomly decompose z\u0304 as z1 \u222a z2 \u222a \u00b7 \u00b7 \u00b7 \u222a zm with |z1| = |z2| = \u00b7 \u00b7 \u00b7 = |zm| = n. For any s \u2208 [m], we write zs = {(xs,i, ys,i)}ni=1. We study the following distributed SGM, with mini-batches and multipass over the data.\nAlgorithm 1. Let b \u2208 [n]. The b-minibatch stochastic gradient methods over the sample zs is defined by fs,1 = 0 and for all t \u2208 [T ],\nfs,t+1 = fs,t\u2212\u03b7t 1\nb bt\u2211 i=b(t\u22121)+1 (fs,t(xs,js,i)\u2212ys,js,i)Kxs,js,i ,\n(4) where {\u03b7t > 0} is a step-size sequence. Here, js,1, js,2, \u00b7 \u00b7 \u00b7 , js,bT are i.i.d. random variables from the uniform distribution on [n].2 The global predictor averaging over these local estimators is given by\nf\u0304t = 1\nm m\u2211 s=1 fs,t.\nIn the above algorithm, at each iteration t, for each s \u2208 [m], the local estimator updates its current solution by subtracting a scaled gradient estimate. It is easy to see that the gradient estimate at each iteration for the s-th local estimator is an unbiased estimate of the full gradient of the empirical risk over zs. The global predictor is the average over these local solutions. In the special case m = 1, the algorithm reduces to the classic multi-pass SGM. There are several free parameters in the algorithm, the stepsize \u03b7t, the mini-batch size b, the total number of iterations/passes, and the number of partition/subsets m. All these parameters will affect the algorithm\u2019s generalization properties and computational complexity. In the coming subsection, we will show how these parameters can be chosen so that the algorithm can generalize optimally, as long as the number of subsets m is not too large. Different choices\n1For the general case, one can consider the weighted averaging scheme, as that in (Lin et al., 2017), and our analysis still applies with a simple modification.\n2Note that the random variables js,1, \u00b7 \u00b7 \u00b7 , js,bT are conditionally independent given the sample zs.\non \u03b7t, b, and T correspond to different regularization strategies. In this paper, we are particularly interested in the cases that both \u03b7t and b are fixed as some universal constants that may depend on the local sample size n, while T is tuned. The total number of iterations T for each local estimator can be bigger than the local sample size n, which means that the algorithm can use the data more than once, or in another words, we can run the algorithm with multiple passes over the data. Here and in what follows, the number of (effective) \u2018passes\u2019 over the data is referred to btn after t iterations of the algorithm. For any finite subsets x and x\u2032 in X , denote the |x| \u00d7 |x\u2032| kernel matrix [K(x, x\u2032)]x\u2208x,x\u2032\u2208x\u2032 by Kxx\u2032 . Obviously, using an inductive argument, one can prove that Algorithm 1 is equivalent to\nf\u0304t = 1\nm m\u2211 s=1 n\u2211 i=1 bs,t(i)Kxs,i ,\nwhere for all s \u2208 [m], bs,t = [bs,t(1), \u00b7 \u00b7 \u00b7 ,bs,t(n)]> \u2208 Rn and it is generated by, with bs,1 = 0 \u2208 Rn, for all t \u2208 [T ],\nbs,t+1 = bs,t\u2212 \u03b7t b bt\u2211 i=b(t\u22121)+1 (b>s,tKxsxs,js,i\u2212ys,js,i)ejs,i . (5) Here, e1, \u00b7 \u00b7 \u00b7 , en are standard basis of Rn. The space and time complexities for each local estimator are\nO(n) and O(bnT ), (6)\nrespectively. The total space and time complexities of the algorithm are\nO(N) and O(bNT ), respectively. (7)\nIn order to see the empirical performance of the studied algorithm, we carried out some numerical simulations on a non-parametric regression problem with simulated datasets. We constructed a training data {(xi, yi)}Ni=1 \u2286 R\u00d7 R with N = 212 from the regression model y = f\u03c1(x) + \u03be, where the regression function f\u03c1(x) = |x \u2212 1/2| \u2212 1/2, the input x is uniformly drawn from [0, 1], and \u03be is a Gaussian noise with zero mean and standard deviation 1. In all the simulations, the RKHS is associated with a Gaussian kernel K(x, x\u2032) = exp(\u2212 |x\u2212x\n\u2032|2 2\u03c32 ) where \u03c3 = 0.2, and the\nmini-batch size b = 1. For each number of partitions m \u2208 {2, 8, 32, 64}, we set the step-size as \u03b7t = 18n as that suggested by Part 1) of Corollary 2 in the coming subsection3, and executed simulation 50 times. In each trial, an approximated generalization error is computed over an\n3It would be interesting to run the algorithm with other stepsizes suggested by Corollary 2.\nempirical measure with 1000 points. The mean and the standard deviation of these computed generalization errors over 50 trials with respect to the number of passes are depicted in the above figures. As we can see from the figures, distributed SGM performs well, and after some number of passes, it achieves the minimal (approximated) generalization error. As the number of subsets m increases, the error and the number of passes to reach minimal error will also slightly increase. Note that the computational cost for n iteration (one pass) of the global estimator is O(N2/m). Thus the total computational cost for the algorithm to reach minimal error would be reduced if one enlarges the number of partition m. Finally, the accuracy is comparable with 0.809\u00d7 10\u22123 of KRR with cross validation."}, {"heading": "3.2. Generalization Properties for Distributed Stochastic Gradient Methods", "text": "In this section, we state our theoretical results on generalization error for distributed SGM, following with simple discussions. To do so, we need to introduce some basic assumptions. Throughout this paper, we make the following\ntwo basic assumptions. Assumption 1. H is separable, K is measurable and furthermore, there exists a constant \u03ba \u2208 [1,\u221e[, such that for all x \u2208 X, K(x, x) \u2264 \u03ba2. (8) Assumption 2. For some M,\u03c3 \u2265 0,\u222b\nY\ny2d\u03c1(y|x) \u2264M, and\n\u222b Y (f\u03c1(x)\u2212 y)2d\u03c1(y|x) \u2264 \u03c32, almost surely. (9)\nThe above two assumptions are quite common in statistical learning theory, see, e.g., (Steinwart & Christmann, 2008; Cucker & Zhou, 2007). The constant \u03c3 from Equation (9) measures the noise level of the studied problem. The condition \u222b Y y2d\u03c1(y|x) \u2264 M implies that the regression function is bounded almost surely,\n|f\u03c1(x)| \u2264M. (10)\nIt is trivially satisfied when the output domain Y is bounded, for example, Y = {\u22121, 1} in the classification problem.\nCorollary 1. Assume that f\u03c1 \u2208 H and\nm \u2264 N\u03b2 , 0 \u2264 \u03b2 < 1 2 .\nConsider Algorithm 1 with any of the following choices on \u03b7t, b and T . 1) \u03b7t ' m/ \u221a N for all t \u2208 [T\u2217], b = 1, and T\u2217 ' N/m.\n2) \u03b7t ' 1logN for all t \u2208 [T\u2217], b ' \u221a N/m, and T\u2217 '\u221a\nN logN. Then,\nEE(f\u0304T\u2217+1)\u2212 E(f\u03c1) . N\u22121/2 logN.\nHere, we use the notations a1 . a2 to mean a1 \u2264 Ca2 for some positive constant C which is depending only on (a polynomial function) \u03ba,M, \u03c3, \u2016T \u2016, \u2016f\u03c1\u2016H , and a1 ' a2 to mean a2 . a1 . a2.\nThe above result provides generalization error bounds for distributed SGM with two different choices on step-size \u03b7t, mini-batch size b and total number of iterations/passes. The convergence rate is optimal up to a logarithmic factor, in the sense that it matches the minimax rate in (Caponnetto & De Vito, 2007) and the convergence rate for KRR (Smale & Zhou, 2007; Caponnetto & De Vito, 2007). The number of passes to achieve optimal error bounds in both cases is roughly one. The above result asserts that distributed SGM generalizes optimally after one pass over the data for two different choices on step-size and mini-batch size, provided that the partition level m is not too large. In the case that m ' \u221a N, according to (7), the computational complexities are O(N) in space and O(N1.5) in time, comparing with O(N) in space and O(N2) in time of classic SGM. Corollary 1 provides statistical results on generalization error bounds with a convergence rate of order O(N\u22121/2 logN) for distributed SGM. It does not consider any benign assumptions about the learning problem, such as the regularity of the regression function and the capacity of the RKHS. In what follows, we will show how the convergence rate can be further improved, if we make two benign assumptions of the learning problem. The first benign assumption relates to the regularity of the regression function. We introduce the integer operator L : L2\u03c1X \u2192 L 2 \u03c1X , defined by Lf = \u222b X f(x)K(x, \u00b7)d\u03c1X . Under Assumption (8), L is positive and trace-class, and hence L\u03b6 is well defined using the spectral theory. Assumption 3. There exist \u03b6 > 0 and R > 0, such that \u2016L\u2212\u03b6f\u03c1\u2016\u03c1 \u2264 R.\nThis assumption characterizes how large the subspace that the regression function lies in. The bigger the \u03b6 is, the smaller the subspace is, the stronger the assumption is, and the easier the learning problem is, as L\u03b61(L2\u03c1X ) \u2286 L\u03b62(L2\u03c1X ) if \u03b61 \u2265 \u03b62. Moreover, if \u03b6 = 0, we are making no assumption, and if \u03b6 = 12 , we are requiring that there\nexists some f\u2217 \u2208 H such that fH = f\u03c1 almost surely (Steinwart & Christmann, 2008). The next assumption relates to the capacity of the hypothesis space.\nAssumption 4. For some \u03b3 \u2208 [0, 1] and c\u03b3 > 0, L satisfies\ntr(L(L+ \u03bbI)\u22121) \u2264 c\u03b3\u03bb\u2212\u03b3 , for all \u03bb > 0. (11)\nThe left hand-side of (11) is called effective dimension (Zhang, 2005) or degrees of freedom (Caponnetto & De Vito, 2007). It is related to covering/entropy number conditions, see (Steinwart & Christmann, 2008). The condition (11) is naturally satisfied with \u03b3 = 1, since L is a trace class operator which implies that its eigenvalues {\u03c3i}i satisfy \u03c3i . i\u22121. Moreover, if the eigenvalues of L satisfy a polynomial decaying condition \u03c3i \u223c i\u2212c for some c > 1, or if L is of finite rank, then the condition (11) holds with \u03b3 = 1/c, or with \u03b3 = 0. The case \u03b3 = 1 is refereed as the capacity independent case. A smaller \u03b3 allows deriving faster convergence rates for the studied algorithms, as will be shown in the following results. Making these two assumptions, we have the following general results on generalization error for the studied algorithms.\nTheorem 1. Under Assumptions 3 and 4, let \u03b6 \u2264 1 and \u03b7t = \u03b7 for all t \u2208 [T ] with \u03b7 satisfying\n0 < \u03b7 \u2264 1 4\u03ba2 log T . (12)\nThen for all t \u2208 [T ] and any \u03bb\u0303 = n\u03b8\u22121 with \u03b8 \u2208 [0, 1],\nEE(f\u0304t+1)\u2212 E(f\u03c1) . [ 1\n(\u03b7t)2\u03b6 +\n1\nN\u03bb\u0303\u03b3 +\n\u03b7\nmb ]\n\u00d7 ((\u03bb\u0303\u03b7t)2 \u2228 [\u03b3(\u03b8\u22121 \u2227 log n)]2\u03b6\u22281 \u2228 1 \u2228 log t). (13)\nHere and throughout the rest of this paper, we use the notation a1 . a2 to mean a1 \u2264 Ca2 for some positive constantC which is depending only on \u03ba,M, \u03b6,R, \u03b3, C\u03b3 , \u03c3 and \u2016T \u2016.\nIn the above result, we only consider the setting of a fixed step-size. Results with a decaying step-size can be directly derived following our proofs in the coming sections, combining with some basic estimates from (Lin & Rosasco, 2017b). The derived error bound from (13) depends on the number of iteration t, the step-size \u03b7, the mini-batch size, the number of sample points N and the partition level m. It holds for any pseudo regularization parameter \u03bb\u0303 where \u03bb\u0303 \u2208 [n\u22121, 1]. When t \u2264 n/\u03b7, we can choose \u03bb\u0303 = (\u03b7t)\u22121, and ignoring the logarithmic factor, (13) reads as\nEE(f\u0304t+1)\u2212 E(f\u03c1) . 1\n(\u03b7t)2\u03b6 +\n(\u03b7t)\u03b3\nN +\n\u03b7\nmb . (14)\nThe right-hand side of the above inequality is composed of three terms. The first term is related to the regularity parameter \u03b6 of the regression function f\u03c1, and it results from estimating bias. The second term depends on the sample size N, and it results from estimating sample variance. The last term results from estimating computational variance due to random choices of the sample points. In comparing with the error bounds derived for classic SGM performed on a local machine, one can see that averaging over the local solutions can reduce sample and computational variances, but keeps bias unchanged. As the number of iteration t increases, the bias term decreases, and the sample variance term increases. This is a so-called trade-off problem in statistical learning theory. Solving this trade-off problem leads to the best choice on number of iterations. Notice that the computational variance term is independent of the number of iterations t and it depends on the step-size, the mini-batch size, and the partition level. To derive optimal rates, it is necessary to choose a small step-size, and/or a large minibatch size, and a suitable partition level. In what follows, we provide different choices of these algorithmic parameters, corresponding to different regularization strategies, while leading to the same optimal convergence rates. Corollary 2. Under Assumptions 3 and 4, let \u03b6 \u2264 1, 2\u03b6 + \u03b3 > 1 and\nm \u2264 N\u03b2 , with 0 \u2264 \u03b2 < 2\u03b6 + \u03b3 \u2212 1 2\u03b6 + \u03b3 . (15)\nConsider Algorithm 1 with any of the following choices on \u03b7t, b and T . 1) \u03b7t ' n\u22121 for all t \u2208 [T\u2217], b = 1, and T\u2217 ' N 1 2\u03b6+\u03b3 n. 2) \u03b7t ' n\u22121/2 for all t \u2208 [T\u2217], b ' \u221a n, and T\u2217 ' N 1 2\u03b6+\u03b3 \u221a n. 3) \u03b7t ' N\u2212 2\u03b6\n2\u03b6+\u03b3m for all t \u2208 [T\u2217], b = 1, and T\u2217 ' N 2\u03b6+1 2\u03b6+\u03b3 /m. 4) \u03b7t ' 1logN for all t \u2208 [T\u2217], b ' N 2\u03b6 2\u03b6+\u03b3 /m, and T\u2217 ' N 1 2\u03b6+\u03b3 logN. Then, EE(f\u0304T\u2217+1)\u2212 E(f\u03c1) . N \u2212 2\u03b62\u03b6+\u03b3 logN.\nWe add some comments on the above theorem. First, the convergence rate is optimal, as it is the same as that for KRR from (Caponnetto & De Vito, 2007; Smale & Zhou, 2007) and also it matches the minimax rate in (Caponnetto & De Vito, 2007), up to a logarithmic factor. Second, distributed SGM saturates when \u03b6 > 1. The reason for this is that averaging over local solutions can only reduce sample and computational variances, not bias. Similar saturation phenomenon is also observed when analyzing distributed KRR in (Zhang et al., 2015; Lin et al., 2017). Third, the condition 2\u03b6 + \u03b3 > 1 is equivalent to assuming that the learning problem can not be too difficult. We believe that such a condition is necessary for applying distributed learning technique to reduce computational costs, as there are no\nmeans to reduce computational costs if the learning problem itself is not easy. Fourth, as the learning problem becomes easier (corresponds to a bigger \u03b6), the faster the convergence rate is, and moreover the larger the number of partition m can be. Finally, different parameter choices leads to different regularization strategies. In the first two regimes, the step-size and the mini-batch size are fixed as some prior constants (which only depends on n), while the number of iterations depends on some unknown distribution parameters. In this case, the regularization parameter is the number of iterations, which in practice can be tuned by using crossvalidation methods. Besides, the step-size and the number of iterations in the third regime, or the mini-batch size and the number of iterations in the last regime, depend on the unknown distribution parameters, and they have some regularization effects. The above theorem asserts that distributed SGM with differently suitable choices of parameters can generalize optimally, provided the partition level m is not too large."}, {"heading": "3.3. Optimal Convergence for Multi-pass SGM on a Single Dataset", "text": "As a byproduct of our new analysis in the coming sections, we derive the following results for classic multi-pass SGM.\nTheorem 2. Under Assumptions 3 and 4, consider Algorithm 1 with m = 1 and any of the following choices on \u03b7t, b and T . 1) \u03b7t ' N\u22121 for all t \u2208 [T\u2217], b = 1, and T\u2217 ' N\u03b1+1. 2) \u03b7t ' N\u22121/2 for all t \u2208 [T\u2217], b ' \u221a N , and T\u2217 ' N\u03b1+1/2. 3) \u03b7t ' N\u22122\u03b6\u03b1 for all t \u2208 [T\u2217], b = 1, and T\u2217 ' N\u03b1(2\u03b6+1). 4) \u03b7t ' 1logN for all t \u2208 [T\u2217], b ' N\n2\u03b6\u03b1, and T\u2217 '"}, {"heading": "N\u03b1 log T.", "text": "Here, \u03b1 = 1(2\u03b6+\u03b3)\u22281 . Then,\nEE(f\u0304T\u2217+1)\u2212 E(f\u03c1) .\n{ N\u2212 2\u03b6 2\u03b6+\u03b3 logN, if 2\u03b6 + \u03b3 > 1;\nN\u22122\u03b6 logN, otherwise. (16)\nThe above results provide generalization error bounds for multi-pass SGM trained on a single dataset. The derived convergence rate is optimal in the minimax sense (Caponnetto & De Vito, 2007; Blanchard & Mu\u0308cke, 2017). Note that SGM does not have a saturation effect, and optimal convergence rates can be derived for any \u03b6 \u2208]0,\u221e]. Theorem 2 improves the result in (Lin & Rosasco, 2017b) in two aspects. First, the convergence rates are better than those (i.e., O(N\u2212 2\u03b6 2\u03b6+\u03b3 logN) if 2\u03b6 + \u03b3 \u2265 1 or O(N\u22122\u03b6 log4N) otherwise) from (Lin & Rosasco, 2017b). Second, the above theorem does not require the extra condition m \u2265 m\u03b4 made in (Lin & Rosasco, 2017b)."}, {"heading": "3.4. Error Decomposition", "text": "The key to our proof is an error decomposition. To introduce the error decomposition, we need to introduce two auxiliary sequences. The first auxiliary sequence is generated by distributed GM.\nAlgorithm 2. For any s \u2208 [m], the GM over the sample set zs is defined by gs,1 = 0 and for t = 1, \u00b7 \u00b7 \u00b7 , T,\ngs,t+1 = gs,t \u2212 \u03b7t 1\nn n\u2211 i=1 (gs,t(xs,i)\u2212 ys,i)Kxs,i , (17)\nwhere {\u03b7t > 0} is a step-size sequence given by Algorithm 1. The average estimator over these local estimators is given by\ng\u0304t = 1\nm m\u2211 s=1 gs,t. (18)\nThe second auxiliary sequence is generated by distributed pseudo GM as follows.\nAlgorithm 3. For any s \u2208 [m], the pseudo GM over the input set xs is defined by hs,1 = 0 and for t = 1, \u00b7 \u00b7 \u00b7 , T,\nhs,t+1 = hs,t\u2212\u03b7t 1\nn n\u2211 i=1 (hs,t(xs,i)\u2212f\u03c1(xs,i))Kxs,i , (19)\nwhere {\u03b7t > 0} is a step-size sequence given by Algorithm 1. The average estimator over these local estimators is given by\nh\u0304t = 1\nm m\u2211 s=1 hs,t. (20)\nNote that Algorithm (19) can not be implemented in practice, as f\u03c1(x) is unknown in general.\nFor any s \u2208 [m], using an inductive argument, one can prove that (Lin & Rosasco, 2017b)\nEJs|zs [fs,t] = gs,t. (21)\nHere EJs|zs (or abbreviated as EJs ) denotes the conditional expectation with respect to Js given zs. Similarly, using the definition of the regression function (2) and an inductive argument, one can also prove that\nEys [gs,t] = hs,t. (22)\nWith the above two equalities, we can prove the following error decomposition. We introduce the inclusion operator S\u03c1 : H \u2192 L2\u03c1X . Proposition 1. We have that for any t \u2208 [T ],\nEE(f\u0304t)\u2212 E(f\u03c1) = E\u2016S\u03c1h\u0304t \u2212 f\u03c1\u20162\u03c1 + E[\u2016S\u03c1(g\u0304t \u2212 h\u0304t)\u20162\u03c1] + E\u2016S\u03c1(f\u0304t \u2212 g\u0304t)\u20162\u03c1.\n(23)\nThe error decomposition is similar as the one given in (Lin & Rosasco, 2017b) for classic multi-pass SGM. There are three terms in the right-hand side of (23). The first term depends on the regularity of the regression function (Assumption 3) and it is called as bias. The second term depends on the noise level \u03c32 from (9) and it is called as sample variance. The last term is caused by the random estimates of the full gradients and it is called as computational variance. In the appendix, we will estimate these three terms separately. Total error bounds can be thus derived by substituting these estimates into the error decomposition."}, {"heading": "4. Discussion", "text": "We briefly review convergence results for SGM. SGM (Robbins & Monro, 1951) has been widely used in convex optimization and machine learning, see e.g. (Cesa-Bianchi et al., 2004; Nemirovski et al., 2009; Bottou et al., 2016) and references therein. In what follows, we will briefly recall some recent works on generalization error for nonparametric regression on a RKHS considering the square loss. We will use the term \u201conline learning algorithm\u201d (OL) to mean one-pass SGM, i.e, SGM that each sample can be used only once. Different variants of OL, either with or without regularization, have been studied. Most of them take the form\nft+1 = (1\u2212 \u03bbt)ft \u2212 \u03b7t(ft(xt)\u2212 yt)Kxt , t = 1 \u00b7 \u00b7 \u00b7 , N.\nHere, the regularization parameter \u03bbt could be zero (Zhang, 2004; Ying & Pontil, 2008), or a positive (Smale & Yao, 2006; Ying & Pontil, 2008) and possibly time-varying constant (Tarres & Yao, 2014). Particularly, (Tarres & Yao, 2014) studied OL with time-varying regularization parameters and convergence rate of order O(N\n\u22122\u03b6 2\u03b6+1 ) (\u03b6 \u2208 [ 12 , 1])\nin high probability was proved. (Ying & Pontil, 2008) studied OL without regularization and convergence rate of order O(N\u2212 2\u03b6 2\u03b6+1 ) in expectation was shown. Both convergence rates from (Ying & Pontil, 2008; Tarres & Yao, 2014) are capacity-independently optimal and they do not take the capacity assumption into account. Considering an averaging step (Polyak & Juditsky, 1992) and a proof technique motivated by (Bach & Moulines, 2013), (Dieuleveut & Bach, 2016) proved capacity-dependently optimal rate O(N\u2212 2\u03b6 (2\u03b6+\u03b3)\u22281 ) for OL in the case that \u03b6 \u2264 1. Recently, (Lin & Rosasco, 2017b) studied (multi-pass) SGM, i.e, Algorithm 1 with m = 1. They showed that SGM with suitable parameter choices, achieves convergence rate of order O(N\u2212 2\u03b1 (2\u03b1+\u03b3)\u22281 log\u03b2 N) with \u03b2 = 2 when 2\u03b1 + \u03b3 > 1 or \u03b2 = 4 otherwise, after some number of iterations. In comparisons, the derived results for SGM in Theorem 2 are better than those from (Lin & Rosasco, 2017b), and the convergence rates are the same as those from (Dieuleveut & Bach, 2016) for averaging OL when\n\u03b6 \u2264 1 and 2\u03b6 + \u03b3 \u2265 1. For the case 2\u03b6 + \u03b3 \u2264 1, the convergence rate O(N\u22122\u03b6(1 \u2228 logN\u03b3)) for SGM in Theorem 2 is worser than O(N\u22122\u03b6) in (Dieuleveut & Bach, 2016) for averaging OL. However, averaging OL saturates for \u03b6 > 1, while SGM does not. To meet the challenge of large-scale learning, a line of research focus on designing learning algorithms with Nystro\u0308m subsampling, or more generally sketching. Interestingly, the latter has also been applied to compressed sensing, low rank matrix recovery and kernel methods, see e.g. (Cande\u0300s et al., 2006; Yurtsever et al., 2017; Yang et al., 2012) and references therein. The basic idea of Nystro\u0308m subsampling is to replace a standard large matrix with a smaller matrix obtained by subsampling (Smola & Scho\u0308lkopf, 2000; Williams & Seeger, 2000). For kernel methods, Nystro\u0308m subsampling has been successfully combined with KRR (Alaoui & Mahoney, 2015; Yang et al., 2015; Rudi et al., 2015) and SGM (Lu et al., 2016; Lin & Rosasco, 2017a). Generalization error bounds of order O(N \u22122\u03b6 2\u03b6+\u03b3 ) (Rudi et al., 2015; Lin & Rosasco, 2017a) were derived, provided that the subsampling level is suitably chosen, considering the case \u03b6 \u2208 [ 12 , 1]. Computational advantages of these algorithms were highlighted. Here, we summarize their computational costs in Table 1, from which we see that distributed SGM has advantages on both memory and time. Another line of research for large-scale learning focus on distributed (parallelizing) learning. Distributed learning, based on a divide-and-conquer approach, has been used for, e.g., perceptron-based algorithms (Mcdonald et al., 2009), parametric smooth convex optimization problems (Zhang et al., 2012), and sparse regression (Lee et al., 2017). Recently, this approach has been successfully applied to learning algorithms with kernel methods, such as KRR (Zhang et al., 2015), and SRA (Guo et al., 2017; Blanchard & Mu\u0308cke, 2017). (Zhang et al., 2015) first studied distributed KRR and showed that distributed KRR retains optimal rates O(N\u2212\n2\u03b6 2\u03b6+\u03b3 ) (for \u03b6 \u2208 [ 12 , 1]) provided the partition level is\nnot too large. The number of partition to retain optimal rate shown in (Zhang et al., 2015) for distributed KRR depends on some conditions which may be less well understood and thus potentially leads to a suboptimal partition number. (Lin et al., 2017) provided an alternative and refined analysis for distributed KRR, leading to a less strict condition on the partition number. (Guo et al., 2017) extended the analysis to distributed SRA, an proved optimal convergence rate for the case \u03b6 \u2265 1/2, if the number of partitions m \u2264 N 2\u03b6\u22121 2\u03b6+\u03b3 . In comparison, the condition on partition number from Corollary 2 for distributed SGM is less strict. Moreover, Corollary 2 shows that distributed SGM can retain optimal rate even in the non-attainable case. According to Corollary 2, distributed SGM with appropriate choices of parameters can achieve optimal rate if the partition number is not too large. In comparison of the derived results for distributed KRR\nNote: 1) For AveOL and DSGM, 2\u03b6 + \u03b3 > 1. 2) The costs here for the distributed algorithms\nare the costs of running the distributed algorithms on a single machine.\nwith those for distributed SGM, we see from Table 1 that the latter has advantages on both memory and time. The most related to our works are (Zinkevich et al., 2010; Jain et al., 2016). (Zinkevich et al., 2010) studied distributed OL for optimization problems over a finite-dimensional domain, and proved convergence results assuming that the objective function is strongly convex. (Jain et al., 2016) considered distributed OL with averaging for least square regression problems over a finite-dimension space and proved certain convergence results that may depend on the smallest eigenvalue of the covariance matrix. These results do not apply to our cases, as we consider distributed multi-pass SGM for nonparametric regression over a RKHS and our objective function is not strongly convex. We finally remark that using a partition approach (Thomann et al., 2016; Tandon et al., 2016), one can also scale up the kernel methods, with a computational advantage similar as those of using distributed learning technique. We conclude this section with some further questions. First, in this paper, we assume that all parameter choices are given priorly. In practice, these parameters can be possibly tuned by cross-validation method. Second, the derived rate for SGM in the case 2\u03b6 + \u03b3 \u2264 1 is O(N\u22122\u03b6(1 \u2228 logN\u03b3)), which is worser thanO(N\u22122\u03b6) of averaging OL (Dieuleveut & Bach, 2016). It would be interesting to improve the rate, or to derive a minimax rate for the case 2\u03b6 + \u03b3 \u2264 1. Third, all results stated in this paper are in expectation, and it would be interesting to derive high-probability results in the future (and possibly by a proof technique from (London, 2017))."}, {"heading": "Acknowledgements", "text": "This work was sponsored by the Department of the Navy, Office of Naval Research (ONR) under a grant number N62909-17-1-2111. It has also received funding from Hasler Foundation Program: Cyber Human Systems (project number 16066), and from the European Research Council (ERC) under the European Unions Horizon 2020 research and innovation program (grant agreement n 725594-timedata)."}], "year": 2018, "references": [{"title": "Fast randomized kernel ridge regression with statistical guarantees", "authors": ["Alaoui", "Ahmed", "Mahoney", "Michael W"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2015}, {"title": "Non-strongly-convex smooth stochastic approximation with convergence rate O(1/n)", "authors": ["Bach", "Francis", "Moulines", "Eric"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2013}, {"title": "On regularization algorithms in learning theory", "authors": ["Bauer", "Frank", "Pereverzev", "Sergei", "Rosasco", "Lorenzo"], "venue": "Journal of Complexity,", "year": 2007}, {"title": "Optimal learning rates for kernel conjugate gradient regression", "authors": ["Blanchard", "Gilles", "Kr\u00e4mer", "Nicole"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2010}, {"title": "Parallelizing spectral algorithms for kernel learning", "authors": ["Blanchard", "Gilles", "M\u00fccke", "Nicole"], "venue": "arXiv preprint arXiv:1610.07487,", "year": 2016}, {"title": "Optimal rates for regularization of statistical inverse learning problems", "authors": ["Blanchard", "Gilles", "M\u00fccke", "Nicole"], "venue": "Foundations of Computational Mathematics,", "year": 2017}, {"title": "Optimization methods for large-scale machine learning", "authors": ["Bottou", "Leon", "Curtis", "Frank E", "Nocedal", "Jorge"], "venue": "arXiv preprint arXiv:1606.04838,", "year": 2016}, {"title": "Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information", "authors": ["Cand\u00e8s", "Emmanuel J", "Romberg", "Justin", "Tao", "Terence"], "venue": "IEEE Transactions on Information theory,", "year": 2006}, {"title": "Optimal learning rates for regularization operators in learning theory", "authors": ["Caponnetto", "Andrea"], "venue": "Technical report,", "year": 2006}, {"title": "Optimal rates for the regularized least-squares algorithm", "authors": ["Caponnetto", "Andrea", "De Vito", "Ernesto"], "venue": "Foundations of Computational Mathematics,", "year": 2007}, {"title": "On the generalization ability of on-line learning algorithms", "authors": ["Cesa-Bianchi", "Nicolo", "Conconi", "Alex", "Gentile", "Claudio"], "venue": "IEEE Transactions on Information Theory,", "year": 2004}, {"title": "Learning theory: an approximation theory viewpoint, volume 24", "authors": ["Cucker", "Felipe", "Zhou", "Ding Xuan"], "year": 2007}, {"title": "Kernel ridge vs. principal component regression: Minimax bounds and the qualification of regularization operators", "authors": ["Dicker", "Lee H", "Foster", "Dean P", "Hsu", "Daniel"], "venue": "Electronic Journal of Statistics,", "year": 2017}, {"title": "Nonparametric stochastic approximation with large step-sizes", "authors": ["Dieuleveut", "Aymeric", "Bach", "Francis"], "venue": "The Annals of Statistics,", "year": 2016}, {"title": "Regularization of inverse problems, volume 375", "authors": ["Engl", "Heinz Werner", "Hanke", "Martin", "Neubauer", "Andreas"], "venue": "Springer Science & Business Media,", "year": 1996}, {"title": "Norm inequalities equivalent to heinz inequality", "authors": ["Fujii", "Junichi", "Masatoshi", "Furuta", "Takayuki", "Nakamoto", "Ritsuo"], "venue": "Proceedings of the American Mathematical Society,", "year": 1993}, {"title": "Spectral algorithms for supervised learning", "authors": ["Gerfo", "L Lo", "Rosasco", "Lorenzo", "Odone", "Francesca", "De Vito", "Ernesto", "Verri", "Alessandro"], "venue": "Neural Computation,", "year": 2008}, {"title": "Learning theory of distributed spectral algorithms", "authors": ["Guo", "Zheng-Chu", "Lin", "Shao-Bo", "Zhou", "Ding-Xuan"], "venue": "Inverse Problems,", "year": 2017}, {"title": "Parallelizing stochastic approximation through mini-batching and tail-averaging", "authors": ["Jain", "Prateek", "Kakade", "Sham M", "Kidambi", "Rahul", "Netrapalli", "Praneeth", "Sidford", "Aaron"], "venue": "arXiv preprint arXiv:1610.03774,", "year": 2016}, {"title": "Communication-efficient sparse regression", "authors": ["Lee", "Jason D", "Liu", "Qiang", "Sun", "Yuekai", "Taylor", "Jonathan E"], "venue": "Journal of Machine Learning Research,", "year": 2017}, {"title": "Optimal convergence for distributed learning with stochastic gradient methods and spectralregularization algorithms", "authors": ["Lin", "Junhong", "Cevher", "Volkan"], "venue": "arXiv preprint arXiv:1801.07226,", "year": 2018}, {"title": "Optimal rates for learning with Nystr\u00f6m stochastic gradient methods", "authors": ["Lin", "Junhong", "Rosasco", "Lorenzo"], "venue": "arXiv preprint arXiv:1710.07797,", "year": 2017}, {"title": "Optimal rates for multi-pass stochastic gradient methods", "authors": ["Lin", "Junhong", "Rosasco", "Lorenzo"], "venue": "Journal of Machine Learning Research,", "year": 2017}, {"title": "Distributed learning with regularized least squares", "authors": ["Lin", "Shao-Bo", "Guo", "Xin", "Zhou", "Ding-Xuan"], "venue": "Journal of Machine Learning Research,", "year": 2017}, {"title": "A PAC-bayesian analysis of randomized learning with application to stochastic gradient descent", "authors": ["London", "Ben"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2017}, {"title": "Large scale online kernel learning", "authors": ["Lu", "Jing", "Hoi", "Steven CH", "Wang", "Jialei", "Zhao", "Peilin", "Liu", "ZhiYong"], "venue": "Journal of Machine Learning Research,", "year": 2016}, {"title": "Moduli of continuity for operator valued functions", "authors": ["Math\u00e9", "Peter", "Pereverzev", "Sergei V"], "year": 2002}, {"title": "Efficient large-scale distributed training of conditional maximum entropy models", "authors": ["Mcdonald", "Ryan", "Mohri", "Mehryar", "Silberman", "Nathan", "Walker", "Dan", "Mann", "Gideon S"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2009}, {"title": "On some extensions of bernstein\u2019s inequality for self-adjoint operators", "authors": ["Minsker", "Stanislav"], "venue": "arXiv preprint arXiv:1112.5448,", "year": 2011}, {"title": "Robust stochastic approximation approach to stochastic programming", "authors": ["Nemirovski", "Arkadi", "Juditsky", "Anatoli", "Lan", "Guanghui", "Shapiro", "Alexander"], "venue": "SIAM Journal on Optimization,", "year": 2009}, {"title": "Remarks on inequalities for large deviation probabilities", "authors": ["IF Pinelis", "Sakhanenko", "AI"], "venue": "Theory of Probability & Its Applications,", "year": 1986}, {"title": "Acceleration of stochastic approximation by averaging", "authors": ["Polyak", "Boris T", "Juditsky", "Anatoli B"], "venue": "SIAM Journal on Control and Optimization,", "year": 1992}, {"title": "A stochastic approximation method", "authors": ["Robbins", "Herbert", "Monro", "Sutton"], "venue": "The Annals of Mathematical Statistics,", "year": 1951}, {"title": "Less is more: Nystr\u00f6m computational regularization", "authors": ["Rudi", "Alessandro", "Camoriano", "Raffaello", "Rosasco", "Lorenzo"], "venue": "Advances in Neural Information Processing Systems,", "year": 2015}, {"title": "Learning with kernels: support vector machines, regularization, optimization, and beyond", "authors": ["Sch\u00f6lkopf", "Bernhard", "Smola", "Alexander J"], "venue": "MIT press,", "year": 2002}, {"title": "Kernel methods for pattern analysis", "authors": ["Shawe-Taylor", "John", "Cristianini", "Nello"], "venue": "Cambridge university press,", "year": 2004}, {"title": "Online learning algorithms", "authors": ["Smale", "Steve", "Yao", "Yuan"], "venue": "Foundations of Computational Mathematics,", "year": 2006}, {"title": "Learning theory estimates via integral operators and their approximations", "authors": ["Smale", "Steve", "Zhou", "Ding-Xuan"], "venue": "Constructive Approximation,", "year": 2007}, {"title": "Sparse greedy matrix approximation for machine learning", "authors": ["Smola", "Alex J", "Sch\u00f6lkopf", "Bernhard"], "year": 2000}, {"title": "Support vector machines", "authors": ["Steinwart", "Ingo", "Christmann", "Andreas"], "venue": "Springer Science & Business Media,", "year": 2008}, {"title": "Kernel ridge regression via partitioning", "authors": ["Tandon", "Rashish", "Si", "Ravikumar", "Pradeep", "Dhillon", "Inderjit"], "venue": "arXiv preprint arXiv:1608.01976,", "year": 2016}, {"title": "Online learning as stochastic approximation of regularization paths: Optimality and almost-sure convergence", "authors": ["Tarres", "Pierre", "Yao", "Yuan"], "venue": "IEEE Transactions on Information Theory,", "year": 2014}, {"title": "Spatial decompositions for large scale SVMs", "authors": ["Thomann", "Philipp", "Steinwart", "Ingo", "Blaschzyk", "Ingrid", "Meister", "Mona"], "venue": "arXiv preprint arXiv:1612.00374,", "year": 2016}, {"title": "User-friendly tools for random matrices: An introduction", "authors": ["Tropp", "Joel A"], "venue": "Technical report, DTIC Document,", "year": 2012}, {"title": "Using the Nystr\u00f6m method to speed up kernel machines", "authors": ["Williams", "Christopher KI", "Seeger", "Matthias"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2000}, {"title": "Nystr\u00f6m method vs random fourier features: A theoretical and empirical comparison", "authors": ["Yang", "Tianbao", "Li", "Yu-Feng", "Mahdavi", "Mehrdad", "Jin", "Rong", "Zhou", "Zhi-Hua"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2012}, {"title": "Randomized sketches for kernels: Fast and optimal non-parametric regression", "authors": ["Yang", "Yun", "Pilanci", "Mert", "Wainwright", "Martin J"], "venue": "arXiv preprint arXiv:1501.06195,", "year": 2015}, {"title": "On early stopping in gradient descent learning", "authors": ["Yao", "Yuan", "Rosasco", "Lorenzo", "Caponnetto", "Andrea"], "venue": "Constructive Approximation,", "year": 2007}, {"title": "Online gradient descent learning algorithms", "authors": ["Ying", "Yiming", "Pontil", "Massimiliano"], "venue": "Foundations of Computational Mathematics,", "year": 2008}, {"title": "Sketchy decisions: Convex low-rank matrix optimization with optimal storage", "authors": ["Yurtsever", "Alp", "Udell", "Madeleine", "Tropp", "Joel Aaron", "Cevher", "Volkan"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "year": 2017}, {"title": "Solving large scale linear prediction problems using stochastic gradient descent algorithms", "authors": ["Zhang", "Tong"], "venue": "In International Conference on Machine learning,", "year": 2004}, {"title": "Learning bounds for kernel regression using effective data dimensionality", "authors": ["Zhang", "Tong"], "venue": "Neural Computation,", "year": 2005}, {"title": "Boosting with early stopping: Convergence and consistency", "authors": ["Zhang", "Tong", "Yu", "Bin"], "venue": "The Annals of Statistics,", "year": 2005}, {"title": "Communication-efficient algorithms for statistical optimization", "authors": ["Zhang", "Yuchen", "Wainwright", "Martin J", "Duchi", "John C"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2012}, {"title": "Divide and conquer kernel ridge regression: a distributed algorithm with minimax optimal rates", "authors": ["Zhang", "Yuchen", "Duchi", "John C", "Wainwright", "Martin J"], "venue": "Journal of Machine Learning Research,", "year": 2015}, {"title": "Parallelized stochastic gradient descent", "authors": ["Zinkevich", "Martin", "Weimer", "Markus", "Li", "Lihong", "Smola", "Alex J"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2010}], "id": "SP:b82c622b6235b645b8713fb205589c8dc1b41d32", "authors": [{"name": "Junhong Lin", "affiliations": []}, {"name": "Volkan Cevher", "affiliations": []}], "abstractText": "We study generalization properties of distributed algorithms in the setting of nonparametric regression over a reproducing kernel Hilbert space (RKHS). We investigate distributed stochastic gradient methods (SGM), with mini-batches and multi-passes over the data. We show that optimal generalization error bounds can be retained for distributed SGM provided that the partition level is not too large. Our results are superior to the state-of-the-art theory, covering the cases that the regression function may not be in the hypothesis spaces. Particularly, our results show that distributed SGM has a smaller theoretical computational complexity, compared with distributed kernel ridge regression (KRR) and classic SGM.", "title": "Optimal Distributed Learning with Multi-pass Stochastic Gradient Methods"}