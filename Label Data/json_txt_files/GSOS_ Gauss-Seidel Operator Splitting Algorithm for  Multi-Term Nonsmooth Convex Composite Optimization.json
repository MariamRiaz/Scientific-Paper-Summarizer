{"sections": [{"heading": "1. Introduction", "text": "In this paper, we focus on the multi-term nonsmooth convex composite optimization\nmin x\u2208X f(x) + n\u2211 i=1 gi(x), (1)\nwhere X is a linear space, gi : X \u2192 (\u2212\u221e,+\u221e] is a proper, lower semicontinuous convex function for all i = 1, \u00b7 \u00b7 \u00b7 , n, and f : X \u2192 (\u2212\u221e,+\u221e) is a continuous\n1Tencent AI Lab, China 2Sun Yat-sen University, China 3The Chinese University of Hong Kong, China. Correspondence to: Li Shen <mathshenli@gmail.com>, Wei Liu <wliu@ee.columbia.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\ndifferentiable convex function with its gradient satisfying the inequality that\n1\nL \u2225\u2225\u2207f(x)\u2212\u2207f(y)\u2225\u22252 \u2264 \u2329\u2207f(x)\u2212\u2207f(y), x\u2212 y\u232a. (2) The above multi-term nonsmooth convex composite optimization problem (1) covers a large class of applications in machine learning such as simultaneous low-rank and sparsity (Richard et al., 2012; Zhou et al., 2013), overlapping group Lasso (Zhao et al., 2009; Jacob et al., 2009; Mairal et al., 2010), graph-guided fused Lasso (Chen et al., 2012; Kim & Xing, 2009), graph-guided logistic regression (Chen et al., 2011; Zhong & Kwok, 2014), variational image restoration (Combettes & Pesquet, 2011; Dupe\u0301 et al., 2009; Pustelnik et al., 2011), and other types of structure regularization paradigms (Teo et al., 2010; 2007). By introducing the multi-term nonsmooth regularization term\u2211n i=1 gi(x) such as structured sparsity (Huang et al., 2011; Bach et al., 2012; Bach, 2010) and nonnegativity (Chen & Plemmons, 2015; Xu & Yin, 2013), more prior information can be included to enhance the accuracy of regularization models. However, due to the multi-term nonsmooth regularization term \u2211n i=1 gi(x), the optimization problem (1) is too complicated to be solved even for small n. For n \u2264 2, some existing popular first-order optimization methods are accelerated proximal gradient method (Beck & Teboulle, 2009; Nesterov, 2007), smoothing accelerated proximal gradient method (Nesterov, 2005a;b), three operator splitting method (Davis & Yin, 2015), and some primal-dual operator splitting methods such as majorized alternating direction method of multiplier (ADMM) (Cui et al., 2016; Lin et al., 2011), fast proximity method (Li & Zhang, 2016), and so on.\nOn the other hand, when n \u2265 3, there also exist some algorithms for solving problem (1). A directly method for (1) is smoothing accelerated proximal gradient (S-APG) proposed by Nesterov (Nesterov, 2005a;b). Then, Yu (Yu, 2013) proposed a new approximation method called PAAPG for handling (1) by combining the proximal average approximation technique and Nesterov\u2019s acceleration technique, which has been enhanced very recently by Shen et al. (Shen et al., 2017). Their proposed method called APA-APG adopts an adaptive stepsize strategy. However,\nthe above mentioned methods S-APG, PA-APG and its enhanced version APA-APG all need a strict restriction on the nonsmooth functions {gi(x)} that each gi(x) must be Lipschitz continuous. In addition, some primal-dual parallel splitting methods (Briceno-Arias et al., 2011; Combettes & Pesquet, 2007; 2008; Condat, 2013; Vu\u0303, 2013) generalized from traditional operator splitting, such as forward backward splitting method (Chen & Rockafellar, 1997) and Douglas Rachford splitting method (Eckstein & Bertsekas, 1992), can also solve the multi-term nonsmooth convex composite optimization problem (1). Different from prior work, Raguet et al. (Raguet et al., 2013) proposed an efficient primal operator splitting method called generalized forward backward splitting method using the classic forward backward splitting technique, which has shown the superiority over numerous existing primal-dual splitting methods (Monteiro & Svaiter, 2013; Combettes & Pesquet, 2012; Chambolle & Pock, 2011) in dealing with variational image restoration problems. All the above mentioned methods for problem (1) with n \u2265 3 share a common feature that they all split the nonsmooth composite term\u2211n i=1 gi(x) in the Jacobi iteration manner, i.e., parallelly. This is one of the main differences between existing splitting methods and our proposed method in this paper.\nTo split the nonsmooth composite term \u2211n i=1 gi(x) more efficiently, we propose a novel operator splitting algorithm to solve problem (1) by harnessing the advantage of GaussSeidel iterations, i.e., the computation of the proximal mapping of the current function gi(x) uses the proximal mappings of gj(x) for all j < i which have already been computed ahead. In addition, to further improve the algorithm\u2019s efficiency, we leverage the over-relaxation acceleration technique. What\u2019s more, we provide a new strategy that the over-relaxation stepsize can be determined adaptively, ensuring a larger value to accelerate the algorithm. The most important is that the convergence of our proposed GSOS algorithm is established by a newly developed analysis technique. In detail, given an invertible linear operator R, we first argue that the optimal solution set [\u2207f + \u2211n i=1 \u2202gi] \u22121 (0) of problem (1) can be recovered\nby the zero point set [ (R\u2217)\u22121SR, \u2202g+A\u25e6\u2207f\u25e6A,NV ]\u22121 (0). This is fulfilled through adopting the tool of operator optimization theory, in which the composite operator SR, \u2202g+A\u25e6\u2207f\u25e6A,NV is generalized from the definition of the composite monotone operator S\u03bb,A,B in (Eckstein & Bertsekas, 1992). Next, by unitizing the definition of the -enlargement of maximal monotone (Burachik et al., 1998; 1997; Burachik & Svaiter, 1999; Svaiter, 2000), we establish a key property for SR, \u2202g+A\u25e6\u2207f\u25e6A,NV , that is, gph ( SR, (\u2202g+A\u2217\u25e6\u2207f\u25e6A)[ ],NV ) \u2286\ngph ( R\u2217[(R\u2217)\u22121SR, \u2202g+A\u2217\u25e6\u2207f\u25e6A,NV ][ ] ) . Based on this observation, we equivalently reformulate the GSOS algorithm as a two-step iterations algorithm. Then, the\nglobal convergence of the proposed GSOS algorithm is easily established based on this reformulation.\nThe closest algorithm to our proposed GSOS algorithm is the generalized forward backward splitting method proposed by Raguet et al. (Raguet et al., 2013). By carefully selecting the scaling matrix H in the forthcoming GSOS algorithm, it is easy to check that GSOS covers the generalized forward backward splitting method as a special case. Another highly related algorithm to our proposed GSOS algorithm is the matrix splitting method (Luo & Tseng, 1991; Yuan et al., 2016). Choosing the scaling matrixH suitably, the proposed GSOS algorithm can inherit the advantage of the matrix splitting technique which has shown the efficiency in (Yuan et al., 2016) for coping with a special class of coordinate separable composite optimization problems.\nThe rest of this paper is organized as follows. In Section 2, we first give the definitions of some useful notations which can make the paper much more readable. We also establish some lemmas and propositions based on monotone operator theory (Bauschke & Combettes, 2011), which are the key to the convergence of the GSOS algorithm. In Section 3, we present the proposed GSOS algorithm and then analyze its convergence and iteration complexity. In Section 4, we conduct numerical experiments on overlapping group Lasso and graph-guided fused Lasso problems to evaluate the efficacy of the GSOS algorithm. Finally, we draw conclusions in Section 5."}, {"heading": "2. Preliminaries and Notations", "text": "Let Y = \u220fn i=1 Xi be the product space of Xi with Xi = X for all i \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , n}. Let V be a linear space and V\u22a5 be its complementary space with the following definitions V= { y \u2208 Y | y1 = \u00b7 \u00b7 \u00b7 = yn } , V\u22a5= { y \u2208 Y |\nn\u2211 i yi = 0 } .\nLet IX : X \u2192 X be the identity map and EY : X \u2192 Y be a block linear operator defined as EY =( IX \u00b7 \u00b7 \u00b7 IX )\u2217 . Let A : Y \u2192 X be a linear operator defined as Ay = 1nE \u2217 Yy = 1 n \u2211n i=1 yi. Hence, its adjoint operator A\u2217 : X \u2192 Y is defined as A\u2217x = 1nEYx. Let H,R : Y \u2192 Y be block lower triangular linear invertible operators satisfying (R\u2217)\u22121 = H and H + H\u2217 0. Moreover,H is defined as H1,1 0 \u00b7 \u00b7 \u00b7 0 ... . . . ...\n... Hn\u22121,1 \u00b7 \u00b7 \u00b7 Hn\u22121,n\u22121 0 Hn,1 \u00b7 \u00b7 \u00b7 Hn\u22121,n Hn,n  , (3) where Hi,j : X \u2192 X is a linear operator for all (i, j) \u2208 {1, \u00b7 \u00b7 \u00b7 , n}. It is worthwhile to emphasize that Hi,i is also possible to be a lower triangular linear operator satisfying\nHi,i +H\u2217i,i 0. Next, we abuse the notation \u2016 \u00b7 \u2016H which is induced by the inner product \u3008\u00b7,H\u00b7\u3009 satisfying\n\u2016 \u00b7 \u2016H : = \u221a \u3008\u00b7,H\u00b7\u3009 = \u221a \u3008\u00b7,H\u2217\u00b7\u3009\n= \u221a \u3008\u00b7, H+H \u2217\n2 \u00b7\u3009 = \u2016 \u00b7 \u2016H+H\u2217 2 . (4)\nIn addition, we define the generalized proximal mapping of a proper, lower semicontinuous convex function gi(x) with respect to the invertible linear operatorHi,i.\nDefinition 1 For a given x, the proximal mapping denoted by ProxH\u22121i,i gi(x) of a proper, lower semicontinuous convex function gi with respect to an invertible linear operator Hi,i satisfying Hi,i + H\u2217i,i 0 is defined to be the zero point of the following inclusion equation\n0 \u2208 \u2202gi(\u00b7) +Hi,i(\u00b7 \u2212 x). (5)\nMoreover, if Hi,i is symmetric, it can be reformulated as the following convex minimization\nProxHi,igi(x) := arg min y\u2208X\ngi(y) + 1\n2 \u2016y \u2212 x\u20162Hi,i .\nNext, we recall the definition of -enlargement of monotone operators (Burachik et al., 1998; 1997; Burachik & Svaiter, 1999; Svaiter, 2000), which is an effective tool for establishing the convergence of the proposed GSOS algorithm.\nDefinition 2 Given a maximal monotone operator T : X \u21d2 X, the (\u2265 0)-enlargement of T is defined as the set T [ ](x) := { v \u2208 Y | \u3008w \u2212 v, z \u2212 x\u3009 \u2265 \u2212 for all z \u2208\nX, w \u2208 T (z) } .\nRecall that f(x) is a gradient Lipschitz convex function satisfying inequality (2). There exits 0 \u03a3 \u03a3\u0302 LI such that the following two inequalities hold for any x, x\u2032 \u2208 X\nf(x) \u2264 f(x\u2032) + \u3008\u2207f(x\u2032), x\u2212 x\u2032\u3009+ 1 2 \u2016x\u2212 x\u2032\u20162 \u03a3\u0302 , (6) f(x) \u2265 f(x\u2032) + \u3008\u2207f(x\u2032), x\u2212 x\u2032\u3009+ 1 2 \u2016x\u2212 x\u2032\u20162\u03a3. (7)\nActually, when f(x) is a quadratic function, it holds \u03a3 = \u03a3\u0302 directly in inequalities (6) and (7). The following lemma establishes the property of the enlargement of the composite operatorA\u2217 \u25e6\u2207f \u25e6A with f satisfying inequalities (6)- (7) or (2), which is an essential ingredient for reformulating the GSOS algorithm as a two-step iterations algorithm.\nProposition 1 Assume that f is a gradient Lipschitz continuous convex function satisfying inequality (2). For any x1, x2 \u2208 Y , it holds that\n(A\u2217 \u25e6 \u2207f \u25e6 A)(x2) \u2208 (A\u2217 \u25e6 \u2207f \u25e6 A)[ ](x1) (8)\nwith = L4 \u2016Ax1\u2212Ax2\u2016 2. In addition, if f further satisfies inequalities (6)-(7), it holds that\n(A\u2217 \u25e6 \u2207f \u25e6 A)(x2) \u2208 (A\u2217 \u25e6 \u2207f \u25e6 A)[ ](x1) (9)\nwith = 14\u2016Ax1 \u2212Ax2\u2016 2 2\u03a3\u0302\u2212\u03a3 .\nRemark 1 Two comments are made for Proposition 1:\n(1) This proposition gives two types of estimations for in( A\u2217 \u25e6\u2207f \u25e6A )[ ] in (8) and (9). When f is a quadratic\nfunction, it is easy to check that\n1 4 \u2016Ax1 \u2212Ax2\u201622\u03a3\u0302\u2212\u03a3 \u2264 L 4 \u2016Ax1 \u2212Ax2\u20162\ndue to \u03a3\u0302 = \u03a3 LI. When f is a general gradient Lipschitz continuous function, we do not know which estimation for is tighter in (8) and (9).\n(2) The second part of this proposition can be regarded as an intensified version of Lemma 2.2 in (Svaiter, 2014) for a specified composite operator A\u2217 \u25e6 \u2207f \u25e6 A. The first part of the proposition coincides with the results by applying Lemma 2.2 in (Svaiter, 2014) for A\u2217 \u25e6 \u2207f \u25e6 A.\nNext, we generalize the notation S\u03bb,T1,T2 in (Eckstein & Bertsekas, 1992) for a given \u03bb > 0 and two maximal monotone operators T1, T2 as SR,T1,T2 for a given invertible linear operatorR defined as\ngphSR, T1, T2 (10) := { (x1 +Ry2, x2 \u2212 x1) | y1 \u2208 T1(x1),\ny2 \u2208 T2(x2), x1 +R\u2217y1 = x2 \u2212R\u2217y2 } .\nBy (Eckstein & Bertsekas, 1992), we know that S\u03bb,T1,T2 is maximal monotone if T1 and T2 are both maximal monotone. However, its generalized operator SR,T1,T2 is not monotone unless the invertible linear operator R reduces to be a constant. Very interesting, it can be shown that its composition with (R\u2217)\u22121, i.e., (R\u2217)\u22121SR,T1,T2 , is maximal monotone for any invertible linear operatorR.\nLemma 1 For any given invertible linear operator R, operator (R\u2217)\u22121SR,T1,T2 is maximal monotone if T1 and T2 are both maximal monotone operators.\nSetting T1 = \u2202g + A\u2217 \u25e6 \u2207f \u25e6 A, T2 = NV , we obtain SR, \u2202g+A\u2217\u25e6\u2207f\u25e6A,NV , which is defined as\ngph ( SR,\u2202g+A\u2217\u25e6\u2207f\u25e6A,NV ) (11)\n:= { (x1+Ry2, x2\u2212x1) | y1\u2208(\u2202g +A\u2217 \u25e6 \u2207f \u25e6 A)(x1),\ny2 \u2208 NV(x2), x1 +R\u2217y1 = x2 \u2212R\u2217y2 } .\nBy Lemma 1, we know that (R\u2217)\u22121SR, \u2202g+A\u2217\u25e6\u2207f\u25e6A,NV is maximal monotone due to the maximal monotonicity of \u2202g + A\u2217 \u25e6 \u2207f \u25e6 A and NV . Hence, given a constant \u2265 0, the enlargement [(R\u2217)\u22121SR, \u2202g+A\u2217\u25e6\u2207f\u25e6A,NV ][ ] is well defined. In addition, based on the definition of SR,T1,T2 again, we set T1 = \u2202g + (A\u2217 \u25e6 \u2207f \u25e6 A)[ ], or T1 = (\u2202g + A\u2217 \u25e6 \u2207f \u25e6 A)[ ] and T2 = NV in (10). Then we have the definition of SR,\u2202g+(A\u2217\u25e6\u2207f\u25e6A)[ ],NV or SR,(\u2202g+A\u2217\u25e6\u2207f\u25e6A)[ ],NV for any given invertible linear operatorR and constant \u2265 0 as follows\ngph ( SR,\u2202g+(A\u2217\u25e6\u2207f\u25e6A)[ ],NV ) (12)\n:= { (x1+Ry2, x2\u2212x1)|y1\u2208(\u2202g+(A\u2217\u25e6\u2207f \u25e6A)[ ])(x1),\ny2 \u2208 NV(x2), x1 +R\u2217y1 = x2 \u2212R\u2217y2 } ,\ngph ( SR,(\u2202g+A\u2217\u25e6\u2207f\u25e6A)[ ],NV ) (13)\n:= { (x1+Ry2, x2\u2212x1)|y1\u2208(\u2202g +A\u2217\u25e6\u2207f \u25e6A)[ ])(x1),\ny2 \u2208 NV(x2), x1 +R\u2217y1 = x2 \u2212R\u2217y2 } .\nIn the proposition below, we will establish the relationships among the above mentioned three operators SR,\u2202g+(A\u2217\u25e6\u2207f\u25e6A)[ ],NV , SR,(\u2202g+A\u2217\u25e6\u2207f\u25e6A)[ ],NV and [(R\u2217)\u22121SR, \u2202g+A\u2217\u25e6\u2207f\u25e6A,NV ][ ].\nProposition 2 Given a constant \u2265 0 and an invertible linear operatorR, it holds that\ngph ( SR, \u2202g+(A\u2217\u25e6\u2207f\u25e6A)[ ],NV ) \u2286 gph ( SR, (\u2202g+A\u2217\u25e6\u2207f\u25e6A)[ ],NV\n) \u2286 gph ( R\u2217[(R\u2217)\u22121SR, \u2202g+A\u2217\u25e6\u2207f\u25e6A,NV ][ ] ) .\nIn the following, we establish the relationship between the optimal solution set [\u2207f + \u2211n i=1 \u2202gi] \u22121 (0) of prob-\nlem (1) and [ (R\u2217)\u22121SR, \u2202g+A\u2217\u25e6\u2207f\u25e6A,NV ]\u22121 (0), which means that we can recover the solution of problem (1) through [ (R\u2217)\u22121SR, \u2202g+A\u2217\u25e6\u2207f\u25e6A,NV ]\u22121 (0).\nLemma 2 Let linear operators H and R satisfy (R\u2217)\u22121 = H and H satisfy (3). Denote \u2126 = [ (R\u2217)\u22121SR, (\u2202g+A\u2217\u25e6\u2207f\u25e6A),NV ]\u22121 (0). It holds that[ \u2207f +\nn\u2211 i=1 \u2202gi\n]\u22121 (0) = ( ETYH\u2217EY\n)\u22121ETYH\u2217(\u2126)."}, {"heading": "3. GSOS Algorithm", "text": "In this section, we first propose the Gauss-Seidel operator splitting algorithm for solving the multi-term nonsmooth convex composite problem (1). Then, based on the preliminaries in Section 2, we establish the convergence and iteration complexity of the GSOS algorithm.\nAlgorithm 1 GSOS Algorithm Parameters: Choose \u03c3 \u2208 (0, 1), a linear operator H satisfying (3) and a starting point z0 \u2208 Z . Set \u03b8fix1 \u2208( \u2212 1, \u03b81 ] and \u03b8fix2 \u2208 ( \u2212 1, \u03b82 ] , where \u03b81 and \u03b82 are\ndefined via equations (14a) and (14b), respectively. for k = 0, 1, 2, \u00b7 \u00b7 \u00b7 ,K do xk := EY ( ETYHEY )\u22121ETYHzk; for i = 1, 2 \u00b7 \u00b7 \u00b7 , n do yki := ProxH\u22121i,i gi ( H\u22121i,i [ \u2211i j=1Hi,j(2xkj \u2212 zkj ) \u2212\n1 n\u2207f( 1 n \u2211n i=1 x k i )\u2212 \u2211i\u22121 j=1Hi,jykj ] ) ;\nend for set \u03b8adap1k as (14c) and \u03b8 adap2 k as (14d); set \u03b8k \u2208 [\u03b8fix1, \u03b8adap1k ] \u222a [\u03b8fix2, \u03b8 adap2 k ]; zk+1 := zk + (1 + \u03b8k)(y k \u2212 xk);\nend for return \u03c9K := ( ETYH\u2217EY )\u22121ETYH\u2217zK . In Algorithm 1, parameters \u03b81, \u03b81, \u03b8 adap1 k , \u03b8 adap1 k are defined as \u03b81 = max { \u03b8|(\u03b8 \u2212 \u03c3)(H+H\u2217) + LA\u2217A 0 } ; (14a) \u03b82 = max { \u03b8 | (\u03b8 \u2212 \u03c3)(H+H\u2217) (14b) +A\u2217(2\u03a3\u0302\u2212 \u03a3)A 0 } ; \u03b8adap1k = \u03c3 \u2212 L\u2016A(xk \u2212 yk)\u20162 \u2016xk \u2212 yk\u20162H+H\u2217 ; (14c) \u03b8adap2k = \u03c3 \u2212 \u2016A(xk \u2212 yk)\u20162\n2\u03a3\u0302\u2212\u03a3 \u2016xk \u2212 yk\u20162H+H\u2217 . (14d)\nRemark 2 We make some comments on GSOS below.\n(1) For the updating step of xk, we obtain xk = EY (\u2211K i,j=1Hij )\u22121\u2211K j=1 \u2211K i=j Hijzkj by using the\nnotations H and EY . Similarly, we have \u03c9k =(\u2211K i,j=1Hij )\u22121\u2211K j=1 \u2211j i=iH\u2217jizkj . Hence, we need\nto compute the inverse of \u2211n i,j=1Hi,j . However, if Hi,j is a lower triangular matrix operator, xk and \u03c9k can be obtained easily.\n(2) By the definitions of ProxH\u22121i,i gi and y k, we need to\nsolve the following inclusion equation\nGki \u2208 Hi,iyki + \u2202gi(yki ),\nwhere Gki = H \u22121 i,i [\u2211i j=1Hi,j(2xkj \u2212 zkj ) \u2212 1 n\u2207f( 1 n \u2211n i=1 x k i ) \u2212 \u2211i\u22121 j=1Hi,jykj ] . Usually, it is easy to choose a suitable Hi,i such that the solution of the above inclusion equation has a closed form.\n(3) \u03b8k is the over-relaxation stepsize for accelerating the GSOS algorithm. If the computations of \u03b8adap1k and \u03b8adap2k are time consuming, we can set \u03b8k = max{\u03b8fix1, \u03b8fix2}.\n(4) WhenH is a diagonal matrix, i.e.,Hi,j = 0 andHi,i = aiI with some nonnegative constant ai, and the over relaxation stepsize \u03b8k is fixed to a smaller region, the GSOS algorithm reduces to the generalized forward backward splitting method in (Raguet et al., 2013).\nIn the following, we reformulate the GSOS algorithm as a two-step iterations algorithm by utilizing monotone optimization theory established in Section 2, which is the key to the convergence of the GSOS algorithm.\nProposition 3 Let g : Y \u2192 (\u2212\u221e,+\u221e] be the function defined as g(x) = \u2211n i=1 gi(xi). Assume that the sequences (xk, yk) and zk are generated by Algorithm 1 with \u03c3 \u2208 (0, 1). Let vk = (R\u2217)\u22121(xk \u2212 yk) and zk = yk + R(R\u2217)\u22121(zk \u2212 xk). Then, for all k \u2208 N, there exists k \u2265 0 such that the iterations in Algorithm 1 can be reformulated as the following two-step iterations algorithm: vk \u2208 [(R\u2217)\u22121SR, \u2202g+(A\u2217\u25e6\u2207f\u25e6A),NV ] [ k](zk), (15a) \u03b8k\u2016R\u2217vk\u20162R\u22121 + \u2016R \u2217vk + zk \u2212 zk\u20162R\u22121\n+2 k \u2264 \u03c3\u2016zk \u2212 zk\u20162R\u22121 , (15b)\nand zk+1 = zk \u2212 (1 + \u03b8k)R\u2217vk.\nRemark 3 Based on Proposition 3, the GSOS algorithm can be regarded as an inexact over-relaxed metric proximal point algorithm for the composite inclusion\n0 \u2208 (R\u2217)\u22121SR,\u2202g+A\u2217\u25e6\u2207f\u25e6A,NV (z).\nBy Proposition 3 and Lemma 2, we can establish the convergence of the GSOS algorithm based on the relationship\nbetween the two zero point sets [\u2207f+ n\u2211 i=1 \u2202gi] \u22121(0) and \u2126.\nTheorem 1 Let {(xk, yk, zk)} be the sequence generated by Algorithm 1. We have:\n(i) for any z\u2217 \u2208 [(R\u2217)\u22121SR,\u2202g+A\u2217\u25e6\u2207f\u25e6A,NV ]\u22121(0), it holds that\n\u2016zk+1 \u2212 z\u2217\u20162R\u22121 \u2264 \u2016z k \u2212 z\u2217\u20162R\u22121 (16)\n\u2212 (1\u2212 \u03c3)(1 + \u03b8k)\u2016xk \u2212 yk\u20162R\u22121 ;\n(ii) zk converges to a point belonging to zero point set [(R\u2217)\u22121SR,\u2202g+A\u2217\u25e6\u2207f\u25e6A,NV ]\u22121(0) and \u03c9k converges to a point belonging to [\u2207f + \u2211n i=1 \u2202gi] \u22121 (0), i.e., the optimal solution set\nof problem (1).\nTheorem 1 indicates that \u2016xk \u2212 yk\u2016 approaching to zero implies the convergence of the GSOS algorithm. In the theorem below, we measure the convergence rates of two sequences \u2016xk \u2212 yk\u2016 and \u2016\u03c9k \u2212 \u03c9k+1\u2016.\nTheorem 2 Let zk be the sequence generated by the GSOS algorithm. Then, there exists i \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , k} such that\n\u2016xi \u2212 yi\u20162 \u2264 O (1 k ) , \u2225\u2225\u03c9i+1 \u2212 \u03c9i\u2225\u22252 \u2264 O(1 k ) .\nDue to the space limit, all proofs of the propositions, lemmas and theorems are placed into the supplementary material."}, {"heading": "4. Experiments", "text": "In this section, we apply the proposed algorithm to the overlapping group Lasso (Zhao et al., 2009; Jacob et al., 2009; Mairal et al., 2010) and graph-guided fused Lasso problems (Chen et al., 2012; Kim & Xing, 2009), which can be formulated as\nmin 1\n2 \u2016Sx\u2212 b\u20162 + K\u2211 i=1 gi(x). (17)\nFor overlapping group Lasso problem (21), gi(x) = \u03bd\u03b1i\u2016xGi\u2016 andK denotes the number of groups. For graphguided Lasso problem (25), gi(x) = \u03bd\u03b1ij\u2016xi\u2212xj\u2016 and K denotes the number of edges in the graph edge set E.\nWe describe the detailed techniques in the experimental implementation for (17). Given a > 12 and a positive definite operator D satisfying D STS, we set\nHi,j = {\n1 K2D, i \u2265 j \u2208 {1, 2, \u00b7 \u00b7 \u00b7 ,K}; a K2D, i = j \u2208 {1, 2, \u00b7 \u00b7 \u00b7 ,K}.\n(18)\nHence, it easy to check that H + H\u2217 = A\u2217DA + 2a\u22121 K2 Diag ( EYD ) 0. Due to the smooth term in overlapping group Lasso (21) is quadratic, the two estimations \u03b82 and \u03b8adap2k in (14b) and (14d) are preferred to be used. By specific H, we obtain \u2211K i,j=1Hi,j =\nK(K\u22121)+2\u03b1K 2K2 D and\u2211K\nj=1 \u2211K i=j Hi,jzkj = D K2 \u2211K j=1(a+K\u2212j)zkj ,which fur-\nther imply xk = (\u2211K i,j=1Hi,j )\u22121\u2211K j=1 \u2211K i=j Hi,jzkj =\n2 \u2211K j=1(a+K\u2212j)z k j\nK(K\u22121)+2aK . Moreover, by the positive definiteness of Hi,i and D, it holds that \u2211n j=1 \u2211j i=iH\u2217j,izkj =\nD K2 \u2211K j=1(a + j \u2212 1)zkj . Hence, we attain \u03c9k =\n2 \u2211K j=1(a+j\u22121)z k j\nK(K\u22121)+2aK . In addition, by the definition of H, we reformulate the estimation (14b) for \u03b8k as the following form:\n\u03b8 = max { \u03b8 | EY [ (\u03c3 \u2212 \u03b8)D \u2212 STS ] E\u2217Y\n+ (2a\u2212 1) ( \u03c3 \u2212 \u03b8 ) Diag(EYD) 0 } .\nDue to a \u2265 12 and the positive definiteness of D, a sufficient condition satisfying the constraint in the above set is{\n(\u03c3 \u2212 \u03b8)D \u2212 STS 0, \u03b8 \u2264 \u03c3 }\n. Hence, we have an alternative estimation for \u03b8 as\n\u03b8 = max { \u03b8 | (\u03c3 \u2212 \u03b8)D \u2212 STS 0, \u03b8 \u2264 \u03c3 } . (19)\nSimilarly, the adaptive stepsize estimation (14d) is reformulated as\n\u03b8adapk = \u03c3 \u2212\n1 2K2 \u2225\u2225\u2225\u2225 K\u2211 i=1 (xk \u2212 yki ) \u2225\u2225\u2225\u22252 STS\nK\u2211 j=1 K\u2211 i=j (xk \u2212 yki )THij(xk \u2212 ykj ) . (20)\nTherefore, the GSOS algorithm can be specified as the following form for solving problem (17).\nAlgorithm 2 GSOS Algorithm for Solving Problem (17) Parameters: Choose \u03c3 \u2208 (0, 1), positive definite operators D and Hi,j satisfying (18), and a starting point z0 \u2208 Z . Set \u03b8 as (19) and \u03b8fix \u2208 ( \u2212 1, \u03b81 ] .\nfor k = 0, 1, 2, \u00b7 \u00b7 \u00b7 , do xk := 2 \u2211K j=1(\u03b1+K\u2212j)z k j\nK(K\u22121)+2\u03b1K ; for i = 1, 2 \u00b7 \u00b7 \u00b7 ,K do yki := ProxH\u22121i,i gi ( H\u22121i,i [ \u2211i j=1Hi,j(2xk \u2212 zkj ) \u2212\n1 KS T (Sxk \u2212 b)\u2212 \u2211i\u22121 j=1Hi,jykj ] ) ;\nend for set \u03b8k \u2208 [\u03b8fixk , \u03b8 adap k ], where \u03b8 adap k is defined via (20); for j = 1, 2 \u00b7 \u00b7 \u00b7 ,K do zk+1j := z k j + (1 + \u03b8k)(y k j \u2212 xk);\nend for end for return \u03c9N = 2 \u2211K j=1(\u03b1+j\u22121)z N j\nK(K\u22121)+2\u03b1K .\nIn this paper, we compare the proposed GSOS algorithm with four state-of-the-art algorithms below.\n\u2022 GFB (Raguet et al., 2013): Generalized Forward Backward (GFB) splitting algorithm is a primal firstorder operator splitting algorithm for solving (1) proposed by Raguet et al. (Raguet et al., 2013), which has been shown to outperform other competing algorithms such as (Monteiro & Svaiter, 2013; Combettes & Pesquet, 2012; Chambolle & Pock, 2011) for variational image restoration.\n\u2022 PDM (Condat, 2013): A first-order Primal-Dual splitting Method (PDM) (Condat, 2013) for solving jointly the primal and dual formulations of large-scale convex minimization problems involving Lipschitz, proximal and linear composite terms.\n\u2022 PA-APG (Yu, 2013): Proximal Average approximated Accelerated Proximal Gradient (PA-APG) algorithm (Yu, 2013) is a primal first-order method, which utilizes the proximal average technique (Bauschke et al., 2008) to separate the multi-term nonsmooth function in (1). It has been shown to outperform the smoothing accelerated proximal gradient method (Nesterov, 2005b;a).\n\u2022 APA-APG (Shen et al., 2017): An enhanced version of PA-APG, which incorporates the Adaptive Proximal Average approximation technique with the Accelerated Proximal Gradient (APA-APG) method to improve the efficiency of the optimization procedure.\nIt is worthwhile to emphasize that PA-APG and APA-APG algorithms can only be applied to a specific class of problems (1), in which the multi-term nonsmooth regularization is Lipschitz continuous. Since the nonsmooth regularization terms in overlapping group Lasso and graph-guided fused Lasso are all exactly Lipschitz continuous, the two efficient solvers PG-APG (Yu, 2013) and its enhanced version APA-APG (Shen et al., 2017) are also compared with the GSOS algorithm to illustrate the efficacy of GSOS. In the implementation, the approximation parameter for PAAPG is set as 1.0e\u2212 5."}, {"heading": "4.1. Overlapping Group Lasso", "text": "In this subsection, we apply the proposed GSOS algorithm to the overlapping group Lasso problem, which takes the following formal definition:\nmin 1\n2 \u2016Sx\u2212 b\u20162 + \u03bd K\u2211 i=1 \u03b1i\u2016xGi\u2016, (21)\nwhere S \u2208 Rn\u00d7d is the sampling matrix, b is the noisy observation vector, G = {G1, \u00b7 \u00b7 \u00b7 ,GK} denotes the set of overlapping groups (Gi \u2282 {1, \u00b7 \u00b7 \u00b7 , d} satisfying \u22c3K i=1 Gi =\n{1, \u00b7 \u00b7 \u00b7 , d} and Gi \u22c2 Gj 6= \u2205 for some i, j), xGi \u2208 Rd is a duplication of x with x{1,\u00b7\u00b7\u00b7 ,d}\\Gi = 0, \u03b1i is the weight for the i-th group, and \u03bd is the regularization parameter controlling group sparsity.\nDuring the implementation of Algorithm 2, we need to calculate the generalized proximal mapping of \u2016xGi\u2016 in the updating step of yki . By the positive definiteness of Hi,i, the calculation of yki in Algorithm 2 is equivalent to solving the following problem:\nyki := arg min x\n1 2 \u2016x\u2212 bk\u20162Hi,i + \u03bd\u03b1i\u2016xGi\u2016,\nwhere bk = H\u22121i,i [\u2211i j=1Hi,j(2xk \u2212 zkj ) \u2212 1 KS\nT (Sxk \u2212 b)\u2212 \u2211i\u22121 j=1Hi,jykj ] . In the proposition below, given c, diag-\nonal positive definite operatprHi,i and group G, we solve\nx\u2217 := arg min x\n1 2 \u2016x\u2212 c\u20162Hi,i + \u03bd\u2016xG\u2016. (22)\nWhen Hi,i is identity matrix I, (22) has the closed-form solution\nx\u2217 = { x\u2217G , i \u2208 G, ci, else,\nwhere x\u2217G = { (1\u2212 \u03bd/\u2016cG\u2016)cG , \u2016cG\u2016 \u2265 t; 0, else.\nProposition 4 Let (Hi,i)G be the subdiagonal matrix of Hi,i with the index set G, and t\u2217 be the optimal solution of the one-dimensional optimization problem\nmin t\u22650\n{ 1\n2\n\u2329 cG , [ (Hi,i)\u22121G + 2tI ]\u22121 cG \u232a + t\u03bd2 } . (23)\nHence, the optimal solution of (22) has the following form\nx\u2217 =\n{ cG \u2212 [ I + 2t\u2217(Hi,i)G ]\u22121 cG , i \u2208 G;\nci, else. (24)\nLike (Chen et al., 2012; Yu, 2013), the entries of sampling matrix S \u2208 Rn\u00d7d are sampled from an i.i.d. normal distribution, and x \u2208 Rd with xj = (\u22121)j exp\u2212(j\u22121)/100 and d = 90K + 10. Let \u03be be the noise sampled from the standard normal distribution, and the noisy observation satisfies b = Sx + \u03be. In addition, we set \u03bd = 1 and \u03b1i = 1K2 for each group Gi and the groups {Gi} are overlapped by 10 elements, that is{\nG1 = {1, \u00b7 \u00b7 \u00b7 , 100} G2 = {91, \u00b7 \u00b7 \u00b7 , 190} \u00b7 \u00b7 \u00b7 GK = {d\u2212 99, \u00b7 \u00b7 \u00b7 , d}\n} .\nThe sampling size and the number of groups (n,K) are chosen from the following set\n(n,K) \u2208 {\n(1000, 20), (2000, 40), (4000, 60), (4000, 80), (5000, 80), (5000, 100)\n} .\nTo further reduce the computations, in Algorithm 2 we set Hi,i = \u2016STS\u2016I and the over-relaxation stepsize \u03b8k as \u03b8 in (19). Hence, the compared five solvers GSOS, GFB, PDM, PA-APG and APA-APG have the same computational cost in each iteration. To be fair, all the compared algorithms start with the same initial point. The following six pictures in Figures 1 and 2 display the comparisons of the five solvers for a variety of (n,K). It is apparent that our proposed GSOS algorithm shows great superiorities over the other four solvers. The primal-dual solver PDM is slightly faster than the primal solver GFB. PA-APG is the slowest algorithm, because the prespecified proximal average approximation precision is 1.0e \u2212 5 which leads to a very small stepsize. Also, APA-APG is much faster than the other four solvers at the first 50 iterations. However, it is slowed down since the stepsize used in AP-APG becomes smaller and smaller as the iterations go on."}, {"heading": "4.2. Graph-Guided Fused Lasso", "text": "In this subsection, we perform experiments on graphguided fused Lasso which is formulated as\nmin 1\n2 \u2016Sx\u2212 b\u20162 + \u03bd \u2211 (i,j)\u2208E \u03b1ij |xi \u2212 xj |, (25)\nwhere \u03b1ij \u2265 0 is the weight for the fused term \u2016xi \u2212 xj\u2016 for all (i, j) \u2208 E (E is the given graph edge set), and \u03bd is\nthe regularization parameter.\nIn the implementation of Algorithm 2 for tackling graphguided fused Lasso (25), we need to solve the following optimization in the updating step of yk:\nx\u2217 := arg min x\n1 2 \u2016x\u2212 b\u20162Hi,i + \u03bd|xi \u2212 xj |, (26)\nwhereHi,i is a diagonal positive definite matrix, and b and \u03bd are given constants. Let hii and hjj be the i-th and j-th diagonal elements ofHi,i, respectively.\nProposition 5 The optimal solution of (26) takes the following closed-form:\nx\u2217 =  bl \u2212 h \u22121 ll \u03bb \u2217, l = i, bl + h \u22121 ll \u03bb\n\u2217, l = j, bl, l 6= i, j,\n(27)\nwhere \u03bb\u2217 is defined as\n\u03bb\u2217 =  bi\u2212bj h\u22121ii +h \u22121 jj , \u2223\u2223\u2223 bi\u2212bj h\u22121ii +h \u22121 jj \u2223\u2223\u2223 \u2264 \u03bd; sign (bi \u2212 bj) \u03bd, \u2223\u2223\u2223 bi\u2212bj h\u22121ii +h \u22121 jj\n\u2223\u2223\u2223 > \u03bd. In the implementation, we use the similar parameter settings of S, \u03bd as above. The dimension parameter pair (n, d) is chosen from the following set\n(n, d) \u2208 {\n(2000, 500), (2000, 1000), (5000, 1000), (5000, 2000), (10000, 2000), (10000, 4000)\n} ,\nand the parameter \u03b1i = 100/|E|2. Similarly, all the compared algorithms start with the same initial point. The following six pictures in Figures 3 and 4 display the comparisons of the five solvers for six kinds of choices of (n, d). It\nis obvious that the other four solvers GFB, PDM, AP-APG and APA-APG are not as efficient as the proposed GSOS algorithm, which demonstrates that the Gauss-Seidel technique is very useful for addressing nonsmooth optimization. It is worthwhile to point out that the primal solver GFB is faster than the primal-dual solver PDM on graphguided fused Lasso. One possible reason is that the number of nonsmooth terms is too large, which will lead to a large quantity of dual variables introduced in PDM and hence slow down the updating of primal variables."}, {"heading": "5. Conclusions", "text": "In this paper, we proposed a novel first-order algorithm called GSOS for addressing multi-term nonsmooth convex composite optimization. This algorithm inherits the advantages of the Gauss-Seidel technique and the operator splitting technique, therefore being largely accelerated. We found that the GSOS algorithm includes the generalized forward backward splitting method (Raguet et al., 2013) as a special case. In addition, we developed a new technique to establish the global convergence and iteration complexity of the GSOS algorithm. Last, we applied the proposed GSOS algorithm to solve overlapping group Lasso and graph-guided fused Lasso problems, and compared it against several state-of-the-art algorithms. The experimental results show the great superiority of the GSOS algorithm in terms of both efficiency and effectiveness."}, {"heading": "Acknowledgements", "text": "Yuan is supported by NSF-China (61402182)."}], "year": 2017, "references": [{"title": "Structured sparsity-inducing norms through submodular functions", "authors": ["F.R. Bach"], "venue": "Advances in Neural Information Processing Systems,", "year": 2010}, {"title": "Structured sparsity through convex optimization", "authors": ["F.R. Bach", "R. Jenatton", "J. Mairal", "G. Obozinski"], "venue": "Statistical Science,", "year": 2012}, {"title": "Convex Analysis and Monotone Operator Theory in Hilbert Space", "authors": ["H.H. Bauschke", "P.L. Combettes"], "year": 2011}, {"title": "The proximal average: basic theory", "authors": ["H.H. Bauschke", "R. Goebel", "Y. Lucet", "X. Wang"], "venue": "SIAM Journal on Optimization,", "year": 2008}, {"title": "A fast iterative shrinkagethresholding algorithm for linear inverse problems", "authors": ["A. Beck", "M. Teboulle"], "venue": "SIAM journal on imaging sciences,", "year": 2009}, {"title": "Proximal algorithms for multicomponent image recovery problems", "authors": ["L.M. Briceno-Arias", "P.L. Combettes", "J.C. Pesquet", "N. Pustelnik"], "venue": "Journal of Mathematical Imaging and Vision,", "year": 2011}, {"title": "\u03b5-enlargements of maximal monotone operators in banach spaces", "authors": ["R.S. Burachik", "B.F. Svaiter"], "venue": "Set-Valued Analysis,", "year": 1999}, {"title": "Enlargement of monotone operators with applications to variational inequalities", "authors": ["R.S. Burachik", "A.N. Iusem", "B.F. Svaiter"], "venue": "Set-Valued and Variational Analysis,", "year": 1997}, {"title": "\u03b5enlargements of maximal monotone operators: Theory and applications. In Reformulation: nonsmooth, piecewise smooth, semismooth and smoothing methods", "authors": ["R.S. Burachik", "C.A. Sagastiz\u00e1bal", "B.F. Svaiter"], "year": 1998}, {"title": "A first-order primal-dual algorithm for convex problems with applications to imaging", "authors": ["A. Chambolle", "T. Pock"], "venue": "Journal of Mathematical Imaging and Vision,", "year": 2011}, {"title": "Nonnegativity constraints in numerical analysis", "authors": ["D. Chen", "R.J. Plemmons"], "year": 2015}, {"title": "Convergence rates in forward\u2013backward splitting", "authors": ["G.H.G. Chen", "R.T. Rockafellar"], "venue": "SIAM Journal on Optimization,", "year": 1997}, {"title": "An efficient proximal gradient method for general structured sparse learning", "authors": ["X. Chen", "Q. Lin", "S. Kim", "J.G. Carbonell", "E.P. Xing"], "year": 2011}, {"title": "Smoothing proximal gradient method for general structured sparse regression", "authors": ["X. Chen", "Q. Lin", "S. Kim", "J.G. Carbonell", "E.P. Xing"], "venue": "The Annals of Applied Statistics,", "year": 2012}, {"title": "A douglas\u2013rachford splitting approach to nonsmooth convex variational signal recovery", "authors": ["P.L. Combettes", "J.C. Pesquet"], "venue": "IEEE Journal of Selected Topics in Signal Processing,", "year": 2007}, {"title": "A proximal decomposition method for solving convex variational inverse problems", "authors": ["Combettes", "P. L", "J.C. Pesquet"], "venue": "Inverse problems,", "year": 2008}, {"title": "Proximal splitting methods in signal processing. In Fixed-point algorithms for inverse problems in science and engineering", "authors": ["P.L. Combettes", "J.C. Pesquet"], "year": 2011}, {"title": "Primal-dual splitting algorithm for solving inclusions with mixtures of composite, lipschitzian, and parallel-sum type monotone operators. Set-Valued and variational analysis", "authors": ["P.L. Combettes", "J.C. Pesquet"], "year": 2012}, {"title": "A primal-dual splitting method for convex optimization involving lipschitzian, proximable and linear composite terms", "authors": ["L. Condat"], "venue": "Journal of Optimization Theory and Applications,", "year": 2013}, {"title": "A three-operator splitting scheme and its optimization applications", "authors": ["D. Davis", "W. Yin"], "year": 2015}, {"title": "A proximal iteration for deconvolving poisson noisy images using sparse representations", "authors": ["F.X. Dup\u00e9", "J.M. Fadili", "J.L. Starck"], "venue": "IEEE Transactions on Image Processing,", "year": 2009}, {"title": "On the douglas\u0142rachford splitting method and the proximal point algorithm for maximal monotone operators", "authors": ["J. Eckstein", "D.P. Bertsekas"], "venue": "Mathematical Programming,", "year": 1992}, {"title": "Learning with structured sparsity", "authors": ["J. Huang", "T. Zhang", "D. Metaxas"], "venue": "Journal of Machine Learning Research,", "year": 2011}, {"title": "Group lasso with overlap and graph lasso", "authors": ["L. Jacob", "G. Obozinski", "J.P. Vert"], "venue": "In International Conference on Machine Learning,", "year": 2009}, {"title": "Statistical estimation of correlated genome associations to a quantitative trait network", "authors": ["S. Kim", "E.P. Xing"], "venue": "PLoS Genet,", "year": 2009}, {"title": "Fast proximity-gradient algorithms for structured convex optimization problems", "authors": ["Q. Li", "N. Zhang"], "venue": "Applied & Computational Harmonic Analysis,", "year": 2016}, {"title": "Linearized alternating direction method with adaptive penalty for low-rank representation", "authors": ["Z. Lin", "R. Liu", "Z. Su"], "venue": "Advances in Neural Information Processing Systems,", "year": 2011}, {"title": "On the convergence of a matrix splitting algorithm for the symmetric monotone linear complementarity problem", "authors": ["Z.Q. Luo", "P. Tseng"], "venue": "SIAM Journal on Control and Optimization,", "year": 1991}, {"title": "Network flow algorithms for structured sparsity", "authors": ["J. Mairal", "R. Jenatton", "F.R. Bach", "G.R. Obozinski"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2010}, {"title": "Iteration-complexity of block-decomposition algorithms and the alternating direction method of multipliers", "authors": ["R.D.C. Monteiro", "B.F. Svaiter"], "venue": "SIAM Journal on Optimization,", "year": 2013}, {"title": "Excessive gap technique in nonsmooth convex minimization", "authors": ["Y. Nesterov"], "venue": "SIAM Journal on Optimization,", "year": 2005}, {"title": "Smooth minimization of non-smooth functions", "authors": ["Y. Nesterov"], "venue": "Mathematical programming,", "year": 2005}, {"title": "Gradient methods for minimizing composite functions", "authors": ["Y. Nesterov"], "venue": "Mathematical Programming,", "year": 2007}, {"title": "Parallel proximal algorithm for image restoration using hybrid regularization", "authors": ["N. Pustelnik", "C. Chaux", "J.C. Pesquet"], "venue": "IEEE Transactions on Image Processing,", "year": 2011}, {"title": "A generalized forwardbackward splitting", "authors": ["H. Raguet", "J. Fadili", "G. Peyr\u00e9"], "venue": "SIAM Journal on Imaging Sciences,", "year": 2013}, {"title": "Estimation of simultaneously sparse and low rank matrices", "authors": ["E. Richard", "P.A. Savalle", "N. Vayatis"], "venue": "arXiv preprint arXiv:1206.6474,", "year": 2012}, {"title": "Adaptive proximal average approximation for composite convex minimization", "authors": ["L. Shen", "W. Liu", "J. Huang", "Y.G. Jiang", "S. Ma"], "venue": "In AAAI,", "year": 2017}, {"title": "A family of enlargements of maximal monotone operators", "authors": ["B.F. Svaiter"], "venue": "Set-Valued Analysis,", "year": 2000}, {"title": "A class of fej\u00e9r convergent algorithms, approximate resolvents and the hybrid proximalextragradient method", "authors": ["B.F. Svaiter"], "venue": "Journal of Optimization Theory and Applications,", "year": 2014}, {"title": "A scalable modular convex solver for regularized risk minimization", "authors": ["C.H. Teo", "A. Smola", "S. Vishwanathan", "Q.V. Le"], "venue": "In Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining,", "year": 2007}, {"title": "Bundle methods for regularized risk minimization", "authors": ["C.H. Teo", "S. Vishwanthan", "A.J. Smola", "Q.V. Le"], "venue": "Journal of Machine Learning Research,", "year": 2010}, {"title": "A splitting algorithm for dual monotone inclusions involving cocoercive operators", "authors": ["B.C. V\u0169"], "venue": "Advances in Computational Mathematics,", "year": 2013}, {"title": "A block coordinate descent method for regularized multiconvex optimization with applications to nonnegative tensor factorization and completion", "authors": ["Y. Xu", "W. Yin"], "venue": "Siam Journal on Imaging Sciences,", "year": 2013}, {"title": "Better approximation and faster algorithm using the proximal average", "authors": ["Y. Yu"], "venue": "Advances in Neural Information Processing Systems,", "year": 2013}, {"title": "A matrix splitting method for composite function minimization", "authors": ["G. Yuan", "W.S. Zheng", "B. Ghanem"], "venue": "arXiv preprint arXiv:1612.02317,", "year": 2016}, {"title": "The composite absolute penalties family for grouped and hierarchical variable selection", "authors": ["P. Zhao", "G. Rocha", "B. Yu"], "venue": "The Annals of Statistics,", "year": 2009}, {"title": "Accelerated stochastic gradient method for composite regularization", "authors": ["W. Zhong", "J.T.Y. Kwok"], "venue": "In AISTATS, pp", "year": 2014}, {"title": "Learning social infectivity in sparse low-rank networks using multi-dimensional hawkes processes", "authors": ["K. Zhou", "H. Zha", "L. Song"], "venue": "In AISTATS,", "year": 2013}], "id": "SP:4061a745797e0dd7bf84b452cab71191623ccde2", "authors": [{"name": "Li Shen", "affiliations": []}, {"name": "Wei Liu", "affiliations": []}, {"name": "Ganzhao Yuan", "affiliations": []}, {"name": "Shiqian Ma", "affiliations": []}], "abstractText": "In this paper, we propose a fast Gauss-Seidel Operator Splitting (GSOS) algorithm for addressing multi-term nonsmooth convex composite optimization, which has wide applications in machine learning, signal processing and statistics. The proposed GSOS algorithm inherits the advantage of the Gauss-Seidel technique to accelerate the optimization procedure, and leverages the operator splitting technique to reduce the computational complexity. In addition, we develop a new technique to establish the global convergence of the GSOS algorithm. To be specific, we first reformulate the iterations of GSOS as a twostep iterations algorithm by employing the tool of operator optimization theory. Subsequently, we establish the convergence of GSOS based on the two-step iterations algorithm reformulation. At last, we apply the proposed GSOS algorithm to solve overlapping group Lasso and graph-guided fused Lasso problems. Numerical experiments show that our proposed GSOS algorithm is superior to the state-of-the-art algorithms in terms of both efficiency and effectiveness.", "title": "GSOS: Gauss-Seidel Operator Splitting Algorithm for  Multi-Term Nonsmooth Convex Composite Optimization"}