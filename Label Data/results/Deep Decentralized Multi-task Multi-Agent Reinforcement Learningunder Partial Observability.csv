0,1,label2,summary_sentences
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2832–2838 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"Bidirectional Long Short-Term Memory (BLSTM) based models (Graves and Schmidhuber, 2005), along with word embeddings and character embeddings, have shown competitive performance on Part-of-Speech (POS) tagging given sufficient amount of training examples (Ling et al., 2015; Lample et al., 2016; Plank et al., 2016; Yang et al., 2017).
",1 Introduction,[0],[0]
"Given insufficient training examples, we can improve the POS tagging performance by cross-
lingual POS tagging, which exploits affluent POS tagging corpora from other source languages.",1 Introduction,[0],[0]
"This approach usually requires linguistic knowledge or resources about the relation between the source language and the target language such as parallel corpora (Täckström et al., 2013; Duong et al., 2013; Kim et al., 2015a; Zhang et al., 2016), morphological analyses (Hana et al., 2004), dictionaries (Wisniewski et al., 2014), and gaze features (Barrett et al., 2016).
",1 Introduction,[0],[0]
"Given no linguistic resources between the source language and the target language, transfer learning methods can be utilized instead.",1 Introduction,[0],[0]
"Transfer learning for cross-lingual cases is a type of transductive transfer learning, where the input domains of the source and the target are different (Pan and Yang, 2010) since each language has its own vocabulary space.",1 Introduction,[0],[0]
"When the input space is the same, lower layers of hierarchical models can be shared for knowledge transfer (Collobert et al., 2011; Kim et al., 2015b; Yang et al., 2017), but that approach is not directly applicable when the input spaces differ.
",1 Introduction,[0],[0]
Yang et al. (2017) used shared character embeddings for different languages as a cross-lingual transfer method while using different word embeddings for different languages.,1 Introduction,[0],[0]
"Although the approach showed improved performance on Named Entity Recognition, it is limited to character-level representation transfer and it is not applicable for knowledge transfer between languages without overlapped alphabets.
",1 Introduction,[0],[0]
"In this work, we introduce a cross-lingual transfer learning model for POS tagging requiring no cross-lingual resources, where knowledge transfer is made in the BLSTM layers on top of word embeddings and character embeddings.",1 Introduction,[0],[0]
"Inspired by Kim et al. (2016)’s multi-task slot-filling model, our model utilizes a common BLSTM for representing language-generic information, which al-
2832
lows knowledge transfer from other languages, and private BLSTMs for representing languagespecific information.",1 Introduction,[0],[0]
"The common BLSTM is additionally encouraged to be language-agnostic with language-adversarial training (Chen et al., 2016) so that the language-general representations to be more compatible among different languages.
",1 Introduction,[0],[0]
"Evaluating on POS datasets from 14 different target languages with English as the source language in the Universal Dependencies corpus 1.4 (Nivre et al., 2016), the proposed model showed significantly better performance when the source language and the target language are in the same language family, and competitive performance when the language families are different.",1 Introduction,[0],[0]
Cross-Lingual Training Figure 1 shows the overall architecture of the proposed model.,2 Model,[0],[0]
"The baseline POS tagging model is similar to Plank et al. (2016)’s model, and it corresponds to having only word+char embeddings, common BLSTM, and Softmax Output in Figure 1.",2 Model,[0],[0]
"Given an input
word sequence, a BLSTM is used for the character sequence of each word, where the outputs of the ends of the character sequences from the forward LSTM and the backward LSTM are concatenated to the word vector of the current word to supplement the word representation.",2 Model,[0],[0]
"These serve as an input to a BLSTM, and an output layer are used for POS tag prediction.
",2 Model,[0],[0]
"For the cross-lingual transfer learning, the character embedding, the BLSTM with the character embedding (Yang et al., 2017),1 and the common BLSTM are shared for all the given languages while word embeddings and private BLSTMs have different parameters for different languages.
",2 Model,[0],[0]
The outputs of the common BLSTM and the private BLSTM of the current language are summed to be used as the input to the softmax layer to predict the POS tags of given word sequences.,2 Model,[0],[0]
"The loss function of the POS tagging can be formulate as:
Lp = − S∑
i=1",2 Model,[0],[0]
"N∑ j=1 pi,j log (p̂i,j) , (1)
where S is the number of sentences in the current minibatch,N is the number of words in the current sentence, pi,j is the label of the j-th tag of the i-th sentence in the minibatch, and p̂i,j is the predicted tag.",2 Model,[0],[0]
"In addition to this main objective, two more objectives for improving the transfer learning are described in the following subsections.
",2 Model,[0],[0]
"Language-Adversarial Training We encourage the outputs of the common BLSTM to be language-agnostic by using language-adversarial training (Chen et al., 2016) inspired by domainadversarial training (Ganin et al., 2016; Bousmalis et al., 2016).",2 Model,[0],[0]
"First, we encode a BLSTM output sequence as a single vector using a CNN/MaxPool encoder, which is implemented the same as a CNN for text classification (Kim, 2014).",2 Model,[0],[0]
"The encoder is with three convolution filters whose sizes are 3, 4, and 5.",2 Model,[0],[0]
"For each filter, we pass the BLSTM output sequence as the input sequence and obtain a single vector from the filter output by using max pooling, and then tanh activation function is used for transforming the vector.",2 Model,[0],[0]
"Then, the vector outputs of the three filters are concatenated and forwarded to the language discriminator through the gradient reversal layer.",2 Model,[0],[0]
"The discriminator is implemented
1We also tried isolated character-level modules but the overall performance was worse.
as a fully-connected neural network with a single hidden layer, whose activation function is Leaky ReLU (Maas et al., 2013), where we multiply 0.2 to negative input values as the outputs.
",2 Model,[0],[0]
"Since the gradient reversal layer is below the language classifier, the gradients minimizing language classification errors are passed back with opposed sign to the sentence encoder, which adversarially encourages the sentence encoder to be language-agnostic.",2 Model,[0],[0]
"The loss function of the language classifier is formulated as:
La = − S∑
i=1
li log l̂i, (2)
where S is the number of sentences, li is the language of the i-th sentence, and l̂i is the softmax output of the tagging.",2 Model,[0],[0]
"Note that though the language classifier is optimized to minimize the language classification error, the gradient from the language classifier is negated so that the bottom layers are trained to be language-agnostic.
",2 Model,[0],[0]
"Bidirectional Language Modeling Rei (2017) showed the effectiveness of the bidirectional language modeling objective, where each time step of the forward LSTM outputs predicts the word of the next time step, and each of the backward LSTM outputs predicts the previous word.",2 Model,[0],[0]
"For example, if the current sentence is “I am happy”, the forward LSTM predicts “am happy <eos>” and the backward LSTM predicts “<bos> I am”.",2 Model,[0],[0]
"This objective encourages the BLSTM layers and the embedding layers to learn linguistically general-purpose representations, which are also useful for specific downstream tasks (Rei, 2017).",2 Model,[0],[0]
"We adopted the bidirectional language modeling objective, where the sum of the common BLSTM and the private BLSTM is used as the input to the language modeling module.",2 Model,[0],[0]
"It can be formulated as:
Ll = − S∑
i=1",2 Model,[0],[0]
N∑ j=1 log (P (wj+1|fj)),2 Model,[0],[0]
"+
log (P (wj−1|bj)) , (3)
where fj and bj represent the j-th outputs of the forward direction and the backward direction, respectively, given the output sum of the common BLSTM and the private BLSTM.
",2 Model,[0],[0]
"All the three loss functions are added to be optimized altogether as:
L = ws",2 Model,[0],[0]
"(Lp + λLa + λLl) , (4)
where λ is gradually increased from 0 to 1 as epoch increases so that the model is stably trained with auxiliary objectives (Ganin et al., 2016).",2 Model,[0],[0]
ws is used to give different weights to the source language and the target language.,2 Model,[0],[0]
"Since the source language has a larger train set and we are focusing on improving the performance of the target language, ws is set to 1 when training the target language.",2 Model,[0],[0]
"For the source language, instead, it is set as the size of the target train set divided by the size of the source train set.",2 Model,[0],[0]
"For the evaluation, we used the POS datasets from 14 different languages in Universal Dependencies corpus 1.4 (Nivre et al., 2016).",3 Experiments,[0],[0]
"We used English as the source language, which is with 12,543 training sentences.2",3 Experiments,[0],[0]
We chose datasets with 1k to 14k training sentences.,3 Experiments,[0],[0]
"The number of tag labels differs for each language from 15 to 18 though most of them are overlapped within the languages.
",3 Experiments,[0],[0]
"Table 1 shows the POS tagging accuracies of different transfer learning models when we limited the number of training sentences of the target languages to be the same as 1,280 for fair comparison among different languages.",3 Experiments,[0],[0]
The remainder training examples of the target languages are still used for both language-adversarial training and bidirectional language modeling since the objectives do not require tag labels.,3 Experiments,[0],[0]
Training with only the train sets in the target languages (c) showed 91.61% on average.,3 Experiments,[0],[0]
"When bidirectional language modeling objective is used (c, l), the accuracies were significantly increased to 92.82% on average.",3 Experiments,[0],[0]
"Therefore, we used the bidirectional language modeling for all the transfer learning evaluations.
",3 Experiments,[0],[0]
"With transfer learning, the three cases of using only the common BLSTM (c), using only the private BLSTMs (p), and using both (c, p) were evaluated.",3 Experiments,[0],[0]
"They showed better average accuracies than target only cases, but they showed mixed results.",3 Experiments,[0],[0]
"However, our proposed model (c, p, l + a), which utilizes both the common BLSTM with language-adversarial training and the private BLSTMs, showed the highest average score, 93.26%.",3 Experiments,[0],[0]
"For all the Germanic languages, where the source language also belongs to, the accuracies are significantly higher than those of
2The accuracies of English POS tagging are 94.01 and 94.33 for models without the bidirectional language modeling and with it, respectively.
",3 Experiments,[0],[0]
other transfer learning models.,3 Experiments,[0],[0]
"For the languages belonging to Slavic, Romance, or Indo-Iranian, our model shows competitive performance with the highest average accuracies among the compared models.",3 Experiments,[0],[0]
"Since languages in the same family are more likely to be similar and compatible, it is expected that the gain from the knowledge transfer to the languages in the same family to be higher than transferring to the languages in different families, which was shown in the results.",3 Experiments,[0],[0]
"This shows that utilizing both language-general representations that are encouraged to be more language-agnostic and language-specific representations effectively helps improve the POS tagging performance with transfer learning.
",3 Experiments,[0],[0]
Table 2 shows the results when using 320 taglabeled training sentences.,3 Experiments,[0],[0]
"In this case, transfer learning methods still show better accuracies than target-only approaches on average.",3 Experiments,[0],[0]
"However, the performance gain is weakened compared to using 1,280 labeled training sentences and there are some mixed results.",3 Experiments,[0],[0]
"In several cases, just utilizing private BLSTMs without the common BLSTM showed better accuracies than utilizing the common BLSTM.
",3 Experiments,[0],[0]
"When training with only 32 tag-labeled sentences, which is an extremely low-resourced setting, transfer learning methods still showed better accuracies than target-only methods on average.",3 Experiments,[0],[0]
"However, not using the common BLSTM
in transfer learning models showed better performance than using it on average.3",3 Experiments,[0],[0]
The main reason would be that we are not given a sufficient number of labeled training sentences to train both the common BLSTM and the private BLSTMs.,3 Experiments,[0],[0]
"In this case, just having private BLSTMs without the common BLSTM can show better performance.",3 Experiments,[0],[0]
"We also evaluated the opposite cases, which use all the tag-labeled training sentences in the target languages, and they showed mixed results.",3 Experiments,[0],[0]
"For example, the accuracy of German with the target only model is 93.31% while that of the proposed model is 93.04%.",3 Experiments,[0],[0]
"This is expected since transfer learning is effective when the target train set is small.
",3 Experiments,[0],[0]
An extension of this work is utilizing multiple languages as the source languages.,3 Experiments,[0],[0]
"Since we have four languages for each of Germanic, Slavic, and Romance language families, we evaluated the performance of those languages using the other languages in the same families as the source languages expecting that languages in the same language family are more likely to be helpful each other.",3 Experiments,[0],[0]
"For the efficiency, we performed multi-task learning for multiple languages rather than differentiating the targets from sources.",3 Experiments,[0],[0]
"When we tried to use 1,280, 320, and 32 tag-labeled training sentences for each language in the multi-source settings, the results showed noticeably better per-
3The results in detail are shown in the first authors dissertation Kim (2017).
formance than the results of using English as a single source language.",3 Experiments,[0],[0]
"Considering that utilizing 1,280*3=3,840, 320*3=960, or 32*3=96 tag labels from three other languages showed better results than using 12,543 English tag labels as the source, we can see that the knowledge transfer from multiple languages can be more helpful than that from single resource-rich source language.",3 Experiments,[0],[0]
"We also tried to use Wasserstein distance (Arjovsky et al., 2017) for the adversarial training in the multi-source settings, but there were no significant differences on average.4
Implementation Details All the models were optimized using ADAM (Kingma and Ba, 2015)5 with minibatch size 32 for total 100 epochs and we picked the parameters showing the best accuracy on the development set to report the score on the test set.",3 Experiments,[0],[0]
The dimensionalites of all the BLSTM related layers follow Plank et al. (2016)’s model.,3 Experiments,[0],[0]
Each word vector is 128 dimensional and each character vector is 100 dimensional.,3 Experiments,[0],[0]
"They are randomly initialized with Xavier initialization (Glorot and Bengio, 2010).",3 Experiments,[0],[0]
"For stable training, we use gradient clipping, where the threshold is set to 5.",3 Experiments,[0],[0]
"The dimensionality of each hidden output of LSTMs is 100, and the hidden outputs of both forward LSTM and backward LSTM are concatenated, thereby the output of each BLSTM for each time step is 200.",3 Experiments,[0],[0]
"Therefore, the input to the common BLSTM and the private BLSTM is 128+200=328
4The extended work in detail are shown in Kim (2017).",3 Experiments,[0],[0]
"5learning rate=0.001, β1 = 0.9, β2 = 0.999, = 1e− 8.
dimensional.",3 Experiments,[0],[0]
"The inputs and the outputs of the BLSTMs are regularized with dropout rate 0.5 (Pham et al., 2014).",3 Experiments,[0],[0]
"For the consistent dropout usages, we let the dropout masks to be identical for all the time steps of each sentence (Gal and Ghahramani, 2016).",3 Experiments,[0],[0]
"For all the BLSTMs, forget biases are initialized with 1 (Jozefowicz et al., 2015) and the other biases are initialized with 0.",3 Experiments,[0],[0]
"Each convolution filter output for the sentence encoding is 64 dimensional, and the three filter outputs are concatenated to represent each sentence with a 192 dimensional vector.",3 Experiments,[0],[0]
We introduced a cross-lingual transfer learning model for POS tagging which uses separate BLSTMs for language-general and languagespecific representations.,4 Conclusion,[0],[0]
"Evaluating on 14 different languages, including the source language improved tagging accuracies in almost all the cases.",4 Conclusion,[0],[0]
"Specifically, our model showed noticeably better performance when the source language and the target languages belong to the same language family, and competitively performed with the highest average accuracies for target languages in different families.",4 Conclusion,[0],[0]
We thank the anonymous reviewers for their helpful comments.,Acknowledgments,[0],[0]
All the experiments in this work were conducted with machines at Ohio Supercomputer Center (1987).,Acknowledgments,[0],[0]
Training a POS tagging model with crosslingual transfer learning usually requires linguistic knowledge and resources about the relation between the source language and the target language.,abstractText,[0],[0]
"In this paper, we introduce a cross-lingual transfer learning model for POS tagging without ancillary resources such as parallel corpora.",abstractText,[0],[0]
"The proposed cross-lingual model utilizes a common BLSTM that enables knowledge transfer from other languages, and private BLSTMs for language-specific representations.",abstractText,[0],[0]
The cross-lingual model is trained with language-adversarial training and bidirectional language modeling as auxiliary objectives to better represent language-general information while not losing the information about a specific target language.,abstractText,[0],[0]
"Evaluating on POS datasets from 14 languages in the Universal Dependencies corpus, we show that the proposed transfer learning model improves the POS tagging performance of the target languages without exploiting any linguistic knowledge between the source language and the target language.",abstractText,[0],[0]
Cross-Lingual Transfer Learning for POS Tagging without Cross-Lingual Resources,title,[0],[0]
"Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 998–1008, Baltimore, Maryland, USA, June 23-25 2014. c©2014 Association for Computational Linguistics",text,[0],[0]
"Discourse structure, logical flow of sentences, and context play a large part in ordering medical events based on temporal relations within a clinical narrative.",1 Introduction,[0],[0]
"However, cross-narrative temporal relation ordering is a challenging task as it is difficult to learn temporal relations among medical events which are not part of the logically coherent discourse of a single narrative.",1 Introduction,[0],[0]
"Resolving crossnarrative temporal relationships between medical events is essential to the task of generating an event timeline from across unstructured clinical narratives such as admission notes, radiology reports, history and physical reports and discharge summaries.",1 Introduction,[0],[0]
"Such a timeline has multiple applications in clinical trial recruitment (Luo et al., 2011), medical document summarization (Bramsen et al.,
2006, Reichert et al., 2010) and clinical decision making (Demner-Fushman et al., 2009).
",1 Introduction,[0],[0]
"Given multiple temporally ordered medical event sequences generated from each clinical narrative in a patient record, how can we combine the events to create a timeline across all the narratives?",1 Introduction,[0],[0]
"The tendency to copy-paste text and summarize past information in newly generated clinical narratives leads to multiple mentions of the same medical event across narratives (Cohen et al., 2013).",1 Introduction,[0],[0]
These cross-narrative coreferences act as important anchors for reasoning with information across narratives.,1 Introduction,[0],[0]
We leverage crossnarrative coreference information along with confident cross-narrative temporal relation predictions and learn to align and temporally order medical event sequences across longitudinal clinical narratives.,1 Introduction,[0],[0]
We model the problem as a sequence alignment task and propose solving this using two approaches.,1 Introduction,[0],[0]
"First, we use weighted finite state machines to represent medical events sequences, thus enabling composition and search to obtain the most probable combined sequence of medical events.",1 Introduction,[0],[0]
"As a contrast, we adapt dynamic programming algorithms (Needleman et al., 1970, Smith and Waterman, 1981) used to produce global and local alignments for aligning sequences of medical events across narratives.",1 Introduction,[0],[0]
"We also compare the proposed methods with an Integer Linear Programming (ILP) based method for timeline construction (Do et al., 2012).",1 Introduction,[0],[0]
"The cross-narrative coreference and temporal relation scores used in both these approaches are learned from a corpus of patient narratives from The Ohio State University Wexner Medical Center.
",1 Introduction,[0],[0]
The main contribution of this paper is a general framework that allows aligning multiple event sequences using cascaded weighted finite state transducers (WFSTs) with the help of efficient composition and decoding.,1 Introduction,[0],[0]
"Moreover, we demonstrate that this method can be used for more accurate multiple sequence alignment when compared to
998
dynamic programming or other ILP-based methods proposed in literature.",1 Introduction,[0],[0]
"In the areas of summarization and text-to-text generation, there has been prior work on several ordering strategies to order pieces of information extracted from different input documents (Barzilay et al., 2002, Lapata, 2003, Bollegala et al., 2010).",2 Related Work,[0],[0]
"In this paper, we focus on temporal ordering of information, as discussed next.
",2 Related Work,[0],[0]
"Recent state-of-the art research has focused on the problem of temporal relation learning within the same document, and in many cases within the same sentence (Mani et al., 2006, Verhagen et al., 2009, Lapata and Lascarides, 2011).",2 Related Work,[0],[0]
"Chambers and Jurafsky (2009) describe a process to induce a partially ordered set of events related by a common protagonist by using an unsupervised distributional method to learn relations between events sharing coreferring arguments, followed by temporal classification to induce partial order.",2 Related Work,[0],[0]
"The task was carried out on the Timebank newswire corpus, but was limited to an intra-document setting.",2 Related Work,[0],[0]
"More recently, (Do et al., 2012) proposed an ILP-based method to combine the outputs of an event-interval and an event-event classifier for timeline construction on the ACE 2005 corpus.",2 Related Work,[0],[0]
"However, this approach is also restricted to events within documents and requires annotations for event intervals.",2 Related Work,[0],[0]
We empirically compare our methods for timeline creation from longitudinal clinical narratives to such an ILP-based approach in Section 7.,2 Related Work,[0],[0]
"While a lot of this work has been done in the news domain, there is also some recent work in rule-based algorithms (Zhou et al., 2006) and machine learning (Roberts et al., 2008) applied to temporal relations between medical events in clinical text.",2 Related Work,[0],[0]
"Clinical narratives are written in a distinct sub-language with domain specific terminology and temporal characteristics, making them markedly different from newswire text.
",2 Related Work,[0],[0]
There is limited prior work in learning relations across documents.,2 Related Work,[0],[0]
"Ji and Grishman (2008) extended the one sense per discourse idea (Yarowsky, 1995) to multiple topically related documents and propagate consistent event arguments across sentences and documents.",2 Related Work,[0],[0]
Barzilay and McKeown (2005) propose a text-to-text generation technique for synthesizing common information across documents using sentence fusion.,2 Related Work,[0],[0]
"This involves multisequence dependency tree alignment to identify phrases conveying sim-
ilar information and statistical generation to combine common phrases into a sentence.",2 Related Work,[0],[0]
"Along with syntactic features, they combine knowledge from resources like WordNet to find similar sentences.",2 Related Work,[0],[0]
"In case of clinical narratives and medical event alignment, the objective is to identify a unique sequence of temporally ordered medical events from across longitudinal clinical data.
",2 Related Work,[0],[0]
"To the best of our knowledge, there is no prior work on cross-document alignment of event sequences.",2 Related Work,[0],[0]
"Multiple sequence alignment is a problem that arises in a variety of domains including gene/protein alignments in bioinformatics (Notredame, 2002), word alignments in machine translation (Kumar and Byrne, 2003), and sentence alignments for summarization (Lacatusu et al., 2004).",2 Related Work,[0],[0]
"Dynamic programming algorithms have been popularly leveraged to produce pairwise and global genetic alignments, where edit distance based metrics are used to compute the cost of insertions, deletions and substitutions.",2 Related Work,[0],[0]
"We use dynamic programming to compute the best alignment, given the temporal and coreference information between medical events across these sequences.",2 Related Work,[0],[0]
"More importantly, we propose a cascaded WFST-based framework for crossdocument temporal ordering of medical event sequences.",2 Related Work,[0],[0]
"Composition and search operations can be used to build a single transducer that integrates these components, directly mapping from input states to desired outputs, and obtain the best alignment (Mohri et al., 2000).",2 Related Work,[0],[0]
"In natural language processing, WFSTs have seen varied applications in machine translation (Kumar and Byrne, 2003), morphology (Sproat, 2006), named entity recognition (Krstev et al., 2011) and biological sequence alignment / generation (Whelan et al., 2010) among others.",2 Related Work,[0],[0]
We demonstrate that the WFST-based approach outperforms popularly used dynamic programming algorithms for multiple sequence alignment.,2 Related Work,[0],[0]
"Medical events are temporally-associated concepts in clinical text that describe a medical condition affecting the patient’s health, or procedures performed on a patient.",3 Problem Description,[0],[0]
We represent medical events by splitting each event into a start and a stop.,3 Problem Description,[0],[0]
"When there is insufficient information to discern the start or stop of an event, it is represented as a single concept.",3 Problem Description,[0],[0]
"If only the start is known then the stop is set to +∞, whereas when only the stop is known , the start is set to the date of birth of the
patient.1 Often, for chronic ailments like hypertension, we would only associate a start with the medical event and set the stop to +∞. The start of hypertension may be associated with the temporal expression history of in the narrative.",3 Problem Description,[0],[0]
"This, when considered along with the admission date, allows us to relatively order hypertension with respect to other medical events.",3 Problem Description,[0],[0]
"A medical event occurrence like chest pain may be associated with a start and a stop, where the start may be determined by the mention of “patient was complaining of chest pain yesterday” in the narrative text.",3 Problem Description,[0],[0]
"Further, the narrative may state that “he continued to have chest pain on admission, but currently he is chest pain free”; this may be used to infer the relative stop of chest pain.",3 Problem Description,[0],[0]
"Medical events may also be instantaneous, for e.g., injected with antibiotic.",3 Problem Description,[0],[0]
Such events are represented with the start and stop as being the same.,3 Problem Description,[0],[0]
Temporal relations exist between the start and stop of events as shown in Figure 1.,3 Problem Description,[0],[0]
"Learning temporal relations before, after and simultaneous between the medical event starts and stops corresponds to learning all of Allen’s temporal relations (Allen, 1981) between the medical events.",3 Problem Description,[0],[0]
"Following our previous work (Raghavan et al., 2012c), such a representation allows us to temporally order the event starts and stops within each clinical narrative by learning to rank them in relative order of time.",3 Problem Description,[0],[0]
"The problem definition is as follows:
1Patient date of birth, admission/ discharge date are usually available in the metadata associated with a clinical narrative.
",3 Problem Description,[0],[0]
Input: Sequences of temporally ordered medical event starts and stops.,3 Problem Description,[0],[0]
"This corresponds to N1, N2, and N3 in Figure 2.",3 Problem Description,[0],[0]
Each sequence corresponds to a clinical narrative.,3 Problem Description,[0],[0]
"The total number of sequences correspond to the number of clinical narratives for a patient.
",3 Problem Description,[0],[0]
Problem:,3 Problem Description,[0],[0]
"Combine medical events across these sequences to generate a timeline i.e., a single comprehensive sequence of medical events over all clinical narratives of the patient.
",3 Problem Description,[0],[0]
Expected Output:,3 Problem Description,[0],[0]
"In the example shown in Figure 2, the output would be as follows:",3 Problem Description,[0],[0]
"Timeline (N1, N2, N3)= {cocaine usestart < hypertensionstart = hypertensionstart < admission1 < chest painstart ∼ palpitationsstart < chest painstop < heart attackstart = myocardial infarctionstart <",3 Problem Description,[0],[0]
"admission2 < infectionstart < MRSAstart < admission3 < woundsstart}.
",3 Problem Description,[0],[0]
The goal of multiple sequence alignment is to find an alignment that maximizes some overall alignment score.,3 Problem Description,[0],[0]
"Thus, in order to align event sequences, we need to compute scores corresponding to cross-narrative medical event coreference resolution and cross-narrative temporal relations.",3 Problem Description,[0],[0]
"The first approach to learning a temporal ordering of medical events across all clinical narratives is to consider all pairs of events across all narratives and learn to classify them as sharing one of Allen’s temporal relations (Allen, 1981) using a single learning model.",4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[0],[0]
"Alternatively, a ranking ap-
proach, similar to the one used to generate intranarrative temporal ordering, can also be extended to the cross-narrative case.",4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[0],[0]
"However, the features related to narrative structure and relative and implicit temporal expressions used for temporal ordering within a clinical narrative may not be applicable across narratives.",4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[0],[0]
"For instance, a history and physical report may have sections like “past medical history”, “history of present illness”, “assessment and plan”, and a certain logical pattern to the flow of text within and across these sections.",4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[0],[0]
"Further, temporal cues like “thereafter”, “subsequently”, follow from the context around an event mention.",4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[0],[0]
"The absence of such features in the cross-narrative case does not allow such a model to generate accurate temporal relation predictions.
",4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[0],[0]
"Thus, for use in our sequence alignment models, we learn two independent classifiers for medical event coreference and temporal relation learning across narratives.",4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[0],[0]
We train a classifier to resolve cross-narrative coreferences by extracting semantic and temporal relatedness feature sets for each pair of medical concepts.,4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[0],[0]
"Extracting these feature sets helps us train a classifier to predict medical event coreferences (Raghavan et al., 2012a).",4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[0],[0]
"Another classifier is then trained to classify pairs of medical event starts and stops across narratives as sharing temporal relations {before, after, overlaps}.",4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[0],[0]
The learned cross-narrative coreference predictions can then be used along with confident temporal relation predictions to derive a joint probability to enable cross-narrative temporal ordering.,4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[0],[0]
Sequence alignment algorithms have been developed and popularly used in bioinformatics.,5 Narrative Sequence Alignment for Cross-narrative Temporal Ordering,[0],[0]
"However, multiple sequence alignment (MSA) has been shown to be NP complete (Wang and Jiang, 1994) and various heuristic algorithms have been proposed to solve this problem (Notredame, 2002).",5 Narrative Sequence Alignment for Cross-narrative Temporal Ordering,[0],[0]
"We propose a novel WFST-based representation that enables accurate decoding for MSA when compared to popularly used dynamic programming algorithms (Needleman et al., 1970, Smith and Waterman, 1981) or other state of the art methods (Do et al., 2012).
",5 Narrative Sequence Alignment for Cross-narrative Temporal Ordering,[0],[0]
"In the problem of aligning events across multiple narrative sequences, we want to align temporally ordered medical events corresponding to clinical narratives of a patient.",5 Narrative Sequence Alignment for Cross-narrative Temporal Ordering,[0],[0]
"Unlike problems in biological sequence alignment where the sym-
bols to be aligned across sequences are restricted to a fixed set, our symbol set is not fixed or certain because the symbols correspond to medical events in clinical narratives.",5 Narrative Sequence Alignment for Cross-narrative Temporal Ordering,[0],[0]
"Moreover, we cannot have fixed scores for symbol transformations since our transformations correspond to coreference and temporal relations between the medical events across sequences.",5 Narrative Sequence Alignment for Cross-narrative Temporal Ordering,[0],[0]
The computation of these scores is described next.,5 Narrative Sequence Alignment for Cross-narrative Temporal Ordering,[0],[0]
"Let us assume a, b are medical events in the first clinical narrative and have been temporally ordered so a < b.",5.1 Scoring Scheme,[0],[0]
"Similarly, x, y are medical events in the second clinical narrative such that x",5.1 Scoring Scheme,[0],[0]
< y.,5.1 Scoring Scheme,[0],[0]
"There exists a match or an alignment between a pair of medical events, across the sequences, in the following cases:
1.",5.1 Scoring Scheme,[0],[0]
"If the medical events are simultaneous and coreferring, denoted as a = x.
2.",5.1 Scoring Scheme,[0],[0]
"If the medical events are simultaneous and non-coreferring, denoted as a ∼ x.
3.",5.1 Scoring Scheme,[0],[0]
"If the a medical event from one sequence is before a medical event from another sequence, denoted as a < x.
4.",5.1 Scoring Scheme,[0],[0]
"If the a medical event from one sequence is after a medical event from another sequence, denoted as a > x.
We now illustrate how the scores for candidate aligned sequences are computed using the learned cross-narrative coreference and temporal probabilities for the following three scenarios:
• The medical events across sequences are simultaneous and corefer as illustrated in Figure 3.",5.1 Scoring Scheme,[0],[0]
"The joint score considers the probability of event temporal relations simultaneous conditioned on coreference.
",5.1 Scoring Scheme,[0],[0]
•,5.1 Scoring Scheme,[0],[0]
Some medical events across sequences are simultaneous but do not corefer as illustrated in Figure 4.,5.1 Scoring Scheme,[0],[0]
"Here, the joint score considers the joint probability of temporal relations simultaneous or before and no-coreference.
",5.1 Scoring Scheme,[0],[0]
•,5.1 Scoring Scheme,[0],[0]
The medical events across sequences are not simultaneous and do not corefer as illustrated in Figure 5.,5.1 Scoring Scheme,[0],[0]
"In this case, the joint score considers the probability of the temporal relation before and no coreference.
",5.1 Scoring Scheme,[0],[0]
"Thus, the coreference and temporal relation scores can be leveraged for aligning sequences of medical events.",5.1 Scoring Scheme,[0],[0]
"These scores are used in both the WFSTbased representation and decoding, as well as for dynamic programming.",5.1 Scoring Scheme,[0],[0]
"A weighted finite-state transducer (WFST) is an automaton in which each transition between states
is associated with an input symbol, an output symbol, and a weight (Mohri et al., 2005).",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
WFSTs can be used to efficiently represent and combine sequences of medical events based coreference and temporal relation information.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
The WFST representation gives us the ability to talk about the global joint probability derived from coreference and temporal relation scores described in Section 5.1.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
It allows us to build a weighted lattice of sequences that can be searched for the most probable sequence of medical events from across all clinical narratives of a patient.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"We use unweighted FSAs to represent the input described in Section 3, i.e. temporally ordered sequences of medical events corresponding to clinical narratives.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"This corresponds to N1 and N2 in Figure 6.
",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"Based on whether we want to align the sequences purely based on coreference scores or both coreference and temporal relation scores, the arc weights for the WFST can be determined.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
M c12 is a WFST that maps input symbols from N1 to output symbols inN2 and is weighted by the probability of coreference or no-coreference between medical events across N1 and N2.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
The representation in WFST M c+t12 shown in Figure 7 allows us to align N1 and N2 based on both coreference as well as temporal relation probabilities.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
The WFST has transitions to accommodate insertion and deletion of medical events when combining the sequences.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
Deletions correspond to the case when an event in the first sequence does not map to any event in the second sequence; similarly insertions correspond to the case where an event in the second sequence does not map to any event in the first sequence.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
The WFST composition operation allows the outputs of one WFST to be fed to the inputs of a second WFST or FSA.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"Thus, we build our final machine by composing the three sub-machines as,
D = N1 ◦M i12 ◦N2.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
(1) where i = c or i = c + t.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
This gives us a combined weighted graph by mapping the output symbols of the first medical event sequence to the input symbols of the second medical event sequence.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"The scores on the decoding graph are derived from only the coreference probabilities if i = c and both coreference and temporal relation probabilities if i = c+ t.
In the medical event sequence alignment problem, we want to align multiple sequences of medical events that correspond to multiple clinical narratives of a patient.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"Since we want to now combine
all narrative chains belonging to the same patient, the composition cascade to build the final combined sequence will be as,
Df = N1◦M i12◦N2◦M i23◦N3◦M i34...◦",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"Nn (2)
where i = c",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
or i = c + t and n is the number of medical event sequences corresponding to clinical narratives for a patient.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"During composition we retain intermediate paths like M i23 utilizing the ability to do lazy composition (Mohri and Pereira, 1998) in order to facilitate beam search through the multi-alignment.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
The best hypothesis corresponds to the highest scoring path which can be obtained using shortest path algorithms like Djikstra’s algorithm.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"The best path corresponds to the best alignment across all medical event sequences based on the joint probability of cross-narrative medical event coreferences and temporal relations across the narrative sequences.
",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"The complexity of decoding increases exponentially with the number of narrative sequences in
the composition, and exact decoding becomes infeasible.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"One solution to this problem is to do the alignment greedily pairwise, starting from the most recent medical event sequences, finding the best path, and iteratively moving on to the next sequence, and proceeding until the oldest medical event sequence.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"The disadvantage of such a method is that it does not take into account constraints between medical events across multiple event sequences and may lead to a less accurate solution.
",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
An alternative method is to use lazy composition to perform more efficient composition as it allows practical memory usage.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"We also use beam search to make for an efficient approximation to the best-path computation (Mohri et al., 2005).",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
This allows accommodating constraints from across multiple sequences and generates a more accurate best path.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"Thus, this method generates more accurate alignments when we have more than two sequences to be aligned.
",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"For instance, instance say a, b ∈ N1, x, y ∈ N2, and m,",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"n ∈ N3 are temporally medical event sequences corresponding to narratives N1, N2 and N3.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"Based on the learned pairwise temporal relations, if we have the following constraints a < x, m > x, m < a.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"Aligning N1 and N2 greedily pairwise may give us the best combined sequence as a, x, b, y ∈ N12.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"Now in aligning N12 with N3, we won’t be able to accommodate m > x",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
and m < a.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"However, performing a beam search over the composed WFST in equation 2 allows us to accommodate such constraints across multiple sequences.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"The complexity of composing two transducers is O(V1V2D1(logD2 + M2)) where each edge from the first sequence matches every edge in the second sequence and Vi is the number of states, Di is the maximum out-degree and Mi maximum multiplicity for the ith FST (Mohri et al., 2005).
",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"We also use popular dynamic programming algorithms (Needleman et al., 1970, Smith and Waterman, 1981) for sequence alignment of medical events across narratives and compare it to the WFST-based representation and decoding.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"As a contrast, we adapt two dynamic programming algorithms for sequence alignment: global alignment using the Needleman Wunsch algorithm (NW) (Needleman et al., 1970) and local alignment using the Smith-Waterman algorithm (SW) (Smith and Waterman, 1981).",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
NW allows us to align all events in one sequence with all events in another sequence.,5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
A drawback of NW is that short and highly similar sequences maybe missed because they get overweighted by the rest of the sequence.,5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
NW is suitable when the two sequences are of similar length with significant degree of similarity throughout.,5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"On the other hand, SW gives the longest sub-sequence pair that yields maximum degree of similarity between the two original sequences.",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
It does not force all events in a sequence to align with another sequence.,5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
SW is useful in aligning sequences that differ in length and have short patches of similarity.,5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"The time complexity of these methods for sequences of length m and n are O(mn).
",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
The scoring scheme described earlier is used to update the scoring matrix for dynamic programming.,5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"In order to accommodate the temporal relations before and after, we insert a null symbol after every medical event in each sequence in the scoring matrix.",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"A vertical or horizontal gap arises when cases 1, 2, 3 and 4 in Section 5.1 mentioned
above are not true.",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"If the medical events are not simultaneous, not before or not after, the medical events will not align.",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"Thus, the value of each cell in the scoring matrix is determined by computing the maximum score at each position C(i, j) as,
max{(C(i−1, j−1)+Sij), (C(i, j−1)+w), (C(i− 1, j) + w)} (3)
where, Sij = max{P (i = j), P (i < j), P (i > j)}, and w = max{(1",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"− P (i = j)), (1 − P (i < j)), (1 − P (i > j))}.",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"Here, C(i − 1, j − 1) corresponds to a match, whereas C(i, j − 1) and C(i − 1, j) correspond to a gaps in sequence one and two.
",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"In case of the SW algorithm, the negative scoring matrix cells are set to zero, thus making the positively scoring local alignments visible.",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"Backtracking starts at the highest scoring matrix cell and proceeds until a cell with score zero is encountered, yielding the highest scoring local alignment.
",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
The time and space complexity grows exponentially with the number of sequences to be aligned and finding the global optimum has been shown to be a NP-complete problem.,5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"The time complexity of aligning N sequences of length L is O(2NLN ) (Wang and Jiang, 1994).",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"Thus, for MSA using dynamic programming, we use a heuristic method where we combine pairwise alignments iteratively starting with the latest narrative and progressing towards the oldest narrative.",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
Corpus Description.,6 Experiments and Evaluation,[0],[0]
The corpus consists of a dataset of clinical narratives obtained from the [redacted] medical center.,6 Experiments and Evaluation,[0],[0]
"The corpus has a total of 2060 patients, and 100704 clinical narratives.",6 Experiments and Evaluation,[0],[0]
"We gathered a gold standard set of seven patients (80 clinical narratives overall) with manual annotation of all medical events mentioned in the narratives, coreferences, and medical event sequence information.",6 Experiments and Evaluation,[0],[0]
"The annotation agreement across annotators is high, with 89.5% agreement corresponding to inter-annotator Cohen’s kappa statistic of 0.86 (Raghavan et al., 2012b).",6 Experiments and Evaluation,[0],[0]
"The types of clinical narratives included 27 discharge summaries, 30 history and physical reports, 15 radiology reports and 8 pathology reports.",6 Experiments and Evaluation,[0],[0]
The distribution of the number of medical event sequences and unique medical events across patients is shown in Table 1.,6 Experiments and Evaluation,[0],[0]
"The annotated dataset is used to crossvalidate and train our coreference and temporal relation learning models and to evaluate our crossnarrative medical event timeline.
",6 Experiments and Evaluation,[0],[0]
Evaluation Metric.,6 Experiments and Evaluation,[0],[0]
"For each patient and each method (WFST or dynamic programming), the output timeline to evaluate is the highest scoring candidate hypothesis derived as described above.",6 Experiments and Evaluation,[0],[0]
Accuracy of the timeline is calculated as the number of transformations required to obtain the reference sequence in the annotated gold-standard from the one generated by our system.,6 Experiments and Evaluation,[0],[0]
"Transformations are measured in terms of the minimum edit distance, insertions, deletions, and substitutions of medical events.
",6 Experiments and Evaluation,[0],[0]
Experiments and Results.,6 Experiments and Evaluation,[0],[0]
"We first temporally order medical events within each clinical narrative by learning to rank them in relative order of occurence as described in our previous work (Raghavan et al., 2012c).",6 Experiments and Evaluation,[0],[0]
The overall accuracy of ranking medical events using leave-one-out cross validation is 82.1%.,6 Experiments and Evaluation,[0],[0]
"The resulting medical event sequences serve as the input to the problem of crossnarrative sequence alignment.
",6 Experiments and Evaluation,[0],[0]
The cross-narrative coreference and temporal relation pairwise classification models described in Section 4 are trained using a Maximum entropy classifier.,6 Experiments and Evaluation,[0],[0]
The coreference resolution performs with 71.5% precision and 82.3% recall.,6 Experiments and Evaluation,[0],[0]
The temporal relation classifier performs with 60.2% precision and 76.3% recall.,6 Experiments and Evaluation,[0],[0]
"The learned pairwise coreference and temporal relation probabilities are now used to derive the score for the WFST and dynamic programming approaches.
WFST representation and decoding.",6 Experiments and Evaluation,[0],[0]
We build finite-state machines using the open source OpenFST,6 Experiments and Evaluation,[0],[0]
library.2,6 Experiments and Evaluation,[0],[0]
We use a tropical semi-ring weighted using the negative log-likelihood of the computed scores.,6 Experiments and Evaluation,[0],[0]
"OpenFST provides tools that can search for the highest scoring sequences accepted by the machine, and can sample from highscoring sequences probabilistically, by treating the
2www.openfst.org
scores of each transition within the machine as a negative log probability.",6 Experiments and Evaluation,[0],[0]
The decoding process to compute the most likely combined medical event sequence can be defined as searching for the best path in the combined graph representation (Equation 2).,6 Experiments and Evaluation,[0],[0]
The best path is the one that minimizes the total weight on a path (since the arcs are negative log probabilities).,6 Experiments and Evaluation,[0],[0]
"In searching for the best path, the beam size is set to 5.",6 Experiments and Evaluation,[0],[0]
"The accuracy of the WFST-based representation and beam search across all sequences using the coreference and temporal relation scores to obtain the combined aligned sequence is 78.9%.
",6 Experiments and Evaluation,[0],[0]
Dynamic Programming.,6 Experiments and Evaluation,[0],[0]
We use the NW and SW algorithms described in Section 5.3 to produce local and global alignments respectively.,6 Experiments and Evaluation,[0],[0]
We use the scoring scheme described in Section 5.1 to update the cost matrix for dynamic programming and implement the algorithms as described in Section 5.3.,6 Experiments and Evaluation,[0],[0]
The overall accuracy of sequence alignment with both coreference and temporal relation scores using NW is 68.7% whereas SW gives an accuracy of 72.1%.,6 Experiments and Evaluation,[0],[0]
"In case of aligning just two sequences, both methods yield the same results.",6 Experiments and Evaluation,[0],[0]
"The accuracy of cross-narrative MSA for each patient, for each method, using cross validation, is shown in Table 1.",6 Experiments and Evaluation,[0],[0]
Results indicate that the WFSTbased method outperforms the dynamic programming approach for multi-sequence alignment (statistical significance p<0.05).,6 Experiments and Evaluation,[0],[0]
"Morever, the results using both coreference and temporal realtion scores for alignment outperform using only coreference scores for alignment using all approaches.",6 Experiments and Evaluation,[0],[0]
This indicates that cross-narrative temporal relations are important for accurately aligning medical event sequences across narratives.,6 Experiments and Evaluation,[0],[0]
"We propose and evaluate different approaches to multiple sequence alignment of medical events.
",7 Discussion,[0],[0]
Approaches to multi-alignment.,7 Discussion,[0],[0]
We address the problem of aligning medical event sequences using a novel WFST-based framework and empirically demonstrate that it outperforms pairwise progressive alignment using dynamic programming.,7 Discussion,[0],[0]
"This is mainly because the WFST-based allows us to consider temporal constraints from across multiple sequences when performing the alignment.
",7 Discussion,[0],[0]
"Moreover, it also outperforms the integer linear programming (ILP) method for timeline construction proposed in (Do et al., 2012).",7 Discussion,[0],[0]
We implemented the proposed method that also allows combining the output of classifiers subject to some constraints.,7 Discussion,[0],[0]
We derive intervals from event starts and stops and learn two perceptron classifiers for classifying the temporal relations between events and assigning events to intervals.,7 Discussion,[0],[0]
The classifier probabilities are then used to solve the optimization problem using the lpsolve solver.3,7 Discussion,[0],[0]
We also use intra-document coreference information to resolve coreference before performing the global optimization.,7 Discussion,[0],[0]
"We observe that in case of MSA, the optimal solution using ILP is still intractable as the number of constraints increases exponentially with the number of sequences.",7 Discussion,[0],[0]
Aligning pairwise iteratively gives us an overall average accuracy of 68.2% similar to dynamic programming.,7 Discussion,[0],[0]
"While this is comparable to the dynamic programming performance, the WFST-based method significantly outperforms this in case of multialignments for cross-narrative temporal ordering.
",7 Discussion,[0],[0]
Performance and error analysis.,7 Discussion,[0],[0]
"We perform multi-alignments over medical event sequences for a patient, where each sequence corresponds to temporally ordered medical events in a clinical narrative generated using the ranking model described in (Raghavan et al., 2012c).",7 Discussion,[0],[0]
The accuracy of intra-narrative temporal ordering is 82.1%.,7 Discussion,[0],[0]
The errors in performing this intra-narrative ordering may propagate to the cross-narrative model resulting in reduced accuracy.,7 Discussion,[0],[0]
"This may be addressed by considering n-best temporally ordered medical event sequences, generated by the ranking process, and aligning the n-best sequences using the WFST-based framework.",7 Discussion,[0],[0]
"This could be feasible as, practically, the WFST-based method for multialignment takes only a few secs to align a pair of medical event sequences with average length 40.
",7 Discussion,[0],[0]
The accuracy of alignments across multiple medical event sequences is also affected by the error induced by the coreference and temporal relation scores.,7 Discussion,[0],[0]
"Often, insufficient temporal cues leads
3http://lpsolve.sourceforge.net/5.5/
to misclassification of events incorrectly as sharing the “simultaneous” temporal relation and often as coreferring.",7 Discussion,[0],[0]
This induces errors in the score calculation and hence the alignments.,7 Discussion,[0],[0]
"Better methods to address the challenging problem of crossdocument temporal relation learning, perhaps with the help of structured data from the patient record, could improve the accuracy of alignments.
",7 Discussion,[0],[0]
"There is no clear trend with respect to the number of medical events and narratives for a patient (Table 1.), and the alignment accuracy.",7 Discussion,[0],[0]
"In future work, it would be interesting to examine any such correlation and also study the scalability of the WFST-based method for sequence alignment on longer medical event sequences and a larger dataset of patients.",7 Discussion,[0],[0]
"Further, the WFST-based method may be used to model multi-alignment tasks in other speech and language problems as well.",7 Discussion,[0],[0]
We propose a novel framework for aligning medical event sequences across clinical narratives based on coreference and temporal relation information using cascaded WFSTs.,8 Conclusion,[0],[0]
FSTs provide a convenient and flexible framework to model sequences of temporally ordered medical events and compose them into a combined graph representation.,8 Conclusion,[0],[0]
Decoding this graph allows us to jointly maximize coreference as well as temporal relation probabilities to derive a timeline of the most likely temporal ordering of medical events.,8 Conclusion,[0],[0]
This approach to aligning multiple sequences of medical events significantly outperforms other approaches such as dynamic programming.,8 Conclusion,[0],[0]
"Moreover, we demonstrate the importance of learning temporal relations for the task timeline generation from across multiple clinical narratives by empirically proving that decoding using both coreference and temporal relation scores is far more accurate than decoding with only coreference scores.",8 Conclusion,[0],[0]
The project was supported by Award Number Grant R01LM011116 from the National Library of Medicine.,Acknowledgments,[0],[0]
The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Library of Medicine or the National Institutes of Health.,Acknowledgments,[0],[0]
The authors would like to thank Yanzhang He for his input on the WFST-based model.,Acknowledgments,[0],[0]
Cross-narrative temporal ordering of medical events is essential to the task of generating a comprehensive timeline over a patient’s history.,abstractText,[0],[0]
"We address the problem of aligning multiple medical event sequences, corresponding to different clinical narratives, comparing the following approaches: (1) A novel weighted finite state transducer representation of medical event sequences that enables composition and search for decoding, and (2) Dynamic programming with iterative pairwise alignment of multiple sequences using global and local alignment algorithms.",abstractText,[0],[0]
The cross-narrative coreference and temporal relation weights used in both these approaches are learned from a corpus of clinical narratives.,abstractText,[0],[0]
We present results using both approaches and observe that the finite state transducer approach performs performs significantly better than the dynamic programming one by 6.8% for the problem of multiple-sequence alignment.,abstractText,[0],[0]
Cross-narrative temporal ordering of medical events,title,[0],[0]
Relation extraction has made great strides in newswire and Web domains.,1 Introduction,[0],[0]
"Recently, there has
∗",1 Introduction,[0],[0]
"This research was conducted when the authors were at Microsoft Research.
been increasing interest in applying relation extraction to high-value domains such as biomedicine.",1 Introduction,[0],[0]
"The advent of $1000 human genome1 heralds the dawn of precision medicine, but progress in personalized cancer treatment has been hindered by the arduous task of interpreting genomic data using prior knowledge.",1 Introduction,[0],[0]
"For example, given a tumor sequence, a molecular tumor board needs to determine which genes and mutations are important, and what drugs are available to treat them.",1 Introduction,[0],[0]
"Already the research literature has a wealth of relevant knowledge, and it is growing at an astonishing rate.",1 Introduction,[0],[0]
"PubMed2, the online repository of biomedical articles, adds two new papers per minute, or one million each year.",1 Introduction,[0],[0]
"It is thus imperative to advance relation extraction for machine reading.
",1 Introduction,[0],[0]
"In the vast literature on relation extraction, past work focused primarily on binary relations in single sentences, limiting the available information.",1 Introduction,[0],[0]
"Consider the following example: “The deletion mutation on exon-19 of EGFR gene was present in 16 patients, while the L858E point mutation on exon-21 was noted in 10.",1 Introduction,[0],[0]
All patients were treated with gefitinib and showed a partial response.”.,1 Introduction,[0],[0]
"Collectively, the two sentences convey the fact that there is a ternary interaction between the three entities in bold, which is not expressed in either sentence alone.",1 Introduction,[0],[0]
"Namely, tumors with L858E mutation in EGFR gene can be treated with gefitinib.",1 Introduction,[0],[0]
Extracting such knowledge clearly requires moving beyond binary relations and single sentences.,1 Introduction,[0],[0]
N -ary relations and cross-sentence extraction have received relatively little attention in the past.,1 Introduction,[0],[0]
"Prior
1http://www.illumina.com/systems/ hiseq-x-sequencing-system.html
2https://www.ncbi.nlm.nih.gov/pubmed
ar X
iv :1
70 8.
03 74
3v 1
[ cs
.C",1 Introduction,[0],[0]
"L
] 1
2 A
ug 2
work on n-ary relation extraction focused on single sentences (Palmer et al., 2005; McDonald et al., 2005) or entity-centric attributes that can be extracted largely independently (Chinchor, 1998; Surdeanu and Heng, 2014).",1 Introduction,[0],[0]
"Prior work on cross-sentence extraction often used coreference to gain access to arguments in a different sentence (Gerber and Chai, 2010; Yoshikawa et al., 2011), without truly modeling inter-sentential relational patterns.",1 Introduction,[0],[0]
(See Section 7 for a more detailed discussion.),1 Introduction,[0],[0]
"A notable exception is Quirk and Poon (2017), which applied distant supervision to general cross-sentence relation extraction, but was limited to binary relations.
",1 Introduction,[0],[0]
"In this paper, we explore a general framework for cross-sentence n-ary relation extraction, based on graph long short-term memory networks (graph LSTMs).",1 Introduction,[0],[0]
"By adopting the graph formulation, our framework subsumes prior approaches based on chain or tree LSTMs, and can incorporate a rich set of linguistic analyses to aid relation extraction.",1 Introduction,[0],[0]
"Relation classification takes as input the entity representations learned from the entire text, and can be easily extended for arbitrary relation arity n. This approach also facilitates joint learning with kindred relations where the supervision signal is more abundant.
",1 Introduction,[0],[0]
We conducted extensive experiments on two important domains in precision medicine.,1 Introduction,[0],[0]
"In both distant supervision and supervised learning settings, graph LSTMs that encode rich linguistic knowledge outperformed other neural network variants, as well as a well-engineered feature-based classifier.",1 Introduction,[0],[0]
Multitask learning with sub-relations led to further improvement.,1 Introduction,[0],[0]
"Syntactic analysis conferred a significant benefit to the performance of graph LSTMs, especially when syntax accuracy was high.
",1 Introduction,[0],[0]
"In the molecular tumor board domain, PubMedscale extraction using distant supervision from a
small set of known interactions produced orders of magnitude more knowledge, and cross-sentence extraction tripled the yield compared to single-sentence extraction.",1 Introduction,[0],[0]
Manual evaluation verified that the accuracy is high despite the lack of annotated examples.,1 Introduction,[0],[0]
"Let e1, · · · , em be entity mentions in text T .",2 Cross-sentence n-ary relation extraction,[0],[0]
"Relation extraction can be formulated as a classification problem of determining whether a relation R holds for e1, · · · , em in T .",2 Cross-sentence n-ary relation extraction,[0],[0]
"For example, given a cancer patient with mutation v in gene g, a molecular tumor board seeks to find if this type of cancer would respond to drug d. Literature with such knowledge has been growing rapidly; we can help the tumor board by checking if the Respond relation holds for the (d, g, v) triple.
",2 Cross-sentence n-ary relation extraction,[0],[0]
"Traditional relation extraction methods focus on binary relations where all entities occur in the same sentence (i.e., m = 2 and T is a sentence), and cannot handle the aforementioned ternary relations.",2 Cross-sentence n-ary relation extraction,[0],[0]
"Moreover, as we focus on more complex relations and n increases, it becomes increasingly rare that the related entities will be contained entirely in a single sentence.",2 Cross-sentence n-ary relation extraction,[0],[0]
"In this paper, we generalize extraction to cross-sentence, n-ary relations, where m > 2 and T can contain multiple sentences.",2 Cross-sentence n-ary relation extraction,[0],[0]
"As will be shown in our experiments section, n-ary relations are crucial for high-value domains such as biomedicine, and expanding beyond the sentence boundary enables the extraction of more knowledge.
",2 Cross-sentence n-ary relation extraction,[0],[0]
"In the standard binary-relation setting, the dominant approaches are generally defined in terms of the shortest dependency path between the two entities in question, either by deriving rich features from the path or by modeling it using deep neural
networks.",2 Cross-sentence n-ary relation extraction,[0],[0]
"Generalizing this paradigm to the n-ary setting is challenging, as there are ( n 2 ) paths.",2 Cross-sentence n-ary relation extraction,[0],[0]
"One apparent solution is inspired by Davidsonian semantics: first, identify a single trigger phrase that signifies the whole relation, then reduce the n-ary relation to n binary relations between the trigger and an argument.",2 Cross-sentence n-ary relation extraction,[0],[0]
"However, challenges remain.",2 Cross-sentence n-ary relation extraction,[0],[0]
"It is often hard to specify a single trigger, as the relation is manifested by several words, often not contiguous.",2 Cross-sentence n-ary relation extraction,[0],[0]
"Moreover, it is expensive and time-consuming to annotate training examples, especially if triggers are required, as is evident in prior annotation efforts such as GENIA (Kim et al., 2009).",2 Cross-sentence n-ary relation extraction,[0],[0]
"The realistic and widely adopted paradigm is to leverage indirect supervision, such as distant supervision (Craven and Kumlien, 1999; Mintz et al., 2009), where triggers are not available.
",2 Cross-sentence n-ary relation extraction,[0],[0]
"Additionally, lexical and syntactic patterns signifying the relation will be sparse.",2 Cross-sentence n-ary relation extraction,[0],[0]
"To handle such sparsity, traditional feature-based approaches require extensive engineering and large data.",2 Cross-sentence n-ary relation extraction,[0],[0]
"Unfortunately, this challenge becomes much more severe in crosssentence extraction when the text spans multiple sentences.
",2 Cross-sentence n-ary relation extraction,[0],[0]
"To overcome these challenges, we explore a general relation extraction framework based on graph LSTMs.",2 Cross-sentence n-ary relation extraction,[0],[0]
"By learning a continuous representation for words and entities, LSTMs can handle sparsity effectively without requiring intense feature engineering.",2 Cross-sentence n-ary relation extraction,[0],[0]
"The graph formulation subsumes prior LSTM approaches based on chains or trees, and can incorporate rich linguistic analyses.
",2 Cross-sentence n-ary relation extraction,[0],[0]
This approach also opens up opportunities for joint learning with related relations.,2 Cross-sentence n-ary relation extraction,[0],[0]
"For example, the Response relation over d, g, v also implies a binary sub-relation over drug d and mutation v, with the gene underspecified.",2 Cross-sentence n-ary relation extraction,[0],[0]
"Even with distant supervision, the supervision signal for n-ary relations will likely be sparser than their binary sub-relations.",2 Cross-sentence n-ary relation extraction,[0],[0]
Our approach makes it very easy to use multi-task learning over both the n-ary relations and their sub-relations.,2 Cross-sentence n-ary relation extraction,[0],[0]
Learning a continuous representation can be effective for dealing with lexical and syntactic sparsity.,3 Graph LSTMs,[0],[0]
"For sequential data such as text, recurrent neural networks (RNNs) are quite popular.",3 Graph LSTMs,[0],[0]
"They resemble hidden
Markov models (HMMs), except that discrete hidden states are replaced with continuous vectors, and emission and transition probabilities with neural networks.",3 Graph LSTMs,[0],[0]
"Conventional RNNs with sigmoid units suffer from gradient diffusion or explosion, making training very difficult (Bengio et al., 1994; Pascanu et al., 2013).",3 Graph LSTMs,[0],[0]
"Long short-term memory (LSTMs) (Hochreiter and Schmidhuber, 1997) combats these problems by using a series of gates (input, forget and output) to avoid amplifying or suppressing gradients during backpropagation.",3 Graph LSTMs,[0],[0]
"Consequently, LSTMs are much more effective in capturing long-distance dependencies, and have been applied to a variety of NLP tasks.",3 Graph LSTMs,[0],[0]
"However, most approaches are based on linear chains and only explicitly model the linear context, which ignores a variety of linguistic analyses, such as syntactic and discourse dependencies.
",3 Graph LSTMs,[0],[0]
"In this section, we propose a general framework that generalizes LSTMs to graphs.",3 Graph LSTMs,[0],[0]
"While there is some prior work on learning tree LSTMs (Tai et al., 2015; Miwa and Bansal, 2016), to the best of our knowledge, graph LSTMs have not been applied to any NLP task yet.",3 Graph LSTMs,[0],[0]
Figure 2 shows the architecture of this approach.,3 Graph LSTMs,[0],[0]
The input layer is the word embedding of input text.,3 Graph LSTMs,[0],[0]
Next is the graph LSTM which learns a contextual representation for each word.,3 Graph LSTMs,[0],[0]
"For the entities in question, their contextual representations are concatenated and become the input to the relation classifiers.",3 Graph LSTMs,[0],[0]
"For a multi-word entity, we simply used the average of its word representations and leave the exploration of more sophisticated aggregation approaches to future work.",3 Graph LSTMs,[0],[0]
The layers are trained jointly with backpropagation.,3 Graph LSTMs,[0],[0]
"This framework is
agnostic to the choice of classifiers.",3 Graph LSTMs,[0],[0]
"Jointly designing classifiers with graph LSTMs would be interesting future work.
",3 Graph LSTMs,[0],[0]
At the core of the graph LSTM is a document graph that captures various dependencies among the input words.,3 Graph LSTMs,[0],[0]
"By choosing what dependencies to include in the document graph, graph LSTMs naturally subsumes linear-chain or tree LSTMs.
",3 Graph LSTMs,[0],[0]
"Compared to conventional LSTMs, the graph formulation presents new challenges.",3 Graph LSTMs,[0],[0]
"Due to potential cycles in the graph, a straightforward implementation of backpropagation might require many iterations to reach a fixed point.",3 Graph LSTMs,[0],[0]
"Moreover, in the presence of a potentially large number of edge types (adjacent-word, syntactic dependency, etc.), parametrization becomes a key problem.
",3 Graph LSTMs,[0],[0]
"In the remainder of this section, we first introduce the document graph and show how to conduct backpropagation in graph LSTMs.",3 Graph LSTMs,[0],[0]
We then discuss two strategies for parametrizing the recurrent units.,3 Graph LSTMs,[0],[0]
"Finally, we show how to conduct multi-task learning with this framework.",3 Graph LSTMs,[0],[0]
"To model various dependencies from linguistic analysis at our disposal, we follow Quirk and Poon (2017) and introduce a document graph to capture intra- and inter-sentential dependencies.",3.1 Document Graph,[0],[0]
"A document graph consists of nodes that represent words and edges that represent various dependencies such as linear context (adjacent words), syntactic dependencies, and discourse relations (Lee et al., 2013; Xue et al., 2015).",3.1 Document Graph,[0],[0]
"Figure 1 shows the document graph for our running example; this instance suggests that tumors with L858E mutation in EGFR gene responds to the drug gefitinib.
",3.1 Document Graph,[0],[0]
This document graph acts as the backbone upon which a graph LSTM is constructed.,3.1 Document Graph,[0],[0]
"If it con-
tains only edges between adjacent words, we recover linear-chain LSTMs.",3.1 Document Graph,[0],[0]
"Similarly, other prior LSTM approaches can be captured in this framework by restricting edges to those in the shortest dependency path or the parse tree.",3.1 Document Graph,[0],[0]
Conventional LSTMs are essentially very deep feedforward neural networks.,3.2 Backpropagation in Graph LSTMs,[0],[0]
"For example, a left-to-right linear LSTM has one hidden vector for each word.",3.2 Backpropagation in Graph LSTMs,[0],[0]
This vector is generated by a neural network (recurrent unit) that takes as input the embedding of the given word and the hidden vector of the previous word.,3.2 Backpropagation in Graph LSTMs,[0],[0]
"In discriminative learning, these hidden vectors then serve as input for the end classifiers, from which gradients are backpropagated through the whole network.
",3.2 Backpropagation in Graph LSTMs,[0],[0]
"Generalizing such a strategy to graphs with cycles typically requires unrolling recurrence for a number of steps (Scarselli et al., 2009; Li et al., 2016; Liang et al., 2016).",3.2 Backpropagation in Graph LSTMs,[0],[0]
"Essentially, a copy of the graph is created for each step that serves as input for the next.",3.2 Backpropagation in Graph LSTMs,[0],[0]
"The result is a feed-forward neural network through time, and backpropagation is conducted accordingly.
",3.2 Backpropagation in Graph LSTMs,[0],[0]
"In principle, we could adopt the same strategy.",3.2 Backpropagation in Graph LSTMs,[0],[0]
"Effectively, gradients are backpropagated in a manner similar to loopy belief propagation (LBP).",3.2 Backpropagation in Graph LSTMs,[0],[0]
"However, this makes learning much more expensive as each update step requires multiple iterations of backpropagation.",3.2 Backpropagation in Graph LSTMs,[0],[0]
"Moreover, loopy backpropagation could suffer from the same problems encountered to in LBP, such as oscillation or failure to converge.
",3.2 Backpropagation in Graph LSTMs,[0],[0]
"We observe that dependencies such as coreference and discourse relations are generally sparse, so the backbone of a document graph consists of the linear chain and the syntactic dependency tree.",3.2 Backpropagation in Graph LSTMs,[0],[0]
"As in belief propagation, such structures can be leveraged to make backpropagation more efficient by replac-
ing synchronous updates, as in the unrolling strategy, with asynchronous updates, as in linear-chain LSTMs.",3.2 Backpropagation in Graph LSTMs,[0],[0]
"This opens up opportunities for a variety of strategies in ordering backpropagation updates.
",3.2 Backpropagation in Graph LSTMs,[0],[0]
"In this paper, we adopt a simple strategy that performed quite well in preliminary experiments, and leave further exploration to future work.",3.2 Backpropagation in Graph LSTMs,[0],[0]
"Specifically, we partition the document graph into two directed acyclic graphs (DAGs).",3.2 Backpropagation in Graph LSTMs,[0],[0]
"One DAG contains the left-to-right linear chain, as well as other forwardpointing dependencies.",3.2 Backpropagation in Graph LSTMs,[0],[0]
The other DAG covers the right-to-left linear chain and the backward-pointing dependencies.,3.2 Backpropagation in Graph LSTMs,[0],[0]
Figure 3 illustrates this strategy.,3.2 Backpropagation in Graph LSTMs,[0],[0]
"Effectively, we partition the original graph into the forward pass (left-to-right), followed by the backward pass (right-to-left), and construct the LSTMs accordingly.",3.2 Backpropagation in Graph LSTMs,[0],[0]
"When the document graph only contains linear chain edges, the graph LSTMs is exactly a bi-directional LSTMs (BiLSTMs).",3.2 Backpropagation in Graph LSTMs,[0],[0]
"A standard LSTM unit consists of an input vector (word embedding), a memory cell and an output vector (contextual representation), as well as several gates.",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"The input gate and output gate control the information flowing into and out of the cell, whereas the forget gate can optionally remove information from the recurrent connection to a precedent unit.
",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"In linear-chain LSTMs, each unit contains only one forget gate, as it has only one direct precedent (i.e., the adjacent-word edge pointing to the previous word).",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"In graph LSTMs, however, a unit may have several precedents, including connections to the same word via different edges.",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"We thus introduce a forget gate for each precedent, similar to the approach taken by Tai et al. (2015) for tree LSTMs.
",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"Encoding rich linguistic analysis introduces many distinct edge types besides word adjacency, such as syntactic dependencies, which opens up many possibilities for parametrization.",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"This was not considered in prior syntax-aware LSTM approaches (Tai et al., 2015; Miwa and Bansal, 2016).",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"In this paper, we explore two schemes that introduce more fined-grained parameters based on the edge types.
",3.3 The Basic Recurrent Propagation Unit,[0],[0]
Full Parametrization,3.3 The Basic Recurrent Propagation Unit,[0],[0]
"Our first proposal simply introduces a different set of parameters for each edge type, with computation specified below.
",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"it = σ(Wixt + ∑
j∈P (t) U
m(t,j) i hj + bi)
ot = σ(Woxt + ∑
j∈P (t) Um(t,j)o hj + bo) c̃t = tanh(Wcxt + ∑
j∈P (t) Um(t,j)c hj + bc)
ftj = σ(Wfxt",3.3 The Basic Recurrent Propagation Unit,[0],[0]
+,3.3 The Basic Recurrent Propagation Unit,[0],[0]
"U m(t,j) f hj + bf )",3.3 The Basic Recurrent Propagation Unit,[0],[0]
ct = it c̃t,3.3 The Basic Recurrent Propagation Unit,[0],[0]
"+ ∑
j∈P (t) ftj",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"cj
ht = ot tanh(ct)
",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"As in standard chain LSTMs, xt is the input word vector for node t, ht is the hidden state vector for node t, W ’s are the input weight matrices, and b’s are the bias vectors.",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"σ, tanh, and represent the sigmoid function, the hyperbolic tangent function, and the Hadamard product (pointwise multiplication), respectively.",3.3 The Basic Recurrent Propagation Unit,[0],[0]
The main differences lie in the recurrence terms.,3.3 The Basic Recurrent Propagation Unit,[0],[0]
"In graph LSTMs, a unit might have multiple predecessors (P (t)), for each of which (j) there is a forget gate ftj , and a typed weight matrix Um(t,j), where m(t, j) signifies the connection type between t and j. The input and output gates (it, ot) depend on all predecessors, whereas the forget gate (ftj) only depends on the predecessor with which the gate is associated.",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"ct and c̃t represent intermediate computation results within the memory cell, which take into account the input and forget gates, and will be combined with output gate to produce the hidden representation ht.
",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"Full parameterization is straightforward, but it requires a large number of parameters when there are many edge types.",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"For example, there are dozens of syntactic edge types, each corresponding to a Stanford dependency label.",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"As a result, in our experiments we resort to using only the coarse-grained types: word adjacency, syntactic dependency, etc.",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"Next, we will consider a more fine-grained approach by learning an edge-type embedding.
",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"Edge-Type Embedding To reduce the number of parameters and leverage potential correlation among fine-grained edge types, we learned a lowdimensional embedding of the edge types, and conducted an outer product of the predecessor’s hidden vector and the edge-type embedding to generate a “typed hidden representation”, which is a matrix.",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"The new computation is as follows:
it = σ(Wixt + ∑
j∈P (t) Ui ×T (hj ⊗ ej) + bi)
ftj = σ(Wfxt +",3.3 The Basic Recurrent Propagation Unit,[0],[0]
Uf ×T (hj ⊗ ej) + bf ),3.3 The Basic Recurrent Propagation Unit,[0],[0]
"ot = σ(Woxt + ∑
j∈P (t)",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"Uo ×T (hj ⊗ ej) + bo) c̃t = tanh(Wcxt + ∑
j∈P (t) Uc ×T (hj ⊗ ej) + bc) ct",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"= it c̃t + ∑
j∈P (t) ftj cj
ht = ot tanh(ct)
",3.3 The Basic Recurrent Propagation Unit,[0],[0]
U ’s are now l ×,3.3 The Basic Recurrent Propagation Unit,[0],[0]
"l × d tensors (l is the dimension of the hidden vector and d is the dimension for edgetype embedding), and hj ⊗ ej is a tensor product that produces an l × d matrix.",3.3 The Basic Recurrent Propagation Unit,[0],[0]
×T denotes a tensor dot product defined as T ×T,3.3 The Basic Recurrent Propagation Unit,[0],[0]
"A = ∑ d(T:,:,d · A:,d), which produces an l-dimensional vector.",3.3 The Basic Recurrent Propagation Unit,[0],[0]
The edgetype embedding ej is jointly trained with the other parameters.,3.3 The Basic Recurrent Propagation Unit,[0],[0]
The main advantages of a graph formulation are its generality and flexibility.,3.4 Comparison with Prior LSTM Approaches,[0],[0]
"As seen in Section 3.1, linear-chain LSTMs are a special case when the document graph is the linear chain of adjacent words.",3.4 Comparison with Prior LSTM Approaches,[0],[0]
"Similarly, Tree LSTMs (Tai et al., 2015) are a special case when the document graph is the parse tree.
",3.4 Comparison with Prior LSTM Approaches,[0],[0]
"In graph LSTMs, the encoding of linguistic knowledge is factored from the backpropagation strategy (Section 3.2), making it much more flexible, including introducing cycles.",3.4 Comparison with Prior LSTM Approaches,[0],[0]
"For example, Miwa and Bansal (2016) conducted joint entity and binary relation extraction by stacking a LSTM for relation extraction on top of another LSTM for entity recognition.",3.4 Comparison with Prior LSTM Approaches,[0],[0]
"In graph LSTMs, the two can be combined seamlessly using a document graph comprising both the word-adjacency chain and the dependency path between the two entities.
",3.4 Comparison with Prior LSTM Approaches,[0],[0]
The document graph can also incorporate other linguistic information.,3.4 Comparison with Prior LSTM Approaches,[0],[0]
"For example, coreference and discourse parsing are intuitively relevant for cross-sentence relation extraction.",3.4 Comparison with Prior LSTM Approaches,[0],[0]
"Although existing systems have not yet been shown to improve crosssentence relation extraction (Quirk and Poon, 2017), it remains an important future direction to explore incorporating such analyses, especially after adapting them to the biomedical domains (Bell et al., 2016).",3.4 Comparison with Prior LSTM Approaches,[0],[0]
"Multi-task learning has been shown to be beneficial in training neural networks (Caruana, 1998; Collobert and Weston, 2008; Peng and Dredze, 2016).",3.5 Multi-task Learning with Sub-relations,[0],[0]
"By learning contextual entity representations, our framework makes it straightforward to conduct multi-task learning.",3.5 Multi-task Learning with Sub-relations,[0],[0]
The only change is to add a separate classifier for each related auxiliary relation.,3.5 Multi-task Learning with Sub-relations,[0],[0]
"All classifiers share the same graph LSTMs representation learner and word embeddings, and can potentially help each other by pooling their supervision signals.
",3.5 Multi-task Learning with Sub-relations,[0],[0]
"In the molecular tumor board domain, we applied this paradigm to joint learning of both the ternary relation (drug-gene-mutation) and its binary sub-relation (drug-mutation).",3.5 Multi-task Learning with Sub-relations,[0],[0]
Experiment results show that this provides significant gains in both tasks.,3.5 Multi-task Learning with Sub-relations,[0],[0]
"We implemented our methods using the Theano library (Theano Development Team, 2016).",4 Implementation Details,[0],[0]
We used logistic regression for our relation classifiers.,4 Implementation Details,[0],[0]
Hyper parameters were set based on preliminary experiments on a small development dataset.,4 Implementation Details,[0],[0]
Training was done using mini-batched stochastic gradient descent (SGD) with batch size 8.,4 Implementation Details,[0],[0]
"We used a learning rate of 0.02 and trained for at most 30 epochs, with early stopping based on development data (Caruana et al., 2001; Graves et al., 2013).",4 Implementation Details,[0],[0]
"The dimension for the hidden vectors in LSTM units was set to 150, and the dimension for the edge-type embedding was set to 3.",4 Implementation Details,[0],[0]
"The word embeddings were initialized with the publicly available 100-dimensional GloVe word vectors trained on 6 billion words from Wikipedia and web text3 (Pennington et al., 2014).",4 Implementation Details,[0],[0]
"Other model parameters were initialized with random samples drawn uniformly from the range [−1, 1].
",4 Implementation Details,[0],[0]
"In multi-task training, we alternated among all tasks, each time passing through all data for one task4, and updating the parameters accordingly.",4 Implementation Details,[0],[0]
"This was repeated for 30 epochs.
3http://nlp.stanford.edu/projects/glove/ 4However, drug-gene pairs have much more data, so we subsampled the instances down to the same size as the main n-ary relation task.",4 Implementation Details,[0],[0]
"Our main experiments focus on extracting ternary interactions over drugs, genes and mutations, which is important for molecular tumor boards.",5 Domain: Molecular Tumor Boards,[0],[0]
A druggene-mutation interaction is broadly construed as an association between the drug efficacy and the mutation in the given gene.,5 Domain: Molecular Tumor Boards,[0],[0]
There is no annotated dataset for this problem.,5 Domain: Molecular Tumor Boards,[0],[0]
"However, due to the importance of such knowledge, oncologists have been painstakingly curating known relations from reading papers.",5 Domain: Molecular Tumor Boards,[0],[0]
"Such a manual approach cannot keep up with the rapid growth of the research literature, and the coverage is generally sparse and not up to date.",5 Domain: Molecular Tumor Boards,[0],[0]
"However, the curated knowledge can be used for distant supervision.",5 Domain: Molecular Tumor Boards,[0],[0]
"We obtained biomedical literature from PubMed Central5, consisting of approximately one million fulltext articles as of 2015.",5.1 Datasets,[0],[0]
Note that only a fraction of papers contain knowledge about drug-gene-mutation interactions.,5.1 Datasets,[0],[0]
Extracting such knowledge from the vast body of biomedical papers is exactly the challenge.,5.1 Datasets,[0],[0]
"As we will see in later subsections, distant supervision enables us to generate a sizable training set from a small number of manually curated facts, and the learned model was able to extract orders of magnitude more facts.",5.1 Datasets,[0],[0]
"In future work, we will explore incorporating more known facts for distant supervision and extracting from more full-text articles.
",5.1 Datasets,[0],[0]
"We conducted tokenization, part-of-speech tagging, and syntactic parsing using SPLAT (Quirk et al., 2012), and obtained Stanford dependencies (de Marneffe et al., 2006) using Stanford CoreNLP",5.1 Datasets,[0],[0]
"(Manning et al., 2014).",5.1 Datasets,[0],[0]
"We used the entity taggers from Literome (Poon et al., 2014) to identify drug, gene and mutation mentions.
",5.1 Datasets,[0],[0]
"We used the Gene Drug Knowledge Database (GDKD) (Dienstmann et al., 2015) and the Clinical Interpretations of Variants In Cancer (CIVIC) knowledge base6 for distant supervision.",5.1 Datasets,[0],[0]
"The knowledge bases distinguish fine-grained interaction types, which we do not use in this paper.
",5.1 Datasets,[0],[0]
5http://www.ncbi.nlm.nih.gov/pmc/ 6http://civic.genome.wustl.edu,5.1 Datasets,[0],[0]
"After identifying drug, gene and mutation mentions in the text, co-occurring triples with known interactions were chosen as positive examples.",5.2 Distant Supervision,[0],[0]
"However, unlike the single-sentence setting in standard distant supervision, care must be taken in selecting the candidates.",5.2 Distant Supervision,[0],[0]
"Since the triples can reside in different sentences, an unrestricted selection of text spans would risk introducing many obviously wrong examples.",5.2 Distant Supervision,[0],[0]
"We thus followed Quirk and Poon (2017) in restricting the candidates to those occurring in a minimal span, i.e., we retain a candidate only if is no other co-occurrence of the same entities in an overlapping text span with a smaller number of consecutive sentences.",5.2 Distant Supervision,[0],[0]
"Furthermore, we avoid picking unlikely candidates where the triples are far apart in the document.",5.2 Distant Supervision,[0],[0]
"Specifically, we considered entity triples within K consecutive sentences, ignoring paragraph boundaries.",5.2 Distant Supervision,[0],[0]
K = 1 corresponds to the baseline of extraction within single sentences.,5.2 Distant Supervision,[0],[0]
"We explored K ≤ 3, which captured a large fraction of candidates without introducing many unlikely ones.
",5.2 Distant Supervision,[0],[0]
Only 59 distinct drug-gene-mutation triples from the knowledge bases were matched in the text.,5.2 Distant Supervision,[0],[0]
"Even from such a small set of unique triples, we obtained 3,462 ternary relation instances that can serve as positive examples.",5.2 Distant Supervision,[0],[0]
"For multi-task learning, we also considered drug-gene and drug-mutation sub-relations, which yielded 137,469 drug-gene and 3,192 drugmutation relation instances as positive examples.
",5.2 Distant Supervision,[0],[0]
"We generate negative examples by randomly sampling co-occurring entity triples without known interactions, subject to the same restrictions above.",5.2 Distant Supervision,[0],[0]
We sampled the same number as positive examples to obtain a balanced dataset7.,5.2 Distant Supervision,[0],[0]
"To compare the various models in our proposed framework, we conducted five-fold cross-validation, treating the positive and negative examples from distant supervision as gold annotation.",5.3 Automatic Evaluation,[0],[0]
"To avoid traintest contamination, all examples from a document were assigned to the same fold.",5.3 Automatic Evaluation,[0],[0]
"Since our datasets are balanced by construction, we simply report average test accuracy on held-out folds.",5.3 Automatic Evaluation,[0],[0]
"Obviously, the
7We will release the dataset at http://hanover.azurewebsites.net.
results could be noisy (e.g., entity triples not known to have an interaction might actually have one), but this evaluation is automatic and can quickly evaluate the impact of various design choices.
",5.3 Automatic Evaluation,[0],[0]
We evaluated two variants of graph LSTMs: “Graph LSTM-FULL” with full parametrization and “Graph LSTM-EMBED” with edge-type embedding.,5.3 Automatic Evaluation,[0],[0]
"We compared graph LSTMs with three strong baseline systems: a well-engineered feature-based classifier (Quirk and Poon, 2017), a convolutional neural network (CNN)",5.3 Automatic Evaluation,[0],[0]
"(Zeng et al., 2014; Santos et al., 2015; Wang et al., 2016), and a bi-directional LSTM (BiLSTM).",5.3 Automatic Evaluation,[0],[0]
"Following Wang et al. (2016), we used input attention for the CNN and a input window size of 5.",5.3 Automatic Evaluation,[0],[0]
Quirk and Poon (2017) only extracted binary relations.,5.3 Automatic Evaluation,[0],[0]
"We extended it to ternary relations by deriving features for each entity pair (with added annotation to signify the two entity types), and pooling the features
from all pairs.",5.3 Automatic Evaluation,[0],[0]
"For binary relation extraction, prior syntax-aware approaches are directly applicable.",5.3 Automatic Evaluation,[0],[0]
"So we also compared with a state-of-the-art tree LSTM system (Miwa and Bansal, 2016) and a BiLSTM on the shortest dependency path between the two entities (BiLSTM-Shortest-Path) (Xu et al., 2015b).
",5.3 Automatic Evaluation,[0],[0]
"Table 1 shows the results for cross-sentence, ternary relation extraction.",5.3 Automatic Evaluation,[0],[0]
"All neural-network based models outperformed the feature-based classifier, illustrating their advantage in handling sparse linguistic patterns without requiring intense feature engineering.",5.3 Automatic Evaluation,[0],[0]
"All LSTMs significantly outperformed CNN in the cross-sentence setting, verifying the importance in capturing long-distance dependencies.
",5.3 Automatic Evaluation,[0],[0]
"The two variants of graph LSTMs perform on par with each other, though Graph LSTM-FULL has a small advantage, suggesting that further exploration of parametrization schemes could be beneficial.",5.3 Automatic Evaluation,[0],[0]
"In particular, the edge-type embedding might improve by pretraining on unlabeled text with syntactic parses.
",5.3 Automatic Evaluation,[0],[0]
"Both graph variants significantly outperformed BiLSTMs (p < 0.05 by McNemar’s chi-square test), though the difference is small.",5.3 Automatic Evaluation,[0],[0]
This result is intriguing.,5.3 Automatic Evaluation,[0],[0]
"In Quirk and Poon (2017), the best system incorporated syntactic dependencies and outperformed the linear-chain variant (Base) by a large margin.",5.3 Automatic Evaluation,[0],[0]
"So why didn’t graph LSTMs make an equally substantial gain by modeling syntactic dependencies?
",5.3 Automatic Evaluation,[0],[0]
One reason is that linear-chain LSTMs can already captured some of the long-distance dependencies available in syntactic parses.,5.3 Automatic Evaluation,[0],[0]
"BiLSTMs substantially outperformed the feature-based classifier, even without explicit modeling of syntactic dependencies.",5.3 Automatic Evaluation,[0],[0]
"The gain cannot be entirely attributed to word embedding as LSTMs also outperformed CNNs.
",5.3 Automatic Evaluation,[0],[0]
Another reason is that syntactic parsing is less accurate in the biomedical domain.,5.3 Automatic Evaluation,[0],[0]
"Parse errors confuse the graph LSM learner, limiting the potential for gain.",5.3 Automatic Evaluation,[0],[0]
"In Section 6, we show supporting evidence in a domain when gold parses are available.
",5.3 Automatic Evaluation,[0],[0]
"We also reported accuracy on instances within single sentences, which exhibited a broadly similar set of trends.",5.3 Automatic Evaluation,[0],[0]
"Note that single-sentence and crosssentence accuracies are not directly comparable, as the test sets are different (one subsumes the other).
",5.3 Automatic Evaluation,[0],[0]
We conducted the same experiments on the binary sub-relation between drug-mutation pairs.,5.3 Automatic Evaluation,[0],[0]
"Table 2
shows the results, which are similar to the ternary case: Graph LSTM-FULL consistently performed the best for both single sentence and cross-sentence instances.",5.3 Automatic Evaluation,[0],[0]
"BiLSTMs on the shortest path substantially underperformed BiLSTMs or graph LSTMs, losing between 4-5 absolute points in accuracy, which could be attributed to the lower parsing quality in the biomedical domain.",5.3 Automatic Evaluation,[0],[0]
"Interestingly, the state-of-the-art tree LSTMs (Miwa and Bansal, 2016) also underperformed graph LSTMs, even though they encoded essentially the same linguistic structures (word adjacency and syntactic dependency).",5.3 Automatic Evaluation,[0],[0]
"We attributed the gain to the fact that Miwa and Bansal (2016) used separate LSTMs for the linear chain and the dependency tree, whereas graph LSTMs learned a single representation for both.
",5.3 Automatic Evaluation,[0],[0]
"To evaluate whether joint learning with subrelations can help, we conducted multi-task learning using Graph LSTM-FULL to jointly train extractors for both the ternary interaction and the drug-mutation, drug-gene sub-relations.",5.3 Automatic Evaluation,[0],[0]
Table 3 shows the results.,5.3 Automatic Evaluation,[0],[0]
Multi-task learning resulted in a significant gain for both the ternary interaction and the drug-mutation interaction.,5.3 Automatic Evaluation,[0],[0]
"Interestingly, the advantage of graph LSTMs over BiLSTMs is reduced with multi-task learning, suggesting that with more supervision signal, even linear-chain LSTMs can learn to capture long-range dependencies that are were made evident by parse features in graph LSTMs.",5.3 Automatic Evaluation,[0],[0]
"Note that there are many more instances for drug-gene interaction than others, so we only sampled a subset of comparable size.",5.3 Automatic Evaluation,[0],[0]
"Therefore, we do not evaluate the performance gain for drug-gene interaction, as in practice, one would simply learn from all available data, and the sub-sampled results are not competitive.
",5.3 Automatic Evaluation,[0],[0]
We included coreference and discourse relations in our document graph.,5.3 Automatic Evaluation,[0],[0]
"However, we didn’t observe any significant gains, similar to the observation in
Quirk and Poon (2017).",5.3 Automatic Evaluation,[0],[0]
We leave further exploration to future work.,5.3 Automatic Evaluation,[0],[0]
Our ultimate goal is to extract all knowledge from available text.,5.4 PubMed-Scale Extraction,[0],[0]
"We thus retrained our model using the best system from automatic evaluation (i.e., Graph LSTM-FULL) on all available data.",5.4 PubMed-Scale Extraction,[0],[0]
"The resulting model was then used to extract relations from all PubMed Central articles.
",5.4 PubMed-Scale Extraction,[0],[0]
Table 4 shows the number of candidates and extracted interactions.,5.4 PubMed-Scale Extraction,[0],[0]
"With as little as 59 unique druggene-mutation triples from the two databases8, we learned to extract orders of magnitude more unique interactions.",5.4 PubMed-Scale Extraction,[0],[0]
"The results also highlight the benefit of cross-sentence extraction, which yields 3 to 5 times more relations than single-sentence extraction.
",5.4 PubMed-Scale Extraction,[0],[0]
"Table 5 conducts a similar comparison on unique number of drugs, genes, and mutations.",5.4 PubMed-Scale Extraction,[0],[0]
"Again, machine reading covers far more unique entities, especially with cross-sentence extraction.",5.4 PubMed-Scale Extraction,[0],[0]
"Our automatic evaluations are useful for comparing competing approaches, but may not reflect the true classifier precision as the labels are noisy.",5.5 Manual Evaluation,[0],[0]
"Therefore, we randomly sampled extracted relation instances and asked three researchers knowledgeable in precision medicine to evaluate their correctness.",5.5 Manual Evaluation,[0],[0]
"For each instance, the annotators were presented with the provenance: sentences with the drug, gene, and mutation highlighted.",5.5 Manual Evaluation,[0],[0]
"The annotators determined in
8There are more in the databases, but these are the only ones for which we found matching instances in the text.",5.5 Manual Evaluation,[0],[0]
"In future work, we will explore various ways to increase the number, e.g., by matching underspecified drug classes to specific drugs.
",5.5 Manual Evaluation,[0],[0]
each case whether this instance implied that the given entities were related.,5.5 Manual Evaluation,[0],[0]
"Note that evaluation does not attempt to identify whether the relationships are true or replicated in follow-up papers; rather, it focuses on whether the relationships are entailed by the text.
",5.5 Manual Evaluation,[0],[0]
We focused our evaluation efforts on the crosssentence ternary-relation setting.,5.5 Manual Evaluation,[0],[0]
"We considered three probability thresholds: 0.9 for a high-precision but potentially low-recall setting, 0.5, and a random sample of all candidates.",5.5 Manual Evaluation,[0],[0]
"In each case, 150 instances were selected for a total of 450 annotations.",5.5 Manual Evaluation,[0],[0]
"A subset of 150 instances were reviewed by two annotators, and the inter-annotator agreement was 88%.
",5.5 Manual Evaluation,[0],[0]
"Table 6 shows that the classifier indeed filters out a large portion of potential candidates, with estimated instance accuracy of 64% at the threshold of 0.5, and 75% at 0.9.",5.5 Manual Evaluation,[0],[0]
"Interestingly, LSTMs are effective at screening out many entity mention errors, presumably because they include broad contextual features.",5.5 Manual Evaluation,[0],[0]
"We also conducted experiments on extracting genetic pathway interactions using the GENIA Event Extraction dataset (Kim et al., 2009).",6 Domain: Genetic Pathways,[0],[0]
"This dataset contains gold syntactic parses for the sentences, which offered a unique opportunity to investigate the impact of syntactic analysis on graph LSTMs.",6 Domain: Genetic Pathways,[0],[0]
"It also allowed us to test our framework in supervised learning.
",6 Domain: Genetic Pathways,[0],[0]
"The original shared task evaluated on complex, nested events for nine event types, many of which are unary relations (Kim et al., 2009).",6 Domain: Genetic Pathways,[0],[0]
"Following Poon et al. (2015), we focused on gene regulation and reduced it to binary-relation classification for headto-head comparison.",6 Domain: Genetic Pathways,[0],[0]
"We followed their experimental protocol by sub-sampling negative examples to be about three times of positive examples.
",6 Domain: Genetic Pathways,[0],[0]
"Since the dataset is not entirely balanced, we reported precision, recall, and F1.",6 Domain: Genetic Pathways,[0],[0]
We used our best performing graph LSTM from the previous experiments.,6 Domain: Genetic Pathways,[0],[0]
"By default, automatic parses were used in the document graphs, whereas in Graph LSTM (GOLD), gold parses were used instead.",6 Domain: Genetic Pathways,[0],[0]
Table 7 shows the results.,6 Domain: Genetic Pathways,[0],[0]
"Once again, despite the lack of intense feature engineering, linear-chain LSTMs performed on par with the feature-based classifier (Poon et al., 2015).",6 Domain: Genetic Pathways,[0],[0]
"Graph LSTMs exhibited a more commanding advantage over linear-chain LSTMs in this domain, substantially outperforming the latter (p < 0.01 by McNemar’s chi-square test).",6 Domain: Genetic Pathways,[0],[0]
"Most interestingly, graph LSTMs using gold parses significantly outperformed that using automatic parses, suggesting that encoding high-quality analysis is particularly beneficial.",6 Domain: Genetic Pathways,[0],[0]
Most work on relation extraction has been applied to binary relations of entities in a single sentence.,7 Related Work,[0],[0]
"We first review relevant work on the single-sentence bi-
nary relation extraction task, and then review related work on n-ary and cross-sentence relation extraction.
",7 Related Work,[0],[0]
"Binary relation extraction The traditional featurebased methods rely on carefully designed features to learn good models, and often integrate diverse sources of evidence such as word sequences and syntax context (Kambhatla, 2004; GuoDong et al., 2005; Boschee et al., 2005; Suchanek et al., 2006; Chan and Roth, 2010; Nguyen and Grishman, 2014).",7 Related Work,[0],[0]
"The kernel-based methods design various subsequence or tree kernels (Mooney and Bunescu, 2005; Bunescu and Mooney, 2005; Qian et al., 2008) to capture structured information.",7 Related Work,[0],[0]
"Recently, models based on neural networks have advanced the state of the art by automatically learning powerful feature representations (Xu et al., 2015a; Zhang et al., 2015; Santos et al., 2015; Xu et al., 2015b; Xu et al., 2016).
",7 Related Work,[0],[0]
"Most neural architectures resemble Figure 2, where there is a core representation learner (blue) that takes word embeddings as input and produces contextual entity representations.",7 Related Work,[0],[0]
Such representations are then taken by relation classifiers to produce the final predictions.,7 Related Work,[0],[0]
"Effectively representing sequences of words, both convolutional (Zeng et al., 2014; Wang et al., 2016; Santos et al., 2015) and RNN-based architectures (Zhang et al., 2015; Socher et al., 2012; Cai et al., 2016) have been successful.",7 Related Work,[0],[0]
Most of these have focused on modeling either the surface word sequences or the hierarchical syntactic structure.,7 Related Work,[0],[0]
"Miwa and Bansal (2016) proposed an architecture that benefits from both types of information, using a surface sequence layer, followed by a dependency-tree sequence layer.
",7 Related Work,[0],[0]
"N -ary relation extraction Early work on extracting relations between more than two arguments has been done in MUC-7, with a focus on fact/event extraction from news articles (Chinchor, 1998).",7 Related Work,[0],[0]
"Semantic role labeling in the Propbank (Palmer et al., 2005) or FrameNet (Baker et al., 1998) style are also instances of n-ary relation extraction, with extraction of events expressed in a single sentence.",7 Related Work,[0],[0]
"McDonald et al. (2005) extract n-ary relations in a biomedical domain, by first factoring the n-ary relation into pair-wise relations between all entity pairs, and then constructing maximal cliques of related entities.",7 Related Work,[0],[0]
"Recently, neural models have been applied to semantic role labeling (FitzGerald et al., 2015; Roth
and Lapata, 2016).",7 Related Work,[0],[0]
"These works learned neural representations by effectively decomposing the n-ary relation into binary relations between the predicate and each argument, by embedding the dependency path between each pair, or by combining features of the two using a feed-forward network.",7 Related Work,[0],[0]
"Although some re-ranking or joint inference models have been employed, the representations of the individual arguments do not influence each other.",7 Related Work,[0],[0]
"In contrast, we propose a neural architecture that jointly represents n entity mentions, taking into account long-distance dependencies and inter-sentential information.
",7 Related Work,[0],[0]
"Cross-sentence relation extraction Several relation extraction tasks have benefited from crosssentence extraction, including MUC fact and event extraction (Swampillai and Stevenson, 2011), record extraction from web pages (Wick et al., 2006), extraction of facts for biomedical domains (Yoshikawa et al., 2011), and extensions of semantic role labeling to cover implicit inter-sentential arguments (Gerber and Chai, 2010).",7 Related Work,[0],[0]
"These prior works have either relied on explicit co-reference annotation, or on the assumption that the whole document refers to a single coherent event, to simplify the problem and reduce the need for powerful representations of multi-sentential contexts of entity mentions.",7 Related Work,[0],[0]
"Recently, cross-sentence relation extraction models have been learned with distant supervision, and used integrated contextual evidence of diverse types without reliance on these assumptions (Quirk and Poon, 2017), but that work focused on binary relations only and explicitly engineered sparse indicator features.
",7 Related Work,[0],[0]
"Relation extraction using distant supervision Distant supervision has been applied to extraction of binary (Mintz et al., 2009; Poon et al., 2015) and n-ary (Reschke et al., 2014;",7 Related Work,[0],[0]
"Li et al., 2015) relations, traditionally using hand-engineered features.",7 Related Work,[0],[0]
"Neural architectures have recently been applied to distantly supervised extraction of binary relations (Zeng et al., 2015).",7 Related Work,[0],[0]
"Our work is the first to propose a neural architecture for n-ary relation extraction, where the representation of a tuple of entities is not decomposable into independent representations of the individual entities or entity pairs, and which integrates diverse information from multi-sentential context.",7 Related Work,[0],[0]
"To utilize training data more effectively, we show how multitask learning for component binary sub-relations can
improve performance.",7 Related Work,[0],[0]
"Our learned representation combines information sources within a single sentence in a more integrated and generalizable fashion than prior approaches, and can also improve performance on single-sentence binary relation extraction.",7 Related Work,[0],[0]
We explore a general framework for cross-sentence nary relation extraction based on graph LSTMs.,8 Conclusion,[0],[0]
The graph formulation subsumes linear-chain and tree LSTMs and makes it easy to incorporate rich linguistic analysis.,8 Conclusion,[0],[0]
"Experiments on biomedical domains showed that extraction beyond the sentence boundary produced far more knowledge, and encoding rich linguistic knowledge provided consistent gain.
",8 Conclusion,[0],[0]
"While there is much room to improve in both recall and precision, our results indicate that machine reading can already be useful in precision medicine.",8 Conclusion,[0],[0]
"In particular, automatically extracted facts (Section 5.4) can serve as candidates for manual curation.",8 Conclusion,[0],[0]
"Instead of scanning millions of articles to curate from scratch, human curators would just quickly vet thousands of extractions.",8 Conclusion,[0],[0]
The errors identified by curators offer direct supervision to the machine reading system for continuous improvement.,8 Conclusion,[0],[0]
"Therefore, the most important goal is to attain high recall and reasonable precision.",8 Conclusion,[0],[0]
"Our current models are already quite capable.
",8 Conclusion,[0],[0]
Future directions include: interactive learning with user feedback; improving discourse modeling in graph LSTMs; exploring other backpropagation strategies; joint learning with entity linking; applications to other domains.,8 Conclusion,[0],[0]
"We thank Daniel Fried and Ming-Wei Chang for useful discussions, as well as the anonymous reviewers and editor-in-chief Mark Johnson for their helpful comments.",Acknowledgements,[0],[0]
Past work in relation extraction has focused on binary relations in single sentences.,abstractText,[0],[0]
Recent NLP inroads in high-value domains have sparked interest in the more general setting of extracting n-ary relations that span multiple sentences.,abstractText,[0],[0]
"In this paper, we explore a general relation extraction framework based on graph long short-term memory networks (graph LSTMs) that can be easily extended to cross-sentence n-ary relation extraction.",abstractText,[0],[0]
"The graph formulation provides a unified way of exploring different LSTM approaches and incorporating various intra-sentential and intersentential dependencies, such as sequential, syntactic, and discourse relations.",abstractText,[0],[0]
"A robust contextual representation is learned for the entities, which serves as input to the relation classifier.",abstractText,[0],[0]
"This simplifies handling of relations with arbitrary arity, and enables multi-task learning with related relations.",abstractText,[0],[0]
"We evaluate this framework in two important precision medicine settings, demonstrating its effectiveness with both conventional supervised learning and distant supervision.",abstractText,[0],[0]
Cross-sentence extraction produced larger knowledge bases.,abstractText,[0],[0]
and multi-task learning significantly improved extraction accuracy.,abstractText,[0],[0]
A thorough analysis of various LSTM approaches yielded useful insight the impact of linguistic analysis on extraction accuracy.,abstractText,[0],[0]
Cross-Sentence N -ary Relation Extraction with Graph LSTMs,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 778–783 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
778",text,[0],[0]
"Stance classification is the task of automatically identifying users’ positions about a specific target from text (Mohammad et al., 2017).",1 Introduction,[0],[0]
"Table 1 shows an example of this task, where the stance of the sentence is recognized as favorable on the target climate change is concern.",1 Introduction,[0],[0]
"Traditionally, this task is approached by learning a target-specific classifier that is trained for prediction on the same target of interest (Hasan and Ng, 2013; Mohammad et al., 2016; Ebrahimi et al., 2016).",1 Introduction,[0],[0]
"This implies that a new classifier has to be built from scratch on a well-prepared set of ground-truth data whenever predictions are needed for an unseen target.
",1 Introduction,[0],[0]
"An alternative to this approach is to conduct a cross-target classification, where the classifier is adapted from different but related targets (Augenstein et al., 2016), which allows benefiting from the knowledge of existing targets.",1 Introduction,[0],[0]
"For example, in our project we are interested in online users’ stances on the approvals of particular mining projects in the country.",1 Introduction,[0],[0]
"It might be useful to start with a classifier that is adapted from a related target such as climate change is concern (presumably available and annotated), as in both cases
users could discuss the impacts from the targets to some common issues, such as the environment or communities.
",1 Introduction,[0],[0]
Cross-target stance classification is a more challenging task simply because the language models may not be compatible between different targets.,1 Introduction,[0],[0]
"However, for some targets that can be recognized as being related to the same and more general domains, it could be possible to generalize through certain aspects of the domains that reflect users’ major concerns.",1 Introduction,[0],[0]
"For example, from the following sentence, whose stance is against the approval of a mining project, “Environmentalists warn the $16 billion coal facility will damage the Great Barrier Reef”, it can be seen that both this sentence and the one in Table 1 mention the same aspect “reef destruction/damage”, which is closely related to the “environment” domain.
",1 Introduction,[0],[0]
"In this paper, we focus on cross-target stance classification and explore the limits of generalizing models between different but domain-related targets1.",1 Introduction,[0],[0]
"The basic idea is to learn a set of domainspecific aspects from a source target, and then apply them to prediction on a destination target.",1 Introduction,[0],[0]
"To this end, we propose CrossNet, a novel neural model that implements the above idea based on the self-attention mechanism.",1 Introduction,[0],[0]
"Our preliminary analysis shows that the proposed model can find useful domain-specific information from a stancebearing sentence and that the classification performance is improved in certain domains.
",1 Introduction,[0],[0]
"1In this work, the source target is chosen based on common sense.",1 Introduction,[0],[0]
Exploring more sophisticated source target selection methods will be our future work.,1 Introduction,[0],[0]
"In this section, we introduce the proposed model, CrossNet, for cross-target stance classification.",2 Model,[0],[0]
Figure 1 shows the architecture of CrossNet.,2 Model,[0],[0]
It consists of four layers from the Embedding Layer (bottom) to the Prediction Layer (top).,2 Model,[0],[0]
It works by taking a stance-bearing sentence and a target as input and yielding the predicted stance label as output.,2 Model,[0],[0]
"In the following, we present the implementation of each layer in CrossNet.",2 Model,[0],[0]
"There are two inputs in CrossNet: a stance-bearing sentence P and a descriptive target T (e.g, climate change is concern in Table 1).",2.1 Embedding Layer,[0],[0]
"We use word embeddings (Mikolov et al., 2013) to represent each word in the input as a dense vector.",2.1 Embedding Layer,[0],[0]
"The output of this layer are two sequences of vectors P = {p1, ...,p|P |} and T = {t1, ..., t|T |}, where p, t are word vectors.",2.1 Embedding Layer,[0],[0]
"In this layer, we encode the contextual information in the input sentence and target.",2.2 Context Encoding Layer,[0],[0]
"We use a bi-directional Long Short-Term Memory Network (BiLSTM) (Hochreiter and Schmidhuber, 1997) to capture the left and right contexts of each word in the input.",2.2 Context Encoding Layer,[0],[0]
"Moreover, to account for the impact of the target on stance inference, we borrow the idea of conditional encoding (Augenstein et al., 2016) to model the dependency of the sentence on the target.",2.2 Context Encoding Layer,[0],[0]
"Formally, we first use a BiLSTMT to encode the target:
",2.2 Context Encoding Layer,[0],[0]
[ −→ h Ti −→c,2.2 Context Encoding Layer,[0],[0]
"Ti ] = −−−−→ LSTMT (ti, −→ h Ti−1, −→c",2.2 Context Encoding Layer,[0],[0]
Ti−1),2.2 Context Encoding Layer,[0],[0]
"[ ←− h Ti ←−c Ti ] = ←−−−− LSTMT (ti, ←− h Ti+1, ←−c Ti+1)",2.2 Context Encoding Layer,[0],[0]
"(1)
where h ∈ Rh and c ∈",2.2 Context Encoding Layer,[0],[0]
Rh are the hidden state and cell state of LSTM.,2.2 Context Encoding Layer,[0],[0]
The symbol −→(←−) indicates the forward (backward) pass.,2.2 Context Encoding Layer,[0],[0]
"ti is the input word vector at time step i.
",2.2 Context Encoding Layer,[0],[0]
"Then, we learn a conditional encoding of the sentence P , by initializing BiLSTMP (a different BiLSTM) with the final states of BiLSTMT :
",2.2 Context Encoding Layer,[0],[0]
"[ −→ h P1 −→c P1 ] = −−−−→ LSTMP (p1, −→ h",2.2 Context Encoding Layer,[0],[0]
T|T,2.2 Context Encoding Layer,[0],[0]
"|, −→c T|T",2.2 Context Encoding Layer,[0],[0]
"|)
",2.2 Context Encoding Layer,[0],[0]
"[ ←− h P|P | ←−c P|P |] = ←−−−− LSTMP (p|P |, ←− h T1 , ←−c T1 )
(2)
It can be seen that the initialization is done by aligning the forward (backward) pass of the two BiLSTMs.",2.2 Context Encoding Layer,[0],[0]
"The output is a contextually-encoded sequence, HP = {hP1 , ...,hP|P |}, where h =",2.2 Context Encoding Layer,[0],[0]
[ −→ h ; ←− h ] ∈ R2h with [; ] as the vector concatenation operation.,2.2 Context Encoding Layer,[0],[0]
"In this layer, we implement the idea of discovering domain-specific aspects for cross-target stance inference.",2.3 Aspect Attention Layer,[0],[0]
"In particular, the key observation we make is that the domain aspects that reflect users’ major concerns are usually the core of understanding their stances, and could be mentioned by multiple users in a discussion.",2.3 Aspect Attention Layer,[0],[0]
"For example, we find that many users in our corpus mention the aspect “reef” to express their concerns about the impact of a mining project on the Great Barrier Reef.",2.3 Aspect Attention Layer,[0],[0]
"Based on this observation, the perception of the domain aspects can be boiled down to finding the sentence parts that not only carry the core idea of a stance-bearing sentence but also tend to be recurring in the corpus.
",2.3 Aspect Attention Layer,[0],[0]
"First, to capture the recurrences of the domain aspects, a simple way is to make every input sentence be consumed by this layer (see Figure 1), so that the layer parameters are shared across the corpus for being stimulated by all appearances of the domain aspects.
",2.3 Aspect Attention Layer,[0],[0]
"Then, we utilize self-attention to signal the core parts of a stance-bearing sentence.",2.3 Aspect Attention Layer,[0],[0]
"Self-attention is an attention mechanism for selecting specific parts of a sequence by relating its elements at different positions (Vaswani et al., 2017; Cheng et al., 2016).",2.3 Aspect Attention Layer,[0],[0]
"In our case, the self-attention process is based on the assumption that the core parts of a sentence are those that are compatible with the semantics of the entire sentence.",2.3 Aspect Attention Layer,[0],[0]
"To this end, we introduce a compatibility function to score the semantic compatibility between the encoded se-
quence HP and each of its hidden states hP :
ci = w > 2 σ(W1h P i + b1) + b2 (3)
where W1 ∈ Rd×2h, w2 ∈ Rd, b1 ∈ Rd, and b2 ∈ R are trainable parameters, and σ is the activation function.",2.3 Aspect Attention Layer,[0],[0]
Note that all the above parameters are shared by every hidden state in HP .,2.3 Aspect Attention Layer,[0],[0]
"Next, we compute the attention weight ai for each hPi based on its compatibility score via softmax operation:
ai = exp(ci)∑|P | j=1 exp(cj)
(4)
",2.3 Aspect Attention Layer,[0],[0]
"Finally, we can obtain the domain aspect encoded representation based on the attention weights:
AP = |P |∑ i=1",2.3 Aspect Attention Layer,[0],[0]
"aih P i (5)
where AP ∈ R2h is the domain aspect encoding for sentence P and also the output of this layer.",2.3 Aspect Attention Layer,[0],[0]
"We predict the stance label of the sentence based on its domain aspect encoding:
ŷ = softmax(MLP(AP ))",2.4 Prediction Layer,[0],[0]
"(6)
where we use a multilayer perceptron (MLP) to consume the domain aspect encoding AP and apply the softmax to get the predicted probability for each of the C classes, ŷ = {y1, ..., yC}.",2.4 Prediction Layer,[0],[0]
"For model training, we use multi-class crossentropy loss,
J (θ) =",2.5 Model Training,[0],[0]
− N∑ i C∑ j y,2.5 Model Training,[0],[0]
(i) j log ŷ,2.5 Model Training,[0],[0]
(i) j,2.5 Model Training,[0],[0]
"+ λ‖Θ‖ (7)
",2.5 Model Training,[0],[0]
whereN is the size of training set.,2.5 Model Training,[0],[0]
"y is the groundtruth label indicator for each class, and ŷ is the predicted probability.",2.5 Model Training,[0],[0]
λ is the coefficient for L2regularization.,2.5 Model Training,[0],[0]
Θ denotes the set of all trainable parameters in our model.,2.5 Model Training,[0],[0]
This section reports the results of quantitative and qualitative evaluations of the proposed model.,3 Experiments,[0],[0]
SemEval-2016:,3.1 Datasets,[0],[0]
the first dataset is from SemEval2016,3.1 Datasets,[0],[0]
"Task 6 on Twitter stance detection, which contains stance-bearing tweets on different targets.",3.1 Datasets,[0],[0]
"We use the following five targets for our experiments: Climate Change is Concern (CC), Feminist Movement (FM), Hillary Clinton (HC), Legalization of Abortion (LA), and Donald Trump (DT).",3.1 Datasets,[0],[0]
"The class labels are favor, against, and neither, and their distributions are shown in Table 2.",3.1 Datasets,[0],[0]
Tweets on an Australian mining project (AM): the second is our collection of tweets on a mining project in Australia obtained using Twitter API.,3.1 Datasets,[0],[0]
"It includes 220,067 tweets posted from January 2016 to June 2017 that contain the project name in the text.",3.1 Datasets,[0],[0]
"We remove all URL-only tweets and duplicate tweets, and obtain a set of 40,852 (unlabeled) tweets.",3.1 Datasets,[0],[0]
"Due to the lack of annotation, this dataset is only used for our qualitative evaluation.
",3.1 Datasets,[0],[0]
"To align with our scenario, the above targets can be categorized into three different domains: Women’s Rights (FM, LA), American Politics (HC, DT), and Environments (CC, AM).",3.1 Datasets,[0],[0]
We use F1-score to measure the classification performance.,3.2 Metric,[0],[0]
"Due to the imbalanced class distributions of the SemEval dataset, we compute both micro-averaged (large classes dominate) and macro-averaged (small classes dominate) F1scores (Manning et al., 2008), and use their average as the metric, i.e., F = 12(Fmicro + Fmacro).
",3.2 Metric,[0],[0]
"To evaluate the effectiveness of target adaptation, we use the metric transfer ratio (Glorot et al., 2011) to compare the cross-target and in-target performance of a model: Q = F (S,D)Fb(D,D) , where F (S,D) is the cross-target F1-score of a model trained on the source target S and tested on the destination target D, and Fb(D,D) is the in-target F1-score of a baseline model trained and tested on the same target D, which serves as the performance calibration for target adaptation.",3.2 Metric,[0],[0]
"The word embeddings are initialized with the pretrained 200d GloVe word vectors on the 27B Twitter corpus (Pennington et al., 2014), and fixed during training.",3.3 Training setup,[0],[0]
"The model is trained (90%) and validated (10%) on a source target, and tested on a destination target.",3.3 Training setup,[0],[0]
"The following model settings are selected based on a small grid search on the validation set: the LSTM hidden size of 60, the MLP layer size of 60, and dropout 0.1.",3.3 Training setup,[0],[0]
The L2-regularization coefficient λ in the loss is 0.01.,3.3 Training setup,[0],[0]
"ADAM (Kingma and Ba, 2014) is used as the optimizer, with a learning rate of 10−3.",3.3 Training setup,[0],[0]
Stratified 10-fold cross-validation is conducted to produce averaged results.,3.3 Training setup,[0],[0]
This section reports the results of our model and two baseline approaches on cross-target stance classification.,3.4 Classification Performance,[0],[0]
BiLSTM:,3.4 Classification Performance,[0],[0]
this is a base model for our task.,3.4 Classification Performance,[0],[0]
It has two BiLSTMs for encoding the sentence and target separately.,3.4 Classification Performance,[0],[0]
"Then, the concatenation of the resulting encodings is fed into the final Prediction Layer to generate predicted stance labels.",3.4 Classification Performance,[0],[0]
"In our evaluation, this model is treated as the baseline model for deriving the in-target performance calibration Fb(D,D).",3.4 Classification Performance,[0],[0]
"MITRE (Augenstein et al., 2016):",3.4 Classification Performance,[0],[0]
"this is the
best system in SemEval-2016 Task 6.",3.4 Classification Performance,[0],[0]
It utilizes the conditional encoding to learn a targetdependent representation for the input sentence.,3.4 Classification Performance,[0],[0]
"The conditional encoding is realized in the same way as the Context Encoding Layer does in our model, namely by using the hidden states of the target-encoding BiLSTM to initialize the sentence-encoding BiLSTM.
Table 3 shows the results (in-target and crosstarget) on the two domains: Women’s Rights and American Politics.",3.4 Classification Performance,[0],[0]
"First, it is observed that MITRE outperforms BiLSTM over all target configurations, suggesting that, compared to simple concatenation, the conditional encoding of the target information could be more helpful to capture the dependency of the sentence on the target.
",3.4 Classification Performance,[0],[0]
"Second, our model is shown to achieve better results than the two baselines in almost all cases (only slightly worse than MITRE on LA under the in-target setting, and the difference is not statistically significant), which implies that the aspect attention mechanism adopted in our model could benefit target-level generalization while it does not hurt the in-target performance.",3.4 Classification Performance,[0],[0]
"Moreover, by comparing the performance of our model under different target configurations, we see that the improvements brought by our model are more significant on the cross-target task than they are on the intarget task, with an average improvement of 6.6% (cross-target) vs. 3.0% (in-target) over MITRE in F1-score, which demonstrates a greater advantage of our model in the cross-target task.
",3.4 Classification Performance,[0],[0]
"Finally, according to the transfer ratio results, the general drop from the in-target to cross-target performance (26% averaged over all cases) could imply that while the target-independent information (i.e., the domain-specific aspects) is shown to benefit generalization, it could be important to also consider the information that is specific to the destination target for model building (which has not yet been explored in this work).",3.4 Classification Performance,[0],[0]
"To show that our model can select sentence parts that are related to domain aspects, we visualize the self-attention results on some tweet examples that are correctly classified by our model in Table 4.
",3.5 Visualization of Attention,[0],[0]
We can see that the most highlighted parts in each example are relevant to the respective domain.,3.5 Visualization of Attention,[0],[0]
"For example, “feminist”, “rights”, and “equality” are commonly used when talking about women’s rights, and “president” and “dreams” of-
ten appear in text about politics.",3.5 Visualization of Attention,[0],[0]
"It is also interesting to note that words that are specific to the destination target may not be captured by the model learned from the source target, such as “abortion” in sentence 1 and “trumps” in sentence 3.",3.5 Visualization of Attention,[0],[0]
"This makes sense because those words are rare in the source target corpus and thus not well noticed by the model.
",3.5 Visualization of Attention,[0],[0]
"Finally, for our project, we can see from the last two sentences that the model learned from climate change is concern is able to concentrate on words that are central to understanding the authors’ stances on the approval of the mining project, such as “reef”, “destroy”, “environmental”, and “disaster”.",3.5 Visualization of Attention,[0],[0]
"Overall, the above visualization demonstrates that our model could benefit stance inference across related targets through capturing domain-specific information.",3.5 Visualization of Attention,[0],[0]
"Finally, it is also possible to show the learned domain aspects by extracting all sentence parts in a corpus that are highly attended by our model.",3.6 Learned Domain-Specific Aspects,[0],[0]
Table 5 presents a number of samples from the intersections between the sets of highly-attended words on the respective targets in the three domains.,3.6 Learned Domain-Specific Aspects,[0],[0]
"Again, we see that these highly-attended words are specific to the respective domains.",3.6 Learned Domain-Specific Aspects,[0],[0]
"We
also notice that besides the domain-aspect words, our model can find words that carry sentiments as well, such as “great”, “crazy”, and “beautiful”, which contribute to stance prediction.",3.6 Learned Domain-Specific Aspects,[0],[0]
"In this work, we study cross-target stance classification and propose a novel self-attention neural model that can extract target-independent information for model generalization.",4 Conclusion and Future Work,[0],[0]
Experimental results show that the proposed model can perceive high-level domain-specific information in a sentence and achieves superior results over a number of baselines in certain domains.,4 Conclusion and Future Work,[0],[0]
"In the future, there are several ways of extending our model.
",4 Conclusion and Future Work,[0],[0]
"First, selecting the effective source targets to generalize from is crucial for achieving satisfying results on the destination targets.",4 Conclusion and Future Work,[0],[0]
"One possibility could be to learn certain correlations between target closeness and generalization performance, which could further be used for guiding the target selection process.",4 Conclusion and Future Work,[0],[0]
"Second, our current model for identifying users’ stances on mining projects only generalizes from one source target (i.e., Climate Change is Concern).",4 Conclusion and Future Work,[0],[0]
"However, a mining project in general could affect other aspects of our society such as community and economics.",4 Conclusion and Future Work,[0],[0]
It could be useful to also consider other related sources for knowledge transfer.,4 Conclusion and Future Work,[0],[0]
"Finally, it would be interesting to evaluate our model in a multilingual scenario (Taulé et al., 2017), in order to examine its generalization ability (whether it can attend to useful domain-specific information in a new language) and multilingual scope.",4 Conclusion and Future Work,[0],[0]
We thank all anonymous reviewers for their valuable comments.,Acknowledgments,[0],[0]
We would also like to thank Keith Vander Linden for his helpful comments on drafts of this paper.,Acknowledgments,[0],[0]
"In stance classification, the target on which the stance is made defines the boundary of the task, and a classifier is usually trained for prediction on the same target.",abstractText,[0],[0]
"In this work, we explore the potential for generalizing classifiers between different targets, and propose a neural model that can apply what has been learned from a source target to a destination target.",abstractText,[0],[0]
We show that our model can find useful information shared between relevant targets which improves generalization in certain scenarios.,abstractText,[0],[0]
Cross-Target Stance Classification with Self-Attention Networks,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3664–3674 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
3664",text,[0],[0]
Information retrieval and question answering are by now mature technologies that excel at answering factual queries on noncontroversial topics.,1 Introduction,[0],[0]
"However, they provide no specialized support for queries where there is no single canonical answer, as with topics that are controversial or opinion-based.",1 Introduction,[0],[0]
"For such queries, the user may need to carefully assess the stance, source, and supportability for each of the answers.",1 Introduction,[0],[0]
"These processes can be supported by argument mining (AM), a nascent area of natural language processing concerned with the automatic recognition and interpretation of arguments.",1 Introduction,[0],[0]
"In this paper, we apply AM to the task of argument search—that is, searching a large document collection for arguments relevant to a given topic.",1 Introduction,[0],[0]
"Searching for and classifying relevant arguments plays an important role in decision making (Svenson, 1979), legal reasoning (Wyner et al., 2010), and
the critical reading, writing, and summarization of persuasive texts (Kobayashi, 2009; Wingate, 2012).",1 Introduction,[0],[0]
"Automating the argument search process could ease much of the manual effort involved in these tasks, particularly if it can be made to robustly handle arguments from different text types and topics.",1 Introduction,[0],[0]
"But despite its obvious usefulness, this sort of argument search has attracted little attention in the research community.",1 Introduction,[0],[0]
"This may be due in part to the limitations of the underlying models and training resources, particularly as they relate to heterogeneous sources.",1 Introduction,[0],[0]
"That is, most current approaches to AM are designed for use with particular text types, faring poorly when applied to new data (Daxenberger et al., 2017).",1 Introduction,[0],[0]
"Indeed, as Habernal et al. (2014) observe, while there is a great diversity of perspectives on how arguments can be best characterized and modelled, there is no “one-size-fits-all” argumentation theory that applies to the variety of text sources found on the Web.",1 Introduction,[0],[0]
"To approach these challenges, we propose the novel task of topic-based sentential argument mining.",1 Introduction,[0],[0]
Our contributions are as follows: (1) We propose a new argument annotation scheme applicable to the information-seeking perspective of argument search.,1 Introduction,[0],[0]
"We show it to be general enough for use on heterogeneous data sources, and simple enough to be applied manually by untrained annotators at a reasonable cost.",1 Introduction,[0],[0]
"(2) We introduce a novel corpus of heterogeneous text types annotated with topic-based arguments.1 The corpus includes over 25,000 instances covering eight controversial topics.",1 Introduction,[0],[0]
This is the first known resource that can be used to evaluate the performance of argument mining methods across topics in heterogeneous sources.,1 Introduction,[0],[0]
"(3) We investigate different approaches for incorporating topic information into neural networks and
1https://www.ukp.tu-darmstadt.de/sent_am
show that including the topic vector into the i- and c-gates of the LSTM cell outperforms common attention-based approaches in two- and three-label cross-topic experiments.",1 Introduction,[0],[0]
(4) We further improve the performance of the modified LSTM cell by leveraging additional data for topic relevance in a multi-task learning setup.,1 Introduction,[0],[0]
"(5) In the more challenging setup of cross-topic experiments, we show that our models yield considerably better performance than common BiLSTM models when little data of the target topic is available.",1 Introduction,[0],[0]
"Most existing approaches treat argument mining at the discourse level, focusing on tasks such as segmenting argumentative discourse units (Ajjour et al., 2017; Goudas et al., 2014), classifying the function of argumentative discourse units (for example, as claims or premises) (Mochales-Palau and Moens, 2009; Stab and Gurevych, 2014), and recognizing argumentative discourse relations (Eger et al., 2017; Stab and Gurevych, 2017; Nguyen and Litman, 2016).",2 Related work,[0],[0]
"These discourse-level approaches address the identification of argumentative structures within a single document but do not consider relevance to externally defined topics.
",2 Related work,[0],[0]
"To date, there has been little research on the identification of topic-relevant arguments for argument search.",2 Related work,[0],[0]
Wachsmuth et al. (2017) present a generic argument search framework.,2 Related work,[0],[0]
"However, it relies on already-structured arguments from debate portals and is not yet able to retrieve arguments from arbitrary texts.",2 Related work,[0],[0]
"Levy et al. (2014) investigate the identification of topic-relevant claims, an approach that was later extended with evidence extraction to mine supporting statements for claims (Rinott et al., 2015).",2 Related work,[0],[0]
"However, both approaches are designed to mine arguments from Wikipedia articles; it is unclear whether their annotation scheme is applicable to other text types.",2 Related work,[0],[0]
"It is also uncertain that it can be easily and accurately applied by untrained annotators, since it requires unitizing (i.e., finding the boundaries of argument components at the token level).",2 Related work,[0],[0]
Hua and Wang (2017) identify sentences in cited documents that have been used by an editor to formulate an argument.,2 Related work,[0],[0]
"By contrast, we do not limit our approach to the identification of sentences related to a given argument, but rather focus on the retrieval of any argument relevant to a given topic.",2 Related work,[0],[0]
"The fact that we are concerned with retrieval of arguments also sets our work apart from
the discourse-agnostic stance detection task of Mohammad et al. (2016), which is concerned with the identification of sentences expressing support or opposition to a given topic, irrespective of whether those sentences contain supporting evidence (as opposed to mere statements of opinion).
",2 Related work,[0],[0]
"Cross-domain AM experiments have so far been conducted only for discourse-level tasks such as claim identification (Daxenberger et al., 2017), argumentative segment identification (Al-Khatib et al., 2016), and argumentative unit segmentation (Ajjour et al., 2017).",2 Related work,[0],[0]
"However, the discourse-level argumentation models these studies employ seem to be highly dependent on the text types for which they were designed; they do not work well when applied to other text types (Daxenberger et al., 2017).",2 Related work,[0],[0]
The crucial difference between our own work and prior cross-domain experiments is that we investigate AM from heterogeneous texts across different topics instead of studying specific discourse-level AM tasks across restricted text types of existing corpora.,2 Related work,[0],[0]
"There exists a great diversity in models of argumentation, which differ in their perspective, complexity, terminology, and intended applications (Bentahar et al., 2010).",3 Corpus creation,[0],[0]
"For the present study, we propose a model which, though simplistic, is nonetheless well-suited to the argument search scenario.",3 Corpus creation,[0],[0]
We define an argument as a span of text expressing evidence or reasoning that can be used to either support or oppose a given topic.,3 Corpus creation,[0],[0]
"An argument need not be “direct” or self-contained—it may presuppose some common or domain knowledge, or the application of commonsense reasoning—but it must be unambiguous in its orientation to the topic.",3 Corpus creation,[0],[0]
"A topic, in turn, is some matter of controversy for which there is an obvious polarity to the possible outcomes—that is, a question of being either for or against the use or adoption of something, the commitment to some course of action, etc.",3 Corpus creation,[0],[0]
"In some graph-based models of argumentation (Stab, 2017, Ch. 2), what we refer to as a topic would be part of a (major) claim expressing a positive or negative stance, and our arguments would be premises with supporting/attacking consequence relations to the claim.",3 Corpus creation,[0],[0]
"However, unlike these models, which are typically used to represent (potentially deep or complex) argument structures at the discourse level, ours is a flat model that considers arguments in isolation from their surrounding context.",3 Corpus creation,[0],[0]
"A great
advantage of this approach is that it allows annotators to classify text spans without having to read large amounts of context and without having to consider relations to other topics or arguments.",3 Corpus creation,[0],[0]
"In this work, we consider only those topics that can be concisely and implicitly expressed through keywords, and those arguments that consist of individual sentences.",3 Corpus creation,[0],[0]
"Some examples, drawn from our dataset, are shown in Table 1.",3 Corpus creation,[0],[0]
"Note that while the fourth example expresses opposition to the topic, under our definition it is properly classified as a non-argument because it is a mere statement of stance that provides no evidence or reasoning.
",3 Corpus creation,[0],[0]
Data.,3 Corpus creation,[0],[0]
For our experiments we gathered a large collection of manually annotated arguments that cover a variety of topics and that come from a variety of text types.,3 Corpus creation,[0],[0]
"We started by randomly selecting eight topics (see Table 2) from online lists of controversial topics.2 For each topic, we made a Google query for the topic name, removed results not archived by the Wayback Machine,3 and truncated the list to the top 50 results.",3 Corpus creation,[0],[0]
"This resulted in a set of persistent, topic-relevant, largely polemical Web documents representing a range of genres and text types, including news reports, editorials, blogs, debate forums, and encyclopedia articles.",3 Corpus creation,[0],[0]
"We preprocessed each document with Apache Tika (Mattmann and Zitting, 2011) to remove boilerplate text.",3 Corpus creation,[0],[0]
"We then used the Stanford CoreNLP tools (Manning et al., 2014) to perform tokenization, sentence segmentation, and part-ofspeech tagging on the remaining text, and removed all sentences without verbs or with less than three tokens.",3 Corpus creation,[0],[0]
"This left us with a raw dataset of 27,520 sentences (about 2,700 to 4,400 per topic).",3 Corpus creation,[0],[0]
"Annotators classified the sentences using a browser-based interface that presents a set of in2https://www.questia.com/library/ controversial-topics, https://www.procon.org/ 3https://web.archive.org/
structions, a topic, a list of sentences, and amultiplechoice form for specifying whether each sentence is a supporting argument, an opposing argument, or not an argument with respect to the topic.",3 Corpus creation,[0],[0]
"(In preliminary experiments, we presented annotators with a fourth option for sentences that are ambiguous or incomprehensible.",3 Corpus creation,[0],[0]
"However, we found that these constituted less than 1% of the distribution and so mapped all such answers to the “no argument” class.)
",3 Corpus creation,[0],[0]
Annotation experiments.,3 Corpus creation,[0],[0]
"We tested the applicability of our annotation scheme by untrained annotators by performing an experiment where we had a group of “expert” annotators and a group of untrained annotators classify the same set of sentences, and then compared the two groups’ classifications.",3 Corpus creation,[0],[0]
The data for this experiment consisted of 200 sentences randomly selected from each of our eight topics.,3 Corpus creation,[0],[0]
Our expert annotators were two graduatelevel language technology researchers who were fully briefed on the nature and purpose of the argument model.,3 Corpus creation,[0],[0]
Our untrained annotators were anonymous American workers from the Amazon Mechanical Turk (AMT) crowdsourcing platform.,3 Corpus creation,[0],[0]
"Each sentence was independently annotated by the two expert annotators and ten crowd workers.
",3 Corpus creation,[0],[0]
"Inter-annotator agreement for our two experts, as measured by Cohen’s κ, was 0.721; this exceeds the commonly used threshold of 0.7 for assuming the results are reliable (Carletta, 1996).",3 Corpus creation,[0],[0]
"We proceeded by having the two experts resolve their disagreements, resulting in a set of “expert” gold-standard annotations.",3 Corpus creation,[0],[0]
"Similar gold standards were produced for the crowd annotations by applying the MACE denoising tool (Hovy et al., 2013); we tested various thresholds (1.0, 0.9, and 0.8) to discard instances that could be confidently assigned a gold label.",3 Corpus creation,[0],[0]
We then calculated κ between the remaining instances in the expert and crowd gold standards.,3 Corpus creation,[0],[0]
"In order to
determine the relationship between inter-annotator agreement and the number of crowd workers, we performed this procedure with successively lower numbers of crowd workers, going from the original ten annotators per instance down to two.",3 Corpus creation,[0],[0]
The results are visualized in Fig. 1.,3 Corpus creation,[0],[0]
We found that using seven annotators and a MACE threshold of 0.9 results in κ = 0.723; this gives us similar reliability as with the expert annotators without sacrificing much coverage.,3 Corpus creation,[0],[0]
"Table 3 shows the κ and percentage agreement for this setup, as well as the agreement between our expert annotators, broken down by topic.",3 Corpus creation,[0],[0]
"We proceeded with annotating the remaining instances in our dataset using seven crowd workers each, paying a rate corresponding to the US federal minimum wage of $7.25/hour.",3 Corpus creation,[0],[0]
"Our total expenditure, including AMT processing fees, was $2,774.02.",3 Corpus creation,[0],[0]
"After MACE denoising, we were left with 25,492 gold-standard annotations.",3 Corpus creation,[0],[0]
Table 2 provides statistics on the size and class distribution of the final corpus.,3 Corpus creation,[0],[0]
"We are releasing the gold-standard annotations for this dataset, and code for retrieving
the original sentences from the Wayback Machine, under a Creative Commons licence.",3 Corpus creation,[0],[0]
We model the identification of arguments as a sentence-level classification task.,4 Approaches for identifying arguments,[0],[0]
"In particular, given a sentence ς with words u1, . . .",4 Approaches for identifying arguments,[0],[0]
",unς and a topic τ of words v1, . . .",4 Approaches for identifying arguments,[0],[0]
", vnτ (e.g., “gun control” or “school uniforms”), we aim to classify ς",4 Approaches for identifying arguments,[0],[0]
"as a “supporting argument” or “opposing argument” if it includes a relevant reason for supporting or opposing the τ , or as a “non-argument” if it does not include a reason or is not relevant to τ .",4 Approaches for identifying arguments,[0],[0]
We also investigate a two-label classification where we combine supporting and opposing arguments into a single category; this allows us to evaluate argument classification independent of stance.,4 Approaches for identifying arguments,[0],[0]
"We focus on the challenging task of cross-topic experiments, where one topic is withheld from the training data and used for testing.",4 Approaches for identifying arguments,[0],[0]
"Here, we denote scalars by italic lowercase letters (e.g., t), vector representations by italic bold lowercase letters (e.g., c), and matrices as italic bold uppercase letters (e.g.,W ).",4 Approaches for identifying arguments,[0],[0]
"Since arguments need to be relevant to the given topic, we posit that providing topic information to the learner results in a more robust prediction capability in cross-topic setups.",4.1 Integrating topic information,[0],[0]
"Below, we present two models that integrate the topic, one that uses an attention mechanism and another that includes the topic vector directly in the LSTM cell.
",4.1 Integrating topic information,[0],[0]
Outer-attention BiLSTM (outer-att).,4.1 Integrating topic information,[0],[0]
"To let the model learn which parts of the sentence are relevant (or irrelevant) to the given topic, we use an attentionbased neural network (Bahdanau et al., 2014) that learns an importance weighting of the input words depending on the given topic.",4.1 Integrating topic information,[0],[0]
"In particular, we adopt an outer-attention mechanism similar to the one proposed by Hermann et al. (2015), which has achieved state-of-the-art results in related tasks such as natural language inference and recognizing textual entailment (Rocktäschel et al., 2015; Wang and Jiang, 2016).",4.1 Integrating topic information,[0],[0]
"We combine the attention mechanism with a common BiLSTM model and, at time step t, determine the importance weighting for each hidden state h(t) as
m(t) = tanh(W hh(t)",4.1 Integrating topic information,[0],[0]
"+W pp) (1)
fattention(h(t),p) = exp(wTmm(t))∑ t exp(wTmm(t))
",4.1 Integrating topic information,[0],[0]
"(2)
whereW h,W p, andwm are trainable parameters of the attention mechanism and p is the average of all word embeddings of topic words v1, . .",4.1 Integrating topic information,[0],[0]
.,4.1 Integrating topic information,[0],[0]
", vnτ .",4.1 Integrating topic information,[0],[0]
"Using the importance weighting, we determine the final, weighted hidden output state s as
αt ∝ fattention(h(t),p) (3) s = n∑ t=1",4.1 Integrating topic information,[0],[0]
h(t)αt .,4.1 Integrating topic information,[0],[0]
"(4)
Finally, we feed s into a dense layer with a softmax activation function to get predictions for our twoor three-label setups.
",4.1 Integrating topic information,[0],[0]
Contextual BiLSTM (biclstm).,4.1 Integrating topic information,[0],[0]
"A more direct approach to integrating an argument’s topic is the contextual LSTM (CLSTM) architecture (Ghosh et al., 2016), where topic information is added as another term to all four gates of an LSTM cell.",4.1 Integrating topic information,[0],[0]
"We, however, hypothesize that topic information is more relevant at the i- and c-gates, the former because it has the biggest impact on how a new token is processed and the latter because it is closely linked
to how the sequence seen so far is to be interpreted and stored.",4.1 Integrating topic information,[0],[0]
"To this end, we experimented with severalmodifications to the original CLSTMsuch as removing peepholes—i.e., removing gates’ access to the cell state c",4.1 Integrating topic information,[0],[0]
"(Gers and Schmidhuber, 2000)— and removing topic information from one or more gates.",4.1 Integrating topic information,[0],[0]
"Empirical results on the validation set show that topic integration at the i- and c-gates only, and removal of all peephole connections, does indeed outperform the original CLSTM on our task by 1 percentage point.",4.1 Integrating topic information,[0],[0]
"Our modified CLSTM (Fig. 2) is defined as
i t = σ(W xi x t",4.1 Integrating topic information,[0],[0]
+,4.1 Integrating topic information,[0],[0]
"W hiht−1 + bi + Wpi p ) (5)
f t = σ(W x f x t",4.1 Integrating topic information,[0],[0]
+W h f ht−1 + b f ),4.1 Integrating topic information,[0],[0]
"(6) ct = f tct−1 + i tσc(W xc x t +W hcht−1
+bc + Wpc p ) (7)
ot = σ(W xox t",4.1 Integrating topic information,[0],[0]
+,4.1 Integrating topic information,[0],[0]
W hoht−1 + bo) (8) ht = otσc(ct ).,4.1 Integrating topic information,[0],[0]
"(9)
Here i , f , and o represent the input, forget, and output gates; c the cell memory; x t the embedded token of a sentence at timestep t; ht−1 the previous hidden state; and b the bias.",4.1 Integrating topic information,[0],[0]
σ,4.1 Integrating topic information,[0],[0]
"and σc are the activation and recurrent activation functions, respectively.",4.1 Integrating topic information,[0],[0]
The novel terms for topic integration are outlined.,4.1 Integrating topic information,[0],[0]
"We use this model bidirectionally, as we did with our BiLSTM network, and hence refer to it as biclstm.",4.1 Integrating topic information,[0],[0]
"As we want to classify arguments related to specific topics, leveraging information that supports the classifier in the decision of topic-relation is crucial.",4.2 Leveraging additional data,[0],[0]
The multi-task learning (mtl) and transfer learning (trl) models are able to make use of auxiliary data that can potentially improve the results on the main task.,4.2 Leveraging additional data,[0],[0]
"Thus, we extend our previously described models by integrating them into mtl and trl setups.",4.2 Leveraging additional data,[0],[0]
"We also choose to integrate two corpora
from which we expect to learn (a) topic-relevance and (b) the capability to distinguish between supporting and attacking arguments.",4.2 Leveraging additional data,[0],[0]
"The first corpus, DIP2016 (Habernal et al., 2016), consists of 49 queries from the educational domain and 100 documents for each query.",4.2 Leveraging additional data,[0],[0]
Each document has its sentences annotated for relevance (true/false) to the query.4,4.2 Leveraging additional data,[0],[0]
"The second corpus, from SemEval-2016 Task 6 (Mohammad et al., 2016), consists of around 5000 multi-sentence tweets, a corresponding topic (e.g., “atheism”), and the author’s stance on the topic (for/against/neither).
",4.2 Leveraging additional data,[0],[0]
"For our mtl and trl approaches, we consider every possible pairing of a model (biclstm, outer-att, and the bilstm baseline we introduce in §5) with an auxiliary corpus (DIP2016, SemEval).",4.2 Leveraging additional data,[0],[0]
"We formalize our datasets as Sk = {(xki ,pki , yki )|i = 0, . . .",4.2 Leveraging additional data,[0],[0]
", |Sk |}, where k can be either our main dataset or an auxiliary dataset, xki denotes a single sentence as a sequence of word embeddings and yki its corresponding label in k, and pki represents the corresponding averaged topic vector.
",4.2 Leveraging additional data,[0],[0]
Transfer learning (trl).,4.2 Leveraging additional data,[0],[0]
"For trl, we use the approach of parameter transfer (Pan andYang, 2010)— i.e., we do not modify the model used.",4.2 Leveraging additional data,[0],[0]
"Instead, we train the model twice: the first time, we train the model on the chosen auxiliary corpus, and the second time, we keep the trained model’s weights and train it with our own corpus.",4.2 Leveraging additional data,[0],[0]
"For the threelabel setting, we have to modify the transfer model slightly for the DIP2016 corpus, since it provides only two labels for each training sample.",4.2 Leveraging additional data,[0],[0]
"In this case, we simply add a layer with two neurons on top of the layer with three neurons for training with the DIP2016 corpus and remove it afterwards for training with our corpus.
4We only use 300K of the corpus’s 600K samples to ease hyperparameter tuning for our computation-heavy models.
",4.2 Leveraging additional data,[0],[0]
Multi-task learning (mtl).,4.2 Leveraging additional data,[0],[0]
"For mtl, we use a shared–private model (Liu et al., 2017), which showed promising results for text classification and word segmentation (Chen et al., 2017).",4.2 Leveraging additional data,[0],[0]
"(We also experimented with their adversarial approach to learn topic-invariant features, but abandoned this due to low scores.)",4.2 Leveraging additional data,[0],[0]
"The mtl base model consists of a private recurrent neural network (RNN) for both the auxiliary dataset and our dataset, plus a shared RNN that both datasets use (Fig. 3).",4.2 Leveraging additional data,[0],[0]
The last hidden states of the RNNs are concatenated and fed through a dense layer and a softmax activation function.,4.2 Leveraging additional data,[0],[0]
"The model is trained in an alternating fashion—i.e., after each epoch the loss for the other dataset is minimized until each dataset has run for the set number of epochs, where the last epoch is always executed on our dataset.",4.2 Leveraging additional data,[0],[0]
"At prediction time, only the private RNN trained on our dataset and the shared RNN are used.",4.2 Leveraging additional data,[0],[0]
"The core idea is that the shared RNN learns what is relevant for both tasks, while the private ones learn only the task-specific knowledge.
",4.2 Leveraging additional data,[0],[0]
"For the cases of mtl+bilstm+corpus, mtl+biclstm+ corpus, and mtl+outer-att+corpus, we simply switch the RNN with our bilstm, biclstm, and outer-att, respectively.",4.2 Leveraging additional data,[0],[0]
"For mtl+outer-att+corpus, we add the outer attention mechanism (see §4.1), modified for use with the mtl model, after each of the private RNNs, while additionally feeding it a second topic vector—the last hidden state of the shared RNN:
m(t) =",4.2 Leveraging additional data,[0],[0]
tanh(W rhr (t) +,4.2 Leveraging additional data,[0],[0]
W shs,4.2 Leveraging additional data,[0],[0]
"+W pp) (10) fattention(hr (t),hs,p) = exp(wTmm(t))∑ t exp(wTmm(t))",4.2 Leveraging additional data,[0],[0]
"(11)
",4.2 Leveraging additional data,[0],[0]
αt ∝,4.2 Leveraging additional data,[0],[0]
"fattention(hr (t),hs,p) (12)
s = n∑ t=1 hr (t)αt (13)
",4.2 Leveraging additional data,[0],[0]
"whereW r ,W s, andW p are trainable weight matrices, hr (t) is the hidden state of the private bilstm at timestep t, hs is the last hidden state of the shared model, and p is the average of all word embeddings of topic words v1, . . .",4.2 Leveraging additional data,[0],[0]
", vnτ .",4.2 Leveraging additional data,[0],[0]
"To evaluate the robustness of the models, we conduct cross-topic experiments to evaluate how well the models generalize to an unknown topic.",5 Evaluation,[0],[0]
"To this end, we combine training (70%) and validation
data (10%) of seven topics for training and parameter tuning, and use the test data (20%) of the eighth topic for testing.",5 Evaluation,[0],[0]
"For encoding the words of sentence ς and topic τ , we use 300-dimensional word embeddings trained on the Google News dataset by Mikolov et al. (2013).",5 Evaluation,[0],[0]
"To handle out-of-vocabulary words, we create separate random word vectors for each.5 Since reporting single performance scores is insufficient to compare non-deterministic learning approaches like neural networks (Reimers and Gurevych, 2017), we report all results as averages over ten runs with different random seeds.",5 Evaluation,[0],[0]
"As evaluation measures, we report the average macro F1, as well as the precision and the recall for the argument class (Parg, Rarg).",5 Evaluation,[0],[0]
"For the three-label approach, we split the precision and recall for predicting supporting (Parg+, Rarg+) and attacking arguments (Parg−, Rarg−).",5 Evaluation,[0],[0]
"As baselines, we use a simple bidirectional LSTM (Hochreiter and Schmidhuber, 1997), as well as a logistic regression model with lowercased unigram features, which has been shown to be a strong baseline for various other AM tasks (Daxenberger et al., 2017; Stab and Gurevych, 2017).",5 Evaluation,[0],[0]
"We refer to these models as bilstm and lr-uni, respectively.",5 Evaluation,[0],[0]
"All neural networks are trained using the Adam optimizer (Kingma and Ba, 2015) and cross-entropy loss function.",5 Evaluation,[0],[0]
"For finding the best model, we run each for ten epochs and take the best model based on the lowest validation loss.",5 Evaluation,[0],[0]
"In addition to that, we tune the hyperparameters of all
5Each dimension is set to a random number between −0.01 and 0.01.",5 Evaluation,[0],[0]
"Digits are mapped to the same random word vector.
neural networks (see Appendix A).",5 Evaluation,[0],[0]
"To accelerate training, we truncate sentences at 60 words.6",5 Evaluation,[0],[0]
Two-label setup.,5.1 Results,[0],[0]
The results in Table 4 show that all our models outperform the baselines for two-label prediction.7 F1 for biclstm improves by 3.5 percentage points over the bilstm baseline and by 5.6 over lr-uni.,5.1 Results,[0],[0]
A main reason for this proves to be the substantial increase in recall for our topic-integrating models—outer-att and especially biclstm—in comparison to our baselines.,5.1 Results,[0],[0]
These results show that knowledge of the argument’s topic has a strong impact on argument prediction capability.,5.1 Results,[0],[0]
"Further, we observe that integrating biclstm in a multi-task learning setup in order to draw knowledge about topic relevance from the DIP2016 corpus (mtl+biclstm+dip2016) improves F1 by an additional 2.5 percentage points.",5.1 Results,[0],[0]
"It achieves an F1 of 0.6662, which is 19.48 percentage points less than the human upper bound of 0.861.",5.1 Results,[0],[0]
"When using the SemEval corpus, which holds less task-relevant knowledge for our two-label approach, we are able to gain only 1 percentage point when integrating it into mtl+biclstm+corpus.
",5.1 Results,[0],[0]
"For the transfer learning models that integrate the topic (tr+biclstm+corpus and tr+outer-att+corpus), the parameter transfer is mostly ineffective.",5.1 Results,[0],[0]
"If no topic is provided (tr+bilstm+corpus), the transfer learning models are able to improve over the baseline bilstm.",5.1 Results,[0],[0]
"This shows that the parameter transfer
6Only 244 of our sentences (<1%) exceed this length.",5.1 Results,[0],[0]
"7Detailed results per topic are given in Appendix B.
itself can be of use, but confuses the model when combined with topic integration.
",5.1 Results,[0],[0]
"In general, we observe an overall lower score for trl models that use the DIP2016 corpus compared to those using the SemEval corpus.",5.1 Results,[0],[0]
"In contrast to the mtl model, for trl models all parameters are transferred to the main task, not just parameters that represent shared knowledge.",5.1 Results,[0],[0]
"Thus, we suspect the lower scores of the trl models with DIP2016 are due to overfitting on the vast number of samples which shape the parameters much more than the comparatively small SemEval corpus could.
",5.1 Results,[0],[0]
Three-label setup.,5.1 Results,[0],[0]
"For the three-label approach, we observe overall lower scores due to the additional difficulty in distinguishing supporting from opposing arguments.",5.1 Results,[0],[0]
"As already observed in the two-label setup, biclstm outperforms both the bilstm and lr-uni baselines; here, the former by 4.5 and the latter by 4.2 percentage points in F1.",5.1 Results,[0],[0]
"Again, this is caused by a substantial increase in recall and shows the impact that the available topic information has on the classifier’s predictive power.",5.1 Results,[0],[0]
"For transfer learning, we see similar results as for the two-label approach; both the DIP2016 and SemEval corpora have a generally negative impact when compared to the respective base models.",5.1 Results,[0],[0]
The SemEval corpus does not provide the knowledge required to distinguish supporting from attacking arguments.,5.1 Results,[0],[0]
"We conclude that the original purpose of the SemEval task, stance recognition, is too different from our own.",5.1 Results,[0],[0]
"But in multi-task learning, where only the shared parameters are taken, we observe slight improvements when using biclstm with DIP2016; this correlates with the same model in the two-label setup.",5.1 Results,[0],[0]
"To understand the errors of our best model, mtlbiclstm-dip, and the nature of this task, we manually analyzed 100 sentences randomly sampled from the false positive and false negative arguments of the three-label experiments (combining supporting and attacking arguments).",5.2 Error analysis,[0],[0]
"Among the false positives, we found 48 off-topic sentences that were wrongly classified as arguments.",5.2 Error analysis,[0],[0]
The 52 on-topic false positives consist of non-argumentative background information or mere opinions without evidence (as with the first and fourth examples of Table 1) and questions about the topic.,5.2 Error analysis,[0],[0]
"Among the false negatives, we found 65 arguments that did not explicitly refer to the topic but to related aspects that
depend on background knowledge.",5.2 Error analysis,[0],[0]
"For instance, the model fails to establish an argumentative link between the topic “gun control” and the Second Amendment to the US Constitution.",5.2 Error analysis,[0],[0]
"Lastly, we inspected arguments that are incorrectly classified as supporting and/or opposing a topic.",5.2 Error analysis,[0],[0]
We found several samples in which the term “against” is not correctly interpreted and the argument is classified as supporting a topic.,5.2 Error analysis,[0],[0]
"Similarly, for arguments incorrectly classified as attacking, we find various samples where the word “oppose” is used not to oppose the topic but to strengthen a supporting argument, as in “There is reason even for people who oppose the use of marijuana to support its legalization. . . ”",5.2 Error analysis,[0],[0]
"To evaluate the performance of the models in datascarce scenarios, we gradually add target topic data to the training data and analyze the model performance on the target test set.",5.3 Adapting to new topics,[0],[0]
"Figure 4 shows model performance (F1, Parg, and Rarg) on the “marijuana legalization” topicwhen adding different amounts of randomly sampled topic-specific data to the training data (x-axes).8",5.3 Adapting to new topics,[0],[0]
"As the results show, the models that integrate the topic achieve higher recall when adding target topic data to the training data.",5.3 Adapting to new topics,[0],[0]
"For bilstm, we observe a drastic difference when compared to the other models; the recall for arguments stays at around 30% and rises only when integrating more than 60% target topic data.",5.3 Adapting to new topics,[0],[0]
"In strong contrast, topic-integrating models retrieve a much higher number of actual arguments at target topic augmentation levels as low as 20%.",5.3 Adapting to new topics,[0],[0]
"Further, and equally important, this does not come at the cost of precision; on the contrary, the precision is mostly steady and slowly rising after around 20% of target topic integration, leading to an overall higher F1 for these models.",5.3 Adapting to new topics,[0],[0]
"Finally, in comparing F1 between topic-integrating models and bilstm, we conclude that the former need much less target topic data to substantially improve their score, making them more robust in situations of data scarcity.",5.3 Adapting to new topics,[0],[0]
We have presented a new approach for searching a document collection for arguments relevant to a given topic.,6 Conclusion,[0],[0]
"First, we introduced an annotation scheme suited to the information-seeking perspec-
8Each data point in the plot is the average score of ten runs with different random samples of target topic data.
",6 Conclusion,[0],[0]
tive of argument search and showed that it is cheaply but reliably applicable by untrained annotators to arbitrary Web texts.,6 Conclusion,[0],[0]
"Second, we presented a new corpus, including over 25,000 instances over eight topics, that allows for cross-topic experiments using heterogeneous text types.",6 Conclusion,[0],[0]
"Third, we conducted cross-topic experiments and showed that integrating topic information of arguments with our contextual BiLSTM leads to better generalization to unknown topics.",6 Conclusion,[0],[0]
"Fourth, by leveraging knowledge from similar datasets and integrating our contextual BiLSTM into a multi-task learning setup, we were able to gain an improvement over our strongest baseline of 5.9 percentage points in F1 in the two-label setup and 4.6 in the three-label setup.",6 Conclusion,[0],[0]
"Finally, by gradually adding target topic data to our training set, we showed that, when available, even small amounts of target topic data (20%) have a strong positive influence on the recall of arguments.
",6 Conclusion,[0],[0]
"In a separate, simultaneously written paper (Stab et al., 2018) we evaluate our models in real-world application scenarios by applying them to a large document collection and comparing the results to a manually produced gold standard.",6 Conclusion,[0],[0]
"An online argument search engine implementing our approach is now available for noncommercial use at https://www.argumentsearch.com/. Furthermore, we are experimenting with language adaptation and plan to extend the tool to the German language.",6 Conclusion,[0],[0]
Preliminary results are presented in Stahlhut (2018).,6 Conclusion,[0],[0]
We also intend to investigate methods for grouping similar arguments.,6 Conclusion,[0],[0]
This work has been supported by the German Federal Ministry of Education and Research (BMBF) under the promotional reference 03VP02540,Acknowledgements,[0],[0]
"(Ar-
gumenText) and the DFG-funded research training group “Adaptive Preparation of Information form Heterogeneous Sources” (AIPHES, GRK 1994/1).",Acknowledgements,[0],[0]
Argument mining is a core technology for automating argument search in large document collections.,abstractText,[0],[0]
"Despite its usefulness for this task, most current approaches are designed for use onlywith specific text types and fall short when applied to heterogeneous texts.",abstractText,[0],[0]
"In this paper, we propose a new sentential annotation scheme that is reliably applicable by crowd workers to arbitrary Web texts.",abstractText,[0],[0]
"We source annotations for over 25,000 instances covering eight controversial topics.",abstractText,[0],[0]
We show that integrating topic information into bidirectional long short-termmemory networks outperforms vanilla BiLSTMs by more than 3 percentage points in F1 in twoand three-label cross-topic settings.,abstractText,[0],[0]
We also show that these results can be further improved by leveraging additional data for topic relevance using multi-task learning.,abstractText,[0],[0]
Cross-topic Argument Mining from Heterogeneous Sources,title,[0],[0]
"Crowdsourcing is an omnipresent phenomenon: it has emerged as an integral part of the machine learning pipeline in recent years, and one reason for the great advances in deep learning is the presence of large data sets that have been labeled by the crowd (e.g., Deng et al., 2009; Krizhevsky, 2009).",1. Introduction,[0],[0]
"Crowdsourcing is also at the heart of peer grading systems (e.g., Alfaro & Shavlovsky, 2014), which help with rising enrollment at universities, and online rating systems (e.g., Liao et al., 2014), which many of us rely on when choosing the next restaurant, to provide just a few examples.
",1. Introduction,[0],[0]
A crowdsourcing scenario consists of a set of workers and a set of tasks that need to be solved.,1. Introduction,[0],[0]
A data curator utilizing crowdsourcing can aim at estimating various quantities of interest.,1. Introduction,[0],[0]
The first goal might be to estimate the true labels or answers for the tasks at hand.,1. Introduction,[0],[0]
"Typically, additional constraints are involved here such as a worker not being willing
1Department of Computer Science, Rutgers University, Piscataway Township, New Jersey, USA.",1. Introduction,[0],[0]
Correspondence to: Matthäus,1. Introduction,[0],[0]
"Kleindessner <matthaeus.kleindessner@rutgers.edu>, Pranjal Awasthi <pranjal.awasthi@rutgers.edu>.
Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
to solve too many tasks and the data curator wanting to get high-quality labels at a low price.",1. Introduction,[0],[0]
The canonical example of this case is the Amazon Mechanical TurkTM.,1. Introduction,[0],[0]
There one cannot track specific workers as they are fleeting.,1. Introduction,[0],[0]
"However, in scenarios such as peer grading or online rating systems, a second goal might be to estimate worker qualities, especially if workers can be reused at a later time.
",1. Introduction,[0],[0]
"In a seminal paper, Dawid & Skene (1979) proposed a formal model that involves worker quality parameters for crowdsourcing scenarios in the context of classification.",1. Introduction,[0],[0]
"The Dawid-Skene model has become a standard theoretical framework and has led to a flurry of research over the past few years (Liu et al., 2012; Raykar & Yu, 2012; Li et al., 2013; Gao et al., 2016; Zhang et al., 2016; Khetan et al., 2017), in particular in its special symmetric form usually referred to as one-coin model (Ghosh et al., 2011; Karger et al., 2011a;b; Dalvi et al., 2013; Gao & Zhou, 2013; Karger et al., 2014; Bonald & Combes, 2017; Ma et al., 2017).",1. Introduction,[0],[0]
"In its general form for binary classification problems, the DawidSkene model assumes that for each worker, the probability of providing the wrong label only depends on the true label of the task, but not on the task itself.",1. Introduction,[0],[0]
"Moreover, given the true label, the responses provided by different workers are independent.",1. Introduction,[0],[0]
"The one-coin model additionally assumes that for each worker, the probability of providing the wrong label is the same for both classes.",1. Introduction,[0],[0]
We will formally introduce the one-coin model in Section 2.,1. Introduction,[0],[0]
"A discussion of prior work work is provided in Section 5 and Appendix A.
The crucial limitation of the Dawid-Skene and one-coin model is the assumption that workers’ error probabilities are task-independent.",1. Introduction,[0],[0]
"In particular, this excludes the possibility of colluding adversaries (other than those that provide the wrong label all of the time), which might make these models a poor approximation of the real world encountered in such applications as peer grading or online rating.",1. Introduction,[0],[0]
"In this paper, we study a significant extension of the one-coin model that allows for arbitrary, highly colluding adversaries.",1. Introduction,[0],[0]
We provide an algorithm for estimating the workers’ error probabilities and prove that it asymptotically recovers the true error probabilities.,1. Introduction,[0],[0]
"Using our estimates of the error probabilities in weighted majority votes, we also provide strategies to estimate ground-truth labels of the tasks.",1. Introduction,[0],[0]
Experiments on both synthetic and real data show that our approach clearly outperforms existing methods in the presence of adversaries.,1. Introduction,[0],[0]
"We first describe a general model for crowdsourcing with non-adaptive workers and binary classification tasks: there are n workers w1, . . .",2. Setup and problem formulation,[0],[0]
", wn and an i.i.d. sample of m tasklabel pairs ((xi, yi))mi=1 ∼",2. Setup and problem formulation,[0],[0]
"Dm, where D is a joint probability distribution over tasks x ∈ X and corresponding labels y ∈ {−1,+1}.",2. Setup and problem formulation,[0],[0]
"There is a variable gij ∈ {0, 1},",2. Setup and problem formulation,[0],[0]
i ∈,2. Setup and problem formulation,[0],[0]
"[m], j ∈",2. Setup and problem formulation,[0],[0]
"[n], indicating whether worker wj is presented with task xi (for k ∈ N, we use [k] to denote the set {1, . . .",2. Setup and problem formulation,[0],[0]
", k}).",2. Setup and problem formulation,[0],[0]
"If wj is presented with xi, that is gij = 1, wj provides an estimate wj(xi) ∈ {−1,+1} of the ground-truth label yi.",2. Setup and problem formulation,[0],[0]
"Let A ∈ {−1, 0,+1}m×n be a matrix that stores all the responses collected from the workers: Aij = wj(xi) if gij = 1 and Aij = 0",2. Setup and problem formulation,[0],[0]
"if gij = 0.
",2. Setup and problem formulation,[0],[0]
We assume that each worker wj follows some (probabilistic or deterministic) strategy such that wj(xi) only depends on xi.,2. Setup and problem formulation,[0],[0]
"In particular, given xi, any two different workers’ responses wj(xi) and wk(xi) and the ground-truth label yi are independent.",2. Setup and problem formulation,[0],[0]
"Let εwj (x, y) ∈",2. Setup and problem formulation,[0],[0]
"[0, 1] be the conditional error probability that, given x and y, wj(x) does not equal y, that is
εwj (x, y)",2. Setup and problem formulation,[0],[0]
":= Prwj |(x,y)[wj(x) 6=",2. Setup and problem formulation,[0],[0]
"y | (x, y)].",2. Setup and problem formulation,[0],[0]
"(1)
Note that the unconditional probability of wj(x) being incorrect, before seeing x and y, is given by
Pr(x,y)∼D,wj [wj(x) 6=",2. Setup and problem formulation,[0],[0]
"y] = E(x,y)∼D[εwj (x, y)]",2. Setup and problem formulation,[0],[0]
"=: εwj .
",2. Setup and problem formulation,[0],[0]
"Now one may study the following questions:
(i)",2. Setup and problem formulation,[0],[0]
"Given only the matrix A, how can we estimate the ground-truth labels y1, . . .",2. Setup and problem formulation,[0],[0]
", ym?
(ii)",2. Setup and problem formulation,[0],[0]
"Given only the matrix A, how can we estimate the workers’ unconditional error probabilities εw1 , . . .",2. Setup and problem formulation,[0],[0]
", εwn?
(iii)",2. Setup and problem formulation,[0],[0]
"If we can choose gij (either in advance of collecting workers’ responses or adaptively while doing so), how should we choose it such that we can achieve (i) or (ii) with a minimum number of collected responses?
",2. Setup and problem formulation,[0],[0]
"In case of εwj (x, y) as defined in (1) being constant on X × {−1,+1}, that is εwj (x, y) ≡ εwj , for all j ∈",2. Setup and problem formulation,[0],[0]
"[n], our model boils down to what is usually referred to as the one-coin model (e.g., Szepesvari, 2015), for which (i) to (iii) have been studied extensively (see Section 5 and Appendix A for references and a detailed discussion).",2. Setup and problem formulation,[0],[0]
With this paper we initiate the study of a significant extension of the one-coin model.,2. Setup and problem formulation,[0],[0]
"We will allow almost half of the workers to deviate from the one-coin model and for such a worker wj , the conditional error probability εwj (x, y) to be a completely arbitrary random variable.",2. Setup and problem formulation,[0],[0]
"In other words, we will allow for arbitrary adversaries, for which not only error
probabilities can be high, but for which error probabilities can be arbitrarily correlated.",2. Setup and problem formulation,[0],[0]
We mainly study (ii) in this scenario.,2. Setup and problem formulation,[0],[0]
We then make use of existing results for the onecoin model to answer (i) satisfactorily for our purposes.,2. Setup and problem formulation,[0],[0]
"We do not deal with (iii), but instead assume that gij has been specified in advance.",2. Setup and problem formulation,[0],[0]
In this section we want to present the general outline of our approach.,3. General outline of our approach,[0],[0]
"A key insight is that the unconditional probability of workers wj and wk being agreeing is given by
Pr(x,y)∼D,wj ,wk [wj(x) = wk(x)]",3. General outline of our approach,[0],[0]
"= 1− εwj − εwk+ 2εwjεwk + 2 Cov(x,y)∼D[εwj (x, y), εwk(x, y)].
(2)
Cov(x,y)∼D[εwj",3. General outline of our approach,[0],[0]
"(x, y), εwk(x, y)] denotes the covariance between random variables εwj",3. General outline of our approach,[0],[0]
"(x, y) and εwk(x, y), that is
Cov(x,y)∼D[εwj",3. General outline of our approach,[0],[0]
"(x, y), εwk(x, y)]",3. General outline of our approach,[0],[0]
"= E(x,y)∼D[(εwj (x, y)− εwj ) · (εwk(x, y)− εwk)].
",3. General outline of our approach,[0],[0]
"A proof of (2) can be found in Appendix B. The probability on the left-hand side of (2) can be easily estimated from A by the ratio of the number of tasks that wj and wk agreed on to the number of tasks they were both presented with:
Pr[wj(x) = wk(x)]",3. General outline of our approach,[0],[0]
≈ ∑m i=1,3. General outline of our approach,[0],[0]
"gijgik1{Aij = Aik}∑m
i=1",3. General outline of our approach,[0],[0]
"gijgik =: pjk.
(3)
",3. General outline of our approach,[0],[0]
"This suggests to solve the system of equations
1− εj",3. General outline of our approach,[0],[0]
"− εk + 2εjεk + 2cjk = pjk, 1 ≤ j < k ≤ n, (4)
in the unknowns εl, l ∈",3. General outline of our approach,[0],[0]
"[n], and cjk, 1 ≤ j",3. General outline of our approach,[0],[0]
"< k ≤ n, in order to obtain estimates of the workers’ unconditional error probabilities εw1 , . . .",3. General outline of our approach,[0],[0]
", εwn .",3. General outline of our approach,[0],[0]
"However, there is a catch: in general, the system (4) is not identifiable and has several solutions.",3. General outline of our approach,[0],[0]
We will assume that at least n2 + 2 of the workers follow the one-coin model and have error probabilities smaller than one half.,3. General outline of our approach,[0],[0]
"A worker wj following the one-coin model implies
Cov(x,y)∼D[εwj",3. General outline of our approach,[0],[0]
"(x, y), εwk(x, y)] = 0, ∀k 6=",3. General outline of our approach,[0],[0]
"j, (5)
and hence under this assumption we can restrict the search for solutions of (4) to εl, l ∈",3. General outline of our approach,[0],[0]
"[n], and cjk, 1 ≤ j",3. General outline of our approach,[0],[0]
"< k ≤ n, with the property that1
∃L ⊆",3. General outline of our approach,[0],[0]
[n] with |L| ≥ n/2 + 2 such that ∀j ∈ L : (εj < 1/2,3. General outline of our approach,[0],[0]
∧,3. General outline of our approach,[0],[0]
[∀k 6= j : cjk = 0]) .,3. General outline of our approach,[0],[0]
"(6)
1Throughout the paper, we set cjk = ckj if",3. General outline of our approach,[0],[0]
j > k.,3. General outline of our approach,[0],[0]
"We also assume pjk = pkj .
",3. General outline of our approach,[0],[0]
"Note that we never assume to know which workers follow the one-coin model, which corresponds to using the existential quantifier for the set L in (6) rather than considering a “fixed” L. We can show that the system (4) has at most one solution with property (6).",3. General outline of our approach,[0],[0]
We also provide evidence that our assumption of n2 + 2 of the workers following the one-coin model and having error probabilities smaller than one half is a necessary condition for guaranteeing the identifiability of system (4).,3. General outline of our approach,[0],[0]
"If the workers satisfy our assumption and pjk on the right-hand side of (4) are actually true agreement probabilities, then εl = εwl and cjk = Cov[εwj",3. General outline of our approach,[0],[0]
"(x, y), εwk(x, y)] is the unique solution of (4) that satisfies (6).",3. General outline of our approach,[0],[0]
"But if pjk are not exactly true agreement probabilities, there might be no solution of (4) with property (6) at all.",3. General outline of our approach,[0],[0]
"We prove that if estimates pjk are not too bad, we can solve (4) together with (6) approximately, and our approximate solution is guaranteed to be close to true error probabilities εw1 , . . .",3. General outline of our approach,[0],[0]
", εwn and covariances Cov[εwj",3. General outline of our approach,[0],[0]
"(x, y), εwk(x, y)], j < k.",3. General outline of our approach,[0],[0]
This answers (ii) from Section 2 and is the main contribution of our paper: Main result.,3. General outline of our approach,[0],[0]
Assume that at least n2 + 2 of the workers follow the one-coin model and have error probabilities not greater than γTR < 12 .,3. General outline of our approach,[0],[0]
If |Pr[wj(x) = wk(x)],3. General outline of our approach,[0],[0]
"− pjk| ≤ β for all j 6= k and β sufficiently small, we can compute estimates ε̂w1 , . . .",3. General outline of our approach,[0],[0]
", ε̂wn of εw1 , . .",3. General outline of our approach,[0],[0]
.,3. General outline of our approach,[0],[0]
", εwn such that
|εwi",3. General outline of our approach,[0],[0]
"− ε̂wi | ≤ C(γTR) · β1/4.
",3. General outline of our approach,[0],[0]
"We answer (i) from Section 2 and provide two ways to predict ground-truth labels y1, . . .",3. General outline of our approach,[0],[0]
", ym by taking weighted majority votes over the responses provided by the workers.",3. General outline of our approach,[0],[0]
"In these majority votes, the weights depend on our estimates of true error probabilities εw1 , . . .",3. General outline of our approach,[0],[0]
", εwn .",3. General outline of our approach,[0],[0]
"If gij has been specified in advance, we have the following guarantee on the quality of the estimates pjk (see (3)):",4.1. Estimating agreement probabilities,[0],[0]
Lemma 1.,4.1. Estimating agreement probabilities,[0],[0]
Assume ∑m i=1,4.1. Estimating agreement probabilities,[0],[0]
"gijgik > 0, j 6= k.",4.1. Estimating agreement probabilities,[0],[0]
Let δ > 0,4.1. Estimating agreement probabilities,[0],[0]
"and
βjk",4.1. Estimating agreement probabilities,[0],[0]
"= min
{ 1, [ ln(2n2/δ)/ ( 2 ∑m
i=1",4.1. Estimating agreement probabilities,[0],[0]
"gijgik
)]1/2} .
",4.1. Estimating agreement probabilities,[0],[0]
"Then we have with probability at least 1− δ over the sample ((xi, yi))",4.1. Estimating agreement probabilities,[0],[0]
m i=1,4.1. Estimating agreement probabilities,[0],[0]
"and the randomness in workers’ strategies that
|Pr[wj(x) = wk(x)]− pjk| ≤ βjk, 1 ≤ j < k ≤",4.1. Estimating agreement probabilities,[0],[0]
"n.
Proof.",4.1. Estimating agreement probabilities,[0],[0]
A straightforward application of Hoeffding’s inequality and the union bound yields the result.,4.1. Estimating agreement probabilities,[0],[0]
"If all workers follow the one-coin model, that is εwj (x, y) ≡",4.2. Identifiability and approximate solution,[0],[0]
εwj for all j ∈,4.2. Identifiability and approximate solution,[0],[0]
"[n], we have
Cov(x,y)∼D[εwj",4.2. Identifiability and approximate solution,[0],[0]
"(x, y), εwk(x, y)]",4.2. Identifiability and approximate solution,[0],[0]
"= 0, 1 ≤ j < k ≤ n, and system (4) reduces to
1− εj − εk + 2εjεk = pjk, 1 ≤ j < k ≤ n, (7)
in the unknowns εl, l ∈",4.2. Identifiability and approximate solution,[0],[0]
[n].,4.2. Identifiability and approximate solution,[0],[0]
"It is well known that, in general, even (7) is not identifiable.",4.2. Identifiability and approximate solution,[0],[0]
"For example, if pjk = 1 for all 1 ≤ j < k ≤ n, there are the two solutions εl = 0, l ∈",4.2. Identifiability and approximate solution,[0],[0]
"[n], and εl = 1, l ∈",4.2. Identifiability and approximate solution,[0],[0]
"[n], corresponding to either all perfect or all completely erroneous workers.",4.2. Identifiability and approximate solution,[0],[0]
"On the other hand, the system (7) is identifiable if we assume that on average workers are better than random guessing, that is 1 n ∑n j=1 εwj < 1 2 , and there are at least three informative workers with εwj",4.2. Identifiability and approximate solution,[0],[0]
6= 12,4.2. Identifiability and approximate solution,[0],[0]
(,4.2. Identifiability and approximate solution,[0],[0]
"Bonald & Combes, 2017).
",4.2. Identifiability and approximate solution,[0],[0]
"Clearly, these two conditions do not guarantee identifiability of the general system (4).",4.2. Identifiability and approximate solution,[0],[0]
"The next lemma shows that even if we additionally assume half of the workers to follow the one-coin model, the system (4) is not identifiable.",4.2. Identifiability and approximate solution,[0],[0]
Here we only state an informal version of the lemma.,4.2. Identifiability and approximate solution,[0],[0]
"A detailed version and its proof can be found in Appendix B.
Lemma 2.",4.2. Identifiability and approximate solution,[0],[0]
"There exists an instance of the system (4), where n is even, that has two different solutions.",4.2. Identifiability and approximate solution,[0],[0]
"In both solutions, it holds that εl < 12 , l ∈",4.2. Identifiability and approximate solution,[0],[0]
[n].,4.2. Identifiability and approximate solution,[0],[0]
"Furthermore:
(a) In the first solution, cjk = 0 for all j ∈",4.2. Identifiability and approximate solution,[0],[0]
"[n2 ] and k 6= j, and εl is small",4.2. Identifiability and approximate solution,[0],[0]
if l ∈,4.2. Identifiability and approximate solution,[0],[0]
[n2 ] and,4.2. Identifiability and approximate solution,[0],[0]
big if l ∈,4.2. Identifiability and approximate solution,[0],[0]
[n] \,4.2. Identifiability and approximate solution,[0],[0]
[ n 2 ].,4.2. Identifiability and approximate solution,[0],[0]
"(b) In the second solution, cjk = 0 for all j ∈",4.2. Identifiability and approximate solution,[0],[0]
"[n]\ [n2 ] and k 6= j, and εl is small if l ∈",4.2. Identifiability and approximate solution,[0],[0]
[n] \,4.2. Identifiability and approximate solution,[0],[0]
"[n2 ] and big if l ∈ [ n 2 ].
We want to mention that a solution of (4) does not necessarily correspond to actual workers, that is given εl, l ∈",4.2. Identifiability and approximate solution,[0],[0]
"[n], and cjk, 1 ≤ j",4.2. Identifiability and approximate solution,[0],[0]
"< k ≤ n, there might be no collection of workers w1, . . .",4.2. Identifiability and approximate solution,[0],[0]
", wn such that εwl = εl and Cov[εwj",4.2. Identifiability and approximate solution,[0],[0]
"(x, y), εwk(x, y)] = cjk.",4.2. Identifiability and approximate solution,[0],[0]
"By the BhatiaDavis inequality (Bhatia & Davis, 2010) it holds that Var[εwj (x, y)] ≤ εwj − ε 2wj .",4.2. Identifiability and approximate solution,[0],[0]
"Hence, a necessary condition for a solution to correspond to actual workers is that |cjk| ≤ (εj−ε 2j )1/2(εk−ε 2k )1/2",4.2. Identifiability and approximate solution,[0],[0]
(in addition to εl ∈,4.2. Identifiability and approximate solution,[0],[0]
"[0, 1]).",4.2. Identifiability and approximate solution,[0],[0]
"The two solutions in Lemma 2 correspond to actual workers.
",4.2. Identifiability and approximate solution,[0],[0]
"From now on we assume that at least n2 + 2 workers follow the one-coin model and have error probabilities smaller than one half:2
Assumption A.",4.2. Identifiability and approximate solution,[0],[0]
There exists L ⊆,4.2. Identifiability and approximate solution,[0],[0]
"[n] with |L| ≥ n/2 + 2 such that for all j ∈ L, the worker wj follows the one-coin model with error probability εwj < 1/2.
",4.2. Identifiability and approximate solution,[0],[0]
This corresponds to considering (4) together with the constraint (6).,4.2. Identifiability and approximate solution,[0],[0]
"The system (4) together with (6) is identifiable:
Proposition 1.",4.2. Identifiability and approximate solution,[0],[0]
"There exists at most one solution of system (4) that has property (6).
",4.2. Identifiability and approximate solution,[0],[0]
"2All results of Section 4.2 hold true if we assume, more generally, the existence of L ⊆",4.2. Identifiability and approximate solution,[0],[0]
"[n] with |L| ≥ n
2 + 2 such that (5)
together with εwj < 1 2 holds for all j ∈ L.
Proof.",4.2. Identifiability and approximate solution,[0],[0]
"Assuming there are two solutions (εS1l )l∈[n], (c S1jk )",4.2. Identifiability and approximate solution,[0],[0]
"1≤j<k≤n and (ε S2 l )l∈[n], (c S2 jk )1≤j<k≤n with L1 and L2 satisfying (6), there have to be pairwise different i1, i2, i3 ∈ L1 ∩ L2.",4.2. Identifiability and approximate solution,[0],[0]
"It is easy to see that (εS1i1 , ε S1 i2 , εS1i3 ) and (εS2i1 , ε S2 i2 , εS2i3 ) and consequently also all the other components of the two solutions have to coincide.",4.2. Identifiability and approximate solution,[0],[0]
"Details can be found in Appendix B.
If pjk at the right-hand side of (4) are true agreement probabilities, the true error probabilities εw1 , . . .",4.2. Identifiability and approximate solution,[0],[0]
", εwn and covariances Cov[εwj",4.2. Identifiability and approximate solution,[0],[0]
"(x, y), εwk(x, y)], j < k, make up the unique solution of (4) that satisfies (6), but if pjk are not exactly true agreement probabilities, there might be no solution of (4) that satisfies (6) at all.",4.2. Identifiability and approximate solution,[0],[0]
"Our goal is then to find a solution of (4) that satisfies (6) approximately and to show that our approximate solution has to be close to εw1 , . . .",4.2. Identifiability and approximate solution,[0],[0]
", εwn and Cov[εwj",4.2. Identifiability and approximate solution,[0],[0]
"(x, y), εwk(x, y)], j < k.",4.2. Identifiability and approximate solution,[0],[0]
"As a first step towards this goal we need a generalization of Proposition 1:
Proposition 2.",4.2. Identifiability and approximate solution,[0],[0]
Let γ < 1/2 and ν < 1/8− γ/2 +,4.2. Identifiability and approximate solution,[0],[0]
γ2/2.,4.2. Identifiability and approximate solution,[0],[0]
"If there exist two solutions (εSil )l∈[n], (c Si jk )1≤j<k≤n, i ∈ {1, 2}, of system (4) (where pjk ∈",4.2. Identifiability and approximate solution,[0],[0]
"[0, 1]) with the property that εSil ∈",4.2. Identifiability and approximate solution,[0],[0]
"[0, 1], l ∈",4.2. Identifiability and approximate solution,[0],[0]
"[n], and
∃Li ⊆",4.2. Identifiability and approximate solution,[0],[0]
[n] with |Li| ≥ n/2 + 2 such that ∀j ∈ Li : ( εSij ≤,4.2. Identifiability and approximate solution,[0],[0]
γ,4.2. Identifiability and approximate solution,[0],[0]
∧,4.2. Identifiability and approximate solution,[0],[0]
[ ∀k 6=,4.2. Identifiability and approximate solution,[0],[0]
"j : |c Sijk | ≤ ν ]) , (8)
then∣∣εS1l − εS2l ∣∣ ≤ G(γ, ν)√ν, ∣∣c S1jk − c S2jk ∣∣ ≤ 3G(γ, ν)√ν for l ∈",4.2. Identifiability and approximate solution,[0],[0]
"[n], j < k, where G(γ, ν)→ G(γ)",4.2. Identifiability and approximate solution,[0],[0]
> 0,4.2. Identifiability and approximate solution,[0],[0]
"as ν → 0.
",4.2. Identifiability and approximate solution,[0],[0]
"The proof of Proposition 2, which provides an explicit expression for G(γ, ν), can be found in Appendix B.
In a next step, we assume that we are given pairwise different i1, i2, i3 ∈",4.2. Identifiability and approximate solution,[0],[0]
"[n] such that wi1 , wi2 , wi3 follow the onecoin model with εwi1 , εwi2 , εwi3 < 1/2.",4.2. Identifiability and approximate solution,[0],[0]
"In this case, assuming that estimates pjk are close to true agreement probabilities, we can construct a solution of (4) that is guaranteed to be close to the true error probabilities and covariances (and hence approximately satisfies (6)).",4.2. Identifiability and approximate solution,[0],[0]
"This is made precise in the next lemma (its proof can be found in Appendix B).
",4.2. Identifiability and approximate solution,[0],[0]
Lemma 3.,4.2. Identifiability and approximate solution,[0],[0]
Let γTR < 1/2 and consider the system (4) with p TRjk ∈,4.2. Identifiability and approximate solution,[0],[0]
"[0, 1] as right-hand side.",4.2. Identifiability and approximate solution,[0],[0]
"Assume there exists a solution3 (εTRl )l∈[n], (c TR jk )1≤j<k≤n with the property that",4.2. Identifiability and approximate solution,[0],[0]
εTRl ∈,4.2. Identifiability and approximate solution,[0],[0]
"[0, 1] and
∃LTR ⊆",4.2. Identifiability and approximate solution,[0],[0]
[n] with |LTR| ≥ n/2 + 2 such that ∀j ∈ LTR : ( εTRj ≤ γTR,4.2. Identifiability and approximate solution,[0],[0]
∧,4.2. Identifiability and approximate solution,[0],[0]
[ ∀k 6=,4.2. Identifiability and approximate solution,[0],[0]
j :,4.2. Identifiability and approximate solution,[0],[0]
c TRjk = 0 ]) .,4.2. Identifiability and approximate solution,[0],[0]
"(9)
Now consider the system (4) with pjk ∈",4.2. Identifiability and approximate solution,[0],[0]
"[0, 1] as right-hand side.",4.2. Identifiability and approximate solution,[0],[0]
"Assume that |p TRjk − pjk| ≤ β for all j 6= k, where
3By Proposition 1, this solution is unique.
",4.2. Identifiability and approximate solution,[0],[0]
β satisfies β < 1/2− 2γTR + 2γ2TR.,4.2. Identifiability and approximate solution,[0],[0]
"Let i1, i2, i3 ∈",4.2. Identifiability and approximate solution,[0],[0]
"[n] be pairwise different and set
B := −2",4.2. Identifiability and approximate solution,[0],[0]
"+ 4pi1i3 , C := 1 + 2pi1i2pi2i3",4.2. Identifiability and approximate solution,[0],[0]
− pi1i2,4.2. Identifiability and approximate solution,[0],[0]
"− pi1i3 − pi2i3 ,
εRi2 := 1 2 − √ B + 4C 2 √ B , εSi2 := min(γTR,max(0, ε R i2))
(10)
and for all l 6= i2 and for all 1 ≤ j < k ≤ n
εRl := pi2l − 1 + εSi2
2εSi2 − 1 ,
εSl :=
{ min(γTR,max(0, ε R l ))",4.2. Identifiability and approximate solution,[0],[0]
"if l ∈ {i1, i3}
min(1,max(0, εRl ))",4.2. Identifiability and approximate solution,[0],[0]
"if l /∈ {i1, i3} ,
c Sjk := pjk − (1− εSj − εSk + 2εSj εSk )
2 .
(11)
",4.2. Identifiability and approximate solution,[0],[0]
"If all expressions are defined (i.e., B > 0, B + 4C ≥ 0 and εSi2 6= 1 2 ), then (ε S l )l∈[n], (c S jk)1≤j<k≤n is a solution of (4) with pjk as right-hand side.",4.2. Identifiability and approximate solution,[0],[0]
"If i1, i2, i3 ∈ LTR, then all expressions are defined and∣∣εTRl",4.2. Identifiability and approximate solution,[0],[0]
"− εSl ∣∣ ≤ H(γTR, β)√β, l ∈",4.2. Identifiability and approximate solution,[0],[0]
"[n],∣∣c TRjk − c Sjk∣∣ ≤",4.2. Identifiability and approximate solution,[0],[0]
"3H(γTR, β)√β + β/2, j < k, (12) where H(γTR, β)→ H(γTR)",4.2. Identifiability and approximate solution,[0],[0]
> 0,4.2. Identifiability and approximate solution,[0],[0]
"as β → 0.
",4.2. Identifiability and approximate solution,[0],[0]
"In Lemma 3, for constructing the solution (εSl )l∈[n], (c Sjk)1≤j<k≤n as defined in (10) and (11) we need to know γTR < 1/2, which is an upper bound on the error probabilities of at least n2 + 2 workers that follow the one-coin model.",4.2. Identifiability and approximate solution,[0],[0]
"In practice, we might choose γTR depending on the difficulty of the tasks or simply set it conservatively, for example as γTR = 0.45.",4.2. Identifiability and approximate solution,[0],[0]
"If i1, i2, i3 ∈ LTR, then (12) implies that (εSl )l∈[n], (c S jk)1≤j<k≤n satisfies (8) with
γ = γTR +H(γTR, β) √ β, ν = 3H(γTR, β) √ β + β/2.
",4.2. Identifiability and approximate solution,[0],[0]
"(13)
If we know the value of β (using Lemma 1, we easily obtain an upper bound β that holds with high probability), we can compute these quantities.",4.2. Identifiability and approximate solution,[0],[0]
"This suggests the following strategy for obtaining estimates of εw1 , . . .",4.2. Identifiability and approximate solution,[0],[0]
", εwn and Cov[εwj",4.2. Identifiability and approximate solution,[0],[0]
"(x, y), εwk(x, y)], j < k: we sample pairwise different i1, i2, i3 ∈",4.2. Identifiability and approximate solution,[0],[0]
"[n] uniformly at random and construct (εSl )l∈[n], (c S jk)1≤j<k≤n as defined in (10) and (11).",4.2. Identifiability and approximate solution,[0],[0]
"If one of the expressions is not defined, we can immediately discard (i1, i2, i3).",4.2. Identifiability and approximate solution,[0],[0]
"Otherwise, we check whether (εSl )l∈[n], (c Sjk)1≤j<k≤n satisfies (8) with γ and ν as specified in (13).",4.2. Identifiability and approximate solution,[0],[0]
"If it does, since (εTRl )l∈[n], (c TR jk + (pjk− p TRjk )/2)1≤j<k≤n is a solution of (4) with pjk as right-hand side that satisfies
property (8) too, Proposition 2 guarantees that ∣∣εTRl",4.2. Identifiability and approximate solution,[0],[0]
"− εSl ∣∣ ≤√3H(γTR, β)√β",4.2. Identifiability and approximate solution,[0],[0]
"+ β2 · G ( γTR +H(γTR, β)",4.2. Identifiability and approximate solution,[0],[0]
"√ β, 3H(γTR, β)",4.2. Identifiability and approximate solution,[0],[0]
"√ β + β
2
) ∼ β1/4
(14)
for all l ∈",4.2. Identifiability and approximate solution,[0],[0]
"[n] and a similar bound on |c TRjk − c Sjk|, j < k.",4.2. Identifiability and approximate solution,[0],[0]
"If (εSl )l∈[n], (c S jk)1≤j<k≤n does not satisfy (8), we discard (i1, i2, i3) and start anew.",4.2. Identifiability and approximate solution,[0],[0]
"Note that under our Assumption A, the probability of choosing i1, i2, i3 such that i1, i2, i3 ∈ LTR is greater than 1/8.",4.2. Identifiability and approximate solution,[0],[0]
"In expectation we have to discard (i1, i2, i3) for not more than eight times before finding a solution that satisfies (8) and hence (14).
",4.2. Identifiability and approximate solution,[0],[0]
"Assuming that every worker is presented with every task, that is gij",4.2. Identifiability and approximate solution,[0],[0]
= 1,4.2. Identifiability and approximate solution,[0],[0]
for all i ∈,4.2. Identifiability and approximate solution,[0],[0]
[m] and j ∈,4.2. Identifiability and approximate solution,[0],[0]
"[n], it follows from Lemma 1 and (14) that m has to scale as ln(n2/δ)/ρ8 in order that the described strategy is guaranteed to yield, with probability at least 1 − δ, estimates εS1 , . . .",4.2. Identifiability and approximate solution,[0],[0]
", εSn satisfying |εTRl − εSl
∣∣ ≤ ρ,",4.2. Identifiability and approximate solution,[0],[0]
l ∈,4.2. Identifiability and approximate solution,[0],[0]
[n].,4.2. Identifiability and approximate solution,[0],[0]
"This is significantly larger than the rate m ∼ ln(n2/δ)/ρ2 required by the TE algorithm, which solves the estimation problem for the error probabilities in the one-coin model and is claimed to be minimax optimal (Bonald & Combes, 2017).",4.2. Identifiability and approximate solution,[0],[0]
"We suspect that our rate with its dependence on ρ−8 is not optimal and consider it to be an interesting follow-up question to study the minimax rate for our extension of the one-coin model.
",4.2. Identifiability and approximate solution,[0],[0]
"Although the convergence rate that we can guarantee for the described strategy is slow, we might still hope that the strategy performs better in practice.",4.2. Identifiability and approximate solution,[0],[0]
"However, there is an issue that we have to overcome.",4.2. Identifiability and approximate solution,[0],[0]
"Unless β is very small, γ and ν as specified in (13) are too big for being meaningful, that is any solution (εSl )l∈[n], (c S jk)1≤j<k≤n as defined in (10) and (11) will satisfy (8) with these values.",4.2. Identifiability and approximate solution,[0],[0]
"We will not discard any (i1, i2, i3), regardless of whether i1, i2, i3 ∈ LTR holds or not.",4.2. Identifiability and approximate solution,[0],[0]
"We deal with this issue by adapting the strategy as follows: let P ⊆ {(i1, i2, i3) : i1, i2, i3 ∈",4.2. Identifiability and approximate solution,[0],[0]
[n] pairwise different}.,4.2. Identifiability and approximate solution,[0],[0]
"For every p = (i1, i2, i3) ∈ P , we construct (εSl (p))l∈[n], (c Sjk(p))1≤j<k≤n as defined in (10) and (11).",4.2. Identifiability and approximate solution,[0],[0]
We set Qp =,4.2. Identifiability and approximate solution,[0],[0]
"[n] unless γ as specified in (13) is smaller than one, in which case we set Qp = {l ∈",4.2. Identifiability and approximate solution,[0],[0]
"[n] : εSl (p) ≤ γ} and discard any solution (εSl (p))l∈[n], (c S jk(p))1≤j<k≤n for which |Qp| < n2 + 2.",4.2. Identifiability and approximate solution,[0],[0]
Let ν,4.2. Identifiability and approximate solution,[0],[0]
p be the dn2 + 2e-th smallest element of {maxk∈[n]\{l} |c Slk (p)| : l ∈ Qp}.,4.2. Identifiability and approximate solution,[0],[0]
"Then we finally return the solution (εSl (p0))l∈[n], (c S jk(p0)))1≤j<k≤n for which νp is smallest, that is p0 = argminp ν",4.2. Identifiability and approximate solution,[0],[0]
"p.
",4.2. Identifiability and approximate solution,[0],[0]
"If γ is small enough, it follows from Proposition 2 that∣∣εTRl",4.2. Identifiability and approximate solution,[0],[0]
"− εSl (p0)∣∣ ≤√max{νp0 , β/2} · G ( γTR +H(γTR, β) √ β,max{νp0 , β/2} ) .",4.2. Identifiability and approximate solution,[0],[0]
"(15)
Note that if P contains at least one triple of indices i1, i2, i3 ∈ LTR, then νp0 ≤ 3H(γTR, β) √ β + β2 , so that the guarantee (15) is at least as good as (14).",4.2. Identifiability and approximate solution,[0],[0]
We also expect νp0 to be smaller the larger P is.,4.2. Identifiability and approximate solution,[0],[0]
"Hence, we should choose P as large as we can afford due to computational reasons, but in practice, there is one more aspect that we have to consider.",4.2. Identifiability and approximate solution,[0],[0]
"Depending on how gij has been chosen, there might be workers wj and wk that were presented with only a few common tasks or no common tasks at all.",4.2. Identifiability and approximate solution,[0],[0]
"In this case, the estimate pjk of the agreement probability between wj and wk is only poor and there is no uniform bound β on |p TRjk −pjk| (where p TRjk are true agreement probabilities).",4.2. Identifiability and approximate solution,[0],[0]
"We can deal with this aspect by choosing P in a way such that for all p ∈ P , all estimates pjk that are involved in the computation of (εSl (p))l∈[n] are somewhat reliable.",4.2. Identifiability and approximate solution,[0],[0]
We present a concrete implementation of this in Algorithm 1 below.,4.2. Identifiability and approximate solution,[0],[0]
"Once we have estimates ε̂w1 , . . .",4.3. Predicting ground-truth labels,[0],[0]
", ε̂wn of the true error probabilities εw1 , . . .",4.3. Predicting ground-truth labels,[0],[0]
", εwn , we predict ground-truth labels yi by taking a weighted majority vote over the responses collected for the task xi.",4.3. Predicting ground-truth labels,[0],[0]
"Our estimate for yi is given by
ŷi = sign {∑n
l=1 f(ε̂wl) ·Ail
} , (16)
where f :",4.3. Predicting ground-truth labels,[0],[0]
"[0, 1]→",4.3. Predicting ground-truth labels,[0],[0]
"[−∞,+∞].",4.3. Predicting ground-truth labels,[0],[0]
Ties are broken uniformly at random.,4.3. Predicting ground-truth labels,[0],[0]
"We consider two choices for the function f .
",4.3. Predicting ground-truth labels,[0],[0]
"It is well-known that if all workers follow the one-coin model with known error probabilities εw1 , . . .",4.3. Predicting ground-truth labels,[0],[0]
", εwn , groundtruth labels are balanced, that is Pr(x,y)∼D[y = +1] = Pr(x,y)∼D[y = −1], and gij are independent Bernoulli random variables with common success probability α > 0, then the optimal estimator for the ground-truth label yi is given by the weighted majority vote (16) with f(ε̂wl) replaced by f(εwl) =",4.3. Predicting ground-truth labels,[0],[0]
ln,4.3. Predicting ground-truth labels,[0],[0]
"((1− εwl)/εwl) (Nitzan & Paroush, 1982; Berend & Kontorovich, 2015; Bonald & Combes, 2017).",4.3. Predicting ground-truth labels,[0],[0]
"Hence, a common approach for the one-coin model is to first estimate the true error probabilities and then to estimate ground-truth labels by using the majority vote (16) with f(ε̂wl) = ln",4.3. Predicting ground-truth labels,[0],[0]
"((1− ε̂wl)/ε̂wl) (Bonald & Combes, 2017; Ma et al., 2017).",4.3. Predicting ground-truth labels,[0],[0]
"We propose to use the same majority vote, but restricted to answers from workers that we believe to follow the one-coin model.",4.3. Predicting ground-truth labels,[0],[0]
"Using the notation from Section 4.2, this means that we set f(ε̂wl) = ln ((1− ε̂wl)/ε̂wl) for l ∈ Qp0 with maxk∈[n]\{l} |c Slk (p0)| ≤ νp0 and f(ε̂wl) = 0 otherwise.
",4.3. Predicting ground-truth labels,[0],[0]
"Alternatively, we suggest to set f(ε̂wl) = 1 − 2ε̂wl for l ∈",4.3. Predicting ground-truth labels,[0],[0]
[n].,4.3. Predicting ground-truth labels,[0],[0]
With this choice of f we make use of the responses provided by all workers.,4.3. Predicting ground-truth labels,[0],[0]
"The same choice has been used for the one-coin model too (Dalvi et al., 2013).",4.3. Predicting ground-truth labels,[0],[0]
A third option would be to set f(ε̂wl) = 1− 2ε̂wl for l ∈ Qp0 with maxk∈[n]\{l} |c Slk (p0)| ≤ νp0 and f(ε̂wl) = 0,4.3. Predicting ground-truth labels,[0],[0]
"otherwise, but we do not consider this choice any further.",4.3. Predicting ground-truth labels,[0],[0]
"In the interests of clarity, we present our approach as self contained Algorithm 1.",4.4. Algorithm,[0],[0]
Choosing P as the set of triples such that involved pairs of workers have been provided with at least ten or three common tasks might seem somewhat arbitrary here.,4.4. Algorithm,[0],[0]
"Indeed, one could introduce two parameters to the algorithm instead.",4.4. Algorithm,[0],[0]
"Without optimizing for these parameters, we chose them as ten and three in all our experiments on real data, and hence we state Algorithm 1 as is.
",4.4. Algorithm,[0],[0]
"Our analysis best applies to the setting of a full matrix A (or variables gij that are independent Bernoulli random variables with common success probability, as it is assumed by Bonald & Combes, 2017, for example).",4.4. Algorithm,[0],[0]
"In this case, which we consider in our experiments on synthetic data, choosing P as stated in Algorithm 1 reduces to choosing P as the set of all triples of pairwise different indices.",4.4. Algorithm,[0],[0]
"If the number of workers n is small, this is the best one can do.",4.4. Algorithm,[0],[0]
"If n is large, it is infeasible to choose P as the set of all triples though since the running time of Algorithm 1 is in O(n2(m + |P",4.4. Algorithm,[0],[0]
|)).,4.4. Algorithm,[0],[0]
"If n is large and A full, one should sample P uniformly at random.",4.4. Algorithm,[0],[0]
For |P | ≥ ln δ/ ln(7/8),4.4. Algorithm,[0],[0]
our error guarantee (14) holds with probability at least 1− δ then (compare with Section 4.2).,4.4. Algorithm,[0],[0]
We briefly survey related work here.,5. Related work,[0],[0]
A complete discussion can be found in Appendix A.,5. Related work,[0],[0]
"As discussed in Sections 1 and 2, in crowdsourcing one might be interested in estimating ground-truth labels and/or worker qualities given the response matrix A, but also in optimal task assignment.",5. Related work,[0],[0]
"In their seminal paper, Dawid & Skene (1979) proposed an EM based algorithm to address the first two goals.",5. Related work,[0],[0]
"Since then numerous works have followed addressing all three goals for the Dawid-Skene and one-coin model (Ghosh et al., 2011; Karger et al., 2011a;b; 2013; 2014; Dalvi et al., 2013; Gao & Zhou, 2013; Gao et al., 2016; Zhang et al., 2016; Bonald & Combes, 2017; Ma et al., 2017).",5. Related work,[0],[0]
"There have also been efforts to study generalizations of the Dawid-Skene model (Jaffe et al., 2016; Khetan & Oh, 2016; Shah et al., 2016) as well as to explicitly deal with adversaries (Raykar & Yu, 2012; Jagabathula et al., 2017).",5. Related work,[0],[0]
"However, none of the prior work can handle a number of arbitrary adversaries almost as large as the number of reliable workers as we do.",5. Related work,[0],[0]
"On both synthetic and real data, we compared our proposed Algorithm 1 to straightforward majority voting for predicting labels (referred to as Maj) and the following methods from the literature: the spectral algorithms by Ghosh et al. (2011) (GKM), Dalvi et al. (2013) (RoE and EoR) and Karger et al. (2013) (KOS), the two-stage procedure by
Algorithm 1",6. Experiments,[0],[0]
"Input: crowdsourced labels stored in A ∈ {−1, 0,+1}m×n, upper bound 0 < γTR < 12 on the error probabilities of dn2 + 2e workers that follow the one-coin model, confidence parameter 0",6. Experiments,[0],[0]
< δ < 1,6. Experiments,[0],[0]
"Output: estimates (εFl )l∈[n], (c Fjk )j<k, (ŷi)i∈[m] of error probabilities, covariances and ground-truth labels
I Estimating agreement probabilities set gij = 1{Aij 6= 0}, i ∈",6. Experiments,[0],[0]
"[m], j ∈",6. Experiments,[0],[0]
[n] set qjk = ∑m i=1,6. Experiments,[0],[0]
"gijgik, j, k ∈",6. Experiments,[0],[0]
"[n] set pjk as in (3), j, k ∈",6. Experiments,[0],[0]
"[n] (pjk = NaN if qjk = 0)
",6. Experiments,[0],[0]
I Estimating error probabilities and covariances set β =,6. Experiments,[0],[0]
"[ ln(2n2/δ)/ ( 2 minj,k∈[n] qjk )]1/2 ∈ (0,+Inf ] set γ as in (13)",6. Experiments,[0],[0]
if γ /∈,6. Experiments,[0],[0]
"[0, 1] then
set γ = 1 end if set P = { (i1, i2, i3) :",6. Experiments,[0],[0]
"i1, i2, i3 ∈",6. Experiments,[0],[0]
"[n] pairwise different
and qjk ≥ 10, j, k ∈ {i1, i2, i3}, and qi2j ≥ 3, j 6= i2 }
set νold = Inf, (εFl )l∈[n] = 0, (c F jk )1≤j<k≤n = 0, L = ∅ for (i1, i2,",6. Experiments,[0],[0]
"i3) ∈ P do if not all expressions in (10) or (11) are defined then
break end if compute (εSl )l∈[n], (c S jk)1≤j<k≤n as in (10) and (11) set Q = {l ∈",6. Experiments,[0],[0]
[n] : εSl ≤ γ} set ν = dn2 +,6. Experiments,[0],[0]
2e-th smallest element of {maxk∈[n]\{l} |c Slk,6. Experiments,[0],[0]
| : l ∈ Q} (ν = NaN ifQ = ∅) if |Q| ≥ n2 + 2 AND ν <,6. Experiments,[0],[0]
νold then set (εFl )l∈[n] =,6. Experiments,[0],[0]
"(ε S l )l∈[n], (c F jk )",6. Experiments,[0],[0]
j,6. Experiments,[0],[0]
"<k = (c S jk)j<k
set L = {l ∈ Q :",6. Experiments,[0],[0]
"maxk∈[n]\{l} |c Slk | ≤ ν} set νold = ν
end if end for
I Estimating ground-truth labels set f(ε̂wl) =",6. Experiments,[0],[0]
ln ((1− ε̂wl)/ε̂wl) ∈,6. Experiments,[0],[0]
"[−Inf,+Inf], l ∈ L,
and f(ε̂wl) = 0, l ∈",6. Experiments,[0],[0]
"[n] \ L (alternatively set f(ε̂wl) = 1− 2ε̂wl , l ∈",6. Experiments,[0],[0]
"[n]) set ŷi as in (16), i ∈",6. Experiments,[0],[0]
"[m]
Zhang et al. (2016) (S-EM1 and S-EM10, where we run one or ten iterations of the EM algorithm) and the recent method by Bonald & Combes (2017) (TE).",6. Experiments,[0],[0]
"We used the Matlab implementation of KOS, S-EM1 and S-EM10 made available by Zhang et al. (2016).",6. Experiments,[0],[0]
"In our implementations of the other methods, we adapted GKM, RoE and EoR as to assume that the average error of the workers is smaller than one half rather than assuming that the error of the first worker is.",6. Experiments,[0],[0]
"We always called Algorithm 1 with parameters γTR = 0.4 and δ = 0.1, which resulted in γ being set to 1
in the execution of the algorithm in all our experiments.",6. Experiments,[0],[0]
We refer to Algorithm 1 with the logarithmic weights in (16) as Alg. 1 and and with the linear weights as Alt-Alg. 1.,6. Experiments,[0],[0]
"In the following, all results are average results obtained from running an experiment for 100 times.",6. Experiments,[0],[0]
"In our first experiment, we consider n = 50 workers and m = 5000 tasks with balanced ground-truth labels.",6.1. Synthetic data,[0],[0]
Every worker is presented with every task.,6.1. Synthetic data,[0],[0]
"For 0 ≤ t ≤ 25, we choose t workers at random.",6.1. Synthetic data,[0],[0]
"These workers are corrupted workers that all provide the same random response to every task, which is incorrect with error probability 0.5.",6.1. Synthetic data,[0],[0]
"The remaining n − t workers provide responses according to the one-coin model, where the error probability of each of these workers is 0.4.",6.1. Synthetic data,[0],[0]
Figure 1 shows the prediction error for estimating ground-truth labels and the estimation error for estimating error probabilities in both the maximum norm and the normalized 1-norm for the various methods as a function of t.,6.1. Synthetic data,[0],[0]
The prediction error is given by 1m ∑m i=1,6.1. Synthetic data,[0],[0]
1{yi 6= ŷi} for ground-truth labels yi and estimates ŷi and the estimation error is given by maxl∈[n] |εwl,6.1. Synthetic data,[0],[0]
− ε̂wl | or 1n,6.1. Synthetic data,[0],[0]
∑n l=1 |εwl − ε̂wl | for true error probabilities εwl and estimates ε̂wl .,6.1. Synthetic data,[0],[0]
"The methods Maj and KOS, by default, do not provide estimates of the workers’ error probabilities.",6.1. Synthetic data,[0],[0]
"We adapt these two methods in order to return estimates of the error probabilities too as follows: if the method returns label estimates ŷ1, . . .",6.1. Synthetic data,[0],[0]
", ŷm and worker wl provides responses A1l, . . .",6.1. Synthetic data,[0],[0]
", Aml 6= 0, then the method
returns 1m ∑m i=1",6.1. Synthetic data,[0],[0]
1{ŷi 6=,6.1. Synthetic data,[0],[0]
"Ail} as estimate ε̂wl of εwl .
",6.1. Synthetic data,[0],[0]
Our Algorithm 1 is the only method that can handle up to 23 = n2,6.1. Synthetic data,[0],[0]
− 2 corrupted workers (in accordance with our theoretical results).,6.1. Synthetic data,[0],[0]
Its estimation error is constant as the number of corrupted workers increases from 0 to 23.,6.1. Synthetic data,[0],[0]
"Its prediction error depends on which weights we use in (16): the prediction error of Alg. 1 is constant in this range too, the one of Alt-Alg. 1 is slightly increasing.",6.1. Synthetic data,[0],[0]
"If only a few workers are corrupted, Alt-Alg.1 performs better than Alg. 1, while it is the other way round if more than 13 workers are corrupted.",6.1. Synthetic data,[0],[0]
The methods from the literature predict ground-truth labels as badly as random guessing already in the presence of only six corrupted workers.,6.1. Synthetic data,[0],[0]
All these methods are outperformed by majority voting.,6.1. Synthetic data,[0],[0]
We do not have an explanation for the non-monotonic behavior of the estimation error of SEM10 in the maximum norm.,6.1. Synthetic data,[0],[0]
"In Appendix C we present similar experiments, in which the error probability of the workers following the one-coin model is smaller or the error probabilities of the corrupted workers are less correlated.",6.1. Synthetic data,[0],[0]
"Still, the overall picture there is the same.
",6.1. Synthetic data,[0],[0]
One might wonder whether one can combine the considered methods from the literature with one of the algorithms by Jagabathula et al. (2017) in order to first sort the corrupted workers out and then apply the method only to the remaining workers and their responses.,6.1. Synthetic data,[0],[0]
"However, those algorithms cannot deal with the corrupted workers considered in this experiment, which are perfectly colluding, at all.",6.1. Synthetic data,[0],[0]
"Even though provided with the correct number t of corrupted workers as input, when t ≥ 3, the soft-penalty algorithm by Jagabathula et al. (2017) was not able to identify any of the corrupted workers in any of the 100 runs of the experiment.
",6.1. Synthetic data,[0],[0]
"In our next experiment, we study the convergence rate of Algorithm 1.",6.1. Synthetic data,[0],[0]
"We consider n = 50 workers, out of which t = 23 are corrupted in the same way as above.",6.1. Synthetic data,[0],[0]
Figure 2 shows the prediction and estimation error of Algorithm 1 as a function of the number of tasks m varying from 5000 to 20000.,6.1. Synthetic data,[0],[0]
"The prediction error of Alg. 1 decreases only slightly as m increases, the prediction error of Alt-Alg. 1 decreases more significantly.",6.1. Synthetic data,[0],[0]
Most interesting is the decay of the estimation error.,6.1. Synthetic data,[0],[0]
"Apparently, in this experiment it
decreases at a rate ofm−1/2 rather than at a rate ofm−1/8 as suggested by our upper bound (compare with Section 4.2).",6.1. Synthetic data,[0],[0]
We performed experiments on six publicly available data sets that are are commonly used in the literature (cf.,6.2. Real data,[0],[0]
"Snow et al., 2008, Zhang et al., 2016, and Bonald & Combes, 2017).",6.2. Real data,[0],[0]
All six data sets come with ground truth labels for each task.,6.2. Real data,[0],[0]
"For most of the data sets the matrix A, which stores the collected responses, is highly sparse.",6.2. Real data,[0],[0]
"In order to reduce sparseness, we removed workers that provided fewer than 50 labels.",6.2. Real data,[0],[0]
"For two of the data sets, we merged classes in order to end up with binary classification problems in the same way as Bonald & Combes (2017) did (Dog: {0, 2} vs {1, 3}; Web: {0, 1, 2} vs {3, 4}).",6.2. Real data,[0],[0]
Table 2 in Appendix C provides the characteristic values of the data sets.,6.2. Real data,[0],[0]
Note that only for the Bird data set every worker provided a label for every task whereas for the other ones A is still rather sparse.,6.2. Real data,[0],[0]
Figure 5 in Appendix C shows for each data set a histogram of the error probabilities of the workers (computed over those tasks that a worker was presented with).,6.2. Real data,[0],[0]
"Figure 6 shows a heat map of the matrix (|Cov[εwj (x, y), εwk(x, y)]|)nj,k=1 (computed over those tasks that two workers were jointly presented with).
",6.2. Real data,[0],[0]
Table 1 shows the prediction error for the various methods and data sets.,6.2. Real data,[0],[0]
There is no method that performs best on all data sets.,6.2. Real data,[0],[0]
"Overall, S-EM10 seems to be the method of choice.",6.2. Real data,[0],[0]
"Our Algorithm 1 can compete with the other methods, and on four out of the six data sets, the prediction error of Alt-Alg. 1 is smaller or larger only by 0.01 than the prediction error of S-EM10.",6.2. Real data,[0],[0]
Alg. 1 performs slightly worse than Alt-Alg. 1.,6.2. Real data,[0],[0]
"The poor performance of our method on
the Bird data set might be explained by the fact that there the workers clearly deviate from our model: as Figure 6 shows, there are no n2 + 2 workers that follow the one-coin model.
",6.2. Real data,[0],[0]
We performed another experiments on these data sets by corrupting some of the workers (chosen at random).,6.2. Real data,[0],[0]
"Like in the experiments of Section 6.1, the corrupted workers provide the same random response to every task.",6.2. Real data,[0],[0]
Figure 3 shows the prediction errors for the various methods and the first three data sets as functions of the number of corrupted workers.,6.2. Real data,[0],[0]
"Similar plots for the other data sets are shown in Figure 7 in Appendix C. On none of the data sets, any method can handle more corrupted workers than Alt-Alg. 1.",6.2. Real data,[0],[0]
"In this work, we studied an extension of the well-known one-coin model for crowdsourcing that allows for colluding adversaries.",7. Discussion,[0],[0]
"Our results show that even if almost half of the workers are adversarial, one can consistently estimate the workers’ error probabilities with an efficient algorithm.
",7. Discussion,[0],[0]
"For future work, it would be interesting to relax the assumption that the reliable workers follow the one-coin model and to allow for task-dependent error probabilities also for them.",7. Discussion,[0],[0]
It would also be interesting to see whether our approach can be extended to multiclass classification problems.,7. Discussion,[0],[0]
"Another direction concerns improving the sufficient rate m ∼ ρ−8 , which we obtained for our algorithm for recovering worker qualities up to error ρ.",7. Discussion,[0],[0]
"In the absence of adversaries one can achieve a rate m ∼ ρ−2, and we would like to understand whether this gap is inherent or an artifact of our algorithm/proof.",7. Discussion,[0],[0]
"Finally, we wonder about the role of adaptive task assignment in our extension of the one-coin model.",7. Discussion,[0],[0]
This research is supported by a Rutgers Research Council Grant and a Center for Discrete Mathematics and Theoretical Computer Science (DIMACS) postdoctoral fellowship.,Acknowledgements,[0],[0]
"Most existing works on crowdsourcing assume that the workers follow the Dawid-Skene model, or the one-coin model as its special case, where every worker makes mistakes independently of other workers and with the same error probability for every task.",abstractText,[0],[0]
We study a significant extension of this restricted model.,abstractText,[0],[0]
"We allow almost half of the workers to deviate from the one-coin model and for those workers, their probabilities of making an error to be task-dependent and to be arbitrarily correlated.",abstractText,[0],[0]
"In other words, we allow for arbitrary adversaries, for which not only error probabilities can be high, but which can also perfectly collude.",abstractText,[0],[0]
"In this adversarial scenario, we design an efficient algorithm to consistently estimate the workers’ error probabilities.",abstractText,[0],[0]
Crowdsourcing with Arbitrary Adversaries,title,[0],[0]
Reinforcement learning algorithms aim at learning policies for achieving target tasks by maximizing rewards provided by the environment.,1. Introduction,[0],[0]
"In some scenarios, these rewards are supplied to the agent continuously, e.g. the running score in an Atari game (Mnih et al., 2015), or the distance between a robot arm and an object in a reaching task (Lillicrap et al., 2016).",1. Introduction,[0],[0]
"However, in many real-world scenarios, rewards extrinsic to the agent are extremely sparse or miss-
1University of California, Berkeley.",1. Introduction,[0],[0]
"Correspondence to: Deepak Pathak <pathak@berkeley.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, 2017.",1. Introduction,[0],[0]
JMLR: W&CP.,1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"(a) learn to explore in Level-1 (b) explore faster in Level-2
Figure 1.",1. Introduction,[0],[0]
Discovering how to play Super Mario Bros without rewards.,1. Introduction,[0],[0]
"(a) Using only curiosity-driven exploration, the agent makes significant progress in Level-1.",1. Introduction,[0],[0]
(b) The gained knowledge helps the agent explore subsequent levels much faster than when starting from scratch.,1. Introduction,[0],[0]
"Watch the video at http://pathak22. github.io/noreward-rl/
ing altogether, and it is not possible to construct a shaped reward function.",1. Introduction,[0],[0]
This is a problem as the agent receives reinforcement for updating its policy only if it succeeds in reaching a pre-specified goal state.,1. Introduction,[0],[0]
"Hoping to stumble into a goal state by chance (i.e. random exploration) is likely to be futile for all but the simplest of environments.
",1. Introduction,[0],[0]
"As human agents, we are accustomed to operating with rewards that are so sparse that we only experience them once or twice in a lifetime, if at all.",1. Introduction,[0],[0]
"To a three-year-old enjoying a sunny Sunday afternoon on a playground, most trappings of modern life – college, good job, a house, a family – are so far into the future, they provide no useful reinforcement signal.",1. Introduction,[0],[0]
"Yet, the three-year-old has no trouble entertaining herself in that playground using what psychologists call intrinsic motivation (Ryan, 2000) or curiosity (Silvia, 2012).",1. Introduction,[0],[0]
Motivation/curiosity have been used to explain the need to explore the environment and discover novel states.,1. Introduction,[0],[0]
"The French word flâneur perfectly captures the notion of a curiosity-driven observer, the “deliberately aimless pedestrian, unencumbered by any obligation or sense of urgency” (Cornelia Otis Skinner).",1. Introduction,[0],[0]
"More generally, curiosity is a way of learning new skills which might come handy for pursuing rewards in the future.
",1. Introduction,[0],[0]
"Similarly, in reinforcement learning, intrinsic motivation/rewards become critical whenever extrinsic rewards are sparse.",1. Introduction,[0],[0]
"Most formulations of intrinsic reward can be grouped into two broad classes: 1) encourage the agent to explore “novel” states (Bellemare et al., 2016; Lopes
ar X
iv :1
70 5.
05 36
3v 1
[ cs
.L",1. Introduction,[0],[0]
"G
] 1
5 M
ay 2
01 7
et al., 2012; Poupart et al., 2006) or, 2) encourage the agent to perform actions that reduce the error/uncertainty in the agent’s ability to predict the consequence of its own actions (i.e. its knowledge about the environment) (Houthooft et al., 2016; Mohamed & Rezende, 2015; Schmidhuber, 1991; 2010; Singh et al., 2005; Stadie et al., 2015).
",1. Introduction,[0],[0]
"Measuring “novelty” requires a statistical model of the distribution of the environmental states, whereas measuring prediction error/uncertainty requires building a model of environmental dynamics that predicts the next state (st+1) given the current state (st) and the action (at) executed at time t. Both these models are hard to build in highdimensional continuous state spaces such as images.",1. Introduction,[0],[0]
"An additional challenge lies in dealing with the stochasticity of the agent-environment system, both due to the noise in the agent’s actuation, which causes its end-effectors to move in a stochastic manner, and, more fundamentally, due to the inherent stochasticity in the environment.",1. Introduction,[0],[0]
"To give the example from (Schmidhuber, 2010), if the agent receiving images as state inputs is observing a television screen displaying white noise, every state will be novel and it would be impossible to predict the value of any pixel in the future.",1. Introduction,[0],[0]
"Other examples of such stochasticity include appearance changes due to shadows from other moving entities, presence of distractor objects, or other agents in the environment whose motion is not only hard to predict but is also irrelevant to the agent’s goals.",1. Introduction,[0],[0]
"Somewhat different, but related, is the challenge of generalization across physically (and perhaps also visually) distinct but functionally similar parts of an environment, which is crucial for largescale problems.",1. Introduction,[0],[0]
"One proposed solution to all these problems is to only reward the agent when it encounters states that are hard to predict but are “learnable” (Schmidhuber, 1991).",1. Introduction,[0],[0]
"However, estimating learnability is a non-trivial problem (Lopes et al., 2012).
",1. Introduction,[0],[0]
"This work belongs to the broad category of methods that generate an intrinsic reward signal based on how hard it is for the agent to predict the consequences of its own actions, i.e. predict the next state given the current state and the executed action.",1. Introduction,[0],[0]
"However, we manage to escape most pitfalls of previous prediction approaches with the following key insight: we only predict those changes in the environment that could possibly be due to the actions of our agent or affect the agent, and ignore the rest.",1. Introduction,[0],[0]
"That is, instead of making predictions in the raw sensory space (e.g. pixels), we transform the sensory input into a feature space where only the information relevant to the action performed by the agent is represented.",1. Introduction,[0],[0]
We learn this feature space using self-supervision – training a neural network on a proxy inverse dynamics task of predicting the agent’s action given its current and next states.,1. Introduction,[0],[0]
"Since the neural network is only required to predict the action, it has no incentive to represent within its feature embedding space the factors of vari-
ation in the environment that do not affect the agent itself.",1. Introduction,[0],[0]
"We then use this feature space to train a forward dynamics model that predicts the feature representation of the next state, given the feature representation of the current state and the action.",1. Introduction,[0],[0]
"We provide the prediction error of the forward dynamics model to the agent as an intrinsic reward to encourage its curiosity.
",1. Introduction,[0],[0]
The role of curiosity has been widely studied in the context of solving tasks with sparse rewards.,1. Introduction,[0],[0]
"In our opinion, curiosity has two other fundamental uses.",1. Introduction,[0],[0]
Curiosity helps an agent explore its environment in the quest for new knowledge (a desirable characteristic of exploratory behavior is that it should improve as the agent gains more knowledge).,1. Introduction,[0],[0]
"Further, curiosity is a mechanism for an agent to learn skills that might be helpful in future scenarios.",1. Introduction,[0],[0]
"In this paper, we evaluate the effectiveness of our curiosity formulation in all three of these roles.
",1. Introduction,[0],[0]
"We first compare the performance of an A3C agent (Mnih et al., 2016) with and without the curiosity signal on 3-D navigation tasks with sparse extrinsic reward in the VizDoom environment.",1. Introduction,[0],[0]
We show that a curiosity-driven intrinsic reward is crucial in accomplishing these tasks (see Section 4.1).,1. Introduction,[0],[0]
"Next, we show that even in the absence of any extrinsic rewards, a curious agent learns good exploration policies.",1. Introduction,[0],[0]
"For instance, an agent trained only with curiosity as its reward is able to cross a significant portion of Level-1 in Super Mario Bros. Similarly in VizDoom, the agent learns to walk intelligently along the corridors instead of bumping into walls or getting stuck in corners (see Section 4.2).",1. Introduction,[0],[0]
"A question that naturally follows is whether the learned exploratory behavior is specific to the physical space that the agent trained itself on, or if it enables the agent to perform better in unseen scenarios too?",1. Introduction,[0],[0]
"We show that the exploration policy learned in the first level of Mario helps the agent explore subsequent levels faster (shown in Figure 1), while the intelligent walking behavior learned by the curious VizDoom agent transfers to a completely new map with new textures (see Section 4.3).",1. Introduction,[0],[0]
These results suggest that the proposed method enables an agent to learn generalizable skills even in the absence of an explicit goal.,1. Introduction,[0],[0]
Our agent is composed of two subsystems: a reward generator that outputs a curiosity-driven intrinsic reward signal and a policy that outputs a sequence of actions to maximize that reward signal.,2. Curiosity-Driven Exploration,[0],[0]
"In addition to intrinsic rewards, the agent optionally may also receive some extrinsic reward from the environment.",2. Curiosity-Driven Exploration,[0],[0]
Let the intrinsic curiosity reward generated by the agent at time t be rit and the extrinsic reward be ret .,2. Curiosity-Driven Exploration,[0],[0]
"The policy sub-system is trained to maximize the sum of these two rewards rt = rit + r e t , with r e t mostly (if not always) zero.
",2. Curiosity-Driven Exploration,[0],[0]
We represent the policy π(st; θP ) by a deep neural network with parameters θP .,2. Curiosity-Driven Exploration,[0],[0]
"Given the agent in state st, it executes the action at ∼ π(st; θP ) sampled from the policy.",2. Curiosity-Driven Exploration,[0],[0]
"θP is optimized to maximize the expected sum of rewards,
max θP Eπ(st;θP )",2. Curiosity-Driven Exploration,[0],[0]
"[Σtrt] (1)
Unless specified otherwise, we use the notation π(s) to denote the parameterized policy π(s; θP ).",2. Curiosity-Driven Exploration,[0],[0]
"Our curiosity reward model can potentially be used with a range of policy learning methods; in the experiments discussed here, we use the asynchronous advantage actor critic policy gradient (A3C) (Mnih et al., 2016) for policy learning.",2. Curiosity-Driven Exploration,[0],[0]
"Our main contribution is in designing an intrinsic reward signal based on prediction error of the agent’s knowledge about its environment that scales to high-dimensional continuous state spaces like images, bypasses the hard problem of predicting pixels and is unaffected by the unpredictable aspects of the environment that do not affect the agent.",2. Curiosity-Driven Exploration,[0],[0]
"Making predictions in the raw sensory space (e.g. when st corresponds to images) is undesirable not only because it is hard to predict pixels directly, but also because it is unclear if predicting pixels is even the right objective to optimize.",2.1. Prediction error as curiosity reward,[0],[0]
"To see why, consider using prediction error in the pixel space as the curiosity reward.",2.1. Prediction error as curiosity reward,[0],[0]
Imagine a scenario where the agent is observing the movement of tree leaves in a breeze.,2.1. Prediction error as curiosity reward,[0],[0]
"Since it is inherently hard to model breeze, it is even harder to predict the pixel location of each leaf.
",2.1. Prediction error as curiosity reward,[0],[0]
This implies that the pixel prediction error will remain high and the agent will always remain curious about the leaves.,2.1. Prediction error as curiosity reward,[0],[0]
But the motion of the leaves is inconsequential to the agent and therefore its continued curiosity about them is undesirable.,2.1. Prediction error as curiosity reward,[0],[0]
The underlying problem is that the agent is unaware that some parts of the state space simply cannot be modeled and thus the agent can fall into an artificial curiosity trap and stall its exploration.,2.1. Prediction error as curiosity reward,[0],[0]
Novelty-seeking exploration schemes that record the counts of visited states in a tabular form (or their extensions to continuous state spaces) also suffer from this issue.,2.1. Prediction error as curiosity reward,[0],[0]
"Measuring learning progress instead of prediction error has been proposed in the past as one solution (Schmidhuber, 1991).",2.1. Prediction error as curiosity reward,[0],[0]
"Unfortunately, there are currently no known computationally feasible mechanisms for measuring learning progress.
",2.1. Prediction error as curiosity reward,[0],[0]
"If not the raw observation space, then what is the right feature space for making predictions so that the prediction error provides a good measure of curiosity?",2.1. Prediction error as curiosity reward,[0],[0]
"To answer this question, let us divide all sources that can modify the agent’s observations into three cases: (1) things that can be controlled by the agent; (2) things that the agent cannot control but that can affect the agent (e.g. a vehicle driven by another agent), and (3) things out of the agent’s control and not affecting the agent (e.g. moving leaves).",2.1. Prediction error as curiosity reward,[0],[0]
A good feature space for curiosity should model (1) and (2) and be unaffected by (3).,2.1. Prediction error as curiosity reward,[0],[0]
"This latter is because, if there is a source of variation that is inconsequential for the agent, then the agent has no incentive to know about it.",2.1. Prediction error as curiosity reward,[0],[0]
"Instead of hand-designing a feature representation for every environment, our aim is to come up with a general mechanism for learning feature representations such that the prediction error in the learned feature space provides a good intrinsic reward signal.",2.2. Self-supervised prediction for exploration,[0],[0]
"We propose that such a feature space can be learned by training a deep neural network with two sub-modules: the first sub-module encodes the raw state (st) into a feature vector φ(st) and the second submodule takes as inputs the feature encoding φ(st), φ(st+1) of two consequent states and predicts the action (at) taken by the agent to move from state st to st+1.",2.2. Self-supervised prediction for exploration,[0],[0]
"Training this neural network amounts to learning function g defined as:
ât = g ( st, st+1; θI )",2.2. Self-supervised prediction for exploration,[0],[0]
"(2)
where, ât is the predicted estimate of the action at and the the neural network parameters θI are trained to optimize,
min θI",2.2. Self-supervised prediction for exploration,[0],[0]
"LI(ât, at) (3)
where, LI is the loss function that measures the discrepancy between the predicted and actual actions.",2.2. Self-supervised prediction for exploration,[0],[0]
"In case at is discrete, the output of g is a soft-max distribution across all possible actions and minimizing LI amounts to maximum likelihood estimation of θI under a multinomial distribution.",2.2. Self-supervised prediction for exploration,[0],[0]
"The learned function g is also known as the inverse dynamics model and the tuple (st, at, st+1) required to learn g is obtained while the agent interacts with the environment using its current policy π(s).
",2.2. Self-supervised prediction for exploration,[0],[0]
"In addition to inverse dynamics model, we train another neural network that takes as inputs at and φ(st) and predicts the feature encoding of the state at time step t+ 1,
φ̂(st+1)",2.2. Self-supervised prediction for exploration,[0],[0]
"= f ( φ(st), at; θF ) (4)
where φ̂(st+1) is the predicted estimate of φ(st+1) and the neural network parameters θF are optimized by minimizing the loss function LF :
LF ( φ(st), φ̂(st+1) )",2.2. Self-supervised prediction for exploration,[0],[0]
"= 1
2 ‖φ̂(st+1)− φ(st+1)‖22 (5)
The learned function f is also known as the forward dynamics model.",2.2. Self-supervised prediction for exploration,[0],[0]
"The intrinsic reward signal rit is computed as,
rit = η
2 ‖φ̂(st+1)− φ(st+1)‖22 (6)
where η > 0 is a scaling factor.",2.2. Self-supervised prediction for exploration,[0],[0]
"In order to generate the curiosity based intrinsic reward signal, we jointly optimize the forward and inverse dynamics loss described in equations 3 and 5 respectively.",2.2. Self-supervised prediction for exploration,[0],[0]
The inverse model learns a feature space that encodes information relevant for predicting the agent’s actions only and the forward model makes predictions in this feature space.,2.2. Self-supervised prediction for exploration,[0],[0]
"We refer to this proposed
curiosity formulation as Intrinsic Curiosity Module (ICM).",2.2. Self-supervised prediction for exploration,[0],[0]
"As there is no incentive for this feature space to encode any environmental features that are not influenced by the agent’s actions, our agent will receive no rewards for reaching environmental states that are inherently unpredictable and its exploration strategy will be robust to the presence of distractor objects, changes in illumination, or other nuisance sources of variation in the environment.",2.2. Self-supervised prediction for exploration,[0],[0]
"See Figure 2 for illustration of the formulation.
",2.2. Self-supervised prediction for exploration,[0],[0]
"The use of inverse models has been investigated to learn features for recognition tasks (Agrawal et al., 2015; Jayaraman & Grauman, 2015).",2.2. Self-supervised prediction for exploration,[0],[0]
Agrawal et al. (2016) constructed a joint inverse-forward model to learn feature representation for the task of pushing objects.,2.2. Self-supervised prediction for exploration,[0],[0]
"However, they only used the forward model as a regularizer for training the inverse model features, while we make use of the error in the forward model predictions as the curiosity reward for training our agent’s policy.
",2.2. Self-supervised prediction for exploration,[0],[0]
"The overall optimization problem that is solved for learning the agent is a composition of equations 1, 3 and 5 and can be written as,
min θP ,θI ,θF
[ − λEπ(st;θP )",2.2. Self-supervised prediction for exploration,[0],[0]
"[Σtrt] + (1− β)LI + βLF ] (7)
where 0 ≤ β ≤ 1 is a scalar that weighs the inverse model loss against the forward model loss and λ > 0 is a scalar that weighs the importance of the policy gradient loss against the importance of learning the intrinsic reward signal.",2.2. Self-supervised prediction for exploration,[0],[0]
"To evaluate our curiosity module on its ability to improve exploration and provide generalization to novel scenarios, we will use two simulated environments.",3. Experimental Setup,[0],[0]
"This section describes the details of the environments and the experimental setup.
",3. Experimental Setup,[0],[0]
"Environments The first environment we evaluate on is the VizDoom (Kempka et al., 2016) game.",3. Experimental Setup,[0],[0]
"We consider the Doom 3-D navigation task where the action space of the agent consists of four discrete actions – move forward, move left, move right and no-action.",3. Experimental Setup,[0],[0]
"Our testing setup in all the experiments is the ‘DoomMyWayHome-v0’ environment which is available as part of OpenAI Gym (Brockman et al., 2016).",3. Experimental Setup,[0],[0]
Episodes are terminated either when the agent finds the vest or if the agent exceeds a maximum of 2100 time steps.,3. Experimental Setup,[0],[0]
The map consists of 9 rooms connected by corridors and the agent is tasked to reach some fixed goal location from its spawning location.,3. Experimental Setup,[0],[0]
The agent is only provided a sparse terminal reward of +1 if it finds the vest and zero otherwise.,3. Experimental Setup,[0],[0]
"For generalization experiments, we pre-train on
a different map with different random textures from (Dosovitskiy & Koltun, 2016) and each episode lasts for 2100 time steps.",3. Experimental Setup,[0],[0]
"Sample frames from VizDoom are shown in Figure 3a, and maps are explained in Figure 4.",3. Experimental Setup,[0],[0]
"It takes approximately 350 steps for an optimal policy to reach the vest location from the farthest room in this map (sparse reward).
",3. Experimental Setup,[0],[0]
"Our second environment is the classic Nintendo game Super Mario Bros (Paquette, 2016).",3. Experimental Setup,[0],[0]
We consider four levels of the game: pre-training on the first level and showing generalization on the subsequent levels.,3. Experimental Setup,[0],[0]
"In this setup, we reparametrize the action space of the agent into 14 unique actions following (Paquette, 2016).",3. Experimental Setup,[0],[0]
"This game is played using a joystick allowing for multiple simultaneous button presses, where the duration of the press affects what action is being taken.",3. Experimental Setup,[0],[0]
"This property makes the game particularly hard, e.g. to make a long jump over tall pipes or wide gaps, the agent needs to predict the same action up to 12 times in a row, introducing long-range dependencies.",3. Experimental Setup,[0],[0]
"All our experiments on Mario are trained using curiosity signal only, without any reward from the game.
",3. Experimental Setup,[0],[0]
Training details,3. Experimental Setup,[0],[0]
"All agents in this work are trained using visual inputs that are pre-processed in manner similar to (Mnih et al., 2016).",3. Experimental Setup,[0],[0]
The input RGB images are converted into gray-scale and re-sized to 42 × 42.,3. Experimental Setup,[0],[0]
"In order to model temporal dependencies, the state representation (st) of the environment is constructed by concatenating the current frame with the three previous frames.",3. Experimental Setup,[0],[0]
"Closely following (Mnih et al., 2015; 2016), we use action repeat of four during training time in VizDoom and action repeat of six in Mario.",3. Experimental Setup,[0],[0]
"However, we sample the policy without any action repeat during inference.",3. Experimental Setup,[0],[0]
"Following the asynchronous training protocol in A3C, all the agents were trained asynchronously with twenty workers using stochastic gradient descent.",3. Experimental Setup,[0],[0]
"We used ADAM optimizer with its parameters not shared across the workers.
A3C architecture The input state st is passed through a sequence of four convolution layers with 32 filters each,
kernel size of 3x3, stride of 2 and padding of 1.",3. Experimental Setup,[0],[0]
"An exponential linear unit (ELU; (Clevert et al., 2015)) is used after each convolution layer.",3. Experimental Setup,[0],[0]
The output of the last convolution layer is fed into a LSTM with 256 units.,3. Experimental Setup,[0],[0]
"Two seperate fully connected layers are used to predict the value function and the action from the LSTM feature representation.
",3. Experimental Setup,[0],[0]
Intrinsic Curiosity Module (ICM) architecture The intrinsic curiosity module consists of the forward and the inverse model.,3. Experimental Setup,[0],[0]
"The inverse model first maps the input state (st) into a feature vector φ(st) using a series of four convolution layers, each with 32 filters, kernel size 3x3, stride of 2 and padding of 1.",3. Experimental Setup,[0],[0]
ELU non-linearity is used after each convolution layer.,3. Experimental Setup,[0],[0]
The dimensionality of φ(st) (i.e. the output of the fourth convolution layer) is 288.,3. Experimental Setup,[0],[0]
"For the inverse model, φ(st) and φ(st+1) are concatenated into a single feature vector and passed as inputs into a fully connected layer of 256 units followed by an output fully connected layer with 4 units to predict one of the four possible actions.",3. Experimental Setup,[0],[0]
The forward model is constructed by concatenating φ(st) with at and passing it into a sequence of two fully connected layers with 256 and 288 units respectively.,3. Experimental Setup,[0],[0]
"The value of β is 0.2, and λ is 0.1.",3. Experimental Setup,[0],[0]
"The Equation (7) is minimized with learning rate of 1e− 3.
",3. Experimental Setup,[0],[0]
"Baseline Methods ‘ICM + A3C’ denotes our full algorithm which combines intrinsic curiosity model with A3C. Across different experiments, we compare our approach with three baselines.",3. Experimental Setup,[0],[0]
First is the vanilla ‘A3C’ algorithm with -greedy exploration.,3. Experimental Setup,[0],[0]
"Second is ‘ICM-pixels + A3C’, which is a variant of our ICM without the inverse model, and has curiosity reward dependent only on the forward model loss in predicting next observation in pixel space.",3. Experimental Setup,[0],[0]
"To design this, we remove the inverse model layers and append
deconvolution layers to the forward model.",3. Experimental Setup,[0],[0]
ICM-pixels is close to ICM in architecture but incapable of learning embedding that is invariant to the uncontrollable part of environment.,3. Experimental Setup,[0],[0]
"Note that ICM-pixels is representative of previous methods which compute information gain by directly using the observation space (Schmidhuber, 2010; Stadie et al., 2015).",3. Experimental Setup,[0],[0]
We show that directly using observation space for computing curiosity is significantly worse than learning an embedding as in ICM.,3. Experimental Setup,[0],[0]
"Finally, we include comparison with state-of-the-art exploration methods based on variational information maximization (VIME) (Houthooft et al., 2016) which is trained with TRPO.",3. Experimental Setup,[0],[0]
"We qualitatively and quantitatively evaluate the performance of the learned policy with and without the proposed intrinsic curiosity signal in two environments, VizDoom and Super Mario Bros. Three broad settings are evaluated: a) sparse extrinsic reward on reaching a goal (Section 4.1); b) exploration with no extrinsic reward (Section 4.2); and c) generalization to novel scenarios (Section 4.3).",4. Experiments,[0],[0]
"In VizDoom generalization is evaluated on a novel map with novel textures, while in Mario it is evaluated on subsequent game levels.",4. Experiments,[0],[0]
We perform extrinsic reward experiments on VizDoom using ‘DoomMyWayHome-v0’ setup described in Section 3.,4.1. Sparse Extrinsic Reward Setting,[0],[0]
The extrinsic reward is sparse and only provided when the agent finds the goal (a vest) located at a fixed location in the map.,4.1. Sparse Extrinsic Reward Setting,[0],[0]
"We systematically varied the difficulty of this goaldirected exploration task by varying the distance between
the initial spawning location of the agent and the location of the goal.",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"A larger distance means that the chances of reaching the goal location by random exploration is lower and consequently the reward is said to be sparser.
",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"Varying the degree of reward sparsity: We consider three setups with “dense”, “sparse” and “very-sparse” rewards (see Figure 4b).",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"In these settings, the reward is always terminal and the episode terminates upon reaching goal or after a maximum of 2100 steps.",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"In the “dense” reward case, the agent is randomly spawned in any of the 17 possible spawning locations uniformly distributed across the map.",4.1. Sparse Extrinsic Reward Setting,[0],[0]
This is not a hard exploration task because sometimes the agent is randomly initialized close to the goal and therefore by random -greedy exploration it can reach the goal with reasonably high probability.,4.1. Sparse Extrinsic Reward Setting,[0],[0]
"In the “sparse” and “very sparse” reward cases, the agent is always spawned in Room-13 and Room-17 respectively which are 270 and 350 steps away from the goal under an optimal policy.",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"A long sequence of directed actions is required to reach the goals from these rooms, making these settings hard goal directed exploration problems.
",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"Results shown in Figure 5 indicate that while the performance of the baseline A3C degrades with sparser rewards, curious A3C agents are superior in all cases.",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"In the “dense” reward case, curious agents learn much faster indicating more efficient exploration of the environment as compared to -greedy exploration of the baseline agent.",4.1. Sparse Extrinsic Reward Setting,[0],[0]
One possible explanation of the inferior performance of ICM-pixels in comparison to ICM is that in every episode the agent is spawned in one out of seventeen rooms with different textures.,4.1. Sparse Extrinsic Reward Setting,[0],[0]
"It is hard to learn a pixel-prediction model as the number of textures increases.
",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"In the “sparse” reward case, as expected, the baseline A3C agent fails to solve the task, while the curious A3C agent is able to learn the task quickly.",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"Note that ICM-pixels and ICM have similar convergence because, with a fixed spawning location of the agent, the ICM-pixels encounters the same textures at the starting of each episode which makes learning the pixel-prediction model easier as compared to the “dense” reward case.",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"Finally, in the “very sparse” reward case, both the A3C agent and ICM-pixels never succeed, while the ICM agent achieves a perfect score in 66% of the random runs.",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"This indicates that ICM is better suited than ICM-pixels and vanilla A3C for hard goal directed exploration tasks.
",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"Robustness to uncontrollable dynamics For testing the robustness of the proposed ICM formulation to changes in the environment that do not affect the agent, we augmented the agent’s observation with a fixed region of white noise which made up 40% of the image (see Figure 3b).",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"In VizDoom 3-D navigation, ideally the agent should be unaffected by this noise as the noise does not affect the agent in anyway and is merely a nuisance.",4.1. Sparse Extrinsic Reward Setting,[0],[0]
Figure 6 compares the performance of ICM against some baseline methods on the “sparse” reward setup described above.,4.1. Sparse Extrinsic Reward Setting,[0],[0]
"While, the proposed ICM agent achieves a perfect score, ICM-pixels suffers significantly despite having succeeded at the “sparse reward” task when the inputs were not augmented with any noise (see Figure 5b).",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"This indicates that in contrast to ICM-pixels, ICM is insensitive to nuisance changes in the environment.
",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"Comparison to TRPO-VIME We now compare our curious agent against variational information maximization agent trained with TRPO (Houthooft et al., 2016) for the
VizDoom “sparse” reward setup described above.",4.1. Sparse Extrinsic Reward Setting,[0],[0]
TRPO is in general more sample efficient than A3C but takes a lot more wall-clock time.,4.1. Sparse Extrinsic Reward Setting,[0],[0]
We do not show these results in plots because TRPO and A3C have different setups.,4.1. Sparse Extrinsic Reward Setting,[0],[0]
"The hyperparameters and accuracy for the TRPO and VIME results follow from the concurrent work (Fu et al., 2017).",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"Despite the sample efficiency of TRPO, we see that our ICM agents work significantly better than TRPO and TRPO-VIME, both in terms of convergence rate and accuracy.",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"Results are shown in the Table below:
Method Mean (Median) Score (“sparse” reward setup) (at convergence)
TRPO 26.0 % ( 0.0 %) A3C 0.0 % ( 0.0 %) VIME + TRPO 46.1 % ( 27.1 %)
",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"ICM + A3C 100.0 % (100.0 %)
",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"As a sanity check, we replaced the curiosity network with random noise sources and used them as the curiosity reward.",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"We performed systematic sweep across different distribution parameters in the “sparse” reward case: uniform, Gaussian and Laplacian.",4.1. Sparse Extrinsic Reward Setting,[0],[0]
We found that none of these agents were able to reach the goal showing that our curiosity module does not learn degenerate solutions.,4.1. Sparse Extrinsic Reward Setting,[0],[0]
A good exploration policy is one which allows the agent to visit as many states as possible even without any goals.,4.2. No Reward Setting,[0],[0]
"In the case of 3-D navigation, we expect a good exploration policy to cover as much of the map as possible; in the case of playing a game, we expect it to visit as many game states as possible.",4.2. No Reward Setting,[0],[0]
"In order to test if our agent can learn a good exploration policy, we trained it on VizDoom and Mario without any rewards from the environment.",4.2. No Reward Setting,[0],[0]
"We then evaluated what portion of the map was explore (for VizDoom), and how much progress it made (for Mario) in this setting.",4.2. No Reward Setting,[0],[0]
"To our surprise, we have found that in both cases, the noreward agent was able to perform quote well (see video at http://pathak22.github.io/noreward_rl/).
",4.2. No Reward Setting,[0],[0]
VizDoom: Coverage during Exploration.,4.2. No Reward Setting,[0],[0]
"An agent trained with no extrinsic rewards was able to learn to navigate corridors, walk between rooms and explore many rooms in the 3-D Doom environment.",4.2. No Reward Setting,[0],[0]
On many occasions the agent traversed the entire map and reached rooms that were farthest away from the room it was initialized in.,4.2. No Reward Setting,[0],[0]
"Given that the episode terminates in 2100 steps and farthest rooms are over 250 steps away (for an optimally-moving agent), this result is quite remarkable, demonstrating that it is possible to learn useful skills without the requirement of any external supervision of rewards.",4.2. No Reward Setting,[0],[0]
Example explorations are shown in Figure 7.,4.2. No Reward Setting,[0],[0]
"The first 3 maps show our agent ex-
plore a much larger state space without any extrinsic signal, compared to a random exploration agent (last two maps), which often has hard time getting around local minima of state spaces, e.g. getting stuck against a wall and not able to move (see video).
",4.2. No Reward Setting,[0],[0]
Mario: Learning to play with no rewards.,4.2. No Reward Setting,[0],[0]
We train our agent in the Super Mario World using only curiosity based signal.,4.2. No Reward Setting,[0],[0]
"Without any extrinsic reward from environment, our Mario agent can learn to cross over 30% of Level-1.",4.2. No Reward Setting,[0],[0]
"The agent received no reward for killing or dodging enemies or avoiding fatal events, yet it automatically discovered these behaviors (see video).",4.2. No Reward Setting,[0],[0]
"One possible reason is because getting killed by the enemy will result in only seeing a small part of the game space, making its curiosity saturate.",4.2. No Reward Setting,[0],[0]
"In order to remain curious, it is in the agent’s interest to learn how to kill and dodge enemies so that it can reach new parts of the game space.",4.2. No Reward Setting,[0],[0]
"This suggests that curiosity provides indirect supervision for learning interesting behaviors.
",4.2. No Reward Setting,[0],[0]
"To the best of our knowledge, this is the first demonstration where the agent learns to navigate in a 3D environment and discovers how to play a game by making use of relatively complex visual imagery directly from pixels, without any extrinsic rewards.",4.2. No Reward Setting,[0],[0]
"There are several prior works that use reinforcement learning to navigate in 3D environments from pixel inputs or playing ATARI games such as (Mirowski et al., 2017; Mnih et al., 2015; 2016), but they rely on intermediate external rewards provided by the environment.",4.2. No Reward Setting,[0],[0]
In the previous section we showed that our agent learns to explore large parts of the space where its curiosity-driven exploration policy was trained.,4.3. Generalization to Novel Scenarios,[0],[0]
"However, it remains unclear whether the agent has done this by learning “generalized skills” for efficiently exploring its environment, or if it simply memorized the training set.",4.3. Generalization to Novel Scenarios,[0],[0]
"In other words we would like to know, when exploring a space, how much of the learned behavior is specific to that particular space and how much is general enough to be useful in novel scenar-
ios?",4.3. Generalization to Novel Scenarios,[0],[0]
"To investigate this question, we train a no reward exploratory behavior in one scenario (e.g. Level-1 of Mario) and then evaluate the resulting exploration policy in three different ways: a) apply the learned policy “as is” to a new scenario; b) adapt the policy by fine-tuning with curiosity reward only; c) adapt the policy to maximize some extrinsic reward.",4.3. Generalization to Novel Scenarios,[0],[0]
"Happily, in all three cases, we observe some promising generalization results:
Evaluate “as is”: We evaluate the policy trained by maximizing curiosity on Level-1 of Mario on subsequent levels without adapting the learned policy in any way.",4.3. Generalization to Novel Scenarios,[0],[0]
"We measure the distance covered by the agent as a result of executing this policy on Levels 1, 2, and 3, as shown in Table 1.",4.3. Generalization to Novel Scenarios,[0],[0]
"We note that the policy performs surprisingly well on Level 3, suggesting good generalization, despite the fact that Level-3 has different structures and enemies compared to Level-1.",4.3. Generalization to Novel Scenarios,[0],[0]
"However, note that the running “as is” on Level2 does not do well.",4.3. Generalization to Novel Scenarios,[0],[0]
"At first, this seems to contradict the generalization results on Level-3.",4.3. Generalization to Novel Scenarios,[0],[0]
"However, note that Level-3 has similar global visual appearance (day world with sunlight) to Level-1, whereas Level-2 is significantly different (night world).",4.3. Generalization to Novel Scenarios,[0],[0]
"If this is indeed the issue, then it should be possible to quickly adapt the exploration policy to Level-2 with a little bit of “fine-tuning”.",4.3. Generalization to Novel Scenarios,[0],[0]
"We will explore this below.
",4.3. Generalization to Novel Scenarios,[0],[0]
Fine-tuning with curiosity only: From Table 1 we see that when the agent pre-trained (using only curiosity as reward) on Level-1 is fine-tuned (using only curiosity as reward) on Level-2 it quickly overcomes the mismatch in global visual appearance and achieves a higher score than training from scratch with the same number of iterations.,4.3. Generalization to Novel Scenarios,[0],[0]
"Interestingly, training “from scratch” on Level-2 is worse than the fine-tuned policy, even when training for more iterations than pre-training + fine-tuning combined.",4.3. Generalization to Novel Scenarios,[0],[0]
"One possible reason is that Level-2 is more difficult than Level1, so learning the basic skills such as moving, jumping, and killing enemies from scratch is much more dangerous than in the relative “safety” of Level-1.",4.3. Generalization to Novel Scenarios,[0],[0]
"This result, therefore might suggest that first pre-training on an earlier level
and then fine-tuning on a later one produces a form of curriculum which aids learning and generalization.",4.3. Generalization to Novel Scenarios,[0],[0]
"In other words, the agent is able to use the knowledge it acquired by playing Level-1 to better explore the subsequent levels.",4.3. Generalization to Novel Scenarios,[0],[0]
"Of course, the game designers do this on purpose to allow the human players to gradually learn to play the game.
",4.3. Generalization to Novel Scenarios,[0],[0]
"However, interestingly, fine-tuning the exploration policy pre-trained on Level-1 to Level-3 deteriorates the performance, compared to running “as is”.",4.3. Generalization to Novel Scenarios,[0],[0]
This is because Level3 is very hard for the agent to cross beyond a certain point – the agent hits a curiosity blockade and is unable to make any progress.,4.3. Generalization to Novel Scenarios,[0],[0]
"As the agent has already learned about parts of the environment before the hard point, it receives almost no curiosity reward and as a result it attempts to update its policy with almost zero intrinsic rewards and the policy slowly degenerates.",4.3. Generalization to Novel Scenarios,[0],[0]
"This behavior is vaguely analogous to boredom, where if the agent is unable to make progress it gets bored and stops exploring.
",4.3. Generalization to Novel Scenarios,[0],[0]
"Fine-tuning with extrinsic rewards: If it is the case that the agent has actually learned useful exploratory behavior, then it should be able to learn quicker than starting from scratch even when external rewards are provided by environment.",4.3. Generalization to Novel Scenarios,[0],[0]
We perform this evaluation on VizDoom where we pre-train the agent using curiosity reward on a map showed in Figure 4a.,4.3. Generalization to Novel Scenarios,[0],[0]
"We then test on the “very sparse” reward setting of ‘DoomMyWayHome-v0’ environment which uses a different map with novel textures (see Figure 4b) as described earlier in Section 4.1.
",4.3. Generalization to Novel Scenarios,[0],[0]
Results in Figure 8 show that the ICM agent pre-trained only with curiosity and then fine-tuned with external reward learns faster and achieves higher reward than an ICM agent trained from scratch to jointly maximize curiosity and the external rewards.,4.3. Generalization to Novel Scenarios,[0],[0]
This result confirms that the learned exploratory behavior is also useful when the agent is required to achieve goals specified by the environment.,4.3. Generalization to Novel Scenarios,[0],[0]
It is also worth noting that ICM-pixels does not generalize to this test environment.,4.3. Generalization to Novel Scenarios,[0],[0]
This indicates that the proposed mechanism of measuring curiosity is significantly better for learning skills that generalize as compared to measuring curiosity in the raw sensory space.,4.3. Generalization to Novel Scenarios,[0],[0]
"Curiosity-driven exploration is a well studied topic in the reinforcement learning literature and a good summary can be found in (Oudeyer & Kaplan, 2009; Oudeyer et al., 2007).",5. Related Work,[0],[0]
Schmidhuber (1991; 2010) and Sun et al. (2011) use surprise and compression progress as intrinsic rewards.,5. Related Work,[0],[0]
Classic work of Kearns et al. (1999) and Brafman et al. (2002) propose exploration algorithms polynomial in the number of state space parameters.,5. Related Work,[0],[0]
"Others have used empowerment, which is the information gain based on entropy of actions, as intrinsic rewards (Klyubin et al., 2005; Mohamed & Rezende, 2015).",5. Related Work,[0],[0]
Stadie et al. (2015) use prediction error in the feature space of an auto-encoder as a measure of interesting states to explore.,5. Related Work,[0],[0]
"State visitation counts have also been investigated for exploration (Bellemare et al., 2016; Oh et al., 2015; Tang et al., 2016).",5. Related Work,[0],[0]
"Osband et al. (2016) train multiple value functions and makes
use of bootstrapping and Thompson sampling for exploration.",5. Related Work,[0],[0]
"Many approaches measure information gain for exploration (Little & Sommer, 2014; Still & Precup, 2012; Storck et al., 1995).",5. Related Work,[0],[0]
Houthooft et al. (2016) use an exploration strategy that maximizes information gain about the agent’s belief of the environment’s dynamics.,5. Related Work,[0],[0]
"Our approach of jointly training forward and inverse models for learning a feature space has similarities to (Agrawal et al., 2016; Jordan & Rumelhart, 1992; Wolpert et al., 1995), but these works use the learned models of dynamics for planning a sequence of actions instead of exploration.",5. Related Work,[0],[0]
"The idea of using a proxy task to learn a semantic feature embedding has been used in a number of works on self-supervised learning in computer vision (Agrawal et al., 2015; Doersch et al., 2015; Goroshin et al., 2015; Jayaraman & Grauman, 2015; Pathak et al., 2016; Wang & Gupta, 2015).
",5. Related Work,[0],[0]
Concurrent work: A number of interesting related papers have appeared on Arxiv while the present work was in submission.,5. Related Work,[0],[0]
Sukhbaatar et al. (2017) generates supervision for pre-training via asymmetric self-play between two agents to improve data efficiency during fine-tuning.,5. Related Work,[0],[0]
"Several methods propose improving data efficiency of RL algorithms using self-supervised prediction based auxiliary tasks (Jaderberg et al., 2017; Shelhamer et al., 2017).",5. Related Work,[0],[0]
"Fu et al. (2017) learn discriminative models, and Gregor et al. (2017) use empowerment based measure to tackle exploration in sparse reward setups.",5. Related Work,[0],[0]
"In this work we propose a mechanism for generating curiosity-driven intrinsic reward signal that scales to high dimensional visual inputs, bypasses the difficult problem of predicting pixels and ensures that the exploration strategy of the agent is unaffected by nuisance factors in the environment.",6. Discussion,[0],[0]
"We demonstrate that our agent significantly outperforms the baseline A3C with no curiosity, a recently proposed VIME (Houthooft et al., 2016) formulation for exploration, and a baseline pixel-predicting formulation.
",6. Discussion,[0],[0]
In VizDoom our agent learns the exploration behavior of moving along corridors and across rooms without any rewards from the environment.,6. Discussion,[0],[0]
In Mario our agent crosses more than 30% of Level-1 without any rewards from the game.,6. Discussion,[0],[0]
One reason why our agent is unable to go beyond this limit is the presence of a pit at 38% of the game that requires a very specific sequence of 15-20 key presses in order to jump across it.,6. Discussion,[0],[0]
"If the agent is unable to execute this sequence, it falls in the pit and dies, receiving no further rewards from the environment.",6. Discussion,[0],[0]
Therefore it receives no gradient information indicating that there is a world beyond the pit that could potentially be explored.,6. Discussion,[0],[0]
"This issue is somewhat orthogonal to developing models of curiosity, but presents a challenging problem for policy learning.
",6. Discussion,[0],[0]
It is common practice to evaluate reinforcement learning approaches in the same environment that was used for training.,6. Discussion,[0],[0]
"However, we feel that it is also important to evaluate on a separate “testing set” as well.",6. Discussion,[0],[0]
"This allows us to gauge how much of what has been learned is specific to the training environment (i.e. memorized), and how much might constitute “generalizable skills” that could be applied to new settings.",6. Discussion,[0],[0]
"In this paper, we evaluate generalization in two ways: 1) by applying the learned policy to a new scenario “as is” (no further learning), and 2) by finetuning the learned policy on a new scenario (we borrow the pre-training/fine-tuning nomenclature from the deep feature learning literature).",6. Discussion,[0],[0]
We believe that evaluating generalization is a valuable tool and will allow the community to better understand the performance of various reinforcement learning algorithms.,6. Discussion,[0],[0]
"To further aid in this effort, we will make the code for our algorithm, as well as testing and environment setups freely available online.
",6. Discussion,[0],[0]
"An interesting direction of future research is to use the learned exploration behavior/skill as a motor primitive/lowlevel policy in a more complex, hierarchical system.",6. Discussion,[0],[0]
"For example, our VizDoom agent learns to walk along corridors instead of bumping into walls.",6. Discussion,[0],[0]
"This could be a useful primitive for a navigation system.
",6. Discussion,[0],[0]
"While the rich and diverse real world provides ample opportunities for interaction, reward signals are sparse.",6. Discussion,[0],[0]
Our approach excels in this setting and converts unexpected interactions that affect the agent into intrinsic rewards.,6. Discussion,[0],[0]
However our approach does not directly extend to the scenarios where “opportunities for interactions” are also rare.,6. Discussion,[0],[0]
"In theory, one could save such events in a replay memory and use them to guide exploration.",6. Discussion,[0],[0]
"However, we leave this extension for future work.
",6. Discussion,[0],[0]
"Acknowledgements: We would like to thank Sergey Levine, Evan Shelhamer, Saurabh Gupta, Phillip Isola and other members of the BAIR lab for fruitful discussions and comments.",6. Discussion,[0],[0]
We thank Jacob Huh for help with Figure 2 and Alexey Dosovitskiy for VizDoom maps.,6. Discussion,[0],[0]
"This work was supported in part by NSF IIS-1212798, IIS-1427425, IIS1536003, IIS-1633310, ONR MURI N00014-14-1-0671, Berkeley DeepDrive, equipment grant from Nvidia, and the Valrhona Reinforcement Learning Fellowship.",6. Discussion,[0],[0]
"In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether.",abstractText,[0],[0]
"In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life.",abstractText,[0],[0]
We formulate curiosity as the error in an agent’s ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model.,abstractText,[0],[0]
"Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent.",abstractText,[0],[0]
"The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch.",abstractText,[0],[0]
Curiosity-driven Exploration by Self-supervised Prediction,title,[0],[0]
"Biological organisms can learn to perform tasks (and often do) by observing a sequence of labeled events, just like supervised machine learning.",1. Introduction,[0],[0]
"But unlike machine learning, in human learning supervision is often accompanied by a curriculum.",1. Introduction,[0],[0]
Thus the order of presented examples is rarely random when a human teacher teaches another human.,1. Introduction,[0],[0]
"Likewise, the task may be divided by the teacher into smaller sub-tasks, a process sometimes called shaping (Krueger & Dayan, 2009) and typically studied in the context of rein-
1School of Computer Science and Engineering, The Hebrew University of Jerusalem, Jerusalem 91904, Israel.",1. Introduction,[0],[0]
"Correspondence to: Daphna Weinshall <daphna@mail.huji.ac.il>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"forcement learning (e.g. Graves et al., 2017).",1. Introduction,[0],[0]
"Although it remained for the most part in the fringes of machine learning research, curriculum learning has been identified as a key challenge for machine learning throughout (e.g., Mitchell, 1980; 2006; Wang & Cottrell, 2015).
",1. Introduction,[0],[0]
"We focus here on curriculum learning based on ranking (or weighting as in (Bengio et al., 2009)) of the training examples, which is used to guide the order of presentation of examples to the learner.",1. Introduction,[0],[0]
"Risking over simplification, the idea is to first present the learner primarily with examples of higher weight or rank, later to be followed by examples with lower weight or rank.",1. Introduction,[0],[0]
"Ranking may be based on the difficulty of each training example as evaluated by the teacher, from easiest to the most difficult.
",1. Introduction,[0],[0]
"In Section 2 we investigate this strict definition of curriculum learning theoretically, in the context of stochastic gradient descent used to optimize the convex linear regression loss function.",1. Introduction,[0],[0]
We first define the (ideal) difficulty of a training point as its loss with respect to the optimal classifier.,1. Introduction,[0],[0]
"We then prove that curriculum learning, when given the ranking of training points by their difficulty thus defined, is expected (probabilistically) to significantly speed up learning especially at the beginning of training.",1. Introduction,[0],[0]
"This theoretical result is supported by empirical evidence obtained in the deep learning scenario of curriculum learning described in Section 3, where similar behavior is observed.",1. Introduction,[0],[0]
"We also show that when the difficulty of the sampled training points is fixed, convergence is faster when sampling points that incur higher loss with respect to the current hypothesis as suggested in (Shrivastava et al., 2016).",1. Introduction,[0],[0]
"This result is not always true when the difficulty of the sampled training points is not fixed.
",1. Introduction,[0],[0]
But such ideal ranking is rarely available.,1. Introduction,[0],[0]
"In fact, the need for such supervision has rendered curriculum learning less useful in machine learning, since ranking by difficulty is hard to obtain.",1. Introduction,[0],[0]
"Moreover, even when it is provided by a human teacher, it may not reflect the true difficulty as it affects the machine learner.",1. Introduction,[0],[0]
"For example, in visual object recognition it has been demonstrated that what makes an image difficult to a neural network classifier may not always match whatever makes it difficult to a human observer, an observation that has been taken advantage of in the recent
work on adversarial examples (Szegedy et al., 2013).",1. Introduction,[0],[0]
"Possibly, this is one of the reasons why curriculum learning is rarely used in practice (but see, e.g., Zaremba & Sutskever, 2014; Amodei et al., 2016; Jesson et al., 2017).
",1. Introduction,[0],[0]
In the second part of this paper we focus on this question - how to rank (or weight) the training examples without the aid of a human teacher.,1. Introduction,[0],[0]
"This is paramount when a human teacher cannot provide a reliable difficulty score for the task at hand, or when obtaining such a score by human teachers is too costly.",1. Introduction,[0],[0]
This question is also closely related to transfer learning: here we investigate the use of another classifier to provide the ranking of the training examples by their presumed difficulty.,1. Introduction,[0],[0]
"This form of transfer should not be confused with the notion of transfer discussed in (Bengio et al., 2009) in the context of multi-task and lifelong learning (Thrun & Pratt, 2012), where knowledge is transferred from earlier tasks (e.g. the discrimination of easy examples) to later tasks (e.g. the discrimination of difficult examples).",1. Introduction,[0],[0]
"Rather, we investigate the transfer of knowledge from one classifier to another, as in teacher classifier to student classifier.",1. Introduction,[0],[0]
"In this form curriculum learning has not been studied in the context of deep learning, and hardly ever in the context of other classification paradigms.
",1. Introduction,[0],[0]
"Differently from previous work, it is not the instance representation which is being transferred but rather the ranking of training examples.",1. Introduction,[0],[0]
Why is this a good idea?,1. Introduction,[0],[0]
"This kind of transfer assumes that a powerful pre-trained network is only available at train time, and cannot be used at test time even for the computation of a test point’s representation.",1. Introduction,[0],[0]
"This may be the case, for example, when the powerful network is too big to run on the target device.",1. Introduction,[0],[0]
"One can no longer expect to have access to the transferred representation at test time, while ranking can be used at train time in order to improve the learning of the target smaller network (see related discussion of network compression in (Chen et al., 2015; Kim et al., 2015), for example).
",1. Introduction,[0],[0]
"In Section 3 we describe our method, an algorithm which uses the ranking to construct a schedule for the order of presentation of training examples.",1. Introduction,[0],[0]
"In subsequent empirical evaluations we compare the performance of the method when using a curriculum which is based on different scheduling options, including 2 control conditions where difficult examples are presented first or when using arbitrary scheduling.",1. Introduction,[0],[0]
"The main results of this empirical study can be summarized as follows: (i) Learning rate is always faster with curriculum learning, especially at the beginning of training.",1. Introduction,[0],[0]
"(ii) Final generalization is sometimes improved with curriculum learning, especially when the conditions for learning are hard: the task is difficult, the network is small, or when strong regularization is enforced.",1. Introduction,[0],[0]
"These results are consistent with prior art (see e.g. Bengio et al., 2009).",1. Introduction,[0],[0]
"We start with some notations in Section 2.1, followed in Sections 2.2 by the rigorous analysis of curriculum learning when used to optimize the linear regression loss.",2. Theoretical analysis,[0],[0]
"In Section 2.3 we report supporting empirical evidence for the main theoretical results, obtained using the deep learning setup described later in Section 3.",2. Theoretical analysis,[0],[0]
"Let X = {(xi, yi)}ni=1 denote the training data, where xi ∈ Rd denotes the i-th data point and yi its corresponding label.",2.1. Notations and definitions,[0],[0]
"In general, the goal is to find a hypothesis h̄(x) ∈ H that minimizes the risk function (the expected loss).",2.1. Notations and definitions,[0],[0]
"In order to minimize this objective, Stochastic Gradient Descent (SGD) is often used with various extensions and regularization.
",2.1. Notations and definitions,[0],[0]
"We start with two definitions:
Definition 1 (Ideal Difficulty Score).",2.1. Notations and definitions,[0],[0]
"The difficulty of point x is measured by its minimal loss with respect to the set of optimal hypotheses {L(h̄(xi), yi)}.",2.1. Notations and definitions,[0],[0]
Definition 2 (Stochastic Curriculum Learning).,2.1. Notations and definitions,[0],[0]
"SCL is a variation on Stochastic Gradient Descent (SGD), where the learner is exposed to the data gradually based on the difficulty score of the training points.
",2.1. Notations and definitions,[0],[0]
"In vanilla SGD training, at each iteration the learner is presented with a new datapoint (or mini-batch) sampled from the training data based on some probability function D(X).",2.1. Notations and definitions,[0],[0]
"In SCL, the sampling is biased to favor easier examples at the beginning of the training.",2.1. Notations and definitions,[0],[0]
This bias is decreased following some scheduling procedure.,2.1. Notations and definitions,[0],[0]
"At the end of training, points are sampled according to D(X) as in vanilla SGD.
",2.1. Notations and definitions,[0],[0]
"In practice, an SCL algorithm should solve two problems: (i) Score the training points by difficulty; in prior art this score was typically provided by the teacher in a supervised manner.",2.1. Notations and definitions,[0],[0]
(ii) Define the scheduling procedure.,2.1. Notations and definitions,[0],[0]
Here we analyze SCL when used to minimize the linear regression model.,2.2. The linear regression loss,[0],[0]
"Specifically, we investigate the differential effect of a point’s Difficulty Score on convergence towards the global minimum of the expected least squares loss, when the family of hypotheses H includes the linear functions h(x) = atx + b and y ∈ R.
",2.2. The linear regression loss,[0],[0]
"The risk function of the regression model is the following
R(X,w) = ED(X)L(hw(x), y) L(hw(xi), yi) =",2.2. The linear regression loss,[0],[0]
"(h(xi)− yi)2 = (atxi + b− yi)2
, (xtiw",2.2. The linear regression loss,[0],[0]
"− yi)2 , L(Xi,w) (1)
",2.2. The linear regression loss,[0],[0]
"In the last transition above, w =",2.2. The linear regression loss,[0],[0]
"[
a b
] ∈ Rd+1.",2.2. The linear regression loss,[0],[0]
"With some
abuse of notation, xi denotes the vector [ xi 1 ] .",2.2. The linear regression loss,[0],[0]
Xi denotes the vector,2.2. The linear regression loss,[0],[0]
"[xi, yi], with Difficulty Score L(Xi, w̄).
",2.2. The linear regression loss,[0],[0]
"In general the output hypothesis hw(x) = xtiw is determined by minimizing R(X,w) with respect to w. The global minimum w̄ of the empirical loss can be computed in closed form from the training data.",2.2. The linear regression loss,[0],[0]
"However, gradient descent can be used to find w̄ with guaranteed convergence, which is efficient when n is very large.
",2.2. The linear regression loss,[0],[0]
Recall that SCL computes a sequence of estimators {wt}Tt=1 for the parameters of the optimal hypothesis w̄.,2.2. The linear regression loss,[0],[0]
"This is based on a sequence of training points {Xt = [xt, yt]}Tt=1, sampled from the training data while favoring easy points at the beginning of training.",2.2. The linear regression loss,[0],[0]
"Other than sampling probability, the update step at time t follows SGD:
wt+1 =",2.2. The linear regression loss,[0],[0]
"wt − η ∂L(Xt,w)
∂w",2.2. The linear regression loss,[0],[0]
|w,2.2. The linear regression loss,[0],[0]
=wt (2),2.2. The linear regression loss,[0],[0]
The main theorem in this sub-section states that the expected rate of convergence of gradient descent is monotonically decreasing with the Difficulty Score of the sample,CONVERGENCE RATE DECREASES WITH DIFFICULTY,[0],[0]
Xt.,CONVERGENCE RATE DECREASES WITH DIFFICULTY,[0],[0]
We prove it below for the gradient step as defined in (2).,CONVERGENCE RATE DECREASES WITH DIFFICULTY,[0],[0]
"If the size of the gradient step is fixed at η, a somewhat stronger theorem can be obtained where the constraint on the step size being small is not required.
",CONVERGENCE RATE DECREASES WITH DIFFICULTY,[0],[0]
"We first derive the gradient step at time t:
s = −η ∂L(Xi,w) ∂w",CONVERGENCE RATE DECREASES WITH DIFFICULTY,[0],[0]
= −2η(xtiw,CONVERGENCE RATE DECREASES WITH DIFFICULTY,[0],[0]
"− yi)xi (3)
",CONVERGENCE RATE DECREASES WITH DIFFICULTY,[0],[0]
"Let Ωi denote the hyperplane on which this gradient vanishes ∂L(Xi,w)∂w = 0.",CONVERGENCE RATE DECREASES WITH DIFFICULTY,[0],[0]
"This hyperplane is defined by x t iw = yi, namely, xi defines its normal direction.",CONVERGENCE RATE DECREASES WITH DIFFICULTY,[0],[0]
Thus (3) implies that the gradient step at time t is perpendicular to Ωi.,CONVERGENCE RATE DECREASES WITH DIFFICULTY,[0],[0]
Let z̄ denote the projection of w̄ on Ωi.,CONVERGENCE RATE DECREASES WITH DIFFICULTY,[0],[0]
"Let Ψ2 = L(Xi, w̄) denote the Difficulty Score of Xi.
",CONVERGENCE RATE DECREASES WITH DIFFICULTY,[0],[0]
Lemma 1.,CONVERGENCE RATE DECREASES WITH DIFFICULTY,[0],[0]
Fix the training point Xi.,CONVERGENCE RATE DECREASES WITH DIFFICULTY,[0],[0]
The Difficulty Score of Xi is Ψ2 = r2‖w̄,CONVERGENCE RATE DECREASES WITH DIFFICULTY,[0],[0]
− z̄‖2.,CONVERGENCE RATE DECREASES WITH DIFFICULTY,[0],[0]
"Ψ2 = L(Xi, w̄) = L(Xi, z̄ + (w̄ − z̄))",Proof.,[0],[0]
=,Proof.,[0],[0]
[xtiz̄ + x t i(w̄,Proof.,[0],[0]
"− z̄)− yi]2
=",Proof.,[0],[0]
"[xti(w̄ − z̄)]2 = ‖xi‖2‖w̄ − z̄‖2 (4)
",Proof.,[0],[0]
"Recall that xi,w ∈ Rd+1.",Proof.,[0],[0]
"We continue the analysis in the parameter space w ∈ Rd+1, where parameter vector w corresponds to a point, and data vector xi describes a hyperplane.",Proof.,[0],[0]
"In this space we represent each vector
xi in a hyperspherical coordinate system [r, ϑ,Φ], with pole (origin) fixed at w̄ and polar axis (zenith direction) ~O = w̄",Proof.,[0],[0]
− wt (see Fig. 1).,Proof.,[0],[0]
"r denotes the vector’s length, while 0 ≤ ϑ ≤ π denotes the polar angle with respect to ~O.",Proof.,[0],[0]
Let Φ = [ϕ1 . . .,Proof.,[0],[0]
", ϕd−1] denote the remaining polar angles.
",Proof.,[0],[0]
"To illustrate, Fig. 1 shows a planar section of the parameter space, the 2D plane formed by the two intersecting lines ~O and z̄− w̄.",Proof.,[0],[0]
The gradient step s points from wt towards Ωi.,Proof.,[0],[0]
"Ωi is perpendicular to xi, which is parallel to z̄− w̄ and to s, and therefore Ωi is projected onto a line in this plane.",Proof.,[0],[0]
"We introduce the notation λ = ‖w̄ −wt‖.
Let sO denote the projection of the gradient vector s on the polar axis ~O, and let s⊥ denote the perpendicular component.",Proof.,[0],[0]
"From (3) and the definition of Ψ
s = −2ηxi(xtiwt",Proof.,[0],[0]
− yi) =,Proof.,[0],[0]
−2ηxi[xti(wt − w̄)±Ψ] sO = s · w̄,Proof.,[0],[0]
"−wt
λ = 2
η λ",Proof.,[0],[0]
[r2λ2 cos2 ϑ∓Ψrλ,Proof.,[0],[0]
"cosϑ]
(5)
",Proof.,[0],[0]
"Let x = (r, ϑ,Φ).",Proof.,[0],[0]
"Let fD(X) = f(r, ϑ,Φ)fY (|y − xtw̄|) denote the density function of the data X.",Proof.,[0],[0]
"This choice assumes that the density of the label y only depends on the absolute error |y − xtw̄|.
",Proof.,[0],[0]
"For the subsequent derivations we need the conditional distribution of the data X given difficulty score Ψ. Fixing the difficulty score determines one of two labels y(x,Ψ) = xtw̄",Proof.,[0],[0]
± Ψ.,Proof.,[0],[0]
"We further assume that both labels are equally likely1, and therefore fD(X)/Ψ ([x, y]) = 12f(r, ϑ,Φ).
",Proof.,[0],[0]
"Let ∆(Ψ) denote the expected convergence rate at time t, given fixed difficulty score Ψ.
∆(Ψ) =",Proof.,[0],[0]
E[‖wt − w̄‖2 − ‖wt+1,Proof.,[0],[0]
"− w̄‖2/Ψ] (6)
Lemma 2.
∆(Ψ) = 2λE[sO/Ψ]− E[s 2/Ψ] (7) 1This assumption can be somewhat relaxed, but the strict form is used to simplify the exposition.
",Proof.,[0],[0]
Proof.,Proof.,[0],[0]
"From (6)
E[∆] = (−λ)2 − E[(−λ+ sO)2 + s2⊥] = λ2 − (λ2 − 2λE[sO] + E[s2O])− E[s2⊥] = 2λE[sO]− E[s2]
From Lemma 2 and (5)2
1 4 ∆(Ψ) = ηE[r2λ2 cos2 ϑ]− η2E[r4λ2 cos2 ϑ]
−η2Ψ2E[r2] (8) −ηE[(±Ψ)rλ cosϑ]− 2η2E[(±Ψ)r3λ cosϑ]
Lemma 3.
E[(±Ψ)rλ cosϑ] = E[(±Ψ)r3λ cosϑ] = 0
Proof.",Proof.,[0],[0]
"The lemma follows from the assumed symmetry of D(X) with respect to the sign of yi − xtiw̄.
",Proof.,[0],[0]
"It follows from Lemma 3 that 1
4 ∆(Ψ) =ηE[r2λ2 cos2 ϑ]− η2E[r4λ2 cos2 ϑ]
− η2Ψ2E[r2] (9)
We can now state the main theorem of this section.",Proof.,[0],[0]
Theorem 1.,Proof.,[0],[0]
At time t the expected convergence rate for training point x is monotonically decreasing with the Difficulty Score Ψ of x.,Proof.,[0],[0]
If the step size coefficient is sufficiently small so that η ≤,Proof.,[0],[0]
"E[r
2 cos2 ϑ] E[r4 cos2 ϑ] , it is likewise monotonically
increasing with the distance λ between the current estimate of the hypothesis wt and the optimal hypothesis w̄.
Proof.",Proof.,[0],[0]
"From (9)
∂∆(Ψ)
∂Ψ = −8η2E[r2]Ψ ≤ 0
which proves the first statement.",Proof.,[0],[0]
"In addition,
∂∆(Ψ)
∂λ = 8ηλ
( E[r2 cos2 ϑ]− ηE[r4 cos2 ϑ] )",Proof.,[0],[0]
"If η ≤ E[r
2 cos2 ϑ] E[r4 cos2 ϑ] then ∂∆(Ψ) ∂λ ≥ 0, and the second statement
follows.
",Proof.,[0],[0]
Corollary 1.,Proof.,[0],[0]
"Although E[∆(Ψ)] may be negative, wt always converges faster to w̄ when the training points are sampled from easier examples with smaller Ψ. Corollary 2.",Proof.,[0],[0]
If the step size coefficient η is small enough so that η ≤,Proof.,[0],[0]
"E[r
2 cos2 ϑ] E[r4 cos2 ϑ] , we should expect faster convergence
at the beginning of SCL.",Proof.,[0],[0]
"2Below the short-hand notation E[(±Ψ)] implies that the 2 cases of y(x,Ψ) = xtw̄",Proof.,[0],[0]
"± Ψ should be considered, with equal probability 1
2 by assumption.",Proof.,[0],[0]
"The main theorem in this sub-section states that for a fixed difficulty score Ψ, when the gradient step is small enough, convergence is monotonically increasing with the loss of the point with respect to the current hypothesis.",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
This is not true in general.,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"The second theorem in this section shows that when the difficulty score is not fixed, there exist hypotheses w ∈ H for which the convergence rate is decreasing with the current loss.
",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"Let Υ2 = L(Xi,wt) denote the loss of Xi with respect to the current hypothesis wt.",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
Define the angle β ∈,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"[0, π2 ) as follows (see Fig. 1)
β = β(r,Ψ, λ) = arccos(min",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"( Ψ
λr , 1))",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"(10)
Lemma 4.",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"The relation between Υ,Ψ, r, ϑ can be written separately in 4 regions as follows (see Fig. 1):
A1 0 ≤ ϑ ≤",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"π−β, yi = xtiw̄+ Ψ =⇒ yi",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"= xtiwt+ Υ, λr cosϑ = xti(w̄ −wt) = −Ψ",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"+ Υ
A2 π−β ≤ ϑ ≤ π, yi = xtiw̄+Ψ =⇒ yi = xtiwt−Υ, λr cosϑ = −Ψ−Υ
A3 0 ≤ ϑ ≤ β, yi",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
= xtiw̄ −Ψ =⇒ yi = xtiwt,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"+ Υ, λr cosϑ = Ψ + Υ
A4 β ≤ ϑ ≤ π,",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"yi = xtiw̄ −Ψ =⇒ yi = xtiwt −Υ, λr cosϑ = Ψ−Υ
Proof.",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"We keep in mind that ∀xi and Ψ, there are 2 possible yi with equal probability.",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
Recall that z̄ denotes the projection of w̄ on Ωi.,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"In the planar section shown in Fig. 1,
z̄ lies in the upper half space ⇐⇒ yi =",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"xtiw̄ + Ψ
z̄ lies in the lower half space ⇐⇒ yi =",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"xtiw̄ −Ψ
",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"This follows from 3 observations: x̄i lies in the upper half space by the definition of the polar coordinate system, xtiw̄",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"− yi = ±Ψ, and
0 = xtiz̄− yi = xti(z̄− w̄) +",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
xtiw̄,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"− yi
Next, let zt denote the projection of wt on Ωi.",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"Then
0 = xtizt",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
− yi = xti(zt −wt) +,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
xtiwt,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"− yi
When z̄ lies in the upper half space, the following can be verified geometrically from Fig. 1:
0 ≤ ϑ ≤ π−β ⇒ xti(zt−wt)",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
≥ 0 ⇒ yi = xtiwt,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
+,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"Υ
π−β ≤ ϑ ≤ π ⇒",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"xti(zt−wt) ≤ 0 ⇒ yi = xtiwt−Υ
When z̄ lies in the lower half space
0 ≤ ϑ ≤ β =⇒ xti(zt −wt) ≥ 0",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"=⇒ yi = xtiwt + Υ
β ≤ ϑ ≤ π =⇒ xti(zt −wt) ≤ 0",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"=⇒ yi = xtiwt −Υ
Next we analyze how the convergence rate at xi changes with Υ. Let ∆(Ψ,Υ) denote the expected convergence rate at time t, given fixed difficulty score Ψ and fixed loss Υ. From (9) ∆(Ψ,Υ)",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"= 4ηE[r2λ2 cos2 ϑ/Υ] +O(η 2).
",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"It is easier to analyze ∆(Ψ,Υ) when using the Cartesian coordinates, rather than polar, in the 2D plane defined by the vectors ~O = w̄−wt and z̄− w̄",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"(see Fig. 1); thus we define u = r cosϑ, v = r sinϑ.",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"The 4 cases listed in Lemma 4 can be readily transformed to this coordinate system as follows {0 ≤ ϑ ≤ β} ⇔ {λu ≥ Ψ}, {β ≤ ϑ ≤ π",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"− β} ⇔ {−Ψ ≤ λu ≤ Ψ}, and {π − β ≤ ϑ ≤ π} ⇔ {λu ≤ −Ψ}:
A1 λu ≥ −Ψ : λu = −Ψ + Υ
A2 λu ≤ −Ψ",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
": λu = −Ψ−Υ
A3 λu ≥ Ψ : λu = Ψ + Υ
A4 λu ≤ Ψ : λu = Ψ−Υ
Define
∇ = f(ψ+Υλ )",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
− f( ψ−Υ λ ),CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"− f( −ψ+Υ λ ) + f( −ψ−Υ λ )
f(ψ+Υλ )",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
+,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
f( ψ−Υ λ ) +,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
f( −ψ+Υ λ ) +,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"f( −ψ−Υ λ )
",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
Clearly −1 ≤ ∇ ≤ 1.,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
Theorem 2.,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"Assume that the gradient step size is small enough so that we can neglect second order terms O(η2), and",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
that ∂∇∂Υ ≥ ψ Υ,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"− Υ ψ ∀Υ. Fix the difficulty score at Ψ. At time t the expected convergence rate is monotonically increasing with the loss Υ of the training point x.
Proof.",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"In the coordinate system defined above ∆(Ψ,Υ) = 4ηE[λ2u2/Υ]",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"+ O(η
2)",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
.,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"We compute ∆(Ψ,Υ) separately in each region, marginalizing out v based on the following∫ ∫ ∞
0
λ2u2vd−1f(u, v)dvdu = ∫",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"λ2u2f(u)du
where f(u) denotes the marginal distribution of u.
Let ui denote the value of u corresponding to loss Υ in each region A1-A4, and 12f(ui) its density.",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"∆(Ψ,Υ) takes 4 discrete values, one in each region, and its expected value is therefore ∆(Ψ,Υ) =",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
4η ∑4 i=1,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
λ 2u2i f(ui)∑4 i=1,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"f(ui)
.",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"It can readily be shown that
1
4η ∆(ψ,Υ) = ψ2 +",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"Υ2 + 2ψΥ∇ (11)
and subsequently
1
4η
∂∆(ψ,Υ)
∂Υ",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
= 2Υ + 2ψΥ ∂∇ ∂Υ,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"+ 2ψ ∇
≥ 2Υ + 2ψΥ ∂∇ ∂Υ",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"− 2ψ
(12)
",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
Using the assumption that ∂∇∂Υ ≥ ψ Υ,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"− Υ ψ ∀Υ, we have that
1
8η
∂∆(ψ,Υ) ∂Υ",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
≥ Υ + ψΥ ψ,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
−Υ ψΥ,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
− ψ,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"= 0
Corollary 3.",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"For any c ∈ R+, if∇ is (c− 1c )-lipschitz",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"then ∂∆(ψ,Υ) ∂Υ",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
≥ 0 for any Υ ≥ c ψ.,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
Corollary 4.,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"If D(X/Ψ) = k(Ψ) over a compact region and η small enough, then ∂∆(ψ,Υ)∂Υ ≥ 0 for all Υ excluding the boundaries of the compact region.",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"If in addition Υ > Ψ, then ∂∆(ψ,Υ)∂Υ ≥ 0 almost surely.",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
Theorem 3.,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
Assume that D(X) is continuous and that w̄ is realizable.,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"Then there are always hypotheses w ∈ H for which the expected convergence rate under D(X) is monotonically decreasing with the loss Υ of the sampled points.
",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
Proof.,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"We shift to a hyperspherical coordinate system in Rd+1 similar as before, but now the pole (origin) is fixed at wt.",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"For the gradient step s, it can be shown that:
s = − sgn (xtiwt",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"− yi)2ηxiΥ
sO = s · w̄ −wt",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"λ = ±2η λ rλ cosϑ Υ
(13)
Let ∆(Υ) denote the expected convergence rate at time t, given a fixed loss Υ. From Lemma 2
∆(Υ) = 2ηΥ",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"( E[r cosϑ/xtiwt − yi = −Υ ]−
E[r cosϑ/xtiwt − yi = Υ ]
)",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"− E[(2ηrΥ)2]
, 2ηΥQ(r, ϑ,wt)− 4η2Υ2E[r2]
If w = w̄, then Q(r, ϑ,w) = 0 from the symmetry of D(X) with respect to Ψ. From the continuity of D(X), there exists δ > 0",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"such that if ‖w − w̄‖2 < δ, then ‖Q(r, ϑ,w)−Q(r, ϑ, w̄)‖2 < ηΥE[r2], which implies that ∆(Υ) <",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
−2η2Υ2E[r2] < 0.,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"While the corollaries above apply to a rather simple situation, when using the Difficulty Score to guide SGD while
minimizing the convex regression loss, their predictions can be empirically tested with the deep learning architecture and loss which are described in Section 3.",2.3. Deep learning: simulation results,[0],[0]
"There an additional challenge is posed by the fact that the empirical ranking is not based on the ideal definition given in Def. 1, but rather on an estimate derived from another classifier.
",2.3. Deep learning: simulation results,[0],[0]
"Still, the empirical results as shown in Fig. 2 demonstrate agreement with the theoretical analysis of the linear regression loss.",2.3. Deep learning: simulation results,[0],[0]
"Specifically, in epoch 0",2.3. Deep learning: simulation results,[0],[0]
"there is a big difference between the average errors in estimating the gradient direction, which is smallest for the easiest examples and highest for the most difficult examples as predicted by Corollary 1.",2.3. Deep learning: simulation results,[0],[0]
"This difference in significantly reduced after 10 epochs, and becomes insignificant after 20 epochs, in agreement with Corollary 2.
",2.3. Deep learning: simulation results,[0],[0]
Discussion.,2.3. Deep learning: simulation results,[0],[0]
"Fig. 2 shows that the variance in the direction of the gradient step defined by easier points is significantly smaller than that defined by difficult points, especially at the beginning of training.",2.3. Deep learning: simulation results,[0],[0]
"This is advantageous when the initial point w0 does not lie in the basin of attraction of the desired global minimum w̄, and if, in agreement with Lemma 1, the pronounced shared component of the easy gradient steps points in the direction of the global minimum, or a more favorable local minimum; then the likelihood of escaping the local minimum decreases with a point’s Difficulty Score.",2.3. Deep learning: simulation results,[0],[0]
This scenario suggests another possible advantage for curriculum learning at the initial stages of training.,2.3. Deep learning: simulation results,[0],[0]
"As discussed in the introduction, a practical curriculum learning method should address two main questions: how to rank the training examples, and how to modify the sampling procedure based on this ranking.",3. Curriculum learning in deep networks,[0],[0]
Solutions to these issues are discussed in Section 3.1.,3. Curriculum learning in deep networks,[0],[0]
In Section 3.2 we discuss the empirical evaluation of our method.,3. Curriculum learning in deep networks,[0],[0]
"The main novelty of our proposed method lies in this step, where we rank the training examples by estimated difficulty in the absence of human supervision.",RANKING EXAMPLES BY KNOWLEDGE TRANSFER,[0],[0]
Difficulty is estimated based on knowledge transfer from another classifier.,RANKING EXAMPLES BY KNOWLEDGE TRANSFER,[0],[0]
"Here we investigate transfer from a more powerful learner.
",RANKING EXAMPLES BY KNOWLEDGE TRANSFER,[0],[0]
It is a common practice now to treat one of the upstream layers of a pre-trained network as a representation (or embedding) layer.,RANKING EXAMPLES BY KNOWLEDGE TRANSFER,[0],[0]
"This layer activation is then used for representing similar objects and train a simpler classifier (such as SVM, or shallower NNs) to perform a different task, related but not identical to the original task the network had been trained on.",RANKING EXAMPLES BY KNOWLEDGE TRANSFER,[0],[0]
"In computer vision such embeddings are commonly obtained by training a deep network on the recognition of a very large database such as ImageNet (Deng et al., 2009).",RANKING EXAMPLES BY KNOWLEDGE TRANSFER,[0],[0]
"These embeddings have been shown to provide better semantic representations of images (as compared to more traditional image features) in a number of related tasks, including the classification of small datasets (Sharif Razavian et al., 2014), image annotation (Donahue et al., 2015) and structured predictions (Hu et al., 2016).
",RANKING EXAMPLES BY KNOWLEDGE TRANSFER,[0],[0]
"Following this practice, the activation in the penultimate layer of a large and powerful pre-trained network is the loci of knowledge transfer from one network to another.",RANKING EXAMPLES BY KNOWLEDGE TRANSFER,[0],[0]
"Repeatedly, as in (Sharif Razavian et al., 2014), it has been shown that competitive performance can be obtained by training a shallow classifier on this representation in a new related task.",RANKING EXAMPLES BY KNOWLEDGE TRANSFER,[0],[0]
"Here we propose to use the confidence of such a classifier, e.g. the margin of an SVM classifier, as the estimator for the difficulty of each training example.",RANKING EXAMPLES BY KNOWLEDGE TRANSFER,[0],[0]
This measure is then used to sort the training data.,RANKING EXAMPLES BY KNOWLEDGE TRANSFER,[0],[0]
"We note that unlike the traditional practice of reusing a pre-trained network, here we only transfer information from one learner to another.",RANKING EXAMPLES BY KNOWLEDGE TRANSFER,[0],[0]
"The goal is to achieve a smaller classifier that can conceivably be used with simpler hardware, without depending on access to the powerful learner at test time.",RANKING EXAMPLES BY KNOWLEDGE TRANSFER,[0],[0]
"In agreement with prior art, e.g. the definition of curriculum in (Bengio et al., 2009), we investigate curriculum learning
where the scheduling of examples changes with time, giving priority to easier examples at the beginning of training.",SCHEDULING THE APPEARANCE OF TRAINING EXAMPLES,[0],[0]
"We explored two variants of the basic scheduling idea:
Fixed.",SCHEDULING THE APPEARANCE OF TRAINING EXAMPLES,[0],[0]
The distribution used to sample examples from the training data is gradually changed in fixed steps.,SCHEDULING THE APPEARANCE OF TRAINING EXAMPLES,[0],[0]
Initially all the weight is put on the easiest examples.,SCHEDULING THE APPEARANCE OF TRAINING EXAMPLES,[0],[0]
"In subsequent steps the weight of more difficult examples is gradually increased, until the final step in which the training data is sampled uniformly (or based on some prior distribution on the training set).
",SCHEDULING THE APPEARANCE OF TRAINING EXAMPLES,[0],[0]
Adaptive.,SCHEDULING THE APPEARANCE OF TRAINING EXAMPLES,[0],[0]
"Similar to the previous mode, but where the length of each step is not fixed, but is being determined adaptively based on the current loss of the training data.",SCHEDULING THE APPEARANCE OF TRAINING EXAMPLES,[0],[0]
Datasets.,EXPERIMENTAL SETUP,[0],[0]
"For evaluation we used 2 data sets: CIFAR-100 (Krizhevsky & Hinton, 2009) and STL-10 (Coates et al., 2010).",EXPERIMENTAL SETUP,[0],[0]
"In all cases, as is commonly done, the data was pre-processed using global contrast normalization; cropping and flipping were used for STL-10.
Network architecture.",EXPERIMENTAL SETUP,[0],[0]
We used convolutional Neural Networks (CNN) which excel at image classification tasks.,EXPERIMENTAL SETUP,[0],[0]
"Specifically, we used two architectures which are henceforth denoted Large and Small, in accordance with the number of parameters.",EXPERIMENTAL SETUP,[0],[0]
"The Large network is comprised of four blocks, each with two convolutional layers, ELU activation, and max-pooling.",EXPERIMENTAL SETUP,[0],[0]
"This is followed by a fully connected layer, for a total of 1,208,101 parameters.",EXPERIMENTAL SETUP,[0],[0]
"The Small network consists of only three hidden layers, for a total of 4,557 parameters.",EXPERIMENTAL SETUP,[0],[0]
"During training, we applied dropout and l2 regularization on the weights, and used either SGD or ADAM to optimize the cross-entropy loss.
",EXPERIMENTAL SETUP,[0],[0]
Scheduling mechanisms: control.,EXPERIMENTAL SETUP,[0],[0]
"As described above, our method is based on a scheduling design which favors the presentation of easier examples at the beginning of training.",EXPERIMENTAL SETUP,[0],[0]
"In order to isolate the contribution of scheduling by increasing level of difficulty as against other spurious consequences of data scheduling, we compared performance with the following control conditions: control-curriculum, identical scheduling mechanism but where the underlying ranking of the training examples is random and unrelated to estimated difficulty; and anti-curriculum, identical scheduling mechanism but favoring the more difficult examples at the beginning of training.",EXPERIMENTAL SETUP,[0],[0]
"Evidence from prior art is conflicting regarding where the benefits of curriculum learning lie, which is to be expected given the variability in the unknown sources of the curricu-
lum supervision information and its quality.",CONTROLLING FOR TASK DIFFICULTY,[0],[0]
We observed in our empirical study that the benefits depended to a large extent on the difficulty of the task.,CONTROLLING FOR TASK DIFFICULTY,[0],[0]
"We always saw faster learning at the beginning of the training process, while lower generalization error was seen only when the task was relatively difficult.",CONTROLLING FOR TASK DIFFICULTY,[0],[0]
"We therefore employed controls for the following 3 sources of task difficulty:
Inherent task difficulty.",CONTROLLING FOR TASK DIFFICULTY,[0],[0]
"To investigate this factor, we take advantage of the fact that CIFAR-100 is a hierarchical dataset with 100 classes and 20 super-classes, each including 5 member classes.",CONTROLLING FOR TASK DIFFICULTY,[0],[0]
We therefore trained a network to discriminate the 5 member classes of 2 super-classes as 2 separate tasks: ‘small mammals’ (task 1) and ‘aquatic mammals’ (task 2).,CONTROLLING FOR TASK DIFFICULTY,[0],[0]
These are expected to be relatively hard learning tasks.,CONTROLLING FOR TASK DIFFICULTY,[0],[0]
"We also trained a network to discriminate 5 random well separated classes: ‘camel’, ‘clock’, ‘bus’, ‘dolphin’ and ‘orchid’ (task 3).",CONTROLLING FOR TASK DIFFICULTY,[0],[0]
"This task is expected to be relatively easy.
",CONTROLLING FOR TASK DIFFICULTY,[0],[0]
Size of classification network.,CONTROLLING FOR TASK DIFFICULTY,[0],[0]
"For a given task, classification performance is significantly affected by the size of the network and its architecture.",CONTROLLING FOR TASK DIFFICULTY,[0],[0]
"We assume, of course, that we operate in the domain where the number of model parameters is smaller than can be justified by the training data (i.e., there is no overfit).",CONTROLLING FOR TASK DIFFICULTY,[0],[0]
We therefore used networks of different sizes in order to evaluate how curriculum learning is affected by task difficulty as determined by the network’s strength (see Fig. 3a-b).,CONTROLLING FOR TASK DIFFICULTY,[0],[0]
"In this comparative evaluation, the smaller the network is, the more difficult the task is likely to be (clearly, many other factors participate in the determination of task difficulty).
",CONTROLLING FOR TASK DIFFICULTY,[0],[0]
Regularization and optimization.,CONTROLLING FOR TASK DIFFICULTY,[0],[0]
"Regularization is used to constrain the family of hypotheses, or models, so that they possess such desirable properties as smoothness.",CONTROLLING FOR TASK DIFFICULTY,[0],[0]
Regularization effectively decreases the number of degrees of freedom in the model.,CONTROLLING FOR TASK DIFFICULTY,[0],[0]
"In fact, most optimization methods, other then vanilla stochastic gradient descent, incorporate some form of regularization and smoothing, among other inherent properties.",CONTROLLING FOR TASK DIFFICULTY,[0],[0]
Therefore the selection of optimization method also plays a role in determining the effective size of the final network.,CONTROLLING FOR TASK DIFFICULTY,[0],[0]
"Fig. 3a shows typical results when training the Large CNN (see network’s details above) to classify a subset of 5 CIFAR100 images (task 1 as defined above), using slow learning rate and Adam optimization.",RESULTS,[0],[0]
"In this setup we see that curriculum learning speeds up the learning rate at the beginning of the training, but converges to the same performance as regular training.",RESULTS,[0],[0]
"When we make learning more difficulty by using the Small network, performance naturally decreases, but now we see that curriculum learning also improves the final generalization performance (Fig. 3b).
",RESULTS,[0],[0]
"Similar results are shown for the STL-10 dataset (Fig. 3c).
",RESULTS,[0],[0]
"Fig. 4 shows comparative results when controlling for inherent task difficulty in the 3 tasks described above, using faster learning rate and SGD optimization.",RESULTS,[0],[0]
Task difficulty can be evaluated in retrospect from the final performance seen in each plot.,RESULTS,[0],[0]
"As can be clearly seen in the figure, the improvement in final accuracy with curriculum learning is larger when the task is more difficult.",RESULTS,[0],[0]
"When manipulating the level of regularization, we see that while too much regularization always harms performance, curriculum learning is least affected by this degradation (results are omitted).",RESULTS,[0],[0]
"We investigated curriculum learning, an extension of stochastic gradient descent in which easy examples are more frequently sampled at the beginning of training.",4. Summary and Discussion,[0],[0]
"We started
with the theoretical investigation of this strict definition in the context of linear regression, showing that curriculum learning accelerates the learning rate in agreement with prior empirical evidence.",4. Summary and Discussion,[0],[0]
"While not shedding light on its affect on the classifier’s final performance, our analysis suggests that the direction of a gradient step based on ”easy” examples may be more effective in traversing the input space towards the ideal minimum of the loss function.",4. Summary and Discussion,[0],[0]
"Specifically, we have empirically shown that the variance in the gradient direction of points increases with their difficulty when optimizing a non-convex loss function.",4. Summary and Discussion,[0],[0]
"Over-sampling the more coherent easier examples may therefore increase the likelihood to escape the basin of attraction of a low quality local minimum in favor of higher quality local minima even in the general non-convex case.
",4. Summary and Discussion,[0],[0]
"We also showed theoretically that when the difficulty score of the training points is fixed, convergence is faster if the loss with respect to the current hypothesis is higher.",4. Summary and Discussion,[0],[0]
"This seems to be a very intuitive result, an intuition that underlies the boosting method for example.",4. Summary and Discussion,[0],[0]
"However, as intuitive as it might be, this is not always true when the prior data density is assumed to be continuous and when the optimal hypothesis is realizable.",4. Summary and Discussion,[0],[0]
"Thus the requirement that the difficulty score is fixed is necessary.
",4. Summary and Discussion,[0],[0]
In the second part of this paper we described a curriculum learning method for deep networks.,4. Summary and Discussion,[0],[0]
The method relies on knowledge transfer from other (pre-trained) networks in order to rank the training examples by difficulty.,4. Summary and Discussion,[0],[0]
We described extensive experiments where we evaluated our proposed method under different task difficulty conditions and against a variety of control conditions.,4. Summary and Discussion,[0],[0]
"In all cases curriculum learning has been shown to increase the rate of convergence at the beginning of training, in agreement with the theoretical results.",4. Summary and Discussion,[0],[0]
"With more difficult tasks, curriculum learning improved generalization performance.",4. Summary and Discussion,[0],[0]
This work was supported in part by a grant from the Israel Science Foundation (ISF) and by the Gatsby Charitable Foundations.,Acknowledgements,[0],[0]
We provide theoretical investigation of curriculum learning in the context of stochastic gradient descent when optimizing the convex linear regression loss.,abstractText,[0],[0]
We prove that the rate of convergence of an ideal curriculum learning method is monotonically increasing with the difficulty of the examples.,abstractText,[0],[0]
"Moreover, among all equally difficult points, convergence is faster when using points which incur higher loss with respect to the current hypothesis.",abstractText,[0],[0]
We then analyze curriculum learning in the context of training a CNN.,abstractText,[0],[0]
"We describe a method which infers the curriculum by way of transfer learning from another network, pre-trained on a different task.",abstractText,[0],[0]
"While this approach can only approximate the ideal curriculum, we observe empirically similar behavior to the one predicted by the theory, namely, a significant boost in convergence speed at the beginning of training.",abstractText,[0],[0]
"When the task is made more difficult, improvement in generalization performance is also observed.",abstractText,[0],[0]
"Finally, curriculum learning exhibits robustness against unfavorable conditions such as excessive regularization.",abstractText,[0],[0]
Curriculum Learning by Transfer Learning: Theory and Experiments with Deep Networks,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 570–575 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
570",text,[0],[0]
Building Artificial Intelligence (AI) algorithms to teach machines to read and to comprehend text is a long-standing challenge in Natural Language Processing (NLP).,1 Introduction,[0],[0]
A common strategy for assessing these AI algorithms is by treating them as RC tasks.,1 Introduction,[0],[0]
This can be formulated as finding an answer to a question given the document(s) as evidence.,1 Introduction,[0],[0]
"Recently, many deep-learning based models (Seo et al., 2017; Xiong et al., 2017; Wang et al., 2017; Shen et al., 2017; Clark and Gardner, 2017) have been proposed to solve RC tasks based on the SQuAD (Rajpurkar et al., 2016) and TriviaQA (Joshi et al., 2017) datasets, reaching human level performance.",1 Introduction,[0],[0]
"A common approach in these models is to score and/or extract candidate spans conditioned on a given question-document pair.
",1 Introduction,[0],[0]
Most of these models have limited applicability to real problems for the following reasons.,1 Introduction,[0],[0]
"They do not generalize well to scenarios where the answer is not present as a span, or where several discontinuous parts of the document are required to
∗ To whom correspondence should be addressed.
",1 Introduction,[0],[0]
form the answer.,1 Introduction,[0],[0]
"In addition, unlike humans, they can not easily skip through irrelevant parts to comprehend long documents (Masson, 1983).
",1 Introduction,[0],[0]
"To address the issues above we develop a novel context zoom-in network (ConZNet) for RC tasks, which can skip through irrelevant parts of a document and generate an answer using only the relevant regions of text.",1 Introduction,[0],[0]
The ConZNet architecture consists of two phases.,1 Introduction,[0],[0]
In the first phase we identify the relevant regions of text by employing a reinforcement learning algorithm.,1 Introduction,[0],[0]
"These relevant regions are not only useful to generate the answer, but can also be presented to the user as supporting information along with the answer.",1 Introduction,[0],[0]
"The second phase is based on an encoder-decoder architecture, which comprehends the identified regions of text and generates the answer by using a residual self-attention network as encoder and a RNNbased sequence generator along with a pointer network (Vinyals et al., 2015) as the decoder.",1 Introduction,[0],[0]
"It has the ability to generate better well-formed answers not verbatim present in the document than span prediction models.
",1 Introduction,[0],[0]
"Recently, there have been several attempts to adopt condensing documents in RC tasks.",1 Introduction,[0],[0]
Wang et al. (2018) retrieve a relevant paragraph based on the question and predict the answer span.,1 Introduction,[0],[0]
Choi et al. (2017) select sentence(s) to make a summary of the entire document with a feed-forward network and generate an answer based on the summary.,1 Introduction,[0],[0]
"Unlike existing approaches, our method has the ability to select relevant regions of text not just based on the question but also on how well regions are related to each other.",1 Introduction,[0],[0]
"Moreover, our decoder combines span prediction and sequence generation.",1 Introduction,[0],[0]
"This allows the decoder to copy words from the relevant regions of text as well as to generate words from a fixed vocabulary.
",1 Introduction,[0],[0]
"We evaluate our model using one of the challenging RC datasets, called ‘NarrativeQA’, which
was released recently by Kočiskỳ et al. (2017).",1 Introduction,[0],[0]
Experimental results show the usefulness of our framework for RC tasks and we outperform stateof-the-art results on this dataset.,1 Introduction,[0],[0]
"An overview of our architecture is shown in Figure 1, which consists of two phases.",2 Proposed Architecture,[0],[0]
"First, the identification of relevant regions of text is computed by the Co-attention and Context Zoom layers as explained in Sections 2.1 and 2.2.",2 Proposed Architecture,[0],[0]
"Second, the comprehension of identified regions of text and output generation is computed by Answer Generation block as explained in Section 2.3.",2 Proposed Architecture,[0],[0]
"The words in the document, question and answer are represented using pre-trained word embeddings (Pennington et al., 2014).",2.1 Co-attention layer,[0],[0]
These wordbased embeddings are concatenated with their corresponding char embeddings.,2.1 Co-attention layer,[0],[0]
"The char embeddings are learned by feeding all the characters of a word into a Convolutional Neural Network (CNN) (Kim, 2014).",2.1 Co-attention layer,[0],[0]
"We further encode the document and question embeddings using a shared bi-directional GRU (Cho et al., 2014) to get context-aware representations.
",2.1 Co-attention layer,[0],[0]
We compute the co-attention between document and question to get question-aware representations for the document by using tri-linear attention as proposed by Seo et al. (2017).,2.1 Co-attention layer,[0],[0]
"Let di be the vector representation for the document word i, qj be the vector for the question word j, and ld and lq be the lengths of the document and question respectively.",2.1 Co-attention layer,[0],[0]
"The tri-linear attention is calculated as
aij = wddi + wqqj + wdq(di qj), (1)
where wd, wq, and wdq are learnable parameters and denotes the element-wise multiplication.
",2.1 Co-attention layer,[0],[0]
We compute the attended document word d̃i by first computing λi = softmax(ai:) and followed by d̃i = ∑lq j=1 λijqj .,2.1 Co-attention layer,[0],[0]
"Similarly, we compute a question to document attention vector q̃ by first computing b = softmax(max(ai:)) and followed by q̃",2.1 Co-attention layer,[0],[0]
= ∑ld i=i dibi.,2.1 Co-attention layer,[0],[0]
"Finally, di, d̃i, di d̃i, d̃i q̃ are concatenated to yield a query-aware contextual representation for each word in the document.",2.1 Co-attention layer,[0],[0]
This layer finds relevant regions of text.,2.2 Context Zoom Layer,[0],[0]
"We use reinforcement learning to do that, with the goal of improving answer generation accuracy – see Section 2.4.
",2.2 Context Zoom Layer,[0],[0]
The Split Context operation splits the attended document vectors into sentences or fixed size chunks (useful when sentence tokenization is not available for a particular language).,2.2 Context Zoom Layer,[0],[0]
"This results in n text regions with each having length lk,",2.2 Context Zoom Layer,[0],[0]
where ld = ∑n k=1 lk.,2.2 Context Zoom Layer,[0],[0]
"We then get the representations, denoted as zk, for each text region by running a BiGRU and concatenating the last states of the forward and backward GRUs.
",2.2 Context Zoom Layer,[0],[0]
"The text region representations, zk, encode how well they are related to the question, and their surrounding context.",2.2 Context Zoom Layer,[0],[0]
"Generating an answer may depend on multiple regions, and it is important for
each text region to collect cues from other regions which are outside of their surroundings.",2.2 Context Zoom Layer,[0],[0]
We can compute this by using a Self-Attention layer.,2.2 Context Zoom Layer,[0],[0]
"It is a special case of co-attention where both operands (di and qj) are the text fragment itself, computed by setting aij = −∞ when i = j in Eq. 1.
",2.2 Context Zoom Layer,[0],[0]
"These further self-attended text region representations, z̃k, are passed through a linear layer with tanh activation and softmax layer as follows:
u = tanh(Wc[z̃1, · · · , z̃n] + bc), (2) ψ = softmax(u), (3)
where ψ is the probability distribution of text regions, which is the evidence used to generate the answer.",2.2 Context Zoom Layer,[0],[0]
"The policy of the reinforcement learner is defined as π(r|u; θz) = ψr, where ψr is the probability of a text region r (agent’s action) being selected, u is the environment state as defined in Eq. 2, and θz are the learnable parameters.",2.2 Context Zoom Layer,[0],[0]
"During the training time we sample text regions using ψ, in inference time we follow greedy evaluation by selecting most probable region(s).",2.2 Context Zoom Layer,[0],[0]
"This component is implemented based on the encoder-decoder architecture of (Sutskever et al., 2014).",2.3 Answer Generation,[0],[0]
"The selected text regions from the Context Zoom layer are given as input to the encoder, where its output is given to the decoder in order to generate the answer.
",2.3 Answer Generation,[0],[0]
The encoder block uses residual connected selfattention layer followed by a BiGRU.,2.3 Answer Generation,[0],[0]
"The selected relevant text regions (∈ ψr) are first passed through a separate BiGRU, then we apply a selfattention mechanism similar to the Context Zoom layer followed by a linear layer with ReLU activations.",2.3 Answer Generation,[0],[0]
"The encoder’s output consists of representations of the relevant text regions, denoted by ei.
",2.3 Answer Generation,[0],[0]
"The decoder block is based on an attention mechanism (Bahdanau et al., 2015) and a copy mechanism by using a pointer network similar to (See et al., 2017).",2.3 Answer Generation,[0],[0]
This allows the decoder to predict words from the relevant regions as well as from the fixed vocabulary.,2.3 Answer Generation,[0],[0]
"At time step t, the decoder predicts the next word in the answer using the attention distribution, context vector and current word embedding.",2.3 Answer Generation,[0],[0]
"The attention distribution and context vector are obtained as follows:
oti = v T tanh(Weei +Whht + bo), (4)
γt = softmax(oti), (5)
where ht is hidden state of the decoder, v, We, Wh, bo are learnable parameters.",2.3 Answer Generation,[0],[0]
The γt represents a probability distribution over words of relevant regions ei.,2.3 Answer Generation,[0],[0]
"The context vector is given by ct = ∑ i γ t iei.
",2.3 Answer Generation,[0],[0]
"The probability distribution to predict word wt from the fixed vocabulary (Pfv) is computed by passing state ht and context vector ct to a linear layer followed by a softmax function denoted as
Pfv = softmax(Wv(Xv[ht, ct] + bp)+ bq).",2.3 Answer Generation,[0],[0]
"(6)
To allow decoder to copy words from the encoder sequence, we compute a soft gate (Pcopy), which helps the decoder to generate a word by sampling from the fixed vocabulary or by copying from a selected text regions (ψr).",2.3 Answer Generation,[0],[0]
"The soft gate is calculated as
Pcopy = σ(w T p ct + v T h",2.3 Answer Generation,[0],[0]
"ht + w T x xt + bc), (7)
where xt is current word embedding, ht is hidden state of the decoder, ct is the context vector, and wp, vh, wx, and bc are learnable parameters.",2.3 Answer Generation,[0],[0]
We maintain a list of out-of-vocabulary (OOV) words for each document.,2.3 Answer Generation,[0],[0]
The fixed vocabulary along with this OOV list acts as an extended vocabulary for each document.,2.3 Answer Generation,[0],[0]
"The final probability distribution (unnormalized) over this extended vocabulary (Pev) is given by
Pev(wt) =",2.3 Answer Generation,[0],[0]
"(1−Pcopy)Pfv(wt)+Pcopy ∑
i:wi=wt
γti .
(8)",2.3 Answer Generation,[0],[0]
"We jointly estimate the parameters of our model coming from the Co-attention, Context Zoom, and Answer Generation layers, which are denoted as θa, θz , and θg respectively.",2.4 Training,[0],[0]
"Estimating θa and θg is straight-forward by using the cross-entropy objective J1({θa, θg}) and the backpropagation algorithm.",2.4 Training,[0],[0]
"However, selecting text regions in the Context Zoom layer makes it difficult to estimate θz
given their discrete nature.",2.4 Training,[0],[0]
We therefore formulate the estimation of θz as a reinforcement learning problem via a policy gradient method.,2.4 Training,[0],[0]
"Specifically, we design a reward function over θz .
",2.4 Training,[0],[0]
"We use mean F-score of ROUGE-1, ROUGE-2, and ROUGE-L (Lin and Hovy, 2003) as our reward function R.",2.4 Training,[0],[0]
"The objective function to maximize is the expected reward under the probability distribution of current text regions ψr, i.e., J2(θz) = Ep(r|θz)[R].",2.4 Training,[0],[0]
"We approximate the gradient ∇θzJ2(θz) by following the REINFORCE (Williams, 1992) algorithm.",2.4 Training,[0],[0]
To reduce the high variance in estimating∇θzJ2(θz) one widely used mechanism is to subtract a baseline value from the reward.,2.4 Training,[0],[0]
"It is shown that any number will reduce the variance (Williams, 1992; Zaremba and Sutskever, 2015), here we used the mean of the mini-batch reward b as our baseline.",2.4 Training,[0],[0]
"The final objective is to minimize the following equation:
J(θ) = J1({θa, θg})−J2(θz)+ B∑ i=1",2.4 Training,[0],[0]
"(Ri−b), (9)
where, B is the size of mini-batch, and Ri is the reward of example i ∈",2.4 Training,[0],[0]
B. J(θ) is now fully differentiable and we use backpropagation to estimate θ.,2.4 Training,[0],[0]
"The NarrativeQA dataset (Kočiskỳ et al., 2017) consists of fictional stories gathered from books and movie scripts, where corresponding summaries and question-answer pairs are generated with the help of human experts and Wikipedia articles.",3.1 Dataset,[0],[0]
The summaries in NarrativeQA are 4-5 times longer than documents in the SQuAD dataset.,3.1 Dataset,[0],[0]
"Moreover, answers are well-formed by human experts and are not verbatim in the story, thus making this dataset ideal for testing our model.",3.1 Dataset,[0],[0]
The statistics of NarrativeQA are available in Table 11.,3.1 Dataset,[0],[0]
"We compare our model against reported models in Kočiskỳ et al. (2017) (Seq2Seq, ASR, BiDAF) and the Multi-range Reasoning Unit (MRU) in Tay et al. (2018).",3.2 Baselines,[0],[0]
"We implemented two baseline models (Baseline 1, Baseline 2) with Context Zoom layer similar to Wang et al. (2018).",3.2 Baselines,[0],[0]
In both baselines we replace the span prediction layer with an answer generation layer.,3.2 Baselines,[0],[0]
"In Baseline 1 we use an
1please refer Kočiskỳ et al. (2017) for more details
attention based seq2seq layer without using copy mechanism in the answer generation unit similar to Choi et al. (2017).",3.2 Baselines,[0],[0]
In Baseline 2 the answer generation unit is similar to our ConZNet architecture.,3.2 Baselines,[0],[0]
"We split each document into sentences using the sentence tokenizer of the NLTK toolkit (Bird and Loper, 2004).",3.3 Implementation Details,[0],[0]
"Similarly, we further tokenize each sentence, corresponding question and answer using the word tokenizer of NLTK.",3.3 Implementation Details,[0],[0]
"The model is implemented using Python and Tensorflow (Abadi et al., 2015).",3.3 Implementation Details,[0],[0]
"All the weights of the model are initialized by Glorot Initialization (Glorot et al., 2011) and biases are initialized with zeros.",3.3 Implementation Details,[0],[0]
"We use a 300 dimensional word vectors from GloVe (Pennington et al., 2014) (with 840 billion pre-trained vectors) to initialize the word embeddings, which we kept constant during training.",3.3 Implementation Details,[0],[0]
"All the words that do not appear in Glove are initialized by sampling from a uniform random distribution between [-0.05, 0.05].",3.3 Implementation Details,[0],[0]
"We apply dropout (Srivastava et al., 2014) between the layers with keep probability of 0.8 (i.e dropout=0.2).",3.3 Implementation Details,[0],[0]
The number of hidden units are set to 100.,3.3 Implementation Details,[0],[0]
"We trained our model with the AdaDelta (Zeiler, 2012) optimizer for 50 epochs, an initial learning rate of 0.1, and a minibatch size of 32.",3.3 Implementation Details,[0],[0]
The hyperparameter ‘sample size’ (number of relevant sentences) is chosen based on the model performance on the devset.,3.3 Implementation Details,[0],[0]
Table 2 shows the performance of various models on NarrativeQA.,3.4 Results,[0],[0]
It can be noted that our model with sample size 5 (choosing 5 relevant sentences) outperforms the best ROUGE-L score available so far by 12.62% compared to Tay et al. (2018).,3.4 Results,[0],[0]
"The low performance of Baseline 1 shows that the hybrid approach (ConZNet) for generating words from a fixed vocabulary as well as copying words from the document is better suited than span prediction models (Seq2Seq, ASR, BiDAF, MRU).
",3.4 Results,[0],[0]
"To validate the importance of finding relevant sentences in contrast to using an entire document for answer generation, we experimented with sample sizes beyond 5.",3.4 Results,[0],[0]
The performance of our model gradually dropped from sample size 7 onwards.,3.4 Results,[0],[0]
"This result shows evidence that only a few relevant sentences are sufficient to answer a question.
",3.4 Results,[0],[0]
"We also experimented with various sample sizes to see the effect of intra sentence relations for an-
swer generation.",3.4 Results,[0],[0]
The performance of the model improved dramatically with sample sizes 3 and 5 compared to the sample size of 1.,3.4 Results,[0],[0]
These results show that the importance of selecting multiple relevant sentences for generating an answer.,3.4 Results,[0],[0]
"In addition, the low performance of Baseline 2 indicates that just selecting multiple sentences is not enough, they should also be related to each other.",3.4 Results,[0],[0]
This result points out that the self-attention mechanism in the Context zoom layer is an important component to identify related relevant sentences.,3.4 Results,[0],[0]
We have proposed a new neural-based architecture which condenses an original document to facilitate fast comprehension in order to generate better well-formed answers than span based prediction models.,4 Conclusion,[0],[0]
Our model achieved the best performance on the challenging NarrativeQA dataset.,4 Conclusion,[0],[0]
"Future work can focus for example on designing an inexpensive preprocess layer, and other strategies for improved performance on answer generation.",4 Conclusion,[0],[0]
In recent years many deep neural networks have been proposed to solve Reading Comprehension (RC) tasks.,abstractText,[0],[0]
Most of these models suffer from reasoning over long documents and do not trivially generalize to cases where the answer is not present as a span in a given document.,abstractText,[0],[0]
We present a novel neural-based architecture that is capable of extracting relevant regions based on a given question-document pair and generating a well-formed answer.,abstractText,[0],[0]
"To show the effectiveness of our architecture, we conducted several experiments on the recently proposed and challenging RC dataset ‘NarrativeQA’.",abstractText,[0],[0]
"The proposed architecture outperforms state-of-the-art results (Tay et al., 2018) by 12.62% (ROUGE-L) relative improvement.",abstractText,[0],[0]
Cut to the Chase: A Context Zoom-in Network for Reading Comprehension,title,[0],[0]
"1 INTRODUCTION
Dance Dance Revolution (DDR) is a popular rhythm-based video game with millions of players worldwide (Hoysniemi, 2006). Players perform steps atop a dance platform, containing four buttons, each labeled with an arrow. An on-screen step chart prompts players to step on the buttons at specific, musically salient points in time. Scores depend upon both hitting the right buttons and hitting them at the right time. Step charts vary in difficulty with harder charts containing more steps and more complex sequences.
Despite the game’s popularity, players have some reasonable complaints: For one, packs are limited to songs with favorable licenses, meaning players may be unable to dance to their favorite songs. Even when charts are available, players may tire of repeatedly performing the same charts. Although players can produce their own charts, the process is painstaking and requires significant expertise.
This paper introduces learning to choreograph, the task of producing a step chart from raw audio. We break the problem into two subtasks: First, step placement consists of identifying a set of timestamps in the song at which to place steps. This process can be conditioned on a user-specified difficulty level. Second, step selection consists of choosing which steps to place at each timestamp. Running these two steps in sequence yields a playable step chart (Figure 1). 1
For both prediction stages of learning to choreograph, we demonstrate the superior performance of neural networks over strong alternatives. Our best model for step placement jointly learns convolutional neural network (CNN) representations and a recurrent neural network (RNN), which
1 Demonstration video showing human choreography and the output of Dance Dance Convolution side-byside: https://youtu.be/yUc3O237p9M
integrates information across consecutive time slices. Our best model for step selection consists of a conditional LSTM generative model which receives high-level rhythm features as auxiliary information.",text,[0],[0]
"Before applying our step placement algorithms, we transform raw audio samples into perceptuallyinformed representations.",2 METHODS,[0],[0]
Music files arrive as lossy encodings at 44.1kHz .,2 METHODS,[0],[0]
We decode the audio files into stereo PCM and average the two channels to produce a monophonic representation.,2 METHODS,[0],[0]
"We then compute a multiple-timescale short-time Fourier transform (STFT) using window lengths of 23ms , 46ms , and 93ms and a stride of 10ms .",2 METHODS,[0],[0]
We reduce the dimensionality of the STFT magnitude spectrum by applying a Mel-scale filterbank yielding 80 frequency bands.,2 METHODS,[0],[0]
Then we scale the filter outputs logarithmically in accordance with human perception of loudness.,2 METHODS,[0],[0]
"Finally, we prepend and append seven frames of past and future context to each frame.
",2 METHODS,[0],[0]
"2.1 STEP PLACEMENT
We consider several models to address the step placement task.",2 METHODS,[0],[0]
Each model’s output consists of a single sigmoid unit which estimates the probability that a step is placed.,2 METHODS,[0],[0]
"For all models, we augment the audio features with a one-hot representation of difficulty.
",2 METHODS,[0],[0]
"Following state-of-the-art work on musical onset detection (Schlüter & Böck, 2014), we adopt a convolutional neural network (CNN) architecture.",2 METHODS,[0],[0]
This model consists of two convolutional layers followed by two fully connected layers.,2 METHODS,[0],[0]
Our first convolutional layer has 10 filter kernels that are 7-wide in time and 3-wide in frequency.,2 METHODS,[0],[0]
The second layer has 20 filter kernels that are 3-wide in time and 3-wide in frequency.,2 METHODS,[0],[0]
"We apply 1D max-pooling after each convolutional layer, only in the frequency dimension, with a width and stride of 3.",2 METHODS,[0],[0]
Both convolutional layers use rectified linear units (ReLU) (Glorot et al.).,2 METHODS,[0],[0]
"Following the convolutional layers, we add two fully connected layers with ReLU activation functions and 256 and 128 nodes respectively.
",2 METHODS,[0],[0]
"To improve upon the CNN, we propose a C-LSTM model (Figure 2), combining a convolutional encoding with an LSTM-RNN (Hochreiter & Schmidhuber, 1997) that integrates information across longer windows of time.",2 METHODS,[0],[0]
Our C-LSTM contains two convolutional layers (of the same shape as the CNN) applied across the full unrolling length.,2 METHODS,[0],[0]
"The output of the second convolutional layer is a 3D tensor, which we flatten along the channel and frequency axes (preserving the temporal dimension).",2 METHODS,[0],[0]
The flattened features at each time step then become the inputs to a two-layer LSTM with 200 nodes per layer.,2 METHODS,[0],[0]
We train this model using 100 unrollings for backpropagation through time.,2 METHODS,[0],[0]
We treat the step selection task as a sequence generation problem.,2.2 STEP SELECTION,[0],[0]
"Our approach follows related work in language modeling where RNNs are well-known to produce coherent text that captures long-range relationships (Mikolov et al., 2010; Sutskever et al., 2011; Sundermeyer et al., 2012).
",2.2 STEP SELECTION,[0],[0]
Our LSTM model passes over the ground truth step placements and predicts the next token given the previous sequence of tokens.,2.2 STEP SELECTION,[0],[0]
The output is a softmax distribution over the game’s 256 possible steps.,2.2 STEP SELECTION,[0],[0]
"As inputs, we use a more compact bag-of-arrows representation containing 16 features (4 per
arrow) to depict the previous step.",2.2 STEP SELECTION,[0],[0]
"For each arrow, the 4 corresponding features represent the states on, off, hold, and release.",2.2 STEP SELECTION,[0],[0]
We add an additional feature that functions as a start token to denote the first step of a chart.,2.2 STEP SELECTION,[0],[0]
"For this task, our LSTM consists of 2 layers of 128 cells each.",2.2 STEP SELECTION,[0],[0]
"We use 64 steps of unrolling, an average of 100 seconds for the easiest charts and 9 seconds for the hardest.
To inform our LSTM of the non-uniform rhythmic spacing of the step placements, we provide the following two pieces of auxiliary information: (1) ∆-beat adds two features representing the number of beats since the previous and until the next step; (2) beat phase adds four features representing which sixteenth note subdivision of the beat the current step most closely aligns to.",2.2 STEP SELECTION,[0],[0]
"We collected a dataset consisting of 203 songs, labeled by 9 annotators.",3 EXPERIMENTS,[0],[0]
"One particularly prolific annotator, Fraxtil, annotated 90 of these songs for all five difficulty levels.",3 EXPERIMENTS,[0],[0]
The remaining songs are from a large multi-author collection called In The Groove (ITG).,3 EXPERIMENTS,[0],[0]
"In total, across all five difficulty settings, we obtain around 35 hours of annotated audio and 350, 000 steps.",3 EXPERIMENTS,[0],[0]
"2 We augment our dataset for step selection by synthesizing mirror images of each chart (i.e., interchanging left and right) which we found to improve performance for all models
For step placement, we compare the performance of our proposed CNN and C-LSTM models against a logistic regressor (LogReg) and a 2-layer MLP.",3 EXPERIMENTS,[0],[0]
"For step selection, we compare our proposed LSTM model against a fixed-window MLP and an n-gram model using modified Kneser-Ney smoothing (Chen & Goodman, 1998) with backoff.",3 EXPERIMENTS,[0],[0]
Both the MLP (MLP5) and n-gram model (KN5) predict the next step from four steps of history and the MLP received the same auxiliary information as the LSTM.,3 EXPERIMENTS,[0],[0]
"We also show the performance of an LSTM model trained with only 5 steps of unrolling (LSTM5) to demonstrate the advantage of longer context.
",3 EXPERIMENTS,[0],[0]
"Model Dataset PPL AUC F-score
LogReg Fraxtil 1.205 0.601 0.609 MLP Fraxtil 1.097 0.659 0.665 CNN Fraxtil 1.082 0.671 0.678 C-LSTM Fraxtil 1.070 0.682 0.681
LogReg ITG 1.123 0.599 0.634 MLP ITG 1.090 0.637 0.671 CNN ITG 1.083 0.677 0.689 C-LSTM ITG 1.072 0.680 0.697
Table 1: Perplexity, area under curve and Fscore assessed for the step placement task.
",3 EXPERIMENTS,[0],[0]
"Model Dataset PPL Acc.
",3 EXPERIMENTS,[0],[0]
KN5 Fraxtil 3.681 0.528 MLP5 Fraxtil 3.428 0.557 LSTM5 Fraxtil 3.185 0.581 LSTM64,3 EXPERIMENTS,[0],[0]
"Fraxtil 3.011 0.613
KN5 ITG 5.847 0.356",3 EXPERIMENTS,[0],[0]
"MLP5 ITG 4.786 0.401 LSTM5 ITG 4.447 0.441 LSTM64 ITG 4.342 0.444
Table 2: Perplexity and per-token accuracy assessed for the step selection task.
",3 EXPERIMENTS,[0],[0]
"Our experiments demonstrate that on both the step placement (Table 1) and the step selection (Table 2) tasks, deep neural network models outperform traditional baselines.",3 EXPERIMENTS,[0],[0]
"For the step placement task, the best performing method by all metrics is the C-LSTM.",3 EXPERIMENTS,[0],[0]
"For the step selection task, LSTMs outperform other models.
",3 EXPERIMENTS,[0],[0]
Data augmentation and the inclusion of ∆-beat and beat phase side information give a significant increase in performance to both the MLP and LSTMs for step selection.,3 EXPERIMENTS,[0],[0]
"For example, the LSTM64 model trained on the Fraxtil dataset without side information or data augmentation only achieves a PPL of 3.526 and accuracy of 0.562.",3 EXPERIMENTS,[0],[0]
Step selection models perform better on the single-author Fraxtil dataset in comparison to the multi-author ITG.,3 EXPERIMENTS,[0],[0]
Author style tends to be distinctive and thus a collection of single-author sequences is more predictable.,3 EXPERIMENTS,[0],[0]
"A few prior systems attempt automatic synthesis of step charts (O’Keeffe, 2003; Nogaj, 2005), however neither establishes a reproducible evaluation methodology or learns the semantics of steps from data.",4 RELATED WORK,[0],[0]
"The most closely related work to our step placement task is concerned with onset detection
2All data shall be made available at publication time
(Bello et al., 2005; Dixon, 2006), which has previously been attempted with deep neural networks (Eyben et al., 2010; Schlüter & Böck, 2014).",4 RELATED WORK,[0],[0]
Our step selection task most closely resembles conditional language modeling.,4 RELATED WORK,[0],[0]
"A recent wave of work in RNNs for language modeling began with (Mikolov et al., 2010; Sutskever et al., 2011).",4 RELATED WORK,[0],[0]
"Inspired by this work, several recent papers extend the methods to polyphonic music generation and transcription (Boulanger-Lewandowski et al., 2012; Chu et al., 2016; Sigtia et al., 2016).",4 RELATED WORK,[0],[0]
"To our knowledge, ours is the first paper to attempt end-to-end DDR choreography from raw audio with deep learning.",4 RELATED WORK,[0],[0]
Dance Dance Revolution (DDR) is a popular rhythm-based video game.,abstractText,[0],[0]
Players perform steps on a dance platform in synchronization with music as directed by on-screen step charts.,abstractText,[0],[0]
"While many step charts are available in standardized packs, users may grow tired of existing charts, or wish to dance to a song for which no chart exists.",abstractText,[0],[0]
We introduce the task of learning to choreograph.,abstractText,[0],[0]
"Given a raw audio track, the goal is to produce a new step chart.",abstractText,[0],[0]
This task decomposes naturally into two subtasks: deciding when to place steps and deciding which steps to select.,abstractText,[0],[0]
We demonstrate deep learning solutions for both tasks and establish strong benchmarks for future work.,abstractText,[0],[0]
"lem in deep reinforcement learning (RL). In many scenarios of interest data is hard to obtain, so agents may learn a source policy in a setting where data is readily available, with the hope that it generalises well to the target domain. We propose a new multi-stage RL agent, DARLA (DisentAngled Representation Learning Agent), which learns to see before learning to act. DARLA’s vision is based on learning a disentangled representation of the observed environment. Once DARLA can see, it is able to acquire source policies that are robust to many domain shifts - even with no access to the target domain. DARLA significantly outperforms conventional baselines in zero-shot domain adaptation scenarios, an effect that holds across a variety of RL environments (Jaco arm, DeepMind Lab) and base RL algorithms (DQN, A3C and EC).",text,[0],[0]
Autonomous agents can learn how to maximise future expected rewards by choosing how to act based on incoming sensory observations via reinforcement learning (RL).,1. Introduction,[0],[0]
"Early RL approaches did not scale well to environments with large state spaces and high-dimensional raw observations (Sutton & Barto, 1998).",1. Introduction,[0],[0]
"A commonly used workaround was to embed the observations in a lower-dimensional space, typically via hand-crafted and/or privileged-information features.",1. Introduction,[0],[0]
"Recently, the advent of deep learning and its successful combination with RL has enabled end-to-end learning of such embeddings directly from raw inputs, sparking success in a wide variety of previously challenging RL domains (Mnih et al., 2015; 2016; Jaderberg et al., 2017).",1. Introduction,[0],[0]
"Despite the seemingly universal
*
Equal contribution
1
DeepMind, 6 Pancras Square, Kings
Cross, London, N1C 4AG, UK.",1. Introduction,[0],[0]
"Correspondence to: Irina Higgins <irinah@google.com>, Arka Pal <arkap@google.com>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
efficacy of deep RL, however, fundamental issues remain.",1. Introduction,[0],[0]
"These include data inefficiency, the reactive nature and general brittleness of learnt policies to changes in input data distribution, and lack of model interpretability (Garnelo et al., 2016; Lake et al., 2016).",1. Introduction,[0],[0]
"This paper focuses on one of these outstanding issues: the ability of RL agents to deal with changes to the input distribution, a form of transfer learning known as domain adaptation (Bengio et al., 2013).",1. Introduction,[0],[0]
"In domain adaptation scenarios, an agent trained on a particular input distribution with a specified reward structure (termed the source domain) is placed in a setting where the input distribution is modified but the reward structure remains largely intact (the target domain).",1. Introduction,[0],[0]
We aim to develop an agent that can learn a robust policy using observations and rewards obtained exclusively within the source domain.,1. Introduction,[0],[0]
"Here, a policy is considered as robust if it generalises with minimal drop in performance to the target domain without extra fine-tuning.
",1. Introduction,[0],[0]
"Past attempts to build RL agents with strong domain adaptation performance highlighted the importance of learning good internal representations of raw observations (Finn et al., 2015; Raffin et al., 2017; Pan & Yang, 2009; Barreto et al., 2016; Littman et al., 2001).",1. Introduction,[0],[0]
"Typically, these approaches tried to align the source and target domain representations by utilising observation and reward signals from both domains (Tzeng et al., 2016; Daftry et al., 2016; Parisotto et al., 2015; Guez et al., 2012; Talvitie & Singh, 2007; Niekum et al., 2013; Gupta et al., 2017; Finn et al., 2017; Rajendran et al., 2017).",1. Introduction,[0],[0]
"In many scenarios, such as robotics, this reliance on target domain information can be problematic, as the data may be expensive or difficult to obtain (Finn et al., 2017; Rusu et al., 2016).",1. Introduction,[0],[0]
"Furthermore, the target domain may simply not be known in advance.",1. Introduction,[0],[0]
"On the other hand, policies learnt exclusively on the source domain using existing deep RL approaches that have few constraints on the nature of the learnt representations often overfit to the source input distribution, resulting in poor domain adaptation performance (Lake et al., 2016; Rusu et al., 2016).
",1. Introduction,[0],[0]
We propose tackling both of these issues by focusing instead on learning representations which capture an underlying low-dimensional factorised representation of the world and are therefore not task or domain specific.,1. Introduction,[0],[0]
"Many nat-
uralistic domains such as video game environments, simulations and our own world are well described in terms of such a structure.",1. Introduction,[0],[0]
"Examples of such factors of variation are object properties like colour, scale, or position; other examples correspond to general environmental factors, such as geometry and lighting.",1. Introduction,[0],[0]
"We think of these factors as a set of high-level parameters that can be used by a world graphics engine to generate a particular natural visual scene (Kulkarni et al., 2015).",1. Introduction,[0],[0]
"Learning how to project raw observations into such a factorised description of the world is addressed by the large body of literature on disentangled representation learning (Schmidhuber, 1992; Desjardins et al., 2012; Cohen & Welling, 2014; 2015; Kulkarni et al., 2015; Hinton et al., 2011; Rippel & Adams, 2013; Reed et al., 2014; Yang et al., 2015; Goroshin et al., 2015; Kulkarni et al., 2015; Cheung et al., 2015; Whitney et al., 2016; Karaletsos et al., 2016; Chen et al., 2016; Higgins et al., 2017).",1. Introduction,[0],[0]
"Disentangled representations are defined as interpretable, factorised latent representations where either a single latent or a group of latent units are sensitive to changes in single ground truth factors of variation used to generate the visual world, while being invariant to changes in other factors (Bengio et al., 2013).",1. Introduction,[0],[0]
"The theoretical utility of disentangled representations for supervised and reinforcement learning has been described before (Bengio et al., 2013; Higgins et al., 2017; Ridgeway, 2016); however, to our knowledge, it has not been empirically validated to date.
",1. Introduction,[0],[0]
"We demonstrate how disentangled representations can improve the robustness of RL algorithms in domain adaptation scenarios by introducing DARLA (DisentAngled Representation Learning Agent), a new RL agent capable of learning a robust policy on the source domain that achieves significantly better out-of-the-box performance in domain adaptation scenarios compared to various baselines.",1. Introduction,[0],[0]
"DARLA relies on learning a latent state representation that is shared between the source and target domains, by learning a disentangled representation of the environment’s generative factors.",1. Introduction,[0],[0]
"Crucially, DARLA does not require target domain data to form its representations.",1. Introduction,[0],[0]
"Our approach utilises a three stage pipeline: 1) learning to see, 2) learning to act, 3) transfer.",1. Introduction,[0],[0]
"During the first stage,
DARLA develops its vision, learning to parse the world in terms of basic visual concepts, such as objects, positions, colours, etc. by utilising a stream of raw unlabelled observations – not unlike human babies in their first few months of life (Leat et al., 2009; Candy et al., 2009).",1. Introduction,[0],[0]
"In the second stage, the agent utilises this disentangled visual representation to learn a robust source policy.",1. Introduction,[0],[0]
"In stage three, we demonstrate that the DARLA source policy is more robust to domain shifts, leading to a significantly smaller drop in performance in the target domain even when no further policy finetuning is allowed (median 270.3% improvement).",1. Introduction,[0],[0]
"These effects hold consistently across a number of different RL environments (DeepMind Lab and Jaco/MuJoCo: Beattie et al., 2016; Todorov et al., 2012) and algorithms (DQN, A3C and Episodic Control: Mnih et al., 2015; 2016; Blundell et al., 2016).",1. Introduction,[0],[0]
We now formalise domain adaptation scenarios in a reinforcement learning (RL) setting.,2.1. Domain adaptation in Reinforcement Learning,[0],[0]
"We denote the source and target domains as DS and DT , respectively.",2.1. Domain adaptation in Reinforcement Learning,[0],[0]
Each domain corresponds to an MDP defined as a tuple DS ⌘,2.1. Domain adaptation in Reinforcement Learning,[0],[0]
"(SS ,AS , TS , RS) or DT ⌘ (ST ,AT , TT , RT ) (we assume a shared fixed discount factor ), each with its own state space S , action space A, transition function T and reward function R.1",2.1. Domain adaptation in Reinforcement Learning,[0],[0]
"In domain adaptation scenarios the states S of the source and the target domains can be quite different, while the action spaces A are shared and the transitions T and reward functions R have structural similarity.",2.1. Domain adaptation in Reinforcement Learning,[0],[0]
"For example, consider a domain adaptation scenario for the Jaco robotic arm, where the MuJoCo (Todorov et al., 2012) simulation of the arm is the source domain, and the real world setting is the target domain.",2.1. Domain adaptation in Reinforcement Learning,[0],[0]
"The state spaces (raw pixels) of the source and the target domains differ significantly due to the perceptual-reality gap (Rusu et al., 2016); that is to say, SS 6= ST .",2.1. Domain adaptation in Reinforcement Learning,[0],[0]
"Both domains, however, share action spaces (AS = AT ), since the policy learns to control the same set of actuators within the arm.",2.1. Domain adaptation in Reinforcement Learning,[0],[0]
"Finally, the source and target domain transition and reward functions share structural similarity (TS ⇡ TT and RS ⇡ RT ), since in both domains transitions between states are governed by the physics of the world and the performance on the task depends on the relative position of the arm’s end effectors (i.e. fingertips) with respect to an object of interest.",2.1. Domain adaptation in Reinforcement Learning,[0],[0]
"In order to describe our proposed DARLA framework, we assume that there exists a set M of MDPs that is the set 1
For further background on the notation relating to the RL
paradigm, see Section A.1 in the Supplementary Materials.
of all natural world MDPs, and each MDP Di is sampled from M. We define M in terms of the state space ˆS that contains all possible conjunctions of high-level factors of variation necessary to generate any naturalistic observation in any Di 2 M. A natural world MDP Di is then one whose state space S corresponds to some subset of ˆS .",2.2. DARLA,[0],[0]
"In simple terms, we assume that there exists some shared underlying structure between the MDPs Di sampled from M. We contend that this is a reasonable assumption that permits inclusion of many interesting problems, including being able to characterise our own reality (Lake et al., 2016).
",2.2. DARLA,[0],[0]
"We now introduce notation for two state space variables that may in principle be used interchangeably within the source and target domain MDPs DS and DT – the agent observation state space So, and the agent’s internal latent state space Sz .2",2.2. DARLA,[0],[0]
"Soi in Di consists of raw (pixel) observations soi generated by the true world simulator from a sampled set of data generative factors ŝi, i.e. soi ⇠ Sim(̂si).",2.2. DARLA,[0],[0]
"ŝi is sampled by some distribution or process Gi on ˆS , ŝi ⇠ Gi( ˆS).
",2.2. DARLA,[0],[0]
"Using the newly introduced notation, domain adaptation scenarios can be described as having different sampling processes GS and GT such that ŝS ⇠ GS( ˆS) and ŝT ⇠ GT ( ˆS) for the source and target domains respectively, and then using these to generate different agent observation states soS ⇠ Sim(̂sS) and soT ⇠ Sim(̂sT).",2.2. DARLA,[0],[0]
"Intuitively, consider a source domain where oranges appear in blue rooms and apples appear in red rooms, and a target domain where the object/room conjunctions are reversed and oranges appear in red rooms and apples appear in blue rooms.",2.2. DARLA,[0],[0]
"While the true data generative factors of variation
ˆS remain the same - room colour (blue or red) and object type (apples and oranges) - the particular source and target distributions GS and GT differ.
",2.2. DARLA,[0],[0]
"Typically deep RL agents (e.g. Mnih et al., 2015; 2016) operating in an MDP Di 2 M learn an end-to-end mapping from raw (pixel) observations soi 2 Soi to actions ai 2 Ai (either directly or via a value function Qi(soi , ai) from which actions can be derived).",2.2. DARLA,[0],[0]
"In the process of doing so, the agent implicitly learns a function F : Soi !",2.2. DARLA,[0],[0]
Szi that maps the typically high-dimensional raw observations soi to typically low-dimensional latent states s z,2.2. DARLA,[0],[0]
i ; followed by a policy function ⇡i : Szi !,2.2. DARLA,[0],[0]
Ai that maps the latent states szi to actions ai 2 Ai.,2.2. DARLA,[0],[0]
"In the context of domain adaptation, if the agent learns a naive latent state mapping function FS : SoS !",2.2. DARLA,[0],[0]
"SzS on the source domain using reward signals to shape the representation learning, it is likely that FS will overfit to the source domain and will not generalise well to the target domain.",2.2. DARLA,[0],[0]
"Returning to our
2
Note that we do not assume these to be Markovian i.e. it is not necessarily the case that p(so t+1|sot )",2.2. DARLA,[0],[0]
"= p(sot+1|sot , sot 1, . . .",2.2. DARLA,[0],[0]
", so1), and similarly for sz .",2.2. DARLA,[0],[0]
"Note the index t here corresponds to time.
",2.2. DARLA,[0],[0]
"intuitive example, imagine an agent that has learnt a policy to pick up oranges and avoid apples on the source domain.",2.2. DARLA,[0],[0]
Such a source policy ⇡S is likely to be based on an entangled latent state space SzS of object/room conjunctions: oranges/blue !,2.2. DARLA,[0],[0]
"good, apples/red !",2.2. DARLA,[0],[0]
"bad, since this is arguably the most efficient representation for maximising expected rewards on the source task in the absence of extra supervision signals suggesting otherwise.",2.2. DARLA,[0],[0]
"A source policy ⇡S(a|szS ; ✓) based on such an entangled latent representation szS will not generalise well to the target domain without further fine-tuning, since FS(soS) 6= FS(soT ) and therefore crucially SzS 6=",2.2. DARLA,[0],[0]
SzT .,2.2. DARLA,[0],[0]
"On the other hand, since both ŝS ⇠ GS( ˆS) and ŝT ⇠ GT ( ˆS) are sampled from the same natural world state space
ˆS for the source and target domains respectively, it should be possible to learn a latent state mapping function ˆF :",2.2. DARLA,[0],[0]
"So ! SzŜ , which projects the agent observation state space So to a latent state space SzŜ expressed in terms of factorised data generative factors that are representative of the natural world i.e. Sz Ŝ ⇡ ˆS. Consider again our intuitive example, where
ˆF maps agent observations (soS : orange in a blue room) to a factorised or disentangled representation expressed in terms of the data generative factors (szŜ : room type = blue; object type = orange).",2.2. DARLA,[0],[0]
"Such a disentangled latent state mapping function should then directly generalise to both the source and the target domains, so that ˆF(soS) = ˆF(soT ) = szŜ .",2.2. DARLA,[0],[0]
"Since S z Ŝ is a disentangled representation of object and room attributes, the source policy ⇡S can learn a decision boundary that ignores the irrelevant room attributes: oranges !",2.2. DARLA,[0],[0]
"good, apples !",2.2. DARLA,[0],[0]
bad.,2.2. DARLA,[0],[0]
"Such a policy would then generalise well to the target domain out of the box, since ⇡S(a| ˆF(soS); ✓) = ⇡T (a| ˆF(soT ); ✓) = ⇡T (a|szŜ ; ✓).",2.2. DARLA,[0],[0]
"Hence, DARLA is based on the idea that a good quality
ˆF learnt exclusively on the source domain DS 2 M will zero-shot-generalise to all target domains Di 2 M, and therefore the source policy ⇡(a|SzŜ ; ✓) will also generalise to all target domains Di 2 M out of the box.
",2.2. DARLA,[0],[0]
"Next we describe each of the stages of the DARLA pipeline that allow it to learn source policies ⇡S that are robust to domain adaptation scenarios, despite being trained with no knowledge of the target domains (see Fig. 1 for a graphical representation of these steps): 1) Learn to see (unsupervised learning of FU ) – the task of inferring a factorised set of generative factors SzŜ =
ˆS from observations So is the goal of the extensive disentangled factor learning literature (e.g. Chen et al., 2016; Higgins et al., 2017).",2.2. DARLA,[0],[0]
"Hence, in stage one we learn a mapping FU : SoU ! SzU , where SzU ⇡ SzŜ (U stands for ‘unsupervised’) using an unsupervised model for learning disentangled factors that utilises observations collected by an agent with a random policy ⇡U from a visual pre-training
MDP DU 2 M. Note that we require sufficient variability of factors and their conjunctions in DU in order to have SzU ⇡ SzŜ ; 2) Learn to act (reinforcement learning of ⇡S in the source domain DS utilising previously learned FU ) – an agent that has learnt to see the world in stage one in terms of the natural data generative factors is now exposed to a source domain DS 2 M. The agent is tasked with learning the source policy ⇡S(a|szS ; ✓), where szS = FU (soS) ⇡ szŜ , via a standard reinforcement learning algorithm.",2.2. DARLA,[0],[0]
"Crucially, we do not allow FU to be modified (e.g. by gradient updates) during this phase; 3) Transfer (to a target domain DT ) – in the final step, we test how well the policy ⇡S learnt on the source domain generalises to the target domain DT 2 M in a zero-shot domain adaptation setting, i.e. the agent is evaluated on the target domain without retraining.",2.2. DARLA,[0],[0]
We compare the performance of policies learnt with a disentangled latent state SzŜ to various baselines where the latent state mapping function FU projects agent observations so to entangled latent state representations sz .,2.2. DARLA,[0],[0]
"In order to learn FU , DARLA utilises -VAE (Higgins et al., 2017), a state-of-the-art unsupervised model for automated discovery of factorised latent representations from raw image data.",2.3. Learning disentangled representations,[0],[0]
"-VAE is a modification of the variational autoencoder framework (Kingma & Welling, 2014; Rezende et al., 2014) that controls the nature of the learnt latent representations by introducing an adjustable hyperparameter to balance reconstruction accuracy with latent channel capacity and independence constraints.",2.3. Learning disentangled representations,[0],[0]
"It maximises the objective:
L(✓, ;x, z, ) =",2.3. Learning disentangled representations,[0],[0]
"Eq (z|x)[log p✓(x|z)] DKL(q (z|x)||p(z)) (1)
where , ✓ parametrise the distributions of the encoder and the decoder respectively.",2.3. Learning disentangled representations,[0],[0]
"Well-chosen values of - usually larger than one ( > 1) - typically result in more disentangled latent representations z by limiting the capacity of the latent information channel, and hence encouraging a more efficient factorised encoding through the increased pressure to match the isotropic unit Gaussian prior p(z) (Higgins et al., 2017).",2.3. Learning disentangled representations,[0],[0]
"The cost of increasing is that crucial information about the scene may be discarded in the latent representation z, particularly if that information takes up a small proportion of the observations x in pixel space.",2.3.1. PERCEPTUAL SIMILARITY LOSS,[0],[0]
"We encountered this issue in some of our tasks, as discussed in Section 3.1.",2.3.1. PERCEPTUAL SIMILARITY LOSS,[0],[0]
"The shortcomings of calculating the log-likelihood term
Eq (z|x)[log p✓(x|z)] on a per-pixel basis are known and have been addressed in the past by calculating the reconstruction cost in an abstract, high-level feature space given by another neural network model, such as a GAN (Goodfellow et al., 2014) or a pre-trained AlexNet (Krizhevsky et al., 2012; Larsen et al., 2016; Dosovitskiy & Brox, 2016; Warde-Farley & Bengio, 2017).",2.3.1. PERCEPTUAL SIMILARITY LOSS,[0],[0]
"In practice we found that pre-training a denoising autoencoder (Vincent et al., 2010) on data from the visual pre-training MDP DU 2 M worked best as the reconstruction targets for -VAE to match (see Fig. 1 for model architecture and Sec.",2.3.1. PERCEPTUAL SIMILARITY LOSS,[0],[0]
A.3.1 in Supplementary Materials for implementation details).,2.3.1. PERCEPTUAL SIMILARITY LOSS,[0],[0]
"The new -VAEDAE model was trained according to Eq. 2:
L(✓, ;x, z, )",2.3.1. PERCEPTUAL SIMILARITY LOSS,[0],[0]
"=Eq (z|x) kJ(ˆx) J(x)k 2 2
DKL(q (z|x)||p(z))",2.3.1. PERCEPTUAL SIMILARITY LOSS,[0],[0]
"(2)
where
ˆ x ⇠ p✓(x|z) and J : RW⇥H⇥C !",2.3.1. PERCEPTUAL SIMILARITY LOSS,[0],[0]
RN is the function that maps images from pixel space with dimensionality W ⇥H ⇥ C to a high-level feature space with dimensionality N given by a stack of pre-trained DAE layers up to a certain layer depth.,2.3.1. PERCEPTUAL SIMILARITY LOSS,[0],[0]
"Note that by replacing the pixel based reconstruction loss in Eq. 1 with high-level feature reconstruction loss in Eq. 2 we are no longer optimising the variational lower bound, and -VAEDAE with = 1 loses its equivalence to the Variational Autoencoder (VAE) framework as proposed by (Kingma & Welling, 2014; Rezende et al., 2014).",2.3.1. PERCEPTUAL SIMILARITY LOSS,[0],[0]
"In this setting, the only way to interpret is as a mixing coefficient that balances the capacity of the latent channel z of -VAEDAE against the pressure to match the high-level features within the pre-trained DAE.",2.3.1. PERCEPTUAL SIMILARITY LOSS,[0],[0]
"We used various RL algorithms (DQN, A3C and Episodic Control: Mnih et al., 2015; 2016; Blundell et al., 2016) to learn the source policy ⇡S during stage two of the pipeline using the latent states sz acquired by -VAE based models during stage one of the DARLA pipeline.",2.4. Reinforcement Learning Algorithms,[0],[0]
"Deep Q Network (DQN) (Mnih et al., 2015) is a variant of the Q-learning algorithm (Watkins, 1989) that utilises deep learning.",2.4. Reinforcement Learning Algorithms,[0],[0]
"It uses a neural network to parametrise an approximation for the action-value function Q(s, a; ✓) using parameters ✓.",2.4. Reinforcement Learning Algorithms,[0],[0]
"Asynchronous Advantage Actor-Critic (A3C) (Mnih et al., 2016) is an asynchronous implementation of the advantage actor-critic paradigm (Sutton & Barto, 1998; Degris & Sutton, 2012), where separate threads run in parallel and perform updates to shared parameters.",2.4. Reinforcement Learning Algorithms,[0],[0]
"The different threads each hold their own instance of the environment and have different exploration policies, thereby decorrelating parameter updates without the need for experience replay.",2.4. Reinforcement Learning Algorithms,[0],[0]
"Therefore, A3C is an online algorithm, whereas DQN learns its policy offline, resulting in different learning dynamics be-
tween the two algorithms.",2.4. Reinforcement Learning Algorithms,[0],[0]
"Model-Free Episodic Control (EC) (Blundell et al., 2016) was proposed as a complementary learning system to the other RL algorithms described above.",2.4. Reinforcement Learning Algorithms,[0],[0]
"The EC algorithm relies on near-determinism of state transitions and rewards in RL environments; in settings where this holds, it can exploit these properties to memorise which action led to high returns in similar situations in the past.",2.4. Reinforcement Learning Algorithms,[0],[0]
"Since in its simplest form EC relies on a lookup table, it learns good policies much faster than value-function-approximation based deep RL algorithms like DQN trained via gradient descent - at the cost of generality (i.e. potentially poor performance in non-deterministic environments).",2.4. Reinforcement Learning Algorithms,[0],[0]
"We also compared our approach to that of UNREAL (Jaderberg et al., 2017), a recently proposed RL algorithm which also attempts to utilise unsupervised data in the environment.",2.4. Reinforcement Learning Algorithms,[0],[0]
"The UNREAL agent takes as a base an LSTM A3C agent (Mnih et al., 2016) and augments it with a number of unsupervised auxiliary tasks that make use of the rich perceptual data available to the agent besides the (sometimes very sparse) extrinsic reward signals.",2.4. Reinforcement Learning Algorithms,[0],[0]
This auxiliary learning tends to improve the representation learnt by the agent.,2.4. Reinforcement Learning Algorithms,[0],[0]
See Sec.,2.4. Reinforcement Learning Algorithms,[0],[0]
A.6 in Supplementary Materials for further details of the algorithms above.,2.4. Reinforcement Learning Algorithms,[0],[0]
We evaluate the performance of DARLA on different task and environment setups that probe subtly different aspects of domain adaptation.,3. Tasks,[0],[0]
"As a reminder, in Sec. 2.2 we defined ˆS as a state space that contains all possible conjunctions of high-level factors of variation necessary to generate any naturalistic observation in any Di 2 M. During domain adaptation scenarios agent observation states are generated according to soS ⇠ SimS(̂sS) and soT ⇠ SimT(̂sT) for the source and target domains respectively, where ŝS and ŝT are sampled by some distributions or processes GS and GT according to ŝS ⇠ GS( ˆS)",3. Tasks,[0],[0]
"and ŝT ⇠ GT ( ˆS).
",3. Tasks,[0],[0]
"We use DeepMind Lab (Beattie et al., 2016) to test a version of domain adaptation setup where the source and target domain observation simulators are equal (Sim
S
= Sim
T
),
but the processes used to sample ŝS and ŝT are different (GS 6= GT ).",3. Tasks,[0],[0]
"We use the Jaco arm with a matching MuJoCo simulation environment (Todorov et al., 2012) in two domain adaptation scenarios: simulation to simulation (sim2sim) and simulation to reality (sim2real).",3. Tasks,[0],[0]
The sim2sim domain adaptation setup is relatively similar to DeepMind Lab i.e. the source and target domains differ in terms of processes GS and GT .,3. Tasks,[0],[0]
"However, there is a significant point of difference.",3. Tasks,[0],[0]
"In DeepMind Lab, all values of factors in the target domain, ŝT , are previously seen in the source domain; however, ŝS 6=",3. Tasks,[0],[0]
"ŝT as the conjunctions of
these factor values are different.",3. Tasks,[0],[0]
"In sim2sim, by contrast, novel factor values are experienced in the target domain (this accordingly also leads to novel factor conjunctions).",3. Tasks,[0],[0]
"Hence, DeepMind Lab may be considered to be assessing domain interpolation performance, whereas sim2sim tests domain extrapolation.
",3. Tasks,[0],[0]
"The sim2real setup, on the other hand, is based on identical processes GS = GT , but different observation simulators Sim
S 6=",3. Tasks,[0],[0]
"Sim T corresponding to the MuJoCo simulation and the real world, which results in the so-called ‘perceptual reality gap’ (Rusu et al., 2016).",3. Tasks,[0],[0]
More details of the tasks are given below.,3. Tasks,[0],[0]
DeepMind Lab is a first person 3D game environment with rich visuals and realistic physics.,3.1. DeepMind Lab,[0],[0]
"We used a standard seekavoid object gathering setup, where a room is initialised with an equal number of randomly placed objects of two different types.",3.1. DeepMind Lab,[0],[0]
"One of the object varieties is ‘good’ (its collection is rewarded +1), while the other is ‘bad’ (its collection is punished -1).",3.1. DeepMind Lab,[0],[0]
"The full state space
ˆS consisted of all conjunctions of two room types (pink and green based on the colour of the walls) and four object types (hat, can, cake and balloon) (see Fig. 2A).",3.1. DeepMind Lab,[0],[0]
"The source domain DS con-
tained environments with hats/cans presented in the green room, and balloons/cakes presented in either the green or the pink room.",3.1. DeepMind Lab,[0],[0]
The target domain DT contained hats/cans presented in the pink room.,3.1. DeepMind Lab,[0],[0]
In both domains cans and balloons were the rewarded objects.,3.1. DeepMind Lab,[0],[0]
1) Learn to see: we used -VAEDAE to learn the disentangled latent state representation sz that includes both the room and the object generative factors of variation within DeepMind Lab.,3.1. DeepMind Lab,[0],[0]
"We had to use the high-level feature space of a pre-trained DAE within the -VAEDAE framework (see Section 2.3.1), instead of the pixel space of vanilla - VAE , because we found that objects failed to reconstruct when using the values of necessary to disentangle the generative factors of variation within DeepMind Lab (see Fig. 2B).
",3.1. DeepMind Lab,[0],[0]
-VAEDAE was trained on observations soU collected by an RL agent with a simple wall-avoiding policy ⇡U (otherwise the training data was dominated by close up images of walls).,3.1. DeepMind Lab,[0],[0]
"In order to enable the model to learn F(soU ) ⇡ ˆS , it is important to expose the agent to at least a minimal set of environments that span the range of values for each factor, and where no extraneous correlations are added between different factors 3 (see Fig.",3.1. DeepMind Lab,[0],[0]
"2A, yellow).",3.1. DeepMind Lab,[0],[0]
See Section A.3.1 in Supplementary Materials for details of -VAEDAE training.,3.1. DeepMind Lab,[0],[0]
2) Learn to act: the agent was trained with the algorithms detailed in Section 2.4 on a seek-avoid task using the source domain (DS) conjunctions of object/room shown in Fig.,3.1. DeepMind Lab,[0],[0]
2A (green).,3.1. DeepMind Lab,[0],[0]
"Pre-trained -VAEDAE from stage one was used as the ‘vision’ part of various RL algorithms (DQN, A3C and Episodic Control: Mnih et al., 2015; 2016; Blundell et al., 2016) to learn a source policy ⇡S that picks up balloons and avoids cakes in both the green and the pink rooms, and picks up cans and avoids hats in the green rooms.",3.1. DeepMind Lab,[0],[0]
"See Section A.3.1 in Supplementary Materials for more details of the various versions of DARLA we have tried, each based on a different base RL algorithm.",3.1. DeepMind Lab,[0],[0]
3) Transfer: we tested the ability of DARLA to transfer the seek-avoid policy ⇡S it had learnt on the source domain in stage two using the domain adaptation condition DT illustrated in Figure 2A (red).,3.1. DeepMind Lab,[0],[0]
"The agent had to continue picking up cans and avoid hats in the pink room, even though these objects had only been seen in the green room during source policy training.",3.1. DeepMind Lab,[0],[0]
The optimal policy ⇡T is one that maintains the reward polarity from the source domain (cans are good and hats are bad).,3.1. DeepMind Lab,[0],[0]
"For further details, see Appendix A.2.1.
3
In our setup of DeepMind Lab domain adaptation task, the object and environment factors are supposed to be independent.",3.1. DeepMind Lab,[0],[0]
"In order to ensure that -VAE DAE learns a factorised representation that reflects this ground truth independence, we present observations of all possible conjunctions of room and individual object types.",3.1. DeepMind Lab,[0],[0]
"We used frames from an RGB camera facing a robotic Jaco arm, or a matching rendered camera view from a MuJoCo physics simulation environment (Todorov et al., 2012) to investigate the performance of DARLA in two domain adaptation scenarios: 1) simulation to simulation (sim2sim), and 2) simulation to reality (sim2real).",3.2. Jaco Arm and MuJoCo,[0],[0]
"The sim2real setup is of particular importance, since the progress that deep RL has brought to control tasks in simulation (Schulman et al., 2015; Mnih et al., 2016; Levine & Abbeel, 2014; Heess et al., 2015; Lillicrap et al., 2015; Schulman et al., 2016) has not yet translated as well to reality, despite various attempts (Tobin et al., 2017; Tzeng et al., 2016; Daftry et al., 2016; Finn et al., 2015; Rusu et al., 2016).",3.2. Jaco Arm and MuJoCo,[0],[0]
"Solving control problems in reality is hard due to sparse reward signals, expensive data acquisition and the attendant danger of breaking the robot (or its human minders) during exploration.
",3.2. Jaco Arm and MuJoCo,[0],[0]
"In both sim2sim and sim2real, we trained the agent to perform an object reaching policy where the goal is to place the end effector as close to the object as possible.",3.2. Jaco Arm and MuJoCo,[0],[0]
"While conceptually the reaching task is simple, it is a hard control problem since it requires correct inference of the arm and object positions and velocities from raw visual inputs.",3.2. Jaco Arm and MuJoCo,[0],[0]
1) Learn to see: -VAE was trained on observations collected in MuJoCo simulations with the same factors of variation as in DS .,3.2. Jaco Arm and MuJoCo,[0],[0]
"In order to enable the model to learn F(soU ) ⇡ ŝ, a reaching policy was applied to phantom objects placed in random positions - therefore ensuring that the agent learnt the independent nature of the arm position and object position (see Fig. 2C, left); 2) Learn to act: a feedforward-A3C based agent with the vision module pre-trained in stage one was taught a source reaching policy ⇡S towards the real object in simulation (see Fig. 2C (left) for an example frame, and Sec.",3.2. Jaco Arm and MuJoCo,[0],[0]
A.4 in Supplementary Materials for a fuller description of the agent),3.2. Jaco Arm and MuJoCo,[0],[0]
.,3.2. Jaco Arm and MuJoCo,[0],[0]
In the source domain DS the agent was trained on a distribution of camera angles and positions.,3.2. Jaco Arm and MuJoCo,[0],[0]
The colour of the tabletop on which the arm rests and the object colour were both sampled anew every episode.,3.2. Jaco Arm and MuJoCo,[0],[0]
3) Transfer:,3.2. Jaco Arm and MuJoCo,[0],[0]
"sim2sim: in the target domain, DT , the agent was faced with a new distribution of camera angles and positions with little overlap with the source domain distributions, as well as a completely held out set of object colours (see Fig. 2C, middle).",3.2. Jaco Arm and MuJoCo,[0],[0]
"sim2real: in the target domain DT the camera position and angle as well as the tabletop colour and object colour were sampled from the same distributions as seen in the source domain DS , but the target domain DT was now the real world.",3.2. Jaco Arm and MuJoCo,[0],[0]
"Many details present in the real world such as shadows, specularity, multiple light sources and so on are not modelled in the simulation;
3
-3
0
z1 z2 z3 z4 z5 z6
Object Arm close/far left/right close/far right/leftup/downwrist turn
3
-3
0
z1 z2 z3 z4
Disentangled Entangled
Figure 4.",3.2. Jaco Arm and MuJoCo,[0],[0]
Plot of traversals of -VAE on MuJoCo.,3.2. Jaco Arm and MuJoCo,[0],[0]
"Using a disentangled -VAE model, single latents directly control for the factors responsible for the object or arm placements.
",3.2. Jaco Arm and MuJoCo,[0],[0]
the physics engine is also not a perfect model of reality.,3.2. Jaco Arm and MuJoCo,[0],[0]
"Thus sim2real tests the ability of the agent to cross the perceptual-reality gap and generalise its source policy ⇡S to the real world (see Fig. 2C, right).",3.2. Jaco Arm and MuJoCo,[0],[0]
"For further details, see Appendix A.2.2.",3.2. Jaco Arm and MuJoCo,[0],[0]
We evaluated the robustness of DARLA’s policy ⇡S learnt on the source domain to various shifts in the input data distribution.,4. Results,[0],[0]
"In particular, we used domain adaptation scenarios based on the DeepMind Lab seek-avoid task and the Jaco arm reaching task described in Sec. 3.",4. Results,[0],[0]
On each task we compared DARLA’s performance to that of various baselines.,4. Results,[0],[0]
"We evaluated the importance of learning ‘good’ vision during stage one of the pipeline, i.e one that maps the input observations so to disentangled representations sz ⇡ ŝ. In order to do this, we ran the DARLA pipeline with different vision models: the encoders of a
disentangled -VAE 4 (the original DARLA), an entangled -VAE (DARLA
ENT
), and a denoising autoencoder
(DARLA
DAE
).",4. Results,[0],[0]
"Apart from the nature of the learnt rep-
resentations sz , DARLA and all versions of its baselines were equivalent throughout the three stages of our proposed pipeline in terms of architecture and the observed data distribution (see Sec. A.3 in Supplementary Materials for more details).
",4. Results,[0],[0]
"Figs. 3-4 display the degree of disentanglement learnt by the vision modules of DARLA and DARLA
ENT
on Deep-
Mind Lab and MuJoCo.",4. Results,[0],[0]
"DARLA’s vision learnt to independently represent environment variables (such as room colour-scheme and geometry) and object-related variables (change of object type, size, rotation) on DeepMind Lab (Fig. 3, left).",4. Results,[0],[0]
Disentangling was also evident in MuJoCo.,4. Results,[0],[0]
"Fig. 4, left, shows that DARLA’s single latent units zi learnt to represent different aspects of the Jaco arm, the object, and the camera.",4. Results,[0],[0]
"By contrast, in the representations learnt by DARLA
ENT
, each latent is responsible for changes to
both the environment and objects (Fig. 3, right) in DeepMind Lab, or a mixture of camera, object and/or arm movements (Fig. 4, right) in MuJoCo.
",4. Results,[0],[0]
The table in Fig. 5 shows the average performance (across different seeds) in terms of rewards per episode of the various agents on the target domain with no fine-tuning of the source policy ⇡S .,4. Results,[0],[0]
"It can be seen that DARLA is able to zero-shot-generalise significantly better than DARLA
ENT
or DARLA
DAE
, highlighting the importance of learning a
disentangled representation sz = szŜ during the unsupervised stage one of the DARLA pipeline.",4. Results,[0],[0]
"In particular, this also demonstrates that the improved domain transfer performance is not simply a function of increased exposure to training observations, as both DARLA
ENT
and DARLA
DAE
were exposed to the same data.",4. Results,[0],[0]
The results are mostly consistent across target domains and in most cases DARLA is significantly better than the second-best-performing agent.,4. Results,[0],[0]
"This holds in the sim2real task 5 , where being able to perform zero-shot policy transfer is highly valuable due to the particular difficulties of gathering data in the real world.
",4. Results,[0],[0]
"DARLA’s performance is particularly surprising as it actually preserves less information about the raw observations so than DARLA
ENT
and DARLA
DAE
.",4. Results,[0],[0]
"This is due to the
nature of the -VAE and how it achieves disentangling; the disentangled model utilised a significantly higher value of the hyperparameter than the entangled model (see Appendix A.3 for further details), which constrains the ca-
4
In this section of the paper, we use the term -VAE to refer to a standard -VAE for the MuJoCo experiments, and a -VAE
DAE
for the DeepMind Lab experiments (as described in
stage 1 of Sec. 3.1).
",4. Results,[0],[0]
"5
See https://youtu.be/sZqrWFl0wQ4 for example sim2sim and sim2real zero-shot transfer policies of DARLA and baseline A3C agent.
165 166
Figure 5.",4. Results,[0],[0]
Table: Zero-shot performance (avg. reward per episode) of the source policy ⇡ S in target domains within DeepMind Lab and Jaco/MuJoCo environments.,4. Results,[0],[0]
Baseline agent refers to vanilla DQN/A3C/EC (DeepMind Lab) or A3C (Jaco) agents.,4. Results,[0],[0]
See main text for more detailed model descriptions.,4. Results,[0],[0]
Figure:,4. Results,[0],[0]
"Correlation between zero-shot performance transfer performance on the DeepMind Lab task obtained by EC based DARLA and the level of disentanglement as measured by the transfer/disentanglement score (r = 0.6, p < 0.001)
pacity of the latent channel.",4. Results,[0],[0]
"Indeed, DARLA’s -VAE only utilises 8 of its possible 32 Gaussian latents to store observation-specific information for MuJoCo/Jaco (and 20 in DeepMind Lab), whereas DARLA
ENT
utilises all 32 for
both environments (as does DARLA
DAE
).
",4. Results,[0],[0]
"Furthermore, we examined what happens if DARLA’s vision (i.e. the encoder of the disentangled -VAE ) is allowed to be fine-tuned via gradient updates while learning the source policy during stage two of the pipeline.",4. Results,[0],[0]
"This is denoted by DARLA
FT
in the table in Fig. 5.",4. Results,[0],[0]
"We see
that it exhibits significantly worse performance than that of DARLA in zero-shot domain adaptation using an A3Cbased agent in all tasks.",4. Results,[0],[0]
"This suggests that a favourable initialisation does not make up for subsequent overfitting to the source domain for the on-policy A3C. However, the off-policy DQN-based fine-tuned agent performs very well.",4. Results,[0],[0]
"We leave further investigation of this curious effect for future work.
",4. Results,[0],[0]
"Finally, we compared the performance of DARLA to an UNREAL (Jaderberg et al., 2017) agent with the same architecture.",4. Results,[0],[0]
"Despite also exploiting the unsupervised data available in the source domain, UNREAL performed worse than baseline A3C on the DeepMind Lab domain adaptation task.",4. Results,[0],[0]
"This further demonstrates that use of unsupervised data in itself is not a panacea for transfer performance; it must be utilised in a careful and structured manner conducive to learning disentangled latent states sz = szŜ .
",4. Results,[0],[0]
"In order to quantitatively evaluate our hypothesis that disentangled representations are essential for DARLA’s performance in domain adaptation scenarios, we trained various DARLAs with different degrees of learnt disentanglement in sz by varying (of -VAE) during stage one of the pipeline.",4. Results,[0],[0]
"We then calculated the correlation between the performance of the EC-based DARLA on the DeepMind Lab domain adaptation task and the transfer metric, which approximately measures the quality of disentanglement of DARLA’s latent representations sz (see Sec. A.5.2
in Supplementary Materials).",4. Results,[0],[0]
"This is shown in the chart in Fig. 5; as can be seen, there is a strong positive correlation between the level of disentanglement and DARLA’s zeroshot domain transfer performance (r = 0.6, p < 0.001).
",4. Results,[0],[0]
"Having shown the robust utility of disentangled representations in agents for domain adaptation, we note that there is evidence that they can provide an important additional benefit.",4. Results,[0],[0]
"We found significantly improved speed of learning of ⇡S on the source domain itself, as a function of how disentangled the model was.",4. Results,[0],[0]
"The gain in data efficiency from disentangled representations for source policy learning is not the main focus of this paper so we leave it out of the main text; however, we provide results and discussion in Section A.7 in Supplementary Materials.",4. Results,[0],[0]
We have demonstrated the benefits of using disentangled representations in a deep RL setting for domain adaptation.,5. Conclusion,[0],[0]
"In particular, we have proposed DARLA, a multi-stage RL agent.",5. Conclusion,[0],[0]
"DARLA first learns a visual system that encodes the observations it receives from the environment as disentangled representations, in a completely unsupervised manner.",5. Conclusion,[0],[0]
"It then uses these representations to learn a robust source policy that is capable of zero-shot domain adaptation.
",5. Conclusion,[0],[0]
"We have demonstrated the efficacy of this approach in a range of domains and task setups: a 3D naturalistic firstperson environment (DeepMind Lab), a simulated graphics and physics engine (MuJoCo), and crossing the simulation to reality gap (MuJoCo to Jaco sim2real).",5. Conclusion,[0],[0]
"We have also shown that the effect of disentangling is consistent across very different RL algorithms (DQN, A3C, EC), achieving significant improvements over the baseline algorithms (median 2.7 times improvement in zero-shot transfer across tasks and algorithms).",5. Conclusion,[0],[0]
"To the best of our knowledge, this is the first comprehensive empirical demonstration of the strength of disentangled representations for domain adaptation in a deep RL setting.",5. Conclusion,[0],[0]
Domain adaptation is an important open problem in deep reinforcement learning (RL).,abstractText,[0],[0]
"In many scenarios of interest data is hard to obtain, so agents may learn a source policy in a setting where data is readily available, with the hope that it generalises well to the target domain.",abstractText,[0],[0]
"We propose a new multi-stage RL agent, DARLA (DisentAngled Representation Learning Agent), which learns to see before learning to act.",abstractText,[0],[0]
DARLA’s vision is based on learning a disentangled representation of the observed environment.,abstractText,[0],[0]
"Once DARLA can see, it is able to acquire source policies that are robust to many domain shifts even with no access to the target domain.",abstractText,[0],[0]
"DARLA significantly outperforms conventional baselines in zero-shot domain adaptation scenarios, an effect that holds across a variety of RL environments (Jaco arm, DeepMind Lab) and base RL algorithms (DQN, A3C and EC).",abstractText,[0],[0]
DARLA: Improving Zero-Shot Transfer in Reinforcement Learning,title,[0],[0]
"In the context of machine learning, it is not uncommon to have to repeatedly optimize a set of functions that are fundamentally related to each other.",1. Introduction,[0],[0]
"In this paper, we focus on a class of functions called submodular functions.",1. Introduction,[0],[0]
These functions exhibit a mathematical diminishing returns property that allows us to find nearly-optimal solutions in linear time.,1. Introduction,[0],[0]
"However, modern datasets are growing so large that even linear time solutions can be computationally expensive.",1. Introduction,[0],[0]
"Ideally, we want to find a sublinear summary of the given dataset so that optimizing these related functions over this reduced subset is nearly as effective, but not nearly as expensive, as optimizing them over the full dataset.
",1. Introduction,[0],[0]
"As a concrete example, suppose Uber is trying to give their
*Equal contribution 1Department of Computer Science, Yale University, New Haven, Connecticut, USA 2Google Research, Zurich, Switzerland.",1. Introduction,[0],[0]
"Correspondence to: Ehsan Kazemi <ehsan.kazemi@yale.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
drivers suggested waiting locations across New York City based on historical rider pick-ups.,1. Introduction,[0],[0]
"Even if they discretize the potential waiting locations to just include points at which pick-ups have occurred in the past, there are still hundreds of thousands, if not millions, of locations to consider.",1. Introduction,[0],[0]
"If they wish to update these ideal waiting locations every day (or at any routine interval), it would be invaluable to be able to drastically reduce the number of locations that need to be evaluated, and still achieve nearly optimal results.
",1. Introduction,[0],[0]
"In this scenario, each day would have a different function that quantifies the value of a set of locations for that particular day.",1. Introduction,[0],[0]
"For example, in the winter months, spots near ice skating rinks would be highly valuable, while in the summer months, waterfront venues might be more prominent.",1. Introduction,[0],[0]
"On the other hand, major tourist destinations like Times Square will probably be busy year-round.
",1. Introduction,[0],[0]
"In other words, although the most popular pick-up locations undoubtedly vary over time, there is also some underlying distribution of the user behavior that remains relatively constant and ties the various days together.",1. Introduction,[0],[0]
"This means that even though the functions for future days are technically unknown, if we can select a good reduced subset of candidate locations based on the functions derived from historical data, then this same reduced subset should perform well on future functions that we cannot explicitly see yet.
",1. Introduction,[0],[0]
"In more mathematical terms, consider some unknown distribution of functions D and a ground set Ω of n elements to pick from.",1. Introduction,[0],[0]
We want to select a subset S of ` elements (with ` n) such that optimizing functions (drawn from this distribution D) over the reduced subset S is comparable to optimizing them over the entire ground set,1. Introduction,[0],[0]
"Ω.
This problem was first introduced by Balkanski et al. (2016) as two-stage submodular maximization.",1. Introduction,[0],[0]
This name comes from the idea that the overall framework can be viewed as two separate stages.,1. Introduction,[0],[0]
"First, we want to use the given functions to select a representative subset S, that is ideally sublinear in size of the entire ground set Ω.",1. Introduction,[0],[0]
"In the second stage, for any functions drawn from this same distribution, we can optimize over S, which will be much faster than optimizing over Ω.
Our Contributions.",1. Introduction,[0],[0]
"In today’s era of massive data, an algorithm is rarely practical if it is not scalable.",1. Introduction,[0],[0]
"In this
paper, we build on existing work to provide solutions for two-stage submodular maximization in both the streaming and distributed settings.",1. Introduction,[0],[0]
Table 1 summarizes the theoretical results of this paper and compares them with the previous state of the art.,1. Introduction,[0],[0]
The proofs of all the theoretical results are deferred to the Supplementary Material.,1. Introduction,[0],[0]
Data summarization is one of the most natural applications that falls under the umbrella of submodularity.,2. Related Work,[0],[0]
"As such, there are many existing works applying submodular theory to a variety of important summarization settings.",2. Related Work,[0],[0]
"For example, Mirzasoleiman et al. (2013) used an exemplar-based clustering approach to select representative images from the Tiny Images dataset (Torralba et al., 2008).",2. Related Work,[0],[0]
"Kirchhoff & Bilmes (2014) and Feldman et al. (2018) also worked on submodular image summarization, while Lin & Bilmes (2011) and Wei et al. (2013) focused on document summarization.
",2. Related Work,[0],[0]
"In addition to data summarization, submodularity appears in a wide variety of other machine learning applications including variable selection (Krause & Guestrin, 2005), recommender systems (Gabillon et al., 2013), crowd teaching (Singla et al., 2014), neural network interpretability (Elenberg et al., 2017), robust optimization (Kazemi et al., 2017), network monitoring (Gomez Rodriguez et al., 2010), and influence maximization in social networks (Kempe et al., 2003).
",2. Related Work,[0],[0]
There have also been many successful efforts in scalable submodular optimization.,2. Related Work,[0],[0]
For our distributed implementation we will primarily build on the framework developed by Barbosa et al. (2015).,2. Related Work,[0],[0]
"Other similar algorithms include works by Mirzasoleiman et al. (2013) and Mirrokni & Zadimoghaddam (2015), as well as Kumar et al. (2015).",2. Related Work,[0],[0]
"In terms of the streaming setting, there are two existing works we will focus on: Badanidiyuru et al. (2014) and Buchbinder et al. (2015).",2. Related Work,[0],[0]
"The key difference between the two is that Badanidiyuru et al. (2014) relies on thresholding and will terminate as soon as k elements are selected from the stream, while Buchbinder et al. (2015) will continue through the end of the stream, swapping elements in and out when required.
",2. Related Work,[0],[0]
"Repeated optimization of related submodular functions has
been a well-studied problem with works on structured prediction (Lin & Bilmes, 2012), submodular bandits (Yue & Guestrin, 2011; Chen et al., 2017), and online submodular optimization (Jegelka & Bilmes, 2011).",2. Related Work,[0],[0]
"However, unlike our work, these approaches are not concerned with data summarization as a key pre-processing step.
",2. Related Work,[0],[0]
The problem of two-stage submodular maximization was first introduced by Balkanski et al. (2016).,2. Related Work,[0],[0]
"They present two algorithms with strong approximation guarantees, but both runtimes are prohibitively expensive.",2. Related Work,[0],[0]
"Recently, Stan et al. (2017) presented a new algorithm known as REPLACEMENT-GREEDY that improved the approximation guarantee from 12 (1 − 1e ) to 12 (1 − 1e2 ) and the run time from O(km`n2 log(n)) to O(km`n).",2. Related Work,[0],[0]
"They also show that, under mild conditions over the functions, maximizing over the sublinear summary can be arbitrarily close to maximizing over the entire ground set.",2. Related Work,[0],[0]
"In a nutshell, their method indirectly constructs the summary S by greedily building up solutions Ti for each given function fi simultaneously over ` rounds.
",2. Related Work,[0],[0]
"Although Balkanski et al. (2016) and Stan et al. (2017) presented centralized algorithms with constant factor approximation guarantees, there is a dire need for scalable solutions in order for the algorithm to be practically useful.",2. Related Work,[0],[0]
"In particular, the primary purpose of two-stage submodular maximization is to tackle problems where the dataset is too large to be repeatedly optimized by simple greedy-based approaches.",2. Related Work,[0],[0]
"As a result, in many cases, the datasets can be so large that existing algorithms cannot even be run once.",2. Related Work,[0],[0]
"The greedy approach requires that the entire data must fit into main memory, which may not be possible, thus requiring a streaming-based solution.",2. Related Work,[0],[0]
"Furthermore, even if we have enough memory, the problem may simply be so large that it requires a distributed approach in order to run in any reasonable amount of time.",2. Related Work,[0],[0]
"In general, if we want to optimally choose ` out of n items, we need to consider every single one of the exponentially many possibilities.",3. Problem Definition,[0],[0]
"This makes the problem intractable for any reasonable number of elements, let alone the billions of elements that are common in modern datasets.",3. Problem Definition,[0],[0]
"Fortunately,
many data summarization formulations satisfy an intuitive diminishing returns property known as submodularity.
",3. Problem Definition,[0],[0]
"More formally, a set function f : 2V → R is submodular (Fujishige, 2005; Krause & Golovin, 2012)",3. Problem Definition,[0],[0]
"if, for all sets A ⊆ B ⊆ V and every element v ∈ V \ B, we have f(A + v)",3. Problem Definition,[0],[0]
− f(A) ≥ f(B + v),3. Problem Definition,[0],[0]
"− f(B).1 That is, the marginal contribution of any element v to the value of f(A) diminishes as the set A grows.
",3. Problem Definition,[0],[0]
"Moreover, a submodular function f is said to be monotone if f(A) ≤ f(B) for all sets A ⊆ B ⊆ V .",3. Problem Definition,[0],[0]
"That is, adding elements to a set cannot decrease its value.",3. Problem Definition,[0],[0]
"Thanks to a celebrated result by Nemhauser et al. (1978), we know that if our function f is monotone submodular, then the classical greedy algorithm will obtain a (1 − 1/e)-approximation to the optimal value.",3. Problem Definition,[0],[0]
"Therefore, we can nearly-optimize monotone submodular functions in linear time.
",3. Problem Definition,[0],[0]
"Now we formally re-state the problem we are going to solve.
",3. Problem Definition,[0],[0]
Problem Statement.,3. Problem Definition,[0],[0]
Consider some unknown distribution D of monotone submodular functions and a ground set Ω of n elements to choose from.,3. Problem Definition,[0],[0]
"We want to select a set S of at most ` items that maximizes the following function:
G(S) =",3. Problem Definition,[0],[0]
"Ef∼D[ max T⊆S,|T |≤k f(T )].",3. Problem Definition,[0],[0]
"(1)
That is, the set S we choose should be optimal in expectation over all functions in this distribution D. However, in general, the distribution D is unknown and we only have access to a small set of functions F = (f1, . . .",3. Problem Definition,[0],[0]
", fm) drawn from D. Therefore, the best approximation we have is to optimize the following related function:
Gm(S) = 1
",3. Problem Definition,[0],[0]
m m∑ i=1,3. Problem Definition,[0],[0]
"max T∗i ⊆S,|T∗i |≤k fi(T ∗",3. Problem Definition,[0],[0]
i ).,3. Problem Definition,[0],[0]
"(2)
Stan et al. (2017, Theorem 1) shows that with enough sample functions, Gm(S) becomes an arbitrarily good approximation to G(S).
",3. Problem Definition,[0],[0]
"To be clear, each T ∗i ⊂ S is the corresponding size k optimal solution for fi.",3. Problem Definition,[0],[0]
"However, in general we cannot find the true optimal T ∗i , so throughout the paper we will use Ti to denote the approximately-optimal size k solution we select for each fi.",3. Problem Definition,[0],[0]
"Table 2 (Appendix A) summarizes the important terminology and can be used as a reference, if needed.
",3. Problem Definition,[0],[0]
"It is very important to note that although each function fi is monotone submodular, G(S) is not submodular (Balkanski et al., 2016), and thus using the regular greedy algorithm to directly build up S will give no theoretical guarantees.",3. Problem Definition,[0],[0]
"We also note that although G(S) is an instance of an XOS function (Feige, 2009), existing methods that use the XOS property would require an exponential number of evaluations in this scenario (Stan et al., 2017).
",3. Problem Definition,[0],[0]
"1For notational convenience, we use A+ v = A ∪ {v}.",3. Problem Definition,[0],[0]
"In many applications, the data naturally arrives in a streaming fashion.",4. Streaming Algorithm,[0],[0]
"This may be because the data is too large to fit in memory, or simply because the data is arriving faster than we can store it.",4. Streaming Algorithm,[0],[0]
"Therefore, in the streaming setting we are shown one element at a time and we must immediately decide whether or not to keep this element.",4. Streaming Algorithm,[0],[0]
"There is a limited number of elements we can hold at any one time and once an element is rejected it cannot be brought back.
",4. Streaming Algorithm,[0],[0]
There are two general approaches for submodular maximization (under the cardinality constraint k) in the streaming setting: (i) Badanidiyuru et al. (2014) introduced a thresholding-based framework where each element from the stream is added only if its marginal value is at least 12k of the optimum value.,4. Streaming Algorithm,[0],[0]
"The optimum is usually not known a priori, but they showed that with only a logarithmic increase in memory requirement, it is possible to efficiently guess the optimum value.",4. Streaming Algorithm,[0],[0]
(ii) Buchbinder et al. (2015) introduced streaming submodular maximization with preemption.,4. Streaming Algorithm,[0],[0]
"At each step, they keep a solution A of size k with value f(A).",4. Streaming Algorithm,[0],[0]
Each incoming element is added if and only if it can be exchanged with a current element of A for a net gain of at least f(A)k .,4. Streaming Algorithm,[0],[0]
"In this paper, we combine these two approaches in a novel and non trivial way in order to design a streaming algorithm (called REPLACEMENT-STREAMING) for the two-stage submodular maximization problem.
",4. Streaming Algorithm,[0],[0]
"The goal of REPLACEMENT-STREAMING is to pick a set S of at most ` elements from the data stream, where we keep sets Ti ⊆ S, 1 ≤",4. Streaming Algorithm,[0],[0]
i ≤ m as the solutions for functions fi.,4. Streaming Algorithm,[0],[0]
"We continue to process elements until one of the two following conditions holds: (i) ` elements are chosen, or (ii) the data stream ends.",4. Streaming Algorithm,[0],[0]
This algorithm starts from empty sets S and {Ti}.,4. Streaming Algorithm,[0],[0]
"For every incoming element ut, we use the subroutine EXCHANGE to determine whether we should keep that element or not.",4. Streaming Algorithm,[0],[0]
"To formally describe EXCHANGE, we first need to define a few notations.
",4. Streaming Algorithm,[0],[0]
We define the marginal gain of adding an element x to a set A as follows: fi(x|A) = fi(x + A) − fi(A).,4. Streaming Algorithm,[0],[0]
"For an element x and set A, REPi(x,A) is an element of A such that removing it from A and replacing it with x results in the largest gain for function fi, i.e.,
REPi(x,A) = arg max y∈A fi(A+ x− y)− fi(A).",4. Streaming Algorithm,[0],[0]
"(3)
The value of this largest gain is represented by ∆i(x,A) = fi(A+",4. Streaming Algorithm,[0],[0]
x−,4. Streaming Algorithm,[0],[0]
"REPi(x,A))− fi(A).",4. Streaming Algorithm,[0],[0]
"(4)
We define the gain of an element x with respect to a set A as follows:
∇i(x,A)",4. Streaming Algorithm,[0],[0]
=,4. Streaming Algorithm,[0],[0]
"{
1{fi(x|A)≥(α/k)·fi(A)}fi(x|A)",4. Streaming Algorithm,[0],[0]
"if |A| < k, 1{∆i(x,A)≥(α/k)·fi(A)}∆i(x,A) o.w.,
where 1 is the indicator function",4. Streaming Algorithm,[0],[0]
.,4. Streaming Algorithm,[0],[0]
"That is, ∇i(x,A) tells us how much we can increase the value of fi(A) by either
Algorithm 1 EXCHANGE 1:",4. Streaming Algorithm,[0],[0]
"Input: u, S, {Ti}, τ and α {∇i terms use α} 2: if |S| < ` then 3: if 1m ∑m i=1∇i(u, Ti) ≥ τ then
4: S ← S + u 5: for 1 ≤",4. Streaming Algorithm,[0],[0]
"i ≤ m do 6: if∇i(u, Ti) > 0",4. Streaming Algorithm,[0],[0]
"then 7: if |Ti| < k then 8: Ti ← Ti + u 9: else
10: Ti ← Ti + u− REP(u, Ti)
adding x to A (if |A| < k) or optimally swapping it in (if |A| = k).",4. Streaming Algorithm,[0],[0]
"However, if this potential increase is less than α k · fi(A), then ∇i(x,A) = 0.",4. Streaming Algorithm,[0],[0]
"In other words, if the gain of an element does not pass a threshold of αk · fi(A), we consider its contribution to be 0.
",4. Streaming Algorithm,[0],[0]
An incoming element is picked if the average of the ∇i terms is larger than or equal to a threshold τ .,4. Streaming Algorithm,[0],[0]
"Indeed, for ut, the EXCHANGE routine computes the average gain 1 m ∑m i=1∇i(ut, Ti).",4. Streaming Algorithm,[0],[0]
"If this average gain is at least τ , then ut is added to S; ut is also added to all sets Ti with ∇i(ut, Ti) > 0.",4. Streaming Algorithm,[0],[0]
Algorithm 1 explains EXCHANGE in detail.,4. Streaming Algorithm,[0],[0]
Now we define the optimum solution to Eq.,4. Streaming Algorithm,[0],[0]
"(2) by
Sm,` = arg max S⊆Ω,|S|≤`
1
m m∑ i=1 max |T |≤k,T⊆S fi(T ),
where the optimum solution to each function is defined by
Sm,`i = arg max S⊆Sm,`,|S|≤k fi(S).
",4. Streaming Algorithm,[0],[0]
"We define OPT = 1m ∑m i=1 fi(S m,` i ).
",4. Streaming Algorithm,[0],[0]
"In Section 4.1, we assume that the value of OPT is known a priori.",4. Streaming Algorithm,[0],[0]
"This allows us to design REPLACEMENTSTREAMING-KNOW-OPT, which has a constant factor approximation guarantee.",4. Streaming Algorithm,[0],[0]
"Furthermore, in Section 4.2, we show how we can efficiently guess the value of OPT by a moderate increase in the memory requirement.",4. Streaming Algorithm,[0],[0]
This enables us to finally explain REPLACEMENT-STREAMING.,4. Streaming Algorithm,[0],[0]
"If OPT is somehow known a priori, we can use REPLACEMENT-STREAMING-KNOW-OPT.",4.1. Knowing OPT,[0],[0]
"As shown in Algorithm 2, we begin with empty sets S and {Ti}.",4.1. Knowing OPT,[0],[0]
"For each incoming element ut, it uses EXCHANGE to update sets S and {Ti}.",4.1. Knowing OPT,[0],[0]
The threshold parameter τ in EXCHANGE is set to OPTβ` for a constant value of β.,4.1. Knowing OPT,[0],[0]
"This threshold guarantees that if an element is added to S, then the average of functions fi over Ti’s is increased by a value of at least OPTβ` .",4.1. Knowing OPT,[0],[0]
"Therefore, if we end up with ` elements in S, we guarantee that 1m ∑m i=1",4.1. Knowing OPT,[0],[0]
fi(Ti),4.1. Knowing OPT,[0],[0]
≥ OPTβ .,4.1. Knowing OPT,[0],[0]
"On the other hand, if |S| < `, we are still able to prove that our algorithm has picked good
Algorithm 2 REPLACEMENT-STREAMING-KNOW-OPT 1: Input: OPT, α and β 2: Output: Sets S and {Ti}1≤i≤m, where Ti ⊂ S 3: S ← ∅ and 4: Ti ← ∅ for all 1 ≤",4.1. Knowing OPT,[0],[0]
"i ≤ m 5: for every arriving element ut do 6: EXCHANGE(ut, S, {Ti}, OPTβ` , α) 7: Return: S and {Ti}1≤i≤m
enough elements such that 1m ∑m i=1",4.1. Knowing OPT,[0],[0]
fi(Ti) ≥ α·(β−1)·OPT β·((α+1)2+α) .,4.1. Knowing OPT,[0],[0]
The pseudocode of REPLACEMENT-STREAMING-KNOWOPT is provided in Algorithm 2.,4.1. Knowing OPT,[0],[0]
Theorem 1.,4.1. Knowing OPT,[0],[0]
"The approximation factor of REPLACEMENT-STREAMING-KNOW-OPT is at least min{ α(β−1)β·((α+1)2+α) , 1β }.",4.1. Knowing OPT,[0],[0]
"Hence, for α = 1 and β = 6 the competitive ratio is at least 1/6.",4.1. Knowing OPT,[0],[0]
"In this section, we discuss ideas on how to efficiently guess the value of OPT, which is generally not known a priori.",4.2. Guessing OPT in the Streaming Setting,[0],[0]
"First consider Lemma 1, which provides bounds on OPT.",4.2. Guessing OPT in the Streaming Setting,[0],[0]
Lemma 1.,4.2. Guessing OPT in the Streaming Setting,[0],[0]
Assume δ,4.2. Guessing OPT in the Streaming Setting,[0],[0]
= 1m maxu∈Ω ∑m i=1,4.2. Guessing OPT in the Streaming Setting,[0],[0]
fi(u).,4.2. Guessing OPT in the Streaming Setting,[0],[0]
"Then we have δ ≤ OPT ≤ ` · δ.
",4.2. Guessing OPT in the Streaming Setting,[0],[0]
"Now consider the following set:
Γ = {(1 + )",4.2. Guessing OPT in the Streaming Setting,[0],[0]
"l | l ∈ Z, δ 1 + ≤",4.2. Guessing OPT in the Streaming Setting,[0],[0]
(1 + ),4.2. Guessing OPT in the Streaming Setting,[0],[0]
l ≤,4.2. Guessing OPT in the Streaming Setting,[0],[0]
"` · δ}
We define τl = (1 + )",4.2. Guessing OPT in the Streaming Setting,[0],[0]
"l. From Lemma 1, we know that one of the τl ∈ Γ is a good estimate of OPT.",4.2. Guessing OPT in the Streaming Setting,[0],[0]
"More formally, there exists a τl ∈ Γ such that OPT1+ ≤ τl ≤ OPT.",4.2. Guessing OPT in the Streaming Setting,[0],[0]
"For this reason, we should run parallel instances of Algorithm 2, one for each τl ∈ Γ.",4.2. Guessing OPT in the Streaming Setting,[0],[0]
The number of such thresholds is O( log ` ).,4.2. Guessing OPT in the Streaming Setting,[0],[0]
"The final answer is the best solution obtained among all the instances.
",4.2. Guessing OPT in the Streaming Setting,[0],[0]
Note that we do not know the value of δ in advance.,4.2. Guessing OPT in the Streaming Setting,[0],[0]
"So we would need to make one pass over the data to learn δ, which is not possible in the streaming setting.",4.2. Guessing OPT in the Streaming Setting,[0],[0]
"The question is, can we get a good enough estimate of δ within a single pass over the data?",4.2. Guessing OPT in the Streaming Setting,[0],[0]
"Let’s define δt = 1m maxut′ ,t′≤t ∑m i=1",4.2. Guessing OPT in the Streaming Setting,[0],[0]
"fi(u
t′)",4.2. Guessing OPT in the Streaming Setting,[0],[0]
as our current guess for the maximum value of δ,4.2. Guessing OPT in the Streaming Setting,[0],[0]
.,4.2. Guessing OPT in the Streaming Setting,[0],[0]
"Unfortunately, getting δt as an estimate of δ does not resolve the problem.",4.2. Guessing OPT in the Streaming Setting,[0],[0]
This is due to the fact that a newly instantiated threshold τ could potentially have already seen elements with additive value of τ/(β`).,4.2. Guessing OPT in the Streaming Setting,[0],[0]
"For this reason, we instantiate thresholds for an increased range of δt/(1 + )",4.2. Guessing OPT in the Streaming Setting,[0],[0]
≤ τl ≤ ` · β · δt.,4.2. Guessing OPT in the Streaming Setting,[0],[0]
"To show that this new range would solve the problem, first consider the next lemma.",4.2. Guessing OPT in the Streaming Setting,[0],[0]
Lemma 2.,4.2. Guessing OPT in the Streaming Setting,[0],[0]
"For the maximum gain of an incoming element ut, we have 1m ∑m i=1∇i(ut, T t−1i ) ≤ δt.
",4.2. Guessing OPT in the Streaming Setting,[0],[0]
"We need to show that for a newly instantiated threshold τ at time t+ 1, the gain of all elements which arrived before
Algorithm 3 REPLACEMENT-STREAMING
1: Γ0 = {(1 + )l|l ∈ Z} 2: For each τ ∈ Γ0 set Sτ ← ∅ and Tτ,i ← ∅ for all
1 ≤ i ≤ m {Maintain the sets lazily} 3: δ0 ← 0 4: for every arriving element ut do 5: δt = max{δt−1, 1m ∑m i=1",4.2. Guessing OPT in the Streaming Setting,[0],[0]
"fi(u
t)}",4.2. Guessing OPT in the Streaming Setting,[0],[0]
6:,4.2. Guessing OPT in the Streaming Setting,[0],[0]
Γt = {(1 + ),4.2. Guessing OPT in the Streaming Setting,[0],[0]
"l | l ∈ Z, δt(1+ )·",4.2. Guessing OPT in the Streaming Setting,[0],[0]
β·` ≤ (1 + ),4.2. Guessing OPT in the Streaming Setting,[0],[0]
"l ≤ δt} 7: Delete all Sτ and Tτ,i such that τ /∈",4.2. Guessing OPT in the Streaming Setting,[0],[0]
Γt 8: for all τ ∈,4.2. Guessing OPT in the Streaming Setting,[0],[0]
"Γt do 9: EXCHANGE(ut, Sτ , {Tτ,i}1≤i≤m, τ, α)
10: Return: arg maxτ∈Γn{ 1m ∑m i=1",4.2. Guessing OPT in the Streaming Setting,[0],[0]
"fi(Tτ,i)}
time t+ 1 is less than τ ; therefore this new instance of the algorithm would not have picked them if it was instantiated from the beginning.",4.2. Guessing OPT in the Streaming Setting,[0],[0]
"To prove this, note that since τ is a new threshold at time t + 1, we have τ >",4.2. Guessing OPT in the Streaming Setting,[0],[0]
"`·β·δ t
β·` = δ t.
From Lemma 2 we conclude that the marginal gain of all the ut ′ , t′ ≤ t is less than τ and EXCHANGE would not have picked them.",4.2. Guessing OPT in the Streaming Setting,[0],[0]
"The REPLACEMENT-STREAMING algorithm is shown pictorially in Figure 1 and the pseudocode is given in Algorithm 3.
",4.2. Guessing OPT in the Streaming Setting,[0],[0]
Theorem 2.,4.2. Guessing OPT in the Streaming Setting,[0],[0]
"Algorithm 3 satisfies the following properties:
• It outputs sets S and {Ti} ⊂ S for 1 ≤",4.2. Guessing OPT in the Streaming Setting,[0],[0]
"i ≤ m, such that |S| ≤",4.2. Guessing OPT in the Streaming Setting,[0],[0]
"`, |Ti| ≤ k and 1m ∑m i=1",4.2. Guessing OPT in the Streaming Setting,[0],[0]
fi(Ti),4.2. Guessing OPT in the Streaming Setting,[0],[0]
"≥
min{ α(β−1)β((α+1)2+α) , 1β(1+ )} · OPT.
",4.2. Guessing OPT in the Streaming Setting,[0],[0]
• For α = 1 and β = 6+ 1+ the approximation factor is at least 16+ .,4.2. Guessing OPT in the Streaming Setting,[0],[0]
"For = 1.0 the approximation factor is 1/7.
",4.2. Guessing OPT in the Streaming Setting,[0],[0]
• It makes one pass over the dataset and stores at most O( ` log ` ) elements.,4.2. Guessing OPT in the Streaming Setting,[0],[0]
"The update time per each element is O(km log ` ).
",4.2. Guessing OPT in the Streaming Setting,[0],[0]
Algorithm 4 REPLACEMENT-DISTRIBUTED 1: for e ∈ Ω do 2: Assign e to a machine chosen uniformly at random 3:,4.2. Guessing OPT in the Streaming Setting,[0],[0]
"Run REPLACEMENT-GREEDY on each machine l to
obtain Sl and {T li } for 1 ≤",4.2. Guessing OPT in the Streaming Setting,[0],[0]
"i ≤ m 4: S, {Ti} ← arg maxSl,{T li } 1 m ∑m i=1",4.2. Guessing OPT in the Streaming Setting,[0],[0]
fi(T,4.2. Guessing OPT in the Streaming Setting,[0],[0]
"l i )
5: S′, {T ′i} ← REPLACEMENT-GREEDY( ⋃ l S
l) 6:",4.2. Guessing OPT in the Streaming Setting,[0],[0]
Return: arg max{ 1m ∑m i=1,4.2. Guessing OPT in the Streaming Setting,[0],[0]
"fi(Ti), 1 m ∑m i=1",4.2. Guessing OPT in the Streaming Setting,[0],[0]
fi(T ′,4.2. Guessing OPT in the Streaming Setting,[0],[0]
i )},4.2. Guessing OPT in the Streaming Setting,[0],[0]
"In recent years, there have been several successful approaches to the problem of distributed submodular maximization (Kumar et al., 2015; Mirzasoleiman et al., 2013; Mirrokni & Zadimoghaddam, 2015; Barbosa et al., 2015).",5. Distributed Algorithm,[0],[0]
"Specifically, Barbosa et al. (2015) proved that the following simple procedure results in a distributed algorithm with a constant factor approximation guarantee: (i) randomly split the data amongst M machines, (ii) run the classical greedy on each machine and pass outputs to a central machine, (iii) run another instance of the greedy algorithm over the union of all the collected outputs from all M machines, and (iv) output the maximizing set amongst all the collected solutions.",5. Distributed Algorithm,[0],[0]
"Although our objective function G(S) is not submodular, we use a similar framework and still manage to prove that our algorithms achieve constant factor approximations to the optimal solution.
",5. Distributed Algorithm,[0],[0]
"In REPLACEMENT-DISTRIBUTED (Algorithm 4), a central machine first randomly partitions data among M machines.",5. Distributed Algorithm,[0],[0]
"Next, each machine runs REPLACEMENT-GREEDY (Stan et al., 2017) on its assigned data.",5. Distributed Algorithm,[0],[0]
"The outputs Sl, {T li } of all the machines are sent to the central machine, which runs another instance of REPLACEMENT-GREEDY over the union of all the received answers.",5. Distributed Algorithm,[0],[0]
"Finally, the highest value set amongst all collected solutions is returned as the final answer.",5. Distributed Algorithm,[0],[0]
"See Appendix F for a detailed explanation of REPLACEMENT-GREEDY.
Theorem 3.",5. Distributed Algorithm,[0],[0]
"The REPLACEMENT-DISTRIBUTED algorithm outputs sets S∗, {T ∗i } ⊂ S, with |S∗| ≤ `, |T ∗i | ≤ k, such that
E[ 1
m m∑ i=1 fi(T ∗",5. Distributed Algorithm,[0],[0]
i )],5. Distributed Algorithm,[0],[0]
"≥ α 2 · OPT,
where α = 12 (1− 1e2 ).",5. Distributed Algorithm,[0],[0]
"The time complexity of algorithm is O(km`n/M + Mkm`2).
",5. Distributed Algorithm,[0],[0]
"Unfortunately, for very large datasets, the time complexity of REPLACEMENT-GREEDY could be still prohibitive.",5. Distributed Algorithm,[0],[0]
"For this reason, we can use a modified version of REPLACEMENT-STREAMING (called REPLACEMENTPSEUDO-STREAMING) to design an even more scalable distributed algorithm.",5. Distributed Algorithm,[0],[0]
"This algorithm receives all elements in a centralized way, but it uses a predefined order to generate a (pseudo) stream before processing the data.",5. Distributed Algorithm,[0],[0]
"This
consistent ordering is used to ensure that the output of REPLACEMENT-PSEUDO-STREAMING is independent of the random ordering of the elements.",5. Distributed Algorithm,[0],[0]
"The only other difference between REPLACEMENT-PSEUDO-STREAMING and REPLACEMENT-STREAMING is that it outputs all sets Sτ , {Tτ,i} for all τ ∈ Γn (instead of just the maximum).",5. Distributed Algorithm,[0],[0]
"We use this modified algorithm as one of the main building blocks for DISTRIBUTED-FAST (outlined in Appendix E).
",5. Distributed Algorithm,[0],[0]
Theorem 4.,5. Distributed Algorithm,[0],[0]
"The DISTRIBUTED-FAST algorithm outputs sets S∗, {T ∗i } ⊂ S, with |S∗| ≤ `, |T ∗i | ≤ k, such that
E[ 1
m m∑ i=1 fi(T ∗",5. Distributed Algorithm,[0],[0]
i )],5. Distributed Algorithm,[0],[0]
"≥ α · γ α+ γ · OPT,
where α = 12 (1 − 1e2 ) and γ = 16+ .",5. Distributed Algorithm,[0],[0]
The time complexity of algorithm is O(kmn log /̀M,5. Distributed Algorithm,[0],[0]
"+ Mkm`2 log `).
",5. Distributed Algorithm,[0],[0]
"From Theorems 3 and 4, we conclude that the optimum number of machines M for REPLACEMENT-DISTRIBUTED and DISTRIBUTED-FAST isO( √ n/`)",5. Distributed Algorithm,[0],[0]
"andO( √ n/`), respectively.",5. Distributed Algorithm,[0],[0]
"Therefore, DISTRIBUTED-FAST is a factor of O( √ n/log `) and O( √ /̀log `) faster than REPLACEMENT-GREEDY and REPLACEMENT-DISTRIBUTED, respectively.",5. Distributed Algorithm,[0],[0]
"In this section, we evaluate the performance of our algorithms in both the streaming and distributed settings.",6. Applications,[0],[0]
We compare our work against several different baselines.,6. Applications,[0],[0]
"In this experiment, we will use a subset of the VOC2012 dataset (Everingham et al.).",6.1. Streaming Image Summarization,[0],[0]
"This dataset has images containing objects from 20 different classes, ranging from birds to boats.",6.1. Streaming Image Summarization,[0],[0]
"For the purposes of this application, we will use n = 756 different images and we will consider all m = 20 classes that are available.",6.1. Streaming Image Summarization,[0],[0]
"Our goal is to choose a small subset S of images that provides a good summary of the entire ground set Ω. In general, it can be difficult to even define what a good summary of a set of images should look like.",6.1. Streaming Image Summarization,[0],[0]
"Fortunately, each image in this dataset comes with a human-labelled annotation that lists the number of objects from each class that appear in that image.
",6.1. Streaming Image Summarization,[0],[0]
"Using the exemplar-based clustering approach (Mirzasoleiman et al., 2013), for each image we generate an mdimensional vector x such that xi represents the number of objects from class i that appear in the image (an example is given in Appendix G).",6.1. Streaming Image Summarization,[0],[0]
"We define Ωi to be the set of all images that contain objects from class i, and correspondingly Si = Ωi ∩ S (i.e. the images we have selected that contain objects from class i).
",6.1. Streaming Image Summarization,[0],[0]
We want to optimize the following monotone submodular functions: fi(S) = Li({e0}),6.1. Streaming Image Summarization,[0],[0]
"− Li(S ∪ {e0}), where Li(S) =
1 |Ωi| ∑ x∈Ωi miny∈Si d(x, y).",6.1. Streaming Image Summarization,[0],[0]
"We use d(x, y) to
denote the “distance” between two images x and y. More accurately, we measure the distance between two images as the `2 norm between their characteristic vectors.",6.1. Streaming Image Summarization,[0],[0]
"We also use e0 to denote some auxiliary element, which in our case is the all-zero vector.
",6.1. Streaming Image Summarization,[0],[0]
"Since image data is generally quite storage-intensive, streaming algorithms can be particularly desirable.",6.1. Streaming Image Summarization,[0],[0]
"With this in mind, we will compare our streaming algorithm REPLACEMENT-STREAMING against the non-streaming baseline of REPLACEMENT-GREEDY.",6.1. Streaming Image Summarization,[0],[0]
We also compare against a heuristic streaming baseline that we call STREAMSUM.,6.1. Streaming Image Summarization,[0],[0]
This baseline first greedily optimizes the submodular function F (S) = ∑m i=1 fi(S) using the streaming algorithm developed by Buchbinder et al. (2015).,6.1. Streaming Image Summarization,[0],[0]
"Having selected ` elements from the stream, it then constructs each Ti by greedily selecting k of these elements for each fi.
To evaluate the various algorithms, we consider two primary metrics: the objective value, which we define as∑m i=1",6.1. Streaming Image Summarization,[0],[0]
"fi(Ti), and the wall-clock running time.",6.1. Streaming Image Summarization,[0],[0]
We note that the trials were run using Python 2.7 on a quad-core Linux machine with 3.3 GHz Intel Core i5 processors and 8 GB of RAM.,6.1. Streaming Image Summarization,[0],[0]
"Figure 2 shows our results.
",6.1. Streaming Image Summarization,[0],[0]
"The graphs are organized so that each column shows the effects of varying a particular parameter, with the objective value being shown in the top row and the running time in the bottom row.",6.1. Streaming Image Summarization,[0],[0]
"The primary observation across all the graphs is that our streaming algorithm REPLACEMENT-STREAMING not only achieves an objective value that is similar to that of the non-streaming baseline REPLACEMENT-GREEDY, but it also speeds up the running time by a full order of magnitude.",6.1. Streaming Image Summarization,[0],[0]
"We also see that REPLACEMENT-STREAMING outperforms the streaming baseline STREAM-SUM in both objective value and running time.
",6.1. Streaming Image Summarization,[0],[0]
Another noteworthy observation from Figure 2(c) is that can be increased all the way up to = 0.5 before we start to see loss in the objective value.,6.1. Streaming Image Summarization,[0],[0]
Recall that is the parameter that trades off the accuracy of REPLACEMENT-STREAMING with the running time by changing the granularity of our guesses for OPT.,6.1. Streaming Image Summarization,[0],[0]
"As seen Figure 2(f), increasing up to 0.5 also covers the majority of running time speed-up, with diminishing returns kicking in as we get close to = 1.
",6.1. Streaming Image Summarization,[0],[0]
"Also in the context of running time, we see in Figure 2(e) that REPLACEMENT-STREAMING actually speeds up as k increases.",6.1. Streaming Image Summarization,[0],[0]
"This seems counter-intuitive at first glance, but one possible reason is that the majority of the time cost for these replacement-based algorithms comes from the swapping that must be done when the Ti’s fill up.",6.1. Streaming Image Summarization,[0],[0]
"Therefore, the longer each Ti is not completely full, the faster the overall algorithm will run.
",6.1. Streaming Image Summarization,[0],[0]
"Figure 3 shows some sample images selected by REPLACEMENT-GREEDY (top) and REPLACEMENT-
STREAMING (bottom).",6.1. Streaming Image Summarization,[0],[0]
"Although the two summaries contain only one image that is exactly the same, we see that the different images still have a similar theme.",6.1. Streaming Image Summarization,[0],[0]
"For example, both images in the second column contain bikes and people; while in the third column, both images contain sheep.",6.1. Streaming Image Summarization,[0],[0]
In this application we want to use past Uber data to select optimal waiting locations for idle drivers.,6.2. Distributed Ride-Share Optimization,[0],[0]
"Towards this end, we analyze a dataset of 100,000 Uber pick-ups in Manhattan from September 2014 (UberDataset), where each entry in the dataset is given as a (latitude, longitude) coordinate pair.",6.2. Distributed Ride-Share Optimization,[0],[0]
"We model this problem as a classical facility location problem, which is known to be monotone submodular.
",6.2. Distributed Ride-Share Optimization,[0],[0]
"Given a set of potential waiting locations for drivers, we want to pick a subset of these locations so that the distance from each customer to his closest driver is minimized.",6.2. Distributed Ride-Share Optimization,[0],[0]
"In
particular, given a customer location a = (xa, ya), and a waiting driver location b = (xb, yb), we define a “convenience score” c(a, b) as follows: c(a, b) = 2− 2
1+e−200d(a,b) ,
where d(a, b) = |xa",6.2. Distributed Ride-Share Optimization,[0],[0]
− xb|+ |ya,6.2. Distributed Ride-Share Optimization,[0],[0]
"− yb| is the Manhattan distance between the two points.
",6.2. Distributed Ride-Share Optimization,[0],[0]
"Next, we need to introduce some functions we want to maximize.",6.2. Distributed Ride-Share Optimization,[0],[0]
"For this experiment, we can think about different functions corresponding to different (possibly overlapping) regions around Manhattan.",6.2. Distributed Ride-Share Optimization,[0],[0]
"The overlap means that there will still be some inherent connection between the functions, but they are still relatively distinct from each other.",6.2. Distributed Ride-Share Optimization,[0],[0]
"More specifically, we construct regions R1, . . .",6.2. Distributed Ride-Share Optimization,[0],[0]
", Rm by randomly picking m points across Manhattan.",6.2. Distributed Ride-Share Optimization,[0],[0]
"Then, for each point pi, we want to define the corresponding region Ri by all the pick-ups that have occurred within one kilometer of pi.",6.2. Distributed Ride-Share Optimization,[0],[0]
"However, to keep the problem computationally tractable, we instead randomly select only ten pick-up locations within that same radius.",6.2. Distributed Ride-Share Optimization,[0],[0]
"Figure 4(a) shows the center points of the m = 20 randomly selected regions, overlaid on top of a heat map of all the customer pick-up locations.
",6.2. Distributed Ride-Share Optimization,[0],[0]
"Given any set of driver waiting locations Ti, we define fi(Ti) as follows: fi(Ti) = ∑ a∈Ri maxb∈Ti c(a, b).",6.2. Distributed Ride-Share Optimization,[0],[0]
"For this application, we will use every customer pick-up location as a potential waiting location for a driver, meaning we have 100,000 elements in our ground set Ω. This large number of elements, combined with the fact that each single function evaluation is computationally intensive, means running the regular REPLACEMENT-GREEDY will be prohibitively ex-
pensive.",6.2. Distributed Ride-Share Optimization,[0],[0]
"Hence, we will use this setup to evaluate the two distributed algorithms we presented in Section 5.",6.2. Distributed Ride-Share Optimization,[0],[0]
We will also compare our algorithms against a heuristic baseline that we call DISTRIBUTED-GREEDY.,6.2. Distributed Ride-Share Optimization,[0],[0]
"This baseline will first select ` elements using the greedy distributed framework introduced by Mirzasoleiman et al. (2013), and then greedily optimize each fi over these ` elements.
",6.2. Distributed Ride-Share Optimization,[0],[0]
"Each algorithm produces two outputs: a small subset S of potential waiting locations (with size ` = 30), as well as a solution Ti (of size k = 3) for each function fi.",6.2. Distributed Ride-Share Optimization,[0],[0]
"In other words, each algorithm will reduce the number of potential waiting locations from 100,000 to 30, and then choose 3 different waiting locations for drivers in each region.
",6.2. Distributed Ride-Share Optimization,[0],[0]
"In Figure 4(b), we graph the average distance from each customer to his closest driver, which we will refer to as the cost.",6.2. Distributed Ride-Share Optimization,[0],[0]
"One interesting observation is that while the cost of DISTRIBUTED-FAST decreases with the number of machines, the costs of the other two algorithms stay relatively constant, with REPLACEMENT-DISTRIBUTED marginally outperforming DISTRIBUTED-GREEDY.",6.2. Distributed Ride-Share Optimization,[0],[0]
"In Figure 4(c), we graph the run time of each algorithm.",6.2. Distributed Ride-Share Optimization,[0],[0]
"We see that the algorithms achieve their optimal speeds at different values of M , verifying the theory at the end of Section 5.",6.2. Distributed Ride-Share Optimization,[0],[0]
"Overall, we see that while all three algorithms have very comparable costs, DISTRIBUTED-FAST is significantly faster than the others.
",6.2. Distributed Ride-Share Optimization,[0],[0]
"While in the previous application we only looked at the
objective value for the given functions f1, . .",6.2. Distributed Ride-Share Optimization,[0],[0]
.,6.2. Distributed Ride-Share Optimization,[0],[0]
", fm, in this experiment we also evaluate the utility of our summary on new functions drawn from the same distribution.",6.2. Distributed Ride-Share Optimization,[0],[0]
"That is, using the regions shown in Figure 4(a), each algorithm will select a subset S of potential waiting locations.",6.2. Distributed Ride-Share Optimization,[0],[0]
"Using only these reduced subsets, we then greedily select k waiting locations for each of the twenty new regions shown in 4(d).
",6.2. Distributed Ride-Share Optimization,[0],[0]
"In Figure 4(e), we see that the summaries from all three algorithms achieve a similar cost, which is significantly better than RANDOM.",6.2. Distributed Ride-Share Optimization,[0],[0]
"In this scenario, RANDOM is defined as the cost achieved when optimizing over a random size ` subset and OPTIMAL is defined as the cost that is achieved when optimizing the functions over the entire ground set rather than a reduced subset.",6.2. Distributed Ride-Share Optimization,[0],[0]
"In Figure 4(f), we confirm that DISTRIBUTED-FAST is indeed the fastest algorithm for constructing each summary.",6.2. Distributed Ride-Share Optimization,[0],[0]
"Note that 4(f) is demonstrating how long each algorithm takes to construct a size ` summary, not how long it is taking to optimize over this summary.",6.2. Distributed Ride-Share Optimization,[0],[0]
"To satisfy the need for scalable data summarization algorithms, this paper focused on the two-stage submodular maximization framework and provided the first streaming and distributed solutions to this problem.",7. Conclusion,[0],[0]
"In addition to constant factor theoretical guarantees, we demonstrated the effectiveness of our algorithms on real world applications in image summarization and ride-share optimization.",7. Conclusion,[0],[0]
Amin Karbasi was supported by a DARPA Young Faculty Award (D16AP00046) and a AFOSR Young Investigator Award (FA9550-18-1-0160).,Acknowledgements,[0],[0]
Ehsan Kazemi was supported by the Swiss National Science Foundation (Early Postdoc.,Acknowledgements,[0],[0]
Mobility) under grant number 168574.,Acknowledgements,[0],[0]
The sheer scale of modern datasets has resulted in a dire need for summarization techniques that can identify representative elements in a dataset.,abstractText,[0],[0]
"Fortunately, the vast majority of data summarization tasks satisfy an intuitive diminishing returns condition known as submodularity, which allows us to find nearly-optimal solutions in linear time.",abstractText,[0],[0]
We focus on a two-stage submodular framework where the goal is to use some given training functions to reduce the ground set so that optimizing new functions (drawn from the same distribution) over the reduced set provides almost as much value as optimizing them over the entire ground set.,abstractText,[0],[0]
"In this paper, we develop the first streaming and distributed solutions to this problem.",abstractText,[0],[0]
"In addition to providing strong theoretical guarantees, we demonstrate both the utility and efficiency of our algorithms on real-world tasks including image summarization and ride-share optimization.",abstractText,[0],[0]
Data Summarization at Scale:A Two-Stage Submodular Approach,title,[0],[0]
Stochastic gradient descent (SGD) has become one of the workhorses of modern machine learning.,1. Introduction,[0],[0]
"In particular, it is the optimization method of choice for training highly complex and non-convex models, such as neural networks.",1. Introduction,[0],[0]
"When it was observed that these models generalize better (suffer less from overfitting) than classical machine learning theory suggests, a large theoretical interest emerged to explain this phenomenon.",1. Introduction,[0],[0]
"Given that SGD at best finds a local minimum of the non-convex objective function, it has been argued that all such minima might be equally good.",1. Introduction,[0],[0]
"However, at the same time, a large body of empirical work and tricks of trade, such as early stopping, suggests that in prac-
1University of Milan, Italy 2IST Austria.",1. Introduction,[0],[0]
"Correspondence to: Ilja Kuzborskij <ilja.kuzborskij@gmail.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"tice one might not even reach a minimum, yet nevertheless observes excellent performance.
",1. Introduction,[0],[0]
In this work we follow an alternative route that aims to directly analyze the generalization ability of SGD by studying how sensitive it is to small perturbations in the training set.,1. Introduction,[0],[0]
"This is known as algorithmic stability approach (Bousquet & Elisseeff, 2002) and was used recently (Hardt et al., 2016) to establish generalization bounds for both convex and non-convex learning settings.",1. Introduction,[0],[0]
"To do so they employed a rather restrictive notion of stability that does not depend on the data, but captures only intrinsic characteristics of the learning algorithm and global properties of the objective function.",1. Introduction,[0],[0]
"Consequently, their analysis results in worst-case guarantees that in some cases tend to be too pessimistic.",1. Introduction,[0],[0]
"As recently pointed out in (Zhang et al., 2017), deep learning might indeed be such a case, as this notion of stability is insufficient to give deeper theoretical insights, and a less restrictive one is desirable.
",1. Introduction,[0],[0]
"As our main contribution in this work we establish that a data-dependent notion of algorithmic stability, very similar to the On-Average Stability (Shalev-Shwartz et al., 2010), holds for SGD when applied to convex as well as nonconvex learning problems.",1. Introduction,[0],[0]
As a consequence we obtain new generalization bounds that depend on the data-generating distribution and the initialization point of an algorithm.,1. Introduction,[0],[0]
"For convex loss functions, the bound on the generalization error is essentially multiplicative in the risk at the initialization point when noise of stochastic gradient is not too high.",1. Introduction,[0],[0]
"For the non-convex loss functions, besides the risk, it is also critically controlled by the expected second-order information about the objective function at the initialization point.",1. Introduction,[0],[0]
"We further corroborate our findings empirically and show that, indeed, the data-dependent generalization bound is tighter than the worst-case counterpart on non-convex objective functions.",1. Introduction,[0],[0]
"Finally, the nature of the data-dependent bounds allows us to state optimistic bounds that switch to the faster rate of convergence subject to the vanishing empirical risk.
",1. Introduction,[0],[0]
"In particular, our findings justify the intuition that SGD is more stable in less curved areas of the objective function and link it to the generalization ability.",1. Introduction,[0],[0]
This also backs up numerous empirical findings in the deep learning literature that solutions with low generalization error occur in less curved regions.,1. Introduction,[0],[0]
"At the same time, in pessimistic scenarios,
our bounds are no worse than those of (Hardt et al., 2016).
",1. Introduction,[0],[0]
"Finally, we exemplify an application of our bounds, and propose a simple yet principled transfer learning scheme for the convex and non-convex case, which is guaranteed to transfer from the best source of information.",1. Introduction,[0],[0]
"In addition, this approach can also be used to select a good initialization given a number of random starting positions.",1. Introduction,[0],[0]
"This is a theoretically sound alternative to the purely random commonly used in non-convex learning.
",1. Introduction,[0],[0]
The rest of the paper is organized as follows.,1. Introduction,[0],[0]
We revisit the connection between stability and generalization of SGD in Section 3 and introduce a data-dependent notion of stability in Section 4.,1. Introduction,[0],[0]
"We state the main results in Section 5, in particular, Theorem 3 for the convex case, and Theorem 4 for the non-convex one.",1. Introduction,[0],[0]
Next we demonstrate empirically that the bound shown in Theorem 4 is tighter than the worstcase one in Section 5.2.1.,1. Introduction,[0],[0]
"Finally, we suggest application of these bounds by showcasing principled transfer learning approaches in Section 5.3, and we conclude in Section 7.",1. Introduction,[0],[0]
"Algorithmic stability has been a topic of interest in learning theory for a long time, however, the modern approach on the relationship between stability and generalization goes back to the milestone work of (Bousquet & Elisseeff, 2002).",2. Related Work,[0],[0]
"They analyzed several notions of stability, which fall into two categories: distribution-free and distribution-dependent ones.",2. Related Work,[0],[0]
The first category is usually called uniform stability and focuses on the intrinsic stability properties of an algorithm without regard to the data-generating distribution.,2. Related Work,[0],[0]
"Uniform stability was used to analyze many algorithms, including regularized Empirical Risk Minimization (ERM) (Bousquet & Elisseeff, 2002), randomized aggregation schemes (Elisseeff et al., 2005), and recently SGD by (Hardt et al., 2016; London, 2016), and (Poggio et al., 2011).",2. Related Work,[0],[0]
"Despite the fact that uniform stability has been shown to be sufficient to guarantee learnability, it can be too pessimistic, resulting in worst-case rates.
",2. Related Work,[0],[0]
"In this work we are interested in the data-dependent behavior of SGD, thus the emphasis will fall on the distributiondependent notion of stability, known as on-average stability, explored throughly in (Shalev-Shwartz et al., 2010).",2. Related Work,[0],[0]
The attractive quality of this less restrictive stability type is that the resulting bounds are controlled by how stable the algorithm is under the data-generating distribution.,2. Related Work,[0],[0]
"For instance, in (Bousquet & Elisseeff, 2002) and (Devroye & Wagner, 1979), the on-average stability is related to the variance of an estimator.",2. Related Work,[0],[0]
"In (Shalev-Shwartz & Ben-David, 2014, Sec. 13), the authors show risk bounds that depend on the expected empirical risk of a solution to the regularized ERM.",2. Related Work,[0],[0]
"In turn, one can exploit this fact to state improved optimistic risk
bounds, for instance, ones that exhibit fast-rate regimes (Koren & Levy, 2015; Gonen & Shalev-Shwartz, 2017), or even to design enhanced algorithms that minimize these bounds in a data-driven way, e.g. by exploiting side information as in transfer (Kuzborskij & Orabona, 2013; Ben-David & Urner, 2013) and metric learning (Perrot & Habrard, 2015).",2. Related Work,[0],[0]
"Here, we mainly focus on the later direction in the context of SGD: how stable is SGD under the data-generating distribution given an initialization point?",2. Related Work,[0],[0]
"We also touch the former direction by taking advantage of our data-driven analysis and show optimistic bounds as a corollary.
",2. Related Work,[0],[0]
We will study the on-average stability of SGD for both convex and non-convex loss functions.,2. Related Work,[0],[0]
"In the convex setting, we will relate stability to the risk at the initialization point, while previous data-driven stability arguments usually consider minimizers of convex ERM rather than a stochastic approximation (Shalev-Shwartz & Ben-David, 2014; Koren & Levy, 2015).",2. Related Work,[0],[0]
"Beside convex problems, our work also covers the generalization ability of SGD on non-convex problems.",2. Related Work,[0],[0]
"Here, we borrow techniques of (Hardt et al., 2016) and extend them to the distribution-dependent setting.",2. Related Work,[0],[0]
"That said, while bounds of (Hardt et al., 2016) are stated in terms of worst-case quantities, ours reveal new connections to the data-dependent second-order information.",2. Related Work,[0],[0]
"These new insights also partially justify empirical observations in deep learning about the link between the curvature and the generalization error (Hochreiter & Schmidhuber, 1997; Keskar et al., 2017; Chaudhari et al., 2017).",2. Related Work,[0],[0]
"At the same time, our work is an alternative to the theoretical studies of neural network objective functions (Choromanska et al., 2015; Kawaguchi, 2016), as we focus on the direct connection between the generalization and the curvature.
",2. Related Work,[0],[0]
"In this light, our work is also related to non-convex optimization by SGD.",2. Related Work,[0],[0]
"Literature on this subject typically studies rates of convergence to the stationary points (Ghadimi & Lan, 2013; Allen-Zhu & Hazan, 2016; Reddi et al., 2016), and ways to avoid saddles (Ge et al., 2015; Lee et al., 2016).",2. Related Work,[0],[0]
"However, unlike these works, and similarly to (Hardt et al., 2016), we are interested in the generalization ability of SGD, and thanks to the stability approach, involvement of stationary points in our analysis is not necessary.
",2. Related Work,[0],[0]
"Finally, we propose an example application of our findings in Transfer Learning (TL).",2. Related Work,[0],[0]
"For instance, by controlling the stability bound in a data-driven way, one can choose an initialization that leads to improved generalization.",2. Related Work,[0],[0]
"This is related to TL where one transfers from pre-trained models (Kuzborskij & Orabona, 2016; Tommasi et al., 2014; Pentina & Lampert, 2014; Ben-David & Urner, 2013), especially popular in deep learning due to its data-demanding nature (Galanti et al., 2016).",2. Related Work,[0],[0]
"Literature on this topic is mostly focused on the ERM setting and PAC-bounds, while our analysis of SGD yields such guarantees as a corollary.",2. Related Work,[0],[0]
"First, we introduce definitions used in the rest of the paper.",3. Stability of Stochastic Gradient Descent,[0],[0]
"We will denote with small and capital bold letters respectively column vectors and matrices, e.g., a “ ra1, a2, . . .",3.1. Definitions,[0],[0]
", adsT P Rd and A P Rd1ˆd2 , }a} is understood as a Euclidean norm and }A}2 as the spectral norm.",3.1. Definitions,[0],[0]
"We denote enumeration by rns “ t1, . . .",3.1. Definitions,[0],[0]
", nu for n P N.
",3.1. Definitions,[0],[0]
We indicate an example space by Z and its member by z P Z .,3.1. Definitions,[0],[0]
"For instance, in a supervised setting Z “ X ˆ Y , such that X is the input and Y is the output space of a learning problem.",3.1. Definitions,[0],[0]
We assume that training and testing examples are drawn iid from a probability distribution D over Z .,3.1. Definitions,[0],[0]
"In particular, we will denote the training set as S “ tziumi“1 „ Dm.
",3.1. Definitions,[0],[0]
"For a parameter space H, we define a learning algorithm as a map A : Zm ÞÑ H and for brevity we will use the notation AS “ ApSq.",3.1. Definitions,[0],[0]
In the following we assume that H Ď Rd.,3.1. Definitions,[0],[0]
To measure the accuracy of a learning algorithm,3.1. Definitions,[0],[0]
"A, we have a loss function fpw, zq, which measures the cost incurred by predicting with parameters w P H on an example z.",3.1. Definitions,[0],[0]
"The risk of w, with respect to the distribution D, and the empirical risk given a training set S are defined as
Rpwq :“ E z„D
rfpw, zqs, and pRSpwq :“ 1
m
m ÿ i“1 fpw, ziq .
",3.1. Definitions,[0],[0]
"Finally, define R‹ :“ infwPHRpwq.",3.1. Definitions,[0],[0]
"On an intuitive level, a learning algorithm is said to be stable whenever a small perturbation in the training set does not affect its outcome too much.",3.2. Uniform Stability and Generalization,[0],[0]
"Of course, there is a number of ways to formalize the perturbation and the extent of the change in the outcome, and we will discuss some of them below.",3.2. Uniform Stability and Generalization,[0],[0]
The most important consequence of a stable algorithm is that it generalizes from the training set to the unseen data sampled from the same distribution.,3.2. Uniform Stability and Generalization,[0],[0]
"In other words, the difference between the risk RpASq and the empirical risk pRSpASq of the algorithm’s output is controlled by the quantity that captures how stable the algorithm is.",3.2. Uniform Stability and Generalization,[0],[0]
"So, to observe good performance, or a decreasing true risk, we must have a stable algorithm and decreasing empirical risk (training error), which usually comes by design of the algorithm.",3.2. Uniform Stability and Generalization,[0],[0]
"In this work we focus on the stability of the Stochastic Gradient Descent (SGD) algorithm, and thus, as a consequence, we study its generalization ability.
",3.2. Uniform Stability and Generalization,[0],[0]
"Recently, (Hardt et al., 2016) used a stability argument to prove generalization bounds for learning with SGD.",3.2. Uniform Stability and Generalization,[0],[0]
"Specifically, the authors extended the notion of the uniform stability
originally proposed by (Bousquet & Elisseeff, 2002), to accommodate randomized algorithms.
",3.2. Uniform Stability and Generalization,[0],[0]
Definition 1 (Uniform stability).,3.2. Uniform Stability and Generalization,[0],[0]
"A randomized algorithm A is -uniformly stable if for all datasets S,",3.2. Uniform Stability and Generalization,[0],[0]
"Spiq P Zm such that S and Spiq differ in the i-th example, we have
sup zPZ,iPrms
!
",3.2. Uniform Stability and Generalization,[0],[0]
"E A rfpAS , zq ´ fpASpiq , zqs
)
ď .
",3.2. Uniform Stability and Generalization,[0],[0]
"Since SGD is a randomized algorithm, we have to cope with two sources of randomness: the data-generating process and the randomization of the algorithm A itself, hence we have statements in expectation.",3.2. Uniform Stability and Generalization,[0],[0]
"The following theorem of (Hardt et al., 2016) shows that the uniform stability implies generalization in expectation.
",3.2. Uniform Stability and Generalization,[0],[0]
Theorem 1.,3.2. Uniform Stability and Generalization,[0],[0]
Let A be -uniformly stable.,3.2. Uniform Stability and Generalization,[0],[0]
"Then, ˇ
ˇ ˇ ˇ E S,A
” pRSpASq ´RpASq ı
ˇ ˇ ˇ ˇ ď .
",3.2. Uniform Stability and Generalization,[0],[0]
Thus it suffices to characterize the uniform stability of an algorithm to state a generalization bound.,3.2. Uniform Stability and Generalization,[0],[0]
"In particular, (Hardt et al., 2016) showed generalization bounds for SGD under different assumptions on the loss function f .",3.2. Uniform Stability and Generalization,[0],[0]
"Despite that these results hold in expectation, other forms of generalization bounds, such as high-probability ones, can be derived from the above (Shalev-Shwartz et al., 2010).
",3.2. Uniform Stability and Generalization,[0],[0]
"Apart from SGD, uniform stability has been used before to prove generalization bounds for many learning algorithms (Bousquet & Elisseeff, 2002).",3.2. Uniform Stability and Generalization,[0],[0]
"However, these bounds typically suggest worst-case generalization rates, and rather reflect intrinsic stability properties of an algorithm.",3.2. Uniform Stability and Generalization,[0],[0]
"In other words, uniform stability is oblivious to the data-generating process and any other side information, which might reveal scenarios where generalization occurs at a faster rate.",3.2. Uniform Stability and Generalization,[0],[0]
"In turn, these insights could motivate the design of improved learning algorithms.",3.2. Uniform Stability and Generalization,[0],[0]
In the following we address some limitations of analysis through uniform stability by using a less restrictive notion of stability.,3.2. Uniform Stability and Generalization,[0],[0]
"We extend the setting of (Hardt et al., 2016) by proving data-dependent stability bounds for convex and non-convex loss functions.",3.2. Uniform Stability and Generalization,[0],[0]
"In addition, we also take into account the initialization point of an algorithm as a form of supplementary information, and we dedicate special attention to its interplay with the data-generating distribution.",3.2. Uniform Stability and Generalization,[0],[0]
"Finally, we discuss situations where one can explicitly control the stability of SGD in a data-dependent way.",3.2. Uniform Stability and Generalization,[0],[0]
"In this section we describe a notion of data-dependent algorithmic stability, that allows us to state generalization bounds which depend not only on the properties of the learning algorithm, but also on the additional parameters of the
algorithm.",4. Data-dependent Stability Bounds for SGD,[0],[0]
"We indicate such additional parameters by θ, and therefore we denote stability as a function pθq.",4. Data-dependent Stability Bounds for SGD,[0],[0]
"In particular, in the following we will be interested in scenarios where θ describes the data-generating distribution and the initialization point of SGD.
",4. Data-dependent Stability Bounds for SGD,[0],[0]
Definition 2 (On-Average stability).,4. Data-dependent Stability Bounds for SGD,[0],[0]
"A randomized algorithm A is pθq-on-average stable if it is true that
sup iPrms
""
E A E S,z rfpAS , zq ´ fpASpiq , zqs
*
ď pθq ,
where S iid„ Dm and Spiq is its copy with i-th example replaced by z iid„D.
Our definition of on-average stability resembles the notion introduced by (Shalev-Shwartz et al., 2010).",4. Data-dependent Stability Bounds for SGD,[0],[0]
The difference lies in the fact that we take supremum over index of replaced example.,4. Data-dependent Stability Bounds for SGD,[0],[0]
"A similar notion was also used by (Bousquet & Elisseeff, 2002) and later by (Elisseeff et al., 2005) for analysis of a randomized aggregation schemes, however their definition involves absolute difference of losses.",4. Data-dependent Stability Bounds for SGD,[0],[0]
"The dependence on θ also bears similarity to recent work of (London, 2016), however, there, it is used in the context of uniform stability.",4. Data-dependent Stability Bounds for SGD,[0],[0]
"The following theorem shows that on-average -stable random algorithm is guaranteed to generalize in expectation.
",4. Data-dependent Stability Bounds for SGD,[0],[0]
Theorem 2.,4. Data-dependent Stability Bounds for SGD,[0],[0]
Let an algorithm A be pθq-on-average stable.,4. Data-dependent Stability Bounds for SGD,[0],[0]
"Then,
E S E A
” RpASq ´ pRSpASq ı ď pθq .",4. Data-dependent Stability Bounds for SGD,[0],[0]
"Before presenting our main results in this section, we discuss algorithmic details and assumptions.",5. Main Results,[0],[0]
"We will study the following variant of SGD: given a training set S “ tziumi“1
iid„ Dm, step sizes tαtuTt“1, random indices I “ tjtuTt“1, and an initialization point w1, perform updates
wt`1 “ wt ´ αt∇fpwt, zjtq
for T ď m steps.",5. Main Results,[0],[0]
"Moreover we will use the notation wS,t to indicate the output of SGD ran on a training set S, at step t. We assume that the indices in I are sampled from the uniform distribution over rms without replacement, and that this is the only source of randomness for SGD.",5. Main Results,[0],[0]
"In practice this corresponds to permuting the training set before making a pass through it, as it is commonly done in practical applications.",5. Main Results,[0],[0]
"We also assume that the variance of stochastic gradients obeys
E S,z
” }∇fpwS,t, zq ´∇RpwS,tq}2 ı ď σ2 @t P rT s .
",5. Main Results,[0],[0]
"Next, we introduce statements about the loss functions f used in the following.
",5. Main Results,[0],[0]
Definition 3 (Lipschitz f ).,5. Main Results,[0],[0]
"A loss function f is L-Lipschitz if }∇fpw, zq} ď L, @w P H and @z P Z .",5. Main Results,[0],[0]
"Note that this also implies that |fpw, zq ´ fpv, zq| ď L}w ´ v} .",5. Main Results,[0],[0]
Definition 4 (Smooth f ).,5. Main Results,[0],[0]
"A loss function is β-smooth if @w,v P H and @z P Z , }∇fpw, zq ´ ∇fpv, zq} ď β}w ´ v} , which also implies fpw, zq ´ fpv, zq ď ∇fpv, zqJpw ´ vq ` β2 }w ´ v} 2 .",5. Main Results,[0],[0]
Definition 5 (Lipschitz Hessians).,5. Main Results,[0],[0]
"A loss function f has a ρLipschitz Hessian if @w,v P H and @z P Z , }∇2fpw, zq´ ∇2fpv, zq}2 ď ρ}w ´ v} .
",5. Main Results,[0],[0]
"The last condition is occasionally used in analysis of SGD (Ge et al., 2015) and holds whenever f has a bounded third derivative.",5. Main Results,[0],[0]
"All presented theorems assume that the loss function is non-negative, Lipschitz, and β-smooth.",5. Main Results,[0],[0]
Examples of such commonly used loss functions are the logistic/softmax losses and neural networks with sigmoid activations.,5. Main Results,[0],[0]
"Convexity of loss functions or Lipschitzness of Hessians will only be required for some results, and we will denote it when necessary.",5. Main Results,[0],[0]
Proofs for all the statements in this section are given in the arXiv version of the paper1.,5. Main Results,[0],[0]
"First, we present a new and data-dependent stability result for convex losses.
",5.1. Convex Losses,[0],[0]
Theorem 3.,5.1. Convex Losses,[0],[0]
"Assume that f is convex, and that SGD’s step sizes satisfy αt “ c?t ď 1 β ,",5.1. Convex Losses,[0],[0]
@t P rT s.,5.1. Convex Losses,[0],[0]
"Then SGD is
pD,w1q-on-average stable with
pD,w1q “ O ˜ a c pRpw1q ´R‹q ¨ 4 ?",5.1. Convex Losses,[0],[0]
"T
m ` cσ
?",5.1. Convex Losses,[0],[0]
"T
m
¸
.
",5.1. Convex Losses,[0],[0]
"Under the same assumptions, taking step size of order Op1{ ?",5.1. Convex Losses,[0],[0]
"tq, Theorem 3.7 of (Hardt et al., 2016) implies a uniform stability bound “ Op ?",5.1. Convex Losses,[0],[0]
T {mq.,5.1. Convex Losses,[0],[0]
Our bound differs since it involves a multiplicative risk at the initialization point.,5.1. Convex Losses,[0],[0]
"Thus, our bound corroborates the intuition that whenever we start at a good location of the objective function, the algorithm is more stable and thus generalizes better.",5.1. Convex Losses,[0],[0]
"However, this is only the case, whenever the variance of stochastic gradient σ2 is not too large.",5.1. Convex Losses,[0],[0]
"In the deterministic case, and of Rpw1q “ R‹, the theorem confirms that SGD, in expectation, does not need to make any updates and is therefore perfectly stable.",5.1. Convex Losses,[0],[0]
"On the other hand, when the variance σ2 is large enough to make the second summand in Theorem 3 dominant, the bound does not offer improvement compared to (Hardt et al., 2016).",5.1. Convex Losses,[0],[0]
"Note, that a result of this type cannot be obtained through the more restrictive uniform stability, precisely because such bounds on the stability must hold even for a worst-case choice of data distribution and initialization.",5.1. Convex Losses,[0],[0]
"In contrast, the notion of stability we
1https://arxiv.org/abs/1703.01678
employ depends on the data-generating distribution, which allowed us to introduce dependency on the risk.
",5.1. Convex Losses,[0],[0]
"Furthermore, consider that we start at arbitrary location w1: assuming that the loss function is bounded for a concrete H and Z , the rate of our bound up to a constant is no worse than that of (Hardt et al., 2016).",5.1. Convex Losses,[0],[0]
"Finally, one can always tighten this result by taking the minimum of two bounds.",5.1. Convex Losses,[0],[0]
"Now we state a new stability result for non-convex losses.
",5.2. Non-convex Losses,[0],[0]
Theorem 4.,5.2. Non-convex Losses,[0],[0]
"Assume that fp¨, zq P r0, 1s and has a ρLipschitz Hessian, and that step sizes of a form αt “ ct satisfy c ď",5.2. Non-convex Losses,[0],[0]
"min !
",5.2. Non-convex Losses,[0],[0]
"1 β , 1 4p2β lnpT qq2
)
.",5.2. Non-convex Losses,[0],[0]
"Then SGD is pD,w1qon-average stable with
pD,w1q ď 1` 1cγ m ` 2cL2 ˘ 1 1`cγ ˆ E S,A rRpASqs ¨ T ˙ cγ 1`cγ ,
(1)
where
γ :“ Õ ´ min !
",5.2. Non-convex Losses,[0],[0]
"β, E z
“ › ›∇2fpw1, zq › ›
2
‰ `∆‹1,σ2 )¯",5.2. Non-convex Losses,[0],[0]
", (2)
∆‹1,σ2 :“ ρ ´ cσ ` a c pRpw1q ´R‹q ¯ .
",5.2. Non-convex Losses,[0],[0]
"In particular, γ characterizes how the curvature at the initialization point affects stability, and hence the generalization error of SGD.",5.2. Non-convex Losses,[0],[0]
"Since γ heavily affects the rate of convergence in (1), and in most situations smaller γ yields higher stability, we now look at a few cases of its behavior.",5.2. Non-convex Losses,[0],[0]
"Consider a regime such that γ is of the order Θ̃ ´ Er}∇2fpw1, zq}2s ` a Rpw1q ` σ ¯
, or in other words, that stability is controlled by the curvature, the risk of the initialization point w1, and the variance of the stochastic gradient σ2.",5.2. Non-convex Losses,[0],[0]
"This suggests that starting from a point in a less curved region with low risk should yield higher stability, and therefore as predicted by our theory, allow for faster generalization.",5.2. Non-convex Losses,[0],[0]
"In addition, we observe that the considered stability regime offers a principled way to pre-screen a good initialization point in practice, by choosing the one that minimizes spectral norm of the Hessian and the risk.
",5.2. Non-convex Losses,[0],[0]
"Next, we focus on a more specific case.",5.2. Non-convex Losses,[0],[0]
"Suppose that we choose a step size αt “ ct such that γ “ Θ̃ ` Er}∇2fpw1, zq}2s ˘
, yet not too small, so that the empirical risk can still be decreased.",5.2. Non-convex Losses,[0],[0]
"Then, stability is dominated by the curvature around w1.",5.2. Non-convex Losses,[0],[0]
"Indeed, lower generalization errors on non-convex problems, such as training deep neural networks, have been observed empirically when SGD is actively guided (Hochreiter & Schmidhuber, 1997; Goodfellow et al., 2016; Chaudhari et al., 2017) or converges to solutions with low curvature (Keskar et al., 2017).",5.2. Non-convex Losses,[0],[0]
"However, to the best of our knowledge, Theorem 4 is the first
to establish a theoretical link between the curvature of the loss function and the generalization ability of SGD in a data-dependent sense.
",5.2. Non-convex Losses,[0],[0]
"Theorem 4 immediately implies the following statement that further reinforces the effect of the initialization point on the generalization error, assuming that ESrRpASqs ď Rpw1q.",5.2. Non-convex Losses,[0],[0]
Corollary 1.,5.2. Non-convex Losses,[0],[0]
"Under conditions of Theorem 4 we have that SGD is pD,w1q-on-average stable with
pD,w1q “ O ˜ 1` 1cγ m pRpw1q ¨ T q cγ 1`cγ ¸ .",5.2. Non-convex Losses,[0],[0]
"(3)
We take a moment to discuss the role of the risk term in pRpw1q ¨ T q cγ 1`cγ .",5.2. Non-convex Losses,[0],[0]
"Observe that pD,w1q Ñ 0",5.2. Non-convex Losses,[0],[0]
"as Rpw1q Ñ 0, in other words, the generalization error approaches zero as the risk of the initialization point vanishes.",5.2. Non-convex Losses,[0],[0]
"This is an intuitive behavior, however, uniform stability does not capture this due to its distribution-free nature.",5.2. Non-convex Losses,[0],[0]
"Finally, we note that (Hardt et al., 2016, Theorem 3.8) showed a bound similar to (1), however, in place of γ their bound has a Lipschitz constant of the gradient.",5.2. Non-convex Losses,[0],[0]
"The crucial difference lies in term γ which is now not merely a Lipschitz constant, but rather depends on the data-generating distribution and initialization point of SGD.",5.2. Non-convex Losses,[0],[0]
"We compare to their bound by considering the worst case scenario, namely, that SGD is initialized in a point with high curvature, or altogether, that the objective function is highly curved everywhere.",5.2. Non-convex Losses,[0],[0]
"Then, at least our bound is no worse than the one of (Hardt et al., 2016), since γ ď β.",5.2. Non-convex Losses,[0],[0]
"Finally, it should be noted that our bound can be compared to the one of (Hardt et al., 2016) only in a setting of a single pass.",5.2. Non-convex Losses,[0],[0]
"In a multiple-pass case, data-dependent analysis in a current form would not hold, since the output of SGD would not be independent from a newly observed example after the first pass.",5.2. Non-convex Losses,[0],[0]
"On the other hand, our results focus on the gains due to data-dependent initialization.
",5.2. Non-convex Losses,[0],[0]
Theorem 4 also allows us to prove an optimistic generalization bound for learning with SGD on non-convex objectives.,5.2. Non-convex Losses,[0],[0]
Corollary 2.,5.2. Non-convex Losses,[0],[0]
"Under conditions of Theorem 4 we have that the output of SGD obeys
E S,A
” RpASq ´ pRSpASq ı “
O
˜
1` 1 cγ
m ¨max
#
ˆ
E S,A
” pRSpASq ı ¨ T ˙
cγ 1`cγ
,
ˆ
T
m
˙cγ +¸
.
",5.2. Non-convex Losses,[0],[0]
"An important consequence of Corollary 2, is that for a vanishing expected empirical risk, in particular for ES,Ar pRSpASqs “ O ` T cγ
m1`cγ
˘
, the generalization error behaves as O ` T cγ
m1`cγ
˘
.",5.2. Non-convex Losses,[0],[0]
"Considering the full pass, that is m “ OpT q, we have an optimistic generalization error of order O p1{mq instead of Opm´ 1 1`cγ q.",5.2. Non-convex Losses,[0],[0]
"We note that
PAC bounds with similar optimistic message (although not directly comparable), but without curvature information can also be obtained through empirical Bernstein bounds as in (Maurer & Pontil, 2009).",5.2. Non-convex Losses,[0],[0]
"However, a PAC bound does not suggest a way to minimize non-convex empirical risk in general, where SGD is known to work reasonably well.",5.2. Non-convex Losses,[0],[0]
Next we empirically assess the tightness of our non-convex generalization bounds on real data.,5.2.1. TIGHTNESS OF NON-CONVEX BOUNDS,[0],[0]
"In the following experiment we train a neural network with three convolutional layers interlaced with max-pooling, followed by the fully connected layer with 16 units, on the MNIST dataset.",5.2.1. TIGHTNESS OF NON-CONVEX BOUNDS,[0],[0]
This totals in a model with 18K parameters.,5.2.1. TIGHTNESS OF NON-CONVEX BOUNDS,[0],[0]
"Figure 1 compares our data-dependent bound (1) to the distribution-free one of (Hardt et al., 2016, Theorem 3.8).",5.2.1. TIGHTNESS OF NON-CONVEX BOUNDS,[0],[0]
As as a reference we also include an empirical estimate of the generalization error taken as an absolute difference of the validation and training average losses.,5.2.1. TIGHTNESS OF NON-CONVEX BOUNDS,[0],[0]
"Since our bound also depends on the initialization point, we plot (1) for multiple “warm-starts”, ie.with SGD initialized from a pre-trained position.",5.2.1. TIGHTNESS OF NON-CONVEX BOUNDS,[0],[0]
"We consider 7 such warm-starts at every 200 steps, and report data-dependent quantities used to compute (1) just beneath the graph.",5.2.1. TIGHTNESS OF NON-CONVEX BOUNDS,[0],[0]
"Our first observation is that, clearly, the datadependent bound gives tighter estimate, by roughly one order of magnitude.",5.2.1. TIGHTNESS OF NON-CONVEX BOUNDS,[0],[0]
"Second, simulating start from a pretrained position suggests even tighter estimates: we suspect that this is due to decreasing validation error which is used as an empirical estimate for Rpw1q which affects bound (1).
",5.2.1. TIGHTNESS OF NON-CONVEX BOUNDS,[0],[0]
"We compute an empirical estimate of the expected Hessian spectral norm by the power iteration method using an efficient Hessian-vector multiplication method (Pearlmutter, 1994).",5.2.1. TIGHTNESS OF NON-CONVEX BOUNDS,[0],[0]
"Since bounds depend on constants L, β, and ρ, we estimate them by tracking maximal values of the
gradient and Hessian norms throughout optimization.",5.2.1. TIGHTNESS OF NON-CONVEX BOUNDS,[0],[0]
"We compute bounds with estimates pL “ 78.72, pβ “ 1692.28, pρ “ 3823.73, and c “ 10´3.",5.2.1. TIGHTNESS OF NON-CONVEX BOUNDS,[0],[0]
"One example application of data-dependent bounds presented before lies in Transfer Learning (TL), where we are interested in achieving faster generalization on a target task by exploiting side information that originates from different but related source tasks.",5.3. Application to Transfer Learning,[0],[0]
"The literature on TL explored many ways to do so, and here we will focus on the one that is most compatible with our bounds.",5.3. Application to Transfer Learning,[0],[0]
"More formally, suppose that the target task at hand is characterized by a joint probability distribution D, and as before we have a training set S iid„Dm.",5.3. Application to Transfer Learning,[0],[0]
Some TL approaches also assume access to the data sampled from the distributions associated with the source tasks.,5.3. Application to Transfer Learning,[0],[0]
"Here we follow a conservative approach – instead of the source data, we receive a set of source hypotheses twsrck u K k“1 Ă H, trained on the source tasks.",5.3. Application to Transfer Learning,[0],[0]
"The goal of a learner is to come up with a target hypothesis, which in the optimistic scenario generalizes better by relying on source hypotheses.",5.3. Application to Transfer Learning,[0],[0]
"In the TL literature this is known as Hypothesis Transfer Learning (HTL) (Kuzborskij & Orabona, 2016), that is, we transfer from the source hypotheses which act as a proxy to the source tasks and the risk Rpwsrck q quantifies how much source and target tasks are related.",5.3. Application to Transfer Learning,[0],[0]
"In the following we will consider SGD for HTL, where the source hypotheses act as initialization points.",5.3. Application to Transfer Learning,[0],[0]
"First, consider learning with convex losses: Theorem 3 depends on Rpw1q, thus it immediately quantifies the relatedness of source and target tasks.",5.3. Application to Transfer Learning,[0],[0]
So it is enough to pick the point that minimizes the stability bound to transfer from the most related source.,5.3. Application to Transfer Learning,[0],[0]
"Then, bounding Rpwsrck q by pRSpwsrck q through Hoeffding bound along with union bound gives with high probability that
min kPrKs pD,wsrck q ď min kPrKs O
˜
pRSpwsrck q",5.3. Application to Transfer Learning,[0],[0]
"` c logpKq m
¸
.
",5.3. Application to Transfer Learning,[0],[0]
"Hence, the most related source is the one that simply minimizes empirical risk.",5.3. Application to Transfer Learning,[0],[0]
"Similar conclusions where drawn in HTL literature, albeit in the context of ERM.",5.3. Application to Transfer Learning,[0],[0]
Matters are slightly more complicated in the non-convex case.,5.3. Application to Transfer Learning,[0],[0]
"We take a similar approach, however, now we minimize stability bound (3), and for the sake of simplicity assume that we make a full pass over the data, so T “ m. Minimizing the following empirical upper bound select the best source.",5.3. Application to Transfer Learning,[0],[0]
Proposition 1.,5.3. Application to Transfer Learning,[0],[0]
"Let pγ˘k “ Θ ´ 1 m řm i“1 }∇2fpwsrck , ziq}2 ` b
pRSpwsrck q ˘ 4 a
logpKq{m ¯
.",5.3. Application to Transfer Learning,[0],[0]
"Then with high probability the generalization error of wsrck is bounded by
min kPrKs
O
¨
˝
ˆ
1` 1 cpγ´k
˙ pRSpwsrck q cpγ",5.3. Application to Transfer Learning,[0],[0]
"` k 1`cpγ` k ¨ a logpKq
m 1 1`cpγ` k
˛
‚ .
",5.3. Application to Transfer Learning,[0],[0]
"Note that pγ˘k involves estimation of the spectral norm of the Hessian, which is computationally cheaper to evaluate compared to the complete Hessian matrix (Pearlmutter, 1994).",5.3. Application to Transfer Learning,[0],[0]
"This is particularly relevant for deep learning, where computation of the Hessian matrix can be prohibitively expensive.",5.3. Application to Transfer Learning,[0],[0]
"In this section we discuss the proof of Theorem 4, which bounds data-dependent stability for non-convex losses.
",6. Proof Outline of Theorem 4,[0],[0]
"We say that the SGD gradient update rule is an operator Gt : H ÞÑ H, such that
Gtpwq :“ w ´ αt∇fpw, zitq ,
and it is also a function of the training set S and a random index set I .",6. Proof Outline of Theorem 4,[0],[0]
"Then, wt`1 “ Gtpwtq, throughout t “ 1, . . .",6. Proof Outline of Theorem 4,[0],[0]
", T .",6. Proof Outline of Theorem 4,[0],[0]
"Recall the use of notation wS,t to indicate the output of SGD ran on a training set S, at step t, and define
δtpS, zq :“ }wS,t ´wSpiq,t} .
",6. Proof Outline of Theorem 4,[0],[0]
The following propoperty of Gt will be central to our proof.,6. Proof Outline of Theorem 4,[0],[0]
Definition 6 (Expansiveness).,6. Proof Outline of Theorem 4,[0],[0]
"A gradient update rule is η-expansive if for all w,v, }Gtpwq´Gtpvq} ď η}w´v} .
",6. Proof Outline of Theorem 4,[0],[0]
The following lemma characterizes expansiveness for the gradient update rule under different assumptions on f .,6. Proof Outline of Theorem 4,[0],[0]
"Lemma 1 ((Hardt et al., 2016)).",6. Proof Outline of Theorem 4,[0],[0]
Assume that f is β-smooth.,6. Proof Outline of Theorem 4,[0],[0]
"Then, we have that Gt is p1` αtβq-expansive.
",6. Proof Outline of Theorem 4,[0],[0]
"The following lemma is similar to Lemma 3.11 of (Hardt et al., 2016), and is instrumental in bounding the stability of SGD.",6. Proof Outline of Theorem 4,[0],[0]
"However, we make an adjustment and state it in expectation over the data.",6. Proof Outline of Theorem 4,[0],[0]
Note that it does not require convexity of the loss function.,6. Proof Outline of Theorem 4,[0],[0]
Lemma 2.,6. Proof Outline of Theorem 4,[0],[0]
"Assume that the loss function fp¨, zq P r0, 1s is L-Lipschitz for all z. Then, for every t0 P t0, 1, 2, . . .mu",6. Proof Outline of Theorem 4,[0],[0]
"we have that,
E S,z E A
“ fpwS,T , zq ´ fpwSpiq,T , zq ‰
(4)
ď L E S,z
”
E A rδT pS, zq | δt0pS, zq “ 0s
ı
` E S,A rRpASqs t0 m .
",6. Proof Outline of Theorem 4,[0],[0]
"(5)
We spend a moment to highlight the role of conditional expectation.",6. Proof Outline of Theorem 4,[0],[0]
"Observe that we could naively bound (4) by the Lipschitzness of f , but Lemma 2 follows a more careful argument.",6. Proof Outline of Theorem 4,[0],[0]
First note that t0 is a free parameter.,6. Proof Outline of Theorem 4,[0],[0]
"The expected distance in (5) between SGD outputs wS,t and wSpiq,t is conditioned on the fact that at step t0 outputs of SGD are still the same.",6. Proof Outline of Theorem 4,[0],[0]
This means that the perturbed point is encountered after t0.,6. Proof Outline of Theorem 4,[0],[0]
"Then, the conditional expectation should be a decreasing function of t0: the later the perturbation occurs, the smaller deviation between wS,t and wSpiq,t we should expect.",6. Proof Outline of Theorem 4,[0],[0]
Eventually we minimize (5) over t0.,6. Proof Outline of Theorem 4,[0],[0]
"Our proof of a stability bound for non-convex loss functions, Theorem 4, follows a general outline of (Hardt et al., 2016, Theorem 3.8).",6.1. Non-convex Losses,[0],[0]
"Namely, the outputs of SGD run on a training set S and its perturbed version Spiq will not differ too much, because by the time a perturbation is encountered, the step size has already decayed enough.",6.1. Non-convex Losses,[0],[0]
"So, on the one hand, stabilization is enforced by the diminishing the step size, and on the other hand, by how much updates expand the distance between the gradients after the perturbation.",6.1. Non-convex Losses,[0],[0]
"Since (Hardt et al., 2016) work with uniform stability, they capture the expansiveness of post-perturbation update by the Lipschitzness of the gradient.",6.1. Non-convex Losses,[0],[0]
"In combination with a recursive argument, their bound has exponential dependency on the Lipschitz constant of the gradient.",6.1. Non-convex Losses,[0],[0]
We argue that the Lipschitz continuity of the gradient can be too pessimistic in general.,6.1. Non-convex Losses,[0],[0]
"Instead, we rely on a local data-driven argument: considering that we initialize SGD at point w1, how much do updates expand the gradient under the distribution of interest?",6.1. Non-convex Losses,[0],[0]
"The following crucial lemma characterizes such behavior in terms of the curvature at w1.
",6.1. Non-convex Losses,[0],[0]
Lemma 3.,6.1. Non-convex Losses,[0],[0]
"Assume that the loss function fp¨, zq is β-smooth and that its Hessian is ρ-Lipschitz.",6.1. Non-convex Losses,[0],[0]
"Then, ›
›GtpwS,tq ´GtpwSpiq,tq › › ď p1` αtξtpS, zqq δtpS, zq
Furthermore, for any t P rT s,
E S,z rξtpS, zqs “ Õ
ˆ
E S,z
“ › ›∇2fpw1, ztq › ›
2
‰ `∆‹1,σ2 ˙ ,
∆‹1,σ2 :“ ρ ´ cσ ` a c pRpw1q ´R‹q ¯ .
",6.1. Non-convex Losses,[0],[0]
"Next, we need the following statement.
",6.1. Non-convex Losses,[0],[0]
Proposition 2 (Bernstein-type inequality).,6.1. Non-convex Losses,[0],[0]
"Let Z be a zeromean real-valued r.v., such that |Z| ď b and ErZ2s ď σ2.",6.1. Non-convex Losses,[0],[0]
"Then for all |c| ď 12b , we have that E “ ecZ ‰ ď ec2σ2 .
",6.1. Non-convex Losses,[0],[0]
"Now we are ready to prove Theorem 4.
",6.1. Non-convex Losses,[0],[0]
Sketch proof of Theorem 4.,6.1. Non-convex Losses,[0],[0]
We start from Lemma 2.,6.1. Non-convex Losses,[0],[0]
Most of the proof is dedicated to bounding the first term in (5).,6.1. Non-convex Losses,[0],[0]
"We deal with this similarly as in (Hardt et al., 2016).",6.1. Non-convex Losses,[0],[0]
"Specifically, we state the bound on EA rδT pS, zq|δt0pS, zq “ 0s by using a recursion.",6.1. Non-convex Losses,[0],[0]
"In our case, however, we also have an expectation w.r.t.",6.1. Non-convex Losses,[0],[0]
"the data, and to avoid complications with dependencies, we first unroll the recursion for the random quantities, and only then take the expectation.",6.1. Non-convex Losses,[0],[0]
"At this point the proof crucially relies on the product of exponentials arising from the recursion, and all relevant random quantities end up inside of them.",6.1. Non-convex Losses,[0],[0]
We alleviate this by Proposition 2.,6.1. Non-convex Losses,[0],[0]
"Finally, we conclude by minimizing (5) w.r.t. t0.",6.1. Non-convex Losses,[0],[0]
"Thus we have three steps: 1) recursion, 2) bounding Erexpp¨",6.1. Non-convex Losses,[0],[0]
"¨ ¨ qs, and 3) tuning of t0.
1) Recursion.",6.1. Non-convex Losses,[0],[0]
"We begin by stating the bound on EA rδT pS, zq|δt0pS, zq “ 0s by recursion.",6.1. Non-convex Losses,[0],[0]
"Thus we will first state the bound on EA rδt`1pS, zq|δt0pS, zq “ 0s in terms of EA rδtpS, zq|δt0pS, zq “ 0s, and other relevant quantities and then unravel the recursion.",6.1. Non-convex Losses,[0],[0]
"We distinguish two cases: 1) SGD encounters the perturbed point at step t, that is t “ i, and 2)",6.1. Non-convex Losses,[0],[0]
"the current point is the same in S and Spiq, so t ‰ i.",6.1. Non-convex Losses,[0],[0]
"For the first case, we will use worst-case boundedness of Gt, that is, we observe that }GtpwS,tq ´ GtpwSpiq,tq} ď δtpS, zq ` 2αtL. To handle the second case we will use Lemma 3, namely, ›
›GtpwS,tq ´GtpwSpiq,tq › › ď p1` αtξtpS, zqq δtpS, zq .
",6.1. Non-convex Losses,[0],[0]
"In addition, as a safety measure we will also take into account that the gradient update rule is at most p1 ` αtβqexpansive by Lemma 1.",6.1. Non-convex Losses,[0],[0]
"So we will work with the function ψtpS, zq :“ min tξtpS, zq, βu instead of ξtpS, zq.",6.1. Non-convex Losses,[0],[0]
"Now, introduce ∆tpS, zq :“ EArδtpS, zq | δt0pS, zq “ 0s, and decompose the expectation w.r.t.",6.1. Non-convex Losses,[0],[0]
A for a step t.,6.1. Non-convex Losses,[0],[0]
"Noting that SGD encounters the perturbed example with probability 1m ,
∆t`1pS, zq ď ˆ 1´ 1 m ˙ p1` αtψtpS, zqq∆tpS, zq
` 1 m p2αtL`∆tpS, zqq “ ˆ 1` ˆ
1´ 1 m
˙ αtψtpS, zq ˙ ∆tpS, zq ` 2αtL
m
ď exp pαtψtpS, zqq∆tpS, zq ` 2αtL
m , (6)
where the last inequality follows from 1`x ď exppxq.",6.1. Non-convex Losses,[0],[0]
"This inequality is not overly loose for x P r0, 1s, and, in our case it becomes instrumental in handling the recursion.
",6.1. Non-convex Losses,[0],[0]
"Now, observe that relation xt`1 ď atxt ` bt with xt0 “ 0 unwinds from T to t0 as xT ď řT t“t0`1 bt śT k“t`1 ak.",6.1. Non-convex Losses,[0],[0]
"Consequently, having ∆t0pS, zq “ 0, we unwind (6) to get
∆T pS, zq ď T ÿ
t“t0`1
˜
T ź
k“t`1 exp
ˆ
cψkpS, zq k
˙
¸
2cL
mt
“ T ÿ
t“t0`1 exp
˜
c T ÿ
k“t`1
ψkpS, zq k
¸
2cL mt .",6.1. Non-convex Losses,[0],[0]
"(7)
2) Bounding Erexpp¨",6.1. Non-convex Losses,[0],[0]
¨ ¨ qs.,6.1. Non-convex Losses,[0],[0]
We take expectation w.r.t.,6.1. Non-convex Losses,[0],[0]
S and z on both sides and focus on the expectation of the exponential in (7).,6.1. Non-convex Losses,[0],[0]
"First, introduce µk :“ ES,zrψkpS, zqs, and observe that the zero-mean version of ψkpS, zq is bounded as
řT k“t`1 1 k |ψkpS, zq ´ µk| ď 2β lnpT q.",6.1. Non-convex Losses,[0],[0]
"Assuming the
setting of c ď 12p2β lnpT qq2 , we apply Proposition 2 and get
E S,z
«
exp
˜
c T ÿ
k“t`1
ψkpS, zq k
¸ff ď exp ˜",6.1. Non-convex Losses,[0],[0]
"c T ÿ
k“t`1
2µk",6.1. Non-convex Losses,[0],[0]
"k
¸
,
(8)
where we bounded variance by µk thanks to the setting of c. Next, we give an upper-bound on µk, that is µk ď min tβ,ES,zrξkpS, zqsu.",6.1. Non-convex Losses,[0],[0]
"Finally, we bound ES,zrξkpS, zqs using the second result of Lemma 3, which holds for any k P rT s, to get that µk ď γ, with γ defined in (2).
3) Tuning of t0.",6.1. Non-convex Losses,[0],[0]
Now we turn our attention back to (7).,6.1. Non-convex Losses,[0],[0]
Considering that we took an expectation w.r.t.,6.1. Non-convex Losses,[0],[0]
"the data, we use (8) and the fact that µk ď γ to get that
E S,z r∆T pS, zqs ď
T ÿ
t“t0`1 exp
˜
2cγ T ÿ
k“t`1
1
k
¸
2cL
mt
ď T ÿ
t“t0`1 exp
ˆ
2cγ",6.1. Non-convex Losses,[0],[0]
"ln
ˆ
T
t
˙˙
2cL mt ď L γm
ˆ
T
t0
˙2cγ
.
",6.1. Non-convex Losses,[0],[0]
"Plug the above into (5) to get
E S,z E A
“ fpwS,T , zq ´ fpwSpiq,T , zq ‰
ď L 2
γm
ˆ
T
t0
˙2cγ
` t0 m .",6.1. Non-convex Losses,[0],[0]
"(9)
Let q “ 2cγ.",6.1. Non-convex Losses,[0],[0]
"Then, setting t0 “ ` 2cL2 ˘ 1 1`q T q 1`q minimizes (9).",6.1. Non-convex Losses,[0],[0]
Plugging t0 back we get that (9) equals to 1` 1q m ` 2cL2 ˘ 1 1`q T q 1`q .,6.1. Non-convex Losses,[0],[0]
This completes the proof.,6.1. Non-convex Losses,[0],[0]
In this work we proved data-dependent stability bounds for SGD and revisited its generalization ability.,7. Conclusions and Future Work,[0],[0]
"We presented novel bounds for convex and non-convex smooth loss functions, partially controlled by data-dependent quantities, while previous stability bounds for SGD were derived through the worst-case analysis.",7. Conclusions and Future Work,[0],[0]
"In particular, for non-convex learning, we demonstrated theoretically that generalization of SGD is heavily affected by the expected curvature around the initialization point.",7. Conclusions and Future Work,[0],[0]
We demonstrated empirically that our bound is indeed tighter compared to the uniform one.,7. Conclusions and Future Work,[0],[0]
"In addition, our data-dependent analysis also allowed us to show optimistic bounds on the generalization error of SGD, which exhibit fast rates subject to the vanishing empirical risk of the algorithm’s output.
",7. Conclusions and Future Work,[0],[0]
In future work we further intend to explore our theoretical findings experimentally and evaluate the feasibility of the transfer learning based on the second-order information.,7. Conclusions and Future Work,[0],[0]
Another direction lies in making our bounds adaptive.,7. Conclusions and Future Work,[0],[0]
"So far we have presented bounds that have data-dependent components, however the step size cannot be adjusted depending on the data, e.g. as in (Zhao & Zhang, 2015).",7. Conclusions and Future Work,[0],[0]
"This was partially addressed by (London, 2016), albeit in the context of uniform stability, and we plan to extend this idea to the context of data-dependent stability.",7. Conclusions and Future Work,[0],[0]
This work was in parts funded by the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement no 637076).,Acknowledgments,[0],[0]
This work was in parts funded by the European Research Council under the European Union’s Seventh Framework Programme (FP7/2007-2013)/ERC grant agreement no 308036.,Acknowledgments,[0],[0]
"We establish a data-dependent notion of algorithmic stability for Stochastic Gradient Descent (SGD), and employ it to develop novel generalization bounds.",abstractText,[0],[0]
This is in contrast to previous distribution-free algorithmic stability results for SGD which depend on the worst-case constants.,abstractText,[0],[0]
"By virtue of the data-dependent argument, our bounds provide new insights into learning with SGD on convex and non-convex problems.",abstractText,[0],[0]
"In the convex case, we show that the bound on the generalization error depends on the risk at the initialization point.",abstractText,[0],[0]
"In the non-convex case, we prove that the expected curvature of the objective function around the initialization point has crucial influence on the generalization error.",abstractText,[0],[0]
"In both cases, our results suggest a simple data-driven strategy to stabilize SGD by pre-screening its initialization.",abstractText,[0],[0]
"As a corollary, our results allow us to show optimistic generalization bounds that exhibit fast convergence rates for SGD subject to a vanishing empirical risk and low noise of stochastic gradient.",abstractText,[0],[0]
Data-Dependent Stability of Stochastic Gradient Descent,title,[0],[0]
"Many sequential decision problems, including diabetes treatment (Bastani, 2014), digital marketing (Theocharous et al., 2015), and robot control (Lillicrap et al., 2015), are modeled as Markov decision processes (MDPs) and solved using reinforcement learning (RL) algorithms.",1. Introduction,[0],[0]
One important problem when applying RL to real problems is policy evaluation.,1. Introduction,[0],[0]
The goal in policy evaluation is to estimate the expected return (sum of rewards) produced by a policy.,1. Introduction,[0],[0]
"We refer to this policy as the evaluation policy, πe.",1. Introduction,[0],[0]
The standard policy evaluation approach is to repeatedly deploy πe and average the resulting returns.,1. Introduction,[0],[0]
"While this naı̈ve Monte Carlo estimator is unbiased, it may have high variance.
",1. Introduction,[0],[0]
"1The University of Texas at Austin, Austin, Texas, USA 2The University of Massachusetts, Amherst, Massachusetts, USA 3Carnegie Mellon University, Pittsburgh, Pennsylvania, USA.",1. Introduction,[0],[0]
"Correspondence to: Josiah P. Hanna <jphanna@cs.utexas.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
Methods that evaluate πe while selecting actions according to πe are termed on-policy.,1. Introduction,[0],[0]
"Previous work has addressed variance reduction for on-policy returns (Zinkevich et al., 2006; White & Bowling, 2009; Veness et al., 2011).",1. Introduction,[0],[0]
"An alternative approach is to estimate the performance of πe while following a different, behavior policy, πb. Methods that evaluate πe with data generated from πb are termed offpolicy.",1. Introduction,[0],[0]
Importance sampling (IS) is one standard approach for using off-policy data in RL.,1. Introduction,[0],[0]
"IS reweights returns observed while executing πb such that they are unbiased estimates of the performance of πe.
",1. Introduction,[0],[0]
"Presently, IS is usually used when off-policy data is already available or when executing πe is impractical.",1. Introduction,[0],[0]
"If πb is not chosen carefully, IS often has high variance (Thomas et al., 2015).",1. Introduction,[0],[0]
"For this reason, an implicit assumption in the RL community has generally been that on-policy evaluation is more accurate when it is feasible.",1. Introduction,[0],[0]
"However, IS can also be used for variance reduction when done with an appropriately selected distribution of returns (Hammersley & Handscomb, 1964).",1. Introduction,[0],[0]
"While IS-based variance reduction has been explored in RL, this prior work has required knowledge of the environment’s transition probabilities and remains onpolicy (Desai & Glynn, 2001; Frank et al., 2008; Ciosek & Whiteson, 2017).",1. Introduction,[0],[0]
"In contrast to this earlier work, we show how careful selection of the behavior policy can lead to lower variance policy evaluation than using the evaluation policy and do not require knowledge of the environment’s transition probabilities.
",1. Introduction,[0],[0]
"In this paper, we formalize the selection of πb as the behavior policy search problem.",1. Introduction,[0],[0]
We introduce a method for this problem that adapts the policy parameters of πb with gradient descent on the variance of importance-sampling.,1. Introduction,[0],[0]
Empirically we demonstrate behavior policy search with our method lowers the mean squared error of estimates compared to on-policy estimates.,1. Introduction,[0],[0]
"To the best of our knowledge, this work is the first to propose adapting the behavior policy to obtain better policy evaluation in RL.",1. Introduction,[0],[0]
Furthermore we present the first method to address this problem.,1. Introduction,[0],[0]
"This section details the policy evaluation problem setting, the Monte Carlo and Advantage Sum on-policy methods, and importance-sampling for off-policy evaluation.",2. Preliminaries,[0],[0]
"We use notational standard MDPNv1 (Thomas, 2015), and for simplicity, we assume that S,A, and R are finite.1 Let H := (S0, A0, R0, S1, . . .",2.1. Background,[0],[0]
", SL, AL, RL) be a trajectory and g(H) := ∑L t=0 γ
tRt be the discounted return of trajectory H .",2.1. Background,[0],[0]
Let ρ(π),2.1. Background,[0],[0]
:= E[g(H)|H ∼ π] be the expected discounted return when the stochastic policy π is used from S0 sampled from the initial state distribution.,2.1. Background,[0],[0]
"In this work, we consider parameterized policies, πθ, where the distribution over actions is determined by the vector θ.",2.1. Background,[0],[0]
"We assume that the transitions and reward function are unknown and that L is finite.
",2.1. Background,[0],[0]
"We are given an evaluation policy, πe, for which we would like to estimate ρ(πe).",2.1. Background,[0],[0]
We assume there exists a policy parameter vector θe such that πe = πθe and that this vector is known.,2.1. Background,[0],[0]
"We consider an incremental setting where, at iteration i, we sample a single trajectory Hi with a policy πθi and add {Hi,θi} to a set D. We use Di to denote the set at iteration i. Methods that always (i.e., ∀i) choose θi = θe are on-policy; otherwise, the method is off-policy.",2.1. Background,[0],[0]
"A policy evaluation method, PE, uses all trajectories in Di to estimate ρ(πe).",2.1. Background,[0],[0]
Our goal is to design a policy evaluation algorithm that produces estimates of ρ(πe) that have low mean squared error (MSE).,2.1. Background,[0],[0]
"Formally, the goal of policy evaluation with PE is to minimize (PE(Di)− ρ(πe))2.",2.1. Background,[0],[0]
"While other measures of policy evaluation accuracy could be considered, we follow earlier work in using MSE (e.g., (Thomas & Brunskill, 2016; Precup et al., 2000)).
",2.1. Background,[0],[0]
We focus on unbiased estimators of ρ(πe).,2.1. Background,[0],[0]
"While biased estimators (e.g., bootstrapping methods (Sutton & Barto, 1998), approximate models (Kearns & Singh, 2002), etc.) can sometimes produce lower MSE estimates they are problematic for high risk applications requiring confidence intervals.",2.1. Background,[0],[0]
"For unbiased estimators, minimizing variance is equivalent to minimizing MSE.",2.1. Background,[0],[0]
Perhaps the most commonly used policy evaluation method is the on-policy Monte-Carlo (MC) estimator.,2.2. Monte-Carlo Estimates,[0],[0]
"The estimate of ρ(πe) at iteration i is the average return:
MC(Di) :",2.2. Monte-Carlo Estimates,[0],[0]
"= 1
i+ 1 i∑ j=0 L∑ t=0 γtRt = 1 i+ 1 i∑ j=0 g(Hj).
",2.2. Monte-Carlo Estimates,[0],[0]
This estimator is unbiased and strongly consistent given mild assumptions.2,2.2. Monte-Carlo Estimates,[0],[0]
"However, this method can have high variance.
1The methods, and theoretical results discussed in this paper are applicable to both finite and infinite S,A and R as well as partially-observable Markov decision processes.
",2.2. Monte-Carlo Estimates,[0],[0]
2Being a strongly consistent estimator of ρ(πe) means that,2.2. Monte-Carlo Estimates,[0],[0]
"Like the Monte-Carlo estimator, the advantage sum (ASE) estimator selects θi = θe for all i. However, it introduces a control variate to reduce the variance without introducing bias.",2.3. Advantage Sum Estimates,[0],[0]
This control variate requires an approximate model of the MDP to be provided.,2.3. Advantage Sum Estimates,[0],[0]
"Let the reward function of this model be given as r̂(s, a).",2.3. Advantage Sum Estimates,[0],[0]
"Let q̂πe(st, at) =",2.3. Advantage Sum Estimates,[0],[0]
"E[ ∑L t′=t γ
t′ r̂(st′ , at′)] and v̂πe(st) =",2.3. Advantage Sum Estimates,[0],[0]
"E[q̂πe(st, at)|at ∼ πe], i.e., the action-value function and state-value function of πe in this approximate model.",2.3. Advantage Sum Estimates,[0],[0]
"Then, the advantage sum estimator is given by:
AS(Di) := 1
i+ 1 i∑ j=0 L∑ t=0 γt(Rt− q̂πe(St, At)+ v̂πe(St)).
",2.3. Advantage Sum Estimates,[0],[0]
"Intuitively, ASE is replacing part of the randomness of the Monte Carlo return with the known expected return under the approximate model.",2.3. Advantage Sum Estimates,[0],[0]
"Provided qπe(St, At)− v̂πe(St) is sufficiently correlated with Rt, the variance of ASE is less than that of MC.
Notice that, like the MC estimator, the ASE estimator is on-policy, in that the behavior policy is always the policy that we wish to evaluate.",2.3. Advantage Sum Estimates,[0],[0]
Intuitively it may seems like this choice should be optimal.,2.3. Advantage Sum Estimates,[0],[0]
"However, we will show that it is not—selecting behavior policies that are different from the evaluation policy can result in estimates of ρ(πe) that have lower variance.",2.3. Advantage Sum Estimates,[0],[0]
"Importance Sampling is a method for reweighting returns from a behavior policy, θ, such that they are unbiased returns from the evaluation policy.",2.4. Importance Sampling,[0],[0]
"In RL, the re-weighted IS return of a trajectory, H , sampled from πθ is:
IS(H,θ) := g(H) L∏ t=0 πe(St|At) πθ(St|At) .
",2.4. Importance Sampling,[0],[0]
The IS off-policy estimator is then a Monte Carlo estimate of E,2.4. Importance Sampling,[0],[0]
"[IS(H,θ)|H ∼ πθ]:
IS(Di) := 1
i+ 1 i∑ j=0 IS(Hj ,θj).
",2.4. Importance Sampling,[0],[0]
"In RL, importance sampling allows off-policy data to be used as if it were on-policy.",2.4. Importance Sampling,[0],[0]
"In this case the variance of the IS estimate is often much worse than the variance of on-policy MC estimates because the behavior policy is not
Pr ( lim i→∞ MC(Di) = ρ(πe) )",2.4. Importance Sampling,[0],[0]
= 1.,2.4. Importance Sampling,[0],[0]
"If ρ(πe) exists, MC is strongly consistent by the Khintchine Strong law of large numbers (Sen & Singer, 1993).
chosen to minimize variance, but is a policy that is dictated by circumstance.",2.4. Importance Sampling,[0],[0]
"Importance sampling was originally intended as a variance reduction technique for Monte Carlo evaluation (Hammersley & Handscomb, 1964).",3. Behavior Policy Search,[0],[0]
When an evaluation policy rarely samples trajectories with high magnitude returns a Monte Carlo evaluation will have high variance.,3. Behavior Policy Search,[0],[0]
If a behavior policy can increase the probability of observing such trajectories then the off-policy IS estimate will have lower variance than an on-policy Monte Carlo estimate.,3. Behavior Policy Search,[0],[0]
In this section we first describe the theoretical potential for variance reduction with an appropriately selected behavior policy.,3. Behavior Policy Search,[0],[0]
In general this policy will be unknown.,3. Behavior Policy Search,[0],[0]
"Thus, we propose a policy evaluation subproblem — the behavior policy search problem — solutions to which will adapt the behavior policy to provide lower mean squared error policy performance estimates.",3. Behavior Policy Search,[0],[0]
"To the best of our knowledge, we are the first to propose behavior policy adaptation for policy evaluation.",3. Behavior Policy Search,[0],[0]
An appropriately selected behavior policy can lower variance to zero.,3.1. The Optimal Behavior Policy,[0],[0]
"While this fact is generally known for importance-sampling, we show here that this policy exists for any MDP and evaluation policy under two restrictive assumptions: all returns are positive and the domain is deterministic.",3.1. The Optimal Behavior Policy,[0],[0]
"In the following section we describe how an initial policy can be adapted towards the optimal behavior policy even when these conditions fail to hold.
",3.1. The Optimal Behavior Policy,[0],[0]
Let wπ(H) := ∏L t=0 π(At|St).,3.1. The Optimal Behavior Policy,[0],[0]
"Consider a behavior policy π?b such that for any trajectory, H:
ρ(πe) = IS(H,π ?",3.1. The Optimal Behavior Policy,[0],[0]
"b ) = g(H)
wπe(H) wπ?b (H) .
",3.1. The Optimal Behavior Policy,[0],[0]
"Rearranging the terms of this expressions yields:
wπ?b (H) = g(H) wπe(H)
ρ(πe) .
",3.1. The Optimal Behavior Policy,[0],[0]
"Thus, if we can select π?b such that the probability of observing any H ∼ π?b is g(H) ρ(πe)
times the likelihood of observing H ∼ πe then the IS estimate has zero MSE with only a single sampled trajectory.",3.1. The Optimal Behavior Policy,[0],[0]
"Regardless of g(H), the importance-sampled return will equal ρ(πe).
",3.1. The Optimal Behavior Policy,[0],[0]
"Furthermore, the policy π?b exists within the space of all feasible stochastic policies.",3.1. The Optimal Behavior Policy,[0],[0]
"Consider that a stochastic policy can be viewed as a mixture policy over time-dependent (i.e., action selection depends on the current time-step) deterministic policies.",3.1. The Optimal Behavior Policy,[0],[0]
"For example, in an MDP with one state, two actions and a horizon of L there are 2L possible time-dependent deterministic policies, each of which
defines a unique sequence of actions.",3.1. The Optimal Behavior Policy,[0],[0]
We can express any evaluation policy as a mixture of these deterministic policies.,3.1. The Optimal Behavior Policy,[0],[0]
"The optimal behavior policy π?b can be expressed similarly and thus the optimal behavior policy exists.
",3.1. The Optimal Behavior Policy,[0],[0]
"Unfortunately, the optimal behavior policy depends on the unknown value ρ(πe) as well as the unknown reward function R (via g(H)).",3.1. The Optimal Behavior Policy,[0],[0]
"Thus, while there exists an optimal behavior policy for IS – which is not πe – in practice we cannot analytically determine π?b .",3.1. The Optimal Behavior Policy,[0],[0]
"Additionally, π ?",3.1. The Optimal Behavior Policy,[0],[0]
b may not be representable by any θ in our policy class.,3.1. The Optimal Behavior Policy,[0],[0]
"Since the optimal behavior policy cannot be analytically determined, we instead propose the behavior policy search (BPS) problem for finding πb that lowers the MSE of estimates of ρ(πe).",3.2. The Behavior Policy Search Problem,[0],[0]
"A BPS problem is defined by the inputs:
1.",3.2. The Behavior Policy Search Problem,[0],[0]
An evaluation policy πe with policy parameters θe. 2.,3.2. The Behavior Policy Search Problem,[0],[0]
"An off-policy policy evaluation algorithm,
OPE(H,θ), that takes a trajectory, H ∼ πθ, or, alternatively, a set of trajectories, and returns an estimate of ρ(πe).
",3.2. The Behavior Policy Search Problem,[0],[0]
"A BPS solution is a policy, πθb such that off-policy estimates with OPE have lower MSE than on-policy estimates.",3.2. The Behavior Policy Search Problem,[0],[0]
"Methods for this problem are BPS algorithms.
",3.2. The Behavior Policy Search Problem,[0],[0]
Recall we have formalized policy evaluation within an incremental setting where one trajectory for policy evaluation is generated each iteration.,3.2. The Behavior Policy Search Problem,[0],[0]
"At the ith iteration, a BPS algorithm selects a behavior policy that will be used to generate a trajectory, Hi.",3.2. The Behavior Policy Search Problem,[0],[0]
"The policy evaluation algorithm, OPE, then estimates ρ(πe) using trajectories in Di.",3.2. The Behavior Policy Search Problem,[0],[0]
"Naturally, the selection of the behavior policy depends on how OPE estimates ρ(πe).
",3.2. The Behavior Policy Search Problem,[0],[0]
"In a BPS problem, the ith iteration proceeds as follows.",3.2. The Behavior Policy Search Problem,[0],[0]
"First, given all of the past behavior policies, {θi}i−1i=0, and the resulting trajectories, {Hi}i−1i=0, the BPS algorithm must select θi.",3.2. The Behavior Policy Search Problem,[0],[0]
The policy πθi is then run for one episode to create the trajectory Hi.,3.2. The Behavior Policy Search Problem,[0],[0]
"Then the BPS algorithm uses OPE to estimate ρ(πe) given the available data, Di := {(θi, Hi)}ii=0.",3.2. The Behavior Policy Search Problem,[0],[0]
"In this paper, we consider the one-step problem of selecting θi and estimating ρ(πe) at iteration i in a way that minimizes MSE.",3.2. The Behavior Policy Search Problem,[0],[0]
"That is, we do not consider how our selection of θi will impact our future ability to select an appropriate θj for j > i and thus to produce more accurate estimates in the future.
",3.2. The Behavior Policy Search Problem,[0],[0]
"One natural question is: if we are given a limit on the number of trajectories that can be sampled, is it better to “spend” some of our limited trajectories on BPS instead of using on-policy estimates?",3.2. The Behavior Policy Search Problem,[0],[0]
"Since each OPE(Hi,θi) is an unbiased estimator of ρ(πe), we can use all sampled trajectories to compute OPE(Di).",3.2. The Behavior Policy Search Problem,[0],[0]
"Provided for all itera-
tions, Var[OPE(H,θi)] ≤ V ar[MC] then, in expectation, a BPS algorithm will always achieve lower MSE than MC, showing that it is, in fact, worthwhile to do so.",3.2. The Behavior Policy Search Problem,[0],[0]
This claim is supported by our empirical study.,3.2. The Behavior Policy Search Problem,[0],[0]
We now introduce our primary contributions: an analytic expression for the gradient of the mean squared error of the IS estimator and a stochastic gradient descent algorithm that adapts θ to minimize the MSE between the IS estimate and ρ(πe).,4. Behavior Policy Gradient Theorem,[0],[0]
Our algorithm — Behavior Policy Gradient (BPG) — begins with on-policy estimates and adapts the behavior policy with gradient descent on the MSE with respect to θ.,4. Behavior Policy Gradient Theorem,[0],[0]
"The gradient of the MSE with respect to the policy parameters is given by the following theorem: Theorem 1.
∂
∂θ MSE[IS(H,θ)]",4. Behavior Policy Gradient Theorem,[0],[0]
"= E
[ − IS(H,θ)2
L∑ t=0 ∂ ∂θ log πθ(At|St)
]
where the expectation is taken over H ∼ πθ.
",4. Behavior Policy Gradient Theorem,[0],[0]
Proof.,4. Behavior Policy Gradient Theorem,[0],[0]
"Proofs for all theoretical results are included in Appendix A of an extended version available at http: //arxiv.org/abs/1706.03469.
",4. Behavior Policy Gradient Theorem,[0],[0]
BPG uses stochastic gradient descent in place of exact gradient descent: replacing the intractable expectation in Theorem 1 with an unbiased estimate of the true gradient.,4. Behavior Policy Gradient Theorem,[0],[0]
"In our experiments, we sample a batch, Bi, of k trajectories with πθi to lower the variance of the gradient estimate at iteration i.",4. Behavior Policy Gradient Theorem,[0],[0]
"In the BPS setting, sampling a batch of trajectories is equivalent to holding θ fixed for k iterations and then updating θ with the k most recent trajectories used to compute the gradient estimate.
",4. Behavior Policy Gradient Theorem,[0],[0]
Full details of BPG are given in Algorithm 1.,4. Behavior Policy Gradient Theorem,[0],[0]
"At iteration i, BPG samples a batch, Bi, of k trajectories and adds {(θi, Hi)ki=0} to a data set D (Lines 4-5).",4. Behavior Policy Gradient Theorem,[0],[0]
Then BPG updates θ with an empirical estimate of Theorem 1 (Line 6).,4. Behavior Policy Gradient Theorem,[0],[0]
"After n iterations, the BPG estimate of ρ(πe) is IS(Dn) as defined in Section 2.4.
",4. Behavior Policy Gradient Theorem,[0],[0]
"Given that the step-size, αi, is consistent with standard gradient descent convergence conditions, BPG will converge to a behavior policy that locally minimizes the variance (Bertsekas & Tsitsiklis, 2000).",4. Behavior Policy Gradient Theorem,[0],[0]
"At best, BPG converges to the globally optimal behavior policy within the parameterization of πe.",4. Behavior Policy Gradient Theorem,[0],[0]
Since the parameterization of πe determines the class of representable distributions it is possible that the theoretically optimal behavior policy is unrepresentable under this parameterization.,4. Behavior Policy Gradient Theorem,[0],[0]
"Nevertheless, a suboptimal behavior policy still yields better estimates of ρ(πe), provided it decreases variance compared to on-policy returns.
",4. Behavior Policy Gradient Theorem,[0],[0]
"Algorithm 1 Behavior Policy Gradient Input: Evaluation policy parameters, θe, batch size k, a step-size for each iteration, αi, and number of iterations n. Output: Final behavior policy parameters θn and the IS estimate of ρ(πe) using all sampled trajectories.",4. Behavior Policy Gradient Theorem,[0],[0]
1: θ0 ← θe 2: D0 = {} 3: for all i ∈ 0...n,4. Behavior Policy Gradient Theorem,[0],[0]
"do 4: Bi = Sample k trajectories H ∼ πθi 5: Di+1 = Di ∪ Bi
6: θi+1 = θi + αik ∑ H∈B",4. Behavior Policy Gradient Theorem,[0],[0]
"IS(H,θ)2 L∑ t=0 ∂ ∂θ log πθi(At|St) 7: end for 8: Return θn, IS(Dn)",4. Behavior Policy Gradient Theorem,[0],[0]
"In cases where an approximate model is available, we can further lower variance adapting the behavior policy of the doubly robust estimator (Jiang & Li, 2016; Thomas & Brunskill, 2016).",4.1. Control Variate Extension,[0],[0]
"Based on a similar intuition as the Advantage Sum estimator (Section 2.3), the Doubly Robust (DR) estimator uses the value functions of an approximate model as a control variate to lower the variance of importancesampling.3 We show here that we can adapt the behavior policy to lower the mean squared error of DR estimates.",4.1. Control Variate Extension,[0],[0]
"We denote this new method DR-BPG for Doubly Robust Behavior Policy Gradient.
",4.1. Control Variate Extension,[0],[0]
"Let wπ,t(H) = ∏t i=0 π(At|St) and recall that v̂πe and q̂πe are the state and action value functions of πe in the approximate model.",4.1. Control Variate Extension,[0],[0]
"The DR estimator is:
DR(H,θ) := v̂(S0)+ L∑ t=0 wπe,t wπθ ,t (Rt−q̂πe(St, At)+v̂πe(St+1)).
",4.1. Control Variate Extension,[0],[0]
"We can reduce the mean squared error of DR with gradient descent using unbiased estimates of the following corollary to Theorem 1: Corollary 1.
∂
∂θ MSE",4.1. Control Variate Extension,[0],[0]
"[DR(H,θ)]",4.1. Control Variate Extension,[0],[0]
"= E[(DR(H,θ)2 L∑ t=0 ∂ ∂θ log πθ(At|St)
",4.1. Control Variate Extension,[0],[0]
"− 2DR(H,θ)( L∑ t=0 γtδt wπe,t wθ,t t∑ i=0 ∂ ∂θ log πθ(Ai|Si))",4.1. Control Variate Extension,[0],[0]
"]
where δt = Rt− q̂(St, At) + v̂(St+1) and the expectation is taken over H ∼ πθ.
",4.1. Control Variate Extension,[0],[0]
"The first term of ∂∂θMSE is analogous to the gradient of the importance-sampling estimate with IS(H,θ) replaced
3DR lowers the variance of per-decision importance-sampling which importance samples the per time-step reward.
by DR(H,θ).",4.1. Control Variate Extension,[0],[0]
"The second term accounts for the covariance of the DR terms.
",4.1. Control Variate Extension,[0],[0]
"AS and DR both assume access to a model, however, they make no assumption about where the model comes from except that it must be independent of the trajectories used to compute the final estimate.",4.1. Control Variate Extension,[0],[0]
"In practice, AS and DR perform best when all trajectories are used to estimate the model and then used to estimate ρ(πe) (Thomas & Brunskill, 2016).",4.1. Control Variate Extension,[0],[0]
"However, for DR-BPG, changes to the model change the surface of the MSE objective we seek to minimize and thus DR-BPG will only converge once the model stops changing.",4.1. Control Variate Extension,[0],[0]
"In our experiments, we consider both a changing and a fixed model.",4.1. Control Variate Extension,[0],[0]
"BPG is closely related to existing work in policy gradient RL (c.f., (Sutton et al., 2000))",4.2. Connection to REINFORCE,[0],[0]
and we draw connections between one such method and BPG to illustrate how BPG changes the distribution of trajectories.,4.2. Connection to REINFORCE,[0],[0]
"REINFORCE (Williams, 1992) attempts to maximize ρ(πθ) through gradient ascent on ρ(πθ) using the following unbiased gradient of ρ(πθ):
∂
∂θ ρ(πθ) =",4.2. Connection to REINFORCE,[0],[0]
"E
[ g(H)
L∑ t=0 ∂ ∂θ log πθ(At|St) ∣∣∣∣∣H ∼ πθ ] .
",4.2. Connection to REINFORCE,[0],[0]
"Intuitively, REINFORCE increases the probability of all actions taken during H as a function of g(H).",4.2. Connection to REINFORCE,[0],[0]
This update increases the probability of actions that lead to high return trajectories.,4.2. Connection to REINFORCE,[0],[0]
BPG can be interpreted as REINFORCE where the return of a trajectory is the square of its importance-sampled return.,4.2. Connection to REINFORCE,[0],[0]
"Thus BPG increases the probability of all actions taken along H as a function of IS(H,θ)2.",4.2. Connection to REINFORCE,[0],[0]
"The magnitude of IS(H,θ)2 depends on two qualities of H:
1.",4.2. Connection to REINFORCE,[0],[0]
"g(H)2 is large (i.e., a high magnitude event).",4.2. Connection to REINFORCE,[0],[0]
2.,4.2. Connection to REINFORCE,[0],[0]
"H is rare relative to its probability under the evalua-
tion policy (i.e., ∏L t=0 πe(At|St) πθ(At|St) is large).
",4.2. Connection to REINFORCE,[0],[0]
These two qualities demonstrate a balance in how BPG changes trajectory probabilities.,4.2. Connection to REINFORCE,[0],[0]
"Increasing the probability of a trajectory under πθ will decrease IS(H,θ)2 and so BPG increases the probability of a trajectory when g(H)2 is large enough to offset the decrease in IS(H,θ)2 caused by decreasing the importance weight.",4.2. Connection to REINFORCE,[0],[0]
This section presents an empirical study of variance reduction through behavior policy search.,5. Empirical Study,[0],[0]
"We design our experiments to answer the following questions:
• Can behavior policy search with BPG reduce policy evaluation MSE compared to on-policy estimates in
both tabular and continuous domains?",5. Empirical Study,[0],[0]
"• Does adapting the behavior policy of the Doubly Ro-
bust estimator with DR-BPG lower the MSE of the Advantage Sum estimator?",5. Empirical Study,[0],[0]
•,5. Empirical Study,[0],[0]
Does the rarety of actions that cause high magnitude rewards affect the performance gap between BPG and Monte Carlo estimates?,5. Empirical Study,[0],[0]
We address our first experimental question by evaluating BPG in three domains.,5.1. Experimental Set-up,[0],[0]
"We briefly describe each domain here; full details are available in appendix C.
The first domain is a 4x4 Gridworld.",5.1. Experimental Set-up,[0],[0]
"We obtain two evaluation policies by applying REINFORCE to this task, starting from a policy that selects actions uniformly at random.",5.1. Experimental Set-up,[0],[0]
"We then select one evaluation policy, π1, from the early stages of learning – an improved policy but still far from converged – and one after learning has converged, π2.",5.1. Experimental Set-up,[0],[0]
"We run all experiments once with πe := π1 and a second time with πe := π2.
",5.1. Experimental Set-up,[0],[0]
"Our second and third tasks are the continuous control Cartpole Swing Up and Acrobot tasks implemented within RLLAB (Duan et al., 2016).",5.1. Experimental Set-up,[0],[0]
The evaluation policy in each domain is a neural network that maps the state to the mean of a Gaussian distribution.,5.1. Experimental Set-up,[0],[0]
"Policies are partially optimized with trust-region policy optimization (Schulman et al., 2015) applied to a randomly initialized policy.",5.1. Experimental Set-up,[0],[0]
"Gridworld Experiments Figure 1 compares BPG to Monte Carlo for both Gridworld policies, π1 and π2.",5.2. Main Results,[0],[0]
Our main point of comparison is the mean squared error (MSE) of both estimates at iteration i over 100 trials.,5.2. Main Results,[0],[0]
"For π1, BPG significantly reduces the MSE of on-policy estimates (Figure 1a).",5.2. Main Results,[0],[0]
"For π2, BPG also reduces MSE, however, it is only a marginal improvement.
",5.2. Main Results,[0],[0]
At the end of each trial we used the final behavior policy to collect 100 more trajectories and estimate ρ(πe).,5.2. Main Results,[0],[0]
"In comparison to a Monte Carlo estimate with 100 trajectories from π1, MSE is 85.48 % lower with this improved behavior policy.",5.2. Main Results,[0],[0]
"For π2, the MSE is 31.02 % lower.",5.2. Main Results,[0],[0]
"This result demonstrates that BPG can find behavior policies that substantially lower MSE.
",5.2. Main Results,[0],[0]
"To understand the disparity in performance between these two instances of policy evaluation, we plot the distribution of g(H) under πe (Figures 1c and 1d).",5.2. Main Results,[0],[0]
These plots show the variance of π1 to be much higher; it sometimes samples returns with twice the magnitude of any sampled by π2.,5.2. Main Results,[0],[0]
"To quantify this difference, we also measure the variance of IS(H,θi) as E [ IS(H)2
∣∣H ∼ πθi]−E",5.2. Main Results,[0],[0]
"[IS(H)|H ∼ πθi ]2 where the expectations are estimated with 10,000 trajecto-
ries.",5.2. Main Results,[0],[0]
This evaluation is repeated 5 times per iteration and the reported variance is the mean over these evaluations.,5.2. Main Results,[0],[0]
The decrease in variance for each policy is shown in Figure 1e.,5.2. Main Results,[0],[0]
"The high initial variance means there is much more room for BPG to improve the behavior policy when θe is the partially optimized policy.
",5.2. Main Results,[0],[0]
We also test the sensitivity of BPG to the learning rate parameter.,5.2. Main Results,[0],[0]
"A critical issue in the use of BPG is selecting the
step size parameter α.",5.2. Main Results,[0],[0]
If α is set too high we risk making too large of an update to θ — potentially stepping to a worse behavior policy.,5.2. Main Results,[0],[0]
If we are too conservative then it will take many iterations for a noticeable improvement over Monte Carlo estimation.,5.2. Main Results,[0],[0]
Figure 1f shows variance reduction for a number of different α values in the GridWorld domain.,5.2. Main Results,[0],[0]
We found BPG in this domain was robust to a variety of step size values.,5.2. Main Results,[0],[0]
"We do not claim this result is representative for all problem domains; stepsize selection in the behavior policy search problem is an important area for future work.
",5.2. Main Results,[0],[0]
Continuous Control Figure 2 shows reduction of MSE on the Cartpole Swing-up and Acrobot domains.,5.2. Main Results,[0],[0]
Again we see that BPG reduces MSE faster than Monte Carlo evaluation.,5.2. Main Results,[0],[0]
"In contrast to the discrete Gridworld experiment, this experiment demonstrates the applicability of BPG to the continuous control setting.",5.2. Main Results,[0],[0]
"While BPG significantly outperforms Monte Carlo evaluation in Cart-pole Swingup, the gap is much smaller in Acrobot.",5.2. Main Results,[0],[0]
This result also demonstrates BPG (and behavior policy search) when the policy must generalize across different states.,5.2. Main Results,[0],[0]
"In this section, we evaluate the combination of modelbased control variates with behavior policy search.",5.3. Control Variate Extensions,[0],[0]
"Specifically, we compare the AS estimator with Doubly Robust BPG (DR-BPG).",5.3. Control Variate Extensions,[0],[0]
In these experiments we use a 10x10 stochastic gridworld.,5.3. Control Variate Extensions,[0],[0]
"The added stochasticity increases the difficulty of building an accurate model from trajectories.
",5.3. Control Variate Extensions,[0],[0]
Since these methods require a model we construct this model in one of two ways.,5.3. Control Variate Extensions,[0],[0]
The first method uses all trajectories in D to build the model and then uses the same set to estimate ρ(πe) with ASE or DR.,5.3. Control Variate Extensions,[0],[0]
The second method uses trajectories from the first 10 iterations to build the model and then fixes the model for the remaining iterations.,5.3. Control Variate Extensions,[0],[0]
"For DR-BPG, behavior policy search starts at iteration 10 un-
der this second condition.",5.3. Control Variate Extensions,[0],[0]
We call the first method “update” and the second method “fixed.”,5.3. Control Variate Extensions,[0],[0]
The update method invalidates the theoretical guarantees of these methods but learns a more accurate model.,5.3. Control Variate Extensions,[0],[0]
"In both instances, we build maximum likelihood tabular models.
",5.3. Control Variate Extensions,[0],[0]
Figure 3 demonstrates that combining BPG with a modelbased control variate (DR-BPG) can lead to further reduction of MSE compared to the control variate alone (ASE).,5.3. Control Variate Extensions,[0],[0]
"Specifically, with the fixed model, DR-BPG outperformed all other methods.",5.3. Control Variate Extensions,[0],[0]
DR-BPG using the update method for building the model performed competitively with ASE although not statistically significantly better.,5.3. Control Variate Extensions,[0],[0]
We also evaluate the final learned behavior policy of the fixed model variant of DR-BPG.,5.3. Control Variate Extensions,[0],[0]
"For a batch size of 100 trajectories, the DR estimator with this behavior policy improves upon the ASE estimator with the same model by 56.9 %.
",5.3. Control Variate Extensions,[0],[0]
"For DR-BPG, estimating the model with all data still allowed steady progress towards lower variance.",5.3. Control Variate Extensions,[0],[0]
This result is interesting since a changing model changes the surface of our variance objective and thus gradient descent on the variance has no theoretical guarantees of convergence.,5.3. Control Variate Extensions,[0],[0]
"Empirically, we observe that setting the learning rate for DRBPG was more challenging for either model type.",5.3. Control Variate Extensions,[0],[0]
"Thus while we have shown BPG can be combined with control variates, more work is needed to produce a robust method.",5.3. Control Variate Extensions,[0],[0]
Our final experiment aims to understand how the gap between on- and off-policy variance is affected by the probability of rare events.,5.4. Rareness of Event,[0],[0]
The intuition for why behavior policy search can lower the variance of on-policy estimates is that a well selected behavior policy can cause rare and high magnitude events to occur.,5.4. Rareness of Event,[0],[0]
"We test this intuition by varying the probability of a rare, high magnitude event and observing how this change affects the performance gap between on- and off-policy evaluation.",5.4. Rareness of Event,[0],[0]
"For this experiment, we use a variant of the deterministic Gridworld where taking the UP action in the initial state (the upper left corner) causes a transition to the terminal state with a reward of +50.",5.4. Rareness of Event,[0],[0]
We use π1 from our earlier Gridworld experiments but we vary the probability of choosing UP when in the initial state.,5.4. Rareness of Event,[0],[0]
So with probability p the agent will receive a large reward and end the trajectory.,5.4. Rareness of Event,[0],[0]
We use a constant learning rate of 10−5 for all values of p and run BPG for 500 iterations.,5.4. Rareness of Event,[0],[0]
We plot the relative decrease of the variance as a function of p over 100 trials for each value of p.,5.4. Rareness of Event,[0],[0]
We use relative variance to normalize across problem instances.,5.4. Rareness of Event,[0],[0]
"Note that under this measure, even when p is close to 1, the relative variance is not equal to zero because as p approaches 1 the initial variance also goes to zero.
",5.4. Rareness of Event,[0],[0]
"This experiment illustrates that as the initial variance increases, the amount of improvement BPG can achieve increases.",5.4. Rareness of Event,[0],[0]
"As p becomes closer to 1, the initial variance becomes closer to zero and BPG barely improves over the variance of Monte Carlo (in terms of absolute variance there is no improvement).",5.4. Rareness of Event,[0],[0]
"When the πe rarely takes the high rewarding UP action (p close to 0), BPG improves policy evaluation by increasing the probability of this action.",5.4. Rareness of Event,[0],[0]
This experiment supports our intuition for why off-policy evaluation can outperform on-policy evaluation.,5.4. Rareness of Event,[0],[0]
Behavior policy search and BPG are closely related to existing work on adaptive importance-sampling.,6. Related Work,[0],[0]
"While adaptive importance-sampling has been studied in the estimation literature, we focus here on adaptive importancesampling for MDPs and Markov Reward Processes (i.e., an MDP with a fixed policy).",6. Related Work,[0],[0]
"Existing work on adaptive IS in RL has considered changing the transition probabilities to lower the variance of policy evaluation (Desai & Glynn, 2001; Frank et al., 2008) or lower the variance of policy gradient estimates (Ciosek & Whiteson, 2017).",6. Related Work,[0],[0]
"Since the transition probabilities are typically unknown in RL, adapting the behavior policy is a more general approach to adaptive IS.",6. Related Work,[0],[0]
"Ciosek and Whiteson also adapt the distribution of trajectories with gradient descent on the variance (Ciosek & Whiteson, 2017) with respect to parameters of the transition probabilities.",6. Related Work,[0],[0]
"The main focus of this work is increasing
the probability of simulated rare events so that policy improvement can learn an appropriate response.",6. Related Work,[0],[0]
"In contrast, we address the problem of policy evaluation and differentiate with respect to the (known) policy parameters.
",6. Related Work,[0],[0]
The cross-entropy method (CEM) is a general method for adaptive importance-sampling.,6. Related Work,[0],[0]
CEM attempts to minimize the Kullback-Leibler divergence between the current sampling distribution and the optimal sampling distribution.,6. Related Work,[0],[0]
"As discussed in Section 3.1, this optimal behavior policy only exists under a set of restrictive conditions.",6. Related Work,[0],[0]
"In contrast we adapt the behavior policy by minimizing variance.
",6. Related Work,[0],[0]
Other methods exist for lowering the variance of on-policy estimates.,6. Related Work,[0],[0]
"In addition to the control variate technique used by the Advantage Sum estimator (Zinkevich et al., 2006; White & Bowling, 2009), Veness et al. consider using common random numbers and antithetic variates to reduce the variance of roll-outs in Monte Carlo Tree Search (MCTS) (2011).",6. Related Work,[0],[0]
These techniques require a model of the environment (as is typical for MCTS) and do not appear to be applicable to the general RL policy evaluation problem.,6. Related Work,[0],[0]
"BPG could potentially be applied to find a lower variance rollout policy for MCTS.
",6. Related Work,[0],[0]
In this work we have focused on unbiased policy evaluation.,6. Related Work,[0],[0]
"When the goal is to minimize MSE it is often permissible to use biased methods such as temporal difference learning (van Seijen & Sutton, 2014), model-based policy evaluation (Kearns & Singh, 2002; Strehl et al., 2009), or variants of weighted importance sampling (Precup et al., 2000).",6. Related Work,[0],[0]
It may be possible to use similar ideas to BPG to reduce bias and variance although this appears to be difficult since the bias contribution to the mean squared error is squared and thus any gradient involving bias requires knowledge of the estimator’s bias.,6. Related Work,[0],[0]
We leave behavior policy search with biased off-policy methods to future work.,6. Related Work,[0],[0]
Our experiments demonstrate that behavior policy search with BPG can lower the variance of policy evaluation.,7. Discussion and Future Work,[0],[0]
One open question is characterizing the settings where adapting the behavior policy substantially improves over on-policy estimates.,7. Discussion and Future Work,[0],[0]
"Towards answering this question, our Gridworld experiment showed that when πe has little variance, BPG can only offer marginal improvement.",7. Discussion and Future Work,[0],[0]
BPG increases the probability of observing rare events with a high magnitude.,7. Discussion and Future Work,[0],[0]
If the evaluation policy never sees such events then there is little benefit to using BPG.,7. Discussion and Future Work,[0],[0]
"However, in expectation and with an appropriately selected step-size, BPG will never lower the data-efficiency of policy evaluation.
",7. Discussion and Future Work,[0],[0]
It is also necessary that the evaluation policy contributes to the variance of the returns.,7. Discussion and Future Work,[0],[0]
"If all variance is due to the environment then it seems unlikely that BPG will offer much
improvement.",7. Discussion and Future Work,[0],[0]
"For example, Ciosek and Whiteson (2017) consider a variant of the Mountain Car task where the dynamics can trigger a rare event — independent of the action — in which rewards are multiplied by 1000.",7. Discussion and Future Work,[0],[0]
"No behavior policy adaptation can lower the variance due to this event.
",7. Discussion and Future Work,[0],[0]
One limitation of gradient-based BPS methods is the necessity of good step-size selection.,7. Discussion and Future Work,[0],[0]
"In theory, BPG can never lead to worse policy evaluation compared to on-policy estimates.",7. Discussion and Future Work,[0],[0]
"In practice, a poorly selected step-size may cause a step to a worse behavior policy at step iwhich may increase the variance of the gradient estimate at step i + 1.",7. Discussion and Future Work,[0],[0]
"Future work could consider methods for adaptive step-sizes, second order methods, or natural behavior policy gradients.
",7. Discussion and Future Work,[0],[0]
One interesting direction for future work is incorporating behavior policy search into policy improvement.,7. Discussion and Future Work,[0],[0]
A similar idea was explored by Ciosek and Whiteson who explored off-environment learning to improve the performance of policy gradient methods (2017).,7. Discussion and Future Work,[0],[0]
The method presented in that work is limited to simulated environments with differential dynamics.,7. Discussion and Future Work,[0],[0]
Adapting the behavior policy is a potentially much more general approach.,7. Discussion and Future Work,[0],[0]
We have introduced the behavior policy search problem in order to improve estimation of ρ(πe) for an evaluation policy πe.,8. Conclusion,[0],[0]
We present a solution — Behavior Policy Gradient — for this problem which adapts the behavior policy with stochastic gradient descent on the variance of the importance-sampling estimator.,8. Conclusion,[0],[0]
Experiments demonstrate BPG lowers the mean squared error of estimates of ρ(πe) compared to on-policy estimates.,8. Conclusion,[0],[0]
We also demonstrate BPG can further decrease the MSE of estimates in conjunction with a model-based control variate method.,8. Conclusion,[0],[0]
We thank Daniel Brown and the anonymous reviewers for useful comments on the work and its presentation.,9. Acknowledgements,[0],[0]
"This work has taken place in the Personal Autonomous Robotics Lab (PeARL) and Learning Agents Research Group (LARG) at the Artificial Intelligence Laboratory, The University of Texas at Austin.",9. Acknowledgements,[0],[0]
"PeARL research is supported in part by NSF (IIS-1638107, IIS-1617639).",9. Acknowledgements,[0],[0]
"LARG research is supported in part by NSF (CNS-1330072, CNS-1305287, IIS-1637736, IIS-1651089), ONR (21C184-01), AFOSR (FA9550-14-1-0087), Raytheon, Toyota, AT&T, and Lockheed Martin.",9. Acknowledgements,[0],[0]
Josiah Hanna is supported by an NSF Graduate Research Fellowship.,9. Acknowledgements,[0],[0]
"Peter Stone serves on the Board of Directors of Cogitai, Inc.",9. Acknowledgements,[0],[0]
The terms of this arrangement have been reviewed and approved by the University of Texas at Austin in accordance with its policy on objectivity in research.,9. Acknowledgements,[0],[0]
We consider the task of evaluating a policy for a Markov decision process (MDP).,abstractText,[0],[0]
The standard unbiased technique for evaluating a policy is to deploy the policy and observe its performance.,abstractText,[0],[0]
"We show that the data collected from deploying a different policy, commonly called the behavior policy, can be used to produce unbiased estimates with lower mean squared error than this standard technique.",abstractText,[0],[0]
We derive an analytic expression for the optimal behavior policy—the behavior policy that minimizes the mean squared error of the resulting estimates.,abstractText,[0],[0]
"Because this expression depends on terms that are unknown in practice, we propose a novel policy evaluation sub-problem, behavior policy search: searching for a behavior policy that reduces mean squared error.",abstractText,[0],[0]
We present a behavior policy search algorithm and empirically demonstrate its effectiveness in lowering the mean squared error of policy performance estimates.,abstractText,[0],[0]
Data-Efficient Policy Evaluation Through Behavior Policy Search,title,[0],[0]
Convolutional Neural Network (CNN) has become one of the most successful computational models in machine learning and artificial intelligence.,1. Introduction,[0],[0]
"Remarkable progress has been achieved in the design of successful CNN network structures, such as the VGG-Net (Simonyan & Zisserman, 2014), ResNet (He et al., 2016), and DenseNet (Huang et al., 2016).",1. Introduction,[0],[0]
Less attention has been paid to the design of filter structures in CNNs.,1. Introduction,[0],[0]
"Filters, namely the weights in the convolutional layers, are one of the most important ingredients of a CNN model, as filters contain the actual model parameters learned from enormous amounts of data.",1. Introduction,[0],[0]
"Filters in CNNs are typically randomly initialized, and then updated using variants and extensions of gradient descent (“back-propagation”).
",1. Introduction,[0],[0]
"1Duke University, Durham, North Carolina, USA.",1. Introduction,[0],[0]
"Work partially supported by NSF, DoD, NIH and AFOSR.",1. Introduction,[0],[0]
"Correspondence to: Xiuyuan Cheng <xiuyuan.cheng@duke.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"As a result, trained CNN filters have no specific structures, which often leads to significant redundancy in the learned model (Denton et al., 2014; Han et al., 2015; Iandola et al., 2016).",1. Introduction,[0],[0]
"Filters with improved properties will have a direct impact on the accuracy and efficiency of CNN, and the theoretical analysis of filters is also of central importance to the mathematical understanding of deep networks.
",1. Introduction,[0],[0]
"This paper suggests to decompose convolutional filters in CNN into a truncated expansion with pre-fixed bases in the spatial domain, namely the Decomposed Convolutional Filters network (DCFNet), where the expansion coefficients remain learned from data.",1. Introduction,[0],[0]
"By representing the filters in terms of functional bases, which can come from prior data or task knowledge, rather than as pixel values, the number of trainable parameters is reduced to the expansion coefficients; and furthermore, regularity conditions can be imposed on the filters via the truncated expansion.",1. Introduction,[0],[0]
"For image classification tasks, we empirically observe that DCFNet is able to maintain the accuracy with a significant reduction in the number of parameters.",1. Introduction,[0],[0]
"Such observation holds even when random bases are used.
",1. Introduction,[0],[0]
"In particular, we adopt in DCFNet the leading FourierBessel (FB) bases (Abramowitz & Stegun, 1964), which correspond to the low-frequency components in the input.",1. Introduction,[0],[0]
We experimentally observe the superior performance of DCFNet with FB bases (DCF-FB) in both image classification and denoising tasks.,1. Introduction,[0],[0]
"DCF-FB network reduces the response to the high-frequency components in the input, which are least stable under image variations such as deformation and often do not affect recognition after being
suppressed.",1. Introduction,[0],[0]
"Such an intuition is further supported by a mathematical analysis of the CNN representation, where we firstly develop a general result for the CNN representation stability when the input image undergoes a deformation, under proper boundedness conditions of the convolutional filters (Propositions 3.1, 3.3, 3.4).",1. Introduction,[0],[0]
"After imposing the DCF structure, we show that as long as the trainable expansion coefficients at each layer of a DCF-FB network satisfy a boundedness condition, the L-th-layer output is stable with respect to input deformation and the difference is bounded by the magnitude of the distortion (Theorems 3.7, 3.8).
",1. Introduction,[0],[0]
"Apart from FB bases, the DCFNet structure studied in this paper is compatible with general choices of bases, such as standard Fourier bases, wavelet bases, random bases and PCA bases.",1. Introduction,[0],[0]
We numerically test several options in Section 4.,1. Introduction,[0],[0]
"The stability analysis for DCF-FB networks can be extended to other bases choices as well, based upon the general theory developed for CNN representation and using similar techniques.
",1. Introduction,[0],[0]
"Our work is related to recent results on the topics of the usage of bases in deep networks, the model reduction of CNN, as well as the stability analysis of the deep representation.",1. Introduction,[0],[0]
We review these connections in Section 1.1.,1. Introduction,[0],[0]
"Finally, though the current paper focuses on supervised networks for classification and recognition applications in image data, the introduced DCF layers are a generic concept and can potentially be used in reconstruction and generative models as well.",1. Introduction,[0],[0]
We discuss possible extensions in the last section.,1. Introduction,[0],[0]
Deep network with bases and representation stability.,1.1. Related works,[0],[0]
"The usage of bases in deep networks has been previously studied, including wavelet bases, PCA bases, learned dictionary atoms, etc.",1.1. Related works,[0],[0]
"Wavelets are a powerful tool in signal processing (Mallat, 2008) and have been shown to be the optimal basis for data representation under generic settings (Donoho & Johnstone, 1994).",1.1. Related works,[0],[0]
"As a pioneering mathematical model of CNN, the scattering transform (Mallat, 2012; Bruna & Mallat, 2013; Sifre & Mallat, 2013) used pre-fixed weights in the network which are wavelet filters, and showed that the representation produced by a scattering network is stable with respect to certain variations in the input.",1.1. Related works,[0],[0]
"The extension of the scattering transform has been studied in (Wiatowski & Bölcskei, 2015; 2017) which includes a larger class of bases used in the network.",1.1. Related works,[0],[0]
"Apart from wavelet, deep network with PCA bases has been studied in (Chan et al., 2015).",1.1. Related works,[0],[0]
"Making a connection to dictionary learning (Aharon et al., 2006), (Papyan et al., 2016) studied deep networks in form of a cascade of convolutional sparse coding layers with theoretical analysis.",1.1. Related works,[0],[0]
"Deep networks with random weights have been studied in (Giryes et al., 2016), with proved representation stability.",1.1. Related works,[0],[0]
"The DCFNet studied in this
paper incorporates structured pre-fixed bases combined by adapted expansion coefficients learned from data in a supervised way, and demonstrates comparable and even improved classification accuracy on image datasets.",1.1. Related works,[0],[0]
"While the combination of fixed bases and learned coefficients has been studied in classical signal processing (Freeman et al., 1991; Mahalanobis et al., 1987), dictionary learning (Rubinstein et al., 2010) and computer vision (Henriques et al., 2013; Bertinetto et al., 2016), they were not designed with deep architectures in mind.",1.1. Related works,[0],[0]
"Meanwhile, the representation stability of DCFNet is inherited thanks to the filter regularity imposed by the truncated bases decomposition.
",1.1. Related works,[0],[0]
Network redundancy.,1.1. Related works,[0],[0]
"Various approaches have been studied to suppress redundancy in the weights of trained CNNs, including model compression and sparse connections.",1.1. Related works,[0],[0]
"In model compression, network pruning has been studied in (Han et al., 2015) and combined with quantization and Huffman encoding in (Han et al., 2016).",1.1. Related works,[0],[0]
"(Chen et al., 2015) used hash functions to reduce model size without sacrificing generalization performance.",1.1. Related works,[0],[0]
"Low-rank compression of filters in CNN has been studied in (Denton et al., 2014; Ioannou et al., 2015).",1.1. Related works,[0],[0]
"(Iandola et al., 2016; Lin et al., 2014) explored model compression with specific CNN architectures, e.g., replacing regular filters with 1× 1 filters.",1.1. Related works,[0],[0]
"Sparse connections in CNNs have been recently studied in (Ioannou et al., 2016; Anwar et al., 2017; Changpinyo et al., 2017).",1.1. Related works,[0],[0]
"On the theoretical side, (Bölcskei et al., 2017) showed that a sparsely-connected network can achieve certain asymptotic statistical optimality.",1.1. Related works,[0],[0]
The proposed DCFNet relates model redundancy compression to the regularity conditions imposed on the filters.,1.1. Related works,[0],[0]
"In DCF-FB network, redundancy reduction is achieved by suppressing network response to the high-frequency components in the inputs.",1.1. Related works,[0],[0]
"The output at the l-th layer of a convolutional neural network (CNN) can be written as {x(l)(u, λ)}u∈R2,λ∈[Ml], where Ml is the number of channels in that layer and",2.1. Notations of CNN,[0],[0]
"[M ] = {1, · · · ,M} for any integer M .",2.1. Notations of CNN,[0],[0]
"A CNN with L layers can be written as a mapping from {x(0)(u, λ)}u∈R2,λ∈[M0] to {x(L)(u, λ)}u∈R2,λ∈[ML], recursively defined via x(l)(u, λ) = σ(x(l)1
2 (u, λ) + b(l)(λ)), σ being the nonlinear mapping, e.g., ReLU, and
x (l) 1 2 (u, λ) = Ml−1∑ λ′=1 ∫ W (l) λ′,λ(v",2.1. Notations of CNN,[0],[0]
"′)x(l−1)(u+ v′, λ′)dv′. (1)
",2.1. Notations of CNN,[0],[0]
"The filtersW (l)λ′,λ(u) and the biases b (l) are the parameters of the CNN.",2.1. Notations of CNN,[0],[0]
"In practice, both x(l)(u, λ) and W (l)λ′,λ(u) are discretized on a Cartesian grid, and the continuous convolution
in (1) is approximated by its discrete analogue.",2.1. Notations of CNN,[0],[0]
Throughout the paper we use the continuous spatial variable u for simplicity.,2.1. Notations of CNN,[0],[0]
"Very importantly, the filters W (l)λ′,λ(u) are locally supported, e.g., on 3× 3 or 5× 5 image patches.",2.1. Notations of CNN,[0],[0]
"CNNs typically represent and store filters as vectors of the size of the local patches, which is equivalent to expanding the filters under the delta bases.",2.2. Decomposition of convolutional filters,[0],[0]
Delta bases are not optimal for representing smooth functions.,2.2. Decomposition of convolutional filters,[0],[0]
"For example, regular functions have fast decaying coefficients under Fourier bases, and natural images have sparse representation under wavelet bases.",2.2. Decomposition of convolutional filters,[0],[0]
"DCF layers represent the convolutional filters as a truncated expansion under basis functions which are non-adapted through the training process, while adaption comes via the combination of such bases.",2.2. Decomposition of convolutional filters,[0],[0]
"Specifically, suppose that the convolutional filters Wλ′,λ(u) at certain layer, after a proper rescaling of the spatial variable (detailed in Section 3), are supported on the unit disk D in R2.",2.2. Decomposition of convolutional filters,[0],[0]
"Given a bases {ψk}k of the space L2(D), the filters can be represented as
Wλ′,λ(u) = K∑ k=1 (aλ′,λ)kψk(u), (2)
where K is the truncation.",2.2. Decomposition of convolutional filters,[0],[0]
"The decomposition (2) is illustrated in Figure 1, and conceptually, it can be viewed as a two-step scheme of a convolutional layer:
1.",2.2. Decomposition of convolutional filters,[0],[0]
"(Ψ-step) the input is convolved with each of the basis ψk, k = 1, · · · ,K, which are pre-fixed.",2.2. Decomposition of convolutional filters,[0],[0]
"The convolution for each input channel is independent from other channels, adding computational efficiency.
",2.2. Decomposition of convolutional filters,[0],[0]
2.,2.2. Decomposition of convolutional filters,[0],[0]
"(a-step) the intermediate output is linearly transformed by an effectively fully-connected weight matrix (aλ′,λ)k mapping from index (λ′, k) to λ, which is adapted to data.
",2.2. Decomposition of convolutional filters,[0],[0]
"In (2), ψk can be any bases, and we numerically test on different choices in Section 4, including data-adapted bases and random bases.",2.2. Decomposition of convolutional filters,[0],[0]
"All experiments consistently show that the convolutional layers can be drastically decomposed and compressed with almost no reduction on the classification accuracy, and sometimes even using random bases gives strong performance.",2.2. Decomposition of convolutional filters,[0],[0]
"In particular, motivated by classical results of harmonic analysis, we use FB bases in DCFNet, with which the regularity of the filters Wλ′,λ can be imposed though constraining the magnitude the coefficients {(aλ′,λ)k}k (Proposition 3.6).",2.2. Decomposition of convolutional filters,[0],[0]
"As an example, Gabor filters approximated using the leading FB bases are plotted in the right of Figure 2.",2.2. Decomposition of convolutional filters,[0],[0]
"In experiments, DCFNet with FB bases shows superior performance in image classification and denoising tasks compared to original CNN and other bases being tested (Section 4).",2.2. Decomposition of convolutional filters,[0],[0]
"Theoretically, Section 3 analyzes the representation stability of DCFNet with respect to input variations, which provides a theoretical explanation of the advantage of FB bases.",2.2. Decomposition of convolutional filters,[0],[0]
Suppose that the original convolutional layer is of size L× L×M ′,2.3. Parameter and computation reduction,[0],[0]
"×M , as shown in Figure 1, where typically L = 3, 5 and usually less than 11, M ′ and M grow from 3 (number of input channels) to a few hundreds in the deep layers in CNN.",2.3. Parameter and computation reduction,[0],[0]
"After switching to the DCFNet as in (2), there are M ′×M×K tunable parameters (aλ′,λ)k.",2.3. Parameter and computation reduction,[0],[0]
"Thus the number of parameters in that layer is a factor KL2 smaller, which can be significant if K is allowed to be small, particularly when M ′",2.3. Parameter and computation reduction,[0],[0]
"and M are large.
",2.3. Parameter and computation reduction,[0],[0]
The theoretical computational complexity can be calculated directly.,2.3. Parameter and computation reduction,[0],[0]
"Suppose that the input and output activation is W × W in spatial size, the original convolutional layer needs M ′W 2 ·M(1 + 2L2) flops (the number of convolution operations is M ′M , each take 2L2W 2 flops, and the summation over channels take an extra W 2M ′M ).",2.3. Parameter and computation reduction,[0],[0]
"In contract, a DCF layer takesM ′W 2 ·2K(L2 +M) flops, (M ′K
many convolutions in the Ψ step, and 2KM ′MW 2 flops in the a step).",2.3. Parameter and computation reduction,[0],[0]
"Thus when M L2, the leading computation cost is KL2 of that of a regular CNN layer.
",2.3. Parameter and computation reduction,[0],[0]
"The reduction rate of KL2 in both model complexity and theoretical computational flops is confirmed on actual networks used in experiments, c.f. Table 3.",2.3. Parameter and computation reduction,[0],[0]
The analysis in this section is firstly done for regular CNN and then the conditions on filters are reduced to generic conditions on learnt coefficients in a DCF Net.,3. Analysis of Representation Stability,[0],[0]
"In the latter, the proof is for the Fourier-Bessel (FB) bases, and can be extended to other bases using similar techniques.",3. Analysis of Representation Stability,[0],[0]
"We consider the spatial deformation operator denoted by Dτ , where τ : R2 → R2 and is C2, ρ(u) = u− τ(u), and
Dτx(u, λ) = x(ρ(u), λ), ∀u, λ.
",3.1. Stable representation by CNN,[0],[0]
"We assume that the distortion is controlled, and specifically,
(A0) |∇τ |∞ = supu ‖∇τ(u)‖ < 15 , ‖ · ‖ being the operator norm.
",3.1. Stable representation by CNN,[0],[0]
The choice of the constant 15 is purely technical.,3.1. Stable representation by CNN,[0],[0]
"Thus ρ−1 exists, at least locally.",3.1. Stable representation by CNN,[0],[0]
"Our goal is to control ‖x(L)[Dτx(0)]− x(L)[x(0)]‖, namely when the input undergoes a deformation the output at L-the layer is not severely changed.",3.1. Stable representation by CNN,[0],[0]
We achieve this in two steps: (1) We show that ‖Dτx(L)[x(0)],3.1. Stable representation by CNN,[0],[0]
"− x(L)[Dτx(0)]‖ is bounded by the magnitude of deformation up to a constant proportional to the norm of the signal, c.f. Proposition 3.3.",3.1. Stable representation by CNN,[0],[0]
"(2) We show that x(L) is stable under Dτ when L is large,",3.1. Stable representation by CNN,[0],[0]
c.f. Proposition 3.4.,3.1. Stable representation by CNN,[0],[0]
"To proceed, define the L2 norm of x(u, λ) to be
‖x‖2 = 1 M ∑ λ∈[M ] 1 |Ω| ∫ R2 |x(u, λ)|2du, (3)
where |Ω|2 = (2 · 2J)2 is the area of the image-support domain,",3.1. Stable representation by CNN,[0],[0]
c.f. Figure 2.,3.1. Stable representation by CNN,[0],[0]
"We assume that
(A1) σ",3.1. Stable representation by CNN,[0],[0]
": R→ R is non-expansive,
which holds for ReLU.",3.1. Stable representation by CNN,[0],[0]
"We also define the constants
Bl := max{sup λ Ml−1∑ λ′=1 ‖W (l)λ′,λ‖1, sup λ′",3.1. Stable representation by CNN,[0],[0]
"Ml−1 Ml Ml∑ λ=1 ‖W (l)λ′,λ‖1},
Cl := max{sup λ Ml−1∑ λ′=1 ‖|v||∇W",3.1. Stable representation by CNN,[0],[0]
"(l)λ′,λ(v)|‖1,
sup λ′",3.1. Stable representation by CNN,[0],[0]
"Ml−1 Ml Ml∑ λ=1 ‖|v||∇W (l)λ′,λ(v)|‖1}, (4)
where ‖|v||∇W",3.1. Stable representation by CNN,[0],[0]
(v)|‖1 denotes ∫ R2 |v||∇W,3.1. Stable representation by CNN,[0],[0]
"(v)|dv.
Firstly, the following proposition shows that the layer-wise mapping is non-expansive whenever Bl ≤ 1, the proof of which is left to Supplementary Material (S.M.).
",3.1. Stable representation by CNN,[0],[0]
Proposition 3.1.,3.1. Stable representation by CNN,[0],[0]
"In a CNN, under (A1), if Bl ≤ 1 for all l,
(a) The mapping of the l-th convolutional layer (including σ), denoted as x(l)[x(l−1)], is non-expansive, i.e., ‖x(l)[x1]",3.1. Stable representation by CNN,[0],[0]
− x(l)[x2]‖ ≤ ‖x1,3.1. Stable representation by CNN,[0],[0]
"− x2‖ for arbitrary x1 and x2.
(b) ‖x(l)c ‖ ≤ ‖x(l−1)c ‖ for all l, where x(l)c (u, λ) = x(l)(u, λ)−x(l)0 (λ) is the centered version of x(l), x (l) 0 being the output at the l-th layer from a zero input at the bottom layer.",3.1. Stable representation by CNN,[0],[0]
"As a result, ‖x(l)c ‖ ≤ ‖x(0)c ‖ = ‖x(0)‖.
To switch the operator Dτ with the L-layer mapping x(L)[x(0)], the idea is to control the residual of the switching at each layer, which is the following lemma proved in S.M..
Lemma 3.2.",3.1. Stable representation by CNN,[0],[0]
"In a CNN, under (A0) (A1), Bl, Cl as in (4),
‖Dτx(l)[x(l−1)]− x(l)[Dτx(l−1)]‖ ≤ 4(Bl + Cl) · |∇τ |∞‖x(l−1)c ‖,
where x(l)c is as in Proposition 3.1.
",3.1. Stable representation by CNN,[0],[0]
"We thus impose the assumption on the filters to be
(A2) For all l, Bl and Cl as in (4) are less than 1.
",3.1. Stable representation by CNN,[0],[0]
"The assumption (A2) corresponds to a proper scaling of the convolutional filters so that the mapping in each convolutional layer is non-expansive (Proposition 3.1), and in practice, this can be qualitatively maintained by the standard normalization layers in CNN.
",3.1. Stable representation by CNN,[0],[0]
"Now we can bound the residual of a L-layer switching to be additive as L increases:
Proposition 3.3.",3.1. Stable representation by CNN,[0],[0]
"In a CNN, under (A0), (A1), (A2),
‖Dτx(L)[x(0)]− x(L)[Dτx(0)]‖ ≤",3.1. Stable representation by CNN,[0],[0]
8L|∇τ,3.1. Stable representation by CNN,[0],[0]
"|∞‖x(0)‖. (5)
",3.1. Stable representation by CNN,[0],[0]
"Proof is left to S.M. We remark that it is possible to derive a more technical bound in terms of the constants Bl, Cl without assuming (A2), using the same technique.",3.1. Stable representation by CNN,[0],[0]
"We present the simplified result here.
",3.1. Stable representation by CNN,[0],[0]
"In the later analysis of DCF Net, (A2) will be implied by a single condition on the bases expansion coefficients, c.f. (A2’).
",3.1. Stable representation by CNN,[0],[0]
To be able to control ‖Dτx(L),3.1. Stable representation by CNN,[0],[0]
"− x(L)‖, we have the following proposition, proved in S.M.
Proposition 3.4.",3.1. Stable representation by CNN,[0],[0]
"In a CNN, under (A1),
‖Dτx(l) − x(l)‖ ≤ 2|τ |∞Dl‖x(l−1)c",3.1. Stable representation by CNN,[0],[0]
"‖,
where x(l)c is as in Proposition 3.1, and Dl := max{supλ ∑Ml−1 λ′=1 ‖∇W (l) λ′,λ‖1, supλ′ Ml−1",3.1. Stable representation by CNN,[0],[0]
"Ml ∑Ml λ=1 ‖∇W (l) λ′,λ‖1}.
",3.1. Stable representation by CNN,[0],[0]
"One may notice that |τ |∞ is not proportional to |∇τ |∞ when the deformation happens on a large domain, e.g., a rotation.",3.1. Stable representation by CNN,[0],[0]
"It turns out that the multi-scale architecture of CNN induces a decrease of the quantity Dl proportional to the inverse of the domain diameter, which compensate the increase of |τ |∞ as scale grows, as long as the rescaled filters are properly bounded in integral.",3.1. Stable representation by CNN,[0],[0]
"Thus a unified deformation theory can be derived for DCFNets, see next section.",3.1. Stable representation by CNN,[0],[0]
"Due to the downsampling (“pooling”) in CNN, the support of the l-th layer filters W (l)λ′,λ enlarges as l increases.",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
Suppose that the input is supported on Ω which is a (2 · 2J),3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"× (2 · 2J) domain, and the CNN has L layers.",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"In accordance with the 2× 2 pooling, we assume that W (l)λ′,λ is supported on D(jl), vanishing on the boundary, where D(j) is a disk of radius 2j , j0 ≤ · · · ≤ jL ≤",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"J , and D(j0) is of size of patches at the smallest scale.",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"Let {ψk}k be a set of bases supported on the unit disk D(0), and we introduce the rescaled bases
ψj,k(u)",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
= 2,3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"−2jψk(2 −ju), u ∈ D(j),
where the normalization 2−2j is introduced so that ‖ψj,k‖1 = ‖ψk‖1, where ‖f‖1 := ∫ R2 |f(u)|du.",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
The multiscale filters and bases are illustrated in the left of Figure 2.,3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"By (2), we have that
W (l) λ′,λ(u)",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"= ∑ k (a (l) λ′,λ)kψjl,k(u), u ∈ D(jl).",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"(6)
While DCFNet is compatible with general choices of bases, we focus on the FB bases in this section as an example.",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"FB bases ψk are indexed by k = (m, q) where m and q are the angular and radial frequencies respectively.",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"They are supported on the unit disk D = D(0), and in polar coordinates,
ψm,q(r, θ) = cm,qJm(Rm,qr)e imθ, r ∈",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"[0, 1], θ ∈",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"[0, 2π],
where Jm is the Bessel function of the first kind, m are integers, q = 1, 2, · · · , Rm,q is the q-th root of Jm, and cm,q is the normalizing constant s.t. 〈ψm,q, ψm′,q′〉 =∫ D ψm,q(u)ψ ∗",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"m′,q′(u)du = πδm,m′δq,q′ .",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"Furthermore, FB bases are eigenfunctions of the Dirichlet Laplacian on D, i.e., −4ψk = µkψk, where µm,q = R2m,q.",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
The eigenvalue µk grows as k increases (Weyl’s law).,3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"Thus FB bases can be ordered by k so that µk increases, of which the leading few are shown in Table 1 and illustrated in Fig. 2.",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"In principle, the frequency q and m should be truncated according to the Nyquist sampling rate.",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"This truncation turned out to be
k 1 2,3 4,5 6 7,8 9,10 11,12 13,14 m 0 1 2 0 3 1 4 2 q 1 1 1 2 1 2 1 2 µk 5.78 14.68 26.37 30.47 40.71 49.22 57.58 70.85
Table 1.",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"The angular frequency m, radial frequency q and Dirichlet eigenvalue µk of the first 14 Fourier-Bessel bases.",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"Two k corresponds to one pair of (m, q) when m 6= 0",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"due to that both real and complex parts of the bases are used as real-valued bases.
",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"not often used in our setting, due to the significant bases truncation in DCFNet.
",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"The key technical quantities in the stability analysis of CNN are ‖W (l)λ′,λ‖1 and ‖|v||∇W",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"(l) λ′,λ(v)|‖1, and with FB bases, these integrals are bounded by a µk-weighted L2-norm of a (l) λ′,λ defined as ‖a‖FB = ( ∑ k µka 2 k)
1/2 for all l. The following lemma and proposition are proved in S.M. Lemma 3.5.",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"Suppose that {ψk} are FB bases, the function F (u) = ∑ k akψk(u) is smooth on the unit disk.",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"Then 1√ π ‖∇F‖2 = ‖a‖FB , where µk are the eigenvalues of ψk as eigenfunctions of the negative Dirichlet laplacian on the unit disk.",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"As a result, ‖∇F‖1 ≤ π‖a‖FB .",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
Proposition 3.6.,3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"Using FB bases, ‖|v||∇W (l)λ′,λ(v)|‖1 and ‖W (l)λ′,λ‖1 are bounded by π‖a (l) λ′,λ‖FB for all λ′, λ and l.
Notice that the boundedness of ‖a‖FB implies a decay of |ak| at least as fast as µ−1/2k .",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"This justifies the truncation of the FB expansion to the leading few bases, which correspond to the low-frequency modes.
",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"Proposition 3.6 implies that Bl and Cl are all bounded by Al defined as
Al := πmax{sup λ Ml−1∑ λ′=1 ‖a(l)λ′,λ‖FB ,
sup λ′",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"Ml−1 Ml Ml∑ λ=1 ‖a(l)λ′,λ‖FB}.
",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"Then we introduce
(A2’)",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"For all l, Al ≤ 1,
and the result of Proposition 3.3 extends to DCFNet: Theorem 3.7.",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"In a DCFNet with FB bases, under (A0),(A1), (A2’), then
‖Dτx(L)[x(0)]− x(L)[Dτx(0)]‖ ≤ 8L|∇τ |∞‖x(0)‖.
Combined with Proposition 3.4, we have the following deformation stability bound, proved in S.M.: Theorem 3.8.",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"In a DCFNet with FB bases, under (A0),(A1), (A2’),
‖x(L)[x(0)]− x(L)[Dτx(0)]‖ ≤ (8L|∇τ |∞ + 2 · 2−jL |τ |∞)‖x(0)‖. (7)",3.2. Multi-scale filters and Fourier Bessel (FB) bases,[0],[0]
"In this section, we experimentally demonstrate that convolutional filters in CNN can be decomposed as a truncated expansion with pre-fixed bases, where the expansion coefficients remain learned from data.",4. Experiments,[0],[0]
"Though the number of trainable parameters are significantly reduced, the accuracy in tasks such as image classification and face verification is still maintained.",4. Experiments,[0],[0]
"Such empirical observations hold for data-independent Fourier-Bessel (FB) and random bases, and data-dependent PCA bases.",4. Experiments,[0],[0]
"We perform an experimental evaluation on DCFNets using the following public datasets:
MNIST.",4.1. Datasets,[0],[0]
"28 × 28 grayscale images of digits from 0 to 9, with 60,000 training and 10,000 testing samples.
SVHN.",4.1. Datasets,[0],[0]
"The Street View House Numbers (SVHN) dataset (Netzer et al., 2011) contains 32× 32 colored images of digits 0 to 9, with 73,257 training and 26,032 testing samples.",4.1. Datasets,[0],[0]
"The additional training images were not used.
CIFAR10.",4.1. Datasets,[0],[0]
"The dataset (Krizhevsky, 2009) contains 32×32 colored images from 10 object classes, with 50,000 training
and 10,000 testing samples.
",4.1. Datasets,[0],[0]
VGG-Face.,4.1. Datasets,[0],[0]
"A large-scale face dataset, which contains about 2.6M face images from over 2.6K people (Parkhi et al., 2015).",4.1. Datasets,[0],[0]
1,4.1. Datasets,[0],[0]
"In our object classification experiments, we evaluate the DCFNet with three types of predefined bases: FourierBessel bases (DCF-FB), random bases which are generated by Gaussian vectors (DCF-RB), and PCA bases which are principal components of the convolutional filters in a pre-trained corresponding CNN model (DCF-PCA).
",4.2. Object classification,[0],[0]
"Three CNN network architectures are used for classification, Conv-2 and Conv-3 shown in Table 2, and VGG-16 (Simonyan & Zisserman, 2014).",4.2. Object classification,[0],[0]
"To generate the corresponding DCFNet structure from CNN, each CNN conv layer is expended over a set of pre-defined bases, and the obtained trainable expansion coefficients are implemented as a 1× 1 conv layer.",4.2. Object classification,[0],[0]
"For example, a 5× 5×M ′ ×M conv layer is expended over K 5 × 5 bases for trainable coefficients in a 1 × 1 ×M ′K ×M convolutional layer.",4.2. Object classification,[0],[0]
"K denotes the number of basis used, and we evaluate multiple K for different levels of parameter reduction.",4.2. Object classification,[0],[0]
"In order to be compatible with existing deep learning frameworks, pre-fixed bases are currently implemented as regular convolutional layers with zero learning rate.",4.2. Object classification,[0],[0]
"The additional memory cost incurred in such convenient implementation can be eliminated with a more careful implementation, as bases are pre-fixed and the addition across channels can be computed on the fly.
",4.2. Object classification,[0],[0]
The classification accuracy using DCFNets on various datasets are shown in Table 3.,4.2. Object classification,[0],[0]
"We observe that, by using only 3 Fourier-Bessel (FB) bases, we already obtain comparable accuracy as the original full CNN models on all
1The software is publicly available at https://github.",4.2. Object classification,[0],[0]
"com/xycheng/DCFNet.
",4.2. Object classification,[0],[0]
"datasets, while using 12% parameters for 5×5 filters.",4.2. Object classification,[0],[0]
"When more FB bases are used, DCFNets outperform corresponding CNN models, still with significantly less parameters.",4.2. Object classification,[0],[0]
"As FB bases correspond to the low-frequency components in the inputs, DCF-FB network responds less to the highfrequency nuance details, which are often irrelevant for classification tasks.",4.2. Object classification,[0],[0]
The superiority of DCF-FB network is further shown with less training data.,4.2. Object classification,[0],[0]
"For SVHN with 500 training samples, the testing accuracy (on a 50,000 testing set) of regular CNN and DCF-FB are 63.88% and 66.79% respectively.",4.2. Object classification,[0],[0]
"With 1000 training samples, the test accuracy are 73.53% v.s. 75.45%.",4.2. Object classification,[0],[0]
"Surprisingly, we observe that DCF with random bases also report acceptable performance.
",4.2. Object classification,[0],[0]
Both the FB and random bases are data independent.,4.2. Object classification,[0],[0]
"For comparison purposes, we also evaluate DCFNets with data dependent PCA bases, which are principal components of corresponding convolutional filters in pre-trained CNN models.",4.2. Object classification,[0],[0]
"When the CNN model is pre-trained with all training data, PCA bases (pca-f) shows comparable performance as FB bases.",4.2. Object classification,[0],[0]
"However, the quality of the PCA bases (pca-s) degenerates, when only a randomly selected subset of the training set is used for the pre-training.",4.2. Object classification,[0],[0]
"To gain intuitions behind the superior classification performance of DCFNet, we conduct a set of “toy” image denoising experiments on the SVHN image dataset.",4.3. Image denoising,[0],[0]
"We take the first three 5 × 5 convolution blocks from the Conv-3 CNN network in Table 2, which is used in our SVHN object
classification experiments.",4.3. Image denoising,[0],[0]
"We remove all pooling layers, and append at the end an FC-256 followed with a Euclidean loss layer.",4.3. Image denoising,[0],[0]
"We then decompose each 5 × 5 conv layer in this CNN network over 3 random bases and 3 FB bases respectively, to produce DCF-RB and DCF-FB networks.
",4.3. Image denoising,[0],[0]
We use SVHN training images with their gray-scale version as labels to train all three networks to simply reconstruct an input image (in gray-scale).,4.3. Image denoising,[0],[0]
Figure 4 shows how three trained networks behave while reconstructing examples from the SVHN testing images.,4.3. Image denoising,[0],[0]
"Without noise added to input images, Figure 4a, all three networks report decent reconstruction, while DCF-RB shows inferior to both CNN and DCF-FB.",4.3. Image denoising,[0],[0]
"PSNR values indicate CNN often produces more precise reconstructions; however, those missing high-frequency components in DCF-FB reconstructions are mostly nuance details.",4.3. Image denoising,[0],[0]
"With noise added as in figures 4b and 4c, DCF-FB produces significantly superior reconstruction over both CNN and DCF-RB, with about one tenth of the parameter number of CNN.
",4.3. Image denoising,[0],[0]
"The above empirical observations clearly indicate that Fourier-Bessel bases, which correspond to the lowfrequency components in the inputs, enable DCF to ignore the high-frequency nuance details, which are often less stable under input variations, and mostly irrelevant for tasks such as classification.",4.3. Image denoising,[0],[0]
"Such empirical observation provides good intuitions behind the superior classification performance of DCF, and is also consistent with the theoretical analysis on representation stability in Section 3.",4.3. Image denoising,[0],[0]
"We present a further evaluation of DCFNet on face verification tasks using “very deep” network architectures, which comprise a long sequence of convolutional layers.",4.4. Face verification,[0],[0]
"In order to train such complex networks, we adopt a very large scale VGG-face (Parkhi et al., 2015) dataset, which contains about 2.6M face images from over 2.6K people.
",4.4. Face verification,[0],[0]
"As shown in Table 4, we adopt the VGG-Very-Deep-16 CNN architecture as detailed in (Parkhi et al., 2015) by modifying layer 32 and 35 to change output features from 4,096 dimension to 512.",4.4. Face verification,[0],[0]
"Such CNN network comprises 16 weight layers, and all except the last Fully-Connected (FC) layer utilize 3× 3 or 5× 5 filters.
",4.4. Face verification,[0],[0]
The input to both CNN and DCFNet are face images of size 224 × 224 (with the average face image subtracted).,4.4. Face verification,[0],[0]
"As shown in Table 5, with FB bases, even only using 13 parameters at weight layers (K = 3 for 3 × 3, K = 8 for 5× 5), the DCFNet shows similar verification accuracy as the CNN structure on the challenging LFW benchmark.",4.4. Face verification,[0],[0]
"Note that our CNN model outperforms the VGG-face model in (Parkhi et al., 2015), and such improvement is mostly due to the smaller output dimension we adopted, as both models share similar architecture and are trained on the same face dataset.",4.4. Face verification,[0],[0]
The paper studies CNNs where the convolutional filters are represented as a truncated expansion under pre-fixed bases and the expansion coefficients are learned from labeled data.,5. Conclusion and Discussion,[0],[0]
"Experimentally, we observe that on various object recognition datasets the classification accuracy are maintained with a significant reduction of the number of parameters, and the performance of Fourier-Bessel (FB) bases is constantly superior.",5. Conclusion and Discussion,[0],[0]
The truncated FB expansion in DCFNet can be viewed as a regularization of the filters.,5. Conclusion and Discussion,[0],[0]
"In other words, DCF-FB is less susceptible to the high-frequency components in the input, which are least stable under expected input variations and often do not affect recognition when suppressed.",5. Conclusion and Discussion,[0],[0]
"This interpretation is supported by image denoising experiments, where DCF-FB performs preferably over the original CNN and other basis options on noisy inputs.",5. Conclusion and Discussion,[0],[0]
"The stability of DCFNet representation is also proved theoretically, showing that the perturbation of the deep features with respect to input variations can be bounded under generic conditions on the decomposed filters.
",5. Conclusion and Discussion,[0],[0]
"To extend the work, firstly, DCF layers can be incorporated in networks for unsupervised learning, for which the denoising experiment serves as a first step.",5. Conclusion and Discussion,[0],[0]
The stability analysis can be extended by testing the resilience to adversarial noise.,5. Conclusion and Discussion,[0],[0]
"Finally, more structures may be imposed across the channels, concurrently with the structures of the filters in space.",5. Conclusion and Discussion,[0],[0]
Filters in a Convolutional Neural Network (CNN) contain model parameters learned from enormous amounts of data.,abstractText,[0],[0]
"In this paper, we suggest to decompose convolutional filters in CNN as a truncated expansion with pre-fixed bases, namely the Decomposed Convolutional Filters network (DCFNet), where the expansion coefficients remain learned from data.",abstractText,[0],[0]
"Such a structure not only reduces the number of trainable parameters and computation, but also imposes filter regularity by bases truncation.",abstractText,[0],[0]
"Through extensive experiments, we consistently observe that DCFNet maintains accuracy for image classification tasks with a significant reduction of model parameters, particularly with Fourier-Bessel (FB) bases, and even with random bases.",abstractText,[0],[0]
"Theoretically, we analyze the representation stability of DCFNet with respect to input variations, and prove representation stability under generic assumptions on the expansion coefficients.",abstractText,[0],[0]
The analysis is consistent with the empirical observations.,abstractText,[0],[0]
DCFNet: Deep Neural Network with Decomposed Convolutional Filters,title,[0],[0]
"ar X
iv :1
70 6.
05 12
5v 1
[ cs
.A I]
1 6
Ju n
20 17
cooperative settings, where agents with different goals attempt to agree on common decisions. Negotiations require complex communication and reasoning skills, but success is easy to measure, making this an interesting task for AI. We gather a large dataset of human-human negotiations on a multi-issue bargaining task, where agents who cannot observe each other’s reward functions must reach an agreement (or a deal) via natural language dialogue. For the first time, we show it is possible to train end-to-end models for negotiation, which must learn both linguistic and reasoning skills with no annotated dialogue states. We also introduce dialogue rollouts, in which the model plans ahead by simulating possible complete continuations of the conversation, and find that this technique dramatically improves performance. Our code and dataset are publicly available.1",text,[0],[0]
"Intelligent agents often need to cooperate with others who have different goals, and typically use natural language to agree on decisions.",1 Introduction,[0],[0]
"Negotiation is simultaneously a linguistic and a reasoning problem, in which an intent must be formulated and then verbally realised.",1 Introduction,[0],[0]
"Such dialogues contain both cooperative and adversarial elements, and require agents to understand, plan, and generate utterances to achieve their goals (Traum et al., 2008; Asher et al., 2012).
",1 Introduction,[0],[0]
"We collect the first large dataset of natural language negotiations between two people, and show
1 https://github.com/facebookresearch/end-to-end-negotiator
that end-to-end neural models can be trained to negotiate by maximizing the likelihood of human actions.",1 Introduction,[0],[0]
"This approach is scalable and domainindependent, but does not model the strategic skills required for negotiating well.",1 Introduction,[0],[0]
"We further show that models can be improved by training and decoding to maximize reward instead of likelihood—by training with self-play reinforcement learning, and using rollouts to estimate the expected reward of utterances during decoding.
",1 Introduction,[0],[0]
"To study semi-cooperative dialogue, we gather a dataset of 5808 dialogues between humans on a negotiation task.",1 Introduction,[0],[0]
"Users were shown a set of items with a value for each, and asked to agree how to divide the items with another user who has a different, unseen, value function (Figure 1).
",1 Introduction,[0],[0]
We first train recurrent neural networks to imitate human actions.,1 Introduction,[0],[0]
"We find that models trained to maximise the likelihood of human utterances can generate fluent language, but make comparatively poor negotiators, which are overly willing to compromise.",1 Introduction,[0],[0]
"We therefore explore two methods for improving the model’s strategic reasoning skills— both of which attempt to optimise for the agent’s goals, rather than simply imitating humans:
Firstly, instead of training to optimise likelihood, we show that our agents can be considerably improved using self play, in which pre-trained models practice negotiating with each other in order to optimise performance.",1 Introduction,[0],[0]
"To avoid the models diverging from human language, we interleave reinforcement learning updates with supervised updates.",1 Introduction,[0],[0]
"For the first time, we show that end-toend dialogue agents trained using reinforcement learning outperform their supervised counterparts in negotiations with humans.
",1 Introduction,[0],[0]
"Secondly, we introduce a new form of planning for dialogue called dialogue rollouts, in which an agent simulates complete dialogues during decoding to estimate the eward of utterances.",1 Introduction,[0],[0]
"We show
that decoding to maximise the reward function (rather than likelihood) significantly improves performance against both humans and machines.
",1 Introduction,[0],[0]
"Analysing the performance of our agents, we find evidence of sophisticated negotiation strategies.",1 Introduction,[0],[0]
"For example, we find instances of the model feigning interest in a valueless issue, so that it can later ‘compromise’ by conceding it.",1 Introduction,[0],[0]
"Deceit is a complex skill that requires hypothesising the other agent’s beliefs, and is learnt relatively late in child development (Talwar and Lee, 2002).",1 Introduction,[0],[0]
"Our agents have learnt to deceive without any explicit human design, simply by trying to achieve their goals.
",1 Introduction,[0],[0]
The rest of the paper proceeds as follows: §2 describes the collection of a large dataset of humanhuman negotiation dialogues.,1 Introduction,[0],[0]
"§3 describes a baseline supervised model, which we then show can be improved by goal-based training (§4) and decoding (§5).",1 Introduction,[0],[0]
"§6 measures the performance of our models and humans on this task, and §7 gives a detailed analysis and suggests future directions.",1 Introduction,[0],[0]
"To enable end-to-end training of negotiation agents, we first develop a novel negotiation task and curate a dataset of human-human dialogues for this task.",2.1 Overview,[0],[0]
This task and dataset follow our proposed general framework for studying semicooperative dialogue.,2.1 Overview,[0],[0]
"Initially, each agent is shown an input specifying a space of possible actions and a reward function which will score the outcome of the negotiation.",2.1 Overview,[0],[0]
"Agents then sequentially take turns of either sending natural language messages, or selecting that a final decision has been reached.",2.1 Overview,[0],[0]
"When one agent selects that an
agreement has been made, both agents independently output what they think the agreed decision was.",2.1 Overview,[0],[0]
"If conflicting decisions are made, both agents are given zero reward.",2.1 Overview,[0],[0]
"Our task is an instance of multi issue bargaining (Fershtman, 1990), and is based on DeVault et al. (2015).",2.2 Task,[0],[0]
"Two agents are both shown the same collection of items, and instructed to divide them so that each item assigned to one agent.
",2.2 Task,[0],[0]
"Each agent is given a different randomly generated value function, which gives a non-negative value for each item.",2.2 Task,[0],[0]
The value functions are constrained so that: (1) the total value for a user of all items is 10; (2) each item has non-zero value to at least one user; and (3) some items have nonzero value to both users.,2.2 Task,[0],[0]
"These constraints enforce that it is not possible for both agents to receive a maximum score, and that no item is worthless to both agents, so the negotiation will be competitive.",2.2 Task,[0],[0]
"After 10 turns, we allow agents the option to complete the negotiation with no agreement, which is worth 0 points to both users.",2.2 Task,[0],[0]
"We use 3 item types (books, hats, balls), and between 5 and 7 total items in the pool.",2.2 Task,[0],[0]
Figure 1 shows our interface.,2.2 Task,[0],[0]
We collected a set of human-human dialogues using Amazon Mechanical Turk.,2.3 Data Collection,[0],[0]
"Workers were paid $0.15 per dialogue, with a $0.05 bonus for maximal scores.",2.3 Data Collection,[0],[0]
We only used workers based in the United States with a 95% approval rating and at least 5000 previous HITs.,2.3 Data Collection,[0],[0]
"Our data collection interface was adapted from that of Das et al. (2016).
",2.3 Data Collection,[0],[0]
"We collected a total of 5808 dialogues, based on 2236 unique scenarios (where a scenario is the
available items and values for the two users).",2.3 Data Collection,[0],[0]
We held out a test set of 252 scenarios (526 dialogues).,2.3 Data Collection,[0],[0]
Holding out test scenarios means that models must generalise to new situations.,2.3 Data Collection,[0],[0]
"We propose a simple but effective baseline model for the conversational agent, in which a sequenceto-sequence model is trained to produce the complete dialogue, conditioned on an agent’s input.",3 Likelihood Model,[0],[0]
"Each dialogue is converted into two training examples, showing the complete conversation from the perspective of each agent.",3.1 Data Representation,[0],[0]
"The examples differ on their input goals, output choice, and whether utterances were read or written.
",3.1 Data Representation,[0],[0]
"Training examples contain an input goal g, specifying the available items and their values, a dialogue x, and an output decision o specifying which items each agent will receive.",3.1 Data Representation,[0],[0]
"Specifically, we represent g as a list of six integers corresponding to the count and value of each of the three item types.",3.1 Data Representation,[0],[0]
Dialogue x is a list of tokens x0..,3.1 Data Representation,[0],[0]
"T containing the turns of each agent interleaved with symbols marking whether a turn was written by the agent or their partner, terminating in a special token indicating one agent has marked that an agree-
ment has been made.",3.1 Data Representation,[0],[0]
Output o is six integers describing how many of each of the three item types are assigned to each agent.,3.1 Data Representation,[0],[0]
See Figure 2.,3.1 Data Representation,[0],[0]
"We train a sequence-to-sequence network to generate an agent’s perspective of the dialogue conditioned on the agent’s input goals (Figure 3a).
",3.2 Supervised Learning,[0],[0]
"The model uses 4 recurrent neural networks, implemented as GRUs (Cho et al., 2014): GRUw, GRUg, GRU−→o , and GRU←−o .
",3.2 Supervised Learning,[0],[0]
The agent’s input goals g are encoded using GRUg.,3.2 Supervised Learning,[0],[0]
We refer to the final hidden state as h g .,3.2 Supervised Learning,[0],[0]
"The model then predicts each token xt from left to right, conditioned on the previous tokens and hg .",3.2 Supervised Learning,[0],[0]
"At each time step t, GRUw takes as input the previous hidden state ht−1, previous token xt−1 (embedded with a matrix E), and input encoding hg .",3.2 Supervised Learning,[0],[0]
"Conditioning on the input at each time step helps the model learn dependencies between language and goals.
",3.2 Supervised Learning,[0],[0]
"ht = GRUw(ht−1, [Ext−1, h g]) (1)
The token at each time step is predicted with a softmax, which uses weight tying with the embedding matrix E (Mao et al., 2015):
pθ(xt|x0..t−1, g) ∝ exp(E",3.2 Supervised Learning,[0],[0]
"Tht) (2)
Note that the model predicts both agent’s words, enabling its use as a forward model in Section 5.
",3.2 Supervised Learning,[0],[0]
"At the end of the dialogue, the agent outputs a set of tokens o representing the decision.",3.2 Supervised Learning,[0],[0]
"We generate each output conditionally independently, using a separate classifier for each.",3.2 Supervised Learning,[0],[0]
"The classifiers share bidirectional GRUo and attention mechanism (Bahdanau et al., 2014) over the dialogue, and additionally conditions on the input goals.
",3.2 Supervised Learning,[0],[0]
"h −→o t = GRU−→o (h −→o t−1,",3.2 Supervised Learning,[0],[0]
"[Ext, ht]) (3) h ←−o",3.2 Supervised Learning,[0],[0]
"t = GRU←−o (h ←−o t+1,",3.2 Supervised Learning,[0],[0]
"[Ext, ht]) (4)
hot =",3.2 Supervised Learning,[0],[0]
"[h ←−o t , h −→o t ] (5) hat = W",3.2 Supervised Learning,[0],[0]
[tanh(W ′hot )],3.2 Supervised Learning,[0],[0]
"(6) αt = exp(w · hat ) ∑
t′ exp(w · h a t′)
(7)
hs",3.2 Supervised Learning,[0],[0]
"= tanh(W s[hg, ∑
t
αtht]) (8)
The output tokens are predicted using softmax:
pθ(oi|x0..t, g) ∝",3.2 Supervised Learning,[0],[0]
"exp(W oihs) (9)
",3.2 Supervised Learning,[0],[0]
The model is trained to minimize the negative log likelihood of the token sequence,3.2 Supervised Learning,[0],[0]
"x0..T conditioned on the input goals g, and of the outputs o conditioned on x and g. The two terms are weighted with a hyperparameter α.
L(θ) =− ∑
x,g
∑
t
log pθ(xt|x0..t−1, g)
︸ ︷︷ ︸
Token prediction loss
− α ∑
x,g,o
∑
j
log pθ(oj |x0..T , g)
︸ ︷︷ ︸
Output choice prediction loss
(10)
Unlike the Neural Conversational Model (Vinyals and Le, 2015), our approach shares all parameters for reading and generating tokens.",3.2 Supervised Learning,[0],[0]
"During decoding, the model must generate an output token xt conditioned on dialogue history x0..t−1 and input goals g, by sampling from pθ:
xt ∼ pθ(xt|x0..t−1, g) (11)
",3.3 Decoding,[0],[0]
"If the model generates a special end-of-turn token, it then encodes a series of tokens output by the other agent, until its next turn (Figure 3b).
",3.3 Decoding,[0],[0]
The dialogue ends when either agent outputs a special end-of-dialogue token.,3.3 Decoding,[0],[0]
The model then outputs a set of choices o.,3.3 Decoding,[0],[0]
"We choose each item independently, but enforce consistency by checking the solution is in a feasible set O:
o∗ = argmax o∈O
∏
i
pθ(oi|x0..T , g) (12)
",3.3 Decoding,[0],[0]
"In our task, a solution is feasible if each item is assigned to exactly one agent.",3.3 Decoding,[0],[0]
The space of solutions is small enough to be tractably enumerated.,3.3 Decoding,[0],[0]
"Supervised learning aims to imitate the actions of human users, but does not explicitly attempt to maximise an agent’s goals.",4 Goal-based Training,[0],[0]
"Instead, we explore pre-training with supervised learning, and then fine-tuning against the evaluation metric using reinforcement learning.",4 Goal-based Training,[0],[0]
"Similar two-stage learning strategies have been used previously (e.g. Li et al. (2016); Das et al. (2017)).
",4 Goal-based Training,[0],[0]
"During reinforcement learning, an agent A attempts to improve its parameters from conversations with another agent B. While the other agent B could be a human, in our experiments we used our fixed supervised model that was trained to imitate humans.",4 Goal-based Training,[0],[0]
The second model is fixed as we found that updating the parameters of both agents led to divergence from human language.,4 Goal-based Training,[0],[0]
"In effect,
agent A learns to improve by simulating conversations with the help of a surrogate forward model.
",4 Goal-based Training,[0],[0]
Agent A reads its goals g and then generates tokens x0..n by sampling from pθ.,4 Goal-based Training,[0],[0]
"When x generates an end-of-turn marker, it then reads in tokens xn+1..m generated by agent B. These turns alternate until one agent emits a token ending the dialogue.",4 Goal-based Training,[0],[0]
Both agents then output a decision o and collect a reward from the environment (which will be 0 if they output different decisions).,4 Goal-based Training,[0],[0]
"We denote the subset of tokens generated by A as XA (e.g. tokens with incoming arrows in Figure 3b).
",4 Goal-based Training,[0],[0]
"After a complete dialogue has been generated, we update agent A’s parameters based on the outcome of the negotiation.",4 Goal-based Training,[0],[0]
"Let rA be the score agent A achieved in the completed dialogue, T be the length of the dialogue, γ be a discount factor that rewards actions at the end of the dialogue more strongly, and µ be a running average of completed dialogue rewards so far2.",4 Goal-based Training,[0],[0]
We define the future reward R for an action xt ∈,4 Goal-based Training,[0],[0]
X,4 Goal-based Training,[0],[0]
"A as follows:
R(xt) = ∑
xt∈XA
γT−t(rA(o)− µ) (13)
",4 Goal-based Training,[0],[0]
"We then optimise the expected reward of each
action xt ∈",4 Goal-based Training,[0],[0]
X,4 Goal-based Training,[0],[0]
"A:
LRLθ = Ext∼pθ(xt|x0..t−1,g)[R(xt)]",4 Goal-based Training,[0],[0]
"(14)
The gradient of LRLθ is calculated as in REINFORCE (Williams, 1992):
∇θL RL θ =
∑
xt∈XA
Ext[R(xt)∇θ log(pθ(xt|x0..t−1, g))",4 Goal-based Training,[0],[0]
"]
(15)
2As all rewards are non-negative, we instead re-scale them by subtracting the mean reward found during self play.",4 Goal-based Training,[0],[0]
"Shifting in this way can reduce the variance of our estimator.
",4 Goal-based Training,[0],[0]
"Algorithm 1 Dialogue Rollouts algorithm.
1: procedure ROLLOUT(x0..i, g) 2: u∗ ← ∅ 3: for c ∈ {1..C} do ⊲",4 Goal-based Training,[0],[0]
C candidate moves 4: j ← i 5: do ⊲,4 Goal-based Training,[0],[0]
"Rollout to end of turn 6: j ← j + 1 7: xj ∼ pθ(xj |x0..j−1, g) 8: while xk /∈ {read:, choose:} 9: u← xi+1..xj ⊲",4 Goal-based Training,[0],[0]
u is candidate move 10: for s ∈ {1..S} do ⊲,4 Goal-based Training,[0],[0]
S samples per move 11: k ←,4 Goal-based Training,[0],[0]
j ⊲,4 Goal-based Training,[0],[0]
Start rollout from end of u 12: while xk 6= choose: do ⊲,4 Goal-based Training,[0],[0]
"Rollout to end of dialogue 13: k ← k + 1 14: xk ∼ pθ(xk|x0..k−1, g)
⊲",4 Goal-based Training,[0],[0]
"Calculate rollout output and reward 15: o← argmaxo′∈O p(o
′|x0..k, g) 16: R(u)← R(u)",4 Goal-based Training,[0],[0]
"+ r(o)p(o′|x0..k, g) 17: if R(u) > R(u∗)",4 Goal-based Training,[0],[0]
then 18: u∗ ← u 19: return u∗ ⊲,4 Goal-based Training,[0],[0]
Return best move,4 Goal-based Training,[0],[0]
Likelihood-based decoding (§3.3) may not be optimal.,5 Goal-based Decoding,[0],[0]
"For instance, an agent may be choosing between accepting an offer, or making a counter offer.",5 Goal-based Decoding,[0],[0]
"The former will often have a higher likelihood under our model, as there are fewer ways to agree than to make another offer, but the latter may lead to a better outcome.",5 Goal-based Decoding,[0],[0]
Goal-based decoding also allows more complex dialogue strategies.,5 Goal-based Decoding,[0],[0]
"For example, a deceptive utterance is likely to have a low model score (as users were generally honest in the supervised data), but may achieve high reward.
",5 Goal-based Decoding,[0],[0]
We instead explore decoding by maximising expected reward.,5 Goal-based Decoding,[0],[0]
"We achieve this by using pθ as a
forward model for the complete dialogue, and then deterministically computing the reward.",5 Goal-based Decoding,[0],[0]
"Rewards for an utterance are averaged over samples to calculate expected future reward (Figure 4).
",5 Goal-based Decoding,[0],[0]
"We use a two stage process: First, we generate c candidate utterances U = u0..c, representing possible complete turns that the agent could make, which are generated by sampling from pθ until the end-of-turn token is reached.",5 Goal-based Decoding,[0],[0]
Let x0..n−1 be current dialogue history.,5 Goal-based Decoding,[0],[0]
"We then calculate the expected reward R(u) of candidate utterance u = xn,n+k by repeatedly sampling xn+k+1,T from pθ, then choosing the best output o using Equation 12, and finally deterministically computing the reward r(o).",5 Goal-based Decoding,[0],[0]
"The reward is scaled by the probability of the output given the dialogue, because if the agents select different outputs then they both receive 0 reward.
",5 Goal-based Decoding,[0],[0]
R(xn..n+k),5 Goal-based Decoding,[0],[0]
"= Ex(n+k+1..T ;o)∼pθ [r(o)pθ(o|x0..T )]
(16)
",5 Goal-based Decoding,[0],[0]
"We then return the utterance maximizing R.
u∗ = argmax u∈U R(u) (17)
We use 5 rollouts for each of 10 candidate turns.",5 Goal-based Decoding,[0],[0]
We implement our models using PyTorch.,6.1 Training Details,[0],[0]
All hyper-parameters were chosen on a development dataset.,6.1 Training Details,[0],[0]
"The input tokens are embedded into a 64-dimensional space, while the dialogue tokens are embedded with 256-dimensional embeddings (with no pre-training).",6.1 Training Details,[0],[0]
The input GRUg has a hidden layer of size 64 and the dialogue GRUw is of size 128.,6.1 Training Details,[0],[0]
"The output GRU−→o and GRU←−o both have a hidden state of size 256, the size of hs is 256 as well.",6.1 Training Details,[0],[0]
"During supervised training, we optimise using stochastic gradient descent with a minibatch size of 16, an initial learning rate of 1.0, Nesterov momentum with µ=0.1 (Nesterov, 1983), and clipping gradients whose L2 norm exceeds 0.5.",6.1 Training Details,[0],[0]
We train the model for 30 epochs and pick the snapshot of the model with the best validation perplexity.,6.1 Training Details,[0],[0]
We then annealed the learning rate by a factor of 5 each epoch.,6.1 Training Details,[0],[0]
We weight the terms in the loss function (Equation 10) using α=0.5.,6.1 Training Details,[0],[0]
We do not train against output decisions where humans selected different agreements.,6.1 Training Details,[0],[0]
"Tokens occurring fewer than 20 times are replaced with an ‘unknown’ token.
",6.1 Training Details,[0],[0]
"During reinforcement learning, we use a learning rate of 0.1, clip gradients above 1.0, and use a discount factor of γ=0.95.",6.1 Training Details,[0],[0]
"After every 4 reinforcement learning updates, we make a supervised update with mini-batch size 16 and learning rate 0.5, and we clip gradients at 1.0.",6.1 Training Details,[0],[0]
"We used 4086 simulated conversations.
",6.1 Training Details,[0],[0]
"When sampling words from pθ, we reduce the variance by doubling the values of logits (i.e. using temperature of 0.5).",6.1 Training Details,[0],[0]
"We compare the performance of the following: LIKELIHOOD uses supervised training and decoding (§3), RL is fine-tuned with goal-based selfplay (§4), ROLLOUTS uses supervised training combined with goal-based decoding using rollouts (§5), and RL+ROLLOUTS uses rollouts with a base model trained with reinforcement learning.",6.2 Comparison Systems,[0],[0]
"For development, we use measured the perplexity of user generated utterances, conditioned on the input and previous dialogue.
",6.3 Intrinsic Evaluation,[0],[0]
"Results are shown in Table 3, and show that the simple LIKELIHOOD model produces the most human-like responses, and the alternative training and decoding strategies cause a divergence from human language.",6.3 Intrinsic Evaluation,[0],[0]
"Note however, that this divergence may not necessarily correspond to lower quality language—it may also indicate different strategic decisions about what to say.",6.3 Intrinsic Evaluation,[0],[0]
Results in §6.4 show all models could converse with humans.,6.3 Intrinsic Evaluation,[0],[0]
"We measure end-to-end performance in dialogues both with the likelihood-based agent and with humans on Mechanical Turk, on held out scenarios.
Humans were told that they were interacting with other humans, as they had been during the collection of our dataset (and few appeared to realize they were in conversation with machines).
",6.4 End-to-End Evaluation,[0],[0]
"We measure the following statistics:
",6.4 End-to-End Evaluation,[0],[0]
Score:,6.4 End-to-End Evaluation,[0],[0]
"The average score for each agent (which could be a human or model), out of 10.",6.4 End-to-End Evaluation,[0],[0]
Agreement:,6.4 End-to-End Evaluation,[0],[0]
The percentage of dialogues where both agents agreed on the same decision.,6.4 End-to-End Evaluation,[0],[0]
Pareto Optimality: The percentage of Pareto optimal solutions for agreed deals (a solution is Pareto optimal if neither agent’s score can be improved without lowering the other’s score).,6.4 End-to-End Evaluation,[0],[0]
"Lower scores indicate inefficient negotiations.
",6.4 End-to-End Evaluation,[0],[0]
Results are shown in Table 1.,6.4 End-to-End Evaluation,[0],[0]
"Firstly, we see that the RL and ROLLOUTS models achieve significantly better results when negotiating with the LIKELIHOOD model, particularly the RL+ROLLOUTS model.",6.4 End-to-End Evaluation,[0],[0]
"The percentage of Pareto optimal solutions also increases, showing a better exploration of the solution space.",6.4 End-to-End Evaluation,[0],[0]
"Compared to human-human negotiations (Table 2), the best models achieve a higher agreement rate, better scores, and similar Pareto efficiency.",6.4 End-to-End Evaluation,[0],[0]
"This result confirms that attempting to maximise reward can outperform simply imitating humans.
",6.4 End-to-End Evaluation,[0],[0]
"Similar trends hold in dialogues with humans, with goal-based reasoning outperforming imitation learning.",6.4 End-to-End Evaluation,[0],[0]
"The ROLLOUTS model achieves comparable scores to its human partners, and the RL+ROLLOUTS model actually achieves higher scores.",6.4 End-to-End Evaluation,[0],[0]
"However, we also find significantly more cases of the goal-based models failing to agree a deal with humans—largely a consequence of their more aggressive negotiation tactics (see §7).",6.4 End-to-End Evaluation,[0],[0]
Table 1 shows large gains from goal-based methods.,7 Analysis,[0],[0]
"In this section, we explore the strengths and weaknesses of our models.
",7 Analysis,[0],[0]
Goal-based models negotiate harder.,7 Analysis,[0],[0]
"The RL+ROLLOUTS model has much longer dialogues with humans than LIKELIHOOD (7.2 turns vs. 5.3 on average), indicating that the model is accepting deals less quickly, and negotiating harder.
",7 Analysis,[0],[0]
"A negative consequence of this more aggressive negotiation strategy is that humans were more likely to walk away with no deal, which is reflected in the lower agreement rates.",7 Analysis,[0],[0]
"Even though failing to agree was worth 0 points, people often preferred this course over capitulating to an uncompromising opponent—a factor not well captured by the simulated partner in reinforcement learning training or rollouts (as reflected by the larger gains from goal-based models in dialogues with the LIKELIHOOD model).",7 Analysis,[0],[0]
"In particular, the goal-based models are prone to simply rephrasing the same demand each turn, which is a more effective strategy against the LIKELIHOOD model than humans.",7 Analysis,[0],[0]
"Future work should address this issue.
",7 Analysis,[0],[0]
"Figure 5 shows an example of our goal-based model stubbornly negotiating until it achieves a good outcome.
",7 Analysis,[0],[0]
Models learn to be deceptive.,7 Analysis,[0],[0]
Deception can be an effective negotiation tactic.,7 Analysis,[0],[0]
"We found numerous cases of our models initially feigning interest in a valueless item, only to later ‘compromise’ by conceding it.",7 Analysis,[0],[0]
"Figure 7 shows an example.
",7 Analysis,[0],[0]
Models produce meaningful novel sentences.,7 Analysis,[0],[0]
"One interesting question is whether our models are capable of generating novel sentences in the new circumstances they find themselves in, or if they simply repeat messages from the training data verbatim.",7 Analysis,[0],[0]
We find that 76% of messages produced by the LIKELIHOOD model in self-play were found in the training data.,7 Analysis,[0],[0]
"We manually examined the novel
utterances produced by our model, and found that the overwhelming majority were fluent English sentences in isolation—showing that the model has learnt a good language model for the domain (in addition to results that show it uses language effectively to achieve its goals).",7 Analysis,[0],[0]
"These results suggest that although neural models are prone to the safer option of repeating sentences from training data, they are capable of generalising when necessary.",7 Analysis,[0],[0]
"Future work should choose domains that force a higher degree of diversity in utterances.
",7 Analysis,[0],[0]
Maintaining multi-sentence coherence is challenging.,7 Analysis,[0],[0]
"One common linguistic error we see RL+ROLLOUTS make is to start a message by indicating agreement (e.g. I agree or Deal), but then going on to propose a counter offer—a behaviour that human partners found frustrating.",7 Analysis,[0],[0]
"One explanation is that the model has learnt that in the supervised data, messages beginning with I agree are often at the end of the dialogue, and partners rarely reply with further negotiation—so the models using rollouts and reinforcement learning believe this tactic will help their offer to be accepted.",7 Analysis,[0],[0]
"Most work on goal orientated dialogue systems has assumed that state representations are anno-
tated in the training data (Williams and Young, 2007; Henderson et al., 2014; Wen et al., 2016).",8 Related Work,[0],[0]
"The use of state annotations allows a cleaner separation of the reasoning and natural language aspects of dialogues, but our end-to-end approach makes data collection cheaper and allows tasks where it is unclear how to annotate state.",8 Related Work,[0],[0]
Bordes and Weston (2016) explore end-toend goal orientated dialogue with a supervised model—we show improvements over supervised learning with goal-based training and decoding.,8 Related Work,[0],[0]
"Recently, He et al. (2017) use task-specific rules to combine the task input and dialogue history into a more structured state representation than ours.
",8 Related Work,[0],[0]
Reinforcement learning (RL) has been applied in many dialogue settings.,8 Related Work,[0],[0]
"RL has been widely used to improve dialogue managers, which manage transitions between dialogue states (Singh et al., 2002; Pietquin et al., 2011; Rieser and Lemon, 2011; Gašic et al., 2013; Fatemi et al., 2016).",8 Related Work,[0],[0]
"In contrast, our end-toend approach has no explicit dialogue manager.",8 Related Work,[0],[0]
"Li et al. (2016) improve metrics such as diversity for non-goal-orientated dialogue using RL, which would make an interesting extension to our work.",8 Related Work,[0],[0]
Das et al. (2017) use reinforcement learning to improve cooperative bot-bot dialogues.,8 Related Work,[0],[0]
"RL has also been used to allow agents to invent new languages (Das et al., 2017; Mordatch and Abbeel, 2017).",8 Related Work,[0],[0]
"To our knowledge, our model is the first to use RL to improve the performance of an end-toend goal orientated dialogue system in dialogues with humans.
",8 Related Work,[0],[0]
"Work on learning end-to-end dialogues has concentrated on ‘chat’ settings, without explicit goals (Ritter et al., 2011; Vinyals and Le, 2015; Li et al., 2015).",8 Related Work,[0],[0]
"These dialogues contain a much greater diversity of vocabulary than our domain, but do not
have the challenging adversarial elements.",8 Related Work,[0],[0]
"Such models are notoriously hard to evaluate (Liu et al., 2016), because the huge diversity of reasonable responses, whereas our task has a clear objective.",8 Related Work,[0],[0]
"Our end-to-end approach would also be much more straightforward to integrate into a generalpurpose dialogue agent than one that relied on annotated dialogue states (Dodge et al., 2016).
",8 Related Work,[0],[0]
"There is a substantial literature on multi-agent bargaining in game-theory, e.g. Nash Jr (1950).",8 Related Work,[0],[0]
"There has also been computational work on modelling negotiations (Baarslag et al., 2013)—our work differs in that agents communicate in unrestricted natural language, rather than pre-specified symbolic actions, and our focus on improving performance relative to humans rather than other automated systems.",8 Related Work,[0],[0]
"Our task is based on that of DeVault et al. (2015), who study natural language negotiations for pedagogical purposes—their version includes speech rather than textual dialogue, and embodied agents, which would make interesting extensions to our work.",8 Related Work,[0],[0]
"The only automated natural language negotiations systems we are aware of have first mapped language to domainspecific logical forms, and then focused on choosing the next dialogue act (Rosenfeld et al., 2014; Cuayáhuitl et al., 2015; Keizer et al., 2017).",8 Related Work,[0],[0]
"Our end-to-end approach is the first to to learn comprehension, reasoning and generation skills in a domain-independent data driven way.
",8 Related Work,[0],[0]
"Our use of a combination of supervised and reinforcement learning for training, and stochastic rollouts for decoding, builds on strategies used in game playing agents such as AlphaGo (Silver et al., 2016).",8 Related Work,[0],[0]
Our work is a step towards real-world applications for these techniques.,8 Related Work,[0],[0]
"Our use of rollouts could be extended by choosing the other agent’s responses based on sampling, using Monte Carlo Tree Search (MCTS) (Kocsis and Szepesvári, 2006).",8 Related Work,[0],[0]
"However, our setting has a higher branching factor than in domains where MCTS has been successfully applied, such as Go (Silver et al., 2016)—future work should explore scaling tree search to dialogue modelling.",8 Related Work,[0],[0]
"We have introduced end-to-end learning of natural language negotiations as a task for AI, arguing that it challenges both linguistic and reasoning skills while having robust evaluation metrics.",9 Conclusion,[0],[0]
"We gathered a large dataset of human-human ne-
gotiations, which contain a variety of interesting tactics.",9 Conclusion,[0],[0]
"We have shown that it is possible to train dialogue agents end-to-end, but that their ability can be much improved by training and decoding to maximise their goals, rather than likelihood.",9 Conclusion,[0],[0]
"There remains much potential for future work, particularly in exploring other reasoning strategies, and in improving the diversity of utterances without diverging from human language.",9 Conclusion,[0],[0]
"We will also explore other negotiation tasks, to investigate whether models can learn to share negotiation strategies across domains.",9 Conclusion,[0],[0]
"We would like to thank Luke Zettlemoyer and the anonymous EMNLP reviewers for their insightful comments, and the Mechanical Turk workers who helped us collect data.",Acknowledgments,[0],[0]
"Much of human dialogue occurs in semicooperative settings, where agents with different goals attempt to agree on common decisions.",abstractText,[0],[0]
"Negotiations require complex communication and reasoning skills, but success is easy to measure, making this an interesting task for AI.",abstractText,[0],[0]
"We gather a large dataset of human-human negotiations on a multi-issue bargaining task, where agents who cannot observe each other’s reward functions must reach an agreement (or a deal) via natural language dialogue.",abstractText,[0],[0]
"For the first time, we show it is possible to train end-to-end models for negotiation, which must learn both linguistic and reasoning skills with no annotated dialogue states.",abstractText,[0],[0]
"We also introduce dialogue rollouts, in which the model plans ahead by simulating possible complete continuations of the conversation, and find that this technique dramatically improves performance.",abstractText,[0],[0]
Our code and dataset are publicly available.,abstractText,[0],[0]
"In recent years, we have reached unprecedented data volumes that are high dimensional and sit over (clouds of) networked machines.",1. Introduction,[0],[0]
"As a result, decentralized collection of these data sets along with accompanying distributed op-
1Laboratory for Information and Decision Systems, Massachusetts Institute of Technology 2Department of Electrical and Systems Engineering, University of Pennsylvania 3Department of Electrical Engineering and Computer Science, Yale University.",1. Introduction,[0],[0]
"Correspondence to: Aryan Mokhtari <aryanm@mit.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"timization methods are not only desirable but very often necessary (Boyd et al., 2011).
",1. Introduction,[0],[0]
"The focus of this paper is on decentralized optimization, the goal of which is to maximize/minimize a global objective function –distributed over a network of computing units– through local computation and communications among nodes.",1. Introduction,[0],[0]
"A canonical example in machine learning is fitting models using M-estimators where given a set of data points the parameters of the model are estimated through an empirical risk minimization (Vapnik, 1998).",1. Introduction,[0],[0]
"Here, the global objective function is defined as an average of local loss functions associated with each data point.",1. Introduction,[0],[0]
"Such local loss functions can be convex (e.g., logistic regression, SVM, etc) or non-convex (e.g., non-linear square loss, robust regression, mixture of Gaussians, deep neural nets, etc) (Mei et al., 2016).",1. Introduction,[0],[0]
"Due to the sheer volume of data points, these optimization tasks cannot be fulfilled on a single computing cluster node.",1. Introduction,[0],[0]
"Instead, we need to opt for decentralized solutions that can efficiently exploit dispersed (and often distant) computational resources linked through a tightly connected network.",1. Introduction,[0],[0]
"Furthermore, local computations should be light so that they can be done on single machines.",1. Introduction,[0],[0]
"In particular, when the data is high dimensional, extra care should be given to any optimization procedure that relies on projections over the feasibility domain.
",1. Introduction,[0],[0]
"In addition to large scale machine learning applications, decentralized optimization is a method of choice in many other domains such as Internet of Things (IoT) (Abu-Elkheir et al., 2013), remote sensing (Ma et al., 2015), multi-robot systems (Tanner & Kumar, 2005), and sensor networks (Rabbat & Nowak, 2004).",1. Introduction,[0],[0]
"In such scenarios, individual entities can communicate over a network and interact with the environment by exchanging the data generated through sensing.",1. Introduction,[0],[0]
At the same time they can react to events and trigger actions to control the physical world.,1. Introduction,[0],[0]
"These applications highlight another important aspect of decentralized optimization where private data is collected by different sensing units (Yang et al., 2017).",1. Introduction,[0],[0]
"Here again, we aim to optimize a global objective function while avoiding to share the private data among computing units.",1. Introduction,[0],[0]
"Thus, by design, one cannot solve such private optimization problems in a centralized manner and should rely on decentralized solutions where local private
computation is done where the data is collected.
",1. Introduction,[0],[0]
"Continuous submodular functions, a broad subclass of nonconvex functions with diminishing returns property, have recently received considerable attention (Bach, 2015; Bian et al., 2017).",1. Introduction,[0],[0]
"Due to their interesting structures that allow strong approximation guarantees (Mokhtari et al., 2018a; Bian et al., 2017), they have found various applications, including the design of online experiments (Chen et al., 2018), budget and resource allocations (Eghbali & Fazel, 2016; Staib & Jegelka, 2017), and learning assignments (Golovin et al., 2014).",1. Introduction,[0],[0]
"However, all the existing work suffer from centralized computing.",1. Introduction,[0],[0]
"Given that many information gathering, data summarization, and non-parametric learning problems are inherently related to large-scale submodular maximization, the demand for a fully decentralized solution is immediate.",1. Introduction,[0],[0]
"In this paper, we develop the first decentralized framework for both continuous and discrete submodular functions.",1. Introduction,[0],[0]
"Our contributions are as follows:
• Continuous submodular maximization: For any global objective function that is monotone and continuous DR-submodular and subject to any downclosed and bounded convex body, we develop Decentralized Continuous Greedy, a decentralized and projection-free algorithm that achieves the tight (1 1/e ✏) approximation guarantee in O(1/✏2) rounds of local communication.
",1. Introduction,[0],[0]
"• Discrete submodular maximization: For any global objective function that is monotone and submodular and subject to any matroid constraint, we develop a discrete variant of Decentralized Continuous Greedy that achieves the tight (1 1/e ✏) approximation ratio in O(1/✏3) rounds of communication.
",1. Introduction,[0],[0]
All proofs are provided in the supplementary material.,1. Introduction,[0],[0]
"Decentralized optimization is a challenging problem as nodes only have access to separate components of the global objective function, while they aim to collectively reach the global optimum point.",2. Related Work,[0],[0]
"Indeed, one naive approach to tackle this problem is to broadcast local objective functions to all the nodes in the network and then solve the problem locally.",2. Related Work,[0],[0]
"However, this scheme requires high communication overhead and disregards the privacy associated with the data of each node.",2. Related Work,[0],[0]
"An alternative approach is the master-slave setting (Bekkerman et al., 2011; Shamir et al., 2014; Zhang & Lin, 2015) where at each iteration, nodes use their local data to compute the information needed by the master node.",2. Related Work,[0],[0]
"Once the master node receives all the local information, it updates its decision and broadcasts the decision to all the nodes.",2. Related Work,[0],[0]
"Although this scheme protects the privacy of nodes it is not robust to machine failures and is prone to high overall
communication time.",2. Related Work,[0],[0]
"In decentralized methods, these issues are overcame by removing the master node and considering each node as an independent unit that is allowed to exchange information with its neighbors.
",2. Related Work,[0],[0]
"Convex decentralized consensus optimization is a relatively mature area with a myriad of primal and dual algorithms (Bertsekas & Tsitsiklis, 1989).",2. Related Work,[0],[0]
"Among primal methods, decentralized (sub)gradient descent is perhaps the most well known algorithm which is a mix of local gradient descent and successive averaging (Nedic & Ozdaglar, 2009; Yuan et al., 2016).",2. Related Work,[0],[0]
It also can be interpreted as a penalty method that encourages agreement among neighboring nodes.,2. Related Work,[0],[0]
"This latter interpretation has been exploited to solve the penalized objective function using accelerated gradient descent (Jakovetić et al., 2014; Qu & Li, 2017), Newton’s method (Mokhtari et al., 2017; Bajovic et al., 2017), or quasi-Newton algorithms (Eisen et al., 2017).",2. Related Work,[0],[0]
The methods that operate in the dual domain consider a constraint that enforces equality between nodes’ variables and solve the problem by ascending on the dual function to find optimal Lagrange multipliers.,2. Related Work,[0],[0]
"A short list of dual methods are the alternating directions method of multipliers (ADMM) (Boyd et al., 2011), dual ascent algorithm (Rabbat et al., 2005), and augmented Lagrangian methods (Jakovetic et al., 2015; Chatzipanagiotis & Zavlanos, 2015).",2. Related Work,[0],[0]
"Recently, there have been many attempts to extend the tools in decentralized consensus optimization to the case that the objective function is non-convex (Di Lorenzo & Scutari, 2016; Sun et al., 2016; Hajinezhad et al., 2016; Tatarenko & Touri, 2017).",2. Related Work,[0],[0]
"However, such works are mainly concerned with reaching a stationary point and naturally cannot provide any optimality guarantee.
",2. Related Work,[0],[0]
"In this paper, our focus is to provide the first decentralized algorithms for both discrete and continuous submodular functions.",2. Related Work,[0],[0]
"It is known that the centralized greedy approach of (Nemhauser et al., 1978), and its many variants (Feige et al., 2011; Buchbinder et al., 2015; 2014; Feldman et al., 2017; Mirzasoleiman et al., 2016), reach tight approximation guarantees in various scenarios.",2. Related Work,[0],[0]
"As such methods are sequential in nature, they do not scale to massive datasets.",2. Related Work,[0],[0]
"To partially resolve this issue, MapReduce style methods, with a master-slave architecture, have been proposed (Mirzasoleiman et al., 2013; Kumar et al., 2015; da Ponte Barbosa et al., 2015; Mirrokni & Zadimoghaddam, 2015; Qu et al., 2015).
",2. Related Work,[0],[0]
"One can extend the notion of diminishing returns to continuous domains (Wolsey, 1982; Bach, 2015).",2. Related Work,[0],[0]
"Even though continuous submodular functions are not generally convex (nor concave) Hassani et al. (2017) showed that in the monotone setting and subject to a general bounded convex body constraint, stochastic gradient methods can achieve a 1/2 approximation guarantee.",2. Related Work,[0],[0]
"The approximation guarantee can be tightened to (1 1/e) by using Frank-Wolfe (Bian et al.,
2017) or stochastic Frank-Wolfe (Mokhtari et al., 2018a).",2. Related Work,[0],[0]
"In this section, we review the notation that we use throughout the paper.",3. Notation and Background,[0],[0]
"We then give the precise definition of submodularity in discrete and continuous domains.
Notation.",3. Notation and Background,[0],[0]
Lowercase boldface v denotes a vector and uppercase boldface W a matrix.,3. Notation and Background,[0],[0]
"The i-th element of v is written as vi and the element on the i-th row and j-th column of W is denoted by wi,j .",3. Notation and Background,[0],[0]
We use kvk to denote the Euclidean norm of vector v and kWk to denote the spectral norm of matrix W.,3. Notation and Background,[0],[0]
The null space of matrix W is denoted by null(W).,3. Notation and Background,[0],[0]
"The inner product of vectors x,y is indicated by hx,yi, and the transpose of a vector v or matrix W are denoted by v† and W†, respectively.",3. Notation and Background,[0],[0]
The vector 1n 2,3. Notation and Background,[0],[0]
"Rn is the vector of all ones with n components, and the vector 0p 2",3. Notation and Background,[0],[0]
Rp is the vector of all zeros with p components.,3. Notation and Background,[0],[0]
Submodulary.,3. Notation and Background,[0],[0]
A set function f : 2V !,3. Notation and Background,[0],[0]
"R+, defined on the ground set V , is called submodular if for all A,B ✓ V , we have f(A)+f(B) f(A\B)+f(A[B).",3. Notation and Background,[0],[0]
We often need to maximize submodular functions subject to a down-closed set family I .,3. Notation and Background,[0],[0]
"In particular, we say I ⇢ 2V is a matroid if 1) for any A ⇢ B ⇢ V , if B 2 I, then A 2 I and 2) for any A,B 2 I if |A| < |B|, then there is an element e 2 B such that A [ {e} 2 I.",3. Notation and Background,[0],[0]
"The notion of submodularity goes beyond the discrete domain (Wolsey, 1982; Vondrák, 2007; Bach, 2015).",3. Notation and Background,[0],[0]
Consider a continuous function F : X ! R+ where the set X ✓,3. Notation and Background,[0],[0]
Rp is of the form X =Qpi=1 Xi and each Xi is a compact subset of R+.,3. Notation and Background,[0],[0]
"We call the continuous function F submodular if for all x,y 2 X",3. Notation and Background,[0],[0]
"we have
F (x) + F (y) F",3. Notation and Background,[0],[0]
(x _ y) + F,3. Notation and Background,[0],[0]
"(x ^ y), (1) where x_y := max(x,y) (component-wise) and x^y",3. Notation and Background,[0],[0]
":= min(x,y) (component-wise).",3. Notation and Background,[0],[0]
"In this paper, our focus is on differentiable continuous submodular functions with two additional properties: monotonicity and diminishing returns.",3. Notation and Background,[0],[0]
"Formally, a submodular function F is monotone if
x  y =) F (x)  ",3. Notation and Background,[0],[0]
"F (y), (2) for all x,y 2 X .",3. Notation and Background,[0],[0]
"Note that x  y in (2) means that xi  yi for all i = 1, . . .",3. Notation and Background,[0],[0]
",",3. Notation and Background,[0],[0]
p.,3. Notation and Background,[0],[0]
"Furthermore, a differentiable submodular function F is called DR-submodular (i.e., shows diminishing returns) if the gradients are antitone, namely, for all x,y 2 X we have
x  y =) rF (x) rF (y).",3. Notation and Background,[0],[0]
"(3) When the function F is twice differentiable, submodularity implies that all cross-second-derivatives are non-positive (Bach, 2015), and DR-submodularity implies that all secondderivatives are non-positive (Bian et al., 2017)",3. Notation and Background,[0],[0]
"In this work,
we consider the maximization of continuous submodular functions subject to down-closed convex bodies C ⇢ Rp+ defined as follows.",3. Notation and Background,[0],[0]
"For any two vectors x,y 2 Rp+, where x  y, down-closedness means that if y 2 C, then so is x 2 C. Note that for a down-closed set we have 0p 2 C.",3. Notation and Background,[0],[0]
"In this section, we state the problem of decentralized submodular maximization in continuous and discrete settings.
",4. Decentralized Submodular Maximization,[0],[0]
Continuous Case.,4. Decentralized Submodular Maximization,[0],[0]
We consider a set of n computing machines/sensors that communicate over a graph to maximize a global objective function.,4. Decentralized Submodular Maximization,[0],[0]
"Each machine can be viewed as a node i 2 N , {1, · · · , n}.",4. Decentralized Submodular Maximization,[0],[0]
"We further assume that the possible communication links among nodes are given by a bidirectional connected communication graph G = (N , E) where each node can only communicate with its neighbors in G. We formally use Ni to denote node i’s neighbors.",4. Decentralized Submodular Maximization,[0],[0]
"In our setting, we assume that each node i 2 N has access to a local function Fi : X !",4. Decentralized Submodular Maximization,[0],[0]
R+.,4. Decentralized Submodular Maximization,[0],[0]
The nodes cooperate in order to maximize the aggregate monotone and continuous DR-submodular function F : X !,4. Decentralized Submodular Maximization,[0],[0]
"R+ subject to a down-closed convex body C ⇢ X ⇢ Rp+, i.e.,
max x2C F (x) = max x2C
1
n
nX
i=1
Fi(x).",4. Decentralized Submodular Maximization,[0],[0]
"(4)
The goal is to design a message passing algorithm to solve (4) such that: (i) at each iteration t, the nodes send their messages (and share their information) to their neighbors in G, and (ii) as t grows, all the nodes reach to a point x 2 C that provides a (near-) optimal solution for (4).
",4. Decentralized Submodular Maximization,[0],[0]
Discrete Case.,4. Decentralized Submodular Maximization,[0],[0]
Let us now consider the discrete counterpart of problem (4).,4. Decentralized Submodular Maximization,[0],[0]
"In this setting, each node i 2 N has access to a local set function fi : 2V !",4. Decentralized Submodular Maximization,[0],[0]
R+.,4. Decentralized Submodular Maximization,[0],[0]
The nodes cooperate in maximizing the aggregate monotone submodular function f : 2V !,4. Decentralized Submodular Maximization,[0],[0]
R+ subject to a matroid constraint,4. Decentralized Submodular Maximization,[0],[0]
"I, i.e.
max S2I f(S) = max S2I
1
n
nX
i=1
fi(S).",4. Decentralized Submodular Maximization,[0],[0]
"(5)
Note that even in the centralized case, and under reasonable complexity-theoretic assumptions, the best approximation guarantee we can achieve for Problems (4) and (5) is (1 1/e) (Feige, 1998).",4. Decentralized Submodular Maximization,[0],[0]
"In the following, we show that it is possible to achieve the same approximation guarantee in a decentralized setting.",4. Decentralized Submodular Maximization,[0],[0]
"In this section, we introduce the Decentralized Continuous Greedy (DCG) algorithm for solving Problem (4).",5. Decentralized Continuous Greedy Method,[0],[0]
"Recall that in a decentralized setting, the nodes
have to cooperate (i.e., send messages to their neighbors) in order to solve the global optimization problem.",5. Decentralized Continuous Greedy Method,[0],[0]
We will explain how such messages are designed and communicated in DCG.,5. Decentralized Continuous Greedy Method,[0],[0]
"Each node i in the network keeps track of two local variables xi,di 2 Rp which are iteratively updated at each round t using the information gathered from the neighboring nodes.",5. Decentralized Continuous Greedy Method,[0],[0]
The vector xti is the local decision variable of node i at step t whose value we expect to eventually converge to the (1 1/e) fraction of the optimal solution of Problem (4).,5. Decentralized Continuous Greedy Method,[0],[0]
"The vector dti is the estimate of the gradient of the global objective function that node i keeps at step t.
To properly incorporate the received information from their neighbors, nodes should assign nonnegative weights to their neighbors.",5. Decentralized Continuous Greedy Method,[0],[0]
Define wij 0,5. Decentralized Continuous Greedy Method,[0],[0]
to be the weight that node i assigns to node j. These weights indicate the effect of (variable or gradient) information nodes received from their neighbors in order to update their local (variable or gradient) information.,5. Decentralized Continuous Greedy Method,[0],[0]
"Indeed, the weights wij must fulfill some requirements (later described in Assumption 1), but they are design parameters of DCG and can be properly chosen by the nodes prior to the implementation of the algorithm.
",5. Decentralized Continuous Greedy Method,[0],[0]
The first step at each round t of DCG is updating the local gradient approximation vectors dti using local and neighboring gradient information.,5. Decentralized Continuous Greedy Method,[0],[0]
"In particular, node i computes its vector dti according to the update rule
d t i = (1 ↵)
X
j2Ni[{i}
wijd t 1 j + ↵rFi(xti), (6)
where ↵ 2 [0, 1] is an averaging coefficient.",5. Decentralized Continuous Greedy Method,[0],[0]
"Note that the sum
P j2Ni[{i} wijd t 1 j in (6) is a weighted average of
node i’s vector dt 1i and its neighbors d t 1 j , evaluated at step t 1.",5. Decentralized Continuous Greedy Method,[0],[0]
"Hence, node i computes the vector dti by evaluating a weighted average of its current local gradient rFi(xti) and the local and neighboring gradient information at step t 1, i.e.,Pj2Ni[{i} wijdt 1j .",5. Decentralized Continuous Greedy Method,[0],[0]
"Since the vector dti is evaluated by aggregating gradient information from neighboring nodes, it is reasonable to expect that dti becomes a proper approximation for the global objective function gradient (1/n)",5. Decentralized Continuous Greedy Method,[0],[0]
Pn k=1 rfk(x) as time progresses.,5. Decentralized Continuous Greedy Method,[0],[0]
"Note that to implement the update in (6) nodes should exchange their local vectors dti with their neighbors.
",5. Decentralized Continuous Greedy Method,[0],[0]
"Using the gradient approximation vector dti, each node i evaluates its local ascent direction vti by solving
v t i = argmax v2C hdti,vi. (7)
",5. Decentralized Continuous Greedy Method,[0],[0]
The update in (7) is also known as conditional gradient update.,5. Decentralized Continuous Greedy Method,[0],[0]
"Ideally, in a conditional gradient method, we should choose the feasible direction v 2 C that maximizes the inner product by the full gradient vector 1n",5. Decentralized Continuous Greedy Method,[0],[0]
Pn k=1 rFk(xti).,5. Decentralized Continuous Greedy Method,[0],[0]
"However, since in the decentralized setting the exact gradient 1n Pn k=1 rFk(xti) is not available at the i-th node,",5. Decentralized Continuous Greedy Method,[0],[0]
"we
Algorithm 1 DCG at node i Require:",5. Decentralized Continuous Greedy Method,[0],[0]
Stepsize ↵ and weights wij for j 2 Ni,5. Decentralized Continuous Greedy Method,[0],[0]
"[ {i}
1: Initialize local vectors as x0i = d0i = 0p 2:",5. Decentralized Continuous Greedy Method,[0],[0]
"Initialize neighbor’s vectors as x0j = d0j = 0p if j2Ni 3: for t = 1, 2, . . .",5. Decentralized Continuous Greedy Method,[0],[0]
", T do 4: Compute dti = (1 ↵) X
j2Ni[{i}
wijd t 1 j + ↵rFi(xti);
5: Exchange dti with neighboring nodes j 2 Ni 6: Evaluate vti = argmaxv2C hdti,vi; 7: Update the variable xt+1i =",5. Decentralized Continuous Greedy Method,[0],[0]
"X
j2Ni[{i}
wijx t j + 1
T v
t i ;
8: Exchange xt+1i with neighboring nodes j 2 Ni 9: end for
replace it by its current approximation dti",5. Decentralized Continuous Greedy Method,[0],[0]
"and hence we obtain the update rule (7).
",5. Decentralized Continuous Greedy Method,[0],[0]
"After computing the local ascent directions vti , the nodes update their local variables xti by averaging their local and neighboring iterates and ascend in the direction vti with stepsize 1/T where T is the total number of iterations, i.e.,
x t+1 i =
X
j2Ni[{i}
wijx t j + 1
T v
t i .",5. Decentralized Continuous Greedy Method,[0],[0]
"(8)
The update rule (8) ensures that the neighboring iterates are not far from each other via the averaging termP
j2Ni[{i} wijx t j , while the iterates approach the optimal maximizer of the global objective function by ascending in the conditional gradient direction vti .",5. Decentralized Continuous Greedy Method,[0],[0]
The update in (8) requires a round of local communication among neighbors to exchange their local variables xti.,5. Decentralized Continuous Greedy Method,[0],[0]
"The steps of the DCG method are summarized in Algorithm 1.
",5. Decentralized Continuous Greedy Method,[0],[0]
"Indeed, the weights wij that nodes assign to each other cannot be arbitrary.",5. Decentralized Continuous Greedy Method,[0],[0]
"In the following, we formalize the conditions that they should satisfy (Yuan et al., 2016).
",5. Decentralized Continuous Greedy Method,[0],[0]
"Assumption 1 The weights that nodes assign to each other are nonegative, i.e., wij 0 for all i, j 2 N , and if node j is not a neighbor of node",5. Decentralized Continuous Greedy Method,[0],[0]
"i then the corresponding weight is zero, i.e., wij = 0",5. Decentralized Continuous Greedy Method,[0],[0]
if j /2 Ni.,5. Decentralized Continuous Greedy Method,[0],[0]
"Further, the weight matrix W 2 Rn⇥n with entries wij satisfies W † = W, W1n = 1n, null(I W) = span(1n).",5. Decentralized Continuous Greedy Method,[0],[0]
"(9)
The first condition in (9) ensures that the weights are symmetric, i.e., wij = wji.",5. Decentralized Continuous Greedy Method,[0],[0]
"The second condition guarantees the weights that each node assigns to itself and its neighbors sum up to 1, i.e., Pn j=1 wij = 1 for all i. Note that the condition",5. Decentralized Continuous Greedy Method,[0],[0]
W1n = 1n implies that I W is rank deficient.,5. Decentralized Continuous Greedy Method,[0],[0]
"Hence, the last condition in (9) ensures that the rank of I W is exactly n 1.",5. Decentralized Continuous Greedy Method,[0],[0]
"Indeed, it is possible to optimally
design the weight matrix W to accelerate the averaging process as discussed in (Boyd et al., 2004), but this is not the focus of this paper.",5. Decentralized Continuous Greedy Method,[0],[0]
"We emphasize that W is not a problem parameter, and we design it prior to running DCG.
",5. Decentralized Continuous Greedy Method,[0],[0]
"Notice that the stepsize 1/T and the conditions in Assumption 1 on the weights wij are needed to ensure that the local variables xti are in the feasible set C, as stated in the following proposition.
",5. Decentralized Continuous Greedy Method,[0],[0]
Proposition 1 Consider the DCG method outlined in Algorithm 1.,5. Decentralized Continuous Greedy Method,[0],[0]
"If Assumption 1 holds and nodes start from x
0",5. Decentralized Continuous Greedy Method,[0],[0]
"i = 0p 2 C, then the local iterates xti are always in the
feasible set C, i.e., xti 2 C for all i 2 N and t = 1, . . .",5. Decentralized Continuous Greedy Method,[0],[0]
", T .
Let us now explain how DCG relates to and innovates beyond the exisiting work in submodular maximization as well as decentralized convex optimization.",5. Decentralized Continuous Greedy Method,[0],[0]
"Note that in order to solve Problem (4) in a centralized fashion (i.e., when every node has access to all the local functions) we can use the continuous greedy algorithm (Vondrák, 2008), a variant of the conditional gradient method.",5. Decentralized Continuous Greedy Method,[0],[0]
"However, in decentralized settings, nodes have only access to their local gradients, and therefore, continuous greedy is not implementable.",5. Decentralized Continuous Greedy Method,[0],[0]
"Similar to the decentralized convex optimization, we can address this issue via local information aggregation.",5. Decentralized Continuous Greedy Method,[0],[0]
"Our proposed DCG method incorporates the idea of choosing the ascent direction according to a conditional gradient update as is done in the continuous greedy algorithm (i.e., the update rule (7)), while it aggregates the global objective function information through local communications with neighboring nodes (i.e., the update rule (8)).",5. Decentralized Continuous Greedy Method,[0],[0]
"Unlike traditional consensus optimization methods that require exchanging nodes’ local variables only (Nedic & Ozdaglar, 2009; Nedic et al., 2010), DCG also requires exchanging local gradient vectors to achieve a (1 1/e) fraction of the optimal solution at each node (i.e., the update rule (6)).",5. Decentralized Continuous Greedy Method,[0],[0]
"This major difference is due to the fact that in conditional gradient methods, unlike proximal gradient algorithms, the local gradients can not be used instead of the global gradient.",5. Decentralized Continuous Greedy Method,[0],[0]
"In other words, in the update rule (7), we can not use the local gradients rFi(xti) in lieu of dti.",5. Decentralized Continuous Greedy Method,[0],[0]
"Indeed, there are settings for which such a replacement provides arbitrarily bad solutions.",5. Decentralized Continuous Greedy Method,[0],[0]
We formally characterize the convergence of DCG in Theorem 1.,5. Decentralized Continuous Greedy Method,[0],[0]
"In this section we show how DCG can be used for maximizing a decentralized submodular set function f , namely Problem (5), through its continuous relaxation.",5.1. Extension to the Discrete Setting,[0],[0]
"Formally, in lieu of solving Problem (5), we can form the following decentralized continuous optimization problem
max x2C
1
n
nX
i=1
Fi(x), (10)
",5.1. Extension to the Discrete Setting,[0],[0]
"Algorithm 2 Discrete DCG at node i Require: ↵, 2 [0, 1] and weights wij for",5.1. Extension to the Discrete Setting,[0],[0]
j 2 Ni,5.1. Extension to the Discrete Setting,[0],[0]
"[ {i}
1: Initialize local vectors as x0i = d0i = g0i = 0 2: Initialize neighbor’s vectors as x0j = d0j = 0",5.1. Extension to the Discrete Setting,[0],[0]
"if j 2 Ni 3: for t = 1, 2, . . .",5.1. Extension to the Discrete Setting,[0],[0]
", T do 4: Compute gti = (1 )gt 1i + r ˜Fi(xti); 5: Compute dti = (1 ↵) X
j2Ni[{i}
wijd t 1 j + ↵g t i ;
6: Exchange dti with neighboring nodes j 2 Ni 7: Evaluate vti = argmaxv2C hdti,vi; 8: Update the variable xt+1i =",5.1. Extension to the Discrete Setting,[0],[0]
"X
j2Ni[{i}
wijx t j + 1
T v
t i ;
9: Exchange xt+1i with neighboring nodes j 2 Ni; 10: end for 11:",5.1. Extension to the Discrete Setting,[0],[0]
"Apply proper rounding to obtain a solution for (5);
where Fi is the multilinear extension of fi defined as
Fi(x) = X
S⇢V fi(S)
Y i2S xi Y j /2S (1 xj), (11)
and the down-closed convex set C = conv{1I :",5.1. Extension to the Discrete Setting,[0],[0]
I 2 I} is the matroid polytope.,5.1. Extension to the Discrete Setting,[0],[0]
"Note that the discrete and continuous optimization formulations lead to the same optimal value (Calinescu et al., 2011).
",5.1. Extension to the Discrete Setting,[0],[0]
"Based on the expression in (11), computing the full gradient rFi at each node i will require an exponential computation in terms of |V |, since the number of summands in (11) is 2
|V |.",5.1. Extension to the Discrete Setting,[0],[0]
"As a result, in the discrete setting, we will slightly modify the DCG algorithm and work with unbiased estimates of the gradient that can be computed in time O(|V |) (see Appendix 9.7 for one such estimator).",5.1. Extension to the Discrete Setting,[0],[0]
"More precisely, in the discrete setting, each node i 2 N updates three local variables xti,dti,gti 2 R|V |.",5.1. Extension to the Discrete Setting,[0],[0]
"The variables xti,dti play the same role as in DCG and are updated using the messages received from the neighboring nodes.",5.1. Extension to the Discrete Setting,[0],[0]
The variable gti at node i is defined to approximate the local gradient rFi(xti).,5.1. Extension to the Discrete Setting,[0],[0]
"Consider the vector r ˜Fi(xti) as an unbiased estimator of the local gradient rFi(xti) at time t, and define the vector g
t i as the outcome of the recursion
g t i = (1 )gt 1i + r ˜Fi(xti), (12)
where 2 [0, 1] is the averaging parameter.",5.1. Extension to the Discrete Setting,[0],[0]
We initialize all vectors as g0i = 0 2 R|V |.,5.1. Extension to the Discrete Setting,[0],[0]
"It was shown recently (Mokhtari et al., 2018a;b) that the averaging technique in (12) reduces the noise of the gradient approximations.",5.1. Extension to the Discrete Setting,[0],[0]
"Therefore, the sequence of gti approaches the true local gradient rFi(xti) as time progresses.
",5.1. Extension to the Discrete Setting,[0],[0]
"The steps of the Decentralized Continuous Greedy for the discrete setting is summarized in Algo-
rithm 2.",5.1. Extension to the Discrete Setting,[0],[0]
Note that the major difference between the Discrete DCG method (Algorithm 2) and the continuous DCG method (Algorithm 1) is in Step 5 in which the exact local gradient rFi(xti) is replaced by the stochastic approximation gti which only requires access to the computationally cheap unbiased gradient estimator r ˜Fi(xti).,5.1. Extension to the Discrete Setting,[0],[0]
The communication complexity of both the discrete and continuous versions of DCG are the same at each round.,5.1. Extension to the Discrete Setting,[0],[0]
"However, since we are using unbiased estimations of the local gradients rFi(xi), the Discrete DCG takes more rounds to converge to a near-optimal solution compared to continuous DCG.",5.1. Extension to the Discrete Setting,[0],[0]
We characterize the convergence of Discrete DCG in Theorem 2.,5.1. Extension to the Discrete Setting,[0],[0]
"Further, the implementation of Discrete DCG requires rounding the continuous solution to obtain a discrete solution for the original problem without any loss in terms of the objective function value.",5.1. Extension to the Discrete Setting,[0],[0]
"The provably lossless rounding schemes include the pipage rounding (Calinescu et al., 2011) and contention resolution (Chekuri et al., 2014).",5.1. Extension to the Discrete Setting,[0],[0]
"In this section, we study the convergence properties of DCG in both continuous and discrete settings.",6. Convergence Analysis,[0],[0]
"In this regard, we assume that the following conditions hold.
",6. Convergence Analysis,[0],[0]
"Assumption 2 Euclidean distance of the elements in the set C are uniformly bounded, i.e., for all x,y 2 C we have
kx yk  D. (13) Assumption 3",6. Convergence Analysis,[0],[0]
The local objective functions Fi(x) are monotone and DR-submodular.,6. Convergence Analysis,[0],[0]
"Further, their gradients are L-Lipschitz continuous over the set X , i.e., for all x,y 2 X
krFi(x) rFi(y)k  ",6. Convergence Analysis,[0],[0]
Lkx yk.,6. Convergence Analysis,[0],[0]
(14) Assumption 4,6. Convergence Analysis,[0],[0]
"The norm of gradients krFi(x)k are bounded over the convex set C, i.e., for all x 2 C, i 2 N ,
krFi(x)k  G. (15)
",6. Convergence Analysis,[0],[0]
The condition in Assumption 2 guarantees that the diameter of the convex set C is bounded.,6. Convergence Analysis,[0],[0]
Assumption 3 is needed to ensure that the local objective functions Fi are smooth.,6. Convergence Analysis,[0],[0]
"Finally, the condition in Assumption 4 enforces the gradients norm to be bounded over the convex set C. All these assumptions are customary and necessary in the analysis of decentralized algorithms.",6. Convergence Analysis,[0],[0]
"For more details, please check Section VII-B in Jakovetić et al. (2014).
",6. Convergence Analysis,[0],[0]
We proceed to derive a constant factor approximation for DCG.,6. Convergence Analysis,[0],[0]
Our main result is stated in Theorem 1.,6. Convergence Analysis,[0],[0]
"However, to better illustrate the main result, we first need to provide several definitions and technical lemmas.",6. Convergence Analysis,[0],[0]
"Let us begin by defining the average variables ¯xt as
¯ x t = 1
n
nX
i=1
x t i. (16)
",6. Convergence Analysis,[0],[0]
"In the following lemma, we establish an upper bound on the variation in the sequence of average variables {¯xt}.",6. Convergence Analysis,[0],[0]
Lemma 1 Consider the proposed DCG algorithm defined in Algorithm 1.,6. Convergence Analysis,[0],[0]
"Further, recall the definition of ¯xt in (16).",6. Convergence Analysis,[0],[0]
"If Assumptions 1 and 2 hold, then the difference between two consecutive average vectors is upper bounded by
k¯xt+1 ¯xtk  D T .",6. Convergence Analysis,[0],[0]
"(17)
Recall that at every node i, the messages are mixed using the coefficients wij , i.e., the i-th row of the matrix W. It is thus not hard to see that the spectral properties of W (e.g. the spectral gap) play an important role in the the speed of achieving consensus in decentralized methods.
",6. Convergence Analysis,[0],[0]
Definition 1 Consider the eigenvalues of W which can be sorted in a nonincreasing order as 1 = 1(W) 2(W) · · · n(W),6. Convergence Analysis,[0],[0]
> 1.,6. Convergence Analysis,[0],[0]
"Define as the second largest magnitude of the eigenvalues of W, i.e.,
:= max{| 2(W)|, | n(W)|}.",6. Convergence Analysis,[0],[0]
"(18)
",6. Convergence Analysis,[0],[0]
"As we will see, a mixing matrix W with smaller has a larger spectral gap 1 which yields faster convergence (Boyd et al., 2004; Duchi et al., 2012).",6. Convergence Analysis,[0],[0]
"In the following lemma, we derive an upper bound on the sum of the distances between the local iterates xti and their average ¯xt, where the bound is a function of the graph spectral gap 1 , size of the network n, and the total number of iterations T .
",6. Convergence Analysis,[0],[0]
Lemma 2 Consider the proposed DCG algorithm defined in Algorithm 1.,6. Convergence Analysis,[0],[0]
"Further, recall the definition of ¯xt in (16).",6. Convergence Analysis,[0],[0]
"If Assumptions 1 and 2 hold, then for all t  T we have
nX
i=1
x t i ¯xt
2 !1/2  p nD
T (1 ) .",6. Convergence Analysis,[0],[0]
"(19)
Let us now define ¯dt as the average of local gradient approximations dti at step t, i.e., ¯dt = 1 n",6. Convergence Analysis,[0],[0]
Pn i=1 d t i.,6. Convergence Analysis,[0],[0]
"We will show in the following that the vectors dti also become uniformly close to ¯dt.
",6. Convergence Analysis,[0],[0]
Lemma 3 Consider the proposed DCG algorithm defined in Algorithm 1.,6. Convergence Analysis,[0],[0]
"If Assumptions 1 and 3 hold, then
nX
i=1
kdti ¯dtk2 !1/2  ↵ p nG
1 (1 ↵)",6. Convergence Analysis,[0],[0]
.,6. Convergence Analysis,[0],[0]
"(20)
Lemma 3 guarantees that the individual local gradient approximation vectors dti are close to the average vector ¯dt if the parameter ↵ is small.",6. Convergence Analysis,[0],[0]
"To show that the gradient vectors dti, generated by DCG, approximate the gradient of the
global objective function, we further need to show that the average vector ¯dt approaches the global objective function gradient rF .",6. Convergence Analysis,[0],[0]
We prove this claim in the following lemma.,6. Convergence Analysis,[0],[0]
Lemma 4 Consider the proposed DCG algorithm defined in Algorithm 1.,6. Convergence Analysis,[0],[0]
"If Assumptions 1-4 hold, then
¯ d t 1 n
nX
i=1
rFi(¯xt)
 (1 ↵)tG+ ✓ (1 ↵)LD
↵T +
LD T (1 ) ◆ .",6. Convergence Analysis,[0],[0]
"(21)
",6. Convergence Analysis,[0],[0]
"By combining Lemmas 3 and 4 and setting ↵ = 1/ p T we can conclude that the local gradient approximation vector d
t",6. Convergence Analysis,[0],[0]
"i of each node i is within O(1/
p T )",6. Convergence Analysis,[0],[0]
"distance of the global
objective gradient rF (¯xt) evaluated at ¯xt.",6. Convergence Analysis,[0],[0]
"We use this observation in the following theorem to show that the sequence of iterates generated by DCG achieves the tight (1 1/e) approximation ratio of the optimum global solution.
",6. Convergence Analysis,[0],[0]
Theorem 1 Consider the proposed DCG method outlined in Algorithm 1.,6. Convergence Analysis,[0],[0]
"Further, consider x⇤ as the global maximizer of Problem (4).",6. Convergence Analysis,[0],[0]
"If Assumptions 1-4 hold and we set ↵ = 1/ p T , for all nodes j 2 N , the local variable xTj obtained after T iterations satisfies
F (xTj ) (1 e 1)F",6. Convergence Analysis,[0],[0]
"(x⇤) LD2 +GD T 1/2 GD
T 1/2(1 )",6. Convergence Analysis,[0],[0]
"LD 2
2T GD + LD
2
T (1 ) .",6. Convergence Analysis,[0],[0]
"(22)
Theorem 1 shows that the sequence of the local variables x
t j , generated by DCG, is able to achieve the optimal approximation ratio (1 1/e), while the error term vanishes at a sublinear rate of O(1/T 1/2), i.e.,
F (xTj ) (1 1/e)F (x⇤) O",6. Convergence Analysis,[0],[0]
"✓
1 (1 )T 1/2 ◆ , (23)
which implies that the iterate of each node reaches an objective value larger than (1 1/e ✏)OPT after O(1/✏2) rounds of communication.",6. Convergence Analysis,[0],[0]
It is worth mentioning that the result in Theorem 1 is consistent with classical results in decentralized optimization that the error term vanishes faster for the graphs with larger spectral gap 1 .,6. Convergence Analysis,[0],[0]
We proceed to study the convergence properties of Discrete DCG in Algorithm 2.,6. Convergence Analysis,[0],[0]
"To do so, we first assume that the variance of the stochastic gradients r ˜Fi(x) used in Discrete DCG is bounded.",6. Convergence Analysis,[0],[0]
"We justify this assumption in Remark 1.
",6. Convergence Analysis,[0],[0]
Assumption 5,6. Convergence Analysis,[0],[0]
The variance of the unbiased estimators r ˜F,6. Convergence Analysis,[0],[0]
"(x) is bounded above by 2 over the convex set C, i.e., for any i 2 N and any vector x 2 C we can write
E h kr ˜Fi(x) rFi(x)k2 i  2, (24)
where the expectation is with respect to the randomness of the unbiased estimator.
",6. Convergence Analysis,[0],[0]
"In the following theorem, we show that Discrete DCG achieves a (1 1/e) approximation ration for Problem (5).
",6. Convergence Analysis,[0],[0]
Theorem 2 Consider our proposed Discrete DCG algorithm outlined in Algorithm 2.,6. Convergence Analysis,[0],[0]
Recall the definition of the multilinear extension function Fi in (11).,6. Convergence Analysis,[0],[0]
"If Assumptions 1-5 hold and we set ↵ = T 1/2 and = T 2/3, then for all nodes j 2 N the local variables xTj obtained after running Discrete DCG for T iterations satisfy
E ⇥",6. Convergence Analysis,[0],[0]
F (xTj ) ⇤ (1 e 1)F,6. Convergence Analysis,[0],[0]
(x⇤) O,6. Convergence Analysis,[0],[0]
"✓
1 (1 )T 1/3 ◆ , (25)
where x⇤ is the global maximizer of Problem (10).
",6. Convergence Analysis,[0],[0]
Theorem 2 states that the sequence of iterates generated by Discrete DCG achieves the tight (1 1/e ✏) approximation guarantee for Problem (10) after O(1/✏3) iterations.,6. Convergence Analysis,[0],[0]
Remark 1,6. Convergence Analysis,[0],[0]
For any submodular set function h : 2V !,6. Convergence Analysis,[0],[0]
"R with associated multilinear extension H , it can be shown that its Lipschitz constant L and the gradient norm G are both bounded above by mf
p|V |, where mf is the maximum marginal value of f , i.e., mf = maxi2V f({i}) (see, Hassani et al. (2017)).",6. Convergence Analysis,[0],[0]
"Similarly, it can be shown that for the unbiased estimator in Appendix 9.7 we have  mf p|V |.",6. Convergence Analysis,[0],[0]
We will consider a discrete setting for our experiments and use Algorithm 2 to find a decentralized solution.,7. Numerical Experiments,[0],[0]
"The main objective is to demonstrate how consensus is reached and how the global objective increases depending on the topology of the network and the parameters of the algorithm.
",7. Numerical Experiments,[0],[0]
"For our experiments, we have used the MovieLens data set.",7. Numerical Experiments,[0],[0]
It consists of 1 million ratings (from 1 to 5) by M = 6000 users for p = 4000 movies.,7. Numerical Experiments,[0],[0]
We consider a network of n = 100 nodes.,7. Numerical Experiments,[0],[0]
"The data has been distributed equally between the nodes of the network, i.e., the set of users has been partitioned into 100 equally-sized sets and each node in the network has access to only one chunk (partition) of the data.",7. Numerical Experiments,[0],[0]
The global task is to find a set of k movies that are most satisfactory to all the users (the precise formulation will appear shortly).,7. Numerical Experiments,[0],[0]
"However, as each of the nodes in the network has access to the data of a small portion of the users, the nodes have to cooperate to fulfill the global task.
",7. Numerical Experiments,[0],[0]
We consider a well motivated objective function for the experiments.,7. Numerical Experiments,[0],[0]
"Let r`,j denote the rating of user ` for movie j (if such a rating does not exist in the data we assign r`,j to 0).",7. Numerical Experiments,[0],[0]
We associate to each user ` a “facility location” objective function g`(S),7. Numerical Experiments,[0],[0]
"= maxj2S r`,j , where S is any subset of
the movies (i.e. the ground set V is the set of the movies).",7. Numerical Experiments,[0],[0]
Such a function shows how much user ` will be “satisfied” by a subset S of the movies.,7. Numerical Experiments,[0],[0]
Recall that each node i in the network has access to the data of a (small) subset of users which we denote by Ui.,7. Numerical Experiments,[0],[0]
The objective function associated with node i is given by fi(S) = P `2Ui g`(S).,7. Numerical Experiments,[0],[0]
"With such a choice of the local functions, our global task is hence to solve problem (5) when the matroid I is the k-uniform matroid (a.k.a.",7. Numerical Experiments,[0],[0]
"the k-cardinality constraint).
",7. Numerical Experiments,[0],[0]
"We consider three different choices for the underlying communication graph between the 100 nodes: A line graph (which looks like a simple path from node 1 to node 100), an Erdos-Renyi random graph (with average degree 5), and a complete graph.",7. Numerical Experiments,[0],[0]
The matrix W is chosen as follows (based on each of the three graphs).,7. Numerical Experiments,[0],[0]
"If (i, j) is and edge of the graph, we let wi,j = 1/(1+max(di, dj)).",7. Numerical Experiments,[0],[0]
"If (i, j) is not an edge and i, j are distinct integers, we have wi,j = 0.",7. Numerical Experiments,[0],[0]
"Finally we let wi,i = 1 P j2N wi,j .",7. Numerical Experiments,[0],[0]
"It is not hard to show that the above choice for W satisfies Assumption 1.
",7. Numerical Experiments,[0],[0]
Figure 1 shows how consensus is reached w.r.t each of the three underlying networks.,7. Numerical Experiments,[0],[0]
"To measure consensus, we plot the (logarithm of) distance-to-average value 1n",7. Numerical Experiments,[0],[0]
Pn i=1,7. Numerical Experiments,[0],[0]
"||xTi
¯",7. Numerical Experiments,[0],[0]
x T ||,7. Numerical Experiments,[0],[0]
as a function of the total number of iterations T averaged over many trials (see (16) for the definition of ¯xT ).,7. Numerical Experiments,[0],[0]
It is easy to see that the distance to average is small if and only if all the local decisions xTi are close to the average decision ¯xT .,7. Numerical Experiments,[0],[0]
"As expected, it takes much less time to reach consensus when the underlying graph is fully connected (i.e. complete graph).",7. Numerical Experiments,[0],[0]
"For the line graph, the convergence is very slow as this graph has the least degree of connectivity.
",7. Numerical Experiments,[0],[0]
Figure 2 depicts the obtained objective value of Discrete DCG (Algorithm 2) for the three networks considered above.,7. Numerical Experiments,[0],[0]
"More precisely, we plot the value 1n",7. Numerical Experiments,[0],[0]
Pn i=1,7. Numerical Experiments,[0],[0]
"f(x T i ) obtained
at the end of Algorithm 2 as a function of the cardinality constraint k.",7. Numerical Experiments,[0],[0]
We also compare these values with the value obtained by the centralized greedy algorithm (i.e. the centralized solution).,7. Numerical Experiments,[0],[0]
A few comments are in order.,7. Numerical Experiments,[0],[0]
The performance of Algorithm 2 is close to the centralized solution when the underlying graph is the Erdos-Renyi (with average degree 5) graph or the complete graphs.,7. Numerical Experiments,[0],[0]
This is because for both such graphs consensus is achieved from the early stages of the algorithm.,7. Numerical Experiments,[0],[0]
"By increasing T , we see that the performance becomes closer to the centralized solution.",7. Numerical Experiments,[0],[0]
"However, when the underlying graph is the line graph, then consensus will not be achieved unless the number of iterations is significantly increased.",7. Numerical Experiments,[0],[0]
"Consequently, for small number of iterations (e.g. T  1000) the performance of the algorithm will not be close to the centralized solution.",7. Numerical Experiments,[0],[0]
"In this paper, we proposed the first fully decentralized optimization method for maximizing discrete and continuous submodular functions.",8. Conclusion,[0],[0]
"We developed Decentralized Continuous Greedy (DCG) that achieves a (1 1/e ✏) approximation guarantee with O(1/✏2) and (1/✏3) local rounds of communication in the continuous and discrete settings, respectively.",8. Conclusion,[0],[0]
"This work was done while A. Mokhtari was visiting the Simons Institute for the Theory of Computing, and his work was partially supported by the DIMACS/Simons Collaboration on Bridging Continuous and Discrete Optimization through NSF grant #CCF-1740425.",Acknowledgements,[0],[0]
The work of A. Karbasi was supported by DARPA Young Faculty Award (D16AP00046) and AFOSR YIP (FA9550-18-1-0160).,Acknowledgements,[0],[0]
"In this paper, we showcase the interplay between discrete and continuous optimization in network-structured settings.",abstractText,[0],[0]
We propose the first fully decentralized optimization method for a wide class of non-convex objective functions that possess a diminishing returns property.,abstractText,[0],[0]
"More specifically, given an arbitrary connected network and a global continuous submodular function, formed by a sum of local functions, we develop Decentralized Continuous Greedy (DCG), a message passing algorithm that converges to the tight (1 1/e) approximation factor of the optimum global solution using only local computation and communication.",abstractText,[0],[0]
We also provide strong convergence bounds as a function of network size and spectral characteristics of the underlying topology.,abstractText,[0],[0]
"Interestingly, DCG readily provides a simple recipe for decentralized discrete submodular maximization through the means of continuous relaxations.",abstractText,[0],[0]
"Formally, we demonstrate that by lifting the local discrete functions to continuous domains and using DCG as an interface we can develop a consensus algorithm that also achieves the tight (1 1/e) approximation guarantee of the global discrete solution once a proper rounding scheme is applied.",abstractText,[0],[0]
Decentralized Submodular Maximization: Bridging Discrete and Continuous Settings,title,[0],[0]
"Some decisions are easier to make than others—for example, large, unoccluded objects are easier to recognize.",1. Introduction,[0],[0]
"Additionally, different difficult decisions may require different expertise—an avid birder may know very little about identifying cars.",1. Introduction,[0],[0]
"We hypothesize that complex decision-making tasks like visual classification can be meaningfully divided into specialized subtasks, and that a system designed to perform a complex task should first attempt to identify the subtask being presented to it, then use that information to select the most suitable algorithm for its solution.
",1. Introduction,[0],[0]
"This approach—dynamically routing signals through an inference system, based on their content—has already been incorporated into machine vision pipelines via methods such as boosting (Viola et al., 2005), coarse-to-fine cascades (Zhou et al., 2013), and random decision forests (Ho, 1995).",1. Introduction,[0],[0]
"Dynamic routing is also performed in the primate visual system: spatial information is processed somewhat separately from object identity information (Goodale &
1California Institute of Technology, Pasadena, California, USA.",1. Introduction,[0],[0]
"Correspondence to: Mason McGill <mmcgill@caltech.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"Milner, 1992), and faces and other behaviorally-relevant stimuli ellicit responses in anatomically distinct, specialized regions (Moeller et al., 2008; Kornblith et al., 2013).",1. Introduction,[0],[0]
"However, state-of-the-art artificial neural networks (ANNs) for visual inference are routed statically (Simonyan & Zisserman, 2014; He et al., 2016; Dosovitskiy et al., 2015; Newell et al., 2016); every input triggers an identical sequence of operations.
",1. Introduction,[0],[0]
"With this in mind, we propose a mechanism for introducing cascaded evaluation to arbitrary feedforward ANNs, focusing on the task of object recognition as a proof of concept.",1. Introduction,[0],[0]
"Instead of classifying images only at the final layer, every layer in the network may attempt to classify images in lowambiguity regions of its input space, passing ambiguous images forward to subsequent layers for further consideration (see Fig. 1 for an illustration).",1. Introduction,[0],[0]
"We propose three approaches to training these networks, test them on small image datasets synthesized from MNIST (LeCun et al., 1998) and CIFAR-10 (Krizhevsky & Hinton, 2009), and quantify the accuracy/efficiency trade-off that occurs when the network parameters are tuned to yield more aggressive early classification policies.",1. Introduction,[0],[0]
"Additionally, we propose and evaluate methods for appropriating regularization and optimization techniques developed for statically-routed networks.",1. Introduction,[0],[0]
"Since the late 1980s, researchers have combined artificial neural networks with decision trees in various
Deciding How to Decide: Dynamic Routing in Artificial Neural Networks
ways (Utgoff, 1989)",2. Related Work,[0],[0]
"(Sirat & Nadal, 1990).",2. Related Work,[0],[0]
"More recently, Kontschieder et al. (2015) performed joint optimization of ANN and decision tree parameters, and Bulo & Kontschieder (2014) used randomized multi-layer networks to compute decision tree split functions.
",2. Related Work,[0],[0]
"To our knowledge, the family of inference systems we discuss was first described by Denoyer & Gallinari (2014).",2. Related Work,[0],[0]
"Additionally, Bengio et al. (2015) explored dynamically skipping layers in neural networks, and Ioannou et al. (2016) explored dynamic routing in networks with equallength paths.",2. Related Work,[0],[0]
"Some recently-developed visual detection systems perform cascaded evaluation of convolutional neural network layers (Li et al., 2015; Cai et al., 2015; Girshick, 2015; Ren et al., 2015); though highly specialized for the task of visual detection, these modifications can radically improve efficiency.
",2. Related Work,[0],[0]
"While these approaches lend evidence that dynamic routing can be effective, they either ignore the cost of computation, or do not represent it explicitly, and instead use opaque heuristics to trade accuracy for efficiency.",2. Related Work,[0],[0]
"We build on this foundation by deriving training procedures from arbitrary application-provided costs of error and computation, comparing one actor-style and two critic-style strategies, and considering regularization and optimization in the context of dynamically-routed networks.",2. Related Work,[0],[0]
"In a statically-routed, feedforward artificial neural network, every layer transforms a single input feature vector into a single output feature vector.",3. Setup,[0],[0]
"The output feature vector is then used as the input to the following layer (which we’ll refer to as the current layer’s sink), if it exists, or as the ouptut of the network as a whole, if it does not.
",3. Setup,[0],[0]
We consider networks in which layers may have more than one sink.,3. Setup,[0],[0]
"In such a network, for every n-way junction j a signal reaches, the network must make a decision, dj ∈ {0..n}, such that the signal will propagate through the ith sink if and only if dj = i (this is illustrated in Fig. 2).",3. Setup,[0],[0]
"We compute dj as the argmax of the score vector sj , a learned function of the last feature vector computed before reaching j. We’ll refer to this rule for generating d from s as the inference routing policy.",3. Setup,[0],[0]
Convolutional network layers compute collections of local descriptions of the input signal.,3.1. Multipath Architectures for Convolutional Networks,[0],[0]
"It is unreasonable to expect that this kind of feature vector can explicitly encode the global information relevant to deciding how to route the entire signal (e.g., in the case of object recognition, whether the image was taken indoors, whether the image contains
dj = 0 dj = 1 Source Sink 1 Sink 0",3.1. Multipath Architectures for Convolutional Networks,[0],[0]
"Source Sink 1 Sink 0
Figure 2.",3.1. Multipath Architectures for Convolutional Networks,[0],[0]
"A 2-way junction, j. dj is an integer function of the source features.",3.1. Multipath Architectures for Convolutional Networks,[0],[0]
"When dj = 0, the signal is propagated through the top sink, and the bottom sink is inactive.",3.1. Multipath Architectures for Convolutional Networks,[0],[0]
"When dj = 1, the signal is propagated through the bottom sink, and the top sink is inactive.
an animal, or the prevalence of occlusion in the scene).
",3.1. Multipath Architectures for Convolutional Networks,[0],[0]
"To address this, instead of computing a 2-dimensional array of local features at each layer, we compute a pyramid of features (resembling the pyramids described by Ke et al. (2016)), with local descriptors at the bottom and global descriptors at the top.",3.1. Multipath Architectures for Convolutional Networks,[0],[0]
"At every junction j, the score vector sj is computed by a small routing network operating on the last-computed global descriptor.",3.1. Multipath Architectures for Convolutional Networks,[0],[0]
Our multipath architecture is illustrated in Fig. 3.,3.1. Multipath Architectures for Convolutional Networks,[0],[0]
"For a given input, network ν, and set of routing decisions d, we define the cost of performing inference:
",3.2. Balancing Accuracy and Efficiency,[0],[0]
"cinf(ν, d) = cerr(ν, d) + ccpt(ν, d), (1)
where cerr(ν, d) is the cost of the inference errors made by the network, and ccpt(ν, d) is the cost of computation.",3.2. Balancing Accuracy and Efficiency,[0],[0]
"In our experiments, unless stated otherwise, cerr is the crossentropy loss and
ccpt(ν, d) = kcptnops(ν, d), (2)
where nops(ν, d) is the number of multiply-accumulate operations performed and kcpt is a scalar hyperparameter.",3.2. Balancing Accuracy and Efficiency,[0],[0]
"This definition assumes a time- or energy-constrained system—every operation consumes roughly the same amount of time and energy, so every operation is equally expensive.",3.2. Balancing Accuracy and Efficiency,[0],[0]
ccpt may be defined differently under other constraints (e.g. memory bandwidth).,3.2. Balancing Accuracy and Efficiency,[0],[0]
"We propose three approaches to training dynamicallyrouted networks, along with complementary approaches to regularization and optimization, and a method for adapting to changes in the cost of computation.
4×4 8×8
16×16 32×32
16 chan.",4. Training,[0],[0]
16 chan.,4. Training,[0],[0]
32 chan. 32 chan.,4. Training,[0],[0]
64 chan.,4. Training,[0],[0]
64 chan.,4. Training,[0],[0]
128 chan.,4. Training,[0],[0]
"128 chan.
“Horse”
Convolution, Batch Normalization, RectificationLinear Transformation, Batch Normalization, RectificationLinear Transformation, SoftmaxLinear Transformation, Argmax“Stop” Signal“Go” Signal
RoutingSubnetworks
Figure 3.",4. Training,[0],[0]
Our multiscale convolutional architecture.,4. Training,[0],[0]
"Once a column is evaluated, the network decides whether to classify the image or evaluate subsequent columns.",4. Training,[0],[0]
"Deeper columns operate at coarser scales, but compute higher-dimensional representations at each location.",4. Training,[0],[0]
"All convolutions use 3×3 kernels, downsampling is achieved via 2×2 max pooling, and all routing layers have 16 channels.",4. Training,[0],[0]
"Since d is discrete, cinf(ν, d) cannot be minimized via gradient-based methods.",4.1. Training Strategy I: Actor Learning,[0],[0]
"However, if d is replaced by a stochastic approximation, d̂, during training, we can engineer the gradient of E[cinf(ν, d̂)] to be nonzero.",4.1. Training Strategy I: Actor Learning,[0],[0]
"We can then learn the routing parameters and classification parameters simultaneously by minimizing the loss
Lac = E[cinf(ν, d̂)].",4.1. Training Strategy I: Actor Learning,[0],[0]
"(3)
In our experiments, the training routing policy samples d̂ such that
Pr(d̂j = i) = softmax(sj/τ)i, (4)
where τ is the network “temperature”: a scalar hyperparameter that decays over the course of training, converging the training routing policy towards the inference routing policy.",4.1. Training Strategy I: Actor Learning,[0],[0]
"Alternatively, we can attempt to learn to predict the expected utility of making every routing decision.",4.2. Training Strategy II: Pragmatic Critic Learning,[0],[0]
"In this case, we minimize the loss
Lcr = E cinf(ν, d̂)",4.2. Training Strategy II: Pragmatic Critic Learning,[0],[0]
"+∑ j∈J cjure  , (5)
where J is the set of junctions encountered when making the routing decisions d̂, and cure is the utility regression error cost, defined:
cjure = kure‖sj − uj‖2, (6)
where
uij = −cinf(νij , d), (7)
kure is a scalar hyperparameter, and νij is the subnetwork consisting of the ith child of νj , and all of its descendants.",4.2. Training Strategy II: Pragmatic Critic Learning,[0],[0]
"Since we want to learn the policy indirectly (via cost prediction), d̂ is treated as constant with respect to optimization.",4.2. Training Strategy II: Pragmatic Critic Learning,[0],[0]
"To improve the stability of the loss and potentially accelerate training, we can adjust the routing utility function u such that, for every junction j, uj is independent of the routing parameters downstream of j. Instead of predicting the cost of making routing decisions given the current downstream routing policy, we can predict the cost of making routing decisions given the optimal downstream routing policy.",4.3. Training Strategy III: Optimistic Critic Learning,[0],[0]
"In this optimistic variant of the critic method,
uij = −mind′(cinf(νij , d′)).",4.3. Training Strategy III: Optimistic Critic Learning,[0],[0]
(8),4.3. Training Strategy III: Optimistic Critic Learning,[0],[0]
"Many regularization techniques involve adding a modelcomplexity term, cmod, to the loss function to influence learning, effectively imposing soft constraints upon the network parameters (Hoerl & Kennard, 1970; Rudin et al., 1992; Tibshirani, 1996).",4.4. Regularization,[0],[0]
"However, if such a term affects layers in a way that is independent of the amount of signal routed through them, it will either underconstrain frequently-used layers or overconstrain infrequently-used layers.",4.4. Regularization,[0],[0]
"To support both frequently- and infrequently-used layers, we regularize subnetworks as they are activated by d̂, instead of regularizing the entire network directly.
",4.4. Regularization,[0],[0]
"For example, to apply L2 regularization to critic networks, we define cmod:
cmod = E [ kL2
∑ w∈W w2
] , (9)
where W is the set of weights associated with the layers activated by d̂, and kL2 is a scalar hyperparameter.
",4.4. Regularization,[0],[0]
"For actor networks, we apply an extra term to control the magnitude of s, and therefore the extent to which the net explores subpotimal paths:
cmod = E kL2 ∑ w∈W w2 + kdec ∑ j∈J ‖sj‖2  , (10) where kdec is a scalar hyperparameter indicating the relative cost of decisiveness.
",4.4. Regularization,[0],[0]
cmod is added to the loss function in all of our experiments.,4.4. Regularization,[0],[0]
"Within cmod, unless stated otherwise, d̂ is treated as constant with respect to optimization.",4.4. Regularization,[0],[0]
"Throughput Variations
Both training techniques attempt to minimize the expected cost of performing inference with the network, over the training routing policy.",4.5. Adjusting Learning Rates to Compensate for,[0],[0]
"With this setup, if we use a constant learning rate for every layer in the network, then layers through which the policy routes examples more frequently will receive larger parameter updates, since they contribute more to the expected cost.
",4.5. Adjusting Learning Rates to Compensate for,[0],[0]
"To allow every layer to learn as quickly as possible, we scale the learning rate of each layer ` dynamically, by a factor α`, such that the elementwise variance of the loss gradient with respect to `’s parameters is independent of the amount of probability density routed through it.
",4.5. Adjusting Learning Rates to Compensate for,[0],[0]
"To derive α`, we consider an alternative routing policy, d∗` , that routes all signals though `, then routes through subse-
quent layers based on d̂. With this policy, at every training interation, mini-batch stochastic gradient descent shifts the parameters associated with layer ` by a vector δ∗` , defined:
δ∗` = −λ ∑ i gi`, (11)
where λ is the global learning rate and gi` is the gradient of the loss with respect to the parameters in `, for training example i, under d∗` .",4.5. Adjusting Learning Rates to Compensate for,[0],[0]
"Analogously, the scaled parameter adjustment under d̂ can be written
δ` = −α`λ ∑ i pi`g i `, (12)
where pi` is the probability with which d̂ routes example i through `.
",4.5. Adjusting Learning Rates to Compensate for,[0],[0]
"We want to select α` such that
Var(δ`) = Var(δ ∗ ` ).",4.5. Adjusting Learning Rates to Compensate for,[0],[0]
"(13)
Substituting the definitions of δ` and δ∗` ,
Var ( α` ∑ i pi`g i ` ) =",4.5. Adjusting Learning Rates to Compensate for,[0],[0]
Var (∑ i gi` ) .,4.5. Adjusting Learning Rates to Compensate for,[0],[0]
"(14)
Since every gi` is sampled independently, we can rewrite this equation:
nexv`α 2 `‖p`‖2 = nexv`, (15)
where nex is the number of training examples in the minibatch and v` is the elementwise variance of gi`, for any i (since every example is sampled via the same mechanism).",4.5. Adjusting Learning Rates to Compensate for,[0],[0]
"We can now show that
α` = ‖p`‖−1.",4.5. Adjusting Learning Rates to Compensate for,[0],[0]
"(16)
",4.5. Adjusting Learning Rates to Compensate for,[0],[0]
"So, for every layer `, we can scale the learning rate by ‖p`‖−1, and the variance of the weight updates will be similar thoughout the network.",4.5. Adjusting Learning Rates to Compensate for,[0],[0]
"We use this technique, unless otherwise specified, in all of our experiments.",4.5. Adjusting Learning Rates to Compensate for,[0],[0]
We may want a single network to perform well in situations with various degrees of computational resource scarcity (e.g. computation may be more expensive when a device battery is low).,4.6. Responding to Changes in the Cost of Computation,[0],[0]
"To make the network’s routing behavior responsive to a dynamic ccpt, we can concatenate ccpt’s known
parameters—in our case, {kcpt}—to the input of every routing subnetwork, to allow them to modulate the routing policy.",4.6. Responding to Changes in the Cost of Computation,[0],[0]
"To match the scale of the image features and facilitate optimization, we express kcpt in units of cost per tenmillion operations.",4.6. Responding to Changes in the Cost of Computation,[0],[0]
"In all of our experiments, we use a mini-batch size, nex, of 128, and run 80,000 training iterations.",4.7. Hyperparameters,[0],[0]
We perform stochastic gradient descent with initial learning rate 0.1/nex and momentum 0.9.,4.7. Hyperparameters,[0],[0]
"The learning rate decays continuously with a half-life of 10,000 iterations.
",4.7. Hyperparameters,[0],[0]
"The weights of the final layers of routing networks are zero-initialized, and we initialize all other weights using the Xavier initialization method (Glorot & Bengio, 2010).",4.7. Hyperparameters,[0],[0]
All biases are zero-initialized.,4.7. Hyperparameters,[0],[0]
"We perform batch normalization (Ioffe & Szegedy, 2015) before every rectification operation, with an of 1×10−6, and an exponential moving average decay constant of 0.9.
",4.7. Hyperparameters,[0],[0]
"τ is initialized to 1.0 for actor networks and 0.1 for critic networks, and decays with a half-life of 10,000 iterations.",4.7. Hyperparameters,[0],[0]
"kdec = 0.01, kure = 0.001, and kL2 = 1 × 10−4.",4.7. Hyperparameters,[0],[0]
"We selected these values (for τ , kdec, kure, and kL2) by exploring the hyperparameter space logarithmically, by powers of 10, training and evaluating on the hybrid MNIST/CIFAR-10 dataset (described in section 5.1).",4.7. Hyperparameters,[0],[0]
"At a coarse level, these values are locally optimal—multiplying or dividing any of them by 10 will not improve performance.",4.7. Hyperparameters,[0],[0]
"We augment our data using an approach that is popular for use with CIFAR-10 (Lin et al., 2013) (Srivastava et al., 2015) (Clevert et al., 2015).",4.8. Data Augmentation,[0],[0]
"We augment each image by applying vertical and horizontal shifts sampled uniformly from the range [-4px,4px], and, if the image is from CIFAR-10, flipping it horizontally with probability 0.5.",4.8. Data Augmentation,[0],[0]
We fill blank pixels introduced by shifts with the mean color of the image (after gamma-decoding).,4.8. Data Augmentation,[0],[0]
"We compare approaches to dynamic routing by training 153 networks to classify small images, varying the policy-learning strategy, regularization strategy, optimization strategy, architecture, cost of computation, and details of the task.",5. Experiments,[0],[0]
"The results of these experiments are reported in
Fig. 5–10.",5. Experiments,[0],[0]
Our code is available via GitLab.,5. Experiments,[0],[0]
"To compare routing strategies in the context of a simple dataset with a high degree of difficulty variation, we train
networks to classify images from a small-image dataset synthesized from MNIST (LeCun et al., 1998) and CIFAR10 (Krizhevsky & Hinton, 2009) (see Fig. 4).",5.1. Comparing Policy-Learning Strategies,[0],[0]
"Our dataset includes the classes “0”, “1”, “2”, “3”, and “4” from MNIST and “airplane”, “automobile”, “deer”, “horse”, and “frog” from CIFAR-10 (see Fig. 4).",5.1. Comparing Policy-Learning Strategies,[0],[0]
"The images from MNIST are resized to match the scale of images from CIFAR-10 (32×32), via linear interpolation, and are colormodulated to make them more difficult to trivially distinguish from CIFAR-10 images (MNIST is a grayscale dataset).
",5.1. Comparing Policy-Learning Strategies,[0],[0]
"For a given computational budget, dynamically-routed networks achieve higher accuracy rates than architecturematched statically-routed baselines (networks composed of the first n columns of the architecture illustrated in Fig. 3, for n ∈ {1..8}).",5.1. Comparing Policy-Learning Strategies,[0],[0]
"Additionally, dynamically-routed networks tend to avoid routing data along deep paths at the beginning of training (see Fig. 8).",5.1. Comparing Policy-Learning Strategies,[0],[0]
"This is possibly because the error surfaces of deeper networks are more complicated, or because deeper paths are less stable—changing the parameters in any component layer to better classify images routed along other, overlapping paths may decrease performance.",5.1. Comparing Policy-Learning Strategies,[0],[0]
"Whatever the mechanism, this tendency to initially find simpler solutions seems to prevent some of the overfitting that occurs with 7- and 8-layer statically-routed networks.
",5.1. Comparing Policy-Learning Strategies,[0],[0]
"Compared to other dynamically-routed networks, optimistic critic networks perform poorly, possibly because optimal routers are a poor approximation for our small, lowcapacity router networks.",5.1. Comparing Policy-Learning Strategies,[0],[0]
"Actor networks perform better than critic networks, possibly because critic networks are forced to learn a potentially-intractable auxilliary task (i.e. it’s easier to decide who to call to fix your printer than it is to predict exactly how quickly and effectively everyone you know would fix it).",5.1. Comparing Policy-Learning Strategies,[0],[0]
"Actor networks also consistently achieve higher peak accuracy rates than comparable statically-routed networks, across experiments.
4 8
40k
80k
E p
o ch
In d
ex
kcpt = 0
4 8
kcpt = 1×10−9
4 8
kcpt = 2×10−9
4 8
kcpt = 4×10−9
0.0
0.2
0.4
0.6
0.8
1.0
Layer Index
Figure 8.",5.1. Comparing Policy-Learning Strategies,[0],[0]
Dataflow over the course of training.,5.1. Comparing Policy-Learning Strategies,[0],[0]
"The heatmaps illustrate the fraction of validation images classified at every terminal node in the bottom four networks in Fig. 6, over the course of training.
",5.1. Comparing Policy-Learning Strategies,[0],[0]
"Although actor networks may be more performant, critic networks are more flexible.",5.1. Comparing Policy-Learning Strategies,[0],[0]
"Since critic networks don’t require E[cinf(ν, d̂)] to be a differentiable function of d̂, they can be trained by sampling d̂, saving memory, and they support a wider selection of training routing policies (e.g. -greedy) and cinf definitions.",5.1. Comparing Policy-Learning Strategies,[0],[0]
"In addition to training the standard critic networks, we train networks using a variant of the pragmatic critic training policy, in which we replace
the cross-entropy error in the cure term with the classification error.",5.1. Comparing Policy-Learning Strategies,[0],[0]
"Although these networks do not perform as well as the original pragmatic critic networks, they still outperform comparable statically-routed networks.",5.1. Comparing Policy-Learning Strategies,[0],[0]
"Based on our experiments with the hybrid dataset, regularizing d̂, as described in section 4.4, discourages networks from routing data along deep paths, reducing peak accuracy.",5.2. Comparing Regularization Strategies,[0],[0]
"Additionally, some mechanism for encouraging exploration (in our case, a nonzero kdec) appears to be necessary to train effective actor networks.",5.2. Comparing Regularization Strategies,[0],[0]
"Throughput-adjusting the learning rates (TALR), as described in section 4.5, improves the hybrid dataset performance of both actor and critic networks in computationalresource-abundant, high-accuracy contexts.",5.3. Comparing Optimization Strategies,[0],[0]
"For a given computational budget, architectures with both 2- and 3-way junctions have a higher capacity than subtrees with only 2-way junctions.",5.4. Comparing Architectures,[0],[0]
"On the hybrid dataset, under tight computational constraints, we find that trees with higher degrees of branching achieve higher accuracy rates.",5.4. Comparing Architectures,[0],[0]
"Unconstrained, however, they are prone to overfitting.
",5.4. Comparing Architectures,[0],[0]
"In dynamically-routed networks, early classification layers tend to have high accuracy rates, pushing difficult decisions downstream.",5.4. Comparing Architectures,[0],[0]
"Even without energy contraints, terminal layers specialize in detecting instances of certain classes of images.",5.4. Comparing Architectures,[0],[0]
These classes are usually related (they either all come from MNIST or all come from CIFAR-10.),5.4. Comparing Architectures,[0],[0]
"In networks with both 2- and 3-way junctions, branches specialize to an even greater extent.",5.4. Comparing Architectures,[0],[0]
(See Fig. 6 and 7.),5.4. Comparing Architectures,[0],[0]
"We train a single actor network to classify images from the hybrid datset under various levels of computational constraints, using the approach described in section 4.6, sampling kcpt randomly from the set mentioned in Fig. 5 for each training example.",5.5. Comparing Specialized and Adaptive Networks,[0],[0]
"This network performs comparably to a collection of 8 actor nets trained with various static values of kcpt, over a significant, central region of the accuracy/efficiency curve, with an 8-fold reduction in memory consumption and training time.",5.5. Comparing Specialized and Adaptive Networks,[0],[0]
"To probe the effect of the inference task’s difficulty distribution on the performance of dynamically-routed net-
works, we train networks to classify images from CIFAR10, adjusting the classification task to vary the frequency of difficult decisions (see Fig. 9).",5.6. Exploring the Effects of the Decision Difficulty Distribution,[0],[0]
"We call these variants CIFAR-2—labelling images as “horse” or “other”— and CIFAR-5—labelling images as “cat”, “dog”, “deer”, “horse”, or “other”.",5.6. Exploring the Effects of the Decision Difficulty Distribution,[0],[0]
"In this experiment, we compare actor networks (the best-performing networks from the first set of experiments) to architecture-matched statically-routed networks.
",5.6. Exploring the Effects of the Decision Difficulty Distribution,[0],[0]
"We find that dynamic routing is more beneficial when the task involves many low-difficulty decisions, allowing the network to route more data along shorter paths.",5.6. Exploring the Effects of the Decision Difficulty Distribution,[0],[0]
"While dynamic routing offers only a slight advantage on CIFAR-10, dynamically-routed networks achieve a higher peak accuracy rate on CIFAR-2 than statically-routed networks, at a third of the computational cost.",5.6. Exploring the Effects of the Decision Difficulty Distribution,[0],[0]
"To test whether dynamic routing is advantageous in highercapacity settings, we train actor networks and architecturematched statically-routed networks to classify images from CIFAR-10, varying the width of the networks (see Fig. 10).",5.7. Exploring the Effects of Model Capacity,[0],[0]
"Increasing the model capacity either increases or does not affect the relative advantage of dynamically-routed networks, suggesting that our approach is applicable to more complicated tasks.",5.7. Exploring the Effects of Model Capacity,[0],[0]
"Our experiments suggest that dynamically-routed networks trained under mild computational constraints can operate 2–3 times more efficiently than comparable staticallyrouted networks, without sacrificing performance.",6. Discussion,[0],[0]
"Additionally, despite their higher capacity, dynamically-routed networks may be less prone to overfitting.
",6. Discussion,[0],[0]
"When designing a multipath architecture, we suggest supporting early decision-making wherever possible, since cheap, simple routing networks seem to work well.",6. Discussion,[0],[0]
"In convolutional architectures, pyramidal layers appear to be reasonable sites for branching.
",6. Discussion,[0],[0]
The actor strategy described in section 4.1 is generally an effective way to learn a routing policy.,6. Discussion,[0],[0]
"However, the pragmatic critic strategy described in section 4.2 may be better suited for very large networks (trained via decision sampling to conserve memory) or networks designed for applications with nonsmooth cost-of-inference functions—e.g. one in which kcpt has units errors/operation.",6. Discussion,[0],[0]
"Adjusting learning rates to compensate for throughput variations, as described in section 4.5, may improve the performance of deep networks.",6. Discussion,[0],[0]
"If the cost of computation is dynamic, a single network, trained with the procedure described in section 5.5, may still be sufficient.
",6. Discussion,[0],[0]
"While we test our approach on tasks with some degree of difficulty variation, it is possible that dynamic routing is even more advantageous when performing more complex tasks.",6. Discussion,[0],[0]
"For example, video annotation may require specialized modules to recognize locations, objects, faces, human actions, and other scene components or attributes, but having every module constantly operating may be extremely inefficient.",6. Discussion,[0],[0]
"A dynamic routing policy could fuse these modules, allowing them to share common components, and activate specialized components as necessary.
",6. Discussion,[0],[0]
Another interesting topic for future research is growing and shrinking dynamically-routed networks during training.,6. Discussion,[0],[0]
"With such a network, it is not necessary to specify an architecture.",6. Discussion,[0],[0]
"The network will instead take shape over the course of training, as computational contraints, memory contraints, and the data dictate.",6. Discussion,[0],[0]
This work was funded by a generous grant from Google Inc.,Acknowledgements,[0],[0]
"We would also like to thank Krzysztof Chalupka, Cristina Segalin, and Oisin Mac Aodha for their thoughtful comments.",Acknowledgements,[0],[0]
We propose and systematically evaluate three strategies for training dynamically-routed artificial neural networks: graphs of learned transformations through which different input signals may take different paths.,abstractText,[0],[0]
"Though some approaches have advantages over others, the resulting networks are often qualitatively similar.",abstractText,[0],[0]
"We find that, in dynamically-routed networks trained to classify images, layers and branches become specialized to process distinct categories of images.",abstractText,[0],[0]
"Additionally, given a fixed computational budget, dynamically-routed networks tend to perform better than comparable staticallyrouted networks.",abstractText,[0],[0]
Deciding How to Decide:  Dynamic Routing in Artificial Neural Networks,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 869–874 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
869",text,[0],[0]
Breaking substitution ciphers recovers the plaintext from a ciphertext that uses a 1:1 or homophonic cipher key.,1 Introduction,[0],[0]
"Previous work using pretrained language models (LMs) for decipherment use n-gram LMs (Ravi and Knight, 2011; Nuhn et al., 2013).",1 Introduction,[0],[0]
"Some methods use the ExpectationMaximization (EM) algorithm (Knight et al., 2006) while most state-of-the-art approaches for decipherment of 1:1 and homophonic substitution ciphers use beam search and rely on the clever use of n-gram LMs (Nuhn et al., 2014; Hauer et al., 2014).",1 Introduction,[0],[0]
"Neural LMs globally score the entire candidate plaintext sequence (Mikolov et al., 2010).",1 Introduction,[0],[0]
"However, using a neural LM for decipherment is not trivial because scoring the entire candidate partially deciphered plaintext is computationally challenging.",1 Introduction,[0],[0]
We solve both of these problems in this paper and provide an improved beam search based decipherment algorithm for homophonic ciphers that exploits pre-trained neural LMs for the first time.,1 Introduction,[0],[0]
We use the notation from Nuhn et al. (2013).,2 Decipherment Model,[0],[0]
Ciphertext fN1 = f1..fi..fN and plaintext e N 1 = e1..ei..eN consist of vocabularies,2 Decipherment Model,[0],[0]
fi ∈ Vf and ei ∈ Ve respectively.,2 Decipherment Model,[0],[0]
The beginning tokens in the ciphertext (f0) and plaintext (e0) are set to “$” denoting the beginning of a sentence.,2 Decipherment Model,[0],[0]
The substitutions are represented by a function φ :,2 Decipherment Model,[0],[0]
Vf → Ve such that 1:1 substitutions are bijective while homophonic substitutions are general.,2 Decipherment Model,[0],[0]
"A cipher function φwhich does not have every φ(f) fixed is called a partial cipher function (Corlett and Penn, 2010).",2 Decipherment Model,[0],[0]
The number of fs that are fixed in φ is given by its cardinality. φ′,2 Decipherment Model,[0],[0]
"is called an extension of φ, if f is fixed in φ′ such that δ(φ′(f), φ(f))",2 Decipherment Model,[0],[0]
yields true ∀f ∈,2 Decipherment Model,[0],[0]
Vf which are already fixed in φ where δ is Kronecker delta.,2 Decipherment Model,[0],[0]
"Decipherment is then the task of finding the φ for which the probability of the deciphered text is maximized.
φ̂ = argmax φ p(φ(f1)...φ(fN ))",2 Decipherment Model,[0],[0]
"(1)
where p(.) is the language model (LM).",2 Decipherment Model,[0],[0]
"Finding this argmax is solved using a beam search algorithm (Nuhn et al., 2013) which incrementally finds the most likely substitutions using the language model scores as the ranking.",2 Decipherment Model,[0],[0]
The advantage of a neural LM is that it can be used to score the entire candidate plaintext for a hypothesized partial decipherment.,2.1 Neural Language Model,[0],[0]
"In this work, we use a state of the art byte (character) level neural LM using a multiplicative LSTM (Radford et al., 2017).
Consider a sequence S = w1, w2, w3, ..., wN .",2.1 Neural Language Model,[0],[0]
"The LM score of S is SCORE(S):
P (S) = P (w1, w2, w3, ..., wN )
P (S) = N∏ i=1",2.1 Neural Language Model,[0],[0]
"P (wi | w1, w2, ..., wi−1))
SCORE(S) =",2.1 Neural Language Model,[0],[0]
− N∑ i=1,2.1 Neural Language Model,[0],[0]
log(P,2.1 Neural Language Model,[0],[0]
"(wi | w<i))
(2)",2.1 Neural Language Model,[0],[0]
"Algorithm 1 is the beam search algorithm (Nuhn et al., 2013, 2014) for solving substitution ciphers.",2.2 Beam Search,[0],[0]
It monitors all partial hypotheses in lists Hs and Ht based on their quality.,2.2 Beam Search,[0],[0]
"As the search progresses, the partial hypotheses are extended, scored with SCORE and appended to Ht.",2.2 Beam Search,[0],[0]
EXT LIMITS determines which extensions should be allowed and EXT ORDER picks the next cipher symbol for extension.,2.2 Beam Search,[0],[0]
The search continues after pruning:,2.2 Beam Search,[0],[0]
Hs ← HISTOGRAM_PRUNE(Ht).,2.2 Beam Search,[0],[0]
"We augment this algorithm by updating the SCORE function with a neural LM.
Algorithm 1 Beam Search for Decipherment 1: function (BEAM SEARCH (EXT ORDER, EXT LIM-
ITS)) 2: initialize sets Hs, Ht 3: CARDINALITY = 0 4: Hs.ADD((∅,0)) 5: while CARDINALITY < |Vf",2.2 Beam Search,[0],[0]
| do 6: f = EXT ORDER[CARDINALITY] 7: for all φ ∈,2.2 Beam Search,[0],[0]
Hs do 8: for all e ∈,2.2 Beam Search,[0],[0]
"Ve do 9: φ’ := φ ∪ {(e, f)}
10: if EXT LIMITS(φ’)",2.2 Beam Search,[0],[0]
"then 11: Ht.ADD(φ’,SCORE(φ’)) 12: HISTOGRAM PRUNE(Ht) 13: CARDINALITY = CARDINALITY",2.2 Beam Search,[0],[0]
"+ 1 14: Hs = Ht 15: Ht.CLEAR() 16: return WINNER(Hs)
3 Score Estimation (SCORE)
",2.2 Beam Search,[0],[0]
Score estimation evaluates the quality of the partial hypotheses φ.,2.2 Beam Search,[0],[0]
"Using the example from Nuhn et al. (2014), consider the vocabularies Ve = {a, b, c, d} and Vf = {A,B,C,D}, extension order (B,A,C,D), and ciphertext $ ABDDCABCDADCABDC $.",2.2 Beam Search,[0],[0]
"Let φ = {(a,A), (b, B))} be the partial hypothesis.",2.2 Beam Search,[0],[0]
Then SCORE(φ) scores this hypothesized partial decipherment (only A and B are converted to plaintext) using a pre-trained language model in the hypothesized plaintext language.,2.2 Beam Search,[0],[0]
The initial rest cost estimator introduced by Nuhn et al. nuhnbeam computes the score of hypotheses only based on partially deciphered text that builds a shard of n adjacent solved symbols.,3.1 Baseline,[0],[0]
"As a heuristic, n-grams which still consist of unsolved cipher-symbols are assigned a trivial estimate of probability 1.",3.1 Baseline,[0],[0]
"An improved version of rest cost es-
timation (Nuhn et al., 2014) consults lower order n-grams to score each position.",3.1 Baseline,[0],[0]
"The baseline scoring method greatly relies on local context, i.e. the estimation is strictly based on partial character sequences.",3.2 Global Rest Cost Estimation,[0],[0]
"Since this depends solely on the n-gram LM, the true conditional probability under Markov assumption is not modeled and, therefore, context dependency beyond the window of (n− 1) is ignored.",3.2 Global Rest Cost Estimation,[0],[0]
"Thus, attempting to utilize a higher amount of context can lower the probability of some tokens resulting in poor scores.
",3.2 Global Rest Cost Estimation,[0],[0]
We address this issue with a new improved version of the rest cost estimator by supplementing the partial decipherment φ(fN1 ) with predicted plaintext text symbols using our neural language model (NLM).,3.2 Global Rest Cost Estimation,[0],[0]
"Applying φ = {(a,A), (b, B))} to the ciphertext above, we get the following partial hypothesis: φ(fN1 ) = $a1b2...a6b7..a10..a13b14..$ We introduce a scoring function that is able to score the entire plaintext including the missing plaintext symbols.",3.2 Global Rest Cost Estimation,[0],[0]
"First, we sample1 the plaintext symbols from the NLM at all locations depending on the deciphered tokens from the partial hypothesis φ such that these tokens maintain their respective positions in the sequence, and at the same time are sampled from the neural LM to fit (probabilistically) in this context.",3.2 Global Rest Cost Estimation,[0],[0]
"Next, we determine the probability of the entire sequence including the scores of sampled plaintext as our rest cost estimate.
",3.2 Global Rest Cost Estimation,[0],[0]
"NLM
In our running example, this would yield a score estimation of the partial decipherment, φ(fN1 ) :
φ(fN1 ) = $ a1b2d3c4c5a6b7c8d9a10d11d12a13b14d15c16 $
Thus, the neural LM is used to predict the score of the full sequence.",3.2 Global Rest Cost Estimation,[0],[0]
"This method of global scoring evaluates each candidate partial decipherment by scoring the entire message, augmented by the sam-
1The char-level sampling is done incrementally from left to right to generate a sequence that contains the deciphered tokens from φ at the exact locations they occur in the above φ(fN1 ).",3.2 Global Rest Cost Estimation,[0],[0]
"If the LM prediction contradicts the hypothesized decipherment we stop sampling and start from the next character.
pled plaintext symbols from the NLM.",3.2 Global Rest Cost Estimation,[0],[0]
"Since more terms participate in the rest cost estimation with global context, we use the plaintext LM to provide us with a better rest cost in the beam search.",3.2 Global Rest Cost Estimation,[0],[0]
"Alignment by frequency similarity (Yarowsky and Wicentowski, 2000) assumes that two forms belong to the same lemma when their relative frequency fits the expected distribution.",3.3 Frequency Matching Heuristic,[0],[0]
"We use this heuristic to augment the score estimation (SCORE):
FMH(φ′) = ∣∣∣∣log(ν(f)ν(e) )",3.3 Frequency Matching Heuristic,[0],[0]
∣∣∣∣ f ∈,3.3 Frequency Matching Heuristic,[0],[0]
"Vf , e ∈",3.3 Frequency Matching Heuristic,[0],[0]
"Ve (3) ν(f) is the percentage relative frequency of the ciphertext symbol f , while ν(e) is the percentage relative frequency of the plaintext token e in the plaintext language model.",3.3 Frequency Matching Heuristic,[0],[0]
"The closer this value to 0, the more likely it is that f is mapped to e.
Thus given a φ with the SCORE(φ), the extension φ′",3.3 Frequency Matching Heuristic,[0],[0]
(Algo. 1) is scored as: SCORE(φ′) = SCORE(φ) + NEW(φ′)− FMH(φ′) (4) where NEW is the score for symbols that have been newly fixed in φ′ while extending φ to φ′.,3.3 Frequency Matching Heuristic,[0],[0]
Our experimental evaluations show that the global rest cost estimator and the frequency matching heuristic contribute positively towards the accuracy of different ciphertexts.,3.3 Frequency Matching Heuristic,[0],[0]
"We carry out 2 sets of experiments: one on letter based 1:1, and another on homophonic substitution ciphers.",4 Experimental Evaluation,[0],[0]
"We report Symbol Error Rate (SER) which is the fraction of characters in the deciphered text that are incorrect.
",4 Experimental Evaluation,[0],[0]
"The character NLM uses a single layer multiplicative LSTM (mLSTM) (Radford et al., 2017) with 4096 units.",4 Experimental Evaluation,[0],[0]
The model was trained for a single epoch on mini-batches of 128 subsequences of length 256 for a total of 1 million weight updates.,4 Experimental Evaluation,[0],[0]
"States were initialized to zero at the beginning of each data shard and persisted across updates to simulate full-backprop and allow for the forward propagation of information outside of a given sub-
sequence.",4 Experimental Evaluation,[0],[0]
In all the experiments we use a character NLM trained on English Gigaword corpus augmented with a short corpus of plaintext letters of about 2000 words authored by the Zodiac killer2.,4 Experimental Evaluation,[0],[0]
"In this experiment we use a synthetic 1:1 letter substitution cipher dataset following Ravi and Knight (2008), Nuhn et al. (2013) and Hauer et al. (2014).",4.1 1:1 Substitution Ciphers,[0],[0]
"The text is from English Wikipedia articles about history3, preprocessed by stripping the text of all images, tables, then lower-casing all characters, and removing all non-alphabetic and non-space characters.",4.1 1:1 Substitution Ciphers,[0],[0]
"We create 50 cryptograms for each length 16, 32, 64, 128 and 256 using a random Caesar-cipher 1:1 substitution.
",4.1 1:1 Substitution Ciphers,[0],[0]
"2https://en.wikisource.org/wiki/Zodiac Killer letters 3http://en.wikipedia.org/wiki/History
Fig 1 plots the results of our method for cipher lengths of 16, 32, 64, 128 and 256 alongside Beam 6-gram (the best performing model) model (Nuhn et al., 2013)",4.1 1:1 Substitution Ciphers,[0],[0]
"Zodiac-408, a homophonic cipher, is commonly used to evaluate decipherment algorithms.
",4.2 An Easy Cipher: Zodiac-408,[0],[0]
"Our neural LM model with global rest cost estimation and frequency matching heuristic with a beam size of 1M has SER of 1.2% compared to the beam search algorithm (Nuhn et al., 2013) with beam size of 10M with a 6-gram LM which gives an SER of 2%.",4.2 An Easy Cipher: Zodiac-408,[0],[0]
"The improved beam search (Nuhn et al., 2014) with an 8-gram LM, however, gets 52 out of 54 mappings correct on the Zodiac-408 cipher.",4.2 An Easy Cipher: Zodiac-408,[0],[0]
Part 2 of the Beale Cipher is a more challenging homophonic cipher because of a much larger search space of solutions.,4.3 A Hard Cipher: Beale Pt 2,[0],[0]
"Nunh et al. (2014) were the first to automatically decipher this Beale Cipher.
",4.3 A Hard Cipher: Beale Pt 2,[0],[0]
"With an error of 5% with beam size of 1M vs 5.4% with 8-gram LM and a pruning size of 10M, our system outperforms the state of the art (Nuhn et al., 2014) on this task.
!",4.3 A Hard Cipher: Beale Pt 2,[0],[0]
1,4.3 A Hard Cipher: Beale Pt 2,[0],[0]
"Automatic decipherment for substitution ciphers started with dictionary attacks (Hart, 1994; Jakobsen, 1995; Olson, 2007).",5 Related Work,[0],[0]
Ravi and Knight (2008) frame the decipherment problem as an integer linear programming (ILP) problem.,5 Related Work,[0],[0]
Knight et al. (2006) use an HMM-based EM algorithm for solving a variety of decipherment problems.,5 Related Work,[0],[0]
"Ravi and Knight (2011) extend the HMM-based EM approach with a Bayesian approach, and report the
first automatic decipherment of the Zodiac-408 cipher.
",5 Related Work,[0],[0]
Berg-Kirkpatrick and Klein (2013) show that a large number of random restarts can help the EM approach.,5 Related Work,[0],[0]
Corlett and Penn (2010) presented an efficient A* search algorithm to solve letter substitution ciphers.,5 Related Work,[0],[0]
Nuhn et al. (2013) produce better results in faster time compared to ILP and EM-based decipherment methods by employing a higher order language model and an iterative beam search algorithm.,5 Related Work,[0],[0]
Nuhn et al. (2014) present various improvements to the beam search algorithm in Nuhn et al. (2013) including improved rest cost estimation and an optimized strategy for ordering decipherment of the cipher symbols.,5 Related Work,[0],[0]
Hauer et al. (2014) propose a novel approach for solving mono-alphabetic substitution ciphers which combines character-level and word-level language model.,5 Related Work,[0],[0]
"They formulate decipherment as a tree search problem, and use Monte Carlo Tree Search (MCTS) as an alternative to beam search.",5 Related Work,[0],[0]
"Their approach is the best for short ciphers.
",5 Related Work,[0],[0]
Greydanus (2017) frames the decryption process as a sequence-to-sequence translation task and uses a deep LSTM-based model to learn the decryption algorithms for three polyalphabetic ciphers including the Enigma cipher.,5 Related Work,[0],[0]
"However, this approach needs supervision compared to our approach which uses a pre-trained neural LM.",5 Related Work,[0],[0]
Gomez et al. (2018) (CipherGAN) use a generative adversarial network to learn the mapping between the learned letter embedding distributions in the ciphertext and plaintext.,5 Related Work,[0],[0]
They apply this approach to shift ciphers (including Vigenère ciphers).,5 Related Work,[0],[0]
Their approach cannot be extended to homophonic ciphers and full message neural LMs as in our work.,5 Related Work,[0],[0]
"This paper presents, to our knowledge, the first application of large pre-trained neural LMs to the decipherment problem.",6 Conclusion,[0],[0]
We modify the beam search algorithm for decipherment from Nuhn et al. (2013; 2014) and extend it to use global scoring of the plaintext message using neural LMs.,6 Conclusion,[0],[0]
To enable full plaintext scoring we use the neural LM to sample plaintext characters which reduces the beam size required.,6 Conclusion,[0],[0]
For challenging ciphers such as Beale Pt 2 we obtain lower error rates with smaller beam sizes when compared to the state of the art in decipherment for such ciphers.,6 Conclusion,[0],[0]
We would like to thank the anonymous reviewers for their helpful remarks.,Acknowledgments,[0],[0]
The research was also partially supported by the Natural Sciences and Engineering Research Council of Canada grants NSERC RGPIN-2018-06437 and RGPAS-2018522574 and a Department of National Defence (DND) and NSERC grant DGDND-2018-00025 to the third author.,Acknowledgments,[0],[0]
Decipherment of homophonic substitution ciphers using language models (LMs) is a wellstudied task in NLP.,abstractText,[0],[0]
Previous work in this topic scores short local spans of possible plaintext decipherments using n-gram LMs.,abstractText,[0],[0]
The most widely used technique is the use of beam search with n-gram LMs proposed by Nuhn et al. (2013).,abstractText,[0],[0]
We propose a beam search algorithm that scores the entire candidate plaintext at each step of the decipherment using a neural LM.,abstractText,[0],[0]
We augment beam search with a novel rest cost estimation that exploits the prediction power of a neural LM.,abstractText,[0],[0]
We compare against the state of the art n-gram based methods on many different decipherment tasks.,abstractText,[0],[0]
On challenging ciphers such as the Beale cipher we provide significantly better error rates with much smaller beam sizes.,abstractText,[0],[0]
Decipherment of Substitution Ciphers with Neural Language Models,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 22–32 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
22",text,[0],[0]
"Motivation: Modern media (e.g., news feeds, microblogs, etc.) exhibit an increasing fraction of misleading and manipulative content, from questionable claims and “alternative facts” to completely faked news.",1 Introduction,[0],[0]
The media landscape is becoming a twilight zone and battleground.,1 Introduction,[0],[0]
"This societal challenge has led to the rise of fact-checking and debunking websites, such as Snopes.com and PolitiFact.com, where people research claims, manually assess their credibility, and present their verdict along with evidence (e.g., background articles, quotations, etc.).",1 Introduction,[0],[0]
"However, this manual verification is time-consuming.",1 Introduction,[0],[0]
"To keep up with the scale and speed at which misinformation spreads, we need tools to automate this debunking process.
",1 Introduction,[0],[0]
State of the Art and Limitations:,1 Introduction,[0],[0]
"Prior work on “truth discovery” (see Li et al. (2016) for survey)1 largely focused on structured facts, typically in the form of subject-predicate-object triples, or on social media platforms like Twitter, Sina Weibo, etc.",1 Introduction,[0],[0]
"Recently, methods have been proposed to assess the credibility of claims in natural language form (Popat et al., 2017; Rashkin et al., 2017; Wang, 2017), such as news headlines, quotes from speeches, blog posts, etc.
",1 Introduction,[0],[0]
The methods geared for general text input address the problem in different ways.,1 Introduction,[0],[0]
"On the one hand, methods like Rashkin et al. (2017); Wang (2017) train neural networks on labeled claims from sites like PolitiFact.com, providing credibility assessments without any explicit feature modeling.",1 Introduction,[0],[0]
"However, they use only the text of questionable claims and no external evidence or interactions that provide limited context for credibility analysis.",1 Introduction,[0],[0]
These approaches also do not offer any explanation of their verdicts.,1 Introduction,[0],[0]
"On the other hand, Popat et al. (2017) considers external evidence in the form of other articles (retrieved from the Web) that confirm or refute a claim, and jointly assesses the language style (using subjectivity lexicons), the trustworthiness of the sources, and the credibility of the claim.",1 Introduction,[0],[0]
This is achieved via a pipeline of supervised classifiers.,1 Introduction,[0],[0]
"On the upside, this method generates user-interpretable explanations by pointing to informative snippets of evidence articles.",1 Introduction,[0],[0]
"On the downside, it requires substantial feature modeling and rich lexicons to detect bias and subjectivity in the language style.",1 Introduction,[0],[0]
"Approach and Contribution: To overcome the limitations of the prior works, we present DeClarE2, an end-to-end neural network model for assessing and explaining the credibility of arbi-
1As fully objective and unarguable truth is often elusive or ill-defined, we use the term credibility rather than “truth”.
",1 Introduction,[0],[0]
"2Debunking Claims with Interpretable Evidence
trary claims in natural-language text form.",1 Introduction,[0],[0]
Our approach combines the best of both families of prior methods.,1 Introduction,[0],[0]
"Similar to Popat et al. (2017), DeClarE incorporates external evidence or counterevidence from the Web as well as signals from the language style and the trustworthiness of the underlying sources.",1 Introduction,[0],[0]
"However, our method does not require any feature engineering, lexicons, or other manual intervention.",1 Introduction,[0],[0]
"Rashkin et al. (2017); Wang (2017) also develop an end-to-end model, but DeClarE goes far beyond in terms of considering external evidence and joint interactions between several factors, and also in its ability to generate userinterpretable explanations in addition to highly accurate assessments.",1 Introduction,[0],[0]
"For example, given the natural-language input claim “the gun epidemic is the leading cause of death of young AfricanAmerican men, more than the next nine causes put together” by Hillary Clinton, DeClarE draws on evidence from the Web to arrive at its verdict credible, and returns annotated snippets like the one in Table 6 as explanation.",1 Introduction,[0],[0]
"These snippets, which contain evidence in the form of statistics and assertions, are automatically extracted from web articles from sources of varying credibility.
",1 Introduction,[0],[0]
"Given an input claim, DeClarE searches for web articles related to the claim.",1 Introduction,[0],[0]
"It considers the context of the claim via word embeddings and the (language of) web articles captured via a bidirectional LSTM (biLSTM), while using an attention mechanism to focus on parts of the articles according to their relevance to the claim.",1 Introduction,[0],[0]
"DeClarE then aggregates all the information about claim source, web article contexts, attention weights, and trustworthiness of the underlying sources to assess the claim.",1 Introduction,[0],[0]
"It also derives informative features for interpretability, like source embeddings that capture trustworthiness and salient words captured via attention.",1 Introduction,[0],[0]
"Key contributions of this paper are:
• Model: An end-to-end neural network model which automatically assesses the credibility of natural-language claims, without any handcrafted features or lexicons.
",1 Introduction,[0],[0]
"• Interpretability: An attention mechanism in our model that generates user-comprehensible explanations, making credibility verdicts transparent and interpretable.
",1 Introduction,[0],[0]
"• Experiments: Extensive experiments on four datasets and ablation studies, demonstrating effectiveness of our method over state-of-theart baselines.",1 Introduction,[0],[0]
"Consider a set of N claims 〈Cn〉 from the respective origins/sources 〈CSn〉, where n ∈",2 End-to-end Framework for Credibility Analysis,[0],[0]
"[1, N ].",2 End-to-end Framework for Credibility Analysis,[0],[0]
"Each claim Cn is reported by a set of M articles 〈Am,n〉 along with their respective sources 〈ASm,n〉, where m ∈",2 End-to-end Framework for Credibility Analysis,[0],[0]
"[1,M ].",2 End-to-end Framework for Credibility Analysis,[0],[0]
"Each corresponding tuple of claim and its origin, reporting articles and article sources – 〈Cn, CSn, Am,n, ASm,n〉 forms a training instance in our setting, along with the credibility label of the claim used as ground-truth during network training.",2 End-to-end Framework for Credibility Analysis,[0],[0]
Figure 1 gives a pictorial overview of our model.,2 End-to-end Framework for Credibility Analysis,[0],[0]
"In the following sections, we provide a detailed description of our approach.",2 End-to-end Framework for Credibility Analysis,[0],[0]
"The input claim Cn of length l is represented as [c1, c2, ..., cl] where",2.1 Input Representations,[0],[0]
cl ∈ <d is the d-dimensional word embedding of the l-th word in the input claim.,2.1 Input Representations,[0],[0]
The source/origin of the claim CSn is represented by a ds-dimensional embedding vector csn ∈,2.1 Input Representations,[0],[0]
"<ds .
",2.1 Input Representations,[0],[0]
"A reporting article Am,n consisting of k tokens is represented by [am,n,1, am,n,2, ..., am,n,k], where am,n,k ∈",2.1 Input Representations,[0],[0]
"<d is the d-dimensional word embedding vector for the k-th word in the reporting article Am,n.",2.1 Input Representations,[0],[0]
The claim and article word embeddings have shared parameters.,2.1 Input Representations,[0],[0]
"The source of the reporting article ASm,n is represented as a dsdimensional vector, asm,n ∈",2.1 Input Representations,[0],[0]
<ds .,2.1 Input Representations,[0],[0]
"For the sake of brevity, we drop the notation subscripts n",2.1 Input Representations,[0],[0]
"and m in the following sections by considering only a single training instance – the input claim Cn from source CSn, the corresponding article Am,n and its sources ASm,n given by: 〈C,CS,A,AS〉.",2.1 Input Representations,[0],[0]
"To create a representation of an article, which may capture task-specific features such as whether it contains objective language, we use a bidirectional Long Short-Term Memory (LSTM) network as proposed by Graves et al. (2005).",2.2 Article Representation,[0],[0]
"A basic LSTM cell consists of various gates to control the flow of information through timesteps in a sequence, making LSTMs suitable for capturing long and short range dependencies in text that may be difficult to capture with standard recurrent neural networks (RNNs).",2.2 Article Representation,[0],[0]
"Given an input word embedding of tokens 〈ak〉, an LSTM cell performs various nonlinear transformations to generate a hidden vector state hk for each token at each timestep k.
We use bidirectional LSTMs in place of standard LSTMs.",2.2 Article Representation,[0],[0]
Bidirectional LSTMs capture both the previous timesteps (past features) and the future timesteps (future features) via forward and backward states respectively.,2.2 Article Representation,[0],[0]
"Correspondingly, there are two hidden states that capture past and future information that are concatenated to form the final output as: hk =",2.2 Article Representation,[0],[0]
"[ −→ hk, ←− hk].",2.2 Article Representation,[0],[0]
"As we previously discussed, it is important to consider the relevance of an article with respect to the claim; specifically, focusing or attending to parts of the article that discuss the claim.",2.3 Claim Specific Attention,[0],[0]
"This is in contrast to prior works (Popat et al., 2017; Rashkin et al., 2017; Wang, 2017) that ignore either the article or the claim, and therefore miss out on this important interaction.
",2.3 Claim Specific Attention,[0],[0]
We propose an attention mechanism to help our model focus on salient words in the article with respect to the claim.,2.3 Claim Specific Attention,[0],[0]
"To this end, we compute the importance of each term in an article with respect to an overall representation of the corresponding claim.",2.3 Claim Specific Attention,[0],[0]
"Additionally, incorporating attention helps in making our model transparent and interpretable, because it provides a way to generate the most salient words in an article as evidence of our model’s verdict.
",2.3 Claim Specific Attention,[0],[0]
"Following Wieting et al. (2015), the overall representation of an input claim is generated by taking an average of the word embeddings of all the
words therein:
c̄ = 1
",2.3 Claim Specific Attention,[0],[0]
"l ∑ l cl
We combine this overall representation of the claim with each article term:
âk = ak ⊕ c̄
where, âk ∈ <d+d and ⊕ denotes the concatenate operation.",2.3 Claim Specific Attention,[0],[0]
"We then perform a transformation to obtain claim-specific representations of each article term:
a′k = f(Waâk + ba)
where Wa and ba are the corresponding weight matrix and bias terms, and f is an activation function3, such as ReLU , tanh, or the identity function.",2.3 Claim Specific Attention,[0],[0]
"Following this, we use a softmax activation to calculate an attention score αk for each word in the article capturing its relevance to the claim context:
αk = exp(a′k)∑ k exp(a ′ k)
(1)",2.3 Claim Specific Attention,[0],[0]
"Now that we have article term representations given by 〈hk〉 and their relevance to the claim given by 〈αk〉, we need to combine them to predict the claim’s credibility.",2.4 Per-Article Credibility Score of Claim,[0],[0]
"In order to create an
3In our model, the tanh activation function gives best results.
attention-focused representation of the article considering both the claim and the article’s language, we calculate a weighted average of the hidden state representations for all article tokens based on their corresponding attention scores:
g = 1
k ∑ k αk · hk (2)
We then combine all the different feature representations: the claim source embedding (cs), the attention-focused article representation (g), and the article source embedding (as).",2.4 Per-Article Credibility Score of Claim,[0],[0]
"In order to merge the different representations and capture their joint interactions, we process them with two fully connected layers with non-linear activations.
",2.4 Per-Article Credibility Score of Claim,[0],[0]
"d1 = relu(Wc(g ⊕ cs⊕ as) + bc) d2 = relu(Wdd1 + bd)
where, W and b are the corresponding weight matrix and bias terms.
",2.4 Per-Article Credibility Score of Claim,[0],[0]
"Finally, to generate the overall credibility label of the article for classification tasks, or credibility score for regression tasks, we process the final representation with a final fully connected layer:
Classification: s = sigmoid(d2)",2.4 Per-Article Credibility Score of Claim,[0],[0]
"(3)
Regression: s = linear(d2) (4)",2.4 Per-Article Credibility Score of Claim,[0],[0]
The credibility score in the above step is obtained considering a single reporting article.,2.5 Credibility Aggregation,[0],[0]
"As previously discussed, we have M reporting articles per claim.",2.5 Credibility Aggregation,[0],[0]
"Therefore, once we have the per-article credibility scores from our model, we take an average of these scores to generate the overall credibility score for the claim:
cred(C) = 1
M ∑ m sm (5)
",2.5 Credibility Aggregation,[0],[0]
This aggregation is done after the model is trained.,2.5 Credibility Aggregation,[0],[0]
"We evaluate our approach and demonstrate its generality by performing experiments on four different datasets: a general fact-checking website, a political fact-checking website, a news review community, and a SemEval Twitter rumour dataset.",3 Datasets,[0],[0]
Snopes (www.snopes.com) is a general factchecking website where editors manually investigate various kinds of rumors reported on the Internet.,3.1 Snopes,[0],[0]
We used the Snopes dataset provided by Popat et al. (2017).,3.1 Snopes,[0],[0]
"This dataset consists of rumors analyzed on the Snopes website along with their credibility labels (true or false), sets of reporting articles, and their respective web sources.",3.1 Snopes,[0],[0]
PolitiFact is a political fact-checking website (www.politifact.com) in which editors rate the credibility of claims made by various political figures in US politics.,3.2 PolitiFact,[0],[0]
We extract all articles from PolitiFact published before December 2017.,3.2 PolitiFact,[0],[0]
"Each article includes a claim, the speaker (political figure) who made the claim, and the claim’s credibility rating provided by the editors.
",3.2 PolitiFact,[0],[0]
"PolitiFact assigns each claim to one of six possible ratings: true, mostly true, half true, mostly false, false and pants-on-fire.",3.2 PolitiFact,[0],[0]
"Following Rashkin et al. (2017), we combine true, mostly true and half true ratings into the class label true and the rest as false – hence considering only binary credibility labels.",3.2 PolitiFact,[0],[0]
"To retrieve the reporting articles for each claim (similar to Popat et al. (2017)), we issue each claim as a query to a search engine4 and retrieve the top 30 search results with their respective web sources.",3.2 PolitiFact,[0],[0]
NewsTrust is a news review community in which members review the credibility of news articles.,3.3 NewsTrust,[0],[0]
We use the NewsTrust dataset made available by Mukherjee and Weikum (2015).,3.3 NewsTrust,[0],[0]
This dataset contains NewsTrust stories from May 2006 to May 2014.,3.3 NewsTrust,[0],[0]
"Each story consists of a news article along with its source, and a set of reviews and ratings by community members.",3.3 NewsTrust,[0],[0]
NewsTrust aggregates these ratings and assigns an overall credibility score (on a scale of 1 to 5) to the posted article.,3.3 NewsTrust,[0],[0]
"We map the attributes in this data to the inputs expected by DeClarE as follows: the title and the web source of the posted (news) article are mapped to the input claim and claim source, respectively.",3.3 NewsTrust,[0],[0]
"Reviews and their corresponding user identities are mapped to reporting articles and article sources, respectively.",3.3 NewsTrust,[0],[0]
"We use this dataset for the regression task of predicting the credibility score of the posted article.
",3.3 NewsTrust,[0],[0]
4We use the Bing search API.,3.3 NewsTrust,[0],[0]
"As the fourth dataset, we consider the benchmark dataset released by SemEval-2017 for the task of determining credibility and stance of social media content (Twitter) (Derczynski et al., 2017).",3.4 SemEval-2017 Task 8,[0],[0]
"The objective of this task is to predict the credibility of a questionable tweet (true, false or unverified) along with a confidence score from the model.",3.4 SemEval-2017 Task 8,[0],[0]
"It has two sub-tasks: (i) a closed variant in which models only consider the questionable tweet, and (ii) an open variant in which models consider both the questionable tweet and additional context consisting of snapshots of relevant sources retrieved immediately before the rumor was reported, a snapshot of an associated Wikipedia article, news articles from digital news outlets, and preceding tweets about the same event.",3.4 SemEval-2017 Task 8,[0],[0]
"Testing and development datasets provided by organizers have 28 tweets (1021 reply tweets) and 25 tweets (256 reply tweets), respectively.",3.4 SemEval-2017 Task 8,[0],[0]
"In order to have a minimum support for training, claim sources with less than 5 claims in the dataset are grouped into a single dummy claim source, and article sources with less than 10 articles are grouped similarly (5 articles for SemEval as it is a smaller dataset).
",3.5 Data Processing,[0],[0]
"For Snopes and PolitiFact, we need to extract relevant snippets from the reporting articles for a claim.",3.5 Data Processing,[0],[0]
"Therefore, we extract snippets of 100 words from each reporting article having the maximum relevance score: sim = simbow×simsemantic where simbow is the fraction of claim words that are present in the snippet, and simsemantic represents the cosine similarity between the average of claim word embeddings and snippet word embeddings.",3.5 Data Processing,[0],[0]
We also enforce a constraint that the sim score is at least δ.,3.5 Data Processing,[0],[0]
"We varied δ from 0.2 to 0.8 and found 0.5 to give the optimal perfor-
mance on a withheld dataset.",3.5 Data Processing,[0],[0]
We discard all articles related to Snopes and PolitiFact websites from our datasets to have an unbiased model.,3.5 Data Processing,[0],[0]
Statistics of the datasets after pre-processing is provided in Table 1.,3.5 Data Processing,[0],[0]
All the datasets are made publicly available at https://www.mpi-inf. mpg.de/dl-cred-analysis/.,3.5 Data Processing,[0],[0]
"We evaluate our approach by conducting experiments on four datasets, as described in the previous section.",4 Experiments,[0],[0]
We describe our experimental setup and report our results in the following sections.,4 Experiments,[0],[0]
"When using the Snopes, PolitiFact and NewsTrust datasets, we reserve 10% of the data as validation data for parameter tuning.",4.1 Experimental Setup,[0],[0]
We report 10-fold cross validation results on the remaining 90% of the data; the model is trained on 9-folds and the remaining fold is used as test data.,4.1 Experimental Setup,[0],[0]
"When using the SemEval dataset, we use the data splits provided by the task’s organizers.",4.1 Experimental Setup,[0],[0]
"The objective for Snopes, PolitiFact and SemEval experiments is binary (credibility) classification, while for NewsTrust the objective is to predict the credibility score of the input claim on a scale of 1 to 5 (i.e., credibility regression).",4.1 Experimental Setup,[0],[0]
"We represent terms using pre-trained GloVe Wikipedia 6B word embeddings (Pennington et al., 2014).",4.1 Experimental Setup,[0],[0]
"Since our training datasets are not very large, we do not tune the word embeddings during training.",4.1 Experimental Setup,[0],[0]
The remaining model parameters are tuned on the validation data; the parameters chosen are reported in Table 2.,4.1 Experimental Setup,[0],[0]
We use Keras with a Tensorflow backend to implement our system.,4.1 Experimental Setup,[0],[0]
"All the models are trained using Adam optimizer (Kingma and Ba, 2014) (learning rate: 0.002) with categorical cross-entropy loss for classification and mean squared error loss for regression task.",4.1 Experimental Setup,[0],[0]
"We use L2-regularizers with the
fully connected layers as well as dropout.",4.1 Experimental Setup,[0],[0]
"For all the datasets, the model is trained using each claimarticle pair as a separate training instance.
",4.1 Experimental Setup,[0],[0]
"To evaluate and compare the performance of DeClarE with other state-of-the-art methods, we report the following measures:
• Credibility Classification (Snopes, PolitiFact and SemEval): accuracy of the models in classifying true and false claims separately, macro F1-score and Area-Under-Curve (AUC) for the ROC (Receiver Operating Characteristic) curve.
",4.1 Experimental Setup,[0],[0]
• Credibility Regression (NewsTrust): Mean Square Error (MSE) between the predicted and true credibility scores.,4.1 Experimental Setup,[0],[0]
"We compare our approach with the following state-of-the-art models: (i) LSTM-text, a recent approach proposed by Rashkin et al. (2017).",4.2 Results: Snopes and Politifact,[0],[0]
(ii) CNN-text: a CNN based approach proposed by Wang (2017).,4.2 Results: Snopes and Politifact,[0],[0]
(iii) Distant Supervision: stateof-the-art distant supervision based approach proposed by Popat et al. (2017).,4.2 Results: Snopes and Politifact,[0],[0]
(iv) DeClare (Plain): our approach with only biLSTM (no attention and source embeddings).,4.2 Results: Snopes and Politifact,[0],[0]
(v) DeClarE (Plain+Attn): our approach with only biLSTM and attention (no source embeddings).,4.2 Results: Snopes and Politifact,[0],[0]
(vi) DeClarE (Plain+SrEmb): our approach with only biLSTM and source embeddings (no attention).,4.2 Results: Snopes and Politifact,[0],[0]
(vii) DeClarE,4.2 Results: Snopes and Politifact,[0],[0]
"(Full): end-to-end system with biLSTM, attention and source embeddings.
",4.2 Results: Snopes and Politifact,[0],[0]
"The results when performing credibility classification on the Snopes and PolitiFact datasets are
shown in Table 3.",4.2 Results: Snopes and Politifact,[0],[0]
DeClarE outperforms LSTMtext and CNN-text models by a large margin on both datasets.,4.2 Results: Snopes and Politifact,[0],[0]
"On the other hand, for the Snopes dataset, performance of DeClarE (Full) is slightly lower than the Distant Supervision configuration (p-value of 0.04 with a pairwise t-test).",4.2 Results: Snopes and Politifact,[0],[0]
"However, the advantage of DeClarE over Distant Supervision approach is that it does not rely on hand crafted features and lexicons, and can generalize well to arbitrary domains without requiring any seed vocabulary.",4.2 Results: Snopes and Politifact,[0],[0]
"It is also to be noted that both of these approaches use external evidence in the form of reporting articles discussing the claim, which are not available to the LSTM-text and CNN-text baselines.",4.2 Results: Snopes and Politifact,[0],[0]
"This demonstrates the value of external evidence for credibility assessment.
",4.2 Results: Snopes and Politifact,[0],[0]
"On the PolitiFact dataset, DeClarE outperforms all the baseline models by a margin of 7-9% AUC (p-value of 9.12e−05 with a pairwise t-test) with similar improvements in terms of Macro F1.",4.2 Results: Snopes and Politifact,[0],[0]
"A performance comparison of DeClarE’s various configurations indicates the contribution of each component of our model, i.e, biLSTM capturing article representations, attention mechanism and source embeddings.",4.2 Results: Snopes and Politifact,[0],[0]
The additions of both the attention mechanism and source embeddings improve performance over the plain configuration in all cases when measured by Macro F1 or AUC.,4.2 Results: Snopes and Politifact,[0],[0]
"When performing credibility regression on the NewsTrust dataset, we evaluate the models in terms of mean squared error (MSE; lower is better) for credibility rating prediction.",4.3 Results: NewsTrust,[0],[0]
"We use the
first three models described in Section 4.2 as baselines.",4.3 Results: NewsTrust,[0],[0]
"For CNN-text and LSTM-text, we add a linear fully connected layer as the final layer of the model to support regression.",4.3 Results: NewsTrust,[0],[0]
"Additionally, we also consider the state-of-the-art CCRF+SVR model based on Continuous Conditional Random Field (CCRF) and Support Vector Regression (SVR) proposed by Mukherjee and Weikum (2015).",4.3 Results: NewsTrust,[0],[0]
The results are shown in Table 4.,4.3 Results: NewsTrust,[0],[0]
"We observe that DeClarE (Full) outperforms all four baselines, with a 17% decrease in MSE compared to the bestperforming baselines (i.e., LSTM-text and Distant Supervision).",4.3 Results: NewsTrust,[0],[0]
"The DeClarE (Plain) model performs substantially worse than the full model, illustrating the value of including attention and source embeddings.",4.3 Results: NewsTrust,[0],[0]
CNN-text performs substantially worse than the other baselines.,4.3 Results: NewsTrust,[0],[0]
"On the SemEval dataset, the objective is to perform credibility classification of a tweet while also producing a classification confidence score.",4.4 Results: SemEval,[0],[0]
We compare the following approaches and consider both variants of the SemEval task: (i) NileTMRG,4.4 Results: SemEval,[0],[0]
"(Enayet and El-Beltagy, 2017): the best performing approach for the close variant of the task, (ii) IITP (Singh et al., 2017): the best performing approach for the open variant of the task, (iii) DeClare (Plain): our approach with only biLSTM (no attention and source embeddings), and (iv) DeClarE (Full): our end-to-end system with biLSTM, attention and source embeddings.
",4.4 Results: SemEval,[0],[0]
We use the evaluation measure proposed by the task’s organizers: macro F1-score for overall classification and Root-Mean-Square Error (RMSE) over confidence scores.,4.4 Results: SemEval,[0],[0]
Results are shown in Table 5.,4.4 Results: SemEval,[0],[0]
"We observe that DeClarE (Full) outperforms all the other approaches — thereby, re-affirming its power in harnessing external evidence.",4.4 Results: SemEval,[0],[0]
"In order to assess how our model separates articles reporting false claims from those reporting true ones, we employ dimensionality reduction using Principal Component Analysis (PCA) to project the article representations (g in Equation 2) from a high dimensional space to a 2d plane.",5.1 Analyzing Article Representations,[0],[0]
The projections are shown in Figure 2a.,5.1 Analyzing Article Representations,[0],[0]
We observe that DeClarE obtains clear separability between credible versus non-credible articles in Snopes dataset.,5.1 Analyzing Article Representations,[0],[0]
"Similar to the treatment of article representations, we perform an analysis with the claim and article source embeddings by employing PCA and plotting the projections.",5.2 Analyzing Source Embeddings,[0],[0]
We sample a few popular news sources from Snopes and claim sources from PolitiFact.,5.2 Analyzing Source Embeddings,[0],[0]
"These news sources and claim sources are displayed in Figure 2b and Figure 2c, respectively.",5.2 Analyzing Source Embeddings,[0],[0]
"From Figure 2b we observe that DeClarE clearly separates fake news sources like nationalreport, empirenews, huzlers, etc. from mainstream news sources like nytimes, cnn, wsj, foxnews, washingtonpost, etc.",5.2 Analyzing Source Embeddings,[0],[0]
"Similarly, from Figure 2c we observe that DeClarE locates politicians with similar ideologies and opinions close to each other in the embedding space.",5.2 Analyzing Source Embeddings,[0],[0]
"Attention weights help understand what DeClarE focuses on during learning and how it affects its decisions – thereby, making our model transparent to the end-users.",5.3 Analyzing Attention Weights,[0],[0]
Table 6 illustrates some interesting claims and salient words (highlighted) that DeClarE focused on during learning.,5.3 Analyzing Attention Weights,[0],[0]
Darker shades indicate higher weights given to the corresponding words.,5.3 Analyzing Attention Weights,[0],[0]
"As illustrated in the table, DeClarE gives more attention to important words in the reporting article that are relevant to the claim and also
play a major role in deciding the corresponding claim’s credibility.",5.3 Analyzing Attention Weights,[0],[0]
"In the first example on Table 6, highlighted words such as “..barely true...” and “..sketchy evidence...” help our system to identify the claim as not credible.",5.3 Analyzing Attention Weights,[0],[0]
"On the other hand, highlighted words in the last example, like, “..reveal...” and “..documenting reports...”",5.3 Analyzing Attention Weights,[0],[0]
help our system to assess the claim as credible.,5.3 Analyzing Attention Weights,[0],[0]
Our work is closely related to the following areas:,6 Related Work,[0],[0]
Credibility analysis of Web claims: Our work builds upon approaches for performing credibility analysis of natural language claims in an opendomain Web setting.,6 Related Work,[0],[0]
"The approach proposed in Popat et al. (2016, 2017) employs stylistic lan-
guage features and the stance of articles to assess the credibility of the natural language claims.",6 Related Work,[0],[0]
"However, their model heavily relies on handcrafted language features.",6 Related Work,[0],[0]
"Rashkin et al. (2017); Wang (2017) propose neural network based approaches for determining the credibility of a textual claim, but it does not consider external sources like web evidence and claim sources.",6 Related Work,[0],[0]
These can be important evidence sources for credibility analysis.,6 Related Work,[0],[0]
The method proposed by Samadi et al. (2016) uses the Probabilistic Soft Logic (PSL) framework to estimate source reliability and claim correctness.,6 Related Work,[0],[0]
Vydiswaran et al. (2011) proposes an iterative algorithm which jointly learns the veracity of textual claims and trustworthiness of the sources.,6 Related Work,[0],[0]
"These approaches do not consider
the deeper semantic aspects of language, however.",6 Related Work,[0],[0]
"Wiebe and Riloff (2005); Lin et al. (2011); Recasens et al. (2013) study the problem of detecting bias in language, but do not consider credibility.",6 Related Work,[0],[0]
"Truth discovery: Prior approaches for truth discovery (Yin et al., 2008; Dong et al., 2009, 2015; Li et al., 2011, 2014, 2015; Pasternack and Roth, 2011, 2013; Ma et al., 2015; Zhi et al., 2015; Gao et al., 2015; Lyu et al., 2017) have focused on structured data with the goal of addressing the problem of conflict resolution amongst multisource data.",6 Related Work,[0],[0]
Nakashole and Mitchell (2014) proposed a method to extract conflicting values from the Web in the form of Subject-Predicate-Object (SPO) triplets and uses language objectivity analysis to determine the true value.,6 Related Work,[0],[0]
"Like the other truth discovery approaches, however, this approach is mainly suitable for use with structured data.",6 Related Work,[0],[0]
Credibility analysis in social media: Mukherjee et al. (2014); Mukherjee and Weikum (2015) propose PGM based approaches to jointly infer a statement’s credibility and the reliability of sources using language specific features.,6 Related Work,[0],[0]
"Approaches like (Castillo et al., 2011; Qazvinian et al., 2011; Yang et al., 2012; Xu and Zhao, 2012; Gupta et al., 2013; Zhao et al., 2015; Volkova et al., 2017) propose supervised methods for detecting deceptive content in social media platforms like Twitter, Sina Weibo, etc.",6 Related Work,[0],[0]
"Similarly, approaches like Ma et al. (2016); Ruchansky et al. (2017) use neural network methods to identify fake news and rumors on social media.",6 Related Work,[0],[0]
Kumar et al. (2016) studies the problem of detecting hoax articles on Wikipedia.,6 Related Work,[0],[0]
"All these rely on domain-specific and community-specific features like retweets, likes, upvotes, etc.",6 Related Work,[0],[0]
"In this work, we propose a completely automated end-to-end neural network model, DeClarE, for evidence-aware credibility assessment of natural language claims without requiring hand-crafted features or lexicons.",7 Conclusion,[0],[0]
"DeClarE captures signals from external evidence articles and models joint interactions between various factors like the context of a claim, the language of reporting articles, and trustworthiness of their sources.",7 Conclusion,[0],[0]
Extensive experiments on real world datasets demonstrate our effectiveness over state-of-the-art baselines.,7 Conclusion,[0],[0]
Misinformation such as fake news is one of the big challenges of our society.,abstractText,[0],[0]
"Research on automated fact-checking has proposed methods based on supervised learning, but these approaches do not consider external evidence apart from labeled training instances.",abstractText,[0],[0]
Recent approaches counter this deficit by considering external sources related to a claim.,abstractText,[0],[0]
"However, these methods require substantial feature modeling and rich lexicons.",abstractText,[0],[0]
"This paper overcomes these limitations of prior work with an end-toend model for evidence-aware credibility assessment of arbitrary textual claims, without any human intervention.",abstractText,[0],[0]
"It presents a neural network model that judiciously aggregates signals from external evidence articles, the language of these articles and the trustworthiness of their sources.",abstractText,[0],[0]
It also derives informative features for generating user-comprehensible explanations that makes the neural network predictions transparent to the end-user.,abstractText,[0],[0]
Experiments with four datasets and ablation studies show the strength of our method.,abstractText,[0],[0]
DeClarE: Debunking Fake News and False Claims using Evidence-Aware Deep Learning,title,[0],[0]
"Algorithmic decipherment is a prime example of a truly unsupervised problem. The first step in the decipherment process is the identification of the encrypted language. We propose three methods for determining the source language of a document enciphered with a monoalphabetic substitution cipher. The best method achieves 97% accuracy on 380 languages. We then present an approach to decoding anagrammed substitution ciphers, in which the letters within words have been arbitrarily transposed. It obtains the average decryption word accuracy of 93% on a set of 50 ciphertexts in 5 languages. Finally, we report the results on the Voynich manuscript, an unsolved fifteenth century cipher, which suggest Hebrew as the language of the document.",text,[0],[0]
"The Voynich manuscript is a medieval codex1 consisting of 240 pages written in a unique script, which has been referred to as the world’s most important unsolved cipher (Schmeh, 2013).",1 Introduction,[0],[0]
"The type of cipher that was used to generate the text is unknown; a number of theories have been proposed, including substitution and transposition ciphers, an abjad (a writing system in which vowels are not written), steganography, semi-random schemes, and an elaborate hoax.",1 Introduction,[0],[0]
"However, the biggest obstacle to deci-
1The manuscript was radiocarbon dated to 1404-1438 AD in the Arizona Accelerator Mass Spectrometry Laboratory (http://www.arizona.edu/crack-voynich-code, accessed Nov. 20, 2015).
",1 Introduction,[0],[0]
"phering the manuscript is the lack of knowledge of what language it represents.
",1 Introduction,[0],[0]
"Identification of the underlying language has been crucial for the decipherment of ancient scripts, including Egyptian hieroglyphics (Coptic), Linear B (Greek), and Mayan glyphs (Ch’olti’).",1 Introduction,[0],[0]
"On the other hand, the languages of many undeciphered scripts, such as Linear A, the Indus script, and the Phaistos Disc, remain unknown (Robinson, 2002).",1 Introduction,[0],[0]
"Even the order of characters within text may be in doubt; in Egyptian hieroglyphic inscriptions, for instance, the symbols were sometimes rearranged within a word in order to create a more elegant inscription (Singh, 2011).",1 Introduction,[0],[0]
"Another complicating factor is the omission of vowels in some writing systems.
",1 Introduction,[0],[0]
Applications of ciphertext language identification extend beyond secret ciphers and ancient scripts.,1 Introduction,[0],[0]
Nagy et al. (1987) frame optical character recognition as a decipherment task.,1 Introduction,[0],[0]
"Knight et al. (2006) note that for some languages, such as Hindi, there exist many different and incompatible encoding schemes for digital storage of text; the task of analyzing such an arbitrary encoding scheme can be viewed as a decipherment of a substitution cipher in an unknown language.",1 Introduction,[0],[0]
"Similarly, the unsupervised derivation of transliteration mappings between different writing scripts lends itself to a cipher formulation (Ravi and Knight, 2009).
",1 Introduction,[0],[0]
"The Voynich manuscript is written in an unknown script that encodes an unknown language, which is the most challenging type of a decipherment problem (Robinson, 2002, p. 46).",1 Introduction,[0],[0]
"Inspired by the mystery of both the Voynich manuscript and the undeciphered ancient scripts, we develop a series of
75
Transactions of the Association for Computational Linguistics, vol. 4, pp.",1 Introduction,[0],[0]
"75–86, 2016.",1 Introduction,[0],[0]
Action Editor: Regina Barzilay.,1 Introduction,[0],[0]
"Submission batch: 12/2015; Published 4/2016.
",1 Introduction,[0],[0]
c©2016 Association for Computational Linguistics.,1 Introduction,[0],[0]
"Distributed under a CC-BY 4.0 license.
algorithms for the purpose of decrypting unknown alphabetic scripts representing unknown languages.",1 Introduction,[0],[0]
"We assume that symbols in scripts which contain no more than a few dozen unique characters roughly correspond to phonemes of a language, and model them as monoalphabetic substitution ciphers.",1 Introduction,[0],[0]
"We further allow that an unknown transposition scheme could have been applied to the enciphered text, resulting in arbitrary scrambling of letters within words (anagramming).",1 Introduction,[0],[0]
"Finally, we consider the possibility that the underlying script is an abjad, in which only consonants are explicitly represented.
",1 Introduction,[0],[0]
Our decryption system is composed of three steps.,1 Introduction,[0],[0]
"The first task is to identify the language of a ciphertext, by comparing it to samples representing known languages.",1 Introduction,[0],[0]
The second task is to map each symbol of the ciphertext to the corresponding letter in the identified language.,1 Introduction,[0],[0]
"The third task is to decode the resulting anagrams into readable text, which may involve the recovery of unwritten vowels.
",1 Introduction,[0],[0]
The paper is structured as follows.,1 Introduction,[0],[0]
We discuss related work in Section 2.,1 Introduction,[0],[0]
"In Section 3, we propose three methods for the source language identification of texts enciphered with a monoalphabetic substitution cipher.",1 Introduction,[0],[0]
"In Section 4, we present and evaluate our approach to the decryption of texts composed of enciphered anagrams.",1 Introduction,[0],[0]
"In Section 5, we apply our new techniques to the Voynich manuscript.",1 Introduction,[0],[0]
Section 6 concludes the paper.,1 Introduction,[0],[0]
"In this section, we review particularly relevant prior work on the Voynich manuscript, and on algorithmic decipherment in general.",2 Related Work,[0],[0]
"Since the discovery of the Voynich manuscript (henceforth referred to as the VMS), there have been a number of decipherments claims.",2.1 Voynich Manuscript,[0],[0]
"Newbold and Kent (1928) proposed an interpretation based on microscopic details in the text, which was subsequently refuted by Manly (1931).",2.1 Voynich Manuscript,[0],[0]
"Other claimed decipherments by Feely (1943) and Strong (1945) have also been refuted (Tiltman, 1968).",2.1 Voynich Manuscript,[0],[0]
"A detailed study of the manuscript by d’Imperio (1978) details various other proposed solutions and the arguments against them.
",2.1 Voynich Manuscript,[0],[0]
Numerous languages have been proposed to underlie the VMS.,2.1 Voynich Manuscript,[0],[0]
The properties and the dating of the manuscript imply Latin and Italian as potential candidates.,2.1 Voynich Manuscript,[0],[0]
"On the basis of the analysis of the character frequency distribution, Jaskiewicz (2011) identifies five most probable languages, which include Moldavian and Thai.",2.1 Voynich Manuscript,[0],[0]
"Reddy and Knight (2011) discover an excellent match between the VMS and Quranic Arabic in the distribution of word lengths, as well as a similarity to Chinese Pinyin in the predictability of letters given the preceding letter.
",2.1 Voynich Manuscript,[0],[0]
It has been suggested previously that some anagramming scheme may alter the sequence order of characters within words in the VMS.,2.1 Voynich Manuscript,[0],[0]
Tiltman (1968) observes that each symbol behaves as if it had its own place in an “order of precedence” within words.,2.1 Voynich Manuscript,[0],[0]
Rugg (2004) notes the apparent similarity of the VMS to a text in which each word has been replaced by an alphabetically ordered anagram (alphagram).,2.1 Voynich Manuscript,[0],[0]
"Reddy and Knight (2011) show that the letter sequences are generally more predictable than in natural languages.
",2.1 Voynich Manuscript,[0],[0]
Some researchers have argued that the VMS may be an elaborate hoax created to only appear as a meaningful text.,2.1 Voynich Manuscript,[0],[0]
"Rugg (2004) suggests a tabular method, similar to the sixteenth century technique of the Cardan grille, although recent dating of the manuscript to the fifteenth century provides evidence to the contrary.",2.1 Voynich Manuscript,[0],[0]
Schinner (2007) uses analysis of random walk techniques and textual statistics to support the hoax hypothesis.,2.1 Voynich Manuscript,[0],[0]
"On the other hand, Landini (2001) identifies in the VMS language-like statistical properties, such as Zipf’s law, which were only discovered in the last century.",2.1 Voynich Manuscript,[0],[0]
"Similarly, Montemurro and Zanette (2013) use information theoretic techniques to find long-range relationships between words and sections of the manuscript, as well as between the text and the figures in the VMS.",2.1 Voynich Manuscript,[0],[0]
A monoalphabetic substitution cipher is a wellknown method of enciphering a plaintext by converting it into a ciphertext of the same length using a 1-to-1 mapping of symbols.,2.2 Algorithmic Decipherment,[0],[0]
Knight et al. (2006) propose a method for deciphering substitution ciphers which is based on Viterbi decoding with mapping probabilities computed with the expectationmaximization (EM) algorithm.,2.2 Algorithmic Decipherment,[0],[0]
The method correctly deciphers 90% of symbols in a 400-letter ciphertext when a trigram character language model is used.,2.2 Algorithmic Decipherment,[0],[0]
"They apply their method to ciphertext language identification using 80 different language samples, and report successful outcomes on three ciphers that represent English, Spanish, and a Spanish abjad, respectively.
",2.2 Algorithmic Decipherment,[0],[0]
"Ravi and Knight (2008) present a more complex but slower method for solving substitution ciphers, which incorporates constraints that model the 1-to-1 property of the key.",2.2 Algorithmic Decipherment,[0],[0]
The objective function is again the probability of the decipherment relative to an ngram character language model.,2.2 Algorithmic Decipherment,[0],[0]
"A solution is found by optimally solving an integer linear program.
",2.2 Algorithmic Decipherment,[0],[0]
Knight et al. (2011) describe a successful decipherment of an eighteenth century text known as the Copiale Cipher.,2.2 Algorithmic Decipherment,[0],[0]
Language identification was the first step of the process.,2.2 Algorithmic Decipherment,[0],[0]
The EM-based method of Knight et al. (2006) identified German as the most likely candidate among over 40 candidate character language models.,2.2 Algorithmic Decipherment,[0],[0]
The more accurate method of Ravi and Knight (2008) was presumably either too slow or too brittle for this purpose.,2.2 Algorithmic Decipherment,[0],[0]
"The cipher was eventually broken using a combination of manual and algorithmic techniques.
",2.2 Algorithmic Decipherment,[0],[0]
"Hauer et al. (2014) present an approach to solving monoalphabetic substitution ciphers which is more accurate than other algorithms proposed for this task, including Knight et al. (2006), Ravi and Knight (2008), and Norvig (2009).",2.2 Algorithmic Decipherment,[0],[0]
We provide a detailed description of the method in Section 4.1.,2.2 Algorithmic Decipherment,[0],[0]
"In this section, we propose and evaluate three methods for determining the source language of a document enciphered with a monoalphabetic substitution cipher.",3 Source Language Identification,[0],[0]
"We frame it as a classification task, with the classes corresponding to the candidate languages,
which are represented by short sample texts.",3 Source Language Identification,[0],[0]
"The methods are based on:
1. relative character frequencies,
2.",3 Source Language Identification,[0],[0]
"patterns of repeated symbols within words,
3.",3 Source Language Identification,[0],[0]
the outcome of a trial decipherment.,3 Source Language Identification,[0],[0]
An intuitive way of guessing the source language of a ciphertext is by character frequency analysis.,3.1 Character Frequency,[0],[0]
The key observation is that the relative frequencies of symbols in the text are unchanged after encipherment with a 1-to-1 substitution cipher.,3.1 Character Frequency,[0],[0]
"The idea is to order the ciphertext symbols by frequency, normalize these frequencies to create a probability distribution, and choose the closest matching distribution from the set of candidate languages.
",3.1 Character Frequency,[0],[0]
"More formally, let PT be a discrete probability distribution where PT (i) is the probability of a randomly selected symbol in a text T being the ith most frequent symbol.",3.1 Character Frequency,[0],[0]
"We define the distance between two texts U and V to be the Bhattacharyya (1943) distance between the probability distributions PU and PV :
d(U, V ) =",3.1 Character Frequency,[0],[0]
"− ln ∑
i
√ PU (i) · PV (i)
",3.1 Character Frequency,[0],[0]
"The advantages of this distance metric include its symmetry, and the ability to account for events that have a zero probability (in this case, due to different alphabet sizes).",3.1 Character Frequency,[0],[0]
The language of the closest sample text to the ciphertext is considered to be the most likely source language.,3.1 Character Frequency,[0],[0]
This method is not only fast but also robust against letter reordering and the lack of word boundaries.,3.1 Character Frequency,[0],[0]
Our second method expands on the character frequency method by incorporating the notion of decomposition patterns.,3.2 Decomposition Pattern Frequency,[0],[0]
This method uses multiple occurrences of individual symbols within a word as a clue to the language of the ciphertext.,3.2 Decomposition Pattern Frequency,[0],[0]
"For example, the word seems contains two instances of ‘s’ and ‘e’, and one instance of ‘m’.",3.2 Decomposition Pattern Frequency,[0],[0]
"We are interested in capturing the relative frequency of such patterns in texts, independent of the symbols used.
",3.2 Decomposition Pattern Frequency,[0],[0]
"Formally, we define a function f that maps a word to an ordered n-tuple (t1, t2, . . .",3.2 Decomposition Pattern Frequency,[0],[0]
"tn), where ti ≥ tj if i < j.",3.2 Decomposition Pattern Frequency,[0],[0]
Each ti is the number of occurrences of the ith most frequent character in the word.,3.2 Decomposition Pattern Frequency,[0],[0]
"For example, f(seems) = (2, 2, 1), while f(beams) = (1, 1, 1, 1, 1).",3.2 Decomposition Pattern Frequency,[0],[0]
We refer to the resulting tuple as the decomposition pattern of the word.,3.2 Decomposition Pattern Frequency,[0],[0]
The decomposition pattern is unaffected by monoalphabetic letter substitution or anagramming.,3.2 Decomposition Pattern Frequency,[0],[0]
"As with the character frequency method, we define the distance between two texts as the Bhattacharyya distance between their decomposition pattern distributions, and classify the language of a ciphertext as the language of the nearest sample text.
",3.2 Decomposition Pattern Frequency,[0],[0]
It is worth noting that this method requires word separators to be preserved in the ciphertext.,3.2 Decomposition Pattern Frequency,[0],[0]
"In fact, the effectiveness of the method comes partly from capturing the distribution of word lengths in a text.",3.2 Decomposition Pattern Frequency,[0],[0]
"On the other hand, the decomposition patterns are independent of the ordering of characters within words.",3.2 Decomposition Pattern Frequency,[0],[0]
We will take advantage of this property in Section 4.,3.2 Decomposition Pattern Frequency,[0],[0]
The final method that we present involves deciphering the document in question into each candidate language.,3.3 Trial Decipherment,[0],[0]
"The decipherment is performed with a fast greedy-swap algorithm, which is related to the algorithms of Ravi and Knight (2008) and Norvig (2009).",3.3 Trial Decipherment,[0],[0]
It attempts to find the key that maximizes the probability of the decipherment according to a bigram character language model derived from a sample document in a given language.,3.3 Trial Decipherment,[0],[0]
"The decipherment with the highest probability indicates the most likely plaintext language of the document.
",3.3 Trial Decipherment,[0],[0]
The greedy-swap algorithm is shown in Figure 2.,3.3 Trial Decipherment,[0],[0]
"The initial key is created by pairing the ciphertext and plaintext symbols in the order of decreasing frequency, with null symbols appended to the shorter of the two alphabets.",3.3 Trial Decipherment,[0],[0]
"The algorithm repeatedly attempts to improve the current key k by considering the “best” swaps of ciphertext symbol pairs within the key (if the key is viewed as a permutation of the alphabet, such a swap is a transposition).",3.3 Trial Decipherment,[0],[0]
The best swaps are defined as those that involve a symbol occurring among the 10 least common bigrams in the decipherment induced by the current key.,3.3 Trial Decipherment,[0],[0]
"If any such swap yields a more probable decipherment,
it is incorporated in the current key; otherwise, the algorithm terminates.",3.3 Trial Decipherment,[0],[0]
"The total number of iterations is bounded by m, which is set to 5 times the size of the alphabet.",3.3 Trial Decipherment,[0],[0]
"After the initial run, the algorithm is restarted 20 times with a randomly generated initial key, which often results in a better decipherment.",3.3 Trial Decipherment,[0],[0]
All parameters were established on a development set.,3.3 Trial Decipherment,[0],[0]
We now directly evaluate the three methods described above by applying them to a set of ciphertexts from different languages.,3.4 Evaluation,[0],[0]
We adapted the dataset created by Emerson et al. (2014) from the text of the Universal Declaration of Human Rights (UDHR) in 380 languages.2 The average length of the texts is 1710 words and 11073 characters.,3.4 Evaluation,[0],[0]
"We divided the text in each language into 66% training, 17% development, and 17% test.",3.4 Evaluation,[0],[0]
The training part was used to derive character bigram models for each language.,3.4 Evaluation,[0],[0]
"The development and test parts were separately enciphered with a random substitution cipher.
",3.4 Evaluation,[0],[0]
Table 1 shows the results of the language identification methods on both the development and the test set.,3.4 Evaluation,[0],[0]
We report the average top-1 accuracy on the task of identifying the source language of 380 enciphered test samples.,3.4 Evaluation,[0],[0]
The differences between methods are statistically significant according to McNemar’s test with p < 0.0001.,3.4 Evaluation,[0],[0]
The random baseline of 0.3% indicates the difficulty of the task.,3.4 Evaluation,[0],[0]
"The “oracle” decipherment assumes a perfect decipherment of the text, which effectively reduces the task to standard
2Eight languages from the original set were excluded because of formatting issues.
language identification.
",3.4 Evaluation,[0],[0]
"All three of our methods perform well, with the accuracy gains reflecting their increasing complexity.",3.4 Evaluation,[0],[0]
"Between the two character frequency methods, our approach based on Bhattacharyya distance is significantly more accurate than the method of Jaskiewicz (2011), which uses a specially-designed distribution distance function.",3.4 Evaluation,[0],[0]
"The decomposition pattern method makes many fewer errors, with the correct language ranked second in roughly half of those cases.",3.4 Evaluation,[0],[0]
"Trial decipherment yields the best results, which are close to the upper bound for the character bigram probability approach to language identification.",3.4 Evaluation,[0],[0]
The average decipherment error rate into the correct language is only 2.5%.,3.4 Evaluation,[0],[0]
"In 4 out of 11 identification errors made on the test set, the error rate is above the average; the other 7 errors involve closely related languages, such as Serbian and Bosnian.
",3.4 Evaluation,[0],[0]
"The trial decipherment approach is much slower than the frequency distribution methods, requiring roughly one hour of CPU time in order to classify each ciphertext.",3.4 Evaluation,[0],[0]
"More complex decipherment algorithms are even slower, which precludes their application to this test set.",3.4 Evaluation,[0],[0]
"Our re-implementations of the dynamic programming algorithm of Knight et al. (2006), and the integer programming solver of Ravi and Knight (2008) average 53 and 7000 seconds of CPU time, respectively, to solve a single 256 character cipher, compared to 2.6 seconds with our greedyswap method.",3.4 Evaluation,[0],[0]
The dynamic programming algorithm improves decipherment accuracy over our method by only 4% on a benchmark set of 50 ciphers of 256 characters.,3.4 Evaluation,[0],[0]
We conclude that our greedy-swap algorithm strikes the right balance between accuracy and speed required for the task of cipher language identification.,3.4 Evaluation,[0],[0]
"In this section, we address the challenging task of deciphering a text in an unknown language written using an unknown script, and in which the letters within words have been randomly scrambled.",4 Anagram Decryption,[0],[0]
"The task is designed to emulate the decipherment problem posed by the VMS, with the assumption that its unusual ordering of characters within words reflects some kind of a transposition cipher.",4 Anagram Decryption,[0],[0]
We restrict the source language to be one of the candidate languages for which we have sample texts; we model an unknown script with a substitution cipher; and we impose no constraints on the letter transposition method.,4 Anagram Decryption,[0],[0]
The encipherment process is illustrated in Figure 3.,4 Anagram Decryption,[0],[0]
The goal in this instance is to recover the plaintext in (a) given the ciphertext in (c) without the knowledge of the plaintext language.,4 Anagram Decryption,[0],[0]
"We also consider an additional encipherment step that removes all vowels from the plaintext.
",4 Anagram Decryption,[0],[0]
"Our solution is composed of a sequence of three modules that address the following tasks: language identification, script decipherment, and anagram decoding.",4 Anagram Decryption,[0],[0]
"For the first task we use the decomposition pattern frequency method described in Section 3.2, which is applicable to anagrammed ciphers.",4 Anagram Decryption,[0],[0]
"After identifying the plaintext language, we proceed to reverse the substitution cipher using a heuristic search algorithm guided by a combination of word and character language models.",4 Anagram Decryption,[0],[0]
"Finally, we unscramble the anagrammed words into readable text by framing the decoding as a tagging task, which is efficiently solved with a Viterbi decoder.",4 Anagram Decryption,[0],[0]
Our modular approach makes it easy to perform different levels of analysis on unsolved ciphers.,4 Anagram Decryption,[0],[0]
"For the decipherment step, we adapt the state-of-theart solver of Hauer et al. (2014).",4.1 Script Decipherment,[0],[0]
"In this section, we describe the three main components of the solver: key scoring, key mutation, and tree search.",4.1 Script Decipherment,[0],[0]
"This is followed by the summary of modifications that make the method work on anagrams.
",4.1 Script Decipherment,[0],[0]
The scoring component evaluates the fitness of each key by computing the smoothed probability of the resulting decipherment with both characterlevel and word-level language models.,4.1 Script Decipherment,[0],[0]
"The wordlevel models promote decipherments that contain
in-vocabulary words and high-probability word ngrams, while the character level models allow for the incorporation of out-of-vocabulary words.
",4.1 Script Decipherment,[0],[0]
The key mutation component crucially depends on the notion of pattern equivalence between character strings.,4.1 Script Decipherment,[0],[0]
Two strings are pattern-equivalent if they share the same pattern of repeated letters.,4.1 Script Decipherment,[0],[0]
"For example, MZXCX is pattern-equivalent with there and bases.",4.1 Script Decipherment,[0],[0]
but not with otter.,4.1 Script Decipherment,[0],[0]
"For each word unigram, bigram, and trigram in the ciphertext, a list of the most frequent pattern equivalent n-grams from the training corpus is compiled.",4.1 Script Decipherment,[0],[0]
"The solver repeatedly attempts to improve the current key through a series of transpositions, so that a given cipher ngram maps to a pattern-equivalent n-gram from the provided language sample.",4.1 Script Decipherment,[0],[0]
"The number of substitutions for a given n-gram is limited to the k most promising candidates, where k is a parameter optimized on a development set.
",4.1 Script Decipherment,[0],[0]
"The key mutation procedure generates a tree structure, which is searched for the best-scoring decipherment using a version of beam search.",4.1 Script Decipherment,[0],[0]
"The root of the tree contains the initial key, which is generated according to simple frequency analysis (i.e., by mapping the n-th most common ciphertext character to the n-th most common character in the corpus).",4.1 Script Decipherment,[0],[0]
"New tree leaves are spawned by modifying the keys of current leaves, while ensuring that each node in the tree has a unique key.",4.1 Script Decipherment,[0],[0]
"At the end of computation, the key with the highest score is returned as the solution.
",4.1 Script Decipherment,[0],[0]
"In our anagram adaptation, we relax the definition of pattern equivalence to include strings that have the same decomposition pattern, as defined in Section 3.2.",4.1 Script Decipherment,[0],[0]
"Under the new definition, the order of the letters within a word has no effect on pattern equivalence.",4.1 Script Decipherment,[0],[0]
"For example, MZXCX is equivalent not only with there and bases, but also with three and otter,
because all these words map to the (2, 1, 1, 1) pattern.",4.1 Script Decipherment,[0],[0]
"Internally, we represent all words as alphagrams, in which letters are reshuffled into the alphabetical order (Figure 3d).",4.1 Script Decipherment,[0],[0]
"In order to handle the increased ambiguity, we use a letter-frequency heuristic to select the most likely mapping of letters within an n-gram.",4.1 Script Decipherment,[0],[0]
The trigram language models over both words and characters are derived by converting each word in the training corpus into its alphagram.,4.1 Script Decipherment,[0],[0]
"On a benchmark set of 50 ciphers of length 256, the average error rate of the modified solver is 2.6%, with only a small increase in time and space usage.",4.1 Script Decipherment,[0],[0]
The output of the script decipherment step is generally unreadable (see Figure 3d).,4.2 Anagram Decoder,[0],[0]
The words might be composed of the right letters but their order is unlikely to be correct.,4.2 Anagram Decoder,[0],[0]
"We proceed to decode the sequence of anagrams by framing it as a simple hidden Markov model, in which the hidden states correspond to plaintext words, and the observed sequence is composed of their anagrams.",4.2 Anagram Decoder,[0],[0]
"Without loss of generality, we convert anagrams into alphagrams, so that the emission probabilities are always equal to 1.",4.2 Anagram Decoder,[0],[0]
Any alphagrams that correspond to unseen words are replaced with a single ‘unknown’ type.,4.2 Anagram Decoder,[0],[0]
"We then use a modified Viterbi decoder to determine the most likely word sequence according to a word trigram language model, which is derived from the training corpus, and smoothed using deleted interpolation (Jelinek and Mercer, 1980).",4.2 Anagram Decoder,[0],[0]
"Many writing systems, including Arabic and Hebrew, are abjads that do not explicitly represent vowels.",4.3 Vowel Recovery,[0],[0]
Reddy and Knight (2011) provide evidence that the VMS may encode an abjad.,4.3 Vowel Recovery,[0],[0]
"The removal of vowels represents a substantial loss of information,
and appears to dramatically increase the difficulty of solving a cipher.
",4.3 Vowel Recovery,[0],[0]
"In order to apply our system to abjads, we remove all vowels in the corpora prior to deriving the language models used by the script decipherment step.",4.3 Vowel Recovery,[0],[0]
We assume the ability to partition the plaintext symbols into disjoint sets of vowels and consonants for each candidate language.,4.3 Vowel Recovery,[0],[0]
The anagram decoder is trained to recover complete in-vocabulary words from sequences of anagrams containing only consonants.,4.3 Vowel Recovery,[0],[0]
"At test time, we remove the vowels from the input to the decipherment step of the pipeline.",4.3 Vowel Recovery,[0],[0]
"In contrast with Knight et al. (2006), our approach is able not only to attack abjad ciphers, but also to restore the vowels, producing fully readable text.",4.3 Vowel Recovery,[0],[0]
"In order to test our anagram decryption pipeline on out-of-domain ciphertexts, the corpora for deriving language models need to be much larger than the UDHR samples used in the previous section.",4.4 Evaluation,[0],[0]
"We selected five diverse European languages from Europarl (Koehn, 2005): English, Bulgarian, German, Greek, and Spanish.",4.4 Evaluation,[0],[0]
"The corresponding corpora contain about 50 million words each, with the exception of Bulgarian which has only 9 million words.",4.4 Evaluation,[0],[0]
"We remove punctuation and numbers, and lowercase all text.
",4.4 Evaluation,[0],[0]
"We test on texts extracted from Wikipedia articles on art, Earth, Europe, film, history, language, music, science, technology, and Wikipedia.",4.4 Evaluation,[0],[0]
"The texts are first enciphered using a substitution cipher, and then anagrammed (Figure 3a-c).",4.4 Evaluation,[0],[0]
"Each of the five languages is represented by 10 ciphertexts, which are decrypted independently.",4.4 Evaluation,[0],[0]
"In order to keep the running time reasonable, the length of the ciphertexts is set to 500 characters.
",4.4 Evaluation,[0],[0]
The first step is language identification.,4.4 Evaluation,[0],[0]
"Our decomposition pattern method, which is resistant to both anagramming and substitution, correctly identifies the source language of 49 out of 50 ciphertexts.",4.4 Evaluation,[0],[0]
"The lone exception is the German article on technology, for which German is the second ranked language after Greek.",4.4 Evaluation,[0],[0]
This error could be easily detected by noticing that most of the Greek words “deciphered” by the subsequent steps are out of vocabulary.,4.4 Evaluation,[0],[0]
"We proceed to evaluate the following steps assuming that the source language is known.
",4.4 Evaluation,[0],[0]
The results in Table 2 show that our system is able to effectively break the anagrammed ciphers in all five languages.,4.4 Evaluation,[0],[0]
"For Step 2 (script decipherment), we count as correct all word tokens that contain the right characters, disregarding their order.",4.4 Evaluation,[0],[0]
Step 3 (anagram decoding) is evaluated under the assumption that it has received a perfect decipherment from Step 2.,4.4 Evaluation,[0],[0]
"On average, the accuracy of each individual step exceeds 95%.",4.4 Evaluation,[0],[0]
The values in the column denoted as Both are the actual results of the pipeline composed of Steps 2 and 3.,4.4 Evaluation,[0],[0]
"Our system correctly recovers 93.8% of word tokens, which corresponds to over 97% of the in-vocabulary words within the test files, The percentage of the in-vocabulary words, which are shown in the Ceiling column, constitute the effective accuracy limits for each language.
",4.4 Evaluation,[0],[0]
"The errors fall into three categories, as illustrated in Figure 3e.",4.4 Evaluation,[0],[0]
"Step 2 introduces decipherment errors (e.g., deciphering ‘s’ as ‘k’ instead of ‘z’ in “organized”), which typically preclude the word from being recovered in the next step.",4.4 Evaluation,[0],[0]
"A decoding error in Step 3 may occur when an alphagram corresponds to multiple words (e.g. “greens” instead of “genres”), although most such ambiguities are resolved correctly.",4.4 Evaluation,[0],[0]
"However, the majority of errors are caused by out-of-vocabulary (OOV) words in the plaintext (e.g., “improvisational”).",4.4 Evaluation,[0],[0]
"Since the decoder can only produce words found in the training corpus, an OOV word almost always results in an error.",4.4 Evaluation,[0],[0]
"The German ciphers stand out as having the largest percentage of OOV words (8.2%), which may be attributed to frequent compounding.
",4.4 Evaluation,[0],[0]
Table 3 shows the results of the analogous experiments on abjads (Section 4.3).,4.4 Evaluation,[0],[0]
"Surprisingly, the removal of vowels from the plaintext actually improves the average decipherment step accuracy to 99%.",4.4 Evaluation,[0],[0]
"This is due not only to the reduced number of
distinct symbols, but also to the fewer possible anagramming permutations in the shortened words.",4.4 Evaluation,[0],[0]
"On the other hand, the loss of vowel information makes the anagram decoding step much harder.",4.4 Evaluation,[0],[0]
"However, more than three quarters of in-vocabulary tokens are still correctly recovered, including the original vowels.3 In general, this is sufficient for a human reader to understand the meaning of the document, and deduce the remaining words.",4.4 Evaluation,[0],[0]
"In this section, we present the results of our experiments on the VMS.",5 Voynich Experiments,[0],[0]
We attempt to identify the source language with the methods described in Section 3; we quantify the similarity of the Voynich words to alphagrams; and we apply our anagram decryption algorithm from Section 4 to the text.,5 Voynich Experiments,[0],[0]
"Unless otherwise noted, the VMS text used in our experiments corresponds to 43 pages of the manuscript in the “type B” handwriting (VMS-B), investigated by Reddy and Knight (2011), which we obtained directly from the authors.",5.1 Data,[0],[0]
"It contains 17,597 words and 95,465 characters, transcribed into 35 characters of the Currier alphabet (d’Imperio, 1978).
",5.1 Data,[0],[0]
"For the comparison experiments, we selected five languages shown in Table 4, which have been suggested in the past as the language of the VMS (Kennedy and Churchill, 2006).",5.1 Data,[0],[0]
"Considering the age of the manuscript, we attempt to use corpora that correspond to older versions of the languages, including King James Bible, Bibbia di Gerusalemme, and Vulgate.
3The differences in the Ceiling numbers between Tables 2 and 3 are due to words that are composed entirely of vowels.",5.1 Data,[0],[0]
"In this section, we present the results of our ciphertext language identification methods from Section 3 on the VMS text.
",5.2 Source Language,[0],[0]
"The closest language according to the letter frequency method is Mazatec, a native American language from southern Mexico.",5.2 Source Language,[0],[0]
"Since the VMS was created before the voyage of Columbus, a New World language is an unlikely candidate.",5.2 Source Language,[0],[0]
"The top ten languages also include Mozarabic (3), Italian (8), and Ladino (10), all of which are plausible guesses.",5.2 Source Language,[0],[0]
"However, the experiments in Section 3.4 demonstrate that the frequency analysis is much less reliable than the other two methods.
",5.2 Source Language,[0],[0]
"The top-ranking languages according to the decomposition pattern method are Hebrew, Malay (in Arabic script), Standard Arabic, and Amharic, in this order.",5.2 Source Language,[0],[0]
We note that three of these belong to the Semitic family.,5.2 Source Language,[0],[0]
The similarity of decomposition patterns between Hebrew and the VMS is striking.,5.2 Source Language,[0],[0]
"The Bhattacharyya distance between the respective distributions is 0.020, compared to 0.048 for the second-ranking Malay.",5.2 Source Language,[0],[0]
The histogram in Figure 4 shows Hebrew as a single outlier in the leftmost bin.,5.2 Source Language,[0],[0]
"In fact, Hebrew is closer to a sample of the VMS of a similar length than to any of the remaining 379 UDHR samples.
",5.2 Source Language,[0],[0]
"The ranking produced by the the trial decipherment method is sensitive to parameter changes; however, the two languages that consistently appear near the top of the list are Hebrew and Esperanto.",5.2 Source Language,[0],[0]
The high rank of Hebrew corroborates the outcome of the decomposition pattern method.,5.2 Source Language,[0],[0]
"Being a relatively recent creation, Esperanto itself can be excluded as the ciphertext language, but its high score is remarkable in view of the well-known theory that the VMS text represents a constructed language.4 We hypoth-
4The theory was first presented in the form of an ana-
esize that the extreme morphological regularity of Esperanto (e.g., all plural nouns contain the bigram ‘oj’) yields an unusual bigram character language model which fits the repetitive nature of the VMS words.
",5.2 Source Language,[0],[0]
"In summary, while there is no complete agreement between the three methods about the most likely underlying source language, there appears to be a strong statistical support for Hebrew from the two most accurate methods, one of which is robust against anagramming.",5.2 Source Language,[0],[0]
"In addition, the language is a plausible candidate on historical grounds, being widely-used for writing in the Middle Ages.",5.2 Source Language,[0],[0]
"In fact, a number of cipher techniques, including anagramming, can be traced to the Jewish Cabala (Kennedy and Churchill, 2006).",5.2 Source Language,[0],[0]
"In this section, we quantify the peculiarity of the VMS lexicon by modeling the words as alphagrams.",5.3 Alphagrams,[0],[0]
"We introduce the notion of the alphagram distance, and compute it for the VMS and for natural language samples.
",5.3 Alphagrams,[0],[0]
We define a word’s alphagram distance with respect to an ordering of the alphabet as the number of letter pairs that are in the wrong order.,5.3 Alphagrams,[0],[0]
"For example, with respect to the QWERTY keyboard order, the word rye has an alphagram distance of 2 because it contains two letter pairs that violate the order: (r, e) and (y, e).",5.3 Alphagrams,[0],[0]
A word is an alphagram if and only if its alphagram distance is zero.,5.3 Alphagrams,[0],[0]
"The maximum alphagram distance for a word of length n is equal to the number of its distinct letter pairs.
",5.3 Alphagrams,[0],[0]
"gram (Friedman and Friedman, 1959).",5.3 Alphagrams,[0],[0]
"See also a more recent proposal by Balandin and Averyanov (2014).
",5.3 Alphagrams,[0],[0]
"In order to quantify how strongly the words in a language resemble alphagrams, we first need to identify the order of the alphabet that minimizes the total alphagram distance of a representative text sample.",5.3 Alphagrams,[0],[0]
"The decision version of this problem is NPcomplete, which can be demonstrated by a reduction from the path variant of the traveling salesman problem.",5.3 Alphagrams,[0],[0]
"Instead, we find an approximate solution with the following greedy search algorithm.",5.3 Alphagrams,[0],[0]
"Starting from an initial order in which the letters first occur in the text, we repeatedly consider all possible new positions for a letter within the current order, and choose the one that yields the lowest total alphagram distance of the text.",5.3 Alphagrams,[0],[0]
"This process is repeated until no better order is found for 10 iterations, with 100 random restarts.
",5.3 Alphagrams,[0],[0]
"When applied to a random sample of 10,000 word tokens from the VMS, our algorithm yields the order 4BZOVPEFSXQYWC28ARUTIJ3*GHK69MDLN5, which corresponds to the average alphagram distance of 0.996 (i.e., slightly less than one pair of letters per word).",5.3 Alphagrams,[0],[0]
"The corresponding result on English is jzbqwxcpathofvurimslkengdy, with an average alphagram distance of 2.454.",5.3 Alphagrams,[0],[0]
"Note that the letters at the beginning of the sequence tend to have low frequency, while the ones at the end occur in popular morphological suffixes, such as −ed and −ly.",5.3 Alphagrams,[0],[0]
"For example, the beginning of the first article of the UDHR with the letters transposed to follow this order becomes: “All ahumn biseng are born free and qaule in tiingdy and thrisg.”
To estimate how close the solution produced by our greedy algorithm is to the actual optimal solution, we also calculate a lower bound for the total alphagram distance with any character order.",5.3 Alphagrams,[0],[0]
"The lower bound is ∑ x,y min(bxy, byx), where bxy is the number of times character x occurs before character y within words in the text.
",5.3 Alphagrams,[0],[0]
"Figure 5 shows the average alphagram distances for the VMS and five comparison languages, each represented by a random sample of 10,000 word tokens which exclude single-letter words.",5.3 Alphagrams,[0],[0]
The Expected values correspond to a completely random intra-word letter order.,5.3 Alphagrams,[0],[0]
The Lexicographic values correspond to the standard alphabetic order in each language.,5.3 Alphagrams,[0],[0]
"The actual minimum alphagram distance is between the Lower Bound and the Computed Minimum obtained by our greedy algorithm.
",5.3 Alphagrams,[0],[0]
"The results in Figure 5 show that while the expected alphagram distance for the VMS falls within the range exhibited by natural languages, its minimum alphagram distance is exceptionally low.",5.3 Alphagrams,[0],[0]
"In absolute terms, the VMS minimum is less than half the corresponding number for Hebrew.",5.3 Alphagrams,[0],[0]
"In relative terms, the ratio of the expected distance to the minimum distance is below 2 for any of the five languages, but above 4 for the VMS.",5.3 Alphagrams,[0],[0]
"These results suggest that, if the VMS encodes a natural language text, the letters within the words may have been reordered during the encryption process.",5.3 Alphagrams,[0],[0]
"In this section, we discuss the results of applying our anagram decryption system described in Section 4 to the VMS text.
",5.4 Decipherment Experiments,[0],[0]
We decipher each of the first 10 pages of the VMS-B using the five language models derived from the corpora described in Section 5.1.,5.4 Decipherment Experiments,[0],[0]
"The pages contain between 292 and 556 words, 3726 in total.",5.4 Decipherment Experiments,[0],[0]
Figure 6 shows the average percentage of in-vocabulary words in the 10 decipherments.,5.4 Decipherment Experiments,[0],[0]
"The percentage is significantly higher for Hebrew than for the other languages, which suggests a better match with the VMS.",5.4 Decipherment Experiments,[0],[0]
"Although the abjad versions of English, Italian, and Latin yield similar levels of in-vocabulary words, their distances to the VMS language according to the decomposition pattern method are 0.159, 0.176, and 0.245 respectively, well above Hebrew’s 0.020.
",5.4 Decipherment Experiments,[0],[0]
"None of the decipherments appear to be syntac-
tically correct or semantically consistent.",5.4 Decipherment Experiments,[0],[0]
This is expected because our system is designed for pure monoalphabetic substitution ciphers.,5.4 Decipherment Experiments,[0],[0]
"If the VMS indeed represents one of the five languages, the amount of noise inherent in the orthography and the transcription would prevent the system from producing a correct decipherment.",5.4 Decipherment Experiments,[0],[0]
"For example, in a hypothetical non-standard orthography of Hebrew, some prepositions or determiners could be written as separate one-letter words, or a single phoneme could have two different representations.",5.4 Decipherment Experiments,[0],[0]
"In addition, because of the age of the manuscript and the variety of its hand-writing styles, any transcription requires a great deal of guesswork regarding the separation of individual words into distinct symbols (Figure 1).",5.4 Decipherment Experiments,[0],[0]
"Finally, the decipherments necessarily reflect the corpora that underlie the language model, which may correspond to a different domain and historical period.
",5.4 Decipherment Experiments,[0],[0]
"Nevertheless, it is interesting to take a closer look at specific examples of the system output.",5.4 Decipherment Experiments,[0],[0]
The first line of the VMS (VAS92 9FAE AR APAM ZOE ZOR9 QOR92 9 FOR ZOE89) is deciphered into Hebrew as וישנא ילע ו וחיבל וילא שיא Nהכה הל השעו תוצמה.5,5.4 Decipherment Experiments,[0],[0]
"According to a native speaker of the language, this is not quite a coherent sentence.",5.4 Decipherment Experiments,[0],[0]
"However, after making a couple of spelling corrections, Google Translate is able to convert it into passable English: “She made recommendations to the priest, man of the house and me and people.”",5.4 Decipherment Experiments,[0],[0]
"6
Even though the input ciphertext is certainly too noisy to result in a fluent output, the system might still manage to correctly decrypt individual words in a longer passage.",5.4 Decipherment Experiments,[0],[0]
"In order to limit the influence of context in the decipherment, we restrict the word language model to unigrams, and apply our system to the first 72 words (241 characters)7 from the “Herbal” section of the VMS, which contains drawings of plants.",5.4 Decipherment Experiments,[0],[0]
"An inspection of the output reveals several words that would not be out of place in a medieval herbal, such as רצה ‘narrow’, רכיא ‘farmer’, רוא ‘light’, ריוא ‘air’, שׁא ‘fire’.
",5.4 Decipherment Experiments,[0],[0]
"The results presented in this section could be interpreted either as tantalizing clues for Hebrew as
5Hebrew is written from right to left.",5.4 Decipherment Experiments,[0],[0]
"6https://translate.google.com/ (accessed Nov. 20, 2015).",5.4 Decipherment Experiments,[0],[0]
"7The length of the passage was chosen to match the number
of symbols in the Phaistos Disc inscription.
",5.4 Decipherment Experiments,[0],[0]
"the source language of the VMS, or simply as artifacts of the combinatorial power of anagramming and language models.",5.4 Decipherment Experiments,[0],[0]
"We note that the VMS decipherment claims in the past have typically been limited to short passages, without ever producing a full solution.",5.4 Decipherment Experiments,[0],[0]
"In any case, the output of an algorithmic decipherment of a noisy input can only be a starting point for scholars that are well-versed in the given language and historical period.",5.4 Decipherment Experiments,[0],[0]
"We have presented a multi-stage system for solving ciphers that combine monoalphabetic letter substitution and unconstrained intra-word letter transposition to encode messages in an unknown language.8 We have evaluated three methods of ciphertext language identification that are based on letter frequency, decomposition patterns, and trial decipherment, respectively.",6 Conclusion,[0],[0]
"We have demonstrated that our language-independent approach can effectively break anagrammed substitution ciphers, even when vowels are removed from the input.",6 Conclusion,[0],[0]
"The application of our methods to the Voynich manuscript suggests that it may represent Hebrew, or another abjad script, with the letters rearranged to follow a fixed order.
",6 Conclusion,[0],[0]
There are several possible directions for the future work.,6 Conclusion,[0],[0]
The pipeline approach presented in this paper might be outperformed by a unified generative model.,6 Conclusion,[0],[0]
"The techniques could be made more resistant to noise; for example, by softening the emission model in the anagram decoding phase.",6 Conclusion,[0],[0]
"It would also be interesting to jointly identify both the language and the type of the cipher (Nuhn and Knight, 2014),
8Software at https://www.cs.ualberta.ca/˜kondrak/.
which could lead to the development of methods to handle more complex ciphers.",6 Conclusion,[0],[0]
"Finally, the anagram decoding task could be extended to account for the transposition of words within lines, in addition to the transposition of symbols within words.",6 Conclusion,[0],[0]
We thank Prof. Moshe Koppel for the assessment of the Hebrew examples.,Acknowledgements,[0],[0]
"We thank the reviewers for their comments and suggestions.
",Acknowledgements,[0],[0]
"This research was supported by the Natural Sciences and Engineering Research Council of Canada, and by Alberta Innovates – Technology Futures and Alberta Innovation & Advanced Education.",Acknowledgements,[0],[0]
Algorithmic decipherment is a prime example of a truly unsupervised problem.,abstractText,[0],[0]
The first step in the decipherment process is the identification of the encrypted language.,abstractText,[0],[0]
We propose three methods for determining the source language of a document enciphered with a monoalphabetic substitution cipher.,abstractText,[0],[0]
The best method achieves 97% accuracy on 380 languages.,abstractText,[0],[0]
"We then present an approach to decoding anagrammed substitution ciphers, in which the letters within words have been arbitrarily transposed.",abstractText,[0],[0]
It obtains the average decryption word accuracy of 93% on a set of 50 ciphertexts in 5 languages.,abstractText,[0],[0]
"Finally, we report the results on the Voynich manuscript, an unsolved fifteenth century cipher, which suggest Hebrew as the language of the document.",abstractText,[0],[0]
Decoding Anagrammed Texts Written in an Unknown Language and Script,title,[0],[0]
Many important problems in machine learning require learning functions in the presence of noise.,1. Introduction,[0],[0]
"For example, in reinforcement learning (RL), the transition dynamics of a system is often stochastic.",1. Introduction,[0],[0]
"Ideally, a model for these systems should be able to both express such randomness but also to account for the uncertainty in its parameters.
",1. Introduction,[0],[0]
"Bayesian neural networks (BNN) are probabilistic models that place the flexibility of neural networks in a Bayesian framework (Blundell et al., 2015; Gal, 2016).",1. Introduction,[0],[0]
"In particular, recent work has extended BNNs with latent input variables (BNN+LV) to estimate functions with complex stochasticity such as bimodality or heteroscedasticity (Depeweg et al., 2016).",1. Introduction,[0],[0]
"This model class can describe complex stochastic patterns via a distribution over the latent input variables (aleatoric uncertainty), while, at the same time, account for model uncertainty via a distribution over weights (epistemic uncertainty).
",1. Introduction,[0],[0]
1Siemens AG 2TU Munich 3University of Cambridge 4Harvard University.,1. Introduction,[0],[0]
"Correspondence to: Stefan Depeweg <stdepewe@gmail.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
In this work we show how to perform and utilize a decomposition of uncertainty in aleatoric and epistemic components for decision making purposes.,1. Introduction,[0],[0]
"Our contributions are:
• We derive two decompositions that extract epistemic and aleatoric uncertainties from the predictive distribution of BNN+LV (Section 3).",1. Introduction,[0],[0]
"• We demonstrate that with this uncertainty decomposition, the BNN+LV identifies informative regions even in bimodal and heteroscedastic cases, enabling efficient active learning in the presence of complex noise (Section 4).",1. Introduction,[0],[0]
"• We derive a novel risk-sensitive criterion for model-
based RL based on the uncertainty decomposition, enabling a domain expert to trade-off the risks of reliability, which originates from model bias and the risk induced by stochasticity (Section 5).
",1. Introduction,[0],[0]
"While using uncertainties over transition probabilities to avoid worst-case behavior has been well-studied in discrete MDPs (e.g. (Shapiro & Kleywegt, 2002; Nilim & El Ghaoui, 2005; Bagnell et al., 2001), to our knowledge, our work is the first to consider continuous non-linear functions with complex noise.",1. Introduction,[0],[0]
We review a recent family of probabilistic models for multioutput regression.,2. Background: BNN+LV,[0],[0]
"These models were previously introduced by Depeweg et al. (2016), we refer to them as Bayesian Neural Networks with latent variables (BNN+LV).
",2. Background: BNN+LV,[0],[0]
"Given a dataset D = {xn,yn}Nn=1, formed by feature vectors xn ∈",2. Background: BNN+LV,[0],[0]
"RD and targets yn ∈ RK , we assume that yn = f(xn, zn;W) + n, where f(·, ·;W) is the output of a neural network with weights W and K output units.",2. Background: BNN+LV,[0],[0]
"The network receives as input the feature vector xn and the latent variable zn ∼ N (0, γ).",2. Background: BNN+LV,[0],[0]
"We choose rectifiers, ϕ(x) = max(x, 0), as activation functions for the hidden layers and the identity function, ϕ(x) = x, for the output layer.",2. Background: BNN+LV,[0],[0]
"The network output is corrupted by the additive noise variable n ∼ N (0,Σ) with diagonal covariance matrix",2. Background: BNN+LV,[0],[0]
Σ.,2. Background: BNN+LV,[0],[0]
The role of the latent variable zn is to capture unobserved stochastic features that can affect the network’s output in complex ways.,2. Background: BNN+LV,[0],[0]
"Without zn, randomness
ar X
iv :1
71 0.
07 28
3v 4
[ st
at .M
L ]
1 5
Ju n
20 18
would only be given by the additive Gaussian observation noise n, which can only describe limited stochastic patterns.",2. Background: BNN+LV,[0],[0]
"The network has L layers, with Vl hidden units in layer l, and W = {Wl}Ll=1 is the collection of Vl × (Vl−1 + 1) weight matrices.",2. Background: BNN+LV,[0],[0]
The +1 is introduced here to account for the additional per-layer biases.,2. Background: BNN+LV,[0],[0]
"We approximate the exact posterior p(W, z | D) with:
q(W, z) =
  L∏
l=1
Vl∏
i=1
Vl−1+1∏
j=1
N (wij,l|mwij,l, vwij,l)
 
︸ ︷︷ ︸ q(W)
×
[ N∏
n=1
N",2. Background: BNN+LV,[0],[0]
"(zn |mzn, vzn)
]
︸ ︷︷ ︸ q(z)
.",2. Background: BNN+LV,[0],[0]
"(1)
The parameters mwij,l, v w ij,l and m z n, v z n are determined by minimizing a divergence between p(W, z | D) and the approximation q.",2. Background: BNN+LV,[0],[0]
The reader is referred to the work of Hernández-Lobato et al. (2016);,2. Background: BNN+LV,[0],[0]
Depeweg et al. (2016) for more details on this.,2. Background: BNN+LV,[0],[0]
"In our experiments, we tune q using black-box α-divergence minimization with α = 1.0.",2. Background: BNN+LV,[0],[0]
"While other values of α are possible, this specific value produced better uncertainty decompositions in practice: see Section 4 and the supplementary material for results with α = 0.5 and α = 0",2. Background: BNN+LV,[0],[0]
"(variational Bayes).
",2. Background: BNN+LV,[0],[0]
"BNN+LV can capture complex stochastic patterns, while at the same time account for model uncertainty.",2. Background: BNN+LV,[0],[0]
"They achieve this by jointly learning q(z), which describes the values of the latent variables that were used to generate the training data, and q(W), which represents uncertainty about model parameters.",2. Background: BNN+LV,[0],[0]
"The result is a flexible Bayesian approach for learning conditional distributions with complex stochasticity, e.g. bimodal or heteroscedastic noise.",2. Background: BNN+LV,[0],[0]
"Let us assume that the targets are one-dimensional, that is, K = 1.",3. Uncertainty Decomposition in BNN+LV,[0],[0]
The predictive distribution of a BNN+LV for the target variable y? associated with the test data point,3. Uncertainty Decomposition in BNN+LV,[0],[0]
"x? is
p(y?|x?)",3. Uncertainty Decomposition in BNN+LV,[0],[0]
"= ∫ p(y?|W,x?, z?)p(z?)q(W) dz? dW .",3. Uncertainty Decomposition in BNN+LV,[0],[0]
"(2)
where p(y?|W,x?, z?)",3. Uncertainty Decomposition in BNN+LV,[0],[0]
"= N (y?|f(x?, z?;W),Σ) is the likelihood function, p(z?)",3. Uncertainty Decomposition in BNN+LV,[0],[0]
"= N (z?|0, γ) is the prior on the latent variables and q(W) is the approximate posterior for W given D. In this expression q(z) is not used since the integration with respect to z? must be done using the prior p(z?).",3. Uncertainty Decomposition in BNN+LV,[0],[0]
"The reason for this is that the y? associated with x? is unknown and consequently, there is no other evidence on z?",3. Uncertainty Decomposition in BNN+LV,[0],[0]
"than the one coming from p(z?).
",3. Uncertainty Decomposition in BNN+LV,[0],[0]
In Eq.,3. Uncertainty Decomposition in BNN+LV,[0],[0]
"(2), the randomness or uncertainty on y? has its origin inW ∼ q(W), z?",3. Uncertainty Decomposition in BNN+LV,[0],[0]
∼,3. Uncertainty Decomposition in BNN+LV,[0],[0]
"p(z?) and ∼ N (0, σ2).",3. Uncertainty Decomposition in BNN+LV,[0],[0]
"This means
that there are two types of uncertainties entangled in our predictons for y?: aleatoric and epistemic (Der Kiureghian & Ditlevsen, 2009; Kendall & Gal, 2017).",3. Uncertainty Decomposition in BNN+LV,[0],[0]
The aleatoric uncertainty originates from the randomness of z?,3. Uncertainty Decomposition in BNN+LV,[0],[0]
and and cannot be reduced by collecting more data.,3. Uncertainty Decomposition in BNN+LV,[0],[0]
"By contrast, the epistemic uncertainty originates from the randomness of W and can be reduced by collecting more data, which will typically shrink the approximate posterior q(W).
",3. Uncertainty Decomposition in BNN+LV,[0],[0]
Eq. (2) is the tool to use when making predictions for y?.,3. Uncertainty Decomposition in BNN+LV,[0],[0]
"However, there are many settings in which, for decision making purposes, we may be interested in separating the two forms of uncertainty present in this distribution.",3. Uncertainty Decomposition in BNN+LV,[0],[0]
"We now describe two decompositions , each one differing in the metric used to quantify uncertainty: the first one is based on the entropy, whereas the second one uses the variance.
",3. Uncertainty Decomposition in BNN+LV,[0],[0]
Let H(·) compute the differential entropy of a probability distribution.,3. Uncertainty Decomposition in BNN+LV,[0],[0]
The total uncertainty present in Eq.,3. Uncertainty Decomposition in BNN+LV,[0],[0]
(2) can then be quantified as H(y?|x?).,3. Uncertainty Decomposition in BNN+LV,[0],[0]
"Furthermore, assume that we do not integrateW out in Eq.",3. Uncertainty Decomposition in BNN+LV,[0],[0]
"(2) and, instead, we just condition on a specific value of this variable.",3. Uncertainty Decomposition in BNN+LV,[0],[0]
"The result is then p(y?|W,x?)",3. Uncertainty Decomposition in BNN+LV,[0],[0]
"= ∫ p(y?|W,x?, z?)p(z?) dz?",3. Uncertainty Decomposition in BNN+LV,[0],[0]
"with corresponding uncertainty H(y?|W,x?).",3. Uncertainty Decomposition in BNN+LV,[0],[0]
"The expectation of this quantity under q(W), that is, Eq(W)[H(y?|W,x?)",3. Uncertainty Decomposition in BNN+LV,[0],[0]
"], can then be used to quantify the overall uncertainty in Eq.",3. Uncertainty Decomposition in BNN+LV,[0],[0]
(2) coming from z?,3. Uncertainty Decomposition in BNN+LV,[0],[0]
and .,3. Uncertainty Decomposition in BNN+LV,[0],[0]
"Therefore, Eq(W)[H(y?|W,x?)], measures the aleatoric uncertainty.",3. Uncertainty Decomposition in BNN+LV,[0],[0]
We can then quantify the epistemic part of the uncertainty in Eq.,3. Uncertainty Decomposition in BNN+LV,[0],[0]
"(2) by computing the difference between total and aleatoric uncertainties:
H[y?|x?]−Eq(W)[H(y?|W,x?)]",3. Uncertainty Decomposition in BNN+LV,[0],[0]
"= I(y?,W) , (3)
which is the mutual information between y?",3. Uncertainty Decomposition in BNN+LV,[0],[0]
"andW .
",3. Uncertainty Decomposition in BNN+LV,[0],[0]
"Instead of the entropy, we can use the variance as a measure of uncertainty.",3. Uncertainty Decomposition in BNN+LV,[0],[0]
Let σ2(·) compute the variance of a probability distribution.,3. Uncertainty Decomposition in BNN+LV,[0],[0]
The total uncertainty present in Eq.,3. Uncertainty Decomposition in BNN+LV,[0],[0]
(2) is then σ2(y?|x?).,3. Uncertainty Decomposition in BNN+LV,[0],[0]
"This quantity can then be decomposed using the law of total variance:
σ2(y?|x?) =",3. Uncertainty Decomposition in BNN+LV,[0],[0]
"σ2q(W)(E[y?|W,x?])",3. Uncertainty Decomposition in BNN+LV,[0],[0]
"+Eq(W)[σ2(y?|W,x?)] .
where E[y?|W,x?] and σ2[y?|W,x?] are, respectively, the mean and variance of y?",3. Uncertainty Decomposition in BNN+LV,[0],[0]
"according to p(y?|W,x?).",3. Uncertainty Decomposition in BNN+LV,[0],[0]
"In the expression above, σ2q(W)(E[y?|W,x?]) is the variance of E[y?|W,x?]",3. Uncertainty Decomposition in BNN+LV,[0],[0]
when W ∼ q(W).,3. Uncertainty Decomposition in BNN+LV,[0],[0]
This term ignores any contribution to the variance of y? from z?,3. Uncertainty Decomposition in BNN+LV,[0],[0]
and and only considers the effect ofW .,3. Uncertainty Decomposition in BNN+LV,[0],[0]
"Therefore, it corresponds to the epistemic uncertainty in Eq.",3. Uncertainty Decomposition in BNN+LV,[0],[0]
(2).,3. Uncertainty Decomposition in BNN+LV,[0],[0]
"By contrast, the term Eq(W)[σ2(y?|W,x?)] represents the average value of σ2(y?|W,x?) when W ∼ q(W).",3. Uncertainty Decomposition in BNN+LV,[0],[0]
This term ignores any contribution to the variance of y?,3. Uncertainty Decomposition in BNN+LV,[0],[0]
"fromW and, therefore, it represents the aleatoric uncertainty in Eq.",3. Uncertainty Decomposition in BNN+LV,[0],[0]
"(2).
",3. Uncertainty Decomposition in BNN+LV,[0],[0]
"In some cases, working with variances can be undesirable because they have square units.",3. Uncertainty Decomposition in BNN+LV,[0],[0]
"To avoid this problem, we
can work with the square root of the previous terms.",3. Uncertainty Decomposition in BNN+LV,[0],[0]
"For example, we can represent the total uncertainty using σ(y?|x?)",3. Uncertainty Decomposition in BNN+LV,[0],[0]
"={
σ2q(W)(E[y?|W,x?])",3. Uncertainty Decomposition in BNN+LV,[0],[0]
"+Eq(W)[σ2(y?|W,x?)]",3. Uncertainty Decomposition in BNN+LV,[0],[0]
} 1 2 .,3. Uncertainty Decomposition in BNN+LV,[0],[0]
(4),3. Uncertainty Decomposition in BNN+LV,[0],[0]
"Active learning is the problem of iteratively collecting data so that the final gains in predictive performance are as high as possible (Settles, 2012).",4. Active Learning with Complex Noise,[0],[0]
We consider the case of actively learning arbitrary non-linear functions with complex noise.,4. Active Learning with Complex Noise,[0],[0]
"To do so, we apply the information-theoretic framework for active learning described by MacKay (1992), which is based on the reduction of entropy in the model’s posterior distribution.",4. Active Learning with Complex Noise,[0],[0]
"Below, we show that this framework naturally results in the entropy-based uncertainty decomposition from Section 3.",4. Active Learning with Complex Noise,[0],[0]
"Next, we demonstrate how this framework, applied to BNN+LV enables data-efficient learning in the presence of heteroscedastic and bimodal noise.
",4. Active Learning with Complex Noise,[0],[0]
"Assume a BNN+LV is used to describe a batch of training data D = {(x1,y1), · · · , (xN ,yN )}.",4. Active Learning with Complex Noise,[0],[0]
The expected reduction in posterior entropy forW that would be obtained when collecting the unknown target y?,4. Active Learning with Complex Noise,[0],[0]
"for the input x? is
H(W|D)−Ey?|x?,D",4. Active Learning with Complex Noise,[0],[0]
"[H(W|D ∪ {x?,y?})] = I(W,y?) = H(y?|x?)−Eq(W)",4. Active Learning with Complex Noise,[0],[0]
"[H(y?|W,x?)] .",4. Active Learning with Complex Noise,[0],[0]
"(5)
Note that this is the epistemic uncertainty that we introduced in Section 3, which has arisen naturally in this setting: the most informative",4. Active Learning with Complex Noise,[0],[0]
x? for which to collect y?,4. Active Learning with Complex Noise,[0],[0]
"next is the one for which the epistemic uncertainty in the BNN+LV predictive distribution is the highest.
",4. Active Learning with Complex Noise,[0],[0]
The epistemic uncertainty in Eq.,4. Active Learning with Complex Noise,[0],[0]
"(5) can be approximated using standard entropy estimators, e.g. nearest-neighbor methods (Kozachenko & Leonenko, 1987; Kraskov et al., 2004; Gao et al., 2016).",4. Active Learning with Complex Noise,[0],[0]
"For that, we repeatedly sample W and z? and do forward passes through the BNN+LV to sample y?.",4. Active Learning with Complex Noise,[0],[0]
"The resulting samples of y? can then be used to approximate the respective entropies for each x? using the nearest-neighbor approach:
H(y?|x?)−Eq(W)",4. Active Learning with Complex Noise,[0],[0]
"[H(y?|W,x?)]
≈ Ĥ(y1?, . . .",4. Active Learning with Complex Noise,[0],[0]
",yL? )",4. Active Learning with Complex Noise,[0],[0]
"− 1
M M∑ i=1",4. Active Learning with Complex Noise,[0],[0]
"[ Ĥ(y1,Wi? , . . .",4. Active Learning with Complex Noise,[0],[0]
",y L,Wi ? ) ] .",4. Active Learning with Complex Noise,[0],[0]
"(6)
where Ĥ(·) is a nearest-neighbor entropy estimate given an empirical sample of points, y1?, . .",4. Active Learning with Complex Noise,[0],[0]
.,4. Active Learning with Complex Noise,[0],[0]
",y L ?",4. Active Learning with Complex Noise,[0],[0]
are sampled from p(y?|x?),4. Active Learning with Complex Noise,[0],[0]
according to Eq.,4. Active Learning with Complex Noise,[0],[0]
"(2),W1, . . .",4. Active Learning with Complex Noise,[0],[0]
",WM ∼ q(W) and y1,Wi? , . . .",4. Active Learning with Complex Noise,[0],[0]
",y L,Wi ?",4. Active Learning with Complex Noise,[0],[0]
"∼ p(y?|Wi,x?) for i = 1, . . .",4. Active Learning with Complex Noise,[0],[0]
",M .
",4. Active Learning with Complex Noise,[0],[0]
"There are alternative ways to estimate the entropy, e.g. with histograms or using kernel density estimation (KDE) (Beirlant et al., 1997).",4. Active Learning with Complex Noise,[0],[0]
"We choose nearest neighbor methods
because they tend to work well in low dimensions, are fast to compute (compared to KDE) and do not require much hyperparameter tuning (compared to histograms).",4. Active Learning with Complex Noise,[0],[0]
"However, we note that for high-dimensional problems, estimating entropy is a difficult problem.",4. Active Learning with Complex Noise,[0],[0]
We evaluate the active learning procedure on three problems.,4.1. Experiments,[0],[0]
"In each of them, we first train a BNN+LV with 2 hidden layers and 20 units per layer.",4.1. Experiments,[0],[0]
"Afterwards, we approximate the epistemic uncertainty as outlined in the previous section.",4.1. Experiments,[0],[0]
"Hyper-parameter settings and details for replication can be found in the supplementary material, which includes results for three other inference methods: Hamiltonian Monte Carlo (HMC), black-box α- divergence minimization with α = 0.5 and α = 0",4.1. Experiments,[0],[0]
(variational Bayes).,4.1. Experiments,[0],[0]
"These results show that the decomposition of uncertainty produced by α = 1 and the gold standard HMC are similar, but for lower values of α this is not the case.",4.1. Experiments,[0],[0]
"Our main findings are:
The decomposition of uncertainty allows us to identify informative inputs when the noise is heteroscedastic.",4.1. Experiments,[0],[0]
"We consider a regression problem with heteroscedastic noise where y = 7 sin(x) + 3| cos(x/2)| with ∼ N (0, 1).",4.1. Experiments,[0],[0]
"We sample 750 values of the input x from a mixture of three Gaussians with mean parameters {µ1 = −4, µ2 = 0, µ3 = 4}, variance parameters {σ1 = 25 , σ2 = 0.9, σ3 = 2 5} and with each Gaussian component having weight equal to 1/3 in the mixture.",4.1. Experiments,[0],[0]
Figure 1a shows the data.,4.1. Experiments,[0],[0]
"We have many points at the borders and in the center, but few in between.
",4.1. Experiments,[0],[0]
Figure 1 shows the results obtained (see caption for details).,4.1. Experiments,[0],[0]
"The resulting decomposition of predictive uncertainty is very accurate: the epistemic uncertainty (Figure 1f), is inversely proportional to the density used to sample the data (Figure 1b).",4.1. Experiments,[0],[0]
"This makes sense, since in this toy problem the most informative inputs are located in regions where data is scarce.",4.1. Experiments,[0],[0]
"However, this may not be the case in more complicated settings.",4.1. Experiments,[0],[0]
"Finally, we note that the total predictive uncertainty (Figure 1d) fails to identify informative regions.
",4.1. Experiments,[0],[0]
The decomposition of uncertainty allows us to identify informative inputs when the noise is bimodal.,4.1. Experiments,[0],[0]
Next we consider a toy problem given by a regression task with bimodal data.,4.1. Experiments,[0],[0]
We define x ∈,4.1. Experiments,[0],[0]
"[−0.5, 2] and y = 10 sin(x) + with probability 0.5 and y = 10 cos(x) + , otherwise, where ∼ N (0, 1) and is independent of x.",4.1. Experiments,[0],[0]
We sample 750 values of x from an exponential distribution with λ = 2.,4.1. Experiments,[0],[0]
Figure 2a shows the data.,4.1. Experiments,[0],[0]
"We have many points on the left, but few on the right.
",4.1. Experiments,[0],[0]
Figure 2 shows the results obtained (see caption for details).,4.1. Experiments,[0],[0]
"Figure 2c shows that the BNN+LV has learned the bimodal structure in the data and Figure 2d shows how the total predictive uncertainty increases on the right, where data is scarce.",4.1. Experiments,[0],[0]
"The aleatoric uncertainty (Figure 2e), by contrast,
has an almost symmetric form around x = 0.75, taking lower values at this location.",4.1. Experiments,[0],[0]
This makes sense since the data generating process is symmetric around x = 0.75 and the noise changes from bimodal to unimodal when one gets closer to x = 0.75.,4.1. Experiments,[0],[0]
"Figure 2f shows an estimate of the epistemic uncertainty, which as expected increases with x.
The decomposition of uncertainty identifies informative inputs when noise is both heteroscedastic and bimodal.",4.1. Experiments,[0],[0]
"We consider data sampled from a 2D stochastic system called the wet-chicken (Hans & Udluft, 2009; Depeweg et al., 2016), see the supplementary material for details.",4.1. Experiments,[0],[0]
"The wet-chicken transition dynamics exhibit complex stochastic patterns: bimodality, heteroscedasticity and truncation (the agent cannot move beyond the boundaries of state space:",4.1. Experiments,[0],[0]
"[0, 5]2).",4.1. Experiments,[0],[0]
"The data are 7, 500 state transitions collected by random exploration.",4.1. Experiments,[0],[0]
Figure 3a shows the states visited.,4.1. Experiments,[0],[0]
"For each transition, the BNN+LV predicts the next state given the current one and the action applied.",4.1. Experiments,[0],[0]
"Figure 3d shows that the epistemic uncertainty is highest in the top right corner,
while data is most scarce in the bottom right corner.",4.1. Experiments,[0],[0]
"The reason for this result is that the wet-chicken dynamics bring the agent back to y = 0 whenever the agent goes beyond y = 5, but this does not happen for y = 0 where the agent just bounces back.",4.1. Experiments,[0],[0]
"Therefore, learning the dynamics is more difficult and requires more data at y = 5 than at y = 0.",4.1. Experiments,[0],[0]
"The epistemic uncertainty captures this property, but the total predictive uncertainty (Figure 3b) does not.
",4.1. Experiments,[0],[0]
Active learning with BBN+LV is improved by using the uncertainty decomposition.,4.1. Experiments,[0],[0]
We evaluate the gains obtained by using Eq.,4.1. Experiments,[0],[0]
"(5) with BNN+LV, when collecting
data in three toy active learning problems.",4.1. Experiments,[0],[0]
"We refer to this method as Ibb-α(W, y?) and compare it with two baselines.",4.1. Experiments,[0],[0]
"The first one also uses a BNN+LV to describe data, but does not perform a decomposition of predictive uncertainty, that is, this method uses H(y?|x?) instead of Eq.",4.1. Experiments,[0],[0]
(5) for active learning.,4.1. Experiments,[0],[0]
We call this method Hbb-α(y?|x?).,4.1. Experiments,[0],[0]
The second baseline is given by a Gaussian process (GP) model which collects data according to H(y?|x?),4.1. Experiments,[0],[0]
since in this case the uncertainty decomposition is not necessary because the GP model does not include latent variables.,4.1. Experiments,[0],[0]
"The GP model assumes Gaussian noise and is not able to capture complex stochastic patterns.
",4.1. Experiments,[0],[0]
"The three problems considered correspond to the datasets from Figures 1, 2 and 3.",4.1. Experiments,[0],[0]
The general set-up is as follows.,4.1. Experiments,[0],[0]
We start with the available data shown in the previous figures.,4.1. Experiments,[0],[0]
"At each iteration, we select a batch of data points to label from a pool set which is sampled uniformly at random in input space.",4.1. Experiments,[0],[0]
The selected data is then included in the training set and the log-likelihood is evaluated on a separate test set.,4.1. Experiments,[0],[0]
"This process is performed for 150 iterations and we repeat all experiments 5 times.
",4.1. Experiments,[0],[0]
Table 1 shows the results obtained.,4.1. Experiments,[0],[0]
"Overall, BNN+LV outperform GPs in terms of predictive performance.",4.1. Experiments,[0],[0]
"We also see significant gains of Ibb-α(W, y?) over H(y?|x?) on the heteroscedastic and wet-chicken tasks, whereas their results are similar on the bimodal task.",4.1. Experiments,[0],[0]
"The reason for this is that, in the latter task, the epistemic and the total uncertainty have a similar behaviour as shown in Figures 2d and 2f.",4.1. Experiments,[0],[0]
"Finally, we note that heteroscedastic GPs (Le et al., 2005) will likely perform similar to BNN+LV in the heteroscedastic task from Figure 2, but they will fail in the other two settings considered (Figures 2 and 3).",4.1. Experiments,[0],[0]
"We propose an extension of the “risk-sensitive criterion” for safe model-based RL (Garcı́a & Fernández, 2015) to balance the risks produced by epistemic and aleatoric uncertainties.
",5. Risk-sensitive Reinforcement Learning,[0],[0]
"We focus on batch RL with continuous state and action spaces (Lange et al., 2012): we are given a batch of state transitions D = {(st,at, st+1)} formed by triples contain-
ing the current state st, the action applied at and the next state st+1.",5. Risk-sensitive Reinforcement Learning,[0],[0]
"In addition toD, we are also given a cost function c.",5. Risk-sensitive Reinforcement Learning,[0],[0]
"The goal is to obtain from D a policy in parametric form that minimizes c on average under the system dynamics.
",5. Risk-sensitive Reinforcement Learning,[0],[0]
"In model-based RL, the first step consists in learning a dynamics model fromD. We assume that the true dynamical system can be expressed by an unknown neural network with latent variables:
st = ftrue(st−1,at−1, zt;Wtrue) , zt ∼ N (0, γ) , (7)
whereWtrue denotes the weights of the network and st−1, at−1 and zt are the inputs to the network.",5. Risk-sensitive Reinforcement Learning,[0],[0]
"We use BNN+LV from Section 2 to approximate a posterior q(W, z) using the batch D (Depeweg et al., 2016).
",5. Risk-sensitive Reinforcement Learning,[0],[0]
In model-based policy search we optimize a policy given by a deterministic neural network with weightsWπ.,5. Risk-sensitive Reinforcement Learning,[0],[0]
"This parametric policy returns an action at as a function of st, that is, at = π(st;Wπ).",5. Risk-sensitive Reinforcement Learning,[0],[0]
The policy parameters Wπ can be tuned by minimizing the expectation of the cost C =∑T t=1,5. Risk-sensitive Reinforcement Learning,[0],[0]
"ct over a finite horizon T with respect to the belief q(W), where ct = c(st).",5. Risk-sensitive Reinforcement Learning,[0],[0]
"This expected cost is obtained by averaging across multiple virtual roll-outs as described next.
",5. Risk-sensitive Reinforcement Learning,[0],[0]
"Given s0, we sampleW ∼ q and simulate state trajectories for T steps using the model st+1 = f(st,at, zt;W) + t+1 with policy at = π(st;Wπ), input noise zt ∼ N (0, γ) and additive noise t+1 ∼ N (0,Σ).",5. Risk-sensitive Reinforcement Learning,[0],[0]
"By averaging across these roll-outs, we obtain a Monte Carlo approximation of the expected cost given the initial state s0:
J(Wπ)",5. Risk-sensitive Reinforcement Learning,[0],[0]
=,5. Risk-sensitive Reinforcement Learning,[0],[0]
E [C] = E,5. Risk-sensitive Reinforcement Learning,[0],[0]
[∑T t=1,5. Risk-sensitive Reinforcement Learning,[0],[0]
"ct ] , (8)
where, E[·] denotes here an average across virtual roll-outs starting from s0 and, to simplify our notation, we have made the dependence on s0 implicit.",5. Risk-sensitive Reinforcement Learning,[0],[0]
The policy search algorithm optimizes the expectation of Eq.,5. Risk-sensitive Reinforcement Learning,[0],[0]
"(8) when s0 is sampled uniformly from D, that is, Es0∼D[J(Wπ)].",5. Risk-sensitive Reinforcement Learning,[0],[0]
"This quantity can be easily approximated by Monte Carlo and if model, policy, and cost function are differentiable, we are able to tuneWπ by stochastic gradient descent.
",5. Risk-sensitive Reinforcement Learning,[0],[0]
"The “risk-sensitive criterion” (Garcı́a & Fernández, 2015)
changes Eq.",5. Risk-sensitive Reinforcement Learning,[0],[0]
"(8) to atain a balance between expected cost and risk, where the risk typically penalizes the deviations of C from E[C] during the virtual roll-outs with initial state s0.",5. Risk-sensitive Reinforcement Learning,[0],[0]
"For example, the risk-sensitive objective could be
J(Wπ)",5. Risk-sensitive Reinforcement Learning,[0],[0]
=,5. Risk-sensitive Reinforcement Learning,[0],[0]
"E [C] + βσ(C) , (9)
where σ(C) is the standard deviation of C across virtual roll-outs starting from s0 and the risk-sensitive parameter β determines the amount of risk-avoidance (β ≥ 0) or riskseeking behavior (β < 0) when optimizingWπ .
",5. Risk-sensitive Reinforcement Learning,[0],[0]
"Instead of working directly with the risk on the final cost C, we consider the sum of risks on the individual costs c1, . . .",5. Risk-sensitive Reinforcement Learning,[0],[0]
", cT .",5. Risk-sensitive Reinforcement Learning,[0],[0]
"The reason for this is that the latter is a more restrictive criterion since low risk on the ct will imply low risk on C, but not the other way around.",5. Risk-sensitive Reinforcement Learning,[0],[0]
Let σ(ct) denote the standard deviation of ct over virtual roll-outs starting from s0.,5. Risk-sensitive Reinforcement Learning,[0],[0]
We can then explicitly write σ(ct) in terms of its aleatoric and epistemic components by using the decomposition of uncertainty from Eq.,5. Risk-sensitive Reinforcement Learning,[0],[0]
(4).,5. Risk-sensitive Reinforcement Learning,[0],[0]
"In particular,
σ(ct) = { σ2q(W)(E[ct|W]) +Eq(W)[σ2(ct|W)]",5. Risk-sensitive Reinforcement Learning,[0],[0]
"} 1 2 , (10)
where E[ct|W] and σ2(ct|W) denote the mean and variance of ct under virtual roll-outs from s0 performed with policy Wπ and under the dynamics of a BNN+LV with parameters W .",5. Risk-sensitive Reinforcement Learning,[0],[0]
In a similar manner as in Eq.,5. Risk-sensitive Reinforcement Learning,[0],[0]
"(4), the operators Eq(W)",5. Risk-sensitive Reinforcement Learning,[0],[0]
[·] and σ2q(W)(·) in Eq.,5. Risk-sensitive Reinforcement Learning,[0],[0]
"(10) compute the mean and variance of their arguments whenW ∼ q(W).
",5. Risk-sensitive Reinforcement Learning,[0],[0]
The two terms inside the square root in Eq.,5. Risk-sensitive Reinforcement Learning,[0],[0]
(10) have a clear interpretation.,5. Risk-sensitive Reinforcement Learning,[0],[0]
"The first one, Eq(W)[σ2(ct|W)], represents the risk originating from the sampling of z and in the virtual roll-outs.",5. Risk-sensitive Reinforcement Learning,[0],[0]
We call this term the aleatoric risk.,5. Risk-sensitive Reinforcement Learning,[0],[0]
"The second term, σ2q(W)(E[ct|W]), encodes the risk originating from the sampling ofW in the virtual roll-outs.",5. Risk-sensitive Reinforcement Learning,[0],[0]
"We call this term the epistemic risk.
",5. Risk-sensitive Reinforcement Learning,[0],[0]
We can now extend the objective in Eq.,5. Risk-sensitive Reinforcement Learning,[0],[0]
(8) with a new risk term that balances the epistemic and aleatoric risks.,5. Risk-sensitive Reinforcement Learning,[0],[0]
This term is obtained by first using risk-sensitive parameters β,5. Risk-sensitive Reinforcement Learning,[0],[0]
and γ to balance the epistemic and aleatoric components in Eq.,5. Risk-sensitive Reinforcement Learning,[0],[0]
"(10), and then summing the resulting expression for t = 1, . . .",5. Risk-sensitive Reinforcement Learning,[0],[0]
", T :
σ(γ, β) = T∑ t=1 { β2σ2q(W)(E[ct|W]) + γ2Eq(W)[σ2(ct|W)] } 1 2 .
",5. Risk-sensitive Reinforcement Learning,[0],[0]
"Therefore, our ‘risk-sensitive criterion” uses the function J(Wπ)",5. Risk-sensitive Reinforcement Learning,[0],[0]
=,5. Risk-sensitive Reinforcement Learning,[0],[0]
"E [C] + σ(γ, β), which can be approximated via Monte Carlo and optimized using stochastic gradient descent.",5. Risk-sensitive Reinforcement Learning,[0],[0]
"The Monte Carlo approximation is generated by performing M ×N roll-outs with starting state s0 sampled uniformly fromD. For this,W is sampled from q(W) a total of M times and then, for each of these samples, N roll-outs are performed withW fixed and sampling only the latent
variables and the additive Gaussian noise in the BBN+LV.",5. Risk-sensitive Reinforcement Learning,[0],[0]
"Let zm,nt and m,n t be the samples of the latent variables and the additive Gaussian noise at step t during the n-th roll-out for them-th sample ofW , which we denote byWm.",5. Risk-sensitive Reinforcement Learning,[0],[0]
"Then cm,n(t) = c(s Wm,{zm,n1 ,...,z m,n t },{ m,n 1 ,..., m,n t },Wπ
t ) denotes the cost obtained at time t in that roll-out.",5. Risk-sensitive Reinforcement Learning,[0],[0]
All these cost values obtained at time t are stored in the M ×N matrix C(t).,5. Risk-sensitive Reinforcement Learning,[0],[0]
"The Monte Carlo estimate of J(Wπ) is then
J(Wπ)",5. Risk-sensitive Reinforcement Learning,[0],[0]
≈ T∑ t=1,5. Risk-sensitive Reinforcement Learning,[0],[0]
"{ 1TC(t)1 MN +
{ β2σ̂2",5. Risk-sensitive Reinforcement Learning,[0],[0]
[C(t)1/N ],5. Risk-sensitive Reinforcement Learning,[0],[0]
"+ γ2 1
M M∑ m=1",5. Risk-sensitive Reinforcement Learning,[0],[0]
"σ̂2 [C(t)m,·]
} 1 2 }
, (11)
where 1 denotes a vector with all of its entries equal to 1, C(t)m,· is a vector with the m-th row of C(t) and σ̂2[x] returns the empirical variance of the entries in vector x.
By setting β and γ to specific values in Eq.",5. Risk-sensitive Reinforcement Learning,[0],[0]
"(11), the user can choose different trade-offs between cost, aleatoric and epistemic risk: for γ = 0",5. Risk-sensitive Reinforcement Learning,[0],[0]
the term inside the square root is β2 times σ̂2 [C(t)1/N ],5. Risk-sensitive Reinforcement Learning,[0],[0]
which is a Monte Carlo approximation of the epistemic risk in Eq.,5. Risk-sensitive Reinforcement Learning,[0],[0]
(10).,5. Risk-sensitive Reinforcement Learning,[0],[0]
"Similarly, for β = 0, inside the square root we obtain γ2 times 1M ∑M m=1",5. Risk-sensitive Reinforcement Learning,[0],[0]
"σ̂
2",5. Risk-sensitive Reinforcement Learning,[0],[0]
"[C(t)m,·] which approximates the aleatoric risk.",5. Risk-sensitive Reinforcement Learning,[0],[0]
"For β = γ the standard risk criterion σ(ct) is obtained, weighted by β.",5. Risk-sensitive Reinforcement Learning,[0],[0]
The epistemic risk term in Eq.,5.1. Model-bias and Noise aversion,[0],[0]
(10) can be connected with the concept of model-bias in model-based RL.,5.1. Model-bias and Noise aversion,[0],[0]
A policy with Wπ is optimized on the model but executed on the ground truth system.,5.1. Model-bias and Noise aversion,[0],[0]
"The more model and ground truth differ, the more the policy is ’biased’ by the model (Deisenroth & Rasmussen, 2011).",5.1. Model-bias and Noise aversion,[0],[0]
"Given the initial state s0, we can quantify this bias with respect to the policy parametersWπ as
b(Wπ) =",5.1. Model-bias and Noise aversion,[0],[0]
"T∑ t=1 (Etrue[ct]−E[ct])2 , (12)
where Etrue[ct] is the expected cost obtained at time t across roll-outs starting at s0, under the ground truth dynamics and with policy π(st;Wπ).",5.1. Model-bias and Noise aversion,[0],[0]
"E[ct] is the same expectation but under BNN+LV dynamics sampled from q(W) on each individual roll-out.
",5.1. Model-bias and Noise aversion,[0],[0]
Eq. (12) is impossible to compute in practice because we do not know the ground truth dynamics.,5.1. Model-bias and Noise aversion,[0],[0]
"However, as indicated in Eq.",5.1. Model-bias and Noise aversion,[0],[0]
"(7), we assume that the true dynamic is given by a neural network with latent variables and weightsWtrue.",5.1. Model-bias and Noise aversion,[0],[0]
"We can then rewrite Etrue[ct] as E[ct|Wtrue] and since we do not knowWtrue, we can further assume thatWtrue ∼ q(W).",5.1. Model-bias and Noise aversion,[0],[0]
"The expected model-bias is then
E[b(Wπ)]",5.1. Model-bias and Noise aversion,[0],[0]
"= Eq(Wtrue) { T∑ t=1 (E[ct|Wtrue]−E[ct])2 }
= T∑ t=1",5.1. Model-bias and Noise aversion,[0],[0]
σ2q(Wtrue)(E[ct|Wtrue]) .,5.1. Model-bias and Noise aversion,[0],[0]
"(13)
We see that our definition of epistemic risk also represents an estimate of model-bias in model-based RL.",5.1. Model-bias and Noise aversion,[0],[0]
"This risk term will guide the policy to operate in areas of state space where model-bias is expected to be low.
",5.1. Model-bias and Noise aversion,[0],[0]
The aleatoric risk term in Eq.,5.1. Model-bias and Noise aversion,[0],[0]
(10) can be connected with the concept of noise aversion.,5.1. Model-bias and Noise aversion,[0],[0]
"Let σ2(ct|Wtrue) be the variance obtained at time t across roll-outs starting at s0, under the ground truth dynamics and with policy π(st;Wπ).",5.1. Model-bias and Noise aversion,[0],[0]
"Assuming Wtrue ∼ q(W), the expected variance is then Eq(Wtrue)[σ
2(ct|Wtrue)].",5.1. Model-bias and Noise aversion,[0],[0]
This term will guide the policy to operate in areas of state space where the stochasticity of the cost is low.,5.1. Model-bias and Noise aversion,[0],[0]
Assuming a deterministic cost function this stochasticity is determined by the model’s predictions that originate from zt and t.,5.1. Model-bias and Noise aversion,[0],[0]
We investigate the following questions: To what extent does our new risk criterion reduce model-bias?,5.2. Experiments,[0],[0]
What trade-offs do we observe between average cost and model-bias?,5.2. Experiments,[0],[0]
"How does the decomposition compare to other simple methods?
",5.2. Experiments,[0],[0]
We consider two model-based RL scenarios.,5.2. Experiments,[0],[0]
"The first one is given by the industrial benchmark (IB), a publicly available simulator with properties inspired by real industrial systems (Hein et al., 2017).",5.2. Experiments,[0],[0]
"The second RL scenario is a modified version of the HAWC2 wind turbine simulator (Larsen & Hansen, 2007), which is widely used for the study of wind turbine dynamics (Larsen et al., 2015).
",5.2. Experiments,[0],[0]
"We are given a batch of data formed by state transitions generated by a behavior policy πb, for example, from an already running system.",5.2. Experiments,[0],[0]
The behavioral policy has limited randomness and will keep the system dynamics constrained to a reduced manifold in state space.,5.2. Experiments,[0],[0]
This means that large portions of state space will be unexplored and uncertainty will be high in those regions.,5.2. Experiments,[0],[0]
"The supplementary material contains full details on the experimental setup and the πb.
",5.2. Experiments,[0],[0]
"We consider the risk-sensitive criterion from Section 5 for different choices of β and γ, comparing it with 3 baselines.",5.2. Experiments,[0],[0]
The first baseline is obtained by setting β = γ = 0.,5.2. Experiments,[0],[0]
"In this case, the policy optimization ignores any risk.",5.2. Experiments,[0],[0]
The second baseline is obtained when β = γ.,5.2. Experiments,[0],[0]
"In this case, the risk criterion simplifies to βσ(ct), which corresponds to the traditional risk-sensitive approach in Eq.",5.2. Experiments,[0],[0]
"(9), but applied to the individual costs c1, . . .",5.2. Experiments,[0],[0]
", cT .",5.2. Experiments,[0],[0]
"The last baseline uses a deterministic neural network to model the dynamics and a nearest neighbor approach to quantify risk: for each state st generated in a roll-out, we calculate the Euclidean distance of that state to the nearest one in the training data.",5.2. Experiments,[0],[0]
The average value of the distance metric for st across roll-outs is then an approximation to σ(ct).,5.2. Experiments,[0],[0]
"To reduce computational
cost, we summarize the training data using the centroids returned by an execution of the k-means clustering method.",5.2. Experiments,[0],[0]
"We denote this method as the nn-baseline.
",5.2. Experiments,[0],[0]
Figure 4a shows result on the industrial benchmark.,5.2. Experiments,[0],[0]
The y-axis in the plot is the average total cost at horizon T obtained by the policy in the ground truth system.,5.2. Experiments,[0],[0]
The xaxis is the average model-bias in the ground truth system according to Eq.,5.2. Experiments,[0],[0]
(12).,5.2. Experiments,[0],[0]
Each individual curve in the plot is obtained by fixing γ to a specific value (line colour) and then changing β (circle colour).,5.2. Experiments,[0],[0]
"The policy that ignores risk (β = γ = 0) results in both high model-bias and high cost when evaluated on the ground truth, which indicates overfitting.",5.2. Experiments,[0],[0]
"As β increases, the policies put more emphasis on avoiding model-bias, but at the same time the average cost increases.",5.2. Experiments,[0],[0]
The best tradeoff is obtained by the dark red curve with γ = 0.,5.2. Experiments,[0],[0]
The risk criterion is then β ∑T t=1 σq(W)(E[ct|W]).,5.2. Experiments,[0],[0]
"In this problem, adding aleatoric risk by setting γ > 0 decreases performance.",5.2. Experiments,[0],[0]
"The nn-baseline shows a similar pattern as the BNN+LV approach, but the trade-off between model-bias and cost is worse.
",5.2. Experiments,[0],[0]
Figure 5 shows roll-outs for three different policies and a fixed initial state s0.,5.2. Experiments,[0],[0]
Figure 5a shows results for a policy learned with γ = β = 0.,5.2. Experiments,[0],[0]
"This policy ignores risk, and as a consequence, the mismatch between predicted performance on the model and on the ground truth increases after t = 20.",5.2. Experiments,[0],[0]
This result illustrates how model-bias can lead to policies with high costs at test time.,5.2. Experiments,[0],[0]
"Figure 5b shows results for policy that was trained while penalizing epistemic risk (β = 4, γ = 0).",5.2. Experiments,[0],[0]
"In this case, the average costs under the BNN+LV model and the ground truth are similar, and the overall ground truth cost is lower than in Figure 5a.",5.2. Experiments,[0],[0]
"Finally, Figure 5c shows results for a noise averse policy (β = 0, γ = 4).",5.2. Experiments,[0],[0]
"In this case, the model bias is slightly higher than in the previous figure, but the stochasticity is lower.
",5.2. Experiments,[0],[0]
The results for wind turbine simulator can be found in Figure 4b.,5.2. Experiments,[0],[0]
"In this case, the best trade-offs between expected cost and model-bias are obtained by the policies with γ = 7.5.",5.2. Experiments,[0],[0]
These policies are noise averse and will try to avoid noisy regions in state space.,5.2. Experiments,[0],[0]
"This makes sense because in wind turbines, high noise regions in state space are those where the effect of wind turbulence will have a strong impact on the average cost.",5.2. Experiments,[0],[0]
"The distinction between aleatoric and epistemic uncertainty has been recognized in many fields within machine learning, often within the context of specific subfields, models, and objectives.",6. Related Work,[0],[0]
"Kendall & Gal (2017) consider a decomposition of uncertainty in the context of computer vision with heteroscedastic Gaussian output noise, while McAllister (2016) consider a decomposition in GPs for model-based RL.
",6. Related Work,[0],[0]
"Within reinforcement learning, Bayesian notions of model uncertainty have a long history (Schmidhuber, 1991b;a; Dearden et al., 1999; Still & Precup, 2012; Sun et al., 2011; Maddison et al., 2017).",6. Related Work,[0],[0]
"The mentioned works typically consider the online case, where model uncertainty (a.k.a. curiosity) is used to guide exploration, e.g. in Houthooft et al. (2016) the uncertainty of a BNN model is used to guide exploration assuming deterministic dynamics.",6. Related Work,[0],[0]
"In contrast, we focus on the batch setting with stochastic dynamics.
",6. Related Work,[0],[0]
"Model uncertainty is used in safe or risk-sensitive RL (Mihatsch & Neuneier, 2002; Garcı́a & Fernández, 2015).",6. Related Work,[0],[0]
"In safe RL numerous other approaches exists for safe exploration (Joseph et al., 2013; Hans et al., 2008; Garcia & Fernández, 2012; Berkenkamp et al., 2017).",6. Related Work,[0],[0]
"Uncertainties over transition probabilities have been studied in discrete MDPs since a long time (Shapiro & Kleywegt, 2002; Nilim & El Ghaoui, 2005; Bagnell et al., 2001) often with a focus on worst-case avoidance.",6. Related Work,[0],[0]
Our work extends this to continuous state and action space using scalable probabilistic models.,6. Related Work,[0],[0]
"Our decomposition enables a practitioner to adjust the optimization criterion to specific decision making,.
",6. Related Work,[0],[0]
"Within active learning many approaches exist that follow an information theoretic approach (MacKay, 1992; HernándezLobato & Adams, 2015; Guo & Greiner, 2007).",6. Related Work,[0],[0]
"To our knowledge, all of these approaches however use deterministic methods (mostly GPs) as model class.",6. Related Work,[0],[0]
"Perhaps closest to our work is BALD (Houlsby et al., 2012), however because GPs are used, this approach cannot model problems with complex noise.",6. Related Work,[0],[0]
We have described a decomposition of predictive uncertainty into its epistemic and aleatoric components when working with Bayesian neural networks with latent variables.,7. Conclusion,[0],[0]
"We have shown how this decomposition of uncertainty can be used for active learning, where it naturally arises from an information-theoretic perspective.",7. Conclusion,[0],[0]
We have also used the decomposition to propose a novel risk- sensitive criterion for model-based reinforcement learning which decomposes risk into model-bias and noise aversion components.,7. Conclusion,[0],[0]
Our experiments illustrate how the described decomposition of uncertainty is useful for efficient and risk-sensitive learning.,7. Conclusion,[0],[0]
The goal here is to show that the decomposition we obtained in the active learning examples is not a result of our approximation using black-box α-divergence minimization but a property of BNN+LV themselves.,A. Hamiltonian Monte Carlo Solution to Toy Problems,[0],[0]
To that end we will approximate using HMC.,A. Hamiltonian Monte Carlo Solution to Toy Problems,[0],[0]
"After a burn-in of 500, 000 samples we sample from the posterior 200, 000 samples.",A. Hamiltonian Monte Carlo Solution to Toy Problems,[0],[0]
We thin out 90% by only keeping every tenth sample.,A. Hamiltonian Monte Carlo Solution to Toy Problems,[0],[0]
"We define the stochastic function y = 7 sin(x) + 3| cos(x/2)| with ∼ N (0, 1).",A.1. Heteroscedastic Problem,[0],[0]
The data availability is limited to specific regions of x.,A.1. Heteroscedastic Problem,[0],[0]
"In particular, we sample 750 values of x from a mixture of three Gaussians with mean parameters {µ1 = −4, µ2 = 0, µ3 = 4}, variance parameters {σ1 = 25 , σ2 = 0.9, σ3 = 2 5} and with each Gaussian component having weight equal to 1/3 in the mixture.",A.1. Heteroscedastic Problem,[0],[0]
Figure 6a shows the raw data.,A.1. Heteroscedastic Problem,[0],[0]
"We have lots of points at both borders of the x axis and in the center, but little data available in between.",A.1. Heteroscedastic Problem,[0],[0]
We consider a toy problem given by a regression task with bimodal data.,A.2. Bimodal Problem,[0],[0]
We define x ∈,A.2. Bimodal Problem,[0],[0]
"[−0.5, 2] and y = 10 sin(x) + with probability 0.5 and y = 10 cos(x) + , otherwise, where ∼ N (0, 1) and is independent of x.",A.2. Bimodal Problem,[0],[0]
The data availability is not uniform in x.,A.2. Bimodal Problem,[0],[0]
In particular we sample 750 values of x from an exponential distribution with λ = 2,A.2. Bimodal Problem,[0],[0]
In the main document we pointed out that the decomposition of uncertainty does not work as good with other values of α.,B. Solutions to Toy Problems for different values of α,[0],[0]
"We will see in the following, that lower values of α will put more and more emphasis on the latent variable z.",B. Solutions to Toy Problems for different values of α,[0],[0]
"We observe that the epistemic uncertainty will vanish as the α-divergence minimization approaches variational Bayes.
B.1.",B. Solutions to Toy Problems for different values of α,[0],[0]
"α = 0.5
B.2.",B. Solutions to Toy Problems for different values of α,[0],[0]
"V B solutions to Toy Problems
We approximate the method variational Bayes by setting α to 10−6.",B. Solutions to Toy Problems for different values of α,[0],[0]
All models start with the available described in the respective paragraphs.,C.1. Active Learning,[0],[0]
We train for 5000 epochs a BNN+LV with two-hidden layer and 20 hidden units per layer.,C.1. Active Learning,[0],[0]
We use Adam as optimizer with a learning rate of 0.001.,C.1. Active Learning,[0],[0]
"For Gaussian
processes(GPs) we use the standard RBF kernel using the python GPy implementation.",C.1. Active Learning,[0],[0]
"For the entropy estimation we use a nearest-neighbor approach as explained in the main document with k = 25 and 500 samples of q(W ) and 500 samples of p(z).
",C.1. Active Learning,[0],[0]
For active learning we evaluate performance using a held-out test set of size 500 for the bimodal and heteroscedastic problem and 2500 for the wet-chicken problem.,C.1. Active Learning,[0],[0]
The test data is sampled uniformly in state (and action) space.,C.1. Active Learning,[0],[0]
In each of the n = 150 iterations we sample a pool set of size 50 uniformly in input space.,C.1. Active Learning,[0],[0]
In each iteration a method can decide to include 5 data points into the training set.,C.1. Active Learning,[0],[0]
"After that, the models are re-trained (from scratch) and the performance is evaluated on the test set.",C.1. Active Learning,[0],[0]
We use the continuous two-dimensional version of the problem.,C.2. Wet-chicken,[0],[0]
A canoeist is paddling on a two-dimensional river.,C.2. Wet-chicken,[0],[0]
"The canoeist’s position at time t is (xt, yt).",C.2. Wet-chicken,[0],[0]
"The river has width w = 5 and length l = 5 with a waterfall at the end, that is, at yt = l.",C.2. Wet-chicken,[0],[0]
The canoeist wants to move as close to the waterfall as possible because at time t he gets reward rt = −(l − yt).,C.2. Wet-chicken,[0],[0]
"However, going beyond the waterfall boundary makes the canoeist fall down, having to start back again at the origin (0, 0).",C.2. Wet-chicken,[0],[0]
"At time t the canoeist can choose an action (at,x, at,y) ∈",C.2. Wet-chicken,[0],[0]
"[−1, 1]2 that represents the direction and magnitude of his paddling.",C.2. Wet-chicken,[0],[0]
The river dynamics have stochastic turbulences st and drift vt that depend on the canoeist’s position on the x axis.,C.2. Wet-chicken,[0],[0]
"The larger xt, the larger the drift and the smaller xt, the larger the turbulences.
",C.2. Wet-chicken,[0],[0]
The underlying dynamics are given by the following system of equations.,C.2. Wet-chicken,[0],[0]
"The drift and the turbulence magnitude are given by vt = 3xtw−1 and st = 3.5 − vt, respectively.",C.2. Wet-chicken,[0],[0]
"The new location (xt+1, yt+1) is given by the current location (xt, yt) and current action (at,x, at,y) using
xt+1 =    0 if xt + at,x < 0 0",C.2. Wet-chicken,[0],[0]
if ŷt+1 >,C.2. Wet-chicken,[0],[0]
"l w if xt + at,x > w xt + at,x otherwise , yt+1 =    0 if ŷt+1 < 0 0",C.2. Wet-chicken,[0],[0]
"if ŷt+1 > l ŷt+1 otherwise , (14)
where ŷt+1 = yt",C.2. Wet-chicken,[0],[0]
"+ (at,y − 1) +",C.2. Wet-chicken,[0],[0]
"vt + stτt and τt ∼ Unif([−1, 1]) is a random variable that represents the current turbulence.",C.2. Wet-chicken,[0],[0]
"As the canoeist moves closer to the waterfall, the distribution for the next state becomes increasingly bi-modal because when he is close to the waterfall, the change in the current location can be large if the canoeist falls down the waterfall and starts again at (0, 0).",C.2. Wet-chicken,[0],[0]
The distribution may also be truncated uniform for states close to the borders.,C.2. Wet-chicken,[0],[0]
"Furthermore the system
has heteroscedastic noise, the smaller the value of xt the higher the noise variance.",C.2. Wet-chicken,[0],[0]
C.3.1. INDUSTRIAL,C.3. Reinforcement Learning,[0],[0]
"BENCHMARK
Policies in the industrial benchmark specify changes ∆v , ∆g and ∆s in three steering variables v (velocity), g (gain) and s (shift) as a function of st.",C.3. Reinforcement Learning,[0],[0]
"In the behavior policy these changes are stochastic and sampled according to
∆v",C.3. Reinforcement Learning,[0],[0]
"=    zv , if v(t) < 40 −zv , if v(t) > 60 uv , otherwise
(15)
∆g =    zg , if g(t) < 40",C.3. Reinforcement Learning,[0],[0]
"−zg , if g(t) > 60 ug , otherwise
",C.3. Reinforcement Learning,[0],[0]
"(16)
∆s = us , (17)
where zv, zg ∼ N (0.5, 1√3 ) and uv, ug, us ∼ U(−1, 1).",C.3. Reinforcement Learning,[0],[0]
"The velocity v(t) and gain g(t) can take values in [0, 100].",C.3. Reinforcement Learning,[0],[0]
"Therefore, the data collection policy will try to keep these values only in the medium range given by the interval",C.3. Reinforcement Learning,[0],[0]
"[40, 60].",C.3. Reinforcement Learning,[0],[0]
"Because of this, large parts of the state space will be unobserved.",C.3. Reinforcement Learning,[0],[0]
"After collecting the data, the 30, 000 state transitions are used to train a BNN with latent variables with the same hyperparameters as in (Depeweg et al., 2016).",C.3. Reinforcement Learning,[0],[0]
"Finally, we train different policies using the Monte Carlo approximation and we set the horizon of T = 100 steps, with M = 50 and N = 25 and a minibatch size of 1 for 750 epochs.",C.3. Reinforcement Learning,[0],[0]
The total training time on a single CPU is around 18 hours.,C.3. Reinforcement Learning,[0],[0]
"In this problem we observe the turbine state s(t), with features such as current wind speed and currently produced power.",C.3.2. WIND TURBINE SIMULATOR,[0],[0]
"Our actions a(t) adjust the turbine’s behavior, with known upper and lower bounds.",C.3.2. WIND TURBINE SIMULATOR,[0],[0]
"The goal is to maximize energy output over a T -step horizon.
",C.3.2. WIND TURBINE SIMULATOR,[0],[0]
"We are given a batch of around 5,000 state transitions generated by a behavior policy πb.",C.3.2. WIND TURBINE SIMULATOR,[0],[0]
"The policy does limited exploration around the neutral action a(t) = 0.
",C.3.2. WIND TURBINE SIMULATOR,[0],[0]
The system is expected to be highly stochastic due to the unpredictability of future wind dynamics.,C.3.2. WIND TURBINE SIMULATOR,[0],[0]
"Furthermore the dimensionality of state observation is much higher than the action dimensionality, so, with the limited dataset that we have, we expect it to be very challenging to accurately learn the influence of the action on the reward.
",C.3.2. WIND TURBINE SIMULATOR,[0],[0]
First we train a BNN with two hidden layer and 50 hidden units per layer on the available batch using α-divergence minimization with α = 1.0.,C.3.2. WIND TURBINE SIMULATOR,[0],[0]
"In the second step, using the model, we train a policy with 20 hidden units on each of the two layers in the usual way, using the Monte Carlo estimate.",C.3.2. WIND TURBINE SIMULATOR,[0],[0]
The total training time on a single CPU is around 8 hours.,C.3.2. WIND TURBINE SIMULATOR,[0],[0]
"Bayesian neural networks with latent variables are scalable and flexible probabilistic models: They account for uncertainty in the estimation of the network weights and, by making use of latent variables, can capture complex noise patterns in the data.",abstractText,[0],[0]
We show how to extract and decompose uncertainty into epistemic and aleatoric components for decision-making purposes.,abstractText,[0],[0]
This allows us to successfully identify informative points for active learning of functions with heteroscedastic and bimodal noise.,abstractText,[0],[0]
"Using the decomposition we further define a novel risk-sensitive criterion for reinforcement learning to identify policies that balance expected cost, model-bias and noise aversion.",abstractText,[0],[0]
Decomposition of Uncertainty in Bayesian Deep Learning for Efficient and Risk-sensitive Learning,title,[0],[0]
"A good negotiator needs to decide on the strategy for achieving a certain goal (e.g., proposing $6000) and the realization of that strategy via generation of natural language (e.g., “I really need a car so I can go to work, but all I have is 6000, any more and I won’t be able to feed my children.”).
",1 Introduction,[0],[0]
"Most past work in NLP on negotiation focuses on strategy (dialogue management) with either no natural language (Cuayáhuitl et al., 2015; Cao et al., 2018) or canned responses (Keizer et al., 2017; Traum et al., 2008).",1 Introduction,[0],[0]
"Recently, end-to-end neural models (Lewis et al., 2017; He et al., 2017) are used to simultaneously learn dialogue strategy
and language realization from human-human dialogues, following the trend of using neural network models on both goal-oriented dialogue (Wen et al., 2017a; Dhingra et al., 2017) and opendomain dialogue (Sordoni et al., 2015; Li et al., 2017; Lowe et al., 2017).",1 Introduction,[0],[0]
"However, these models have two problems: (i) it is hard to control and interpret the strategies, and (ii) directly optimizing the agent’s goal through reinforcement learning often leads to degenerate solutions where the utterances become ungrammatical (Lewis et al., 2017) or repetitive (Li et al., 2016).
",1 Introduction,[0],[0]
"To alleviate these problems, our key idea is to decouple strategy and generation, which gives us control over the strategy such that we can achieve different negotiation goals (e.g., maximizing utility, achieving a fair deal) with the same language generator.",1 Introduction,[0],[0]
"Our framework consists of three components shown in Figure 1: First, the parser identifies keywords and entities to map each utterance to a coarse dialogue act capturing the highlevel strategic move.",1 Introduction,[0],[0]
"Then, the dialogue manager chooses a responding dialogue act based on a sequence-to-sequence model over coarse dialogue acts learned from parsed training dialogues.",1 Introduction,[0],[0]
"Finally, the generator produces an utterance given the dialogue act and the utterance history.
",1 Introduction,[0],[0]
"Our framework follows that of traditional goaloriented dialogue systems (Young et al., 2013), with one important difference: coarse dialogue acts are not intended to and cannot capture the full meaning of an utterance.",1 Introduction,[0],[0]
"As negotiation dialogues are fairly open-ended, the generator needs to depend on the full utterance history.",1 Introduction,[0],[0]
"For example, consider the first turn in Figure 1.",1 Introduction,[0],[0]
We cannot generate a response given only the dialogue act inform; we must also look at the previous question.,1 Introduction,[0],[0]
"However, we still optimize the dialogue manager in the coarse dialogue act space using supervised learning, reinforcement learning, or domain-
ar X
iv :1
80 8.
09 63
7v 1
[ cs
.C L
] 2
9 A
ug 2
01 8
specific knowledge.",1 Introduction,[0],[0]
"Existing human-human negotiation datasets are grounded in closed-domain games with a fixed set of objects such as Settlers of Catan (lumber, coal, brick, wheat, and sheep) (Afantenos et al., 2012; Asher et al., 2016) or item division (book, hat, and ball) (DeVault et al., 2015; Lewis et al., 2017).",1 Introduction,[0],[0]
These objects lack the richness of the real world.,1 Introduction,[0],[0]
"To study human negotiation in more open-ended settings that involve real goods, we scraped postings of items for sale from craigslist.org as our negotiation scenario.",1 Introduction,[0],[0]
"By hiring workers on Amazon Mechanical Turk (AMT) to play the role of buyers and sellers, we collected a new dataset (CRAIGSLISTBARGAIN) of negotiation",1 Introduction,[0],[0]
"dialogues.1 Compared to existing datasets, our more realistic scenario invites richer negotiation behavior involving open-ended aspects such as cheap talk or side offers.
",1 Introduction,[0],[0]
"We evaluate two families of systems modeling coarse dialogue acts and words respectively, which are optimized by supervised learning, reinforcement learning, or domain knowledge.",1 Introduction,[0],[0]
Each system is evaluated on our new CRAIGSLISTBARGAIN dataset and the DEALORNODEAL dataset of Lewis et al. (2017) by asking AMT workers to chat with the system in an A/B testing setting.,1 Introduction,[0],[0]
"We focus on two metrics: task-specific scores (e.g., utility) and human-likeness.",1 Introduction,[0],[0]
"We show that reinforcement learning on coarse dialogue acts avoids
1 Available at https://stanfordnlp.github.",1 Introduction,[0],[0]
"io/cocoa.
",1 Introduction,[0],[0]
"degenerate solutions, which was a problem in Li et al. (2016); Lewis et al. (2017).",1 Introduction,[0],[0]
Our modular model maintains reasonable human-like behavior while still optimizes the objective.,1 Introduction,[0],[0]
"Furthermore, we find that models trained over coarse dialogue acts are stronger negotiators (even with only supervised learning) and produce more diverse utterances than models trained over words.",1 Introduction,[0],[0]
"Finally, the interpretability of coarse dialogue acts allows system developers to combine the learned dialogue policy with hand-coded rules, thus imposing stronger control over the desired strategy.",1 Introduction,[0],[0]
Previous negotiation datasets were collected in the context of games.,2 Craigslist Negotiation Dataset,[0],[0]
"For example, Asher et al. (2016) collected chat logs from online Settlers of Catan.",2 Craigslist Negotiation Dataset,[0],[0]
"Lewis et al. (2017) asked two people to divide a set of hats, books, and balls.",2 Craigslist Negotiation Dataset,[0],[0]
"While such games are convenient for grounding and evaluation, it restricts the dialogue domain and the richness of the language.",2 Craigslist Negotiation Dataset,[0],[0]
Most utterances are direct offers such as “has anyone got wood for me?”,2 Craigslist Negotiation Dataset,[0],[0]
"and “I want the ball.”, whereas real-world negotiation would involve more information gathering and persuasion.
",2 Craigslist Negotiation Dataset,[0],[0]
"To encourage more open-ended, realistic negotiation, we propose the CRAIGSLISTBARGAIN task.",2 Craigslist Negotiation Dataset,[0],[0]
Two agents are assigned the role of a buyer and a seller; they are asked to negotiate the price of an item for sale on Craigslist given a description and photos.,2 Craigslist Negotiation Dataset,[0],[0]
"As with the real platform, the listing price is shown to both agents.",2 Craigslist Negotiation Dataset,[0],[0]
"We addition-
ally suggest a private price to the buyer as a target.",2 Craigslist Negotiation Dataset,[0],[0]
Agents chat freely in alternating turns.,2 Craigslist Negotiation Dataset,[0],[0]
"Either agent can enter an offer price at any time, which can be accepted or rejected by the partner.",2 Craigslist Negotiation Dataset,[0],[0]
"Agents also have the option to quit, in which case the task is completed with no agreement.
",2 Craigslist Negotiation Dataset,[0],[0]
"To generate the negotiation scenarios, we scraped postings on sfbay.craigslist.org from the 6 most popular categories (housing, furniture, cars, bikes, phones, and electronics).",2 Craigslist Negotiation Dataset,[0],[0]
"Each posting produces three scenarios with the buyer’s target prices at 0.5x, 0.7x and 0.9x of the listing price.",2 Craigslist Negotiation Dataset,[0],[0]
"Statistics of the scenarios are shown in Table 2.
",2 Craigslist Negotiation Dataset,[0],[0]
We collected 6682 human-human dialogues on AMT using the interface shown in Appendix A Figure 2.,2 Craigslist Negotiation Dataset,[0],[0]
The dataset statistics in Table 3 show that CRAIGSLISTBARGAIN has longer dialogues and more diverse utterances compared to prior datasets.,2 Craigslist Negotiation Dataset,[0],[0]
"Furthermore, workers were encouraged to embellish the item and negotiate side offers such as free delivery or pick-up.",2 Craigslist Negotiation Dataset,[0],[0]
This highly relatable scenario leads to richer dialogues such as the one shown in Table 1.,2 Craigslist Negotiation Dataset,[0],[0]
"We also observed various persuasion techniques listed in Table 4 such as embellishment, side offers, and appeals to sympathy.",2 Craigslist Negotiation Dataset,[0],[0]
"While end-to-end neural models have made promising progress in dialogue systems (Wen et al., 2017a; Dhingra et al., 2017), we find they
struggle to simultaneously learn the strategy and the rich utterances necessary to succeed in the CRAIGSLISTBARGAIN domain, e.g., Table 8(a) shows a typical dialogue between a human and a sequence-to-sequence-based bot, where the bot easily agrees.",3.1 Motivation,[0],[0]
We wish to now separate negotiation strategy and language generation.,3.1 Motivation,[0],[0]
Suppose the buyer says: “All right.,3.1 Motivation,[0],[0]
Well I think 275 is a little high for a 10 year old TV.,3.1 Motivation,[0],[0]
Can you lower the price some?,3.1 Motivation,[0],[0]
How about 150?”,3.1 Motivation,[0],[0]
We can capture the highest-order bit with a coarse dialogue act propose(price=150).,3.1 Motivation,[0],[0]
"Then, to generate the seller’s response, the agent can first focus on this coarse
dialogue act rather than having to ingest the freeform text all at once.",3.1 Motivation,[0],[0]
"Once a counter price is decided, the rest is open-ended justification for the proposed price, e.g., emphasizing the quality of the TV despite its age.
",3.1 Motivation,[0],[0]
"Motivated by these observations, we now describe a modular framework that extracts coarse dialogue acts from utterances, learns to optimize strategy in the dialogue act space, and uses retrieval to fill in the open-ended parts conditioned on the full dialogue history.",3.1 Motivation,[0],[0]
"Our goal is to build a dialogue agent that takes the dialogue history, i.e. a sequence of utterances x1, . . .",3.2 Overview,[0],[0]
", xt−1 along with the dialogue scenario c (e.g., item description), and produces a distribution over the responding utterance xt.
",3.2 Overview,[0],[0]
For each utterance xt,3.2 Overview,[0],[0]
"(e.g., “I am willing to pay $15”), we define a coarse dialogue act zt (e.g., propose(price=15)); the coarse dialogue act serves as a logical skeleton which does not attempt to capture the full semantics of the utterance.",3.2 Overview,[0],[0]
"Following the strategy of traditional goal-oriented dialogue systems (Young et al., 2013), we broadly define our model in terms of the following three modules:
1.",3.2 Overview,[0],[0]
A parser that (deterministically) maps an input utterance xt−1 into a coarse dialogue act zt−1 given the dialogue history,3.2 Overview,[0],[0]
"x<t and z<t, as well as the scenario c.
2.",3.2 Overview,[0],[0]
"A manager that predicts the responding dialogue act zt given past coarse dialogue acts z<t and the scenario c.
3.",3.2 Overview,[0],[0]
"A generator that turns the coarse dialogue act zt to a natural language response xt given the full dialogue history x<t.
Because coarse dialogue acts do not capture the full semantics, the parser and the generator maintains full access to the dialogue history.",3.2 Overview,[0],[0]
"The main
restriction is the manager examining the dialogue acts, which we show will reduce the risk of degeneracy during reinforcement learning Section 4.4.",3.2 Overview,[0],[0]
We now describe each module in detail (Figure 1).,3.2 Overview,[0],[0]
"Our framework is centered around the coarse dialogue act z, which consists of an intent and a set of arguments.",3.3 Parser,[0],[0]
"For example, “I am willing to pay $15” is mapped to propose(price=15).",3.3 Parser,[0],[0]
The fact that our coarse dialogue acts do not intend to capture the full semantics of a sentence allows us to use a simple rule-based parser.,3.3 Parser,[0],[0]
It detects the intent and its arguments by regular expression matching and a few if-then rules.,3.3 Parser,[0],[0]
"Our parser starts by detecting entities (e.g., prices, objects) and matching keyword patterns (e.g., “go lower”).",3.3 Parser,[0],[0]
"These signals are checked against an ordered list of rules, where we choose the first matched intent in the case of multiple matches.",3.3 Parser,[0],[0]
An unknown act is output if no rule is triggered.,3.3 Parser,[0],[0]
The list of intent parsing rules used are shown in Table 5.,3.3 Parser,[0],[0]
Please refer to Appendix B for argument parsing based on entity detection.,3.3 Parser,[0],[0]
"The dialogue manager decides what action zt the dialogue agent should take at each time step t given the sequence of past coarse dialogue acts z<t and the scenario c. Below, we describe three ways to learn the dialogue manager with increasing controllability: modeling human behavior in the training corpus (supervised learning), explicitly optimizing a reward function (reinforcement learning), and injecting hand-coded rules (hybrid policy).
",3.4 Manager,[0],[0]
Supervised learning.,3.4 Manager,[0],[0]
"Given a parsed training corpus, each training example is a sequence of coarse dialogue acts over one dialogue, z1, . . .",3.4 Manager,[0],[0]
", zT .",3.4 Manager,[0],[0]
"We learn the transition probabilities
pθ(zt | z<t, c) by maximizing the likelihood of the training data.
",3.4 Manager,[0],[0]
We use a standard sequence-to-sequence model with attention.,3.4 Manager,[0],[0]
"Each coarse dialogue act is represented as a sequence of tokens, i.e. an intent followed by each of its arguments, e.g., “offer 150”.",3.4 Manager,[0],[0]
"During the agent’s listening turn, an LSTM encodes the received coarse dialogue act; during its speaking turn, another LSTM decodes the tokens in the coarse dialogue act.",3.4 Manager,[0],[0]
"The hidden states are carried over the entire dialogue to provide full history.
",3.4 Manager,[0],[0]
The vocabulary of coarse dialogue acts is much smaller than the word vocabulary.,3.4 Manager,[0],[0]
"For example, our implementation includes fewer than 10 intents and argument values are normalized and binned (see Section 4.2).
",3.4 Manager,[0],[0]
Reinforcement learning.,3.4 Manager,[0],[0]
"Supervised learning aims to mimic the average human behavior, but sometimes we want to directly optimize for a particular dialogue goal.",3.4 Manager,[0],[0]
"In reinforcement learning, we define a reward R(z1:T ) on the entire sequence of coarse dialogue acts.",3.4 Manager,[0],[0]
"Specifically, we experiment with three reward functions:
• Utility is the objective of a self-interested agent.",3.4 Manager,[0],[0]
"For CRAIGSLISTBARGAIN, we set the utility function to be a linear function of the final price, such that the buyer has a utility of
1 at their target price, the seller has a utility of 1 at the listing price, and both agents have a utility of zero at the midpoint of the listing price and the buyer’s target price, making it a zero-sum game.",3.4 Manager,[0],[0]
"For DEALORNODEAL, utility is the total value of objects given to the agent.
",3.4 Manager,[0],[0]
"• Fairness aims to achieve equal outcome for both agents, i.e. the difference between two agents’ utilities.
",3.4 Manager,[0],[0]
"• Length is the number of utterances in a dialogue, thus encourages agents to chat as long as possible.
",3.4 Manager,[0],[0]
The reward is −1 if no agreement is reached.,3.4 Manager,[0],[0]
"We use policy gradient (Williams, 1992) for optimization.",3.4 Manager,[0],[0]
"Given a sampled trajectory z1:T and the final reward r, let ai be the i-th generated token (i.e. “action” taken by the policy) along the trajectory.",3.4 Manager,[0],[0]
"We update the parameters θ by
θ ←",3.4 Manager,[0],[0]
θ − η ∑ i ∇θ log pθ(ai |,3.4 Manager,[0],[0]
"a<i, c)(r − b) (1)
where η is the learning rate and b is a baseline estimated by the average return so far for variance reduction.
",3.4 Manager,[0],[0]
Hybrid policy.,3.4 Manager,[0],[0]
"Given the interpretable coarse dialogue acts, a simple option is to write a rulebased manager with domain knowledge, e.g., if zt−1 = greet, then zt = greet.",3.4 Manager,[0],[0]
We combine these rules with a learned manager to fine-tune the dialogue policy.,3.4 Manager,[0],[0]
"Specifically, the dialogue manager predicts the intent from a learned sequence model but fills in the arguments (e.g., price) using rules.",3.4 Manager,[0],[0]
"For example, given a predicted intent propose, we can set the price to be the average of the buyer’s and seller’s current proposals (a split-thedifference strategy).",3.4 Manager,[0],[0]
We use retrieval-based generation to condition on both the coarse dialogue act and the dialogue history.,3.5 Generator,[0],[0]
"Each candidate in our database for retrieval is a tuple of an utterance xt and its dialogue context xt−1, represented by both templates and coarse dialogue acts.",3.5 Generator,[0],[0]
"i.e. (d(xt−1), zt−1, d(xt), zt), where d is the template extractor.",3.5 Generator,[0],[0]
"Specifically, given a parsed training set, each utterance is converted to a template by delexicalizing arguments in its coarse dialogue act.",3.5 Generator,[0],[0]
"For example, “How about $150?”
becomes “How about [price]?”, where [price] is a placeholder to be filled in at generation time.
",3.5 Generator,[0],[0]
"At test time, given zt from the dialogue manager, the generator first retrieves candidates with the same intent as zt and zt−1.",3.5 Generator,[0],[0]
"Next, candidates are ranked by similarity between their context templates and the current dialogue context.",3.5 Generator,[0],[0]
"Specifically, we represent the context d(xt−1) as a TF-IDF weighted bag-of-words vector and similarity is computed by a dot product of two context vectors.",3.5 Generator,[0],[0]
"To encourage diversity, the generator samples an utterance from the top K candidates according to the distribution given by a trigram language model estimated on the training data.",3.5 Generator,[0],[0]
We test our approach on two negotiation tasks.,4.1 Tasks,[0],[0]
CRAIGSLISTBARGAIN (Section 2) asks a buyer and a seller to negotiate the price of an item for sale given its Craigslist post.,4.1 Tasks,[0],[0]
"DEALORNODEAL (Lewis et al., 2017) asks two agents to divide a set of items given their private utility functions.",4.1 Tasks,[0],[0]
"We compare two families of models: end-to-end neural models that directly map the input dialogue context to a sequence of output words, and our modular models that use coarse dialogue acts as the intermediate representation.
",4.2 Models,[0],[0]
"We start by training the word-based model and the act-based model with supervised learning (SL).
",4.2 Models,[0],[0]
"• SL(word): a sequence-to-sequence model with attention over previous utterances and the scenario, both embedded as a continuous Bag-of-Words;
• SL(act): our model described in Section 3 with a rule-based parser, a learned neural dialogue manager, and a retrieval-based generator.
",4.2 Models,[0],[0]
"To handle the large range of argument values (prices) in CRAIGSLISTBARGAIN for act-based models, we normalize the prices such that an agent’s target price is 1 and the bottomline price is 0.",4.2 Models,[0],[0]
"For the buyer, the target is given and the bottomline is the listing price.",4.2 Models,[0],[0]
"For the seller, the target is the listing price and the bottomline is set to 0.7x of the listing price.",4.2 Models,[0],[0]
"The prices are then
binned according to their approximate values with two digits after the decimal point.
",4.2 Models,[0],[0]
"Next, given the pretrained SL models, we fine-tune them with the three reward functions (Section 3.4), producing RLutility, RLfairness, and RLlength.
",4.2 Models,[0],[0]
"In addition, we compare with the hybrid model, SL(act)+rule.",4.2 Models,[0],[0]
"It predicts the next intent using a trigram language model learned over intent sequences in the training data, and fills in the arguments with hand-coded rules.",4.2 Models,[0],[0]
"For CRAIGSLISTBARGAIN, the only argument is the price.",4.2 Models,[0],[0]
"The agent always splits the difference when making counter proposals, rejects an offer if it is worse than its bottomline and accepts otherwise.",4.2 Models,[0],[0]
"For DEALORNODEAL, the agent maintains an estimate of the partner’s private utility function.",4.2 Models,[0],[0]
"In case of disagreement, it gives up the item with the lowest value of (own utility − partner utility) and takes an item of estimated zero utility to the partner.",4.2 Models,[0],[0]
The agent agrees whenever a proposal is better than the last one or its predefined target.,4.2 Models,[0],[0]
A high-level comparison of all models is shown in Table 6.,4.2 Models,[0],[0]
"CRAIGSLISTBARGAIN For SL(word), we use a sequence-to-sequence model with attention over 3 previous utterances and the negotiation scenario (embedded as a continuous Bag-of-Words).",4.3 Training Details,[0],[0]
"For both SL(word) and SL(act), we use 300- dimensional word vectors initialized by pretrained GloVe word vectors (Pennington et al., 2014), and a two-layer LSTM with 300 hidden units for both the encoder and the decoder.",4.3 Training Details,[0],[0]
Parameters are initialized by sampling from a uniform distribution between -0.1 and 0.1.,4.3 Training Details,[0],[0]
"For optimization, we use AdaGrad (Duchi et al., 2010) with a learning rate of 0.01 and a mini-batch size of 128.",4.3 Training Details,[0],[0]
"We train the model for 20 epochs and choose the model with the lowest validation loss.
",4.3 Training Details,[0],[0]
"For RL, we first fit a partner model using supervised learning (e.g., SL(word)), then run RL
optimization objective.",4.3 Training Details,[0],[0]
"For each group of RL models, the column of the optimization objective is highlighted.",4.3 Training Details,[0],[0]
"For human-likeness, scores that are better than others in the same group with statistical significance (p < 0.05 given by paired t-tests) are in bold.",4.3 Training Details,[0],[0]
"Overall, with SL, all models are human-like, however, act-based models better matches human statistics across all metrics; with RL, word-based models becomes degenerate, whereas act-based models optimize the reward while maintaining human-likeness.
against it.",4.3 Training Details,[0],[0]
One agent is updated by policy gradient and the partner model is fixed during training.,4.3 Training Details,[0],[0]
We use a learning rate of 0.001 and train for 5000 episodes (dialogues).,4.3 Training Details,[0],[0]
"The model with the highest reward on the validation set is chosen.
",4.3 Training Details,[0],[0]
"DEALORNODEAL For act-based models, we use the same parameterization as CRAIGSLISTBARGAIN.",4.3 Training Details,[0],[0]
"For word-based models, we use the implementation from Lewis et al. (2017).2 Note that for fair comparison, we did not apply SL interleaving during RL training and rollouts during inference.",4.3 Training Details,[0],[0]
"We evaluated each system on two metrics: taskspecific scores (e.g., utility) and human-likeness.",4.4 Human Evaluation,[0],[0]
"The scores tell us how well the system is playing the game, and human-likeness tells us whether the bot deviates from human behavior, presumably due to over-optimization.
",4.4 Human Evaluation,[0],[0]
We put up all 9 systems online and hired workers from AMT to chat with the bots.,4.4 Human Evaluation,[0],[0]
"Each worker was randomly paired with one of the bots or another worker, so as to compare the bots with human performance under the same conditions.",4.4 Human Evaluation,[0],[0]
"At
2https://github.com/facebookresearch/ end-to-end-negotiator
the end of a chat, workers were asked the question “Do you think your partner demonstrated reasonable human behavior?”.",4.4 Human Evaluation,[0],[0]
They provided answers on a Likert scale from 1 (not at all) to 5 (definitely).,4.4 Human Evaluation,[0],[0]
Table 7 shows the human evaluation results on CRAIGSLISTBARGAIN and DEALORNODEAL respectively.,4.4 Human Evaluation,[0],[0]
"We also show example human-bot dialogues in Table 8 and Appendix C.
SL(act) learns more human-like behavior.",4.4 Human Evaluation,[0],[0]
We first compare performance of SL models over words and coarse dialogue acts.,4.4 Human Evaluation,[0],[0]
Both SL(word) and SL(act) achieved similar scores on humanlikeness (no statistically significant difference).,4.4 Human Evaluation,[0],[0]
"However, SL(word) better matched human statistics such as dialogue length and utility.",4.4 Human Evaluation,[0],[0]
"For instance, SL(word) tended to produce short, generic utterances as shown in Table 8(a); they also agreed on a deal more quickly because utterances such as “deal” and “I can do that” are frequent in negotiation dialogues.",4.4 Human Evaluation,[0],[0]
"This behavior is reflected by the shorter dialogue length and lower utility of SL(word) models.
",4.4 Human Evaluation,[0],[0]
RL(word) leads to degeneracy.,4.4 Human Evaluation,[0],[0]
"On CRAIGSLISTBARGAIN, all RL(word) models clearly have low scores on human-likeness in Table 7.",4.4 Human Evaluation,[0],[0]
"They merely learned to repeat a few sentences: The three most frequent
sentences of RLutility(word), RLfairness(word), and RLlength(word) account for 81.6%, 100% and 100% of all utterances.",4.4 Human Evaluation,[0],[0]
"For example, RLutility(word) almost always opened with “i can pick it up”, then offer its target price.",4.4 Human Evaluation,[0],[0]
RLlength(word) repeated generic sentences until the partner submitted a price.,4.4 Human Evaluation,[0],[0]
"While they scored high on the reward being optimized, the conversations are unnatural.
",4.4 Human Evaluation,[0],[0]
"On DEALORNODEAL, we have observed similar patterns.",4.4 Human Evaluation,[0],[0]
"A general strategy learned by RL(word) was to pick an offer depending on its objective, then repeat the same utterance over and over again (e.g., “i need the ball.”), resulting in low human-likeness scores.",4.4 Human Evaluation,[0],[0]
"One exception is RLfairness(word), since most of its offers were reasonable and agreed on immediately (it has the shorted dialogue length), the conversations are natural.
RL(act) optimizes different negotiation goals while being human-like.",4.4 Human Evaluation,[0],[0]
"On both tasks, RL(act) models optimized their rewards while maintaining reasonable human-likeness scores.",4.4 Human Evaluation,[0],[0]
We now show that different models demonstrated different negotiation behavior.,4.4 Human Evaluation,[0],[0]
Two main strategies learned by RLlength(act) were to ask questions and to postpone offer submission.,4.4 Human Evaluation,[0],[0]
"On CRAIGSLISTBARGAIN, when acting as a buyer, 42.4% of its utterances were questions, compared to 30.2% for other models.",4.4 Human Evaluation,[0],[0]
"On both tasks, it tended to wait for the partner to submit an offer (even after a deal was agreed on), compared to RLmargin(act) which almost always submitted offers first.",4.4 Human Evaluation,[0],[0]
"For RLfairness(act), it aimed to agree on a price in the middle of the listing price and the buyer’s target price for CRAIGSLISTBARGAIN.",4.4 Human Evaluation,[0],[0]
"Since the buyer’s target was hidden, when the agent was the seller, it tended to wait for the buyer to propose prices first.",4.4 Human Evaluation,[0],[0]
"Similary, on DEALORN-
ODEAL it waited to hear the parter’s offer and sometimes changed its offer afterwards, whereas the other models often insisted on one offer.
",4.4 Human Evaluation,[0],[0]
"On both tasks, RLutility(act) learned to insist on its offer and refuse to budge.",4.4 Human Evaluation,[0],[0]
"This ended up frustrating many people, which is why it has a low agreement rate.",4.4 Human Evaluation,[0],[0]
"The problem is that our human model is simply a SL model trained on humanhuman dialogues, which may not accurately reflects real human behavior during human-bot chat.",4.4 Human Evaluation,[0],[0]
"For example, the SL model often agrees after a few turns of insistence on a proposal, whereas humans get annoyed if the partner is not willing to make compromises at all.",4.4 Human Evaluation,[0],[0]
"However, by injecting domain knowledge to SL(act)+rule, e.g., making a small compromise is better than stubbornly being fixed on a single price, we were able to achieve high utility and human-likeness on both CRAIGSLISTBARGAIN and DEALORNODEAL.",4.4 Human Evaluation,[0],[0]
"Recent work has explored the space between goal-oriented dialogue and open-domain chit-chat through collaborative or competitive language games, such as collecting cards in a maze (Potts, 2012), finding a mutual friend (He et al., 2017), or splitting a set of items (DeVault et al., 2015; Lewis et al., 2017).",5 Related Work and Discussion,[0],[0]
"Our CRAIGSLISTBARGAIN dialogue falls in this category, but exhibits richer and more diverse language than prior datasets.",5 Related Work and Discussion,[0],[0]
"Our dataset calls for systems that can handle both strategic decision-making and open-ended text generation.
",5 Related Work and Discussion,[0],[0]
"Traditional goal-oriented dialogue systems build a pipeline of modules (Young et al., 2013; Williams et al., 2016).",5 Related Work and Discussion,[0],[0]
"Due to the laborious dialogue state design and annotation, recent work has been exploring ways to replace these modules with neural networks and end-to-end training while still having a logical backbone (Wen et al., 2017a; Bordes and Weston, 2017; He et al., 2017).",5 Related Work and Discussion,[0],[0]
"Our work is closely related to the Hybrid Code Network (Williams et al., 2017), but the key difference is that Williams et al. (2017) uses a neural dialogue state, whereas we keep a structured, interpretable dialogue state which allows for stronger top-down control.",5 Related Work and Discussion,[0],[0]
"Another line of work tackles this problem by introducing latent stochastic variables to model the dialogue state (Wen et al., 2017b; Zhao et al., 2017; Cao and Clark, 2017).",5 Related Work and Discussion,[0],[0]
"While the latent discrete variable allows for post-hoc discovery of dialogue acts and increased utterance diver-
sity, it does not provide controllability over the dialogue strategy.
",5 Related Work and Discussion,[0],[0]
"Our work is also related to a large body of literature on dialogue policies in negotiation (English and Heeman, 2005; Efstathiou and Lemon, 2014; Hiraoka et al., 2015; Cao et al., 2018).",5 Related Work and Discussion,[0],[0]
"These work mostly focus on learning good negotiation policies in a domain-specific action space, whereas our model operates in an open-ended space of natural language.",5 Related Work and Discussion,[0],[0]
"An interesting future direction is to connect with game theory (Brams, 2003) for complex multi-issue bargaining.",5 Related Work and Discussion,[0],[0]
"Another direction is learning to generate persuasive utterances, e.g., through framing (Takuya et al., 2014) or accounting for the social and cultural context (Elnaz et al., 2012).
",5 Related Work and Discussion,[0],[0]
"To conclude, we have introduced CRAIGSLISTBARGAIN, a rich dataset of human-human negotiation dialogues.",5 Related Work and Discussion,[0],[0]
We have also presented a modular approach based on coarse dialogue acts that models a rough strategic backbone as well allowing for open-ended generation.,5 Related Work and Discussion,[0],[0]
"We hope this work will spur more research in hybrid approaches that can work in open-ended, goal-oriented settings.
Acknowledgments.",5 Related Work and Discussion,[0],[0]
This work is supported by DARPA Communicating with Computers (CwC) program under ARO prime contract no.,5 Related Work and Discussion,[0],[0]
W911NF15-1-0462.,5 Related Work and Discussion,[0],[0]
"We thank members of the Stanford NLP group for insightful discussion and the anonymous reviewers for constructive feedback.
Reproducibility.",5 Related Work and Discussion,[0],[0]
"All code, data, and experiments for this paper are available on the CodaLab platform: https: //worksheets.codalab.org/worksheets/ 0x453913e76b65495d8b9730d41c7e0a0c/.",5 Related Work and Discussion,[0],[0]
Figure 2 shows our web interface where workers negotiate.,A CRAIGSLISTBARGAIN Web Interface,[0],[0]
Price detection.,B Argument Detection of the Rule-based Parser,[0],[0]
"On CRAIGSLISTBARGAIN, given an utterance, we want to detect mentioned prices in it, which are arguments of intents such as propose and counter.",B Argument Detection of the Rule-based Parser,[0],[0]
"We first detect groud truth prices in the training data, which are numbers starting or ending with the dollar sign.",B Argument Detection of the Rule-based Parser,[0],[0]
"At test time, a number is considered a price if it starts or ends with the dollar sign, or (a) its left and right neighboring words appear next to ground truth prices in the training data and (b) it is not larger than 1.5x of the listing price.
",B Argument Detection of the Rule-based Parser,[0],[0]
Item and count detection.,B Argument Detection of the Rule-based Parser,[0],[0]
"On DEALORNODEAL, given an utterance, we want to parse the proposed split of items, i.e. numbers of balls, hats, and books for each agent.",B Argument Detection of the Rule-based Parser,[0],[0]
"We first detect first/second person pronouns, the three objects (ball, hat, and book), and counts (1 to 10) by regular expression matching.",B Argument Detection of the Rule-based Parser,[0],[0]
"To decide the grouping of agent, object, and count, we process the utterance from left to right; as soon as a pair of object and count is detected, we group it with the most recently referred agent by resolving the pronouns (e.g., “I” or “you”).",B Argument Detection of the Rule-based Parser,[0],[0]
"Examples of human-bot chats on DEALORNODEAL are shown in Table 9, where bot utterances are in bold.",C Example Dialogues,[0],[0]
The full set of evaluation dialogues are available on the Codalab worksheet.,C Example Dialogues,[0],[0]
We consider negotiation settings in which two agents use natural language to bargain on goods.,abstractText,[0],[0]
"Agents need to decide on both high-level strategy (e.g., proposing $50) and the execution of that strategy (e.g., generating “The bike is brand new.",abstractText,[0],[0]
Selling for just $50!”).,abstractText,[0],[0]
"Recent work on negotiation trains neural models, but their end-to-end nature makes it hard to control their strategy, and reinforcement learning tends to lead to degenerate solutions.",abstractText,[0],[0]
"In this paper, we propose a modular approach based on coarse dialogue acts (e.g., propose(price=50)) that decouples strategy and generation.",abstractText,[0],[0]
"We show that we can flexibly set the strategy using supervised learning, reinforcement learning, or domain-specific knowledge without degeneracy, while our retrieval-based generation can maintain context-awareness and produce diverse utterances.",abstractText,[0],[0]
"We test our approach on the recently proposed DEALORNODEAL game, and we also collect a richer dataset based on real items on Craigslist.",abstractText,[0],[0]
Human evaluation shows that our systems achieve higher task success rate and more human-like negotiation behavior than previous approaches.,abstractText,[0],[0]
Decoupling Strategy and Generation in Negotiation Dialogues,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4340–4349 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
4340",text,[0],[0]
Modeling a coherent text is one of the key problems in natural language processing.,1 Introduction,[0],[0]
A wellorganized text with a logical structure is much easier for people to read and understand.,1 Introduction,[0],[0]
"Sentence ordering task (Barzilay and Lapata, 2008) has been proposed to cope with this problem.",1 Introduction,[0],[0]
"It aims to organize a set of sentences into a coherent text with a logically consistent order and has wide applications in natural language generation such as concept-to-text generation (Konstas and Lapata, 2012a,b, 2013), retrieval-based question answering (Yu et al., 2018; Verberne, 2011), and extractive multi-document summarization (Barzilay and Elhadad, 2002; Galanis et al., 2012; Nallapati et al., 2017), where the improper ordering of sentences would introduce ambiguity and degrade readability.",1 Introduction,[0],[0]
"An example of this task is shown in Table 1.
",1 Introduction,[0],[0]
"Traditional methods developed for this task employ handcrafted linguistic features to model the document structure such as Entity Grid (Barzilay and Lapata, 2008), Content Model (Barzilay
∗Corresponding author
and Lee, 2004), and Probabilistic Model (Lapata, 2003).",1 Introduction,[0],[0]
"However, manual feature engineering heavily relies on linguistic knowledge and also limits these systems to be domain specific.",1 Introduction,[0],[0]
"Inspired by the success of deep learning, datadriven approaches based on neural networks have been proposed including Pairwise Ranking Model (Chen et al., 2016) which learns the relative order of sentence pairs to predict the pairwise ordering of sentences, and Window network (Li and Hovy, 2014) sliding a window over the text to evaluate the coherence.
",1 Introduction,[0],[0]
"Recently, hierarchical RNN-based approaches (Gong et al., 2016; Logeswaran et al., 2018) have been proposed to deal with this task.",1 Introduction,[0],[0]
"Such methods exploit LSTMs based paragraph encoder to compute a context representation for the whole sequential sentences and then adopt a pointer network (Vinyals et al., 2015) as the decoder to predict their order.",1 Introduction,[0],[0]
"However, since LSTM works sequentially, paragraph encoder only based on LSTMs suffers from the incorrect input sentence order and has difficulty in capturing a logically reliable representation through the recurrent connections, which makes trouble for the decoder to find the correct order.
",1 Introduction,[0],[0]
"To overcome the above limitation, in this work, we develop a novel deep attentive sentence ordering network (referred as ATTOrderNet) by inte-
grating self-attention mechanism (Vaswani et al., 2017) with LSTMs to learn a relatively reliable paragraph representation for subsequent sentence ordering.",1 Introduction,[0],[0]
"In particular, the bidirectional LSTM is first adopted as a sentence encoder to map the input sentences to the corresponding distributed vectors, and then a self-attention based paragraph encoder is introduced to capture structural relationships across sentences and to obtain a hierarchical context representation of the entire set of sentences.",1 Introduction,[0],[0]
"Consequently, based on the learned paragraph vector, a pointer network is applied to perform sentence ordering by decoding an ordered sequence.",1 Introduction,[0],[0]
"Figure 1 shows the architecture of ATTOrderNet, where self-attention mechanism is introduced to capture the dependencies among sentences.
",1 Introduction,[0],[0]
"In contrast to the previous paragraph encoders with LSTMs, self-attention mechanism is less sensitive to the input order of sentence sequence and is effective in modeling the accurate relationships across sentences, which reduces the influence of the original order of the input sentences and perfectly meets the requirement of our task.",1 Introduction,[0],[0]
"Further, unlike Transformer (Vaswani et al., 2017), we do not add any positional encodings in our model to minimize the influence of the unclear order information.
",1 Introduction,[0],[0]
Extensive evaluations are conducted on the sentence ordering task and order discrimination task to investigate the performances of ATTOrderNet.,1 Introduction,[0],[0]
The experimental results on seven public sentence ordering datasets show the superior performances of the framework to the competing models.,1 Introduction,[0],[0]
"Meanwhile, the visualization of the attention layer in
paragraph encoder is provided for better understanding of the effectivity of self-attention mechanism.",1 Introduction,[0],[0]
"Besides, in the Order Discrimination task, our model also achieves the state-of-the-art performance with remarkable improvements on two benchmark datasets.",1 Introduction,[0],[0]
"In this section, we first formulate the sentence ordering problem and then describe the proposed model ATTOrderNet, which is based on the encoder-decoder architecture applying selfattention mechanism as paragraph encoder and a pointer network as the decoder.",2 Deep Attentive Sentence Ordering Network,[0],[0]
"This combination effectively captures the intrinsic relations across a set of sentences with the desirable property of being invariant to the sentence order, which directly helps address the difficulty of this task.",2 Deep Attentive Sentence Ordering Network,[0],[0]
The sentence ordering task aims to order a set of sentences as a coherent text.,2.1 Problem formulation,[0],[0]
"Specifically, a set of n sentences with the order o =",2.1 Problem formulation,[0],[0]
"[o1, o2, · · · , on] can be described as s =",2.1 Problem formulation,[0],[0]
"[so1, so2, · · · , son ].",2.1 Problem formulation,[0],[0]
"The goal is to find the correct order o∗ for them, o∗ =",2.1 Problem formulation,[0],[0]
"[o∗1, o∗2, · · · , o∗n], with which the whole sentences have the highest coherence probability:
P(o∗ |s) > P(o|s), ∀o ∈ ψ",2.1 Problem formulation,[0],[0]
(1) where o indicates any order of these sentences and ψ denotes the set of all possible orders.,2.1 Problem formulation,[0],[0]
"For instance, in Table 1, the current order o is [4, 1, 3, 2] and o∗ =",2.1 Problem formulation,[0],[0]
"[1, 2, 3, 4] is the correct order for these sentences.",2.1 Problem formulation,[0],[0]
"Given a set of sentences, the existing hierarchical RNN-based models first transform each sentence into a distributed vector with a sentence encoder and then these sentence embeddings are fed to a LSTMs-based paragraph encoder.",2.2 Intuition and Model architecture,[0],[0]
"Consequently, based on the learned paragraph vector, a pointer network is exploited to decode the order of the input sentences.",2.2 Intuition and Model architecture,[0],[0]
"However, since LSTM works sequentially while the order of these sentences is unknown and quite possibly wrong in this problem, LSTMs-based paragraph encoder has difficulty in capturing a convincing representation through the recurrent connections, which influences the performance of sentence ordering.
",2.2 Intuition and Model architecture,[0],[0]
"In this paper, we use self-attention mechanism for paragraph encoder instead.",2.2 Intuition and Model architecture,[0],[0]
"In particular, we employ this mechanism without encoding any positional information of the sentences.",2.2 Intuition and Model architecture,[0],[0]
"Ignoring the current order of the sentences, self-attention based paragraph encoder perfectly meets the requirement of the sentence ordering task and learns a logically reliable representation of the whole paragraph by globally capturing the relationships across sentences.",2.2 Intuition and Model architecture,[0],[0]
"With this learned representation vector, a decoder is then designed to generate a coherent order assignment for the input sentences.
",2.2 Intuition and Model architecture,[0],[0]
"In the following, we elaborate on the main building blocks of our ATTOrderNet in details: a sentence encoder, a self-attention mechanism based paragraph encoder, and a decoder.",2.2 Intuition and Model architecture,[0],[0]
"For a sentence, we first apply word embedding matrix to translate the raw words in the sentence into distributional representations, and then adopt bidirectional LSTMs to learn a sentence-level representation for summarizing its high level semantic concepts.
",2.3 Sentence Encoder,[0],[0]
"Specifically, assume that a sentence soi containing nw raw words as soi =",2.3 Sentence Encoder,[0],[0]
"[w1, · · · ,wnw ], these words are transformed to dense vectors through a word embedding matrix",2.3 Sentence Encoder,[0],[0]
"We: xt = Wewt , t ∈",2.3 Sentence Encoder,[0],[0]
"[1, nw].",2.3 Sentence Encoder,[0],[0]
"The sequence of vectors [x1, · · · , xnw ] is then fed into bidirectional LSTMs sequentially to compute a semantic representation of the sentence.
",2.3 Sentence Encoder,[0],[0]
"Long Short-term Memory networks (LSTMs) (Hochreiter and Schmidhuber, 1997) is capable of learning long-term dependencies and alleviating the problems of gradient vanishment and exploding.",2.3 Sentence Encoder,[0],[0]
"Here, we adopt bidirectional LSTMs (Bi-LSTMs) to take full advantages of additional backward information and enhance the memory capability.",2.3 Sentence Encoder,[0],[0]
"In particular, the Bi-LSTMs contain the forward LSTMs which process the sentence soi from w1 to wnw and backward LSTMs which read soi in the reversed direction:
−→h t,−→c t = LSTM( −→h t−1,−→c t−1, xt) ←−h",2.3 Sentence Encoder,[0],[0]
"t,←−c t = LSTM( ←−h t+1,←−c t+1, xt)
ht =",2.3 Sentence Encoder,[0],[0]
"[ −→h t, ←−h t ]
(2)
where ht denotes the representation of position t by concatenating the forward hidden state
−→h t and backward hidden state
←−h t together.",2.3 Sentence Encoder,[0],[0]
"The output of the last hidden state of the Bi-LSTMs is
taken to be the sentence representation vector as soi = hnw , which incorporates the contextual information from both directions in the sentence.
",2.3 Sentence Encoder,[0],[0]
"So far, we have obtained a syntactic and semantic representation for a single sentence.",2.3 Sentence Encoder,[0],[0]
"In the following, a self-attention based paragraph encoder is proposed to obtain a high level representation for all given sentences by capturing sequential structures and logical relationships among them.",2.3 Sentence Encoder,[0],[0]
"We start by introducing the scaled dot-product attention, which is the foundation of self-attention mechanism used in ATTOrderNet.",2.4.1 Self-attention mechanism,[0],[0]
"Given a matrix of n query vectors Q ∈ Rn×d, keys K ∈ Rn×d, and values V ∈ Rn×d, the scaled dot-product attention computes the output matrix as:
Attention(Q,K,V) = softmax(QK T
√ d )V (3)
",2.4.1 Self-attention mechanism,[0],[0]
"The multi-head attention with h parallel heads is employed, where each head is an independent scaled dot-product attention.",2.4.1 Self-attention mechanism,[0],[0]
"The mathematical formulation is shown below:
Mi = Attention(QWQi ,KWKi ,VWVi ) (4) MH(Q,K,V) = Concat(M1, · · · ,Mh)W (5)
where WQi ,W K i ,W V i ∈",2.4.1 Self-attention mechanism,[0],[0]
"Rd×da with da = d/h are the projection matrices for the i-th head and W ∈ Rhda×d.
",2.4.1 Self-attention mechanism,[0],[0]
"Self-attention (Vaswani et al., 2017; Tan et al., 2017; Shen et al., 2017) is a special case of attention mechanism that only requires a single sequence to compute its representation where queries, keys, and values are all from the same place.",2.4.1 Self-attention mechanism,[0],[0]
"The paragraph encoder is composed of multiple self-attention layers followed by an average pooling layer.
",2.4.2 Self-attention based Paragraph Encoder,[0],[0]
Sentence vectors encoded by the sentence encoder are first packed together into a paragraph matrix S =,2.4.2 Self-attention based Paragraph Encoder,[0],[0]
"[so1, so2, · · · , son ] as E0.",2.4.2 Self-attention based Paragraph Encoder,[0],[0]
"This paragraph matrix S ∈ Rn×d is then fed forward to L self-attention layers, where each layer learns a representation El+1",2.4.2 Self-attention based Paragraph Encoder,[0],[0]
"= U(El) by taking the output
from the previous layer l:
U(El) = Φ(FN(D(El)),D(El))",2.4.2 Self-attention based Paragraph Encoder,[0],[0]
"(6) D(El) = Φ(MH(El,El,El),El) (7) Φ(v,w) =",2.4.2 Self-attention based Paragraph Encoder,[0],[0]
"LayerNorm(v + w) (8)
FN(x) = ReLU(xWl1 + bl1)Wl2 + bl2 (9)
whereΦ(·) performs layer normalization (Ba et al., 2016) on the residual output to preserve the autoregressive property, and FN(·) represents the fully connected feed-forward networks which consists of two linear layers with ReLU nonlinearity in the middle.",2.4.2 Self-attention based Paragraph Encoder,[0],[0]
"Wl1 ∈ Rd×d f , bl1 ∈ Rd f ,Wl2 ∈ Rd f ×d, and bl2 ∈ Rd are trainable parameters.",2.4.2 Self-attention based Paragraph Encoder,[0],[0]
"We set df = 1024 in all our experiments.
Self-attention mechanism adopted in the paragraph encoder directly relate sentences at different positions from the text by computing the attention score (relevance) between each pair of sentences.",2.4.2 Self-attention based Paragraph Encoder,[0],[0]
"This allows each sentence to build links with all other sentences in the text, which enables the encoder to exploit latent dependency relationships among sentences without regarding to their input order.",2.4.2 Self-attention based Paragraph Encoder,[0],[0]
"Then, attention mechanism uses weighted sum operation to establish a higher level representation for the entire sentence set.",2.4.2 Self-attention based Paragraph Encoder,[0],[0]
"As we see, there is no order information used in the encoding process which prevents the model from being affected by the incorrect sentence order.",2.4.2 Self-attention based Paragraph Encoder,[0],[0]
"Therefore, self-attention based paragraph encoder is efficient in modeling of dependencies while being invariant to the sentence order.
",2.4.2 Self-attention based Paragraph Encoder,[0],[0]
The final paragraph representation v is obtained in the average pooling layer by averaging the output matrix EL ∈ Rn×d from the last self-attention layer: v = 1n,2.4.2 Self-attention based Paragraph Encoder,[0],[0]
∑n i=1,2.4.2 Self-attention based Paragraph Encoder,[0],[0]
"e
Li , where n is the number of sentences and eLi denotes the i-th row in EL .",2.4.2 Self-attention based Paragraph Encoder,[0],[0]
This learned representation vector can be viewed as a hierarchical encoding of the entire set of sentences which will then be used as the input of the decoder to perform sentence ordering.,2.4.2 Self-attention based Paragraph Encoder,[0],[0]
"The aim of decoder is to predict a consistent order for the input set of sentences.
",2.5 Decoder,[0],[0]
"Following the previous approaches (Gong et al., 2016; Logeswaran et al., 2018), the coherence probability of given sentences s with the order o is formalized as:
P(o|s) = n∏ i=1",2.5 Decoder,[0],[0]
"P(oi |oi−1, · · · , o1, s) (10)
The higher the probability, the more coherent sentences assignment is.
",2.5 Decoder,[0],[0]
"To calculate P(o|s), we employ the pointer network architecture (Vinyals et al., 2015) as our decoder which consists of LSTMs cells (Equation 11-13).",2.5 Decoder,[0],[0]
The LSTM takes the embedding of the previous sentence as the input to decoder step.,2.5 Decoder,[0],[0]
"During training, the correct order of sentences o∗ is known, so the input sequence",2.5 Decoder,[0],[0]
"[x1, x2, · · · , xn] =",2.5 Decoder,[0],[0]
"[so∗1, so∗2, · · · , so∗n ].",2.5 Decoder,[0],[0]
"For step i, the input to the decoder is xi−1 = so∗
i−1 .",2.5 Decoder,[0],[0]
"At test time, the predicted
sentence assignment sôi−1 is used instead.",2.5 Decoder,[0],[0]
The initial state of the decoder LSTM is initialized with the final paragraph vector from the encoder: h0 = vT .,2.5 Decoder,[0],[0]
And the input at the first step in decoder x0 ∈ Rd is a vector of zeros.,2.5 Decoder,[0],[0]
"The mathematical formulation for the i-th step in decoder is as follows:
hi, ci = LSTM(hi−1, ci−1, xi−1) (11) uij = g
T tanh(W1so j +W2hi) (12) P(oi |oi−1, · · · , o1, s) = softmax(ui) (13)
where g ∈ Rd, W1 ∈ Rd×d, and W2 ∈ Rd×d are learnable parameters and j ∈ (1, · · · , n).",2.5 Decoder,[0],[0]
The softmax function normalizes the vector ui ∈,2.5 Decoder,[0],[0]
Rn to produce an output distribution over all input sentences.,2.5 Decoder,[0],[0]
"And P(oi |oi−1, · · · , o1, s) can be interpreted as the coherence probability for the current output sequence when soi being the sentence choice at position i conditioned on the previous sentences assignment.",2.5 Decoder,[0],[0]
Order Prediction:,2.5 Decoder,[0],[0]
The predicted order ô =,2.5 Decoder,[0],[0]
"[ô1, ô2, · · · , ôn] is the one with the highest coherence probability:
ô = argmax o P(o|s) (14)
",2.5 Decoder,[0],[0]
"In this work, we use beam search strategy to find a sub optimal result.",2.5 Decoder,[0],[0]
"For each ordered document, we use one random permutation of sentences as the input sample at each epoch during the training and testing process.",2.6 Training,[0],[0]
Assume that there are K documents in the training set.,2.6 Training,[0],[0]
"We define (qj, yj)Kj=1, where yj is in the correct order o∗ of original document j and qj denotes the set of sentences with a specific permutation of yj .",2.6 Training,[0],[0]
P(yj |qj),2.6 Training,[0],[0]
"= P(o∗ |s = qj) can be interpreted as the probability that sentences are assigned in the correct order when given sentences qj .
",2.6 Training,[0],[0]
"We aim to train the overall model to maximize this probability by minimizing the loss function:
L =",2.6 Training,[0],[0]
"− 1 K K∑ j=1 log P(yj |qj ; θ) + λ 2 | |θ | |22 (15)
where θ represents all trainable parameters in the networks and λ is a regularization parameter.",2.6 Training,[0],[0]
"Accident, Earthquake: two datasets obtained from (Barzilay and Lapata, 2008).",3.1 Datasets,[0],[0]
The first one is a collection of aviation accident reports from the National Transportation Safety Board and the second one comprises Associated Press articles related to earthquake.,3.1 Datasets,[0],[0]
"Since the original datasets do not provide validation samples, we follow the setup in (Louis and Nenkova, 2012; Li and Hovy, 2014) and use 10-fold cross validation on the training data.",3.1 Datasets,[0],[0]
"NIPS abstract, AAN abstract, NSF abstract: these three datasets are from (Logeswaran et al., 2018) containing abstracts from NIPS papers, ACL papers, and the NSF Research Award Abstracts dataset respectively.",3.1 Datasets,[0],[0]
arXiv,3.1 Datasets,[0],[0]
"abstract, SIND caption: we further consider two datasets used in (Gong et al., 2016).",3.1 Datasets,[0],[0]
"The former consists of abstracts from papers on arXiv website (Chen et al., 2016) and the other contains captions from SIND dataset (Huang et al., 2016).
",3.1 Datasets,[0],[0]
Further statistics about seven datasets are illustrated in Table 2.,3.1 Datasets,[0],[0]
"We use pre-trained 100 dimensional GloVe word embeddings (Pennington et al., 2014).",3.2 Training setup,[0],[0]
"And all the out-of-vocabulary words are replaced with <UNK>, whose embeddings are updated during training process.",3.2 Training setup,[0],[0]
"The nltk sentence tokenizer is
used for word tokenization.1 Parameter optimization is performed using stochastic gradient descent.",3.2 Training setup,[0],[0]
"We adopt Adadelta (Zeiler, 2012) as the optimizer with = 106 and ρ = 0.95.",3.2 Training setup,[0],[0]
"The learning rate is initialized to 1.0, the batch size is 16, and the beam size is set to 64.",3.2 Training setup,[0],[0]
"The hidden layer size of LSTMs in sentence encoder is 256, and is 512 in the decoder.",3.2 Training setup,[0],[0]
"The number of attention layers in the paragraph encoder is 6 for AAN abstract, 4 for NSF abstract and arXiv abstract, and 2 for the rest of datasets.",3.2 Training setup,[0],[0]
We employ 8 parallel heads throughout all self-attention layers and use L2 weight decay on the trainable variables with regularization parameter λ = 10−5.,3.2 Training setup,[0],[0]
The model is implemented with TensorFlow2.,3.2 Training setup,[0],[0]
Hyperparameters are chosen using the validation set.,3.2 Training setup,[0],[0]
"We first evaluate our model on the sentence ordering task, as proposed by Barzilay and Lapata (2008).",3.3 Sentence Ordering,[0],[0]
"Given a set of permuted sentences, our goal is to return the original order for them which is considered to be the most coherent.",3.3 Sentence Ordering,[0],[0]
We compare ATTOrderNet against a random baseline and all the competing models.,3.3.1 Baselines,[0],[0]
"These baseline methods can be categorized into three classes and results are reported in (Soricut and Marcu, 2006; Gong et al., 2016; Logeswaran et al., 2018).",3.3.1 Baselines,[0],[0]
"(1) Traditional approaches: Probabilistic Model (Lapata, 2003); Content Model (Barzilay and Lee, 2004); Utility-Trained model (Soricut and Marcu, 2006);",3.3.1 Baselines,[0],[0]
"Entity Grid (Barzilay and Lapata, 2008).",3.3.1 Baselines,[0],[0]
These four methods employ handcrafted features in modeling the document structure.,3.3.1 Baselines,[0],[0]
"(2) Data-driven methods: Window network (Li and Hovy, 2014); Seq2seq (Li and Jurafsky,
1NLTK implementation: http://www.nltk.org/ 2https://www.tensorflow.org/
2017); Pairwise Ranking Model (Chen et al., 2016).",3.3.1 Baselines,[0],[0]
These three approaches capture the local coherence of text based on the neural networks.,3.3.1 Baselines,[0],[0]
"(3) Hierarchical RNN-based models: VarientLSTM+PtrNet, RNN Decoder (Logeswaran et al., 2018); CNN+PtrNet, LSTM+PtrNet (Gong et al., 2016).",3.3.1 Baselines,[0],[0]
These architectures adopt RNN based approaches to obtain the representation for the input set of sentences and employ the pointer network as the decoder to predict order.,3.3.1 Baselines,[0],[0]
"The main difference between ATTOrderNet and these models lies in the design of paragraph encoder.
",3.3.1 Baselines,[0],[0]
"For thorough comparison, besides the models proposed in the existing literature, we further implement two variants of ATTOrderNet.",3.3.1 Baselines,[0],[0]
ATTOrderNet (ATT): The sentence encoder in this model is also entirely based on self-attention mechanism with 4 self-attention layers and 5 heads.,3.3.1 Baselines,[0],[0]
"Different from the paragraph encoder, the positional encoding method proposed by Vaswani et al. (2017) is applied here to encode temporal information of each input word.",3.3.1 Baselines,[0],[0]
ATTOrderNet (CNN):,3.3.1 Baselines,[0],[0]
This model employs convolutional neural networks to model sentences.,3.3.1 Baselines,[0],[0]
"In experiment, the number of feature maps is set to 512 and the width of convolution filter is 4.",3.3.1 Baselines,[0],[0]
"To provide assessments on the quality of the orderings we predict in this task, we use the following three metrics: Kendall’s tau (τ): Kendall’s tau is one of the most
frequently used metrics for the automatic evaluation of document coherence (Lapata, 2003; Logeswaran et al., 2018; Li and Jurafsky, 2017).",3.3.2 Evaluation Metrics,[0],[0]
It could be formalized as: τ = 1 − 2× (number of inversions) /,3.3.2 Evaluation Metrics,[0],[0]
"(n 2 ) , where n is the length of the sequence and the number of inversions denotes the number of pairs in the predicted sequence with incorrect relative order.",3.3.2 Evaluation Metrics,[0],[0]
This metric ranges from -1 (the worst) to 1 (the best).,3.3.2 Evaluation Metrics,[0],[0]
"Accuracy (Acc): We follow (Logeswaran et al., 2018) in employing Accuracy to measure how often the absolute position of a sentence was correctly predicted.",3.3.2 Evaluation Metrics,[0],[0]
"Compared to τ, it penalizes correctly predicted subsequences that are shifted.",3.3.2 Evaluation Metrics,[0],[0]
"Perfect Match Ratio (PMR): Perfect match ratio (Gong et al., 2016) is the most stringent measurement in this task.",3.3.2 Evaluation Metrics,[0],[0]
It calculates the radio of exactly matching orders:,3.3.2 Evaluation Metrics,[0],[0]
PMR= 1K ∑K i=1,3.3.2 Evaluation Metrics,[0],[0]
"1(̂oi = oi∗), where ôi and oi∗ are predicted and correct orders of the i-th text respectively.",3.3.2 Evaluation Metrics,[0],[0]
The experimental results on all datasets are reported in Table 3.,3.3.3 Results,[0],[0]
"Results show that ATTOrderNet gives the best performance across most datasets and under most evaluation measurements.
",3.3.3 Results,[0],[0]
The improvement is regardless of data sizes.,3.3.3 Results,[0],[0]
"In particular, for smaller datasets such as Accident and Earthquake datasets, ATTOrderNet outperforms the previous best baseline methods by 6% and 7% tau score respectively.",3.3.3 Results,[0],[0]
"As for medium size datasets including NIPS abstract and AAN
abstract, ATTOrderNet shows absolute improvements of 4.54% and 5.03% accuracy score over the previous state-of-the-art.",3.3.3 Results,[0],[0]
Such finding is consistent across larger datasets.,3.3.3 Results,[0],[0]
"ATTOrderNet outperforms the previous state-of-the-art systems by 4.50% accuracy score with 3% tau score on NSF abstract, 1.75% PMR score with 1% tau score on arXiv abstract, and 1.67% PMR score with 1% tau score on SIND caption.",3.3.3 Results,[0],[0]
"Interestingly, ATTOrderNet reaches 42.19% PMR score on arXiv abstract, which means that more than 2/5 texts in the test set can be ordered exactly right.",3.3.3 Results,[0],[0]
"This performance clearly demonstrates the adaptability and flexibility of the proposed model.
",3.3.3 Results,[0],[0]
"As shown in Table 3, ATTOrderNet performs much better than data-driven methods by a significant margin on all corresponding datasets.",3.3.3 Results,[0],[0]
It proves the importance of exploiting the context by self-attention mechanism as these competing models only consider the local coherence in the text.,3.3.3 Results,[0],[0]
"Among the traditional ordering approaches, Content Model (Barzilay and Lee, 2004) representing topics as states and capturing possible orderings for global coherence performs better than other methods with the tau score of 0.81 on Earthquake dataset, which also demonstrates that global context is important to sentence ordering.",3.3.3 Results,[0],[0]
"However, Content Model requires manual feature engineering that costs great human efforts.",3.3.3 Results,[0],[0]
"In contrast, the self-attention mechanism used in ATTOrderNet directly captures the global dependences for the whole text while requiring no linguistic knowledge anymore and enables ATTOrderNet to further improve tau score to 0.92 on the same dataset.
",3.3.3 Results,[0],[0]
"In addition, hierarchical RNN-based models capture the global coherence among sentences with LSTMs and outperform the traditional methods and data-driven approaches in most cases.",3.3.3 Results,[0],[0]
"However, these models still suffer from the permutation of sentences within the document since LSTM works sequentially.",3.3.3 Results,[0],[0]
"ATTOrderNet achieves superior performances to them by adopting the self-attention mechanism to reduce the influence of the permutation of sentences.
",3.3.3 Results,[0],[0]
"Further, ATTOrderNet (CNN) has better performances than ATTOrderNet (ATT) on most of the datasets.",3.3.3 Results,[0],[0]
We conjecture that this is due to the limitation of data size.,3.3.3 Results,[0],[0]
"Since ATTOrderNet (ATT) applies self-attention mechanism in both sentence and paragraph encoders requiring more data to train the model, however the size of the
datasets used in this task is smaller than those in other tasks such as document classification (Yang et al., 2016).",3.3.3 Results,[0],[0]
"Given larger datasets in the future, we believe ATTOrderNet (ATT) would perform much better.",3.3.3 Results,[0],[0]
"Among three sentence encoders, ATTOrderNet presents a superior performance across the board.",3.3.3 Results,[0],[0]
This indicates that LSTM is more efficient in learning semantic representation for sentence level in this task.,3.3.3 Results,[0],[0]
"ATTOrderNet becomes more competitive through combining both advantages of LSTMs and self-attention mechanism.
",3.3.3 Results,[0],[0]
"Since the first and the last sentences of the text are more special to discern (Chen et al., 2016; Gong et al., 2016), we also evaluate the ratio of correctly predicting the first and the last sentences.",3.3.3 Results,[0],[0]
Table 4 summarizes our performances on arXiv abstract and SIND caption.,3.3.3 Results,[0],[0]
"As we see, all models show fair well in predicting the first sentence, and the prediction accuracy declines for the last one.",3.3.3 Results,[0],[0]
It is observed that ATTOrderNet still achieves a boost in predicting two positions compared to the previous state-of-the-art system on both datasets.,3.3.3 Results,[0],[0]
The aim of this section is to visualize the relationship between sentences captured by self-attention mechanism and understand how it helps perform the sentence ordering task.,3.3.4 Visualization of attention,[0],[0]
A technique for visualizing attention mechanism in neural networks is proposed by Vaswani et al. (2017) 3.,3.3.4 Visualization of attention,[0],[0]
"Inspired by this work, we select a text from AAN abstract dataset to visualize the hierarchical attention layer from the paragraph encoder of ATTOrderNet in Figure 2.",3.3.4 Visualization of attention,[0],[0]
"Different from visualizing the dependencies of one word with the other words in the sentence (Vaswani et al., 2017), our visualization
3https://github.com/tensorflow/tensor2tensor
shows the dependencies between each sentence and all other sentences in the text.
",3.3.4 Visualization of attention,[0],[0]
"For the example in Figure 2, the left text is the input sample to our model which contains a set of permuted sentences with the correct order besides it.",3.3.4 Visualization of attention,[0],[0]
"The right side is a copy of the input text, which is presented for showing the relevance between each pair of sentences more clearly.",3.3.4 Visualization of attention,[0],[0]
The line with grey color on the right text is an example sentence chosen to visualize the attention weights with other sentences.,3.3.4 Visualization of attention,[0],[0]
On the top of the text are 8 colored squares representing 8 different attention heads used in the paragraph encoder.,3.3.4 Visualization of attention,[0],[0]
Colored columns on the left text show the performance of their corresponding heads.,3.3.4 Visualization of attention,[0],[0]
The darkness of the color in column denotes the normalized distribution of the attention weight for the example sentence in the head.,3.3.4 Visualization of attention,[0],[0]
"Sentences in darker shades show more attention weight which reflects stronger links they have with the example sentence.
",3.3.4 Visualization of attention,[0],[0]
"In particular, Figure 2 shows the attention distribution for the first sentence in the original document.",3.3.4 Visualization of attention,[0],[0]
We present the weight distribution in four heads as an instance.,3.3.4 Visualization of attention,[0],[0]
"It is interesting to see that all of them showing significant higher attention weights on the true second sentence “However, text use changes ...” than other sentences in the text.",3.3.4 Visualization of attention,[0],[0]
This indicates that these heads are able to learn the latent dependency relationships from sentences and can successfully distinguish which one is the true next following among all sentence candidates.,3.3.4 Visualization of attention,[0],[0]
"These heads build much stronger links between this sentence with the chosen one in order to keep structural information for higher level representation, such as paragraph representation.",3.3.4 Visualization of attention,[0],[0]
"In this section, we assess ATTOrderNet on another common evaluation task which is usually
adopted in the existing literature: order discrimination task.
",3.4 Order Discrimination,[0],[0]
"Order discrimination (Barzilay and Lapata, 2008; Elsner and Charniak, 2011, 2008) aims to compare a document to a randomly permuted version of it.",3.4 Order Discrimination,[0],[0]
"Models are evaluated with Pairwise Accuracy: the ratio of correctly identifying the original document with higher coherence probability (defined in Equation 10) than the probability of its permutation.
",3.4 Order Discrimination,[0],[0]
"Among seven datasets mentioned above, we use two of them to assess the performance of ATTOrderNet on the order discrimination task: Accident and Earthquake datasets.",3.4 Order Discrimination,[0],[0]
"These two have been widely used for this task in the previous literature (Li and Hovy, 2014; Logeswaran et al., 2018).",3.4 Order Discrimination,[0],[0]
This gives us the convenience of directly comparing the result of the proposed model against the reported results.,3.4 Order Discrimination,[0],[0]
"Following the setup in (Barzilay and Lapata, 2008), a maximum of 20 random permutations were generated for each training and testing article to create the pairwise data.",3.4 Order Discrimination,[0],[0]
There are 1986 and 1956 test pairs in Accident and Earthquake datasets respectively.,3.4 Order Discrimination,[0],[0]
"To demonstrate that ATTOrderNet truly improves the order discrimination performance, we compare ATTOrderNet with the following representative models: Graph from (Guinaudeau and Strube, 2013), HMM and HMM+Entity from (Louis and Nenkova, 2012), Entity Grid from (Barzilay and Lapata, 2008), Recurrent and Recursive from (Li and Hovy, 2014), Discriminative model from (Li and Jurafsky, 2017), Varient-LSTM+PtrNet from (Logeswaran et al., 2018), CNN+PtrNet and LSTM+PtrNet from (Gong et al., 2016).",3.4.1 Baselines,[0],[0]
The results of the last two methods were obtained by training their models on two datasets.,3.4.1 Baselines,[0],[0]
Table 5 reports the results of ATTOrderNet and currently competing architectures in this evaluation task.,3.4.2 Results,[0],[0]
"ATTOrderNet also achieves the stateof-the-art performance, showing a remarkable advancement of about 1.8% gain on Accident dataset and further improving the pairwise accuracy to 99.8 on Earthquake dataset.
",3.4.2 Results,[0],[0]
"LSTM+PtrNet and CNN+ PtrNet (Gong et al., 2016) fall short of Varient-LSTM+PtrNet (Logeswaran et al., 2018) in performance.",3.4.2 Results,[0],[0]
This could also be blamed for their paragraph encoder.,3.4.2 Results,[0],[0]
"Documents in both datasets are much longer than those in others, which brings more trouble for LSTMs in paragraph encoder to build logical representations.",3.4.2 Results,[0],[0]
"Compared to the result in the sentence ordering task, Entity Grid (Barzilay and Lapata, 2008) achieves a good performance in this task and even outperforms Recurrent neural networks and Recursive neural networks (Li and Hovy, 2014) on Accident dataset.",3.4.2 Results,[0],[0]
"However, Entity Grid requires hand-engineered features and heavily relies on linguistic knowledge which restrain the model to be adapted to other tasks.",3.4.2 Results,[0],[0]
"In this paper, we develop a novel deep attentive sentence ordering model (referred as ATTOrderNet) integrating self-attention mechanism with LSTMs.",4 Conclusion,[0],[0]
"It enables us to directly capture logical relationships among sentences regardless of their
input order and obtain a reliable representation of the sentence set.",4 Conclusion,[0],[0]
"With this representation, a pointer network is applied to generate an ordered sequence.",4 Conclusion,[0],[0]
ATTOrderNet is evaluated on Sentence Ordering and Order Discrimination tasks.,4 Conclusion,[0],[0]
The experimental results demonstrate its effectiveness and show promising improvements over existing models across most datasets.,4 Conclusion,[0],[0]
"This work was supported by National Natural Science Foundation of China (No. 61702448, 61672456) and the Fundamental Research Funds for the Central Universities (No. 2017QNA5008, 2017FZA5007).",Acknowledgments,[0],[0]
We thank all reviewers for their valuable comments.,Acknowledgments,[0],[0]
"In this paper, we propose a novel deep attentive sentence ordering network (referred as ATTOrderNet) which integrates self-attention mechanism with LSTMs in the encoding of input sentences.",abstractText,[0],[0]
It enables us to capture global dependencies among sentences regardless of their input order and obtains a reliable representation of the sentence set.,abstractText,[0],[0]
"With this representation, a pointer network is exploited to generate an ordered sequence.",abstractText,[0],[0]
The proposed model is evaluated on Sentence Ordering and Order Discrimination tasks.,abstractText,[0],[0]
The extensive experimental results demonstrate its effectiveness and superiority to the state-ofthe-art methods.,abstractText,[0],[0]
Deep Attentive Sentence Ordering Network,title,[0],[0]
A big challenge in many machine learning applications is obtaining labelled data.,1. Introduction,[0],[0]
"This can be a long, laborious, and costly process, often making the deployment of ML systems uneconomical.",1. Introduction,[0],[0]
"A framework where a system could learn from small amounts of data, and choose by itself what data it would like the user to label, would make machine learning much more widely applicable.",1. Introduction,[0],[0]
"Such frameworks for learning are referred to as active learning (Cohn et al., 1996) (also known as “experiment design” in the statistics literature), and have been used successfully in fields such as medical diagnosis, microbiology, and manufacturing (Tong, 2001).",1. Introduction,[0],[0]
"In active learning, a model is trained on a small
1University of Cambridge, UK 2The Alan Turing Institute, UK.",1. Introduction,[0],[0]
"Correspondence to: Yarin Gal <yg279@cam.ac.uk>.
amount of data (the initial training set), and an acquisition function (often based on the model’s uncertainty) decides which data points to ask an external oracle for a label.",1. Introduction,[0],[0]
"The acquisition function selects one or more points from a pool of unlabelled data points, with the pool points lying outside of the training set.",1. Introduction,[0],[0]
"An oracle (often a human expert) labels the selected data points, these are added to the training set, and a new model is trained on the updated training set.",1. Introduction,[0],[0]
"This process is then repeated, with the training set increasing in size over time.",1. Introduction,[0],[0]
"The advantage of such systems is that they often result in dramatic reductions in the amount of labelling required to train an ML system (and therefore cost and time).
",1. Introduction,[0],[0]
"Even though existing techniques for active learning have proven themselves useful in a variety of tasks, a major remaining challenge in active learning is its lack of scalability to high-dimensional data (Tong, 2001).",1. Introduction,[0],[0]
"This data appears often in image form, with a physician classifying MRI scans to diagnose Alzheimer’s for example (Marcus et al., 2010), or an expert clinician diagnosing skin cancer from dermoscopic lesion images.",1. Introduction,[0],[0]
"To perform active learning, a model has to be able to learn from small amounts of data and represent its uncertainty over unseen data.",1. Introduction,[0],[0]
This severely restricts the class of models that can be used within the active learning framework.,1. Introduction,[0],[0]
"As a result most approaches to active learning have focused on low dimensional problems (Tong, 2001; Hernandez-Lobato & Adams, 2015), with only a handful of exceptions (Zhu et al., 2003; Holub et al., 2008; Joshi et al., 2009) relying on kernel or graph-based approaches to handle high-dimensional data.
",1. Introduction,[0],[0]
"In recent years, with the increased availability of data in some domains, attention within the machine learning community has shifted from small data problems to big data problems (Sundermeyer et al., 2012; Krizhevsky et al., 2012; Kalchbrenner & Blunsom, 2013; Sutskever et al., 2014).",1. Introduction,[0],[0]
"And with the increased interest in big data problems, new tools were developed and existing tools were refined for handling high dimensional data within such regimes.",1. Introduction,[0],[0]
"Deep learning, and convolutional neural networks (CNNs) (Rumelhart et al., 1985; LeCun et al., 1989) in particular, are an example of such tools.",1. Introduction,[0],[0]
"Originally developed in 1989 to parse handwritten zip codes, these tools have flourished and were adapted to a point where a CNN is able to beat a human on object recognition tasks (given enough training data)
",1. Introduction,[0],[0]
"ar X
iv :1
70 3.
02 91
0v 1
[ cs
.L",1. Introduction,[0],[0]
"G
] 8
M ar
2 01
7
(He et al., 2015).",1. Introduction,[0],[0]
"New techniques such as dropout (Hinton et al., 2012; Srivastava et al., 2014) are used extensively to regularise these huge models, which often contain millions of parameters (Jozefowicz et al., 2016).",1. Introduction,[0],[0]
"But even though active learning forms an important pillar of machine learning, deep learning tools are not prevalent within it.",1. Introduction,[0],[0]
Deep learning poses several difficulties when used in an active learning setting.,1. Introduction,[0],[0]
"First, we have to be able to handle small amounts of data.",1. Introduction,[0],[0]
"Recent advances in deep learning, on the other hand, are notorious for their dependence on large amounts of data (Krizhevsky et al., 2012).",1. Introduction,[0],[0]
"Second, many AL acquisition functions rely on model uncertainty.",1. Introduction,[0],[0]
"But in deep learning we rarely represent such model uncertainty.
",1. Introduction,[0],[0]
"Relying on Bayesian approaches to deep learning, in this paper we combine recent advances in Bayesian deep learning into the active learning framework in a practical way.",1. Introduction,[0],[0]
"We develop an active learning framework for high dimensional data, a task which has been extremely challenging so far with very sparse existing literature from the past 15 years (Zhu et al., 2003; Li & Guo, 2013; Holub et al., 2008; Joshi et al., 2009).",1. Introduction,[0],[0]
"Taking advantage of specialised models such as Bayesian convolutional neural networks (BCNNs) (Gal & Ghahramani, 2016a;b), we demonstrate our active learning techniques with image data.",1. Introduction,[0],[0]
"Using a small model, our system is able to achieve 5% test error on MNIST with only 295 labelled images without relying on unlabelled data (in comparison, 835 labelled images are needed to achieve 5% test error using random sampling – requiring an expert to label more than twice as many images to achieve the same accuracy), and achieves 1.64% test error with 1000 labelled images.",1. Introduction,[0],[0]
"This is in comparison to 2.40% test error of DGN (Kingma et al., 2014) or 1.53% test error of the Ladder Network Γ-model (Rasmus et al., 2015), both semi-supervised learning techniques which additionally use the entire unlabelled training set.",1. Introduction,[0],[0]
"Finally, we study a realworld application by diagnosing melanoma (skin cancer) from a small number of lesion images by fine-tuning the VGG16 convolutional neural network (Simonyan & Zisserman, 2015) on the ISIC 2016 dataset (Gutman et al., 2016).",1. Introduction,[0],[0]
Past attempts at active learning of image data have concentrated on kernel based methods.,2. Related Research,[0],[0]
"Using ideas from previous research in active learning of low dimensional data (Tong, 2001), Joshi et al. (2009) used “margin-based uncertainty” and extracted probabilistic outputs from support vector machines (SVM) (Cortes & Vapnik, 1995).",2. Related Research,[0],[0]
"They used linear, polynomial, and Radial Basis Function (RBF) kernels on the raw images, picking the kernel that gave best classification accuracy.",2. Related Research,[0],[0]
"Analogously to SVM approaches, Li & Guo (2013) used Gaussian processes (GPs) with RBF kernels to get model uncertainty.",2. Related Research,[0],[0]
"However Li & Guo (2013) fed low dimensional features (such as SIFT features) to their
RBF kernel.",2. Related Research,[0],[0]
"Lastly, making use of unlabelled data as well, Zhu et al. (2003) acquire points using a Gaussian random field model, evaluating an RBF kernel over raw images.",2. Related Research,[0],[0]
"We compare to this last technique and explain it in more detail below.
",2. Related Research,[0],[0]
"Other related work includes semi-supervised learning of image data (Weston et al., 2012; Kingma et al., 2014; Rasmus et al., 2015).",2. Related Research,[0],[0]
"In semi-supervised learning a model is given a fixed set of labelled data, and a fixed set of unlabelled data.",2. Related Research,[0],[0]
"The model can use the unlabelled data to learn about the distribution of the inputs, in the hopes that this information will aid in learning from the small labelled set as well.",2. Related Research,[0],[0]
"Although the learning paradigm is fairly different from active learning, this research forms the closest modern literature to active learning of image data.",2. Related Research,[0],[0]
"We will compare to these techniques below as well, in section 5.4.",2. Related Research,[0],[0]
"In this paper we concentrate on high dimensional image data, and need a model able to represent prediction uncertainty on such data.",3. Bayesian Convolutional Neural Networks,[0],[0]
"Existing approaches such as (Zhu et al., 2003; Li & Guo, 2013; Joshi et al., 2009) rely on kernel methods, and feed image pairs through linear, polynomial, and RBF kernels to capture image similarity as an input to an SVM for example.",3. Bayesian Convolutional Neural Networks,[0],[0]
"In contrast, we rely on specialised models for image data, and in particular on convolutional neural networks (CNNs) (Rumelhart et al., 1985; LeCun et al., 1989).",3. Bayesian Convolutional Neural Networks,[0],[0]
"Unlike the kernels above, which cannot capture spatial information in the input image, CNNs are designed to use this spatial information, and have been used successfully to achieve state-of-the-art results (Krizhevsky et al., 2012).",3. Bayesian Convolutional Neural Networks,[0],[0]
"To perform active learning with image data we make use of the Bayesian equivalent of CNNs, proposed in (Gal & Ghahramani, 2016a)1.",3. Bayesian Convolutional Neural Networks,[0],[0]
These Bayesian CNNs are CNNs with prior probability distributions placed over a set of model parameters ω,3. Bayesian Convolutional Neural Networks,[0],[0]
"= {W1, ...,WL}: ω ∼ p(ω), with for example a standard Gaussian prior p(ω).",3. Bayesian Convolutional Neural Networks,[0],[0]
"We further define a likelihood model p(y = c|x,ω) = softmax(fω(x)) for the case of classification, or a Gaussian likelihood for the case of regression, with fω(x) model output (with parameters ω).
",3. Bayesian Convolutional Neural Networks,[0],[0]
"To perform approximate inference in the Bayesian CNN model we make use of stochastic regularisation techniques such as dropout (Hinton et al., 2012; Srivastava et al., 2014), originally used to regularise these models.",3. Bayesian Convolutional Neural Networks,[0],[0]
"As shown in (Gal & Ghahramani, 2016b; Gal, 2016) dropout and various
1As far as we are aware, there are no other tools in current literature that offer model uncertainty in specialised models for image data, which perform as well as CNNs.
",3. Bayesian Convolutional Neural Networks,[0],[0]
other stochastic regularisation techniques can be used to perform practical approximate inference in complex deep models.,3. Bayesian Convolutional Neural Networks,[0],[0]
"Inference is done by training a model with dropout before every weight layer, and by performing dropout at test time as well to sample from the approximate posterior (stochastic forward passes, referred to as MC dropout).
",3. Bayesian Convolutional Neural Networks,[0],[0]
"More formally, this approach is equivalent to performing approximate variational inference where we find a distribution q∗θ(ω) in a tractable family which minimises the Kullback-Leibler (KL) divergence to the true model posterior p(ω|Dtrain) given a training set Dtrain.",3. Bayesian Convolutional Neural Networks,[0],[0]
"Dropout can be interpreted as a variational Bayesian approximation, where the approximating distribution is a mixture of two Gaussians with small variances and the mean of one of the Gaussians is fixed at zero.",3. Bayesian Convolutional Neural Networks,[0],[0]
"The uncertainty in the weights induces prediction uncertainty by marginalising over the approximate posterior using Monte Carlo integration:
p(y = c|x,Dtrain) = ∫",3. Bayesian Convolutional Neural Networks,[0],[0]
"p(y = c|x,ω)p(ω|Dtrain)dω
≈ ∫ p(y = c|x,ω)q∗θ(ω)dω
≈ 1 T T∑ t=1",3. Bayesian Convolutional Neural Networks,[0],[0]
"p(y = c|x, ω̂t)
with ω̂t ∼ q∗θ(ω), where qθ(ω) is the Dropout distribution (Gal, 2016).
",3. Bayesian Convolutional Neural Networks,[0],[0]
"Bayesian CNNs work well with small amounts of data (Gal & Ghahramani, 2016a), and possess uncertainty information that can be used with existing acquisition functions (Gal, 2016).",3. Bayesian Convolutional Neural Networks,[0],[0]
Such acquisition functions for the case of classification are discussed next.,3. Bayesian Convolutional Neural Networks,[0],[0]
"Given a modelM, pool data Dpool, and inputs x ∈ Dpool, an acquisition function a(x,M) is a function of x that the AL system uses to decide where to query next: x∗ = argmaxx∈Dpoola(x,M).",4. Acquisition Functions and their Approximations,[0],[0]
"We next explore various acquisition functions appropriate for our image data setting, and develop tractable approximations for us to use with our Bayesian CNNs.",4. Acquisition Functions and their Approximations,[0],[0]
In tasks involving regression we often use the predictive variance or a quantity derived from this for our acquisition function (although we still need to be careful to query from informative areas rather than querying noise).,4. Acquisition Functions and their Approximations,[0],[0]
"For example, we might look for images with high predictive variance and choose those to ask an expert to label – in the hope that these will decrease model uncertainty.",4. Acquisition Functions and their Approximations,[0],[0]
"However, many tasks involving image data are often phrased as classification problems.",4. Acquisition Functions and their Approximations,[0],[0]
"For classification, several acquisition functions are available:
1.",4. Acquisition Functions and their Approximations,[0],[0]
"Choose pool points that maximise the predictive en-
tropy (Max Entropy, (Shannon, 1948))",4. Acquisition Functions and their Approximations,[0],[0]
"H[y|x,Dtrain] :=
− ∑ c p(y = c|x,Dtrain) log p(y = c|x,Dtrain).
",4. Acquisition Functions and their Approximations,[0],[0]
2.,4. Acquisition Functions and their Approximations,[0],[0]
"Choose pool points that are expected to maximise the information gained about the model parameters, i.e. maximise the mutual information between predictions and model posterior (BALD, (Houlsby et al., 2011))",4. Acquisition Functions and their Approximations,[0],[0]
"I[y,ω|x,Dtrain] = H[y|x,Dtrain]−Ep(ω|Dtrain) [ H[y|x,ω]
] with ω the model parameters (here H[y|x,ω] is the entropy of y given model weights ω).",4. Acquisition Functions and their Approximations,[0],[0]
"Points that maximise this acquisition function are points on which the model is uncertain on average, but there exist model parameters that produce disagreeing predictions with high certainty.",4. Acquisition Functions and their Approximations,[0],[0]
"This is equivalent to points with high variance in the input to the softmax layer (the logits) – thus each stochastic forward pass through the model would have the highest probability assigned to a different class.
3.",4. Acquisition Functions and their Approximations,[0],[0]
"Maximise the Variation Ratios (Freeman, 1965) variation-ratio[x] := 1−max
y p(y|x,Dtrain)
",4. Acquisition Functions and their Approximations,[0],[0]
"Like Max Entropy, Variation Ratios measures lack of confidence.
",4. Acquisition Functions and their Approximations,[0],[0]
4.,4. Acquisition Functions and their Approximations,[0],[0]
"Maximise mean standard deviation (Mean STD) (Kampffmeyer et al., 2016; Kendall et al., 2015)
σc = √ Eq(ω)[p(y = c|x,ω)2]−",4. Acquisition Functions and their Approximations,[0],[0]
"Eq(ω)[p(y = c|x,ω)]2
σ(x) = 1
C ∑ c σc
averaged over all c classes x can take.",4. Acquisition Functions and their Approximations,[0],[0]
"Compared to the above acquisition functions, this is more of an ad-hoc technique used in recent literature.
5.",4. Acquisition Functions and their Approximations,[0],[0]
Random acquisition (baseline): a(x) = unif() with unif() a function returning a draw from a uniform distribution over the interval,4. Acquisition Functions and their Approximations,[0],[0]
"[0, 1].",4. Acquisition Functions and their Approximations,[0],[0]
"Using this acquisition function is equivalent to choosing a point uniformly at random from the pool.
",4. Acquisition Functions and their Approximations,[0],[0]
"These acquisition functions and their properties are discussed in more detail in (Gal, 2016, pp.",4. Acquisition Functions and their Approximations,[0],[0]
"48–52).
",4. Acquisition Functions and their Approximations,[0],[0]
We can approximate each of these acquisition functions using our approximate distribution q∗θ(ω).,4. Acquisition Functions and their Approximations,[0],[0]
"For BALD, for example, we can write the acquisition function as follows: I[y,ω|x,Dtrain] := H[y|x,Dtrain]− Ep(ω|Dtrain)",4. Acquisition Functions and their Approximations,[0],[0]
"[ H[y|x,ω]
] = −
∑ c p(y = c|x,Dtrain) log p(y = c|x,Dtrain)
+ Ep(ω|Dtrain)",4. Acquisition Functions and their Approximations,[0],[0]
"[∑
c
p(y = c|x,ω) log p(y = c|x,ω) ] ,
with c the possible classes y can take.",4. Acquisition Functions and their Approximations,[0],[0]
"I[y,ω|x,Dtrain] can be approximated in our setting using the identity p(y = c|x,Dtrain) = ∫",4. Acquisition Functions and their Approximations,[0],[0]
"p(y = c|x,ω)p(ω|Dtrain)dω:
I[y,ω|x,Dtrain] =",4. Acquisition Functions and their Approximations,[0],[0]
"− ∑ c ∫ p(y = c|x,ω)p(ω|Dtrain)dω
· log ∫ p(y = c|x,ω)p(ω|Dtrain)dω
+ Ep(ω|Dtrain)",4. Acquisition Functions and their Approximations,[0],[0]
"[∑
c
p(y = c|x,ω) log p(y = c|x,ω) ] .
",4. Acquisition Functions and their Approximations,[0],[0]
"Swapping the posterior p(ω|Dtrain) with our approximate posterior q∗θ(ω), and through MC sampling, we then have:
≈",4. Acquisition Functions and their Approximations,[0],[0]
"− ∑ c ∫ p(y = c|x,ω)q∗θ(ω)dω
· log ∫ p(y = c|x,ω)q∗θ(ω)dω
+ Eq∗θ (ω)",4. Acquisition Functions and their Approximations,[0],[0]
"[∑
c
p(y = c|x,ω) log p(y = c|x,ω) ]
",4. Acquisition Functions and their Approximations,[0],[0]
≈ − ∑,4. Acquisition Functions and their Approximations,[0],[0]
c,4. Acquisition Functions and their Approximations,[0],[0]
"( 1 T ∑ t p̂tc ) log ( 1 T ∑ t p̂tc ) + 1
T ∑ c,t p̂tc log p̂ t c",4. Acquisition Functions and their Approximations,[0],[0]
"=: Î[y,ω|x,Dtrain]
defining our approximation, with p̂tc the probability of input x with model parameters ω̂t ∼ q∗θ(ω) to take class c:
p̂t =",4. Acquisition Functions and their Approximations,[0],[0]
"[p̂t1, ..., p̂ t C ] = softmax(f ω̂t(x)).
",4. Acquisition Functions and their Approximations,[0],[0]
"We then have Î[y,ω|x,Dtrain] −−−−→
T→∞ H[y|x, q∗θ ]",4. Acquisition Functions and their Approximations,[0],[0]
"− Eq∗θ (ω)
[ H[y|x,ω] ]",4. Acquisition Functions and their Approximations,[0],[0]
"≈ I[y,ω|x,Dtrain],
resulting in a computationally tractable estimator approximating the BALD acquisition function.",4. Acquisition Functions and their Approximations,[0],[0]
"The other acquisition functions can be approximated similarly.
",4. Acquisition Functions and their Approximations,[0],[0]
In the next section we will experiment with these acquisition functions and assess them empirically.,4. Acquisition Functions and their Approximations,[0],[0]
"These will be compared to the baseline acquisition function which uniformly acquires new data points from the pool set at random, and to various other techniques for active learning of image data and semi-supervised learning.",4. Acquisition Functions and their Approximations,[0],[0]
This is followed by a real-world case study using cancer diagnosis.,4. Acquisition Functions and their Approximations,[0],[0]
We study the proposed technique for active learning of image data.,5. Active Learning with Bayesian Convolutional Neural Networks,[0],[0]
We compare the various acquisition functions relying on Bayesian CNN uncertainty with a simple image classification benchmark.,5. Active Learning with Bayesian Convolutional Neural Networks,[0],[0]
We then study the importance of model uncertainty by evaluating the same acquisition functions with a deterministic CNN.,5. Active Learning with Bayesian Convolutional Neural Networks,[0],[0]
"This is followed by a com-
parison to a current technique for active learning with image data, which relies on SVMs.",5. Active Learning with Bayesian Convolutional Neural Networks,[0],[0]
We follow with a comparison to the closest modern models to our active learning with image data – semi-supervised techniques with image data.,5. Active Learning with Bayesian Convolutional Neural Networks,[0],[0]
"These semi-supervised techniques have access to much more data (the unlabelled data) than our active learning models, yet we still perform in comparable terms to them.",5. Active Learning with Bayesian Convolutional Neural Networks,[0],[0]
"Finally, we demonstrate the proposed methodology with a real world application of skin cancer diagnosis from a small number of lesion images, relying on fine-tuning of a large CNN model.",5. Active Learning with Bayesian Convolutional Neural Networks,[0],[0]
"We next study all acquisition functions above with our Bayesian CNN trained on the MNIST dataset (LeCun & Cortes, 1998).",5.1. Comparison of various acquisition functions,[0],[0]
"All acquisition functions are assessed with the same model structure: convolution-reluconvolution-relu-max pooling-dropout-dense-relu-dropoutdense-softmax, with 32 convolution kernels, 4x4 kernel size, 2x2 pooling, dense layer with 128 units, and dropout probabilities 0.25 and 0.5 (following the example Keras MNIST CNN implementation (fchollet, 2015)).
",5.1. Comparison of various acquisition functions,[0],[0]
"All models are trained on the MNIST dataset with a (random but balanced) initial training set of 20 data points, and a validation set of 100 points on which we optimise the weight decay (this is a realistic validation set size, in comparison to the standard validation set size of 5K used in similar applications such as semi-supervised learning on MNIST).",5.1. Comparison of various acquisition functions,[0],[0]
"We further use the standard test set of 10K points, and the rest of the points are used as a pool set.",5.1. Comparison of various acquisition functions,[0],[0]
"The test error of each model and each acquisition function was assessed after
each acquisition, using the dropout approximation at test time.",5.1. Comparison of various acquisition functions,[0],[0]
To decide what data points to acquire though we used MC dropout following the derivations above.,5.1. Comparison of various acquisition functions,[0],[0]
"We repeated the acquisition process 100 times, each time acquiring the 10 points that maximised the acquisition function over the pool set.",5.1. Comparison of various acquisition functions,[0],[0]
"Each experiment was repeated three times and the results averaged (the standard deviation for the three repetitions is shown below)2.
",5.1. Comparison of various acquisition functions,[0],[0]
"We compared the acquisition functions BALD, Variation Ratios, Max Entropy, Mean STD, and the baseline Random.",5.1. Comparison of various acquisition functions,[0],[0]
"We found Random and Mean STD to under-perform compared to BALD, Variation Ratios, and Max Entropy (figure 1).",5.1. Comparison of various acquisition functions,[0],[0]
The Variation Ratios acquisition function seems to obtain slightly better accuracy faster than BALD and Max Entropy.,5.1. Comparison of various acquisition functions,[0],[0]
"It is interesting that Mean STD seems to perform similarly to Random – which samples points at random from the pool set.
",5.1. Comparison of various acquisition functions,[0],[0]
"Lastly, in table 1 we give the number of acquisition steps needed to get to test errors of 5% and 10%.",5.1. Comparison of various acquisition functions,[0],[0]
"As can be seen, BALD, Variation Ratios, and Max Entropy attain a small test error with much fewer acquisitions than Mean STD and Random.",5.1. Comparison of various acquisition functions,[0],[0]
This table demonstrates the importance of data efficiency – an expert using the Variation Ratios model for example would have to label less than half the number of images she would have had to label had she acquired new images at random.,5.1. Comparison of various acquisition functions,[0],[0]
2The code for these experiments is available at http://mlg.eng.cam.ac.uk/yarin/publications.,5% 335 295 355 695 835,[0],[0]
html#Gal2016Active.,5% 335 295 355 695 835,[0],[0]
"We assess the importance of model uncertainty in our Bayesian CNN by evaluating three of the acquisition functions (BALD, Variation Ratios, and Max Entropy) with a deterministic CNN.",5.2. Importance of model uncertainty,[0],[0]
"Much like the Bayesian CNN, the deterministic CNN produces a probability vector which can be used with the acquisition functions of §4 (formally, by setting q∗θ(ω) =",5.2. Importance of model uncertainty,[0],[0]
δ(ω − θ) to be a point mass at the location of the model parameters θ).,5.2. Importance of model uncertainty,[0],[0]
"Such deterministic models can capture aleatoric uncertainty – the noise in the data – but cannot capture epistemic uncertainty – the uncertainty over the parameters of the CNN, which we try to minimise during active learning.",5.2. Importance of model uncertainty,[0],[0]
"The models in this experiment still use dropout, but for regularisation only (i.e. we do not perform MC dropout at test time).
",5.2. Importance of model uncertainty,[0],[0]
"A comparison of the Bayesian models to the deterministic models for the BALD, Variation Ratios, and Max Entropy acquisition functions is given in fig.",5.2. Importance of model uncertainty,[0],[0]
2.,5.2. Importance of model uncertainty,[0],[0]
"The Bayesian models, propagating uncertainty throughout the model, attain higher accuracy early on, and converge to a higher accuracy overall.",5.2. Importance of model uncertainty,[0],[0]
This demonstrates that the uncertainty propagated throughout the Bayesian models has a significant effect on the models’ measure of their confidence.,5.2. Importance of model uncertainty,[0],[0]
"We next compare to a method in the sparse existing literature of active learning with image data, concentrating on (Zhu et al., 2003) which relies on a kernel method and further leverages the unlabelled images (which will be discussed in more detail in the next section).",5.3. Comparison to current active learning techniques with image data,[0],[0]
Zhu et al. (2003) evaluate an RBF kernel over the raw images to get a similarity graph which can be used to share information about the unlabelled data.,5.3. Comparison to current active learning techniques with image data,[0],[0]
"Active learning is then performed by greedily selecting unlabelled images to be labelled, such that an estimate to the expected classification error is minimised.",5.3. Comparison to current active learning techniques with image data,[0],[0]
"This will be referred to as MBR.
",5.3. Comparison to current active learning techniques with image data,[0],[0]
"MBR was formulated for the binary classification case,
hence we compared MBR to the acquisition functions",5.3. Comparison to current active learning techniques with image data,[0],[0]
"BALD, Variation Ratios, Max Entropy, and Random on a binary classification task (two digits from the MNIST dataset).",5.3. Comparison to current active learning techniques with image data,[0],[0]
Classification accuracy is shown in fig.,5.3. Comparison to current active learning techniques with image data,[0],[0]
3.,5.3. Comparison to current active learning techniques with image data,[0],[0]
"Note that even a random acquisition function, when coupled with a CNN (a specialised model for image data) outperforms MBR which relies on an RBF kernel.",5.3. Comparison to current active learning techniques with image data,[0],[0]
We further experimented with a CNN version for MBR where we replaced the RBF kernel with a CNN.,5.3. Comparison to current active learning techniques with image data,[0],[0]
It is interesting to note that this did not give improved results.,5.3. Comparison to current active learning techniques with image data,[0],[0]
We continue with a comparison to the closest models (in modern literature) to our active learning with image data: semi-supervised learning with image data.,5.4. Comparison to semi-supervised learning,[0],[0]
"In semisupervised learning a model is given a fixed set of labelled data, and a fixed set of unlabelled data.",5.4. Comparison to semi-supervised learning,[0],[0]
"The model can use the unlabelled dataset to learn about the distribution of the inputs, in the hopes that this information will aid in learning the mapping to the outputs as well.",5.4. Comparison to semi-supervised learning,[0],[0]
"Several semi-supervised models for image data have been suggested in recent years (Weston et al., 2012; Kingma et al., 2014; Rasmus et al., 2015), models which have set benchmarks on MNIST given a small number of labelled images (1000 random images).",5.4. Comparison to semi-supervised learning,[0],[0]
"These models make further use of a (very) large unlabelled set of 49K images, and a large validation set of 5K-10K labelled images to tune model hyper-parameters and model structure (Rasmus et al., 2015).",5.4. Comparison to semi-supervised learning,[0],[0]
"These models have access to much more data than our active learning models, but we still compare to them as they are the most relevant models in the field given the constraint of small amounts of labelled data.
",5.4. Comparison to semi-supervised learning,[0],[0]
"Test error for our active learning models with various acquisition functions (after the acquisition of 1000 training points), as well as the semi-supervised models, is given in table 2.",5.4. Comparison to semi-supervised learning,[0],[0]
"In this experiment, to be comparable to the other techniques, we use a validation set of 5K points.",5.4. Comparison to semi-supervised learning,[0],[0]
"Our model attains similar performance to that of the semi-supervised
models (although note that we use a fairly small model compared to (Rasmus et al., 2015) for example).",5.4. Comparison to semi-supervised learning,[0],[0]
"Rasmus et al. (2015)’s ladder network (full) attains error 0.84% with 1000 labelled images and 59,000 unlabelled images.",5.4. Comparison to semi-supervised learning,[0],[0]
"However, (Rasmus et al., 2015)’s Γ-model architecture is more directly comparable to ours.",5.4. Comparison to semi-supervised learning,[0],[0]
"The Γ-model attains 1.53% error, compared to 1.64% error of our Var Ratio acquisition function which relies on no additional unlabelled data.",5.4. Comparison to semi-supervised learning,[0],[0]
We finish by assessing the proposed technique with a real world test case.,5.5. Cancer diagnosis from lesion image data,[0],[0]
We experiment with melanoma (skin cancer) diagnosis from dermoscopic lesion images.,5.5. Cancer diagnosis from lesion image data,[0],[0]
"In this task we are given image data of skin segments, of both malignant (cancerous) as well as benign (non-cancerous) lesions.",5.5. Cancer diagnosis from lesion image data,[0],[0]
Our task is to classify the images as malignant or benign (an example is shown in fig.,5.5. Cancer diagnosis from lesion image data,[0],[0]
4).,5.5. Cancer diagnosis from lesion image data,[0],[0]
"The data used is the ISIC Archive (Gutman et al., 2016).",5.5. Cancer diagnosis from lesion image data,[0],[0]
"This dataset was collected in order to provide a “large public repository of expertly annotated high quality skin images” to provide clinical support in the identification of skin cancer, and to develop algorithms for skin cancer diagnosis.",5.5. Cancer diagnosis from lesion image data,[0],[0]
"Specifically, we use the training data of the “ISBI 2016:",5.5. Cancer diagnosis from lesion image data,[0],[0]
Skin Lesion Analysis Towards Melanoma Detection – Part 3B: Segmented Lesion Classification” task.,5.5. Cancer diagnosis from lesion image data,[0],[0]
The data contains 900 dermoscopic lesion images in JPEG format with EXIF tags removed.,5.5. Cancer diagnosis from lesion image data,[0],[0]
Malignancy diagnosis for these lesions was obtained from expert consensus and pathology report information.,5.5. Cancer diagnosis from lesion image data,[0],[0]
"The data contains lesion segmentation as well, which we did not use.
",5.5. Cancer diagnosis from lesion image data,[0],[0]
"For our model we replicate the model of (Agarwal et al., 2016).",5.5. Cancer diagnosis from lesion image data,[0],[0]
"This model achieved second place in the “Part 3B: Segmented Lesion Classification” task, with its code opensourced.",5.5. Cancer diagnosis from lesion image data,[0],[0]
"The model relies on data augmentation of the positive examples (flipping the lesions vertically and horizontally), and fine-tunes the VGG16 CNN model (Simonyan & Zisserman, 2015) (i.e. optimises a pre-trained model with a small learning rate).",5.5. Cancer diagnosis from lesion image data,[0],[0]
"The VGG16 model was pre-trained on ImageNet (Deng et al., 2009).",5.5. Cancer diagnosis from lesion image data,[0],[0]
The top layer of the model (1000 logits) was removed and replaced with a 2 dimensional output (for our classification task of malignant/benign).,5.5. Cancer diagnosis from lesion image data,[0],[0]
"Preceding the last layer are two fully connected layers of size 4096, each one followed by a dropout layer with dropout probability 0.5.",5.5. Cancer diagnosis from lesion image data,[0],[0]
"This architecture seems to provide good uncertainty estimates as observed before (Kendall et al., 2015; Gal & Ghahramani, 2016a).
",5.5. Cancer diagnosis from lesion image data,[0],[0]
"The data is unbalanced, containing 727 negative (benign) examples, and 173 positive (malignant) examples (20% positive examples).",5.5. Cancer diagnosis from lesion image data,[0],[0]
"Since the data is so small, to assess model performance reliably we have to take a large balanced test set.",5.5. Cancer diagnosis from lesion image data,[0],[0]
"We randomly partition the data, and set aside 100 negative and 100 positive examples.",5.5. Cancer diagnosis from lesion image data,[0],[0]
"All our experiments are
performed on two different random splits – since even a test set size of 200 gives very different accuracy with different random splits.",5.5. Cancer diagnosis from lesion image data,[0],[0]
"Note that on each such random split we repeat our experiments three times and average the results with respect to the fixed test set.
",5.5. Cancer diagnosis from lesion image data,[0],[0]
We experiment with active learning by following the following procedure.,5.5. Cancer diagnosis from lesion image data,[0],[0]
"We begin by creating an initial training set of 80 negative examples and 20 positive examples from our training data, as well as a pool set from the remaining data.",5.5. Cancer diagnosis from lesion image data,[0],[0]
With each experiment repetition (out of the three experiment repetitions w.r.t.,5.5. Cancer diagnosis from lesion image data,[0],[0]
the fixed test split) the pool is shuffled anew.,5.5. Cancer diagnosis from lesion image data,[0],[0]
"The positive examples in the current training set are augmented following the original training procedure, and a model is trained on the augmented training set for 100 epochs until convergence.",5.5. Cancer diagnosis from lesion image data,[0],[0]
"We use batch size 8 and weight decay set by (1− p)l2/N , where N is the number of training points, p = 0.5 is the dropout probability, and the length-scale squared l2 is set to 0.5.",5.5. Cancer diagnosis from lesion image data,[0],[0]
An acquisition function is then used to select the 100 most informative images from the pool set.,5.5. Cancer diagnosis from lesion image data,[0],[0]
"These points are removed from the pool set and added to the (non-augmented) training set, where we use the original expert-provided labels for these points.",5.5. Cancer diagnosis from lesion image data,[0],[0]
"The process is repeated until all pool points have been exhausted, where at each acquisition step we reset the model to its original pre-trained weights (as we also did in the previous section experiments).",5.5. Cancer diagnosis from lesion image data,[0],[0]
"This reset is done in order to avoid local optima, and to avoid confusing model performance improvement with an improvement resulting from simply using longer (cumulative) optimisation time.
",5.5. Cancer diagnosis from lesion image data,[0],[0]
After each acquisition the test performance of the model is logged using MC dropout with 20 samples.,5.5. Cancer diagnosis from lesion image data,[0],[0]
We further keep track of the number of positive examples acquired after each acquisition.,5.5. Cancer diagnosis from lesion image data,[0],[0]
Model performance is assessed using area-under-the-curve (AUC) as this seems to be the most informative of all metrics used by Gutman et al. (2016).,5.5. Cancer diagnosis from lesion image data,[0],[0]
"We experimented with the average precision metric suggested by Gutman et al. (2016) as well, but managed to get results improving over the competition winner by simply predicting all points as “benign”.",5.5. Cancer diagnosis from lesion image data,[0],[0]
This might be because of the data imbalance.,5.5. Cancer diagnosis from lesion image data,[0],[0]
"AUC on the other hand takes into account all possible decision-thresholds possible to classify a malignant image.
",5.5. Cancer diagnosis from lesion image data,[0],[0]
"We assessed two acquisition functions: a uniform baseline,
and BALD.",5.5. Cancer diagnosis from lesion image data,[0],[0]
"Even though Variation Ratios performs well on MNIST above, the function fails with the melanoma data since most malignant images are given only a slight higher probability of being malignant compared to the probability of benign images of being malignant.",5.5. Cancer diagnosis from lesion image data,[0],[0]
"As a result all pool points are given identical Variation Ratios acquisition value.
",5.5. Cancer diagnosis from lesion image data,[0],[0]
Experiment results are given in fig.,5.5. Cancer diagnosis from lesion image data,[0],[0]
"5, where results are reported on both test splits (top and bottom), and",5.5. Cancer diagnosis from lesion image data,[0],[0]
where with each split the experiment is repeated three times and performance results are averaged on that fixed split.,5.5. Cancer diagnosis from lesion image data,[0],[0]
For each test split we report mean with standard error.,5.5. Cancer diagnosis from lesion image data,[0],[0]
"AUC is reported for each split (left), and number of acquired positive examples is reported as well (right) for each acquisition step.",5.5. Cancer diagnosis from lesion image data,[0],[0]
"BALD achieves better AUC faster than uniform, and acquires more positive examples at each acquisition step than uniform (i.e. BALD finds positive examples as informative and adds these to the training set, whereas uniform simply selects positive examples from the pool set based on their frequency).
",5.5. Cancer diagnosis from lesion image data,[0],[0]
"Note how AUC range varies wildly between the two different test splits, but how AUC is similar for both acquisition functions on each fixed test set before the initial acquisition (when both uniform and BALD models are trained on the
same initial training set).",5.5. Cancer diagnosis from lesion image data,[0],[0]
"This demonstrates the difficulties with handling of small data: each test split gives radically different results, and in this case even though each acquisition function experiment has a relatively small standard error, averaging the AUC of the acquisition functions over the different test splits would artificially increase the standard error.",5.5. Cancer diagnosis from lesion image data,[0],[0]
"Lastly, it is interesting to experiment with a model trained over the entire pool set, i.e. with the settings of the second place winner in the ISIC2016 task.",5.5. Cancer diagnosis from lesion image data,[0],[0]
"For the first test split this model attains AUC 0.71± 0.003, whereas with the second test split it attains AUC 0.75 ± 0.01.",5.5. Cancer diagnosis from lesion image data,[0],[0]
For both test splits this AUC is worse than BALD’s converged AUC after 4 acquisition steps.,5.5. Cancer diagnosis from lesion image data,[0],[0]
This might be because BALD avoided selecting noisy points – near-by images for which there exist multiple noisy labels of different classes.,5.5. Cancer diagnosis from lesion image data,[0],[0]
"Such points have large aleatoric uncertainty – uncertainty which cannot be explained away – rather than large epistemic uncertainty – the uncertainty which BALD captures in order to explain it away, i.e. reduce it.",5.5. Cancer diagnosis from lesion image data,[0],[0]
"We presented a new approach for active learning of image data, relying on recent advances at the intersection of
Bayesian modelling and deep learning, and demonstrated a real-world application in medical diagnosis.",6. Future Research,[0],[0]
"We assessed the performance of the techniques by resetting the models after each acquisition, and training them again to convergence.",6. Future Research,[0],[0]
"This was done to isolate the effects of our acquisition functions, which came at a cost of prolonged training times (20 hours for each melanoma experiment for example).",6. Future Research,[0],[0]
"We showed that even with this long running time, our technique still reduces required expert labels, thus reduces costs for such a system.",6. Future Research,[0],[0]
This running time can be reduced further by not resetting the system – with the potential price of falling into local optima.,6. Future Research,[0],[0]
We leave this problem for future research.,6. Future Research,[0],[0]
"Even though active learning forms an important pillar of machine learning, deep learning tools are not prevalent within it.",abstractText,[0],[0]
Deep learning poses several difficulties when used in an active learning setting.,abstractText,[0],[0]
"First, active learning (AL) methods generally rely on being able to learn and update models from small amounts of data.",abstractText,[0],[0]
"Recent advances in deep learning, on the other hand, are notorious for their dependence on large amounts of data.",abstractText,[0],[0]
"Second, many AL acquisition functions rely on model uncertainty, yet deep learning methods rarely represent such model uncertainty.",abstractText,[0],[0]
In this paper we combine recent advances in Bayesian deep learning into the active learning framework in a practical way.,abstractText,[0],[0]
"We develop an active learning framework for high dimensional data, a task which has been extremely challenging so far, with very sparse existing literature.",abstractText,[0],[0]
"Taking advantage of specialised models such as Bayesian convolutional neural networks, we demonstrate our active learning techniques with image data, obtaining a significant improvement on existing active learning approaches.",abstractText,[0],[0]
"We demonstrate this on both the MNIST dataset, as well as for skin cancer diagnosis from lesion images (ISIC2016 task).",abstractText,[0],[0]
Deep Bayesian Active Learning with Image Data,title,[0],[0]
"Pre-trained word representations (Mikolov et al., 2013; Pennington et al., 2014) are a key component in many neural language understanding models.",1 Introduction,[0],[0]
"However, learning high quality representations can be challenging.",1 Introduction,[0],[0]
"They should ideally model both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy).",1 Introduction,[0],[0]
"In this paper, we introduce a new type of deep contextualized word representation that directly addresses both challenges, can be easily integrated into existing models, and significantly improves the state of the art in every considered case across a range of challenging language understanding problems.
",1 Introduction,[0],[0]
Our representations differ from traditional word type embeddings in that each token is assigned a representation that is a function of the entire input sentence.,1 Introduction,[0],[0]
"We use vectors derived from a bidirectional LSTM that is trained with a coupled lan-
guage model (LM) objective on a large text corpus.",1 Introduction,[0],[0]
"For this reason, we call them ELMo (Embeddings from Language Models) representations.",1 Introduction,[0],[0]
"Unlike previous approaches for learning contextualized word vectors (Peters et al., 2017; McCann et al., 2017), ELMo representations are deep, in the sense that they are a function of all of the internal layers of the biLM.",1 Introduction,[0],[0]
"More specifically, we learn a linear combination of the vectors stacked above each input word for each end task, which markedly improves performance over just using the top LSTM layer.
",1 Introduction,[0],[0]
Combining the internal states in this manner allows for very rich word representations.,1 Introduction,[0],[0]
"Using intrinsic evaluations, we show that the higher-level LSTM states capture context-dependent aspects of word meaning (e.g., they can be used without modification to perform well on supervised word sense disambiguation tasks) while lowerlevel states model aspects of syntax (e.g., they can be used to do part-of-speech tagging).",1 Introduction,[0],[0]
"Simultaneously exposing all of these signals is highly beneficial, allowing the learned models select the types of semi-supervision that are most useful for each end task.
",1 Introduction,[0],[0]
Extensive experiments demonstrate that ELMo representations work extremely well in practice.,1 Introduction,[0],[0]
"We first show that they can be easily added to existing models for six diverse and challenging language understanding problems, including textual entailment, question answering and sentiment analysis.",1 Introduction,[0],[0]
"The addition of ELMo representations alone significantly improves the state of the art in every case, including up to 20% relative error reductions.",1 Introduction,[0],[0]
"For tasks where direct comparisons are possible, ELMo outperforms CoVe (McCann et al., 2017), which computes contextualized representations using a neural machine translation encoder.",1 Introduction,[0],[0]
"Finally, an analysis of both ELMo and CoVe reveals that deep representations outperform ar X
iv :1
80 2.
05 36
5v 2
[ cs
.C",1 Introduction,[0],[0]
"L
] 2
2 M
ar 2
01 8
those derived from just the top layer of an LSTM.",1 Introduction,[0],[0]
"Our trained models and code are publicly available, and we expect that ELMo will provide similar gains for many other NLP problems.1",1 Introduction,[0],[0]
"Due to their ability to capture syntactic and semantic information of words from large scale unlabeled text, pretrained word vectors (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014) are a standard component of most state-ofthe-art NLP architectures, including for question answering (Liu et al., 2017), textual entailment (Chen et al., 2017) and semantic role labeling (He et al., 2017).",2 Related work,[0],[0]
"However, these approaches for learning word vectors only allow a single contextindependent representation for each word.
",2 Related work,[0],[0]
"Previously proposed methods overcome some of the shortcomings of traditional word vectors by either enriching them with subword information (e.g., Wieting et al., 2016; Bojanowski et al., 2017) or learning separate vectors for each word sense (e.g., Neelakantan et al., 2014).",2 Related work,[0],[0]
"Our approach also benefits from subword units through the use of character convolutions, and we seamlessly incorporate multi-sense information into downstream tasks without explicitly training to predict predefined sense classes.
",2 Related work,[0],[0]
Other recent work has also focused on learning context-dependent representations. context2vec,2 Related work,[0],[0]
"(Melamud et al., 2016) uses a bidirectional Long Short Term Memory (LSTM; Hochreiter and Schmidhuber, 1997) to encode the context around a pivot word.",2 Related work,[0],[0]
"Other approaches for learning contextual embeddings include the pivot word itself in the representation and are computed with the encoder of either a supervised neural machine translation (MT) system (CoVe; McCann et al., 2017) or an unsupervised language model (Peters et al., 2017).",2 Related work,[0],[0]
"Both of these approaches benefit from large datasets, although the MT approach is limited by the size of parallel corpora.",2 Related work,[0],[0]
"In this paper, we take full advantage of access to plentiful monolingual data, and train our biLM on a corpus with approximately 30 million sentences (Chelba et al., 2014).",2 Related work,[0],[0]
"We also generalize these approaches to deep contextual representations, which we show work well across a broad range of diverse NLP tasks.
",2 Related work,[0],[0]
"1http://allennlp.org/elmo
Previous work has also shown that different layers of deep biRNNs encode different types of information.",2 Related work,[0],[0]
"For example, introducing multi-task syntactic supervision (e.g., part-of-speech tags) at the lower levels of a deep LSTM can improve overall performance of higher level tasks such as dependency parsing (Hashimoto et al., 2017) or CCG super tagging (Søgaard and Goldberg, 2016).",2 Related work,[0],[0]
"In an RNN-based encoder-decoder machine translation system, Belinkov et al. (2017) showed that the representations learned at the first layer in a 2- layer LSTM encoder are better at predicting POS tags then second layer.",2 Related work,[0],[0]
"Finally, the top layer of an LSTM for encoding word context (Melamud et al., 2016) has been shown to learn representations of word sense.",2 Related work,[0],[0]
"We show that similar signals are also induced by the modified language model objective of our ELMo representations, and it can be very beneficial to learn models for downstream tasks that mix these different types of semi-supervision.
",2 Related work,[0],[0]
Dai and Le (2015) and Ramachandran et al. (2017) pretrain encoder-decoder pairs using language models and sequence autoencoders and then fine tune with task specific supervision.,2 Related work,[0],[0]
"In contrast, after pretraining the biLM with unlabeled data, we fix the weights and add additional taskspecific model capacity, allowing us to leverage large, rich and universal biLM representations for cases where downstream training data size dictates a smaller supervised model.",2 Related work,[0],[0]
"Unlike most widely used word embeddings (Pennington et al., 2014), ELMo word representations are functions of the entire input sentence, as described in this section.",3 ELMo: Embeddings from Language Models,[0],[0]
"They are computed on top of two-layer biLMs with character convolutions (Sec. 3.1), as a linear function of the internal network states (Sec. 3.2).",3 ELMo: Embeddings from Language Models,[0],[0]
"This setup allows us to do semi-supervised learning, where the biLM is pretrained at a large scale (Sec. 3.4) and easily incorporated into a wide range of existing neural NLP architectures (Sec. 3.3).",3 ELMo: Embeddings from Language Models,[0],[0]
"Given a sequence of N tokens, (t1, t2, ..., tN ), a forward language model computes the probability of the sequence by modeling the probability of to-
ken tk given the history (t1, ..., tk−1):
p(t1, t2, . . .",3.1 Bidirectional language models,[0],[0]
", tN ) = N∏ k=1 p(tk | t1, t2, . . .",3.1 Bidirectional language models,[0],[0]
", tk−1).
",3.1 Bidirectional language models,[0],[0]
"Recent state-of-the-art neural language models (Józefowicz et al., 2016; Melis et al., 2017; Merity et al., 2017) compute a context-independent token representation xLMk (via token embeddings or a CNN over characters) then pass it through L layers of forward LSTMs.",3.1 Bidirectional language models,[0],[0]
"At each position k, each LSTM layer outputs a context-dependent representation −→ h LMk,j where j = 1, . . .",3.1 Bidirectional language models,[0],[0]
", L.",3.1 Bidirectional language models,[0],[0]
"The top layer LSTM output, −→ h LMk,L , is used to predict the next token tk+1 with a Softmax layer.",3.1 Bidirectional language models,[0],[0]
"A backward LM is similar to a forward LM, except it runs over the sequence in reverse, predicting the previous token given the future context: p(t1, t2, . . .",3.1 Bidirectional language models,[0],[0]
", tN ) = N∏ k=1 p(tk | tk+1, tk+2, . . .",3.1 Bidirectional language models,[0],[0]
", tN ).
",3.1 Bidirectional language models,[0],[0]
"It can be implemented in an analogous way to a forward LM, with each backward LSTM layer j in a L layer deep model producing representations←− h LMk,j of tk given (tk+1, . . .",3.1 Bidirectional language models,[0],[0]
", tN ).
",3.1 Bidirectional language models,[0],[0]
A biLM combines both a forward and backward LM.,3.1 Bidirectional language models,[0],[0]
"Our formulation jointly maximizes the log likelihood of the forward and backward directions: N∑ k=1 ( log p(tk | t1, . . .",3.1 Bidirectional language models,[0],[0]
", tk−1; Θx, −→ ΘLSTM ,Θs)
+ log p(tk | tk+1, . . .",3.1 Bidirectional language models,[0],[0]
", tN ;",3.1 Bidirectional language models,[0],[0]
"Θx, ←− ΘLSTM ,Θs) ) .
",3.1 Bidirectional language models,[0],[0]
We tie the parameters for both the token representation (Θx) and Softmax layer (Θs) in the forward and backward direction while maintaining separate parameters for the LSTMs in each direction.,3.1 Bidirectional language models,[0],[0]
"Overall, this formulation is similar to the approach of Peters et al. (2017), with the exception that we share some weights between directions instead of using completely independent parameters.",3.1 Bidirectional language models,[0],[0]
"In the next section, we depart from previous work by introducing a new approach for learning word representations that are a linear combination of the biLM layers.",3.1 Bidirectional language models,[0],[0]
ELMo is a task specific combination of the intermediate layer representations in the biLM.,3.2 ELMo,[0],[0]
"For
each token tk, a L-layer biLM computes a set of 2L+ 1 representations
Rk = {xLMk , −→ h LMk,j , ←− h LMk,j | j = 1, . . .",3.2 ELMo,[0],[0]
", L}
= {hLMk,j | j = 0, . . .",3.2 ELMo,[0],[0]
", L},
where hLMk,0 is the token layer and h LM k,j = [ −→ h LMk,j ; ←− h LMk,j ], for each biLSTM layer.
",3.2 ELMo,[0],[0]
"For inclusion in a downstream model, ELMo collapses all layers in R into a single vector, ELMok = E(Rk; Θe).",3.2 ELMo,[0],[0]
"In the simplest case, ELMo just selects the top layer, E(Rk) =",3.2 ELMo,[0],[0]
"hLMk,L , as in TagLM (Peters et al., 2017) and CoVe (McCann et al., 2017).",3.2 ELMo,[0],[0]
"More generally, we compute a task specific weighting of all biLM layers:
ELMotaskk = E(Rk; Θ task) = γtask L∑ j=0",3.2 ELMo,[0],[0]
"staskj h LM k,j .",3.2 ELMo,[0],[0]
"(1) In (1), stask are softmax-normalized weights and the scalar parameter γtask allows the task model to scale the entire ELMo vector.",3.2 ELMo,[0],[0]
γ is of practical importance to aid the optimization process (see supplemental material for details).,3.2 ELMo,[0],[0]
"Considering that the activations of each biLM layer have a different distribution, in some cases it also helped to apply layer normalization (Ba et al., 2016) to each biLM layer before weighting.",3.2 ELMo,[0],[0]
"Given a pre-trained biLM and a supervised architecture for a target NLP task, it is a simple process to use the biLM to improve the task model.",3.3 Using biLMs for supervised NLP tasks,[0],[0]
We simply run the biLM and record all of the layer representations for each word.,3.3 Using biLMs for supervised NLP tasks,[0],[0]
"Then, we let the end task model learn a linear combination of these representations, as described below.
",3.3 Using biLMs for supervised NLP tasks,[0],[0]
First consider the lowest layers of the supervised model without the biLM.,3.3 Using biLMs for supervised NLP tasks,[0],[0]
"Most supervised NLP models share a common architecture at the lowest layers, allowing us to add ELMo in a consistent, unified manner.",3.3 Using biLMs for supervised NLP tasks,[0],[0]
"Given a sequence of tokens (t1, . . .",3.3 Using biLMs for supervised NLP tasks,[0],[0]
", tN ), it is standard to form a context-independent token representation xk for each token position using pre-trained word embeddings and optionally character-based representations.",3.3 Using biLMs for supervised NLP tasks,[0],[0]
"Then, the model forms a context-sensitive representation hk, typically using either bidirectional RNNs, CNNs, or feed forward networks.
",3.3 Using biLMs for supervised NLP tasks,[0],[0]
"To add ELMo to the supervised model, we first freeze the weights of the biLM and then
concatenate the ELMo vector ELMotaskk with xk and pass the ELMo enhanced representation [xk; ELMo task k ] into the task RNN.",3.3 Using biLMs for supervised NLP tasks,[0],[0]
"For some tasks (e.g., SNLI, SQuAD), we observe further improvements by also including ELMo at the output of the task RNN by introducing another set of output specific linear weights and replacing hk with [hk; ELMotaskk ].",3.3 Using biLMs for supervised NLP tasks,[0],[0]
"As the remainder of the supervised model remains unchanged, these additions can happen within the context of more complex neural models.",3.3 Using biLMs for supervised NLP tasks,[0],[0]
"For example, see the SNLI experiments in Sec. 4 where a bi-attention layer follows the biLSTMs, or the coreference resolution experiments where a clustering model is layered on top of the biLSTMs.
",3.3 Using biLMs for supervised NLP tasks,[0],[0]
"Finally, we found it beneficial to add a moderate amount of dropout to ELMo (Srivastava et al., 2014) and in some cases to regularize the ELMo weights by adding λ‖w‖22 to the loss.",3.3 Using biLMs for supervised NLP tasks,[0],[0]
This imposes an inductive bias on the ELMo weights to stay close to an average of all biLM layers.,3.3 Using biLMs for supervised NLP tasks,[0],[0]
"The pre-trained biLMs in this paper are similar to the architectures in Józefowicz et al. (2016) and Kim et al. (2015), but modified to support joint training of both directions and add a residual connection between LSTM layers.",3.4 Pre-trained bidirectional language model architecture,[0],[0]
"We focus on large scale biLMs in this work, as Peters et al. (2017) highlighted the importance of using biLMs over forward-only LMs and large scale training.
",3.4 Pre-trained bidirectional language model architecture,[0],[0]
"To balance overall language model perplexity with model size and computational requirements for downstream tasks while maintaining a purely character-based input representation, we halved all embedding and hidden dimensions from the single best model CNN-BIG-LSTM in Józefowicz et al. (2016).",3.4 Pre-trained bidirectional language model architecture,[0],[0]
The final model uses L = 2 biLSTM layers with 4096 units and 512 dimension projections and a residual connection from the first to second layer.,3.4 Pre-trained bidirectional language model architecture,[0],[0]
"The context insensitive type representation uses 2048 character n-gram convolutional filters followed by two highway layers (Srivastava et al., 2015) and a linear projection down to a 512 representation.",3.4 Pre-trained bidirectional language model architecture,[0],[0]
"As a result, the biLM provides three layers of representations for each input token, including those outside the training set due to the purely character input.",3.4 Pre-trained bidirectional language model architecture,[0],[0]
"In contrast, traditional word embedding methods only provide one layer of representation for tokens in a fixed vocabulary.
",3.4 Pre-trained bidirectional language model architecture,[0],[0]
"After training for 10 epochs on the 1B Word Benchmark (Chelba et al., 2014), the average forward and backward perplexities is 39.7, compared to 30.0 for the forward CNN-BIG-LSTM.",3.4 Pre-trained bidirectional language model architecture,[0],[0]
"Generally, we found the forward and backward perplexities to be approximately equal, with the backward value slightly lower.
",3.4 Pre-trained bidirectional language model architecture,[0],[0]
"Once pretrained, the biLM can compute representations for any task.",3.4 Pre-trained bidirectional language model architecture,[0],[0]
"In some cases, fine tuning the biLM on domain specific data leads to significant drops in perplexity and an increase in downstream task performance.",3.4 Pre-trained bidirectional language model architecture,[0],[0]
This can be seen as a type of domain transfer for the biLM.,3.4 Pre-trained bidirectional language model architecture,[0],[0]
"As a result, in most cases we used a fine-tuned biLM in the downstream task.",3.4 Pre-trained bidirectional language model architecture,[0],[0]
See supplemental material for details.,3.4 Pre-trained bidirectional language model architecture,[0],[0]
Table 1 shows the performance of ELMo across a diverse set of six benchmark NLP tasks.,4 Evaluation,[0],[0]
"In every task considered, simply adding ELMo establishes a new state-of-the-art result, with relative error reductions ranging from 6 - 20% over strong base models.",4 Evaluation,[0],[0]
This is a very general result across a diverse set model architectures and language understanding tasks.,4 Evaluation,[0],[0]
"In the remainder of this section we provide high-level sketches of the individual task results; see the supplemental material for full experimental details.
",4 Evaluation,[0],[0]
"Question answering The Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) contains 100K+ crowd sourced questionanswer pairs where the answer is a span in a given Wikipedia paragraph.",4 Evaluation,[0],[0]
"Our baseline model (Clark and Gardner, 2017) is an improved version of the Bidirectional Attention Flow model in Seo et al. (BiDAF; 2017).",4 Evaluation,[0],[0]
"It adds a self-attention layer after the bidirectional attention component, simplifies some of the pooling operations and substitutes the LSTMs for gated recurrent units (GRUs; Cho et al., 2014).",4 Evaluation,[0],[0]
"After adding ELMo to the baseline model, test set F1 improved by 4.7% from 81.1% to 85.8%, a 24.9% relative error reduction over the baseline, and improving the overall single model state-of-the-art by 1.4%.",4 Evaluation,[0],[0]
"A 11 member ensemble pushes F1 to 87.4, the overall state-of-the-art at time of submission to the leaderboard.2",4 Evaluation,[0],[0]
"The increase of 4.7% with ELMo is also significantly larger then the 1.8% improvement from adding CoVe to a baseline model (McCann et al., 2017).
",4 Evaluation,[0],[0]
"2As of November 17, 2017.
",4 Evaluation,[0],[0]
"Textual entailment Textual entailment is the task of determining whether a “hypothesis” is true, given a “premise”.",4 Evaluation,[0],[0]
"The Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015) provides approximately 550K hypothesis/premise pairs.",4 Evaluation,[0],[0]
"Our baseline, the ESIM sequence model from Chen et al. (2017), uses a biLSTM to encode the premise and hypothesis, followed by a matrix attention layer, a local inference layer, another biLSTM inference composition layer, and finally a pooling operation before the output layer.",4 Evaluation,[0],[0]
"Overall, adding ELMo to the ESIM model improves accuracy by an average of 0.7% across five random seeds.",4 Evaluation,[0],[0]
"A five member ensemble pushes the overall accuracy to 89.3%, exceeding the previous ensemble best of 88.9% (Gong et al., 2018).
",4 Evaluation,[0],[0]
"Semantic role labeling A semantic role labeling (SRL) system models the predicate-argument structure of a sentence, and is often described as answering “Who did what to whom”.",4 Evaluation,[0],[0]
"He et al. (2017) modeled SRL as a BIO tagging problem and used an 8-layer deep biLSTM with forward and backward directions interleaved, following Zhou and Xu (2015).",4 Evaluation,[0],[0]
"As shown in Table 1, when adding ELMo to a re-implementation of He et al. (2017) the single model test set F1 jumped 3.2% from 81.4% to 84.6% – a new state-of-the-art on the OntoNotes benchmark (Pradhan et al., 2013), even improving over the previous best ensemble result by 1.2%.
",4 Evaluation,[0],[0]
Coreference resolution Coreference resolution is the task of clustering mentions in text that refer to the same underlying real world entities.,4 Evaluation,[0],[0]
Our baseline model is the end-to-end span-based neural model of Lee et al. (2017).,4 Evaluation,[0],[0]
"It uses a biLSTM
and attention mechanism to first compute span representations and then applies a softmax mention ranking model to find coreference chains.",4 Evaluation,[0],[0]
"In our experiments with the OntoNotes coreference annotations from the CoNLL 2012 shared task (Pradhan et al., 2012), adding ELMo improved the average F1 by 3.2% from 67.2 to 70.4, establishing a new state of the art, again improving over the previous best ensemble result by 1.6% F1.
",4 Evaluation,[0],[0]
"Named entity extraction The CoNLL 2003 NER task (Sang and Meulder, 2003) consists of newswire from the Reuters RCV1 corpus tagged with four different entity types (PER, LOC, ORG, MISC).",4 Evaluation,[0],[0]
"Following recent state-of-the-art systems (Lample et al., 2016; Peters et al., 2017), the baseline model uses pre-trained word embeddings, a character-based CNN representation, two biLSTM layers and a conditional random field (CRF) loss (Lafferty et al., 2001), similar to Collobert et al. (2011).",4 Evaluation,[0],[0]
"As shown in Table 1, our ELMo enhanced biLSTM-CRF achieves 92.22% F1 averaged over five runs.",4 Evaluation,[0],[0]
"The key difference between our system and the previous state of the art from Peters et al. (2017) is that we allowed the task model to learn a weighted average of all biLM layers, whereas Peters et al. (2017) only use the top biLM layer.",4 Evaluation,[0],[0]
"As shown in Sec. 5.1, using all layers instead of just the last layer improves performance across multiple tasks.
",4 Evaluation,[0],[0]
"Sentiment analysis The fine-grained sentiment classification task in the Stanford Sentiment Treebank (SST-5; Socher et al., 2013) involves selecting one of five labels (from very negative to very positive) to describe a sentence from a movie review.",4 Evaluation,[0],[0]
"The sentences contain diverse linguistic phenomena such as idioms and complex syntac-
λ=1 λ=0.001
tic constructions such as negations that are difficult for models to learn.",4 Evaluation,[0],[0]
"Our baseline model is the biattentive classification network (BCN) from McCann et al. (2017), which also held the prior state-of-the-art result when augmented with CoVe embeddings.",4 Evaluation,[0],[0]
Replacing CoVe with ELMo in the BCN model results in a 1.0% absolute accuracy improvement over the state of the art.,4 Evaluation,[0],[0]
This section provides an ablation analysis to validate our chief claims and to elucidate some interesting aspects of ELMo representations.,5 Analysis,[0],[0]
"Sec. 5.1 shows that using deep contextual representations in downstream tasks improves performance over previous work that uses just the top layer, regardless of whether they are produced from a biLM or MT encoder, and that ELMo representations provide the best overall performance.",5 Analysis,[0],[0]
Sec.,5 Analysis,[0],[0]
"5.3 explores the different types of contextual information captured in biLMs and uses two intrinsic evaluations to show that syntactic information is better represented at lower layers while semantic information is captured a higher layers, consistent with MT encoders.",5 Analysis,[0],[0]
It also shows that our biLM consistently provides richer representations then CoVe.,5 Analysis,[0],[0]
"Additionally, we analyze the sensitivity to where ELMo is included in the task model (Sec. 5.2), training set size (Sec. 5.4), and visualize the ELMo learned weights across the tasks (Sec. 5.5).",5 Analysis,[0],[0]
There are many alternatives to Equation 1 for combining the biLM layers.,5.1 Alternate layer weighting schemes,[0],[0]
"Previous work on contextual representations used only the last layer, whether it be from a biLM (Peters et al., 2017) or an MT encoder (CoVe; McCann et al., 2017).",5.1 Alternate layer weighting schemes,[0],[0]
"The choice of the regularization parameter λ is also important, as large values such as λ = 1 effectively reduce the weighting function to a simple average over the layers, while smaller values (e.g., λ = 0.001) allow the layer weights to vary.
",5.1 Alternate layer weighting schemes,[0],[0]
"Table 2 compares these alternatives for SQuAD, SNLI and SRL.",5.1 Alternate layer weighting schemes,[0],[0]
"Including representations from all layers improves overall performance over just using the last layer, and including contextual representations from the last layer improves performance over the baseline.",5.1 Alternate layer weighting schemes,[0],[0]
"For example, in the case of SQuAD, using just the last biLM layer improves development F1 by 3.9% over the baseline.",5.1 Alternate layer weighting schemes,[0],[0]
"Averaging all biLM layers instead of using just the last layer improves F1 another 0.3% (comparing “Last Only” to λ=1 columns), and allowing the task model to learn individual layer weights improves F1 another 0.2% (λ=1 vs. λ=0.001).",5.1 Alternate layer weighting schemes,[0],[0]
"A small λ is preferred in most cases with ELMo, although for NER, a task with a smaller training set, the results are insensitive to λ (not shown).
",5.1 Alternate layer weighting schemes,[0],[0]
The overall trend is similar with CoVe but with smaller increases over the baseline.,5.1 Alternate layer weighting schemes,[0],[0]
"For SNLI, averaging all layers with λ=1 improves development accuracy from 88.2 to 88.7% over using just the last layer.",5.1 Alternate layer weighting schemes,[0],[0]
SRL F1 increased a marginal 0.1% to 82.2 for the λ=1 case compared to using the last layer only.,5.1 Alternate layer weighting schemes,[0],[0]
All of the task architectures in this paper include word embeddings only as input to the lowest layer biRNN.,5.2 Where to include ELMo?,[0],[0]
"However, we find that including ELMo at the output of the biRNN in task-specific architectures improves overall results for some tasks.",5.2 Where to include ELMo?,[0],[0]
"As shown in Table 3, including ELMo at both the input and output layers for SNLI and SQuAD improves over just the input layer, but for SRL (and coreference resolution, not shown) performance is highest when it is included at just the input layer.",5.2 Where to include ELMo?,[0],[0]
"One possible explanation for this result is that both the SNLI and SQuAD architectures use attention layers after the biRNN, so introducing ELMo at this layer allows the model to attend directly to the biLM’s internal representations.",5.2 Where to include ELMo?,[0],[0]
"In the SRL case,
the task-specific context representations are likely more important than those from the biLM.",5.2 Where to include ELMo?,[0],[0]
"Since adding ELMo improves task performance over word vectors alone, the biLM’s contextual representations must encode information generally useful for NLP tasks that is not captured in word vectors.",5.3 What information is captured by the biLM’s representations?,[0],[0]
"Intuitively, the biLM must be disambiguating the meaning of words using their context.",5.3 What information is captured by the biLM’s representations?,[0],[0]
"Consider “play”, a highly polysemous word.",5.3 What information is captured by the biLM’s representations?,[0],[0]
The top of Table 4 lists nearest neighbors to “play” using GloVe vectors.,5.3 What information is captured by the biLM’s representations?,[0],[0]
"They are spread across several parts of speech (e.g., “played”, “playing” as verbs, and “player”, “game” as nouns) but concentrated in the sportsrelated senses of “play”.",5.3 What information is captured by the biLM’s representations?,[0],[0]
"In contrast, the bottom two rows show nearest neighbor sentences from the SemCor dataset (see below) using the biLM’s context representation of “play” in the source sentence.",5.3 What information is captured by the biLM’s representations?,[0],[0]
"In these cases, the biLM is able to disambiguate both the part of speech and word sense in the source sentence.
",5.3 What information is captured by the biLM’s representations?,[0],[0]
These observations can be quantified using an,5.3 What information is captured by the biLM’s representations?,[0],[0]
intrinsic evaluation of the contextual representations similar to Belinkov et al. (2017).,Model Acc.,[0],[0]
"To isolate the information encoded by the biLM, the representations are used to directly make predictions for a fine grained word sense disambiguation (WSD) task and a POS tagging task.",Model Acc.,[0],[0]
"Using this approach, it is also possible to compare to CoVe, and across each of the individual layers.
",Model Acc.,[0],[0]
Word sense disambiguation,Model Acc.,[0],[0]
"Given a sentence, we can use the biLM representations to predict the sense of a target word using a simple 1- nearest neighbor approach, similar to Melamud et al. (2016).",Model Acc.,[0],[0]
"To do so, we first use the biLM to compute representations for all words in SemCor 3.0, our training corpus (Miller et al., 1994), and then take the average representation for each sense.",Model Acc.,[0],[0]
"At test time, we again use the biLM to compute representations for a given target word and take the nearest neighbor sense from the training set, falling back to the first sense from WordNet for lemmas not observed during training.
",Model Acc.,[0],[0]
Table 5 compares WSD results using the evaluation framework from Raganato et al. (2017b) across the same suite of four test sets in Raganato et al. (2017a).,Model Acc.,[0],[0]
"Overall, the biLM top layer rep-
resentations have F1 of 69.0 and are better at WSD then the first layer.",Model Acc.,[0],[0]
"This is competitive with a state-of-the-art WSD-specific supervised model using hand crafted features (Iacobacci et al., 2016) and a task specific biLSTM that is also trained with auxiliary coarse-grained semantic labels and POS tags (Raganato et al., 2017a).",Model Acc.,[0],[0]
"The CoVe biLSTM layers follow a similar pattern to those from the biLM (higher overall performance at the second layer compared to the first); however, our biLM outperforms the CoVe biLSTM, which trails the WordNet first sense baseline.
",Model Acc.,[0],[0]
"POS tagging To examine whether the biLM captures basic syntax, we used the context representations as input to a linear classifier that predicts POS tags with the Wall Street Journal portion of the Penn Treebank (PTB) (Marcus et al., 1993).",Model Acc.,[0],[0]
"As the linear classifier adds only a small amount of model capacity, this is direct test of the biLM’s representations.",Model Acc.,[0],[0]
"Similar to WSD, the biLM representations are competitive with carefully tuned, task specific biLSTMs",Model Acc.,[0],[0]
"(Ling et al., 2015; Ma and Hovy, 2016).",Model Acc.,[0],[0]
"However, unlike WSD, accuracies using the first biLM layer are higher than the top layer, consistent with results from deep biLSTMs in multi-task training (Søgaard and Goldberg, 2016; Hashimoto et al., 2017) and MT (Belinkov et al., 2017).",Model Acc.,[0],[0]
"CoVe POS tagging accuracies follow the same pattern as those from the biLM, and just like for WSD, the biLM achieves higher accuracies than the CoVe encoder.
",Model Acc.,[0],[0]
"Implications for supervised tasks Taken together, these experiments confirm different layers in the biLM represent different types of information and explain why including all biLM layers is important for the highest performance in downstream tasks.",Model Acc.,[0],[0]
"In addition, the biLM’s representations are more transferable to WSD and POS tagging than those in CoVe, helping to illustrate why ELMo outperforms CoVe in downstream tasks.",Model Acc.,[0],[0]
"Adding ELMo to a model increases the sample efficiency considerably, both in terms of number of parameter updates to reach state-of-the-art performance and the overall training set size.",5.4 Sample efficiency,[0],[0]
"For example, the SRL model reaches a maximum development F1 after 486 epochs of training without ELMo.",5.4 Sample efficiency,[0],[0]
"After adding ELMo, the model exceeds the baseline maximum at epoch 10, a 98% relative decrease in the number of updates needed to reach
the same level of performance.
",5.4 Sample efficiency,[0],[0]
"In addition, ELMo-enhanced models use smaller training sets more efficiently than models without ELMo.",5.4 Sample efficiency,[0],[0]
Figure 1 compares the performance of baselines models with and without ELMo as the percentage of the full training set is varied from 0.1% to 100%.,5.4 Sample efficiency,[0],[0]
Improvements with ELMo are largest for smaller training sets and significantly reduce the amount of training data needed to reach a given level of performance.,5.4 Sample efficiency,[0],[0]
"In the SRL case, the ELMo model with 1% of the training set has about the same F1 as the baseline model with 10% of the training set.",5.4 Sample efficiency,[0],[0]
Figure 2 visualizes the softmax-normalized learned layer weights.,5.5 Visualization of learned weights,[0],[0]
"At the input layer, the task model favors the first biLSTM layer.",5.5 Visualization of learned weights,[0],[0]
"For coreference and SQuAD, the this is strongly favored, but the distribution is less peaked for the other tasks.",5.5 Visualization of learned weights,[0],[0]
"The output layer weights are relatively balanced, with a slight preference for the lower layers.",5.5 Visualization of learned weights,[0],[0]
"We have introduced a general approach for learning high-quality deep context-dependent representations from biLMs, and shown large improvements when applying ELMo to a broad range of NLP tasks.",6 Conclusion,[0],[0]
"Through ablations and other controlled experiments, we have also confirmed that the biLM layers efficiently encode different types of syntactic and semantic information about wordsin-context, and that using all layers improves overall task performance.",6 Conclusion,[0],[0]
"Deep contextualized word representations
This supplement contains details of the model architectures, training routines and hyper-parameter choices for the state-of-the-art models in Section 4.
",A Supplemental Material to accompany,[0],[0]
All of the individual models share a common architecture in the lowest layers with a context independent token representation below several layers of stacked RNNs – LSTMs in every case except the SQuAD model that uses GRUs.,A Supplemental Material to accompany,[0],[0]
"As noted in Sec. 3.4, fine tuning the biLM on task specific data typically resulted in significant drops in perplexity.",A.1 Fine tuning biLM,[0],[0]
"To fine tune on a given task, the supervised labels were temporarily ignored, the biLM fine tuned for one epoch on the training split and evaluated on the development split.",A.1 Fine tuning biLM,[0],[0]
"Once fine tuned, the biLM weights were fixed during task training.
",A.1 Fine tuning biLM,[0],[0]
Table 7 lists the development set perplexities for the considered tasks.,A.1 Fine tuning biLM,[0],[0]
"In every case except CoNLL 2012, fine tuning results in a large improvement in perplexity, e.g., from 72.1 to 16.8 for SNLI.
",A.1 Fine tuning biLM,[0],[0]
The impact of fine tuning on supervised performance is task dependent.,A.1 Fine tuning biLM,[0],[0]
"In the case of SNLI, fine tuning the biLM increased development accuracy 0.6% from 88.9% to 89.5% for our single best model.",A.1 Fine tuning biLM,[0],[0]
"However, for sentiment classification development set accuracy is approximately the same regardless whether a fine tuned biLM was used.
",A.1 Fine tuning biLM,[0],[0]
A.2 Importance of γ in Eqn.,A.1 Fine tuning biLM,[0],[0]
"(1)
The γ parameter in Eqn.",A.1 Fine tuning biLM,[0],[0]
"(1) was of practical importance to aid optimization, due to the different distributions between the biLM internal representations and the task specific representations.",A.1 Fine tuning biLM,[0],[0]
It is especially important in the last-only case in Sec. 5.1.,A.1 Fine tuning biLM,[0],[0]
"Without this parameter, the last-only case performed poorly (well below the baseline) for SNLI and training failed completely for SRL.",A.1 Fine tuning biLM,[0],[0]
Our baseline SNLI model is the ESIM sequence model from Chen et al. (2017).,A.3 Textual Entailment,[0],[0]
"Following the original implementation, we used 300 dimensions for all LSTM and feed forward layers and pretrained 300 dimensional GloVe embeddings that were fixed during training.",A.3 Textual Entailment,[0],[0]
"For regularization, we
added 50% variational dropout (Gal and Ghahramani, 2016) to the input of each LSTM layer and 50% dropout (Srivastava et al., 2014) at the input to the final two fully connected layers.",A.3 Textual Entailment,[0],[0]
All feed forward layers use ReLU activations.,A.3 Textual Entailment,[0],[0]
"Parameters were optimized using Adam (Kingma and Ba, 2015) with gradient norms clipped at 5.0 and initial learning rate 0.0004, decreasing by half each time accuracy on the development set did not increase in subsequent epochs.",A.3 Textual Entailment,[0],[0]
"The batch size was 32.
",A.3 Textual Entailment,[0],[0]
"The best ELMo configuration added ELMo vectors to both the input and output of the lowest layer LSTM, using (1) with layer normalization and λ = 0.001.",A.3 Textual Entailment,[0],[0]
"Due to the increased number of parameters in the ELMo model, we added `2 regularization with regularization coefficient 0.0001 to all recurrent and feed forward weight matrices and 50% dropout after the attention layer.
",A.3 Textual Entailment,[0],[0]
Table 8 compares test set accuracy of our system to previously published systems.,A.3 Textual Entailment,[0],[0]
"Overall, adding ELMo to the ESIM model improved accuracy by 0.7% establishing a new single model state-of-the-art of 88.7%, and a five member ensemble pushes the overall accuracy to 89.3%.",A.3 Textual Entailment,[0],[0]
Our QA model is a simplified version of the model from Clark and Gardner (2017).,A.4 Question Answering,[0],[0]
"It embeds tokens by concatenating each token’s case-sensitive 300 dimensional GloVe word vector (Pennington et al., 2014) with a character-derived embedding produced using a convolutional neural network followed by max-pooling on learned character embeddings.",A.4 Question Answering,[0],[0]
"The token embeddings are passed through a shared bi-directional GRU, and then the bi-directional attention mechanism from BiDAF (Seo et al., 2017).",A.4 Question Answering,[0],[0]
"The augmented con-
text vectors are then passed through a linear layer with ReLU activations, a residual self-attention layer that uses a GRU followed by the same attention mechanism applied context-to-context, and another linear layer with ReLU activations.",A.4 Question Answering,[0],[0]
"Finally, the results are fed through linear layers to predict the start and end token of the answer.
",A.4 Question Answering,[0],[0]
Variational dropout is used before the input to the GRUs and the linear layers at a rate of 0.2.,A.4 Question Answering,[0],[0]
"A dimensionality of 90 is used for the GRUs, and 180 for the linear layers.",A.4 Question Answering,[0],[0]
We optimize the model using Adadelta with a batch size of 45.,A.4 Question Answering,[0],[0]
At test time we use an exponential moving average of the weights and limit the output span to be of at most size 17.,A.4 Question Answering,[0],[0]
"We do not update the word vectors during training.
",A.4 Question Answering,[0],[0]
"Performance was highest when adding ELMo without layer normalization to both the input and output of the contextual GRU layer and leaving the ELMo weights unregularized (λ = 0).
",A.4 Question Answering,[0],[0]
"Table 9 compares test set results from the SQuAD leaderboard as of November 17, 2017 when we submitted our system.",A.4 Question Answering,[0],[0]
"Overall, our submission had the highest single model and ensemble results, improving the previous single model result (SAN) by 1.4% F1 and our baseline by 4.2%.",A.4 Question Answering,[0],[0]
"A 11 member ensemble pushes F1 to 87.4%, 1.0% increase over the previous ensemble best.",A.4 Question Answering,[0],[0]
"Our baseline SRL model is an exact reimplementation of (He et al., 2017).",A.5 Semantic Role Labeling,[0],[0]
"Words are represented using a concatenation of 100 dimensional vector representations, initialized using GloVe (Pennington et al., 2014) and a binary, per-word predicate feature, represented using an 100 dimensional em-
3A comprehensive comparison can be found at https: //nlp.stanford.edu/projects/snli/
bedding.",A.5 Semantic Role Labeling,[0],[0]
"This 200 dimensional token representation is then passed through an 8 layer “interleaved” biLSTM with a 300 dimensional hidden size, in which the directions of the LSTM layers alternate per layer.",A.5 Semantic Role Labeling,[0],[0]
"This deep LSTM uses Highway connections (Srivastava et al., 2015) between layers and variational recurrent dropout (Gal and Ghahramani, 2016).",A.5 Semantic Role Labeling,[0],[0]
This deep representation is then projected using a final dense layer followed by a softmax activation to form a distribution over all possible tags.,A.5 Semantic Role Labeling,[0],[0]
"Labels consist of semantic roles from PropBank (Palmer et al., 2005) augmented with a BIO labeling scheme to represent argument spans.",A.5 Semantic Role Labeling,[0],[0]
"During training, we minimize the negative log likelihood of the tag sequence using Adadelta with a learning rate of 1.0 and ρ = 0.95 (Zeiler, 2012).",A.5 Semantic Role Labeling,[0],[0]
"At test time, we perform Viterbi decoding to enforce valid spans using BIO constraints.",A.5 Semantic Role Labeling,[0],[0]
Variational dropout of 10% is added to all LSTM hidden layers.,A.5 Semantic Role Labeling,[0],[0]
Gradients are clipped if their value exceeds 1.0.,A.5 Semantic Role Labeling,[0],[0]
"Models are trained for 500 epochs or until validation F1 does not improve for 200 epochs, whichever is sooner.",A.5 Semantic Role Labeling,[0],[0]
The pretrained GloVe vectors are fine-tuned during training.,A.5 Semantic Role Labeling,[0],[0]
The final dense layer and all cells of all LSTMs are initialized to be orthogonal.,A.5 Semantic Role Labeling,[0],[0]
"The forget gate bias is initialized to 1 for all LSTMs, with all other gates initialized to 0, as per (Józefowicz et al., 2015).
",A.5 Semantic Role Labeling,[0],[0]
"Table 10 compares test set F1 scores of our ELMo augmented implementation of (He et al., 2017) with previous results.",A.5 Semantic Role Labeling,[0],[0]
"Our single model score of 84.6 F1 represents a new state-of-the-art result on the CONLL 2012 Semantic Role Labeling task, surpassing the previous single model result by 2.9 F1 and a 5-model ensemble by 1.2 F1.",A.5 Semantic Role Labeling,[0],[0]
Our baseline coreference model is the end-to-end neural model from Lee et al. (2017) with all hy-,A.6 Coreference resolution,[0],[0]
"perparameters exactly following the original implementation.
",Model Average F1,[0],[0]
The best configuration added ELMo to the input of the lowest layer biLSTM and weighted the biLM layers using (1) without any regularization (λ = 0) or layer normalization.,Model Average F1,[0],[0]
"50% dropout was added to the ELMo representations.
",Model Average F1,[0],[0]
Table 11 compares our results with previously published results.,Model Average F1,[0],[0]
"Overall, we improve the single model state-of-the-art by 3.2% average F1, and our single model result improves the previous ensemble best by 1.6% F1.",Model Average F1,[0],[0]
Adding ELMo to the output from the biLSTM in addition to the biLSTM input reduced F1 by approximately 0.7% (not shown).,Model Average F1,[0],[0]
"Our baseline NER model concatenates 50 dimensional pre-trained Senna vectors (Collobert et al., 2011) with a CNN character based representation.",A.7 Named Entity Recognition,[0],[0]
"The character representation uses 16 dimensional character embeddings and 128 convolutional filters of width three characters, a ReLU activation and by max pooling.",A.7 Named Entity Recognition,[0],[0]
"The token representation is passed through two biLSTM layers, the first with 200 hidden units and the second with 100 hidden units before a final dense layer and softmax layer.",A.7 Named Entity Recognition,[0],[0]
"During training, we use a CRF loss and at test time perform decoding using the Viterbi algorithm while ensuring that the output tag sequence is valid.
",A.7 Named Entity Recognition,[0],[0]
Variational dropout is added to the input of both biLSTM layers.,A.7 Named Entity Recognition,[0],[0]
During training the gradients are rescaled if their `2 norm exceeds 5.0 and parameters updated using Adam with constant learning rate of 0.001.,A.7 Named Entity Recognition,[0],[0]
The pre-trained Senna embeddings are fine tuned during training.,A.7 Named Entity Recognition,[0],[0]
"We employ early stopping on the development set and report the averaged test set score across five runs with different random seeds.
ELMo was added to the input of the lowest layer task biLSTM.",A.7 Named Entity Recognition,[0],[0]
"As the CoNLL 2003 NER data set is relatively small, we found the best performance by constraining the trainable layer weights to be effectively constant by setting λ = 0.1 with (1).
",A.7 Named Entity Recognition,[0],[0]
Table 12 compares test set F1 scores of our ELMo enhanced biLSTM-CRF tagger with previous results.,A.7 Named Entity Recognition,[0],[0]
"Overall, the 92.22% F1 from our system establishes a new state-of-the-art.",A.7 Named Entity Recognition,[0],[0]
"When compared to Peters et al. (2017), using representations
from all layers of the biLM provides a modest improvement.",A.7 Named Entity Recognition,[0],[0]
"We use almost the same biattention classification network architecture described in McCann et al. (2017), with the exception of replacing the final maxout network with a simpler feedforward network composed of two ReLu layers with dropout.",A.8 Sentiment classification,[0],[0]
"A BCN model with a batch-normalized maxout network reached significantly lower validation accuracies in our experiments, although there may be discrepancies between our implementation and that of McCann et al. (2017).",A.8 Sentiment classification,[0],[0]
"To match the CoVe training setup, we only train on phrases that contain four or more tokens.",A.8 Sentiment classification,[0],[0]
"We use 300-d hidden states for the biLSTM and optimize the model parameters with Adam (Kingma and Ba, 2015) using a learning rate of 0.0001.",A.8 Sentiment classification,[0],[0]
"The trainable biLM layer weights are regularized by λ = 0.001, and we add ELMo to both the input and output of the biLSTM; the output ELMo vectors are computed with a second biLSTM and concatenated to the input.",A.8 Sentiment classification,[0],[0]
"We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy).",abstractText,[0],[0]
"Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus.",abstractText,[0],[0]
"We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis.",abstractText,[0],[0]
"We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",abstractText,[0],[0]
Deep contextualized word representations,title,[0],[0]
"In multi-task reinforcement learning (MTRL) agents are presented several related target tasks (Taylor & Stone, 2009; Caruana, 1998) with shared characteristics.",1. Introduction,[1.0],"['In multi-task reinforcement learning (MTRL) agents are presented several related target tasks (Taylor & Stone, 2009; Caruana, 1998) with shared characteristics.']"
"Rather than specialize on a single task, the objective is to generalize performance across all tasks.",1. Introduction,[0],[0]
"For example, a team of autonomous underwater vehicles (AUVs) learning to detect and repair faults in deep-sea equipment must be able to do so in many settings (varying water currents, lighting, etc.), not just under the circumstances observed during training.
",1. Introduction,[0],[0]
"Many real-world problems involve multiple agents with
1Laboratory for Information and Decision Systems (LIDS), MIT, Cambridge, MA, USA 2College of Computer and Information Science (CCIS), Northeastern University, Boston, MA, USA 3Boeing Research & Technology, Seattle, WA, USA.",1. Introduction,[0],[0]
"Correspondence to: Shayegan Omidshafiei <shayegan@mit.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
partial observability and limited communication (e.g., the AUV example) (Oliehoek & Amato, 2016), but generating accurate models for these domains is difficult due to complex interactions between agents and the environment.",1. Introduction,[0],[0]
"Learning is difficult in these settings due to partial observability and local viewpoints of agents, which perceive the environment as non-stationary due to teammates’ actions.",1. Introduction,[0],[0]
Efficient learners must extract knowledge from past tasks to accelerate learning and improve generalization to new tasks.,1. Introduction,[0],[0]
"Learning specialized policies for individual tasks can be problematic, as not only do agents have to store a distinct policy for each task, but in practice face scenarios where the identity of the task is often non-observable.
",1. Introduction,[0.9999999745207652],"['Learning specialized policies for individual tasks can be problematic, as not only do agents have to store a distinct policy for each task, but in practice face scenarios where the identity of the task is often non-observable.']"
"Existing MTRL methods focus on single-agent and/or fully observable settings (Taylor & Stone, 2009).",1. Introduction,[0],[0]
"By contrast, this work considers cooperative, independent learners operating in partially-observable, stochastic environments, receiving feedback in the form of local noisy observations and joint rewards.",1. Introduction,[0],[0]
This setting is general and realistic for many multi-agent domains.,1. Introduction,[0],[0]
"We introduce the multitask multi-agent reinforcement learning (MT-MARL) under partial observability problem, where the goal is to maximize execution-time performance on a set of related tasks, without explicit knowledge of the task identity.",1. Introduction,[0],[0]
"Each MTMARL task is formalized as a Decentralized Partially Observable Markov Decision Process (Dec-POMDP) (Bernstein et al., 2002), a general formulation for cooperative decision-making under uncertainty.",1. Introduction,[0],[0]
"MT-MARL poses significant challenges, as each agent must learn to coordinate with teammates to achieve good performance, ensure policy generalization across all tasks, and conduct (implicit) execution-time inference of the underlying task ID to make sound decisions using local noisy observations.",1. Introduction,[0],[0]
"As typical in existing MTRL approaches, this work focuses on average asymptotic performance across all tasks (Caruana, 1998; Taylor & Stone, 2009) and sample-efficient learning.
",1. Introduction,[0],[0]
"We propose a two-phase MT-MARL approach that first uses cautiously-optimistic learners in combination with Deep Recurrent Q-Networks (DRQNs) (Hausknecht & Stone, 2015) for action-value approximation.",1. Introduction,[0],[0]
"We introduce Concurrent Experience Replay Trajectories (CERTs), a decentralized extension of ex-
ar X
iv :1
70 3.
06 18
2v 4
[ cs
.L G
] 1
3 Ju
l 2 01
7
perience replay (Lin, 1992; Mnih et al., 2015) targeting sample-efficient and stable MARL.",1. Introduction,[0],[0]
This first contribution enables coordination in single-task MARL under partial observability.,1. Introduction,[0],[0]
The second phase of our approach distills each agent’s specialized action-value networks into a generalized recurrent multi-task network.,1. Introduction,[0],[0]
"Using CERTs and optimistic learners, well-performing distilled policies (Rusu et al., 2015) are learned for multi-agent domains.",1. Introduction,[0],[0]
Both the single-task and multi-task phases of the algorithm are demonstrated to achieve good performance on a set of multi-agent target capture Dec-POMDP domains.,1. Introduction,[0],[0]
The approach makes no assumptions about communication capabilities and is fully decentralized during learning and execution.,1. Introduction,[0],[0]
"To our knowledge, this is the first formalization of decentralized MT-MARL under partial observability.",1. Introduction,[0],[0]
"Single-agent RL under full observability is typically formalized using Markov Decision Processes (MDPs) (Sutton & Barto, 1998), defined as tuple 〈S,A, T ,R, γ〉.",2.1. Reinforcement Learning,[0],[0]
"At timestep t, the agent with state s ∈ S executes action a ∈",2.1. Reinforcement Learning,[0],[0]
"A using policy π(a|s), receives reward rt = R(s) ∈ R, and transitions to state s′ ∈ S with probability P (s′|s, a) = T (s, a, s′).",2.1. Reinforcement Learning,[0],[0]
"Denoting discounted return as Rt = ∑H t′=t γ
t′−trt, with horizon H and discount factor γ ∈",2.1. Reinforcement Learning,[0],[0]
"[0, 1), the action-value (or Q-value) is defined as Qπ(s, a) =",2.1. Reinforcement Learning,[0],[0]
"Eπ[Rt|st = s, at = a].",2.1. Reinforcement Learning,[0],[0]
"Optimal policy π∗ maximizes the Q-value function, Qπ ∗ (s, a) = maxπ Q(s, a).",2.1. Reinforcement Learning,[0],[0]
"In RL, the agent interacts with the environment to learn π∗ without explicit provision of the MDP model.",2.1. Reinforcement Learning,[0],[0]
"Model-based methods first learn T andR, then use a planner to find Qπ ∗ .",2.1. Reinforcement Learning,[0],[0]
"Model-free methods typically directly learn Q-values or policies, so can be more space and computation efficient.
",2.1. Reinforcement Learning,[0],[0]
"Q-learning (Watkins & Dayan, 1992) iteratively estimates the optimal Q-value function using backups, Q(s, a) =",2.1. Reinforcement Learning,[0],[0]
"Q(s, a) + α[r + γmaxa′",2.1. Reinforcement Learning,[0],[0]
"Q(s
′, a′)",2.1. Reinforcement Learning,[0],[0]
"− Q(s, a)], where α ∈",2.1. Reinforcement Learning,[0],[0]
"[0, 1) is the learning rate and the term in brackets is the temporal-difference (TD) error.",2.1. Reinforcement Learning,[0],[0]
"Convergence to Qπ ∗ is guaranteed in the tabular (no approximation) case provided sufficient state/action space exploration; however, tabulated learning is unsuitable for problems with large state/action spaces.",2.1. Reinforcement Learning,[0],[0]
"Practical TD methods instead use function approximators (Gordon, 1995) such as linear combinations of basis functions or neural networks, leveraging inductive bias to execute similar actions in similar states.",2.1. Reinforcement Learning,[0],[0]
"Deep Q-learning is a state-of-the-art approach using a Deep Q-Network (DQN) for Q-value approximation (Mnih et al., 2015).",2.1. Reinforcement Learning,[0],[0]
"At each iteration j, experience tuple 〈s, a, r, s′〉 is sampled from replay memory M and DQN parameters θ are updated to minimize loss Lj(θj) =",2.1. Reinforcement Learning,[0],[0]
"E(s,a,r,s′)∼M[(r +
γmaxa′ Q(s ′, a′; θ̂j)−Q(s, a; θj))2].",2.1. Reinforcement Learning,[0],[0]
Replay memoryM is a first-in first-out queue containing the set of latest experience tuples from -greedy policy execution.,2.1. Reinforcement Learning,[0],[0]
"Target network parameters θ̂j are updated less frequently and, in combination with experience replay, are critical for stable deep Q-learning.
",2.1. Reinforcement Learning,[0],[0]
Agents in partially-observable domains receive observations of the latent state.,2.1. Reinforcement Learning,[0],[0]
"Such domains are formalized as Partially Observable Markov Decision Processes (POMDPs), defined as 〈S,A, T ,R,Ω,O, γ〉 (Kaelbling et al., 1998).",2.1. Reinforcement Learning,[0],[0]
"After each transition, the agent observes o ∈ Ω with probability P (o|s′, a) = O(o, s′, a).",2.1. Reinforcement Learning,[0],[0]
"Due to noisy observations, POMDP policies map observation histories to actions.",2.1. Reinforcement Learning,[0],[0]
"As Recurrent Neural Networks (RNNs) inherently maintain an internal state ht to compress input history until timestep t, they have been demonstrated to be effective for learning POMDP policies (Wierstra et al., 2007).",2.1. Reinforcement Learning,[0],[0]
"Recent work has introduced Deep Recurrent QNetworks (DRQNs) (Hausknecht & Stone, 2015), combining Long Short-Term Memory (LSTM) cells (Hochreiter & Schmidhuber, 1997) with DQNs for RL in POMDPs.",2.1. Reinforcement Learning,[0],[0]
"Our work extends this single-task, single-agent approach to the multi-task, multi-agent setting.",2.1. Reinforcement Learning,[0],[0]
"Multi-agent RL (MARL) involves a set of agents in a shared environment, which must learn to maximize their individual returns (Buşoniu et al., 2010).",2.2. Multi-agent RL,[0],[0]
"Our work focuses on cooperative settings, where agents share a joint return.",2.2. Multi-agent RL,[0],[0]
Claus & Boutilier (1998) dichotomize MARL agents into two classes: Joint Action Learners (JALs) and Independent Learners (ILs).,2.2. Multi-agent RL,[0],[0]
"JALs observe actions taken by all agents, whereas ILs only observe local actions.",2.2. Multi-agent RL,[0],[0]
"As observability of joint actions is a strong assumption in partially observable domains, ILs are typically more practical, despite having to solve a more challenging problem (Claus & Boutilier, 1998).",2.2. Multi-agent RL,[0],[0]
"Our approach utilizes ILs that conduct both learning and execution in a decentralized manner.
",2.2. Multi-agent RL,[0],[0]
"Unique challenges arise in MARL due to agent interactions during learning (Buşoniu et al., 2010; Matignon et al., 2012).",2.2. Multi-agent RL,[0],[0]
"Multi-agent domains are non-stationary from agents’ local perspectives, due to teammates’ interactions with the environment.",2.2. Multi-agent RL,[0],[0]
"ILs, in particular, are susceptible to shadowed equilibria, where local observability and nonstationarity cause locally optimal actions to become a globally sub-optimal joint action (Fulda & Ventura, 2007).",2.2. Multi-agent RL,[0],[0]
"Effective MARL requires each agent to tightly coordinate with fellow agents, while also being robust against destabilization of its own policy due to environmental nonstationarity.",2.2. Multi-agent RL,[0],[0]
"Another desired characteristic is robustness to alter-exploration, or drastic changes in policies due to exploratory actions of teammates (Matignon et al., 2012).",2.2. Multi-agent RL,[0],[0]
"Transfer Learning (TL) aims to generalize knowledge from a set of source tasks to a target task (Pan & Yang, 2010).",2.3. Transfer and Multi-Task Learning,[0],[0]
"In single-agent, fully-observable RL, each task is formalized as a distinct MDP (i.e., MDPs and tasks are synonymous)",2.3. Transfer and Multi-Task Learning,[0],[0]
"(Taylor & Stone, 2009)",2.3. Transfer and Multi-Task Learning,[0],[0]
.,2.3. Transfer and Multi-Task Learning,[0],[0]
"While TL assumes sequential transfer, where source tasks have been previously learned and even may not be related to the target task, Multi-Task Reinforcement Learning (MTRL) aims to learn a policy that performs well on related target tasks from an underlying task distribution (Caruana, 1998; Pan & Yang, 2010).",2.3. Transfer and Multi-Task Learning,[0],[0]
"MTRL tasks can be learned simultaneously or sequentially (Taylor & Stone, 2009).",2.3. Transfer and Multi-Task Learning,[0],[0]
"MTRL directs the agent’s attention towards pertinent training signals learned on individual tasks, enabling a unified policy to generalize well across all tasks.",2.3. Transfer and Multi-Task Learning,[0],[0]
"MTRL is most beneficial when target tasks share common features (Wilson et al., 2007), and most challenging when the task ID is not explicitly specified to agents during execution – the setting addressed in this paper.",2.3. Transfer and Multi-Task Learning,[0],[0]
Buşoniu et al. (2010) present a taxonomy of MARL approaches.,3.1. Multi-agent RL,[0],[0]
Partially-observable MARL has received limited attention.,3.1. Multi-agent RL,[0],[0]
"Works include model-free gradient-ascent based methods (Peshkin et al., 2000; Dutech et al., 2001), simulator-supported methods to improve policies using a series of linear programs (Wu et al., 2012), and modelbased approaches where agents learn in an interleaved fashion to reduce destabilization caused by concurrent learning (Banerjee et al., 2012).",3.1. Multi-agent RL,[0],[0]
"Recent scalable methods use Expectation Maximization to learn finite state controller (FSC) policies (Wu et al., 2013; Liu et al., 2015; 2016).
",3.1. Multi-agent RL,[0],[0]
"Our approach is most related to IL algorithms that learn Qvalues, as their representational power is more conducive to transfer between tasks, in contrast to policy tables or FSCs.",3.1. Multi-agent RL,[0],[0]
The majority of existing IL approaches assume full observability.,3.1. Multi-agent RL,[0],[0]
"Matignon et al. (2012) survey these approaches, the most straightforward being Decentralized Qlearning (Tan, 1993), where each agent performs independent Q-learning.",3.1. Multi-agent RL,[0],[0]
"This simple approach has some empirical success (Matignon et al., 2012).",3.1. Multi-agent RL,[0],[0]
"Distributed Q-learning (Lauer & Riedmiller, 2000) is an optimal algorithm for deterministic domains; it updates Q-values only when they are guaranteed to increase, and the policy only for actions that are no longer greedy with respect to Q-values.",3.1. Multi-agent RL,[0],[0]
Bowling & Veloso (2002) conduct Policy Hill Climbing using the Win-or-Learn Fast heuristic to decrease (increase) each agent’s learning rate when it performs well (poorly).,3.1. Multi-agent RL,[0],[0]
"Frequency Maximum Q-Value heuristics (Kapetanakis & Kudenko, 2002) bias action selection towards those con-
sistently achieving max rewards.",3.1. Multi-agent RL,[0],[0]
"Hysteretic Q-learning (Matignon et al., 2007) addresses miscoordination using cautious optimism to stabilize policies while teammates explore.",3.1. Multi-agent RL,[0],[0]
"Its track record of empirical success against complex methods (Xu et al., 2012; Matignon et al., 2012; Barbalios & Tzionas, 2014) leads us to use it as a foundation for our MT-MARL approach.",3.1. Multi-agent RL,[0],[0]
"Foerster et al. (2016) present architectures to learn communication protocols for DecPOMDP RL, noting best performance using a centralized approach with inter-agent backpropagation and parameter sharing.",3.1. Multi-agent RL,[0],[0]
"They also evaluate a model combining Decentralized Q-learning with DRQNs, which they call Reinforced Inter-Agent Learning.",3.1. Multi-agent RL,[0],[0]
"Given the decentralized nature of this latter model (called Dec-DRQN herein for clarity), we evaluate our method against it.",3.1. Multi-agent RL,[0],[0]
"Concurrent to our work, Foerster et al. (2017) investigated an alternate means of stabilizing experience replay for the centralized learning case.",3.1. Multi-agent RL,[0],[0]
"Taylor & Stone (2009) and Torrey & Shavlik (2009) provide excellent surveys of transfer and multi-task RL, which almost exclusively target single-agent, fully-observable settings.",3.2. Transfer and Multi-task RL,[0],[0]
"Tanaka & Yamamura (2003) use first and secondorder statistics to compute a prioritized sweeping metric for MTRL, enabling an agent to maximize lifetime reward over task sequences.",3.2. Transfer and Multi-task RL,[0],[0]
"Fernández & Veloso (2006) introduce an MDP policy similarity metric, and learn a policy library that generalizes well to tasks within a shared domain.",3.2. Transfer and Multi-task RL,[0],[0]
"Wilson et al. (2007) consider TL for MDPs, learning a Dirichlet Process Mixture Model over source MDPs, used as an informative prior for a target MDP.",3.2. Transfer and Multi-task RL,[0],[0]
"They extend the work to multi-agent MDPs by learning characteristic agent roles (Wilson et al., 2008).",3.2. Transfer and Multi-task RL,[0],[0]
"Brunskill & Li (2013) introduce an MDP clustering approach that reduces negative transfer in MTRL, and prove reduction of sample complexity of exploration using transfer.",3.2. Transfer and Multi-task RL,[0],[0]
Taylor et al. (2013) introduce parallel transfer to accelerate multi-agent learning using interagent transfer.,3.2. Transfer and Multi-task RL,[0],[0]
"Recent work extends the notion of neural network distillation (Hinton et al., 2015) to DQNs for single-agent, fully-observable MTRL, first learning a set of specialized teacher DQNs, then distilling teachers to a single multi-task network (Rusu et al., 2015).",3.2. Transfer and Multi-task RL,[0],[0]
The efficacy of the distillation technique for single-agent MDPs with large state spaces leads our work to use it as a foundation for the proposed MT-MARL under partial observability approach.,3.2. Transfer and Multi-task RL,[0],[0]
This section introduces MT-MARL under partial observability.,4. Multi-task Multi-agent RL,[0],[0]
"We formalize single-task MARL using the Decentralized Partially Observable Markov Decision Process (Dec-POMDP), defined as 〈I,S,A, T ,R,Ω,O, γ〉, where I is a set of n agents, S is the state space, A =
×iA(i) is the joint action space, and Ω = ×iΩ(i) is the joint observation space (Bernstein et al.,",4. Multi-task Multi-agent RL,[0.9936508519718732],"['We formalize single-task MARL using the Decentralized Partially Observable Markov Decision Process (Dec-POMDP), defined as 〈I,S,A, T ,R,Ω,O, γ〉, where I is a set of n agents, S is the state space, A = ×iA(i) is the joint action space, and Ω = ×iΩ(i) is the joint observation space (Bernstein et al., 2002).1 Each agent i executes action a(i) ∈ A(i), where joint action a = 〈a(1), .']"
2002).1 Each agent i executes action a(i) ∈,4. Multi-task Multi-agent RL,[0],[0]
"A(i), where joint action a = 〈a(1), . .",4. Multi-task Multi-agent RL,[0],[0]
.,4. Multi-task Multi-agent RL,[0],[0]
", a(n)〉 causes environment state s ∈ S to transition with probability P (s′|s,a) = T (s,a, s′).",4. Multi-task Multi-agent RL,[0],[0]
"At each timestep, each agent receives observation o(i) ∈ Ω(i), with joint observation probability P (o|s′,a) = O(o, s′,a), where o = 〈o(1), . . .",4. Multi-task Multi-agent RL,[0],[0]
", o(n)〉.",4. Multi-task Multi-agent RL,[0],[0]
Let local observation history at timestep t be ~ot (i) =,4. Multi-task Multi-agent RL,[0],[0]
"(o (i) 1 , . . .",4. Multi-task Multi-agent RL,[0],[0]
", o (i) t ), where ~ot
(i) ∈ ~Ot (i) .",4. Multi-task Multi-agent RL,[0],[0]
Single-agent policy π(i) : ~Ot (i) 7→,4. Multi-task Multi-agent RL,[0],[0]
"A(i) conducts action selection, and the joint policy is denoted π = 〈π(1), . . .",4. Multi-task Multi-agent RL,[0],[0]
", π(n)〉.",4. Multi-task Multi-agent RL,[0],[0]
"For simplicity, we consider only pure joint policies, as finite-horizon Dec-POMDPs have at least one pure joint optimal policy (Oliehoek et al., 2008).",4. Multi-task Multi-agent RL,[0],[0]
"The team receives a joint reward rt = R(st,at) ∈ R at each timestep t, the objective being to maximize the value (or expected return), V = E[ ∑H t=0 γ
trt].",4. Multi-task Multi-agent RL,[1.0000000457837819],"['The team receives a joint reward rt = R(st,at) ∈ R at each timestep t, the objective being to maximize the value (or expected return), V = E[ ∑H t=0 γ trt].']"
"While Dec-POMDP planning approaches assume agents do not observe intermediate rewards, we make the typical RL assumption that they do.",4. Multi-task Multi-agent RL,[0],[0]
"This assumption is consistent with prior work in MARL (Banerjee et al., 2012; Peshkin et al., 2000).
",4. Multi-task Multi-agent RL,[0],[0]
"ILs provide a scalable way to learn in Dec-POMDPs, as each agent’s policy maps local observations to actions.",4. Multi-task Multi-agent RL,[0],[0]
"However, the domain appears non-stationary from the perspective of each Dec-POMDP agent, a property we formalize by extending the definition by Laurent et al. (2011).",4. Multi-task Multi-agent RL,[0],[0]
Definition 1.,4. Multi-task Multi-agent RL,[0],[0]
Let a−(i) = a \ {a(i)}.,4. Multi-task Multi-agent RL,[0],[0]
Local decision process for agent i is stationary,4. Multi-task Multi-agent RL,[0],[0]
"if, for all timesteps t, u ∈ N,
∑ a −(i) t ∈A −(i) P (s′|s,〈a(i),a−(i)t 〉) = ∑ a−(i)u ∈A −(i) P (s′|s,〈a(i),a−(i)u 〉), (1) and∑",4. Multi-task Multi-agent RL,[0],[0]
"a −(i) t ∈A −(i) P (o(i)|s′,〈a(i),a−(i)t 〉) =",4. Multi-task Multi-agent RL,[0],[0]
"∑ a−(i)u ∈A −(i) P (o(i)|s′,〈a(i),a−(i)u 〉).
",4. Multi-task Multi-agent RL,[0],[0]
"(2)
Letting π−(i) = π \ {π(i)}, non-stationarity from the local perspective of agent i follows as in general a−(i)t = π−(i)(~ot) 6= π−(i)(~ou) = a−(i)u , which causes violation of (1) and (2).",4. Multi-task Multi-agent RL,[0],[0]
"Thus, MARL extensions of single-agent algorithms that assume stationary environments, such as Dec-DRQN, are inevitably ill-fated.",4. Multi-task Multi-agent RL,[0],[0]
"This motivates our decision to first design a single-task, decentralized MARL approach targeting non-stationarity in Dec-POMDP learning.
",4. Multi-task Multi-agent RL,[0],[0]
"The MT-MARL problem in partially observable settings is now introduced by extending the single-agent, fullyobservable definition of Fernández & Veloso (2006).",4. Multi-task Multi-agent RL,[0],[0]
Definition 2.,4. Multi-task Multi-agent RL,[0],[0]
"A partially-observable MT-MARL DomainD is a tuple 〈I,S,A,Ω, γ〉, where I is the set of agents, S is
1Superscripts indicate local parameters for agent i ∈",4. Multi-task Multi-agent RL,[0],[0]
"I.
the environment state space, A is the joint action space, Ω is the joint observation space, and γ is the discount factor.
",4. Multi-task Multi-agent RL,[0],[0]
Definition 3.,4. Multi-task Multi-agent RL,[0],[0]
"A partially-observable MT-MARL Task Tj is a tuple 〈D, Tj ,Rj ,Oj〉, where D is a shared underlying domain; Tj , Rj , Oj are, respectively, the task-specific transition, reward, and observation functions.
",4. Multi-task Multi-agent RL,[0],[0]
"In MT-MARL, each episode e ∈ {1, . . .",4. Multi-task Multi-agent RL,[0],[0]
", E} consists of a randomly sampled Task Tj from domain D.",4. Multi-task Multi-agent RL,[0],[0]
"The team observes the task ID, j, during learning, but not during execution.",4. Multi-task Multi-agent RL,[0],[0]
"The objective is to find a joint policy that maximizes average empirical execution-time return in all E episodes, V̄ = 1E ∑E e=0",4. Multi-task Multi-agent RL,[0],[0]
"∑He t=0 γ
tRe(st,at), where He is the time horizon of episode e.",4. Multi-task Multi-agent RL,[0],[0]
"This section introduces a two-phase approach for partiallyobservable MT-MARL; the approach first conducts singletask specialization, and subsequently unifies task-specific DRQNs into a joint policy that performs well in all tasks.",5. Approach,[1.0],"['This section introduces a two-phase approach for partiallyobservable MT-MARL; the approach first conducts singletask specialization, and subsequently unifies task-specific DRQNs into a joint policy that performs well in all tasks.']"
"As Dec-POMDP RL is notoriously complex (and solving for the optimal policy is NEXP-complete even with a known model (Bernstein et al., 2002)), we first introduce an approach for stable single-task MARL.",5.1. Phase I: Dec-POMDP Single-Task MARL,[0],[0]
"This enables agents to learn coordination, while also learning Q-values needed for computation of a unified MT-MARL policy.",5.1. Phase I: Dec-POMDP Single-Task MARL,[1.0],"['This enables agents to learn coordination, while also learning Q-values needed for computation of a unified MT-MARL policy.']"
"Due to partial observability and local non-stationarity, model-based Dec-POMDP MARL is extremely challenging (Banerjee et al., 2012).",5.1.1. DECENTRALIZED HYSTERETIC DEEP RECURRENT Q-NETWORKS (DEC-HDRQNS),[0],[0]
"Our approach is model-free and decentralized, learning Q-values for each agent.",5.1.1. DECENTRALIZED HYSTERETIC DEEP RECURRENT Q-NETWORKS (DEC-HDRQNS),[0],[0]
"In contrast to policy tables or FSCs, Q-values are amenable to the multi-task distillation process as they inherently measure quality of all actions, rather than just the optimal action.
",5.1.1. DECENTRALIZED HYSTERETIC DEEP RECURRENT Q-NETWORKS (DEC-HDRQNS),[1.0000000736005032],"['In contrast to policy tables or FSCs, Q-values are amenable to the multi-task distillation process as they inherently measure quality of all actions, rather than just the optimal action.']"
"Overly-optimistic MARL approaches (e.g., Distributed Qlearning (Lauer & Riedmiller, 2000)) completely ignore low returns, which are assumed to be caused by teammates’ exploratory actions.",5.1.1. DECENTRALIZED HYSTERETIC DEEP RECURRENT Q-NETWORKS (DEC-HDRQNS),[0],[0]
This causes severe overestimation of Q-values in stochastic domains.,5.1.1. DECENTRALIZED HYSTERETIC DEEP RECURRENT Q-NETWORKS (DEC-HDRQNS),[0],[0]
"Hysteretic Qlearning (Matignon et al., 2007), instead, uses the insight that low returns may also be caused by domain stochasticity, which should not be ignored.",5.1.1. DECENTRALIZED HYSTERETIC DEEP RECURRENT Q-NETWORKS (DEC-HDRQNS),[0],[0]
"This approach uses two learning rates: nominal learning rate, α, is used when the TD-error is non-negative; a smaller learning rate, β, is used otherwise (where 0 <",5.1.1. DECENTRALIZED HYSTERETIC DEEP RECURRENT Q-NETWORKS (DEC-HDRQNS),[0],[0]
β < α < 1).,5.1.1. DECENTRALIZED HYSTERETIC DEEP RECURRENT Q-NETWORKS (DEC-HDRQNS),[0],[0]
"The result is hysteresis (lag) of Q-value degradation for actions associated with positive past experiences that occurred due to successful
cooperation.",5.1.1. DECENTRALIZED HYSTERETIC DEEP RECURRENT Q-NETWORKS (DEC-HDRQNS),[0],[0]
"Agents are, therefore, robust against negative learning due to teammate exploration and concurrent actions.",5.1.1. DECENTRALIZED HYSTERETIC DEEP RECURRENT Q-NETWORKS (DEC-HDRQNS),[1.0],"['Agents are, therefore, robust against negative learning due to teammate exploration and concurrent actions.']"
"Notably, unlike Distributed Q-learning, Hysteretic Q-learning permits eventual degradation of Q-values that were overestimated due to outcomes unrelated to their associated action.
",5.1.1. DECENTRALIZED HYSTERETIC DEEP RECURRENT Q-NETWORKS (DEC-HDRQNS),[0],[0]
"Hysteretic Q-learning has enjoyed a strong empirical track record in fully-observable MARL (Xu et al., 2012; Matignon et al., 2012; Barbalios & Tzionas, 2014), exhibiting similar performance as more complex approaches.",5.1.1. DECENTRALIZED HYSTERETIC DEEP RECURRENT Q-NETWORKS (DEC-HDRQNS),[0],[0]
"Encouraged by these results, we introduce Decentralized Hysteretic Deep Recurrent Q-Networks (Dec-HDRQNs) for partially-observable domains.",5.1.1. DECENTRALIZED HYSTERETIC DEEP RECURRENT Q-NETWORKS (DEC-HDRQNS),[0],[0]
"This approach exploits the robustness of hysteresis to non-stationarity and alterexploration, in addition to the representational power and memory-based decision making of DRQNs.",5.1.1. DECENTRALIZED HYSTERETIC DEEP RECURRENT Q-NETWORKS (DEC-HDRQNS),[0],[0]
"As later demonstrated, Dec-HDRQN is well-suited to Dec-POMDP MARL, as opposed to non-hysteretic Dec-DRQN.",5.1.1. DECENTRALIZED HYSTERETIC DEEP RECURRENT Q-NETWORKS (DEC-HDRQNS),[0],[0]
"Experience replay (sampling a memory bank of experience tuples 〈s, a, r, s′〉 for TD learning) was first introduced by Lin (1992) and recently shown to be crucial for stable deep Q-learning (Mnih et al., 2015).",5.1.2. CONCURRENT EXPERIENCE REPLAY TRAJECTORIES (CERTS),[0],[0]
"With experience replay, sampling cost is reduced as multiple TD updates can be conducted using each sample, enabling rapid Q-value propagation to preceding states without additional environmental interactions.",5.1.2. CONCURRENT EXPERIENCE REPLAY TRAJECTORIES (CERTS),[0],[0]
"Experience replay also breaks temporal correlations of samples used for Q-value updates—crucial for reducing generalization error, as the stochastic optimization algorithms used for training DQNs typically assume i.i.d.",5.1.2. CONCURRENT EXPERIENCE REPLAY TRAJECTORIES (CERTS),[1.0],"['Experience replay also breaks temporal correlations of samples used for Q-value updates—crucial for reducing generalization error, as the stochastic optimization algorithms used for training DQNs typically assume i.i.d.']"
"data (Bengio, 2012).
",5.1.2. CONCURRENT EXPERIENCE REPLAY TRAJECTORIES (CERTS),[0],[0]
"Despite the benefits in single-agent settings, existing MARL approaches have found it necessary to disable experience replay (Foerster et al., 2016).",5.1.2. CONCURRENT EXPERIENCE REPLAY TRAJECTORIES (CERTS),[0],[0]
"This is due to the non-concurrent (and non-stationary) nature of local experiences when sampled independently for each agent, despite the agents learning concurrently.",5.1.2. CONCURRENT EXPERIENCE REPLAY TRAJECTORIES (CERTS),[0],[0]
"A contributing factor is that inter-agent desynchronization of experiences compounds the prevalence of earlier-mentioned shadowed equilibria challenges, destabilizing coordination.",5.1.2. CONCURRENT EXPERIENCE REPLAY TRAJECTORIES (CERTS),[0],[0]
"As a motivating example, consider a 2 agent game where A(1) = A(2)",5.1.2. CONCURRENT EXPERIENCE REPLAY TRAJECTORIES (CERTS),[0],[0]
"= {a1, a2}.",5.1.2. CONCURRENT EXPERIENCE REPLAY TRAJECTORIES (CERTS),[0],[0]
"Let there be two optimal joint actions: 〈a1, a1〉 and 〈a2, a2〉 (e.g., only these joint actions have positive, equal reward).",5.1.2. CONCURRENT EXPERIENCE REPLAY TRAJECTORIES (CERTS),[0],[0]
"Given independent experience samples for each agent, the first agent may learn action a1 as optimal, whereas the second agent learns a2, resulting in arbitrarily poor joint action 〈a1, a2〉.",5.1.2. CONCURRENT EXPERIENCE REPLAY TRAJECTORIES (CERTS),[1.0],"['Given independent experience samples for each agent, the first agent may learn action a1 as optimal, whereas the second agent learns a2, resulting in arbitrarily poor joint action 〈a1, a2〉.']"
This motivates a need for concurrent (synchronized) sampling of experiences across the team in MARL settings.,5.1.2. CONCURRENT EXPERIENCE REPLAY TRAJECTORIES (CERTS),[0],[0]
"Concurrent experiences induce correlations in local policy updates, so
that given existence of multiple equilibria, agents tend to converge to the same one.",5.1.2. CONCURRENT EXPERIENCE REPLAY TRAJECTORIES (CERTS),[1.0000000506361602],"['Concurrent experiences induce correlations in local policy updates, so that given existence of multiple equilibria, agents tend to converge to the same one.']"
"Thus, we introduce Concurrent Experience Replay Trajectories (CERTs), visualized in Fig. 1a.",5.1.2. CONCURRENT EXPERIENCE REPLAY TRAJECTORIES (CERTS),[0],[0]
"During execution of each learning episode e ∈ N+, each agent i collects experience tuple 〈o(i)t , a (i) t , rt, o (i) t+1〉 at timestep t, where ot, at, and rt are current observation, action, and reward, and ot+1 is the subsequent observation.",5.1.2. CONCURRENT EXPERIENCE REPLAY TRAJECTORIES (CERTS),[0],[0]
Fig.,5.1.2. CONCURRENT EXPERIENCE REPLAY TRAJECTORIES (CERTS),[0],[0]
1a visualizes each experience tuple as a cube.,5.1.2. CONCURRENT EXPERIENCE REPLAY TRAJECTORIES (CERTS),[0],[0]
Experiences in each episode are stored in a sequence (along time axis t of Fig.,5.1.2. CONCURRENT EXPERIENCE REPLAY TRAJECTORIES (CERTS),[0],[0]
"1a), as Dec-HDRQN assumes an underlying RNN architecture that necessitates sequential samples for each training iteration.",5.1.2. CONCURRENT EXPERIENCE REPLAY TRAJECTORIES (CERTS),[0],[0]
"Importantly, as all agents are aware of timestep t and episode e, they store their experiences concurrently (along agent index axis i of Fig. 1a).",5.1.2. CONCURRENT EXPERIENCE REPLAY TRAJECTORIES (CERTS),[0],[0]
"Upon episode termination, a new sequence is initiated (a new row along episode axis e of Fig. 1a).",5.1.2. CONCURRENT EXPERIENCE REPLAY TRAJECTORIES (CERTS),[0],[0]
"No restrictions are imposed on terminal conditions (i.e., varying trajectory lengths are permitted along axis t of Fig. 1a).",5.1.2. CONCURRENT EXPERIENCE REPLAY TRAJECTORIES (CERTS),[0],[0]
"CERTs are a first-in first-out circular queue along the episode axis e, such that old episodes are eventually discarded.",5.1.2. CONCURRENT EXPERIENCE REPLAY TRAJECTORIES (CERTS),[0],[0]
"Each agent i maintains DRQN Q(i)(o(i)t , h (i) t−1, a (i); θ(i)), where o(i)t is the latest local observation, h (i) t−1 is the RNN hidden state, a(i) is the action, and θ(i) are the local DRQN parameters.",5.1.3. TRAINING DEC-HDRQNS USING CERTS,[0],[0]
DRQNs are trained on experience sequences (traces) with tracelength τ .,5.1.3. TRAINING DEC-HDRQNS USING CERTS,[0],[0]
"Figure 1b visualizes the minibatch sampling procedure for training, with τ = 4.",5.1.3. TRAINING DEC-HDRQNS USING CERTS,[0],[0]
"In each training iteration, agents first sample a concurrent minibatch of episodes.",5.1.3. TRAINING DEC-HDRQNS USING CERTS,[0],[0]
"All agents’ sampled traces have the same starting timesteps (i.e., are coincident along agent axis i in Fig. 1b).",5.1.3. TRAINING DEC-HDRQNS USING CERTS,[0],[0]
Guaranteed concurrent sampling merely requires a one-time (offline) consensus of agents’ random number generator seeds prior to initiating learning.,5.1.3. TRAINING DEC-HDRQNS USING CERTS,[1.0],['Guaranteed concurrent sampling merely requires a one-time (offline) consensus of agents’ random number generator seeds prior to initiating learning.']
"This ensures our approach is fully decentralized and assumes no explicit communication, even during learning.",5.1.3. TRAINING DEC-HDRQNS USING CERTS,[0],[0]
Fig.,5.1.3. TRAINING DEC-HDRQNS USING CERTS,[0],[0]
"1b shows a minibatch of 3 episodes, e, sampled in red.",5.1.3. TRAINING DEC-HDRQNS USING CERTS,[0],[0]
"To train DRQNs, Hausknecht & Stone (2015) suggest randomly
sampling a timestep within each episode, and training using τ backward steps.",5.1.3. TRAINING DEC-HDRQNS USING CERTS,[0],[0]
"However, this imposes a bias where experiences in each episode’s final τ timesteps are used in fewer recurrent updates.",5.1.3. TRAINING DEC-HDRQNS USING CERTS,[0],[0]
"Instead, we propose that for each sampled episode e, agents sample a concurrent start timestep t0 for the trace from interval {−τ + 1, . . .",5.1.3. TRAINING DEC-HDRQNS USING CERTS,[0],[0]
",He}, where He is the timestep of the episode’s final experience.",5.1.3. TRAINING DEC-HDRQNS USING CERTS,[1.0],"[',He}, where He is the timestep of the episode’s final experience.']"
"For example, the three sampled (red) traces in Fig.",5.1.3. TRAINING DEC-HDRQNS USING CERTS,[0],[0]
"1b start at timesteps +1, −1, and +2, respectively.",5.1.3. TRAINING DEC-HDRQNS USING CERTS,[0],[0]
"This ensures all experiences have equal probability of being used in updates, which we found especially critical for fast training on tasks with only terminal rewards.
",5.1.3. TRAINING DEC-HDRQNS USING CERTS,[1.000000036227172],"['This ensures all experiences have equal probability of being used in updates, which we found especially critical for fast training on tasks with only terminal rewards.']"
Sampled traces sometimes contain elements outside the episode interval (indicated as ∅ in Fig. 1b).,5.1.3. TRAINING DEC-HDRQNS USING CERTS,[0],[0]
"We discard ∅ experiences and zero-pad the suffix of associated traces (to ensure all traces have equal length τ , enabling seamless use of fixed-length minibatch optimizers in standard deep learning libraries).",5.1.3. TRAINING DEC-HDRQNS USING CERTS,[0],[0]
Suffix (rather than prefix) padding ensures RNN internal states of non-∅ samples are unaffected.,5.1.3. TRAINING DEC-HDRQNS USING CERTS,[0],[0]
"In training iteration j, agent i uses the sampling procedure to collect a minibatch of traces from CERT memoryM(i),
B = {〈〈obt0 , a b t0 , r b t0 , o b t0+1〉, . . .",5.1.3. TRAINING DEC-HDRQNS USING CERTS,[0],[0]
", (3)
〈obt0+τ−1, a b t0+τ−1, r b t0+τ−1, o b",5.1.3. TRAINING DEC-HDRQNS USING CERTS,[0],[0]
"t0+τ 〉〉}b={1,...,B},
where t0 is the start timestep for each trace, b is trace index, andB is number of traces (minibatch size).2 Each trace b is used to calculate a corresponding sequence of target values,
{〈〈ybt0〉, . . .",5.1.3. TRAINING DEC-HDRQNS USING CERTS,[0],[0]
", 〈y b t0+τ−1〉〉}b={1,...,B}, (4)
where ybt = r b t + γmaxa′ Q(o b t+1, h b t , a ′; θ̂ (i) j ).",5.1.3. TRAINING DEC-HDRQNS USING CERTS,[0],[0]
"Target network parameters θ̂(i)j are updated less frequently, for stable learning (Mnih et al., 2015).",5.1.3. TRAINING DEC-HDRQNS USING CERTS,[0],[0]
"Loss over all traces is,
Lj(θ (i) j ) =",5.1.3. TRAINING DEC-HDRQNS USING CERTS,[0],[0]
"E(obt ,abt ,rbt ,obt+1)∼M(i)",5.1.3. TRAINING DEC-HDRQNS USING CERTS,[0],[0]
"[(δ b t ) 2], (5)
where δbt = y b t − Q(obt , hbt−1, abt ; θ (i) j ).",5.1.3. TRAINING DEC-HDRQNS USING CERTS,[0],[0]
Loss contributions of suffix ∅-padding elements are masked out.,5.1.3. TRAINING DEC-HDRQNS USING CERTS,[0],[0]
Parameters are updated via gradient descent on Eq.,5.1.3. TRAINING DEC-HDRQNS USING CERTS,[0],[0]
"(5), with the caveat of hysteretic learning rates 0",5.1.3. TRAINING DEC-HDRQNS USING CERTS,[0],[0]
< β,5.1.3. TRAINING DEC-HDRQNS USING CERTS,[0],[0]
<,5.1.3. TRAINING DEC-HDRQNS USING CERTS,[0],[0]
α,5.1.3. TRAINING DEC-HDRQNS USING CERTS,[0],[0]
"< 1, where learning rate α is used if δbt ≥ 0, and β is used otherwise.",5.1.3. TRAINING DEC-HDRQNS USING CERTS,[0],[0]
"Following task specialization, the second phase involves distillation of each agent’s set of DRQNs into a unified DRQN that performs well in all tasks without explicit provision of task ID.",5.2. Phase II: Dec-POMDP MT-MARL,[1.0],"['Following task specialization, the second phase involves distillation of each agent’s set of DRQNs into a unified DRQN that performs well in all tasks without explicit provision of task ID.']"
"Using DRQNs, our approach extends the single-agent, fully-observable MTRL method proposed by Rusu et al. (2015) to Dec-POMDP MT-MARL.",5.2. Phase II: Dec-POMDP MT-MARL,[0],[0]
"Specifically, once Dec-HDRQN specialization is conducted for
2For notational simplicity, agent superscripts (i) are excluded from local experiences 〈o(i), a(i), r, o′(i)〉 in Eqs.",5.2. Phase II: Dec-POMDP MT-MARL,[0],[0]
"(3) to (6).
",5.2. Phase II: Dec-POMDP MT-MARL,[0],[0]
"each task, multi-task learning can be treated as a regression problem over Q-values.",5.2. Phase II: Dec-POMDP MT-MARL,[0],[0]
"During multi-task learning, our approach iteratively conducts data collection and regression.
",5.2. Phase II: Dec-POMDP MT-MARL,[0],[0]
"For data collection, agents use each specialized DRQN (from Phase I) to execute actions in corresponding tasks, resulting in a set of regression CERTs {MR} (one per task), each containing sequences of regression experiences 〈o(i)t , Q (i) t 〉, where Q (i) t = Q (i) t (~ot
(i); θ(i)) is the specialized DRQN’s Q-value vector for agent i at timestep t. Supervised learning of Q-values is then conducted.",5.2. Phase II: Dec-POMDP MT-MARL,[0.999999959168775],"['For data collection, agents use each specialized DRQN (from Phase I) to execute actions in corresponding tasks, resulting in a set of regression CERTs {MR} (one per task), each containing sequences of regression experiences 〈o(i)t , Q (i) t 〉, where Q (i) t = Q (i) t (~ot (i); θ(i)) is the specialized DRQN’s Q-value vector for agent i at timestep t. Supervised learning of Q-values is then conducted.']"
Each agent samples experiences from its local regression CERTs to train a single distilled DRQN with parameters θ (i) R .,5.2. Phase II: Dec-POMDP MT-MARL,[0],[0]
"Given a minibatch of regression experience traces BR = {〈〈obt0 , Q b t0〉, . . .",5.2. Phase II: Dec-POMDP MT-MARL,[0],[0]
", 〈o b t0+τ−1, Q b t0+τ−1〉〉}b={1,...,B}, the following tempered Kullback-Leibler (KL) divergence loss is minimized for each agent,
LKL(BR, θ(i)R ;T ) (6) =",5.2. Phase II: Dec-POMDP MT-MARL,[0],[0]
"E(obt ,Qbt)∼{MR(i)} |A(i)|∑ a=1 softmaxa( Qbt T )",5.2. Phase II: Dec-POMDP MT-MARL,[0],[0]
ln softmaxa( Qbt T ),5.2. Phase II: Dec-POMDP MT-MARL,[0],[0]
"softmaxa(Qbt,R) ,
where Qbt,R = Q b t,R(~ot b; θ (i) R ) is the vector of actionvalues predicted by distilled DRQN given the same input as the specialized DRQN, T is the softmax temperature, and softmaxa refers to the a-th element of the softmax output.",5.2. Phase II: Dec-POMDP MT-MARL,[0],[0]
The motivation behind loss function (6) is that low temperatures (0 <,5.2. Phase II: Dec-POMDP MT-MARL,[0],[0]
T,5.2. Phase II: Dec-POMDP MT-MARL,[0],[0]
"< 1) lead to sharpening of specialized DRQN action-values, Qbt , ensuring that the distilled DRQN ultimately chooses similar actions as the specialized policy it was trained on.",5.2. Phase II: Dec-POMDP MT-MARL,[0],[0]
We refer readers to Rusu et al. (2015) for additional analysis of the distillation loss.,5.2. Phase II: Dec-POMDP MT-MARL,[1.0],['We refer readers to Rusu et al. (2015) for additional analysis of the distillation loss.']
"Note that concurrent sampling is not necessary during the distillation phase, as it is entirely supervised; CERTs are merely used for storage of the regression experiences.",5.2. Phase II: Dec-POMDP MT-MARL,[0],[0]
We first evaluate single-task performance of the introduced Dec-HDRQN approach on a series of increasingly challenging domains.,6.1. Task Specialization using Dec-HDRQN,[0],[0]
"Domains are designed to support a large number of task variations, serving as a useful MT-MARL benchmarking tool.",6.1. Task Specialization using Dec-HDRQN,[0],[0]
"All experiments use DRQNs with 2 multi-layer perceptron (MLP) layers, an LSTM layer (Hochreiter & Schmidhuber, 1997) with 64 memory cells, and another 2 MLP layers.",6.1. Task Specialization using Dec-HDRQN,[0],[0]
"MLPs have 32 hidden units each and rectified linear unit nonlinearities are used throughout, with the exception of the final (linear) layer.",6.1. Task Specialization using Dec-HDRQN,[0],[0]
"Experiments use γ = 0.95 and Adam optimizer (Kingma & Ba, 2014) with base learning rate 0.001.",6.1. Task Specialization using Dec-HDRQN,[0],[0]
Dec-HDRQNs use hysteretic learning rate β = 0.2 to 0.4.,6.1. Task Specialization using Dec-HDRQN,[0],[0]
"All results are reported for batches of 50 randomly-initialized episodes.
",6.1. Task Specialization using Dec-HDRQN,[0],[0]
"Performance is evaluated on both multi-agent single-target (MAST) and multi-agent multi-target (MAMT) capture domains, variations of the existing meeting-in-a-grid DecPOMDP benchmark (Amato et al., 2009).",6.1. Task Specialization using Dec-HDRQN,[0],[0]
Agents i ∈,6.1. Task Specialization using Dec-HDRQN,[0],[0]
"{1, . . .",6.1. Task Specialization using Dec-HDRQN,[0],[0]
", n} in an m × m toroidal grid receive +1 terminal reward only when they simultaneously capture moving targets (1 target in MAST, and n targets in MAMT).",6.1. Task Specialization using Dec-HDRQN,[0],[0]
"Each agent always observes its own location, but only sometimes observes targets’ locations.",6.1. Task Specialization using Dec-HDRQN,[0],[0]
Target dynamics are unknown to agents and vary across tasks.,6.1. Task Specialization using Dec-HDRQN,[1.0],['Target dynamics are unknown to agents and vary across tasks.']
"Similar to the Pong POMDP domain of Hausknecht & Stone (2015), our domains include observation flickering: in each timestep, observations of targets are sometimes obscured, with probability Pf .",6.1. Task Specialization using Dec-HDRQN,[0],[0]
"In MAMT, each agent is assigned a unique target to capture, yet is unaware of the assignment (which also varies across tasks).",6.1. Task Specialization using Dec-HDRQN,[1.0],"['In MAMT, each agent is assigned a unique target to capture, yet is unaware of the assignment (which also varies across tasks).']"
Agent/target locations are randomly initialized in each episode.,6.1. Task Specialization using Dec-HDRQN,[0],[0]
"Actions are ‘move north’,
‘south’, ‘east’, ‘west’, and ‘wait’, but transitions are noisy (0.1 probability of moving to an unintended adjacent cell).
",6.1. Task Specialization using Dec-HDRQN,[0],[0]
"In the MAST domain, each task is specified by a unique grid size m; in MAMT, each task also has a unique agenttarget assignment.",6.1. Task Specialization using Dec-HDRQN,[0],[0]
The challenge is that agents must learn particular roles (to ensure coordination) and also discern aliased states (to ensure quick capture of targets) using local noisy observations.,6.1. Task Specialization using Dec-HDRQN,[0],[0]
"Tasks end after H timesteps, or upon simultaneous target capture.",6.1. Task Specialization using Dec-HDRQN,[0],[0]
Cardinality of local policy space for agent i at timestep t is O(|A(i)| |Ω(i)|t−1 |Ω(i)|−1 ),6.1. Task Specialization using Dec-HDRQN,[0],[0]
"(Oliehoek et al., 2008), where |A(i)| = 5, |Ω(i)| = m4 for MAST, and |Ω(i)| = m2(n+1) for MAMT.",6.1. Task Specialization using Dec-HDRQN,[0],[0]
"Across all tasks, non-zero reward signals are extremely sparse, appearing in the terminal experience tuple only if targets are simultaneously captured.",6.1. Task Specialization using Dec-HDRQN,[0],[0]
"Readers are referred to the supplementary material for domain visualizations.
",6.1. Task Specialization using Dec-HDRQN,[0],[0]
"The failure point of Dec-DRQN is first compared to DecHDRQN in the MAST domain with n = 2 and Pf = 0 (full observability) for increasing task size, starting from 4× 4.",6.1. Task Specialization using Dec-HDRQN,[1.0],"['The failure point of Dec-DRQN is first compared to DecHDRQN in the MAST domain with n = 2 and Pf = 0 (full observability) for increasing task size, starting from 4× 4.']"
"Despite the domain simplicity, Dec-DRQN fails to match Dec-HDRQN at the 8×8 mark, receiving value 0.05±0.16 in contrast to Dec-HDRQN’s 0.76±0.11 (full results reported in supplementary material).",6.1. Task Specialization using Dec-HDRQN,[1.0],"['Despite the domain simplicity, Dec-DRQN fails to match Dec-HDRQN at the 8×8 mark, receiving value 0.05±0.16 in contrast to Dec-HDRQN’s 0.76±0.11 (full results reported in supplementary material).']"
"Experiments are then scaled up to a 2 agent, 2 target MAMT domain with Pf = 0.3.",6.1. Task Specialization using Dec-HDRQN,[0],[0]
Empirical returns throughout training are shown in Fig. 2.,6.1. Task Specialization using Dec-HDRQN,[0],[0]
"In the MAMT tasks, a well-coordinated policy induces agents to capture targets simultaneously (yielding joint +1 reward).",6.1. Task Specialization using Dec-HDRQN,[0],[0]
"If any agent strays from this strategy during learning (e.g., while exploring), teammates receive no reward even while executing optimal local policies, leading them to deviate from learned strategies.",6.1. Task Specialization using Dec-HDRQN,[0],[0]
"Due to lack of robustness against alter-exploration/non-stationarity, the Dec-DRQN becomes unstable in the 5 × 5 task, and fails to learn altogether in the 6 × 6 and 7 × 7 tasks (Fig. 2a).",6.1. Task Specialization using Dec-HDRQN,[0],[0]
Hysteresis affords Dec-HDRQN policies the stability necessary to consistently achieve agent coordination (Fig. 2b).,6.1. Task Specialization using Dec-HDRQN,[0],[0]
"A centralized-learning variation of Dec-DRQN with interagent parameter sharing (similar to RIAL-PS in Foerster et al. (2016)) was also tested, but was not found to improve performance (see supplementary material).",6.1. Task Specialization using Dec-HDRQN,[0],[0]
"These re-
sults further validate that, despite its simplicity, hysteretic learning significantly improves the stability of MARL in cooperative settings.",6.1. Task Specialization using Dec-HDRQN,[0],[0]
Experiments are also conducted for the n = 3 MAMT domain (Fig. 3).,6.1. Task Specialization using Dec-HDRQN,[0],[0]
This domain poses significant challenges due to reward sparsity.,6.1. Task Specialization using Dec-HDRQN,[0],[0]
"Even in the 4 × 4 task, only 0.02% of the joint state space has a nonzero reward signal.",6.1. Task Specialization using Dec-HDRQN,[0],[0]
"Dec-DRQN fails to find a coordinated joint policy, receiving near-zero return after training.",6.1. Task Specialization using Dec-HDRQN,[0],[0]
DecHDRQN successfully coordinates the 3 agents.,6.1. Task Specialization using Dec-HDRQN,[0],[0]
"Note the high variance in empirical return for the 3 × 3 task is due to flickering probability being increased to Pf = 0.6.
",6.1. Task Specialization using Dec-HDRQN,[0],[0]
"Sensitivity of Dec-HDRQN empirical performance to hysteretic learning rate β is shown in Fig. 4, where lower β corresponds to higher optimism; β = 0 causes monotonic increase of approximated Q-values during learning, whereas β = 1 corresponds to Dec-DRQN.",6.1. Task Specialization using Dec-HDRQN,[0],[0]
"Due to the optimistic assumption, anticipated returns at the initial timestep, Q(o0, a0), overestimate true empirical return.",6.1. Task Specialization using Dec-HDRQN,[0],[0]
"Despite this, β ∈",6.1. Task Specialization using Dec-HDRQN,[0],[0]
"[0.1, 0.6] consistently enables learning of a well-coordinated policy, with β ∈",6.1. Task Specialization using Dec-HDRQN,[0],[0]
"[0.4, 0.5] achieving best performance.",6.1. Task Specialization using Dec-HDRQN,[0],[0]
Readers are referred to the supplementary material for additional sensitivity analysis of convergence trends with varying β and CERT tracelength τ .,6.1. Task Specialization using Dec-HDRQN,[0],[0]
We now evaluate distillation of specialized Dec-HDRQN policies (as learned in Section 6.1) for MT-MARL.,6.2. Multi-tasking using Distilled Dec-HDRQN,[1.0],['We now evaluate distillation of specialized Dec-HDRQN policies (as learned in Section 6.1) for MT-MARL.']
A first approach is to forgo specialization and directly learn a Dec-HDRQN using a pool of experiences from all tasks.,6.2. Multi-tasking using Distilled Dec-HDRQN,[0],[0]
"This approach, called Multi-DQN by Rusu et al. (2015), is susceptible to convergence issues even in single-agent, fully-observable settings.",6.2. Multi-tasking using Distilled Dec-HDRQN,[0],[0]
"In Fig. 5, we compare these approaches (where we label ours as ‘Distilled’, and MultiHDRQN as ‘Multi’).",6.2. Multi-tasking using Distilled Dec-HDRQN,[0],[0]
"Both approaches were trained to perform multi-tasking on 2-agent MAMT tasks ranging from 3×3 to 6×6, with Pf = 0.3.",6.2. Multi-tasking using Distilled Dec-HDRQN,[0],[0]
"Our distillation approach uses no task-specific MLP layers, unlike Rusu et al. (2015), due to our stronger assumptions on task relatedness and lack of
execution-time observability of task identity.
",6.2. Multi-tasking using Distilled Dec-HDRQN,[0],[0]
"In Fig. 5, our MT-MARL approach first performs DecHDRQN specialization training on each task for 70K epochs, and then performs distillation for 100K epochs.",6.2. Multi-tasking using Distilled Dec-HDRQN,[0],[0]
A grid search was conducted for temperature hyperparameter in Eq.,6.2. Multi-tasking using Distilled Dec-HDRQN,[0],[0]
(6) (T = 0.01 was found suitable).,6.2. Multi-tasking using Distilled Dec-HDRQN,[0],[0]
"Note that performance is plotted only for the 4× 4 and 6× 6 tasks, simply for plot clarity (see supplementary material for MT-MARL evaluation results on all tasks).",6.2. Multi-tasking using Distilled Dec-HDRQN,[1.0],"['Note that performance is plotted only for the 4× 4 and 6× 6 tasks, simply for plot clarity (see supplementary material for MT-MARL evaluation results on all tasks).']"
"Multi-HDRQN exhibits poor performance across all tasks due to the complexity involved in concurrently learning over multiple DecPOMDPs (with partial observability, transition noise, nonstationarity, varying domain sizes, varying target dynamics, and random initializations).",6.2. Multi-tasking using Distilled Dec-HDRQN,[0],[0]
"We experimented with larger and smaller network sizes for Multi-HDRQN, with no major difference in performance (we also include training results for 500K Multi-HDRQN iterations in the supplementary).",6.2. Multi-tasking using Distilled Dec-HDRQN,[0],[0]
"By contrast, our proposed MT-MARL approach achieves near-nominal execution-time performance on all tasks using a single distilled policy for each agent – despite not explicitly being provided the task identity.",6.2. Multi-tasking using Distilled Dec-HDRQN,[0],[0]
This paper introduced the first formulation and approach for multi-task multi-agent reinforcement learning under partial observability.,7. Contribution,[0],[0]
"Our approach combines hysteretic learners, DRQNs, CERTs, and distillation, demonstrably achieving multi-agent coordination using a single joint policy in a set of Dec-POMDP tasks with sparse rewards, despite not being provided task identities during execution.",7. Contribution,[0],[0]
"The parametric nature of the capture tasks used for evaluation (e.g., variations in grid size, target assignments and dynamics, sensor failure probabilities) makes them good candidates for ongoing benchmarks of multi-agent multitask learning.",7. Contribution,[0],[0]
"Future work will investigate incorporation of skills (macro-actions) into the framework, extension to domains with heterogeneous agents, and evaluation on more complex domains with much larger numbers of tasks.",7. Contribution,[1.0],"['Future work will investigate incorporation of skills (macro-actions) into the framework, extension to domains with heterogeneous agents, and evaluation on more complex domains with much larger numbers of tasks.']"
The authors thank the anonymous reviewers for their insightful feedback and suggestions.,Acknowledgements,[0],[0]
"This work was supported by Boeing Research & Technology, ONR MURI Grant N000141110688 and BRC Grant N000141712072.",Acknowledgements,[0],[0]
"Multi-agent Single-target (MAST) domain results for Dec-DRQN and Dec-HDRQN, with 2 agents and Pf = 0.0 (observation flickering disabled).",B. Empirical Results: Learning on Multi-agent Single-Target (MAST) Domain,[0],[0]
These results mainly illustrate that Dec-DRQN sometimes has some empirical success in low-noise domains with small state-space.,B. Empirical Results: Learning on Multi-agent Single-Target (MAST) Domain,[0],[0]
"Note that in the 8×8 task, Dec-HDRQN significantly outperforms Dec-DRQN, which converges to a sub-optimal policy despite domain simplicity.",B. Empirical Results: Learning on Multi-agent Single-Target (MAST) Domain,[0],[0]
"Multi-agent Single-target (MAMT) domain results, with 2 agents and Pf = 0.3 (observation flickering disabled).",C. Empirical Results: Learning on MAMT Domain,[0],[0]
We also evaluated performance of inter-agent parameter sharing (a centralized approach) in Dec-DRQN (which we called Dec-DRQN-PS).,C. Empirical Results: Learning on MAMT Domain,[0],[0]
"Additionally, performance of a Double-DQN was deemed to have negligible impacts (Dec-DDRQN).
",C. Empirical Results: Learning on MAMT Domain,[0],[0]
"0.0 10K 20K 30K 40K
Training Epoch
0.0
0.2
0.4
0.6
0.8
1.0
E m
p ir
ic a l
R e tu
rn
3× 3 (Dec-DRQN)",C. Empirical Results: Learning on MAMT Domain,[0],[0]
4× 4 (Dec-DRQN) 3× 3 (Dec-HDRQN),C. Empirical Results: Learning on MAMT Domain,[0],[0]
"4× 4 (Dec-HDRQN)
(a) Empirical returns during training.",C. Empirical Results: Learning on MAMT Domain,[0],[0]
"For batch of 50 randomlyinitialized games.
0.0 10K 20K 30K 40K
Training Epoch
0.0
0.2
0.4
0.6
0.8
1.0
Q (s
0 , a
0 ) 3× 3 (Dec-DRQN)
4× 4 (Dec-DRQN) 3× 3 (Dec-HDRQN)",C. Empirical Results: Learning on MAMT Domain,[0],[0]
"4× 4 (Dec-HDRQN)
(b) Anticipated values during training.",C. Empirical Results: Learning on MAMT Domain,[0],[0]
"For specific starting states and actions undertaken in the same 50 randomly-initialized games as Fig. 10a.
Figure 10.",C. Empirical Results: Learning on MAMT Domain,[0],[0]
"MAMT domain results for Dec-DRQN and Dec-HDRQN, with n = 3 agents.",C. Empirical Results: Learning on MAMT Domain,[0],[0]
"Pf = 0.6 for the 3× 3 task, and Pf = 0.1 for the 4× 4 task.",C. Empirical Results: Learning on MAMT Domain,[0],[0]
The below plots show multi-tasking performance of both the distillation and Multi-HDRQN approaches.,F. Empirical Results: Multi-tasking Performance Comparison,[0],[0]
Both approaches were trained on the 3 × 3 through 6 × 6 MAMT tasks.,F. Empirical Results: Multi-tasking Performance Comparison,[0],[0]
"Multi-DRQN failed to achieve specialized-level performance on all tasks, despite 500K training epochs.",F. Empirical Results: Multi-tasking Performance Comparison,[0],[0]
"By contrast, the proposed MT-MARL distillation approach achieves nominal performance after 100K epochs.",F. Empirical Results: Multi-tasking Performance Comparison,[0],[0]
Many real-world tasks involve multiple agents with partial observability and limited communication.,abstractText,[0],[0]
"Learning is challenging in these settings due to local viewpoints of agents, which perceive the world as non-stationary due to concurrentlyexploring teammates.",abstractText,[0],[0]
"Approaches that learn specialized policies for individual tasks face problems when applied to the real world: not only do agents have to learn and store distinct policies for each task, but in practice identities of tasks are often non-observable, making these approaches inapplicable.",abstractText,[0],[0]
This paper formalizes and addresses the problem of multi-task multi-agent reinforcement learning under partial observability.,abstractText,[0],[0]
"We introduce a decentralized single-task learning approach that is robust to concurrent interactions of teammates, and present an approach for distilling single-task policies into a unified policy that performs well across multiple related tasks, without explicit provision of task identity.",abstractText,[0],[0]
Deep Decentralized Multi-task Multi-Agent Reinforcement Learningunder Partial Observability,title,[0],[0]
