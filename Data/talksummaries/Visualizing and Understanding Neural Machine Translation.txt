10	26	Although visualizing and interpreting neural models for natural language processing has started to attract attention recently (Karpathy et al., 2016; Li et al., 2016), to the best of our knowledge, there is no existing work on visualizing NMT models.
11	18	Note that the attention mechanism (Bahdanau et al., 2015) is restricted to demonstrate the connection between words in source and target languages and unable to offer more insights in interpreting how target words are generated (see Section 4.5).
23	13	As shown in Figure 1, given a source sentence x, the encoder first uses source word embeddings to map each source word xi to a real-valued vector xi.1 Then, a forward recurrent neural network (RNN) with GRU units (Cho et al., 2014) runs to calculate source forward hidden states: −→ h i = f( −→ h i−1,xi), (2) where f(·) is a non-linear function.
24	22	Similarly, the source backward hidden states can be obtained using a backward RNN: ←− h i = f( ←− h i+1,xi).
31	54	Though projecting word embedding space into two dimensions (Faruqui and Dyer, 2014) and the attention matrix (Bahdanau et al., 2015) shed partial light on how NMT works, how to interpret the entire network still remains a challenge.
41	10	More formally, we introduce a number of definitions to facilitate the presentation.
49	16	We use a simple feed-forward network shown in Figure 3 to illustrate the central idea of LRP.
51	12	Output: Vector-level relevance setR.
52	19	1 for u ∈ G in a forward topological order do 2 for v ∈ OUT(u) do 3 calculating weight ratios wu→v; 4 end 5 end 6 for v ∈ V do 7 for v ∈ v do 8 rv←v = v; // initializing neuron-level relevance 9 end 10 for u ∈ G in a backward topological order do 11 ru←v = ∑ z∈OUT(u)wu→zrz←v ; // calculating neuron-level relevance 12 end 13 for u ∈ C(v) do 14 Ru←v = ∑ u∈u ∑ v∈v ru←v ; // calculating vector-level relevance 15 R = R∪ {Ru←v}; // Update vector-level relevance set 16 end 17 end Algorithm 1: Layer-wise relevance propagation for neural machine translation.
66	21	The output is a set of vector-level relevance between intended hidden states and their contextual words R. The algorithm first computes weight ratios for each neuron in a forward pass (lines 1-4).
80	15	Figure 4 visualizes the source hidden states for a source content word “nian” (years).
86	26	Clearly, the concatenation of forward and backward hidden states h3 capture contexts in both directions.
92	20	For the target hidden state s2, the contextual word set includes the first target word “my”.
104	12	But the ordering of UNK usually becomes worse if multiple UNK words exist on the source side.
111	29	This example demonstrates that only using attention matrices does not suffice to analyze the internal workings of NMT.
114	11	The translation error is that “history” repeats four times in the translation.
116	34	According to the relevance of the target word embedding Ry6 , the first source word “meiguoren” (american), the second source word “lishi” (history) and the 5-th target word “the” are most relevant to the generation of “history”.
118	14	This finding confirms the importance of controlling source and target contexts to improve fluency and adequacy (Tu et al., 2017).
120	40	One translation error is that the 9-th English word “forge” is totally unrelated to the source sentence.
121	51	Figure 9 visualizes the hidden states of the 9-th target word “forge”.
122	19	We find that while the attention identifies the 10-th source word “kuadaxiyang” (cross-atlantic) to be most relevant, the relevance vector of the target word Ry9 finds that multiple source and target words should contribute to the generation of the next target word.
124	21	Given a source sentence “bu jiejue shengcun wenti , jiu tan bu shang fa zhan , geng tan bu shang ke chixu fazhan” (without solution to the issue of subsistence , there will be no development to speak of , let alone sustainable development), the model prediction is “if we do not solve the problem of living , we will talk about development and still less can we talk about sustainable development”.
125	25	The translation error is that the 8-th negation source word “bu” (not) is untranslated.
128	10	One possible reason is that target context words “will talk” take the lead in determining the next target word.
135	10	Target-side context also plays a critical role in determining the next target word being generated.
137	14	Generating the end-of-sentence token too early might lead to many problems such as word omission, unrelated word generation, and truncated translation (Figures 7 and 9).
148	10	In this work, we propose to use layer-wise relevance propagation to visualize and interpret neural machine translation.
149	24	Our approach is capable of calculating the relevance between arbitrary hidden states and contextual words by back-propagating relevance along the network recursively.
151	30	In the future, we plan to apply our approach to more NMT approaches (Sutskever et al., 2014; Shen et al., 2016; Tu et al., 2016; Wu et al., 2016) on more language pairs to further verify its effectiveness.
152	50	It is also interesting to develop relevancebased neural translation models to explicitly control relevance to produce better translations.
