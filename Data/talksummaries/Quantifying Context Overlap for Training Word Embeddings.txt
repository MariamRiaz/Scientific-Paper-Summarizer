9	12	Few attempts are carried out to indirectly deal with unobserved co-occurrence for dense neural word embeddings.
11	17	Swivel (Shazeer et al., 2016) improves GloVe by using a “soft hinge” loss to prevent from over-estimating zero co-occurrences.
14	19	In this paper, we explore an approach that utilizes context overlap information to dig up more effective co-occurrence relations and propose extensions for GloVe and Swivel to validate the positive impact of introducing context overlap.
15	16	In this work, we explore quantifying context overlap based on the observation that to a certain extent the overlap of Point-wise Mutual Information (PMI) (Church and Hanks, 1990) reflects context overlap.
16	56	As shown in Figure 1, two separate words may exhibit a particular aspect of interest or be semantically related when the overlap area between their PMI is relatively large.
17	19	The calculation of complete PMI-weighted context overlap may be time-consuming when the number of words is large.
18	26	To make the time complexity affordable, only the context words that have strong lexical association with a target word i are considered: Si = {k ∈ V |PMI(i, k) > hPMI} (1) in which V is the vocabulary, hPMI is a threshold which acts as a magnitude to shift PMI, and Si denotes the set that consists of the context words that have enough large PMI values with the target word i.
20	37	Then, we measure the degree of context overlap (CO) between two target words i, j as a function of their PMI values over the intersection of Si and Sj , i.e., CO(i, j) = ∑ k∈Si∩Sj min(f(PMI(i, k)), f(PMI(j, k))) (2) where f is a monotonic mapping function to rectify the data characteristics for certain objective function in word embedding training.
21	16	Compared to identity function f(x) = x, we find exponential function f(x) = exp(x) works much better in our experiments.
23	7	We consider the original co-occurrence matrix as a description of first order co-occurrence relations, while the quantized context overlap as a description of second order co-occurrence relations (Schütze, 1998), i.e., co-co-occurrences, which is represented by “non-logarithmic PMI-weighted context overlap” in this work.
30	9	λij is a weight whose value equals to (min(Xij , xmax)/xmax) α.
37	28	The multi-task (Ruder, 2017) loss function is the weighted sum of the two tasks, i.e., J = JGloV e+β ·J (2) GloV e, where the weight β is a hyperparameter.
38	13	Swivel As pointed out by (Levy et al., 2015) , if the bias terms in GloVe are fixed to the logarithmic count of the corresponding word, the dot products of target word vectors and context word vectors are almost equivalent to the approximation of logarithmic PMI matrix with a shift of log ∑ i,j Xij .
41	47	In our extended version, we add a supplementary loss function to handle second order co-occurrences.
43	7	1 2 λ (2) ij (Aw T i wj +B − PMI(2)(i, j))2 (5) in which A, B are word independent learnable scale parameters, and PMI(2)(i, j) is the Pointwise Mutual Information computed on the second order co-occurrence matrix [X(2)ij ].
44	33	Corpus The training dataset contains 6 billion tokens collected from diversified corpora, including the News Crawl corpus (Chelba et al., 2013), the April 2010 Wikipedia dump (Shaoul, 2010; Lee and Chen, 2017), and a year-2012 subset of the Reddit comment datasets 1.
49	19	For GloVe, recommended parameters in (Pennington et al., 2014) are used.
56	9	In this paper, hPMI , x (2) max and β are set to log 100, 10000 and 0.2 respectively.
73	11	Inspired from the graph in (Shazeer et al., 2016), relations between word analogy accuracy and the log mean frequency of the words in analogy questions and answers are plotted on Figure 2.
75	34	An obvious semantic performance improvement is observed in the range of low frequency.
77	53	We randomly sample 1 million word pairs, and rank these word pairs in descending order by their quantized context overlap.
79	9	However, it is only 1676.1 in the top 0.1% word pairs, it is 3984.8 in the top 1%, and it is 7904.9 in the top 10%.
80	13	This may be caused by PMI’s bias towards infrequent words, but it illustrates infrequent words carry more information in second order co-occurrence relations.
81	13	In this paper, we propose an empirical metric to enhance the word embeddings through estimating second order co-occurrence relations using con- text overlap.
82	14	Instead of only local statistical information, context overlap leverages global association distribution to measure word pairs correlation.
83	18	The proposed method is easy to extend to existing models, such as GloVe and Swivel, by an auxiliary objective function.
84	13	The improvement in experimental results helps to validate the positive impact of introducing quantized context overlap.
92	16	We think it is helpful to revive the classical method in a modern, embedding driven way.
93	44	How to integrate second order co-occurrence information for approaches like SGNS, CBOW should be an interesting future work.
94	12	As future works, we suggest further investigating the characteristics of context overlap in diversified ways.
