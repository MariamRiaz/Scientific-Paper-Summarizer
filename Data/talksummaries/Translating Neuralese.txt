0	70	Several recent papers have described approaches for learning deep communicating policies (DCPs): decentralized representations of behavior that enable multiple agents to communicate via a differentiable channel that can be formulated as a recurrent neural network.
1	24	DCPs have been shown to solve a variety of coordination problems, including reference games (Lazaridou et al., 2016b), logic puzzles (Foerster et al., 2016), and simple control (Sukhbaatar et al., 2016).
2	49	Appealingly, the agents’ communication protocol can be learned via direct 1 We have released code and data at http://github.
9	19	However, DCP agents instead communicate with an automatically induced protocol of unstructured, real-valued recurrent state vectors—an artificial language we might call “neuralese,” which superficially bears little resemblance to natural language, and thus frustrates attempts at direct interpretation.
10	19	232 We propose to understand neuralese messages by translating them.
13	24	While structurally quite similar to the task of machine translation between pairs of human languages, interpretation of neuralese poses a number of novel challenges.
14	19	First, there is no natural source of parallel data: there are no bilingual “speakers” of both neuralese and natural language.
18	45	Based on this intuition, we introduce a translation criterion that matches neuralese messages with natural language strings by minimizing statistical distance in a common representation space of distributions over speaker states.
19	21	We explore several related questions: • What makes a good translation, and under what conditions is translation possible at all?
21	28	(Section 5) • What kinds of theoretical guarantees can we provide about the behavior of agents communicating via this translation model?
22	28	(Section 6) Our translation model and analysis are general, and in fact apply equally to human–computer and human–human translation problems grounded in gameplay.
23	83	In this paper, we focus our experiments specifically on the problem of interpreting communication in deep policies, and apply our approach to the driving game in Figure 1 and two reference games of the kind shown in Figure 2.
24	50	We find that this approach outperforms a more conventional machine translation criterion both when attempting to interoperate with neuralese speakers and when predicting their state.
49	36	We would like to use the representation of πh, the behavior of which is transparent to human users, in order to understand the behavior of πr (which is in general an uninterpretable learned model); we will do this by inducing bilingual dictionaries that map message vectors zr of πr to natural language strings zh of πh and vice-versa.
66	23	The existing literature suggests two broad approaches: Semantic representation The meaning of a message za is given by its denotations: that is, by the set of world states of which za may be felicitously predicated, given the existing context available to a listener.
89	91	We understand the meaning of a message za to be represented by the distribution p(xa|za, xb) it induces over speaker states given listener context.
90	82	We can formalize this by defining the belief distribution β for a message z and context xb as: β(za, xb) = p(xa|za, xb) = p(za|xa)p(xb|xa)∑ x′a p(za|x′a)p(xb|x′a) Here we have modeled the listener as performing a single step of Bayesian inference, using the listener state and the message generation model (by assumption shared between players) to compute the posterior over speaker states.
91	23	While in general neither humans nor DCP agents compute explicit representations of this posterior, past work has found that both humans and suitably-trained neural networks can be modeled as Bayesian reasoners (Frank et al., 2009; Paige and Wood, 2016).
94	61	To translate, we would like to compute tr(zr) = argminzh q(zr, zh) and tr(zh) = argminzr q(zh, zr).
96	32	While this translation criterion directly encodes the semantic notion of meaning described in Section 4, it is doubly intractable: the KL divergence and outer expectation involve a sum over all observations xa and xb respectively; these sums are not in general possible to compute efficiently.
98	108	We draw a collection of samples (xa, xb) from the prior over world states, and then generate for each sample a sequence of distractors (x′a, xb) from p(x ′ a|xb) (we assume access to both of these distributions from the problem representation).
122	27	Our second observation is that it is possible to exactly recover a translation of a DCP strategy from a mixture of humans playing different strategies: Proposition 2.
159	27	The evaluation selects a full game trace from a human player, and replays both the human’s actions and messages exactly (disregarding any incoming messages); the evaluation measures the quality of the natural-language-toneuralese translator, and the extent to which the learned agent model can accommodate a (real) human given translations of the human’s messages.
164	20	The end-to-end trained model achieves nearly perfect accuracy in both cases, while a model trained to communicate in natural language achieves somewhat lower performance.
165	21	Regardless of whether the speaker is a DCP and the listener a model human or vice-versa, translation based on the belief-matching criterion in Section 5 achieves the best performance; indeed, when translating neuralese color names to natural language, the listener is able to achieve a slightly higher score than it is natively.
173	62	After introducing a translation criterion based on matching listener beliefs about speaker states, we presented both theoretical and empirical evidence that this criterion outperforms a conventional machine translation approach at recovering the content of message vectors and facilitating collaboration between humans and learned agents.
174	22	While our evaluation has focused on understanding the behavior of deep communicating policies, the framework proposed in this paper could be much more generally applied.
175	220	Any encoder– decoder model (Sutskever et al., 2014) can be thought of as a kind of communication game played between the encoder and the decoder, so we can analogously imagine computing and translating “beliefs” induced by the encoding to explain what features of the input are being transmitted.
176	64	The current work has focused on learning a purely categorical model of the translation process, supported by an unstructured inventory of translation candidates, and future work could explore the compositional structure of messages, and attempt to synthesize novel natural language or neuralese messages from scratch.
177	65	More broadly, the work here shows that the denotational perspective from formal semantics provides a framework for precisely framing the demands of interpretable machine learning (Wilson et al., 2016), and particularly for ensuring that human users without prior exposure to a learned model are able to interoperate with it, predict its behavior, and diagnose its errors.
