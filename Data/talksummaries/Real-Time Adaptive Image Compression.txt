0	12	Streaming of digital media makes 70% of internet traffic, and is projected to reach 80% by 2020 (CIS, 2015).
4	19	Even though the world of compression seems a natural domain for machine learning approaches, it has not yet benefited from these advancements, for two main reasons.
5	11	First, our deep learning primitives, in their raw forms, are not well-suited to construct representations sufficiently compact.
7	14	Second, it is difficult to develop a deep learning compression approach sufficiently efficient for deployment in environments constrained by computation power, memory footprint and battery life.
8	51	In this work, we present progress on both performance and computational feasibility of ML-based image compression.
11	34	On a GTX 980 Ti GPU, it takes around 9ms to encode and 10ms to decode an image from these datasets: for JPEG, encode/decode times are 18ms/12ms, for JP2 350ms/80ms and for WebP 70ms/80ms.
14	18	We additionally supplement our algorithm with adversarial training specialized towards use in a compression setting.
49	17	We design our feature extraction architecture to recognize these.
50	42	It consists of a pyramidal decomposition which analyzes individual scales, followed by an interscale alignment procedure which exploits structure shared across scales.
81	16	The quantized tensor ŷ is transformed into a binary tensor suitable for encoding via a lossless bitplane decomposition: b := BITPLANEDECOMPOSEB(ŷ) ∈ {0, 1}B×C×H×W .
97	44	We exploit this low entropy by lossless compression via adaptive arithmetic coding.
105	38	The bottleneck may be too small to represent complex patterns well, which affects quality, and it may be too large for simple patterns, which results in inefficient compression.
112	15	The first term penalizes the magnitude of each tensor element, and the second penalizes deviations between spatial neighbors.
114	51	As we train our model, we continuously modulate the scalar coefficient αt to pursue our target codelength.
122	33	In our compression approach, we take the generator as the encoder-decoder pipeline, to which we append a discriminator — albeit with a few key differences from existing GAN formulations.
123	43	In many GAN approaches featuring both a reconstruction and a discrimination loss, the target and the reconstruction are treated independently: each is separately assigned a label indicating whether it is real or fake.
125	16	To do this, we first swap between the target and reconstruction in each input pair to the discriminator with uniform probability.
137	21	More concretely, given lower and upper accuracy bounds L,U ∈ [0, 1] and discriminator accuracy a(DΘ), we apply the following procedure: • If a < L: freeze propagation of confusion signal through the reconstructor, and train the discriminator.
147	14	In this work, we provide comparisons with both traditional and ML-based codecs, and present results in both the RGB domain with equal color weights, as well as in YCbCr with weights as above.
152	46	We trained all models on 128× 128 patches sampled at random from the Yahoo Flickr Creative Com- mons 100 Million dataset (Thomee et al., 2016).
156	14	During runtime we deployed the model on arbitrarily-sized images in a fully-convolutional way.
158	27	We present several types of results: 1.
160	34	Average compressed file sizes relative to ours as function of the MS-SSIM fixed for each image, found in Figures 5 and 6, and Table 1.
162	72	Visual examples of reconstructions of different compression approaches for the same BPP, found in Figure 1 and in the appendix.
164	27	While the Kodak dataset is very popular for testing compression performance, it contains only 24 images, and hence is susceptible to overfitting and does not necessarily fully capture broader statistics of natural images.
165	18	As such, we additionally present performance on the RAISE-1k dataset (Dang-Nguyen et al., 2015) which contains 1,000 raw images.
169	94	See the appendix for a plot demonstrating this effect.
172	17	We use the best-performing configuration we can find of JPEG, JPEG 2000, WebP, and BPG, and reduce their bitrates by their respective header lengths for fair comparison.
173	24	For each image in each test set, each compression approach, each color space, and for the selection of available compression rates, we recorded (1) the BPP, (2) the MS-SSIM (with components weighted appropriately for the color space), and (3) the computation times for encoding and decoding.
174	75	It is important to take great care in the design of the performance evaluation procedure.
