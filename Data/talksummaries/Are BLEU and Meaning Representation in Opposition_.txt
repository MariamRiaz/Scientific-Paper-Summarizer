0	27	Deep learning has brought the possibility of automatically learning continuous representations of sentences.
4	28	A different approach is to obtain sentence representations from training neural machine translation models (Hill et al., 2016).
30	17	, xT using a bidirectional recurrent network with gated recurrent units (GRU, Cho et al., 2014): −→ ht = −−→ GRU(xt, −−→ ht−1), ←− ht = ←−− GRU(xt, ←−− ht+1), ht = [ −→ ht , ←− ht ].
33	51	, hT ) = H of the encoder into a vector of fixed dimensionality that represents the entire sentence.
34	20	Traditional seq2seq models concatenate the final states of both encoder RNNs ( −→ hT and ←− h1) to obtain the sentence representation (denoted as FINAL in Table 1).
35	26	Another option is to combine all encoder states using the average or maximum over time (Collobert and Weston, 2008; Schwenk and Douze, 2017) (AVGPOOL and MAXPOOL in Table 1 and following).
36	41	We adopt an alternative approach, which is to use inner attention1 (Liu et al., 2016; Li et al., 2016) to compute several weighted averages of the encoder states (Lin et al., 2017).
40	44	(1) A vector representation of the source sentence (the “sentence embedding”) can be obtained by flattening the matrixM .
59	18	The Transformer (Vaswani et al., 2017) is a recently proposed model based entirely on feedforward layers and attention.
64	28	Continuous sentence representations can be evaluated in many ways, see e.g. Kiros et al. (2015), Conneau et al. (2017) or the RepEval workshops.2 We evaluate our learned representations with classification and similarity tasks from SentEval (Section 4.1) and by examining clusters of sentence paraphrase representations (Section 4.2).
67	44	Table 2 describes the classification tasks (number of classes, data size, task type and an example) and Table 3 lists the similarity tasks.
68	47	The similarity (relatedness) datasets contain pairs of sentences labeled with a real-valued similarity score.
74	16	COCO (Common Objects in Context; Lin et al., 2014) is an object recognition and image captioning dataset, containing 5 captions for each image.
77	41	HyTER Networks (Dreyer and Marcu, 2014) are large finite-state networks representing a sub set of all possible English translations of 102 Arabic and 102 Chinese sentences.
87	25	For each point, we find its nearest neighbor according to cosine or L2 distance, and count how often the neighbor lies in the same cluster as the original point.
89	39	The Davies-Bouldin index (Davies and Bouldin, 1979) measures cluster separation.
109	37	We estimate translation quality of the various models using single-reference case-sensitive BLEU (Papineni et al., 2002) as implemented in Neural Monkey (the reference implementation is mteval-v13a.pl from NIST or Moses).
114	42	In both cases, the best performing is the ATTN Bahdanau et al. model, followed by Transformer (de only) and our ATTN-ATTN (compound attention).
149	21	The negative correlation between the number of attention heads and the representation metrics from Fig.
150	34	3 (−0.81±0.12 for cs and−0.18±0.19 for de, on average) can be partly explained by the following observation.
152	28	4) and noticed that the heads tend to “divide” the sentence into segments.
153	37	While one would hope that the segments correspond to some meaningful units of the sentence (e.g. subject, predicate, object), we failed to find any such interpretation for ATTN-ATTN and for csmodels in general.
154	16	Instead, the heads divide the source sentence more or less equidistantly, as documented by Fig.
156	28	For de-ATTN-CTX models, we observed a much flatter distribution of attention weights for each head and, unlike in the other models, we were often able to identify a head focusing on the main verb.
157	80	This difference between ATTN-ATTN and some ATTN-CTX models could be explained by the fact that in the former, the decoder is oblivious to the ordering of the heads (because of decoder attention), and hence it may not be useful for a given head to look for a specific syntactic or semantic role.
158	90	We presented a novel variation of attentive NMT models (Bahdanau et al., 2014; Vaswani et al., 2017) that again provides a single meeting point with a continuous representation of the source sentence.
159	16	We evaluated these representations with a number of measures reflecting how well the meaning of the source sentence is captured.
160	59	While our proposed “compound attention” leads to translation quality not much worse than the fully attentive model, it generally does not perform well in the meaning representation.
161	114	Quite on the contrary, the better the BLEU score, the worse the meaning representation.
162	51	We believe that this observation is important for representation learning where bilingual MT now seems less likely to provide useful data, but perhaps more so for MT itself, where the struggle towards a high single-reference BLEU score (or even worse, cross entropy) leads to systems that refuse to consider the meaning of the sentence.
