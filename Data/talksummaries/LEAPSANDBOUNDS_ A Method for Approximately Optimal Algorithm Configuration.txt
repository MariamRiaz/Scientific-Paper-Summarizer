0	19	For computational problems of major practical interest (satisfiability, planning, etc.)
1	51	the computing science community has developed a large number of highly configurable “solvers.” The reason is that while the hardest problem instances take a long time to solve by any of the solvers, the instances that one encounters in practical applications may exhibit specific properties so that the appropriate solver with an appropriate configuration may finish much faster.
2	31	The plethora of solvers and their configurations, which for simplicity of presentation we will just treat as configurations from this point on, is explained by the diversity of applications.
3	121	Which configuration to use in a specific application can then be treated as a learning problem, where an application is identified with an unknown distribution over problem instances that one can sample from, the learning algorithm can run any configuration on any sampled instance until a timeout of its choice, and the goal is to find a configuration with nearly optimal expected runtime while using the least amount of time during the search.1 There has been much practical success on designing such blackbox configuration search methods, especially in the context of satisfiability problems.
4	21	Examples of successful methods include ParamILS (Hutter, 2007; Hutter et al., 2009), SMAC (Hutter et al., 2011; 2013), irace (Birattari et al., 2002; López-Ibánez et al., 2011), and GGA (Ansótegui et al., 2009; 2015).
5	13	These methods themselves rely on many heuristics and as such lack theoretical guarantees.
6	13	Recently, Kleinberg et al. (2017) explored this problem, presenting a general-purpose configuration optimizer called Structured Procrastination, with guarantees on both (i) how close to the optimal configuration the algorithm’s result is, and (ii) how long it takes to find such a configuration.
7	9	For (ii), Kleinberg et al. (2017) prove that the expected runtime of their algorithm is within a logarithmic factor of the optimal runtime in a worst-case sense.
8	7	Furthermore, they show that the gap between worst-case runtimes of existing algorithms (SMAC, ROAR, ParamILS, GGA, irace) and their solution can be arbitrarily large.
10	45	The main novelty of their work is that it comes with theoretical guarantees (lower and upper bounds on the runtime), but no empirical illustration is provided.
11	17	This paper builds on the results of Kleinberg et al. (2017), and our problem statement closely follows theirs.
12	111	Our main technical contributions are as follows: We present an (arguably simpler) algorithm (LEAPSANDBOUNDS) that finds an approximately optimal configuration with a worstcase runtime bound that improves upon that of Kleinberg et al. (2017), while we consider a broader class of problems (we don not need their global runtime cap).
13	62	We also present instance-dependent runtime bounds that show that LEAPSANDBOUNDS finishes faster if the runtime of the configurations over different problem instances has small variance.
14	72	Experiments were carried out to assess practical performance of both Structured Procrastination and LEAPSANDBOUNDS on configuring the open-source minisat solver.
15	46	LEAPSANDBOUNDS runs every configuration for less time than Structured Procrastination, and returns significantly faster.
16	3	Finally, to facilitate further research and enable direct comparison to our results, our large-scale measurements on running times of the minisat solver are published together with the paper.2 The rest of the paper is organized as follows: The problem is introduced formally in Section 2.
17	3	For clarity, the most basic version of our algorithm is presented first in Section 3, and its performance is analyzed in Section 4.
20	5	Finally, conclusions are drawn in Section 8.
21	9	Following Kleinberg et al. (2017), the algorithm configuration problem is defined by a tuple (N ,Γ, R, κ0) as follows:3 Here, N is a family of configurations and Γ is a distribution over input instances.4 For now, we consider the case when N is a finite set.
22	2	If we have a benchmark set of instances, we let Γ be the uniform distribution over these benchmark instances.
23	4	For configuration i ∈ N and instance j, R(i, j) ∈ [0,∞] is the execution time of configuration i on instance j.
24	3	Finally, κ0 > 0 is the minimum runtime: For all i, j pairs, R(i, j) ≥ κ0.
25	13	We let R(i) = EJ∼Γ [R(i, J)] denote the average runtime of configuration i on instances distributed according to Γ, and define OPT = miniR(i) as the mean runtime of an optimal configuration.
29	9	The main difficulty in organizing the search is that some configurations may take a long, or even infinite time to execute on some instances.
33	21	Thus, any sampling method needs to use at least Ω(b) time, despite that the expected runtime is constant 1 (independently of b).
