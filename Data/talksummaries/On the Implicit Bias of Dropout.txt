6	50	In particular, dropout, which is the focus of this paper, randomly drops hidden nodes along with their connections at training time.
7	29	Dropout was introduced by Srivastava et al. (2014) as a way of breaking up co-adaptation among neurons, drawing insights from the success of the sexual reproduction model in the evolution of advanced organisms.
8	23	While dropout has enjoyed tremendous success in training deep neural networks, the theoretical understanding of how dropout (and other algorithmic heuristics) provide regularization in deep learning remains somewhat limited.
9	30	We argue that a prerequisite for understanding implicit regularization due to various algorithmic heuristics in deep learning, including dropout, is to analyze their behavior in simpler models.
11	34	Let x ∈ Rd2 represent an input feature vector with some unknown distribution D such that Ex∼D[xx>] = I.
12	10	The output label vector y ∈ Rd1 is given as y = Mx for some M ∈ Rd1×d2 .
13	32	We consider the hypothesis class represented by a single hidden-layer linear network parametrized as hU,V(x) = UV>x, where V ∈ Rd2×r and U ∈ Rd1×r are the weight matrices in the first and the second layers, respectively.
14	12	The goal of learning is to find weight matrices U,V that minimize the expected loss `(U,V) := Ex∼D[‖y−hU,V(x)‖2] = Ex∼D[‖y−UV>x‖2].
19	48	Note that while the goal was to minimize the expected squared loss, using dropout with gradient descent amounts to finding a minimum of the objective in equation (2); we argue that the additional term in the objective serves as a regularizer, R(U,V) := λ ∑r i=1 ‖ui‖2‖vi‖2, and is an explicit instantiation of the implicit bias of dropout.
20	21	Furthermore, we note that this regularizer is closely related to path regularization which is given as the square-root of the sum over all paths, from input to output, of the product of the squared weights along the path (Neyshabur et al., 2015).
22	17	(3) Interestingly, the dropout regularizer is equal to the square of the path regularizer, i.e. R(U,V) = λψ22(U,V).
23	16	While this observation is rather immediate, it has profound implications owing to the fact that path regularization provides size-independent capacity control in deep learning, thereby supporting empirical evidence that dropout finds good solutions in over-parametrized settings.
24	7	In this paper, we focus on studying the optimization landscape of the objective in equation (2) for a single hiddenlayer linear network with dropout and the special case of an autoencoder with tied weights.
40	12	We denote matrices, vectors, scalar variables and sets by Roman capital letters, Roman small letters, small letters and script letters respectively (e.g. X, x, x, and X ).
42	10	For any integer i, ei denotes the i-th standard basis.
52	7	Note that the loss function `(U,U) is invariant under rotations, i.e., for any orthogonal transformation Q ∈ Rd×d,Q>Q = QQ> = Id, it holds that `(U,U)=Ex∼D[‖y− UQQ>U>x‖2]=`(UQ,UQ), so that applying a rotation matrix to a candidate solution U does not change the value of the loss function.
80	17	Since the goal is to understand the implicit bias of dropout, we specify the global optimum in terms of the true concept, M. Theorem 2.4.
87	49	Figure 1 shows how optimization landscape changes with different dropout rates for a single hidden layer linear autoencoder with one dimensional input and output and with a hidden layer of width two.
99	7	A pair of weight matrices (U,V) ∈ Rd1×r×Rd2×r is said to be jointly equalized if ‖ui‖‖vi‖ = ‖u1‖‖v1‖ for all i ∈ [r].
100	21	A single hidden-layer linear network is said to be equalized if the product of the norms of the incoming and outgoing weights are equal for all hidden nodes.
110	22	, r. Theorem 3.3 implies that for any network hU,V there exists an equalized network hŪ,V̄ such that hŪ,V̄ = hU,V.
114	32	As in the case of autoencoders with tied weights in Section 2, a complete characterization of the implicit bias of dropout is given by considering the global optimality in terms of the network, i.e. in terms of the product of the weight matrices UV>.
115	20	Not surprisingly, even in the case of single hidden-layer networks, dropout promotes sparsity, i.e. favors low-rank weight matrices.
116	9	Then, if (U∗,V∗) is a global optimum of Problem 6, it satisfies that U∗V>∗ = S λρκρ r+λρ (M).
118	40	Since dropout is a first-order method (see Algorithm 1) and the landscape of Problem 4 is highly non-convex, we can perhaps only hope to find a local minimum, that too provided if the problem has no degenerate saddle points (Lee et al., 2016; Ge et al., 2015).
119	29	Therefore, in this section, we pose the following questions: What is the implicit bias of dropout in terms of local minima?
120	50	Do local minima share anything with global minima structurally or in terms of the objective?
121	17	Can dropout find a local optimum?
122	21	For the sake of simplicity of analysis, we focus on the case of autoencoders with tied weight as in Section 2.
123	17	We show in Section 4.1 that (a) local minima of Problem 4 inherit the same implicit bias as the global optima, i.e. all local minima are equalized.
