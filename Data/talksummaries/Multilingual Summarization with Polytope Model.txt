9	15	One of the introduced objective functions implements this idea.
10	12	Our method ranks and extracts significant sentences into a summary, without any need in morphological text analysis.
11	17	It was applied for both single-document (MSS) and multi-document (MMS) MultiLing 2015 summarization tasks, in three languages–English, Hebrew, and Arabic.
12	63	In this paper we present experimental results in comparison with other systems that participated in the same tasks, using the same languages.
14	18	Documents are split into sentences S1, ..., Sn.
18	32	Unique stemmed words are called terms and are denoted by T1, ..., Tm.
19	37	Every sentence is modeled as a sequence of terms from T1, ..., Tm where each 227 term may appear zero or more times in a sentence.
20	16	We are also given the desired number of words for a summary, denoted by MaxWords .
22	42	Because it is difficult, or even impossible, to know what humans consider to be the best summary, we approximate the human decision process by optimizing certain objective functions over representation of input documents constructed according to our model.
25	48	A row i of matrix A is used to define a linear constraint for sentence Si as follows: m∑ j=1 aijxij ≤ m∑ j=1 aij (1) Equation (1) also defines the lower half-space in Rmn corresponding to sentence Si.
26	43	Together with additional constraints, such as a bound MaxWords on the number of words in the summary, we obtain a system of linear inequalities that describes the intersection of corresponding lower half-spaces of Rmn, forming a closed convex polyhedron called a polytope: ∑m j=1 aijxij ≤ ∑m j=1 aij , ∀i = 1..n 0 ≤ xij ≤ 1, ∀i = 1..n, j = 1..m∑n i=1 ∑m j=1 aijxij ≤ MaxWords (2) All possible extractive summaries are represented by vertices of the polytope defined in (2).
27	37	It remains only to define an objective function which optimum on the polytope boundary will define the summary we seek.
28	53	Because such an optimum may be achieved not on a polytope vertex but rather on one of polytope faces (because we use linear programming over rationals), we need only to locate the vertex of a polytope closest to the point of optimum.
29	11	This task is done by finding distances from the optimum to every one of the sentence hyperplanes and selecting those with minimal distance to the point of optimum.
32	13	In this section, we describe the objective functions we used in our system.
36	18	We define relevance cosrel i of a sentence Si as a cosine similarity between the sentence, viewed as a weighted vector of its terms, and the document.
44	19	The second proposed objective function maximizes the weighted sum of bigrams (consecutive term pairs appearing in sentences), where the weight of a bigram denotes its importance.
55	17	Database size is n. Let s = (Ti1, .
59	9	Then we sort F first by decreasing sequence size and then by decreasing support, and finally we keep only top B sequences for a user-defined boundary B.
60	10	We modify the general model (2) by representing sentences as sums of their frequent sequences from F .
61	9	, fk}, sorted by decreasing size and then by decreasing support.
63	19	Let count ij denote the number of times sentence Si contains frequent term sequence fj .
67	10	Relevance freqrel i of a sentence Si is defined as a cosine similarity between the vector of terms in Si covered by members of F , and the entire document.
69	59	The resulting objective function maximizes relevance of chosen sentences while minimizing redundancy defined in (5): max n∑ i=1 freqrel isi − n∑ i=1 n∑ j=1 cosred ijred ij (13)
71	43	The quality of the summaries is measured by ROUGE-1 (Recall, Precision, and Fmeasure).
72	21	(Lin, 2004) We also demonstrate the absolute ranks of each submission–P-Rank, R-Rank, and F-Rank–when their scores are sorted by Precision, Recall, and F-measure, respectively.
74	52	Two systems–Oracles and Lead–were used as topline and baseline summarizers, respectively.
76	21	Because Oracles can actually “see” the human summaries, it is considered as the optimal algorithm and its scores are the best scores that extractive approaches can achieve.
77	23	Lead simply extracts the leading substring of the body text of the articles having the same length as the human summary of the article.
82	22	Arabic: 5th place out of 7 systems in MSS task, and 4th place out of 9 participants and the highest recall score in MMS task.
