0	45	One of the biggest mysteries of deep learning is why neural networks are successfully trained in practice using gradient-based methods, despite the inherent nonconvexity of the associated optimization problem.
3	85	For example, recent work has shown that other non-convex learning problems, such as phase retrieval, matrix completion, dictionary learning, and tensor decomposition, do not have spurious local minima under suitable assumptions, in which case local search methods have a chance of succeeding (e.g., (Ge et al., 2015; Sun et al., 2015; Ge et al., 2016; Bhojanapalli et al., 2016)).
5	19	In this paper, we focus on perhaps the simplest non-trivial ReLU neural networks, namely predictors of the form x 7→ k∑ i=1 [w>i x]+ for some k > 1, where [z]+ = max{0, z} is the ReLU function, x is a vector in Rd, and w1, .
6	93	We consider directly optimizing the expected squared loss, where the input is standard Gaussian, and in the realizable case – namely, that the target values are generated by a network of a similar architecture: min w1,...,wk Ex∼N (0,I) 1 2 ( k∑ i=1 [w>i x]+ − k∑ i=1 [v>i x]+ )2  .
8	27	, k and any permutation σ) is a global minimum with zero expected loss.
9	20	Several recent papers analyzed such objectives, in the hope of showing that it does not suffer from spurious local minima (see related work below for more details).
13	27	In fact, since in high dimensions randomly-chosen vectors are approximately orthogonal, and the landscape of the objective function is robust to small perturbations, we can show that spurious local minima exist for nearly all neural network problems as in Eq.
18	20	(1) in closed form (without the expectation), it is not clear how to get analytical expressions for its roots, and hence characterize the stationary points of Eq.
20	106	Instead, we employed the following strategy: We ran standard gradient descent with random initialization on the objective function, until we reached a point which is both suboptimal (function value being significantly higher than 0); approximate stationary (gradient norm very close to 0); and with a strictly positive definite Hessian (with minimal eigenvalue significantly larger than 0).
21	21	We use a computer to verify these conditions in a formal manner, avoiding floatingpoint arithmetic and the possibility of rounding errors.
22	196	Relying on these numbers, we employ a Taylor expansion argument, to show that we must have arrived at a point very close to a local (non-global) minimum of Eq.
90	16	In our work, we chose to use variable precision arithmetic (VPA), a standard package of MATLAB which is based on symbolic arithmetic, and allows performing elementary numerical computations with an arbitrary number of guaranteed digits of precision.
92	12	The bulk of the proof consists of showing how we bound the quantities relevant to Lemma 1 in an elementary manner.
96	14	Therefore, if we take a local minima for Rk, and pad it with d− k zeros, we get a point in Rd for which the gradient’s norm is unchanged, the Hessian has the same spectrum for any d ≥ k + 1, and the third derivatives are still bounded.
107	15	, 20}, we ran 1000 instantiations of gradient descent on the objective in Eq.
110	17	Points obtaining objective values less than 10−3 were ignored as those are likely to be close to a global minimum.
126	68	Out of 1000 gradient descent instantiations for n = k = 6, three converged close to a local minimum.
129	10	where the parameter vector of each of the 6 neurons corresponds to a column of the matrix denoted w61.
131	64	This implied that all three suspicious points found for n = k = 6 are of distance at most r = 1.12 · 10−7 from a local minimum with objective value at least 0.02508.
140	43	This implied that w91 is of distance at most r = 7.8 · 10−8 from a local minimum with objective value at least 0.02056.
141	14	It is interesting to note that the points found in examples 1 and 2, as well as all other local minima detected, have a nice symmetric structure: We see that most of the trained neurons are very close to the target neurons in most of the dimensions.
149	9	Given a natural number k, we let [k] be shorthand for {1, .
152	18	,vk, since any other choice amounts to rotating or reflecting the same objective function (which of course does not change the existence or non-existence of its local minima).
154	20	As we show in Subsection 4.2 below, the objective function in Eq.
156	17	We first ran standard gradient descent, starting from random initialization and using a fixed step size of 0.1, till we reached a point wn1 , such that the gradient norm w.r.t.
157	142	Given this point, we use Lemma 1 and Lemma 2 to prove that it is close to a local minimum.
162	17	Provide a rigorous Lipschitz bound on the objective F (wn1 ), establishing Lemma 2 (see Subsection A.3 in the appendix for the relevant calculations).
164	9	The code we used is freely available at https://github.com/ItaySafran/OneLayerGDconvergence.git.
166	18	For convenience, we will now state closed-form expressions (without an expectation) for the objective function F in Eq.
171	27	Using the above representation, Brutzkus & Globerson (2017) compute the gradient of f (w,v) with respect to w, given by g (w,v) := ∂ ∂w f (w,v) = 1 2π (||v|| sin (θw,v) w̄ + (π − θw,v)v) .
