13	10	(iv) We demonstrate the effectiveness of the injected knowledge by case studies and data statistics in a qualitative evaluation study.
14	95	In this work, we examine the impact of using external knowledge as supporting information for the task of cloze style reading comprehension.
18	94	It uses the input of the story context tokens d1..m, the question tokens q1..n, the set of answer candidates a1..k and a set of ‘relevant’ background knowledge facts f1..p in order to select the right answer.
19	14	To include external knowledge for the RC task, we encode each fact f1..p and use attention to select the most relevant among them for each token in the story and question.
21	66	See Figure 1 for illustration.
22	20	In our experiments we use knowledge from the Open Mind Common Sense (OMCS, Singh et al. (2002)) part of ConceptNet, a crowd-sourced resource of commonsense knowledge with a total of ∼630k facts.
37	13	We implement our Knowledgeable Reader (KnReader) using as a basis the Attention Sum Reader as one of the strongest core models for single-hop RC.
42	11	The model calculates the attention between the question representation rq and the story token context encodings of the candidate tokens a1..10 and sums the attention scores for the candidates that appear multiple times in the story.
46	8	To represent the document and question contexts, we first encode the tokens with a Bi-directional GRU (Gated Recurrent Unit) (Chung et al., 2014) to obtain context-encoded representations for document (cctxd1..n) and question (cctxq1..m) encoding: cctxd1..n = BiGRU ctx(ed1..n) ∈ Rn×2h (1) cctxq1..m = BiGRU ctx(eq1..m) ∈ Rm×2h (2) , where di and qi denote the ith token of a text sequence d (document) and q (question), respectively, n and m is the size of d and q and h the output hidden size (256) of a single GRU unit.
54	88	Answer Prediction: Qctx to Dctx Attention.
55	49	In order to predict the correct answer to the given question, we rank the given answer candidates a1..aL according to the normalized attention sum score between the context (ctx) representation of the question placeholder rctxq and the representation of the candidate tokens in the document: P (ai|q, d) = softmax( ∑ αij ) (5) αij = Att(r ctx q , c ctx dj ), i ∈ [1..L] (6) , where j is an index pointer from the list of indices that point to the candidate ai token occurrences in the document context representation cd.
56	18	Enriching Context Representations with Knowledge (Context+Knowledge).
57	12	To enhance the representation of the context, we add knowledge, retrieved as a set of knowledge facts.
58	15	For each instance in the dataset, we retrieve a number of relevant facts (cf.
59	14	Each retrieved fact is represented as a triple f = (wsubj1..Lsubj , w rel 0 , w obj 1..Lobj ), where wsubj1..Lsubj and w obj 1..Lobj are a multi-word expressions representing the subject and object with sequence lengths Lsubj and Lobj , and wrel0 is a word token corresponding to a relation.3 As a result of fact encoding, we obtain a separate knowledge memory for each instance in the data.
60	132	To encode the knowledge we use a BiGRU to encode the triple argument tokens into the following context-encoded representations: fsubjlast = BiGRU(Emb(w subj 1..Lsubj ), 0) (7) f rellast = BiGRU(Emb(w rel 0 ), f subj last ) (8) fobjlast = BiGRU(Emb(w obj 1..Lsubj ), f rellast) (9) , where fsubjlast , f rel last, f obj last are the final hidden states of the context encoder BiGRU , that are also used as initial representations for the encoding of the next triple attribute in left-to-right order.
61	110	See Supplement for comprehensive visualizations.
62	80	The motivation behind this encoding is: (i) We encode the knowledge fact attributes in the same vector space as the plain tokens; (ii) we preserve the triple directionality; (iii) we use the relation type as a way of filtering the subject information to initialize the object.
63	52	Querying the Knowledge Memory.
64	134	To enrich the context representation of the document and question tokens with the facts collected in the knowledge memory, we select a single sum of weighted fact representations for each token using Key-Value retrieval (Miller et al., 2016).
65	18	In our model the key Mk(ey)i can be either f subj last or f obj last and the value Mv(alue)i is f obj last.
66	32	For each context-encoded token cctxsi (s = d, q; i the token index) we attend over all knowledge memory keys Mki in the retrieved P knowledge facts.
67	64	We use an attention function Att, scale the scalar attention value using softmax, multiply it with the value representation Mvi and sum the result into a single vector value representation cknsi : cknsi = ∑ softmax(Att(cctx,Mk1..P )) TMv1..P (10) Att is a dot product, but it can be replaced with another attention function.
69	67	Combine Context and Knowledge (ctx+kn).
72	16	To rank answer candidates a1..aL we use attention sum similar to Eq.5 over an attention αensembleij that combines attentions between context (ctx) and context+knowledge (ctx+kn) representations of the question (rctx(+kn)q ) and candidate token occurrences aij in the document c ctx(+kn) dj : P (ai|q, d) = softmax( ∑ αensembleij ) (12) αensembleij = W1Att(r ctx q , c ctx dj ) +W2Att(r ctx q , c ctx+kn dj ) +W3Att(r ctx+kn q , c ctx dj ) +W4Att(r ctx+kn q , c ctx+kn dj ) (13) , where j is an index pointer from the list of indices that point to the candidate ai token occurrences in the document context representation c ctx(+kn) d .
74	17	We experiment with knowledge-enhanced clozestyle reading comprehension using the Common Nouns and Named Entities partitions of the Children’s Book Test (CBT) dataset (Hill et al., 2015).
75	16	In the CBT cloze-style task a system is asked to read a children story context of 20 sentences.
80	23	Since the best neural model (Munkhdalai and Yu, 2016) achieves only 72.0% on the task, we hypothesize that the task itself can benefit from external knowledge.
81	15	The characteristics of the data are shown in Table 1.
83	13	As a source of commonsense knowledge we use the Open Mind Common Sense part of ConceptNet 5.0 that contains 630k fact triples.
