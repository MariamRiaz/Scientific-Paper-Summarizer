{"sections": [{"text": "Recent years have seen a flurry of activities in designing provably efficient nonconvex optimization procedures for solving statistical estimation problems. For various problems like phase retrieval or low-rank matrix completion, state-of-the-art nonconvex procedures require proper regularization (e.g. trimming, regularized cost, projection) in order to guarantee fast convergence. When it comes to vanilla procedures such as gradient descent, however, prior theory either recommends highly conservative learning rates to avoid overshooting, or completely lacks performance guarantees. This paper uncovers a striking phenomenon in several nonconvex problems: even in the absence of explicit regularization, gradient descent follows a trajectory staying within a basin that enjoys nice geometry, consisting of points incoherent with the sampling mechanism. This \u201cimplicit regularization\u201d feature allows gradient descent to proceed in a far more aggressive fashion without overshooting, which in turn results in substantial computational savings. Focusing on two statistical estimation problems, i.e. solving random quadratic systems of equations and low-rank matrix completion, we establish that gradient descent achieves near-optimal statistical and computational guarantees without explicit regularization. As a byproduct, for noisy matrix completion, we demonstrate that gradient descent enables optimal control of both entrywise and spectral-norm errors.\n1Department of Operations Research and Financial Engineering, Princeton University, Princeton, NJ 08544, USA 2Department of Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh, PA 15213, USA 3Department of Electrical Engineering, Princeton University, Princeton, NJ 08544, USA. Correspondence to: Cong Ma <congm@princeton.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s)."}, {"heading": "1. Introduction", "text": "A wide spectrum of science and engineering applications calls for solutions to a nonlinear system of equations. Imagine we have collected a set of data points y = {yj}1\u2264j\u2264m, generated by a nonlinear sensing system,\nyj \u2248 Aj ( x\\ ) , 1 \u2264 j \u2264 m,\nwhere x\\ is the unknown object of interest, and the Aj\u2019s are certain nonlinear maps known a priori. Can we hope to reconstruct the underlying object x\\ in a faithful yet efficient manner? Problems of this kind abound in information and statistical science, prominent examples including low-rank matrix recovery (Keshavan et al., 2010; Cande\u0300s & Recht, 2009), phase retrieval (Cande\u0300s et al., 2013; Jaganathan et al., 2015), and learning neural networks (Soltanolkotabi et al., 2017; Zhong et al., 2017), to name just a few.\nIn principle, one can attempt reconstruction by seeking a solution that minimizes the empirical loss, namely,\nminimizex f(x) = m\u2211\nj=1\n\u2223\u2223yj \u2212Aj(x) \u2223\u22232. (1)\nUnfortunately, this empirical loss minimization problem is, in many cases, highly nonconvex, making it NP-hard in general. For example, this non-convexity issue comes up in:\n\u2022 Solving random quadratic systems of equations (a.k.a. phase retrieval): where one wishes to solve for x\\\nin m quadratic equations yj = ( a>j x \\ )2\n, 1 \u2264 j \u2264 m, with {aj}1\u2264j\u2264m denoting the known design vectors. In this case, the empirical risk minimization is given by\nminimizex\u2208Rn f(x) = 1\n4m\nm\u2211\nj=1\n[ yj \u2212 ( a>j x )2]2 . (2)\n\u2022 Low-rank matrix completion: which aims to predict all entries of a low-rank matrix M \\ = X\\X\\> from partial entries (those from an index subset \u2126), where X\\ \u2208 Rn\u00d7r (r n). Here, the nonconvex problem to solve is\nminimize X\u2208Rn\u00d7r\nf(X) = n2\n4m\n\u2211\n(j,k)\u2208\u2126\n( M \\j,k \u2212 e>j XX>ek )2 .\nImplicit Regularization in Nonconvex Statistical Estimation\nTable 1. Prior theory for gradient descent (with spectral initialization)\nVanilla gradient descent Regularized gradient descent sample iteration step sample iteration type of\ncomplexity complexity size complexity complexity regularization Phase\nn logn n log 1\n1 n\nn log 1 trimming retrieval e.g. (Chen & Cande\u0300s, 2017)\nn/a n/a n/a nr7 nr log 1\nregularized loss Matrix e.g. (Sun & Luo, 2016)\ncompletion nr2 r2 log 1\nprojection e.g. (Chen & Wainwright, 2015)"}, {"heading": "1.1. Nonconvex Optimization via Regularized GD", "text": "First-order methods have been a popular heuristic in practice for solving nonconvex problems including (1). For instance, a widely adopted procedure is gradient descent (GD), which follows the update rule\nxt+1 = xt \u2212 \u03b7t\u2207f ( xt ) , t \u2265 0, (3)\nwhere \u03b7t is the learning rate (or step size) and x0 is some proper initial guess. Given that it only performs a single gradient calculation \u2207f(\u00b7) per iteration (which typically can be completed within near-linear time), this paradigm emerges as a candidate for solving large-scale problems. The natural questions are: whether xt converges to the global solution and, if so, how long it takes for convergence, especially since (1) is highly nonconvex.\nFortunately, despite the worst-case hardness, appealing convergence properties have been discovered in various statistical estimation problems; the blessing being that the statistical models help rule out ill-behaved instances. For the average case, the empirical loss often enjoys benign geometry, particularly in a local region surrounding the global optimum. In light of this, an effective nonconvex iterative method typically consists of two parts:\n1. an initialization scheme (e.g. spectral methods); 2. an iterative refinement procedure (e.g. gradient descent).\nThis strategy has recently spurred a great deal of interest, owing to its promise of achieving computational efficiency and statistical accuracy at once for a growing list of problems, e.g. (Keshavan et al., 2010; Jain et al., 2013; Chen & Wainwright, 2015; Sun & Luo, 2016; Cande\u0300s et al., 2015; Chen & Cande\u0300s, 2017). However, rather than directly applying vanilla GD (3), existing theory often suggests enforcing proper regularization. Such explicit regularization enables improved computational convergence by properly \u201cstabilizing\u201d the search directions. The following regularization schemes, among others, have been suggested to obtain or improve computational guarantees. We refer to these algorithms collectively as Regularized Gradient Descent.\n\u2022 Trimming/truncation, which truncates a subset of the gradient components when forming the descent direction. For instance, when solving quadratic systems of equations, one can modify the gradient descent update rule as\nxt+1 = xt \u2212 \u03b7tT ( \u2207f ( xt )) , (4)\nwhere T is an operator that effectively drops samples bearing too much influence on the search direction (Chen & Cande\u0300s, 2017; Zhang et al., 2016b; Wang et al., 2017). \u2022 Regularized loss, which attempts to optimize a regular-\nized empirical risk function through\nxt+1 = xt \u2212 \u03b7t ( \u2207f ( xt ) +\u2207R ( xt )) , (5)\nwhere R(x) stands for an additional penalty term in the empirical loss. For example, in matrix completion, R(\u00b7) penalizes the `2 row norm (Keshavan et al., 2010; Sun & Luo, 2016) as well as the Frobenius norm (Sun & Luo, 2016) of the decision matrix. \u2022 Projection, which projects the iterates onto certain sets based on prior knowledge, that is,\nxt+1 = P ( xt \u2212 \u03b7t\u2207f ( xt )) , (6)\nwhere P is a certain projection operator used to enforce, for example, incoherence properties. This strategy has been employed in low-rank matrix completion (Chen & Wainwright, 2015; Zheng & Lafferty, 2016).\nEquipped with such regularization procedures, existing works uncover appealing computational and statistical properties under various statistical models. Table 1 summarizes the performance guarantees derived in the prior literature; for simplicity, only orderwise results are provided."}, {"heading": "1.2. Regularization-free Procedures?", "text": "The regularized gradient descent algorithms, while exhibiting appealing performance, usually introduce more tuning parameters depending on the assumed statistical models. In contrast, vanilla gradient descent (cf. (3)) \u2014 which is perhaps the very first method that comes into mind and requires minimal tuning parameters \u2014 is far less understood (cf. Table 1). Take matrix completion as an example: to the best of our knowledge, there is currently no theoretical guarantee derived for vanilla gradient descent.\nThe situation is better for phase retrieval: the local convergence of vanilla gradient descent, also known as Wirtinger flow (WF), has been investigated in (Cande\u0300s et al., 2015).\nImplicit Regularization in Nonconvex Statistical Estimation\nUnder i.i.d. Gaussian design and with near-optimal sample complexity, WF (combined with spectral initialization) provably achieves -accuracy (in a relative sense) within O(n log(1/\u03b5)) iterations. Nevertheless, the computational guarantee is significantly outperformed by the regularized version (called truncated Wirtinger flow (Chen & Cande\u0300s, 2017)), which only requires O(log(1/\u03b5)) iterations to converge with similar per-iteration cost. On closer inspection, the high computational cost of WF is largely due to the vanishingly small step size \u03b7t = O(1/(n\u2016x\\\u201622)) \u2014 and hence slow movement \u2014 suggested by the theory (Cande\u0300s et al., 2015). While this is already the largest possible step size allowed in the theory published in (Cande\u0300s et al., 2015), it is considerably more conservative than the choice \u03b7t = O ( 1/\u2016x\\\u201622 ) theoretically justified for the regularized version (Chen & Cande\u0300s, 2017; Zhang et al., 2016b).\nThe lack of understanding and the suboptimal results about vanilla GD raise a natural question: are regularization-free iterative algorithms inherently suboptimal for solving nonconvex statistical estimation problems?"}, {"heading": "1.3. Numerical Surprise of Unregularized GD", "text": "To answer the preceding question, it is perhaps best to first collect some numerical evidence. In what follows, we test the performance of vanilla GD for solving random quadratic systems using a constant step size. The initial guess is obtained by means of the standard spectral method.\nFor each n, set m = 10n, take x\\ \u2208 Rn to be a random vector with unit norm, and generate the design vectors aj\ni.i.d.\u223c N (0, In), 1 \u2264 j \u2264 m. Figure 1 illustrates the relative `2 error min{\u2016xt\u2212x\\\u20162, \u2016xt+x\\\u20162}/\u2016x\\\u20162 (modulo the unrecoverable global phase) vs. the iteration count. The results are shown for n = 20, 100, 200, 1000, with the step size taken to be \u03b7t = 0.1 in all settings.\nIn all settings, vanilla gradient descent enjoys remarkable linear convergence, always yielding an accuracy of 10\u22125 (in a relative sense) within around 200 iterations. In particular, the step size is taken to be \u03b7t = 0.1 although we vary the problem size from n = 20 to n = 1000. The consequence is that the convergence rates experience little changes when the\nproblem sizes vary. In comparison, the theory published in (Cande\u0300s et al., 2015) seems overly pessimistic, as it suggests a diminishing step size inversely proportional to n and, as a result, an iteration complexity that worsens as the problem size grows.\nIn short, the above empirical results are surprisingly positive yet puzzling. Why was the computational efficiency of vanilla gradient descent unexplained or substantially underestimated in prior theory?"}, {"heading": "1.4. This Paper", "text": "The main contribution of this paper is towards demystifying the \u201cunreasonable\u201d effectiveness of regularization-free nonconvex gradient methods. As asserted in previous work, regularized gradient descent succeeds by properly enforcing/promoting certain incoherence conditions throughout the execution of the algorithm. In contrast, we discover that\nVanilla gradient descent automatically forces the iterates to stay incoherent with the measurement mechanism, thus implicitly regularizing the search directions.\nThis \u201cimplicit regularization\u201d phenomenon is of fundamental importance, suggesting that vanilla gradient descent proceeds as if it were properly regularized. This explains the remarkably favorable performance of unregularized gradient descent in practice. Focusing on two fundamental statistical estimation problems, our theory guarantees both statistical and computational efficiency of vanilla gradient descent under random designs and spectral initialization. With near-optimal sample complexity, to attain -accuracy, vanilla gradient descent converges in an almost dimensionfree O(log(1/ )) iterations, possibly up to a log n factor. As a byproduct of our theory, we show that gradient descent provably controls the entrywise and spectral-norm estimation errors for noisy matrix completion."}, {"heading": "2. Implicit Regularization \u2013 A Case Study", "text": "To reveal reasons behind the effectiveness of vanilla gradient descent, we first examine the existing theory of gradient descent and identify the geometric properties that enable linear convergence. We then develop an understanding as to why prior theory is conservative, and describe the phenomenon of implicit regularization that helps explain the effectiveness of vanilla gradient descent. To facilitate discussion, we will use the problem of solving random quadratic systems of equations (or phase retrieval) and Wirtinger flow as a case study, but our diagnosis applies more generally."}, {"heading": "2.1. Gradient Descent Theory Revisited", "text": "It is well-known that for an unconstrained optimization problem, if the objective function f is both \u03b1-strongly convex and \u03b2-smooth, then vanilla gradient descent (3) enjoys `2\nImplicit Regularization in Nonconvex Statistical Estimation\nerror contraction (Bubeck, 2015), namely, for t \u2265 0 \u2225\u2225xt+1 \u2212 x\\\u20162 \u2264 ( 1\u2212 2\n\u03b2/\u03b1+ 1\n)\u2225\u2225xt \u2212 x\\ \u2225\u2225\n2 , (7)\nas long as the step size is chosen as \u03b7t = 2/(\u03b1 + \u03b2). Here, x\\ denotes the global minimum. This immediately reveals the iteration complexity for gradient descent: the number of iterations taken to attain -accuracy is bounded by O((\u03b2/\u03b1) log(1/ )). In other words, the iteration complexity is dictated by and scales linearly with the condition number \u2014 the ratio \u03b2/\u03b1 of smoothness to strong convexity parameters.\nMoving beyond convex optimization, one can easily extend the above theory to nonconvex problems with local strong convexity and smoothness. More precisely, suppose the objective function f satisfies\n\u22072f(x) \u03b1I and \u2225\u2225\u22072f(x) \u2225\u2225 \u2264 \u03b2\nover a local `2 ball surrounding the global minimum x\\:\nB\u03b4(x) := { x | \u2016x\u2212 x\\\u20162 \u2264 \u03b4\u2016x\\\u20162 } . (8)\nThe contraction result (7) continues to hold, as long as the algorithm starts with an initial point that falls inside B\u03b4(x)."}, {"heading": "2.2. Local Geometry for Solving Quadratic Systems", "text": "To invoke generic gradient descent theory, it is critical to characterize the local strong convexity and smoothness properties of the loss function. Take the problem of solving random quadratic systems as an example. Consider the i.i.d. Gaussian design in which aj\ni.i.d.\u223c N (0, In), 1 \u2264 j \u2264 m, and suppose without loss of generality that the underlying signal obeys \u2016x\\\u20162 = 1. In the regime where m n log n (which is the regime considered in (Cande\u0300s et al., 2015)), local strong convexity is present, in the sense that f(\u00b7) as defined in (2) obeys\n\u22072f(x) (1/2) \u00b7 In, \u2200x : \u2225\u2225x\u2212 x\\ \u2225\u2225 2 \u2264 \u03b4 \u2225\u2225x\\ \u2225\u2225 2\nwith high probability, provided that \u03b4 > 0 is sufficiently small (see (Soltanolkotabi, 2014; White et al., 2015) and (Ma et al., 2017)). The smoothness parameter, however, is not well-controlled. In fact, it can be as large as (up to logarithmic factors) \u2225\u2225\u22072f(x) \u2225\u2225 . n even when we restrict attention to the local `2 ball (8) with \u03b4 > 0 being a fixed small constant. This means that the condition number \u03b2/\u03b1 (defined in Section 2.1) may scale as O(n), leading to the step size recommendation \u03b7t 1/n, and, as a consequence, a high iteration complexity O(n log(1/ )). This underpins the analysis in (Cande\u0300s et al., 2015).\nIn summary, the geometric properties of the loss function \u2014 even in the local `2 ball centering around the global minimum \u2014 is not as favorable as one anticipates. A direct\napplication of generic gradient descent theory leads to an overly conservative learning rate and a pessimistic convergence rate, unless the number of samples is enormously larger than the number of unknowns."}, {"heading": "2.3. Which Region Enjoys Nicer Geometry?", "text": "Interestingly, our theory identifies a local region surrounding x\\ with a large diameter that enjoys much nicer geometry. This region does not mimic an `2 ball, but rather, the intersection of an `2 ball and a polytope. We term it the region of incoherence and contraction (RIC). For phase retrieval, the RIC includes all points x \u2208 Rn obeying\n\u2225\u2225x\u2212 x\\ \u2225\u2225 2 \u2264 \u03b4 \u2225\u2225x\\ \u2225\u2225 2\nand (9a)\nmax 1\u2264j\u2264m\n\u2223\u2223a>j ( x\u2212 x\\ )\u2223\u2223 . \u221a log n \u2225\u2225x\\ \u2225\u2225 2 , (9b)\nwhere \u03b4 > 0 is some small numerical constant. As will be formalized in (Ma et al., 2017), with high probability the Hessian matrix satisfies\n(1/2) \u00b7 In \u22072f(x) O(log n) \u00b7 In simultaneously for all x in the RIC. In words, the Hessian matrix is nearly well-conditioned (with the condition number bounded by O(log n)), as long as (i) the iterate is not very far from the global minimizer (cf. (9a)), and (ii) the iterate remains incoherent1 with respect to the sensing vectors (cf. (9b)). See Figure 2(a) for an illustration.\nThe following observation is thus immediate: one can safely adopt a far more aggressive step size (as large as \u03b7t = O(1/ log n)) to achieve acceleration, as long as the iterates stay within the RIC. This, however, fails to be guaranteed by generic gradient descent theory. To be more precise, if the current iterate xt falls within the desired region, then in view of (7), we can ensure `2 error contraction after one iteration, namely,\n\u2016xt+1 \u2212 x\\\u20162 \u2264 \u2016xt \u2212 x\\\u20162 and hence xt+1 stays within the local `2 ball and hence satisfies (9a). However, it is not immediately obvious that xt+1 would still stay incoherent with the sensing vectors and satisfy (9b). If xt+1 leaves the RIC, then it no longer enjoys the benign local geometry of the loss function, and the algorithm has to slow down in order to avoid overshooting. See Fig. 2(b) for a visual illustration. In fact, in almost all regularized gradient descent algorithms mentioned in Section 1.1, the regularization procedures are mainly proposed to enforce such incoherence constraints."}, {"heading": "2.4. Implicit Regularization", "text": "However, is regularization really necessary for the iterates to stay within the RIC? To answer this question, we plot\n1If x is aligned with (and hence very coherent with) one vector aj , then with high probability one has \u2223\u2223a>j (x\u2212x\\)| & \u2223\u2223a>j x| \u221a n\u2016x\u20162, which is significantly larger than \u221a logn\u2016x\u20162.\nImplicit Regularization in Nonconvex Statistical Estimation\n\u00b7\nminimizex f(x) = 1 m mX i=1 (a>i x) 2 yi 2 s x 2 Rns.t. a>i x 2 = a>i x \\ 2 , 1  i  m minimizeX f(X) = X (i,j)2\u2326 \u21e3 e>i XX >ej e>i X\\X\\>ej 2 minimizeh,x f(h, x) = mX i=1 \u21e3 e>i XX >ej e>i X\\X\\>ej 2\nfind X 2 Rn\u21e5r s.t. e>j XX>ei = e>j X\\X\\>ei, (i, j) 2 \u2326\nfind h, x 2 Cn s.t. b\u21e4i hix\u21e4i ai = b\u21e4i h\\ix\\\u21e4i ai, 1  i  m max(r2f(x)) max(r2f(x))\na1 a2 x \\\na>1 (x x\\)\nkx x\\k2 . log n\na1 a2 x \\\na>2 (x x\\)\nkx x\\k2 . log n\n1\nminimizex f(x) = 1 m mX i=1 (a>i x) 2 yi 2 s x 2 Rns.t. a>i x 2 = a>i x \\ 2 , 1  i  m minimizeX f(X) = X (i,j)2\u2326 \u21e3 e>i X >ej e>i X\\X\\>ej 2 minimizeh,x f(h, x) = mX i=1 \u21e3 e>i XX >ej e>i X\\X\\> j 2\nfind X 2 Rn\u21e5r s.t. e>j XX>ei = e>j X\\X\\> i, (i, j) 2 \u2326\nfind h, x 2 Cn s.t. b\u21e4i hix\u21e4i ai = b\u21e4i h\\ix\\\u21e4i ai, 1  i  m max(r2f(x)) max(r2f(x))\na1 a2 x \\\na>1 (x x\\)\nkx x\\k2 . log n\na1 a2 x \\\na>2 (x x\\)\nkx x\\k2 . log n\nminimizex f(x) = 1 m mX i=1 (a>i x) 2 yi 2 s x 2 Rns.t. a>i x 2 = a>i x \\ 2 , 1  i  m minimizeX f(X) = X (i,j)2\u2326 \u21e3 e>i XX >ej e>i X\\X\\>ej 2 minimizeh,x f(h, x) = m\ni=1\n\u21e3 e>i XX >ej e>i X\\X\\>ej 2\nfind X 2 Rn\u21e5r s.t. e>j XX>ei = e>j X\\X\\>ei, (i, j) 2 \u2326\nfind h, x 2 Cn s.t. b\u21e4i hix\u21e4i ai = b\u21e4i h\\ix\\\u21e4i ai, 1  i  m max(r2f( )) max(r2f(x))\na1 a2 x \\\na>1 (x x\\)\nkx x\\k2 . log n\na1 a2 x \\\na>2 (x x\\)\nkx x\\k2 . log n\n1\na>1 (x x\\) .\np log n\na>2 (x x\\) .\np log n\n1\na>1 (x x\\) .\np log n\na>2 (x x\\) .\np log n\n1\n\u00b7x0 x1 x2 x3\n1\nx0 x1 x2 x3\n1\nx0 x1 x2 x3\n1\nx0 1 x2 x3\n1\nP\u21e5(\u27130)PX|\u21e5(x | \u27130) H0 > < H1 P\u21e5(\u27131)PX|\u21e5(x | \u27131) L(x) = PX|\u21e5(x | \u27131) PX|\u21e5(x | \u27130) H1 > < H0 \u21e0 H0 ! N (0, 1) H1 ! N (1, 1) L(x) = fX(x | H1) fX(x | H0) = 1p 2\u21e1 exp \u21e3 (x 1) 2 2 \u2318 1p 2\u21e1 exp x22 = exp \u2713 x 1 2 \u25c6 L(x) H1 > < H0 \u21e0 () x H1 > < H0 1 2 + log \u21e0 Pe,MAP = P\u21e5(\u27130)\u21b5 + P\u21e5(\u27131) \u21b5 S1,k = (Wk + c1Fk)e i 1,k\nS2,k = (Wk + c2Fk)e i 2,k\nS1,k = (Wk + c1Fk)e i 1,k S2,k = (Wk + c2Fk)e i 2,k , 8pixel k\nWk = f1(S1,k, S2,k)\nFk = f2(S1,k, S2,k)\nx\\\n1\n\u00b7x0 x1 x2 x3\n1\nx0 x1 x2 x3\n1\n0 1 2 x3\n1\nx0 x1 x2 x3\n1\nP\u21e5(\u27130)PX|\u21e5(x | \u27130) H0 > < H1 P\u21e5(\u27131)PX|\u21e5(x | \u27131) L(x) = PX|\u21e5(x | \u27131) PX|\u21e5(x | \u27130) H1 > < H0 \u21e0 H0 ! N (0, 1) H1 ! N (1, 1) L(x) = fX(x | H1) fX(x | H0) = 1p 2\u21e1 exp \u21e3 (x 1) 2 2 \u2318 1p 2\u21e1 exp x22 = exp \u2713 x 1 2 \u25c6 L(x) H1 > < H0 \u21e0 () x H1 > < H0 1 2 + log \u21e0 Pe,MAP = P\u21e5(\u27130)\u21b5 + P\u21e5(\u27131) \u21b5 S1,k = (Wk + c1Fk)e i 1,k\nS2,k = (Wk + c2Fk)e i 2,k\nS1,k = (Wk + c1Fk)e i 1,k S2,k = (Wk + c2Fk)e i 2,k , 8pixel k\nWk = f1(S1,k, S2,k)\nFk = f2(S1,k, S2,k)\nx\\\n1\n(a) (b) (c)\nFigure 2. (a) The shaded region is an illustration of the incoherence region, which satisfies \u2223\u2223a>j (x\u2212 x\\)\u2223\u2223 . \u221alogn for all points x in the region. (b) When x0 resides in the desired region, we know that x1 remains within the `2 ball but might fall out of the incoherence region (the shaded region). Once x1 leaves the incoherence region, we lose control and may overshoot. (c) Our theory reveals that with high probability, all iterates will stay within the incoherence region, enabling fast convergence.\n0 5 10 15 20 25 30 0\n0.5\n1\n1.5\n2\n2.5\ngradient iterates vs. iteration count for the phase retrieval problem. The results are shown for n \u2208 {20, 100, 200, 1000} and m = 10n, with the step size taken to be \u03b7t = 0.1. The problem instances are generated in the same way as in Figure 1.\nin Fig. 3 the incoherence measure maxj|a>j (xt\u2212x\\)|\u221a\nlogn\u2016x\\\u20162 vs. the\niteration count in a typical Monte Carlo trial, generated in the same way as for Figure 1. Interestingly, the incoherence measure remains bounded by 2 for all iterations t > 1. This important observation suggests that one may adopt a substantially more aggressive step size throughout the whole algorithm. The main objective of this paper is thus to provide a theoretical validation of the above empirical observation. As we will demonstrate shortly, with high probability all iterates throughout the execution of the algorithm (as well as the spectral initialization) are provably constrained within the RIC, implying fast convergence of vanilla gradient descent (cf. Figure 2(c)). The fact that the iterates stay incoherent with the measurement mechanism automatically, without explicit enforcement, is termed \u201cimplicit regularization\u201d in the current work."}, {"heading": "2.5. A Glimpse of the Analysis: A Leave-one-out Trick", "text": "In order to rigorously establish (9b) for all iterates, the current paper develops a powerful mechanism based on the leave-one-out perturbation argument, a trick rooted and widely used in probability and random matrix theory (El Karoui, 2015; Javanmard & Montanari, 2015; Sur et al.,\n2017; Zhong & Boumal, 2017; Chen et al., 2017; Abbe et al., 2017). Note that the iterate xt is statistically dependent of the design vectors {aj}. Under such circumstances, one often resorts to generic bounds like the Cauchy-Schwarz inequality when bounding a>l (x\nt \u2212 x\\), which would not yield a desirable estimate. To address this issue, we introduce a sequence of auxiliary iterates {xt,(l)} for each 1 \u2264 l \u2264 m (for analytical purposes only), obtained by running vanilla gradient descent using all but the lth sample. As one expects, such auxiliary trajectories serve as extremely good surrogates of {xt} in the sense that\nxt \u2248 xt,(l), 1 \u2264 l \u2264 m, t \u2265 0, (10) since their constructions only differ by a single sample. Most importantly, since xt,(l) is statistically independent of the lth design vector, it is much easier to control its incoherence w.r.t. al to the desired level:\u2223\u2223a>l ( xt,(l) \u2212 x\\ )\u2223\u2223 . \u221a log n \u2225\u2225x\\ \u2225\u2225 2 . (11)\nCombining (10) and (11) then leads to (9b). See Figure 4 for a graphical illustration of this argument."}, {"heading": "3. Main Results", "text": "This section formalizes the implicit regularization phenomenon underlying unregularized GD, and presents its consequences, namely near-optimal statistical and computational guarantees for phase retrieval and matrix completion. The complete proofs can be found in (Ma et al., 2017)."}, {"heading": "3.1. Solving Random Quadratic Systems / Phase Retrieval", "text": "Suppose the m quadratic equations\nyj = ( a>j x \\ )2 , j = 1, 2, . . . ,m (12)\nare collected using random design vectors, namely, aj i.i.d.\u223c N (0, In), and the nonconvex problem to solve is\nminimizex\u2208Rn f(x) := 1\n4m\nm\u2211\nj=1\n[( a>j x )2 \u2212 yj ]2 . (13)\nImplicit Regularization in Nonconvex Statistical Estimation\nA\nx\nAx\n1\n1 -3 2 -1 4\n-2 -1 3 4\n1 9 4 1 16\n4 1 9 16\nAx y = |Ax|2\n1\nAx y = |Ax|2\n1\n\u00b7\nminimizex f(x) = 1 m mX i=1 (a>i x) 2 yi 2 s x 2 Rns.t. a>i x 2 = a>i x \\ 2 , 1  i  m\nminimizeX f(X) = X\n(i,j)2\u2326\n\u21e3 e>i XX >ej e>i X\\X\\>ej 2\nminimizeh,x f(h, x) = mX\ni=1\n\u21e3 e>i XX >ej e>i X\\X\\>ej 2\nfind X 2 Rn\u21e5r s.t. e>j XX>ei = e>j X\\X\\>ei, (i, j) 2 \u2326\nfind h, x 2 Cn s.t. b\u21e4i hix\u21e4i ai = b\u21e4i h\\ix\\\u21e4i ai, 1  i  m max(r2f(x)) max(r2f(x))\na1 a2 x \\\na>1 (x x\\)\nkx x\\k2 .\np log n\na1 a2 x \\\na>2 (x x\\)\nkx x\\k2 .\np log n\nincoherence region w.r.t. a1\n1\nminimizex f(x) = 1 m mX i=1 (a>i x) 2 yi 2 s x 2 Rns.t. a>i x 2 = a>i x \\ 2 , 1  i  m minimizeX f(X) = X (i,j)2\u2326 \u21e3 e>i XX >ej e>i X\\X\\>ej 2 minimizeh,x f(h, x) = mX i=1 \u21e3 e>i XX >ej e>i X\\X\\>ej 2 find X 2 Rn\u21e5r s.t. e>j XX>ei = e>j X\\X\\>ei, (i, j) 2 \u2326\nfind h, x 2 Cn s.t. b\u21e4i hix\u21e4i ai = b\u21e4i h\\ix\\\u21e4i ai, 1  i  m max(r2f(x)) max(r2f(x))\na1 a2 x \\\na>1 (x x\\)\nkx x\\k2 .\np log n\na1 a2 x \\\na>2 (x x\\)\nkx x\\k2 .\np log n\nincoherence region w.r.t. a1\n{xt,(1)} {xt}\n1\n{xt,(l)} al w.r.t. al\n1\n{xt,(l)} al w.r.t. al\n1\n{xt,(l)} al w.r.t. al\n1\n(a) (b)\nFigure 4. Illustration of the leave-one-out sequence w.r.t. al. (a) The sequence {xt,(l)}t\u22650 is constructed without using the lth sample. (b) Since the auxiliary sequence {xt,(l)} is constructed without using al, the leave-one-out iterates stay within the incoherence region w.r.t. al with high probability. Meanwhile, {xt} and {xt,(l)} are expected to remain close as their construction differ only in one sample.\nThe Wirtinger flow (WF) algorithm, first introduced in (Cande\u0300s et al., 2015), is a combination of spectral initialization and vanilla gradient descent; see Algorithm 1.\nAlgorithm 1 Wirtinger flow for phase retrieval Input: {aj}1\u2264j\u2264m and {yj}1\u2264j\u2264m. Spectral initialization: Let \u03bb1 (Y ) and x\u03030 be the leading eigenvalue and eigenvector of\nY = 1\nm \u2211m j=1 yjaja > j , (14)\nrespectively, and set x0 = \u221a \u03bb1 (Y ) /3 x\u0303\n0. Gradient updates: for t = 0, 1, 2, . . . , T \u2212 1 do\nxt+1 = xt \u2212 \u03b7t\u2207f ( xt ) . (15)\nRecognizing that the global phase/sign is unrecoverable from quadratic measurements, we introduce the `2 distance modulo the global phase as follows\ndist(x,x\\) := min { \u2016x\u2212 x\\\u20162, \u2016x + x\\\u20162 } . (16)\nOur finding is summarized in the following theorem. Theorem 1. Let x\\ \u2208 Rn be a fixed vector. Suppose aj\ni.i.d.\u223c N (0, In) for each 1 \u2264 j \u2264 m and m \u2265 c0n log n for some sufficiently large constant c0 > 0. Assume the learning rate obeys \u03b7t \u2261 \u03b7 = c1/ ( log n \u00b7 \u2016x0\u201622 ) for any sufficiently small constant c1 > 0. Then there exist some absolute constants 0 < \u03b5 < 1 and c2 > 0 such that with probability at least 1\u2212O ( mn\u22125 ) , the Wirtinger flow iterates (Algorithm 1) satisfy that for all t \u2265 0, dist(xt,x\\) \u2264 \u03b5(1\u2212 \u03b7\u2016x\\\u201622/2)t\u2016x\\\u20162, (17a)\nmax 1\u2264j\u2264m\n\u2223\u2223a>j ( xt \u2212 x\\ )\u2223\u2223 \u2264 c2 \u221a log n\u2016x\\\u20162. (17b)\nTheorem 1 reveals a few intriguing properties of WF.\n\u2022 Implicit regularization: Theorem 1 asserts that the incoherence properties are satisfied throughout the execution\nof the algorithm, including the spectral initialization (see (17b)), which formally justifies the implicit regularization feature we hypothesized. \u2022 Near-constant step size: Consider the case where \u2016x\\\u20162 = 1. Theorem 1 establishes near-linear convergence of WF with a substantially more aggressive step size \u03b7 1/ log n. Compared with the choice \u03b7 . 1/n admissible in (Cande\u0300s et al., 2015), Theorem 1 allows WF to attain -accuracy within O(log n log(1/ )) iterations. The resulting computational complexity of WF is O (mn log n log(1/ )) , which significantly improves upon the result O ( mn2 log (1/ ) ) derived in (Cande\u0300s\net al., 2015). As a side note, if the sample size further increases to m n log2 n, then \u03b7 1 is also feasible, resulting in an iteration complexity log(1/ ). This follows since with high probability, the entire trajectory resides within a more refined incoherence region maxj \u2223\u2223a>j ( xt \u2212x\\\n)\u2223\u2223 . \u2016x\\\u20162. We omit the details here. Finally, we remark that similar implicit regularization phenomenon holds even in the presence of random initialization. See (Chen et al., 2018) for details."}, {"heading": "3.2. Low-rank Matrix Completion", "text": "We move on to the low-rank matrix completion problem.\nLet M \\ \u2208 Rn\u00d7n be a positive semidefinite matrix2 with rank r, and suppose its eigendecomposition is\nM \\ = U \\\u03a3\\U \\>, (18)\nwhere U \\ \u2208 Rn\u00d7r consists of orthonormal columns, and \u03a3\\ is an r \u00d7 r diagonal matrix with eigenvalues in a descending order, i.e. \u03c3max = \u03c31 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3r = \u03c3min > 0. Throughout this paper, we assume the condition number \u03ba := \u03c3max/\u03c3min is bounded by a fixed constant, independent of the problem size (i.e. n and r). Denoting\n2Here, we assume M \\ to be positive semidefinite to simplify the presentation, but note that our analysis easily extends to asymmetric low-rank matrices.\nImplicit Regularization in Nonconvex Statistical Estimation\nAlgorithm 2 Vanilla gradient descent for matrix completion (with spectral initialization)\nInput: Y = [Yj,k]1\u2264j,k\u2264n, r, p. Spectral initialization: Let U0\u03a30U0> be the rank-r eigendecomposition of\nM0 := p\u22121P\u2126(Y ) = p\u22121P\u2126 ( M \\ + E ) ,\nand set X0 = U0 ( \u03a30 )1/2\n. Gradient updates: for t = 0, 1, 2, . . . , T \u2212 1 do\nXt+1 = Xt \u2212 \u03b7t\u2207f ( Xt ) . (21)\nX\\ = U \\(\u03a3\\)1/2 allows us to factorize M \\ as\nM \\ = X\\X\\>. (19)\nConsider a random sampling model such that each entry of M \\ is observed independently with probability 0 < p \u2264 1, i.e. for 1 \u2264 j \u2264 k \u2264 n,\nYj,k = { M \\j,k + Ej,k with probability p, 0, else,\n(20)\nwhere the entries of E = [Ej,k]1\u2264j\u2264k\u2264n are independent sub-Gaussian noise with sub-Gaussian norm \u03c3 (see (Vershynin, 2012)). We denote by \u2126 the set of locations being sampled, and P\u2126(Y ) represents the projection of Y onto the set of matrices supported in \u2126. We note here that the sampling rate p, if not known, can be faithfully estimated by the sample proportion |\u2126|/n2. To fix ideas, we consider the following nonconvex optimization problem\nminimize X\u2208Rn\u00d7r\nf (X) := 1\n4p\n\u2211\n(j,k)\u2208\u2126\n( e>j XX >ek \u2212 Yj,k )2 .\nThe vanilla gradient descent algorithm (with spectral initialization) is summarized in Algorithm 2.\nBefore proceeding to the main theorem, we first introduce a standard incoherence parameter required for matrix completion (Cande\u0300s & Recht, 2009). Definition 1 (Incoherence for matrix completion). A rank-r matrix M \\ with eigendecomposition M \\ = U \\\u03a3\\U \\> is said to be \u00b5-incoherent if\n\u2225\u2225U \\ \u2225\u2225 2,\u221e \u2264 \u221a \u00b5/n \u2225\u2225U \\ \u2225\u2225 F = \u221a \u00b5r/n, (22)\nwhere \u2016 \u00b7 \u20162,\u221e denotes the largest `2 norm of the rows.\nIn addition, recognizing that X\\ is identifiable only up to orthogonal transformation, we define the optimal transform from the tth iterate Xt to X\\ as\nH\u0302t := argmin R\u2208Or\u00d7r\n\u2225\u2225XtR\u2212X\\ \u2225\u2225\nF , (23)\nwhere Or\u00d7r is the set of r \u00d7 r orthonormal matrices. With these definitions in place, we have the following theorem.\nTheorem 2. Let M \\ be a rank r, \u00b5-incoherent PSD matrix, and its condition number \u03ba is a fixed constant. Suppose the sample size satisfies n2p \u2265 C\u00b53r3n log3 n for some sufficiently large constant C > 0, and the noise satisfies\n\u03c3\n\u221a n\np \u03c3min\u221a \u03ba3\u00b5r log3 n . (24)\nWith probability at least 1\u2212O ( n\u22123 ) , the iterates of Algorithm 2 satisfy\n\u2225\u2225XtH\u0302t \u2212X\\\u2225\u2225 F \u2264 ( C4\u03c1 t\u00b5r 1 \u221a np + C1\u03c3 \u03c3min \u221a n p )\u2225\u2225X\\\u2225\u2225 F ,\n\u2225\u2225XtH\u0302t \u2212X\\\u2225\u2225 2,\u221e \u2264 ( C5\u03c1 t\u00b5r \u221a logn np + C8\u03c3 \u03c3min \u221a n logn p ) \u00b7 \u2225\u2225X\\\u2225\u2225\n2,\u221e,\u2225\u2225XtH\u0302t \u2212X\\\u2225\u2225 \u2264 (C9\u03c1t\u00b5r 1\u221a np + C10\u03c3 \u03c3min \u221a n p )\u2225\u2225X\\\u2225\u2225 for all 0 \u2264 t \u2264 T = O(n5),3 where C1, C4, C5, C8, C9 and C10 are some absolute positive constants and 1\u2212 (\u03c3min/5) \u00b7 \u03b7 \u2264 \u03c1 < 1, provided that 0 < \u03b7t \u2261 \u03b7 \u2264 2/ (25\u03ba\u03c3max).\nTheorem 2 provides the first theoretical guarantee of unregularized gradient descent for matrix completion, demonstrating near-optimal statistical accuracy and computational complexity, under near-minimal sample complexity.\n\u2022 Implicit regularization: In Theorem 2, we bound the `2/`\u221e error of the iterates in a uniform manner. Note that \u2225\u2225X \u2212X\\ \u2225\u2225 2,\u221e = maxj \u2225\u2225e>j ( X \u2212X\\ )\u2225\u2225 2 , which\nimplies the iterates remain incoherent with the sensing vectors throughout and have small incoherence parameters, including the spectral initialization (cf. (22)). In comparison, prior works either include a penalty term on {\u2016e>j X\u20162}1\u2264j\u2264n (Keshavan et al., 2010; Sun & Luo, 2016) and/or \u2016X\u2016F (Sun & Luo, 2016) to encourage an incoherent and/or low-norm solution, or add an extra projection operation to enforce incoherence (Chen & Wainwright, 2015; Zheng & Lafferty, 2016). Our results demonstrate that such explicit regularization is unnecessary for the success of gradient descent. \u2022 Constant step size: Without loss of generality we may assume that \u03c3max = \u2016M \\\u2016 = O(1), which can be done by choosing proper scaling of M \\. Hence we have a constant step size \u03b7t 1. Actually it is more convenient to consider the scale invariant parameter \u03c1: Theorem 2 guarantees linear convergence of the vanilla gradient descent at a constant rate \u03c1. Remarkably, the convergence\n3Theorem 2 remains valid if the total number T of iterations obeys T = nO(1). In the noiseless case where \u03c3 = 0, the theory allows arbitrarily large T .\nImplicit Regularization in Nonconvex Statistical Estimation\noccurs with respect to three different unitarily invariant norms: the Frobenius norm \u2016\u00b7\u2016F, the `2/`\u221e norm \u2016\u00b7\u20162,\u221e, and the spectral norm \u2016 \u00b7 \u2016. As far as we know, the latter two are established for the first time. Note that our result even improves upon that for regularized GD; see Table 1. \u2022 Near-minimal Euclidean error: As the number of iterations t increases, the Euclidean error of vanilla GD converges to\n\u2225\u2225XtH\u0302t \u2212X\\ \u2225\u2225\nF . \u03c3 \u03c3min\n\u221a n\np\n\u2225\u2225X\\ \u2225\u2225\nF , (25)\nwhich coincides with the theoretical guarantee in (Chen & Wainwright, 2015) and matches the minimax lower bound established in (Negahban & Wainwright, 2012; Koltchinskii et al., 2011). \u2022 Near-optimal entrywise error: The `2/`\u221e error bound immediately yields entrywise control of the empirical risk. Specifically, as soon as the number of iterations t is sufficiently large, we have\n\u2225\u2225XtXt> \u2212M \\ \u2225\u2225 \u221e . \u03c3\n\u03c3min\n\u221a n log n\np\n\u2225\u2225M \\ \u2225\u2225 \u221e .\nCompared with the Euclidean loss (25), this implies that when r = O(1), the entrywise error of XtXt> is uniformly spread out across all entries. As far as we know, this is the first result that reveals near-optimal entrywise error control for noisy matrix completion using nonconvex optimization, without resorting to sample splitting."}, {"heading": "4. Related Work", "text": "Convex relaxations have received much attention for solving nonlinear systems of equations in the past decade. Instead of directly attacking the nonconvex formulation, convex relaxation lifts the object of interest into a higher dimensional space and then attempts recovery via semidefinite programming (e.g. (Recht et al., 2010; Cande\u0300s et al., 2013; Cande\u0300s & Recht, 2009)). This has enjoyed great success in both theory and practice. Despite appealing statistical guarantees, SDP is in general prohibitively expensive when processing large-scale datasets.\nIn comparison, nonconvex approaches have been under extensive study in the last few years, due to their computational advantages. There is a growing list of statistical estimation problems for which nonconvex approaches are guaranteed to find global optimal solutions, including but not limited to phase retrieval (Netrapalli et al., 2013; Cande\u0300s et al., 2015; Chen & Cande\u0300s, 2017), low-rank matrix sensing and completion (Tu et al., 2016; Bhojanapalli et al., 2016; Park et al., 2016; Chen & Wainwright, 2015; Zheng & Lafferty, 2015; Ge et al., 2016), dictionary learning (Sun et al., 2017), blind deconvolution (Li et al., 2016a; Cambareri & Jacques, 2016; Lee et al., 2017), tensor decomposition (Ge & Ma, 2017), joint alignment (Chen & Cande\u0300s, 2018), learning shallow\nneural networks (Soltanolkotabi et al., 2017; Zhong et al., 2017). In several problems (Sun et al., 2016; 2017; Ge & Ma, 2017; Ge et al., 2016; Li et al., 2016b; Li & Tang, 2016; Mei et al., 2016; Maunu et al., 2017), it is further suggested that the optimization landscape is benign under sufficiently large sample complexity, in the sense that all local minima are globally optimal, and hence nonconvex iterative algorithms become promising in solving such problems.\nWhen it comes to noisy matrix completion, to the best of our knowledge, no rigorous guarantees have been established for gradient descent without explicit regularization. A notable exception is (Jin et al., 2016), which studies unregularized stochastic gradient descent for online matrix completion with fresh samples used in each iteration.\nFinally, we note that the notion of implicit regularization \u2014 broadly defined \u2014 arises in settings far beyond what considered herein. For instance, it has been in matrix factorization, over-parameterized stochastic gradient descent effectively enforces certain norm constraints, allowing it to converge to a minimal-norm solution as long as it starts from the origin (Li et al., 2017; Gunasekar et al., 2017). The stochastic gradient methods have also been shown to implicitly enforce Tikhonov regularization in several statistical learning settings (Lin et al., 2016). More broadly, this phenomenon seems crucial in enabling efficient training of deep neural networks (Neyshabur et al., 2017; Zhang et al., 2016a; Soudry et al., 2017; Keskar et al., 2016)."}, {"heading": "5. Discussions", "text": "This paper showcases an important phenomenon in nonconvex optimization: even without explicit enforcement of regularization, the vanilla form of gradient descent effectively achieves implicit regularization for a large family of statistical estimation problems. We believe this phenomenon arises in problems far beyond the two cases studied herein, and our results are initial steps towards understanding this fundamental phenomenon. That being said, there are numerous avenues that remain open. For instance, it remains unclear how to generalize the proposed leave-one-out tricks for more general designs beyond the i.i.d. Gaussian design. It would also be interesting to see whether the message conveyed in this paper can shed light on why simple forms of gradient descent and variants work so well in learning complicated neural networks. We leave these for future investigation."}, {"heading": "Acknowledgements", "text": "Y. Chi is supported in part by the grants AFOSR FA9550-151-0205, ONR N00014-18-1-2142, ARO grant W911NF-181-0303, NSF CCF-1527456 and ECCS-1818571. Y. Chen is supported by ARO grant W911NF-18-1-0303 and by Princeton SEAS innovation award.\nImplicit Regularization in Nonconvex Statistical Estimation"}], "year": 2018, "references": [{"title": "Entrywise eigenvector analysis of random matrices with low expected rank", "authors": ["E. Abbe", "J. Fan", "K. Wang", "Y. Zhong"], "venue": "arXiv preprint arXiv:1709.09565,", "year": 2017}, {"title": "Global optimality of local search for low rank matrix recovery", "authors": ["S. Bhojanapalli", "B. Neyshabur", "N. Srebro"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2016}, {"title": "A non-convex blind calibration method for randomised sensing strategies", "authors": ["V. Cambareri", "L. Jacques"], "venue": "arXiv preprint arXiv:1605.02615,", "year": 2016}, {"title": "Exact matrix completion via convex optimization", "authors": ["E.J. Cand\u00e8s", "B. Recht"], "venue": "Foundations of Computational Mathematics,", "year": 2009}, {"title": "Phaselift: Exact and stable signal recovery from magnitude measurements via convex programming", "authors": ["E.J. Cand\u00e8s", "T. Strohmer", "V. Voroninski"], "venue": "Communications on Pure and Applied Mathematics,", "year": 2013}, {"title": "Phase retrieval via Wirtinger flow: Theory and algorithms", "authors": ["E.J. Cand\u00e8s", "X. Li", "M. Soltanolkotabi"], "venue": "IEEE Transactions on Information Theory,", "year": 2015}, {"title": "The projected power method: An efficient algorithm for joint alignment from pairwise differences", "authors": ["Y. Chen", "E. Cand\u00e8s"], "venue": "Communications on Pure and Applied Mathematics,", "year": 2018}, {"title": "Solving random quadratic systems of equations is nearly as easy as solving linear systems", "authors": ["Y. Chen", "E.J. Cand\u00e8s"], "venue": "Communications on Pure and Applied Mathematics,", "year": 2017}, {"title": "Fast low-rank estimation by projected gradient descent: General statistical and algorithmic guarantees", "authors": ["Y. Chen", "M.J. Wainwright"], "year": 2015}, {"title": "Spectral method and regularized MLE are both optimal for top-K ranking", "authors": ["Y. Chen", "J. Fan", "C. Ma", "K. Wang"], "venue": "Annals of Statistics,", "year": 2017}, {"title": "Gradient descent with random initialization: Fast global convergence for nonconvex phase retrieval", "authors": ["Y. Chen", "Y. Chi", "J. Fan", "C. Ma"], "venue": "arXiv preprint arXiv:1803.07726,", "year": 2018}, {"title": "On the impact of predictor geometry on the performance on high-dimensional ridge-regularized generalized robust regression estimators", "authors": ["N. El Karoui"], "venue": "Probability Theory and Related Fields, pp", "year": 2015}, {"title": "On the optimization landscape of tensor decompositions", "authors": ["R. Ge", "T. Ma"], "venue": "arXiv preprint arXiv:1706.05598,", "year": 2017}, {"title": "Matrix completion has no spurious local minimum", "authors": ["R. Ge", "J.D. Lee", "T. Ma"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2016}, {"title": "Implicit regularization in matrix factorization", "authors": ["S. Gunasekar", "B. Woodworth", "S. Bhojanapalli", "B. Neyshabur", "N. Srebro"], "venue": "arXiv preprint arXiv:1705.09280,", "year": 2017}, {"title": "Phase retrieval: An overview of recent developments", "authors": ["K. Jaganathan", "Y.C. Eldar", "B. Hassibi"], "venue": "arXiv preprint arXiv:1510.07713,", "year": 2015}, {"title": "Low-rank matrix completion using alternating minimization", "authors": ["P. Jain", "P. Netrapalli", "S. Sanghavi"], "venue": "In ACM symposium on Theory of computing,", "year": 2013}, {"title": "De-biasing the lasso: Optimal sample size for Gaussian designs", "authors": ["A. Javanmard", "A. Montanari"], "venue": "arXiv preprint arXiv:1508.02757,", "year": 2015}, {"title": "Provable efficient online matrix completion via non-convex stochastic gradient descent", "authors": ["C. Jin", "S.M. Kakade", "P. Netrapalli"], "venue": "In NIPS,", "year": 2016}, {"title": "Matrix completion from a few entries", "authors": ["R.H. Keshavan", "A. Montanari", "S. Oh"], "venue": "IEEE Transactions on Information Theory,", "year": 2010}, {"title": "On large-batch training for deep learning: Generalization gap and sharp minima", "authors": ["N.S. Keskar", "D. Mudigere", "J. Nocedal", "M. Smelyanskiy", "P.T.P. Tang"], "venue": "arXiv preprint arXiv:1609.04836,", "year": 2016}, {"title": "Nuclearnorm penalization and optimal rates for noisy low-rank matrix completion", "authors": ["V. Koltchinskii", "K. Lounici", "A.B. Tsybakov"], "venue": "Ann. Statist.,", "year": 2011}, {"title": "Blind recovery of sparse signals from subsampled convolution", "authors": ["K. Lee", "Y. Li", "M. Junge", "Y. Bresler"], "venue": "IEEE Transactions on Information Theory,", "year": 2017}, {"title": "The nonconvex geometry of low-rank matrix optimizations with general objective functions", "authors": ["Q. Li", "G. Tang"], "venue": "arXiv preprint arXiv:1611.03060,", "year": 2016}, {"title": "Rapid, robust, and reliable blind deconvolution via nonconvex optimization. CoRR, abs/1606.04933, 2016a", "authors": ["X. Li", "S. Ling", "T. Strohmer", "K. Wei"], "venue": "URL http://arxiv. org/abs/1606.04933", "year": 2016}, {"title": "Symmetry, saddle points, and global geometry of nonconvex matrix factorization", "authors": ["X. Li", "Z. Wang", "J. Lu", "R. Arora", "J. Haupt", "H. Liu", "T. Zhao"], "venue": "arXiv preprint arXiv:1612.09296,", "year": 2016}, {"title": "Algorithmic regularization in over-parameterized matrix recovery", "authors": ["Y. Li", "T. Ma", "H. Zhang"], "venue": "pp. arXiv preprint arXiv:1712.09203,", "year": 2017}, {"title": "Generalization properties and implicit regularization for multiple passes SGM", "authors": ["J. Lin", "R. Camoriano", "L. Rosasco"], "venue": "In International Conference on Machine Learning,", "year": 2016}, {"title": "Implicit regularization in nonconvex statistical estimation: Gradient descent converges linearly for phase retrieval, matrix completion and blind deconvolution", "authors": ["C. Ma", "K. Wang", "Y. Chi", "Y. Chen"], "venue": "arXiv preprint arXiv:1711.10467,", "year": 2017}, {"title": "A well-tempered landscape for non-convex robust subspace recovery", "authors": ["T. Maunu", "T. Zhang", "G. Lerman"], "venue": "arXiv preprint arXiv:1706.03896,", "year": 2017}, {"title": "The landscape of empirical risk for non-convex losses", "authors": ["S. Mei", "Y. Bai", "A. Montanari"], "venue": "arXiv preprint arXiv:1607.06534,", "year": 2016}, {"title": "Restricted strong convexity and weighted matrix completion: optimal bounds with noise", "authors": ["S. Negahban", "M.J. Wainwright"], "venue": "J. Mach. Learn. Res.,", "year": 2012}, {"title": "Phase retrieval using alternating minimization", "authors": ["P. Netrapalli", "P. Jain", "S. Sanghavi"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "year": 2013}, {"title": "Geometry of optimization and implicit regularization in deep learning", "authors": ["B. Neyshabur", "R. Tomioka", "R. Salakhutdinov", "N. Srebro"], "venue": "arXiv preprint arXiv:1705.03071,", "year": 2017}, {"title": "Non-square matrix sensing without spurious local minima via the Burer-Monteiro approach", "authors": ["D. Park", "A. Kyrillidis", "C. Caramanis", "S. Sanghavi"], "venue": "arXiv preprint arXiv:1609.03240,", "year": 2016}, {"title": "Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization", "authors": ["B. Recht", "M. Fazel", "P.A. Parrilo"], "venue": "SIAM Review,", "year": 2010}, {"title": "Algorithms and Theory for Clustering and Nonconvex Quadratic Programming", "authors": ["M. Soltanolkotabi"], "venue": "PhD thesis, Stanford University,", "year": 2014}, {"title": "Theoretical insights into the optimization landscape of overparameterized shallow neural networks", "authors": ["M. Soltanolkotabi", "A. Javanmard", "J.D. Lee"], "venue": "arXiv preprint arXiv:1707.04926,", "year": 2017}, {"title": "The implicit bias of gradient descent on separable data", "authors": ["D. Soudry", "E. Hoffer", "N. Srebro"], "venue": "arXiv preprint arXiv:1710.10345,", "year": 2017}, {"title": "A geometric analysis of phase retrieval", "authors": ["J. Sun", "Q. Qu", "J. Wright"], "venue": "In ISIT,", "year": 2016}, {"title": "Complete dictionary recovery over the sphere i: Overview and the geometric picture", "authors": ["J. Sun", "Q. Qu", "J. Wright"], "venue": "IEEE Transactions on Information Theory,", "year": 2017}, {"title": "Guaranteed matrix completion via non-convex factorization", "authors": ["R. Sun", "Luo", "Z.-Q"], "venue": "IEEE Transactions on Information Theory,", "year": 2016}, {"title": "The likelihood ratio test in high-dimensional logistic regression is asymptotically a rescaled chi-square", "authors": ["P. Sur", "Y. Chen", "E.J. Cand\u00e8s"], "venue": "arXiv preprint arXiv:1706.01191,", "year": 2017}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "authors": ["R. Vershynin"], "venue": "Compressed Sensing, Theory and Applications, pp", "year": 2012}, {"title": "Solving systems of random quadratic equations via truncated amplitude flow", "authors": ["G. Wang", "G.B. Giannakis", "Y.C. Eldar"], "venue": "IEEE Transactions on Information", "year": 2017}, {"title": "The local convexity of solving quadratic equations", "authors": ["C.D. White", "R. Ward", "S. Sanghavi"], "venue": "arXiv preprint arXiv:1506.07868,", "year": 2015}, {"title": "Understanding deep learning requires rethinking generalization", "authors": ["C. Zhang", "S. Bengio", "M. Hardt", "B. Recht", "O. Vinyals"], "venue": "arXiv preprint arXiv:1611.03530,", "year": 2016}, {"title": "Provable non-convex phase retrieval with outliers: Median truncated Wirtinger flow", "authors": ["H. Zhang", "Y. Chi", "Y. Liang"], "venue": "In International conference on machine learning,", "year": 2016}, {"title": "A convergent gradient descent algorithm for rank minimization and semidefinite programming from random linear measurements", "authors": ["Q. Zheng", "J. Lafferty"], "venue": "In NIPS, pp", "year": 2015}, {"title": "Convergence analysis for rectangular matrix completion using Burer-Monteiro factorization and gradient descent", "authors": ["Q. Zheng", "J. Lafferty"], "venue": "arXiv preprint arXiv:1605.07051,", "year": 2016}, {"title": "Recovery guarantees for one-hidden-layer neural networks", "authors": ["K. Zhong", "Z. Song", "P. Jain", "P.L. Bartlett", "I.S. Dhillon"], "year": 2017}, {"title": "Near-optimal bounds for phase synchronization", "authors": ["Y. Zhong", "N. Boumal"], "venue": "arXiv preprint arXiv:1703.06605,", "year": 2017}], "id": "SP:1e8e4468c5b2f3caa04234b7fa7f31a20aa8825d", "authors": [{"name": "Cong Ma", "affiliations": []}, {"name": "Kaizheng Wang", "affiliations": []}, {"name": "Yuejie Chi", "affiliations": []}, {"name": "Yuxin Chen", "affiliations": []}], "abstractText": "Recent years have seen a flurry of activities in designing provably efficient nonconvex optimization procedures for solving statistical estimation problems. For various problems like phase retrieval or low-rank matrix completion, state-of-the-art nonconvex procedures require proper regularization (e.g. trimming, regularized cost, projection) in order to guarantee fast convergence. When it comes to vanilla procedures such as gradient descent, however, prior theory either recommends highly conservative learning rates to avoid overshooting, or completely lacks performance guarantees. This paper uncovers a striking phenomenon in several nonconvex problems: even in the absence of explicit regularization, gradient descent follows a trajectory staying within a basin that enjoys nice geometry, consisting of points incoherent with the sampling mechanism. This \u201cimplicit regularization\u201d feature allows gradient descent to proceed in a far more aggressive fashion without overshooting, which in turn results in substantial computational savings. Focusing on two statistical estimation problems, i.e. solving random quadratic systems of equations and low-rank matrix completion, we establish that gradient descent achieves near-optimal statistical and computational guarantees without explicit regularization. As a byproduct, for noisy matrix completion, we demonstrate that gradient descent enables optimal control of both entrywise and spectral-norm errors. Department of Operations Research and Financial Engineering, Princeton University, Princeton, NJ 08544, USA Department of Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh, PA 15213, USA Department of Electrical Engineering, Princeton University, Princeton, NJ 08544, USA. Correspondence to: Cong Ma <congm@princeton.edu>. Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).", "title": "Implicit Regularization in Nonconvex Statistical Estimation: Gradient Descent Converges Linearly for Phase Retrieval and Matrix Completion"}