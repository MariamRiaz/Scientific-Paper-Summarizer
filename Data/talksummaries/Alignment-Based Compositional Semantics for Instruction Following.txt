47	70	We do so by presenting the model with a sequence of training pairs (x,y), where each x is a sequence of natural language instructions (x1, x2, .
48	170	, xm), e.g.: (Go down the yellow hall., Turn left., .
49	78	and each y is a demonstrated action sequence (y1, y2, .
50	59	, yn), e.g.: (rotate(90), move(2), .
51	168	Given a start state, y can equivalently be characterized by a sequence of (state, action, state) triples resulting from execution of the environment model.
53	34	An example action, situated in the environment where it occurs, is shown in Figure 2e.
55	51	Thus we interpret xi and yj not as raw strings and primitive actions, but rather as structured objects.
71	27	Indeed, if L is the set of all atomic entities and relations, fV returns a unique label for every v ∈ V , and fE always returns a vector with one active feature, we recover the existentially-quantified portion of first order logic exactly, and in this form can implement large parts of classical neo-Davidsonian semantics (Parsons, 1990) using grounding graphs.
72	49	Crucially, with an appropriate choice of L this formalism also makes it possible to go beyond settheoretic relations, and incorporate string-valued features (like names of entities and landmarks) and real-valued features (like colors and positions) as well.
91	41	We take ψ(n; θ) and ψ(y; θ) to be linear functions of θ.
101	30	As structure is essential for some of our tasks, ψ(x, y) must instead fill the role of a semantic parser in a conventional compositional model.
116	45	Let xi be the ith word of the sentence, and let yj be the jth node in the action graph (under some topological ordering).
124	27	Ideally, we would like Algorithm 1 Computing structure-to-structure alignments xi are words in reverse topological order yj are grounding graph nodes (root last) chart is an m× n array for i = 1 to |x| do for j = 1 to |y| do score← exp{θ>φ(xi, yj)} for (k, l) ∈ d(i, j) do s←∑l∈c(j) [ exp{θ>φ(xik, yjl)} · chart[k, l] ] score← score · s end for chart[i, j]← score end for end for return chart[n,m] to sum over the latent variables, but that sum is intractable.
127	43	Conditioned on a, the sum over structure-to-structure ψ(x, y) = ∑ b ψ(x, y, b) can be performed exactly using a simple dynamic program which runs in time O(|x||y|) (assuming out-degree bounded by a constant, and with |x| and |y| the number of words and graph nodes respectively).
131	153	Thus we make an additional approximation, constructing a set Ỹ of alternative actions and taking p(y,a|x) ≈ n∑ j=1 exp { ψ(yj)+ ∑m i=1 1[ai=j]ψ(xi,yi) } ∑ ỹ∈Ỹ exp { ψ(ỹ)+ ∑m i=1 1[ai=j]ψ(xi,ỹ) } Ỹ is constructed by sampling alternative actions from the environment model.
134	61	We again perform iterated conditional modes, here on the alignments a and the unknown output path y. Maximization of a is accomplished with the Viterbi algorithm, exactly as before; maximization of y also uses the Viterbi algorithm, or a beam search when this is computationally infeasible.
144	28	This map is accompanied by a set of instructions specifying a path from the starting position to some (unlabeled) destination point.
145	36	These instruction sets are informal and redundant, involving as many as a hundred utterances.
153	39	We can also see pragmatics at work: the model learns useful text-independent constraints—in this case, that near destinations should be preferred to far ones.
157	42	Despite superficial similarity to the previous navigation task, the language and plans required for this task are quite different.
158	48	The proportion of instructions to actions is much higher (so redundancy much lower), and the interpretation of language is highly compositional.
162	56	Recent work by Kim and Mooney (2013) and Artzi et al. (2014) has achieved better results; these systems make use of techniques and resources (respectively, discriminative reranking and a seed lexicon of handannotated logical forms) that are largely orthogonal to the ones used here, and might be applied to improve our own results as well.
167	33	Here it is nontrivial to find any sequence that eliminates all the blocks (the goal of the puzzle).
172	33	Thus it can be seen that text induces a useful heuristic, allowing the model to solve a considerable fraction of problem instances not solved by naı̈ve beam search.
173	164	The problem of inducing planning heuristics from side information like text is an important one in its own right, and future work might focus specifically on coupling our system with a more sophisticated planner.
179	92	A more comprehensive solution might explicitly describe the process by which instruction-givers’ own beliefs (expressed as distributions over sequences) give rise to instructions.
180	110	Compositional semantics The graph alignment model of semantics presented here is an expressive and computationally efficient generalization of classical logical techniques to accommodate environments like the map task, or those explored in our previous work (Andreas and Klein, 2014).
181	53	More broadly, our model provides a compositional approach to semantics that does not require an explicit formal language for encoding sentence meaning.
182	92	Future work might extend this approach to tasks like question answering, where logicbased approaches have been successful.
183	37	Our primary goal in this paper has been to explore methods for integrating compositional semantics and the pragmatic context provided by sequential structures.
