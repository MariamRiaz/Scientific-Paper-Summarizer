3	111	Discourse phenomenon such as pronominal anaphora and lexical consistency, may depend on long-range dependency going farther than a few previous sentences, are neglected in sentencebased translation (Bawden et al., 2017).
4	17	There are only a handful of attempts to document-wide machine translation in statistical and neural MT camps.
6	28	More recently, there have been a few attempts to incorporate source side context into neural MT (Jean et al., 2017; Wang et al., 2017; Bawden et al., 2017); however, these works only consider a very local context including a few previous source/target sentences, ignoring the global source and target documental contexts.
8	34	In this paper, we present a document-level machine translation model which combines sentencebased NMT (Bahdanau et al., 2015) with memory networks (Sukhbaatar et al., 2015).
9	14	We capture the global source and target document context with two memory components, one each for the source and target side, and incorporate it into the sentence-based NMT by changing the decoder to condition on it as the sentence translation is generated.
12	23	Our document NMT model is grounded on sentence-based NMT model (Bahdanau et al., 2015) which contains an encoder to read the source sentence as well as an attentional decoder to generate the target translation.
13	15	Encoder It is a bidirectional RNN consisting of two RNNs running in opposite directions over the source sentence: −→ hi = −−→ RNN( −→ h i−1,ES [xi]), ←− h i = ←−− RNN( ←− h i+1,ES [xi]) where ES [xi] is embedding of the word xi from the embedding table ES of the source language, and −→ h i and ←− h i are the hidden states of the forward and backward RNNs which can be based on the LSTM (Hochreiter and Schmidhuber, 1997) or GRU (Cho et al., 2014) units.
15	23	Decoder The generation of each word yj is conditioned on all of the previously generated words y<j via the state of the RNN decoder sj , and the source sentence via a dynamic context vector cj : yj ∼ softmax(Wy · rj + br) rj = tanh(sj +Wrc · cj +Wrj ·ET [yj−1]) sj = tanh(Ws · sj−1 +Wsj ·ET [yj−1] +Wsc · cj) where ET [yj ] is embedding of the word yj from the embedding table ET of the target language, and W matrices and br vector are the parameters.
26	12	We achieve this by the factor graph in Figure 1 to model the probability of the target document given the source document.
28	16	Hence, the probability of a document translation given the source document is P (y1, .
34	17	Decoding To generate the best translation for a document according to our model, we need to solve the following optimisation problem: arg max y1,...,y|d| |d|∏ t=1 Pθ(yt|xt,y−t,x−t) which is hard (due to similar reasons as mentioned earlier).
36	21	More specifically, we initialise the translation of each sentence using the base neural MT model P (yt|xt).
37	49	We then repeatedly visit each sentence in the document, and update its translation using our document-context dependent NMT model P (yt|xt,y−t,x−t) while the translations of other sentences are kept fixed.
38	15	We augment the sentence-level attentional NMT model by incorporating the document context (both source and target) using memory networks when generating the translation of a sentence, as shown in Figure 2.
39	16	Our model generates the target translation word-by-word from left to right, similar to the vanilla attentional neural translation model.
46	61	We make use of ht as the query to get the relevant context from the source external memory: csrct = MemNet(M [x−t],ht) Furthermore, for the t-th sentence, we get the relevant information from the target context: ctrgt = MemNet(M [y−t], st +Wat · ht) where the query consists of the representation of the translation st from the decoder endowed with that of the source sentence ht from the encoder to make the query robust to potential noises in the current translation and circumvent error propagation, and Wat projects the source representation into the hidden state space.
48	44	2 can be re-written as: Pθ(yt|xt,y−t,x−t) = |yt|∏ j=1 Pθ(yt,j |yt,<j ,xt, ctrgt , c src t ) (3) More specifically, the memory contexts csrct and ctrgt are incorporated into the NMT decoder as: • Memory-to-Context in which the memory contexts are incorporated when computing the next decoder hidden state: st,j = tanh(Ws · st,j−1 +Wsj ·ET [yt,j ] + Wsc · ct,j +Wsm · csrct +Wst · c trg t ) • Memory-to-Output in which the memory contexts are incorporated in the output layer: yt,j ∼ softmax(Wy · rt,j +Wym · csrct + Wyt · ctrgt + br) where Wsm, Wst, Wym, and Wyt are the new parameter matrices.
49	12	We use only the source, only the target, or both external memories as the additional conditioning contexts.
67	36	We conducted experiments on three language pairs: French-English, German-English and Estonian-English.
116	83	For the Memory-to-Context model, we see massive improvements of +0.72 and +1.44 METEOR scores for the source memory and dual memory model respectively, when compared to the baseline.
129	25	From Table 6, we observe that for French→English and Estonian→English, using all sentences in the target context or just the previous target sentence gives comparable results.
133	22	It can be seen that the source sentence has the noun “Qimonda” but the sentencelevel NMT model fails to attend to it when generating the translation.
134	35	On the other hand, the single memory models are better in delivering some, if not all, of the underlying information in the source sentence but the dual memory model’s translation quality surpasses them.
135	118	This is because the word “Qimonda” was being repeated in this specific document, providing a strong contextual signal to our global document context model while the local context model by Wang et al. (2017) is still unable to correctly translate the noun even when it has access to the word-level information of previous sentences.
136	33	We resort to manual evaluation as there is no standard metric which evaluates document-level discourse information like consistency or pronominal anaphora.
137	19	By manual inspection, we observe that our models can identify nouns in the source sentence to resolve coreferent pronouns, as shown in the second example of Table 7.
138	73	Here the topic of the sentence is “the country under the dictatorship of Lukashenko” and our target and dual memory models are able to generate the appropriate pronoun/determiner as well as accurately translate the word ‘diktatuur’, hence producing much better translation as compared to both baselines.
159	13	We have proposed a document-level neural MT model that captures global source and target document context.
160	19	Our model augments the vanilla sentence-based NMT model with external memories to incorporate documental interdependencies on both source and target sides.
162	19	For future work, we intend to investigate models which incorporate specific discourse-level phenomena.
