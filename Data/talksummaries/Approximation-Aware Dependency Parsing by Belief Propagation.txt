6	30	In reality, it achieves O(n3tmax) runtime by relying on three approximations during inference: (1) variational inference by loopy belief propagation (BP) on a factor graph, (2) truncating inference after tmax iterations prior to convergence, and (3) a first-order pruning model to limit the number of edges considered in the higherorder model.
8	33	We treat the entire parsing computation as a differentiable circuit, and backpropagate the evaluation function through our approximate inference and decoding methods to improve its parameters by gradient descent.
9	44	The system also learns to cope with model misspecification, where the model couldn’t perfectly fit the distribution even absent the approximations.
10	61	For standard graphical models, Stoyanov and Eisner (2012) call this approach ERMA, for “empirical risk minimization under approximations.” For objectives besides empirical risk, Domke (2011) refers to it as “learning with truncated message passing.” Our primary contribution is the application of this approximation-aware learning method in the parsing setting, for which the graphical model involves a global constraint.
14	20	Submission batch: 4/2015; Published 8/2015.
26	119	It is a bipartite graph between variables Yi and factors α. Edges connect each factor α to a subset of the variables {Yα1 , Yα2 , .
43	35	Thus, our parser seeks a well-formed parse hθ(x) whose individual edges have a high probability of being correct according to pθ (since it lacks knowledge y∗ of which edges are truly correct).
44	61	MBR is the principled way to take a loss function into account under a probabilistic model.
45	21	By contrast, maximum a posteriori (MAP) decoding does not consider the loss function.
52	50	Then we maximize (3) by running a first-order dependency parser with edge scores equal to those probabilities.3 When our inference algorithm is approximate, we replace the exact marginal with its approximation—the belief from BP, given by bi(ON) in (6) below.
55	22	The algorithm proceeds by iteratively sending messages from variables, yi, to factors, α: m (t) i→α(yi) ∝ ∏ β∈N (i)\α m (t−1) β→i (yi) (4) and from factors to variables: m (t) α→i(yi) ∝ ∑ yα∼yi ψα(yα) ∏ j∈N (α)\i m (t−1) j→α (yi) (5) where N (i) and N (α) denote the neighbors of yi and α respectively, and where yα ∼ yi is standard notation to indicate that yα ranges over all assignments to the variables participating in the factor α provided that the ith variable has value yi.
56	51	Note that the messages at time t are computed from those at time (t−1).
82	51	Inference, Decoding, and Loss as a Feedfoward Circuit The backpropagation algorithm is often applied to neural networks, where the topology of a feedforward circuit is statically specified and can be applied to any input.
87	68	We zoom in on two submodules: the first computes messages from the PTREE factor efficiently (C.1–C.3); the second computes a softened version of our loss function (E.1–E.3).
120	21	We temporarily remove the top layers of our network (i.e. the decoder and loss module, Fig.
133	25	Backpropagating through an algorithm proceeds by similar application of the chain rule, where the intermediate quantities are determined by the topology of the circuit—just as in Figure 2.
165	20	Similarly, we exploit the structure of this algorithm to compute the adjoints ðm(t)j→PTREE(yj).
168	22	Though we focus here on projective dependency parsing, our techniques are also applicable to non-projective parsing and the TREE factor; we leave this to future work.
215	30	The first, our baseline, is conditional log- likelihood training (CLL) (§6).
227	49	For English PTB-YM, Figure 3 shows accuracy as a function of the number of BP iterations for our second-order model with both arbitrary sibling and grandparent factors on English.
228	55	We find that our training methods (L2 and L2+AR) obtain higher accuracy than standard training (CLL), particularly when a small number of BP iterations are used and the inference is a worse approximation.
237	32	Notice that our advantage is not restricted to the case of loopy graphs.
239	52	We speculate that this improvement is due to our method’s ability to better deal with model misspecification—a first-order model is quite misspecified!
240	116	Note the following subtle point: when inference is exact, the CLL estimator is actually a special case of our approximation-aware learner— that is, CLL computes the same gradient that our training by backpropagation would if we used loglikelihood as the objective.
241	25	Exact Inference with Grandparents §2 noted that since we always do MBR decoding, the ideal strategy is to fit the true distribution with a good model.
244	50	Table 1 shows that CLL training with exact inference indeed does well on test data—but that accuracy falls if we substitute fast approximate inference (4 iterations of BP).
246	41	That is, we succesfully train a few iterations of an approximate O(n3) algorithm to behave as well as an exact O(n4) algorithm.
250	59	Notice that the approximation-aware training doesn’t always outperform CLL training—only in the aggregate.
262	21	Experiments on the English Penn Treebank and 19 languages from CoNLL-2006/2007 shows that our estimator is able to train more accurate dependency parsers with fewer iterations of belief propagation than standard conditional log-likelihood training, by taking approximations into account.
263	28	For additional details, see the tech report version of this paper (Gormley et al., 2015a).
