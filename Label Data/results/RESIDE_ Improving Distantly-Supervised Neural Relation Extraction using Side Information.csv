0,1,label2,summary_sentences
"Given a dataset, similarity relationship between examples can be represented by a graph in which each example is represented by a vertex.",1. Introduction,[0],[0]
"While pairwise relationship between two vertices can be represented by an edge in a normal graph, a higher order relationship involving multiple vertices can be captured by a hyperedge, which means that all the corresponding examples are similar to one another.",1. Introduction,[0],[0]
"Hypergraphs have been used in several learning applications such as clustering of categorical data (Gibson et al., 1998), multi-label classification (Sun et al., 2008), Laplacian sparse coding (Gao et al., 2013), image classification (Yu et al., 2012), image retrieval (Huang et al., 2010), mapping users across different social networks (Tan et al., 2014) and predicting edge labels in hypernode graphs (Ricatte et al., 2014).
",1. Introduction,[0.9730539087042102],"['Recently proposed Graph Convolution Networks (GCN) (Defferrard et al., 2016) have been effectively employed for encoding this information (Marcheggiani and Titov, 2017; Bastings et al., 2017).']"
"*Equal contribution 1University of Hong Kong, Hong Kong.",1. Introduction,[0],[0]
2This research was partially supported by the Hong Kong RGC under the grant 17200214.,1. Introduction,[0],[0]
"Correspondence to: T-H. Hubert Chan <hubert@cs.hku.hk>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"In this paper, we consider semi-supervised learning on an edge-weighted hypergraph H = (V,E,w), with a set L of labeled vertices, whose labels are given by f∗L ∈ {−1,+1}L. The task is to predict the labels of the unlabeled vertices N , with the working principle that vertices contained in a hyperedge e ∈ E are more similar to one another if the edge weight we is larger.",1. Introduction,[0],[0]
"This problem is also known as transductive inference and has been studied in (Zhou et al., 2006) and (Hein et al., 2013).
",1. Introduction,[0],[0]
"However, the methods in (Zhou et al., 2006) have been criticized by (Agarwal et al., 2006), because essentially a hypergraph is converted into a normal graph.",1. Introduction,[0],[0]
"For instance, given a hyperedge e containing vertices S, either (i) a clique is added between the vertices in S, or (ii) a star is formed by adding a new vertex ve connecting every vertex in S to ve.",1. Introduction,[0],[0]
"Then, a standard convex program using a regularization potential function for normal graphs can be applied (Zhu et al., 2003).",1. Introduction,[0],[0]
"By choosing appropriate edge weights, it was shown in (Agarwal et al., 2006) that the two approaches are equivalent to the following convex program relaxation:
min Φold(f) := 1
2 ∑ e∈E we ∑
{u,v}∈(e2)
(fu − fv)2
subject to fu ∈",1. Introduction,[0],[0]
"[−1, 1], ∀u ∈ V fu = f ∗ u , ∀u ∈",1. Introduction,[0],[0]
"L.
On the other hand, it was proposed in (Hein et al., 2013) that the following regularization function is more suitable to capture hyperedge expansion:
Φnew(f) := 1
2 ∑ e∈E we · (max u∈e fu −min v∈e fv) 2.
",1. Introduction,[0],[0]
"Indeed, it was shown in (Hein et al., 2013) that their approach outperforms (Zhou et al., 2006) on several datasets from the UCI Machine Learning Repository (Lichman, 2013).
",1. Introduction,[0],[0]
Loss Function.,1. Introduction,[0],[0]
"In (Hein et al., 2013), a squared loss function was added by considering the convex program with objective function Φnew(f) + µ ‖f − f∗‖22 on f ∈",1. Introduction,[0],[0]
"[−1, 1]V , where µ > 0 is a parameter to be tuned, f∗L is given by the labeled vertices L, and for the unlabeled vertices f∗N = 0.
",1. Introduction,[0],[0]
"The loss function allows errors in the labeled vertices, and also ensures that the minimizer is unique.",1. Introduction,[0],[0]
"However, as a result, unlabeled vertices have a tendency to acquire f values close to 0.",1. Introduction,[0],[0]
"This might remove useful information as illustrated in the following example.
",1. Introduction,[0],[0]
Example.,1. Introduction,[0],[0]
"In Figure 1.1, vertices a, b ∈ L are labeled as +1 and c ∈ L is labeled as −1.",1. Introduction,[0],[0]
"Vertices x, y ∈ N are unlabeled.",1. Introduction,[0],[0]
"There are three (undirected) edges: {a, x}, {b, x} and {x, y, c}, each with unit weight.
",1. Introduction,[0],[0]
"By choosing µ = 12 for squared loss function, the unique minimizer gives fx = 15 and fy = 0.",1. Introduction,[0],[0]
"Hence, this solution gives no useful information regarding the label for vertex y.
On the other hand, if we just use the objective function Φnew(f) with the constraints fL = f∗L, then in an optimal solution, fx = 13 , but fy could be anywhere in the confidence interval",1. Introduction,[0],[0]
"[−1, 13 ].",1. Introduction,[0],[0]
"Hence, in this case, we could use the average value − 13 to predict −1 for vertex y.
Our Contributions.",1. Introduction,[0],[0]
"In this paper, we revisit the approach used in (Hein et al., 2013) and consider several extensions and simplifications.",1. Introduction,[0],[0]
"We summarize our results and give an outline of the paper as follows.
1.",1. Introduction,[0],[0]
Unified Framework for Directed Hypergraphs.,1. Introduction,[0],[0]
"Inspired also from the recent result on Laplacians for directed normal graphs (Yoshida, 2016), we introduce a semisupervised learning framework using directed hypergraphs that can capture higher order causal relationships.",1. Introduction,[0],[0]
"This notion of directed hypergraph was first introduced in (Gallo et al., 1993), who considered applications in propositional logic, analyzing dependency in relational database, and traffic analysis.",1. Introduction,[0],[0]
"On a high level, a directed hyperedge e consists of a tail set Te pointing to a head set He such that a vertex in Te labeled +1 implies that a vertex in He is more likely to be labeled +1.",1. Introduction,[0],[0]
"(Equivalently in terms of its contrapositive, a vertex in He labeled −1 implies that a vertex in Te is more likely to be labeled −1.)",1. Introduction,[0],[0]
"In Section 2, we formally define the model and the corresponding potential function Φ. An additional advantage of our potential function is that there is no need to tune any parameters.
2.",1. Introduction,[0],[0]
Confidence Interval for Unlabeled Vertices.,1. Introduction,[0],[0]
Observe that the minimizer for our convex program might not be unique.,1. Introduction,[0],[0]
"In Section 3, we introduce the concept of confidence interval for each unlabeled vertex that can be useful for predicting its label.",1. Introduction,[0],[0]
"Furthermore, we provide an algorithm to calculate the confidence interval given an optimal solution.
3.",1. Introduction,[0],[0]
Simpler Subgradient Method.,1. Introduction,[0],[0]
"Since the new potential function is not everywhere differentiable but still convex, we use the subgradient method (Shor et al., 1985) to obtain an estimated minimizer for label prediction.",1. Introduction,[0],[0]
"Inspired by the diffusion processes used for defining Laplacians in hypergraphs (Louis, 2015) and directed graphs (Yoshida, 2016), in Section 4, we define a simple Markov operator that returns a subgradient for Φ, which is used to solve the underlying convex program.",1. Introduction,[0],[0]
"We remark that our framework is very easy to understand, because it is a variation on the well-known gradient descent.
",1. Introduction,[0],[0]
"In contrast, the primal-dual approach in (Hein et al., 2013) considers the convex conjugate of the primal objective and involves complicated update operations on the primal and dual variables.",1. Introduction,[0],[0]
"The subgradient used in our approach gives the update direction, and we can actually solve exactly the same convex program with a much simpler method.",1. Introduction,[0],[0]
"Section 5, we revisit some datasets in the UCI Machine Learning Repository (Lichman, 2013), and experiments confirm that our prediction model based on confidence interval gives better accuracy than that in (Hein et al., 2013).",4. Experimental Results on Real-World Datasets. In,[0],[0]
"Our simpler subgradient method takes more iterations than the primal-dual method (Hein et al., 2013), but each iteration is much faster.",4. Experimental Results on Real-World Datasets. In,[0],[0]
"Experiments show that overall both methods have similar running times, and the subgradient method has an advantage when the number of vertices is much larger than the number of edges.
",4. Experimental Results on Real-World Datasets. In,[0],[0]
"Moreover, using the DBLP dataset (Ley, 2009), our experiments also support that using directed hypergraphs to capture causal relationships can improve the prediction accuracy.",4. Experimental Results on Real-World Datasets. In,[0],[0]
The experiments for directed hypergraphs are described in the full version.,4. Experimental Results on Real-World Datasets. In,[0],[0]
"We consider an edge-weighted directed hypergraph H = (V,E,w) with vertex set V (with n = |V |), edge set E and weight function",2. Preliminaries,[0],[0]
w : E → R+.,2. Preliminaries,[0],[0]
Each hyperedge e ∈ E consists of a tail set Te ⊆ V and a head set He ⊆ V (which are not necessarily disjoint); we use the convention that the direction is from tail to head.,2. Preliminaries,[0],[0]
"For x ∈ R, we denote [x]+ := max{x, 0}.
",2. Preliminaries,[0],[0]
"In our application, each vertex v ∈ V is supposed to have a label in {−1,+1}.",2. Preliminaries,[0],[0]
"Intuitively, the directed hypergraph attempts to capture the rule that for each edge e ∈ E, if there is a vertex in Te having label +1, then it is more likely for vertices in He to receive label +1.",2. Preliminaries,[0],[0]
"In terms of its contrapositive, if there is a vertex in He having label −1, then it is more likely for vertices in Te to receive label −1.
",2. Preliminaries,[0],[0]
"We use f ∈ RV to denote a vector, where the coordi-
nates are labeled by vertices in V .",2. Preliminaries,[0],[0]
"For U ⊆ V , we use fU ∈ RU to denote the vector restricting f to coordinates inU .",2. Preliminaries,[0],[0]
"In semi-supervised learning, we consider a setL ⊆ V of labeled vertices, which have labels f∗L ∈",2. Preliminaries,[0],[0]
"{−1,+1}L. Typically, |L| |V | and the task is to assign a label in {−1,+1} to each unlabeled vertex in N := V \ L, using information from the directed hypergraph H .
",2. Preliminaries,[0],[0]
"By relaxing labels to be in the interval [−1, 1], we consider the following regularization potential function Φ : RV → R:
Φ(f)",2. Preliminaries,[0],[0]
"= 1
2 ∑ e∈E we · ([∆e(f)]+)2,
where ∆e(f) := max(u,v)∈Te×He(fu − fv) = maxu∈Te fu −minv∈He fv .
",2. Preliminaries,[0],[0]
"In particular, there is a penalty due to edge e only if some vertex in Te receives a label larger than that of some vertex in He.",2. Preliminaries,[0],[0]
"The convexity of Φ is proved in the full version.
",2. Preliminaries,[0],[0]
Our approach is to consider the following convex program to obtain an estimated minimizer f ∈,2. Preliminaries,[0],[0]
"[−1, 1]V , which can be rounded to an integer solution for labeling all vertices.
min Φ(f) (CP1) subject to fu ∈",2. Preliminaries,[0],[0]
"[−1, 1], ∀u ∈ V
fu = f ∗ u , ∀u",2. Preliminaries,[0],[0]
"∈ L
Since the f values for the labeled vertices L are fixed in (CP1), we also view Φ : RN → R as a function on the f values of unlabeled vertices N .",2. Preliminaries,[0],[0]
"We use OPT ⊂ RV to denote the set of optimal solutions to (CP1).
",2. Preliminaries,[0],[0]
Trivial Edges.,2. Preliminaries,[0],[0]
An edge e ∈ E is trivial if there exist vertices u ∈ Te ∩ L and v ∈,2. Preliminaries,[0],[0]
He ∩ L such that f∗u = +1 and f∗v = −1.,2. Preliminaries,[0],[0]
"As trivial edges contribute constant towards the objective function Φ, we shall assume that there are no trivial edges in the convex program (CP1).
",2. Preliminaries,[0],[0]
Special Cases.,2. Preliminaries,[0],[0]
"Our directed hypergraph model can capture other graph models as follows.
1.",2. Preliminaries,[0],[0]
Undirected Hypergraphs.,2. Preliminaries,[0],[0]
"For each hyperedge e, we can set Te = He to the corresponding subset of vertices.",2. Preliminaries,[0],[0]
2.,2. Preliminaries,[0],[0]
Undirected Normal Graphs.,2. Preliminaries,[0],[0]
"For each edge e = {u, v}, we can set Te = He = e. Observe that in this case, the potential function becomes Φ(f) =∑
(u,v)∈E wuv(fu− fv)2, which is differentiable, and hence, (CP1) can be solved by standard techniques like gradient descent.
",2. Preliminaries,[0],[0]
Soft Constraints.,2. Preliminaries,[0],[0]
"In (Hein et al., 2013), each labeled vertex u ∈ L can also have some weight µu ∈ R+, which can, for instance, indicate how trustworthy the label
f∗u ∈ {−1,+1} is.",2. Preliminaries,[0],[0]
"The following relaxation is considered.
",2. Preliminaries,[0],[0]
"min Φ̂(f) := Φ(f) + 1
2 ∑ u∈L µu(fu",2. Preliminaries,[0],[0]
"− f∗u)2 (CP2)
subject to fu ∈",2. Preliminaries,[0],[0]
"[−1, 1],∀u ∈ V.
Observe that (CP2) can also be expressed in the framework of (CP1).",2. Preliminaries,[0],[0]
"We simply consider an augmented hypergraph Ĥ such that all vertices V are treated as unlabeled, and for each u ∈ L, we add a new vertex û with label f∗u and a new undirected edge {u, û} with weight µu.",2. Preliminaries,[0],[0]
"Then, it follows that the convex program (CP1) for the augmented instance for Ĥ is exactly the same as (CP2).
",2. Preliminaries,[0],[0]
Challenges Ahead.,2. Preliminaries,[0],[0]
"We next outline how we resolve the encountered challenges when we use (CP1) for semisupervised learning.
",2. Preliminaries,[0],[0]
"• Unlike the case for normal graphs, the set OPT can contain more than one optimal solution for (CP1).",2. Preliminaries,[0],[0]
"In Section 3, we prove some structural properties of the convex program, and illustrate that each u ∈ N has some confidence interval from which we can predict its label.",2. Preliminaries,[0],[0]
• The function Φ is not everywhere differentiable.,2. Preliminaries,[0],[0]
"Hence, we use the subgradient method (Shor et al., 1985).",2. Preliminaries,[0],[0]
"In Section 4, we give a method to generate a subgradient, which is inspired by the continuous diffusion processes for hypergraphs (Louis, 2015) and directed graphs (Yoshida, 2016), and our method can in fact be viewed as a discretized version.",2. Preliminaries,[0],[0]
"In general, a minimizer for (CP1) might not be unique.",3. Confidence Interval for Semi-supervised Learning,[0],[0]
"Hence, we introduce the concept of confidence interval.
",3. Confidence Interval for Semi-supervised Learning,[0],[0]
Definition 3.1 (Confidence Interval),3. Confidence Interval for Semi-supervised Learning,[0],[0]
"For each u ∈ V , we define its confidence interval to be [mu,Mu], where mu := minf∈OPT fu and Mu := maxf∈OPT fu.",3. Confidence Interval for Semi-supervised Learning,[0],[0]
"The confidence intervals induce the lower and the upper confidence vectors, ~m and ~M ∈ RV , respectively.
",3. Confidence Interval for Semi-supervised Learning,[0],[0]
"In Section 3.1, we give the proof of the following lemma, which states that the confidence vectors ~m and ~M are optimal solutions, and so are their convex combinations.
",3. Confidence Interval for Semi-supervised Learning,[0],[0]
Lemma 3.1 (Confidence Vectors Give Optimal Solutions),3. Confidence Interval for Semi-supervised Learning,[0],[0]
For any λ ∈,3. Confidence Interval for Semi-supervised Learning,[0],[0]
"[0, 1], the convex combination λ~m + (1− λ) ~M",3. Confidence Interval for Semi-supervised Learning,[0],[0]
"∈ OPT is optimal for (CP1).
",3. Confidence Interval for Semi-supervised Learning,[0],[0]
Semi-supervised Learning via Confidence Interval.,3. Confidence Interval for Semi-supervised Learning,[0],[0]
Lemma 3.1 suggests what one can do when (CP1) has more than one optimal solution.,3. Confidence Interval for Semi-supervised Learning,[0],[0]
"Specifically, in Algorithm 1, the
average vector 12 (~m + ~M) ∈ OPT can be used for label prediction.",3. Confidence Interval for Semi-supervised Learning,[0],[0]
"We show that the confidence vectors ~m and ~M can be recovered from any optimal solution f ∈ OPT, which in turn can be estimated by the subgradient method described in Section 4.",3. Confidence Interval for Semi-supervised Learning,[0],[0]
"Algorithm 1 Semi-Supervised Learning
1: Input: Directed hypergraph H = (V,E,w), labels f∗L for labeled vertices L 2: Compute (estimated) confidence vectors (~m, ~M) ∈",3. Confidence Interval for Semi-supervised Learning,[0],[0]
"RN × RN , either by Algorithm 2 or 3.",3. Confidence Interval for Semi-supervised Learning,[0],[0]
3: Compute average vector fN ← 12 (~m+ ~M).,3. Confidence Interval for Semi-supervised Learning,[0],[0]
4: Compute threshold θ ← 1|N | ∑ u∈N fu.,3. Confidence Interval for Semi-supervised Learning,[0],[0]
"5: for each u ∈ N do 6: if fu ≥ θ then 7: f̂u ← +1; 8: else 9: f̂u ← −1;
10: end if 11: end for 12: return f̂N
Fine-Tuning Parameters.",3. Confidence Interval for Semi-supervised Learning,[0],[0]
"In view of Lemma 3.1, one could further optimize the choice of λ ∈",3. Confidence Interval for Semi-supervised Learning,[0],[0]
"[0, 1] in defining fN ← λ~m+ (1−λ) ~M in Line 3.",3. Confidence Interval for Semi-supervised Learning,[0],[0]
"Similarly, one could pick the threshold θ to be the ϑ-percentile of the sorted coordinates of fN , for some choice of ϑ ∈",3. Confidence Interval for Semi-supervised Learning,[0],[0]
"[0, 1].",3. Confidence Interval for Semi-supervised Learning,[0],[0]
The parameters λ and ϑ can be tuned using standard techniques like cross-validation.,3. Confidence Interval for Semi-supervised Learning,[0],[0]
"However, to illustrate our concepts, we keep the description simple without introducing too many free parameters.",3. Confidence Interval for Semi-supervised Learning,[0],[0]
We derive some properties of the confidence vectors to prove Lemma 3.1.,3.1. Properties of Confidence Vectors,[0],[0]
"The full proofs of Lemma 3.2 and 3.3 are given in the full version.
",3.1. Properties of Confidence Vectors,[0],[0]
"Given a feasible solution f ∈ RV to (CP1), we define the following:
1. Se(f) := arg maxu∈Te fu ⊆ Te and Ie(f) := arg minv∈He fv ⊆ He.",3.1. Properties of Confidence Vectors,[0],[0]
2. f(Se),3.1. Properties of Confidence Vectors,[0],[0]
:= maxu∈Te fu and f(Ie) := minv∈He fv .,3.1. Properties of Confidence Vectors,[0],[0]
"Hence, we have ∆e(f) = f(Se)− f(Ie).",3.1. Properties of Confidence Vectors,[0],[0]
3.,3.1. Properties of Confidence Vectors,[0],[0]
"The set of active edges with respect to f is E(f) := {e ∈ E : ∆e(f) > 0}.
",3.1. Properties of Confidence Vectors,[0],[0]
"The following lemma states even though a minimizer for (CP1) might not be unique, there are still some structural properties for any optimal solution.
",3.1. Properties of Confidence Vectors,[0],[0]
Lemma 3.2 (Active Edges in an Optimal Solution) Suppose f and g are optimal solutions to (CP1).,3.1. Properties of Confidence Vectors,[0],[0]
"Then, for all e ∈ E, [∆e(f)]+ = [∆e(g)]+.",3.1. Properties of Confidence Vectors,[0],[0]
"In particular, this implies that the set of active edges E∗",3.1. Properties of Confidence Vectors,[0],[0]
":= E(f) = E(g) in any op-
timal solution is uniquely determined.",3.1. Properties of Confidence Vectors,[0],[0]
"Hence, for e ∈ E∗, we can define the corresponding ∆∗e = ∆e(f).
",3.1. Properties of Confidence Vectors,[0],[0]
"Definition 3.2 (Pinned Vertex) An unlabeled vertex u is pinned in a solution f ∈ RV if there exist active edges e and e′ ∈ E(f) such that u ∈ Se(f)∩ Ie′(f), in which case we say that the edges e and e′ pin the vertex u under f .
",3.1. Properties of Confidence Vectors,[0],[0]
Lemma 3.3 (Extending an Active Edge),3.1. Properties of Confidence Vectors,[0],[0]
Suppose edge e ∈ E(f) is active in an optimal solution f .,3.1. Properties of Confidence Vectors,[0],[0]
"If He does not contain a vertex labeled with −1, then there exist u ∈ Ie(f) and another active edge e′ ∈ E(f) such that the following holds.
",3.1. Properties of Confidence Vectors,[0],[0]
(a) The edges e and e′,3.1. Properties of Confidence Vectors,[0],[0]
"pin u under f , i.e., u ∈ Se′(f).",3.1. Properties of Confidence Vectors,[0],[0]
(b),3.1. Properties of Confidence Vectors,[0],[0]
"If g is an optimal solution, then Ie(f) ∩ Se′(f) =
Ie(g) ∩ Se′(g) and fu = gu.",3.1. Properties of Confidence Vectors,[0],[0]
vertex labeled with +1.,An analogous result holds when Te does not contain any,[0],[0]
∗(Ie),"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
":= minu∈He fu are uniquely determined by any optimal solution f .
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
Corollary 3.1 (Pinned Vertices),"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"In any optimal solution, the set of pinned vertices is uniquely determined.","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
We use L∗ to denote the set of labeled or pinned vertices in an optimal solution.,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Then, for each u ∈ L∗, its value f∗u in any optimal solution is also uniquely determined.
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"From Corollary 3.1, the confidence interval for any u ∈ L∗ contains exactly one value, namely the unique value f∗u in any optimal solution.","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"The following lemma gives a characterization of an optimal solution.
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
Lemma 3.4 Characterization of Optimal Solutions,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"A solution f to (CP1) is optimal iff the following conditions hold.
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"(a) For each u ∈ L∗, fu = f∗u .","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"(b) For each active edge e ∈ E∗, both the maximum
maxu∈Te fu and the minimum minv∈He fv are attained by vertices in L∗. (c) For each inactive edge e /∈","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"E∗,","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
for all u ∈ Te and v ∈,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"He, fu ≤ fv .
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
Proof: We first observe that Corollary 3.1 states that the values of the vertices in L∗ are uniquely determined in any optimal solution.,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Hence, any optimal solution must satisfy the three conditions.","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"We next show that the three conditions implies that the objective value is optimal.
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Once the values for vertices in L∗ are fixed, Lemma 3.3 and condition (b) implies that the contribution of all active edges E∗ are determined and are the same as any optimal solution.
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Finally, condition (c) implies that edges not in E∗ do not have any contribution towards the objective function.","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Hence, any solution satisfying the three conditions must be optimal.
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
Deriving Confidence Vectors.,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"To prove Lemma 3.5, we define a procedure that returns a vector ~m ∈ V R such that for any optimal f ∈ OPT, we have f ≥ ~m.","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Moreover, we shall show that ~m ∈ OPT and hence ~m is the lower confidence vector.","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
The argument for the upper confidence vector ~M is similar.,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"For the special case of undirected hypergraphs, the procedure can be simplified to Algorithm 2 in Section 3.2.
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
Lemma 3.5 (Confidence Vectors are Optimal: Proof of Lemma 3.1),"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
The confidence vectors ~m and ~M defined in Definition 3.1 are optimal solutions to (CP1).,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"This implies that any of their convex combination is also optimal.
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Proof: We give a procedure that returns a vector ~m such that at any moment during the procedure, the following invariant is maintained: for any f ∈ OPT, f ≥ ~m.
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"The following steps correspond to maintaining the conditions in Lemma 3.4.
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
(a) Initialization.,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"For v ∈ L∗, set mv := f∗v ; for v /∈ L∗, set mv := −1.","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"This satisfies the invariant, because for any f ∈ OPT and any v ∈ L∗, fv = f∗v .
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
(b) Preserving Active Edges.,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"For each v /∈ L∗, set mv ← max{mv,maxe∈E∗:v∈He f∗(Ie)}.","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Observe that Lemma 3.4(b) implies that for any optimal f ∈ OPT, any e ∈ E∗ and any v ∈","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"He, fv ≥ f∗(Ie).","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Hence, the invariant is maintained.
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
(c) Preserving Inactive Edges.,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
While there is an inactive edge e /∈ E∗,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"such that u ∈ Te, v ∈","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"He and mu > mv , set mv ← mu.","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
We argue why each such update preserves the invariant.,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
Consider any optimal f ∈ OPT.,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Before this update, the invariant holds.","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Hence, we have mu ≤ fu.","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Moreover, Lemma 3.4 implies that fu ≤ fv .","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Therefore, after setting mv ← mu, we still have mv ≤ fv .
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Finally, observe that after step (b), the coordinates of ~m can take at most n distinct values.","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Moreover, after each update in step (c), one coordinate of ~m must increase strictly.","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Hence, this procedure will terminate.
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"We next argue that ~m is an optimal solution by checking that it satisfies the conditions in Lemma 3.4.
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
Condition (a).,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Observe that for each v ∈ L∗, mv is initialized to f∗v .","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
Afterwards the value mv could only be increased.,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"However, because the invariant holds when the procedure terminates, it must be the case that mv = f∗v at the end.
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
Condition (b).,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"The procedure makes sure that at the end of
step (b), for every active edge e ∈ E∗, minv∈He mv can be attained by some vertex in L∗. Since only mv for v /∈ L∗ can be increased in step (c), it follows that in the end, the minimum can still be attained by some vertex in L∗.
Next, consider u ∈ Te, where e ∈ E∗. For any optimal solution f , Lemma 3.3 implies that fu ≤ f∗(Se).","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Hence, the invariant implies thatmu ≤ fu ≤ f∗(Se).","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Since condition (a) holds, this means that maxv∈Te mv can be attained by some vertex in L∗.
Condition (c).","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"This is clearly satisfied because of the while-termination condition.
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Therefore, we have ~m ∈ OPT, as required.
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
The proof for the upper confidence vector ~M is similar.,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"We omit the detailed proof and just give the corresponding procedure to return ~M .
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
(a) Initialization.,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"For v ∈ L∗, set Mv := f∗v ; for v /∈ L∗, set Mv := +1.
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
(b) Preserving Active Edges.,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"For each v /∈ L∗, set Mv ← min{Mv,mine∈E∗:v∈Te f∗(Se)}.
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
(c) Preserving Inactive Edges.,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
While there is an inactive edge e /∈ E∗,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"such that u ∈ Te, v ∈","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"He and Mu > Mv , set Mu ←Mv .
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"The same argument can show that for any optimal f ∈ OPT, we have f ≤ ~M .","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Moreover, we also have ~M ∈ OPT.","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"As mentioned before, the proof of Lemma 3.5 implicitly gives a procedure to compute the confidence vectors from any optimal solution.",3.2. Computing the Confidence Interval,[0],[0]
"For the special case of undirected hypergraphs, a simplified version of the procedure is given in Algorithm 2.
",3.2. Computing the Confidence Interval,[0],[0]
"Alternatively, we can try to solve the convex program (CP1), for example using Algorithm 5 in Section 4, from two initial feasible solutions to heuristically estimate the confidence vectors.",3.2. Computing the Confidence Interval,[0],[0]
"In Algorithm 3, one instance approaches an optimal solution from high f values and the other from low f values.",3.2. Computing the Confidence Interval,[0],[0]
Resolving Ties.,4. Subgradient Method via Markov Operator,[0],[0]
Observe that Φ : RN → R is differentiable at fN ∈ RN that has distinct coordinates.,4. Subgradient Method via Markov Operator,[0],[0]
"For the purpose of computing a subgradient, we assume that there is some global ordering π on V to resolve ties among coordinates with the same value.",4. Subgradient Method via Markov Operator,[0],[0]
"In particular, the vertices in L having label +1 are the highest, and those in L labeled −1 are the lowest.",4. Subgradient Method via Markov Operator,[0],[0]
"Hence, in this section, we may assume that any arg max or arg min operator over a subset of vertices
Algorithm 2 Confidence Intervals for Undirected Hypergraphs
1: Input: Undirected hypergraph H = (V,E,w), label vector f∗L and tolerance ≥ 0.",4. Subgradient Method via Markov Operator,[0],[0]
"2: Let f be a solution of (CP1), either by Algorithm 5 or by PDHG method (Hein et al., 2013) 3: For all v ∈ V , set p(v)← v, mv ← −1, Mv ← +1.",4. Subgradient Method via Markov Operator,[0],[0]
"4: Ê := {e ∈ E : ∆e(f) ≤ } 5: while ∃e1 6= e2 ∈ Ê, e1 ∩ e2 6= ∅",4. Subgradient Method via Markov Operator,[0],[0]
"do 6: Ê ← (Ê \ {e1, e2}) ∪ {e1 ∪ e2} 7: end while 8: for each e ∈ Ê do 9: x← an arbitrary vertex in e
10: for each vertex v ∈ e do 11: p(v)← p(x) 12: end for 13: end for 14: for each vertex v ∈ L do 15: mp(v) ← f∗v , Mp(v) ← f∗v 16: end for 17: for each edge e ∈ E such that ∆e(f) >",4. Subgradient Method via Markov Operator,[0],[0]
do 18: for each vertex v ∈,4. Subgradient Method via Markov Operator,[0],[0]
e,4. Subgradient Method via Markov Operator,[0],[0]
"do 19: mp(v) ← max{mp(v), f(Ie)} 20: Mp(v) ← min{Mp(v), f(Se)} 21: end for 22: end for 23: for each vertex v ∈ V do 24: mv ← mp(v), Mv ←Mp(v) 25: end for 26: return vectors (~m, ~M)
will return a unique vertex.
",4. Subgradient Method via Markov Operator,[0],[0]
"We next define a Markov operator that is inspired from the diffusion processes on hypergraphs (Louis, 2015) and directed graphs (Yoshida, 2016) in the context of defining Laplacians.",4. Subgradient Method via Markov Operator,[0],[0]
"We denote the projection operator ΠN : RV → RN that takes f ∈ RV and returns the restricted vector fN ∈ RN .
",4. Subgradient Method via Markov Operator,[0],[0]
Lemma 4.1 For f ∈,4. Subgradient Method via Markov Operator,[0],[0]
"[−1, 1]V that is feasible in (CP1), the Markov operator Mf given in Algorithm 4 returns a subgradient of Φ : RN → R at fN .
",4. Subgradient Method via Markov Operator,[0],[0]
"Proof: (Sketch) Observe that if fN ∈ RN has distinct coordinates, then Φ is differentiable at fN , and Mf gives exactly the gradient (which is the only possible subgradient in this case).",4. Subgradient Method via Markov Operator,[0],[0]
"Observe that in our subgradient method application, we could imagine that at every iteration, infinitesimal perturbation is performed on the current solution to ensure that all coordinates are distinct, and ties are resolved according to our global ordering π.
",4. Subgradient Method via Markov Operator,[0],[0]
"Algorithm 3 Estimate confidence interval 1: Input: Directed hypergraph H = (V,E,w), labels f∗L
for labeled vertices L 2: Construct feasible f (0,+)N ← +1 ∈ RN with all entries
being +1; 3: Construct feasible f (0,−)N ← −1 ∈",4. Subgradient Method via Markov Operator,[0],[0]
"RN with all entries
being −1; 4: ~M ← SGM(f (0,+)N ); 5: ~m← SGM(f (0,−)N ); 6: return the vectors (~m, ~M)
",4. Subgradient Method via Markov Operator,[0],[0]
"Algorithm 4 Markov Operator M : RV → RN
1: Input: Directed hypergraph H = (V,E,w), feasible f ∈ RV for (CP1) 2: Construct symmetric matrix A ∈ RV×V ; set A← 0.",4. Subgradient Method via Markov Operator,[0],[0]
3: for each e ∈ E such that ∆e(f) > 0,4. Subgradient Method via Markov Operator,[0],[0]
do 4: u← arg maxu∈Te fu; 5: v ← arg minv∈He fv; 6: Auv ← Auv + we; 7: (The same is done forAvu becauseA is symmetric.),4. Subgradient Method via Markov Operator,[0],[0]
"8: end for 9: Construct diagonal matrix W ∈ RN×N ; set W ← 0.
10: for each u ∈ N",4. Subgradient Method via Markov Operator,[0],[0]
do 11:,4. Subgradient Method via Markov Operator,[0],[0]
"Wuu ← ∑ v∈V Auv; 12: end for 13: return (WΠN −ΠNA)f
Hence, as the magnitude of the perturbation tends to zero, if the global ordering π is preserved, then the gradient remains the same, which implies that the gradient is also the subgradient when the perturbation reaches 0.
",4. Subgradient Method via Markov Operator,[0],[0]
"Using the Markov operator M as a subroutine to generate a subgradient, we have the following subgradient method (SGM) (Shor et al., 1985).
",4. Subgradient Method via Markov Operator,[0],[0]
"Algorithm 5 Subgradient Method SGM(f (0)N ∈ RN ) 1: Input: Directed hypergraph H = (V,E,w) with la-
bels f∗L for labeled vertices L, initial feasible solution f (0) N ∈",4. Subgradient Method via Markov Operator,[0],[0]
"RN , step size {ηt := 1 t }t≥1
2: t← 1; 3: (Throughout the algorithm, f (t)L = f ∗ L is given by the
labeled vertices.)",4. Subgradient Method via Markov Operator,[0],[0]
4: while Solution f (t)N has not “stabilized” do 5: g(t)N ← Mf (t−1) ∈ RN ; 6: f (t)N = f (t−1) N,4. Subgradient Method via Markov Operator,[0],[0]
"− ηt ·
g (t)",4. Subgradient Method via Markov Operator,[0],[0]
"N∥∥∥g(t)N ∥∥∥
2
;
7: t← t+ 1; 8: end while 9: return f (t)
",4. Subgradient Method via Markov Operator,[0],[0]
Stabilizing Condition.,4. Subgradient Method via Markov Operator,[0],[0]
"Our experiments in Section 5 suggest that it suffices to run the solver for a short time, after which a better feasible solution f does not improve the prediction accuracy.",4. Subgradient Method via Markov Operator,[0],[0]
Our experiments are run on a standard PC.,5. Experimental Results,[0],[0]
"In our graphs, each point refers to a sample mean, and the height of the vertical bar is the standard error of the mean.",5. Experimental Results,[0],[0]
"We show that our treatment of hypergraphs performs better than the previously best method in (Hein et al., 2013).
",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
Hypergraph Model.,5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
"We use three datasets from the UCI Machine Learning Repository (Lichman, 2013): mushroom, covertype45 and covertype67.",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
"As in (Hein et al., 2013), each dataset fits into the hypergraph learning model in the following way.",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
"Each entry in the dataset corresponds to a vertex, which is labeled either +1 or −1.",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
"Moreover, each entry has some categorical attributes.",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
"For each attribute and each realized value for that attribute, we form a unit-weight hyperedge containing all the vertices corresponding to entries having that attribute value.",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
"To summarize, below are the properties of the resulting hypergraphs.
",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
"Dataset mushroom covertype45 covertype67
n = |V",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
"| 8124 12240 37877 m = |E| 112 104 123 k =∑
e∈E |e| m
1523 1412 3695
Semi-supervised Learning Framework.",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
"We compare our semi-supervised learning framework with that in (Hein et al., 2013), which was previously the best (compared to (Zhou et al., 2006), for instance).",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
"Specifically, we compare the prediction accuracy of the following two prediction algorithms.
1.",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
Confidence Interval (CI).,5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
"We use hard constraints (CP1) and confidence intervals for prediction, as described in Algorithm 1 in Section 3. 2.",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
"Hein et al. We implement the method described in (Hein et al., 2013), which uses soft constraints (regularized version), plus 5-fold cross validation to determine the regularization parameter.
",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
Testing Methodology.,5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
"Since we focus on prediction accuracy, using either subgradient method or PDHG (Hein et al., 2013) for solving the underlying convex programs in each algorithm produces the same results.",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
"For each algorithm candidate, we try different sizes of labeled vertices L, where l = |L| ranges from 20 to 200.",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
"For each size l
of labeled vertices, we randomly pick l vertices from the dataset to form the set L and treat the rest as unlabeled vertices; we re-sample if only one label (+1 or −1) appears in L. For each size",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
"l, we perform 100 trials to report the average error rate together with its standard error.
Results.",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
"Our experiment can recover the results reported in (Hein et al., 2013).",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
"The test error for the two algorithms on the three datasets is presented in Figure 5.1, which shows that our CI method consistently has lower test error than the one in (Hein et al., 2013).",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
Different Solvers.,5.2. Comparing Running Times of Solvers,[0],[0]
"We compare the running times of the following two convex program solvers:
• Subgradient Method (SG), proposed by us.",5.2. Comparing Running Times of Solvers,[0],[0]
"Empirically, the step size ηt := 1
(t+1) min( 0.16t 105 ,1)
gives good
performance.",5.2. Comparing Running Times of Solvers,[0],[0]
"For large t, ηt grows like 1t and so the method converges; however, for small t, we would like a larger step size to speed up convergence.",5.2. Comparing Running Times of Solvers,[0],[0]
"• Primal-Dual Hybrid Gradient (PDHG), proposed in (Hein et al., 2013).",5.2. Comparing Running Times of Solvers,[0],[0]
"We choose σ = τ = 1√
1+d ,
where d is the maximum degree.
",5.2. Comparing Running Times of Solvers,[0],[0]
Theoretical Analysis.,5.2. Comparing Running Times of Solvers,[0],[0]
"Given a hypergraph with n vertices and m edges, where the average size of an edge is k, each vertex on average appears in mkn edges.",5.2. Comparing Running Times of Solvers,[0],[0]
"For SG, we use a heap-based data structure to maintain the vertices within a hyperedge.",5.2. Comparing Running Times of Solvers,[0],[0]
"Vertices attaining the maximum and the minimum value within a hyperedge can be retrieved in O(1) time, and a value update takes O(log k) time.",5.2. Comparing Running Times of Solvers,[0],[0]
"In each iteration, at most 2m vertices will have their values updated.",5.2. Comparing Running Times of Solvers,[0],[0]
"Hence, in each iteration, SG takes time 2m·mkn ·O(log k) = O(m
2k n log k).",5.2. Comparing Running Times of Solvers,[0],[0]
"In the description of PDHG in (Hein et al., 2013), each iteration takesO(mk log k) time.",5.2. Comparing Running Times of Solvers,[0],[0]
"Hence, when n m, each iteration of SG will be significantly faster, although in general, the number of iterations required by the subgradient method can be larger than that for PDHG.
",5.2. Comparing Running Times of Solvers,[0],[0]
Testing Methodology.,5.2. Comparing Running Times of Solvers,[0],[0]
"In each experiment, we consider the hypergraph from one of the above three datasets.",5.2. Comparing Running Times of Solvers,[0],[0]
"We pick l = 160 vertices at random as the labeled vertices L, and form the corresponding convex program (CP1) for the two solvers, where the initial values for unlabeled vertices are chosen independently to be uniformly at random from [−1, 1].",5.2. Comparing Running Times of Solvers,[0],[0]
"To compare the performance, we run the two solvers on the same convex program, and record each trajectory of the objective value versus the time duration.",5.2. Comparing Running Times of Solvers,[0],[0]
"According to experience, 100 seconds is good enough for either solver to reach an almost optimal solution, and we use the minimum value achieved by the two solvers after 100 seconds as an estimate for the true optimal value OPT.",5.2. Comparing Running Times of Solvers,[0],[0]
"Then, we scan each trajectory, and for each relative gap
∈ {10−i : i = 1, 2, . . .",5.2. Comparing Running Times of Solvers,[0],[0]
", 6}, we find the smallest time T ( ) after which the objective value is at most OPT away from the estimate OPT.",5.2. Comparing Running Times of Solvers,[0],[0]
Each instance of the experiment is repeated 100 times (with different sets of labeled vertices) to obtain an average of those T ( )’s and their standard error.,5.2. Comparing Running Times of Solvers,[0],[0]
"For each relative gap , we also report the test error for using a feasible solution that is OPT away from the presumed optimal value OPT.
Results.",5.2. Comparing Running Times of Solvers,[0],[0]
Both solvers have similar performance.,5.2. Comparing Running Times of Solvers,[0],[0]
"As predicted by our theoretical analysis, we see in Figure 5.2 that SG has an advantage when the number n of vertices is much larger than the number m of edges, which is the case for the the last dataset covertype67.",5.2. Comparing Running Times of Solvers,[0],[0]
"Moreover, in Figure 5.3, we see that achieving a relative gap smaller than 10−4 has almost no effect on improving the prediction accuracy.",5.2. Comparing Running Times of Solvers,[0],[0]
"Hence, we can conclude that for either solver, it takes roughly 10 to 20 seconds to produce a solution for the underlying convex program that can give good predic-
tion accuracy.",5.2. Comparing Running Times of Solvers,[0],[0]
DBLP Dataset.,5.3. Directed Hypergraph: More Powerful,[0],[0]
"We use the DBLP (Ley, 2009) dataset.",5.3. Directed Hypergraph: More Powerful,[0],[0]
Each paper is represented by a vertex.,5.3. Directed Hypergraph: More Powerful,[0],[0]
"We include papers from year 2000 to 2015 from conferences belonging to the following research areas to conduct our experiments:
• 7049 papers from machine learning (ML): NIPS, ICML • 2539 papers from theoretical computer science (TCS): STOC, FOCS • 3374 papers from database (DB): VLDB, SIGMOD
We perform the following prediction tasks: (a) ML (+1) vs TCS (-1), and (b) ML (+1) vs DB (-1).
",5.3. Directed Hypergraph: More Powerful,[0],[0]
The details of the experiment setup and the results are given in the full version.,5.3. Directed Hypergraph: More Powerful,[0.9563907094359033],['RESIDE outperforms PCNN+ATT and BGWA which indicates that incorporating side information helps in improving the performance of the model.']
We revisit semi-supervised learning on hypergraphs.,abstractText,[0],[0]
"Same as previous approaches, our method uses a convex program whose objective function is not everywhere differentiable.",abstractText,[0],[0]
"We exploit the non-uniqueness of the optimal solutions, and consider confidence intervals which give the exact ranges that unlabeled vertices take in any optimal solution.",abstractText,[0],[0]
"Moreover, we give a much simpler approach for solving the convex program based on the subgradient method.",abstractText,[0],[0]
"Our experiments on real-world datasets confirm that our confidence interval approach on hypergraphs outperforms existing methods, and our sub-gradient method gives faster running times when the number of vertices is much larger than the number of edges.",abstractText,[0],[0]
Re-revisiting Learning on Hypergraphs:  Confidence Interval and Subgradient Method,title,[0],[0]
"The Fisher Information Metric (FIM) I(Θ) = (Iij) of a statistical parametric model p(x |Θ) of order D is defined by a D×D positive semidefinite (psd) matrix (I(Θ) 0) with coefficients Iij = Ep [ ∂l ∂Θi ∂l ∂Θj ] , where l(Θ) denotes the log-density function log p(x |Θ).",1. Fisher Information Metric,[0],[0]
"Under light regularity conditions, FIM can be rewritten equivalently as
Iij = −Ep",1. Fisher Information Metric,[0],[0]
"[ ∂2l
∂Θi∂Θj
] = 4 ∫ ∂ √ p(x |Θ) ∂Θi ∂ √ p(x |Θ) ∂Θj dx.
",1. Fisher Information Metric,[0],[0]
"As its empirical counterpart, the observed FIM (Efron & Hinkley, 1978) with respect to (wrt) a sample set Xn = {xk}nk=1 is Î(Θ |Xn) = −∇2l(Θ",1. Fisher Information Metric,[0],[0]
"|Xn), which is often evaluated at the maximum likelihood estimate Θ = Θ̂(Xn).",1. Fisher Information Metric,[0],[0]
"By the law of large numbers, Î(Θ) converges to the (expected) FIM I(Θ) as n→∞.
1King Abdullah University of Science and Technology (KAUST), Saudi Arabia 2École Polytechnique, France 3Sony Computer Science Laboratories Inc., Japan.",1. Fisher Information Metric,[0],[0]
"Correspondence to: Ke Sun <sunk@ieee.org>, Frank Nielsen <Frank.Nielsen@acm.org>.
",1. Fisher Information Metric,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Fisher Information Metric,[0],[0]
"Copyright 2017 by the author(s).
",1. Fisher Information Metric,[0],[0]
The FIM is not invariant and depends on the parameterization.,1. Fisher Information Metric,[0],[0]
We can optionally write I(Θ) as IΘ(Θ) to emphasize the coordinate system.,1. Fisher Information Metric,[0],[0]
"By definition, IΘ(Θ) = JᵀIΛ(Λ)J where J = (Jij), Jij = ∂Λi∂Θj is the Jacobian matrix.",1. Fisher Information Metric,[0],[0]
"For example, the FIM of regular natural exponential families (NEFs) l(Θ) = Θᵀt(x)",1. Fisher Information Metric,[0],[0]
− F (Θ) (loglinear models with sufficient statistics t(x)) is I(Θ),1. Fisher Information Metric,[0],[0]
"= ∇2F (Θ) 0, the Hessian of the log-normalizer function F (Θ).",1. Fisher Information Metric,[0],[0]
"Although exponential families can approximate arbitrarily any smooth density (Cobb et al., 1983), the lognormalizer function may not be available in closed-form nor computationally tractable (Montanari, 2015).
",1. Fisher Information Metric,[0],[0]
The FIM is an important concept for statistical machine learning.,1. Fisher Information Metric,[0],[0]
"It gives a Riemannian metric (Hotelling, 1929; Rao, 1945) of the learning parameter space which is unique (Čencov, 1982; Dowty, 2017).",1. Fisher Information Metric,[0],[0]
"Hence any learning is in a space that is intrinsically curved based on the FIM, regardless of the choice of the coordinate system.",1. Fisher Information Metric,[0],[0]
"It also gives a bound (Fréchet, 1943; Cramér, 1946; Nielsen, 2013) of learning efficiency saying that the variance of any unbiased learning of Θ is at least I−1(Θ)/n, where n is the i.i.d. sample size.",1. Fisher Information Metric,[0],[0]
"The FIM is applied to neural network optimization (Amari, 1997), metric learning (Lebanon, 2005), reinforcement learning (Thomas, 2014) and manifold learning (Sun & Marchand-Maillet, 2014).
",1. Fisher Information Metric,[0],[0]
However computing the FIM is expensive.,1. Fisher Information Metric,[0],[0]
"Besides the fact that learning machines have often singularities (Watanabe, 2009) (|I(Θ)| = 0, not full rank) characterized by plateaux in gradient learning, computing/estimating the FIM of a large neuron system (e.g. one with millions of parameters, Szegedy, Christian et al. 2015) is very challenging due to the finiteness of data, and the huge number D(D+1)2 of matrix coefficients to evaluate.",1. Fisher Information Metric,[0.9536256280073298],"['Distant Supervision (DS) (Mintz et al., 2009) helps with the construction of this dataset automatically, under the assumption that if two entities have a relationship in a KB, then all sentences mentioning those entities express the same relation.']"
"Furthermore, gradient descent techniques require inverting this large matrix and tuning the learning rate.
",1. Fisher Information Metric,[0],[0]
"To tackle this problem, past works mainly focus on how to approximate the FIM with a block diagonal form (Kurita, 1994; Le Roux et al., 2008; Martens, 2010; Pascanu & Bengio, 2014; Martens & Grosse, 2015) or quasi-diagonal form (Ollivier, 2013; Marceau-Caron & Ollivier, 2016).",1. Fisher Information Metric,[0],[0]
"This global approach faces increasing approximation error and increasing computational cost as the system scales up
and as complex and dynamic structures (Looks et al., 2017) emerge.
",1. Fisher Information Metric,[0],[0]
This work aims at a different local approach.,1. Fisher Information Metric,[0],[0]
"The idea is to accurately describe the information geometry (IG) in a subsystem of the large learning system, which is invariant to the scaling up and structural change of the global system, so that the local machinery, including optimization, can be discussed regardless of the other parts.
",1. Fisher Information Metric,[0.9505303361120563],"['RESIDE makes principled use of entity type and relation alias information from KBs, to impose soft constraints while predicting the relation.']"
"For this purpose, a novel concept, the Relative Fisher Information Metric (RFIM), is defined.",1. Fisher Information Metric,[0],[0]
"Unlike the traditional geometric view of a high-dimensional parameter manifold, RFIMs defines multiple projected low-dimensional geometries of subsystems.",1. Fisher Information Metric,[0],[0]
This geometry is correlated to the parameters beyond the subsystem and is therefore considered dynamic.,1. Fisher Information Metric,[0],[0]
It can be used to characterize the efficiency of a local learning process.,1. Fisher Information Metric,[0],[0]
Taking this stance has potential in deep learning because a deep neural network can be decomposed into many local components such as neurons or layers.,1. Fisher Information Metric,[0],[0]
The RFIM is well suited to the compositional block structures of neural networks.,1. Fisher Information Metric,[0],[0]
"The RFIM can be used for out-of-core learning.
",1. Fisher Information Metric,[0],[0]
The paper is organized as follows.,1. Fisher Information Metric,[0],[0]
Sec. 2 reviews natural gradient within the context of Multi-Layer Perceptrons (MLPs).,1. Fisher Information Metric,[0],[0]
"Sec. 3 formally defines the RFIM, and gives a table of RFIMs of several commonly used subsystems.",1. Fisher Information Metric,[0],[0]
Sec. 4 discusses the advantages of using the RFIM as compared to the FIM. Sec. 5 gives an algorithmic framework and proof-of-concept experiments on neural network optimization.,1. Fisher Information Metric,[0],[0]
Sec. 6 presents related works on parameter diagonalization.,1. Fisher Information Metric,[0],[0]
Sec. 7 concludes this work and further hints at perspectives.,1. Fisher Information Metric,[0],[0]
"Consider a MLP x θ1−→ h1 · · ·hL−1 θL−−→ y, whose statistical model is the following conditional distribution
p(y |x,Θ) = ∑
h1,··· ,hL−1
p(h1 |x,θ1) · · · p(y |hL−1,θL).
",2. Natural Gradient: Review and Insights,[0],[0]
"The often intractable sum over h1, · · · ,hL−1 can be get rid off by deteriorating p(h1 |x,θ1), · · · , p(hL−1 |hL−2,θL−1) to Dirac’s deltas δ, and letting merely the last layer p(y |hL−1,θL) be stochastic.",2. Natural Gradient: Review and Insights,[0],[0]
"Other models such as restricted Boltzmann machines (Nair & Hinton, 2010; Montavon & Müller, 2012), deep belief networks (Hinton et al., 2006), dropout (Wager et al., 2013), and variational autoencoders (Kingma & Welling, 2014) do consider the hi’s to be stochastic.
",2. Natural Gradient: Review and Insights,[0],[0]
"The tensor metric of the neuromanifold (Amari, 1995) M, consisting of all MLPs with the same architecture but different parameter values, is locally defined by the FIM.",2. Natural Gradient: Review and Insights,[0],[0]
"Because a MLP corresponds to a con-
ditional distribution, its FIM is a function of the input x. By taking an empirical average over the input samples {xk}nk=1, the FIM of a MLP can be expressed as IΘ(Θ) = 1n",2. Natural Gradient: Review and Insights,[0],[0]
"∑n k=1Ep(y |xk,Θ) [ ∂lk ∂Θ ∂lk ∂Θᵀ ] , where lk(Θ) = log p(y |xk, Θ) denotes the conditional log-likelihood function wrt xk.
",2. Natural Gradient: Review and Insights,[0],[0]
"To understand the meaning of the Riemannian metric IΘ(Θ), it measures the intrinsic difference between two nearby neural networks around Θ ∈ M. A learning step can be regarded as a tiny displacement δΘ",2. Natural Gradient: Review and Insights,[0],[0]
onM.,2. Natural Gradient: Review and Insights,[0],[0]
"According to the FIM, the infinitesimal square distance
〈δΘ, δΘ〉IΘ(Θ) = 1
n n∑ k=1 Ep(y |xk,Θ)
[( δΘᵀ
∂lk ∂Θ )",2. Natural Gradient: Review and Insights,[0],[0]
"2] (1)
measures how much δΘ",2. Natural Gradient: Review and Insights,[0],[0]
"(with a radius constraint) is statistically along ∂l∂Θ , or equivalently how much δΘ affects intrinsically the conditional distribution p(y |x, Θ).
",2. Natural Gradient: Review and Insights,[0],[0]
Consider the negative log-likelihood function L(Θ) =,2. Natural Gradient: Review and Insights,[0],[0]
"− ∑n k=1 log p(yk |xk,Θ) wrt the observed pairs {(xk,yk)}nk=1, we try to minimize the loss while maintaining a small learning step size 〈δΘ, δΘ〉IΘ(Θ) on M. At Θt ∈ M, the target is to minimize wrt δΘ",2. Natural Gradient: Review and Insights,[0],[0]
"the Lagrange function
L(Θt + δΘ)",2. Natural Gradient: Review and Insights,[0],[0]
"+ 1
2γ 〈δΘ, δΘ〉IΘ(Θt)
",2. Natural Gradient: Review and Insights,[0],[0]
"≈ L(Θt) + δΘᵀ 5Θ L(Θt) + 1
2γ δΘᵀIΘ(Θt)δΘ,
where γ > 0 is a learning rate.",2. Natural Gradient: Review and Insights,[0],[0]
"The optimal solution of the above quadratic optimization gives a learning step
δΘt = −γI−1Θ (Θt)5Θ L(Θt).
",2. Natural Gradient: Review and Insights,[0],[0]
"In this update procedure, ∇̃ΘL(Θ) = I−1Θ (Θ)5Θ L(Θ) replaces the role of the usual gradient ∇ΘL(Θ) and is called the natural gradient (Amari, 1997).
",2. Natural Gradient: Review and Insights,[0],[0]
"Although the FIM depends on the chosen parameterization, the natural gradient is invariant to reparameterization.",2. Natural Gradient: Review and Insights,[0],[0]
Let Λ be another coordinate system and J be the Jacobian matrix of the mapping,2. Natural Gradient: Review and Insights,[0],[0]
"Θ→ Λ. Then we have
I−1Θ (Θ)5Θ L(Θ) =",2. Natural Gradient: Review and Insights,[0],[0]
"(J ᵀIΛ(Λ)J)−1 Jᵀ 5Λ L(Λ)
",2. Natural Gradient: Review and Insights,[0],[0]
"= J−1I−1Λ (Λ)5Λ L(Λ),
showing that ∇̃ΘL(Θ) and ∇̃ΛL(Λ) are the same dynamic up to coordinate transformation.",2. Natural Gradient: Review and Insights,[0],[0]
"As the learning rate γ is not infinitesimal in practice, natural gradient descent actually depends on the coordinate system (see e.g. Martens 2014).",2. Natural Gradient: Review and Insights,[0],[0]
"Other intriguing properties of natural gradient optimization lie in being free from getting trapped in plateaux of the error surface, and attaining Fisher efficiency in online learning (see Sec. 4 Amari 1998).
",2. Natural Gradient: Review and Insights,[0],[0]
"MΘ
Θ yx
Mθ1
x
x+ ∆x
θ1x
Mθ2h1
h1 + ∆h1
θ2h1
Mθ3
h2
h2 + ∆h2
θ3h2",2. Natural Gradient: Review and Insights,[0],[0]
"y
Model:
Manifold:
Computational graph:
Metric:
Θ
Θ I(Θ)
θ3 h2
θ3
h2
gy(θ3)
",2. Natural Gradient: Review and Insights,[0],[0]
"θ2 h1
θ2
h1
gh2(θ2)
θ1
θ1 gh1(θ1)
p(y |Θ,x) =",2. Natural Gradient: Review and Insights,[0],[0]
"∑ h1 ∑ h2 p(h1 |θ1,x) p(h2 |θ2,h1) p(y |θ3,h2)
",2. Natural Gradient: Review and Insights,[0],[0]
Figure 1.,2. Natural Gradient: Review and Insights,[0],[0]
(left),2. Natural Gradient: Review and Insights,[0],[0]
The traditional global geometry of a MLP; (right) information geometry of subsystems.,2. Natural Gradient: Review and Insights,[0],[0]
The gray and blue meshes show that the subsystem geometry is dynamic when the reference variable makes a tiny move.,2. Natural Gradient: Review and Insights,[0],[0]
"The square under the (sub-)system means the (R-)FIM is computed by (i) computing the FIM in the traditional way wrt all free parameters that affect the system output; (ii) choosing a sub-block that contains only the internal parameters of the (sub-)system and regarding the remaining variables as the reference.
",2. Natural Gradient: Review and Insights,[0],[0]
"For the sake of simplicity, we do not discuss singular FIMs with a subset of parameters having zero metric.",2. Natural Gradient: Review and Insights,[0],[0]
"This set of parameters forms an analytic variety (Watanabe, 2009), and technically the MLP as a statistical model is said to be non-regular (and the parameter Θ is not identifiable).",2. Natural Gradient: Review and Insights,[0],[0]
"The natural gradient has been extended (Thomas, 2014) to cope with singular FIMs having positive semi-definite matrices by taking the Moore-Penrose pseudo-inverse (that coincides with the inverse matrix for full rank matrices).
",2. Natural Gradient: Review and Insights,[0],[0]
"In the family of 2nd-order optimization methods, a fuzzy line can be drawn from the natural gradient and alternative methods such as the Hessian-free optimization (Martens, 2010).",2. Natural Gradient: Review and Insights,[0],[0]
"By definition, the FIM is a property of the parameter space which is independent or weakly dependent on the input samples.",2. Natural Gradient: Review and Insights,[0],[0]
"For example, the FIM of a MLP is independent of {yi}.",2. Natural Gradient: Review and Insights,[0],[0]
"In contrast, the Hessian (or related concepts such as the Gauss-Newton matrix, Martens 2014) is a property of the learning cost function wrt the input samples.
",2. Natural Gradient: Review and Insights,[0],[0]
"Bonnabel (Bonnabel, 2013) proposed to use the Riemannian exponential map to define a gradient descent step, thus ensuring to stay on the manifold for any chosen learning rate.",2. Natural Gradient: Review and Insights,[0],[0]
Convergence is proven for Hadamard manifolds (of negative curvatures).,2. Natural Gradient: Review and Insights,[0],[0]
"However, it is not mathematically tractable to express the exponential map of hierarchical model manifolds like the neuromanifold.",2. Natural Gradient: Review and Insights,[0],[0]
"In general, for large parametric systems, it is impossible to diagonalize or decorrelate all the parameters, so that we split instead all random variables into three parts θf , θ and h.",3. RFIM: Definition and Expressions,[0],[0]
We examine their intuitive meanings before giving the formal definition.,3. RFIM: Definition and Expressions,[0],[0]
"The reference, θf , consists of the majority of the random variables that are considered fixed (therefore allowing us to simplify the analysis).",3. RFIM: Definition and Expressions,[0],[0]
This is in analogy to the notion of a reference frame in physics.,3. RFIM: Definition and Expressions,[0],[0]
"θ is the
subsystem parameters, resembling the long-term memory adapting slowly to the observations (e.g. neural network weights).",3. RFIM: Definition and Expressions,[0],[0]
The response h is a random variable that reacts to the variations of θ.,3. RFIM: Definition and Expressions,[0],[0]
"Usually, h is the output of the subsystem that is connected to neighbour subsystems (e.g. hidden layer outputs).",3. RFIM: Definition and Expressions,[0],[0]
"Formally, a subsystem which factorizes the learning machine is characterized by the conditional distribution p(h |θ,θf ), where θ can be estimated based on h and θf .",3. RFIM: Definition and Expressions,[0],[0]
We make the following definition.,3. RFIM: Definition and Expressions,[0],[0]
Definition 1 (RFIM).,3. RFIM: Definition and Expressions,[0],[0]
"Given θf , the RFIM 1 of θ wrt h is
gh (θ |θf )",3. RFIM: Definition and Expressions,[0],[0]
"def = Ep(h | θ, θf ) [ ∂
∂θ log p(h |θ, θf )
∂
∂θᵀ log p(h |θ, θf )
] ,
or simply gh (θ), corresponding to the estimation of θ based on observations of h given θf .
",3. RFIM: Definition and Expressions,[0],[0]
"For example, consider a MLP.",3. RFIM: Definition and Expressions,[0],[0]
"If we choose θf to be the input features x, choose h to be the final output y, and choose θ to be all the network weights Θ, then the RFIM becomes the FIM: I(Θ) = gy(Θ |x).
",3. RFIM: Definition and Expressions,[0],[0]
"More generally, we can choose the response h to be other than the observables to compute the Fisher information of subsystems, especially dynamically during the learning of the global machine.",3. RFIM: Definition and Expressions,[0],[0]
"To see the meaning of the RFIM, similar to eq.",3. RFIM: Definition and Expressions,[0],[0]
"(1), the infinitesimal square distance 〈δθ, δθ〉gh(θ) =",3. RFIM: Definition and Expressions,[0],[0]
"Ep(h | θ, θf ) [( δθᵀ ∂∂θ log p(h |θ, θf )
)2] measures how much δθ impacts intrinsically the stochastic mapping θ → h which features the subsystem.",3. RFIM: Definition and Expressions,[0],[0]
"We have the following proposition following definition 1.
",3. RFIM: Definition and Expressions,[0],[0]
Proposition 2 (Relative Geometry Consistency).,3. RFIM: Definition and Expressions,[0],[0]
"If θ1 consists of a subset of θ2 so that θ2 = (θ1, θ̃1), then ∀θ̃1, Mθ1 with the metric gh(θ1 | θ̃1) has exactly the same Rie-
1We use the same term “relative FIM” (Zegers, 2015) with a different definition.
",3. RFIM: Definition and Expressions,[0],[0]
"mannian metric with the sub-manifold {θ2 ∈ Mθ2 : θ̃1 is fixed} induced by the ambient metric gh (θ2).
",3. RFIM: Definition and Expressions,[0],[0]
"When the response h is chosen, then different splits of (θ,θf ) are consistent with the same ambient geometry.
",3. RFIM: Definition and Expressions,[0],[0]
"Figure 1 shows the traditional global geometry of a learning system, where the curvature is defined by the learner’s parameter sensitivity to the external environment (x and y), as compared to the information geometry of subsystems, where the curvature is defined by the parameter sensitivity wrt hidden interface variables h.",3. RFIM: Definition and Expressions,[0],[0]
"The two-colored meshes show that the geometry structure is dynamic and varies with the reference variable θf .
",3. RFIM: Definition and Expressions,[0],[0]
"One should not confuse the RFIM with the diagonal blocks of the FIM (Kurita, 1994).",3. RFIM: Definition and Expressions,[0],[0]
Both their meanings and expressions are different.,3. RFIM: Definition and Expressions,[0],[0]
The RFIM is computed by integrating out the hidden response variables h.,3. RFIM: Definition and Expressions,[0],[0]
The FIM is always computed by integrating out the observables x and y.,3. RFIM: Definition and Expressions,[0],[0]
Hence the RFIM is a more general concept and includes the FIM as a special case.,3. RFIM: Definition and Expressions,[0],[0]
"This highlights a main difference with the backpropagated metric (Ollivier, 2013), which essentially considers parameter sensitivity wrt the final output.",3. RFIM: Definition and Expressions,[0],[0]
"Despite the fact that the FIMs of small parametric structures such as single neurons was studied (Amari, 1997), we are not looking at a small single-component system but a component embedded in a large system, targeting at improving the large system.
",3. RFIM: Definition and Expressions,[0],[0]
"In the following we provide a short table of commonly used RFIMs for future reference (the RFIMs listed are mostly straightforward from definition 1, with detailed derivations given in the supplementary material).",3. RFIM: Definition and Expressions,[0],[0]
This is meaningful since the RFIM is a new concept.,3. RFIM: Definition and Expressions,[0],[0]
We also want to demonstrate these simple closed form expressions without any approximations.,3. RFIM: Definition and Expressions,[0],[0]
We start from the RFIM of single neuron models.,3.1. RFIMs of One Neuron,[0],[0]
"Consider a stochastic neuron with input x and weights w. After a nonlinear activation function f , the output y is randomized surrounding the mean f(wᵀx̃) with a variance.",3.1. RFIMs of One Neuron,[0],[0]
"Throughout this paper x̃ = (xᵀ, 1)ᵀ denotes the augmented vector of x (homogeneous coordinates) so that wᵀx̃ contains a bias term, and a general linear transformation can be written simply asAx̃.
Using x as the reference, the RFIM of w with respect to y has a common form gy(w |x)",3.1. RFIMs of One Neuron,[0],[0]
"= νf (w,x)x̃x̃ᵀ, where νf (w,x) is a positive coefficient with large values in the linear region, or the effective learning zone of the neuron.",3.1. RFIMs of One Neuron,[0],[0]
"This agrees with early studies on single neuron FIMs (Amari, 1997; Kurita, 1994).
",3.1. RFIMs of One Neuron,[0],[0]
"If f(t) = tanh(t) is the hyperbolic tangent func-
tion, then νf (w,x) = sech2(wᵀx̃), where sech(t) = 2 exp(t)+exp(−t) is the hyperbolic secant function.",3.1. RFIMs of One Neuron,[0],[0]
"Similarly, if f(t) = sigm(t) is the sigmoid function, then νf (w,x) = sigm (w ᵀx̃)",3.1. RFIMs of One Neuron,[0],[0]
"[ 1− sigm (wᵀx̃) ] .
",3.1. RFIMs of One Neuron,[0],[0]
"If f is defined by Parametric Rectified Linear Unit (PReLU) (He et al., 2015), which includes Rectified Linear Unit (ReLU) (Nair & Hinton, 2010) as a special case, so that f(t) = t (t ≥ 0), f(t) = ιt (t < 0), 0 ≤ ι < 1, then under certain approximations (see supplementary material)
",3.1. RFIMs of One Neuron,[0],[0]
"νf (w,x) =
[ ι+ (1− ι)sigm",3.1. RFIMs of One Neuron,[0],[0]
"( 1− ι ω wᵀx̃ )]2 ,
where ω > 0 is a hyper-parameter (e.g. ω = 1).
",3.1. RFIMs of One Neuron,[0],[0]
"For the exponential linear unit (ELU) (Clevert et al., 2015), f(t) = t (t ≥ 0), f(t) = α (exp(t)− 1) (t < 0), where α > 0 is a hyper-parameter.",3.1. RFIMs of One Neuron,[0],[0]
"We get
νf (w,x) =",3.1. RFIMs of One Neuron,[0],[0]
{ 1 if wᵀx̃ ≥ 0 α2 exp (2wᵀx̃),3.1. RFIMs of One Neuron,[0],[0]
if wᵀx̃ < 0.,3.1. RFIMs of One Neuron,[0],[0]
Let D denote the dimensionality of the corresponding variable.,3.2. RFIM of One Layer,[0],[0]
"A linear layer with input x, connection weights W =",3.2. RFIM of One Layer,[0],[0]
"[ w1, · · · ,wDy ] , and stochastic output y can be represented by y ∼ G(W ᵀx̃, σ2I), where I is the identity matrix, and σ is the scale of the observation noise, and G(µ,Σ) is a multivariate Gaussian distribution with mean µ and covariance matrix Σ. We vectorize W by stacking its columns {wi}.",3.2. RFIM of One Layer,[0],[0]
"Then gy(W |x) is a tensor of size (Dx + 1)Dy× (Dx + 1)Dy , given by gy(W |x)",3.2. RFIM of One Layer,[0],[0]
"= diag [x̃x̃ᵀ, · · · , x̃x̃ᵀ], where diag(·) means the (block) diagonal matrix constructed by the given matrix entries.
",3.2. RFIM of One Layer,[0],[0]
"A nonlinear layer increments a linear layer by adding an element-wise activation function applied on W ᵀx̃, and then randomized wrt the choice of the neuron.",3.2. RFIM of One Layer,[0],[0]
"By definition 1, its RFIM is given by
gy (",3.2. RFIM of One Layer,[0],[0]
W |x) =,3.2. RFIM of One Layer,[0],[0]
"diag [ νf (w1,x)x̃x̃ ᵀ, · · · , νf (wm,x)x̃x̃ᵀ ] , (2)
where νf (wi,x) is given in Subsec.",3.2. RFIM of One Layer,[0],[0]
"3.1.
",3.2. RFIM of One Layer,[0],[0]
"A softmax layer, which often appears as the last layer of a MLP, is given by y ∈ {1, . . .",3.2. RFIM of One Layer,[0],[0]
",m}, where p(y) = ηy = exp(wyx̃)∑m i=1 exp(wix̃) .",3.2. RFIM of One Layer,[0],[0]
"Its RFIM is a dense matrix given by
gy(W )",3.2. RFIM of One Layer,[0],[0]
=  (η1 − η21)x̃x̃ᵀ · · · −η1ηmx̃x̃ᵀ −η2η1x̃x̃ᵀ · · · −η2ηmx̃x̃ᵀ ... . .,3.2. RFIM of One Layer,[0],[0]
".
...",3.2. RFIM of One Layer,[0],[0]
−ηmη1x̃x̃ᵀ · · · (ηm − η2m)x̃x̃ᵀ  .,3.2. RFIM of One Layer,[0],[0]
Notice that its i’th diagonal block (ηi − η2i ),3.2. RFIM of One Layer,[0],[0]
x̃x̃ᵀ resembles the RFIM of a single sigm neuron.,3.2. RFIM of One Layer,[0],[0]
"By eq. (2), the one-layer RFIM is a product metric (Jost, 2011) and does not consider the inter-neuron correlations, which must be obtained by looking at a larger subsystem.",3.3. RFIM of Two Layers,[0],[0]
"Consider a two-layer model with stochastic output y around the mean vector f(Cᵀh̃), where h = f (W ᵀx̃).",3.3. RFIM of Two Layers,[0],[0]
"For simplicity, we ignore inter-layer correlations between the first layer and the second layer and focus on the interneuron correlations within the first layer.",3.3. RFIM of Two Layers,[0],[0]
"To do this, both x and C are considered as references to compute the RFIM of W .",3.3. RFIM of Two Layers,[0],[0]
"By definition 1, gy(W |x,C) =",3.3. RFIM of Two Layers,[0],[0]
"[Gij ]Dh×Dh and each block has the form
Gij = Dy∑ l=1 cilcjlνf (cl,h)νf (wi,x)νf (wj ,x)x̃x̃ ᵀ.
Now that we have the one-layer and two-layer RFIMs, we can either split a given feed-forward neural network into one-layer subsystems or into two-layer subsystems.",3.3. RFIM of Two Layers,[0],[0]
"A trade-off is that using a larger subsystem entails greater analytical and computational difficulty, although it could more accurately model the global system dynamics.",3.3. RFIM of Two Layers,[0],[0]
"In the extreme case, the FIM is obtained if the whole system is considered as one single subsystem.",3.3. RFIM of Two Layers,[0],[0]
This section discusses the theoretical advantages of the RFIM over the FIM.,4. RFIM: Key Advantages,[0],[0]
"Consider wlog a MLP with Bernoulli outputs y ∈ {0, 1}m, whose mean µ is a deterministic function depending on the input x and the network parameters Θ. By Sec. 2, the FIM of the MLP can be computed as (see supplementary for proof)
I(Θ)",4. RFIM: Key Advantages,[0],[0]
= 1 n n∑ i=1,4. RFIM: Key Advantages,[0],[0]
"m∑ j=1
1 µj(xi)(1− µj(xi))",4. RFIM: Key Advantages,[0],[0]
"∂µj(xi) ∂Θ ∂µj(xi) ∂Θᵀ .
",4. RFIM: Key Advantages,[0],[0]
(3) Therefore rank(I(Θ)),4. RFIM: Key Advantages,[0],[0]
≤,4. RFIM: Key Advantages,[0],[0]
nm.,4. RFIM: Key Advantages,[0],[0]
The rank of a diagonal block of I(Θ) corresponding to one layer is even smaller.,4. RFIM: Key Advantages,[0],[0]
"In a deep neural network (e.g. Szegedy, Christian et al. 2015), if the sample size n < dim(Θ)/m, then I(Θ) is doomed to be singular.",4. RFIM: Key Advantages,[0],[0]
All methods trying to approximate the FIM suffer from this problem and therefore rely on proper regularizations.,4. RFIM: Key Advantages,[0],[0]
"If the network is decomposed into layers, the RFIM of each subsystem (layer) is given by eq.",4. RFIM: Key Advantages,[0],[0]
(2).,4. RFIM: Key Advantages,[0],[0]
Each sample can contribute maximally 1 to the rank of the neuron-RFIM and can contribute maximally Dy to the rank of the layer-RFIM.,4. RFIM: Key Advantages,[0],[0]
"It only requires maxi{dim(wi)} (the maximum layer width) observations to have a full rank RFIM, where wi is the weight vector of the i’th neuron.",4. RFIM: Key Advantages,[0],[0]
The RFIM is expected to have a much higher rank than the FIM.,4. RFIM: Key Advantages,[0],[0]
Higher rank means less singularity and more information is captured.,4. RFIM: Key Advantages,[0],[0]
"Models that can
be distinguished by the RFIM may be identical in the sense of the FIM.",4. RFIM: Key Advantages,[0],[0]
"Essentially, the RFIM integrates the internal randomness (Bengio, 2013) of the neural system by considering the output of each layer as a random variable.",4. RFIM: Key Advantages,[0],[0]
"In theory, the FIM should also consider stochastic neurons.",4. RFIM: Key Advantages,[0],[0]
"However it requires marginalizing the joint distribution of h1, h2, · · · , y. This makes the already infeasible computation even more challenging.
",4. RFIM: Key Advantages,[0],[0]
"The RFIM is not an approximation of the FIM but is an accurate metric, defining the geometry of θ wrt to its direct response h in the system, or adjacent nodes in a graphical model.",4. RFIM: Key Advantages,[0],[0]
By the example in fig.,4. RFIM: Key Advantages,[0],[0]
"1, gy(θL) of the last layer is exactly the corresponding block in I(Θ): they both characterize how θL affects the mapping hL−1",4. RFIM: Key Advantages,[0],[0]
→ y. They start to diverge from the second to last layer.,4. RFIM: Key Advantages,[0],[0]
"To compute the geometry of θL−1, the RFIM looks at how θL−1 affects the local mapping hL−2 → hL−1, which can be measured reliably regardless of the rest of the system (think of a “debugging” process to separate and measure a single component).",4. RFIM: Key Advantages,[0],[0]
"In contrast, the FIM examines how θL−1 affects the non-local mapping hL−2 → y.",4. RFIM: Key Advantages,[0],[0]
This is a difficult task because it must consider the correlation between different layers.,4. RFIM: Key Advantages,[0],[0]
"As an approximation, the block diagonalized version of the FIM ignores such correlations and therefore faces the loss of accuracy.
",4. RFIM: Key Advantages,[0],[0]
The RFIM makes it possible to maintain global system stability so that the intrinsic variations of different subsystems are balanced during learning.,4. RFIM: Key Advantages,[0],[0]
Consider a set of interconnected subsystems with internal parameters {θl} and the corresponding response variables {hl}.,4. RFIM: Key Advantages,[0],[0]
The RFIM ghl(θl) measures how much the likelihood surface of hl is curved wrt a small learning step δθl.,4. RFIM: Key Advantages,[0],[0]
"By constraining the squared Riemannian distance δθᵀl g
hl(θl)δθl having similar scales, different subsystems will present similar variations during learning.",4. RFIM: Key Advantages,[0],[0]
"Within one subsystem, the learning along sensitive parameter directions is penalized.",4. RFIM: Key Advantages,[0],[0]
"Among different subsystems, the learning of sensitive subsystems is penalized.",4. RFIM: Key Advantages,[0],[0]
"Globally, the inter-subsystem stochastic connections have similar variance, maintaining a stable reference system and achieving efficient learning.",4. RFIM: Key Advantages,[0],[0]
"This is similar to the idea of batch normalization (BN) (Ioffe & Szegedy, 2015) but has a deeper theoretical foundation.
",4. RFIM: Key Advantages,[0],[0]
"Formally, we have the following theorem.
",4. RFIM: Key Advantages,[0],[0]
Theorem 3.,4. RFIM: Key Advantages,[0],[0]
"Consider a learning system represented by a joint distribution p(x,h) of x (observables) and h (hidden variables which connect subsystems).",4. RFIM: Key Advantages,[0],[0]
"The joint FIM J (Θ) = Ep ( log p(x,h |Θ) ∂Θ",4. RFIM: Key Advantages,[0],[0]
"log p(x,h |Θ) ∂Θᵀ ) has a block diagonal form.",4. RFIM: Key Advantages,[0],[0]
"Each block isEp(gh(θ)), where θ is the parameters within a subsystem and h is its response variables to neighour subsystems.
",4. RFIM: Key Advantages,[0],[0]
"The global correspondence of the local RFIM is the joint
FIM.",4. RFIM: Key Advantages,[0],[0]
"By theorem 3, the square distance dΘᵀJ (Θ)dΘ = Ep( ∑ l dθ ᵀ l g hl(θl)dθl) measures the system variance, including both the observables x and the hidden variables h.",4. RFIM: Key Advantages,[0],[0]
An intrinsic trade-off between the RFIM and the FIM is learning system stability versus efficiency.,4. RFIM: Key Advantages,[0],[0]
"Normalizing the FIM is more efficient because it helps to achieve Fisher efficiency (Amari, 1998).",4. RFIM: Key Advantages,[0],[0]
"Normalizing the RFIM is more stable since the hidden variations are bounded, which only guarantees subsystem Fisher efficiency characterized by the Cramér-Rao lower bound of local parameters.",4. RFIM: Key Advantages,[0],[0]
The traditional non-parametric way of applying natural gradient requires re-calculating the FIM and solving a large linear system in each learning step.,5. Relative Natural Gradient Descent,[0],[0]
"Besides the huge computational cost, it has a large approximation error.",5. Relative Natural Gradient Descent,[0],[0]
"For example during online learning, a mini-batch of samples cannot faithfully reflect the “true” geometry, which has to integrate the risk of sample variations.",5. Relative Natural Gradient Descent,[0],[0]
"That is, the FIM of a mini-batch is likely to be singular or poorly conditioned.
",5. Relative Natural Gradient Descent,[0],[0]
"A recent series of efforts (Montavon & Müller, 2012; Raiko et al., 2012; Desjardins et al., 2015) are gearing towards a parametric approach to applying natural gradient, which memorizes and learns a geometry.",5. Relative Natural Gradient Descent,[0],[0]
"For example, natural neural networks (Desjardins et al., 2015) augment each layer with a redundant linear layer, and let these linear layers parametrize the geometry of the neural manifold.
",5. Relative Natural Gradient Descent,[0],[0]
"By dividing the learning system into subsystems, the RFIM potentially gives a systematical implementation of parametric natural gradient descent.",5. Relative Natural Gradient Descent,[0],[0]
"The memory complexity of storing the Riemannian metric has been reduced from O(D2) to O( ∑ iD 2 i ), where Di = dim(wi) is the size of the i’th neuron.",5. Relative Natural Gradient Descent,[0],[0]
"Consider there are M neurons in total, then the memory cost is reduced by a factor of M .",5. Relative Natural Gradient Descent,[0],[0]
"The computational complexity has been reduced from O(D%) (% ≈ 2.373, Williams 2012) to O( ∑ iD % i ).",5. Relative Natural Gradient Descent,[0],[0]
"Optimization based on RFIM is called Relative Natural Gradient Descent (RNGD).
",5. Relative Natural Gradient Descent,[0],[0]
"The good performance of batch normalization (Ioffe & Szegedy, 2015) provides an empirical support for the RFIM.",5. Relative Natural Gradient Descent,[0],[0]
"Basically, BN uses an inter-sample normalization layer to transform the layer input x to z with zero mean and unit variance and thus reduces “internal covariate shift”.",5. Relative Natural Gradient Descent,[0],[0]
"In a typical case, above this normalization layer is a linear layer given by y = W ᵀz̃.",5. Relative Natural Gradient Descent,[0],[0]
"If each dimension of z is normalized, then the diagonal blocks of the linear layer RFIM gy(W )",5. Relative Natural Gradient Descent,[0],[0]
"= diag[z̃z̃ᵀ, · · · , z̃z̃ᵀ] become a covariance matrix with identity diagonal entries (after taking an empirical average).",5. Relative Natural Gradient Descent,[0],[0]
"This gives the coordinate system W a well conditioned RFIM for efficient learning.
5.1.",5. Relative Natural Gradient Descent,[0],[0]
"RNGD with a relu MLP
",5. Relative Natural Gradient Descent,[0],[0]
This subsection builds a proof-of-concept experiment on MLP optimization.,5. Relative Natural Gradient Descent,[0],[0]
We partition the MLP into layers (one layer consists of a linear layer plus an element-wise nonlinear activation function) as the subsystems.,5. Relative Natural Gradient Descent,[0],[0]
"By eq. (2), the RFIM of layer l (l = 1, · · · , L) with input hl−1 (h0 = x) and weights {wl1, · · · ,wlml} is
diag [ νf (wl1,hl−1)h̃l−1h̃ ᵀ l−1, · · · , νf (wlml ,hl−l)h̃l−1h̃ ᵀ l−1 ] .
",5. Relative Natural Gradient Descent,[0],[0]
The subsystem stability during one learning step δw can be measured geometrically by∑L l=1 ∑ml i=1,5. Relative Natural Gradient Descent,[0],[0]
"νf (wli,hl−1)(δw ᵀ lih̃l−1)
2.",5. Relative Natural Gradient Descent,[0],[0]
"Using this term as the geometric cost (the Lagrange term) in the trust region approach in Sec. 2, we get the following RNGD method.",5. Relative Natural Gradient Descent,[0],[0]
"In a stochastic gradient descent scenario, each neuron i in layer l is updated by
wnewli ← woldli −G−1li ∂E
∂wli ,
where E is the cost function and Gli is a learned metric.",5. Relative Natural Gradient Descent,[0],[0]
"The consideration is that a mini-batch of samples do not contain enough information to compute the RFIM, which should be averaged over all training samples.",5. Relative Natural Gradient Descent,[0],[0]
"Therefore, for the i’th neuron in layer l, Gli is initialized to identity, and is updated based on
Gnewli ← (1− λ)Goldli + λνf (wli,hl−1)h̃l−1h̃ ᵀ",5. Relative Natural Gradient Descent,[0],[0]
l−1,5. Relative Natural Gradient Descent,[0],[0]
+,5. Relative Natural Gradient Descent,[0],[0]
"I,
where > 0 is a hyper-parameter to avoid singularity caused by small sample size, and the average is taken over all samples in a mini-batch, and λ is a learning rate.",5. Relative Natural Gradient Descent,[0],[0]
"In theory, λ should be gradually reduced to zero to guarantee the convergence of this geometry learning.",5. Relative Natural Gradient Descent,[0],[0]
"To avoid solving a linear system in each iteration, every T iterations we recompute and store G−1li based on the most updated Gli.",5. Relative Natural Gradient Descent,[0],[0]
"In the next T iterations, this G−1li will be used as an approximation of the inverse RFIM.",5. Relative Natural Gradient Descent,[0],[0]
"For the input layer which scales with the number of input features, and the final softmax layer, we apply instead the RFIM of the corresponding linear layer to improve the computational efficiency.
",5. Relative Natural Gradient Descent,[0],[0]
We compare different optimizers on classifying MNIST digits.,5. Relative Natural Gradient Descent,[0],[0]
"The network has shape 784-80-80-80-10, with relu activation units, a final soft-max layer, and uses the persample average cross-entropy with L2-regularization as the learning cost function.",5. Relative Natural Gradient Descent,[0],[0]
"We experiment on two different architectures: one is a plain MLP (PLAIN); the other has a batch normalization layer after each hidden layer (BNA), where a rescaling parameter is applied to ensure enough flexibility of the parametric structure (Ioffe & Szegedy, 2015).",5. Relative Natural Gradient Descent,[0],[0]
"For simplicity, the architecture, mini-batch size (50), and L2 regularization strength (10−3) are fixed to be the same for all compared methods.",5. Relative Natural Gradient Descent,[0],[0]
"The observations are consistent when these configurations vary.
",5. Relative Natural Gradient Descent,[0],[0]
Figure 2 shows the learning curves of different methods.,5. Relative Natural Gradient Descent,[0],[0]
SGD is stochastic gradient descent.,5. Relative Natural Gradient Descent,[0],[0]
"ADAM is the Adam optimizer (Kingma & Ba, 2014) with β1 = 0.9, β2 = 0.999 and = 10−8.",5. Relative Natural Gradient Descent,[0],[0]
"Our RNGD is implemented by modifying TensorFlow’s (Abadi, Martı́n",5. Relative Natural Gradient Descent,[0],[0]
"et al., 2015) SGD optimizer.",5. Relative Natural Gradient Descent,[0],[0]
"We set empirically T = 100, λ = 0.005 and ω = 1.
RNGD presents a sharper learning curve and better generalization, especially when it is combined with BN.",5. Relative Natural Gradient Descent,[0],[0]
"In this case, the final tranining error of RNGD is slightly larger than ADAM because by validation it favors a larger learning rate, which is applied on the neural network weights (based on RNGD) and BN parameters (based on SGD).",5. Relative Natural Gradient Descent,[0],[0]
"For the ReLU activation, νf (wi,x) is approximately binary, emphasizing such informative samples with wᵀi x̃ > 0, which are the ones contributing to the learning of wi with non-zero gradient values.",5. Relative Natural Gradient Descent,[0],[0]
Each output neuron has a different subset of informative samples.,5. Relative Natural Gradient Descent,[0],[0]
"RNGD normalizes x differently wrt different output neurons, so that the in-
formative samples for each output neuron are centered and decorrelated.
",5. Relative Natural Gradient Descent,[0],[0]
"In the above experiment, RNGD’s computational time per each epoch is roughly 4 ∼ 10 times more than SGD and ADAM on a modern graphic card.",5. Relative Natural Gradient Descent,[0],[0]
Therefore in terms of wall clock time RNGD does not show advantages.,5. Relative Natural Gradient Descent,[0],[0]
This can be improved by more efficient implementations with low rank approximation techniques and early stopping.,5. Relative Natural Gradient Descent,[0],[0]
Our RNGD prototype hints at a promising direction to develop scalable 2nd-order deep learning optimizers based on the RFIM.,5. Relative Natural Gradient Descent,[0],[0]
One may ponder whether we can always find a suitable parameterization that yields a diagonal FIM that is straightforward to invert.,6. Related Works on FIM Diagonalization,[0],[0]
This fundamental problem of parameter orthogonalization was first investigated by Jeffreys (1998) for decorrelating the parameters of interest from the nuisance parameters.,6. Related Works on FIM Diagonalization,[0],[0]
"Fisher diagonalization yields parameter orthogonalization (Cox & Reid, 1987), and is proved useful when estimating Θ̂ using a maximum likelihood estimator (MLE) that is asymptotically normally distributed, Θ̂n ∼ G(Θ, I−1(Θ)/n), and efficient since the variance of the estimator matches the Cramér-Rao lower bound.",6. Related Works on FIM Diagonalization,[0],[0]
"Using the chain rule, this amounts to find a suitable parameterization Ω = Ω(Θ) satisfying∑
",6. Related Works on FIM Diagonalization,[0],[0]
"i,j
E
[ ∂2l
∂Θi∂Θj ] ∂Θi",6. Related Works on FIM Diagonalization,[0],[0]
"∂Ωk ∂Θj ∂Ωl = 0, ∀k 6=",6. Related Works on FIM Diagonalization,[0],[0]
"l.
Thus in general, we end up with ( D 2 ) = D(D−1)2 (nonlinear) partial differential equations to satisfy (Huzurbazar, 1950).",6. Related Works on FIM Diagonalization,[0],[0]
"Therefore, in general there is no solution when( D 2 )",6. Related Works on FIM Diagonalization,[0],[0]
"> D, that is when D > 3.",6. Related Works on FIM Diagonalization,[0],[0]
"When D = 2, the single differential equation is usually solvable and tractable, and the solution may not be unique: For example, Huzurbazar (1950) reports two orthogonalization schemes for the location-scale families { 1σp0( x−µ σ )} that include the Gaussian family and the Cauchy family.",6. Related Works on FIM Diagonalization,[0],[0]
"Sometimes, the structure of the differential equation system yields a solution: For example, Jeffreys (1998) reported a parameter orthogonalization for Pearson’s distributions of type I which is of orderD = 4.",6. Related Works on FIM Diagonalization,[0],[0]
"Cox and Reid (1987) further investigated this topic with application to conditional inference, and provide examples (including the Weibull distribution).
",6. Related Works on FIM Diagonalization,[0],[0]
"From the viewpoint of geometry, the FIM induces a Riemannian manifold with metric tensor g(Θ) = I(Θ).",6. Related Works on FIM Diagonalization,[0],[0]
"When the FIM may be degenerate, this yields a pseudoRiemannian manifold (Thomas, 2014).",6. Related Works on FIM Diagonalization,[0],[0]
"In differential geometry, orthogonalization amounts to transforming the square length infinitesimal element gijdΘiΘj of a Riemannian geometry into an orthogonal system ω with match-
ing square length infinitesimal element ΩiidΩ2i .",6. Related Works on FIM Diagonalization,[0],[0]
"However, such a global orthogonal metric does not exist (Huzurbazar, 1950)",6. Related Works on FIM Diagonalization,[0],[0]
"when D > 3 for an arbitrary metric tensor, although interesting Riemannian parameterization structures may be derived in Riemannian 4D geometry (Grant & Vickers, 2009).
",6. Related Works on FIM Diagonalization,[0],[0]
"For NEFs, the FIM can be made block-diagonal easily by using the mixed coordinate system (Amari, 2016) (Θ1:k,Hk+1:D), where H = Ep[t(x)] = ∇F (Θ) is the moment parameter, for any k ∈ {1, ..., D − 1}, where vb:e denotes the subvector (vb, ..., ve)ᵀ of v. The geometry of NEFs is a dually flat structure (Amari, 2016) induced by the convex mgf, the potential function.",6. Related Works on FIM Diagonalization,[0],[0]
"It defines a dual affine coordinate systems ei = ∂i = ∂∂Hi and ej = ∂
j = ∂∂Θj that are orthogonal: 〈ei, ej〉 = δij , where δij = 1 iff i = j and δij = 0 otherwise.",6. Related Works on FIM Diagonalization,[0],[0]
Hence the FIM has two diagonal blocks.,6. Related Works on FIM Diagonalization,[0],[0]
"Those dual affine coordinate systems are defined up to an affine invertible transformation: Θ̃ = AΘ + b, H̃ = A−1H + c.",6. Related Works on FIM Diagonalization,[0],[0]
"In particular, for any order-2 NEF (D = 2), we can always obtain two mixed parameterizations (Θ1, H2) or (H1,Θ2).
",6. Related Works on FIM Diagonalization,[0],[0]
The RFIM contributes another line of thought in parameter diagonalization.,6. Related Works on FIM Diagonalization,[0],[0]
"We investigate the Fisher information of hidden variables, or internal interfaces in the learning machine.",6. Related Works on FIM Diagonalization,[0],[0]
"This is novel since the majority of previous works concentrate on the FIM of the observables, or the external interface of the machine.",6. Related Works on FIM Diagonalization,[0],[0]
"From a causality perspective, we factor out the main cause (parameters within the subsystem) of the response variable with a direct action-reaction relationship, and regard the remaining parameters as a reference that can be easily estimated by the empirical distribution.",6. Related Works on FIM Diagonalization,[0],[0]
"This simplification may lead to broader applications of Fisher information in machine learning.
",6. Related Works on FIM Diagonalization,[0],[0]
"The particular case of a mixed coordinate system (that is not an affine coordinate system) induces in information geometry (Amari, 2016) a dual pair of orthogonal e- and morthogonal foliations.",6. Related Works on FIM Diagonalization,[0],[0]
"Our splits in RFIMs consider general non-orthogonal foliations that provide the factorization decompositions of the whole manifold into submanifolds, that are the leaves of the foliation (see section 3.7 of Amari & Nagaoka 2000).",6. Related Works on FIM Diagonalization,[0],[0]
We investigate local structures of large learning systems using the new concept of Relative Fisher Information Metric.,7. Conclusion and Discussions,[0],[0]
The key advantage of this approach is that the local learning dynamics can be analyzed in an accurate way without approximation.,7. Conclusion and Discussions,[0],[0]
"We present a core list of such local structures in neural networks, and give their corresponding RFIMs.",7. Conclusion and Discussions,[0],[0]
"This list of recipes can be used to provide guiding principles to design new optimizers for deep learning.
",7. Conclusion and Discussions,[0],[0]
"Our work applies to mirror descent as well since natural gradient is related to mirror descent (Raskutti & Mukherjee, 2015) as follows:",7. Conclusion and Discussions,[0],[0]
"In mirror descent to minimize a cost function E(Θ), given a strictly convex distance function D(·, ·) in the first argument (playing the role of the proximity function), we express the gradient descent step as:
Θt+1 = arg min Θ
{ Θ>∇E(Θt) + 1
γ D(Θ,Θt)
} .
",7. Conclusion and Discussions,[0],[0]
"When D(Θ,Θ′) is chosen as a Bregman divergence BF (Θ,Θ
′) = F (Θ)− F (Θ′)− (Θ−Θ′)>∇F (Θ′) wrt to a convex function F , it has been proved that the mirror descent on the Θ-parameterization is equivalent (Raskutti & Mukherjee, 2015) to the natural gradient optimization on the induced Riemannian manifold with metric tensor (∇2F (Θ)) parameterized by the dual coordinate system H = ∇F (Θ).
",7. Conclusion and Discussions,[0],[0]
"In general, to perform a Riemannian gradient descent for minimizing a real-valued function f(Θ) on the manifold, one needs to choose a proper metric tensor given in matrix form G(Θ).",7. Conclusion and Discussions,[0],[0]
Thomas (2014) constructed a toy example showing that the natural gradient may diverge while the ordinary gradient (for G = I) converges.,7. Conclusion and Discussions,[0],[0]
"Recently, Thomas et al. (2016) proposed a new kind of descent method based on what they called the Energetic Natural Gradient that generalizes the natural gradient.",7. Conclusion and Discussions,[0],[0]
"The energy distance DE(p(Θ1), p(Θ2))2 = E[2dp(Θ1)(X,Y )",7. Conclusion and Discussions,[0],[0]
"− dp(Θ1)(X,X
′)",7. Conclusion and Discussions,[0],[0]
"− dp(Θ1)(Y, Y ′)] where X,X ′ ∼ p(Θ1) and Y, Y ′ ∼ p(Θ2), where dp(Θ1)(·, ·) is a distance metric over the support.",7. Conclusion and Discussions,[0],[0]
"Using a Taylor’s expansion on their energy distance, they get the Energy Information Matrix (in a way similar to recovering the FIM from a Taylor’s expansion of any f -divergence like the Kullback-Leibler divergence).",7. Conclusion and Discussions,[0],[0]
Their idea is to incorporate prior knowledge on the structure of the support (observation space) to define energy distance.,7. Conclusion and Discussions,[0],[0]
"Twisting the geometry of the support (say, Wasserstein’s optimal transport) with the geometry of the parametric distributions (Fisher-Rao geodesic distances) is indeed important (Chizat et al., 2015).",7. Conclusion and Discussions,[0],[0]
"In information geometry, invariance on the support is provided by a Markov morphism that is a probabilistic mapping of the support to itself (Čencov, 1982).",7. Conclusion and Discussions,[0],[0]
There is no neighbourhood structure on the support in IG.,7. Conclusion and Discussions,[0],[0]
Markov morphism includes deterministic transformation of a random variable by a statistic.,7. Conclusion and Discussions,[0],[0]
It is well-known that IT (Θ) IX(Θ) with equality iff.,7. Conclusion and Discussions,[0],[0]
T = T (X) is a sufficient statistic of X .,7. Conclusion and Discussions,[0],[0]
"Thus to get the same invariance for the energy distance (Thomas et al., 2016), one shall further require dp(Θ)(T (X), T (Y ))",7. Conclusion and Discussions,[0],[0]
"= dp(Θ)(X,Y ).
",7. Conclusion and Discussions,[0],[0]
We believe that RFIMs will provide a sound methodology to build further efficient systems for deep learning.,7. Conclusion and Discussions,[0],[0]
The full source codes to reproduce the experimental results are available at https://www.lix.polytechnique.,7. Conclusion and Discussions,[0],[0]
fr/˜nielsen/RFIM.,7. Conclusion and Discussions,[0],[0]
The authors would like to thank the anonymous reviewers and Yann Ollivier for the helpful comments.,Acknowledgements,[0],[0]
This work was mainly conducted when the first author was a postdoctoral researcher at École Polytechnique.,Acknowledgements,[0],[0]
Fisher information and natural gradient provided deep insights and powerful tools to artificial neural networks.,abstractText,[0],[0]
However related analysis becomes more and more difficult as the learner’s structure turns large and complex.,abstractText,[0],[0]
This paper makes a preliminary step towards a new direction.,abstractText,[0],[0]
"We extract a local component from a large neural system, and define its relative Fisher information metric that describes accurately this small component, and is invariant to the other parts of the system.",abstractText,[0],[0]
This concept is important because the geometry structure is much simplified and it can be easily applied to guide the learning of neural networks.,abstractText,[0],[0]
"We provide an analysis on a list of commonly used components, and demonstrate how to use this concept to further improve optimization.",abstractText,[0],[0]
1.,abstractText,[0],[0]
Fisher Information Metric The Fisher Information Metric (FIM) I(Θ) =,abstractText,[0],[0]
"(Iij) of a statistical parametric model p(x |Θ) of order D is defined by a D×D positive semidefinite (psd) matrix (I(Θ) 0) with coefficients Iij = Ep [ ∂l ∂Θi ∂l ∂Θj ] , where l(Θ) denotes the log-density function log p(x |Θ).",abstractText,[0],[0]
"Under light regularity conditions, FIM can be rewritten equivalently as Iij = −Ep [ ∂l ∂Θi∂Θj ]",abstractText,[0],[0]
Relative Fisher Information and Natural Gradient for Learning Large Modular Models,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 44–54, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.
Our model is evaluated both for accuracy in predicting target order, and for its impact on translation quality. We report significant performance gains over phrase reordering, and over two known preordering baselines for English-Japanese.",text,[0],[0]
"Preordering (Collins et al., 2005) aims at permuting the words of a source sentence s into a new order ś, hopefully close to a plausible target word order.",1 Introduction,[0],[0]
"Preordering is often used to bridge long distance reorderings (e.g., in Japanese- or GermanEnglish), before applying phrase-based models (Koehn et al., 2007).",1 Introduction,[0],[0]
"Preordering is often broken down into two steps: finding a suitable tree structure, and then finding a transduction function over it.",1 Introduction,[0],[0]
"A common approach is to use monolingual syntactic trees and focus on finding a transduction function of the sibling subtrees under the nodes (Lerner and Petrov, 2013; Xia and Mccord, 2004).",1 Introduction,[0],[0]
"The (direct correspondence) assumption
underlying this approach is that permuting the siblings of nodes in a source syntactic tree can produce a plausible target order.",1 Introduction,[0],[0]
"An alternative approach creates reordering rules manually and then learns the right structure for applying these rules (Katz-Brown et al., 2011).",1 Introduction,[0],[0]
"Others attempt learning the transduction structure and the transduction function in two separate, consecutive steps (DeNero and Uszkoreit, 2011).",1 Introduction,[0],[0]
"Here we address the challenge of learning both the trees and the transduction functions jointly, in one fell swoop, from word-aligned parallel corpora.
",1 Introduction,[0],[0]
Learning both trees and transductions jointly raises two questions.,1 Introduction,[0],[0]
How to obtain suitable trees for the source sentence and how to learn a distribution over random variables specifically aimed at reordering in a hierarchical model?,1 Introduction,[0],[0]
"In this work we solve both challenges by using the factorizations of permutations into Permutation Trees (PETs) (Zhang and Gildea, 2007).",1 Introduction,[0],[0]
"As we explain next, PETs can be crucial for exposing the hierarchical reordering patterns found in wordalignments.
",1 Introduction,[0],[0]
We obtain permutations in the training data by segmenting every word-aligned source-target pair into minimal phrase pairs; the resulting alignment between minimal phrases is written as a permutation (1:1 and onto) on the source side.,1 Introduction,[0],[0]
"Every permutation can be factorized into a forest of PETs (over the source sentences) which we use as a latent treebank for training a Probabilistic ContextFree Grammar (PCFG) tailor made for preordering as we explain next.
",1 Introduction,[0],[0]
Figure 1 shows two alternative PETs for the same permutation over minimal phrases.,1 Introduction,[0],[0]
"The nodes have labels (like P3142) which stand for local permutations (called prime permutation) over the child nodes; for example, the root label P3142 stands for prime permutation 〈3, 1, 4, 2〉, which says that the first child of the root becomes 3rd on the target side, the second becomes 1st, the third
44
becomes 4th and the fourth becomes 2nd.",1 Introduction,[0],[0]
"The prime permutations are non-factorizable permutations like 〈1, 2〉, 〈2, 1〉 and 〈2, 4, 1, 3〉.
",1 Introduction,[0],[0]
We think PETs are suitable for learning preordering for two reasons.,1 Introduction,[0],[0]
"Firstly, PETs specify exactly the phrase pairs defined by the permutation.",1 Introduction,[0],[0]
"Secondly, every permutation is factorizable into prime permutations only (Albert and Atkinson, 2005).",1 Introduction,[0],[0]
"Therefore, PETs expose maximal sharing between different permutations in terms of both phrases and their reordering.",1 Introduction,[0],[0]
"We expect this to be advantageous for learning hierarchical reordering.
",1 Introduction,[0],[0]
"For learning preordering, we first extract an initial PCFG from the latent treebank of PETs over the source sentences only.",1 Introduction,[0],[0]
We initialize the nonterminal set of this PCFG to the prime permutations decorating the PET nodes.,1 Introduction,[0],[0]
"Subsequently we split these coarse labels in the same way as latent variable splitting is learned for treebank parsing (Matsuzaki et al., 2005; Prescher, 2005; Petrov et al., 2006; Saluja et al., 2014).",1 Introduction,[0.9579886579359024],"['Following (Lin et al., 2016; Liu et al., 2017), we also evaluate our method with different number of sentences.']"
"Unlike treebank parsing, however, our training treebank is latent because it consists of a whole forest of PETs per training instance (s).
",1 Introduction,[0],[0]
"Learning the splits on a latent treebank of PETs results in a Reordering PCFG which we use to parse input source sentences into split-decorated trees, i.e., the labels are the splits of prime permutations.",1 Introduction,[0],[0]
"After parsing s, we map the splits back on their initial prime permutations, and then retrieve a reordered version ś of s.",1 Introduction,[0],[0]
"In this sense, our latent splits are dedicated to reordering.
",1 Introduction,[0],[0]
We face two technical difficulties alien to work on latent PCFGs in treebank parsing.,1 Introduction,[0],[0]
"Firstly, as mentioned above, permutations may factorize into more than one PET (a forest) leading to a latent training treebank.1",1 Introduction,[0],[0]
"And secondly, after we parse a source string s, we are interested in ś, the permuted version of s, not in the best derivation/PET.",1 Introduction,[0],[0]
"Exact computation is a known NP-Complete problem (Sima’an, 2002).",1 Introduction,[0],[0]
"We solve this by a new Minimum-Bayes Risk decoding approach using Kendall reordering score as loss function, which is an efficient measure over permutations (Birch and Osborne, 2011; Isozaki et al., 2010a).
",1 Introduction,[0],[0]
"In summary, this paper contributes: • A novel latent hierarchical source reordering
model working over all derivations of PETs
1All PETs for the same permutation share the same set of prime permutations but differ only in bracketing structure (Zhang and Gildea, 2007).
•",1 Introduction,[0],[0]
"A label splitting approach based on PCFGs over minimal phrases as terminals, learned from an ambiguous treebank, where the label splits start out from prime permutations.",1 Introduction,[0],[0]
•,1 Introduction,[0],[0]
"A fast Minimum Bayes Risk decoding over
Kendall τ reordering score for selecting ś. We report results for extensive experiments on English-Japanese showing that our Reordering PCFG gives substantial improvements when used as preordering for phrase-based models, outperforming two existing baselines for this task.",1 Introduction,[0],[0]
"We aim at learning a PCFG which we will use for parsing source sentences s into synchronous trees, from which we can obtain a reordered source version ś. Since PCFGs are non-synchronous grammars, we will use the nonterminal labels to encode reordering transductions, i.e., this PCFG is implicitly an SCFG.",2 PETs and the Hidden Treebank,[0],[0]
"We can do this because s and ś are over the same alphabet.
",2 PETs and the Hidden Treebank,[0],[0]
"Here, we have access only to a word-aligned parallel corpus, not a treebank.",2 PETs and the Hidden Treebank,[0],[0]
"The following steps summarize our approach for acquiring a latent treebank and how it is used for learning a Reordering PCFG:
1.",2 PETs and the Hidden Treebank,[0],[0]
Obtain a permutation over minimal phrases from every word-alignment.,2 PETs and the Hidden Treebank,[0],[0]
2.,2 PETs and the Hidden Treebank,[0],[0]
Obtain a latent treebank of PETs by factorizing the permutations.,2 PETs and the Hidden Treebank,[0],[0]
3. Extract a PCFG from the PETs with initial nonterminals taken from the PETs.,2 PETs and the Hidden Treebank,[0],[0]
4.,2 PETs and the Hidden Treebank,[0],[0]
"Learn to split the initial nonterminals and estimate rule probabilities.
",2 PETs and the Hidden Treebank,[0],[0]
"These steps are detailed in the next section, but we will start out with an intuitive exposition of PETs, the latent treebank and the Reordering Grammar.
",2 PETs and the Hidden Treebank,[0],[0]
"Figure 1 shows examples of how PETs look like – see (Zhang and Gildea, 2007) for algorithmic details.",2 PETs and the Hidden Treebank,[0],[0]
Here we label the nodes with nonterminals which stand for prime permutations from the operators on the PETs.,2 PETs and the Hidden Treebank,[0],[0]
"For example, nonterminals P12, P21 and P3142 correspond respectively to reordering transducers 〈1, 2〉, 〈2, 1〉 and 〈3, 1, 4, 2〉.",2 PETs and the Hidden Treebank,[0],[0]
"A prime permutation on a source node µ is a transduction dictating how the children of µ are reordered at the target side, e.g., P21 inverts the child order.",2 PETs and the Hidden Treebank,[0],[0]
"We must stress that any similarity with ITG (Wu, 1997) is restricted to the fact that the straight and inverted operators of ITG are the binary case of prime permutations
in PETs (P12 and P21).",2 PETs and the Hidden Treebank,[0],[0]
"ITGs recognize only the binarizable permutations, which is a major restriction when used on the data: there are many nonbinarizable permutations in actual data (Wellington et al., 2006).",2 PETs and the Hidden Treebank,[0],[0]
"In contrast, our PETs are obtained by factorizing permutations obtained from the data, i.e., they exactly fit the range of prime permutations in the parallel corpus.",2 PETs and the Hidden Treebank,[0],[0]
"In practice we limit them to maximum arity 5.
",2 PETs and the Hidden Treebank,[0],[0]
"We can extract PCFG rules from the PETs, e.g., P21 → P12 P2413.",2 PETs and the Hidden Treebank,[0],[0]
"However, these rules are decorated with too coarse labels.",2 PETs and the Hidden Treebank,[0],[0]
"A similar problem was encountered in non-lexicalized monolingual parsing, and one solution was to lexicalize the productions (Collins, 2003) using head words.",2 PETs and the Hidden Treebank,[0],[0]
"But linguistic heads do not make sense for PETs, so we opt for the alternative approach (Matsuzaki et al., 2005), which splits the nonterminals and softly percolates the splits through the trees gradually fitting them to the training data.",2 PETs and the Hidden Treebank,[0],[0]
"Splitting has a shadow side, however, because it leads to combinatorial explosion in grammar size.
",2 PETs and the Hidden Treebank,[0],[0]
Suppose for example node P21 could split into P211 and P212 and similarly P2413 splits into P24131 and 24132.,2 PETs and the Hidden Treebank,[0],[0]
"This means that rule P21 → P12 P2413 will form eight new rules:
P211 → P121 P24131",2 PETs and the Hidden Treebank,[0],[0]
P211 → P121 P24132 P211,2 PETs and the Hidden Treebank,[0],[0]
→ P122 P24131,2 PETs and the Hidden Treebank,[0],[0]
P211 → P122 P24132 P212 → P121 P24131,2 PETs and the Hidden Treebank,[0],[0]
P212 → P121 P24132 P212 → P122 P24131,2 PETs and the Hidden Treebank,[0],[0]
"P212 → P122 P24132
Should we want to split each nonterminal into 30 subcategories, then an n-ary rule will split into 30n+1 new rules, which is prohibitively large.",2 PETs and the Hidden Treebank,[0],[0]
Here we use the “unary trick” as in Figure 2.,2 PETs and the Hidden Treebank,[0],[0]
The superscript on the nonterminals denotes the child position from left to right.,2 PETs and the Hidden Treebank,[0],[0]
"For example P2121 means that this node is a second child, and the
mother nonterminal label is P211.",2 PETs and the Hidden Treebank,[0],[0]
"For the running example rule, this gives the following rules:
P211 → P2111 P2121 P212",2 PETs and the Hidden Treebank,[0],[0]
→ P2112 P2122,2 PETs and the Hidden Treebank,[0],[0]
P2111,2 PETs and the Hidden Treebank,[0],[0]
→ P121 P2121 → P24131,2 PETs and the Hidden Treebank,[0],[0]
P2111,2 PETs and the Hidden Treebank,[0],[0]
→ P122 P2121 → P24132 P2112 → P121 P2122 → P24131,2 PETs and the Hidden Treebank,[0],[0]
P2112 → P122 P2122,2 PETs and the Hidden Treebank,[0],[0]
"→ P24132
",2 PETs and the Hidden Treebank,[0],[0]
"The unary trick leads to substantial reduction in grammar size, e.g., for arity 5 rules and 30 splits we could have had 306 = 729000000 split-rules, but with the unary trick we only have 30+302∗5 = 4530 split rules.",2 PETs and the Hidden Treebank,[0],[0]
"The unary trick was used in early lexicalized parsing work (Carroll and Rooth, 1998).2 This split PCFG constitutes a latent PCFG because the splits cannot be read of a treebank.",2 PETs and the Hidden Treebank,[0],[0]
"It must be learned from the latent treebank of PETs, as described next.",2 PETs and the Hidden Treebank,[0],[0]
"Obtaining permutations Given a source sentence s and its alignment a to a target sentence
2After applying the unary trick, we add a constraint on splitting: all nonterminals on an n-ary branching rule must be split simultaneously.
",3 Details of Latent Reordering PCFG,[0],[0]
"t in the training corpus, we segment 〈s,a, t〉 into a sequence of minimal phrases sm (maximal sequence) such that the reordering between these minimal phrases constitutes a permutation πm.",3 Details of Latent Reordering PCFG,[0],[0]
"We do not extract non-contiguous or non-minimal phrases because reordering them often involves complicated transductions which could hamper the performance of our learning algorithm.3
Unaligned words Next we describe the use of the factorization of permutations into PET forests for training a PCFG model.",3 Details of Latent Reordering PCFG,[0],[0]
But first we need to extend the PETs to allow for unaligned words.,3 Details of Latent Reordering PCFG,[0],[0]
"An unaligned word is joined with a neighboring phrase to the left or the right, depending on the source language properties (e.g., whether the language is head-initial or -final (Chomsky, 1970)).",3 Details of Latent Reordering PCFG,[0],[0]
"Our experiments use English as source language (head-initial), so the unaligned words are joined to phrases to their right.",3 Details of Latent Reordering PCFG,[0],[0]
This modifies a PET by adding a new binary branching node µ (dominating the unaligned word and the phrase it is joined to) which is labeled with a dedicated nonterminal: P01 if the unaligned word joins to the right and P10 if it joins to the left.,3 Details of Latent Reordering PCFG,[0],[0]
"We decompose the permutation πm into a forest of permutation trees PEF (πm) in O(n3), following algorithms in (Zhang et al., 2008; Zhang and Gildea, 2007) with trivial modifications.",3.1 Probability model,[0],[0]
Each PET ∆ ∈ PEF (πm) is a different bracketing (differing in binary branching structure only).,3.1 Probability model,[0],[0]
"We consider the bracketing hidden in the latent treebank, and apply unsupervised learning to induce a distribution over possible bracketings.",3.1 Probability model,[0],[0]
Our probability model starts from the joint probability of a sequence of minimal phrases sm and a permutation πm over it.,3.1 Probability model,[0],[0]
"This demands summing over all PETs ∆ in the forest PEF (πm), and for every PET also over all its label splits, which are given by the grammar derivations",3.1 Probability model,[0],[0]
"d:
P (sm, πm) = ∑
∆∈PEF (πm) ∑ d∈∆ P (d, sm) (1)
",3.1 Probability model,[0],[0]
"The probability of a derivation d is a product of probabilities of all the rules r that build it:
P (sm, πm) = ∑
∆∈PEF (πm) ∑ d∈∆ ∏ r∈d P (r) (2)
3Which differs from (Quirk and Menezes, 2006).
",3.1 Probability model,[0],[0]
"As usual, the parameters of this model are the PCFG rule probabilities which are estimated from the latent treebank using EM as explained next.",3.1 Probability model,[0],[0]
"For training the latent PCFG over the latent treebank, we resort to EM (Dempster et al., 1977) which estimates PCFG rule probabilities to maximize the likelihood of the parallel corpus instances.",3.2 Learning Splits on Latent Treebank,[0],[0]
"Computing expectations for EM is done efficiently using Inside-Outside (Lari and Young, 1990).",3.2 Learning Splits on Latent Treebank,[0],[0]
"As in other state splitting models (Matsuzaki et al., 2005), after splitting the nonterminals, we distribute the probability uniformly over the new rules, and we add to each new rule some random noise to break the symmetry.",3.2 Learning Splits on Latent Treebank,[0],[0]
"We split the non-terminals only once as in (Matsuzaki et al., 2005) (unlike (Petrov et al., 2006)).",3.2 Learning Splits on Latent Treebank,[0],[0]
For estimating the distribution for unknown words we replace all words that appear ≤ 3 times with the “UNKNOWN” token.,3.2 Learning Splits on Latent Treebank,[0],[0]
"We use CKY+ (Chappelier and Rajman, 1998) to parse a source sentence s into a forest using the learned split PCFG.",3.3 Inference,[0],[0]
"Unfortunately, computing the most-likely permutation (or alternatively ś) as in
argmax π∈Π ∑ ∆∈PEF (π) ∑ d∈∆ P (d, πm)
from a lattice of permutations Π using a PCFG is NP-complete (Sima’an, 2002).",3.3 Inference,[0],[0]
"Existing techniques, like variational decoding or MinimumBayes Risk (MBR), used for minimizing loss over trees as in (Petrov and Klein, 2007), are not directly applicable here.",3.3 Inference,[0],[0]
"Hence, we opt for minimizing the risk of making an error under a loss function over permutations using the MBR decision rule (Kumar and Byrne, 2004):
π̂ = argmin π ∑ πr Loss(π, πr)P (πr) (3)
",3.3 Inference,[0],[0]
The loss function we minimize is Kendall τ,3.3 Inference,[0],[0]
"(Birch and Osborne, 2011; Isozaki et al., 2010a) which is a ratio of wrongly ordered pairs of words (including gapped pairs) to the total number of pairs.",3.3 Inference,[0],[0]
We do Monte Carlo sampling of 10000 derivations from the chart of the s and then find the least risky permutation in terms of this loss.,3.3 Inference,[0],[0]
"We sample from the true distribution by sampling edges recursively
using their inside probabilities.",3.3 Inference,[0],[0]
"An empirical distribution over permutations P (π) is given by the relative frequency of π in the sample.
",3.3 Inference,[0],[0]
With large samples it is hard to efficiently compute expected Kendall τ loss for each sampled hypothesis.,3.3 Inference,[0],[0]
For sentence of length k and sample of size n the complexity of a naive algorithm is O(n2k2).,3.3 Inference,[0],[0]
Computing Kendall τ alone takes O(k2).,3.3 Inference,[0],[0]
"We use the fact that Kendall τ decomposes as a linear function over all skip-bigrams b that could be built for any permutation of length k:
Kendall(π, πr) = ∑ b 1− δ(π, b) k(k−1) 2 δ(πr, b) (4)
",3.3 Inference,[0],[0]
"Here δ returns 1 if permutation π contains the skip bigram b, otherwise it returns 0.",3.3 Inference,[0],[0]
"With this decomposition we can use the method from (DeNero et al., 2009) to efficiently compute the MBR hypothesis.",3.3 Inference,[0],[0]
"Combining Equations 3 and 4 we get:
π̂ = argmin π ∑ πr ∑ b 1− δ(π, b) k(k−1) 2 δ(πr, b)P (πr) (5)
",3.3 Inference,[0],[0]
"We can move the summation inside and reformulate the expected Kendall τ loss as expectation over the skip-bigrams of the permutation.
",3.3 Inference,[0],[0]
= argmin π ∑ b,3.3 Inference,[0],[0]
"(1− δ(π, b))",3.3 Inference,[0],[0]
"[∑ πr δ(πr, b)P (πr) ] (6)
= argmin π ∑ b (1− δ(π, b))EP (πr)δ(πr, b) (7)
= argmax π ∑ b δ(π, b)EP (πr)δ(πr, b) (8)
This means we need to pass through the sampled list only twice: (1) to compute expectations over skip bigrams and (2) to compute expected loss of each sampled permutation.",3.3 Inference,[0],[0]
The time complexity is O(nk2) which is quite fast in practice.,3.3 Inference,[0],[0]
We conduct experiments with three baselines:,4 Experiments,[0],[0]
• Baseline A: No preordering.,4 Experiments,[0],[0]
"• Baseline B: Rule based preordering (Isozaki
et al., 2010b), which first obtains an HPSG parse tree using Enju parser 4 and after that swaps the children by moving the syntactic head to the final position to account for different head orientation in English and Japanese.
",4 Experiments,[0],[0]
"4http://www.nactem.ac.uk/enju/
• Baseline C: LADER (Neubig et al., 2012): latent variable preordering that is based on ITG and large-margin training with latent variables.",4 Experiments,[0],[0]
"We used LADER in standard settings without any linguistic features (POS tags or syntactic trees).
",4 Experiments,[0],[0]
And we test four variants of our model:,4 Experiments,[0],[0]
• RGleft - only canonical left branching PET •,4 Experiments,[0],[0]
"RGright - only canonical right branching PET • RGITG-forest - all PETs that are binary (ITG) • RGPET-forest - all PETs.
",4 Experiments,[0],[0]
We test these models on English-Japanese NTCIR-8 Patent Translation (PATMT) Task.,4 Experiments,[0],[0]
For tuning we use all NTCIR-7 dev sets and for testing the test set from NTCIR-9 from both directions.,4 Experiments,[0],[0]
All used data was tokenized (English with Moses tokenizer and Japanese with KyTea 5) and filtered for sentences between 4 and 50 words.,4 Experiments,[0],[0]
"A subset of this data is used for training the Reordering Grammar, obtained by filtering out sentences that have prime permutations of arity > 5, and for the ITG version arity > 2.",4 Experiments,[0],[0]
Baseline C was trained on 600 sentences because training is prohibitively slow.,4 Experiments,[0],[0]
"Table 1 shows the sizes of data used.
",4 Experiments,[0],[0]
The Reordering Grammar was trained for 10 iterations of EM on train RG data.,4 Experiments,[0],[0]
We use 30 splits for binary non-terminals and 3 for non-binary.,4 Experiments,[0],[0]
Training on this dataset takes 2 days and parsing tuning and testing set without any pruning takes 11 and 18 hours respectively.,4 Experiments,[0],[0]
We test how well our model predicts gold reorderings before translation by training the alignment model using MGIZA++ 6 on the training corpus and using it to align the test corpus.,4.1 Intrinsic evaluation,[0],[0]
"Gold reorderings for the test corpus are obtained by sorting words by their average target position and (unaligned words follow their right neighboring
5http://www.phontron.com/kytea/ 6http://www.kyloo.net/software/doku.php/mgiza:overview
word).",4.1 Intrinsic evaluation,[0],[0]
"We use Kendall τ score for evaluation (note the difference with Section 3.3 where we defined it as a loss function).
",4.1 Intrinsic evaluation,[0],[0]
Table 2 shows that our models outperform all baselines on this task.,4.1 Intrinsic evaluation,[0],[0]
"The only strange result here is that rule-based preordering obtains a lower score than no preordering, which might be an artifact of the Enju parser changing the tokenization of its input, so the Kendall τ of this system might not really reflect the real quality of the preordering.",4.1 Intrinsic evaluation,[0],[0]
All other systems use the same tokenization.,4.1 Intrinsic evaluation,[0],[0]
"The reordered output of all the mentioned baselines and versions of our model are translated with phrase-based MT system (Koehn et al., 2007) (distortion limit set to 6 with distance based reordering model) that is trained on gold preordering of the training data 7 ś − t.",4.2 Extrinsic evaluation in MT,[0],[0]
"The only exception is Baseline A which is trained on original s− t.
We use a 5-gram language model trained with KenLM 8, tune 3 times with kb-mira (Cherry and Foster, 2012) to account for tuner instability and evaluated using Multeval 9 for statistical significance on 3 metrics: BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and TER (Snover et al., 2006).",4.2 Extrinsic evaluation in MT,[0],[0]
"We additionally report RIBES score (Isozaki et al., 2010a) that concentrates on word order more than other metrics.
",4.2 Extrinsic evaluation in MT,[0],[0]
Single or all PETs?,4.2 Extrinsic evaluation in MT,[0],[0]
In Table 3 we see that using all PETs during training makes a big impact on performance.,4.2 Extrinsic evaluation in MT,[0],[0]
"Only the all PETs variants
7Earlier work on preordering applies the preordering model to the training data to obtain a parallel corpus of guessed ś − t pairs, which are the word re-aligned and then used for training the back-end MT system (Khalilov and Sima’an, 2011).",4.2 Extrinsic evaluation in MT,[0],[0]
"We skip this, we take the risk of mismatch between the preordering and the back-end system, but this simplifies training and saves a good amount of training time.
8http://kheafield.com/code/kenlm/ 9https://github.com/jhclark/multeval
(RGITG-forest and RGPET-forest) significantly outperform all baselines.",4.2 Extrinsic evaluation in MT,[0],[0]
"If we are to choose a single PET per training instance, then learning RG from only left-branching PETs (the one usually chosen in other work, e.g. (Saluja et al., 2014)) performs slightly worse than the right-branching PET.",4.2 Extrinsic evaluation in MT,[0],[0]
This is possibly because English is mostly rightbranching.,4.2 Extrinsic evaluation in MT,[0],[0]
"So even though both PETs describe the same reordering, RGright captures reordering over English input better than RGleft.
",4.2 Extrinsic evaluation in MT,[0],[0]
All PETs or binary only?,4.2 Extrinsic evaluation in MT,[0],[0]
RGPET-forest performs significantly better than RGITG-forest (p < 0.05).,4.2 Extrinsic evaluation in MT,[0],[0]
"Non-ITG reordering operators are predicted rarely (in only 99 sentences of the test set), but they make a difference, because these operators often appear high in the predicted PET.",4.2 Extrinsic evaluation in MT,[0],[0]
"Furthermore, having these operators during training might allow for better fit to the data.
",4.2 Extrinsic evaluation in MT,[0],[0]
How much reordering is resolved by the Reordering Grammar?,4.2 Extrinsic evaluation in MT,[0],[0]
"Obviously, completely factorizing out the reordering from the translation process is impossible because reordering depends to a certain degree on target lexical choice.",4.2 Extrinsic evaluation in MT,[0],[0]
"To quantify the contribution of Reordering Grammar, we tested decoding with different distortion limit values in the SMT system.",4.2 Extrinsic evaluation in MT,[0],[0]
"We compare the phrase-based (PB) system with distance based cost function for reordering (Koehn et al., 2007) with and without preordering.
",4.2 Extrinsic evaluation in MT,[0],[0]
Figure 3 shows that Reordering Grammar gives substantial performance improvements at all distortion limits (both BLEU and RIBES).,4.2 Extrinsic evaluation in MT,[0],[0]
RGPET-forest is less sensitive to changes in decoder distortion limit than standard PBSMT.,4.2 Extrinsic evaluation in MT,[0],[0]
"The perfor-
mance of RGPET-forest varies only by 1.1 BLEU points while standard PBSMT by 4.3 BLEU points.",4.2 Extrinsic evaluation in MT,[0],[0]
Some local reordering in the decoder seems to help RGPET-forest but large distortion limits seem to degrade the preordering choice.,4.2 Extrinsic evaluation in MT,[0],[0]
"This shows also that the improved performance of RGPET-forest is not only a result of efficiently exploring the full space of permutations, but also a result of improved scoring of permutations.
",4.2 Extrinsic evaluation in MT,[0],[0]
Does the improvement remain for a decoder with MSD reordering model?,4.2 Extrinsic evaluation in MT,[0],[0]
"We compare the RGPET-forest preordered model against a decoder that uses the strong MSD model (Tillmann, 2004; Koehn et al., 2007).",4.2 Extrinsic evaluation in MT,[0],[0]
Table 4 shows that using Reordering Grammar as front-end to MSD reordering (full Moses) improves performance by 2.8 BLEU points.,4.2 Extrinsic evaluation in MT,[0],[0]
"The improvement is confirmed by METEOR, TER and RIBES.",4.2 Extrinsic evaluation in MT,[0],[0]
"Our preordering model and MSD are complementary – the Reordering Grammar captures long distance reordering, while MSD possibly does better local reorderings, especially reorderings conditioned on the lexical part of translation units.
",4.2 Extrinsic evaluation in MT,[0],[0]
"Interestingly, the MSD model (BLEU 29.6) improves over distance-based reordering (BLEU 27.8) by (BLEU 1.8), whereas the difference between these systems as back-ends to Reordering Grammar (respectively BLEU 32.4 and 32.0) is
far smaller (0.4 BLEU).",4.2 Extrinsic evaluation in MT,[0],[0]
This suggests that a major share of reorderings can be handled well by preordering without conditioning on target lexical choice.,4.2 Extrinsic evaluation in MT,[0],[0]
"Furthermore, this shows that RGPET-forest preordering is not very sensitive to the decoder’s reordering model.
",4.2 Extrinsic evaluation in MT,[0],[0]
Comparison to a Hierarchical model (Hiero).,4.2 Extrinsic evaluation in MT,[0],[0]
"Hierarchical preordering is not intended for a hierarchical model as Hiero (Chiang, 2005).",4.2 Extrinsic evaluation in MT,[0],[0]
"Yet, here we compare our preordering system (PB MSD+RG) to Hiero for completeness, while we should keep in mind that Hiero’s reordering model has access to much richer training data.",4.2 Extrinsic evaluation in MT,[0],[0]
"We will discuss these differences shortly.
",4.2 Extrinsic evaluation in MT,[0],[0]
"Table 4 shows that the difference in BLEU is not statistically significant, but there is more difference in METEOR and TER. RIBES, which concentrates more on reordering, prefers Reordering Grammar over Hiero.",4.2 Extrinsic evaluation in MT,[0],[0]
It is somewhat surprising that a preordering model combined with a phrase-based model succeeds to rival Hiero’s performance on English-Japanese.,4.2 Extrinsic evaluation in MT,[0],[0]
"Especially when looking at the differences between the two:
1.",4.2 Extrinsic evaluation in MT,[0],[0]
"Reordering Grammar uses only minimal phrases, while Hiero uses composite (longer) phrases which encapsulate internal reorderings, but also non-contiguous phrases.",4.2 Extrinsic evaluation in MT,[0],[0]
2.,4.2 Extrinsic evaluation in MT,[0],[0]
"Hiero conditions its reordering on the lexical target side, whereas the Reordering Grammar does not (by definition).",4.2 Extrinsic evaluation in MT,[0],[0]
3.,4.2 Extrinsic evaluation in MT,[0],[0]
"Hiero uses a range of features, e.g., a language model, while Reordering Grammar is a mere generative PCFG.",4.2 Extrinsic evaluation in MT,[0],[0]
"The advantages of Hiero can be brought to bear upon Reordering Grammar by reformulating it as a discriminative model.
",4.2 Extrinsic evaluation in MT,[0],[0]
Which structure is learned?,4.2 Extrinsic evaluation in MT,[0],[0]
"Figure 4 shows an example PET output showing how our model learns: (1) that the article “the” has no equivalent in Japanese, (2) that verbs go after their object, (3) to use postpositions instead of prepositions, and (4) to correctly group certain syntactic units, e.g. NPs and VPs.",4.2 Extrinsic evaluation in MT,[0],[0]
"The majority of work on preordering is based on syntactic parse trees, e.g., (Lerner and Petrov, 2013; Khalilov and Sima’an, 2011; Xia and Mccord, 2004).",5 Related work,[0],[0]
Here we concentrate on work that has common aspects with this work.,5 Related work,[0],[0]
"Neubig et
al (2012) trains a latent non-probabilistic discriminative model for preordering as an ITG-like grammar limited to binarizable permutations.",5 Related work,[0],[0]
Tromble and Eisner (2009) use ITG but do not train the grammar.,5 Related work,[0],[0]
They only use it to constrain the local search.,5 Related work,[0],[0]
DeNero and Uszkoreit (2011) present two separate consecutive steps for unsupervised induction of hierarchical structure (ITG) and the induction of a reordering function over it.,5 Related work,[0],[0]
"In contrast, here we learn both the structure and the reordering function simultaneously.",5 Related work,[0],[0]
"Furthermore, at test time, our inference with MBR over a measure of permutation (Kendall) allows exploiting both structure and reordering weights for inference, whereas test-time inference in (DeNero and Uszkoreit, 2011) is also a two step process – the parser forwards to the next stage the best parse.
",5 Related work,[0],[0]
Dyer and Resnik (2010) treat reordering as a latent variable and try to sum over all derivations that lead not only to the same reordering but also to the same translation.,5 Related work,[0],[0]
"In their work they consider all permutations allowed by a given syntactic tree.
",5 Related work,[0],[0]
"Saers et al (2012) induce synchronous grammar for translation by splitting the non-terminals, but unlike our approach they split generic nonterminals and not operators.",5 Related work,[0],[0]
Their most expressive grammar covers only binarizable permutations.,5 Related work,[0],[0]
The decoder that uses this model does not try to sum over many derivations that have the same yield.,5 Related work,[0],[0]
They do not make independence assumption like our “unary trick” which is probably the reason they do not split more than 8 times.,5 Related work,[0],[0]
"They do not compare their results to any other SMT system and test on a very small dataset.
",5 Related work,[0],[0]
"Saluja et al (2014) attempts inducing a refined Hiero grammar (latent synchronous CFG) from Normalized Decomposition Trees (NDT) (Zhang et al., 2008).",5 Related work,[0],[0]
"While there are similarities with
the present work, there are major differences.",5 Related work,[0],[0]
"On the similarity side, NDTs are decomposing alignments in ways similar to PETs, and both Saluja’s and our models refine the labels on the nodes of these decompositions.",5 Related work,[0],[0]
"However, there are major differences between the two:
• Our model is completely monolingual and unlexicalized (does not condition its reordering on the translation) in contrast with the Latent SCFG used in (Saluja et al., 2014), • Our Latent PCFG label splits are defined
as refinements of prime permutations, i.e., specifically designed for learning reordering, whereas (Saluja et al., 2014) aims at learning label splitting that helps predicting NDTs from source sentences, • Our model exploits all PETs and all deriva-
tions, both during training (latent treebank) and during inferences.",5 Related work,[0],[0]
"In (Saluja et al., 2014) only left branching NDT derivations are used for learning the model.",5 Related work,[0],[0]
•,5 Related work,[0],[0]
"The training data used by (Saluja et al., 2014)
is about 60 times smaller in number of words than the data used here; the test set of (Saluja et al., 2014) also consists of far shorter sentences where reordering could be less crucial.
",5 Related work,[0],[0]
"A related work with a similar intuition is presented in (Maillette de Buy Wenniger and Sima’an, 2014), where nodes of a tree structure similar to PETs are labeled with reordering patterns obtained by factorizing word alignments into Hierarchical Alignment Trees.",5 Related work,[0],[0]
These patterns are used for labeling the standard Hiero grammar.,5 Related work,[0],[0]
"Unlike this work, the labels extracted by (Maillette de Buy Wenniger and Sima’an, 2014) are clustered manually into less than a dozen labels without the possibility of fitting the labels to the training data.",5 Related work,[0],[0]
We present a generative Reordering PCFG model learned from latent treebanks over PETs obtained by factorizing permutations over minimal phrase pairs.,6 Conclusion,[0],[0]
Our Reordering PCFG handles non-ITG reordering patterns (up to 5-ary branching) and it works with all PETs that factorize a permutation (rather than a single PET).,6 Conclusion,[0],[0]
To the best of our knowledge this is the first time both extensions are shown to improve performance.,6 Conclusion,[0],[0]
"The empirical results on English-Japanese show that (1) when used for preordering, the Reordering PCFG helps particularly with relieving the phrase-based model from long range reorderings, (2) combined with a state-of-the-art phrase model, Reordering PCFG shows performance not too different from Hiero, supporting the common wisdom of factorizing long range reordering outside the decoder, (3) Reordering PCFG generates derivations that seem to coincide well with linguistically-motivated reordering patterns for English-Japanese.",6 Conclusion,[0],[0]
"There are various direction we would like to explore, the most obvious of which are integrating the learned reordering with other feature functions in a discriminative setting, and extending the model to deal with non-contiguous minimal phrases.",6 Conclusion,[0],[0]
This work is supported by STW grant nr. 12271 and NWO VICI grant nr. 277-89-002.,Acknowledgments,[0],[0]
We thank Wilker Aziz for comments on earlier version of the paper and discussions about MBR and sampling.,Acknowledgments,[0],[0]
"We present a novel approach for unsupervised induction of a Reordering Grammar using a modified form of permutation trees (Zhang and Gildea, 2007), which we apply to preordering in phrase-based machine translation.",abstractText,[0],[0]
"Unlike previous approaches, we induce in one step both the hierarchical structure and the transduction function over it from word-aligned parallel corpora.",abstractText,[0],[0]
"Furthermore, our model (1) handles non-ITG reordering patterns (up to 5-ary branching), (2) is learned from all derivations by treating not only labeling but also bracketing as latent variable, (3) is entirely unlexicalized at the level of reordering rules, and (4) requires no linguistic annotation.",abstractText,[0],[0]
"Our model is evaluated both for accuracy in predicting target order, and for its impact on translation quality.",abstractText,[0],[0]
"We report significant performance gains over phrase reordering, and over two known preordering baselines for English-Japanese.",abstractText,[0],[0]
Reordering Grammar Induction,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2401–2410 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"Deep neural models are known to be computationally expensive to train even with fast hardware (Sutskever et al., 2014; Wu et al., 2016).",1 Introduction,[0],[0]
"For example, it takes three weeks to train a deep neural machine translation system on 100 Graphics Processing Units (GPUs) (Wu et al., 2016).",1 Introduction,[0],[0]
"Furthermore, a large amount of data is usually required to train effective neural models (Goodfellow et al., 2016; Hirschberg and Manning, 2015).
",1 Introduction,[0],[0]
Bengio et al. (2009) and Kumar et al. (2010) developed training paradigms which are inspired by the learning principle that humans can learn more effectively when training starts with easier concepts and gradually proceeds with more difficult concepts.,1 Introduction,[0],[0]
"Since these approaches are motivated by
1Our code is available at scholar.harvard.edu/ hadi/RbF/
a “starting small” strategy they are called curriculum or self-paced learning.
",1 Introduction,[0],[0]
"In this paper, we present a novel training paradigm which is inspired by the broad evidence in psychology that shows human ability to retain information improves with repeated exposure and exponentially decays with delay since last exposure (Cepeda et al., 2006; Averell and Heathcote, 2011).",1 Introduction,[0],[0]
"Spaced repetition was presented in psychology (Dempster, 1989) and forms the building block of many educational devices, including flashcards, in which small pieces of information are repeatedly presented to a learner on a schedule determined by a spaced repetition algorithm.",1 Introduction,[0],[0]
"Such algorithms show that human learners can learn efficiently and effectively by increasing intervals of time between subsequent reviews of previously learned materials (Dempster, 1989; Novikoff et al., 2012).
",1 Introduction,[0],[0]
"We investigate the analogy between training neural models and findings in psychology about human memory model and develop a spaced repetition algorithm (named Repeat before Forgetting, RbF) to efficiently and effectively train neural models.",1 Introduction,[0],[0]
The core part of our algorithm is a scheduler that ensures a given neural network spends more time working on difficult training instances and less time on easier ones.,1 Introduction,[0],[0]
"Our scheduler is inspired by factors that affect human memory retention, namely, difficulty of learning materials, delay since their last review, and strength of memory.",1 Introduction,[0],[0]
The scheduler uses these factors to lengthen or shorten review intervals with respect to individual learners and training instances.,1 Introduction,[0],[0]
"We evaluate schedulers based on their scheduling accuracy, i.e., accuracy in estimating network memory retention with respect to previously-seen instances, as well as their effect on the efficiency and effectiveness of downstream neural networks.2
2 In this paper, we use the terms memory retention, recall, and learning interchangeably.
2401
The contributions of this paper are: (1) we show that memory retention in neural networks is affected by the same (known) factors that affect memory retention in humans, (2) we present a novel training paradigm for neural networks based on spaced repetition, and (3) our approach can be applied without modification to any neural network.
",1 Introduction,[0],[0]
"Our best RbF algorithm uses 34-50% of training data per epoch while producing similar results to state-of-the-art systems on three tasks, namely sentiment classification, image categorization, and arithmetic addition.3",1 Introduction,[0],[0]
"It also runs 2.9-4.8 times faster than standard training, and outperforms competing state-of-the-art baselines.",1 Introduction,[0],[0]
"Research in psychology describes the following memory model for human learning: the probability that a human recalls a previously-seen item (e.g., the Korean translation of a given English word) depends on the difficulty of the item, delay since last review of the item, and the strength of the human memory.",2 Neural and Brain Memory Models,[0],[0]
"The relation between these indicators and memory retention has the following functional form (Reddy et al., 2016; Ebbinghaus, 1913):
Pr(recall) = exp(−difficulty × delay strength ).",2 Neural and Brain Memory Models,[0],[0]
"(1)
An accurate memory model enables estimating the time by which an item might be forgotten by a learner so that a review can be scheduled for the learner before that time.
",2 Neural and Brain Memory Models,[0],[0]
We investigate the analogy between the above memory model and memory model of artificial neural networks.,2 Neural and Brain Memory Models,[0],[0]
"Our intuition is that if the probability that a network recalls an item (e.g., correctly predicts its category) depends on the same factors (difficulty of the item, delay since last review of the item, or strength of the network), then we can develop spaced repetition algorithms to efficiently and effectively train neural networks.",2 Neural and Brain Memory Models,[0],[0]
We design a set of preliminarily experiments to directly evaluate the effect of the aforementioned factors (recall indicators) on memory retention in neural networks.,2.1 Recall Indicators,[0],[0]
"For this purpose, we use a set of training instances that are partially made available to the network during training.",2.1 Recall Indicators,[0],[0]
"This scheme
3We obtained similar results on QA tasks (Weston et al., 2016) but they are excluded due to space limit.
will allow us to intrinsically examine the effect of recall indicators on memory retention in isolation from external effects such as size of training data, number of training epochs, etc.
",2.1 Recall Indicators,[0],[0]
"We first define the following concepts to ease understanding the experiments (see Figure 1):
• First and Last review points (fRev and lRev) of a training instance are the first and last epochs in which the instance is used to train the network respectively,
• Recall point (Rec) is the epoch in which network retention is computed against some training instances; network retention is the probability that a neural network recalls (i.e. correctly classifies) a previously-seen training instance, and
• Delay since last review of a training instance is the difference between the recall point and the last review point of the training instance.
",2.1 Recall Indicators,[0],[0]
"Given training data and a neural network, we uniformly at random divide the data into three disjoint sets: a base set A, a review set B, and a replacement set C that respectively contain 80%, 10%, and 10% of the data.",2.1 Recall Indicators,[0],[0]
"As depicted in Figure 1, instances of A are used for training at every epoch, while those in B and C are partially used for training.",2.1 Recall Indicators,[0],[0]
The network initially starts to train with {A ∪ C} instances.,2.1 Recall Indicators,[0],[0]
"Then, starting from the first review point, we inject the review set B and remove C, training with {A ∪ B} instances at every epoch until the last review point.",2.1 Recall Indicators,[0],[0]
The network will then continue training with {A ∪ C} instances until the recall point.,2.1 Recall Indicators,[0],[0]
"At this point, network retention is computed against set B instances, with delay defined as the number of epochs since last review point.",2.1 Recall Indicators,[0],[0]
"The intuition behind using review and replacement sets, B and C respectively, is to avoid external effects (e.g.
size of data or network generalization and learning capability) for our intrinsic evaluation purpose.
",2.1 Recall Indicators,[0],[0]
"To conduct these experiments, we identify different neural models designed for different tasks.4 For each network, we fix the recall point to either the epoch in which the network is fully trained (i.e., obtains its best performance based on standard or “rote” training in which all instances are used for training at every iteration), or partially trained (i.e., obtains half of its best performance based on rote training).",2.1 Recall Indicators,[0],[0]
We report average results across these networks for each experiment.,2.1 Recall Indicators,[0],[0]
"As aforementioned, delay since last review of a training instance is the difference between the recall point (Rec) and the last review point (lRev) of the training instance.",2.1.1 Delay since Last Review,[0],[0]
We evaluate the effect of delay on network retention (against set B instances) by keeping the recall point fixed while moving the sliding window in Figure 1.,2.1.1 Delay since Last Review,[0],[0]
Figures 2(a) and 2(b) show average network retention across networks for the fully and partially trained recall points respectively.,2.1.1 Delay since Last Review,[0],[0]
The results show an inverse relationship between network retention and delay since last review in neural networks.,2.1.1 Delay since Last Review,[0],[0]
We define difficulty of training instances by the loss values generated by a network for the instances.,2.1.2 Item Difficulty,[0],[0]
Figure 2(c) shows the difficulty of set B instances at the last review point against average network retention on these instances at recall point.,2.1.2 Item Difficulty,[0],[0]
"We normalize loss values to unit vectors (to make them com-
4See section 4, we use Addition and CIFAR10 datasets and their corresponding neural networks for these experiments.
",2.1.2 Item Difficulty,[0],[0]
parable across networks) and then average them across networks for both fully and partially trained recall points.,2.1.2 Item Difficulty,[0],[0]
"As the results show, network retention decreases as item difficulty increases.",2.1.2 Item Difficulty,[0],[0]
We define strength of a network by its performance on validation data.,2.1.3 Network Strength,[0],[0]
"To understand the effect of network strength on its retention, we use the same experimental setup as before except that we keep the delay (difference between recall point and last review point) fixed while gradually increasing the recall point; this will make the networks stronger by training them for more epochs.",2.1.3 Network Strength,[0],[0]
"Then, at every recall point, we record network retention on set B instances and network accuracy on validation data.",2.1.3 Network Strength,[0],[0]
Average results across networks for two sets of 10 consecutive recall points (before fully and partially trained recall points) are shown in Figure 2(d).,2.1.3 Network Strength,[0],[0]
"As the results show, network retention increases as memory strength increases.
",2.1.3 Network Strength,[0],[0]
"The above experiments show that memory retention in neural networks is affected by the same factors that affect memory retention in humans: (a) neural networks forget training examples after a certain period of intervening training data (b): the period of recall is shorter for more difficult examples, and (c): recall improves as networks achieve better overall performance.",2.1.3 Network Strength,[0],[0]
"We conclude that delay since last review, item difficulty (loss values of training instances), and memory strength (network performance on validation data) are key indicators that affect network retention and propose to design spaced repetition algorithms that take such indicators into account in training neural networks.",2.1.3 Network Strength,[0],[0]
"We present two spaced repetition-based algorithms: a modified version of the Leitner system developed in (Reddy et al., 2016) and our Repeat before Forgetting (RbF) model respectively.",3 Spaced Repetition,[0],[0]
"Suppose we have n queues {q0, q1, . . .",3.1 Leitner System,[0],[0]
", qn−1}.",3.1 Leitner System,[0],[0]
"The Leitner system initially places all training instances in the first queue, q0.",3.1 Leitner System,[0],[0]
"As Algorithm 1 shows, at each training iteration, the Leitner scheduler chooses some queues to train a downstream neural network.",3.1 Leitner System,[0],[0]
Only instances in the selected queues will be used for training the network.,3.1 Leitner System,[0],[0]
"During training, if an instance from qi is recalled (e.g. correctly classified) by the network, the instance will be “promoted” to qi+1, otherwise it will be “demoted” to the first queue, q0.5
The Leitner scheduler reviews instances of qi at every 2i iterations.",3.1 Leitner System,[0],[0]
"Therefore, instance in lower queues (difficult/forgotten instances) are reviewed more frequently than those in higher queues (easy/recalled ones).",3.1 Leitner System,[0],[0]
Figure 3 (bottom) provides examples of queues and their processing epochs.,3.1 Leitner System,[0],[0]
"Note that the overhead imposed on training by
5 Note that in (Reddy et al., 2016) demoted instances are moved to qi−1.",3.1 Leitner System,[0],[0]
"We observed significant improvement in Leitner system by moving such instances to q0 instead of qi−1.
the Leitner system is O(|current batch|) at every epoch for moving instances between queues.",3.1 Leitner System,[0],[0]
The challenge in developing memory models is to estimate the time by which a training instance should be reviewed before it is forgotten by the network.,3.2.1 RbF Memory Models,[0],[0]
Accurate estimation of the review time leads to efficient and effective training.,3.2.1 RbF Memory Models,[0],[0]
"However, a heuristic scheduler such as Leitner system is suboptimal as its hard review schedules (i.e. only 2iiteration delays) may lead to early or late reviews.
",3.2.1 RbF Memory Models,[0],[0]
We develop flexible schedulers that take recall indicators into account in the scheduling process.,3.2.1 RbF Memory Models,[0],[0]
Our schedulers lengthen or shorten inter-repetition intervals with respect to individual training instances.,3.2.1 RbF Memory Models,[0],[0]
"In particular, we propose using density kernel functions to estimate the latest epoch in which a given training instance can be recalled.",3.2.1 RbF Memory Models,[0],[0]
"We aim to investigate how much improvement (in terms of efficiency and effectiveness) can be achieved using more flexible schedulers that utilize the recall indicators.
",3.2.1 RbF Memory Models,[0],[0]
"We propose considering density kernels as schedulers that favor (i.e., more confidently delay) less difficult training instances in stronger networks.",3.2.1 RbF Memory Models,[0],[0]
"As a kernel we can use any non-increasing function of the following quantity:
xi = di × ti se , (2)
where di indicates the loss of network for a training instance hi ∈ H, ti indicates the number of epochs to next review of hi, and se indicates the performance of network— on validation data— at epoch e. We investigate the Gaussian, Laplace, Linear, Cosine, Quadratic, and Secant kernels as described below respectively:
fgau(x, τ) = exp(−τx2), (3) flap(x, τ) = exp(−τx), (4)
flin(x, τ) = { 1− τx x < 1τ 0",3.2.1 RbF Memory Models,[0],[0]
"otherwise , (5)
fcos(x, τ) =
{ 1 2 cos(τπx)",3.2.1 RbF Memory Models,[0],[0]
"+ 1 x < 1 τ
0 otherwise ,
(6)
fqua(x, τ) = { 1− τx2",3.2.1 RbF Memory Models,[0],[0]
x2 < 1τ 0,3.2.1 RbF Memory Models,[0],[0]
"otherwise , (7)
fsec(x, τ) = 2
exp(−τx2) + exp(τx2) , (8)
where τ is a learning parameter.",3.2.1 RbF Memory Models,[0],[0]
Figure 4 depicts these kernels with τ = 1.,3.2.1 RbF Memory Models,[0],[0]
"As we will discuss in the next section, we use these kernels to optimize delay with respect to item difficulty and network strength for each training instance.",3.2.1 RbF Memory Models,[0],[0]
"Our Repeat before Forgetting (RbF) model is a spaced repetition algorithm that takes into account the previously validated recall indicators to train neural networks, see Algorithm 2.",3.2.2 RbF Algorithm,[0],[0]
RbF divides training instances into current and delayed batches based on their delay values at each iteration.,3.2.2 RbF Algorithm,[0],[0]
Instances in the current batch are those that RbF is less confident about their recall and therefore are reviewed (used to re-train the network) at current iteration.,3.2.2 RbF Algorithm,[0],[0]
"On the other hand, instances in the delayed batch are those that are likely to be recalled by the network in the future and therefore are not reviewed at current epoch.",3.2.2 RbF Algorithm,[0],[0]
"At each iteration, the RbF scheduler estimates the optimum delay (number of epochs to next review) for each training instance in the current batch.",3.2.2 RbF Algorithm,[0],[0]
"RbF makes such item-specific estimations as follows:
Given the difficulty of a training instance di, the memory strength of the neural network at epoch e, se, and an RbF memory model f (see section 3.2.1), RbF scheduler estimates the maximum delay t̂i for the instance such that it can be recalled with a confidence greater than the given threshold η ∈",3.2.2 RbF Algorithm,[0],[0]
"(0, 1) at time e+ t̂i.",3.2.2 RbF Algorithm,[0],[0]
"As described before, di and se can be represented by the current loss of the network for the instance and the current performance of the network on validation data respectively.",3.2.2 RbF Algorithm,[0],[0]
"Therefore, the maximum delay between the current (epoch e) and next reviews of the instance can be estimated as follows:
t̂i = arg min ti
( f(xi, τ̂)− η )2 , (9)
",3.2.2 RbF Algorithm,[0],[0]
s.t 1 ≤,3.2.2 RbF Algorithm,[0],[0]
ti ≤ k,3.2.2 RbF Algorithm,[0],[0]
"− e
where τ̂ is the optimum value for the learning parameter obtained from validation data, see Equation (10).",3.2.2 RbF Algorithm,[0],[0]
"In principle, reviewing instances could be delayed for any number of epochs; in practice however, delay is bounded both below and above (e.g., by queues in the Leitner system).",3.2.2 RbF Algorithm,[0],[0]
"Thus, we assume that, at each epoch e, instances could be delayed for at least one iteration and at most k − e iterations where k is the total number of training epochs.",3.2.2 RbF Algorithm,[0],[0]
"We also note that ti is a lower bound of the maximum delay as se is expected to increase and di is expected to decrease as the network trains in next iterations.
",3.2.2 RbF Algorithm,[0],[0]
Algorithm 2 shows the outline of the proposed RbF model.,3.2.2 RbF Algorithm,[0],[0]
We estimate the optimum value of τ (line 5 of Algorithm 2) for RbF memory models using validation data.,3.2.2 RbF Algorithm,[0],[0]
"In particular, RbF uses the loss values of validation instances and strength of the network obtained at the previous epoch to estimate network retention for validation instances at the current epoch (therefore ti = 1 for every validation instance).",3.2.2 RbF Algorithm,[0],[0]
"The parameter τ for each memory model is computed as follows:
τ̂ = arg min τ
( f(xj , τ)− aj )2 ,∀hj ∈ V, aj ≥ η,
(10) where aj ∈ (0, 1) is the current accuracy of the model for the validation instance hj .",3.2.2 RbF Algorithm,[0],[0]
RbF then predicts the delay for current batch instances and reduces the delay for those in the delayed batch by one epoch.,3.2.2 RbF Algorithm,[0],[0]
The overhead of RbF is O(|H|) to compute delays and O(|V|) to compute τ̂ .,3.2.2 RbF Algorithm,[0],[0]
Note that (9) and (10) have closed form solutions.,3.2.2 RbF Algorithm,[0],[0]
"Table 1 describes the tasks, datasets, and models that we consider in our experiments.",4 Experiments,[0],[0]
It also reports the training epochs for which the models produce their best performance on validation data (based on rote training).,4 Experiments,[0],[0]
"We note that the Addition dataset is randomly generated and contains numbers with at most 4 digits.6
We consider three schedulers as baselines: a slightly modified version of the Leitner scheduler (Lit) developed in Reddy et al. (2016) for human learners (see Footnote 5), curriculum learning (CL) in which training instances are scheduled with respect to their easiness (Jiang et al., 2015), and the uniform scheduler of rote training (Rote) in which all instances are used for training at every epoch.",4 Experiments,[0],[0]
"For Lit, we experimented with different queue lengths, n = {3, 5, 7}, and set n = 5 in the experiments as this value led to the best performance of this scheduler across all datasets.
",4 Experiments,[0],[0]
Curriculum learning starts training with easy instances and gradually introduces more complex instances for training.,4 Experiments,[0],[0]
"Since easiness information is not readily available in most datasets, previous approaches have used heuristic techniques (Spitkovsky et al., 2010; Basu and Christensen, 2013) or optimization algorithms (Jiang et al., 2015, 2014) to quantify easiness of training instances.",4 Experiments,[0],[0]
These approaches consider an instance as easy if its loss is smaller than a threshold (λ).,4 Experiments,[0],[0]
"We adopt this technique as follows: at each iteration e, we divide the entire training data into easy and hard sets using iteration-specific λe and the loss values of instances, obtained from the current partially-trained network.",4 Experiments,[0],[0]
All easy instances in conjunction with αe ∈,4 Experiments,[0],[0]
"[0, 1] fraction of easiest hard instances (those with smallest loss values greater than λe) are used for training at",4 Experiments,[0],[0]
"iteration e. We set
6https://github.com/fchollet/keras/ blob/master/examples/addition_rnn.py
each λe to the average loss of training instances that are correctly classified by the current partiallytrained network.",4 Experiments,[0],[0]
"Furthermore, at each iteration e, we set αe = e/k to gradually introduce complex instances at every new iteration.7 Note that we treat all instances as easy at e = 0.
",4 Experiments,[0],[0]
Performance values reported in experiments are averaged over 10 runs of systems and the confidence parameter η is always set to 0.5 unless otherwise stated.,4 Experiments,[0],[0]
"In these experiments, we evaluate memory schedulers with respect to their accuracy in predicting network retention for delayed instances.",4.1 Evaluation of Memory Models,[0],[0]
"Since curriculum learning does not estimate delay for training instances, we only consider Leitner and RbF schedulers in these experiments.
",4.1 Evaluation of Memory Models,[0],[0]
"For this evaluation, if a scheduler predicts a delay t for a training instance h at epoch e, we evaluate network retention with respect to h at epoch e+ t. If the network recalls (correctly classifies) the instance at epoch e+ t, the scheduler has correctly predicted network retention for h, and otherwise, it has made a wrong prediction.",4.1 Evaluation of Memory Models,[0],[0]
We use this binary outcome to evaluate the accuracy of each scheduler.,4.1 Evaluation of Memory Models,[0],[0]
Note that the performance of schedulers on instances that have not been delayed is not a major concern.,4.1 Evaluation of Memory Models,[0],[0]
"Although failing to delay an item inversely affects efficiency, it makes the network stronger by providing more instances to train from.",4.1 Evaluation of Memory Models,[0],[0]
"Therefore, we consider a good scheduler as the one that accurately delays more items.
",4.1 Evaluation of Memory Models,[0],[0]
Figure 6 depicts the average accuracy of schedulers in predicting networks’ retention versus the average fraction of training instances that they delayed per epoch.,4.1 Evaluation of Memory Models,[0],[0]
"As the results show, all schedulers
7k is the total number of iterations.
",4.1 Evaluation of Memory Models,[0],[0]
delay substantial amount of instances per epoch.,4.1 Evaluation of Memory Models,[0],[0]
"In particular, Cos and Qua outperform Lit in both predicting network retention and delaying items, delaying around 50% of training instances per epoch.",4.1 Evaluation of Memory Models,[0],[0]
This is while Gau and Sec show comparable accuracy to Lit but delay more instances.,4.1 Evaluation of Memory Models,[0],[0]
"On the other hand, Lap, which has been found effective in Psychology, and Lin are less accurate in predicting network retention.",4.1 Evaluation of Memory Models,[0],[0]
This is because of the tradeoff between delaying more instances and creating stronger networks.,4.1 Evaluation of Memory Models,[0],[0]
"Since these schedulers are more flexible in delaying greater amount of instances, they might not provide networks with enough data to fully train.
",4.1 Evaluation of Memory Models,[0],[0]
"Figure 7 shows the performance of RbF schedulers with respect to the recall confidence parameter η, see Equation (9).",4.1 Evaluation of Memory Models,[0],[0]
"As the results show, schedulers have poor performance with smaller values of η.",4.1 Evaluation of Memory Models,[0],[0]
This is because smaller values of η make schedulers very flexible in delaying instances.,4.1 Evaluation of Memory Models,[0],[0]
"However, the performance of schedulers are not dramatically low even with very small ηs.",4.1 Evaluation of Memory Models,[0],[0]
"Our further analyses on the delay patterns show that although a smaller η leads to more delayed instances, the delays are significantly shorter.",4.1 Evaluation of Memory Models,[0],[0]
"Therefore, most delayed instances will be “reviewed” shortly in next epochs.",4.1 Evaluation of Memory Models,[0],[0]
"These bulk reviews make the network stronger and help it to recall most delayed instance in future iterations.
",4.1 Evaluation of Memory Models,[0],[0]
"On the other hand, greater ηs lead to more accurate schedulers at the cost of using more training data.",4.1 Evaluation of Memory Models,[0],[0]
"In fact, we found that larger ηs do not delay most training instances in the first few iterations.",4.1 Evaluation of Memory Models,[0],[0]
"However, once the network obtains a reasonably high performance, schedulers start delaying instances for longer durations.",4.1 Evaluation of Memory Models,[0],[0]
We will further study this effect in the next section.,4.1 Evaluation of Memory Models,[0],[0]
We compare RbF against Leitner and curriculum learning in terms of efficiency of training and effectiveness of trained models.,4.2 Efficiency and Effectiveness,[0],[0]
"We define effectiveness as the accuracy of a trained network on balanced test data, and efficiency as (a): fraction of instances used for training per epoch, and (b): required time for training the networks.",4.2 Efficiency and Effectiveness,[0],[0]
"For RbF schedulers, we set η to 0.5 and consider the best performing kernel Cosine with η = 0.9 based on results in Figure 7.
",4.2 Efficiency and Effectiveness,[0],[0]
The results in Table 2 show that all training paradigms have comparable effectiveness (Accuracy) to that of rote training (Rote).,4.2 Efficiency and Effectiveness,[0],[0]
Our RbF schedulers use less data per epoch (34-50% of data) and run considerably faster than Rote (2.90-4.78 times faster for η = 0.5).,4.2 Efficiency and Effectiveness,[0],[0]
"The results also show that Lit is slightly less accurate but runs 2.87 time faster than Rote; note that, as a scheduler, Lit is less accurate than RbF models, see Figures 6 and 7.
",4.2 Efficiency and Effectiveness,[0],[0]
"In addition, CL leads to comparable performance to RbF but is considerably slower than other schedulers.",4.2 Efficiency and Effectiveness,[0],[0]
This is because this scheduler has to identify easier instances and sort the harder ones to sample training data at each iteration.,4.2 Efficiency and Effectiveness,[0],[0]
"Overall, the performance of Lit, CL, Cos η = .5 and Cos η = .9 are only 2.76, 1.90, 1.88, and 0.67 absolute values lower than that of Rote respectively.",4.2 Efficiency and Effectiveness,[0],[0]
"Considering the achieved efficiency, these differences are negligible (see the overall gain in Table 2).
",4.2 Efficiency and Effectiveness,[0],[0]
Figure 8 reports detailed efficiency and effectiveness results across datasets and networks.,4.2 Efficiency and Effectiveness,[0],[0]
"For clear illustration, we report accuracy at iterations 2i ∀i in which Lit is trained on the entire data, and consider Cos η = .5",4.2 Efficiency and Effectiveness,[0],[0]
as RbF scheduler.,4.2 Efficiency and Effectiveness,[0],[0]
"In terms of efficiency (first row of Figure 8), CL starts with (small set of)
easier instances and gradually increases the amount of training data by adding slightly harder instances into its training set.",4.2 Efficiency and Effectiveness,[0],[0]
"On the other hand, Lit and RbF start big and gradually delay reviewing (easy) instances that the networks have learned.",4.2 Efficiency and Effectiveness,[0],[0]
"The difference between these two training paradigms is apparent in Figures 8(a)-8(c).
",4.2 Efficiency and Effectiveness,[0],[0]
The results also show that the efficiency of a training paradigm depends on the initial effectiveness of the downstream neural network.,4.2 Efficiency and Effectiveness,[0],[0]
"For CL to be efficient, the neural network need to initially have low performance (accuracy) so that the scheduler works on smaller set of easy instances.",4.2 Efficiency and Effectiveness,[0],[0]
"For example, in case of Addition, Figures 8(b) and 8(e), the initial network accuracy is only 35%, therefore most instances are expected to be initially treated as hard instances and don’t be used for training.",4.2 Efficiency and Effectiveness,[0],[0]
"On the other hand, CL shows a considerably lower efficiency for networks with slightly high initial accuracy, e.g. in case of IMDb or CIFAR10 where the initial network accuracy is above 56%, see Figures 8(a) and 8(d), and 8(c) and 8(f) respectively.
",4.2 Efficiency and Effectiveness,[0],[0]
"In contrast to CL, Lit and RbF are more efficient when the network has a relatively higher initial performance.",4.2 Efficiency and Effectiveness,[0],[0]
"A higher initial performance helps the
schedulers to more confidently delay “reviewing” most instances and therefore train with a much smaller set of instances.",4.2 Efficiency and Effectiveness,[0],[0]
"For example, since the initial network accuracy in IMDb or CIFAR10 is above 56%, Lit and RbF are considerably more efficient from the beginning of the training process.",4.2 Efficiency and Effectiveness,[0],[0]
"However, in case of low initial performance, Lit and RbF tend to avoid delaying instances at lower iterations which leads to poor efficiency at the beginning.",4.2 Efficiency and Effectiveness,[0],[0]
"This is the case for the Addition dataset in which instances are gradually delayed by these two schedulers even at epoch 8 when the performance of the network reaches above 65%, see Figures 8(e) and 8(b).",4.2 Efficiency and Effectiveness,[0],[0]
"However, Lit gains its true efficiency after iteration 12, see Figure 8(b), while RbF still gradually improves the efficiency.",4.2 Efficiency and Effectiveness,[0],[0]
"This might be because of the lower bound delays that RbF estimates, see Equation (9).
",4.2 Efficiency and Effectiveness,[0],[0]
"Furthermore, the effectiveness results in Figure 8 (bottom) show that all schedulers produce comparable accuracy to the Rote scheduler throughout the training process, not just at specific iterations.",4.2 Efficiency and Effectiveness,[0],[0]
"This indicates that these training paradigms can much faster achieve the same generalizability as standard training, see Figures 8(b) and 8(e).",4.2 Efficiency and Effectiveness,[0],[0]
We investigate the effect of spaced repetition on overtraining.,4.3 Robustness against Overtraining,[0],[0]
The optimal number of training epochs required to train fastText on the IMDb dataset is 8 epochs (see Table 1).,4.3 Robustness against Overtraining,[0],[0]
"In this experiment, we run fastText on IMDb for greater number of iterations to investigate the robustness of different schedulers against overtraining.",4.3 Robustness against Overtraining,[0],[0]
The results in Figure 9 show that Lit and RbF (Cos η = 0.5) are more robust against overtraining.,4.3 Robustness against Overtraining,[0],[0]
"In fact, the performance of Lit and RbF further improve at epoch 16 while CL and Rote overfit at epoch 16 (note that CL and Rote also require considerably more amount of time to reach to higher iterations).",4.3 Robustness against Overtraining,[0],[0]
We attribute the robustness of Lit and RbF to the scheduling mechanism which helps the networks to avoid retraining with easy instances.,4.3 Robustness against Overtraining,[0],[0]
"On the other hand, overtraining affects Lit and RbF at higher training iterations, compare performance of each scheduler at epochs 8 and 32.",4.3 Robustness against Overtraining,[0],[0]
This might be because these training paradigms overfit the network by paying too much training attention to very hard instances which might introduce noise to the model.,4.3 Robustness against Overtraining,[0],[0]
"Ebbinghaus (1913, 2013), and recently Murre and Dros (2015), studied the hypothesis of the exponential nature of forgetting, i.e. how information is lost over time when there is no attempt to retain it.",5 Related Work,[0],[0]
"Previous research identified three critical indicators that affect the probability of recall: repeated exposure to learning materials, elapsed time since their last review (Ebbinghaus, 1913; Wixted, 1990; Dempster, 1989), and more recently item difficulty (Reddy et al., 2016).",5 Related Work,[0],[0]
We based our investigation on these findings and validated that these indicators indeed affect memory retention in neural networks.,5 Related Work,[0],[0]
"We then developed training paradigms that utilize the above indicators to train networks.
",5 Related Work,[0],[0]
Bengio et al. (2009) and Kumar et al. (2010) also developed cognitively-motivated training paradigms which are inspired by the principle that learning can be more effective when training starts with easier concepts and gradually proceeds with more difficult ones.,5 Related Work,[0],[0]
"Our idea is motivated by the spaced repetition principle which indicates learning improves with repeated exposure and decays with delay since last exposure (Ebbinghaus, 1913; Dempster, 1989).",5 Related Work,[0],[0]
"Based on this principle, we developed schedulers that space the reviews of training instances over time for efficient and effective training of neural networks.",5 Related Work,[0],[0]
We developed a cognitively-motivated training paradigm (scheduler) that space instances over time for efficient and effective training of neural networks.,6 Conclusion and Future Work,[0],[0]
Our scheduler only uses a small fraction of training data per epoch but still effectively train neural networks.,6 Conclusion and Future Work,[0],[0]
It achieves this by estimating the time (number of epochs) by which training could be delayed for each instance.,6 Conclusion and Future Work,[0],[0]
"Our work was inspired by three recall indicators that affect memory retention in humans, namely difficulty of learning materials, delay since their last review, and memory strength of the learner, which we validated in the context of neural networks.
",6 Conclusion and Future Work,[0],[0]
There are several avenues for future work including the extent to which our RbF model and its kernels could be combined with curriculum learning or Leitner system to either predict easiness of novel training instances to inform curriculum learning or incorporate Leitner’s queueing mechanism to the RbF model.,6 Conclusion and Future Work,[0],[0]
"Other directions include extending RbF to dynamically learn the recall confidence parameter with respect to network behavior, or developing more flexible delay functions with theoretical analysis on their lower and upper bounds.",6 Conclusion and Future Work,[0],[0]
We thank Mitra Mohtarami for her constructive feedback during the development of this paper and anonymous reviewers for their thoughtful comments.,Acknowledgments,[0],[0]
This work was supported by National Institutes of Health (NIH) grant R01GM114355 from the National Institute of General Medical Sciences (NIGMS).,Acknowledgments,[0],[0]
The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.,Acknowledgments,[0],[0]
We present a novel approach for training artificial neural networks.,abstractText,[0],[0]
Our approach is inspired by broad evidence in psychology that shows human learners can learn efficiently and effectively by increasing intervals of time between subsequent reviews of previously learned materials (spaced repetition).,abstractText,[0],[0]
We investigate the analogy between training neural models and findings in psychology about human memory model and develop an efficient and effective algorithm to train neural models.,abstractText,[0],[0]
The core part of our algorithm is a cognitively-motivated scheduler according to which training instances and their “reviews” are spaced over time.,abstractText,[0],[0]
"Our algorithm uses only 34-50% of data per epoch, is 2.9-4.8 times faster than standard training, and outperforms competing state-of-the-art baselines.1",abstractText,[0],[0]
Repeat before Forgetting: Spaced Repetition for Efficient and Effective Training of Neural Networks,title,[0],[0]
"With the ever growing amount of textual data from a large variety of languages, domains, and genres, it has become standard to evaluate NLP algorithms on multiple datasets in order to ensure a consistent performance across heterogeneous setups. However, such multiple comparisons pose significant challenges to traditional statistical analysis methods in NLP and can lead to erroneous conclusions. In this paper we propose a Replicability Analysis framework for a statistically sound analysis of multiple comparisons between algorithms for NLP tasks. We discuss the theoretical advantages of this framework over the current, statistically unjustified, practice in the NLP literature, and demonstrate its empirical value across four applications: multi-domain dependency parsing, multilingual POS tagging, cross-domain sentiment classification and word similarity prediction. 1",text,[0.9520258870041864],"['In this paper, we propose RESIDE, a novel neural network based model which makes principled use of relevant side information, such as entity type and relation alias, from Knowledge Base, for improving distant supervised relation extraction.']"
The field of Natural Language Processing (NLP) is going through the data revolution.,1 Introduction,[0],[0]
"With the persistent increase of the heterogeneous web, for the first time in human history, written language from multiple languages, domains, and genres is now abundant.",1 Introduction,[0],[0]
"Naturally, the expectations from NLP algorithms also grow and evaluating a new algorithm on as many languages, domains, and genres as possible is becoming a de-facto standard.
",1 Introduction,[0],[0]
"1Our code is at: https://github.com/rtmdrr/replicabilityanalysis-NLP .
",1 Introduction,[0],[0]
"For example, the phrase structure parsers of Charniak (2000) and Collins (2003) were mostly evaluated on the Wall Street Journal Penn Treebank (Marcus et al., 1993), consisting of written, edited English text of economic news.",1 Introduction,[0],[0]
"In contrast, modern dependency parsers are expected to excel on the 19 languages of the CoNLL 2006-2007 shared tasks on multilingual dependency parsing (Buchholz and Marsi, 2006; Nilsson et al., 2007), and additional challenges, such as the shared task on parsing multiple English Web domains (Petrov and McDonald, 2012), are continuously proposed.
",1 Introduction,[0],[0]
"Despite the growing number of evaluation tasks, the analysis toolbox employed by NLP researchers has remained quite stable.",1 Introduction,[0],[0]
"Indeed, in most experimental NLP papers, several algorithms are compared on a number of datasets where the performance of each algorithm is reported together with per-dataset statistical significance figures.",1 Introduction,[0],[0]
"However, with the growing number of evaluation datasets, it becomes more challenging to draw comprehensive conclusions from such comparisons.",1 Introduction,[0],[0]
"This is because although the probability of drawing an erroneous conclusion from a single comparison is small, with multiple comparisons the probability of making one or more false claims may be very high.
",1 Introduction,[0],[0]
"The goal of this paper is to provide the NLP community with a statistical analysis framework, which we term Replicability Analysis, which will allow us to draw statistically sound conclusions in evaluation setups that involve multiple comparisons.",1 Introduction,[0],[0]
"The classical goal of replicability analysis is to examine the consistency of findings across studies in order to address the basic dogma of science, that a find-
471
Transactions of the Association for Computational Linguistics, vol. 5, pp.",1 Introduction,[0],[0]
"471–486, 2017.",1 Introduction,[0],[0]
Action Editor: Brian Roark.,1 Introduction,[0],[0]
"Submission batch: 3/2017; Revision batch: 7/2017; Published 11/2017.
",1 Introduction,[0],[0]
c©2017 Association for Computational Linguistics.,1 Introduction,[0],[0]
"Distributed under a CC-BY 4.0 license.
",1 Introduction,[0],[0]
"ing is more convincingly true if it is replicated in at least one more study (Heller et al., 2014; Patil et al., 2016).",1 Introduction,[0],[0]
"We adapt this goal to NLP, where we wish to ascertain the superiority of one algorithm over another across multiple datasets, which may come from different languages, domains, and genres.",1 Introduction,[0],[0]
"Finding that one algorithm outperforms another across domains gives a sense of consistency to the results and positive evidence that the better performance is not specific to a selected setup.2
In this work we address two questions: (1) Counting: For how many datasets does a given algorithm outperform another?",1 Introduction,[0],[0]
"and (2) Identification: What are these datasets?
",1 Introduction,[0],[0]
"When comparing two algorithms on multiple datasets, NLP papers often answer informally the questions we address in this work.",1 Introduction,[0],[0]
"In some cases this is done without any statistical analysis, by simply declaring better performance of a given algorithm for datasets where its performance measure is better than that of another algorithm, and counting these datasets.",1 Introduction,[0],[0]
In other cases answers are based on the p-values from statistical tests performed for each dataset: declaring better performance for datasets with p-value below the significance level (e.g. 0.05) and counting these datasets.,1 Introduction,[0],[0]
"While it is clear that the first approach is not statistically valid, it seems that our community is not aware of the fact that the second approach, which may seem statistically sound, is not valid as well.",1 Introduction,[0],[0]
"This may lead to erroneous conclusions, which result in adopting new (and probably complicated) algorithms, while they are not better than previous (probably more simple) ones.
",1 Introduction,[0],[0]
"In this work, we demonstrate this problem and show that it becomes more severe as the number of evaluation sets grows, which seems to be the current trend in NLP.",1 Introduction,[0],[0]
"We adopt a known general statistical methodology for addressing the counting (question (1)) and identification (question (2)) problems, by choosing the tests and procedures which are valid for
2“Replicability” is sometimes referred to as “reproducibility”.",1 Introduction,[0],[0]
"In recent NLP work the term reproducibility was used when trying to get identical results on the same data (Névéol et al., 2016; Marrese-Taylor and Matsuo, 2017).",1 Introduction,[0],[0]
"In this paper, we adopt the meaning of “replicability” and its distinction from “reproducibility” from Peng (2011) and Leek and Peng (2015) and refer to replicability analysis as the effort to show that a finding is consistent over different datasets from different domains or languages, and is not idiosyncratic to a specific scenario.
situations encountered in NLP problems, and giving specific recommendations for such situations.
",1 Introduction,[0],[0]
"Particularly, we first demonstrate (Section 3) that the current prominent approach in the NLP literature, identifying the datasets for which the difference between the performance of the algorithms reaches a predefined significance level according to some statistical significance test, does not guarantee to bound the probability to make at least one erroneous claim.",1 Introduction,[0],[0]
Hence this approach is error-prone when the number of participating datasets is large.,1 Introduction,[0],[0]
We thus propose an alternative approach (Section 4).,1 Introduction,[0],[0]
"For question (1), we adopt the approach of Benjamini et al. (2009) to replicability analysis of multiple studies, based on the partial conjunction framework of Benjamini and Heller (2008).",1 Introduction,[0],[0]
This analysis comes with a guarantee that the probability of overestimating the true number of datasets with effect is upper bounded by a predefined constant.,1 Introduction,[0],[0]
"For question (2), we motivate a multiple testing procedure which guarantees that the probability of making at least one erroneous claim on the superiority of one algorithm over another is upper bounded by a predefined constant.
",1 Introduction,[0],[0]
"In Sections 5 and 6 we demonstrate how to apply the proposed frameworks to two synthetic data toy examples and four NLP applications: multidomain dependency parsing, multilingual POS tagging, cross-domain sentiment classification, and word similarity prediction with word embedding models.",1 Introduction,[0],[0]
"Our results demonstrate that the current practice in NLP for addressing our questions is error-prone, and illustrate the differences between it and the proposed statistically sound approach.
",1 Introduction,[0],[0]
"We hope that this work will encourage our community to increase the number of standard evaluation setups per task when appropriate (e.g. including additional languages and domains), possibly paving the way to hundreds of comparisons per study.",1 Introduction,[0],[0]
This is due to two main reasons.,1 Introduction,[0],[0]
"First, replicability analysis is a statistically sound framework that allows a researcher to safely draw valid conclusions with well defined statistical guarantees.",1 Introduction,[0],[0]
"Moreover, this framework provides a means of summarizing a large number of experiments with a handful of easily interpretable numbers (e.g., see Table 1).",1 Introduction,[0],[0]
"This allows researchers to report results over a large number of comparisons in a concise manner, delving into details of particular comparisons when necessary.",1 Introduction,[0],[0]
"Our work recognizes the current trend in the NLP community where, for many tasks and applications, the number of evaluation datasets constantly increases.",2 Previous Work,[0],[0]
We believe this trend is inherent to language processing technology due to the multiplicity of languages and of linguistic genres and domains.,2 Previous Work,[0],[0]
"In order to extend the reach of NLP algorithms, they have to be designed so that they can deal with many languages and with the various domains of each.",2 Previous Work,[0],[0]
"Having a sound statistical framework that can deal with multiple comparisons is hence crucial for the field.
",2 Previous Work,[0],[0]
This section is hence divided into two.,2 Previous Work,[0],[0]
"We start by discussing representative examples for multiple comparisons in NLP, focusing on evaluations across multiple languages and multiple domains.",2 Previous Work,[0],[0]
"We then discuss existing analysis frameworks for multiple comparisons, both in the NLP and in the machine learning literatures, pointing to the need for establishing new standards for our community.
",2 Previous Work,[0],[0]
"Multiple Comparisons in NLP Multiple comparisons of algorithms over datasets from different languages, domains and genres have become a de-facto standard in many areas of NLP.",2 Previous Work,[0],[0]
Here we survey a number of representative examples.,2 Previous Work,[0],[0]
"A full list of NLP tasks is beyond the scope of this paper.
",2 Previous Work,[0],[0]
"A common multilingual example is, naturally, machine translation, where it is customary to compare algorithms across a large number of sourcetarget language pairs.",2 Previous Work,[0],[0]
"This is done, for example, with the Europarl corpus consisting of 21 European languages (Koehn, 2005; Koehn and Schroeder, 2007) and with the datasets of the WMT workshop series with its multiple domains (e.g. news and biomedical in 2017), each consisting of several language pairs (7 and 14, respectively, in 2017).
",2 Previous Work,[0],[0]
Multiple dataset comparisons are also abundant in domain adaptation work.,2 Previous Work,[0],[0]
"Representative tasks include named entity recognition (Guo et al., 2009), POS tagging (Daumé III, 2007), dependency parsing (Petrov and McDonald, 2012), word sense disambiguation (Chan and Ng, 2007) and sentiment classification (Blitzer et al., 2006; Blitzer et al., 2007).
",2 Previous Work,[0],[0]
"More recently, with the emergence of crowdsourcing that makes data collection cheap and fast (Snow et al., 2008), an ever growing number of datasets is being created.",2 Previous Work,[0],[0]
"This is particularly notice-
able in lexical semantics tasks that have become central in NLP research due to the prominence of neural networks.",2 Previous Work,[0],[0]
"For example, it is customary to compare word embedding models (Mikolov et al., 2013; Pennington et al., 2014; Ó",2 Previous Work,[0],[0]
"Séaghdha and Korhonen, 2014; Levy and Goldberg, 2014; Schwartz et al., 2015) on multiple datasets where word pairs are scored according to the degree to which different semantic relations, such as similarity and association, hold between the members of the pair (Finkelstein et al., 2001a; Bruni et al., 2014; Silberer and Lapata, 2014; Hill et al., 2015).",2 Previous Work,[0],[0]
"In some works (e.g., Baroni et al. (2014))",2 Previous Work,[0],[0]
"these embedding models are compared across a large number of simple tasks.
",2 Previous Work,[0],[0]
"As discussed in Section 1, the outcomes of such comparisons are often summarized in a table that presents numerical performance values, usually accompanied by statistical significance figures and sometimes also with cross-comparison statistics such as average performance figures.",2 Previous Work,[0],[0]
"Here, we analyze the conclusions that can be drawn from this information and suggest that with the growing number of comparisons, a more intricate analysis is required.
",2 Previous Work,[0.9522000778936714],"['Overall, we find that RESIDE gives competitive performance even when very limited amount of relation alias information is available.']"
"Existing Analysis Frameworks Machine learning work on multiple dataset comparisons dates back to Dietterich (1998) who raised the question: “given two learning algorithms and datasets from several domains, which algorithm will produce more accurate classifiers when trained on examples from new domains?”.",2 Previous Work,[0],[0]
The seminal work that proposed practical means for this problem is that of Demšar (2006).,2 Previous Work,[0],[0]
"Given performance measures for two algorithms on multiple datasets, the authors test whether there is at least one dataset on which the difference between the algorithms is statistically significant.",2 Previous Work,[0],[0]
"For this goal they propose methods such as a paired t-test, a nonparametric sign-rank test and a wins/losses/ties count, all computed across the results collected from all participating datasets.",2 Previous Work,[0],[0]
"In contrast, our goal is to count and identify the datasets for which one algorithm significantly outperforms the other, which provides more intricate information, especially when the datasets come from different sources.
",2 Previous Work,[0],[0]
"In NLP, several studies addressed the problem of measuring the statistical significance of results on a single dataset (e.g., Berg-Kirkpatrick et al. (2012); Søgaard (2013); Søgaard et al. (2014)).",2 Previous Work,[0],[0]
"Søgaard
(2013) is, to the best of our knowledge, the only work that addressed the statistical properties of evaluation with multiple datasets.",2 Previous Work,[0],[0]
"For this aim he modified the statistical tests proposed in Demšar (2006) to use a Gumbel distribution assumption on the test statistics, which he considered to suit NLP better than the original Gaussian assumption.",2 Previous Work,[0],[0]
"However, while this procedure aims to estimate the effect size across datasets, it answers neither the counting nor the identification question of Section 1.
",2 Previous Work,[0],[0]
In the next section we provide the preliminary knowledge from the field of statistics that forms the basis for the proposed framework and then proceed with its description.,2 Previous Work,[0],[0]
We start by formulating a general hypothesis testing framework for a comparison between two algorithms.,3 Preliminaries,[0],[0]
"This is a common type of hypothesis testing framework applied in NLP, its detailed formulation will help us develop our ideas.",3 Preliminaries,[0],[0]
"We wish to compare between two algorithms, A and B. Let X be a collection of datasets X = {X1, X2, . . .",3.1 Hypothesis Testing,[0],[0]
", XN}, where for all i ∈ {1, . . .",3.1 Hypothesis Testing,[0],[0]
", N}, Xi = {xi,1, . . .",3.1 Hypothesis Testing,[0],[0]
", xi,ni} .",3.1 Hypothesis Testing,[0],[0]
Each dataset Xi can be of a different language or a different domain.,3.1 Hypothesis Testing,[0],[0]
"We denote by xi,k the granular unit on which results are being measured, that, in most NLP tasks, is a word or a sequence of words.",3.1 Hypothesis Testing,[0],[0]
"The difference in performance between the two algorithms is measured using one or more of the evaluation measures in the setM = {M1, . . .",3.1 Hypothesis Testing,[0],[0]
",Mm}.3
Let us denoteMj(ALG,Xi) as the value of the measureMj when algorithmALG is applied on the dataset Xi.",3.1 Hypothesis Testing,[0],[0]
"Without loss of generality, we assume that higher values of the measure are better.",3.1 Hypothesis Testing,[0],[0]
"We define the difference in performance between two algorithms, A and B, according to the measure",3.1 Hypothesis Testing,[0],[0]
"Mj on the dataset Xi as:
δj(X i) =Mj(A,Xi)−Mj(B,Xi).
",3.1 Hypothesis Testing,[0],[0]
"3To keep the discussion concise, throughout this paper we assume that only one evaluation measure is used.",3.1 Hypothesis Testing,[0],[0]
"Our framework can be easily extended to deal with multiple measures.
",3.1 Hypothesis Testing,[0],[0]
"Finally, using this notation we formulate the following statistical hypothesis testing problem:
H0i(j) :δj(X i) ≤ 0
H1i(j) :δj(X i) > 0.
(1)
The null hypothesis, stating that there is no difference between the performance of algorithm A and algorithmB, or thatB performs better, is tested versus the alternative statement thatA is superior.",3.1 Hypothesis Testing,[0],[0]
"If the statistical test results in rejecting the null hypothesis, one concludes that A outperforms B in this setup.",3.1 Hypothesis Testing,[0],[0]
"Otherwise, there is not enough evidence in the data to make this conclusion.
",3.1 Hypothesis Testing,[0],[0]
"Rejection of the null hypothesis when it is true is termed type I error, and non-rejection of the null hypothesis when the alternative is true is termed type II error.",3.1 Hypothesis Testing,[0],[0]
"The classical approach to hypothesis testing is to find a test that guarantees that the probability of making a type I error is upper bounded by a predefined constant α, the test significance level, while achieving as low probability of type II error as possible, a.k.a “achieving as high power as possible”.
",3.1 Hypothesis Testing,[0],[0]
We next turn to the case where the difference between two algorithms is tested across multiple datasets.,3.1 Hypothesis Testing,[0],[0]
Equation 1 defines a multiple hypothesis testing problem when considering the formulation for all N datasets.,3.2 The Multiplicity Problem,[0],[0]
"If N is large, testing each hypothesis separately at the nominal significance level may result in a high number of erroneously rejected null hypotheses.",3.2 The Multiplicity Problem,[0],[0]
"In our context, when the performance of algorithm A is compared to that of algorithm B across multiple datasets, and for each dataset algorithm A is declared as superior, based on a statistical test at the nominal significance level α, the expected number of erroneous claims may grow as N grows.
",3.2 The Multiplicity Problem,[0],[0]
"For example, if a single test is performed with a significance level of α = 0.05, there is only a 5% chance of incorrectly rejecting the null hypothesis.",3.2 The Multiplicity Problem,[0],[0]
"On the other hand, for 100 tests where all null hypotheses are true, the expected number of incorrect rejections is 100 · 0.05 = 5.",3.2 The Multiplicity Problem,[0],[0]
"Denoting the total number of type I errors as V , we can see below that if the test statistics are independent then the probability of
making at least one incorrect rejection is 0.994:
P(V > 0)",3.2 The Multiplicity Problem,[0],[0]
"= 1− P(V = 0) =
1− 100∏
i=1
P(no type I error in i)",3.2 The Multiplicity Problem,[0],[0]
"=1− (1− 0.05)100.
",3.2 The Multiplicity Problem,[0],[0]
This demonstrates that the naive method of counting the datasets for which significance was reached at the nominal level is error-prone.,3.2 The Multiplicity Problem,[0],[0]
"Similar examples can be constructed for situations where some of the null hypotheses are false.
",3.2 The Multiplicity Problem,[0],[0]
"The multiple testing literature proposes various procedures for bounding the probability of making at least one type I error, as well as other, less restrictive error criteria (see a survey in Farcomeni (2007)).",3.2 The Multiplicity Problem,[0],[0]
"In this paper, we address the questions of counting and identifying the datasets for which algorithm A outperforms B, with certain statistical guarantees regarding erroneous claims.",3.2 The Multiplicity Problem,[0],[0]
"While identifying the datasets gives more information when compared to just declaring their number, we consider these two questions separately.",3.2 The Multiplicity Problem,[0],[0]
"As our experiments show, according to the statistical analysis we propose the estimated number of datasets with effect (question 1) may be higher than the number of identified datasets (question 2).",3.2 The Multiplicity Problem,[0],[0]
We next present the fundamentals of the partial conjunction framework which is at the heart of our proposed methods.,3.2 The Multiplicity Problem,[0],[0]
We start by reformulating the set of hypothesis testing problems of Equation 1 as a unified hypothesis testing problem.,3.3 Partial Conjunction Hypotheses,[0],[0]
This problem aims to identify whether algorithm A is superior to B across all datasets.,3.3 Partial Conjunction Hypotheses,[0],[0]
"The notation for the null hypothesis in this problem is HN/N0 since we test if N out of N alternative hypotheses are true:
H N/N 0",3.3 Partial Conjunction Hypotheses,[0],[0]
":
N⋃
i=1
H0i is true vs. H N/N 1 :
N⋂
i=1
H1i is true.
",3.3 Partial Conjunction Hypotheses,[0],[0]
"Requiring the rejection of the disjunction of all null hypotheses is often too restrictive for it involves observing a significant effect on all datasets, i ∈ {1, . . .",3.3 Partial Conjunction Hypotheses,[0],[0]
", N}.",3.3 Partial Conjunction Hypotheses,[0],[0]
"Instead, one can require a rejection of the global null hypothesis stating that all individual null hypotheses are true, i.e., evidence that
at least one alternative hypothesis is true.",3.3 Partial Conjunction Hypotheses,[0],[0]
"This hypothesis testing problem is formulated as follows:
H 1/N 0",3.3 Partial Conjunction Hypotheses,[0],[0]
":
N⋂
i=1
H0i is true vs. H 1/N 1 :
N⋃
i=1
H1i is true.
",3.3 Partial Conjunction Hypotheses,[0],[0]
"Obviously, rejecting the global null may not provide enough information: it only indicates that algorithm A outperforms B on at least one dataset.",3.3 Partial Conjunction Hypotheses,[0],[0]
"Hence, this claim does not give any evidence for the consistency of the results across multiple datasets.
",3.3 Partial Conjunction Hypotheses,[0],[0]
"A natural compromise between the above two formulations is to test the partial conjunction null, which states that the number of false null hypotheses is lower than u, where 1 ≤ u ≤ N is a pre-specified integer constant.",3.3 Partial Conjunction Hypotheses,[0],[0]
"The partial conjunction test contrasts this statement with the alternative statement that at least u out of the N null hypotheses are false.
",3.3 Partial Conjunction Hypotheses,[0],[0]
Definition 1 (Benjamini and Heller (2008)).,3.3 Partial Conjunction Hypotheses,[0],[0]
"Consider N ≥ 2 null hypotheses: H01, H02, . . .",3.3 Partial Conjunction Hypotheses,[0],[0]
",H0N , and let p1, . . .",3.3 Partial Conjunction Hypotheses,[0],[0]
", pN be their associated p−values.",3.3 Partial Conjunction Hypotheses,[0],[0]
"Let k be the true unknown number of false null hypotheses, then our question “Are at least u out of N null hypotheses false?” can be formulated as follows:
H u/N 0",3.3 Partial Conjunction Hypotheses,[0],[0]
:,3.3 Partial Conjunction Hypotheses,[0],[0]
"k < u vs. H u/N 1 : k ≥ u.
",3.3 Partial Conjunction Hypotheses,[0],[0]
"In our context, k is the number of datasets where algorithm A is truly better, and the partial conjunction test examines whether algorithmA outperforms algorithm B in at least u of N cases.
",3.3 Partial Conjunction Hypotheses,[0],[0]
Benjamini and Heller (2008) developed a general method for testing the above hypothesis for a given u. They also showed how to extend their method in order to answer our counting question.,3.3 Partial Conjunction Hypotheses,[0],[0]
"We next describe their framework and advocate a different, yet related method for dataset identification.",3.3 Partial Conjunction Hypotheses,[0],[0]
"Referred to as the cornerstone of science (Moonesinghe et al., 2007), replicability analysis is of predominant importance in many scientific fields including psychology (Collaboration, 2012), genomics (Heller et al., 2014), economics (Herndon et al., 2014) and medicine (Begley and Ellis, 2012), among others.",4 Replicability Analysis for NLP,[0],[0]
"Findings are usually considered as replicated if they are obtained in two or more
studies that differ from each other in some aspects (e.g. language, domain or genre in NLP).
",4 Replicability Analysis for NLP,[0],[0]
"The replicability analysis framework we employ (Benjamini and Heller, 2008; Benjamini et al., 2009) is based on partial conjunction testing.",4 Replicability Analysis for NLP,[0],[0]
"Particularly, these authors have shown that a lower bound on the number of false null hypotheses with a confidence level of 1 − α can be obtained by finding the largest u for which we can reject the partial conjunction null hypothesis Hu/N0 along with H
1/N 0 , . . .",4 Replicability Analysis for NLP,[0],[0]
",H (u−1)/N 0 at a significance levelα.",4 Replicability Analysis for NLP,[0],[0]
"Since rejecting Hu/N0 means that we see evidence in at least u out of N datasets, algorithm",4 Replicability Analysis for NLP,[0],[0]
"A is superior to B. This lower bound on k is taken as our answer to the Counting question of Section 1.
",4 Replicability Analysis for NLP,[0],[0]
"In line with the hypothesis testing framework of Section 3, the partial conjunction null, Hu/N0 , is rejected at level α if pu/N ≤ α, where pu/N is the partial conjunction p-value.",4 Replicability Analysis for NLP,[0],[0]
"Based on the known methods for testing the global null hypothesis (see, e.g., Loughin (2004)), Benjamini and Heller (2008) proposed methods for combining the p−values p1, . . .",4 Replicability Analysis for NLP,[0],[0]
", pN of H01, H02, . . .",4 Replicability Analysis for NLP,[0],[0]
",H0N in order to obtain pu/N .",4 Replicability Analysis for NLP,[0],[0]
"Below, we describe two such methods and their properties.",4 Replicability Analysis for NLP,[0],[0]
"The methods we focus on were developed by Benjamini and Heller (2008), and are based on Fisher’s and Bonferroni’s methods for testing the global null hypothesis.",4.1 The Partial Conjunction p−value,[0],[0]
"For brevity, we name them Bonferroni and Fisher.",4.1 The Partial Conjunction p−value,[0],[0]
"We choose them because they are valid in different setups that are frequently encountered in NLP (Section 6): Bonferroni for dependent datasets and both Fisher and Bonferroni for independent datasets.4
Bonferroni’s method does not make any assumptions about the dependencies between the participating datasets and it is hence applicable in NLP tasks, since in NLP it is most often hard to determine the type of dependence between the datasets.",4.1 The Partial Conjunction p−value,[0],[0]
"Fisher’s method, while assuming independence across the
4For simplicity we refer to dependent/independent datasets as those for which the test statistics are dependent/independent.",4.1 The Partial Conjunction p−value,[0],[0]
"We assume the test statistics are independent if the corresponding datasets do not have mutual samples, and one dataset is not a transformation of the other.
participating datasets, is often more powerful than Bonferroni’s method (see Loughin (2004) and Benjamini and Heller (2008) for other methods and a comparison between them).",4.1 The Partial Conjunction p−value,[0],[0]
"Our recommendation is hence to use the Bonferroni’s method when the datasets are dependent and to use the more powerful Fisher’s method when the datasets are independent.
",4.1 The Partial Conjunction p−value,[0],[0]
"Let p(i) be the i-th smallest p−value among p1, . . .",4.1 The Partial Conjunction p−value,[0],[0]
", pN .",4.1 The Partial Conjunction p−value,[0],[0]
"The partial conjunction p−values are:
p u/N Bonferroni = (N − u+ 1)p(u) (2)
p u/N Fisher = P ( χ22(N−u+1) ≥ −2",4.1 The Partial Conjunction p−value,[0],[0]
"N∑
i=u
ln p(i)
) (3)
where χ22(N−u+1) denotes a chi-squared random variable with 2(N − u+ 1) degrees of freedom.
",4.1 The Partial Conjunction p−value,[0],[0]
"To understand the reasoning behind these methods, let us consider first the above p−values for testing the global null, i.e., for the case of u = 1.",4.1 The Partial Conjunction p−value,[0],[0]
Rejecting the global null hypothesis requires evidence that at least one null hypothesis is false.,4.1 The Partial Conjunction p−value,[0],[0]
"Intuitively, we would like to see one or more small p−values.
",4.1 The Partial Conjunction p−value,[0],[0]
Both of the methods above agree with this intuition.,4.1 The Partial Conjunction p−value,[0],[0]
"Bonferroni’s method rejects the global null if p(1) ≤ α/N , i.e. if the minimum p−value is small enough, where the threshold guarantees that the significance level of the test is α for any dependency among the p−values p1, . . .",4.1 The Partial Conjunction p−value,[0],[0]
", pN .",4.1 The Partial Conjunction p−value,[0],[0]
"Fisher’s method rejects the global null for large values of −2∑Ni=1 ln p(i), or equivalently for small values of∏N i=1",4.1 The Partial Conjunction p−value,[0],[0]
pi.,4.1 The Partial Conjunction p−value,[0],[0]
"That is, while both these methods are intuitive, they are different.",4.1 The Partial Conjunction p−value,[0],[0]
Fisher’s method requires a small enough product of p−values as evidence that at least one null hypothesis is false.,4.1 The Partial Conjunction p−value,[0],[0]
"Bonferroni’s method, on the other hand, requires as evidence at least one small enough p−value.
",4.1 The Partial Conjunction p−value,[0],[0]
"For the case u = N , i.e., when the alternative states that all null hypotheses are false, both methods require that the maximal p−value is small enough for rejection of HN/N0 .",4.1 The Partial Conjunction p−value,[0],[0]
This is also intuitive because we expect that all the p−values will be small when all the null hypotheses are false.,4.1 The Partial Conjunction p−value,[0],[0]
"For other cases, where 1 < u < N , the reasoning is more complicated and is beyond the scope of this paper.
",4.1 The Partial Conjunction p−value,[0],[0]
The partial conjunction test for a specific u answers the question “Does algorithm A perform better than B on at least u datasets?”,4.1 The Partial Conjunction p−value,[0],[0]
"The next step is
the estimation of the number of datasets for which algorithm A performs better than B.",4.1 The Partial Conjunction p−value,[0],[0]
Recall that the number of datasets where algorithm A outperforms algorithm B (denoted with k in Definition 1) is the true number of false null hypotheses in our problem.,4.2 Dataset Counting (Question 1),[0],[0]
"Benjamini and Heller (2008) proposed to estimate k to be the largest u for which H u/N 0 , along with H 1/N 0 , . . .",4.2 Dataset Counting (Question 1),[0],[0]
",H (u−1)/N 0 is rejected.",4.2 Dataset Counting (Question 1),[0],[0]
"Specifically, the estimator k̂ is defined as follows:
k̂ = max{u : pu/N∗ ≤",4.2 Dataset Counting (Question 1),[0],[0]
"α}, (4)
where pu/N∗ = max{p(u−1)/N∗ , pu/N}, p1/N = p1/N∗",4.2 Dataset Counting (Question 1),[0],[0]
and α is the desired upper bound on the probability to overestimate the true k.,4.2 Dataset Counting (Question 1),[0],[0]
"It is guaranteed that P(k̂ > k) ≤ α as long as the p−value combination method used for constructing pu/N is valid for the given dependency across the test statistics.5 When k̂ is based on pu/NBonferroni it is denoted with k̂Bonferroni; when it is based on p u/N Fisher, it is denoted with k̂Fisher.",4.2 Dataset Counting (Question 1),[0],[0]
"A crucial practical consideration, when choosing between k̂Bonferroni and k̂Fisher, is the assumed dependency between the datasets.",4.2 Dataset Counting (Question 1),[0],[0]
"As discussed in Section 4.1, pu/NFisher is recommended when the participating datasets are assumed to be independent; when this assumption cannot be made, only pu/NBonferroni is appropriate.",4.2 Dataset Counting (Question 1),[0],[0]
"As the k̂ estimators are based on the respective pu/N s, the same considerations hold when choosing between them.
",4.2 Dataset Counting (Question 1),[0],[0]
"With the k̂ estimators, one can answer the counting question of Section 1, reporting that algorithm",4.2 Dataset Counting (Question 1),[0],[0]
A is better than algorithm B in at least k̂ out of N datasets with a confidence level of 1 − α.,4.2 Dataset Counting (Question 1),[0],[0]
"Regarding the identification question, a natural approach would be to declare the k̂ datasets with the smallest p−values as those for which the effect holds.",4.2 Dataset Counting (Question 1),[0],[0]
"However, with k̂Fisher this approach does not guarantee control over type I errors.",4.2 Dataset Counting (Question 1),[0],[0]
"In contrast, for k̂Bonferroni, the above approach comes with such guarantees, as described in the next section.
",4.2 Dataset Counting (Question 1),[0],[0]
5This result is a special case of Theorem 4 in Benjamini and Heller (2008).,4.2 Dataset Counting (Question 1),[0],[0]
"As demonstrated in Section 3.2, identifying the datasets with p−value below the nominal significance level and declaring them as those where algorithm A is better than B may lead to a very high number of erroneous claims.",4.3 Dataset Identification (Question 2),[0],[0]
A variety of methods exist for addressing this problem.,4.3 Dataset Identification (Question 2),[0],[0]
"A classical and very simple method for addressing this problem is named the Bonferroni’s procedure, which compensates for the increased probability of making at least one type I error by testing each individual hypothesis at a significance level of α′ = α/N , where α is the predefined bound on this probability and N is the number of hypotheses tested.6 While Bonferroni’s procedure is valid for any dependency among the p−values, the probability of detecting a true effect using this procedure is often very low, because of its strict p−value threshold.
",4.3 Dataset Identification (Question 2),[0],[0]
"Many other procedures controlling the above or other error criteria, and having less strict p−value thresholds, have been proposed.",4.3 Dataset Identification (Question 2),[0],[0]
"Below we advocate one of these methods: the Holm procedure (Holm, 1979).",4.3 Dataset Identification (Question 2),[0],[0]
This is a simple p−value based procedure that is concordant with the partial conjunction analysis when pu/NBonferroni is used in that analysis.,4.3 Dataset Identification (Question 2),[0],[0]
"Importantly for NLP applications, Holm controls the probability of making at least one type I error for any type of dependency between the participating datasets (see a demonstration in Section 6).
",4.3 Dataset Identification (Question 2),[0],[0]
"Let α be the desired upper bound on the probability that at least one false rejection occurs, let p(1) ≤",4.3 Dataset Identification (Question 2),[0],[0]
p(2) ≤ . . .,4.3 Dataset Identification (Question 2),[0],[0]
≤,4.3 Dataset Identification (Question 2),[0],[0]
p(N) be the ordered p−values and let the associated hypotheses be H(1) . . .,4.3 Dataset Identification (Question 2),[0],[0]
H(N).,4.3 Dataset Identification (Question 2),[0],[0]
"The Holm procedure for identifying the datasets with a significant effect is given below.
",4.3 Dataset Identification (Question 2),[0],[0]
Procedure Holm 1),4.3 Dataset Identification (Question 2),[0],[0]
"Let k be the minimal index such that
p(k)",4.3 Dataset Identification (Question 2),[0],[0]
> α N+1−k . 2) Reject the null hypotheses H(1) . . .,4.3 Dataset Identification (Question 2),[0],[0]
H(k−1),4.3 Dataset Identification (Question 2),[0],[0]
"and
do not reject H(k) . . .",4.3 Dataset Identification (Question 2),[0],[0]
H(N).,4.3 Dataset Identification (Question 2),[0],[0]
"If no such k exists, then reject all null hypotheses.
",4.3 Dataset Identification (Question 2),[0],[0]
The output of the Holm procedure is a rejection 6Bonferroni’s correction is based on similar considerations as pu/NBonferroni for u = 1 (Eq. 2).,4.3 Dataset Identification (Question 2),[0],[0]
"The partial conjunction framework (Sec. 4.1) extends this idea for other values of u.
list of null hypotheses; the corresponding datasets are those we return in response to the identification question of Section 1.",4.3 Dataset Identification (Question 2),[0],[0]
Note that the Holm procedure rejects a subset of hypotheses with p-value below α.,4.3 Dataset Identification (Question 2),[0],[0]
Each p-value is compared to a threshold which is smaller or equal to α and depends on the number of evaluation datasets N.,4.3 Dataset Identification (Question 2),[0],[0]
"The dependence of the thresholds on N can be intuitively explained as follows: the probability of making one or more erroneous claims may increase with N, as demonstrated in Section 3.2.",4.3 Dataset Identification (Question 2),[0],[0]
"Therefore, in order to bound this probability by a pre-specified level α, the thresholds for p-values should depend on N.
It can be shown that the Holm procedure at level α always rejects the k̂Bonferroni hypotheses with the smallest p−values, where k̂Bonferroni is the lower bound for k with a confidence level of 1 − α.",4.3 Dataset Identification (Question 2),[0],[0]
"Therefore, k̂Bonferroni corresponding to a confidence level of 1 − α is always smaller or equal to the number of datasets for which the difference between the compared algorithms is significant at level α.",4.3 Dataset Identification (Question 2),[0],[0]
"This is not surprising in view of the fact that, without making any assumptions on the dependencies among the datasets, k̂Bonferroni guarantees that the probability of making a too optimistic claim (k̂ > k) is bounded by α, when simply counting the number of datasets with p-value below α, the probability of making a too optimistic claim may be close to 1, as demonstrated in Section 5.
",4.3 Dataset Identification (Question 2),[0],[0]
Framework Summary Following Section 4.2 we answer the counting question of Section 1 by reporting either k̂Fisher (when all datasets can be assumed to be independent) or k̂Bonferroni (when such an independence assumption cannot be made).,4.3 Dataset Identification (Question 2),[0],[0]
"Based on Section 4.3 we suggest to answer the identification question of Section 1 by reporting the rejection list returned by the Holm procedure.
",4.3 Dataset Identification (Question 2),[0],[0]
Our proposed framework is based on certain assumptions regarding the experiments conducted in NLP setups.,4.3 Dataset Identification (Question 2),[0],[0]
The most prominent of these assumptions states that for dependent datasets the type of dependency cannot be determined.,4.3 Dataset Identification (Question 2),[0],[0]
"Indeed, to the best of our knowledge, the nature of the dependency between dependent test sets in NLP work has not been analyzed before.",4.3 Dataset Identification (Question 2),[0],[0]
In Section 7 we revisit our assumptions and point to alternative methods for answering our questions.,4.3 Dataset Identification (Question 2),[0],[0]
"These methods may be ap-
propriate under other assumptions that may become relevant in future.
",4.3 Dataset Identification (Question 2),[0],[0]
We next demonstrate the value of the proposed replicability analysis through toy examples with synthetic data (Section 5) as well as analysis of state-of-the-art algorithms for four major NLP applications (Section 6).,4.3 Dataset Identification (Question 2),[0],[0]
"Our point of reference is the standard, yet statistically unjustified, counting method that sets its estimator, k̂count, to the number of datasets for which the difference between the compared algorithms is significant with p−value ≤ α (i.e. k̂count = #{i : pi ≤ α}).7",4.3 Dataset Identification (Question 2),[0],[0]
"For the examples of this section we synthesize p−values to emulate a test with N = 100 hypotheses (domains), and set α to 0.05.",5 Toy Examples,[0],[0]
"We start with a simulation of a scenario where algorithmA is equivalent to B for each domain, and the datasets representing these domains are independent.",5 Toy Examples,[0],[0]
"We sample the 100 p−values from a standard uniform distribution, which is the p−value distribution under the null hypothesis, repeating the simulation 1,000 times.
",5 Toy Examples,[0],[0]
"Since all the null hypotheses are true then k, the number of false null hypotheses, is 0.",5 Toy Examples,[0],[0]
"Figure 1 presents the histogram of k̂ values from all 1,000 iterations according to k̂Bonferroni, k̂Fisher and k̂count.
",5 Toy Examples,[0],[0]
The figure clearly demonstrates that k̂count provides an overestimation of k while k̂Bonferroni and k̂Fisher do much better.,5 Toy Examples,[0],[0]
"Indeed, the histogram yields the following probability estimates: P̂ (k̂count >
7We use α in two different contexts: the significance level of an individual test and the bound on the probability to overestimate k.",5 Toy Examples,[0],[0]
"This is the standard notation in the statistical literature.
",5 Toy Examples,[0],[0]
"k) = 0.963, P̂ (k̂Bonferroni > k) = 0.001 and P̂ (k̂Fisher > k) = 0.021 (only the latter two are lower than 0.05).",5 Toy Examples,[0],[0]
"This simulation strongly supports the theoretical results of Section 4.2.
",5 Toy Examples,[0],[0]
"To consider a scenario where a dependency between the participating datasets does exist, we consider a second toy example.",5 Toy Examples,[0],[0]
"In this example we generate N = 100 p−values corresponding to 34 independent normal test statistics, and two other groups of 33 positively correlated normal test statistics with ρ = 0.2 and ρ = 0.5, respectively.",5 Toy Examples,[0],[0]
"We again assume that all null hypotheses are true and thus all the p−values are distributed uniformly, repeating the simulation 1,000 times.",5 Toy Examples,[0],[0]
"To generate positively dependent p−values, we followed the process described in Section 6.1 of Benjamini et al. (2006).
",5 Toy Examples,[0],[0]
We estimate the probability that k̂ > k,5 Toy Examples,[0],[0]
= 0,5 Toy Examples,[0],[0]
"for the three k̂ estimators based on the 1000 repetitions and get the values of: P̂ (k̂count > k) = 0.943, P̂ (k̂Bonferroni > k) = 0.046 and P̂ (k̂Fisher > k) = 0.234.",5 Toy Examples,[0],[0]
"This simulation demonstrates the importance of using Bonferroni’s method rather than Fisher’s method when the datasets are dependent, even if some of the datasets are independent.",5 Toy Examples,[0],[0]
In this section we demonstrate the potential impact of replicability analysis on the way experimental results are analyzed in NLP setups.,6 NLP Applications,[0],[0]
We explore four NLP applications: (a) two where the datasets are independent: multi-domain dependency parsing and multilingual POS tagging; and (b) two where dependency between the datasets does exist: cross-domain sentiment classification and word similarity prediction with word embedding models.,6 NLP Applications,[0],[0]
"Dependency Parsing We consider a multidomain setup, analyzing the results reported in Choi et al. (2015).",6.1 Data,[0],[0]
"The authors compared ten state-of-the-art parsers from which we pick three: (a) Mate (Bohnet, 2010)8 that performed best on the majority of datasets; (b) Redshift (Honnibal et al., 2013)9 which demonstrated comparable, still somewhat lower, performance compared to Mate;
8code.google.com/p/mate-tools.",6.1 Data,[0],[0]
"9github.com/syllog1sm/Redshift.
",6.1 Data,[0],[0]
"and (c) SpaCy (Honnibal and Johnson, 2015) that was substantially outperformed by Mate.
",6.1 Data,[0],[0]
"All parsers were trained and tested on the English portion of the OntoNotes 5 corpus (Weischedel et al., 2011; Pradhan et al., 2013), a large multigenre corpus consisting of the following 7 genres: broadcasting conversations (BC), broadcasting news (BN), news magazine (MZ), newswire (NW), pivot text (PT), telephone conversations (TC) and web text (WB).",6.1 Data,[0],[0]
"Train and test set size (in sentences) range from 6672 to 34,492 and from 280 to 2327, respectively (see Table 1 of Choi et al. (2015)).",6.1 Data,[0],[0]
"We copy the test set UAS results of Choi et al. (2015) and compute p−values using the data downloaded from http://amandastent.com/dependable/.
POS Tagging We consider a multilingual setup, analyzing the results reported in (Pinter et al., 2017).",6.1 Data,[0],[0]
"The authors compare their MIMICK model with the model of Ling et al. (2015), denoted with CHAR→TAG.",6.1 Data,[0],[0]
"Evaluation is performed on 23 of the 44 languages shared by the Polyglot word embedding dataset (Al-Rfou et al., 2013) and the universal dependencies (UD) dataset (De Marneffe et al., 2014).",6.1 Data,[0],[0]
"Pinter et al. (2017) choose their languages so that they reflect a variety of typological, and particularly morphological, properties.",6.1 Data,[0],[0]
The training/test split is the standard UD split.,6.1 Data,[0],[0]
"We copy the word level accuracy figures of Pinter et al. (2017) for the low resource training set setup, the focus setup of that paper.",6.1 Data,[0],[0]
"The authors kindly sent us their p-values.
",6.1 Data,[0],[0]
"Sentiment Classification In this task, an algorithm is trained on reviews from one domain and should classify the sentiment of reviews from another domain to the positive and negative classes.",6.1 Data,[0],[0]
For replicability analysis we explore the results of Ziser and Reichart (2017) for the cross-domain sentiment classification task of Blitzer et al. (2007).,6.1 Data,[0],[0]
"The data in this task consists of Amazon product reviews from 4 domains: books (B), DVDs (D), electronic items (E), and kitchen appliances (K), for the total of 12 domain pairs, each domain having a 2000 review test set.10 Ziser and Reichart (2017) compared the accuracy of their AE-SCL-SR model to MSDA (Chen et al., 2011), a well known domain adaptation
10http://www.cs.jhu.edu/˜mdredze/ datasets/sentiment
method, and kindly sent us the required p-values.
",6.1 Data,[0],[0]
Word Similarity We compare two state-of-the-art word embedding collections: (a) word2vec,6.1 Data,[0],[0]
"CBOW (Mikolov et al., 2013) vectors, generated by the model titled the best “predict” model in Baroni et al. (2014);11 and (b) GloVe (Pennington et al., 2014) vectors generated by a model trained on a 42B token common web crawl.12 We employed the demo of Faruqui and Dyer (2014) to perform a Spearman correlation evaluation of these vector collections on 12 English word pair datasets: WS-353 (Finkelstein et al., 2001b), WS-353-SIM (Agirre et al., 2009), WS-353-REL (Agirre et al., 2009), MC-30 (Miller and Charles, 1991), RG-65 (Rubenstein and Goodenough, 1965), Rare-Word (Luong et al., 2013), MEN (Bruni et al., 2012), MTurk-287 (Radinsky et al., 2011), MTurk-771",6.1 Data,[0],[0]
"(Halawi et al., 2012), YP-130 (Yang and Powers, ), SimLex-999 (Hill et al., 2016), and Verb-143 (Baker et al., 2014).",6.1 Data,[0],[0]
"We first calculate the p−values for each task and dataset according to the principals of p−values computation for NLP as discussed in Yeh (2000), BergKirkpatrick et al. (2012) and Søgaard et al. (2014).
",6.2 Statistical Significance Tests,[0],[0]
"For dependency parsing, we employ the aparametric paired bootstrap test (Efron and Tibshirani, 1994) that does not assume any distribution on the test statistics.",6.2 Statistical Significance Tests,[0],[0]
We chose this test because the distribution of the values for the measures commonly applied in this task is unknown.,6.2 Statistical Significance Tests,[0],[0]
"We implemented the test as in (Berg-Kirkpatrick et al., 2012) with a bootstrap size of 500 and with 105 repetitions.
",6.2 Statistical Significance Tests,[0],[0]
"For multilingual POS tagging, we employ the Wilcoxon signed-rank test (Wilcoxon, 1945) on the differences of the sentence level accuracy scores of the two compared models.",6.2 Statistical Significance Tests,[0],[0]
"This test is a nonparametric test for differences in measure, testing the null hypothesis that the difference has a symmetric distribution around zero.",6.2 Statistical Significance Tests,[0],[0]
"It is appropriate for tasks with paired continuous measures for each observation, which is the case when comparing sentence level accuracies.
",6.2 Statistical Significance Tests,[0],[0]
11http://clic.cimec.unitn.it/composes/ semantic-vectors.html.,6.2 Statistical Significance Tests,[0],[0]
"Parameters: 5-word context window, 10 negative samples, subsampling, 400 dimensions.
",6.2 Statistical Significance Tests,[0],[0]
"12http://nlp.stanford.edu/projects/glove/. 300 dimensions.
",6.2 Statistical Significance Tests,[0],[0]
"For sentiment classification we employ the McNemar test for paired nominal data (McNemar, 1947).",6.2 Statistical Significance Tests,[0],[0]
"This test is appropriate for binary classification tasks and since we compare the results of the algorithms when applied on the same datasets, we employ its paired version.",6.2 Statistical Significance Tests,[0],[0]
"Finally, for word similarity with its Spearman correlation evaluation, we choose the Steiger test (Steiger, 1980) for comparing elements in a correlation matrix.
",6.2 Statistical Significance Tests,[0],[0]
We consider the case of α = 0.05 for all four applications.,6.2 Statistical Significance Tests,[0],[0]
"For the dependent datasets experiments (sentiment classification and word similarity prediction) with their generally lower p−values (see below), we also consider the case where α = 0.01.",6.2 Statistical Significance Tests,[0],[0]
"Table 1 summarizes the replicability analysis results while Table 2 – 5 present task specific performance measures and p−values.
",6.3 Results,[0],[0]
"Independent Datasets Dependency parsing (Table 2) and multilingual POS tagging (Table 3) are our example tasks for this setup, where k̂Fisher is our recommended valid estimator for the number of cases where one algorithm outperforms another.
",6.3 Results,[0],[0]
"For dependency parsing, we compare two scenarios: (a) where in most domains the differences between the compared algorithms are quite large and the p−values are small (Mate vs. SpaCy); and (b)
where in most domains the differences between the compared algorithms are smaller and the p−values are higher (Mate vs. Redshift).",6.3 Results,[0],[0]
"Our multilingual POS tagging scenario (MIMICK vs. Char→Tag) is more similar to scenario (b) in terms of the differences between the participating algorithms.
",6.3 Results,[0],[0]
Table 1 demonstrates the k̂ estimators for the various tasks and scenarios.,6.3 Results,[0],[0]
"For dependency parsing, as expected, in scenario (a) where all the p−values are small, all estimators, even the error-prone k̂count, provide the same information.",6.3 Results,[0],[0]
"In case (b) of dependency parsing, however, k̂Fisher estimates the number of domains where Mate outperforms Redshift to be 5, while k̂count estimates this number to be 2.",6.3 Results,[0],[0]
This is a substantial difference given that the number of domains is 7.,6.3 Results,[0],[0]
"The k̂Bonferroni estimator, that is valid under arbitrary dependencies, is even more conservative than k̂count and its estimation is only 1.
",6.3 Results,[0],[0]
"Perhaps not surprisingly, the multilingual POS
tagging results are similar to case (b) of dependency parsing.",6.3 Results,[0],[0]
"Here, again, k̂count is too conservative, estimating the number of languages with effect to be 11 (out of 23) while k̂Fisher estimates this number to be 16 (an increase of 5/23 in the estimated number of languages with effect).",6.3 Results,[0],[0]
"k̂Bonferroni is again more conservative, estimating the number of languages with effect to be only 6, which is not very surprising given that it does not exploit the independence between the datasets.",6.3 Results,[0],[0]
"These two examples of case (b) demonstrate that when the differences between the algorithms are quite small, k̂Fisher may be more sensitive than the current practice in NLP for discovering the number of datasets with effect.
",6.3 Results,[0],[0]
"To complete the analysis, we would like to name the datasets with effect.",6.3 Results,[0],[0]
"As discussed in Section 4.2, while this can be straightforwardly done by naming the datasets with the k̂ smallest p−values, in general, this approach does not control the probability of identifying at least one dataset erroneously.",6.3 Results,[0],[0]
"We thus employ the Holm procedure for the identification task, noticing that the number of datasets it identifies should be equal to the value of the k̂Bonferroni estimator (Section 4.3).
",6.3 Results,[0],[0]
"Indeed, for dependency parsing in case (a), the Holm procedure identifies all seven domains as cases where Mate outperforms SpaCy, while in case (b) it identifies only the MZ domain as a case where Mate outperforms Redshift.",6.3 Results,[0],[0]
"For multilingual POS
tagging the Holm procedure identifies Tamil, Hungarian, Basque, Indonesian, Chinese and Czech as languages where MIMICK outperforms Char→Tag.",6.3 Results,[0],[0]
"This analysis demonstrates that when the performance gap between two algorithms becomes narrower, inquiring for more information (i.e. identifying the domains with effect rather than just estimating their number), may result in weaker results.13
Dependent Datasets In cross-domain sentiment classification (Table 4) and word similarity prediction (Table 5), the involved datasets manifest mutual dependence.",6.3 Results,[0],[0]
"Particularly, each sentiment setup shares its test dataset with 2 other setups, while in word similarity WS-353 is the union of WS-353REL and WS-353-SIM.",6.3 Results,[0],[0]
"As discussed in Section 4, k̂Bonferroni is the appropriate estimator of the number of cases one algorithm outperforms another.
",6.3 Results,[0],[0]
"The results in Table 1 manifest the phenomenon demonstrated by the second toy example in Section 5, which shows that when the datasets are dependent, k̂Fisher as well as the error-prone k̂count may be too optimistic regarding the number of datasets with effect.",6.3 Results,[0],[0]
"This stands in contrast to k̂Bonferroni which controls the probability to overestimate the number of such datasets.
",6.3 Results,[0],[0]
"Indeed, k̂Bonferroni is much more conservative, yielding values of 6 (α = 0.05) and 2 (α = 0.01) for sentiment, and of 6 (α = 0.05) and 4 (α = 0.01) for word similarity.",6.3 Results,[0],[0]
The differences from the conclusions that might have been drawn by k̂count are again quite substantial.,6.3 Results,[0],[0]
"The difference between k̂Bonferroni and k̂count in sentiment classification is 4, which accounts to 1/3 of the 12 test setups.",6.3 Results,[0],[0]
"Even for word similarity, the difference between the two methods, which account to 2 for both α values, represents 1/6 of the 12 test setups.",6.3 Results,[0],[0]
"The domains identified by the Holm procedure are marked in the tables.
",6.3 Results,[0],[0]
"Results Overview Our goal in this section is to demonstrate that the approach of simply looking at the number of datasets for which the difference between the performance of the algorithms reaches a predefined significance level, gives different results
13For completeness, we also performed the analysis for the independent dataset setups with α = 0.01.",6.3 Results,[0],[0]
"The results are (k̂count, k̂Bonferroni, k̂Fisher): Mate vs. SpaCy: (7,7,7); Mate vs. Redshift (1,0,2); MIMICK vs. Char→Tag: (7,5,13).",6.3 Results,[0],[0]
"The patterns are very similar to those discussed in the text.
from our suggested statistically sound analysis.",6.3 Results,[0],[0]
This approach is denoted here with k̂count and shown to be statistically not valid in Sections 3.2 and 5.,6.3 Results,[0],[0]
We observe that this happens especially in evaluation setups where the differences between the algorithms are small for most datasets.,6.3 Results,[0],[0]
"In some cases, when the datasets are independent, our analysis has the power to declare a larger number of datasets with effect than the number of individual significant test values (k̂count).",6.3 Results,[0],[0]
"In other cases, when the datasets are interdependent, k̂count is much too optimistic.
",6.3 Results,[0],[0]
Our proposed analysis changes the observations that might have been made based on the papers where the results analyzed here were originally reported.,6.3 Results,[0],[0]
"For example, for the Mate-Redshift comparison (independent evaluation sets), we show that there is evidence that the number of datasets with effect is much higher than one would assume based on counting the significant sets (5 vs. 2 out of 7 evaluation sets), giving a stronger claim regarding the superiority of Mate.",6.3 Results,[0],[0]
"In multilingual POS tagging (again, a setup of independent evaluation sets) our analysis shows evidence for 16 sets with effect compared to only 11 of the erroneous count method - a difference in 5 out of 23 evaluation sets (21.7%).",6.3 Results,[0],[0]
"Finally, in the cross-domain sentiment classification and the word similarity judgment tasks (dependent evaluation sets), the unjustified counting method may be too optimistic (e.g. 10 vs. 6 out of 12 evaluation sets, for α = 0.05 in the sentiment task), in favor of the new algorithms.",6.3 Results,[0],[0]
We proposed a statistically sound replicability analysis framework for cases where algorithms are compared across multiple datasets.,7 Discussion and Future Directions,[0],[0]
"Our main contributions are: (a) analyzing the limitations of the current practice in NLP work; and (b) proposing a framework that addresses both the estimation of the number of datasets with effect and their identification.
",7 Discussion and Future Directions,[0],[0]
The framework we propose addresses two different situations encountered in NLP: independent and dependent datasets.,7 Discussion and Future Directions,[0],[0]
"For dependent datasets, we assumed that the type of dependency cannot be determined.",7 Discussion and Future Directions,[0],[0]
One could use more powerful methods if certain assumptions on the dependency between the test statistics could be made.,7 Discussion and Future Directions,[0],[0]
"For example, one could use
the partial conjunction p-value based on Simes test for the global null hypothesis (Simes, 1986), which was proposed by Benjamini and Heller (2008) for the case where the test statistics satisfy certain positive dependency properties (see Theorem 1 in (Benjamini and Heller, 2008)).",7 Discussion and Future Directions,[0],[0]
"Using this partial conjunction p-value rather than the one based on Bonferroni, one may obtain higher values of k̂ with the same statistical guarantee.",7 Discussion and Future Directions,[0],[0]
"Similarly, for the identification question, if certain positive dependency properties hold, Holm’s procedure could be replaced by Hochberg’s or Hommel’s procedures (Hochberg, 1988; Hommel, 1988) which are more powerful.
",7 Discussion and Future Directions,[0],[0]
"An alternative, more powerful multiple testing procedure for identification of datasets with effect, is the method in Benjamini and Hochberg (1995), that controls the false discovery rate (FDR), a less strict error criterion than the one considered here.",7 Discussion and Future Directions,[0],[0]
"This method is more appropriate in cases where one may tolerate some errors as long as the proportion of errors among all the claims made is small, as expected to happen when the number of datasets grows.
",7 Discussion and Future Directions,[0],[0]
We note that the increase in the number of evaluation datasets may have positive and negative aspects.,7 Discussion and Future Directions,[0],[0]
"As noted in Section 2, we believe that multiple comparisons are integral to NLP research when aiming to develop algorithms that perform well across languages and domains.",7 Discussion and Future Directions,[0],[0]
"On the other hand, experimenting with multiple evaluation sets that reflect very similar linguistic phenomena may only complicate the comparison between alternative algorithms.
",7 Discussion and Future Directions,[0],[0]
"In fact, our analysis is useful mostly where the datasets are heterogeneous, coming from different languages or domains.",7 Discussion and Future Directions,[0],[0]
"When they are just technically different but could potentially be just combined into a one big dataset, then we believe the question of Demšar (2006), whether at least one dataset shows evidence for effect, is more appropriate.",7 Discussion and Future Directions,[0],[0]
The research of M. Bogomolov was supported by the Israel Science Foundation grant No. 1112/14.,Acknowledgement,[0],[0]
We thank Yuval Pinter for his great help with the multilingual experiments and for his useful feedback.,Acknowledgement,[0],[0]
"We also thank Ruth Heller, Marten van Schijndel, Oren Tsur, Or Zuk and the ie@technion NLP group members for their useful comments.",Acknowledgement,[0],[0]
"With the ever growing amount of textual data from a large variety of languages, domains, and genres, it has become standard to evaluate NLP algorithms on multiple datasets in order to ensure a consistent performance across heterogeneous setups.",abstractText,[0],[0]
"However, such multiple comparisons pose significant challenges to traditional statistical analysis methods in NLP and can lead to erroneous conclusions.",abstractText,[0],[0]
In this paper we propose a Replicability Analysis framework for a statistically sound analysis of multiple comparisons between algorithms for NLP tasks.,abstractText,[0],[0]
"We discuss the theoretical advantages of this framework over the current, statistically unjustified, practice in the NLP literature, and demonstrate its empirical value across four applications: multi-domain dependency parsing, multilingual POS tagging, cross-domain sentiment classification and word similarity prediction.",abstractText,[0],[0]
1,abstractText,[0],[0]
Replicability Analysis for Natural Language Processing: Testing Significance with Multiple Datasets,title,[0],[0]
Graphs are a ubiquitous structure that widely occurs in data analysis problems.,1. Introduction,[0],[0]
"Real-world graphs such as social networks, financial networks, biological networks and citation networks represent important rich information which is not seen from the individual entities alone, for example, the communities a person is in, the functional role of a molecule, and the sensitivity of the assets of an enterprise to external shocks.",1. Introduction,[0],[0]
"Therefore, representation learning of nodes in graphs aims to extract high-level features from a node as well as its neighborhood, and has proved extremely useful for many applications, such as node classification, clustering, and link prediction (Perozzi et al., 2014; Monti et al.,
1Massachusetts Institute of Technology (MIT) 2National Institute of Informatics, Tokyo.",1. Introduction,[0],[0]
Correspondence to: Keyulu Xu,1. Introduction,[0],[0]
"<keyulu@mit.edu>, Stefanie Jegelka <stefje@mit.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
2017; Grover & Leskovec, 2016; Tang et al., 2015).
",1. Introduction,[0],[0]
Recent works focus on deep learning approaches to node representation.,1. Introduction,[0],[0]
"Many of these approaches broadly follow a neighborhood aggregation (or “message passing” scheme), and those have been very promising (Kipf & Welling, 2017; Hamilton et al., 2017; Gilmer et al., 2017; Veličković",1. Introduction,[0],[0]
"et al., 2018; Kearnes et al., 2016).",1. Introduction,[0],[0]
"These models learn to iteratively aggregate the hidden features of every node in the graph with its adjacent nodes’ as its new hidden features, where an iteration is parametrized by a layer of the neural network.",1. Introduction,[0],[0]
"Theoretically, an aggregation process of k iterations makes use of the subtree structures of height k rooted at every node.",1. Introduction,[0],[0]
"Such schemes have been shown to generalize the Weisfeiler-Lehman graph isomorphism test (Weisfeiler & Lehman, 1968) enabling to simultaneously learn the topology as well as the distribution of node features in the neighborhood (Shervashidze et al., 2011; Kipf & Welling, 2017; Hamilton et al., 2017).
",1. Introduction,[0],[0]
"Yet, such aggregation schemes sometimes lead to surprises.",1. Introduction,[0],[0]
"For example, it has been observed that the best performance with one of the state-of-the-art models, Graph Convolutional Networks (GCN), is achieved with a 2-layer model.",1. Introduction,[0],[0]
"Deeper versions of the model that, in principle, have access to more information, perform worse (Kipf & Welling, 2017).",1. Introduction,[0],[0]
"A similar degradation of learning for computer vision problems is resolved by residual connections (He et al., 2016a) that greatly aid the training of deep models.",1. Introduction,[0],[0]
"But, even with residual connections, GCNs with more layers do not perform as well as the 2-layer GCN on many datasets, e.g. citation networks.
",1. Introduction,[0],[0]
"Motivated by observations like the above, in this paper, we address two questions.",1. Introduction,[0],[0]
"First, we study properties and resulting limitations of neighborhood aggregation schemes.",1. Introduction,[0],[0]
"Second, based on this analysis, we propose an architecture that, as opposed to existing models, enables adaptive, structure-aware representations.",1. Introduction,[0],[0]
"Such representations are particularly interesting for representation learning on large complex graphs with diverse subgraph structures.
",1. Introduction,[0],[0]
Model analysis.,1. Introduction,[0],[0]
"To better understand the behavior of different neighborhood aggregation schemes, we analyze the effective range of nodes that any given node’s representation draws from.",1. Introduction,[0],[0]
"We summarize this sensitivity analysis by what
we name the influence distribution of a node.",1. Introduction,[0],[0]
This effective range implicitly encodes prior assumptions on what are the “nearest neighbors” that a node should draw information from.,1. Introduction,[0],[0]
"In particular, we will see that this influence is heavily affected by the graph structure, raising the question whether “one size fits all”, in particular in graphs whose subgraphs have varying properties (such as more tree-like or more expander-like).
",1. Introduction,[0],[0]
"In particular, our more formal analysis connects influence distributions with the spread of a random walk at a given node, a well-understood phenomenon as a function of the graph structure and eigenvalues (Lovász, 1993).",1. Introduction,[0],[0]
"For instance, in some cases and applications, a 2-step random walk influence that focuses on local neighborhoods can be more informative than higher-order features where some of the information may be “washed out” via averaging.
",1. Introduction,[0],[0]
Changing locality.,1. Introduction,[0],[0]
"To illustrate the effect and importance of graph structure, recall that many real-world graphs possess locally strongly varying structure.",1. Introduction,[0],[0]
"In biological and citation networks, the majority of the nodes have few connections, whereas some nodes (hubs) are connected to many other nodes.",1. Introduction,[0],[0]
"Social and web networks usually consist of an expander-like core part and an almost-tree (bounded treewidth) part, which represent well-connected entities and the small communities respectively (Leskovec et al., 2009; Maehara et al., 2014; Tsonis et al., 2006).
",1. Introduction,[0],[0]
"Besides node features, this subgraph structure has great impact on the result of neighborhood aggregation.",1. Introduction,[0],[0]
"The speed of expansion or, equivalently, growth of the influence radius, is characterized by the random walk’s mixing time, which changes dramatically on subgraphs with different structures (Lovász, 1993).",1. Introduction,[0],[0]
"Thus, the same number of iterations (layers) can lead to influence distributions of very different locality.",1. Introduction,[0],[0]
"As an example, consider the social network in Figure 1 from GooglePlus (Leskovec & Mcauley, 2012).",1. Introduction,[0],[0]
The figure illustrates the expansions of a random walk starting at the square node.,1. Introduction,[0],[0]
The walk (a) from a node within the core rapidly includes almost the entire graph.,1. Introduction,[0],[0]
"In contrast, the walk (b) starting at a node in the tree part includes only a very small fraction of all nodes.",1. Introduction,[0],[0]
"After 5 steps, the same walk has reached the core and, suddenly, spreads quickly.",1. Introduction,[0],[0]
"Translated to graph representation models, these spreads become the influence distributions or, in other words, the averaged features yield the new feature of the walk’s starting node.",1. Introduction,[0],[0]
"This shows that in the same graph, the same number of steps can lead to very different effects.",1. Introduction,[0],[0]
"Depending on the application, wide-range or smallrange feature combinations may be more desirable.",1. Introduction,[0],[0]
"A too rapid expansion may average too broadly and thereby lose information, while in other parts of the graph, a sufficient neighborhood may be needed for stabilizing predictions.
",1. Introduction,[0],[0]
JK networks.,1. Introduction,[0],[0]
"The above observations raise the question
whether it is possible to adaptively adjust (i.e., learn) the influence radii for each node and task.",1. Introduction,[0],[0]
"To achieve this, we explore an architecture that learns to selectively exploit information from neighborhoods of differing locality.",1. Introduction,[0],[0]
"This architecture selectively combines different aggregations at the last layer, i.e., the representations “jump” to the last layer.",1. Introduction,[0],[0]
"Hence, we name the resulting networks Jumping Knowledge Networks (JK-Nets).",1. Introduction,[0],[0]
"We will see that empirically, when adaptation is an option, the networks indeed learn representations of different orders for different graph substructures.",1. Introduction,[0],[0]
"Moreover, in Section 6, we show that applying our framework to various state-of-the-art neighborhood-aggregation models consistently improves their performance.",1. Introduction,[0],[0]
"We begin by summarizing some of the most common neighborhood aggregation schemes and, along the way, introduce our notation.",2. Background and Neighborhood aggregation schemes,[0],[0]
"Let G = (V,E) be a simple graph with node features Xv ∈",2. Background and Neighborhood aggregation schemes,[0],[0]
Rdi for v ∈ V .,2. Background and Neighborhood aggregation schemes,[0],[0]
Let G̃ be the graph obtained by adding a self-loop to every v ∈ V .,2. Background and Neighborhood aggregation schemes,[0],[0]
The hidden feature of node v learned by the l-th layer of the model is denoted by h (l) v ∈,2. Background and Neighborhood aggregation schemes,[0],[0]
Rdh .,2. Background and Neighborhood aggregation schemes,[0],[0]
"Here, di is the dimension of the input features and dh is the dimension of the hidden features, which, for simplicity of exposition, we assume to be the same across layers.",2. Background and Neighborhood aggregation schemes,[0],[0]
We also use h(0)v = Xv for the node feature.,2. Background and Neighborhood aggregation schemes,[0],[0]
"The neighborhood N(v) = {u ∈ V | (v, u) ∈ E} of node v is the set of adjacent nodes of v. The analogous neighborhood Ñ(v) = {v} ∪ {u ∈ V | (v, u) ∈ E} on G̃ includes v.
A typical neighborhood aggregation scheme can generically be written as follows: for a k-layer model, the l-th layer (l = 1..k) updates h(l)v for every v ∈ V simultaneously as
h(l)v = σ",2. Background and Neighborhood aggregation schemes,[0],[0]
"( Wl · AGGREGATE ({ h(l−1)u ,∀u ∈ Ñ(v) }))",2. Background and Neighborhood aggregation schemes,[0],[0]
"(1)
where AGGREGATE is an aggregation function defined by the specific model, Wl is a trainable weight matrix on the lth layer shared by all nodes, and σ is a non-linear activation function, e.g. a ReLU.
",2. Background and Neighborhood aggregation schemes,[0],[0]
Graph Convolutional Networks (GCN).,2. Background and Neighborhood aggregation schemes,[0],[0]
"Graph Convolutional Networks (GCN) (Kipf & Welling, 2017), initially motivated by spectral graph convolutions (Hammond et al., 2011; Defferrard et al., 2016), are a specific instantiation of this framework (Gilmer et al., 2017), of the form
h(l)v = ReLU ( Wl · ∑ u∈Ñ(v) (deg(v)deg(u))−1/2 h(l−1)u ) (2)
where deg(v) is the degree of node v in G. Hamilton et al. (2017) derived a variant of GCN that also works in inductive settings (previously unseen nodes), by using a different normalization to average:
h(l)v",2. Background and Neighborhood aggregation schemes,[0],[0]
"= ReLU ( Wl · 1
d̃eg(v) ∑ u∈Ñ(v) h(l−1)u ) (3)
where d̃eg(v) is the degree of node v in G̃.
Neighborhood Aggregation with Skip Connections.",2. Background and Neighborhood aggregation schemes,[0],[0]
Instead of aggregating a node and its neighbors at the same time as in Eqn.,2. Background and Neighborhood aggregation schemes,[0],[0]
"(1), a number of recent approaches aggregate the neighbors first and then combine the resulting neighborhood representation with the node’s representation from the last iteration.",2. Background and Neighborhood aggregation schemes,[0],[0]
"More formally, each node is updated as
h (l) N(v) = σ",2. Background and Neighborhood aggregation schemes,[0],[0]
"( Wl · AGGREGATEN ( {h(l−1)u ,∀u ∈ N(v)} )) h(l)v =",2. Background and Neighborhood aggregation schemes,[0],[0]
"COMBINE ( h(l−1)v , h (l) N(v)
) where AGGREGATEN and COMBINE are defined by the specific model.",2. Background and Neighborhood aggregation schemes,[0],[0]
The COMBINE step is key to this paradigm and can be viewed as a form of a ”skip connection” between different layers.,2. Background and Neighborhood aggregation schemes,[0],[0]
"For COMBINE, GraphSAGE (Hamilton et al., 2017) uses concatenation after a feature transform.",2. Background and Neighborhood aggregation schemes,[0],[0]
"Column Networks (Pham et al., 2017) interpolate the neighborhood representation and the node’s previous representation, and Gated GNN (Li et al., 2016) uses the Gated Recurrent Unit (GRU) (Cho et al., 2014).",2. Background and Neighborhood aggregation schemes,[0],[0]
"Another wellknown variant of skip connections, residual connections, use the identity mapping to help signals propagate (He et al., 2016a;b).
",2. Background and Neighborhood aggregation schemes,[0],[0]
"These skip connections are input- but not output-unit specific: If we ”skip” a layer for h(l)v (do not aggregate) or use a certain COMBINE, all subsequent units using this representation will be using this skip implicitly.",2. Background and Neighborhood aggregation schemes,[0],[0]
It is impossible that a certain higher-up representation h(l+j)u uses the skip and another one does not.,2. Background and Neighborhood aggregation schemes,[0],[0]
"As a result, skip connections cannot adaptively adjust the neighborhood sizes of the final-layer representations independently.
",2. Background and Neighborhood aggregation schemes,[0],[0]
Neighborhood Aggregation with Directional Biases.,2. Background and Neighborhood aggregation schemes,[0],[0]
"Some recent models, rather than treating the features of
adjacent nodes equally, weigh “important” neighbors more.",2. Background and Neighborhood aggregation schemes,[0],[0]
"This paradigm can be viewed as neighborhood-aggregation with directional biases because a node will be influenced by some directions of expansion more than the others.
",2. Background and Neighborhood aggregation schemes,[0],[0]
"Graph Attention Networks (GAT) (Veličković et al., 2018) and VAIN (Hoshen, 2017) learn to select the important neighbors via an attention mechanism.",2. Background and Neighborhood aggregation schemes,[0],[0]
"The max-pooling operation in GraphSAGE (Hamilton et al., 2017) implicitly selects the important nodes.",2. Background and Neighborhood aggregation schemes,[0],[0]
"This line of work is orthogonal to ours, because it modifies the direction of expansion whereas our model operates on the locality of expansion.",2. Background and Neighborhood aggregation schemes,[0],[0]
Our model can be combined with these models to add representational power.,2. Background and Neighborhood aggregation schemes,[0],[0]
"In Section 6, we demonstrate that our framework works with not only simple neighborhood-aggregation models (GCN), but also with skip connections (GraphSAGE) and directional biases (GAT).",2. Background and Neighborhood aggregation schemes,[0],[0]
"Next, we explore some important properties of the above aggregation schemes.",3. Influence Distribution and Random Walks,[0],[0]
"Related to ideas of sensitivity analysis and influence functions in statistics (Koh & Liang, 2017) that measure the influence of a training point on parameters, we study the range of nodes whose features affect a given node’s representation.",3. Influence Distribution and Random Walks,[0],[0]
"This range gives insight into how large a neighborhood a node is drawing information from.
",3. Influence Distribution and Random Walks,[0],[0]
"We measure the sensitivity of node x to node y, or the influence of y on x, by measuring how much a change in the input feature of y affects the representation of x in the last layer.",3. Influence Distribution and Random Walks,[0],[0]
"For any node x, the influence distribution captures the relative influences of all other nodes.
",3. Influence Distribution and Random Walks,[0],[0]
Definition 3.1 (Influence score and distribution).,3. Influence Distribution and Random Walks,[0],[0]
"For a simple graph G = (V,E), let h(0)x be the input feature and h
(k) x be the learned hidden feature of node x ∈ V at the k-th (last) layer of the model.",3. Influence Distribution and Random Walks,[0],[0]
"The influence score I(x, y) of node x by any node y ∈ V is the sum of the absolute values
of the entries of the Jacobian matrix [ ∂h(k)x ∂h (0) y ] .",3. Influence Distribution and Random Walks,[0],[0]
"We define the influence distribution Ix of x ∈ V by normalizing the influence scores: Ix(y) = I(x, y)/ ∑ z I(x, z), or
Ix(y) = e T
[ ∂h (k) x
∂h (0) y
] e /(∑
z∈V eT
[ ∂h (k) x
∂h (0) z
] e )
where e is the all-ones vector.
",3. Influence Distribution and Random Walks,[0],[0]
"Later, we will see connections of influence distributions with random walks.",3. Influence Distribution and Random Walks,[0],[0]
"For completeness, we also define random walk distributions.
",3. Influence Distribution and Random Walks,[0],[0]
Definition 3.2.,3. Influence Distribution and Random Walks,[0],[0]
"Consider a random walk on G̃ starting at a node v0; if at the t-th step we are at a node vt, we move to any neighbor of vt (including vt) with equal probability.
",3. Influence Distribution and Random Walks,[0],[0]
"The t-step random walk distribution Pt of v0 is
Pt (i) =",3. Influence Distribution and Random Walks,[0],[0]
Prob (vt = i) .,3. Influence Distribution and Random Walks,[0],[0]
"(4)
Analogous definitions apply for random walks with nonuniform transition probabilities.
",3. Influence Distribution and Random Walks,[0],[0]
An important property of the random walk distribution is that it becomes more spread out as t increases and converges to the limit distribution if the graph is non-bipartite.,3. Influence Distribution and Random Walks,[0],[0]
"The rate of convergence depends on the structure of the subgraph and can be bounded by the spectral gap (or the conductance) of the random walk’s transition matrix (Lovász, 1993).",3. Influence Distribution and Random Walks,[0],[0]
The influence distribution for different aggregation models and nodes can give insights into the information captured by the respective representations.,3.1. Model Analysis,[0],[0]
The following results show that the influence distributions of common aggregation schemes are closely connected to random walk distributions.,3.1. Model Analysis,[0],[0]
"This observation hints at specific implications – strengths and weaknesses – that we will discuss.
",3.1. Model Analysis,[0],[0]
"With a randomization assumption of the ReLU activations similar to that in (Kawaguchi, 2016; Choromanska et al., 2015), we can draw connections between GCNs and random walks:
Theorem 1.",3.1. Model Analysis,[0],[0]
"Given a k-layer GCN with averaging as in Equation (3), assume that all paths in the computation graph of the model are activated with the same probability of success ρ.",3.1. Model Analysis,[0],[0]
"Then the influence distribution Ix for any node
x ∈ V is equivalent, in expectation, to the k-step random walk distribution on G̃ starting at node x.
We prove Theorem 1 in the appendix.
",3.1. Model Analysis,[0],[0]
It is straightforward to modify the proof of Theorem 1 to show a nearly equivalent result for the version of GCN in Equation (2).,3.1. Model Analysis,[0],[0]
"The only difference is that each random walk path v0p, v 1 p, ..., v k p from node x (v 0 p) to y (v k p), in-
stead of probability ρ ∏k l=1 1
d̃eg(vlp) , now has probability
ρ Q ∏k−1 l=1 1
d̃eg(vlp) · (d̃eg(x)d̃eg(y))−1/2, where Q is a nor-
malizing factor.",3.1. Model Analysis,[0],[0]
"Thus, the difference in probability is small, especially when the degree of x and y are close.
",3.1. Model Analysis,[0],[0]
"Similarly, we can show that neighborhood aggregation schemes with directional biases resemble biased random walk distributions.",3.1. Model Analysis,[0],[0]
"This follows by substituting the corresponding probabilities into the proof of Theorem 1.
",3.1. Model Analysis,[0],[0]
"Empirically, we observe that, despite somewhat simplifying assumptions, our theory is close to what happens in practice.",3.1. Model Analysis,[0],[0]
"We visualize the heat maps of the influence distributions for a node (labeled square) for trained GCNs, and compare with the random walk distributions starting at the same node.",3.1. Model Analysis,[0],[0]
Figure 2 shows example results.,3.1. Model Analysis,[0],[0]
Darker colors correspond to higher influence probabilities.,3.1. Model Analysis,[0],[0]
"To show the effect of skip connections, Figure 3 visualizes the analogous heat maps for one example—GCN with residual connections.",3.1. Model Analysis,[0],[0]
"Indeed, we observe that the influence distributions of networks with residual connections approximately correspond to lazy random walks: each step has a higher probability of staying at
the current node.",3.1. Model Analysis,[0],[0]
Local information is retained with similar probabilities for all nodes in each iteration; this cannot adapt to diverse needs of specific upper-layer nodes.,3.1. Model Analysis,[0],[0]
"Further visualizations may be found in the appendix.
",3.1. Model Analysis,[0],[0]
Fast Collapse on Expanders.,3.1. Model Analysis,[0],[0]
"To better understand the implication of Theorem 1 and the limitations of the corresponding neighborhood aggregation algorithms, we revisit the scenario of learning on a social network shown in Figure 1.",3.1. Model Analysis,[0],[0]
"Random walks starting inside an expander converge rapidly in O(log |V |) steps to an almost-uniform distribution (Hoory et al., 2006).",3.1. Model Analysis,[0],[0]
"After O(log |V |) iterations of neighborhood aggregation, by Theorem 1 the representation of every node is influenced almost equally by any other node in the expander.",3.1. Model Analysis,[0],[0]
"Thus, the node representations will be representative of the global graph and carry limited information about individual nodes.",3.1. Model Analysis,[0],[0]
"In contrast, random walks starting at the bounded tree-width (almost-tree) part converge slowly, i.e., the features retain more local information.",3.1. Model Analysis,[0],[0]
"Models that impose a fixed random walk distribution inherit these discrepancies in the speed of expansion and influence neighborhoods, which may not lead to the best representations for all nodes.",3.1. Model Analysis,[0],[0]
"The above observations raise the question whether the fixed but structure-dependent influence radius size induced by
common aggregation schemes really achieves the best representations for all nodes and tasks.",4. Jumping Knowledge Networks,[0],[0]
"Large radii may lead to too much averaging, while small radii may lead to instabilities or insufficient information aggregation.",4. Jumping Knowledge Networks,[0],[0]
"Hence, we propose two simple yet powerful architectural changes – jump connections and a subsequent selective but adaptive aggregation mechanism.
",4. Jumping Knowledge Networks,[0],[0]
"Figure 4 illustrates the main idea: as in common neighborhood aggregation networks, each layer increases the size of the influence distribution by aggregating neighborhoods from the previous layer.",4. Jumping Knowledge Networks,[0],[0]
"At the last layer, for each node, we carefully select from all of those itermediate representations (which “jump” to the last layer), potentially combining a few.",4. Jumping Knowledge Networks,[0],[0]
"If this is done independently for each node, then the model can adapt the effective neighborhood size for each node as needed, resulting in exactly the desired adaptivity.
",4. Jumping Knowledge Networks,[0],[0]
Our model permits general layer-aggregation mechanisms.,4. Jumping Knowledge Networks,[0],[0]
We explore three approaches; others are possible too.,4. Jumping Knowledge Networks,[0],[0]
"Let h (1) v , ..., h (k) v be the jumping representations of node v (from k layers) that are to be aggregated.
",4. Jumping Knowledge Networks,[0],[0]
Concatenation.,4. Jumping Knowledge Networks,[0],[0]
"A concatenation [ h (1) v , ..., h (k) v ] is the
most straightforward way to combine the layers, after which we may perform a linear transformation.",4. Jumping Knowledge Networks,[0],[0]
"If the transformation weights are shared across graph nodes, this approach is not node-adaptive.",4. Jumping Knowledge Networks,[0],[0]
"Instead, it optimizes the weights to combine the subgraph features in a way that works best for the dataset overall.",4. Jumping Knowledge Networks,[0],[0]
"One may expect concatenation to be suitable for small graphs and graphs with regular structure that require less adaptivity; also because weight-sharing helps reduce overfitting.
",4. Jumping Knowledge Networks,[0],[0]
Max-pooling.,4. Jumping Knowledge Networks,[0],[0]
"An element-wise max ( h (1) v , ..., h (k) v ) selects the most informative layer for each feature coordinate.",4. Jumping Knowledge Networks,[0],[0]
"For example, feature coordinates that represent more local properties can use the feature coordinates learned from the close neighbors and those representing global status would favor features from the higher-up layers.",4. Jumping Knowledge Networks,[0],[0]
"Max-pooling is adaptive and has the advantage that it does not introduce any additional parameters to learn.
",4. Jumping Knowledge Networks,[0],[0]
LSTM-attention.,4. Jumping Knowledge Networks,[0],[0]
"An attention mechanism identifies the most useful neighborhood ranges for each node v by computing an attention score s(l)v for each layer l (∑ l s (l) v = 1 ) , which represents the importance of the feature learned on the l-th layer for node v. The aggregated representation for node v is a weighted average of the layer features∑ l s (l) v · h(l)v .",4. Jumping Knowledge Networks,[0],[0]
"For LSTM attention, we input h(1)v , ..., h(k)v into a bi-directional LSTM (Hochreiter & Schmidhuber, 1997) and generate the forward-LSTM and backward-LSTM",4. Jumping Knowledge Networks,[0],[0]
hidden features f (l)v and b (l) v for each layer l.,4. Jumping Knowledge Networks,[0],[0]
A linear mapping of the concatenated features [f (l)v ||b(l)v ] yields the scalar importance score s(l)v .,4. Jumping Knowledge Networks,[0],[0]
"A Softmax layer applied to {s(l)v }kl=1
yields the attention of node v on its neighborhood in different ranges.",4. Jumping Knowledge Networks,[0],[0]
Finally we take the sum of [f (l)v ||b(l)v ] weighted by SoftMax({s(l)v }kl=1) to get the final layer representation.,4. Jumping Knowledge Networks,[0],[0]
Another possible implementation combines LSTM with max-pooling.,4. Jumping Knowledge Networks,[0],[0]
LSTM-attention is node adaptive because the attention scores are different for each node.,4. Jumping Knowledge Networks,[0],[0]
"We shall see that the this approach shines on large complex graphs, although it may overfit on small graphs (fewer training nodes) due to its relatively higher complexity.",4. Jumping Knowledge Networks,[0],[0]
"The key idea for the design of layer-aggregation functions is to determine the importance of a node’s subgraph features at different ranges after looking at the learned features on all layers, rather than to optimize and fix the same weights for all nodes.",4.1. JK-Net Learns to Adapt,[0],[0]
"Under the same assumption on the ReLU activation distribution as in Theorem 1, we show below that layer-wise max-pooling implicitly learns the influence locality adaptively for different nodes.",4.1. JK-Net Learns to Adapt,[0],[0]
"The proof for layerwise attention follows similarly.
",4.1. JK-Net Learns to Adapt,[0],[0]
Proposition 1.,4.1. JK-Net Learns to Adapt,[0],[0]
Assume that paths of the same length in the computation graph are activated with the same probability.,4.1. JK-Net Learns to Adapt,[0],[0]
"The influence score I(x, y) for any x, y ∈ V under a k-layer JK-Net with layer-wise max-pooling is equivalent in expectation to a mixture of 0, .., k-step random walk distributions on G̃ at y starting at x, the coefficients of which depend on the values of the layer features h(l)x .
",4.1. JK-Net Learns to Adapt,[0],[0]
We prove Proposition 1 in the appendix.,4.1. JK-Net Learns to Adapt,[0],[0]
"Contrasting this result with the influence distributions of other aggregation mechanisms, we see that JK-networks indeed differ in their node-wise adaptivity of neighborhood ranges.
",4.1. JK-Net Learns to Adapt,[0],[0]
Figure 5 illustrates how a 6-layer JK-Net with max-pooling aggregation learns to adapt to different subgraph structures on a citation network.,4.1. JK-Net Learns to Adapt,[0],[0]
"Within a tree-like structure, the influence stays in the “small community” the node belongs to.",4.1. JK-Net Learns to Adapt,[0],[0]
"In contrast, 6-layer models whose influence distributions follow random walks, e.g. GCNs, would reach out too far into irrelevant parts of the graph, and models with few layers may not be able to cover the entire “community”, as illustrated in Figure 1, and Figures 7, 8 in the appendix.",4.1. JK-Net Learns to Adapt,[0],[0]
"For
a node affiliated to a “hub”, which presumably plays the role of connecting different types of nodes, JK-Net learns to put most influence on the node itself and otherwise spreads out the influence.",4.1. JK-Net Learns to Adapt,[0],[0]
"GCNs, however, would not capture the importance of the node’s own features in such a structure because the probability at an affiliate node is small after a few random walk steps.",4.1. JK-Net Learns to Adapt,[0],[0]
"For hubs, JK-Net spreads out the influence across the neighboring nodes in a reasonable range, which makes sense because the nodes connected to the hubs are presumably as informative as the hubs’ own features.",4.1. JK-Net Learns to Adapt,[0],[0]
"For comparison, Table 6 in the appendix includes more visualizations of how models with random walk priors behave.",4.1. JK-Net Learns to Adapt,[0],[0]
"Looking at Figure 4, one may wonder whether the same inter-layer connections could be drawn between all layers.",4.2. Intermediate Layer Aggregation and Structures,[0],[0]
"The resulting architecture is approximately a graph correspondent of DenseNets, which were introduced for computer vision problems (Huang et al., 2017), if the layer-wise concatenation aggregation is applied.",4.2. Intermediate Layer Aggregation and Structures,[0],[0]
"This version, however, would require many more features to learn.",4.2. Intermediate Layer Aggregation and Structures,[0],[0]
"Viewing the DenseNet setting (images) from a graph-theoretic perspective, images correspond to regular, in fact, near-planar graphs.",4.2. Intermediate Layer Aggregation and Structures,[0],[0]
"Such graphs are far from being expanders, and do not pose the challenges of graphs with varying subgraph structures.",4.2. Intermediate Layer Aggregation and Structures,[0],[0]
"Indeed, as we shall see, models with concatenation aggregation perform well on graphs with more regular structures such as images and well-structured communities.",4.2. Intermediate Layer Aggregation and Structures,[0],[0]
"As a more general framework, JK-Net admits general layerwise aggregation models and enables better structure-aware representations on graphs with complex structures.",4.2. Intermediate Layer Aggregation and Structures,[0],[0]
"Spectral graph convolutional neural networks apply convolution on graphs by using the graph Laplacian eigenvectors as the Fourier atoms (Bruna et al., 2014; Shuman et al., 2013; Defferrard et al., 2016).",5. Other Related Work,[0],[0]
"A major drawback of the spectral methods, compared to spatial approaches like neighborhoodaggregation, is that the graph Laplacian needs to be known in advance.",5. Other Related Work,[0],[0]
"Hence, they cannot generalize to unseen graphs.",5. Other Related Work,[0],[0]
We evaluate JK-Nets on four benchmark datasets.,6. Experiments,[0],[0]
(I),6. Experiments,[0],[0]
"The task on citation networks (Citeseer, Cora) (Sen et al., 2008) is to classify academic papers into different subjects.",6. Experiments,[0],[0]
The dataset contains bag-of-words features for each document (node) and citation links (edges) between documents.,6. Experiments,[0],[0]
"(II) On Reddit (Hamilton et al., 2017), the task is to predict the community to which different Reddit posts belong.",6. Experiments,[0],[0]
Reddit is an online discussion forum where users comment in different topical communities.,6. Experiments,[0],[0]
Two posts (nodes) are connected if some user commented on both posts.,6. Experiments,[0],[0]
The dataset contains word vectors as node features.,6. Experiments,[0],[0]
"(III) For protein-protein interaction networks (PPI) (Hamilton et al., 2017), the task is to classify protein functions.",6. Experiments,[0],[0]
"PPI consists of 24 graphs, each corresponds to a human tissue.",6. Experiments,[0],[0]
"Each node has positional gene sets, motif gene sets and immunological signatures as features and gene ontology sets as labels.",6. Experiments,[0],[0]
"20 graphs are used for training, 2 graphs are used for validation and the rest for testing.",6. Experiments,[0],[0]
"Statistics of the datasets are summarized in Table 1.
",6. Experiments,[0],[0]
Settings.,6. Experiments,[0],[0]
"In the transductive setting, we are only allowed to access a subset of nodes in one graph as training data, and validate/test on others.",6. Experiments,[0],[0]
"Our experiments on Citeseer, Cora and Reddit are transductive.",6. Experiments,[0],[0]
"In the inductive setting, we use a number of full graphs as training data and use other completely unseen graphs as validation/testing data.",6. Experiments,[0],[0]
"Our experiments on PPI are inductive.
",6. Experiments,[0],[0]
"We compare against three baselines: Graph Convolutional Networks (GCN) (Kipf & Welling, 2017), GraphSAGE (Hamilton et al., 2017) and Graph Attention Networks (GAT) (Veličković et al., 2018).",6. Experiments,[0],[0]
"For experiments on Citeseer and Cora, we choose GCN as the base model since on our data split, it is outperforming GAT.",6.1. Citeseer & Cora,[0],[0]
"We construct JK-Nets by choosing MaxPooling (JKMaxPool), Concatenation (JK-Concat), or LSTM-attention (JK-LSTM) as final aggregation layer.",6.1. Citeseer & Cora,[0],[0]
"When taking the final aggregation, besides normal graph convolutional layers, we also take the first linear-transformed representation into account.",6.1. Citeseer & Cora,[0],[0]
The final prediction is done via a fully connected layer on top of the final aggregated representation.,6.1. Citeseer & Cora,[0],[0]
"We split nodes in each graph into 60%, 20% and 20% for training, validation and testing.",6.1. Citeseer & Cora,[0],[0]
"We vary the number of layers from 1
to 6 for each model and choose the best performing model with respect to the validation set.",6.1. Citeseer & Cora,[0],[0]
"Throughout the experiments, we use the Adam optimizer (Kingma & Ba, 2014) with learning rate 0.005.",6.1. Citeseer & Cora,[0],[0]
"We fix the dropout rate to be 0.5, the dimension of hidden features to be within {16, 32}, and add an L2 regularization of 0.0005 on model parameters.",6.1. Citeseer & Cora,[0],[0]
"The results are shown in Table 2.
Results.",6.1. Citeseer & Cora,[0],[0]
We observe in Table 2 that JK-Nets outperform both GCN and GAT baselines in terms of prediction accuracy.,6.1. Citeseer & Cora,[0],[0]
"Though JK-Nets perform well in general, there is no consistent winner and performance varies slightly across datasets.
",6.1. Citeseer & Cora,[0],[0]
"Taking a closer look at results on Cora, both GCN and GAT achieve their best accuracies with only 2 or 3 layers, suggesting that local information is a stronger signal for classification than global ones.",6.1. Citeseer & Cora,[0],[0]
"However, the fact that JKNets achieve the best performance with 6 layers indicates that global together with local information will help boost performance.",6.1. Citeseer & Cora,[0],[0]
This is where models like JK-Nets can be particularly beneficial.,6.1. Citeseer & Cora,[0],[0]
LSTM-attention may not be suitable for such small graphs because of its relatively high complexity.,6.1. Citeseer & Cora,[0],[0]
The Reddit data is too large to be handled well by current implementations of GCN or GAT.,6.2. Reddit,[0],[0]
"Hence, we use the more scalable GraphSAGE as the base model for JK-Net.",6.2. Reddit,[0],[0]
It has skip connections and different modes of node aggregation.,6.2. Reddit,[0],[0]
"We experiment with Mean and MaxPool node aggregators, which take mean and max-pooling of a linear transformation of representations of the sampled neighbors.",6.2. Reddit,[0],[0]
"Combining each of GraphSAGE modes with MaxPooling, Concatenation or LSTM-attention as the last aggregation layer gives 6 JK-Net variants.",6.2. Reddit,[0],[0]
"We follow exactly the same setting of GraphSAGE as in the original paper (Hamilton et al., 2017), where the model consists of 2 hidden layers, each with 128 hidden units and is trained with Adam with learning rate of 0.01 and no weight decay.",6.2. Reddit,[0],[0]
"Results are shown in Table 3.
Results.",6.2. Reddit,[0],[0]
"With MaxPool as node aggregator and Concat as layer aggregator, JK-Net achieves the best Micro-F1 score
among GarphSAGE and JK-Net variants.",6.2. Reddit,[0],[0]
Note that the original GraphSAGE already performs fairly well with a Micro-F1 of 0.95.,6.2. Reddit,[0],[0]
JK-Net reduces the error by 30%.,6.2. Reddit,[0],[0]
"The communities in the Reddit dataset were explicitly chosen from the well-behaved middle-sized communities to avoid the noisy cores and tree-like small communities (Hamilton et al., 2017).",6.2. Reddit,[0],[0]
"As a result, this graph is more regular than the original Reddit data, and hence not exhibit the problems of varying subgraph structures.",6.2. Reddit,[0],[0]
"In such a case, the added flexibility of the node-specific neighborhood choices may not be as relevant, and the stabilizing properties of concatenation instead come into play.",6.2. Reddit,[0],[0]
"We demonstrate the power of adaptive JK-Nets, e.g., JKLSTM, with experiments on the PPI data, where the subgraphs have more diverse and complex structures than those in the Reddit community detection dataset.",6.3. PPI,[0],[0]
We use both GraphSAGE and GAT as base models for JK-Net.,6.3. PPI,[0],[0]
"The implementation of GraphSAGE and GAT are quite different: GraphSAGE is sample-based, where neighbors of a node are sampled to be a fixed number, while GAT considers all neighbors.",6.3. PPI,[0],[0]
Such differences cause large gaps in terms of both scalability and performances.,6.3. PPI,[0],[0]
"Given that GraphSAGE scales to much larger graphs, it appears particularly valuable to evaluate how much JK-Net can improve upon GraphSAGE.
",6.3. PPI,[0],[0]
"For GraphSAGE we follow the setup as in the Reddit experiments, except that we use 3 layers when possible, and compare the performance after 10 and 30 epochs of training.",6.3. PPI,[0],[0]
The results are shown in Table 4.,6.3. PPI,[0],[0]
"For GAT and its JK-Net variants we stack two hidden layers with 4 attention heads computing 256 features (for a total of 1024 features), and a final prediction layer with 6 attention heads computing 121 features each.",6.3. PPI,[0],[0]
They are further averaged and input into sigmoid activations.,6.3. PPI,[0],[0]
We employ skip connections across intermediate attentional layers.,6.3. PPI,[0],[0]
These models are trained with Batch-size 2 and Adam optimizer with learning rate of 0.005.,6.3. PPI,[0],[0]
"The results are shown in Table 5.
Results.",6.3. PPI,[0],[0]
"JK-Nets with the LSTM-attention aggregators outperform the non-adaptive models GraphSAGE, GAT and JK-Nets with concatenation aggregators.",6.3. PPI,[0],[0]
"In particular, JKLSTM outperforms GraphSAGE by 0.128 in terms of micro-
F1 score after 30 epochs of training.",6.3. PPI,[0],[0]
Structure-aware node adaptive models are especially beneficial on such complex graphs with diverse structures.,6.3. PPI,[0],[0]
"Motivated by observations that reveal great differences in neighborhood information ranges for graph node embeddings, we propose a new aggregation scheme for node representation learning that can adapt neigborhood ranges to nodes individually.",7. Conclusion,[0],[0]
"This JK-network can improve representations in particular for graphs that have subgraphs of diverse local structure, and may hence not be well captured by fixed numbers of neighborhood aggregations.",7. Conclusion,[0],[0]
Interesting directions for future work include exploring other layer aggregators and studying the effect of the combination of various layer-wise and node-wise aggregators on different types of graph structures.,7. Conclusion,[0],[0]
"This research was supported by NSF CAREER award 1553284, and JST ERATO Kawarabayashi Large Graph Project, Grant Number JPMJER1201, Japan.",Acknowledgements,[0],[0]
Recent deep learning approaches for representation learning on graphs follow a neighborhood aggregation procedure.,abstractText,[0],[0]
"We analyze some important properties of these models, and propose a strategy to overcome those.",abstractText,[0],[0]
"In particular, the range of “neighboring” nodes that a node’s representation draws from strongly depends on the graph structure, analogous to the spread of a random walk.",abstractText,[0],[0]
"To adapt to local neighborhood properties and tasks, we explore an architecture – jumping knowledge (JK) networks – that flexibly leverages, for each node, different neighborhood ranges to enable better structure-aware representation.",abstractText,[0],[0]
"In a number of experiments on social, bioinformatics and citation networks, we demonstrate that our model achieves state-of-the-art performance.",abstractText,[0],[0]
"Furthermore, combining the JK framework with models like Graph Convolutional Networks, GraphSAGE and Graph Attention Networks consistently improves those models’ performance.",abstractText,[0],[0]
Representation Learning on Graphs with Jumping Knowledge Networks ,title,[0],[0]
"Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 912–921, Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics
Methods of deep neural networks (DNNs) have recently demonstrated superior performance on a number of natural language processing tasks. However, in most previous work, the models are learned based on either unsupervised objectives, which does not directly optimize the desired task, or singletask supervised objectives, which often suffer from insufficient training data. We develop a multi-task DNN for learning representations across multiple tasks, not only leveraging large amounts of cross-task data, but also benefiting from a regularization effect that leads to more general representations to help tasks in new domains. Our multi-task DNN approach combines tasks of multiple-domain classification (for query classification) and information retrieval (ranking for web search), and demonstrates significant gains over strong baselines in a comprehensive set of domain adaptation.",text,[0],[0]
"Recent advances in deep neural networks (DNNs) have demonstrated the importance of learning vector-space representations of text, e.g., words and sentences, for a number of natural language processing tasks.",1 Introduction,[0],[0]
"For example, the study reported in (Collobert et al., 2011) demonstrated significant accuracy gains in tagging, named entity recognition, and semantic role labeling when using vector space word
∗This research was conducted during the author’s internship at Microsoft Research.
representations learned from large corpora.",1 Introduction,[0],[0]
"Further, since these representations are usually in a lowdimensional vector space, they result in more compact models than those built from surface-form features.",1 Introduction,[0],[0]
"A recent successful example is the parser by (Chen and Manning, 2014), which is not only accurate but also fast.
",1 Introduction,[0],[0]
"However, existing vector-space representation learning methods are far from optimal.",1 Introduction,[0],[0]
"Most previous methods are based on unsupervised objectives such as word prediction for training (Mikolov et al., 2013c; Pennington et al., 2014).",1 Introduction,[0],[0]
"Other methods use supervised training objectives on a single task, e.g. (Socher et al., 2013), and thus are often constrained by limited amounts of training data.",1 Introduction,[0],[0]
"Motivated by the success of multi-task learning (Caruana, 1997), we propose in this paper a multi-task DNN approach for representation learning that leverages supervised data from many tasks.",1 Introduction,[0],[0]
"In addition to the benefit of having more data for training, the use of multi-task also profits from a regularization effect, i.e., reducing overfitting to a specific task, thus making the learned representations universal across tasks.
",1 Introduction,[0],[0]
"Our contributions are of two-folds: First, we propose a multi-task deep neural network for representation learning, in particular focusing on semantic classification (query classification) and semantic information retrieval (ranking for web search) tasks.",1 Introduction,[0],[0]
Our model learns to map arbitrary text queries and documents into semantic vector representations in a low dimensional latent space.,1 Introduction,[0],[0]
"While the general concept of multi-task neural nets is not new, our model is novel in that it successfully combines tasks as disparate as operations necessary for classifica-
912
tion or ranking.",1 Introduction,[0],[0]
"Second, we demonstrate strong results on query classification and web search.",1 Introduction,[0],[0]
Our multi-task representation learning consistently outperforms stateof-the-art baselines.,1 Introduction,[0],[0]
"Meanwhile, we show that our model is not only compact but it also enables agile deployment into new domains.",1 Introduction,[0],[0]
This is because the learned representations allow domain adaptation with substantially fewer in-domain labels.,1 Introduction,[0],[0]
Our multi-task model combines classification and ranking tasks.,2.1 Preliminaries,[0],[0]
"For concreteness, throughout this paper we will use query classification as the classification task and web search as the ranking task.",2.1 Preliminaries,[0],[0]
"These are important tasks in commercial search engines:
Query Classification:",2.1 Preliminaries,[0],[0]
"Given a search query Q, the model classifies in the binary fashion as to whether it belongs to one of the domains of interest.",2.1 Preliminaries,[0],[0]
"For example, if the query Q is “Denver sushi”, the classifier should decide that it belongs to the “Restaurant” domain.",2.1 Preliminaries,[0],[0]
"Accurate query classification enables a richer personalized user experience, since the search engine can tailor the interface and results.",2.1 Preliminaries,[0],[0]
"It is however challenging because queries tend to be short (Shen et al., 2006).",2.1 Preliminaries,[0],[0]
"Surface-form word features that are common in traditional document classification problems tend to be too sparse for query classification, so representation learning is a promising solution.",2.1 Preliminaries,[0],[0]
"In this study, we classify queries into four domains of interest: (“Restaurant”, “Hotel”, “Flight”, “Nightlife”).",2.1 Preliminaries,[0],[0]
Note that one query can belong to multiple domains.,2.1 Preliminaries,[0],[0]
"Therefore, a set of binary classifiers are built, one for each domain, to perform the classification.",2.1 Preliminaries,[0],[0]
We frame the problem as four binary classification tasks.,2.1 Preliminaries,[0],[0]
"Thus, for domain Ct, our goal is binary classification based on P (Ct| Q) (Ct = {0, 1} ).",2.1 Preliminaries,[0],[0]
"For each domain t, we assume supervised data (Q, yt = {0, 1} with yt as binary labels.1
Web Search:",2.1 Preliminaries,[0],[0]
"Given a search queryQ and a document list L, the model ranks documents in the order
1One could frame the problem as a a single multi-class classification task, but our formulation is more practical as it allows adding new domains without retraining existing classifiers.",2.1 Preliminaries,[0],[0]
"This will be relevant in domain adaptation (§3.3).
of relevance.",2.1 Preliminaries,[0],[0]
"For example, if the queryQ is ”Denver sushi”, model returns a list of documents that satisfies such information need.",2.1 Preliminaries,[0],[0]
"Formally, we estimate P (D1|Q), P (D2|Q), . . .",2.1 Preliminaries,[0],[0]
for each document Dn and rank according to these probabilities.,2.1 Preliminaries,[0],[0]
"We assume that supervised data exist; I.e., there is at least one relevant document Dn for each query Q.",2.1 Preliminaries,[0],[0]
"Briefly, our proposed model maps any arbitrary queries Q or documents D into fixed lowdimensional vector representations using DNNs.",2.2 The Proposed Multi-Task DNN Model,[0],[0]
These vectors can then be used to perform query classification or web search.,2.2 The Proposed Multi-Task DNN Model,[0],[0]
"In contrast to existing representation learning methods which employ either unsupervised or single-task supervised objectives, our model learns these representations using multi-task objectives.
",2.2 The Proposed Multi-Task DNN Model,[0],[0]
The architecture of our multi-task DNN model is shown in Figure 1.,2.2 The Proposed Multi-Task DNN Model,[0],[0]
"The lower layers are shared across different tasks, whereas the top layers represent task-specific outputs.",2.2 The Proposed Multi-Task DNN Model,[0],[0]
"Importantly, the input X (either a query or document), initially represented as a bag of words, is mapped to a vector (l2) of dimension 300.",2.2 The Proposed Multi-Task DNN Model,[0],[0]
This is the shared semantic representation that is trained by our multi-task objectives.,2.2 The Proposed Multi-Task DNN Model,[0],[0]
"In the following, we elaborate the model in detail:
Word Hash Layer (l1): Traditionally, each word is represented by a one-hot word vector, where the dimensionality of the vector is the vocabulary size.",2.2 The Proposed Multi-Task DNN Model,[0],[0]
"However, due to the large size of vocabulary in realworld tasks, it is very expensive to learn such kind of models.",2.2 The Proposed Multi-Task DNN Model,[0],[0]
"To alleviate this problem, we adopt the word hashing method (Huang et al., 2013).",2.2 The Proposed Multi-Task DNN Model,[0],[0]
"We map a one-hot word vector, with an extremely high dimensionality, into a limited letter-trigram space (e.g., with the dimensionality as low as 50k).",2.2 The Proposed Multi-Task DNN Model,[0],[0]
"For example, word cat is hashed as the bag of letter trigram {#-c-a, c-a-t, a-t-#}, where # is a boundary symbol.",2.2 The Proposed Multi-Task DNN Model,[0],[0]
"Word hashing complements the one-hot vector representation in two aspects: 1) out of vocabulary words can be represented by letter-trigram vectors; 2) spelling variations of the same word can be mapped to the points that are close to each other in the letter-trigram space.
",2.2 The Proposed Multi-Task DNN Model,[0],[0]
Semantic-Representation Layer (l2):,2.2 The Proposed Multi-Task DNN Model,[0],[0]
This is a shared representation learned across different tasks.,2.2 The Proposed Multi-Task DNN Model,[0],[0]
"this layer maps the letter-trigram inputs into a 300-
1
dimensional vector by
l2 = f(W1 · l1) (1)
where f(·) is the tanh nonlinear activation f(z) = 1−e−2z 1+e−2z .",2.2 The Proposed Multi-Task DNN Model,[0],[0]
"This 50k-by-300 matrix W1 is responsible for generating the cross-task semantic representation for arbitrary text inputs (e.g., Q or D).
",2.2 The Proposed Multi-Task DNN Model,[0],[0]
"Task-Specific Representation (l3): For each task, a nonlinear transformation maps the 300- dimension semantic representation l2 into the 128- dimension task-specific representation by
l3 = f(Wt2 · l2) (2)
where, t denotes different tasks (query classification or web search).
",2.2 The Proposed Multi-Task DNN Model,[0],[0]
Query Classification Output: Suppose QC1 ≡ l3 = f(Wt=C12 · l2) is the 128-dimension taskspecific representation for a query Q.,2.2 The Proposed Multi-Task DNN Model,[0],[0]
"The probability that Q belongs to class C1 is predicted by a logistic regression, with sigmoid g(z)",2.2 The Proposed Multi-Task DNN Model,[0],[0]
"= 1
",2.2 The Proposed Multi-Task DNN Model,[0],[0]
"1+e−z :
",2.2 The Proposed Multi-Task DNN Model,[0],[0]
"P (C1|Q) = g(Wt=C13 ·QC1) (3)
Web Search Output: For the web search task, both the query Q and the document D are mapped into 128-dimension task-specific representations QSq and DSd .",2.2 The Proposed Multi-Task DNN Model,[0],[0]
"Then, the relevance score is
Algorithm 1: Training a Multi-task DNN Initialize model Θ : {W1,Wt2,Wt3} randomly for iteration in 0...∞ do
1.",2.2 The Proposed Multi-Task DNN Model,[0],[0]
Pick a task t randomly 2.,2.2 The Proposed Multi-Task DNN Model,[0],[0]
"Pick sample(s) from task t
(Q, yt = {0, 1}) for query classification (Q,L) for web search
3.",2.2 The Proposed Multi-Task DNN Model,[0],[0]
Compute loss: L(Θ) L(Θ)=Eq.,2.2 The Proposed Multi-Task DNN Model,[0],[0]
5 for query classification L(Θ)=Eq.,2.2 The Proposed Multi-Task DNN Model,[0],[0]
6 for web search 4.,2.2 The Proposed Multi-Task DNN Model,[0],[0]
Compute gradient: ∇(Θ) 5.,2.2 The Proposed Multi-Task DNN Model,[0],[0]
"Update model: Θ = Θ− ∇(Θ)
end The task t is one of the query classification tasks or web search task, as shown in Figure 1.",2.2 The Proposed Multi-Task DNN Model,[0],[0]
"For query classification, each training sample includes one query and its category label.",2.2 The Proposed Multi-Task DNN Model,[0],[0]
"For web search, each training sample includes query and document list.
computed by cosine similarity as:
R(Q,D) = cos(QSq , DSd)",2.2 The Proposed Multi-Task DNN Model,[0],[0]
= QSq ·DSd ||QSq,2.2 The Proposed Multi-Task DNN Model,[0],[0]
||||DSd || (4),2.2 The Proposed Multi-Task DNN Model,[0],[0]
"In order to learn the parameters of our model, we use mini-batch-based stochastic gradient descent (SGD) as shown in Algorithm 1.",2.3 The Training Procedure,[0],[0]
"In each iteration, a task t is selected randomly, and the model is updated ac-
cording to the task-specific objective.",2.3 The Training Procedure,[0],[0]
This approximately optimizes the sum of all multi-task objectives.,2.3 The Training Procedure,[0],[0]
"For query classification of class Ct, we use the cross-entropy loss as the objective: −{yt lnP (Ct|Q)+(1−yt) ln(1−P (Ct|Q))}",2.3 The Training Procedure,[0],[0]
"(5)
where yt = {0, 1} is the label and the loss is summed over all samples in the mini-batch (1024 samples in experiments).
",2.3 The Training Procedure,[0],[0]
"The objective for web search used in this paper follows the pair-wise learning-to-rank paradigm outlined in (Burges et al., 2005).",2.3 The Training Procedure,[0],[0]
"Given a query Q, we obtain a list of documents L that includes a clicked document D+ (positive sample), and J randomlysampled non-clicked documents {D−j }j=1,.,J .",2.3 The Training Procedure,[0],[0]
"We then minimize the negative log likelihood of the clicked document (defined in Eq. 7) given queries across the training data
− log ∏
(Q,D+)
P (D+|Q) (6)
where the probability of a given document D+ is computed
P (D+|Q) =",2.3 The Training Procedure,[0],[0]
"exp(γR(Q,D +))",2.3 The Training Procedure,[0],[0]
"∑
D′∈L exp(γR(Q,D′)) (7)
",2.3 The Training Procedure,[0],[0]
"here, γ is a tuning factor determined on held-out data.",2.3 The Training Procedure,[0],[0]
"Additional training details: (1) Model parameters are initialized with uniform distribution in the range (−√6/(fanin + fanout),√6/(fanin + fanout))",2.3 The Training Procedure,[0],[0]
"(Montavon et al., 2012).",2.3 The Training Procedure,[0],[0]
"Empirically, we have not observed better performance by initialization with layer-wise pre-training.",2.3 The Training Procedure,[0],[0]
"(2) Moment methods and AdaGrad training (Duchi et al., 2011) speed up the convergence speed but gave similar results as plain SGD.",2.3 The Training Procedure,[0],[0]
The SGD learning rate is fixed at = 0.1/1024.,2.3 The Training Procedure,[0],[0]
"(3) We run Algorithm 1 for 800K iterations, taking 13 hours on an NVidia K20 GPU.",2.3 The Training Procedure,[0],[0]
"Our proposed multi-task DNN (Figure 1) can be viewed as a combination of a standard DNN for classification and a Deep Structured Semantic Model (DSSM) for ranking, shown in Figure 2.",2.4 An Alternative View of the Multi-Task Model,[0],[0]
Other ways to merge the models are possible.,2.4 An Alternative View of the Multi-Task Model,[0],[0]
"Figure 3 shows an alternative multi-task architecture, where only the query part is shared among all tasks and the DSSM
retains independent parameters for computing the document representations.",2.4 An Alternative View of the Multi-Task Model,[0],[0]
This is more similar to the original DSSM.,2.4 An Alternative View of the Multi-Task Model,[0],[0]
"We have attempted training this model using Algorithm 1, but it achieves good results on query classification at the expense of web search.",2.4 An Alternative View of the Multi-Task Model,[0],[0]
"This is likely due to unbalanced updates (i.e. parameters for queries are updated more often than that of documents), and implying that the amount of sharing is an important design choice in multi-task models.
",2.4 An Alternative View of the Multi-Task Model,[0],[0]
3,2.4 An Alternative View of the Multi-Task Model,[0],[0]
"We employ large-scale, real data sets in our evaluation.",3.1 Data Sets and Evaluation Metrics,[0],[0]
See Table 1 for statistics.,3.1 Data Sets and Evaluation Metrics,[0],[0]
The test data for query classification were sampled from one-year log files of a commercial search engine with labels (yes or no) judged by humans.,3.1 Data Sets and Evaluation Metrics,[0],[0]
"The test data for web search contains 12,071 English queries, where each query-document pair has a relevance label manually annotated on a 5-level relevance scale: bad, fair,
good, excellent and perfect.",3.1 Data Sets and Evaluation Metrics,[0],[0]
"The evaluation metric for query classification is the Area under of Receiver Operating Characteristic (ROC) curve (AUC) score (Bradley, 1997).",3.1 Data Sets and Evaluation Metrics,[0],[0]
"For web search, we employ the Normalized Discounted Cumulative Gain (NDCG) (Järvelin and Kekäläinen, 2000).",3.1 Data Sets and Evaluation Metrics,[0],[0]
"First, we evaluate whether our model can robustly improve performance, measured as accuracy across multiple tasks.
",3.2 Results on Accuracy,[0],[0]
"Table 2 summarizes the AUC scores for query classification, comparing the following classifiers: • SVM-Word: a SVM model2 with unigram, bi-
gram and trigram surface-form word features.
",3.2 Results on Accuracy,[0],[0]
• SVM-Letter: a SVM model with letter trigram features (i.e. l1 in Figure 1 as input to SVM).,3.2 Results on Accuracy,[0],[0]
• DNN:,3.2 Results on Accuracy,[0],[0]
single-task deep neural net (Figure 2).,3.2 Results on Accuracy,[0],[0]
• MT-DNN: our multi-task proposal (Figure 1).,3.2 Results on Accuracy,[0],[0]
The results show that the proposed MT-DNN performs best in all four domains.,3.2 Results on Accuracy,[0],[0]
"Further, we observe:
1.",3.2 Results on Accuracy,[0],[0]
"MT-DNN outperforms DNN, indicating the usefulness of the multi-task objective (that includes web search) over the single-task objective of query classification.
2.",3.2 Results on Accuracy,[0],[0]
"Both DNN and MT-DNN outperform SVMLetter, which initially uses the same input features (l1).",3.2 Results on Accuracy,[0],[0]
"This indicates the importance of learning a semantic representation l2 on top of these letter trigrams.
3.",3.2 Results on Accuracy,[0],[0]
"Both DNN and MT-DNN outperform a strong SVM-Word baseline, which has a large feature set that consists of 3 billion features.
",3.2 Results on Accuracy,[0],[0]
"Table 3 summarizes the NDCG results on web search, comparing the following models:",3.2 Results on Accuracy,[0],[0]
"2In this paper, we use the liblinear to build SVM classifiers and optimize the corresponding parameter C by using 5-fold cross-validation in training data.",3.2 Results on Accuracy,[0],[0]
"http://www.csie.ntu.edu.tw/ cjlin/liblinear/
• Popular baselines in the web search literature, e.g. BM25, Language Model, PLSA
• DSSM: single-task ranking model (Figure 2) • MT-DNN:",3.2 Results on Accuracy,[0],[0]
"our multi-task proposal (Figure 1)
",3.2 Results on Accuracy,[0],[0]
"Again, we observe that MT-DNN performs best.",3.2 Results on Accuracy,[0],[0]
"For example, MT-DNN achieves NDCG@1=0.334, outperforming the current state-of-the-art single-task DSSM (0.327) and the classic methods like PLSA (0.308) and BM25 (0.305).",3.2 Results on Accuracy,[0],[0]
"This is a statistically significant improvement (p < 0.05) over DSSM and other baselines.
",3.2 Results on Accuracy,[0],[0]
"To recap, our MT-DNN robustly outperforms strong baselines across all web search and query classification tasks.",3.2 Results on Accuracy,[0],[0]
"Further, due to the use of larger training data (from different domains) and the regularization effort as we discussed in Section 1, we confirm the advantage of multi-task models over than single-task ones.3",3.2 Results on Accuracy,[0],[0]
Important criteria for building practical systems are agility of deployment and small memory footprint and fast run-time.,3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"Our model satisfies both with 3We have also trained SVM using Word2Vec (Mikolov et al., 2013b; Mikolov et al., 2013a) features.",3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"Unfortunately, the results are poor at 60-70 AUC, indicating the sub-optimality of unsupervised representation learning objectives for actual prediction tasks.",3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"We optimized the Word2Vec features in the SVM baseline by scaling and normalizing as well, but did not observe much improvement.
high model compactness.",3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
The key to the compactness is the aggressive compression from the 500kdimensional bag-of-words input to 300-dimensional semantic representation l2.,3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
This significantly reduces the memory/run-time requirements compared to systems that rely on surface-form features.,3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"The most expensive portion of the model is storage of the 50k-by-300 W1 and its matrix multiplication with l1, which is sparse: this is trivial on modern hardware.",3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"Our multi-task DNN takes < 150KB in memory whereas e.g. SVM-Word takes about 200MB.
",3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"Compactness is particularly important for query classification, since one may desire to add new domains after discovering new needs from the query logs of an operational system.",3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"On the other hand, it is prohibitively expensive to collect labeled training data for new domains.",3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"Very often, we only have very small training data or even no training data.
",3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"To evaluate the models using the above criteria, we perform domain adaptation experiments on query classification using the following procedure: (1) Select one query classification task t∗.",3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"Train MTDNN on the remaining tasks (including Web Search
task) to obtain a semantic representation (l2); (2) Given a fixed l2, train an SVM on the training data t∗, using varying amounts of labels; (3) Evaluate the AUC on the test data of t∗
We compare three SVM classifiers trained using different feature representations: (1) SemanticRepresentation uses the l2 features generated according to the above procedure.",3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"(2) Word3gram uses unigram, bigram and trigram word features.",3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
(3) Letter3gram uses letter-trigrams.,3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"Note that Word3gram and Letter3gram correspond to SVMWord and SVM-Letter respectively in Table 2.
",3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
The AUC results for different amounts of t∗ training data are shown in Figure 4.,3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"In the Hotel, Flight and Restaurant domains, we observe that our semantic representation dominated the other two feature representations (Word3gram and Letter3gram) in all cases except the extremely large-data regime (more than 1 million training samples in domain t∗).",3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"Given sufficient labels, SVM is able to train well on Word3gram sparse features, but for most cases Se-
manticRepresentation is recommended.4
In a further experiment, we compare the following two DNNs using the same domain adaptation procedure: (1) DNN1: DNN where W1 is randomly initialized and parameters W1,W2,Wt ∗ 3 are trained on varying amounts of data in t∗;",3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"(2) DNN2: DNN where W1 is obtained from other tasks (i.e. SemanticRepresentation) and fixed, while parameters W2,Wt ∗ 3 are trained on varying amounts of data in t∗.",3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
The purpose is to see whether shared semantic representation is useful even under a DNN architecture.,3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"Figure 5 show the AUC results of DNN1 vs. DNN2 (the results SVM denotes the same system as SemanticRepresentation in Figure 4, plotted here for reference).",3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"We observe that when the training data is extremely large (millions of samples), one does best by training all parameters from scratch (DNN1).",3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"Otherwise, one is better off using a shared semantic representation trained by multitask objectives.",3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"Comparing DNN2 and SVM with SemanticRepresentation, we note that SVM works best for training data of several thousand samples; DNN2 works best in the medium data range.",3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"There is a large body of work on representation learning for natural language processing, sometimes using different terminologies for similar concepts; e.g., feature generation, dimensionality reduction, and vector space models.",4 Related Work,[0],[0]
"The main motivation is similar: to abstract away from surface forms in words, sentences, or documents, in order to alleviate sparsity and approximate semantics.",4 Related Work,[0],[0]
"Traditional techniques include LSA (Deerwester et al., 1990), ESA (Gabrilovich and Markovitch, 2007), PCA (Karhunen, 1998), and non-linear kernel variants (Schölkopf et al., 1998).",4 Related Work,[0],[0]
"Recently, learningbased approaches inspired by neural networks, especially DNNs, have gained in prominence, due to their favorable performance (Huang et al., 2013; Baroni et al., 2014; Milajevs et al., 2014).
",4 Related Work,[0.9687770362500614],"['For alleviating noise in distant supervised datasets, attention has been utilized by (Lin et al., 2016; Jat et al., 2018).']"
"Popular methods for learning word representations include (Collobert et al., 2011; Mikolov et al., 2013c; Mnih and Kavukcuoglu, 2013; Pennington et al., 2014): all are based on unsupervised objec-
4The trends differ slightly in the Nightlife domain.",4 Related Work,[0],[0]
"We believe this may be due to data bias on test data (only 298 samples).
tives of predicting words or word frequencies from raw text.",4 Related Work,[0],[0]
"End-to-end neural network models for specific tasks (e.g. parsing) often use these word representations as initialization, which are then iteratively improved by optimizing a supervised objective (e.g. parsing accuracy).",4 Related Work,[0],[0]
"A selection of successful applications of this approach include sequence labeling (Turian et al., 2010), parsing (Chen and Manning, 2014), sentiment (Socher et al., 2013), question answering (Iyyer et al., 2014) and translation modeling (Gao et al., 2014a).
",4 Related Work,[0],[0]
"Our model takes queries and documents as input, so it learns sentence/document representations.",4 Related Work,[0],[0]
"This is currently an open research question, the challenge being how to properly model semantic compositionality of words in vector space (Huang et al., 2013; M. Baroni and Zamparelli, 2013; Socher et al., 2013).",4 Related Work,[0],[0]
"While we adopt a bag-of-words approach for practical reasons (memory and run-time), our multi-task framework is extensible to other methods for sentence/document representations, such as those based on convolutional networks (Kalchbrenner et al., 2014; Shen et al., 2014; Gao et al., 2014b), parse tree structure (Irsoy and Cardie, 2014), and run-time inference (Le and Mikolov, 2014).
",4 Related Work,[0],[0]
"The synergy between multi-task learning and neural nets is quite natural; the general idea dates back to (Caruana, 1997).",4 Related Work,[0],[0]
The main challenge is in designing the tasks and the network structure.,4 Related Work,[0],[0]
"For example, (Collobert et al., 2011) defined part-of-speech tagging, chunking, and named entity recognition as multiple tasks in a single sequence labeler; (Bordes et al., 2012) defined multiple data sources as tasks in their relation extraction system.",4 Related Work,[0],[0]
"While conceptually similar, our model is novel in that it combines tasks as disparate as classification and ranking.",4 Related Work,[0],[0]
"Further, considering that multi-task models often exhibit mixed results (i.e. gains in some tasks but degradation in others), our accuracy improvements across all tasks is a very satisfactory result.",4 Related Work,[0],[0]
"In this work, we propose a robust and practical representation learning algorithm based on multi-task objectives.",5 Conclusion,[0],[0]
"Our multi-task DNN model successfully combines tasks as disparate as classification and ranking, and the experimental results demon-
strate that the model consistently outperforms strong baselines in various query classification and web search tasks.",5 Conclusion,[0],[0]
"Meanwhile, we demonstrated compactness of the model and the utility of the learned query/document representation for domain adaptation.
",5 Conclusion,[0],[0]
Our model can be viewed as a general method for learning semantic representations beyond the word level.,5 Conclusion,[0],[0]
"Beyond query classification and web search, we believe there are many other knowledge sources (e.g. sentiment, paraphrase) that can be incorporated either as classification or ranking tasks.",5 Conclusion,[0],[0]
A comprehensive exploration will be pursued as future work.,5 Conclusion,[0],[0]
"We thank Xiaolong Li, Yelong Shen, Xinying Song, Jianshu Chen, Byungki Byun, Bin Cao and the anonymous reviewers for valuable discussions and comments.",Acknowledgments,[0],[0]
Methods of deep neural networks (DNNs) have recently demonstrated superior performance on a number of natural language processing tasks.,abstractText,[0],[0]
"However, in most previous work, the models are learned based on either unsupervised objectives, which does not directly optimize the desired task, or singletask supervised objectives, which often suffer from insufficient training data.",abstractText,[0],[0]
"We develop a multi-task DNN for learning representations across multiple tasks, not only leveraging large amounts of cross-task data, but also benefiting from a regularization effect that leads to more general representations to help tasks in new domains.",abstractText,[0],[0]
"Our multi-task DNN approach combines tasks of multiple-domain classification (for query classification) and information retrieval (ranking for web search), and demonstrates significant gains over strong baselines in a comprehensive set of domain adaptation.",abstractText,[0],[0]
Representation Learning Using Multi-Task Deep Neural Networks for Semantic Classification and Information Retrieval,title,[0],[0]
"Recently, hyperbolic embeddings have been proposed as a way to capture hierarchy information for network and natural language processing tasks (Nickel & Kiela, 2017; Chamberlain et al., 2017).",1. Introduction,[0],[0]
"This approach is an exciting way to fuse structural information (for example, from knowledge graphs or synonym hierarchies) with the continuous representations favored by modern machine learning methods.
",1. Introduction,[0],[0]
"To understand the intuition behind hyperbolic embeddings’ superior capacity, note that trees can be embedded with arbitrarily low distortion into the Poincaré disk, a twodimensional model of hyperbolic space (Sarkar, 2011).",1. Introduction,[0],[0]
"In contrast, Bourgain’s theorem (Linial et al., 1995) shows that Euclidean space cannot achieve comparably low distortion
1Department of Computer Science, Stanford University 2Department of Computer Science, Cornell University.",1. Introduction,[0],[0]
"Correspondence to: Frederic Sala <fredsala@stanford.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
for trees—even using an unbounded number of dimensions.
",1. Introduction,[0],[0]
"Many graphs, such as complex networks (Krioukov et al., 2010), the Internet (Krioukov et al., 2009), and social networks (Verbeek & Suri, 2016), are known to have tree-like or hyperbolic structure and thus befit hyperbolic embeddings.",1. Introduction,[0.9523577044762922],"['The construction of large-scale Knowledge Bases (KBs) like Freebase (Bollacker et al., 2008) and Wikidata (Vrandečić and Krötzsch, 2014) has proven to be useful in many natural language processing (NLP) tasks like question-answering, web search, etc.']"
"Indeed, recent works show that hyperbolic representations are suitable for many hierarchies (e.g. the question answering (Q/A) system HyperQA in Tay et al. (2018), vertex classifiers in Chamberlain et al. (2017), and link prediction (Nickel & Kiela, 2017)).",1. Introduction,[0],[0]
"However, the optimization problems underlying the embedding techniques in these works are challenging, motivating us to seek fundamental insights and to understand the subtle tradeoffs involved.
",1. Introduction,[0],[0]
"We begin by considering the case where we are given an input graph that is a tree or nearly tree-like, and our goal is to produce a low-dimensional hyperbolic embedding that preserves all distances.",1. Introduction,[0],[0]
This leads to a simple combinatorial strategy that directly places points instead of minimizing a surrogate loss function.,1. Introduction,[0],[0]
It is both fast (nearly linear time) and has formal quality guarantees.,1. Introduction,[0],[0]
"The approach proceeds in two phases: we (1) produce an embedding of a graph into a weighted tree, and (2) embed that tree into the hyperbolic disk.",1. Introduction,[0],[0]
"In particular, we consider an extension of an elegant embedding of trees into the Poincaré disk by Sarkar (2011) and work on low-distortion graph embeddings into tree metrics (Abraham et al., 2007).",1. Introduction,[0],[0]
"For trees, this approach has nearly perfect quality.",1. Introduction,[0],[0]
"On the WordNet hypernym graph reconstruction, it obtains a nearly perfect mean average precision (MAP) of 0.989 using just 2 dimensions.",1. Introduction,[0],[0]
"The best published numbers for WordNet in Nickel & Kiela (2017) range between 0.823 and 0.87 for 5 to 200 dimensions.
",1. Introduction,[0],[0]
We analyze this construction to extract fundamental tradeoffs.,1. Introduction,[0],[0]
"One tradeoff involves the embedding dimension, the properties of the graph, and the number of bits of precision used to represent components of embedded points—an important hidden cost.",1. Introduction,[0],[0]
"We show that for a fixed precision, the dimension required scales linearly with the length of the longest path.",1. Introduction,[0],[0]
"On the other hand, the dimension scales logarithmically with the maximum degree of the tree.",1. Introduction,[0],[0]
"This suggests that hyperbolic embeddings should have high quality on hierarchies like WordNet but require large dimensions or high precision on graphs with long chains.
",1. Introduction,[0],[0]
"To understand how hyperbolic embeddings perform for met-
rics that are far from tree-like, we consider a more general problem: given a matrix of distances that arise from points that are embeddable in hyperbolic space of dimension d (not necessarily from a graph), find a set of points that produces these distances.",1. Introduction,[0],[0]
"In Euclidean space, the problem is known as multidimensional scaling (MDS) and is solvable using PCA.",1. Introduction,[0],[0]
"A key step is a transformation that effectively centers the points, without knowledge of their exact coordinates.",1. Introduction,[0],[0]
"It is not obvious how to center points in hyperbolic space, which is curved.",1. Introduction,[0],[0]
"We show that in hyperbolic space, a centering operation is still possible with respect to a non-standard mean.",1. Introduction,[0],[0]
"In turn, this allows us to reduce the hyperbolic MDS problem (h-MDS) to a standard eigenvalue problem that can be solved with power methods.",1. Introduction,[0],[0]
"We also extend classical PCA perturbation analysis (Sibson, 1978; 1979).",1. Introduction,[0],[0]
"When applied to distances from graphs induced by real data, h-MDS obtains low distortion on far from tree-like graphs.",1. Introduction,[0],[0]
"However, we observe that these solutions may require high precision, which is not surprising in light of our previous analysis.
",1. Introduction,[0],[0]
"Finally, we handle increasing amounts of noise in the model, leading naturally into new SGD-based formulations.",1. Introduction,[0],[0]
"Like in traditional PCA, the underlying problem is nonconvex.",1. Introduction,[0],[0]
"In contrast to PCA, there are local minima that are not global minima—an additional challenge.",1. Introduction,[0],[0]
Our main technical result is that an SGD-based algorithm initialized with an h-MDS solution can recover the submanifold the data is on—even in some cases in which the data is perturbed by noise that can be full dimensional.,1. Introduction,[0],[0]
Our algorithm essentially provides new recovery results for convergence of Principal Geodesic Analysis (PGA) in hyperbolic space.,1. Introduction,[0],[0]
We implemented the resulting SGD-based algorithm using PyTorch.,1. Introduction,[0],[0]
"Finally, we note that all of our algorithms can handle incomplete distance information through standard techniques.",1. Introduction,[0],[0]
"We provide intuition connecting hyperbolic space and tree distances, discuss the metrics used to measure embedding fidelity, and discuss the relationship between the reconstruction and learning problems for graph embeddings.
",2. Background,[0],[0]
"Hyperbolic spaces The Poincaré disk H2 is a twodimensional model of hyperbolic geometry with points located in the interior of the unit disk, as shown in Figure 1.",2. Background,[0],[0]
"A natural generalization of H2 is the Poincaré ball Hr, with elements inside the unit ball.",2. Background,[0],[0]
"The Poincaré models offer several useful properties, chief among which is mapping conformally to Euclidean space.",2. Background,[0],[0]
"That is, angles are preserved between hyperbolic and Euclidean space.",2. Background,[0],[0]
"Distances, on the other hand, are not preserved, but are given by
dH(x, y) = acosh ( 1 + 2 ‖x− y‖2
(1− ‖x‖2)(1− ‖y‖2)
) .
",2. Background,[0],[0]
"There are some potentially unexpected consequences of this formula, and a simple example gives intuition about a key technical property that allows hyperbolic space to embed trees.",2. Background,[0],[0]
"Consider three points inside the unit disk: the origin 0, and points x and y with ‖x‖ = ‖y‖ = t for some t > 0.",2. Background,[0],[0]
"As shown on the right of Figure 1, as t → 1 (i.e., the points move towards the outside of the disk), in flat Euclidean space, the ratio dE(x,y)dE(x,0)+dE(0,y) is constant with respect to t (blue curve).",2. Background,[0],[0]
"In contrast, the ratio
dH(x,y) dH(x,0)+dH(0,y) approaches 1, or, equivalently, the distance dH(x, y) approaches dH(x, 0) +",2. Background,[0],[0]
"dH(0, y) (red and pink curves).",2. Background,[0],[0]
"That is, the shortest path between x and y is almost the same as the path through the origin.",2. Background,[0],[0]
This is analogous to the property of trees in which the shortest path between two sibling nodes is the path through their parent.,2. Background,[0],[0]
This tree-like nature of hyperbolic space is the key property exploited by embeddings.,2. Background,[0],[0]
"Moreover, this property holds for arbitrarily small angles between x and y.
Lines and geodesics There are two types of geodesics (shortest paths) in the Poincaré disk model: segments of circles that are orthogonal to the disk surface, and disk diameters (Brannan et al., 2012).",2. Background,[0],[0]
"Our algorithms and proofs make use of a simple geometric fact: isometric reflection across geodesics (preserving hyperbolic distances) is represented in this Euclidean model as a circle inversion.
",2. Background,[0],[0]
Embeddings and fidelity measures An embedding is a mapping f :,2. Background,[0],[0]
"U → V for spaces U, V with distances dU , dV .",2. Background,[0],[0]
"We measure the quality of embeddings with several fidelity measures, presented here from most local to most global.
",2. Background,[0],[0]
"Recent work (Nickel & Kiela, 2017) proposes using the mean average precision (MAP).",2. Background,[0],[0]
"For a graph G = (V,E), let a ∈ V have neighborhood Na = {b1, b2, . . .",2. Background,[0],[0]
", bdeg(a)}, where deg(a) denotes the degree of a.",2. Background,[0],[0]
"In the embedding f , consider the points closest to f(a), and define Ra,bi to be the smallest set of such points that contains bi (that is, Ra,bi is the smallest set of nearest points required to retrieve the ith neighbor of a in f ).",2. Background,[0],[0]
"Then, the MAP is defined to be
MAP(f) = 1 |V | ∑ a∈V
1
deg(a) |Na|∑ i=1",2. Background,[0],[0]
"|Na ∩Ra,bi | |Ra,bi | .
",2. Background,[0],[0]
"We have MAP(f) ≤ 1, with 1 as the best case.",2. Background,[0],[0]
"MAP is not concerned with explicit distances, but only ranks between the distances of immediate neighbors.",2. Background,[0],[0]
"It is a local metric.
",2. Background,[0],[0]
"The standard metric for graph embeddings is distortion D. For an n point embedding,
D(f)",2. Background,[0],[0]
"= 1( n 2 )  ∑ u,v∈U :u6=v |dV",2. Background,[0],[0]
"(f(u),",2. Background,[0],[0]
"f(v))− dU (u, v)| dU (u, v)  .
",2. Background,[0],[0]
"The best distortion isD(f) = 0, preserving the edge lengths exactly.",2. Background,[0],[0]
"This is a global metric, as it depends directly on the underlying distances rather than the local relationships between distances.",2. Background,[0],[0]
"A variant is the worst-case distortion Dwc, defined by
Dwc(f) =",2. Background,[0],[0]
"maxu,v∈U :u6=v dV (f(u), f(v))/dU (u, v)
minu,v∈U :u6=v dV (f(u), f(v))/dU (u, v) .
",2. Background,[0],[0]
"That is, the wost-case distortion is the ratio of the maximal expansion and the minimal contraction of distances.",2. Background,[0],[0]
Note that scaling the unit distance does not affect Dwc.,2. Background,[0],[0]
"The best worst-case distortion is Dwc(f) = 1.
",2. Background,[0],[0]
"Reconstruction and learning If we lack a full set of distances, we can either use the triangle inequality to recover the missing distances, or we can access the scaled Euclidean distances (the inside of the acosh in dH(x, y)), and apply standard matrix completion techniques (Candes & Tao, 2010).",2. Background,[0],[0]
Then we compute an embedding using any of the approaches discussed in this paper.,2. Background,[0],[0]
We quantify the error introduced by this process experimentally in Section 5.,2. Background,[0],[0]
We first focus on hyperbolic tree embeddings—a natural approach considering the tree-like behavior of hyperbolic space.,3. Combinatorial Constructions,[0],[0]
We review the embedding of Sarkar (2011).,3. Combinatorial Constructions,[0],[0]
"We then provide novel analysis on the precision, revealing fundamental limits of hyperbolic embeddings.",3. Combinatorial Constructions,[0],[0]
"In particular, we characterize the bits of precision needed for hyperbolic representations.",3. Combinatorial Constructions,[0],[0]
"We extend the construction to r dimensions, and propose to use Steiner nodes to better embed general graphs as trees, building on Abraham et al. (2007).
",3. Combinatorial Constructions,[0],[0]
Embedding trees The nature of hyperbolic space lends itself towards excellent tree embeddings.,3. Combinatorial Constructions,[0],[0]
"In fact, it is possible to embed trees into the Poincaré disk H2 with arbitrarily low distortion (Sarkar, 2011).",3. Combinatorial Constructions,[0],[0]
"Remarkably, trees cannot be embedded into Euclidean space with arbitrarily low distortion for any number of dimensions.",3. Combinatorial Constructions,[0],[0]
"These notions motivate the following two-step process for embedding hierarchies
into hyperbolic space: (1) embed the graphG = (V,E) into a tree T , and (2) embed T into the Poincaré ball",3. Combinatorial Constructions,[0],[0]
Hd.,3. Combinatorial Constructions,[0],[0]
We refer to this process as the combinatorial construction.,3. Combinatorial Constructions,[0],[0]
Note that we are not required to minimize a loss function.,3. Combinatorial Constructions,[0],[0]
"We begin by describing the second stage, where we extend an elegant construction from Sarkar (2011).",3. Combinatorial Constructions,[0],[0]
Algorithm 1 performs an embedding of trees into H2.,3.1. Sarkar’s Construction,[0],[0]
The inputs are a scaling factor τ and a node a (of degree deg(a)) from the tree with parent node b. Say a and b have already been embedded into f(a) and f(b) in H2.,3.1. Sarkar’s Construction,[0],[0]
"The algorithm places the children c1, c2, . . .",3.1. Sarkar’s Construction,[0],[0]
", cdeg(a)−1 into H2.
",3.1. Sarkar’s Construction,[0],[0]
A two-step process is used.,3.1. Sarkar’s Construction,[0],[0]
"First, f(a) and f(b) are reflected across a geodesic (using circle inversion) so that f(a) is mapped onto the origin 0 and f(b) is mapped onto some point",3.1. Sarkar’s Construction,[0],[0]
"z. Next, we place the children nodes to vectors y1, . . .",3.1. Sarkar’s Construction,[0],[0]
", yd−1 equally spaced around a circle with radius eτ−1 eτ+1 (which is a circle of radius τ in the hyperbolic metric), and maximally separated from the reflected parent node embedding z.",3.1. Sarkar’s Construction,[0],[0]
"Lastly, we reflect all of the points back across the geodesic.",3.1. Sarkar’s Construction,[0],[0]
The isometric properties of reflections imply that all children are now at hyperbolic distance exactly τ from f(a).,3.1. Sarkar’s Construction,[0],[0]
"To embed the entire tree, we place the root at the origin O and its children in a circle around it (as in Step 5 of Algorithm 1), then recursively place their children until all nodes have been placed.",3.1. Sarkar’s Construction,[0],[0]
This process runs in linear time.,3.1. Sarkar’s Construction,[0],[0]
Sarkar’s construction works by separating children sufficiently in hyperbolic space.,3.2. Analyzing Sarkar’s Construction,[0],[0]
A key technical idea is to scale all the edges by a factor τ before embedding.,3.2. Analyzing Sarkar’s Construction,[0],[0]
We can then recover the original distances by dividing by τ .,3.2. Analyzing Sarkar’s Construction,[0],[0]
This transformation exploits the fact that hyperbolic space is not scale invariant.,3.2. Analyzing Sarkar’s Construction,[0],[0]
"Sarkar’s construction always captures neighbors perfectly, but Figure 1 implies that increasing the scale preserves the distances between farther nodes better.",3.2. Analyzing Sarkar’s Construction,[0],[0]
"Indeed, if one sets τ = 1+εε ( 2 log degmaxπ/2 ) , then the worst-case distortion D of the resulting embedding is no more than
Algorithm 1 Sarkar’s Construction 1: Input: Node a with parent b, children to place c1, c2, . . .",3.2. Analyzing Sarkar’s Construction,[0],[0]
", cdeg(a)−1, partial embedding f containing an embedding for a and b, scaling factor τ
2: (0, z)← reflectf(a)→0(f(a), f(b))",3.2. Analyzing Sarkar’s Construction,[0],[0]
"3: θ ← arg(z) {angle of z from x-axis in the plane} 4: for i ∈ {1, . . .",3.2. Analyzing Sarkar’s Construction,[0],[0]
",deg(a)− 1} do 5: yi ← e τ−1 eτ+1 · ( cos ( θ + 2πideg(a) ) , sin ( θ + 2πideg(a)
))",3.2. Analyzing Sarkar’s Construction,[0],[0]
"6: (f(a), f(b), f(c1), . . .",3.2. Analyzing Sarkar’s Construction,[0],[0]
", f(cdeg(a)−1))",3.2. Analyzing Sarkar’s Construction,[0],[0]
"←
reflect0→f(a)(0, z, y1, . . .",3.2. Analyzing Sarkar’s Construction,[0],[0]
", ydeg(x)−1) 7: Output: Embedded H2 vectors f(c1), f(c2), . . .",3.2. Analyzing Sarkar’s Construction,[0],[0]
", f(cdeg(a)−1)
1 + ε.",3.2. Analyzing Sarkar’s Construction,[0],[0]
"For trees, Sarkar’s construction has arbitrarily high fidelity.",3.2. Analyzing Sarkar’s Construction,[0],[0]
"However, this comes at a cost: the scaling τ affects the bits of precision required.",3.2. Analyzing Sarkar’s Construction,[0],[0]
"In fact, we will show that the precision scales logarithmically with the degree of the tree—but linearly with the maximum path length.
",3.2. Analyzing Sarkar’s Construction,[0],[0]
How many bits of precision do we need to represent points in H2?,3.2. Analyzing Sarkar’s Construction,[0],[0]
"If x ∈ H2, then ‖x‖ < 1, so we need sufficiently many bits so that 1− ‖x‖ will not be rounded to zero.",3.2. Analyzing Sarkar’s Construction,[0],[0]
This requires roughly − log(1− ‖x‖) = log 11−‖x‖ bits.,3.2. Analyzing Sarkar’s Construction,[0],[0]
"Say we are embedding two points x, y at distance d. As described in the background, there is an isometric reflection that takes a pair of points (x, y) in H2 to (0, z) while preserving their distance, so without loss of generality we have that
d = dH(x, y) = dH(0, z) = acosh
( 1 + 2 ‖z‖2
1− ‖z‖2
) .
",3.2. Analyzing Sarkar’s Construction,[0],[0]
"Rearranging the terms, we have (cosh(d) + 1)/2 = (1 − ‖z‖2)−1 ≥ (1 − ‖z‖)−1/2.",3.2. Analyzing Sarkar’s Construction,[0],[0]
"Thus, the number of bits we want so that 1 − ‖z‖ will not be rounded to zero is log(cosh(d)+1).",3.2. Analyzing Sarkar’s Construction,[0],[0]
"Since cosh(d) = (exp(d)+exp(−d))/2, this is roughly d bits.",3.2. Analyzing Sarkar’s Construction,[0],[0]
"That is, in hyperbolic space, we need about d bits to express distances of d (rather than log d in Euclidean space).1 This result will be of use below.
",3.2. Analyzing Sarkar’s Construction,[0],[0]
Consider the largest distance in the embeddings produced by Algorithm 1.,3.2. Analyzing Sarkar’s Construction,[0],[0]
"If the longest path length in the tree is `, and each edge has length τ = 1ε ( 2 log degmax π/2 ) , the largest distance is O( `ε log degmax), and we require this number of bits for the representation.
Let us interpret this expression.",3.2. Analyzing Sarkar’s Construction,[0],[0]
"Note that degmax is inside the log term, so that a bushy tree is not penalized much in precision.",3.2. Analyzing Sarkar’s Construction,[0],[0]
"On the other hand, the longest path length ` is not, so that hyperbolic embeddings struggle with long paths.",3.2. Analyzing Sarkar’s Construction,[0],[0]
"Moreover, by selecting an explicit graph, we derive a matching lower bound, concluding that to achieve a dis-
1Although it is particularly easy to bound precision in the Poincaré model, this fact holds generally for hyperbolic space independent of model (shown in the appendix).
",3.2. Analyzing Sarkar’s Construction,[0],[0]
"tortion ε, any construction requires Ω ( ` ε log(degmax) ) bits.",3.2. Analyzing Sarkar’s Construction,[0],[0]
The argument follows from selecting a graph consisting of m(degmax+1) nodes in a tree with a single root and degmax chains each of length m (shown in the appendix).,3.2. Analyzing Sarkar’s Construction,[0],[0]
Our next contribution is a generalization of the construction from the disk H2 to the ball Hr.,3.3. Improving the Construction,[0],[0]
"Our construction follows the same line as Algorithm 1, but since we have r dimensions, the step where we place children spaced out on a circle around their parent now uses a hypersphere.
",3.3. Improving the Construction,[0],[0]
"Spacing out points on the hypersphere is a classic problem known as spherical coding (Conway & Sloane, 1999).",3.3. Improving the Construction,[0],[0]
"As we shall see, the number of children that we can place for a particular angle grows with the dimension.",3.3. Improving the Construction,[0],[0]
"Since the required scaling factor τ gets larger as the angle decreases, we can reduce τ for a particular embedding by increasing the dimension.",3.3. Improving the Construction,[0],[0]
"Note that increasing the dimension helps with bushy trees (large degmax), but has limited effect on tall trees with small degmax.",3.3. Improving the Construction,[0],[0]
"We show
Proposition 3.1.",3.3. Improving the Construction,[0],[0]
"The generalized Hr combinatorial construction has distortion at most 1 + ε and requires at most O( 1ε ` r log degmax) bits to represent a node component for r ≤ (log degmax) + 1, and O( 1ε `) bits for r > (log degmax) + 1.
To generalize to Hr, we replace Step 5 in Algorithm 1 with a node placement step based on coding theory.",3.3. Improving the Construction,[0],[0]
The children are placed at the vertices of a hypercube inscribed into the unit hypersphere (and then scaled by τ ).,3.3. Improving the Construction,[0],[0]
"Each component of a hypercube vertex has the form ±1√
r .",3.3. Improving the Construction,[0],[0]
"We index these
points using binary sequences a ∈ {0, 1}r in the following way: xa = ( (−1)a1√ r , (−1) a2 √ r , . . .",3.3. Improving the Construction,[0],[0]
", (−1) ar √ r ) .",3.3. Improving the Construction,[0],[0]
We space out the children by controlling the distances by selecting a set of binary sequences a with a prescribed minimum Hamming distance—a binary error-correcting code—and placing the children at the resulting hypercube vertices.,3.3. Improving the Construction,[0],[0]
"We provide more details, including our choice of code in the appendix.",3.3. Improving the Construction,[0],[0]
We revisit the first step of the construction: embedding graphs into trees.,3.4. Embedding into Trees,[0],[0]
"There are fundamental limits to how well graphs can be embedded into trees; in general, breaking long cycles inevitably adds distortion, as shown in Figure 2.",3.4. Embedding into Trees,[0],[0]
"We are inspired by a measure of this limit, the δ-4 points condition introduced in Abraham et al. (2007).",3.4. Embedding into Trees,[0],[0]
A graph on n nodes that satisfies the δ-4 points condition has distortion at most (1 + δ)c1 logn for some constant c1.,3.4. Embedding into Trees,[0],[0]
"This result enables our end-to-end embedding to achieve a distortion of at most D(f) ≤ (1 + δ)c1 logn(1 + ε).
",3.4. Embedding into Trees,[0],[0]
"The result in Abraham et al. (2007) builds a tree with Steiner
nodes.",3.4. Embedding into Trees,[0],[0]
These additional nodes can help control the distances in the resulting weighted tree (Figure 2).,3.4. Embedding into Trees,[0],[0]
"Note that Algorithm 1 readily extends to the case of weighted trees.
",3.4. Embedding into Trees,[0],[0]
"In summary, the key takeaways of our analysis are:
•",3.4. Embedding into Trees,[0],[0]
"There is a fundamental tension between precision and quality in hyperbolic embeddings.
",3.4. Embedding into Trees,[0],[0]
"• Hyperbolic embeddings have an exponential advantage in space compared to Euclidean embeddings for short, bushy hierarchies, but will have less of an advantage for graphs that contain long paths.
",3.4. Embedding into Trees,[0],[0]
• Choosing an appropriate scaling factor τ is critical for quality.,3.4. Embedding into Trees,[0],[0]
"Later, we will propose to learn this scale factor automatically for computing embeddings in PyTorch.
",3.4. Embedding into Trees,[0],[0]
• Steiner nodes can help improve embeddings of graphs.,3.4. Embedding into Trees,[0],[0]
"In this section, we explore a fundamental and more general question than we did in the previous section: if we are given the pairwise distances arising from a set of points in hyperbolic space, can we recover the points?",4. Hyperbolic Multidimensional Scaling,[0],[0]
This enables us to produce an embedding for a desired distance metric.,4. Hyperbolic Multidimensional Scaling,[0],[0]
The equivalent problem for Euclidean distances is solved with multidimensional scaling (MDS).,4. Hyperbolic Multidimensional Scaling,[0],[0]
The goal of this section is to analyze the hyperbolic MDS (h-MDS) problem.,4. Hyperbolic Multidimensional Scaling,[0],[0]
"We describe and overcome the additional technical challenges imposed by hyperbolic distances, and show that exact recovery is possible and interpretable.",4. Hyperbolic Multidimensional Scaling,[0],[0]
Afterwards we propose a technique for dimensionality reduction using principal geodesics analysis (PGA) that provides optimization guarantees.,4. Hyperbolic Multidimensional Scaling,[0],[0]
"In particular, this addresses the shortcomings of h-MDS when recovering points that do not exactly lie on a hyperbolic manifold.",4. Hyperbolic Multidimensional Scaling,[0],[0]
"Suppose that there is a set of hyperbolic points x1, . . .",4.1. Exact Hyperbolic MDS,[0],[0]
", xn ∈ Hr, embedded in the Poincaré ball and written X ∈ Rn×r in matrix form.",4.1. Exact Hyperbolic MDS,[0],[0]
"We observe all the pairwise distances di,j = dH(xi, xj), but do not observe X: our goal is to use the observed di,j’s to recover X (or some other set of points with the same pairwise distances di,j).
",4.1. Exact Hyperbolic MDS,[0],[0]
The MDS algorithm in the Euclidean setting makes an important centering2 assumption: the points have mean 0.,4.1. Exact Hyperbolic MDS,[0],[0]
"If an exact embedding for the distances exists, it can be recovered from a matrix factorization.",4.1. Exact Hyperbolic MDS,[0],[0]
"In other words, Euclidean MDS always recovers a centered embedding.
",4.1. Exact Hyperbolic MDS,[0],[0]
"In hyperbolic space, the same algorithm does not work, but we show that it is possible to find an embedding centered at a different mean.",4.1. Exact Hyperbolic MDS,[0],[0]
"More precisely, we introduce a new mean which we call the pseudo-Euclidean mean, that behaves like the Euclidean mean in that it enables recovery through matrix factorization.",4.1. Exact Hyperbolic MDS,[0],[0]
"Once the points are recovered in hyperbolic space, they can be recentered around a more canonical mean by translating it to the origin.
",4.1. Exact Hyperbolic MDS,[0],[0]
"Algorithm 2 is our complete algorithm, and for the remainder of this section we will describe how and why it works.",4.1. Exact Hyperbolic MDS,[0],[0]
"We first describe the hyperboloid model, an alternate but equivalent model of hyperbolic geometry in which h-MDS is simpler.",4.1. Exact Hyperbolic MDS,[0],[0]
"Of course, we can easily convert between the hyperboloid model and the Poincaré ball model.",4.1. Exact Hyperbolic MDS,[0],[0]
"Next, we show how to reduce the problem to a standard PCA problem, which recovers an embedding centered at the points’ pseudo-Euclidean mean.",4.1. Exact Hyperbolic MDS,[0],[0]
"Finally, we discuss the meaning and implications of centering and prove that the algorithm preserves submanifolds as well—that is, if there is an exact embedding in k < r dimensions centered at their canonical mean, then our algorithm will recover it.
",4.1. Exact Hyperbolic MDS,[0],[0]
The hyperboloid model Define Q to be the diagonal matrix in Rr+1 where Q00 = 1 and Qii = −1 for i > 0.,4.1. Exact Hyperbolic MDS,[0],[0]
"For a vector x ∈ Rr+1, xTQx is called the Minkowski quadratic form.",4.1. Exact Hyperbolic MDS,[0],[0]
"The hyperboloid model is defined as
Mr = { x ∈ Rr+1 ∣∣xTQx = 1 ∧ x0 > 0} , which is endowed with a distance measure dH(x, y) = acosh(xTQy).",4.1. Exact Hyperbolic MDS,[0],[0]
"For convenience, for x ∈Mr let x0 denote 0th coordinate eT0 x, and ~x ∈",4.1. Exact Hyperbolic MDS,[0],[0]
Rr denote the rest of the coordinates3.,4.1. Exact Hyperbolic MDS,[0],[0]
"With this notation, the Minkowski bilinear form can be written xTQy =",4.1. Exact Hyperbolic MDS,[0],[0]
x0y0,4.1. Exact Hyperbolic MDS,[0],[0]
"− ~xT~y.
2We say that points are centered at a particular mean if this mean is at 0.",4.1. Exact Hyperbolic MDS,[0],[0]
"The act of centering refers to applying an isometry that makes the mean of the points 0.
",4.1. Exact Hyperbolic MDS,[0],[0]
"3Since x0 = √
1 + ‖~x‖2 is just a function of ~x, we can equivalently consider just ~x as being a member of a model of hyperbolic space: This representation is sometimes known as the Gans model.
",4.1. Exact Hyperbolic MDS,[0],[0]
"A new mean Given points x1, x2, . . .",4.1. Exact Hyperbolic MDS,[0],[0]
", xn",4.1. Exact Hyperbolic MDS,[0],[0]
"∈Mr in hyperbolic space, define a variance term
Ψ(z;x1, x2, . . .",4.1. Exact Hyperbolic MDS,[0],[0]
", xn) = n∑ i=1",4.1. Exact Hyperbolic MDS,[0],[0]
"sinh2(dH(xi, z)).
",4.1. Exact Hyperbolic MDS,[0],[0]
We define a pseudo-Euclidean mean to be any local minimum of this expression.,4.1. Exact Hyperbolic MDS,[0],[0]
"Notice that this is independent of any particular model of hyperbolic space, since it is defined only through the hyperbolic distance function dH .",4.1. Exact Hyperbolic MDS,[0],[0]
Lemma 4.1.,4.1. Exact Hyperbolic MDS,[0],[0]
Define X ∈ Rn×r such that XT ei = ~xi and u ∈,4.1. Exact Hyperbolic MDS,[0],[0]
"Rn such that ui = x0,i.",4.1. Exact Hyperbolic MDS,[0],[0]
"Then
∇~zΨ(z;x1, x2, . . .",4.1. Exact Hyperbolic MDS,[0],[0]
", xn)|~z=0 = −2 n∑ i=1",4.1. Exact Hyperbolic MDS,[0],[0]
"x0,i~xi = −2XTu.
",4.1. Exact Hyperbolic MDS,[0],[0]
This means that 0 is a pseudo-Euclidean mean if and only if 0 = XTu.,4.1. Exact Hyperbolic MDS,[0],[0]
"Call some hyperbolic points x1, . . .",4.1. Exact Hyperbolic MDS,[0],[0]
", xn pseudoEuclidean centered if their average is 0 in this sense: i.e. if XTu = 0.",4.1. Exact Hyperbolic MDS,[0],[0]
"We can always center a set of points without affecting their pairwise distances by simply finding their average, and then sending it to 0 through an isometry.
",4.1. Exact Hyperbolic MDS,[0],[0]
"Recovery via matrix factorization Suppose we observe the pairwise distances dH(xi, xj) of points x1, x2, . . .",4.1. Exact Hyperbolic MDS,[0],[0]
", xn ∈Mr.",4.1. Exact Hyperbolic MDS,[0],[0]
"This gives the matrix Y such that
Yi,j = cosh (dH(xi, xj))",4.1. Exact Hyperbolic MDS,[0],[0]
"= x0,ix0,j",4.1. Exact Hyperbolic MDS,[0],[0]
− ~xiT ~xj .,4.1. Exact Hyperbolic MDS,[0],[0]
"(1)
DefiningX and u as in Lemma 4.1, then in matrix form Y = uuT−XXT .",4.1. Exact Hyperbolic MDS,[0],[0]
"Without loss of generality, suppose that the xi are centered at their pseudo-Euclidean mean, so thatXTu = 0 by Lemma 4.1.",4.1. Exact Hyperbolic MDS,[0],[0]
"This implies that u is an eigenvector of Y with positive eigenvalue, and the rest of Y ’s eigenvalues are negative.",4.1. Exact Hyperbolic MDS,[0],[0]
"Therefore an eigendecomposition of Y will find u, X̂ such that Y = uuT − X̂X̂T , i.e. it will directly recover X up to rotation.
",4.1. Exact Hyperbolic MDS,[0],[0]
"In fact, running PCA on −Y = XTX − uuT to find the n most significant non-negative eigenvectors will recover X up to rotation, and then u can be found by leveraging the fact that x0 = √ 1 + ‖~x‖2.",4.1. Exact Hyperbolic MDS,[0],[0]
"This leads to Algorithm 2, with optional post-processing steps for converting the embedding to the Poincaré ball model and for re-centering the points.
",4.1. Exact Hyperbolic MDS,[0],[0]
"A word on centering The MDS algorithm in Euclidean geometry returns points centered at their Karcher mean z, which is a point minimizing ∑ d2(z, xi) (where d is the distance metric).",4.1. Exact Hyperbolic MDS,[0],[0]
"The Karcher center is important for interpreting dimensionality reduction; we use the analogous hyperbolic Karcher mean for PGA in Section 4.2.
",4.1. Exact Hyperbolic MDS,[0],[0]
"Although Algorithm 2 returns points centered at their pseudo-Euclidean mean instead of their Karcher mean, they can be easily recentered by finding their Karcher mean and
Algorithm 2 1: Input: Distance matrix di,j and rank r 2: Compute scaled distance matrix Yi,j = cosh(di,j) 3: X → PCA(−Y, r) 4: Project X from hyperboloid model to Poincaré model: x→ x
1+ √ 1+‖x‖2
5:",4.1. Exact Hyperbolic MDS,[0],[0]
"If desired, centerX at a different mean (e.g. the Karcher mean) 6: return X
reflecting it onto the origin.",4.1. Exact Hyperbolic MDS,[0],[0]
"Furthermore, Algorithm 2 preserves the dimension of the embedding:
Lemma 4.2.",4.1. Exact Hyperbolic MDS,[0],[0]
"If a set of points lie in a dimension-k geodesic submanifold, then both their Karcher mean and their pseudo-Euclidean mean lie in the same submanifold.
",4.1. Exact Hyperbolic MDS,[0],[0]
"This implies that centering with the pseudo-Euclidean mean preserves geodesic submanifolds: If it is possible to embed distances in a dimension-k geodesic submanifold centered and rooted at a Karcher mean, then it is also possible to embed the distances in a dimension-k submanifold centered and rooted at a pseudo-Euclidean mean, and vice versa.",4.1. Exact Hyperbolic MDS,[0],[0]
"Given a high-rank embedding (resulting from h-MDS, for example), we may wish to find a lower-rank version.",4.2. Reducing Dimensionality with PGA,[0],[0]
"In Euclidean space, one can get the optimal lower rank embedding by simply discarding components.",4.2. Reducing Dimensionality with PGA,[0],[0]
"However, this may not be the case in hyperbolic space.",4.2. Reducing Dimensionality with PGA,[0],[0]
"Motivated by this, we study dimensionality reduction in hyperbolic space.
",4.2. Reducing Dimensionality with PGA,[0],[0]
"As hyperbolic space does not have a linear subspace structure like Euclidean space, we need to define what we mean by lower-dimensional.",4.2. Reducing Dimensionality with PGA,[0],[0]
"We follow Principal Geodesic Analysis (Fletcher et al., 2004), (Huckemann et al., 2010).",4.2. Reducing Dimensionality with PGA,[0],[0]
"Consider an initial embedding with points x1, . . .",4.2. Reducing Dimensionality with PGA,[0],[0]
", xn ∈ H2 and let dH :",4.2. Reducing Dimensionality with PGA,[0],[0]
H2 × H2 → R+ be the hyperbolic distance.,4.2. Reducing Dimensionality with PGA,[0],[0]
Suppose we want to map this embedding onto a one-dimensional subspace.,4.2. Reducing Dimensionality with PGA,[0],[0]
"(Note that we are considering a two-dimensional embedding and one-dimensional subspace here for simplicity, and these results immediately extend to higher dimensions.)",4.2. Reducing Dimensionality with PGA,[0],[0]
"In this case, the goal of PGA is to find a geodesic γ :",4.2. Reducing Dimensionality with PGA,[0],[0]
"[0, 1] → H2 that passes through the mean of the points and that minimizes the squared error (or variance): f(γ) = ∑n i=1 mint∈[0,1] dH(γ(t), xi) 2.
",4.2. Reducing Dimensionality with PGA,[0],[0]
This expression can be simplified significantly and reduced to a minimization in Euclidean space.,4.2. Reducing Dimensionality with PGA,[0],[0]
"First, we find the mean of the points, the point x̄ which minimizes∑n i=1",4.2. Reducing Dimensionality with PGA,[0],[0]
"dH(x̄, xi)
2.4",4.2. Reducing Dimensionality with PGA,[0],[0]
"Next, we reflect all the points xi so that their mean is 0 in the Poincaré disk model; we can
4The derivative of the hyperbolic distance has a singularity, that is, limy→x ∂x|dH(x, y)| → ∞ for any x ∈ H.",4.2. Reducing Dimensionality with PGA,[0],[0]
"This issue can
do this using a circle inversion that maps x̄ onto 0",4.2. Reducing Dimensionality with PGA,[0],[0]
"Since reflections are isometric, if γ is a line through 0 and Rγ is the reflection across γ, we have that dH(γ, x) = mint∈[0,1] dH(γ(t), x) = 1 2dH(Rlx, x).
",4.2. Reducing Dimensionality with PGA,[0],[0]
"Combining this with the Euclidean reflection formula and the hyperbolic metric produces
f(γ) = 1
4 n∑ i=1 acosh2",4.2. Reducing Dimensionality with PGA,[0],[0]
"( 1 + 8dE(γ, xi) 2 (1− ‖xi‖2)2 ) ,
in which dE is the Euclidean distance from a point to a line.",4.2. Reducing Dimensionality with PGA,[0],[0]
If we define wi = √ 8xi/(1,4.2. Reducing Dimensionality with PGA,[0],[0]
− ‖xi‖2),4.2. Reducing Dimensionality with PGA,[0],[0]
this reduces to the simplified expression f(γ),4.2. Reducing Dimensionality with PGA,[0],[0]
"= 1 4 ∑n i=1 acosh 2 ( 1 + dE(γ,wi) 2 ) .
",4.2. Reducing Dimensionality with PGA,[0],[0]
Notice that the loss function is not convex.,4.2. Reducing Dimensionality with PGA,[0],[0]
"We observe that there can be multiple local minima that are attractive and stable, in contrast to PCA.",4.2. Reducing Dimensionality with PGA,[0],[0]
Figure 3 illustrates this nonconvexity on a simple dataset in H2 with only four examples.,4.2. Reducing Dimensionality with PGA,[0],[0]
"This makes globally optimizing the objective difficult.
",4.2. Reducing Dimensionality with PGA,[0],[0]
"Nevertheless, there will always be a region Ω containing a global optimum γ∗ that is convex and admits an efficient projection, and where f is convex when restricted to Ω.",4.2. Reducing Dimensionality with PGA,[0],[0]
"Thus it is possible to build a gradient descent-based algorithm to recover lower-dimensional subspaces: for example, we built a simple optimizer in PyTorch.",4.2. Reducing Dimensionality with PGA,[0],[0]
"We also give a sufficient condition on the data for f above to be convex.
",4.2. Reducing Dimensionality with PGA,[0],[0]
Lemma 4.3.,4.2. Reducing Dimensionality with PGA,[0],[0]
"For hyperbolic PGA if for all i,
acosh2 ( 1 + dE(γ,wi) 2 ) < min ( 1, 1
3 ‖wi‖2 ) then f is locally convex at γ.
be mitigated by minimizing d2H , which does have a continuous derivative throughout H. The use of dH(x, y) is a minor instability in Nickel & Kiela (2017); Chamberlain et al. (2017)’s formulation, necessitating guarding against NANs.",4.2. Reducing Dimensionality with PGA,[0],[0]
"We discuss this further in the appendix.
",4.2. Reducing Dimensionality with PGA,[0],[0]
"As a result, if we initialize in and optimize over a region that contains γ∗ and where the condition of Lemma 4.3 holds, then gradient descent will be guaranteed to converge to γ∗.",4.2. Reducing Dimensionality with PGA,[0],[0]
"We can turn this result around and read it as a recovery result: if the noise is bounded in this regime, then we are able to provably recover the correct low-dimensional embedding.",4.2. Reducing Dimensionality with PGA,[0],[0]
We evaluate the proposed approaches and compare against existing methods.,5. Experiments,[0],[0]
"We hypothesize that for tree-like data, the combinatorial construction offers the best performance.",5. Experiments,[0],[0]
"For general data, we expect h-MDS to produce the lowest distortion, while it may have low MAP due to precision limitations.",5. Experiments,[0],[0]
We anticipate that dimension is a critical factor (outside of the combinatorial construction).,5. Experiments,[0],[0]
"In the appendix, we report on additional datasets, combinatorial construction parameters, and the effect of hyperparameters.
",5. Experiments,[0],[0]
"Datasets We consider trees, tree-like hierarchies, and graphs that are not tree-like.",5. Experiments,[0],[0]
"Trees include fully-balanced and phylogenetic trees expressing genetic heritage (Hofbauer et al., 2016), available at Sanderson et al. (1994).",5. Experiments,[0],[0]
"Nearly tree-like hierarchies include the WordNet hypernym graph (the largest connected component from Nickel & Kiela (2017)) and a graph of Ph.D. advisor-advisee relationships (De Nooy et al., 2011).",5. Experiments,[0],[0]
"Also included are datasets
that vary in their tree nearness, such as disease relationships (Goh et al., 2007) and protein interactions (Jeong et al., 2001), both available from Rossi & Ahmed (2015).",5. Experiments,[0],[0]
"We also include the general relativity and quantum cosmology (GrQC) arXiv collaboration network (Leskovec et al., 2007).
",5. Experiments,[0],[0]
Approaches Combinatorial embeddings into H2 use the ε = 0.1 precision setting; others are considered in the Appendix.,5. Experiments,[0],[0]
We performed h-MDS in floating point precision.,5. Experiments,[0],[0]
"We include results for our PyTorch implementation (PT) of an SGD-based algorithm (described later), and a warm start version (PWS) initialized with the high-dimensional combinatorial construction.",5. Experiments,[0],[0]
"We compare against classical MDS (i.e., PCA), and the optimization-based approach Nickel & Kiela (2017), which we call FB.",5. Experiments,[0],[0]
"The experiments for h-MDS, PyTorch SGD, PCA, and FB used dimensions of 2,5,10,50,100,200; we recorded the best resulting MAP and distortion.",5. Experiments,[0],[0]
"Due to the large scale, we did not replicate the best FB numbers on large graphs (i.e., Gr-QC and WordNet); we report their best published MAP numbers (their work does not report distortion).",5. Experiments,[0],[0]
These entries are marked with an asterisk.,5. Experiments,[0],[0]
"For the WordNet graph, FB uses the transitive closure; a weighted version of the graph captures the ancestor relationships.",5. Experiments,[0],[0]
"The full details are in appendix.
",5. Experiments,[0],[0]
"Quality In Table 3 (left), we report the distortion.",5. Experiments,[0],[0]
"As expected, for tree or tree-like graphs, the combinatorial construction has exceedingly low distortion.",5. Experiments,[0],[0]
"Because h-MDS is meant to recover points exactly, we hypothesized that h-MDS would offer very low distortion on these datasets.",5. Experiments,[0],[0]
"Table 3 confirms this: among h-MDS, PCA, and FB, hMDS consistently offers the lowest distortion, producing, for example, a distortion of 0.039 on the phylogenetic tree.",5. Experiments,[0],[0]
We observe that floating point h-MDS struggles with MAP.,5. Experiments,[0],[0]
"We separately confirmed that this is due to precision (by
using a high-precision solver).",5. Experiments,[0],[0]
"The optimization-based approach is bolstered by appropriate initialization from the combinatorial construction.
",5. Experiments,[0],[0]
"Table 3 (right) reports the MAP measure (we additionally include WordNet results in Table 2), which is a local measure.",5. Experiments,[0],[0]
"We confirm that the combinatorial construction performs well for tree-like hierarchies, where MAP is close to 1.",5. Experiments,[0],[0]
The construction improves on approaches such as FB that rely on optimization.,5. Experiments,[0],[0]
"On larger graphs like WordNet, our approach yields a MAP of 0.989—while their WordNet MAP result is 0.870 at 200 dimensions.",5. Experiments,[0],[0]
"This is exciting, as our approach is deterministic and linear-time.
",5. Experiments,[0],[0]
A refined understanding of hyperbolic embeddings may be used to improve the quality and runtime of extant algorithms.,5. Experiments,[0],[0]
"Indeed, we embedded WordNet entity-relationship-entity triples (Socher et al., 2013) using the combinatorial construction in 10 dimensions, accurately preserving relationship knowledge (Table 4).",5. Experiments,[0],[0]
"This suggests that hyperbolic embeddings are effective at compressing knowledge and may useful for knowledge base completion and Q/A tasks.
SGD-Based Algorithm We built an SGD-based algorithm implemented in PyTorch.",5. Experiments,[0],[0]
"The loss function is equivalent to the PGA loss, and so is continuously differentiable.
",5. Experiments,[0],[0]
"To evaluate our algorithm’s ability to deal with incomplete information, we sample the distance matrix at a ratio of nonedges to edges at 10 : 1 following Nickel & Kiela (2017).",5. Experiments,[0],[0]
"In Figure 4, we recover a good solution for the phylogenetic tree with a small fraction of the entries; for example, we sampled approximately 4% of the graph for a MAP of 0.74 and distortion of 0.6.",5. Experiments,[0],[0]
We also considered learning the scale of the embedding (details in the appendix).,5. Experiments,[0],[0]
"Finally, all of our techniques scale to graphs with millions of nodes.",5. Experiments,[0],[0]
Hyperbolic embeddings embed hierarchical information with high fidelity and few dimensions.,6. Conclusion and Future Work,[0],[0]
"We explored the limits of this approach by describing scalable, high quality algorithms.",6. Conclusion and Future Work,[0],[0]
We hope the techniques here encourage more follow-on work on the exciting techniques of Nickel & Kiela (2017); Chamberlain et al. (2017).,6. Conclusion and Future Work,[0],[0]
Thanks to Alex Ratner and Avner May for helpful discussion and to Beliz Gunel and Sen Wu for assistance with experiments.,Acknowledgements,[0],[0]
"We gratefully acknowledge the support of DARPA under No. FA87501720095 and FA87501320039, ONR under No. N000141712266, the Moore Foundation, Okawa Research Grant, American Family Insurance, Accenture, Toshiba, the Secure Internet of Things Project, Google, VMware, Qualcomm, Ericsson, Analog Devices, and members of the Stanford DAWN project: Intel, Microsoft, Teradata, and VMware.",Acknowledgements,[0],[0]
The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon.,Acknowledgements,[0],[0]
"Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of DARPA, DOE, NIH, ONR, or the U.S. Government.",Acknowledgements,[0],[0]
Hyperbolic embeddings offer excellent quality with few dimensions when embedding hierarchical data structures.,abstractText,[0],[0]
We give a combinatorial construction that embeds trees into hyperbolic space with arbitrarily low distortion without optimization.,abstractText,[0],[0]
"On WordNet, this algorithm obtains a meanaverage-precision of 0.989 with only two dimensions, outperforming existing work by 0.11 points.",abstractText,[0],[0]
We provide bounds characterizing the precisiondimensionality tradeoff inherent in any hyperbolic embedding.,abstractText,[0],[0]
"To embed general metric spaces, we propose a hyperbolic generalization of multidimensional scaling (h-MDS).",abstractText,[0],[0]
"We show how to perform exact recovery of hyperbolic points from distances, provide a perturbation analysis, and give a recovery result that enables us to reduce dimensionality.",abstractText,[0],[0]
"Finally, we extract lessons from the algorithms and theory above to design a scalable PyTorch-based implementation that can handle incomplete information.",abstractText,[0],[0]
Representation Tradeoffs for Hyperbolic Embeddings,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 613–622 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1057",text,[0],[0]
Speech recognition is one of the success stories of language technology.,1 Introduction,[0],[0]
It works remarkably well in a range of practical settings.,1 Introduction,[0],[0]
"However, this success relies on the use of very heavy supervision where the machine is fed thousands of hours of painstakingly transcribed audio speech signal.",1 Introduction,[0],[0]
Humans are able to learn to recognize and understand speech from notably weaker and noisier supervision: they manage to learn to extract structure and meaning from speech by simply being exposed to utterances situated and grounded in their daily sensory experience.,1 Introduction,[0],[0]
"Modeling and emulating this remarkable skill has been the goal of numerous studies; however in the overwhelming majority of cases researchers used severely simplified settings where either the language input or the extralinguistic sensory input, or both, are small scale and symbolically represented.",1 Introduction,[0],[0]
"Section 2 provides a brief overview of this research.
",1 Introduction,[0],[0]
More recently several lines of work have moved towards more realistic inputs while modeling or emulating language acquisition in a grounded setting.,1 Introduction,[0],[0]
"Gelderloos and Chrupała (2016) use the image captioning dataset MS COCO (Lin et al., 2014) to mimic the setting of grounded language learning: the sensory input consists of images of natural scenes, while the language input are phonetically transcribed descriptions of these scenes.",1 Introduction,[0],[0]
"The use of such moderately large and low-level data allows the authors to train a multi-layer recurrent neural network model, and to explore the nature and localization of the emerging hierarchy of linguistic representations learned in the process.",1 Introduction,[0],[0]
"Furthermore, in a series of recent studies Harwath and Glass (2015); Harwath et al. (2016); Harwath and Glass (2017) use image captioning datasets to model learning to understand spoken language from visual context with convolutional neural network models.",1 Introduction,[0],[0]
"Finally, there is a small but growing body of work dedicated to elucidating the nature of representations learned by neural networks from language data (see Section 2.2 for a brief overview).",1 Introduction,[0],[0]
"In the current work we build on these three strands of research and contribute the following advances:
• We use a multi-layer gated recurrent neural network to properly model the temporal nature of speech signal and substantially improve performance compared to the convolutional architecture from Harwath and Glass (2015); • We carry out an in-depth analysis of the representations used by different components of the trained model and correlate them to representations learned by a text-based model and to human patterns of judgment on linguistic stimuli.",1 Introduction,[0],[0]
"This analysis is especially novel for a model with speech signal as input.
",1 Introduction,[0],[0]
"The general pattern of findings in our analysis is
613
as follows:",1 Introduction,[0],[0]
"The model learns to extract from the acoustic input both form-related and semanticsrelated information, and encodes it in the activations of the hidden layers.",1 Introduction,[0],[0]
Encoding of semantic aspects tends to become richer as we go up the hierarchy of layers.,1 Introduction,[0],[0]
"Meanwhile, encoding of formrelated aspects of the language input, such as utterance length or the presence of specific words, tends to initially increase and then decay.
",1 Introduction,[0],[0]
"We release the code for our models and analyses as open source, available at https://github.com/gchrupala/visually-groundedspeech.",1 Introduction,[0],[0]
"We also release a dataset of synthetically spoken image captions based on MS COCO, available at https://doi.org/10.5281/zenodo.400926.",1 Introduction,[0],[0]
Children learn to recognize and assign meaning to words from continuous perceptual data in extremely noisy context.,2 Related work,[0],[0]
"While there have been many computational studies of human word meaning acquisition, they typically make strong simplifying assumptions about the nature of the input.",2 Related work,[0],[0]
"Often language input is given in the form of word symbols, and the context consists of a set of symbols representing possible referents (e.g. Siskind, 1996; Frank et al., 2007; Fazly et al., 2010).",2 Related work,[0],[0]
"In contrast, several studies presented models that learn from sensory rather than symbolic input, which is rich with regards to the signal itself, but very limited in scale and variation (e.g. Roy and Pentland, 2002; Yu and Ballard, 2004; Lazaridou et al., 2016).",2 Related work,[0],[0]
Chrupała et al. (2015) introduce a model that learns to predict the visual context from image captions.,2.1 Multimodal language acquisition,[0],[0]
"The model is trained on image-caption pairs from MSCOCO (Lin et al., 2014), capturing both rich visual input as well as larger scale input, but the language input still consists of word symbols.",2.1 Multimodal language acquisition,[0],[0]
"Gelderloos and Chrupała (2016) propose a similar architecture that instead takes phonemelevel transcriptions as language input, thereby incorporating the word segmentation problem into the learning task.",2.1 Multimodal language acquisition,[0],[0]
"In this work, we introduce an architecture that learns from continuous speech and images directly.
",2.1 Multimodal language acquisition,[0],[0]
This work is related to research on visual grounding of language.,2.1 Multimodal language acquisition,[0],[0]
"The field is large and growing, with most work dedicated to the ground-
ing of written text, particularly in image captioning tasks (see Bernardi et al. (2016) for an overview).",2.1 Multimodal language acquisition,[0],[0]
"However, learning to ground language to visual information is also interesting from an automatic speech recognition point of view.",2.1 Multimodal language acquisition,[0],[0]
"Potentially, ASR systems could be trained from naturally co-occurring visual context information, without the need for extensive manual annotation – a particularly promising prospect for speech recognition in low-resource languages.",2.1 Multimodal language acquisition,[0],[0]
There have been several attempts along these lines.,2.1 Multimodal language acquisition,[0],[0]
Synnaeve et al. (2014) present a method of learning to recognize spoken words in isolation from cooccurrence with image fragments.,2.1 Multimodal language acquisition,[0],[0]
"Harwath and Glass (2015) present a model that learns to map pre-segmented spoken words in sequence to aspects of the visual context, while in Harwath and Glass (2017)",2.1 Multimodal language acquisition,[0],[0]
"the model also learns to recognize words in the unsegmented signal.
",2.1 Multimodal language acquisition,[0],[0]
"Most closely related to our work is that of Harwath et al. (2016), as it presents an architecture that learns to project images and unsegmented spoken captions to the same embedding space.",2.1 Multimodal language acquisition,[0],[0]
The sentence representation is obtained by feeding the spectrogram to a convolutional network.,2.1 Multimodal language acquisition,[0],[0]
"The architecture is trained on crowd-sourced spoken captions for images from the Places dataset (Zhou et al., 2014), and evaluated on image search and caption retrieval.",2.1 Multimodal language acquisition,[0],[0]
Unfortunately this dataset is not currently available and we were thus unable to directly compare the performance of our model to Harwath et al. (2016).,2.1 Multimodal language acquisition,[0],[0]
We do compare to Harwath and Glass (2015) which was tested on a public dataset.,2.1 Multimodal language acquisition,[0],[0]
"We make different architectural choices, as our models are based on recurrent highway networks (Zilly et al., 2016).",2.1 Multimodal language acquisition,[0],[0]
"As in human cognition, speech is processed incrementally.",2.1 Multimodal language acquisition,[0],[0]
This also allows our architecture to integrate information sequentially from speech of arbitrary duration.,2.1 Multimodal language acquisition,[0],[0]
"While analysis of neural methods in NLP is often limited to evaluation of the performance on the training task, recently methods have been introduced to peek inside the black box and explore what it is that enables the model to perform the task.",2.2 Analysis of neural representations,[0],[0]
"One approach is to look at the contribution of specific parts of the input, or specific units in the model, to final representations or decisions.",2.2 Analysis of neural representations,[0],[0]
"Kádár et al. (2016) propose omission scores, a method to estimate the contribution of input tokens to the fi-
nal representation by removing them from the input and comparing the resulting representations to the ones generated by the original input.",2.2 Analysis of neural representations,[0],[0]
"In a similar approach, Li et al. (2016) study the contribution of individual input tokens as well as hidden units and word embedding dimensions by erasing them from the representation and analyzing how this affects the model.
",2.2 Analysis of neural representations,[0],[0]
Miao et al. (2016) and Tang et al. (2016) use visualization techniques for fine-grained analysis of GRU and LSTM models for ASR.,2.2 Analysis of neural representations,[0],[0]
"Visualization of input and forget gate states allows Miao et al. (2016) to make informed adaptations to gated recurrent architectures, resulting in more efficiently trainable models.",2.2 Analysis of neural representations,[0],[0]
"Tang et al. (2016) visualize qualitative differences between LSTM- and GRUbased architectures, regarding the encoding of information, as well as how it is processed through time.
",2.2 Analysis of neural representations,[0],[0]
We specifically study linguistic properties of the information encoded in the trained model.,2.2 Analysis of neural representations,[0],[0]
"Adi et al. (2016) introduce prediction tasks to analyze information encoded in sentence embeddings about word order, sentence length, and the presence of individual words.",2.2 Analysis of neural representations,[0],[0]
We use related techniques to explore encoding of aspects of form and meaning within components of our stacked architecture.,2.2 Analysis of neural representations,[0],[0]
"We use a multi-layer, gated recurrent neural network (RHN) to model the temporal nature of speech signal.",3 Models,[0],[0]
"Recurrent neural networks are designed for modeling sequential data, and gated variants (GRUs, LSTMs) are widely used with speech and text in both cognitive modeling and engineering contexts.",3 Models,[0],[0]
"RHNs are a simple generalization of GRU networks such that the transform between time points can consist of several steps.
",3 Models,[0],[0]
Our multimodal model projects spoken utterances and images to a joint semantic space.,3 Models,[0],[0]
The idea of projecting different modalities to a shared semantic space via a pair of encoders has been used in work on language and vision (among them Vendrov et al. (2015)).,3 Models,[0],[0]
"The core idea is to encourage inputs representing the same meaning in different modalities to end up nearby, while maintaining a distance from unrelated inputs.
",3 Models,[0],[0]
"The model consists of two parts: an utterance encoder, and an image encoder.",3 Models,[0],[0]
"The utterance encoder starts from MFCC speech features, while
the image encoder starts from features extracted with a VGG-16 pre-trained on ImageNet.",3 Models,[0],[0]
"Our loss function attempts to make the cosine distance between encodings of matching utterances and images greater than the distance between encodings of mismatching utterance/image pairs, by a margin:
(1)
∑
u,i
(∑
u′ max[0, α+d(u, i)−d(u′, i)]
+ ∑
i′ max[0, α+ d(u, i)− d(u, i′)]
)
where d(u, i) is the cosine distance between the encoded utterance u and encoded image i. Here (u, i) is the matching utterance-image pair, u′ ranges over utterances not describing i and i′ ranges over images not described by u.",3 Models,[0],[0]
"The image encoder enci is a simple linear projection, followed by normalization to unit L2 norm:
enci(i) = unit(Ai+ b) (2)
where unit(x) = x (xT x)0.5 and with (A, b) as learned parameters.",3 Models,[0],[0]
"The utterance encoder encu consists of a 1-dimensional convolutional layer of length s, size d and stride z, whose output feeds into a Recurrent Highway Network with k layers and L microsteps, whose output in turn goes through an attention-like lookback operator, and finally L2 normalization:
encu(u) = unit(Attn(RHNk,L(Convs,d,z(u))))",3 Models,[0],[0]
"(3)
The main function of the convolutional layer Convs,d,z is to subsample the input along the temporal dimension.",3 Models,[0],[0]
We use a 1-dimensional convolution with full border mode padding.,3 Models,[0],[0]
"The attention operator simply computes a weighted sum of the RHN activation at all timesteps:
Attn(x) = ∑
t
αtxt (4)
where the weights αt are determined by learned parameters U and W, and passed through the timewise softmax function:
αt = exp(U tanh(Wxt))∑ t′ exp(U tanh(Wxt′))
(5)
",3 Models,[0],[0]
"The main component of the utterance encoder is a recurrent network, specifically a Recurrent Highway Network (Zilly et al., 2016).",3 Models,[0],[0]
"The idea behind
RHN is to increase the depth of the transform between timesteps, or the recurrence depth.",3 Models,[0],[0]
Otherwise they are a type of gated recurrent networks.,3 Models,[0],[0]
"The transition from timestep t − 1 to t is then defined as:
rhn(xt, s (L) t−1) = s (L) t (6)
where xt stands for input at time t, and s (l) t denotes the state at time t at recurrence layer l, with L being the top layer of recurrence.",3 Models,[0],[0]
"Furthermore,
s (l) t = h (l) t t (l) t + s (l−1) t ( 1− t(l)t ) (7)
where is elementwise multiplication, and
h (l) t = tanh ( I[l = 1]WHxt +UHls (l−1) t ) (8)
t (l) t = σ",3 Models,[0],[0]
"( I[l = 1]WTxt +UTls (l−1) )
(9)
",3 Models,[0],[0]
Here I is the indicator function: input is only included in the computation for the first layer of recurrence l = 1.,3 Models,[0],[0]
"By applying the rhn function repeatedly, an RHN layer maps a sequence of inputs to a sequence of states:
(10) RHN(X, s0)
= rhn(xn, . . .",3 Models,[0],[0]
", rhn(x2, rhn(x1, s (L) 0 )))
",3 Models,[0],[0]
"Two or more RHN layers can be composed into a stack:
RHN2(RHN1(X, s1 (L) 0 ), s2 (L) 0 ), (11)
where sn (l) t stands for the state vector of layer n of the stack, at layer l of recurrence, at time t.",3 Models,[0],[0]
"In our version of the Stacked RHN architecture we use residualized layers:
RHNres(X, s0) = RHN(X, s0) +X (12)
",3 Models,[0],[0]
This formulation tends to ease optimization in multi-layer models (cf.,3 Models,[0],[0]
"He et al., 2015; Oord et al., 2016).
",3 Models,[0],[0]
"In addition to the speech model described above, we also define a comparable text model.",3 Models,[0],[0]
"As it takes a sequence of words as input, we replace the convolutional layer with a word embedding lookup table.",3 Models,[0],[0]
"We found the text model did not benefit from the use of the attention mechanism, and thus the sentence embedding is simply the L2-normalized activation vector of the topmost layer, at the last timestep.",3 Models,[0],[0]
Our main goal is to analyze the emerging representations from different components of the model and to examine the linguistic knowledge they encode.,4 Experiments,[0],[0]
"For this purpose, we employ a number of tasks that cover the spectrum from fully formbased to fully semantic.
",4 Experiments,[0],[0]
In Section 4.2 we assess the effectiveness of our architecture by evaluating it on the task of ranking images given an utterance.,4 Experiments,[0],[0]
Sections 4.3 to 4.6 present our analyses.,4 Experiments,[0],[0]
In Sections 4.3 and 4.4 we define auxiliary tasks to investigate to what extent the network encodes information about the surface form of an utterance from the speech input.,4 Experiments,[0],[0]
In Section 4.5 and 4.6 we focus on where semantic information is encoded in the model.,4 Experiments,[0],[0]
"In the analyses, we use the following features: Utterance embeddings: the weighted sum of the
unit activations on the last layer, as calculated by Equation (3).
",4 Experiments,[0],[0]
Average unit activations: hidden layer activations averaged over time and L2-normalized for each hidden layer.,4 Experiments,[0],[0]
Average input vectors: the MFCC vectors averaged over time.,4 Experiments,[0],[0]
We use this feature to examine how much information can be extracted from the input signal only.,4 Experiments,[0],[0]
For the experiments reported in the remainder of the paper we use two datasets of images with spoken captions.,4.1 Data,[0],[0]
"The Flickr8k Audio Caption Corpus was constructed by having crowdsource workers read aloud the captions in the original Flickr8K corpus (Hodosh et al., 2013).",4.1.1 Flickr8K,[0],[0]
For details of the data collection procedure refer to Harwath and Glass (2015).,4.1.1 Flickr8K,[0],[0]
"The datasets consist of 8,000 images, each image with five descriptions.",4.1.1 Flickr8K,[0],[0]
"One thousand images are held out for validation, and another one thousand for the final test set.",4.1.1 Flickr8K,[0],[0]
"We use the splits provided by (Karpathy and Fei-Fei, 2015).",4.1.1 Flickr8K,[0],[0]
"The image features come from the final fully connect layer of VGG-16 (Simonyan and Zisserman, 2014) pre-trained on Imagenet (Russakovsky et al., 2014).
",4.1.1 Flickr8K,[0],[0]
We generate the input signal as follows: we extract 12-dimensional mel-frequency cepstral coefficients (MFCC) plus log of the total energy.,4.1.1 Flickr8K,[0],[0]
"We
then compute and add first order and second order differences (deltas) for a total of 37 dimensions.",4.1.1 Flickr8K,[0],[0]
"We use 25 milisecond windows, sampled every 10 miliseconds.1",4.1.1 Flickr8K,[0],[0]
"We generated synthetic speech for the captions in the MS COCO dataset (Lin et al., 2014) via the Google Text-to-Speech API.2 The audio and the corresponding MFCC features are released as Chrupała et al. (2017)3.",4.1.2 Synthetically spoken COCO,[0],[0]
This TTS system we used produces high-quality realistic-sounding speech.,4.1.2 Synthetically spoken COCO,[0],[0]
"It is nevertheless much simpler than real human speech as it uses a single voice, and lacks tempo variation or ambient noise.",4.1.2 Synthetically spoken COCO,[0],[0]
"The data consists of over 300,000 images, each with five spoken captions.",4.1.2 Synthetically spoken COCO,[0],[0]
Five thousand images each are held out for validation and test.,4.1.2 Synthetically spoken COCO,[0],[0]
"We use the splits and image features provided by Vendrov et al. (2015).4 The image features also come from the VGG-16 network, but are averages of feature vectors for ten crops of each image.",4.1.2 Synthetically spoken COCO,[0],[0]
"For the MS COCO captions we extracted only plain MFCC and total energy features, and did not add deltas in order to keep the amount of computation manageable given the size of the dataset.",4.1.2 Synthetically spoken COCO,[0],[0]
"We evaluate our model on the task of ranking images given a spoken utterance, such that highly ranked images contain scenes described by the utterance.",4.2 Image retrieval,[0],[0]
The performance on this task on validation data is also used to choose the best variant of the model architecture and to tune the hyperparameters.,4.2 Image retrieval,[0],[0]
We compare the speech models to models trained on written sentences split into words.,4.2 Image retrieval,[0],[0]
"The best settings found for the four models were the following: Flickr8K Text RHN 300-dimensional word em-
beddings, 1 hidden layer with 1024 dimensions, 1 microstep, initial learning rate 0.001.
",4.2 Image retrieval,[0],[0]
"Flick8K Speech RHN convolutional layer with length 6, size 64, stride 2, 4 hidden layers with 1024 dimensions, 2 microsteps, atten-
1We noticed that for a number of utterances the audio signal was very long: on inspection it turned out that most of these involved failure to switch off the microphone on the part of the workers, and the audio contained ambient noise or unrelated speech.",4.2 Image retrieval,[0],[0]
"We thus trucated all audio for this dataset at 10,000 miliseconds.
2Available at https://github.com/pndurette/gTTS.",4.2 Image retrieval,[0],[0]
3Available at https://doi.org/10.5281/zenodo.400926.,4.2 Image retrieval,[0],[0]
"4See https://github.com/ivendrov/order-embedding.
tion MLP with 128 hidden units, initial learning rate 0.0002
COCO Text RHN 300-dimensional word embeddings, 1 hidden layer with 1024 dimensions, 1 microstep, initial learning rate 0.001 COCO Speech RHN convolutional layer with length 6, size 64, stride 3, 5 hidden layers with 512 dimensions, 2 microsteps, attention MLP with 512 hidden units, initial learning rate 0.0002
All models were optimized with Adam (Kingma and Ba, 2014) with early stopping: we kept the parameters for the epoch which showed the best recall@10 on validation data.
",4.2 Image retrieval,[0],[0]
Table 1 shows the results for the human speech from the Flickr8K dataset.,4.2 Image retrieval,[0],[0]
The Speech RHN model scores substantially higher than model of Harwath and Glass (2015) on the same data.,4.2 Image retrieval,[0],[0]
However the large gap between its perfomance and the scores of the text model suggests that Flickr8K is rather small for the speech task.,4.2 Image retrieval,[0],[0]
In Table 2 we present the results on the dataset of synthetic speech from MS COCO.,4.2 Image retrieval,[0],[0]
"Here the text model is still better, but the gap is much smaller than for Flickr8K. We attribute this to the much larger size of dataset, and to the less noisy and less variable synthetic speech.
",4.2 Image retrieval,[0],[0]
"While the MS COCO text model is overall better than the speech model, there are cases where it outperforms the text model.",4.2 Image retrieval,[0],[0]
"We listed the top hundred cases where the ratio of the ranks of the correct image according to the two models was the smallest, as well as another hundred cases where it was the largest.",4.2 Image retrieval,[0],[0]
"Manual inspection did not turn
up any obvious patterns for the cases of text being better than speech.",4.2 Image retrieval,[0],[0]
"For the cases where speech outperformed text, two patterns stood out: (i) sentences with spelling mistakes, (ii) unusually long sentences.",4.2 Image retrieval,[0],[0]
"For example for the sentence a yellow
and white birtd is in flight the text model misses the misspelled word birtd and returns an irrelevant image, while the speech model seems robust to some degree of variation in pronunciation and returns the target image at rank 1 (see Figure 1).",4.2 Image retrieval,[0],[0]
"In an attempt to quantify this effect we counted the number of unique words with training set frequencies below 5 in the top 100 utterances with lowest and highest rank ratio: for the utterances where text was better there were 16 such words; for utterances where speech was better there were 28, among them misspellings such as streeet, scears (for skiers), contryside, scull, birtd, devise.
",4.2 Image retrieval,[0],[0]
The distribution of utterance lengths in Figure 2 confirms pattern (ii): the set of 100 sentences where speech beats text by a large margin are longer on average and there are extremely long outliers among them.,4.2 Image retrieval,[0],[0]
"One of them is the 36-word-
long utterance depicted in Figure 3, with ranks 470 and 2 for text and speech respectively.",4.2 Image retrieval,[0],[0]
"We suspect that the speech model’s attention mechanism enables it to cherry pick key fragments of such monster utterances, while the text model lacking this mechanism may struggle.",4.2 Image retrieval,[0],[0]
"Figure 3 shows the plot of the attention weights for this utterance from the
speech model.",4.2 Image retrieval,[0],[0]
"Our first auxiliary task is to predict the length of the utterance, using the features explained at the beginning of Section 4.",4.3 Predicting utterance length,[0],[0]
"Since the length of an utterance directly corresponds to how long it takes to articulate, we also use the number of time steps5 as a feature and expect it to provide the upper bound for our task, especially for synthetic speech.",4.3 Predicting utterance length,[0],[0]
We use a Ridge Regression model for predicting utterance length using each set of features.,4.3 Predicting utterance length,[0],[0]
"The model is trained on 80% of the sentences in the validation set, and tested on the remaining 20%.",4.3 Predicting utterance length,[0],[0]
"For all features regularization penalty α = 1.0 gave the best results.
",4.3 Predicting utterance length,[0],[0]
Figure 4 shows the results for this task on human speech from Flickr8K and synthetic speech from COCO.,4.3 Predicting utterance length,[0],[0]
"With the exception of the average input vectors for Flickr8K, all features can explain a high proportion of variance in the predicted utterance length.",4.3 Predicting utterance length,[0],[0]
"The pattern observed for the two datasets is slightly different: due to the systematic conversion of words to synthetic speech in COCO, using the number of time steps for this dataset yields the highest R2.",4.3 Predicting utterance length,[0],[0]
"However, this feature is not as informative for predicting the utterance length in Flickr8K due to noise and variation in human speech, and is in fact outperformed by some of the features extracted from the model.",4.3 Predicting utterance length,[0],[0]
"Also, the input vectors from COCO are much more informative than Flickr8K due to larger quantity and simpler structure of the speech signal.",4.3 Predicting utterance length,[0],[0]
"However, in both datasets the best (non-ceiling) performance is obtained by using average unit activations from the hidden layers (layer 2 for COCO, and layers 3 and 4 for Flickr8K).",4.3 Predicting utterance length,[0],[0]
"These features outperform utterance embeddings, which are optimized according to the visual grounding objective of the model and most probably learn to ignore the superficial characteristics of the utterance that do not contribute to matching the corresponding image.
",4.3 Predicting utterance length,[0],[0]
"Note that the performance on COCO plateaus after the second layer, which might suggest that form-based knowledge is learned by lower layers.",4.3 Predicting utterance length,[0],[0]
"Since Flickr8K is much smaller in size, the stabilising happens later in layer 3.
",4.3 Predicting utterance length,[0],[0]
5This is approximately duration in milliseconds 10×stride .,4.3 Predicting utterance length,[0],[0]
Results from the previous experiment suggest that our model acquires information about higher level building blocks (words) in the continuous speech signal.,4.4 Predicting word presence,[0],[0]
Here we explore whether it can detect the presence or absence of individual words in an utterance.,4.4 Predicting word presence,[0],[0]
"We formulate detecting a word in an utterance as a binary classification task, for which we use a multi-layer perceptron with a single hidden layer of size 1024, optimized by Adam.",4.4 Predicting word presence,[0],[0]
The input to the model is a concatenation of the feature vector representing an utterance and the one representing a target word.,4.4 Predicting word presence,[0],[0]
"We again use utterance embeddings, average unit activations on each layer, and average input vectors as features, and represent each target word as a vector of MFCC features extracted from the audio signal synthetically produced for that word.
",4.4 Predicting word presence,[0],[0]
"For each utterance in the validation set, we randomly pick one positive and one negative target (i.e., one word that does and one that does not appear in the utterance) that is not a stop word.",4.4 Predicting word presence,[0],[0]
"To balance the probability of a word being positive or negative, we use each positive target as a negative target for another utterance in the validation
set.",4.4 Predicting word presence,[0],[0]
"The MLP model is trained on the positive and negative examples corresponding to 80% of the utterances in the validation set of each dataset, and evaluated on the remaining 20%.
",4.4 Predicting word presence,[0],[0]
Figure 5 shows the mean accuracy of the MLP on Flickr8K and COCO.,4.4 Predicting word presence,[0],[0]
"All results using features extracted from the model are above chance (0.5), with the average unit activations of the hidden layers yielding the best results (0.65 for Flickr8K on layer 3, and 0.79 for COCO on layer 4).",4.4 Predicting word presence,[0],[0]
These numbers show that the speech model infers reliable information about word-level blocks from the low-level audio features it receives as input.,4.4 Predicting word presence,[0],[0]
"The observed trend is similar to the previous task: average unit activations on the higher-level hidden layers are more informative for this task than the utterance embeddings, but the performance plateaus before the topmost layer.",4.4 Predicting word presence,[0],[0]
Next we explore to what extent the model’s representations correspond to those of humans.,4.5 Sentence similarity,[0],[0]
"We employ the Sentences Involving Compositional Knowledge (SICK) dataset (Marelli et al., 2014).",4.5 Sentence similarity,[0],[0]
"SICK consists of image descriptions taken from
Flickr8K and video captions from the SemEval 2012 STS MSRVideo Description data set (STS) (Agirre et al., 2012).",4.5 Sentence similarity,[0],[0]
"Captions were paired at random, as well as modified to obtain semantically similar and contrasting counterparts, and the resulting pairs were rated for semantic similarity.
",4.5 Sentence similarity,[0],[0]
"For all sentence pairs in SICK, we generate synthetic spoken sentences and feed them to the COCO Speech RHN, and calculate the cosine similarity between the averaged MFCC input vectors, the averaged hidden layer activation vectors, and the sentence embeddings.",4.5 Sentence similarity,[0.9508924849498256],"['Then, for each phrase p ∈ P , we calculate its cosine distance from all relation aliases inR and take the relation corresponding to the closest relation alias as a matched relation for the sentence.']"
Z-score transformation was applied before calculating the cosine similarities.,4.5 Sentence similarity,[0],[0]
"We then correlate these cosine similarities with
• semantic relatedness according to human ratings • cosine similarities according to z-score transformed embeddings from COCO Text RHN • edit similarities, a measure of how similar the sentences are in form, specifically, 1−normalized Levenshtein distance over character sequences
Figure 6 shows a boxplot over 10,000 bootstrap samples for all correlations.",4.5 Sentence similarity,[0],[0]
"We observe that (i) correlation with edit similarity initially increases, then decreases; (ii) correlation with human relatedness scores and text model embeddings increases until layer 4, but decreases for hidden layer 5.",4.5 Sentence similarity,[0],[0]
The initially increasing and then decreasing correlation with edit similarity is consistent with the findings that information about form is encoded by lower layers.,4.5 Sentence similarity,[0],[0]
"The overall growing correlation with both human semantic similarity ratings and
the COCO Text RHN indicate that higher layers learn to represent semantic knowledge.",4.5 Sentence similarity,[0],[0]
We were somewhat surprised by the pattern for the correlation with human ratings and the Text model similarities which drops for layer 5.,4.5 Sentence similarity,[0],[0]
We suspect it may be caused by the model at this point in the layer hierarchy being strongly tuned to the specifics of the COCO dataset.,4.5 Sentence similarity,[0],[0]
"To test this, we checked the correlations with COCO Text embeddings on validation sentences from the COCO dataset instead of SICK.",4.5 Sentence similarity,[0],[0]
"These increased monotonically, in support of our conjecture.",4.5 Sentence similarity,[0],[0]
"Next we simulate the task of distinguishing between pairs of homonyms, i.e. words with the same acoustic form but different meaning.",4.6 Homonym disambiguation,[0],[0]
We group the words in the union of the training and validation data of the COCO dataset by their phonetic transcription.,4.6 Homonym disambiguation,[0],[0]
"We then pick pairs of words which have the same pronunciation but different spelling, for example suite/sweet.",4.6 Homonym disambiguation,[0],[0]
"We impose the following conditions: (a) both forms appear more than 20 times, (b) the two forms have different meaning (i.e. they are not simply variant spellings like theater/theatre), (c) neither form is a function word, and (d) the more frequent form constitutes less than 95% of the occurrences.",4.6 Homonym disambiguation,[0],[0]
"This
gives us 34 word pairs.",4.6 Homonym disambiguation,[0],[0]
"For each pair we generate a binary classification task by taking all the utterances where either form appears, using average input vectors, utterance embeddings, and average unit activations as features.",4.6 Homonym disambiguation,[0],[0]
"Instances for all feature sets are normalized to unit L2 norm.
",4.6 Homonym disambiguation,[0],[0]
For each task and feature set we run stratified 10-fold cross validation using Logistic Regression to predict which of the two words the utterance contains.,4.6 Homonym disambiguation,[0],[0]
"Figure 7 shows, for each pair, the relative error reduction of each feature set with respect to the majority baseline.",4.6 Homonym disambiguation,[0],[0]
"There is substantial variation across word pairs, but overall the task becomes easier as the features come from higher layers in the network.",4.6 Homonym disambiguation,[0],[0]
"Some forms can be disambiguated with very high accuracy (e.g. sale/sail, cole/coal, pairs/pears), while some others cannot be distinguished at all (peaking/peeking, great/grate, mantle/mantel).",4.6 Homonym disambiguation,[0],[0]
"We examined the sentences containing the failing forms, and found out that almost all occurrences of peaking and mantle were misspellings of peeking and mantel, which explains the impossibility of disambiguating these cases.",4.6 Homonym disambiguation,[0],[0]
We present a multi-layer recurrent highway network model of language acquisition from visually grounded speech signal.,5 Conclusion,[0],[0]
"Through detailed analysis we uncover how information in the input signal is transformed as it flows through the network: formal aspects of language such as word identities that not directly present in the input are discovered and encoded low in the layer hierarchy, while semantic information is most strongly expressed in the topmost layers.
",5 Conclusion,[0],[0]
Going forward we would like to compare the representations learned by our model to the brain activity of people listening to speech in order to determine to what extent the patterns we found correspond to localized processing in the human cortex.,5 Conclusion,[0],[0]
This will hopefully lead to a better understanding of language learning and processing by both artificial and neural networks.,5 Conclusion,[0],[0]
We would like to thank David Harwath for making the Flickr8k Audio Caption Corpus publicly available.,Acknowledgements,[0],[0]
We present a visually grounded model of speech perception which projects spoken utterances and images to a joint semantic space.,abstractText,[0],[0]
"We use a multi-layer recurrent highway network to model the temporal nature of spoken speech, and show that it learns to extract both form and meaningbased linguistic knowledge from the input signal.",abstractText,[0],[0]
"We carry out an in-depth analysis of the representations used by different components of the trained model and show that encoding of semantic aspects tends to become richer as we go up the hierarchy of layers, whereas encoding of formrelated aspects of the language input tends to initially increase and then plateau or decrease.",abstractText,[0],[0]
Representations of language in a model of visually grounded speech signal,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1499–1509, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
Representing information about real-world entities and their relations in structured knowledge base (KB) form enables numerous applications.,1 Introduction,[0],[0]
"Large, collaboratively created knowledge bases have recently become available e.g., Freebase (Bollacker et al., 2008), YAGO (Suchanek et al., 2007), and DBPedia (Auer et al., 2007), but even though they are impressively large, their coverage is far from complete.",1 Introduction,[0],[0]
"This has motivated research in automatically deriving new facts to extend a manually built knowledge base, by using information from the existing knowledge base, textual mentions of entities, and semi-structured data such as tables and web forms (Nickel et al., 2015).
",1 Introduction,[0],[0]
"In this paper we build upon the work of Riedel et al. (2013), which jointly learns continuous representations for knowledge base and textual relations.",1 Introduction,[0],[0]
"This common representation in the same vector space can serve as a kind of “universal schema” which admits joint inferences among
∗This research was conducted during the author’s internship at Microsoft Research.
KBs and text.",1 Introduction,[0],[0]
The textual relations represent the relationships between entities expressed in individual sentences (see Figure 1 for an example).,1 Introduction,[0],[0]
Riedel et al. (2013) represented each textual mention of an entity pair by the lexicalized dependency path between the two entities (see Figure 2).,1 Introduction,[0],[0]
Each such path is treated as a separate relation in a combined knowledge graph including both KB and textual relations.,1 Introduction,[0],[0]
"Following prior work in latent feature models for knowledge base completion, every textual relation receives its own continuous representation, learned from the pattern of its co-occurrences in the knowledge graph.
",1 Introduction,[0],[0]
"However, largely synonymous textual relations often share common sub-structure, and are composed of similar words and dependency arcs.",1 Introduction,[0],[0]
"For example, Table 1 shows a collection of dependency paths co-occurring with the person/organizations founded relation.
",1 Introduction,[0],[0]
"In this paper we model this sub-structure and share parameters among related dependency paths, using a unified loss function learning entity and relation representations to maximize performance on the knowledge base link prediction task.
",1 Introduction,[0],[0]
"We evaluate our approach on the FB15k-237 dataset, a knowledge base derived from the Free-
1499
base subset FB15k (Bordes et al., 2013) and filtered to remove highly redundant relations (Toutanova and Chen, 2015).",1 Introduction,[0],[0]
"The knowledge base is paired with textual mentions for all entity pairs derived from ClueWeb121 with Freebase entity mention annotations (Gabrilovich et al., 2013).
",1 Introduction,[0],[0]
"We show that using a convolutional neural network to derive continuous representations for textual relations boosts the overall performance on link prediction, with larger improvement on entity pairs that have textual mentions.",1 Introduction,[0],[0]
There has been a growing body of work on learning to predict relations between entities without requiring sentence-level annotations of textual mentions at training time.,2 Related Work,[0],[0]
"We group such related work into three groups based on whether KB, text, or both sources of information are used.",2 Related Work,[0],[0]
"Additionally, we discuss related work in the area of supervised relation extraction using continuous representations of text, even though we do not use supervision at the level of textual mentions.",2 Related Work,[0],[0]
"Nickel et al. (2015) provide a broad overview of machine learning models for knowledge graphs, including models based on observed graph features such as the path ranking algorithm (Lao et al., 2011), models based on continuous representations (latent features), and model combinations (Dong et al., 2014).",Knowledge base completion,[0],[0]
"These models predict new facts in a given knowledge base, based on information from existing entities and relations.",Knowledge base completion,[0],[0]
"From this line of work, most relevant to our study is prior work evaluating continuous representation models on the FB15k dataset.",Knowledge base completion,[0],[0]
"Yang et al. (2015) showed that a simple variant of a bilinear model DISTMULT outperformed TRANSE (Bordes et al., 2013) and more richly parameterized models on this dataset.",Knowledge base completion,[0],[0]
"We therefore build upon the best performing prior model DISTMULT from this line of work, as well as additional models E and F developed in the context of text-augmented knowledge graphs (Riedel et al., 2013), and extend them to incorporate compositional representations of textual relations.
",Knowledge base completion,[0],[0]
"1http://lemurproject.org/clueweb12/ FACC1/
Relation extraction using distant supervision
A number of works have focused on extracting new instances of relations using information from textual mentions, without sophisticated modeling of prior knowledge from the knowledge base.",Knowledge base completion,[0],[0]
"Mintz et al. (2009) demonstrated that both surface context and dependency path context were helpful for the task, but did not model the compositional sub-structure of this context.",Knowledge base completion,[0],[0]
"Other work proposed more sophisticated models that reason about sentence-level hidden variables (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) or model the noise arising from the incompleteness of knowledge bases and text collections (Ritter et al., 2013), inter alia.",Knowledge base completion,[0],[0]
"Our work focuses on representing the compositional structure of sentential context for learning joint continuous representations of text and knowledge bases.
",Knowledge base completion,[0],[0]
"Combining knowledge base and text information
A combination of knowledge base and textual information was first shown to outperform either source alone in the framework of path-ranking algorithms in a combined knowledge base and text graph (Lao et al., 2012).",Knowledge base completion,[0],[0]
"To alleviate the sparsity of textual relations arising in such a combined graph, (Gardner et al., 2013; Gardner et al., 2014) showed how to incorporate clusters or continuous representations of textual relations.",Knowledge base completion,[0],[0]
Note that these vector representations are based on the co-occurrence patterns for the textual relations and not on their compositional structure.,Knowledge base completion,[0],[0]
"Cooccurrence based textual relation representations were also learned in (Neelakantan et al., 2015).",Knowledge base completion,[0],[0]
"Wang et al. (2014a) combined knowledge base and text information by embedding knowledge base entities and the words in their names in the same vector space, but did not model the textual cooccurrences of entity pairs and the expressed textual relations.",Knowledge base completion,[0],[0]
Weston et al. (2013) combined continuous representations from a knowledge base and textual mentions for prediction of new relations.,Knowledge base completion,[0],[0]
"The two representations were trained independently of each other and using different loss functions, and were only combined at inference time.",Knowledge base completion,[0],[0]
"Additionally, the employed representations of text were non-compositional.
",Knowledge base completion,[0],[0]
"In this work we train continuous representations of knowledge base and textual relations jointly, which allows for deeper interactions between the
sources of information.",Knowledge base completion,[0],[0]
"We directly build on the universal schema approach of Riedel et al. (2013) as well as the universal schema extension of the DISTMULT model mentioned previously, to improve the representations of textual relations by capturing their compositional structure.",Knowledge base completion,[0],[0]
"Additionally, we evaluate the approach on a dataset that contains rich prior information from the training knowledge base, as well as a wealth of textual information from a large document collection.
",Knowledge base completion,[0],[0]
"Continuous representations for supervised relation extraction
In contrast to the work reviewed so far, work on sentence-level relation extraction using direct supervision has focused heavily on representing sentence context.",Knowledge base completion,[0],[0]
"Models using hand-crafted features have evolved for more than a decade, and recently, models using continuous representations have been found to achieve new state-of-the-art performance (Zeng et al., 2014; Gormley et al., 2015).",Knowledge base completion,[0],[0]
"Compared to work on representation learning for sentence-level context, such as this recent work using LSTM models on constituency or dependency trees (Tai et al., 2015), our approach using a one-hidden-layer convolutional neural network is relatively simple.",Knowledge base completion,[0],[0]
"However, even such a simple approach has been shown to be very competitive (Kim, 2014).",Knowledge base completion,[0],[0]
"We begin by introducing notation to define the task, largely following the terminology in Nickel et al. (2015).",3 Models for knowledge base completion,[0],[0]
"We assume knowledge bases are represented using RDF triples, in the form (subject, predicate, object), where the subject and object are entities and the predicate is the type of relation.",3 Models for knowledge base completion,[0],[0]
"For example, the KB fragment shown in Figure 1 is shown as a knowledge graph, where the entities are the nodes, and the relations are shown as directed labeled edges: we see three entities participating in three relation instances indicated by the edges.",3 Models for knowledge base completion,[0],[0]
"For brevity, we will denote triples by (es, r, eo), where es and eo denote the subject and object entities, respectively.
",3 Models for knowledge base completion,[0],[0]
"The task is, given a training KB consisting of entities with some relations between them, to predict new relations (links) that do not appear in the training KB.",3 Models for knowledge base completion,[0],[0]
"More specifically, we will build models that rank candidate entities for given queries (es, r, ?) or (?, r, eo), which ask about the object
1
or subject of a given relation.",3 Models for knowledge base completion,[0],[0]
"This task setting has been used in models for KB completion previously, e.g. (Dong et al., 2014; Gardner et al., 2014), even though it has not been standard in evaluations of distant supervision for relation extraction (Mintz et al., 2009; Riedel et al., 2013).",3 Models for knowledge base completion,[0.9650824264380258],"['Syntactic information from dependency parses has been used by (Mintz et al., 2009; He et al., 2018) for capturing long-range dependencies between tokens.']"
"The advantage of this evaluation setting is that it enables automatic evaluation without requiring humans to label candidate extractions, while making only a local closed world assumption for the completeness of the knowledge base — i.e., if one object eo for a certain subject / relation pair (es, r) is present in the knowledge base, it is assumed likely that all other objects (es, r, e′o) will be present.",3 Models for knowledge base completion,[0],[0]
"Such an assumption is particularly justified for nearly functional relations.
",3 Models for knowledge base completion,[0],[0]
"To incorporate textual information, we follow prior work (Lao et al., 2012; Riedel et al., 2013) and represent both textual and knowledge base relations in a single graph of “universal” relations.",3 Models for knowledge base completion,[0],[0]
"The textual relations are represented as full lexicalized dependency paths, as illustrated in Figure 2.",3 Models for knowledge base completion,[0],[0]
"An instance of the textual relation SUBJECT nsubj←−−− president prep−−→ of obj−→OBJECT connecting the entities BARACK OBAMA and UNITED STATES, is added to the knowledge graph based on this sentential occurrence.
",3 Models for knowledge base completion,[0],[0]
"To present the models for knowledge base completion based on such combined knowledge graphs, we first introduce some notation.",3 Models for knowledge base completion,[0],[0]
Let E denote the set of entities in the knowledge graph and let R denote the set of relation types.,3 Models for knowledge base completion,[0],[0]
"We denote each possible triple as T = (es, r, eo) where es, eo ∈ E , r ∈ R, and model its presence with a binary random variable yT ∈ {0, 1} which indicates whether the triple exists.",3 Models for knowledge base completion,[0],[0]
"The models we build score possible triples (es, r, eo) using continuous representations (latent features) of the three elements of the triple.",3 Models for knowledge base completion,[0],[0]
"The models use scoring function f(es, r, eo) to represent the model’s confidence in the existence of the triple.",3 Models for knowledge base completion,[0],[0]
"We present the models and then the loss function used to train
1
their parameters.",3 Models for knowledge base completion,[0],[0]
We begin with presenting the three models from prior work that this research builds upon.,3.1 Basic Models,[0],[0]
"They all learn latent continuous representations of relations and entities or entity pairs, and score possible triples based on the learned continuous representations.",3.1 Basic Models,[0],[0]
"Each of the models can be defined on a knowledge graph containing entities and KB relations only, or on a knowledge graph additionally containing textual relations.",3.1 Basic Models,[0],[0]
"We use models F and E from (Riedel et al., 2013) where they were used for a combined KB+text graph, and model DISTMULT from (Yang et al., 2015), which was originally used for a knowledge graph containing only KB relations.
",3.1 Basic Models,[0],[0]
"As shown in Figure 3, model F learns a Kdimensional latent feature vector for each candidate entity pair (es, eo), as well as a samedimensional vector for each relation r, and the scoring function is simply defined as their inner product: f(es, r, eo) = v(r)ᵀv(es, eo).",3.1 Basic Models,[0],[0]
"Therefore, different pairs sharing the same entity would not share parameters in this model.
",3.1 Basic Models,[0],[0]
"Model E does not have parameters for entity pairs, and instead has parameters for individual entities.",3.1 Basic Models,[0],[0]
"It aims to capture the compatibility be-
tween entities and the subject and object positions of relations.",3.1 Basic Models,[0],[0]
"For each relation type r, the model learns two latent feature vectors v(rs) and v(ro) of dimension K. For each entity (node) ei, the model also learns a latent feature vector of the same dimensionality.",3.1 Basic Models,[0],[0]
"The score of a candidate triple (es, r, eo) is defined as f(es, r, eo) = v(rs)ᵀv(es) + v(ro)ᵀv(eo).",3.1 Basic Models,[0],[0]
"It can be seen that when a subject entity is fixed in a query (es, r, ?), the ranking of candidate object entity fillers according to f does not depend on the subject entity but only on the relation type r.
The third model DISTMULT, is a special form of a bilinear model like RESCAL (Nickel et al., 2011), where the non-diagonal entries in the relation matrices are assumed to be zero.",3.1 Basic Models,[0],[0]
This model was proposed in Yang et al. (2015) and was shown to outperform prior work on the FB15k dataset.,3.1 Basic Models,[0],[0]
"In this model, each entity ei and each relation r is assigned a latent feature vector of dimensionK. The score of a candidate triple (es, r, eo) is defined as f(es, r, eo)",3.1 Basic Models,[0],[0]
"= v(r)ᵀ (v(es) ◦ v(eo)), where ◦ denotes the element-wise vector product.",3.1 Basic Models,[0],[0]
"In this model, entity pairs which share an entity also share parameters, and the ranking of candidate objects for queries (es, r, ?) depends on the subject entity.
",3.1 Basic Models,[0],[0]
"Denote Ne = |E|, Nr = |R|, and K = dimension of latent feature vectors, then model E has KNe +",3.1 Basic Models,[0],[0]
2KNr parameters and model DISTMULT has KNe + KNr parameters.,3.1 Basic Models,[0],[0]
"Model F has KN2e + KNr parameters, although most entity pairs will not co-occur in the knowledge base or text.
",3.1 Basic Models,[0],[0]
"In the basic models, knowledge base and textual relations are treated uniformly, and each textual relation receives its own latent representation of dimensionality K. When textual relations are added to the training knowledge graph, the total number of relations |R| grows substantially (it increases from 237 to more than 2.7 million for the dataset in this study), resulting in a substantial increase in the total number of independent parameters.
",3.1 Basic Models,[0],[0]
"Note that in all of these models queries about the arguments of knowledge base relations (es, r, ?) are answered by scoring functions looking only at the entity and KB relation representations, without using representations of textual mentions.",3.1 Basic Models,[0],[0]
The textual mention information and representations are only used at training time to improve the learned representations of KB relations and entities.,3.1 Basic Models,[0],[0]
"In the standard latent feature models discussed above, each textual relation is treated as an atomic unit receiving its own set of latent features.",3.2 CONV: Compositional Representations of Textual Relations,[0],[0]
"However, many textual relations differ only slightly in the words or dependency arcs used to express the relation.",3.2 CONV: Compositional Representations of Textual Relations,[0],[0]
"For example, Table 1 shows several textual patterns that co-occurr with the relation person/organizations founded in the training KB.",3.2 CONV: Compositional Representations of Textual Relations,[0],[0]
"While some dependency paths occur frequently, many very closely related ones have been observed only once.",3.2 CONV: Compositional Representations of Textual Relations,[0],[0]
The statistical strength of the model could be improved if similar dependency paths have a shared parameterization.,3.2 CONV: Compositional Representations of Textual Relations,[0],[0]
"We build on work using similar intuitions for other tasks and learn compositional representations of textual relations based on their internal structure, so that the derived representations are accurate for the task of predicting knowledge base relations.
",3.2 CONV: Compositional Representations of Textual Relations,[0],[0]
We use a convolutional neural network applied to the lexicalized dependency paths treated as a sequence of words and dependency arcs with direction.,3.2 CONV: Compositional Representations of Textual Relations,[0],[0]
Figure 4 depicts the neural network architecture.,3.2 CONV: Compositional Representations of Textual Relations,[0],[0]
"In the first layer, each word or directed labeled arc is mapped to a continuous representation using an embedding matrix V. In the hidden layer, every window of three elements is mapped to a hidden vector using position-specific maps W, a bias vector b, and a tanh activation function.",3.2 CONV: Compositional Representations of Textual Relations,[0],[0]
"A max-pooling operation over the sequence is applied to derive the final continuous representation for the dependency path.
",3.2 CONV: Compositional Representations of Textual Relations,[0],[0]
The CONV representation of textual relations can be used to augment any of the three basic models.,3.2 CONV: Compositional Representations of Textual Relations,[0],[0]
The difference between a basic model and its CONV-augmented variant is in the parameterization of textual mentions.,3.2 CONV: Compositional Representations of Textual Relations,[0],[0]
"The basic models learn distinct latent feature vectors of dimensionality K for all textual relation types, whereas the CONV models derive the K-dimensional latent feature vectors for textual relation types as the activation at the top layer of the convolutional network in Figure 4, given the corresponding lexicalized dependency path as input.",3.2 CONV: Compositional Representations of Textual Relations,[0],[0]
All basic and CONV-augmented models use the same training loss function.,3.3 Training loss function,[0],[0]
Our loss function is motivated by the link prediction task and the performance measures used.,3.3 Training loss function,[0],[0]
"As previously men-
tioned, the task is to predict the subject or object entity for given held-out triples (es, r, eo), i.e., to rank all entities with respect to their likelihood of filling the respective position in the triple2.",3.3 Training loss function,[0],[0]
"We would thus like the model to score correct triples (es, r, eo) higher than incorrect triples (e′, r, eo) and (es, r, e′) which differ from the correct triple by one entity.",3.3 Training loss function,[0],[0]
"Several approaches (Nickel et al., 2015) use a margin-based loss function.",3.3 Training loss function,[0],[0]
We use an approximation to the negative loglikelihood of the correct entity filler instead3.,3.3 Training loss function,[0],[0]
"We define the conditional probabilities p(eo|es, r) and p(es|r, eo) for object and subject entities given the relation and the other argument as follows:
p(eo|es, r; Θ) = e f(es,r,eo;Θ)∑
e′∈Neg(es,r,?)",3.3 Training loss function,[0],[0]
"e f(es,r,e′;Θ)
",3.3 Training loss function,[0],[0]
"Conditional probabilities for subject entities p(es|eo, r; Θ) are defined analogously.",3.3 Training loss function,[0],[0]
Here Θ denotes all the parameters of latent features.,3.3 Training loss function,[0],[0]
"The denominator is defined using a set of entities that do not fill the object position in any relation triple (es, r, ?) in the training knowledge graph.",3.3 Training loss function,[0],[0]
"Since the number of such entities is impractically large, we sample negative triples from the full set.",3.3 Training loss function,[0],[0]
"We also limit the candidate entities to ones that have types consistent with the position in the relation triple (Chang et al., 2014; Yang et al., 2015), where the types are approximated following Toutanova and Chen (2015).",3.3 Training loss function,[0],[0]
"Additionally, since the task of predicting textual relations is auxiliary to the main task, we use a weighting factor τ for the loss on predicting the arguments of textual relations (Toutanova and Chen, 2015).
",3.3 Training loss function,[0],[0]
"Denote T as a set of triples, we define the loss L(T ; Θ) as:
L(T ; Θ) =",3.3 Training loss function,[0],[0]
"− ∑
(es,r,eo)∈T log p(eo|es, r; Θ)
− ∑
(es,r,eo)∈T log p(es|eo, r; Θ)
Let TKB and Ttext represent the set of knowledge base triples and textual relation triples respectively.",3.3 Training loss function,[0],[0]
"The final training loss function is de-
2Our experimental comparison focuses on predicting object entities only, but we consider both argument types in the training loss function.
3Note that both margin-based and likelihood-based loss functions are susceptible to noise from potential selection of false negative examples.",3.3 Training loss function,[0],[0]
"An empirical comparison of training loss functions would be interesting.
fined as:
L(TKB; Θ) + τL(Ttext; Θ) + λ‖Θ‖2,
where λ is the regularization parameter, and τ is the weighing factor of the textual relations.
",3.3 Training loss function,[0],[0]
The parameters of all models are trained using a batch training algorithm.,3.3 Training loss function,[0],[0]
"The gradients of the basic models are straightforward to compute, and the gradients of the convolutional network parameters for the CONV-augmented models are also not hard to derive using back-propagation.",3.3 Training loss function,[0],[0]
"We use the FB15k-237 4 dataset, which is a subset of FB15k (Bordes et al., 2013) that excludes redundant relations and direct training links for held-out triples, with the goal of making the task more realistic (Toutanova and Chen, 2015).",Dataset and Evaluation Protocol,[0],[0]
"The FB15k dataset has been used in multiple studies on knowledge base completion (Wang et al., 2014b; Yang et al., 2015).",Dataset and Evaluation Protocol,[0],[0]
"Textual relations for
4Check the first author’s website for a release of the dataset.
1504
FB15k-237 are extracted from 200 million sentences in the ClueWeb12 corpus coupled with Freebase mention annotations (Gabrilovich et al., 2013), and include textual links of all co-occurring entities from the KB set.",Dataset and Evaluation Protocol,[0],[0]
"After pruning5, there are 2.7 million unique textual relations that are added to the knowledge graph.",Dataset and Evaluation Protocol,[0],[0]
"The set of textual relations is larger than the set used in Toutanova and Chen (2015) (25,000 versus 2.7 million), leading to improved performance.
",Dataset and Evaluation Protocol,[0],[0]
"The number of relations and triples in the training, validation and test portions of the data are given in Table 2.",Dataset and Evaluation Protocol,[0],[0]
The two rows list statistics for the KB and text portions of the data separately.,Dataset and Evaluation Protocol,[0],[0]
The 2.7 million textual relations occur in 3.9 million text triples.,Dataset and Evaluation Protocol,[0],[0]
"Almost all entities occur in textual relations (13,937 out of 14,541).",Dataset and Evaluation Protocol,[0],[0]
The numbers of triples for textual relations are shown as zero for the validation and test sets because we don’t evaluate on prediction of textual relations (all text triples are used in training).,Dataset and Evaluation Protocol,[0],[0]
"The percentage of KB triples that have textual relations for their pair of entities is 40.5% for the training, 26.6% for the validation, and 28.1% for the test set.",Dataset and Evaluation Protocol,[0],[0]
"While 26.6% of the validation set triples have textual mentions, the percentage with textual relations that have been seen in the training set is 18.4%.",Dataset and Evaluation Protocol,[0],[0]
"Having a mention increases the chance that a random entity pair has a relation from 0.1% to 5.0% — a fifty-fold increase.
",Dataset and Evaluation Protocol,[0],[0]
"Given a set of triples in a set disjoint from a training knowledge graph, we test models on predicting the object of each triple, given the subject and relation type.",Dataset and Evaluation Protocol,[0],[0]
We rank all entities in the training knowledge base in order of their likelihood of filling the argument position.,Dataset and Evaluation Protocol,[0],[0]
"We report the mean reciprocal rank (MRR) of the correct entity, as well as HITS@10 — the percentage of test triples for which the correct entity is ranked in the top 10.",Dataset and Evaluation Protocol,[0],[0]
"We use filtered measures following the protocol proposed in Bordes et al. (2013) — that is, when we rank entities for a given position, we remove all other entities that are known to be part of an existing triple in the training, validation, or test set.",Dataset and Evaluation Protocol,[0],[0]
"This avoids penalizing the model for ranking other correct fillers higher than the tested entity.
",Dataset and Evaluation Protocol,[0],[0]
"5The full set of 37 million textual patterns connecting the entity pairs of interest was pruned based on the count of patterns and their tri-grams, and their precision in indicating that entity pairs have KB relations.",Dataset and Evaluation Protocol,[0],[0]
"We used a value of λ = 1 for the weight of the L2 penalty for the main results in Table 3, and present some results on the impact of λ at the end of this section.",Implementation details,[0],[0]
We used batch optimization after initial experiments with AdaGrad showed inferior performance.,Implementation details,[0],[0]
"L-BFGS (Liu and Nocedal, 1989) and RProp (Riedmiller and Braun, 1993) were found to converge to similar function values, with RProp converging significantly faster.",Implementation details,[0],[0]
We thus used RProp for optimization.,Implementation details,[0],[0]
"We initialized the KB+text models from the KB-only models and also from random initial values (sampled from a Gaussian distribution), and stopped optimization when the overall MRR on the validation set decreased.",Implementation details,[0],[0]
"For each model type, we chose the better of random and KB-only initialization.",Implementation details,[0],[0]
"The word embeddings in the CONV models were initialized using the 50-dimensional vectors from Turian et al. (2010) in the main experiments, with a slight positive impact.",Implementation details,[0],[0]
"The effect of initialization is discussed at the end of the section.
",Implementation details,[0],[0]
The number of negative examples for each triple was set to 200.,Implementation details,[0],[0]
Performance improved substantially when the number of negative examples was increased and reached a plateau around 200.,Implementation details,[0],[0]
"We chose the optimal number of latent feature dimensions via a grid search to optimize MRR on the validation set, testing the values 5, 10, 15, 35, 50, 100, 200 and 500.",Implementation details,[0],[0]
"We also performed a grid search over the values of the parameter τ , testing values in the set {0.01, 0.1, 0.25, 0.5, 1}.",Implementation details,[0],[0]
"The best dimension for latent feature vectors was 10 for most KBonly models (not including model F), and 5 for the two model configurations including F. We used K = 10 for all KB+text models, as higher dimension was also not helpful for them.",Implementation details,[0],[0]
"In Table 3 we show the performance of different models and their combinations6, both when using textual mentions (KB+text), and when using only knowledge base relations (KB only).",Experimental results,[0],[0]
"In the KB+text setting, we evaluate the contribution of the CONV representations of the textual relations.",Experimental results,[0],[0]
"The upper portion of the Table shows the performance of models that have been trained using knowledge graphs including only knowledge
6Different models are combined by simply defining a combined scoring function which adds the scores from individual models.",Experimental results,[0],[0]
"Combined models are trained jointly.
base relations, and are not using any information from textual mentions.",Experimental results,[0],[0]
The lower portion of the Table shows the performance when textual relations are added to the training knowledge graph and the corresponding training loss function.,Experimental results,[0],[0]
"Note that all models predict based on the learned knowledge base relation and entity representations, and the textual relations are only used at training time when they can impact these representations.
",Experimental results,[0],[0]
"The performance of all models is shown as an overall MRR (scaled by 100) and HITS@10, as well as performance on the subset of triples that have textual mentions (column With mentions), and ones that do not (column Without mentions).",Experimental results,[0],[0]
"Around 28% of the test triples have mentions and contribute toward the measures in the With mentions column, and the other 72% of the test triples contribute to the Without mentions column.
",Experimental results,[0],[0]
"For the KB-only models, we see the performance of each individual model F, E, and DISTMULT.",Experimental results,[0],[0]
"Model F was the best performing single model from (Riedel et al., 2013), but it does not perform well when textual mentions are not used.",Experimental results,[0],[0]
"In our implementation of model F, we created entity pair parameters only for entity pairs that cooccur in the text data (Riedel et al. (2013) also trained pairwise vectors for co-occuring entities
only, but all of the training and test tuples in their study were co-occurring)7.",Experimental results,[0],[0]
"Without textual information, model F is performing essentially randomly, because entity pairs in the test sets do not occur in training set relations (by construction of the dataset).",Experimental results,[0],[0]
"Model E is able to do surprisingly well, given that it is making predictions for each object position of a relation without considering the given subject of the relation.",Experimental results,[0],[0]
DISTMULT is the best performing single model.,Experimental results,[0],[0]
"Unlike model F, it is able to share parameters among entity pairs with common subject or object entities, and, unlike model E, it captures some dependencies between the subject and object entities of a relation.",Experimental results,[0],[0]
"The combination of models E+DISTMULT improves performance, but combining model F with the other two is not helpful.
",Experimental results,[0],[0]
The lower portion of Table 3 shows results when textual relations are added to the training knowledge graph.,Experimental results,[0],[0]
The basic models treat the textual relations as atomic and learn a separate latent feature vector for each textual relation.,Experimental results,[0],[0]
"The CONV- models use the compositional representations of tex-
7Learning entity pair parameters for all entity pairs would result in 2.2 billion parameters for vectors with dimensionality 10 for our dataset.",Experimental results,[0],[0]
"This was infeasible and was also not found useful based on experiments with vectors of lower dimensionality.
",Experimental results,[0],[0]
tual relations learned using the convolutional neural network architecture shown in Figure 4.,Experimental results,[0],[0]
We show the performance of each individual model and its corresponding variant with a CONV parameterization.,Experimental results,[0],[0]
"For each model, we also show the optimal value of τ , the weight of the textual relations loss.",Experimental results,[0],[0]
"Model F is able to benefit from textual relations and its performance increases by 2.5 points in MRR, with the gain in performance being particularly large on test triples with textual mentions.",Experimental results,[0],[0]
Model F is essentially limiting its space of considered argument fillers to ones that have cooccurred with the given subject entity.,Experimental results,[0],[0]
"This gives it an advantage on test triples with textual mentions, but model F still does relatively very poorly overall when taking into account the much more numerous test triples without textual mentions.",Experimental results,[0],[0]
"The CONV parameterization performs slightly worse in MRR, but slightly better in HITS@10, compared to the atomic parameterization.",Experimental results,[0],[0]
"For model E and its CONV variant, we see that text does not help as its performance using text is the same as that when not using text and the optimal weight of the text is zero.",Experimental results,[0],[0]
"Model DISTMULT benefits from text, and its convolutional text variant CONVDISTMULT outperforms the basic model, with the gain being larger on test triples with mentions.
",Experimental results,[0],[0]
"The best model overall, as in the KB-only case, is E+DISTMULT.",Experimental results,[0],[0]
"The basic model benefits from text slightly and the model with compositional representations of textual patterns CONVE+CONV-DISTMULT, improves the performance further, by 2.4 MRR overall, and by 5 MRR on triples with textual mentions.",Experimental results,[0],[0]
It is interesting that the text and the compositional representations helped most for this combined model.,Experimental results,[0],[0]
"One hypothesis is that model E, which provides a prior over relation arguments, is needed in combination with DISTMULT to prevent the prediction of unlikely arguments based on noisy inference from textual patterns and their individual words and dependency links.",Experimental results,[0],[0]
"To gain insight into the sensitivity of the model to hyper-parameters and initialization, we report on experiments starting with the best model CONVE + CONV-DISTMULT from Table 3 and varying one parameter at a time.",Hyperparameter Sensitivity,[0],[0]
"This model has weight of the textual relations loss τ = 0.25, weight of the L2 penalty λ = 1, convolution window size of
three, and is initialized randomly for the entity and KB relation vectors, and from pre-trained embeddings for word vectors (Turian et al., 2010).",Hyperparameter Sensitivity,[0],[0]
"The overall MRR of the model is 40.4 on the validation set (test results are shown in the Table).
",Hyperparameter Sensitivity,[0],[0]
"When the weight of τ is changed to 1 (i.e., equal contribution of textual and KB relations), the overall MRR goes down to 39.6 from 40.4, indicating the usefulness of weighting the two kinds of relations non-uniformly.",Hyperparameter Sensitivity,[0],[0]
"When λ is reduced to 0.04, MRR is 40.0 and when λ is increased to 25, MRR goes down to 38.9.",Hyperparameter Sensitivity,[0],[0]
This indicates the L2 penalty hyper-parameter has a large impact on performance.,Hyperparameter Sensitivity,[0],[0]
"When we initialize the word embeddings randomly instead of using pre-trained word vectors, performance drops only slightly to 40.3.",Hyperparameter Sensitivity,[0],[0]
"If we initialize from a model trained using KBonly information, performance goes down substantially to 38.7.",Hyperparameter Sensitivity,[0],[0]
This indicates that initialization is important and there is a small gain from using pre-trained word embeddings.,Hyperparameter Sensitivity,[0],[0]
"There was a drop in performance to MRR 40.2 when using a window size of one for the convolutional architecture in Figure 4, and an increase to 40.6 when using a window size of five.",Hyperparameter Sensitivity,[0],[0]
Here we explored an alternative representation of textual relations for latent feature models that learn to represent knowledge base and textual relations in the same vector space.,5 Conclusion and Future Work,[0],[0]
"We showed that given the large degree of sharing of sub-structure in the textual relations, it was beneficial to compose their continuous representations out of the representations of their component words and dependency arc links.",5 Conclusion and Future Work,[0],[0]
"We applied a convolutional neural network model and trained it jointly with a model mapping entities and knowledge base relations to the same vector space, obtaining substantial improvements over an approach that treats the textual relations as atomic units having independent parameterization.",5 Conclusion and Future Work,[0],[0]
"We would like to thank the anonymous reviewers for their suggestions, and Jianfeng Gao, Scott Wen-tau Yih, and Wei Xu for useful discussions.",Acknowledgements,[0],[0]
"Models that learn to represent textual and knowledge base relations in the same continuous latent space are able to perform joint inferences among the two kinds of relations and obtain high accuracy on knowledge base completion (Riedel et al., 2013).",abstractText,[0],[0]
"In this paper we propose a model that captures the compositional structure of textual relations, and jointly optimizes entity, knowledge base, and textual relation representations.",abstractText,[0],[0]
The proposed model significantly improves performance over a model that does not share parameters among textual relations with common sub-structure.,abstractText,[0],[0]
Representing Text for Joint Embedding of Text and Knowledge Bases,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1257–1266 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
1257",text,[0],[0]
"The construction of large-scale Knowledge Bases (KBs) like Freebase (Bollacker et al., 2008) and Wikidata (Vrandečić and Krötzsch, 2014) has proven to be useful in many natural language processing (NLP) tasks like question-answering, web search, etc.",1 Introduction,[0],[0]
"However, these KBs are not exhaustive.",1 Introduction,[1.0],"['However, these KBs are not exhaustive.']"
Relation Extraction (RE) attempts to fill this gap by extracting semantic relationships between entity pairs from plain text.,1 Introduction,[1.0],['Relation Extraction (RE) attempts to fill this gap by extracting semantic relationships between entity pairs from plain text.']
This task can be modeled as a simple classification problem after the entity pairs are specified.,1 Introduction,[0],[0]
"Formally, given an entity pair (e1,e2) from the KB and an entity annotated sentence (or instance), we aim to predict the
∗Research internship at Indian Institute of Science.
relation r, from a predefined relation set, that exists between e1 and e2.",1 Introduction,[0.9783410131843475],"['Formally, given an entity pair (e1,e2) from the KB and an entity annotated sentence (or instance), we aim to predict the ∗Research internship at Indian Institute of Science.']"
"If no relation exists, we simply label it NA.
",1 Introduction,[0.9999999509659852],"['If no relation exists, we simply label it NA.']"
Most supervised relation extraction methods require large labeled training data which is expensive to construct.,1 Introduction,[1.0],['Most supervised relation extraction methods require large labeled training data which is expensive to construct.']
"Distant Supervision (DS) (Mintz et al., 2009) helps with the construction of this dataset automatically, under the assumption that if two entities have a relationship in a KB, then all sentences mentioning those entities express the same relation.",1 Introduction,[0],[0]
"While this approach works well in generating large amounts of training instances, the DS assumption does not hold in all cases.",1 Introduction,[0],[0]
Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) propose multi-instance based learning to relax this assumption.,1 Introduction,[0],[0]
"However, they use NLP tools to extract features, which can be noisy.
",1 Introduction,[0],[0]
"Recently, neural models have demonstrated promising performance on RE.",1 Introduction,[0],[0]
"Zeng et al. (2014, 2015) employ Convolutional Neural Networks (CNN) to learn representations of instances.",1 Introduction,[0],[0]
"For alleviating noise in distant supervised datasets, attention has been utilized by (Lin et al., 2016; Jat et al., 2018).",1 Introduction,[0],[0]
"Syntactic information from dependency parses has been used by (Mintz et al., 2009; He et al., 2018) for capturing long-range dependencies between tokens.",1 Introduction,[0],[0]
"Recently proposed Graph Convolution Networks (GCN) (Defferrard et al., 2016) have been effectively employed for encoding this information (Marcheggiani and Titov, 2017; Bastings et al., 2017).",1 Introduction,[0],[0]
"However, all the above models rely only on the noisy instances from distant supervision for RE.
",1 Introduction,[0],[0]
Relevant side information can be effective for improving RE.,1 Introduction,[0],[0]
"For instance, in the sentence, Microsoft was started by Bill Gates., the type information of Bill Gates (person) and Microsoft (organization) can be helpful in predicting the correct relation founderOfCompany.",1 Introduction,[0],[0]
"This is because every relation constrains the type of its target en-
tities.",1 Introduction,[0],[0]
"Similarly, relation phrase “was started by” extracted using Open Information Extraction (Open IE) methods can be useful, given that the aliases of relation founderOfCompany, e.g., founded, co-founded, etc., are available.",1 Introduction,[0],[0]
"KBs used for DS readily provide such information which has not been completely exploited by current models.
",1 Introduction,[0],[0]
"In this paper, we propose RESIDE, a novel distant supervised relation extraction method which utilizes additional supervision from KB through its neural network based architecture.",1 Introduction,[0],[0]
"RESIDE makes principled use of entity type and relation alias information from KBs, to impose soft constraints while predicting the relation.",1 Introduction,[0],[0]
"It uses encoded syntactic information obtained from Graph Convolution Networks (GCN), along with embedded side information, to improve neural relation extraction.",1 Introduction,[0],[0]
"Our contributions can be summarized as follows:
• We propose RESIDE, a novel neural method which utilizes additional supervision from KB in a principled manner for improving distant supervised RE.",1 Introduction,[0],[0]
•,1 Introduction,[0],[0]
"RESIDE uses Graph Convolution Networks
(GCN) for modeling syntactic information and has been shown to perform competitively even with limited side information.",1 Introduction,[0.9930994016971655],['• RESIDE uses Graph Convolution Networks (GCN) for modeling syntactic information and has been shown to perform competitively even with limited side information.']
•,1 Introduction,[0],[0]
"Through extensive experiments on benchmark
datasets, we demonstrate RESIDE’s effectiveness over state-of-the-art baselines.
RESIDE’s source code and datasets used in the paper are available at http://github.com/ malllabiisc/RESIDE.",1 Introduction,[0],[0]
Distant supervision: Relation extraction is the task of identifying the relationship between two entity mentions in a sentence.,2 Related Work,[0],[0]
"In supervised paradigm, the task is considered as a multi-class classification problem but suffers from lack of large labeled training data.",2 Related Work,[0],[0]
"To address this limitation, (Mintz et al., 2009) propose distant supervision (DS) assumption for creating large datasets, by heuristically aligning text to a given Knowledge Base (KB).",2 Related Work,[0],[0]
"As this assumption does not always hold true, some of the sentences might be wrongly labeled.",2 Related Work,[0],[0]
"To alleviate this shortcoming, Riedel et al. (2010) relax distant supervision for multi-instance single-label learning.",2 Related Work,[0],[0]
"Subsequently, for handling overlapping relations between entities (Hoffmann et al., 2011; Surdeanu et al., 2012) propose multi-instance multi-label learning paradigm.
",2 Related Work,[0],[0]
Neural Relation Extraction: The performance of the above methods strongly rely on the quality of hand engineered features.,2 Related Work,[0],[0]
"Zeng et al. (2014)
propose an end-to-end CNN based method which could automatically capture relevant lexical and sentence level features.",2 Related Work,[0],[0]
"This method is further improved through piecewise max-pooling by (Zeng et al., 2015).",2 Related Work,[0],[0]
"Lin et al. (2016); Nagarajan et al. (2017) use attention (Bahdanau et al., 2014) for learning from multiple valid sentences.",2 Related Work,[0],[0]
"We also make use of attention for learning sentence and bag representations.
",2 Related Work,[0],[0]
"Dependency tree based features have been found to be relevant for relation extraction (Mintz et al., 2009).",2 Related Work,[0],[0]
He et al. (2018) use them for getting promising results through a recursive tree-GRU based model.,2 Related Work,[0],[0]
"In RESIDE, we make use of recently proposed Graph Convolution Networks (Defferrard et al., 2016; Kipf and Welling, 2017), which have been found to be quite effective for modelling syntactic information (Marcheggiani and Titov, 2017; Nguyen and Grishman, 2018; Vashishth et al., 2018a).
",2 Related Work,[0],[0]
"Side Information in RE: Entity description from KB has been utilized for RE (Ji et al., 2017), but such information is not available for all entities.",2 Related Work,[0],[0]
Type information of entities has been used by Ling and Weld (2012); Liu et al. (2014) as features in their model.,2 Related Work,[0],[0]
Yaghoobzadeh et al. (2017) also attempt to mitigate noise in DS through their joint entity typing and relation extraction model.,2 Related Work,[0],[0]
"However, KBs like Freebase readily provide reliable type information which could be directly utilized.",2 Related Work,[0.9617382546055772],"['Several KBs like Wikidata provide such relation aliases, which can be readily exploited.']"
"In our work, we make principled use of entity type and relation alias information obtained from KB.",2 Related Work,[0],[0]
"We also use unsupervised Open Information Extraction (Open IE) methods (Mausam et al., 2012; Angeli et al., 2015), which automatically discover possible relations without the need of any predefined ontology, which is used as a side information as defined in Section 5.2.",2 Related Work,[0],[0]
"In this section, we provide a brief overview of Graph Convolution Networks (GCN) for graphs with directed and labeled edges, as used in (Marcheggiani and Titov, 2017).",3 Background: Graph Convolution Networks (GCN),[0],[0]
"For a directed graph, G = (V, E), where V and E represent the set of vertices and edges respectively, an edge from node u to node v with label luv is represented as (u, v, luv).",3.1 GCN on Labeled Directed Graph,[0],[0]
"Since, informa-
tion in directed edge does not necessarily propagate along its direction, following (Marcheggiani and Titov, 2017) we define an updated edge set E ′ which includes inverse edges (v, u, l−1uv ) and",3.1 GCN on Labeled Directed Graph,[0],[0]
"selfloops (u, u,>) along with the original edge set E , where > is a special symbol to denote self-loops.",3.1 GCN on Labeled Directed Graph,[0],[0]
"For each node v in G, we have an initial representation",3.1 GCN on Labeled Directed Graph,[0],[0]
"xv ∈ Rd, ∀v ∈ V .",3.1 GCN on Labeled Directed Graph,[0],[0]
"On employing GCN, we get an updated d-dimensional hidden representation hv ∈ Rd, ∀v ∈ V , by considering only its immediate neighbors (Kipf and Welling, 2017).",3.1 GCN on Labeled Directed Graph,[0],[0]
"This can be formulated as:
hv = f  ∑ u∈N (v) (Wluvxu + bluv)  .",3.1 GCN on Labeled Directed Graph,[0],[0]
"Here, Wluv ∈ Rd×d and bluv ∈ Rd are label dependent model parameters which are trained based on the downstream task.",3.1 GCN on Labeled Directed Graph,[0],[0]
N (v) refers to the set of neighbors of v based on E ′,3.1 GCN on Labeled Directed Graph,[0],[0]
and f is any non-linear activation function.,3.1 GCN on Labeled Directed Graph,[0],[0]
"In order to capture multihop neighborhood, multiple GCN layers can be stacked.",3.1 GCN on Labeled Directed Graph,[0],[0]
"Hidden representation of node v in this case after kth GCN layer is given as:
hk+1v = f  ∑ u∈N (v) ( W kluvh k u + b k luv ) .",3.1 GCN on Labeled Directed Graph,[0],[0]
"In automatically constructed graphs, some edges might be erroneous and hence need to be discarded.",3.2 Integrating Edge Importance,[0],[0]
"Edgewise gating in GCN by (Bastings et al., 2017; Marcheggiani and Titov, 2017) allows us to alleviate this problem by subduing the noisy edges.",3.2 Integrating Edge Importance,[0],[0]
This is achieved by assigning a relevance score to each edge in the graph.,3.2 Integrating Edge Importance,[0],[0]
"At kth layer, the importance of an edge (u, v, luv) is computed as:
gkuv = σ",3.2 Integrating Edge Importance,[0],[0]
"( hku · ŵkluv + b̂ k luv ) , (1)
Here, ŵkluv ∈ R m and b̂kluv ∈ R are parameters which are trained and σ(·) is the sigmoid function.",3.2 Integrating Edge Importance,[0],[0]
"With edgewise gating, the final GCN embedding for a node v after kth layer is given as:
hk+1v = f  ∑ u∈N (v) gkuv × ( W kluvh k u + b k luv ) .",3.2 Integrating Edge Importance,[0],[0]
(2),3.2 Integrating Edge Importance,[0],[0]
"In multi-instance learning paradigm, we are given a bag of sentences (or instances) {s1, s2, ...sn} for a given entity pair, the task is to predict the relation between them.",4 RESIDE Overview,[0],[0]
"RESIDE consists of three components for learning a representation of a given bag, which is fed to a softmax classifier.",4 RESIDE Overview,[0],[0]
We briefly present the components of RESIDE below.,4 RESIDE Overview,[0],[0]
Each component will be described in detail in the subsequent sections.,4 RESIDE Overview,[0],[0]
"The overall architecture of RESIDE is shown in Figure 1.
1.",4 RESIDE Overview,[0],[0]
Syntactic Sentence Encoding: RESIDE uses a Bi-GRU over the concatenated positional and word embedding for encoding the local context of each token.,4 RESIDE Overview,[0],[0]
"For capturing long-range dependencies, GCN over dependency tree is employed and its encoding is appended to the representation of each token.",4 RESIDE Overview,[0],[0]
"Finally, attention over tokens is used to subdue irrelevant tokens and get an embedding for the entire sentence.",4 RESIDE Overview,[0],[0]
"More details in Section 5.1.
2.",4 RESIDE Overview,[0],[0]
Side Information Acquisition:,4 RESIDE Overview,[0],[0]
"In this module, we use additional supervision from KBs and utilize Open IE methods for getting relevant side information.",4 RESIDE Overview,[0],[0]
"This information is later utilized by the model as described in Section 5.2.
3.",4 RESIDE Overview,[0],[0]
Instance Set Aggregation:,4 RESIDE Overview,[0],[0]
"In this part, sentence representation from syntactic sentence encoder is concatenated with the matched relation embedding obtained from the previous step.",4 RESIDE Overview,[0],[0]
"Then, using attention over sentences, a representation for the entire bag is learned.",4 RESIDE Overview,[0],[0]
This is then concatenated with entity type embedding before feeding into the softmax classifier for relation prediction.,4 RESIDE Overview,[0],[0]
Please refer to Section 5.3 for more details.,4 RESIDE Overview,[0],[0]
"In this section, we provide the detailed description of the components of RESIDE.",5 RESIDE Details,[0],[0]
"For each sentence in the bag si with m tokens {w1, w2, ...wm}, we first represent each token by k-dimensional GloVe embedding (Pennington et al., 2014).",5.1 Syntactic Sentence Encoding,[0],[0]
"For incorporating relative position of tokens with respect to target entities, we use p-dimensional position embeddings, as used by
(Zeng et al., 2014).",5.1 Syntactic Sentence Encoding,[0],[0]
The combined token embeddings are stacked together to get the sentence representationH ∈ Rm×(k+2p).,5.1 Syntactic Sentence Encoding,[0],[0]
"Then, using Bi-GRU (Cho et al., 2014) over H, we get the new sentence representationHgru ∈ Rm×dgru , where dgru is the hidden state dimension.",5.1 Syntactic Sentence Encoding,[0],[0]
"Bi-GRUs have been found to be quite effective in encoding the context of tokens in several tasks (Sutskever et al., 2014; Graves et al., 2013).
",5.1 Syntactic Sentence Encoding,[0],[0]
"Although Bi-GRU is capable of capturing local context, it fails to capture long-range dependencies which can be captured through dependency edges.",5.1 Syntactic Sentence Encoding,[0],[0]
"Prior works (Mintz et al., 2009; He et al., 2018) have exploited features from syntactic dependency trees for improving relation extraction.",5.1 Syntactic Sentence Encoding,[0],[0]
"Motivated by their work, we employ Syntactic Graph Convolution Networks for encoding this information.",5.1 Syntactic Sentence Encoding,[0],[0]
"For a given sentence, we generate its dependency tree using Stanford CoreNLP",5.1 Syntactic Sentence Encoding,[0],[0]
"(Manning et al., 2014).",5.1 Syntactic Sentence Encoding,[0],[0]
"We then run GCN over the dependency graph and use Equation 2 for updating the embeddings, taking Hgru as the input.",5.1 Syntactic Sentence Encoding,[0],[0]
"Since dependency graph has 55 different edge labels, incorporating all of them overparameterizes the model significantly.",5.1 Syntactic Sentence Encoding,[0],[0]
"Therefore, following (Marcheggiani and Titov, 2017; Nguyen and Grishman, 2018; Vashishth et al., 2018a) we use only three edge labels based on the direction of the edge {forward (→), backward (←), selfloop (>)}.",5.1 Syntactic Sentence Encoding,[0],[0]
"We define the new edge label Luv for an edge (u, v, luv) as follows:
",5.1 Syntactic Sentence Encoding,[0],[0]
"Luv =  → if edge exists in dependency parse ← if edge is an inverse edge > if edge is a self-loop
For each token wi, GCN embedding h gcn ik+1 ∈",5.1 Syntactic Sentence Encoding,[0],[0]
"Rdgcn after kth layer is defined as:
hgcnik+1 =",5.1 Syntactic Sentence Encoding,[0],[0]
f,5.1 Syntactic Sentence Encoding,[0],[0]
"( ∑ u∈N (i) gkiu × ( W kLiuh gcn uk + bkLiu )) .
",5.1 Syntactic Sentence Encoding,[0],[0]
"Here, gkiu denotes edgewise gating as defined in Equation 1 and Liu refers to the edge label defined above.",5.1 Syntactic Sentence Encoding,[0],[0]
"We use ReLU as activation function f , throughout our experiments.",5.1 Syntactic Sentence Encoding,[0],[0]
"The syntactic graph encoding from GCN is appended to Bi-GRU output to get the final token representation, hconcati as [hgrui ;h gcn ik+1
].",5.1 Syntactic Sentence Encoding,[0],[0]
"Since, not all tokens are equally relevant for RE task, we calculate the degree of relevance of each token using attention as used in
(Jat et al., 2018).",5.1 Syntactic Sentence Encoding,[0],[0]
"For token wi in the sentence, attention weight αi is calculated as:
αi = exp(ui)∑m j=1 exp(uj) where, ui = hconcati ·",5.1 Syntactic Sentence Encoding,[0],[0]
"r.
where r is a random query vector and ui is the relevance score assigned to each token.",5.1 Syntactic Sentence Encoding,[0],[0]
Attention values {αi} are calculated by taking softmax over {ui}.,5.1 Syntactic Sentence Encoding,[0],[0]
"The representation of a sentence is given as a weighted sum of its tokens, s =∑m
j=1 αih concat i .",5.1 Syntactic Sentence Encoding,[0],[0]
"Relevant side information has been found to improve performance on several tasks (Ling and Weld, 2012; Vashishth et al., 2018b).",5.2 Side Information Acquisition,[0],[0]
"In distant supervision based relation extraction, since the entities are from a KB, knowledge about them can be utilized to improve relation extraction.",5.2 Side Information Acquisition,[0],[0]
"Moreover, several unsupervised relation extraction methods (Open IE) (Angeli et al., 2015; Mausam et al., 2012) allow extracting relation phrases between target entities without any predefined ontology and thus can be used to obtain relevant side information.",5.2 Side Information Acquisition,[0],[0]
"In RESIDE, we employ Open IE methods and additional supervision from KB for improving neural relation extraction.
",5.2 Side Information Acquisition,[0],[0]
"Relation Alias Side Information RESIDE uses Stanford Open IE (Angeli et al., 2015) for extracting relation phrases between target entities, which we denote by P .",5.2 Side Information Acquisition,[1.0],"['Relation Alias Side Information RESIDE uses Stanford Open IE (Angeli et al., 2015) for extracting relation phrases between target entities, which we denote by P .']"
"As shown in Figure 2, for the sentence Matt Coffin, executive of
lowermybills, a company.., Open IE methods extract “executive of” between Matt Coffin and lowermybills.",5.2 Side Information Acquisition,[1.0000000097548813],"['As shown in Figure 2, for the sentence Matt Coffin, executive of lowermybills, a company.., Open IE methods extract “executive of” between Matt Coffin and lowermybills.']"
"Further, we extend P by including tokens at one hop distance in dependency path from target entities.",5.2 Side Information Acquisition,[0],[0]
"Such features from dependency parse have been exploited in the past by (Mintz et al., 2009; He et al., 2018).",5.2 Side Information Acquisition,[0],[0]
The degree of match between the extracted phrases in P and aliases of a relation can give important clues about the relevance of that relation for the sentence.,5.2 Side Information Acquisition,[0],[0]
"Several KBs like Wikidata provide such relation aliases, which can be readily exploited.",5.2 Side Information Acquisition,[0],[0]
"In RESIDE, we further expand the relation alias set using Paraphrase database (PPDB) (Pavlick et al., 2015).",5.2 Side Information Acquisition,[0],[0]
"We note that even for cases when aliases for relations are not available, providing only the names of relations give competitive performance.",5.2 Side Information Acquisition,[0],[0]
"We shall explore this point further in Section 7.3.
",5.2 Side Information Acquisition,[0],[0]
"For matching P with the PPDB expanded relation alias setR, we project both in a d-dimensional space using GloVe embeddings (Pennington et al., 2014).",5.2 Side Information Acquisition,[1.0],"['For matching P with the PPDB expanded relation alias setR, we project both in a d-dimensional space using GloVe embeddings (Pennington et al., 2014).']"
"Projecting phrases using word embeddings helps to further expand these sets, as semantically similar words are closer in embedding space (Mikolov et al., 2013; Pennington et al., 2014).",5.2 Side Information Acquisition,[0],[0]
"Then, for each phrase p ∈ P , we calculate its cosine distance from all relation aliases inR and take the relation corresponding to the closest relation alias as a matched relation for the sentence.",5.2 Side Information Acquisition,[0],[0]
We use a threshold on cosine distance to remove noisy aliases.,5.2 Side Information Acquisition,[0],[0]
"In RESIDE, we define a kr-dimensional embedding for each relation which we call as matched relation embedding (hrel).",5.2 Side Information Acquisition,[0],[0]
"For a given sentence, hrel is concatenated with its representa-
tion s, obtained from syntactic sentence encoder (Section 5.1) as shown in Figure 1.",5.2 Side Information Acquisition,[1.000000009211132],"['For a given sentence, hrel is concatenated with its representa- tion s, obtained from syntactic sentence encoder (Section 5.1) as shown in Figure 1.']"
"For sentences with |P| > 1, we might get multiple matched relations.",5.2 Side Information Acquisition,[0],[0]
"In such cases, we take the average of their embeddings.",5.2 Side Information Acquisition,[0],[0]
"We hypothesize that this helps in improving the performance and find it to be true as shown in Section 7.
Entity Type Side Information Type information of target entities has been shown to give promising results on relation extraction (Ling and Weld, 2012; Yaghoobzadeh et al., 2017).",5.2 Side Information Acquisition,[0],[0]
Every relation puts some constraint on the type of entities which can be its subject and object.,5.2 Side Information Acquisition,[0],[0]
"For example, the relation person/place of birth can only occur between a person and a location.",5.2 Side Information Acquisition,[0],[0]
"Sentences in distance supervision are based on entities in KBs, where the type information is readily available.
",5.2 Side Information Acquisition,[0],[0]
"In RESIDE, we use types defined by FIGER (Ling and Weld, 2012) for entities in Freebase.",5.2 Side Information Acquisition,[0],[0]
"For each type, we define a kt-dimensional embedding which we call as entity type embedding (htype).",5.2 Side Information Acquisition,[0],[0]
"For cases when an entity has multiple types in different contexts, for instance, Paris may have types government and location, we take the average over the embeddings of each type.",5.2 Side Information Acquisition,[0],[0]
We concatenate the entity type embedding of target entities to the final bag representation before using it for relation classification.,5.2 Side Information Acquisition,[0],[0]
"To avoid over-parameterization, instead of using all fine-grained 112 entity types, we use 38 coarse types which form the first hierarchy of FIGER types.",5.2 Side Information Acquisition,[0],[0]
"For utilizing all valid sentences, following (Lin et al., 2016; Jat et al., 2018), we use attention over sentences to obtain a representation for the entire bag.",5.3 Instance Set Aggregation,[1.0],"['For utilizing all valid sentences, following (Lin et al., 2016; Jat et al., 2018), we use attention over sentences to obtain a representation for the entire bag.']"
"Instead of directly using the sentence representation si from Section 5.1, we concatenate the embedding of each sentence with matched relation embedding hreli as obtained from Section 5.2.",5.3 Instance Set Aggregation,[0],[0]
"The attention score αi for ith sentence is formulated as:
αi = exp(ŝi · q)∑n j=1 exp(ŝj · q) where, ŝi =",5.3 Instance Set Aggregation,[0],[0]
"[si;hreli ].
here q denotes a random query vector.",5.3 Instance Set Aggregation,[0],[0]
"The bag representation B, which is the weighted sum of its sentences, is then concatenated with the entity type embeddings of the subject (htypesub ) and object
(htypeobj ) from Section 5.2 to obtain B̂.
B̂ = [B;htypesub ;h type obj ] where, B = n∑ i=1 αiŝi.
",5.3 Instance Set Aggregation,[0],[0]
"Finally, B̂ is fed to a softmax classifier to get the probability distribution over the relations.
p(y) =",5.3 Instance Set Aggregation,[0.9597789581635369],"['Finally, B̂ is fed to a softmax classifier to get the probability distribution over the relations.']"
Softmax(W · B̂ + b).,5.3 Instance Set Aggregation,[0],[0]
"In our experiments, we evaluate the models on Riedel and Google Distant Supervision (GIDS) dataset.",6.1 Datasets,[0],[0]
Statistics of the datasets is summarized in Table 1.,6.1 Datasets,[0],[0]
"Below we described each in detail1.
1.",6.1 Datasets,[0],[0]
Riedel:,6.1 Datasets,[0],[0]
"The dataset is developed by (Riedel et al., 2010) by aligning Freebase relations with New York Times (NYT) corpus, where sentences from the year 2005-2006 are used for creating the training set and from the year 2007 for the test set.",6.1 Datasets,[0.9982814766675551],"['Riedel: The dataset is developed by (Riedel et al., 2010) by aligning Freebase relations with New York Times (NYT) corpus, where sentences from the year 2005-2006 are used for creating the training set and from the year 2007 for the test set.']"
"The entity mentions are annotated using Stanford NER (Finkel et al., 2005) and are linked to Freebase.",6.1 Datasets,[0],[0]
"The dataset has been widely used for RE by (Hoffmann et al., 2011; Surdeanu et al., 2012) and more recently by (Lin et al., 2016; Feng et al.;",6.1 Datasets,[0],[0]
"He et al., 2018).
2.",6.1 Datasets,[0],[0]
GIDS:,6.1 Datasets,[0],[0]
Jat et al. (2018) created Google Distant Supervision (GIDS) dataset by extending the Google relation extraction corpus2 with additional instances for each entity pair.,6.1 Datasets,[0],[0]
"The dataset assures that the at-least-one assumption of multi-instance learning, holds.",6.1 Datasets,[0],[0]
"This makes automatic evaluation more reliable and thus removes the need for manual verification.
",6.1 Datasets,[0],[0]
1Data splits and hyperparameters are in supplementary.,6.1 Datasets,[0],[0]
"2https://research.googleblog.com/2013/04/50000-
lessons-on-how-to-read-relation.html",6.1 Datasets,[0],[0]
"For evaluating RESIDE, we compare against the following baselines:
• Mintz: Multi-class logistic regression model proposed by (Mintz et al., 2009) for distant supervision paradigm.",6.2 Baselines,[0],[0]
"• MultiR: Probabilistic graphical model for multi
instance learning by (Hoffmann et al., 2011)",6.2 Baselines,[0],[0]
"• MIMLRE: A graphical model which jointly
models multiple instances and multiple labels.",6.2 Baselines,[0],[0]
"More details in (Surdeanu et al., 2012).",6.2 Baselines,[0],[0]
• PCNN:,6.2 Baselines,[0],[0]
"A CNN based relation extraction model
by (Zeng et al., 2015) which uses piecewise max-pooling for sentence representation.",6.2 Baselines,[0],[0]
• PCNN+ATT,6.2 Baselines,[0],[0]
": A piecewise max-pooling over
CNN based model which is used by (Lin et al., 2016) to get sentence representation followed by attention over sentences.",6.2 Baselines,[0],[0]
"• BGWA: Bi-GRU based relation extraction
model with word and sentence level attention (Jat et al., 2018).",6.2 Baselines,[1.0000000239581468],"['• BGWA: Bi-GRU based relation extraction model with word and sentence level attention (Jat et al., 2018).']"
•,6.2 Baselines,[0],[0]
RESIDE:,6.2 Baselines,[0],[0]
"The method proposed in this paper,
please refer Section 5 for more details.",6.2 Baselines,[0],[0]
"Following the prior works (Lin et al., 2016; Feng et al.), we evaluate the models using held-out evaluation scheme.",6.3 Evaluation Criteria,[0],[0]
This is done by comparing the relations discovered from test articles with those in Freebase.,6.3 Evaluation Criteria,[0],[0]
We evaluate the performance of models with Precision-Recall curve and top-N precision (P@N) metric in our experiments.,6.3 Evaluation Criteria,[0],[0]
"In this section we attempt to answer the following questions:
Q1.",7 Results,[0],[0]
Is RESIDE more effective than existing approaches for distant supervised RE?,7 Results,[0],[0]
"(7.1)
Q2.",7 Results,[0],[0]
What is the effect of ablating different components on RESIDE’s performance?,7 Results,[0],[0]
"(7.2)
Q3.",7 Results,[0],[0]
How is the performance affected in the absence of relation alias information?,7 Results,[0],[0]
(7.3),7 Results,[0],[0]
"For evaluating the effectiveness of our proposed method, RESIDE, we compare it against the baselines stated in Section 6.2.",7.1 Performance Comparison,[0],[0]
We use only the neural baselines on GIDS dataset.,7.1 Performance Comparison,[0],[0]
The Precision-Recall curves on Riedel and GIDS are presented in Figure 3.,7.1 Performance Comparison,[0],[0]
"Overall, we find that RESIDE achieves higher precision over the entire recall range on both the datasets.",7.1 Performance Comparison,[1.0],"['Overall, we find that RESIDE achieves higher precision over the entire recall range on both the datasets.']"
All the non-neural baselines could not perform well as the features used by them are mostly derived from NLP tools which can be erroneous.,7.1 Performance Comparison,[0],[0]
RESIDE outperforms PCNN+ATT and BGWA which indicates that incorporating side information helps in improving the performance of the model.,7.1 Performance Comparison,[0],[0]
The higher performance of BGWA and PCNN+ATT over PCNN shows that attention helps in distant supervised RE.,7.1 Performance Comparison,[0],[0]
"Following (Lin et al., 2016; Liu et al., 2017), we also evaluate our method with different number of sentences.",7.1 Performance Comparison,[0],[0]
"Results summarized in Table 2, show the improved precision of RESIDE in all test settings, as compared to the neural baselines, which demonstrates
the efficacy of our model.",7.1 Performance Comparison,[0],[0]
"In this section, we analyze the effect of various components of RESIDE on its performance.",7.2 Ablation Results,[0],[0]
"For this, we evaluate various versions of our model with cumulatively removed components.",7.2 Ablation Results,[0],[0]
The experimental results are presented in Figure 4.,7.2 Ablation Results,[0],[0]
"We observe that on removing different components from RESIDE, the performance of the model degrades drastically.",7.2 Ablation Results,[0],[0]
The results validate that GCNs are effective at encoding syntactic information.,7.2 Ablation Results,[0],[0]
"Further, the improvement from side information shows that it is complementary to the features extracted from text, thus validating the central thesis of this paper, that inducing side information leads to improved relation extraction.",7.2 Ablation Results,[0],[0]
"In this section, we test the performance of the model in setting where relation alias information is not readily available.",7.3 Effect of Relation Alias Side Information,[0],[0]
"For this, we evaluate the performance of the model on four different settings:",7.3 Effect of Relation Alias Side Information,[0.9502410793732436],"['For this, we evaluate the performance of the model on four different settings: • None: Relation aliases are not available.']"
"• None: Relation aliases are not available.
",7.3 Effect of Relation Alias Side Information,[0],[0]
• One: The name of relation is used as its alias.,7.3 Effect of Relation Alias Side Information,[1.0],['• One: The name of relation is used as its alias.']
"• One+PPDB: Relation name extended using
Paraphrase Database (PPDB).",7.3 Effect of Relation Alias Side Information,[0],[0]
•,7.3 Effect of Relation Alias Side Information,[0],[0]
"All: Relation aliases from Knowledge Base3
The overall results are summarized in Figure 5.",7.3 Effect of Relation Alias Side Information,[0],[0]
We find that the model performs best when aliases are provided by the KB itself.,7.3 Effect of Relation Alias Side Information,[0],[0]
"Overall, we find that RESIDE gives competitive performance even when very limited amount of relation alias information is available.",7.3 Effect of Relation Alias Side Information,[0],[0]
We observe that performance improves further with the availability of more alias information.,7.3 Effect of Relation Alias Side Information,[0],[0]
"In this paper, we propose RESIDE, a novel neural network based model which makes principled use of relevant side information, such as entity type and relation alias, from Knowledge Base, for improving distant supervised relation extraction.",8 Conclusion,[0],[0]
"RESIDE employs Graph Convolution Networks for
3Each relation in Riedel dataset is manually mapped to corresponding Wikidata property for getting relation aliases.",8 Conclusion,[0],[0]
"Few examples are presented in supplementary material.
encoding syntactic information of sentences and is robust to limited side information.",8 Conclusion,[0],[0]
"Through extensive experiments on benchmark datasets, we demonstrate RESIDE’s effectiveness over stateof-the-art baselines.",8 Conclusion,[0],[0]
We have made RESIDE’s source code publicly available to promote reproducible research.,8 Conclusion,[1.0],['We have made RESIDE’s source code publicly available to promote reproducible research.']
We thank the anonymous reviewers for their constructive comments.,Acknowledgements,[0],[0]
"This work is supported in part by the Ministry of Human Resource Development (Government of India), CAIR (DRDO) and by a gift from Google.",Acknowledgements,[0],[0]
Distantly-supervised Relation Extraction (RE) methods train an extractor by automatically aligning relation instances in a Knowledge Base (KB) with unstructured text.,abstractText,[0],[0]
"In addition to relation instances, KBs often contain other relevant side information, such as aliases of relations (e.g., founded and co-founded are aliases for the relation founderOfCompany).",abstractText,[0],[0]
RE models usually ignore such readily available side information.,abstractText,[0],[0]
"In this paper, we propose RESIDE, a distantly-supervised neural relation extraction method which utilizes additional side information from KBs for improved relation extraction.",abstractText,[0],[0]
It uses entity type and relation alias information for imposing soft constraints while predicting relations.,abstractText,[0],[0]
RESIDE employs Graph Convolution Networks (GCN) to encode syntactic information from text and improves performance even when limited side information is available.,abstractText,[0],[0]
"Through extensive experiments on benchmark datasets, we demonstrate RESIDE’s effectiveness.",abstractText,[0],[0]
We have made RESIDE’s source code available to encourage reproducible research.,abstractText,[0],[0]
RESIDE: Improving Distantly-Supervised Neural Relation Extraction using Side Information,title,[0],[0]
