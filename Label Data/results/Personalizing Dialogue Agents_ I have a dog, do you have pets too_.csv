0,1,label2,summary_sentences
"Deep neural networks achieve near-human accuracy on many perception tasks (He et al., 2016; Amodei et al., 2015).",1. Introduction,[0],[0]
"However, they lack robustness to small alterations of the inputs at test time (Szegedy et al., 2014).",1. Introduction,[0],[0]
"Indeed when presented with a corrupted image that is barely distinguishable from a legitimate one by a human, they can predict incorrect labels, with high-confidence.",1. Introduction,[0],[0]
"An adversary can design such so-called adversarial examples, by adding a small perturbation to a legitimate input to maximize the likelihood of an incorrect class under constraints on the magnitude of the perturbation (Szegedy et al., 2014; Goodfellow et al., 2015; Moosavi-Dezfooli et al., 2015; Pa-
1Facebook AI Research.",1. Introduction,[0],[0]
"Correspondence to: Moustapha Cisse <moustaphacisse@fb.com>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"pernot et al., 2016a).",1. Introduction,[0],[0]
"In practice, for a significant portion of inputs, a single step in the direction of the gradient sign is sufficient to generate an adversarial example (Goodfellow et al., 2015) that is even transferable from one network to another one trained for the same problem but with a different architecture (Liu et al., 2016; Kurakin et al., 2016).
",1. Introduction,[0],[0]
The existence of transferable adversarial examples has two undesirable corollaries.,1. Introduction,[0],[0]
"First, it creates a security threat for production systems by enabling black-box attacks (Papernot et al., 2016a).",1. Introduction,[0],[0]
"Second, it underlines the lack of robustness of neural networks and questions their ability to generalize in settings where the train and test distributions can be (slightly) different as is the case for the distributions of legitimate and adversarial examples.
",1. Introduction,[0],[0]
"Whereas the earliest works on adversarial examples already suggested that their existence was related to the magnitude of the hidden activations gradient with respect to their inputs (Szegedy et al., 2014), they also empirically assessed that standard regularization schemes such as weight decay or training with random noise do not solve the problem (Goodfellow et al., 2015; Fawzi et al., 2016).",1. Introduction,[0],[0]
The current mainstream approach to improving the robustness of deep networks is adversarial training.,1. Introduction,[0],[0]
"It consists in generating adversarial examples on-line using the current network’s parameters (Goodfellow et al., 2015; Miyato et al., 2015; Moosavi-Dezfooli et al., 2015; Szegedy et al., 2014; Kurakin et al., 2016) and adding them to the training data.",1. Introduction,[0],[0]
"This data augmentation method can be interpreted as a robust optimization procedure (Shaham et al., 2015).
",1. Introduction,[0],[0]
"In this paper, we introduce Parseval networks, a layerwise regularization method for reducing the network’s sensitivity to small perturbations by carefully controlling its global Lipschitz constant.",1. Introduction,[0],[0]
"Since the network is a composition of functions represented by its layers, we achieve increased robustness by maintaining a small Lipschitz constant (e.g., 1) at every hidden layer; be it fully-connected, convolutional or residual.",1. Introduction,[0],[0]
"In particular, a critical quantity governing the local Lipschitz constant in both fully connected and convolutional layers is the spectral norm of the weight matrix.",1. Introduction,[0],[0]
"Our main idea is to control this norm by parameterizing the network with parseval tight frames (Kovačević & Chebira, 2008), a generalization of orthogonal matrices.
",1. Introduction,[0],[0]
"The idea that regularizing the spectral norm of each weight
matrix could help in the context of robustness appeared as early as (Szegedy et al., 2014), but no experiment nor algorithm was proposed, and no clear conclusion was drawn on how to deal with convolutional layers.",1. Introduction,[0],[0]
"Previous work, such as double backpropagation (Drucker & Le Cun, 1992) has also explored jacobian normalization as a way to improve generalization.",1. Introduction,[0],[0]
Our contribution is twofold.,1. Introduction,[0],[0]
"First, we provide a deeper analysis which applies to fully connected networks, convolutional networks, as well as Residual networks (He et al., 2016).",1. Introduction,[0],[0]
"Second, we propose a computationally efficient algorithm and validate its effectiveness on standard benchmark datasets.",1. Introduction,[0],[0]
"We report results on MNIST, CIFAR-10, CIFAR-100 and Street View House Numbers (SVHN), in which fully connected and wide residual networks were trained (Zagoruyko & Komodakis, 2016) with Parseval regularization.",1. Introduction,[0],[0]
"The accuracy of Parseval networks on legitimate test examples matches the state-of-the-art, while the results show notable improvements on adversarial examples.",1. Introduction,[0],[0]
"Besides, Parseval networks train significantly faster than their vanilla counterpart.
",1. Introduction,[0],[0]
"In the remainder of the paper, we first discuss the previous work on adversarial examples.",1. Introduction,[0],[0]
"Next, we give formal definitions of the adversarial examples and provide an analysis of the robustness of deep neural networks.",1. Introduction,[0],[0]
"Then, we introduce Parseval networks and its efficient training algorithm.",1. Introduction,[0],[0]
Section 5 presents experimental results validating the model and providing several insights.,1. Introduction,[0],[0]
"Early papers on adversarial examples attributed the vulnerability of deep networks to high local variations (Szegedy et al., 2014; Goodfellow et al., 2015).",2. Related work,[0],[0]
"Some authors argued that this sensitivity of deep networks to small changes in their inputs is because neural networks only learn the discriminative information sufficient to obtain good accuracy rather than capturing the true concepts defining the classes (Fawzi et al., 2015; Nguyen et al., 2015).
",2. Related work,[0],[0]
"Strategies to improve the robustness of deep networks include defensive distillation (Papernot et al., 2016b), as well as various regularization procedures such as contractive networks (Gu & Rigazio, 2015).",2. Related work,[0],[0]
"However, the bulk of recent proposals relies on data augmentation (Goodfellow et al., 2015; Miyato et al., 2015; Moosavi-Dezfooli et al., 2015; Shaham et al., 2015; Szegedy et al., 2014; Kurakin et al., 2016).",2. Related work,[0],[0]
It uses adversarial examples generated online during training.,2. Related work,[0],[0]
"As we shall see in the experimental section, regularization can be complemented with data augmentation; in particular, Parseval networks with data augmentation appear more robust than either data augmentation or Parseval networks considered in isolation.",2. Related work,[0],[0]
"We consider a multiclass prediction setting, where we have Y classes in Y = {1, ..., Y }.",3. Robustness in Neural Networks,[0],[0]
"A multiclass classifier is a function ĝ : (x ∈ RD,W ∈ W) 7→ argmaxȳ∈Y",3. Robustness in Neural Networks,[0],[0]
"gȳ(x,W ), where W are the parameters to be learnt, and gȳ(x,W ) is the score given to the (input, class) pair (x, ȳ) by a function g :",3. Robustness in Neural Networks,[0],[0]
RD × W → RY .,3. Robustness in Neural Networks,[0],[0]
"We take g to be a neural network, represented by a computation graph G = (N , E), which is a directed acyclic graph with a single root node, and each node n ∈ N takes values in Rd (n) out and is a function of its children in the graph, with learnable parameters W (n):
n : x 7→ φ(n) ( W (n), ( n′(x) )",3. Robustness in Neural Networks,[0],[0]
"n′:(n,n′)∈E ) .",3. Robustness in Neural Networks,[0],[0]
"(1)
The function g we want to learn is the root of G. The training data ((xi, yi))mi=1 ∈",3. Robustness in Neural Networks,[0],[0]
"(X × Y)m is an i.i.d. sample of D, and we assume X ⊂",3. Robustness in Neural Networks,[0],[0]
RD is compact.,3. Robustness in Neural Networks,[0],[0]
"A function ` : RY × Y → R measures the loss of g on an example (x, y); in a single-label classification setting for instance, a common choice for ` is the log-loss:
` ( g(x,W ), y ) = −gy(x,W ) + log (∑ ȳ∈Y egȳ(x,W ) ) .",3. Robustness in Neural Networks,[0],[0]
"(2)
The arguments that we develop below depend only on the Lipschitz constant of the loss, with respect to the norm of interest.",3. Robustness in Neural Networks,[0],[0]
"Formally, we assume that given a p-norm of interest ‖.‖p, there is a constant λp such that
∀z, z′ ∈ RY ,∀ȳ ∈ Y, |`(z, ȳ)−`(z′, ȳ)| ≤",3. Robustness in Neural Networks,[0],[0]
"λp‖z−z′‖p .
",3. Robustness in Neural Networks,[0],[0]
"For the log-loss of (2), we have λ2 ≤ √
2 and λ∞ ≤ 2.",3. Robustness in Neural Networks,[0],[0]
"In the next subsection, we define adversarial examples and the generalization performance of the classifier.",3. Robustness in Neural Networks,[0],[0]
"Then, we make the relationship between robustness to adversarial examples and the lipschitz constant of the networks.",3. Robustness in Neural Networks,[0.9560605388330247],"['We focus on the standard dialogue task of predicting the next utterance given the dialogue history, but consider this task both with and without the profile information being given to the learning agent.']"
"Given an input (train or test) example (x, y), an adversarial example is a perturbation of the input pattern x̃ = x",3.1. Adversarial examples,[0],[0]
"+ δx where δx is small enough so that x̃ is nearly undistinguishable from x (at least from the point of view of a human annotator), but has the network predict an incorrect label.",3.1. Adversarial examples,[0],[0]
"Given the network parameters and structure g(.,W ) and a p-norm, the adversarial example is formally defined as
x̃ = argmax x̃:‖x̃−x‖p≤
` ( g(x̃,W ), y ) , (3)
where represents the strength of the adversary.",3.1. Adversarial examples,[0],[0]
"Since the optimization problem above is non-convex, Shaham et al. (2015) propose to take the first order taylor expansion of x 7→ `(g(x,W ), y) to compute δx by solving
x̃ = argmax x̃:‖x̃−x‖p≤
( ∇x`(g(x,W ), y) )",3.1. Adversarial examples,[0],[0]
T (x̃− x) .,3.1. Adversarial examples,[0],[0]
"(4)
If p = ∞, then x̃ = x + sign(∇x`(g(x,W ), y)).",3.1. Adversarial examples,[0],[0]
This is the fast gradient sign method.,3.1. Adversarial examples,[0],[0]
"For the case p = 2, we obtain x̃ = x + ∇x`(g(x,W ), y).",3.1. Adversarial examples,[0],[0]
"A more involved method is the iterative fast gradient sign method, in which several gradient steps of (4) are performed with a smaller stepsize to obtain a local minimum of (3).",3.1. Adversarial examples,[0],[0]
"In the context of adversarial examples, there are two different generalization errors of interest:
L(W )",3.2. Generalization with adversarial examples,[0],[0]
"= E (x,y)∼D
[ `(g(x,W ), y) ] ,
Ladv(W,p, ) =",3.2. Generalization with adversarial examples,[0],[0]
"E (x,y)∼D
[ max
x̃:‖x̃−x‖p≤ `(g(x̃,W ), y)
] .
",3.2. Generalization with adversarial examples,[0],[0]
"By definition, L(W )",3.2. Generalization with adversarial examples,[0],[0]
"≤ Ladv(W,p, ) for every p and >0.",3.2. Generalization with adversarial examples,[0],[0]
"Reciprocally, denoting by λp and Λp the Lipschitz constant (with respect to ‖.‖p) of ` and g respectively, we have:
Ladv(W,p, ) ≤ L(W )",3.2. Generalization with adversarial examples,[0],[0]
+,3.2. Generalization with adversarial examples,[0],[0]
"E
(x,y)∼D
[ max
x̃:‖x̃−x‖p≤ |`(g(x̃,W ), y)− `(g(x,W ), y)| ] ≤ L(W ) +",3.2. Generalization with adversarial examples,[0],[0]
"λpΛp .
",3.2. Generalization with adversarial examples,[0],[0]
This suggests that the sensitivity to adversarial examples can be controlled by the Lipschitz constant of the network.,3.2. Generalization with adversarial examples,[0],[0]
"In the robustness framework of (Xu & Mannor, 2012), the Lipschitz constant also controls the difference between the average loss on the training set and the generalization performance.",3.2. Generalization with adversarial examples,[0],[0]
"More precisely, let us denote by Cp(X , γ) the covering number of X using γ-balls for ‖.‖p.",3.2. Generalization with adversarial examples,[0],[0]
"Using M = supx,W,y `(g(x,W ), y), Theorem 3 of (Xu & Mannor, 2012) implies that for every δ ∈ (0, 1), with probability 1− δ over the i.i.d. sample ((xi, yi)mi=1, we have:
L(W )",3.2. Generalization with adversarial examples,[0],[0]
≤ 1 m m∑ i=1,3.2. Generalization with adversarial examples,[0],[0]
"`(g(xi,W ), yi)
+ λpΛpγ",3.2. Generalization with adversarial examples,[0],[0]
"+M
√ 2Y Cp(X , γ2 )",3.2. Generalization with adversarial examples,[0],[0]
"ln(2)− 2 ln(δ)
m .
",3.2. Generalization with adversarial examples,[0],[0]
"Since covering numbers of a p-norm ball in RD increases exponentially with RD, the bound above suggests that it is critical to control the Lipschitz constant of g, for both good generalization and robustness to adversarial examples.",3.2. Generalization with adversarial examples,[0.9516598918873991],"['The model we choose is identical to the profile memory network just described in the first hop over profiles, while in the second hop, q+ is used to attend over the keys and output a weighted sum of values as before, producing q++.']"
"From the network structure we consider (1), for every node n ∈",3.3. Lipschitz constant of neural networks,[0],[0]
"N , we have (see below for the definition of Λ(n,n ′) p ):
",3.3. Lipschitz constant of neural networks,[0],[0]
"‖n(x)− n(x̃)‖p ≤ ∑
n′:(n,n′)∈E
Λ(n,n ′) p ‖n′(x)− n′(x̃)‖p ,
for any Λ(n,n ′)
p that is greater than the worst case variation of n with respect to a change in its input n′(x).",3.3. Lipschitz constant of neural networks,[0],[0]
"In particular we can take for Λ(n,n ′)",3.3. Lipschitz constant of neural networks,[0],[0]
"p any value greater than the
supremum over x0 ∈ X of the Lipschitz constant for ‖.‖p of the function (1n′′ = n′ is 1 if n′′ = n′ and 0 otherwise):
x 7→ φ(n) ( W (n), ( n′′(x0+1n ′′ = n′(x−x0)) )",3.3. Lipschitz constant of neural networks,[0],[0]
"n′′:(n,n′′)∈E ) .
",3.3. Lipschitz constant of neural networks,[0],[0]
"The Lipschitz constant of n, denoted by Λ(n)p satisfies:
Λ(n)p ≤ ∑
n′:(n,n′)∈E
Λ(n,n ′) p Λ (n′) p (5)
",3.3. Lipschitz constant of neural networks,[0],[0]
"Thus, the Lipschitz constant of the network g can grow exponentially with its depth.",3.3. Lipschitz constant of neural networks,[0],[0]
"We now give the Lipschitz constants of standard layers as a function of their parameters:
Linear layers: For layer n(x) = W (n)n′(x) where n′ is the unique child of n in the graph, the Lipschitz constant for ‖.‖p is, by definition, the matrix norm of W (n) induced by ‖.‖p, which is usually denoted ‖W (n)‖p and defined by
‖W (n)‖p = sup z:‖z‖p=1 ‖W (n)z‖p .
",3.3. Lipschitz constant of neural networks,[0],[0]
"Then Λ(n)2 = ‖W (n)‖2Λ (n′) 2 , where ‖W (n)‖2, called the spectral norm of W (n), is the maximum singular value of W (n).",3.3. Lipschitz constant of neural networks,[0],[0]
"We also have Λ(n)∞ = ‖W (n)‖∞Λ(n ′) ∞ , where
‖W (n)‖∞ = maxi ∑ j |W (n)",3.3. Lipschitz constant of neural networks,[0],[0]
ij,3.3. Lipschitz constant of neural networks,[0],[0]
| is the maximum 1-norm of the rows.,3.3. Lipschitz constant of neural networks,[0],[0]
"W (n).
",3.3. Lipschitz constant of neural networks,[0],[0]
"Convolutional layers: To simplify notation, let us consider convolutions on 1D inputs without striding, and we take the width of the convolution to be 2k + 1 for k ∈ N. To write convolutional layers in the same way as linear layers, we first define an unfolding operator U , which prepares the input z, denoted by U(z).",3.3. Lipschitz constant of neural networks,[0],[0]
"If the input has length T with din inputs channels, the unfolding operator maps z",3.3. Lipschitz constant of neural networks,[0],[0]
"For a convolution of the unfolding of z considered as a T × (2k + 1)din matrix, its j-th column is:
Uj(z) =",3.3. Lipschitz constant of neural networks,[0],[0]
"[zj−k; ...; zj+k] ,
where “;” is the concatenation along the vertical axis (each zi is seen as a column din-dimensional vector), and zi",3.3. Lipschitz constant of neural networks,[0],[0]
= 0,3.3. Lipschitz constant of neural networks,[0],[0]
if i is out of bounds (0-padding).,3.3. Lipschitz constant of neural networks,[0],[0]
"A convolutional layer with dout output channels is then defined as
n(x) =",3.3. Lipschitz constant of neural networks,[0],[0]
"W (n) ∗ n′(x) = W (n)U(n′(x)) ,
where W (n) is a dout × (2k + 1)din matrix.",3.3. Lipschitz constant of neural networks,[0],[0]
"We thus have Λ
(n) 2 ≤ ‖W‖2‖U(n′(x))‖2.",3.3. Lipschitz constant of neural networks,[0],[0]
"Since U is a linear operator that essentially repeats its input (2k + 1) times, we have ‖U(n′(x))",3.3. Lipschitz constant of neural networks,[0],[0]
− U(n′(x̃))‖22 ≤ (2k + 1)‖n′(x),3.3. Lipschitz constant of neural networks,[0],[0]
"− n′(x̃)‖22, so that Λ(n)2 ≤ √ 2k + 1‖W‖2Λ(n ′) 2 .",3.3. Lipschitz constant of neural networks,[0],[0]
"Also, ‖U(n′(x))",3.3. Lipschitz constant of neural networks,[0],[0]
"− U(n′(x̃))‖∞ = ‖n′(x) − n′(x̃)‖∞, and so for a convolutional layer, Λ(n)∞ ≤ ‖W (n)‖∞Λ(n ′) ∞ .
",3.3. Lipschitz constant of neural networks,[0],[0]
"Aggregation layers/transfer functions: Layers that perform the sum of their inputs, as in Residual Netowrks (He et al., 2016), fall in the case where the values Λ(n,n ′) p in (5) come into play.",3.3. Lipschitz constant of neural networks,[0],[0]
"For a node n that sums its inputs, we have Λ (n,n′) p = 1, and thus Λ (n) p ≤ ∑ n′:(n,n′)∈E Λ (n′) p .",3.3. Lipschitz constant of neural networks,[0],[0]
"If n is a tranfer function layer (e.g., an element-wise application of ReLU)",3.3. Lipschitz constant of neural networks,[0],[0]
"we can check that Λ(n)p ≤ Λ(n ′) p , where n′ is the input node, as soon as the Lipschitz constant of the transfer function (as a function R→ R) is ≤ 1.",3.3. Lipschitz constant of neural networks,[0],[0]
"Parseval regularization, which we introduce in this section, is a regularization scheme to make deep neural networks robust, by constraining the Lipschitz constant (5) of each hidden layer to be smaller than one, assuming the Lipschitz constant of children nodes is smaller than one.",4. Parseval networks,[0],[0]
"That way, we avoid the exponential growth of the Lipschitz constant, and a usual regularization scheme (i.e., weight decay) at the last layer then controls the overall Lipschitz constant of the network.",4. Parseval networks,[0],[0]
"To enforce these constraints in practice, Parseval networks use two ideas: maintaining orthonormal rows in linear/convolutional layers, and performing convex combinations in aggregation layers.",4. Parseval networks,[0],[0]
"Below, we first explain the rationale of these constraints and then describe our approach to efficiently enforce the constraints during training.",4. Parseval networks,[0],[0]
"Orthonormality of weight matrices: For linear layers, we need to maintain the spectral norm of the weight matrix at 1.",4.1. Parseval Regularization,[0],[0]
Computing the largest singular value of weight matrices is not practical in an SGD setting unless the rows of the matrix are kept orthogonal.,4.1. Parseval Regularization,[0],[0]
"For a weight matrix W ∈ Rdout×din with dout ≤ din, Parseval regularization maintains WTW",4.1. Parseval Regularization,[0],[0]
"≈ Idout×dout , where I refers to the identity matrix.",4.1. Parseval Regularization,[0],[0]
"W is then approximately a Parseval tight frame (Kovačević & Chebira, 2008), hence the name of Parseval networks.",4.1. Parseval Regularization,[0],[0]
"For convolutional layers, the matrix W ∈ Rdout×(2k+1)din is constrained to be a Parseval tight frame (with the notations of the previous section), and the output is rescaled by a factor (2k + 1)−1/2.",4.1. Parseval Regularization,[0],[0]
"This maintains all singular values of W to (2k+ 1)−1/2, so that Λ
(n) 2 ≤ Λ (n′) 2 where n
′ is the input node.",4.1. Parseval Regularization,[0],[0]
"More generally, keeping the rows of weight matrices orthogonal makes it possible to control both the spectral norm and the ‖.‖∞ of a weight matrix through the norm of its individual rows.",4.1. Parseval Regularization,[0],[0]
Robustness for ‖.‖∞ is achieved by rescaling the rows so that their 1-norm is smaller than 1.,4.1. Parseval Regularization,[0],[0]
"For now, we only experimented with constraints on the 2-norm of the rows, so we aim for robustness in the sense of ‖.‖2.
",4.1. Parseval Regularization,[0],[0]
Remark 1 (Orthogonality is required).,4.1. Parseval Regularization,[0],[0]
"Without orthogonality, constraints on the 2-norm of the rows of weight ma-
trices are not sufficient to control the spectral norm.",4.1. Parseval Regularization,[0],[0]
"Parseval networks are thus fundamentally different from weight normalization (Salimans & Kingma, 2016).
",4.1. Parseval Regularization,[0],[0]
Aggregation Layers:,4.1. Parseval Regularization,[0],[0]
"In parseval networks, aggregation layers do not make the sum of their inputs, but rather take a convex combination of them:
n(x) = ∑
n′:(n,n′)∈E
α(n,n ′)n′(x)
with ∑ n′:(n,n′)∈E α (n,n′) = 1 and α(n,n ′) ≥ 0.",4.1. Parseval Regularization,[0],[0]
"The parameters α(n,n ′) are learnt, but using (5), these constraint guarantee that Λ(n)p ≤ 1 as soon as the children satisfy the inequality for the same p-norm.",4.1. Parseval Regularization,[0],[0]
Orthonormality constraints: The first significant difference between Parseval networks and its vanilla counterpart is the orthogonality constraint on the weight matrices.,4.2. Parseval Training,[0],[0]
"This requirement calls for an optimization algorithm on the manifold of orthogonal matrices, namely the Stiefel manifold.",4.2. Parseval Training,[0],[0]
"Optimization on matrix manifolds is a well-studied topic (see (Absil et al., 2009) for a comprehensive survey).",4.2. Parseval Training,[0],[0]
The simplest first-order geometry approaches consist in optimizing the unconstrained function of interest by moving in the direction of steepest descent (given by the gradient of the function) while at the same time staying on the manifold.,4.2. Parseval Training,[0],[0]
"To guarantee that we remain in the manifold after every parameter update, we need to define a retraction operator.",4.2. Parseval Training,[0],[0]
"There exist several pullback operators for embedded submanifolds such as the Stiefel manifold based for example on Cayley transforms (Absil et al., 2009).",4.2. Parseval Training,[0],[0]
"However, when learning the parameters of neural networks, these methods are computationally prohibitive.",4.2. Parseval Training,[0],[0]
"To overcome this difficulty, we use an approximate operator derived from the following layer-wise regularizer of weight matrices to ensure their parseval tightness (Kovačević & Chebira, 2008):
Rβ(Wk) =",4.2. Parseval Training,[0],[0]
"β
2 ‖W>k Wk − I‖22.
Optimizing Rβ(Wk) to convergence after every gradient descent step (w.r.t the main objective) guarantees us to stay on the desired manifold but this is an expensive procedure.",4.2. Parseval Training,[0],[0]
"Moreover, it may result in parameters that are far from the ones obtained after the main gradient update.",4.2. Parseval Training,[0],[0]
"We use two approximations to make the algorithm more efficient: First, we only do one step of descent on the function Rα(Wk).",4.2. Parseval Training,[0],[0]
The gradient of this regularization term is∇WkRβ(Wk) =,4.2. Parseval Training,[0],[0]
β(WkW > k − I)Wk.,4.2. Parseval Training,[0],[0]
"Consequently, after every main update we perform the following secondary update:
Wk ← (1 + β)Wk",4.2. Parseval Training,[0],[0]
"− βWkW>k Wk.
Algorithm 1 Parseval Training Θ = {Wk,αk}Kk=1, e← 0",4.2. Parseval Training,[0],[0]
"while e ≤ E do
Sample a minibatch {(xi, yi)}Bi=1.",4.2. Parseval Training,[0],[0]
"for k ∈ {1, . . .",4.2. Parseval Training,[0],[0]
",K} do
Compute the gradient:",4.2. Parseval Training,[0],[0]
"GWk ← ∇Wk`(Θ, {(xi, yi)}), Gαk ← ∇αk`(Θ, {(xi, yi)}).",4.2. Parseval Training,[0],[0]
Update the parameters: Wk ←Wk − ·GWk αk ← αk − ·Gαk .,4.2. Parseval Training,[0],[0]
"if hidden layer then
Sample a subset S of rows of Wk.",4.2. Parseval Training,[0],[0]
Projection: WS ← (1 + β)WS,4.2. Parseval Training,[0],[0]
− βWSW>S WS .,4.2. Parseval Training,[0],[0]
αk ← argminγ∈∆K−1‖αK,4.2. Parseval Training,[0],[0]
"− γ‖22
e← e+ 1.
Optionally, instead of updating the whole matrix, one can randomly select a subset S of rows and perform the update from Eq.",4.2. Parseval Training,[0],[0]
(4.2) on the submatrix composed of rows indexed by S. This sampling based approach reduces the overall complexity to O(|S|2d).,4.2. Parseval Training,[0],[0]
"Provided the rows are carefully sampled, the procedure is an accurate Monte Carlo approximation of the regularizer loss function (Drineas et al., 2006).",4.2. Parseval Training,[0],[0]
"The optimal sampling probabilities, also called statistical leverages are approximately equal if we start from an orthogonal matrix and (approximately) stay on the manifold throughout the optimization since they are proportional to the eigenvalues of W (Mahoney et al., 2011).",4.2. Parseval Training,[0],[0]
"Therefore, we can sample a subset of columns uniformly at random when applying this projection step.
",4.2. Parseval Training,[0],[0]
"While the full update does not result in an increased overhead for convolutional layers, the picture can be very different for large fully connected layers making the sampling approach computationally more appealing for such layers.",4.2. Parseval Training,[0],[0]
We show in the experiments that the weight matrices resulting from this procedure are (quasi)-orthogonal.,4.2. Parseval Training,[0],[0]
"Also, note that quasi-orthogonalization procedures similar to the one described here have been successfully used previously in the context of learning overcomplete representations with independent component analysis (Hyvärinen & Oja, 2000).
",4.2. Parseval Training,[0],[0]
"Convexity constraints in aggregation layers: In Parseval networks, aggregation layers output a convex combination of their inputs instead of e.g., their sum as in Residual networks (He et al., 2016).",4.2. Parseval Training,[0],[0]
"For an aggregation node n of the network, let us denote by α = (α(n,n
′))n′:(n,n′)∈E the K-size vector of coefficients used for the convex combination output by the layer.",4.2. Parseval Training,[0],[0]
"To ensure that the Lipschitz constant at the node n is such that Λ(n)p ≤ 1, the constraints of 4.1 call for a euclidean projection of α onto the positive simplex after a gradient update:
α∗ = argmin γ∈∆K−1 ‖α− γ‖22 ,
where ∆K−1 = {γ ∈ RK |1>γ = 1,γ ≥ 0}.",4.2. Parseval Training,[0],[0]
"This is a well studied problem (Michelot, 1986; Pardalos & Kovoor, 1990; Duchi et al., 2008; Condat, 2016).",4.2. Parseval Training,[0],[0]
"Its solution is of the form: α∗i = max(0, αi − τ(α)), with τ : RK → R the unique function satisfying ∑ i(xi − τ(α))",4.2. Parseval Training,[0],[0]
= 1 for every x ∈ RK .,4.2. Parseval Training,[0],[0]
"Therefore, the solution essentially boils down to a soft thresholding operation.",4.2. Parseval Training,[0],[0]
If we denote α1 ≥ α2 ≥ . . .,4.2. Parseval Training,[0],[0]
"αK the sorted coefficients and k(α) = max{k ∈ (1, . . .",4.2. Parseval Training,[0],[0]
",K)|1+kαk > ∑ j≤k αj}, the optimal thresholding is given by (Duchi et al., 2008):
τ(α) =",4.2. Parseval Training,[0],[0]
"( ∑ j≤k(α) αj)− 1 k(α)
",4.2. Parseval Training,[0],[0]
"Consequently, the complexity of the projection is O(K log(K))",4.2. Parseval Training,[0],[0]
since it is only dominated by the sorting of the coefficients and is typically cheap because aggregation nodes will only have few children in practice (e.g. 2).,4.2. Parseval Training,[0],[0]
"If the number of children is large, there exist efficient linear time algorithms for finding the optimal thresholding τ(α) (Michelot, 1986; Pardalos & Kovoor, 1990; Condat, 2016).",4.2. Parseval Training,[0],[0]
"In this work, we use the method detailed above (Duchi et al., 2008) to perform the projection of the coefficient α after every gradient update step.",4.2. Parseval Training,[0],[0]
"We evaluate the effectiveness of Parseval networks on well-established image classification benchmark datasets namely MNIST, CIFAR-10, CIFAR-100 (Krizhevsky, 2009) and Street View House Numbers (SVHN) (Netzer et al.).",5. Experimental evaluation,[0],[0]
We train both fully connected networks and wide residual networks.,5. Experimental evaluation,[0],[0]
"The details of the datasets, the models, and the training routines are summarized below.",5. Experimental evaluation,[0],[0]
CIFAR.,5.1. Datasets,[0],[0]
Each of the CIFAR datasets is composed of 60K natural scene color images of size 32 × 32 split between 50K training images and 10K test images.,5.1. Datasets,[0],[0]
CIFAR-10 and CIFAR-100 have respectively 10 and 100 classes.,5.1. Datasets,[0],[0]
"For these two datasets, we adopt the following standard preprocessing and data augmentation scheme (Lin et al., 2013;",5.1. Datasets,[0],[0]
"He et al., 2016; Huang et al., 2016a; Zagoruyko & Komodakis, 2016): Each training image is first zero-padded with 4 pixels on each side.",5.1. Datasets,[0],[0]
The resulting image is randomly cropped to produce a new 32 × 32 image which is subsequently horizontally flipped with probability 0.5.,5.1. Datasets,[0],[0]
We also normalize every image with the mean and standard deviation of its channels.,5.1. Datasets,[0],[0]
"Following the same practice as (Huang et al., 2016a), we initially use 5K images from the training as a validation set.",5.1. Datasets,[0],[0]
"Next, we train de novo the best model on the full set of 50K images and report the results on the test set.",5.1. Datasets,[0],[0]
SVHN The Street View House Number dataset is a set of 32× 32 color digit images officially split into 73257 training images and 26032 test images.,5.1. Datasets,[0],[0]
"Following common practice (Zagoruyko & Komodakis, 2016; He et al., 2016; Huang et al., 2016a;b), we randomly sample 10000 images from the available extra set of about 600K images as a validation set and combine the rest of the pictures with the official training set.",5.1. Datasets,[0],[0]
We divide the pixel values by 255 as a preprocessing step and report the test set performance of the best performing model on the validation set.,5.1. Datasets,[0],[0]
ConvNet Models.,5.2. Models and Implementation details,[0],[0]
"For the CIFAR and SVHN datasets, we trained wide residual networks (Zagoruyko & Komodakis, 2016) as they perform on par with standard resnets (He et al., 2016) while being faster to train thanks to a reduced depth.",5.2. Models and Implementation details,[0],[0]
We used wide resnets of depth 28 and width 10 for both CIFAR-10 and CIFAR-100.,5.2. Models and Implementation details,[0],[0]
For SVHN we used wide resnet of depth 16 and width 4.,5.2. Models and Implementation details,[0],[0]
"For each architecture, we compare Parseval networks with the vanilla model trained with standard regularization both in the adversarial and the non-adversarial training settings.
",5.2. Models and Implementation details,[0],[0]
ConvNet Training.,5.2. Models and Implementation details,[0],[0]
We train the networks with stochastic gradient descent using a momentum of 0.9.,5.2. Models and Implementation details,[0],[0]
"On CIFAR datasets, the initial learning rate is set to 0.1 and scaled by a factor of 0.2 after epochs 60, 120 and 160, for a total number of 200 epochs.",5.2. Models and Implementation details,[0],[0]
We used mini-batches of size 128.,5.2. Models and Implementation details,[0],[0]
"For SVHN, we trained the models with mini-batches of size 128 for 160 epochs starting with a learning rate of 0.01 and decreasing it by a factor of 0.1 at epochs 80 and 120.",5.2. Models and Implementation details,[0],[0]
"For all the vanilla models, we applied by default weight decay regularization (with parameter λ = 0.0005) together with batch normalization and dropout since this combination resulted in better accuracy and increased robustness in preliminary experiments.",5.2. Models and Implementation details,[0],[0]
"The dropout rate use
is 0.3 for CIFAR and 0.4 for SVHN.",5.2. Models and Implementation details,[0],[0]
"For Parseval regularized models, we choose the value of the retraction parameter to be β = 0.0003 for CIFAR datasets and β = 0.0001 for SVHN based on the performance on the validation set.",5.2. Models and Implementation details,[0],[0]
"In all cases, We also adversarially trained each of the models on CIFAR-10 and CIFAR-100 following the guidelines in (Goodfellow et al., 2015; Shaham et al., 2015; Kurakin et al., 2016).",5.2. Models and Implementation details,[0],[0]
"In particular, we replace 50% of the examples of every minibatch by their adversarially perturbed version generated using the one-step method to avoid label leaking (Kurakin et al., 2016).",5.2. Models and Implementation details,[0],[0]
"For each mini-batch, the magnitude of the adversarial perturbation is obtained by sampling from a truncated Gaussian centered at 0 with standard deviation 2.
Fully Connected Model.",5.2. Models and Implementation details,[0],[0]
We also train feedforward networks composed of 4 fully connected hidden layers of size 2048 and a classification layer.,5.2. Models and Implementation details,[0],[0]
The input to these networks are images unrolled into a C × 1024 dimensional vector where C is the number of channels.,5.2. Models and Implementation details,[0],[0]
We used these models on MNIST and CIFAR-10 mainly to demonstrate that the proposed approach is also useful on non-convolutional networks.,5.2. Models and Implementation details,[0],[0]
We compare a Parseval networks to vanilla models with and without weight decay regularization.,5.2. Models and Implementation details,[0],[0]
"For adversarially trained models, we follow the guidelines previously described for the convolutional networks.
",5.2. Models and Implementation details,[0],[0]
Fully Connected Training.,5.2. Models and Implementation details,[0],[0]
We train the models with SGD and divide the learning rate by two every 10 epochs.,5.2. Models and Implementation details,[0],[0]
We use mini-batches of size 100 and train the model for 50 epochs.,5.2. Models and Implementation details,[0],[0]
We chose the hyperparameters on the validation set and retrain the model on the union of the training and validation sets.,5.2. Models and Implementation details,[0],[0]
"The hyperparameters are β, the size of the row subset S, the learning rate and its decrease rate.",5.2. Models and Implementation details,[0],[0]
Using a subset S of 30% of all the rows of each of weight matrix for the retraction step worked well in practice.,5.2. Models and Implementation details,[0],[0]
We first validate that Parseval training (Algorithm 1) indeed yields (near)-orthonormal weight matrices.,5.3.1. (QUASI)-ORTHOGONALITY.,[0],[0]
"To do so, we analyze the spectrum of the weight matrices of the different models by plotting the histograms of their singular values, and compare these histograms for Parseval networks to networks trained using standard SGD with and without weight decay (SGD-wd and SGD).
",5.3.1. (QUASI)-ORTHOGONALITY.,[0],[0]
The histograms representing the distribution of singular values at layers 1 and 4 for the fully connected network (using S = 30%) trained on the dataset CIFAR-10 are shown in Fig. 2 (the figures for convolutional networks are similar).,5.3.1. (QUASI)-ORTHOGONALITY.,[0],[0]
The singular values obtained with our method are tightly concentrated around 1.,5.3.1. (QUASI)-ORTHOGONALITY.,[0],[0]
"This experiment confirms that the weight matrices produced by the proposed opti-
mization procedure are (almost) orthonormal.",5.3.1. (QUASI)-ORTHOGONALITY.,[0],[0]
"The distribution of the singular values of the weight matrices obtained with SGD has a lot more variance, with nearly as many small values as large ones.",5.3.1. (QUASI)-ORTHOGONALITY.,[0],[0]
"Adding weight decay to standard SGD leads to a sparse spectrum for the weight matrices, especially in the higher layers of the network suggesting a low-rank structure.",5.3.1. (QUASI)-ORTHOGONALITY.,[0],[0]
"This observation has motivated recent work on compressing deep neural networks (Denton et al., 2014).",5.3.1. (QUASI)-ORTHOGONALITY.,[0],[0]
"We evaluate the robustness of the models to adversarial noise by generating adversarial examples from the test set, for various magnitudes of the noise vector.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"Following common practice (Kurakin et al., 2016), we use the fast gradient sign method to generate the adversarial examples (using ‖.‖∞, see Section 3.1).",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"Since these adversarial examples transfer from one network to the other, the fast gradient sign method allows to benchmark the network for reasonable settings where the opponent does not know the network.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
We report the accuracy of each model as a function of the magnitude of the noise.,5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"To make the results easier to interpret, we compute the corresponding Signal to Noise Ratio (SNR).",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"For an input x and perturbation δx, the SNR is defined as SNR(x, δx) = 20 log10 ‖x‖2 ‖δx‖2 .",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"We show some adversarial examples in Fig. 1.
Fully Connected Nets.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
Figure 3 depicts a comparison of Parseval and vanilla networks with and without adversarial training at various noise levels.,5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"On both MNIST and CIFAR-10, Parseval networks consistently outperforms weight decay regularization.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"In addition, it is as robust as
adversarial training (SGD-wd-da) on CIFAR-10.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"Combining Parseval Networks and adversarial training results in the most robust method on MNIST.
ResNets.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"Table 1 summarizes the results of our experiments with wide residual Parseval and vanilla networks on CIFAR-10, CIFAR-100 and SVHN.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"In the table, we denote Parseval(OC) the Parseval network with orthogonality constraint and without using a convex combination in aggregation layers.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
Parseval indicates the configuration where both of the orthogonality and convexity constraints are used.,5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
We first observe that Parseval networks outperform vanilla ones on all datasets on the clean examples and match the state of the art performances on CIFAR-10 (96.28%) and SVHN (98.44%).,5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"On CIFAR-100, when we use Parseval wide Resnet of depth 40 instead of 28, we achieve an accuracy of 81.76%.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"In comparison, the best performance achieved by a vanilla wide resnet (Zagoruyko & Komodakis, 2016) and a pre-activation resnet (He et al., 2016) are respectively 81.12% and 77.29%.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"Therefore, our proposal is a useful regularizer for legitimate examples.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"Also note that in most cases, Parseval networks combining both the orthogonality constraint and the convexity constraint is superior to use the orthogonality constraint solely.
",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0.951059864817606],"['PERSONA-CHAT is a resource that is particularly strong at providing training data for the beginning of conversations, when the two speakers do not know each other, focusing on asking and answering questions, in contrast to other resources.']"
The results presented in the table validate our most important claim: Parseval networks significantly improve the robustness of vanilla models to adversarial examples.,5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"When no adversarial training is used, the gap in accuracy be-
tween the two methods is significant (particularly in the high noise scenario).",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"For an SNR value of 40, the best Parseval network achieves 55.41% accuracy while the best vanilla model is at 44.62%.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"When the models are adversarially trained, Parseval networks remain superior to vanilla models in most cases.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"Interestingly, adversarial training only slightly improves the robustness of Parseval networks in low noise setting (e.g. SNR values of 45-50) and sometimes even deteriorates it (e.g. on CIFAR-10).",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"In contrast, combining adversarial training and Parseval networks is an effective approach in the high noise setting.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"This result suggests that thanks to the particular form of regularizer (controlling the Lipschitz constant of the network), Parseval networks achieves robustness to adversarial examples located in the immediate vicinity of each data point.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"Therefore, adversarial training only helps for adversarial examples found further away from the legitimate patterns.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"This observation holds consistently across all our datasets.
",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"Better use of capacity Given the distribution of singular values observed in Figure 2, we want to analyze the intrinsic dimensionality of the representation learned by the different networks at every layer.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"To that end, we use the local covariance dimension (Dasgupta & Freund, 2008) which can be measured from the covariance matrix of the data.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"For each layer k of the fully connected network, we compute the activation’s empirical covariance matrix 1 n",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"∑n i=1 φk(x)φk(x)
",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
>,5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
and obtain its sorted eigenvalues σ1 ≥ · · · ≥ σd.,5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"For each method and each layer, we select the smallest integer p such that ∑p i=1",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
σi ≥ 0.99 ∑d i=1,5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
σi.,5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
This gives us the number of dimensions that we need to explain 99% of the covariance.,5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"We can also compute the same quantity for the examples of each class, by only considering in the empirical estimation of the covariance of the examples xi such that yi = c. Table 2 report these numbers for all examples and the per-class average on CIFAR-10.
Table 2 shows that the local covariance dimension of all the data is consistently higher for Parseval networks than all the other approaches at any layer of the network.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"SGDwd-da contracts all the data in very low dimensional spaces at the upper levels of the network by using only 0.4% of the total dimension (layer 3 and 4) while Parseval networks use about 81% and 56% at of the whole dimension respectively
in the same layers.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"This is intriguing given that SGD-wd-da also increases the robustness of the network, apparently not in the same way as Parseval networks.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"For the average local covariance dimension of the classes, SGD-wd-da contracts each class into the same dimensionality as it contracts all the data at the upper layers of the network.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"For Parseval, the data of each class is contracted in about 30% and 19% of the overall dimension.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"These results suggest that Parseval contracts the data of each class in a lower dimensional manifold (compared to the intrinsic dimensionality of the whole data) hence making classification easier.
faster convergence Parseval networks converge significantly faster than vanilla networks trained with batch normalization and dropout as depicted by figure 4.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"Thanks to the orthogonalization step following each gradient update, the weight matrices are well conditioned at each step during the optimization.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
We hypothesize this is the main explanation of this phenomenon.,5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"For convolutional networks (resnets), the faster convergence is not obtained at the expense of larger wall-time since the cost of the projection step is negligible compared to the total cost of the forward pass on modern GPU architecture thanks to the small size of the filters.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"We introduced Parseval networks, a new approach for learning neural networks that are intrinsically robust to adversarial noise.",6. Conclusion,[0],[0]
We proposed an algorithm that allows us to optimize the model efficiently.,6. Conclusion,[0],[0]
Empirical results on three classification datasets with fully connected and wide residual networks illustrate the performance of our approach.,6. Conclusion,[0],[0]
"As a byproduct of the regularization we propose, the model trains faster and makes a better use of its capacity.",6. Conclusion,[0],[0]
Further investigation of this phenomenon is left to future work.,6. Conclusion,[0],[0]
"The authors would like to thank M.A. Ranzato, Y. Tian, A. Bordes and F. Perronnin for their valuable feedback on this work.",Acknowledgements,[0],[0]
"We introduce Parseval networks, a form of deep neural networks in which the Lipschitz constant of linear, convolutional and aggregation layers is constrained to be smaller than 1.",abstractText,[0],[0]
Parseval networks are empirically and theoretically motivated by an analysis of the robustness of the predictions made by deep neural networks when their input is subject to an adversarial perturbation.,abstractText,[0],[0]
"The most important feature of Parseval networks is to maintain weight matrices of linear and convolutional layers to be (approximately) Parseval tight frames, which are extensions of orthogonal matrices to non-square matrices.",abstractText,[0],[0]
We describe how these constraints can be maintained efficiently during SGD.,abstractText,[0],[0]
"We show that Parseval networks match the state-of-the-art in terms of accuracy on CIFAR-10/100 and Street View House Numbers (SVHN), while being more robust than their vanilla counterpart against adversarial examples.",abstractText,[0],[0]
"Incidentally, Parseval networks also tend to train faster and make a better usage of the full capacity of the networks.",abstractText,[0],[0]
Parseval Networks: Improving Robustness to Adversarial Examples,title,[0],[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2331–2336, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics
We recast syntactic parsing as a language modeling problem and use recent advances in neural network language modeling to achieve a new state of the art for constituency Penn Treebank parsing — 93.8 F1 on section 23, using 2-21 as training, 24 as development, plus tri-training. When trees are converted to Stanford dependencies, UAS and LAS are 95.9% and 94.1%.",text,[0],[0]
"Recent work on deep learning syntactic parsing models has achieved notably good results, e.g., Dyer et al. (2016) with 92.4 F1 on Penn Treebank constituency parsing and Vinyals et al. (2015) with 92.8 F1.",1 Introduction,[0],[0]
"In this paper we borrow from the approaches of both of these works and present a neural-net parse reranker that achieves very good results, 93.8 F1, with a comparatively simple architecture.
",1 Introduction,[0.9524632683322195],"['Finally, we introduce a generative model that encodes each of the profile entries as individual memory representations in a memory network.']"
In the remainder of this section we outline the major difference between this and previous work — viewing parsing as a language modeling problem.,1 Introduction,[0],[0]
Section 2 looks more closely at three of the most relevant previous papers.,1 Introduction,[0],[0]
"We then describe our exact model (Section 3), followed by the experimental setup and results (Sections 4 and 5).",1 Introduction,[0],[0]
"Formally, a language model (LM) is a probability distribution over strings of a language:
P (x) = P (x1, · · · , xn)
=
n∏
t=1
P (xt|x1, · · · , xt−1), (1)
where x is a sentence and t indicates a word position.",1.1 Language Modeling,[0],[0]
"The efforts in language modeling go into computing P (xt|x1, · · · , xt−1), which as described next is useful for parsing as well.",1.1 Language Modeling,[0],[0]
"A generative parsing model parses a sentence (x) into its phrasal structure (y) according to
argmax y′∈Y(x)
P (x,y′),
where Y(x) lists all possible structures of x.",1.2 Parsing as Language Modeling,[0],[0]
"If we think of a tree (x,y) as a sequence (z) (Vinyals et
2331
al., 2015) as illustrated in Figure 1, we can define a probability distribution over (x,y) as follows:
P (x,y) = P (z) = P (z1, · · · , zm)
= m∏
t=1
P (zt|z1, · · · , zt−1), (2)
which is equivalent to Equation (1).",1.2 Parsing as Language Modeling,[0],[0]
"We have reduced parsing to language modeling and can use language modeling techniques of estimating P (zt|z1, · · · , zt−1) for parsing.",1.2 Parsing as Language Modeling,[0],[0]
We look here at three neural net (NN) models closest to our research along various dimensions.,2 Previous Work,[0],[0]
"The first (Zaremba et al., 2014) gives the basic language modeling architecture that we have adopted, while the other two (Vinyals et al., 2015; Dyer et al., 2016) are parsing models that have the current best results in NN parsing.",2 Previous Work,[0],[0]
"The LSTM-LM of Zaremba et al. (2014) turns (x1, · · · , xt−1) into ht, a hidden state of an LSTM (Hochreiter and Schmidhuber, 1997; Gers et al., 2003; Graves, 2013), and uses ht to guess xt:
P (xt|x1, · · · , xt−1) = P (xt|ht) = softmax(Wht)[xt],
where W is a parameter matrix and [i] indexes ith element of a vector.",2.1 LSTM-LM,[0],[0]
"The simplicity of the model makes it easily extendable and scalable, which has inspired a character-based LSTM-LM that works well for many languages (Kim et al., 2016) and an ensemble of large LSTM-LMs for English with astonishing perplexity of 23.7 (Jozefowicz et al., 2016).",2.1 LSTM-LM,[0],[0]
"In this paper, we build a parsing model based on the LSTM-LM of Zaremba et al. (2014).",2.1 LSTM-LM,[0],[0]
"Vinyals et al. (2015) observe that a phrasal structure (y) can be expressed as a sequence and build a machine translation parser (MTP), a sequence-tosequence model, which translates x into y using a
conditional probability:
P (y|x)",2.2 MTP,[0],[0]
"= P (y1, · · · , yl|x)
= l∏
t=1
P (yt|x, y1, · · · , yt−1),
where the conditioning event (x, y1, · · · , yt−1) is modeled by an LSTM encoder and an LSTM decoder.",2.2 MTP,[0],[0]
"The encoder maps x into he, a set of vectors that represents x, and the decoder obtains a summary vector (h′t) which is concatenation of the decoder’s hidden state (hdt ) and weighted sum of word representations ( ∑n i=1 αih e i ) with an alignment vector (α).",2.2 MTP,[0],[0]
Finally the decoder predicts yt given h′t.,2.2 MTP,[0],[0]
"Inspired by MTP, our model processes sequential trees.",2.2 MTP,[0],[0]
"Recurrent Neural Network Grammars (RNNG), a generative parsing model, defines a joint distribution over a tree in terms of actions the model takes to generate the tree (Dyer et al., 2016):
P (x,y) = P (a) =
m∏
t=1
P (at|a1, · · · , at−1), (3)
where a is a sequence of actions whose output precisely matches the sequence of symbols in z, which implies Equation (3) is the same as Equation (2).",2.3 RNNG,[0],[0]
"RNNG and our model differ in how they compute the conditioning event (z1, · · · , zt−1): RNNG combines hidden states of three LSTMs that keep track of actions the model has taken, an incomplete tree the model has generated and words the model has generated whereas our model uses one LSTM’s hidden state as shown in the next section.",2.3 RNNG,[0],[0]
"Our model, the model of Zaremba et al. (2014) applied to sequential trees and we call LSTM-LM from now on, is a joint distribution over trees:
P (x,y)",3 Model,[0],[0]
"= P (z) = m∏
t=1
P (zt|z1, · · · , zt−1)
=
m∏
t=1
P (zt|ht)
= m∏
t=1
softmax(Wht)[zt],
where ht is a hidden state of an LSTM.",3 Model,[0],[0]
"Due to lack of an algorithm that searches through an exponentially large phrase-structure space, we use an n-best parser to reduce Y(x) to Y ′(x), whose size is polynomial, and use LSTM-LM to find y that satisfies
argmax y′∈Y ′(x)
P (x,y′).",3 Model,[0],[0]
(4),3 Model,[0],[0]
"The model has three LSTM layers with 1,500 units and gets trained with truncated backpropagation through time with mini-batch size 20 and step size 50.",3.1 Hyper-parameters,[0],[0]
"We initialize starting states with previous minibatch’s last hidden states (Sutskever, 2013).",3.1 Hyper-parameters,[0],[0]
"The forget gate bias is initialized to be one (Jozefowicz et al., 2015) and the rest of model parameters are sampled from U(−0.05, 0.05).",3.1 Hyper-parameters,[0],[0]
"Dropout is applied to non-recurrent connections (Pham et al., 2014) and gradients are clipped when their norm is bigger than 20 (Pascanu et al., 2013).",3.1 Hyper-parameters,[0],[0]
"The learning rate is 0.25 · 0.85max( −15, 0) where is an epoch number.",3.1 Hyper-parameters,[0],[0]
"For simplicity, we use vanilla softmax over an entire vocabulary as opposed to hierarchical softmax (Morin and Bengio, 2005) or noise contrastive estimation (Gutmann and Hyvärinen, 2012).",3.1 Hyper-parameters,[0],[0]
"We describe datasets we use for evaluation, detail training and development processes.1",4 Experiments,[0],[0]
"We use the Wall Street Journal (WSJ) of the Penn Treebank (Marcus et al., 1993) for training (2-21), development (24) and testing (23) and millions of auto-parsed “silver” trees (McClosky et al., 2006; Huang et al., 2010; Vinyals et al., 2015) for tritraining.",4.1 Data,[0],[0]
"To obtain silver trees, we parse the entire section of the New York Times (NYT) of the fifth Gigaword (Parker et al., 2011) with a product of eight Berkeley parsers (Petrov, 2010)2 and ZPar (Zhu et al., 2013) and select 24 million trees on which both parsers agree (Li et al., 2014).",4.1 Data,[0],[0]
"We do not resample trees to match the sentence length distribution of the NYT to that of the WSJ (Vinyals et
1The code and trained models used for experiments are available at github.com/cdg720/emnlp2016.
",4.1 Data,[0],[0]
2We use the reimplementation by Huang et al. (2010).,4.1 Data,[0],[0]
"al., 2015) because in preliminary experiments Charniak parser (Charniak, 2000) performed better when trained on all of 24 million trees than when trained on resampled two million trees.
",500 97.0 91.8 40.0,[0],[0]
"Given x, we produce Y ′(x), 50-best trees, with Charniak parser and find y with LSTM-LM as Dyer et al. (2016) do with their discriminative and generative models.3",500 97.0 91.8 40.0,[0],[0]
"We unk words that appear fewer than 10 times in the WSJ training (6,922 types) and drop activations with probability 0.7.",4.2.1 Supervision,[0],[0]
"At the beginning of each epoch, we shuffle the order of trees in the training data.",4.2.1 Supervision,[0],[0]
Both perplexity and F1 of LSTM-LM (G) improve and then plateau (Figure 2).,4.2.1 Supervision,[0],[0]
"Perplexity, the
3Dyer et al. (2016)’s discriminative model performs comparably to Charniak (89.8 vs. 89.7).
model’s training objective, nicely correlates with F1, what we care about.",4.2.1 Supervision,[0],[0]
Training takes 12 hours (37 epochs) on a Titan X. We also evaluate our model with varying n-best trees including optimal 51-best trees that contain gold trees (51o).,4.2.1 Supervision,[0],[0]
"As shown in Table 1, the LSTM-LM (G) is robust given sufficiently large n, i.e. 50, but does not exhibit its full capacity because of search errors in Charniak parser.",4.2.1 Supervision,[0],[0]
We address this problem in Section 5.3.,4.2.1 Supervision,[0],[0]
"We unk words that appear at most once in the training (21,755 types).",4.2.2 Semi-supervision,[0],[0]
"We drop activations with probability 0.45, smaller than 0.7, thanks to many silver trees, which help regularization.",4.2.2 Semi-supervision,[0],[0]
"We train LSTM-LM (GS) on the WSJ and a different set of 400,000 NYT trees for each epoch except for the last one during which we use the WSJ only.",4.2.2 Semi-supervision,[0],[0]
Training takes 26 epochs and 68 hours on a Titan X. LSTMLM (GS) achieves 92.5 F1 on the development.,4.2.2 Semi-supervision,[0],[0]
"As shown in Table 2, with 92.6 F1 LSTM-LM (G) outperforms an ensemble of five MTPs (Vinyals et al., 2015) and RNNG (Dyer et al., 2016), both of which are trained on the WSJ only.",5.1 Supervision,[0],[0]
"We compare LSTM-LM (GS) to two very strong semi-supervised NN parsers: an ensemble of five MTPs trained on 11 million trees of the highconfidence corpus4 (HC) (Vinyals et al., 2015); and an ensemble of six one-to-many sequence models
4The HC consists of 90,000 gold trees, from the WSJ, English Web Treebank and Question Treebank, and 11 million silver trees, whose sentence length distribution matches that of the WSJ, parsed and agreed on by Berkeley parser and ZPar.
trained on the HC and 4.5 millions of EnglishGerman translation sentence pairs (Luong et al., 2016).",5.2 Semi-supervision,[0],[0]
We also compare LSTM-LM (GS) to best performing non-NN parsers in the literature.,5.2 Semi-supervision,[0],[0]
Parsers’ parsing performance along with their training data is reported in Table 3.,5.2 Semi-supervision,[0],[0]
LSTM-LM (GS) outperforms all the other parsers with 93.1 F1.,5.2 Semi-supervision,[0],[0]
"Due to search errors – good trees are missing in 50-best trees – in Charniak (G), our supervised and semi-supervised models do not exhibit their full potentials when Charniak (G) provides Y ′(x).",5.3 Improved Semi-supervision,[0],[0]
"To mitigate the search problem, we tri-train Charniak (GS) on all of 24 million NYT trees in addition to the WSJ, to yield Y ′(x).",5.3 Improved Semi-supervision,[0],[0]
"As shown in Table 3, both LSTM-LM (G) and LSTM-LM (GS) are affected by the quality of Y ′(x).",5.3 Improved Semi-supervision,[0],[0]
"A single LSTM-LM (GS) together with Charniak (GS) reaches 93.6 and an ensemble of eight LSTM-LMs (GS) with Charniak (GS) achieves a new state of the art, 93.8 F1.",5.3 Improved Semi-supervision,[0],[0]
"When trees are converted to Stanford dependencies,5 UAS and LAS are 95.9% and 94.1%,6 more than 1% higher than those of the state of the art dependency parser (Andor et al., 2016).",5.3 Improved Semi-supervision,[0],[0]
"Why an indirect method (converting trees to dependencies) is more accurate than a direct one (dependency parsing) remains unanswered (Kong and Smith, 2014).",5.3 Improved Semi-supervision,[0],[0]
The generative parsing model we presented in this paper is very powerful.,6 Conclusion,[0],[0]
"In fact, we see that a generative parsing model, LSTM-LM, is more effective than discriminative parsing models (Dyer et al., 2016).",6 Conclusion,[0],[0]
"We suspect building large models with character embeddings would lead to further improvement as in language modeling (Kim et al., 2016; Jozefowicz et al., 2016).",6 Conclusion,[0],[0]
We also wish to develop a complete parsing model using the LSTMLM framework.,6 Conclusion,[0],[0]
"We thank the NVIDIA corporation for its donation of a Titan X GPU, Tstaff of Computer Science
5Version 3.3.0.",Acknowledgments,[0],[0]
6We use the CoNLL evaluator available through the CoNLL website: ilk.uvt.nl/conll/software/eval.pl.,Acknowledgments,[0],[0]
"Following the convention, we ignore punctuation.
at Brown University for setting up GPU machines and David McClosky for helping us train Charniak parser on millions trees.",Acknowledgments,[0],[0]
"We recast syntactic parsing as a language modeling problem and use recent advances in neural network language modeling to achieve a new state of the art for constituency Penn Treebank parsing — 93.8 F1 on section 23, using 2-21 as training, 24 as development, plus tri-training.",abstractText,[0],[0]
"When trees are converted to Stanford dependencies, UAS and LAS are 95.9% and 94.1%.",abstractText,[0],[0]
Parsing as Language Modeling,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 69–81 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,[0],[0]
"While parsing has become a relatively mature technology for written text, parser performance on conversational speech lags behind.",1 Introduction,[0],[0]
"Speech poses challenges for parsing: transcripts may contain errors and lack punctuation; even perfect transcripts can be difficult to handle because of disfluencies (restarts, repetitions, and self-corrections), filled pauses (“um”, “uh”), interjections (“like”), parentheticals (“you know”, “I mean”), and sentence fragments.",1 Introduction,[0],[0]
"Some of these phenomena can be handled in standard grammars, but disfluencies typically require extensions of the model.",1 Introduction,[0],[0]
"Different approaches have been explored in both constituency parsing (Charniak and Johnson, 2001; Johnson and Charniak, 2004) and dependency parsing (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014).
∗Equal Contribution.
",1 Introduction,[0],[0]
"Despite these challenges, speech carries helpful extra information – beyond the words – associated with the prosodic structure of an utterance and encoded via variation in timing and intonation.",1 Introduction,[0],[0]
"Speakers pause in locations that are correlated with syntactic structure (Grosjean et al., 1979), and listeners use prosodic structure in resolving syntactic ambiguities (Price et al., 1991).",1 Introduction,[0],[0]
"Prosodic cues also signal disfluencies by marking the interruption point (Shriberg, 1994).",1 Introduction,[0],[0]
"However, most speech parsing systems in practice take little advantage of these cues.",1 Introduction,[0],[0]
"Our study focuses on this last challenge, aiming to incorporate prosodic cues in a neural parser, handling disfluencies as constituents via a neural attention mechanism.
",1 Introduction,[0],[0]
"A challenge of incorporating prosody in parsing is that multiple acoustic cues interact to signal prosodic structure, including pauses, lengthening, fundamental frequency modulation, and spectral shape.",1 Introduction,[0],[0]
"These cues also vary with the phonetic segment, emphasis, emotion and speaker, so feature extraction typically involves multiple time windows and normalization techniques.",1 Introduction,[0],[0]
"The most successful constituent parsers have mapped these features to prosodic boundary posteriors by using labeled training data (Kahn et al., 2005; Hale et al., 2006; Dreyer and Shafran, 2007).",1 Introduction,[0],[0]
The approach proposed here takes advantage of advances in neural networks to automatically learn a good feature representation without the need to explicitly represent prosodic constituents.,1 Introduction,[0],[0]
"To narrow the scope of this work and facilitate error analysis, our experiments use known transcripts and sentence segmentation.
",1 Introduction,[0],[0]
Our work offers the following contributions.,1 Introduction,[0],[0]
We introduce a framework for directly integrating acoustic-prosodic features with text in a neural encoder-decoder parser that does not require handannotated prosodic structure.,1 Introduction,[0],[0]
"We demonstrate improvements in constituent parsing of conversational
69
speech over a high-quality text-only parser and provide analyses showing where prosodic features help and that assessment of their utility is affected by human transcription errors.",1 Introduction,[0],[0]
Our model maps a sequence of word-level input features to a linearized parse output sequence.,2 Task and Model Description,[0],[0]
"The word-level input feature vector consists of the concatenation of (learnable) word embeddings ei and several types of acoustic-prosodic features, described in Section 2.3.",2 Task and Model Description,[0],[0]
"We assume the availability of a training treebank of conversational speech (in our case, SwitchboardNXT (Calhoun et al., 2010)) and corresponding constituent parses.",2.1 Task Setup,[0],[0]
The transcriptions are preprocessed by removing punctuation and lower-casing all text to better mimic the speech recognition setting.,2.1 Task Setup,[0],[0]
"Following Vinyals et al. (2015), the parse trees are linearized, and pre-terminals are normalized as “XX” (see Appendix A.1).",2.1 Task Setup,[0],[0]
Our attention-based encoder-decoder model is similar to the one used by Vinyals et al. (2015).,2.2 Encoder-Decoder Parser,[0],[0]
"The encoder is a deep long short-term memory recurrent neural network (LSTM-RNN) (Hochreiter and Schmidhuber, 1997) that reads in a word-level inputs,1 represented as a sequence of vectors x = (x1, · · · ,xTs), and outputs high-level features h = (h1, · · · ,hTs) where hi = LSTM(xi,hi−1).2
",2.2 Encoder-Decoder Parser,[0],[0]
"The parse decoder is also a deep LSTM-RNN that predicts the linearized parse sequence y = (y1, · · · , yTo) as follows:
P (y|x) = To∏
t=1
P (yt|h,y<t)
",2.2 Encoder-Decoder Parser,[0],[0]
"In attention-based models, the posterior distribution of the output yt at time step t is given by:
P (yt|h,y<t) =",2.2 Encoder-Decoder Parser,[0],[0]
softmax(W s[ct;dt],2.2 Encoder-Decoder Parser,[0],[0]
"+ bs),
where vector bs and matrix W s are learnable parameters; ct is referred to as a context vector that summarizes the encoder’s output h; and dt is the
1As in Vinyals et al. (2015)",2.2 Encoder-Decoder Parser,[0],[0]
"the input sequence is processed in reverse order, as shown in Figure 1.
",2.2 Encoder-Decoder Parser,[0],[0]
2For brevity we omit the LSTM equations.,2.2 Encoder-Decoder Parser,[0],[0]
"The details can be found, e.g., in Zaremba et al. (2014).
",2.2 Encoder-Decoder Parser,[0],[0]
"decoder hidden state at time step t, which captures the previous output sequence context y<t.
uit = v >",2.2 Encoder-Decoder Parser,[0],[0]
tanh(W 1hi,2.2 Encoder-Decoder Parser,[0],[0]
"+W 2dt + ba)
",2.2 Encoder-Decoder Parser,[0],[0]
"αt = softmax(ut) ct = Ts∑
i=1
αtihi
where vectors v, ba and matrices W 1, W 2 are learnable parameters; ut and αt are the attention score and attention weight vector, respectively, for decoder time step t.
The above attention mechanism is only contentbased, i.e., it is only dependent on hi, dt.",2.2 Encoder-Decoder Parser,[0],[0]
"It is not location-aware, i.e., it does not consider the “location” of the previous attention vector.",2.2 Encoder-Decoder Parser,[0],[0]
"For parsing conversational text, location awareness is beneficial since disfluent structures can have duplicate words/phrases that may “confuse” the attention mechanism.
",2.2 Encoder-Decoder Parser,[0],[0]
"In order to make the model location-aware, the attention mechanism takes into account the previous attention weight vector αt−1.",2.2 Encoder-Decoder Parser,[0],[0]
"In particular, we use the attention mechanism proposed by Chorowski et al. (2015), in which αt−1 is represented via a feature vector f t = F ∗αt−1, where F ∈ Rk×r represents k learnable convolution filters of width r. The filters are used for performing 1-D convolution over αt−1 to extract k features f ti for each time step i of the input sequence.",2.2 Encoder-Decoder Parser,[0],[0]
"The extracted features are then incorporated in the alignment score calculation as:
uit = v >",2.2 Encoder-Decoder Parser,[0],[0]
tanh(W 1hi,2.2 Encoder-Decoder Parser,[0],[0]
+W 2dt,2.2 Encoder-Decoder Parser,[0],[0]
"+W ff ti + ba)
where W f is another learnable parameter matrix.",2.2 Encoder-Decoder Parser,[0],[0]
"Finally, the decoder state dt is computed as dt = LSTM([ỹt−1; ct−1],dt−1), where ỹt−1 is the embedding vector corresponding to the previous output symbol yt−1.",2.2 Encoder-Decoder Parser,[0],[0]
"As we will see in Sec. 4.1, the location-aware attention mechanism is especially useful for handling disfluencies.",2.2 Encoder-Decoder Parser,[0],[0]
"In previous work using encoder-decoder models for parsing (Vinyals et al., 2015; Luong et al., 2016), vector xi is simply the word embedding ei of the word at position i of the input sentence.",2.3 Acoustic-Prosodic Features,[0],[0]
"For parsing conversational speech, we can incorporate acousticprosodic features.",2.3 Acoustic-Prosodic Features,[0],[0]
"Here we explore four types of features widely used in computational models of prosody: pauses, duration lengthening, fundamental frequency, and energy.",2.3 Acoustic-Prosodic Features,[0],[0]
"Since prosodic cues are
at sub- and multi-word time scales, they are integrated with the encoder-decoder using different mechanisms.
",2.3 Acoustic-Prosodic Features,[0],[0]
All features are extracted from transcriptions that are time-aligned at the word level.3,2.3 Acoustic-Prosodic Features,[0],[0]
We use time alignments associated with the corpus to be consistent with other studies.,2.3 Acoustic-Prosodic Features,[0],[0]
"In a small number of cases, the time alignment for a particular word boundary is missing.",2.3 Acoustic-Prosodic Features,[0],[0]
Some cases are due to tokenization.,2.3 Acoustic-Prosodic Features,[0],[0]
"For example, contractions, such as don’t in the original transcript, are treated as separated words for the parser (do and n’t), and the internal word boundary time is missing.",2.3 Acoustic-Prosodic Features,[0],[0]
"In such cases, these internal times are estimated.",2.3 Acoustic-Prosodic Features,[0],[0]
"In other cases, there are transcription mismatches that lead to missing time alignments, where we cannot estimate times.",2.3 Acoustic-Prosodic Features,[0],[0]
"For the roughly 1% of sentences where time alignments are missing, we simply back off to the text-based parser.
Pause.",2.3 Acoustic-Prosodic Features,[0],[0]
"The pause feature vector pi for word i is the concatenation of pre-word pause feature ppre,i and post-word pause feature ppost,i, where each subvector is a learned embedding for 6 pause categories: no pause, missing, 0 < p ≤ 0.05 s, 0.05 s < p ≤ 0.2 s, 0.2 < p ≤ 1 s, and p > 1 s (including turn boundaries).",2.3 Acoustic-Prosodic Features,[0],[0]
The bins are chosen based on the observed distribution (see Appendix A.1).,2.3 Acoustic-Prosodic Features,[0],[0]
"We did not use (real-valued) pause duration directly, for two main reasons: (1) to handle missing time alignments; and (2) duration of pause does
3The assumption of known word alignments is standard for prosodic feature extraction in many spoken language processing studies.",2.3 Acoustic-Prosodic Features,[0],[0]
"Time alignments can be obtained as a by-product of recognition or from forced alignment.
not matter beyond a threshold (e.g. p > 1 s).
",2.3 Acoustic-Prosodic Features,[0],[0]
Word duration.,2.3 Acoustic-Prosodic Features,[0],[0]
"Both word duration and wordfinal duration lengthening are strong cues to prosodic phrase boundaries (Wightman et al., 1992; Pate and Goldwater, 2013).",2.3 Acoustic-Prosodic Features,[0],[0]
"The word duration feature δi is computed as the actual word duration divided by the mean duration of the word, clipped to a maximum value of 5.",2.3 Acoustic-Prosodic Features,[0],[0]
The sample mean is used for frequent words (count ≥ 15).,2.3 Acoustic-Prosodic Features,[0],[0]
For infrequent words we estimate the mean as the sum over the sample means for the phonemes in the word’s dictionary pronunciation.,2.3 Acoustic-Prosodic Features,[0],[0]
"We refer to the manually defined prosodic feature pair of pi and δi as φi.
Fundament frequency (f0) and Energy (E) contours (f0/E).",2.3 Acoustic-Prosodic Features,[0],[0]
We use a CNN to automatically learn the mapping from the time series of f0/E features to a word-level vector.,2.3 Acoustic-Prosodic Features,[0],[0]
"The contour features are extracted from 25-ms frames with 10-ms hops using Kaldi (Povey et al., 2011).",2.3 Acoustic-Prosodic Features,[0],[0]
"Three f0 features are used: warped Normalized Cross Correlation Function (NCCF), log-pitch with Probability of Voicing (",2.3 Acoustic-Prosodic Features,[0],[0]
"POV)-weighted mean subtraction over a 1.5-second window, and the estimated derivative (delta) of the raw log pitch.",2.3 Acoustic-Prosodic Features,[0],[0]
"Three energy features are extracted from the Kaldi 40-mel-frequency filter bank features: Etotal, the log of total energy normalized by dividing by the speaker side’s max total energy; Elow, the log of total energy in the lower 20 mel-frequency bands, normalized by total energy, and Ehigh, the log of total energy in the higher 20 mel-frequency bands, normalized by total energy.",2.3 Acoustic-Prosodic Features,[0],[0]
"Multi-band energy features are used as a
simple mechanism to capture articulatory strengthening at prosodic constituent onsets (Fourgeron and Keating, 1997).
",2.3 Acoustic-Prosodic Features,[0],[0]
Figure 1 summarizes the feature learning approach.,2.3 Acoustic-Prosodic Features,[0],[0]
The f0 and E features are processed at the word level: each sequence of frames corresponding to a time-aligned word (and potentially its surrounding context) is convolved with N filters of m sizes (a total of mN filters).,2.3 Acoustic-Prosodic Features,[0],[0]
The motivation for the multiple filter sizes is to enable the computation of features that capture information on different time scales.,2.3 Acoustic-Prosodic Features,[0],[0]
"For each filter, we perform a 1-D convolution over the 6-dimensional f0/E features with a stride of 1.",2.3 Acoustic-Prosodic Features,[0],[0]
"Each filter output is max-pooled, resulting in mN -dimensional speech features si.",2.3 Acoustic-Prosodic Features,[0],[0]
"Our overall acoustic-prosodic feature vector is the concatenation of pi, δi, and si in various combinations.",2.3 Acoustic-Prosodic Features,[0],[0]
"Our core corpus is Switchboard-NXT (Calhoun et al., 2010), a subset of the Switchboard corpus (Godfrey and Holliman, 1993): 2,400 telephone conversations between strangers; 642 of these were hand-annotated with syntactic parses and further augmented with richer layers of annotation facilitated by the NITE XML toolkit (Calhoun et al., 2010).",3.1 Dataset,[0],[0]
"Our sentence segmentations and syntactic trees are based on the annotations from the Treebank set, with a few manual corrections from the NXT release.",3.1 Dataset,[0],[0]
"This core dataset consists of 100K sentences, totaling 830K tokens forming a vocabulary of 13.5K words.",3.1 Dataset,[0],[0]
"We use the time alignments available from NXT, which is based on a corrected word transcript that occasionally differs from the Treebank, leading to some missing time alignments.",3.1 Dataset,[0],[0]
"We follow the sentence boundaries defined by the parsed data available,4 and the data split (90% train; 5% dev; 5% test) defined by related work done on Switchboard (Charniak and Johnson, 2001; Kahn et al., 2005; Honnibal and Johnson, 2014).",3.1 Dataset,[0],[0]
"The standard evaluation metric for constituent parsing is the parseval metric which uses bracketing precision, recall, and F1, as in the canonical implementation of EVALB.5",3.2 Evaluation Metrics and Baselines,[0],[0]
"For written text, punc-
4Note that these sentence units can be inconsistent with other layers of Switchboard annotations, such as slash units.
",3.2 Evaluation Metrics and Baselines,[0],[0]
"5http://nlp.cs.nyu.edu/evalb/
tuation is sometimes represented as part of the sequence and impacts the final score, but for speech the punctuation is not explicitly available so it does not contribute to the score.",3.2 Evaluation Metrics and Baselines,[0],[0]
Another challenge of transcribed speech is the presence of disfluencies.,3.2 Evaluation Metrics and Baselines,[0],[0]
"Speech repairs are indicated under “EDITED” nodes in Switchboard parse trees, which include structure under these nodes that is not of interest for simple text clean-up.",3.2 Evaluation Metrics and Baselines,[0],[0]
"Therefore, some studies report flattened-edit parseval F1 scores (“flatF1”), which is parseval computed on trees where the structure under edit nodes has been eliminated so that all leaves are immediate children.",3.2 Evaluation Metrics and Baselines,[0],[0]
"We report both scores for the baseline text-only model showing that the differences are small, then use the standard parseval F1 score for most results.6
Disfluencies are particularly problematic for statistical parsers, as explained by Charniak and Johnson (2001), and some systems incorporate a separate disfluency detection stage.",3.2 Evaluation Metrics and Baselines,[0.9569409527483621],"['For example, in both the generative and ranking model cases, models endowed with a persona can be detected by the human conversation partner, as evidenced by the persona detection accuracies, whilst maintaining fluency and consistency compared to their nonpersona driven counterparts.']"
"For this reason, and because it is useful for understanding system performance, most studies also report disfluency detection performance, which is measured in terms of the F1 score for detecting whether a word is in an edit region.",3.2 Evaluation Metrics and Baselines,[0],[0]
"Our approach does not involve a separate disfluency detection stage, but identifies disfluencies implicitly via the parse structure.",3.2 Evaluation Metrics and Baselines,[0],[0]
"Consequently, the disfluency detection results are not competitive with work that directly optimize for disfluency detection.",3.2 Evaluation Metrics and Baselines,[0],[0]
"We report disfluency detection scores primarily as a diagnostic.
",3.2 Evaluation Metrics and Baselines,[0],[0]
"Most previous work on integrating prosody and parsing has used the Switchboard corpus, but it is still difficult to compare results because of differences in constraints, objectives and the use of constituent vs. dependency structure, as discussed further in Section 6.",3.2 Evaluation Metrics and Baselines,[0],[0]
The most relevant prior studies (on constituent parsing) that we compare to are a bit old.,3.2 Evaluation Metrics and Baselines,[0],[0]
The text-only result from our neural parser represents a stronger baseline and is important for decoupling the impact of prosody vs. the parsing framework.,3.2 Evaluation Metrics and Baselines,[0],[0]
Both the encoder and decoder are 3-layer deep LSTM-RNNs with 256 hidden units in each layer.,3.3 Model Training and Inference,[0],[0]
"For the location-aware attention, the convolution operation uses 5 filters of width 40 each.",3.3 Model Training and Inference,[0],[0]
"We use 512-dimensional embedding vectors to repre-
6A variant of the “flat-F1” score is used in (Charniak and Johnson, 2001; Kahn et al., 2005), which uses a relaxed edited node precision and recall but also ignores filled pauses.
sent words and linearized parsing symbols, such as “(S”.7
A number of configurations are explored for the acoustic-prosodic features, tuning based on dev set parsing performance.",3.3 Model Training and Inference,[0],[0]
"Pause embeddings are tuned over {4, 16, 32} dimensions.",3.3 Model Training and Inference,[0],[0]
"For the CNN, we try different configurations of filter widths w ∈ {",3.3 Model Training and Inference,[0],[0]
"[10, 25, 50], [5, 10, 25, 50]} and number of filters N ∈ {16, 32, 64, 128} for each filter width.8 These filter size combinations are chosen to capture f0 and energy phenomena on various levels: w = 5, 10 for sub-word, w = 25 for word, and w = 50 for word and extended context.",3.3 Model Training and Inference,[0],[0]
Our best model uses 32-dimensional pause embeddings and N = 32 filters of widthsw =,3.3 Model Training and Inference,[0],[0]
"[5, 10, 25, 50], which corresponds to m = 4 and 128 filters.
",3.3 Model Training and Inference,[0],[0]
"For optimization we use Adam (Kingma and Ba, 2014) with a minibatch size of 64.",3.3 Model Training and Inference,[0],[0]
"The initial learning rate is 0.001 which is decayed by a factor of 0.9 whenever training loss, calculated after every 500 updates, degrades relative to the worst of its previous 3 values.",3.3 Model Training and Inference,[0],[0]
All models are trained for up to 50 epochs with early stopping.,3.3 Model Training and Inference,[0],[0]
"For regularization, dropout with 0.3 probability is applied on the output of all LSTM layers (Pham et al., 2014).
",3.3 Model Training and Inference,[0],[0]
"For inference, we use a greedy decoder to generate the linearized parse.",3.3 Model Training and Inference,[0],[0]
The output token with maximum posterior probability is chosen at every time step and fed as input in the next time step.,3.3 Model Training and Inference,[0],[0]
The decoder stops upon producing the end-of-sentence symbol.,3.3 Model Training and Inference,[0],[0]
"We use TensorFlow (Abadi et al., 2015) to implement all models.9",3.3 Model Training and Inference,[0],[0]
"7The number of layers, dimension of hidden units, dimension of embedding, and convolutional attention filter parameters of the text-only parser were explored in earlier experiments on the development set and then fixed as described.
8Note that a filter of width 10 has size 6 × 10, since the features are of dimension 6.
",4.1 Text-only Results,[0],[0]
"9Our code resources can be found in Appendix A.1.
",4.1 Text-only Results,[0],[0]
"We first show our results on the model using only text (i.e. xi = ei) to establish a strong baseline, on top of which we can add acousticprosodic features.",4.1 Text-only Results,[0],[0]
We experiment with the contentonly attention model used by Vinyals et al. (2015) and the content+location attention of Chorowski et al. (2015).,4.1 Text-only Results,[0],[0]
"For comparison with previous nonneural models, we use a high-quality latent-variable parser, the Berkeley parser (Petrov et al., 2006), retrained on our Switchboard data.",4.1 Text-only Results,[0],[0]
Table 1 compares the three text-only models.,4.1 Text-only Results,[0],[0]
"In terms of F1, the content+location attention beats the Berkeley parser by about 2.5% and content-only attention by about 4.5%.",4.1 Text-only Results,[0],[0]
"Flat-F1 scores for both encoder-decoder models is lower than their corresponding F1 scores, suggesting that the encoder-decoder models do well on predicting the internal structure of EDIT nodes while the reverse is true for the Berkeley parser.
",4.1 Text-only Results,[0],[0]
"To explain the gains of content+location attention over content-only attention, we compare their scores on fluent (without EDIT nodes) and disfluent sentences, shown in Table 1.",4.1 Text-only Results,[0],[0]
It is clear that most of the gains for content+location attention are from disfluent sentences.,4.1 Text-only Results,[0],[0]
"A possible explanation is the presence of duplicate words or phrases in disfluent sentences, which can be problematic for a contentonly attention model.",4.1 Text-only Results,[0],[0]
"Since our best model is the content+location attention model, we will henceforth refer to it as the “CL-attn” text-only model.",4.1 Text-only Results,[0],[0]
"All models using acoustic-prosodic features are extensions of this model, which provides a strong text-only baseline.",4.1 Text-only Results,[0],[0]
"We extend our CL-attn model with the three kinds of acoustic-prosodic features: pause (p), word duration (δ), and CNN mappings of fundamental frequency (f0) and energy (E) features (f0/E-CNN).
",4.2 Adding Acoustic-Prosodic Features,[0],[0]
The results of several model configurations on our dev set are presented in Table 2.,4.2 Adding Acoustic-Prosodic Features,[0],[0]
"First, we note that adding any combination of acoustic-prosodic features (individually or in sets) improves performance over the text-only baseline.",4.2 Adding Acoustic-Prosodic Features,[0],[0]
"However, certain combinations of acoustic-prosodic features are not always better than their subsets.",4.2 Adding Acoustic-Prosodic Features,[0],[0]
The text + p + δ + f0/E-CNN model that uses all three types of features has the best performance with a gain of 0.7% over the already-strong text-only baseline.,4.2 Adding Acoustic-Prosodic Features,[0],[0]
"We will henceforth refer to the text + p + δ + f0/E-CNN model as our “best model”.
",4.2 Adding Acoustic-Prosodic Features,[0],[0]
"As a robustness check, we report results of averaging 10 runs on the CL-attn text-only and the best model in Table 3.",4.2 Adding Acoustic-Prosodic Features,[0],[0]
"We performed a bootstrap test (Efron and Tibshirani, 1993) that simulates 105 random test draws on the models giving median performance on the dev set.",4.2 Adding Acoustic-Prosodic Features,[0],[0]
These median models gave a statistically significant difference between the text-only and best model (p-value < 0.02).,4.2 Adding Acoustic-Prosodic Features,[0],[0]
"Additionally, a simple t-test over the two sets of 10 results also shows statistical significance p-value < 0.03.
",4.2 Adding Acoustic-Prosodic Features,[0],[0]
Table 4 presents the results on the test set.,4.2 Adding Acoustic-Prosodic Features,[0],[0]
"Again, adding the acoustic-prosodic features improves over the text-only baseline.",4.2 Adding Acoustic-Prosodic Features,[0],[0]
"The gains are statistically significant for the best model with p-value < 0.02, again using a bootstrap test with simulated 105 random test draws on the two models.
",4.2 Adding Acoustic-Prosodic Features,[0],[0]
"Table 5 includes results from prior studies that compare systems using text alone with ones that incorporate prosody, given hand transcripts and sentence segmentation.",4.2 Adding Acoustic-Prosodic Features,[0],[0]
"It is difficult to compare systems directly, because of the many differences in the experimental set-up.",4.2 Adding Acoustic-Prosodic Features,[0],[0]
"For example, the original Charniak and Johnson (2001) result (reporting F=85.9 for parsing and F=78.2 for disfluencies) leverages punctuation in the text stream, which is not realistic for speech transcripts and not used in most other work.",4.2 Adding Acoustic-Prosodic Features,[0],[0]
"Our work benefits from more text training material than others, but others benefit from gold part-of-speech tags.",4.2 Adding Acoustic-Prosodic Features,[0],[0]
Kahn et al. (2005) use a modified sentence segmentation.,4.2 Adding Acoustic-Prosodic Features,[0],[0]
There are probably minor differences in handling of word fragments and scoring edit regions.,4.2 Adding Acoustic-Prosodic Features,[0],[0]
"Thus, this table primarily shows that our framework leads to more benefits from sentence-internal prosodic cues than others have obtained.",4.2 Adding Acoustic-Prosodic Features,[0],[0]
Effect of sentence length.,5 Analysis,[0],[0]
"Figure 2 shows performance differences between our best model and the text-only model for varying sentence lengths.
",5 Analysis,[0],[0]
"Both models do worse on longer sentences, as expected since the corresponding parse trees tend to be more complex.",5 Analysis,[0],[0]
The performance difference between our best model and the text-only model increases with sentence length.,5 Analysis,[0],[0]
"This is likely because longer sentences more often have multiple prosodic phrases and disfluencies.
",5 Analysis,[0],[0]
Effect of disfluencies.,5 Analysis,[0],[0]
"Table 6 presents parse scores on the subsets of fluent and disfluent sentences, showing that the performance gain is in the disfluent set (65% of the dev set sentences).",5 Analysis,[0],[0]
"Because sentence boundaries are given, and so many fluent sentences in spontaneous speech are short, there is less potential for benefit from prosody in the fluent set.
",5 Analysis,[0],[0]
Types of errors.,5 Analysis,[0],[0]
"We use the Berkeley Parser Analyzer (Kummerfeld et al., 2012) to compare the types of errors made by the different parsers.10 Table 7 presents the relative error reductions over the text-only baseline achieved by the text + p model and our best model for disfluent sentences.",5 Analysis,[0],[0]
The two models differ in the types of error reductions they provide.,5 Analysis,[0],[0]
"Including pause information gives largest improvements on PP attachment and Modifier at-
10This analysis omits the 1% of the sentences that did not have timing information.
tachment errors.",5 Analysis,[0],[0]
"Adding the remaining acousticprosodic features helps to correct more types of attachment errors, especially VP and NP attachment.",5 Analysis,[0],[0]
Figure 3 demonstrates one case where the pause feature helps in correcting a PP attachment error made by a text-only parser.,5 Analysis,[0],[0]
"Other interesting examples (see Appendix A.2) suggest that the learned f0/E features help reduce NP attachment errors where the audio reveals a prominent word at the constituent boundary, even though there is no pause at that word.
",5 Analysis,[0],[0]
Effect of transcription errors.,5 Analysis,[0],[0]
The results and analyses so far have assumed that we have reliable transcripts.,5 Analysis,[0],[0]
"In fact, the original transcripts contained errors, and the Treebank annotators used these without reference to audio files.",5 Analysis,[0],[0]
"Mississippi State University (MS-State) ran a clean-up project
that produced more accurate word transcripts and time alignments (Deshmukh et al., 1998).",5 Analysis,[0],[0]
"The NXT corpus provides reconciliation between Treebank and MS-State transcripts in terms of annotating missed/extra/substituted words, but parses were not re-annotated.",5 Analysis,[0],[0]
The transcript errors mean that the acoustic signal is inconsistent with the “gold” parse tree.,5 Analysis,[0],[0]
"Below are some examples of “fluent” sentences (according to the Treebank transcripts) with transcription errors, for which prosodic features “hurt” parsing.",5 Analysis,[0],[0]
Words that transcribers missed are in brackets and those inserted are underlined.,5 Analysis,[0],[0]
S1: and because <,5 Analysis,[0],[0]
uh> like if your spouse died <all of a sudden you be> all alone it ’d be nice to go someplace with people similar to you to have friends S2: uh uh <i have had>,5 Analysis,[0],[0]
"my wife ’s picked up a couple of things saying uh boy if we could refinish that ’d be a beautiful piece of furniture
Multi-syllable errors are especially problematic, leading to serious inconsistencies between the text and the acoustic signal.",5 Analysis,[0],[0]
"Further, the missed words lead to an incorrect attachment in the “gold” parse in S1 and a missing restart edit in S2.",5 Analysis,[0],[0]
"Indeed, for sentences with consecutive transcript errors, which we expect to impact the prosodic features, there is a statistically significant (p-value < 0.05) negative effect on parsing with prosody.",5 Analysis,[0],[0]
"Not included in this analysis are sentence boundary errors, which also change the “gold” parse.",5 Analysis,[0],[0]
"Thus, prosody may be more useful than results here indicate.",5 Analysis,[0],[0]
"Related work on parsing conversational speech has mainly addressed four problems: speech recognition errors, unknown sentence segmentation, disfluencies, and integrating prosodic cues.",6 Related Work,[0],[0]
"Our work addresses the last two problems, which involve studies based on hand-transcribed text and known sentence boundaries, as in much speech parsing work.",6 Related Work,[0],[0]
The related studies are thus the focus of this discussion.,6 Related Work,[0],[0]
"We describe studies using the Switchboard corpus, since it has dominated work in this area, being the largest source of treebanked English spontaneous speech.
",6 Related Work,[0],[0]
"One major challenge of parsing conversational speech is the presence of disfluencies, which are grammatical and prosodic interruptions.",6 Related Work,[0],[0]
Disfluencies include repetitions (‘I am +,6 Related Work,[0],[0]
"I am’), repairs (‘I am +",6 Related Work,[0],[0]
"we are’), and restarts (‘What I",6 Related Work,[0],[0]
+,6 Related Work,[0],[0]
"Today is the...’), where the ‘+’ corresponds to an interruption point.",6 Related Work,[0],[0]
"Repairs often involve parallel grammatical
constructions, but they can be more complex, involving hedging, clarifications, etc.",6 Related Work,[0],[0]
"Charniak and Johnson (Charniak and Johnson, 2001; Johnson and Charniak, 2004) demonstrated that disfluencies are different in character than other constituents and that parsing performance improves from combining a PCFG parser with a separate module for disfluency detection via parse rescoring.",6 Related Work,[0],[0]
Our approach does not use a separate disfluency detection module; we hypothesized that the location-sensitive attention model helps handle these differences based on analysis of the text-only results (Table 1).,6 Related Work,[0],[0]
"However, more explicit modeling of disfluency pattern match characteristics in a dependency parser (Honnibal and Johnson, 2014) leads to better disfluency detection performance (F = 84.1 vs. 76.7 for our text only model).",6 Related Work,[0],[0]
"Pattern match features also benefit a neural model for disfluency detection alone (F = 87.0) (Zayats et al., 2016), and similar gains are observed by formulating disfluency detection in a transition-based framework (F = 87.5) (Wang et al., 2017).",6 Related Work,[0],[0]
"Experiments with oracle disfluencies as features improve the CL-attn text-only parsing performance from 87.85 to 89.38 on the test set, showing that more accurate disfluency modeling is a potential area of improvement.
",6 Related Work,[0],[0]
"It is well known that prosodic features play a role in human resolution of syntactic ambiguities, with more than two decades of studies seeking to incorporate prosodic features in parsing.",6 Related Work,[0],[0]
"A series of studies looked at constituent parsing informed by the presence (or likelihood) of prosodic breaks at word boundaries (Kahn et al., 2004, 2005; Hale et al., 2006; Dreyer and Shafran, 2007).",6 Related Work,[0],[0]
"Our approach improves over performance of these systems using raw acoustic features, without the need for handlabeling prosodic breaks.",6 Related Work,[0],[0]
"The gain is in part due to the improved text-based parser, but the incremental benefit of prosody here is similar to that in these prior studies.",6 Related Work,[0],[0]
"(In prior work using acoustic feature directly (Gregory et al., 2004), prosody actually degraded performance.)",6 Related Work,[0],[0]
"Our analyses of the impact of prosody also extends prior work.
",6 Related Work,[0],[0]
"Prosody is also known to provide useful cues to sentence boundaries (Liu et al., 2006), and automatic sentence segmentation performance has been shown to have a significant impact on parsing performance (Kahn and Ostendorf, 2012).",6 Related Work,[0],[0]
"In our study, sentence boundaries are given so as to focus on the role of prosody in resolving sentenceinternal parse ambiguity, for which prior work had
obtained smaller gains.",6 Related Work,[0],[0]
"Studies have also shown that parsing lattices or confusion networks can improve ASR performance (Kahn and Ostendorf, 2012; Yoshikawa et al., 2016).",6 Related Work,[0],[0]
"Our analysis of performance degradation for the system with prosody when the gold transcript and associated parse are in error suggests that prosody may have benefits for parsers operating on alternative ASR hypotheses.
",6 Related Work,[0],[0]
The results we compare to in Section 4 are relatively old.,6 Related Work,[0],[0]
"More recent parsing results on spontaneous speech involve dependency parsers using only text (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014; Yoshikawa et al., 2016), with the exception of a study on unsupervised dependency parsing (Pate and Goldwater, 2013).",6 Related Work,[0],[0]
"With the recent success of transition-based neural approaches in dependency parsing, researchers have adapted transition-based ideas to constituent parsing (Zhu et al., 2013; Watanabe and Sumita, 2015; Dyer et al., 2016).",6 Related Work,[0],[0]
"These approaches have not yet been used with speech, to our knowledge, but we expect it to be straightforward to extend our prosody integration framework to these systems, both for dependency and constituency parsing.",6 Related Work,[0],[0]
We have presented a framework for directly integrating acoustic-prosodic features with text in a neural encoder-decoder parser that does not require hand-annotated prosodic structure.,7 Conclusion,[0],[0]
"On conversational sentences, we obtained strong results when including word-level acoustic-prosodic features over using only transcriptions.",7 Conclusion,[0],[0]
"The acousticprosodic features provide the largest gains when sentences are disfluent or long, and analysis of error types shows that these features are especially helpful in repairing attachment errors.",7 Conclusion,[0],[0]
"In cases where prosodic features hurt performance, we observe a statistically significant negative effect caused by imperfect human transcriptions that make the “ground truth” parse tree and the acoustic signal inconsistent, which suggests that there is more to be gained from prosody than observed in prior studies.",7 Conclusion,[0],[0]
"We thus plan to investigate aligning the Treebank and MS-State versions of Switchboard for future work.
",7 Conclusion,[0],[0]
"Here, we assumed known sentence boundaries and hand transcripts, leaving open the question of whether increased benefits from prosody can be gained by incorporating sentence segmentation in parsing and/or in parsing ASR lattices.",7 Conclusion,[0],[0]
"Most prior work using prosody in parsing has been on con-
stituent parsing, since prosodic cues tend to align with constituent boundaries.",7 Conclusion,[0],[0]
"However, it remains an open question as to whether dependency, constituency or other parsing frameworks are better suited to leveraging prosody.",7 Conclusion,[0],[0]
"Our study builds on a parser that uses reverse order text processing, since it provides a stronger text-only baseline.",7 Conclusion,[0],[0]
"However, the prosody modeling component relies only on a 1 second lookahead of the current word (for pause binning), so it could be easily incorporated in an incremental parser.",7 Conclusion,[0],[0]
We thank the anonymous reviewers for their helpful feedback.,Acknowledgement,[0],[0]
"We also thank Pranava Swaroop Madhyastha, Hao Tang, Jon Cai, Hao Cheng, and Navdeep Jaitly for their help with initial discussions and code setup.",Acknowledgement,[0],[0]
"This research was partially funded by a Google Faculty Research Award to Mohit Bansal, Karen Livescu, and Kevin Gimpel; and NSF grant no. IIS-1617176.",Acknowledgement,[0],[0]
The opinions expressed in this work are those of the authors and do not necessarily reflect the views of the funding agency.,Acknowledgement,[0],[0]
"A.1 Miscellany
Our main model code is available at https://github.com/shtoshni92/ speech_parsing.",A Appendix,[0],[0]
Most of the data preprocessing code is available at https://github. com/trangham283/seq2seq_parser/ tree/master/src/data_preps.,A Appendix,[0],[0]
"Part of our data preprocessing pipeline also uses https: //github.com/syllog1sm/swbd_tools.
",A Appendix,[0],[0]
Table 8 shows statistics of our Switchboard dataset.,A Appendix,[0],[0]
"As defined, for example, in (Charniak and Johnson, 2001; Honnibal and Johnson, 2014), the splits are: conversations sw2000 to sw3000 for training, sw4500 to sw4936 for validation (dev), and sw4000 to sw4153 for evaluation (test).",A Appendix,[0],[0]
"In addition, previous work has reserved sw4154 to sw4500 for “future use” (dev2), but we added this set to our training set.",A Appendix,[0],[0]
"That is, all of our models are trained on Switchboard conversations sw2000 to sw3000 as well as sw4154 to sw4500.
",A Appendix,[0],[0]
Figure 4 illustrates the data preprocessing step.,A Appendix,[0],[0]
"On the decoder end, we also use a post-processing step that merges the original sentence with the decoder output to obtain the standard constituent tree representation.",A Appendix,[0],[0]
"During inference, in rare cases (and virtually none as our models converge), the decoder does not generate a valid parse sequence, due to the mismatch in brackets and/or the mismatch in the number of pre-terminals and terminals, i.e., num(XX) 6= num(tokens).",A Appendix,[0],[0]
"In such cases, we simply add/remove brackets from either end of the parse, or add/remove pre-terminal symbols XX in the middle of the parse to match the number of input tokens.
",A Appendix,[0],[0]
Figure 5 shows the distribution of pause durations in our training data.,A Appendix,[0],[0]
"Our pause buckets of
0 < p ≤ 0.05 s, 0.05 s < p ≤ 0.2 s, 0.2 < p ≤ 1 s, and p > 1 s described in the main paper were based on this distribution of pause lengths.
",A Appendix,[0],[0]
"Table 9 shows the comprehensive error counts in all error categories defined in the Berkeley Parse Analyzer (Kummerfeld et al., 2012) in both the fluent and disfluent subsets.
",A Appendix,[0],[0]
"A.2 Tree Examples In figures 6, 7, and 8, we follow node correction notations as in (Kummerfeld et al., 2012).",A Appendix,[0],[0]
"In particular, missing nodes are marked in blue on the gold tree, extra nodes are marked red in the predicted tree, and yellow nodes denote crossing.",A Appendix,[0],[0]
"In conversational speech, the acoustic signal provides cues that help listeners disambiguate difficult parses.",abstractText,[0],[0]
"For automatically parsing spoken utterances, we introduce a model that integrates transcribed text and acoustic-prosodic features using a convolutional neural network over energy and pitch trajectories coupled with an attention-based recurrent neural network that accepts text and prosodic features.",abstractText,[0],[0]
"We find that different types of acoustic-prosodic features are individually helpful, and together give statistically significant improvements in parse and disfluency detection F1 scores over a strong text-only baseline.",abstractText,[0],[0]
"For this study with known sentence boundaries, error analyses show that the main benefit of acousticprosodic features is in sentences with disfluencies, attachment decisions are most improved, and transcription errors obscure gains from prosody.",abstractText,[0],[0]
Parsing Speech: A Neural Approach to Integrating Lexical and Acoustic-Prosodic Information,title,[0],[0]
"General treebank analyses are graph structured, but parsers are typically restricted to tree structures for efficiency and modeling reasons. We propose a new representation and algorithm for a class of graph structures that is flexible enough to cover almost all treebank structures, while still admitting efficient learning and inference. In particular, we consider directed, acyclic, one-endpoint-crossing graph structures, which cover most long-distance dislocation, shared argumentation, and similar tree-violating linguistic phenomena. We describe how to convert phrase structure parses, including traces, to our new representation in a reversible manner. Our dynamic program uniquely decomposes structures, is sound and complete, and covers 97.3% of the Penn English Treebank. We also implement a proofof-concept parser that recovers a range of null elements and trace types.",text,[0],[0]
"Many syntactic representations use graphs and/or discontinuous structures, such as traces in Government and Binding theory and f-structure in Lexical Functional Grammar (Chomsky 1981; Kaplan and Bresnan 1982).",1 Introduction,[0],[0]
"Sentences in the Penn Treebank (PTB, Marcus et al. 1993) have a core projective tree structure and trace edges that represent control structures, wh-movement and more.",1 Introduction,[0],[0]
"However, most parsers and the standard evaluation metric ignore these edges and all null elements.",1 Introduction,[0],[0]
"By leaving out parts of the structure, they fail to provide key relations to downstream tasks such as question answering.",1 Introduction,[0],[0]
"While there has been work on capturing
some parts of this extra structure, it has generally either been through post-processing on trees (Johnson 2002; Jijkoun 2003; Campbell 2004; Levy and Manning 2004; Gabbard et al. 2006) or has only captured a limited set of phenomena via grammar augmentation (Collins 1997; Dienes and Dubey 2003; Schmid 2006; Cai et al. 2011).
",1 Introduction,[0],[0]
We propose a new general-purpose parsing algorithm that can efficiently search over a wide range of syntactic phenomena.,1 Introduction,[0],[0]
"Our algorithm extends a non-projective tree parsing algorithm (Pitler et al. 2013; Pitler 2014) to graph structures, with improvements to avoid derivational ambiguity while maintaining an O(n4) runtime.",1 Introduction,[0],[0]
"Our algorithm also includes an optional extension to ensure parses contain a directed projective tree of non-trace edges.
",1 Introduction,[0],[0]
Our algorithm cannot apply directly to constituency parses–it requires lexicalized structures similar to dependency parses.,1 Introduction,[0],[0]
We extend and improve previous work on lexicalized constituent representations (Shen et al. 2007; Carreras et al. 2008; Hayashi and Nagata 2016) to handle traces.,1 Introduction,[0],[0]
"In this form, traces can create problematic structures such as directed cycles, but we show how careful choice of head rules can minimize such issues.
We implement a proof-of-concept parser, scoring 88.1 on trees in section 23 and 70.6 on traces.",1 Introduction,[0],[0]
"Together, our representation and algorithm cover 97.3% of sentences, far above the coverage of projective tree parsers (43.9%).",1 Introduction,[0],[0]
"This work builds on two areas: non-projective tree parsing, and parsing with null elements.
",2 Background,[0],[0]
"Non-projectivity is important in syntax for rep-
441
Transactions of the Association for Computational Linguistics, vol. 5, pp.",2 Background,[0],[0]
"441–454, 2017.",2 Background,[0],[0]
Action Editor: Marco Kuhlmann.,2 Background,[0],[0]
"Submission batch: 4/2017; Published 11/2017.
",2 Background,[0],[0]
c©2017 Association for Computational Linguistics.,2 Background,[0],[0]
"Distributed under a CC-BY 4.0 license.
",2 Background,[0],[0]
"resenting many structures, but inference over the space of all non-projective graphs is intractable.",2 Background,[0],[0]
"Fortunately, in practice almost all parses are covered by well-defined subsets of this space.",2 Background,[0],[0]
"For dependency parsing, recent work has defined algorithms for inference within various subspaces (GómezRodrı́guez and Nivre 2010; Pitler et al. 2013).",2 Background,[0],[0]
We build upon these algorithms and adapt them to constituency parsing.,2 Background,[0],[0]
"For constituency parsing, a range of formalisms have been developed that are mildlycontext sensitive, such as CCG (Steedman 2000), LFG (Kaplan and Bresnan 1982), and LTAG (Joshi and Schabes 1997).
",2 Background,[0],[0]
"Concurrently with this work, Cao et al. (2017) also proposed a graph version of Pitler et al. (2013)’s One-Endpoint Crossing (1-EC) algorithm.",2 Background,[0],[0]
"However, Cao’s algorithm does not consider the direction of edges1 and so it could produce cycles, or graphs with multiple root nodes.",2 Background,[0],[0]
"Their algorithm also has spurious ambiguity, with multiple derivations of the same parse structure permitted.",2 Background,[0],[0]
"One advantage of their algorithm is that by introducing a new item type it can handle some cases of the Locked-Chain we define below (specifically, when N is even), though in practise they also restrict their algorithm to ignore such cases.",2 Background,[0],[0]
"They also show that the class of graphs they generate corresponds to the 1-EC pagenumber-2 space, a property that applies to this work as well2.
",2 Background,[0],[0]
Parsing with Null Elements in the PTB has taken two general approaches.,2 Background,[0],[0]
"The first broadly effective system was Johnson (2002), which post-processed the output of a parser, inserting extra elements.",2 Background,[0],[0]
"This was effective for some types of structure, such as null complementizers, but had difficulty with long distance dependencies.",2 Background,[0],[0]
The other common approach has been to thread a trace through the tree structure on the non-terminal symbols.,2 Background,[0],[0]
"Collins (1997)’s third model used this approach to recover wh-traces, while Cai et al. (2011) used it to recover null pronouns, and others have used it for a range of movement types (Dienes and Dubey 2003; Schmid 2006).",2 Background,[0],[0]
"These approaches have the disadvantage that each
1 To produce directed edges, their parser treats the direction as part of the edge label.
",2 Background,[0],[0]
2,2 Background,[0],[0]
This is a topological space with two half-planes sharing a boundary.,2 Background,[0],[0]
"All edges are drawn on one of the two half-planes and each half-plane contains no crossings.
",2 Background,[0],[0]
additional trace dramatically expands the grammar.,2 Background,[0],[0]
Our representation is similar to LTAG-Spinal (Shen et al. 2007) but has the advantage that it can be converted back into the PTB representation.,2 Background,[0],[0]
Hayashi and Nagata (2016) also incorporated null elements into a spinal structure but did not include a representation of co-indexation.,2 Background,[0],[0]
"In related work, dependency parsers have been used to assist in constituency parsing, with varying degrees of representation design, but only for trees (Hall, Nivre, and Nilsson 2007; Hall and Nivre 2008; FernándezGonzález and Martins 2015; Kong et al. 2015).
",2 Background,[0],[0]
"Kato and Matsubara (2016) described a new approach, modifying a transition-based parser to recover null elements and traces, with strong results, but using heuristics to determine trace referents.",2 Background,[0],[0]
"Our algorithm is a dynamic program, similar at a high level to CKY (Kasami 1966;",3 Algorithm,[0],[0]
Younger 1967; Cocke 1969).,3 Algorithm,[0],[0]
The states of our dynamic program (items) represent partial parses.,3 Algorithm,[0],[0]
"Usually in CKY, items are defined as covering the n words in a sentence, starting and ending at the spaces between words.",3 Algorithm,[0],[0]
"We follow Eisner (1996), defining items as covering the n−1 spaces in a sentence, starting and ending on words, as shown in Figure 1.",3 Algorithm,[0],[0]
"This means that we process each word’s left and right dependents separately, then combine the two halves.
",3 Algorithm,[0],[0]
"We use three types of items: (1) a single edge, linking two words, (2) a continuous span, going from one word to another, representing all edges linking pairs of words within the span, (3) a span (as defined in 2) plus an additional word outside the span, enabling the inclusion of edges between that word and words in the span.
",3 Algorithm,[0],[0]
"Within the CKY framework, the key to defining our algorithm is a set of rules that specify which items are allowed to combine.",3 Algorithm,[0],[0]
"From a bottom-up perspective, a parse is built in a series of steps, which come in three types: (1) adding an edge to an item, (2) combining two items that have non-overlapping adjacent spans to produce a new item with a larger span, (3) combining three items, similarly to (2).
",3 Algorithm,[0],[0]
"Example: To build intuition for the algorithm, we will describe the derivation in Figure 1.",3 Algorithm,[0],[0]
"Note, item sub-types (I, X, and N) are defined below, and in-
cluded here for completeness.",3 Algorithm,[0],[0]
"(1) We initialize with spans of width one, going between adjacent words, e.g. between ROOT and We. ∅",3 Algorithm,[0],[0]
"7→ I0,1 (2) Edges can be introduced in exactly two ways, either by linking the two ends of a span, e.g. like– running, or by linking one end of a span with a word outside the span, e.g. like–. (which in this case forms a new item that has a span and an external word).
",3 Algorithm,[0],[0]
"I2,3 ∧ like–running 7→ I2,3 I3,4 ∧ like–. 7→ X3,4,2
(3) We add a second edge to one of the items.",3 Algorithm,[0],[0]
"I1,2 ∧ running–We 7→ X1,2,3 (4) Now that all the edges to We have been added, the two items either side of it are combined to form an item that covers it.",3 Algorithm,[0],[0]
"I0,1 ∧ X1,2,3 7→",3 Algorithm,[0],[0]
"N0,2,3 (5) We add an edge, creating a crossing because We is an argument of a word to the right of like.",3 Algorithm,[0],[0]
"N0,2,3 ∧ ROOT–like 7→ N0,2,3 (7) We use a ternary rule to combine three adjacent items.",3 Algorithm,[0],[0]
"In the process we create another crossing.
",3 Algorithm,[0],[0]
"N0,2,3 ∧ I2,3 ∧ X3,4,2 7→ I0,6",3 Algorithm,[0],[0]
"Notation Vertices are p, q, etc.",3.1 Algorithm definition,[0],[0]
"Continuous ranges are [pq], [pq), (pq], or (pq), where the brackets indicate inclusion, [ ], or exclusion, ( ), of each endpoint.",3.1 Algorithm definition,[0],[0]
A span [pq] and vertex o that are part of the same item are [pq.o].,3.1 Algorithm definition,[0],[0]
"Two vertices and an arrow indicate an edge, ~pq.",3.1 Algorithm definition,[0],[0]
"Two vertices without an arrow are an edge in either direction, pq. Ranges and/or vertices connected by a dash define a set of edges, e.g. the
set of edges between o and (pq) is o–(pq) (in some places we will also use this to refer to an edge from the set, rather than the whole set).",3.1 Algorithm definition,[0],[0]
"If there is a path from p to q, q is reachable from p.
Item Types As shown in Figure 1, our items start and end on words, fully covering the spaces in between.",3.1 Algorithm definition,[0],[0]
"Earlier we described three item types: an edge, a span, and a span plus an external vertex.",3.1 Algorithm definition,[0],[0]
"Here we define spans more precisely as I , and divide the span plus an external point case into five types differing in the type of edge crossing they contain: p qI , Interval A span for which there are no edges sr :",3.1 Algorithm definition,[0],[0]
r ∈ (pq) and s /∈,3.1 Algorithm definition,[0],[0]
"[pq].
o X , Exterval An interval and either op or oq, where",3.1 Algorithm definition,[0],[0]
o /∈,3.1 Algorithm definition,[0],[0]
[pq].,3.1 Algorithm definition,[0],[0]
"B, Both A span and vertex [pq.o], for which there are no edges sr : r ∈ (pq) and s /∈",3.1 Algorithm definition,[0],[0]
"[pq] ∪ o. Edges o–[pq] may be crossed by pq, p–(pq) or q–(pq), and at least one crossing of the second and third types occurs.",3.1 Algorithm definition,[0],[0]
Edges o–(pq) may not be crossed by (pq)–(pq) edges.,3.1 Algorithm definition,[0],[0]
"L, Left Same as B, but o–(pq) edges may only cross p–(pq] edges.",3.1 Algorithm definition,[0],[0]
"R, Right Symmetric with L. N , Neither An interval and a vertex [pq.o], with at least one o–(pq) edge, which can be crossed by pq, but no other [pq]–[pq] edges.
",3.1 Algorithm definition,[0],[0]
Items are further specified as described in Alg.,3.1 Algorithm definition,[0],[0]
1.,3.1 Algorithm definition,[0],[0]
"Most importantly, for each pair of o, p, and q in an item, the rules specify whether one is a parent of the other, and if they are directly linked by an edge.
",3.1 Algorithm definition,[0],[0]
"For an item H with span [ij], define covered(H) as (ij), and define visible(H) as {i, j}.",3.1 Algorithm definition,[0],[0]
"When an external vertex x is present, it is in visible(H).",3.1 Algorithm definition,[0],[0]
"Call the union of multiple such sets covered(F,G,H), and visible(F,G,H).
",3.1 Algorithm definition,[0],[0]
Deduction Rules To make the deduction rules manageable,3.1 Algorithm definition,[0],[0]
", we use templates to define some constraints explicitly, and then use code to generate the rules.",3.1 Algorithm definition,[0],[0]
"During rule generation, we automatically apply additional constraints to prevent rules that would leave a word in the middle of a span without a parent or that would form a cycle (proven possible below).",3.1 Algorithm definition,[0],[0]
Algorithm 1 presents the explicit constraints.,3.1 Algorithm definition,[0],[0]
"Once expanded, these give rules that specify all properties for each item (general type, external vertex position
Algorithm 1 Dynamic program for Lock-Free, One-Endpoint Crossing, Directed, Acyclic graph parsing.",3.1 Algorithm definition,[0],[0]
Adding Edges: Consider a span [lr] and vertex x /∈,3.1 Algorithm definition,[0],[0]
[lr].,3.1 Algorithm definition,[0],[0]
"Edges between l and r can be added to items I , N , L, R, and B (making L̂ and N̂ in those cases).",3.1 Algorithm definition,[0],[0]
"Edges between l and x can be added to items I (forming an X), R, and N .",3.1 Algorithm definition,[0],[0]
"Edges between r and x can be added to items I (forming an X), L, and N .",3.1 Algorithm definition,[0],[0]
"The l–r edge cannot be added after another edge, and N items cannot get both l–x and r–x edges.",3.1 Algorithm definition,[0],[0]
Combining Items: In the rules below the following notation is used: For this explanation items are T,3.1 Algorithm definition,[0],[0]
[lr crl clr] and T,3.1 Algorithm definition,[0],[0]
[lrx crl cxl clr cxr clx crx].,3.1 Algorithm definition,[0],[0]
T is the type of item.,3.1 Algorithm definition,[0],[0]
Multiple letters indicate any of those types are allowed.,3.1 Algorithm definition,[0],[0]
"For the next three types of notation, if an item does not have a mark, either option is valid.",3.1 Algorithm definition,[0],[0]
˙ T and T : indicate the number of edges between the external vertex and the span: one or more than one respectively.,3.1 Algorithm definition,[0],[0]
·T and T · indicate the position of the external vertex relative to the item’s span (left or right respectively).,3.1 Algorithm definition,[0],[0]
T̂ indicates for N and L that ∀p ∈,3.1 Algorithm definition,[0],[0]
(ij)∃rs : i≤r<p<s≤j.,3.1 Algorithm definition,[0],[0]
"In (11) and (12) it is optional, but true for output iff true for input.",3.1 Algorithm definition,[0],[0]
"l, r, and x: the position of the left end of the span, the right end, and the external vertex, respectively.",3.1 Algorithm definition,[0],[0]
"crl, cxl, etc: connectivity of each pair of visible vertices, from the first subscript to the second.",3.1 Algorithm definition,[0],[0]
"Using crl as an example, these can be .",3.1 Algorithm definition,[0],[0]
"(unconstrained), d ( ~rl must exist), p (l is reachable from r, but ~rl does not exist), n",3.1 Algorithm definition,[0],[0]
"(l is not reachable from r), d (= p ∨ n), n",3.1 Algorithm definition,[0],[0]
(= d ∨ p).,3.1 Algorithm definition,[0],[0]
Note:,3.1 Algorithm definition,[0],[0]
"In the generated rules every value is d, p, or n, leading to multiple rules per template below.
I[ij nd]← max   (Init) j = i+1 (1) I[i i+1 nn] I[i+1 j nn] maxk∈(i,j)   (2) I[ik nd] I[kj",3.1 Algorithm definition,[0],[0]
..],3.1 Algorithm definition,[0],[0]
(3) BLRN · [ikj nndddd] I[kj ..],3.1 Algorithm definition,[0],[0]
"maxl∈(k,j){ (4) RN ·",3.1 Algorithm definition,[0],[0]
[ikl nndddd] I[kl ..] ·LNX[ljk .d..d.] (5) BLRN · [ikl nndddd] I[kl ..] I[lj ..],3.1 Algorithm definition,[0],[0]
"maxl∈(i,k){ (6) I[il n.] ·LN [lki .d.dnn]",3.1 Algorithm definition,[0],[0]
"·N : [kjl ddd.d.]
(7) RNX· [ilk nn.ddd] I[lk ..]",3.1 Algorithm definition,[0],[0]
"·LN :: [kjl .d..d.]
B· [ijx nndddd]← maxk∈(i,j)  
(8) L̂N̂ ·",3.1 Algorithm definition,[0],[0]
[ikx nn.ddd] R·,3.1 Algorithm definition,[0],[0]
[kjx ...d.d] (9) L̂N̂ ·,3.1 Algorithm definition,[0],[0]
[ikx nn.ddd] N ·,3.1 Algorithm definition,[0],[0]
[kjx d.dd.d] (10) L̂N̂ ·,3.1 Algorithm definition,[0],[0]
[ikx nn.ddd] N ·,3.1 Algorithm definition,[0],[0]
"[kjx d.dd.d]
˙ L̂[ijx dddddd]←",3.1 Algorithm definition,[0],[0]
"maxk∈(i,j){
(11) X[ikx .d.dnn] · L̂N̂",3.1 Algorithm definition,[0],[0]
[kji .d.ddd] (12) X[ikx .d.ddd,3.1 Algorithm definition,[0],[0]
],3.1 Algorithm definition,[0],[0]
· L̂N̂,3.1 Algorithm definition,[0],[0]
"[kji .d.ddd]
L :",3.1 Algorithm definition,[0],[0]
"[ijx dddddd]← maxk∈(i,j)   (13) LN",3.1 Algorithm definition,[0],[0]
[ikx .d.ddd],3.1 Algorithm definition,[0],[0]
·N,3.1 Algorithm definition,[0],[0]
[kji dddddd] (14) LN,3.1 Algorithm definition,[0],[0]
[ikx .d.ddd],3.1 Algorithm definition,[0],[0]
·N,3.1 Algorithm definition,[0],[0]
[kji dddddd] (15) L[ikx .d.ddd] I[kj ..],3.1 Algorithm definition,[0],[0]
(16) L[ikx .d.ddd] I[kj ..],3.1 Algorithm definition,[0],[0]
(17) N,3.1 Algorithm definition,[0],[0]
[ikx dddddd] I[kj ..],3.1 Algorithm definition,[0],[0]
(18) N,3.1 Algorithm definition,[0],[0]
[ikx dddddd] I[kj ..],3.1 Algorithm definition,[0],[0]
(19) N,3.1 Algorithm definition,[0],[0]
[ikx dddddd] I[kj ..,3.1 Algorithm definition,[0],[0]
"]
(20) N",3.1 Algorithm definition,[0],[0]
"[ikx dddddd] I[kj ..]
N :",3.1 Algorithm definition,[0],[0]
"[ijx dddddd]← maxk∈(i,j)   (21) ·N [ikx dddddd] I[kj ..]",3.1 Algorithm definition,[0],[0]
(22) ·N [ikx dddddd] I[kj ..] (23) I[ik ..] N ·,3.1 Algorithm definition,[0],[0]
[kjx dddddd] (24) I[ik ..] N ·,3.1 Algorithm definition,[0],[0]
"[kjx dddddd]
˙ N",3.1 Algorithm definition,[0],[0]
"[ijx dddddd]← maxk∈(i,j)   (25) ·X[ikx .d.ddd] I[kj ..] (26) ·X[ikx .d.ddd] I[kj ..] (27) I[ik ..]",3.1 Algorithm definition,[0],[0]
X· [kjx .d.ddd] (28) I[ik ..],3.1 Algorithm definition,[0],[0]
"X· [kjx .d.ddd]
I[ij pn], ·B[ijx ddnndd], R : [ijx dddddd], and ˙ R[ijx dddddd] are symmetric with cases above.
",3.1 Algorithm definition,[0],[0]
"relative to the item spans, connectivity of every pair of vertices in each item, etc).
",3.1 Algorithm definition,[0],[0]
The final item for n vertices is an interval where the left end has a parent.,3.1 Algorithm definition,[0],[0]
For parsing we assume there is a special root word at the end of the sentence.,3.1 Algorithm definition,[0],[0]
Definition 1.,3.2 Properties,[0],[0]
"A graph is One-Endpoint Crossing if, when drawn with vertices along the edge of a halfplane and edges drawn in the open half-plane above, for any edge e, all edges that cross e share a vertex.",3.2 Properties,[0],[0]
"Let that vertex be Pt(e).
",3.2 Properties,[0],[0]
"Aside from applying to graphs, this is the same as
Pitler et al. (2013)’s 1-EC tree definition.
",3.2 Properties,[0],[0]
Definition 2.,3.2 Properties,[0],[0]
"A Locked-Chain (shown in Fig. 2) is formed by a set of consecutive vertices in order from 0 to N , where N > 3, with edges {(0, N−1), (1, N)} ∪ {(i, i+2)∀i ∈",3.2 Properties,[0],[0]
"[0, N−2]}.",3.2 Properties,[0],[0]
Definition 3.,3.2 Properties,[0],[0]
"A graph is Lock-Free if it does not contain edges that form a Locked-Chain.
",3.2 Properties,[0],[0]
"Note that in practice, most parse structures satisfy 1-EC, and the Locked-Chain structure does not occur in the PTB when using our head rules.
",3.2 Properties,[0],[0]
Theorem 1.,3.2 Properties,[0],[0]
"For the space of Lock-Free OneEndpoint Crossing graphs, the algorithm is sound, complete and gives unique decompositions.
",3.2 Properties,[0],[0]
Our proof is very similar in style and structure to Pitler et al. (2013).,3.2 Properties,[0],[0]
"The general approach is to consider the set of structures an item could represent, and divide them into cases based on properties of the internal structure.",3.2 Properties,[0],[0]
"We then show how each case can be decomposed into items, taking care to ensure all the properties that defined the case are satisfied.",3.2 Properties,[0],[0]
Uniqueness follows from having no ambiguity in how a given structure could be decomposed.,3.2 Properties,[0],[0]
"Completeness and soundness follow from the fact that our rules apply equally well in either direction, and so our top-down decomposition implies a bottom-up formation.",3.2 Properties,[0],[0]
"To give intuition for the proof, we show the derivation of one rule below.",3.2 Properties,[0],[0]
The complete proof can be found in Kummerfeld (2016).,3.2 Properties,[0],[0]
"We do not include it here due to lack of space.
",3.2 Properties,[0],[0]
"We do provide the complete set of rule templates in Algorithm 1, and in the proof of Lemma 2 we show that the case in which an item cannot be decomposed occurs if and only if the graph contains a Locked-Chain.",3.2 Properties,[0],[0]
"To empirically check our rule generation code, we checked that our parser uniquely decomposes all 1-EC parses in the PTB and is unable to decompose the rest.
",3.2 Properties,[0],[0]
"Note that by using subsets of our rules, we can restrict the space of structures we generate, giving parsing algorithms for projective DAGs, projective trees (Eisner 1996), or 1-EC trees (Pitler et al. 2013).",3.2 Properties,[0],[0]
"Versions of these spaces with undirected edges could also be easily handled with the same approach.
",3.2 Properties,[0],[0]
p qs t Derivation of rule (4) in Algorithm 1:,3.2 Properties,[0],[0]
"This rule applies to intervals with the substructure shown, and with no parent in this item for p.",3.2 Properties,[0],[0]
They have at least one p–(pq) edge (otherwise rule 1 applies).,3.2 Properties,[0],[0]
"The longest p–(pq) edge, ps, is crossed (otherwise rule 2 applies).",3.2 Properties,[0],[0]
Let C be the set of (ps)–(sq) edges (note: these cross ps).,3.2 Properties,[0],[0]
"Either all of the edges in C have a common endpoint t ∈ (sq), or if |C| = 1 let t be the endpoint in (sq) (otherwise rule 6 or 7 applies).",3.2 Properties,[0],[0]
Let D be the set of s–(tq) edges.,3.2 Properties,[0],[0]
|D| > 0,3.2 Properties,[0],[0]
"(otherwise rule 3 or 5 applies).
",3.2 Properties,[0],[0]
We will break this into three items.,3.2 Properties,[0],[0]
"First, (st)–(tq] edges would violate the 1-EC property and (st)–[ps) edges do not exist by construction.",3.2 Properties,[0],[0]
"Therefore, the middle item is an Interval [st], the left item is [ps.t], and the right item is [tq.s] (since |C| > 0 and |D| > 0).",3.2 Properties,[0],[0]
"The left item can be either
an N or R, but not an L or B because that would violate the 1-EC property for the C edges.",3.2 Properties,[0],[0]
"The right item can be an X , L, or N , but not an R or B because that would violate the 1-EC property for the D edges.",3.2 Properties,[0],[0]
"We will require edge ps to be present in the first item, and not allow pt.",3.2 Properties,[0],[0]
"To avoid a spurious ambiguity, we also prevent the first or third items from having st (which could otherwise occur in any of the three items).",3.2 Properties,[0],[0]
"Now we have broken down the original item into valid sub-items, and we have ensured that those sub-items contain all of the structure used to define the case in a unique way.
",3.2 Properties,[0],[0]
"Now we will further characterize the nature of the Lock-Free restriction to the space of graphs.
",3.2 Properties,[0],[0]
Lemma 1.,3.2 Properties,[0],[0]
No edge in a Locked-Chain in a 1-EC graph is crossed by edges that are not part of it.,3.2 Properties,[0],[0]
Proof.,3.2 Properties,[0],[0]
"First, note that: Pt((0, N−1))",3.2 Properties,[0],[0]
"= N , Pt((1, N))",3.2 Properties,[0],[0]
"= 0, and {Pt((i, i+2))",3.2 Properties,[0],[0]
= i+1,3.2 Properties,[0],[0]
∀i ∈,3.2 Properties,[0],[0]
"[0, N−2]} Call the set {(i, i+2)∀i ∈",3.2 Properties,[0],[0]
"[0, N−2]}, the chain.
Consider an edge e that crosses an edge f in a Locked-Chain.",3.2 Properties,[0],[0]
"Let ein be the end of e that is between the two ends of f , and eout be the other end.",3.2 Properties,[0],[0]
"One of e’s endpoints is at Pt(f), and Pt(e) is an endpoint of f .",3.2 Properties,[0],[0]
"There are three cases:
(i) f",3.2 Properties,[0],[0]
"= (1, N).",3.2 Properties,[0],[0]
"Here, eout = Pt(f) = 0, and ein ∈ (1, N).",3.2 Properties,[0],[0]
"For all vertices v ∈ (1, N) there is an edge g in the chain such that v is between the endpoints of g. Therefore, e will cross such an edge g. To satisfy the 1-EC property, g must share an endpoint with f , which means g is either (1, 3) or (N−2, N).",3.2 Properties,[0],[0]
"In the first case, the 1-EC property forces e = (0, 2), and in the second e = (0, N−1), both of which are part of the Locked-Chain.
(ii) f = (0, N−1), symmetrical with (i).",3.2 Properties,[0],[0]
"(iii) f = (i, i+2), for some i ∈",3.2 Properties,[0],[0]
"[0, N−2].",3.2 Properties,[0],[0]
"Here, ein = Pt(f) = i+1.",3.2 Properties,[0],[0]
"We can assume e does not cross (0, N−1) or (1, N), as those cases are covered by (i).",3.2 Properties,[0],[0]
"As in (i), e must cross another edge in the chain, and that edge must share an endpoint with f .
",3.2 Properties,[0],[0]
"This forces e to be either (i−1, i+1) or (i+1, i+3) (excluding one or both if they cross (0, N−1) or (1, N)), which are both in the Locked-Chain.
",3.2 Properties,[0],[0]
Our rules define a unique way to decompose almost any item into a set of other items.,3.2 Properties,[0],[0]
"The exception is B, which in some cases can not be divided into two items (i.e. has no valid binary division).
",3.2 Properties,[0],[0]
Lemma 2.,3.2 Properties,[0],[0]
A B[ij.x] has no valid binary division if and only if the graph has a Locked-Chain.,3.2 Properties,[0],[0]
Proof.,3.2 Properties,[0],[0]
Consider the k and l that give the longest ik and lj edges in a B with no valid binary division (at least one edge of each type must exist by definition).,3.2 Properties,[0],[0]
"No vertex in (ik) or (jl) is a valid split point, as they would all require one of the items to have two external vertices.
",3.2 Properties,[0],[0]
"Now, consider p ∈",3.2 Properties,[0],[0]
[kj].,3.2 Properties,[0],[0]
"If there is no edge l1r1, where i ≤ l1 < p < r1 ≤ j, then p would be a valid split point.",3.2 Properties,[0],[0]
"Therefore, such an edge must exist.",3.2 Properties,[0],[0]
"Consider l1, either l1 ∈ (ik) or there is an edge l2c, where i ≤ l2",3.2 Properties,[0],[0]
< l1 < c ≤ j (by the same logic as for l1r1).,3.2 Properties,[0],[0]
"Similarly, either r1 ∈ (jl) or there is an edge cr2 (it must be c to satisfy 1-EC).",3.2 Properties,[0],[0]
"We can also apply this logic to edges l2c and cr2, giving edges l3l1 and r1r3.",3.2 Properties,[0],[0]
This pattern will terminate when it reaches lu ∈ (ik) and rv ∈ (jl) with edges lulu−2 and rv−2rv.,3.2 Properties,[0],[0]
"Note that k = lu−1 and l = rv−1, to satisfy 1-EC.
",3.2 Properties,[0],[0]
"Since it is a B, there must be at least two x–(ij) edges.",3.2 Properties,[0],[0]
"To satisfy 1-EC, these end at lu−1 and rv−1.
",3.2 Properties,[0],[0]
"Let x be to the right (the left is symmetrical), and call i = 0, j = N−1, and x = N .",3.2 Properties,[0],[0]
"Comparing with the Locked-Chain definition, we have all the edges except one: 0 to N−1.",3.2 Properties,[0],[0]
"However, that edge must be present in the overall graph, as all B items start with an ij edge (see rules 3 and 5 in Algorithm 1).",3.2 Properties,[0],[0]
"Therefore, if there is no valid split point for a B, the overall graph must contain a Locked-Chain.
",3.2 Properties,[0],[0]
"Now, for a graph that contains a Locked-Chain, consider the items that contain the Locked-Chain.",3.2 Properties,[0],[0]
"Grouping them by their span [ij], there are five valid options:",3.2 Properties,[0],[0]
"[0, N−1], [1, N ], [0, N ], (i ≤ 0",3.2 Properties,[0],[0]
"∧ j > N ), and (i < 0",3.2 Properties,[0],[0]
∧ j ≥ N ).,3.2 Properties,[0],[0]
"Items of the last three types would be divided by our rules into smaller items, one of which contains the whole Locked-Chain.",3.2 Properties,[0],[0]
"The first two are Bs of the type discussed above.
",3.2 Properties,[0],[0]
"Now we will prove that our code to generate rules from the templates can guarantee a DAG is formed.
",3.2 Properties,[0],[0]
Lemma 3.,3.2 Properties,[0],[0]
"For any item H , ∀v ∈ covered(H) ∃u ∈ visible(H) : v is reachable from u. Proof.",3.2 Properties,[0],[0]
"This is true for initial items because covered(H) = ∅. To apply induction, consider adding edges and combing items.",3.2 Properties,[0],[0]
The lemma clearly remains true when adding an edge.,3.2 Properties,[0],[0]
"Consider combining items E, F , G to form H[ij.x], and assume the lemma is true for E, F , and G (the binary case is similar).",3.2 Properties,[0],[0]
"Since all vertices are reachable from visible(E,F,G), we only need to ensure that ∀v ∈",3.2 Properties,[0],[0]
"visible(E,F,G) ∃u ∈",3.2 Properties,[0],[0]
visible(H) : v is reachable from u.,3.2 Properties,[0],[0]
"The connectivity between all pairs {(u, v) | u ∈ visible(H), v ∈",3.2 Properties,[0],[0]
"visible(E,F,G)} can be inferred from the item definitions, and so this requirement can be enforced in rule generation.
",3.2 Properties,[0],[0]
Lemma 4.,3.2 Properties,[0],[0]
The final item is a directed acyclic graph.,3.2 Properties,[0],[0]
Proof.,3.2 Properties,[0],[0]
"First, consider acyclicity.",3.2 Properties,[0],[0]
Initial items do not contain any edges and so cannot contain a cycle.,3.2 Properties,[0],[0]
"For induction, there are two cases:
(i) Adding an Edge ~pq to an item H: Assume that H does not contain any cycles.",3.2 Properties,[0],[0]
"~pq will create a cycle if and only if p is reachable from q. By construction, p and q ∈ visible(H), and so the item definition contains whether p is reachable from q.
(ii)",3.2 Properties,[0],[0]
"Combining Items: Assume that in isolation, none of the items being combined contain cycles.",3.2 Properties,[0],[0]
"Therefore, a cycle in the combined item must be composed of paths in multiple items.",3.2 Properties,[0],[0]
A path in one item can only continue in another item by passing through a visible vertex.,3.2 Properties,[0],[0]
"Therefore, a cycle would have to be formed by a set of paths between visible vertices.",3.2 Properties,[0],[0]
"But the connectivity of every pair of visible vertices is specified in the item definitions.
",3.2 Properties,[0],[0]
"In both cases, rules that create a cycle can be excluded during rule generation.
",3.2 Properties,[0],[0]
"By induction, the items constructed by our algorithm do not contain cycles.",3.2 Properties,[0],[0]
"Together with Lemma 3 and the final item definition, this means the final structure is an acyclic graph with all vertices reachable from vertex n.
Next, we will show two properties that give intuition for the algorithm.",3.2 Properties,[0],[0]
"Specifically, we will prove which rules add edges that are crossed in the final derivation.
",3.2 Properties,[0],[0]
Lemma 5.,3.2 Properties,[0],[0]
An edge ij added to I[ij] is not crossed.,3.2 Properties,[0],[0]
Proof.,3.2 Properties,[0],[0]
"First, we will show three properties of any pair of items in a derivation (using [ij.x] and [kl.y]).
",3.2 Properties,[0],[0]
(1) It is impossible for either i <,3.2 Properties,[0],[0]
k,3.2 Properties,[0],[0]
< j < l or k,3.2 Properties,[0],[0]
< i,3.2 Properties,[0],[0]
<,3.2 Properties,[0],[0]
"l < j, i.e., items cannot have partially overlapping spans.",3.2 Properties,[0],[0]
"As a base case, the final item is an interval spanning all vertices, and so no other item can partially overlap with it.",3.2 Properties,[0],[0]
"Now assume it is true for an item H and consider the rules in reverse, breaking H up.",3.2 Properties,[0],[0]
"By construction, each rule divides H into items with spans that are adjacent, overlapping only at their visible vertices.",3.2 Properties,[0],[0]
"Since the new items are nested within H , they do not overlap with any items H did not overlap with.",3.2 Properties,[0],[0]
"By induction, no pair of items have partially overlapping spans.
",3.2 Properties,[0],[0]
(2) For items with nested spans (i ≤ k,3.2 Properties,[0],[0]
"< l ≤ j), y ∈",3.2 Properties,[0],[0]
[ij]∪{x}.,3.2 Properties,[0],[0]
"Following the argument for the previous case, the [ij.x] item must be decomposed into a set of items that includes [kl.y].",3.2 Properties,[0],[0]
"Now, consider how those items are combined.",3.2 Properties,[0],[0]
"The rules that start with an item with an external vertex produce an item that either has the same external vertex, or with the external vertex inside the span of the new item.",3.2 Properties,[0],[0]
"Therefore, y must either be equal to x or inside [ij].
(3) For items without nested spans, x /∈",3.2 Properties,[0],[0]
(kl).,3.2 Properties,[0],[0]
Assume x ∈,3.2 Properties,[0],[0]
(kl) for two items without nested spans.,3.2 Properties,[0],[0]
"None of the rules combine such a pair of items, or allow one to be extended so that the other is nested within it.",3.2 Properties,[0],[0]
"However, all items are eventually combined to complete the derivation.",3.2 Properties,[0],[0]
"By contradiction, x /∈ (kl).
",3.2 Properties,[0],[0]
"Together, these mean that given an interval H with span [ij], and another item G, either ∀v ∈ visible(G), v ∈",3.2 Properties,[0],[0]
"[ij] or ∀v ∈ visible(G),",3.2 Properties,[0],[0]
v /∈,3.2 Properties,[0],[0]
(ij).,3.2 Properties,[0],[0]
"Since edges are only created between visible vertices, no edge can cross edge ij.
Lemma 6.",3.2 Properties,[0],[0]
"All edges aside from those considered in Lemma 5 are crossed.
",3.2 Properties,[0],[0]
Proof.,3.2 Properties,[0],[0]
"First, consider an edge ij added to an item [ij.x] of type B, L, R, or N. This edge is crossed by all x–(ij) edges, and in these items |x–(ij)| ≥ 1 by definition.",3.2 Properties,[0],[0]
"Note, by the same argument as Lemma 5, the edge is not crossed later in the derivation.
",3.2 Properties,[0],[0]
"Second, consider adding e ∈ {xi, xj}, to H , an item with [ij] or [ij.x], forming an item G[ij.x].",3.2 Properties,[0],[0]
"Note, e does not cross any edges in H .",3.2 Properties,[0],[0]
Let E(F,3.2 Properties,[0],[0]
[kl.y]) be the set of y–[kl] edges in some item F .,3.2 Properties,[0],[0]
Note that e ∈ E(G).,3.2 Properties,[0],[0]
"We will show how this set of edges is affected by the rules and what that implies for e. Consider each input item A[kl.y] in each
rule, with output item C. Every item A falls into one of four categories: (1) ∀f ∈ E(A), f is crossed by an edge in another of the rule’s input items, (2) E(A) ⊆ E(C), (3) A∧ kl 7→ C",3.2 Properties,[0],[0]
"and there are no ky or ly edges in A, (4)",3.2 Properties,[0],[0]
"A contains edge kl and there are no ky or ly edges in A.
Cases 2-4 are straightforward to identify.",3.2 Properties,[0],[0]
"For an example of the first case, consider the rightmost item in rule 4.",3.2 Properties,[0],[0]
"The relevant edges are k–(lj] (by construction, kl is not present).",3.2 Properties,[0],[0]
"Since the leftmost item is either an R or N, |l–(ik)| ≥ 1.",3.2 Properties,[0],[0]
Since i < k,3.2 Properties,[0],[0]
"< l < j, all k–(lj] edges will cross all l–[ik) edges.",3.2 Properties,[0],[0]
"Therefore applying this rule will cross all k–(lj] edges in the rightmost item.
",3.2 Properties,[0],[0]
"Initially, e is not crossed and e ∈ E(G).",3.2 Properties,[0],[0]
"For each rule application, edges in E(A) are either crossed (1 and 3), remain in the set E(C)",3.2 Properties,[0],[0]
"(2), or must already be crossed (4).",3.2 Properties,[0],[0]
Since the final item is an interval and E(Interval) =,3.2 Properties,[0],[0]
"∅, there must be a subsequent rule that is not in case 2.",3.2 Properties,[0],[0]
Therefore e will be crossed.,3.2 Properties,[0],[0]
"Our algorithm is based on Pitler et al. (2013), which had the crucial idea of One-Endpoint crossing and a complete decomposition of the tree case.",3.3 Comparison with Pitler et al. (2013),[0],[0]
"Our changes and extensions provide several benefits:
Extension to graphs: By extending to support multiple parents while preventing cycles, we substantially expand the space of generatable structures.
",3.3 Comparison with Pitler et al. (2013),[0],[0]
Uniqueness:,3.3 Comparison with Pitler et al. (2013),[0],[0]
By avoiding derivational ambiguity we reduce the search space and enable efficient summing as well as maxing.,3.3 Comparison with Pitler et al. (2013),[0],[0]
Most of the cases in which ambiguity arises in Pitler et al. (2013)’s algorithm are due to symmetry that is not explicitly broken.,3.3 Comparison with Pitler et al. (2013),[0],[0]
"For example, the rule we worked through in the previous section defined t ∈ (sq)",3.3 Comparison with Pitler et al. (2013),[0],[0]
when |C| = 1.,3.3 Comparison with Pitler et al. (2013),[0],[0]
"Picking t ∈ (ps) would also lead to a valid set of rules, but allowing either creates a spurious ambiguity.",3.3 Comparison with Pitler et al. (2013),[0],[0]
"This ambiguity is resolved by tracking whether there is only one edge to the external vertex or more than one, and requiring more than one in rules 6 and 7.",3.3 Comparison with Pitler et al. (2013),[0],[0]
"Other changes include ensuring equivalent structures cannot be represented by multiple item types and enforcing a unique split point in B items.
",3.3 Comparison with Pitler et al. (2013),[0],[0]
"More concise algorithm definition: By separating edge creation from item merging, and defining our rules via a combination of templates and code, we are able to define our algorithm more concisely.",3.3 Comparison with Pitler et al. (2013),[0],[0]
Edge labels can be added by calculating either the sum or max over edge types when adding each edge.,3.4.1 Edge Labels and Word Labels,[0],[0]
"Word labels (e.g., POS Tags) must be added to the state, specifying a label for each visible word (p, q and o).",3.4.1 Edge Labels and Word Labels,[0],[0]
This state expansion is necessary to ensure agreement when combining items.,3.4.1 Edge Labels and Word Labels,[0],[0]
"Our algorithm constrains the space of graph structures, but we also want to ensure that our parse contains a projective tree of non-trace edges.
",3.4.2 Ensuring a Structural Tree is Present,[0],[0]
"To ensure every word gets one and only one structural parent, we add booleans to the state, indicating whether p, q and o have structural parents.",3.4.2 Ensuring a Structural Tree is Present,[0],[0]
"When adding edges, a structural edge cannot be added if a word already has a structural parent.",3.4.2 Ensuring a Structural Tree is Present,[0],[0]
"When combining items, no word can receive more than one structural parent, and words that will end up in the middle of the span must have exactly one.",3.4.2 Ensuring a Structural Tree is Present,[0],[0]
"Together, these constraints ensure we have a tree.
",3.4.2 Ensuring a Structural Tree is Present,[0],[0]
"To ensure the tree is projective, we need to prevent structural edges from crossing.",3.4.2 Ensuring a Structural Tree is Present,[0],[0]
"Crossing edges are introduced in two ways, and in both we can avoid structural edges crossing by tracking whether there are structural o–[pq] edges.",3.4.2 Ensuring a Structural Tree is Present,[0],[0]
"Such edges are present if a rule adds a structural op or oq edge, or if a rule combines an item with structural o–[pq] edges and o will still be external in the item formed by the rule.
",3.4.2 Ensuring a Structural Tree is Present,[0],[0]
"For adding edges, every time we add a pq edge in the N , L, R and B items we create a crossing with all o–(pq) edges.",3.4.2 Ensuring a Structural Tree is Present,[0],[0]
"We do not create a crossing with oq or op, but our ordering of edge creation means these are not present when we add a pq edge, so tracking structural o–[pq] edges gives us the information we need to prevent two structural edges crossing.
",3.4.2 Ensuring a Structural Tree is Present,[0],[0]
"For combining items, in Lemma 6 we showed that during combinations, o–[pq] edges in each pair of items will cross.",3.4.2 Ensuring a Structural Tree is Present,[0],[0]
"As a result, knowing whether any o–[pq] edge is structural is sufficient to determine whether two structural edges will cross.",3.4.2 Ensuring a Structural Tree is Present,[0],[0]
"Consider a sentence with n tokens, and let E and S be the number of edge types and word labels in our grammar respectively.
",3.5 Complexity,[0],[0]
"Parses without word or edge labels: Rules have up to four positions, leading to complexity of O(n4).",3.5 Complexity,[0],[0]
"Note, there is also an important constant–once our templates are expanded, there are 49,292 rules.
",3.5 Complexity,[0],[0]
"With edge labels: When using a first-order model, edge labels only impact the rules for edge creation, leading to a complexity of O(n4 + En2).
",3.5 Complexity,[0],[0]
"With word labels: Since we need to track word labels in the state, we need to adjust every n by a factor of S, leading to O(S4n4 + ES2n2).",3.5 Complexity,[0],[0]
Our algorithm relies on the assumption that we can process the dependents to the left and right of a word independently and then combine the two halves.,4 Parse Representation,[0],[0]
"This means we need lexicalized structures, which the PTB does not provide.",4 Parse Representation,[0],[0]
We define a new representation in which each non-terminal symbol is associated with a specific word (the head).,4 Parse Representation,[0],[0]
"Unlike dependency parsing, we retain all the information required to reconstruct the constituency parse.
",4 Parse Representation,[0],[0]
"Our approach is related to Carreras et al. (2008) and Hayashi and Nagata (2016), with three key differences: (1) we encode non-terminals explicitly, rather than implicitly through adjunction operations, which can cause ambiguity, (2) we add representations of null elements and co-indexation, (3) we modify head rules to avoid problematic structures.
",4 Parse Representation,[0],[0]
Figure 3 shows a comparison of the PTB representation and ours.,4 Parse Representation,[0],[0]
"We add lexicalization, assigning each non-terminal to a word.",4 Parse Representation,[0],[0]
"The only other changes are visual notation, with non-terminals moved to be directly above the words to more clearly show the distinction between spines and edges.
",4 Parse Representation,[0],[0]
Spines:,4 Parse Representation,[0],[0]
"Each word is assigned a spine, shown im-
mediately above the word.",4 Parse Representation,[0],[0]
"A spine is the ordered set of non-terminals that the word is the head of, e.g. SVP for like.",4 Parse Representation,[0],[0]
"If a symbol occurs more than once in a spine, we use indices to distinguish instances.
",4 Parse Representation,[0],[0]
"Edges: An edge is a link between two words, with a label indicating the symbols it links in the child and parent spines.",4 Parse Representation,[0],[0]
"In our figures, edge labels are indicated by where edges start and end.
",4 Parse Representation,[0],[0]
"Null Elements: We include each null element in the spine of its parent, unlike Hayashi and Nagata (2016), who effectively treated null elements as words, assigning them independent spines.",4 Parse Representation,[0],[0]
"We also considered encoding null elements entirely on edges but found this led to poorer performance.
",4 Parse Representation,[0],[0]
Co-indexation:,4 Parse Representation,[0],[0]
"The treebank represents movement with index pairs on null elements and nonterminals, e.g. *1 and NP1 in Figure 3.",4 Parse Representation,[0],[0]
"We represent co-indexation with edges, one per reference, going from the null element to the non-terminal.",4 Parse Representation,[0],[0]
There are three special cases of co-indexation: (1) It is possible for trace edges to have the same start and end points as a non-trace edge.,4 Parse Representation,[0],[0]
We restrict this case to allow at most one trace edge.,4 Parse Representation,[0],[0]
This decreases edge coverage in the training set by 0.006%.,4 Parse Representation,[0],[0]
"(2) In some cases the reference non-terminal only spans a null element, e.g. the WHNP in Figure 4a.",4 Parse Representation,[0],[0]
For these we use a reversed edge to avoid creating a cycle.,4 Parse Representation,[0],[0]
"Figure 4a shows a situation where the trace edge links two positions in the same spine, which we assign with the spine during parsing.",4 Parse Representation,[0],[0]
(3) For parallel constructions the treebank coindexes arguments that fulfill the same roles (Fig. 4b).,4 Parse Representation,[0],[0]
These are distinct from the previous cases because neither index is on a null element.,4 Parse Representation,[0],[0]
"We considered two options: add edges from the repetition
to the referent (middle), or add edges from the repetition to the parent of the first occurrence (bottom).",4 Parse Representation,[0],[0]
"Option two produces fewer non-1-EC structures and explicitly represents all predicates, but only implicitly captures the original structure.",4 Parse Representation,[0],[0]
Prior work on parsing with spines has used radjunction to add additional non-terminals to spines.,4.1 Avoiding Adjunction Ambiguity,[0],[0]
"This introduces ambiguity, because edges modifying the same spine from different sides may not have a unique order of application.",4.1 Avoiding Adjunction Ambiguity,[0],[0]
We resolve this issue by using more articulated spines with the complete set of non-terminals.,4.1 Avoiding Adjunction Ambiguity,[0],[0]
"We found that 0.045% of spine instances in the development set are not observed in training, though in 70% of those cases an equivalent spine sans null elements is observed in training.",4.1 Avoiding Adjunction Ambiguity,[0],[0]
"To construct the spines, we lexicalize with head rules that consider the type of each non-terminal and its children.",4.2 Head Rules,[0],[0]
Different heads often represent more syntactic or semantic aspects of the phrase.,4.2 Head Rules,[0],[0]
"For trees, all head rules generate valid structures.",4.2 Head Rules,[0],[0]
"For graphs, head rules influence the creation of two problematic structures:
Cycles: These arise when the head chosen for a phrase is also an argument of another word in the phrase.",4.2 Head Rules,[0],[0]
Figure 5a shows a cycle between which and proposed.,4.2 Head Rules,[0],[0]
"We resolve this by changing the head of an SBAR to be an S rather than a Wh-noun phrase.
",4.2 Head Rules,[0],[0]
"One-Endpoint Crossing Violations: Figure 5b shows an example, with the trace from CEO to Page crossing two edges with no endpoints in common.",4.2 Head Rules,[0],[0]
We resolve this case by changing the head for VPs to be a child VP rather than an auxiliary.,4.2 Head Rules,[0],[0]
Algorithm Coverage: In Table 1 we show the impact of design decisions for our representation.,5 Results,[0],[0]
The percentages indicate how many sentences in the training set are completely recoverable by our algorithm.,5 Results,[0],[0]
"Each row shows the outcome of an addition to the previous row, starting from no traces at all, going to our representation with the head rules of Carreras et al. (2008), then changing the head rules, reversing null-null edges, and changing the target of edges in parallel constructions.",5 Results,[0],[0]
"The largest gain comes from changing the head rules, which is unsurprising since Carreras et al. (2008)’s rules were designed for trees (any set of rules form valid structures for trees).
",5 Results,[0],[0]
"Problematic Structures: Of the sentences we do not cover, 54% contain a cycle, 45% contain a 1- EC violation, and 1% contain both.",5 Results,[0],[0]
"To understand these problematic sentences, we manually inspected a random sample of twenty parses that contained a cycle and twenty parses with a 1-EC violation (these forty are 6% of all problematic parses, enough to identify the key remaining challenges).
",5 Results,[0],[0]
"For the cycles, eleven cases related to sentences containing variations of NP said interposed between two parts of a single quote.",5 Results,[0],[0]
A cycle was present because the top node of the parse was co-indexed with a null argument of said while said was an argument of the head word of the quote.,5 Results,[0],[0]
"The remaining cases were all instances of pseudo-attachment, which the treebank uses to show that non-adjacent constituents are related (Bies et al. 1995).",5 Results,[0],[0]
"These cases were split between use of Expletive (5) and Interpret Constituent Here (4) traces.
",5 Results,[0],[0]
It was more difficult to determine trends for cases where the parse structure has a 1-EC violation.,5 Results,[0],[0]
"The same three cases, Expletive, Interpret Constituent Here, and NP said accounted for half of the issues.",5 Results,[0],[0]
We implemented a parser with a first-order model using our algorithm and representation.,5.1 Implementation,[0],[0]
"Code for the parser, for conversion to and from our representation, and for our metrics is available3.",5.1 Implementation,[0],[0]
"Our parser uses a linear discriminative model, with features based on McDonald et al. (2005).",5.1 Implementation,[0],[0]
"We train
3 https://github.com/jkkummerfeld/ 1ec-graph-parser
with an online primal subgradient approach (Ratliff et al. 2007) as described by Kummerfeld, BergKirkpatrick, et al. (2015), with parallel lock-free sparse updates.
",5.1 Implementation,[0],[0]
"Loss Function: We use a weighted Hamming distance for loss-augmented decoding, as it can be efficiently decomposed within our dynamic program.",5.1 Implementation,[0],[0]
Calculating the loss for incorrect spines and extra edges is easy.,5.1 Implementation,[0],[0]
"For missing edges, we add when a deduction rule joins two spans that cover an end of the edge, since if it does not exist in one of those items it is not going to be created in future.",5.1 Implementation,[0],[0]
"To avoid double counting we subtract when combining two halves that contain the two ends of a gold edge4.
",5.1 Implementation,[0],[0]
Inside–Outside Calculations:,5.1 Implementation,[0],[0]
"Assigning scores to edges is simple, as they are introduced in a single item in the derivation.",5.1 Implementation,[0],[0]
"Spines must be introduced in multiple items (left, right, and external positions) and must be assigned a score in every case to avoid ties in beams.",5.1 Implementation,[0],[0]
"We add the score every time the spine is introduced and then subtract when two items with a spine in common are combined.
",5.1 Implementation,[0],[0]
Algorithm rule pruning: Many 1-EC structures are not seen in our data.,5.1 Implementation,[0],[0]
"We keep only the rules used in gold training parses, reducing the set of 49,292 from the general algorithm to 627 (including rules for both adding arcs and combining items).",5.1 Implementation,[0],[0]
"Almost every template in Algorithm 1 generates some unnecessary rules, and no items of type B are needed.
",5.1 Implementation,[0],[0]
"4 One alternative is to count half of it on each end, removing the need for subtraction later.",5.1 Implementation,[0],[0]
"Another is to add it during the combination step.
",5.1 Implementation,[0],[0]
"The remaining rules still have high coverage of the development set, missing only 15 rules, each applied once (out of 78,692 rule applications).",5.1 Implementation,[0],[0]
"By pruning in this way, we are considering the intersection of 1-EC graphs and the true space of structures used in language.
",5.1 Implementation,[0],[0]
"Chart Pruning: To improve speed we use beams and cube pruning (Chiang 2007), discarding items based on their Viterbi inside score.",5.1 Implementation,[0],[0]
We divide each beam into sub-beams based on aspects of the state.,5.1 Implementation,[0],[0]
"This ensures diversity and enables consideration of only compatible items during binary and ternary compositions.
",5.1 Implementation,[0],[0]
"Coarse to Fine Pruning: Rather than parsing immediately with the full model we use several passes with progressively richer structure (Goodman 1997): (1) Projective parsing without traces or spines, and simultaneously a trace classifier, (2) Non-projective parsing without spines, and simultaneously a spine classifier, (3) Full structure parsing.",5.1 Implementation,[0],[0]
"Each pass prunes using parse max-marginals and classifier scores, tuned on the development set.",5.1 Implementation,[0],[0]
The third pass also prunes spines that are not consistent with any unpruned edge from the second pass.,5.1 Implementation,[0],[0]
"For the spine classifier we use a bidirectional LSTM tagger, implemented in DyNet (Neubig et al. 2017).
",5.1 Implementation,[0],[0]
Speed: Parsing took an average of 8.6 seconds per sentence for graph parsing and 0.5 seconds when the parser is restricted to trees5.,5.1 Implementation,[0],[0]
"Our algorithm is also amenable to methods such as semi-supervised and adaptive supertagging, which can improve the speed of a parser after training (Kummerfeld, Roesner, et al. 2010; Lewis and Steedman 2014).
",5.1 Implementation,[0],[0]
Tree Accuracy:,5.1 Implementation,[0],[0]
"On the standard tree-metric, we score 88.1.",5.1 Implementation,[0],[0]
"Using the same non-gold POS tags as input, Carreras et al. (2008) score 90.9, probably due to their second-order features and head rules tuned for performance6.",5.1 Implementation,[0],[0]
"Shifting to use their head rules, we score 88.9.",5.1 Implementation,[0],[0]
"Second-order features could be added to our model through the use of forest reranking, an improvement that would be orthogonal to this paper’s contributions.
",5.1 Implementation,[0],[0]
We can also evaluate on spines and edges.,5.1 Implementation,[0],[0]
"Since their system produces regular PTB trees, we con-
5 Using a single core of an Amazon EC2 m4.2xlarge instance (2.4 GHz Xeon CPU and 32 Gb of RAM).
",5.1 Implementation,[0],[0]
"6 Previous work has shown that the choice of head can significantly impact accuracy (Schwartz et al. 2012).
vert its output to our representation and compare its results with our system using their head rules.",5.1 Implementation,[0],[0]
"We see slightly lower accuracy for our system on both spines (94.0 vs. 94.3) and edges (90.4 vs. 91.1).
",5.1 Implementation,[0],[0]
Trace Accuracy: Table 2 shows results using Johnson (2002)’s trace metric.,5.1 Implementation,[0],[0]
"Our parser is competitive with previous work that has highly-engineered models: Johnson’s system has complex non-local features on tree fragments, and similarly Kato and Matsubara (K&M 2016) consider complete items in the stack of their transition-based parser.",5.1 Implementation,[0],[0]
On co-indexation our results fall between Johnson and K&M.,5.1 Implementation,[0],[0]
"Converting to our representation, our parser has higher precision than K&M on trace edges (84.1 vs. 78.1) but lower recall (59.5 vs. 71.3).",5.1 Implementation,[0],[0]
"One modeling challenge we observed is class imbalance: of the many places a trace could be added, only a small number are correct, and so our model tends to be conservative (as shown by the P/R tradeoff).",5.1 Implementation,[0],[0]
We propose a representation and algorithm that cover 97.3% of graph structures in the PTB.,6 Conclusion,[0],[0]
"Our algorithm is O(n4), uniquely decomposes parses, and enforces the property that parses are composed of a core tree with additional traces and null elements.",6 Conclusion,[0],[0]
A proof of concept parser shows that our algorithm can be used to parse and recover traces.,6 Conclusion,[0],[0]
"We thank Greg Durrett for advice on parser implementation and debugging, and the action editor and anonymous reviewers for their helpful feedback.",Acknowledgments,[0],[0]
"This research was partially supported by a General Sir John Monash Fellowship and the Office of Naval
Research under MURI Grant",Acknowledgments,[0],[0]
No. N000140911081.,Acknowledgments,[0],[0]
"General treebank analyses are graph structured, but parsers are typically restricted to tree structures for efficiency and modeling reasons.",abstractText,[0],[0]
"We propose a new representation and algorithm for a class of graph structures that is flexible enough to cover almost all treebank structures, while still admitting efficient learning and inference.",abstractText,[0],[0]
"In particular, we consider directed, acyclic, one-endpoint-crossing graph structures, which cover most long-distance dislocation, shared argumentation, and similar tree-violating linguistic phenomena.",abstractText,[0],[0]
"We describe how to convert phrase structure parses, including traces, to our new representation in a reversible manner.",abstractText,[0],[0]
"Our dynamic program uniquely decomposes structures, is sound and complete, and covers 97.3% of the Penn English Treebank.",abstractText,[0],[0]
We also implement a proofof-concept parser that recovers a range of null elements and trace types.,abstractText,[0],[0]
Parsing with Traces: An O(n) Algorithm and a Structural Representation,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2411–2420 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"During the last decade, social media have become extremely popular, on which billions of usergenerated contents are posted every day.",1 Introduction,[0],[0]
Many users have been writing about their thoughts and lives on the go.,1 Introduction,[0],[0]
"The massive unstructured data from social media provides valuable information for a variety of applications such as stock prediction (Bollen et al., 2011), public health analysis (Wilson and Brownstein, 2009; Paul and Dredze, 2011), real-time event detection (Sakaki et al., 2010), and so on.",1 Introduction,[0.9521068330411473],"['Instead, the research community has focused on task-oriented communication, such as airline or restaurant booking (Bordes and Weston, 2016), or else single-turn information seeking, i.e. question answering (Rajpurkar et al., 2016).']"
"The quality of these applications is highly impacted by the performance of natural language processing tasks.
∗Corresponding author.
",1 Introduction,[0],[0]
Part-of-speech (POS) tagging is one of the most important natural language processing tasks.,1 Introduction,[0],[0]
"It has also been widely used in the social media analysis systems (Ritter et al., 2012; Lamb et al., 2013; Kiritchenko et al., 2014).",1 Introduction,[0],[0]
Most stateof-the-art POS tagging approaches are based on supervised methods.,1 Introduction,[0],[0]
"Hence, they usually require a large amount of annotated data to train models.",1 Introduction,[0],[0]
Many datasets have been constructed for POS tagging task.,1 Introduction,[0],[0]
"Because newswire articles are carefully edited, benchmarks usually use them for annotation (Marcus et al., 1993).",1 Introduction,[0],[0]
"However, usergenerated contents on social media are usually informal and contain many nonstandard lexical items.",1 Introduction,[0],[0]
"Moreover, the difference in domains between training data and evaluation data may heavily impact the performance of approaches based on supervised methods (Caruana and NiculescuMizil, 2006).",1 Introduction,[0],[0]
"Hence, most POS tagging methods cannot achieve the same performance as reported on newswire domain when applied on Twitter (Owoputi et al., 2013).
",1 Introduction,[0],[0]
"To perform the Twitter POS tagging task, some approaches have been proposed to perform the task.",1 Introduction,[0],[0]
"Gimpel et al. (2011) manually annotated 1,827 tweets and carefully studied various fea-
2411
tures.",1 Introduction,[0],[0]
"Ritter et al. (2011) also constructed a labeled dataset, which contained 787 tweets, to empirically evaluate the performance of supervised methods on Twitter.",1 Introduction,[0],[0]
Owoputi et al. (2013) incorporated word clusters into the feature sets and further improved the performance.,1 Introduction,[0],[0]
"From these works, we can observe that the size of the training data was much smaller than the newswire domain’s.
",1 Introduction,[0],[0]
"Besides the challenge of lack of training data, the frequent use of out-of-vocabulary words also makes this problem difficult to address.",1 Introduction,[0],[0]
"Social media users often use informal ways of expressing their ideas and often spell words phonetically (e.g., “2mor” for “tomorrow”).",1 Introduction,[0],[0]
"In addition, they also make extensive use of emoticons and abbreviations (e.g., “:-)” for smiling emotion and “LOL” for laughing out loud).",1 Introduction,[0],[0]
"Moreover, new symbols, abbreviations, and words are constantly being created.",1 Introduction,[0],[0]
"Figure 1 shows an example of tagged Tweet.
",1 Introduction,[0],[0]
"To tackle the challenges posed by the lack of training data and the out-of-vocabulary words, in this paper, we propose a novel recurrent neural network, which we call Target Preserved Adversarial Neural Network (TPANN) to perform the task.",1 Introduction,[0],[0]
"It can make use of a large quantity of annotated data from other resourcerich domains, unlabeled in-domain data, and a small amount of labeled in-domain data.",1 Introduction,[0],[0]
All of these datasets can be easily obtained.,1 Introduction,[0],[0]
"To make use of unlabeled data, motivated by the work of Goodfellow et al. (2014) and Chen et al. (2016), the proposed method extends the bi-directional long short-term memory recurrent neural network (bi-LSTM) with an adversarial predictor.",1 Introduction,[0],[0]
"To overcome the defect that adversarial networks can merely learn the common features, we propose to use an autoencoder only acting on target dataset to preserve its own specific features.",1 Introduction,[0],[0]
"For tackling the out-of-vocabulary problem, the proposed method also incorporates a character level convolutional neutral network to leverage subword information.
",1 Introduction,[0],[0]
"The contributions of this work are as follows:
• We propose to incorporate large scale unlabeled in-domain data, out-of-domain labeled data, and in-domain labeled data for Twitter part-of-speech tagging task.
",1 Introduction,[0],[0]
"• We introduce a novel recurrent neural network, which can learn domain-invariant rep-
resentations through in-domain and out-ofdomain data and construct a cross domain POS tagger through the learned representations.",1 Introduction,[0],[0]
"The proposed method also tries to preserve the specific features of target domain.
",1 Introduction,[0],[0]
• Experimental results demonstrate that the proposed method can lead to better performance in most of cases on three different datasets.,1 Introduction,[0],[0]
"In this work, we propose a novel recurrent neural network, Target Preserved Adversarial Neural Network (TPANN), to learn common features between resource-rich domain and target domain, simultaneously to preserve target domain-specific features.",2 Approach,[0],[0]
It extends the bi-directional LSTM with adversarial network and autoencoder.,2 Approach,[0],[0]
The architecture of TPANN is illustrated in Figure 2.,2 Approach,[0],[0]
"The model consists of four components: Feature Extractor, POS Tagging Classifier, Domain Discriminator and Target Domain Autoencoder.",2 Approach,[0],[0]
"In the following sections, we will detail each part of the proposed architecture and training methods.",2 Approach,[0],[0]
"The feature extractor F adopts CNN to extract character embedding features, which can tackle the out-of-vocabulary word problem effectively.",2.1 Feature Extractor,[0],[0]
"To incorporate word embedding features, we concatenate word embedding to character embedding as the input of bi-LSTM on the next layer.",2.1 Feature Extractor,[0],[0]
"Utilizing a bi-LSTM to model sentences, F can extract sequential relations and context information.
",2.1 Feature Extractor,[0],[0]
"We denote the input sentence as x and the i-th word as xi. xi ∈ S(x) and xi ∈ T (x) represent input samples are from source domain and target domain, respectively.",2.1 Feature Extractor,[0],[0]
We denote the parameters of F as θf .,2.1 Feature Extractor,[0],[0]
"Let V be the vocabulary of words, and C be the vocabulary of characters.",2.1 Feature Extractor,[0],[0]
d is the dimensionality of character embedding then Q ∈ Rd×|C| is the representation matrix of vocabulary.,2.1 Feature Extractor,[0],[0]
We assume that word xi ∈ V is made up of a sequence of characters Ci =,2.1 Feature Extractor,[0],[0]
"[c1, c2, . . .",2.1 Feature Extractor,[0],[0]
", cl], where l is the max length of word and every word will be padded to this length.",2.1 Feature Extractor,[0],[0]
"Then Ci ∈ Rd×l would be the inputs of CNN.
",2.1 Feature Extractor,[0],[0]
"We apply a narrow convolution between Ci and filter H ∈ Rd×k, where k is the width of the filter.
",2.1 Feature Extractor,[0],[0]
After that we add a bias and apply nonlinearity to obtain a feature map mi ∈ Rl−k+1.,2.1 Feature Extractor,[0],[0]
"Specifically, the j-th element of mi is given by:
ik[j] = tanh(〈Ci[∗, j : j + k",2.1 Feature Extractor,[0],[0]
"− 1],H〉+ b), (1)
where Ck[∗, j : j + k",2.1 Feature Extractor,[0],[0]
− 1] is the j-to-(j + k,2.1 Feature Extractor,[0],[0]
"− 1)-th column of Ci and 〈A,B〉 = Tr(ABT ) is the Frobenius inner product.",2.1 Feature Extractor,[0],[0]
"We then apply a max-over-time pooling operation (Collobert et al., 2011) over the feature map.",2.1 Feature Extractor,[0],[0]
CNN uses multiple filters with varying widths to obtain the feature vector ~ci for word xi.,2.1 Feature Extractor,[0],[0]
"Then, the character-level feature vector ~ci is concatenated to the word embedding ~wi to form the input of bi-LSTM on the next layer.",2.1 Feature Extractor,[0],[0]
The word embedding ~w is pretrained on 30 million tweets.,2.1 Feature Extractor,[0],[0]
"Then, the hidden states h of bi-LSTM turn into the features that will be transfered to P , Q andR, i.e. F(x) = h.",2.1 Feature Extractor,[0],[0]
POS tagging classifier P and domain discriminator Q take F(x) as input.,2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
They are standard feed-forward networks with a softmax layer for classification.,2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
"P predicts POS tagging label to get classification capacity, and Q discriminates domain label to make F(x) domain-invariant.
",2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
The POS tagging classifier P maps the feature vector F(xi) to its label.,2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
We denote the parameters of this mapping as θy.,2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
"The POS tagging
classifier is trained on Ns samples from the source domain with the cross entropy loss:
",2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
Ltask =,2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
− Ns∑ i=1,2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
"yi ∗ log ŷi, (2)
where yi is the one-hot vector of POS tagging label corresponding to xi ∈ S(x), ŷi is the output of top softmax layer:",2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
ŷi = P(F(xi)).,2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
"During the training time, The parameters θf and θy are optimized to minimize the classification loss Ltask.",2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
"This ensures that P(F(xi)) can make accurate prediction on the source domain.
",2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
"Conversely, domain discriminator maps the same hidden states h to the domain labels with parameters θd.",2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
"The domain discriminator aims to discriminate the domain label with following loss function: Ltype = − Ns+Nt∑
i=1",2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
{di log d̂i+(1−di),2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
"log(1− d̂i)}, (3)
where di is the ground truth domain label for sample",2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
"i, d̂i is the output of top layer: d̂i = Q(F(xi)).",2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
Nt meansNt samples from the target domain.,2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
"The domain discriminator is trained towards a saddle point of the loss function through minimizing the loss over θd while maximizing the loss over θf (Ganin et al., 2016).",2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
"Optimizing θf ensures that the domain discriminator can’t discriminate
the domain, i.e., the feature extractor finds the common features between the two domains.",2.2 POS Tagging Classifier and Domain Discriminator,[0],[0]
"Through training adversarial networks, we can obtain domain-invariant features hcommon, but it will weaken some domain-specific features which are useful for POS tagging classification.",2.3 Target Domain Autoencoder,[0],[0]
"Merely obtaining domain invariant features would therefore limit the classification ability.
",2.3 Target Domain Autoencoder,[0],[0]
"Our model tries to tackle this defect by introducing domain-specific autoencoder R, which attempts to reconstruct target domain data.",2.3 Target Domain Autoencoder,[0],[0]
"Inspired by (Sutskever et al., 2014) but different from (Dai and Le, 2015), we treat the feature extractor F as encoder.",2.3 Target Domain Autoencoder,[0],[0]
"In addition, we combine the last hidden states of the forward LSTM and backward LSTM in F as the initial state h0(dec) of the decoder LSTM.",2.3 Target Domain Autoencoder,[0],[0]
"Hence, we don’t need to reverse the order of words of the input sentences and the model avoids the difficulty of ”establish communication” between the input and the output (Sutskever et al., 2014).
",2.3 Target Domain Autoencoder,[0],[0]
"Similar to (Zhang et al., 2016), we use h0(dec) and embedding vector of the previous word as the inputs of the decoder, but in a computationally more efficient manner by computing previous word representation.",2.3 Target Domain Autoencoder,[0.9532305423334475],"['The key-value (KV) memory network (Miller et al., 2016) was proposed as an improvement to the memory network by performing attention over keys and outputting the values (instead of the same keys as in the original), which can outperform memory networks dependent on the task and definition of the key-value pairs.']"
"We assume that (x̂1, · · · , x̂T ) is the output sequence.",2.3 Target Domain Autoencoder,[0],[0]
"zt is the t-th word representation: zt = MLP (ht), and MLP is the multiple perceptron function.",2.3 Target Domain Autoencoder,[0],[0]
Hidden state ht = LSTM([h0(dec) :,2.3 Target Domain Autoencoder,[0],[0]
"zt−1], ht−1), where [· : ·] is the concatenation operation.",2.3 Target Domain Autoencoder,[0],[0]
"We estimate the conditional probability p(x̂1, · · · , x̂T |h0(dec))",2.3 Target Domain Autoencoder,[0],[0]
"as follows:
p(x̂1, · · · , x̂T |h0(dec))",2.3 Target Domain Autoencoder,[0],[0]
"= T∏
t=1
p(x̂t|h0(dec), z1, · · · , zt−1), (4)
where each p(x̂t|h0(dec), z1, · · · , zt−1) distribution is computed with softmax over all the words in the vacabulary.
",2.3 Target Domain Autoencoder,[0],[0]
"Our aim is to minimize the following loss function with respect to parameters θr:
Ltarget = − Nt∑ i=1",2.3 Target Domain Autoencoder,[0],[0]
"xi ∗ log x̂i, (5)
where xi is the one-hot vector of i-th word.",2.3 Target Domain Autoencoder,[0],[0]
"This makes h0(dec) learn an undercomplete and most salient sentence representation of target domain
data.",2.3 Target Domain Autoencoder,[0],[0]
"When the adversarial networks try to optimize the hidden representation to common representation hcommon, The target domain autoencoder counteracts a tendency of the adversarial network to erase target domain features by optimizing the common representation to be informative on the target-domain data.",2.3 Target Domain Autoencoder,[0],[0]
"Our model can be trained end-to-end with standard back-propagation, which we will detail in this section.
",2.4 Training,[0],[0]
"Our ultimate training goal is to minimize the total loss function with parameters {θf , θy, θr, θd} as follows:
Ltotal = αLtask + βLtarget + γLtype, (6) where α, β, γ are the weights to balance the effects of P ,R and Q.
For obtaining domain-invariant representation hcommon, inspired by (Ganin and Lempitsky, 2015), we introduce a special gradient reversal layer (GRL), which does nothing during forward propagation, but negates the gradients if it receives backward propagation, i.e. g(F(x))",2.4 Training,[0],[0]
= F(x) but ∇g(F(x)),2.4 Training,[0],[0]
= −λ∇F(x).,2.4 Training,[0],[0]
"We insert the GRL between F and Q, which can run standard Stochastic Gradient Descent with respect to θf and θd.",2.4 Training,[0],[0]
The parameter −λ drives the parameters θf not to amplify the dissimilarity of features when minimize Ltpye.,2.4 Training,[0],[0]
"So by introducing a GRL, F can drive its parameters θf to extract hidden representations that help the POS tagging classification and hamper the domain discrimination.
",2.4 Training,[0],[0]
"In order to preserve target domain-specific features, we only optimize the autoencoder on target domain data for reconstruction tasks.
",2.4 Training,[0],[0]
"Through above procedures, the model can learn the common features between domains, simultaneously preserve target domain-specific features.",2.4 Training,[0],[0]
"Finally, we can update the parameters as follows:
θf = θf − µ(α∂L i task
∂θf + β",2.4 Training,[0],[0]
"∂Litarget ∂θf − γ · λ∂L i type ∂θf )
θy = θy − µ · α∂L i task
∂θy
θr = θr − µ · β ∂Litarget ∂θr θd = θd − µ · γ ∂Litype ∂θd ,
(7)
where µ is the learning rate.",2.4 Training,[0],[0]
"Because the size of the WSJ is more than 100 times that of the labeled Twitter dataset, if we directly train the model with the combined dataset, the final results are much worse than those using two training steps.",2.4 Training,[0],[0]
"So, we adopt adversarial training on WSJ and unlabeled Twitter dataset at the first step, then use a small number of in-domain labeled data to fine-tune the parameters with a low learning rate.",2.4 Training,[0],[0]
"In this section, we will detail the datasets used for experiments and experimental setup.",3 Experiments,[0],[0]
"The methods proposed in this work incorporate out-of-domain labeled data from resource-rich domains, large scale unlabeled in-domain data, and a small number of labeled in-domain data.",3.1 Datasets,[0],[0]
The datasets used in this work are as follows: Labeled out-of-domain data.,3.1 Datasets,[0],[0]
"We use a standard benchmark dataset for adversarial POS tagging, namely the Wall Street Journal (WSJ) data from the Penn TreeBank v3 (Marcus et al., 1993), sections 0-24 for the out-of-domain data.",3.1 Datasets,[0],[0]
Labeled in-domain data.,3.1 Datasets,[0],[0]
"For training and evaluating POS tagging approaches, we compare the proposed method with other approaches on three benchmarks: RIT-Twitter (Ritter et al., 2011), NPSCHAT (Forsyth, 2007), and ARKTwitter (Gimpel et al., 2011).",3.1 Datasets,[0],[0]
Unlabeled in-domain data.,3.1 Datasets,[0],[0]
"For training the adversarial network, we need to use a dataset that has large scale unlabeled tweets.",3.1 Datasets,[0],[0]
"Hence, in this work, we construct large scale unlabeled data (UNL), from Twitter using its API.
",3.1 Datasets,[0],[0]
The detailed data statistics of the datasets used in this work are listed in Table 1.,3.1 Datasets,[0],[0]
"We select both state-of-the-art and classic methods for comparison, as follows:
• Stanford POS",3.2 Experimental Setup,[0],[0]
Tagger:,3.2 Experimental Setup,[0],[0]
"Stanford POS Tagger is a widely used tool for newswire domains (Toutanova et al., 2003).",3.2 Experimental Setup,[0],[0]
"In this work, we train it using two different sets, the WSJ (sections 0-18) and a WSJ, IRC, and Twitter mixed corpus.",3.2 Experimental Setup,[0],[0]
"We use StanfordWSJ and Stanford-MIX to represent them, respectively.
",3.2 Experimental Setup,[0],[0]
"• T-POS: T-Pos (Ritter et al., 2011) adopts the Conditional Random Fields and clustering algorithm to perform the task.",3.2 Experimental Setup,[0],[0]
"It was trained from a mixture of hand-annotated tweets and existing POS-labeled data.
",3.2 Experimental Setup,[0],[0]
• GATE,3.2 Experimental Setup,[0],[0]
"Tagger: GATE tagger (Derczynski et al., 2013) is based on vote-constrained bootstrapping with unlabeled data.",3.2 Experimental Setup,[0],[0]
"It combines cases where available taggers use different tagsets.
",3.2 Experimental Setup,[0],[0]
"• ARK Tagger: ARK tagger (Owoputi et al., 2013) is a system that reports the best accuracy on the RIT dataset.",3.2 Experimental Setup,[0],[0]
"It uses unsupervised word clustering and a variety of lexical features.
",3.2 Experimental Setup,[0],[0]
• bi-LSTM:,3.2 Experimental Setup,[0],[0]
"Bidirectional Long Short-Term Memory (LSTM) networks have been widely used in a variety of sequence labeling tasks (Graves and Schmidhuber, 2005).",3.2 Experimental Setup,[0],[0]
"In this work, we evaluate it at character level, word level, and combining them together.",3.2 Experimental Setup,[0],[0]
bi-LSTM (word level) uses one layer of bi-LSTM to extract word-level features and adopts a random initialization method to transform words to vectors.,3.2 Experimental Setup,[0],[0]
"bi-LSTM (character level) represents a method that combines bi-LSTM and CNN-based character embedding, a similar approach with character-aware neural network described in (Kim et al., 2015) to handle the out-ofvocabulary words.",3.2 Experimental Setup,[0],[0]
"bi-LSTM (word level pretrain) architecture is the same as that of bi-LSTM(word level) but adopts word2vec tool (Mikolov et al., 2013) to vectorize.",3.2 Experimental Setup,[0],[0]
"bi-LSTM (combine) concatenates word to character features.
",3.2 Experimental Setup,[0],[0]
The hyper-parameters used for our model are as follows.,3.2 Experimental Setup,[0],[0]
AdaGrad optimizer trained with crossentropy loss is used with 0.1 as the default learning rate.,3.2 Experimental Setup,[0],[0]
The dimensionality of word embedding is set to 200.,3.2 Experimental Setup,[0],[0]
The dimensionality for random initialized character embedding is set to 25.,3.2 Experimental Setup,[0],[0]
We adopt a bi-LSTM for encoding with each layer consisting of 250 hidden neurons.,3.2 Experimental Setup,[0],[0]
We set three layers of standard LSTM for decoding.,3.2 Experimental Setup,[0],[0]
Each LSTM layer consists of 500 hidden neurons.,3.2 Experimental Setup,[0],[0]
Adam optimizer trained with cross-entropy loss is used to fine-tune with 0.0001 as the default learning rate.,3.2 Experimental Setup,[0],[0]
Finetuning is run for 100 epochs using early stop.,3.2 Experimental Setup,[0],[0]
"In this section, we will report experimental results and a detailed analysis of the results for the three different datasets.",4 Results and Discussion,[0],[0]
"The RIT-Twitter is split into training, development and evaluation sets (RIT-Train, RIT-Dev, RITTest).",4.1 Evaluation on RIT-Twitter,[0],[0]
"The splitting method is shown in (Derczynski et al., 2013), and the dataset statistics are listed in Table 1.",4.1 Evaluation on RIT-Twitter,[0],[0]
Table 2 shows the results of our method and other approaches on the RIT-Twitter dataset.,4.1 Evaluation on RIT-Twitter,[0],[0]
"RIT-Twitter uses the PTB tagset with several Twitter-specific tags: retweets, @usernames, hashtags, and urls.",4.1 Evaluation on RIT-Twitter,[0],[0]
"Since words in these
categories can be tagged almost perfectly using simple regular expressions, similar to (Owoputi et al., 2013), we use regular expressions to tags these words appropriately for all systems.
",4.1 Evaluation on RIT-Twitter,[0],[0]
"From the results of the Stanford-WSJ, we can observe that the newswire domain is different from Twitter.",4.1 Evaluation on RIT-Twitter,[0],[0]
"Although the token-level accuracy of the Stanford POS Tagger is higher than 97.0% on the PTB dataset, its performance on Twitter drops sharply to 73.37%.",4.1 Evaluation on RIT-Twitter,[0],[0]
"By incorporating some indomain labeled data for training, the accuracy of Stanford POS Tagger can reach up to 83.14%.",4.1 Evaluation on RIT-Twitter,[0],[0]
"Taking a variety of linguistic features and many other resources into consideration, the T-POS, GATE tagger, and ARK tagger can achieve better performance.
",4.1 Evaluation on RIT-Twitter,[0],[0]
"The second part of Table 2 shows the results of the bi-LSTM based methods, which are trained on the RIT-Train dataset.",4.1 Evaluation on RIT-Twitter,[0],[0]
"According to the results of word level, we can see that word2vec can provide valuable information.",4.1 Evaluation on RIT-Twitter,[0],[0]
"The pre-trained word vectors in bi-LSTM(word level pretrain) give almost 10% higher accuracy than bi-LSTM(word level).
",4.1 Evaluation on RIT-Twitter,[0],[0]
"Comparing the character-level bi-LSTM with word-level bi-LSTM with random initialization, we can observe that the character-level method can achieve better performance than the word-level method.",4.1 Evaluation on RIT-Twitter,[0],[0]
"bi-LSTM(combine) combines word with character features, as described in Section 2.1,
which achieves the best results at 89.48% in the bi-LSTM based baseline systems and shows that the morphological features and pre-trained word vectors are both useful for POS tagging.
",4.1 Evaluation on RIT-Twitter,[0],[0]
"The third part of Table 2 shows the results of our methods incorporating out-of-domain labeled data, in-domain unlabeled data, and in-domain labeled data.",4.1 Evaluation on RIT-Twitter,[0],[0]
"Putting everything together, our model can achieve 90.92% on this dataset.",4.1 Evaluation on RIT-Twitter,[0],[0]
"Compared with the architecture without an adversarial model, our method is almost 1% better.",4.1 Evaluation on RIT-Twitter,[0],[0]
It demonstrates that adversarial networks can significantly help with tasks of this nature.,4.1 Evaluation on RIT-Twitter,[0],[0]
"Through introducing the autoencoder in target domain, we can preserve domain-specific features for better performance.",4.1 Evaluation on RIT-Twitter,[0],[0]
"Compared with the ARK tagger, which achieves the previous best result on this dataset, our model is also 0.52% better, the error reduction rate is more than 5.5%.
",4.1 Evaluation on RIT-Twitter,[0],[0]
"To better understand why adversarial networks can help transfer domains from newswire to Twitter, in this work we also followed the method Ganin and Lempitsky (2015) used to visualize the outputs of LSTM with tSNE (Van Der Maaten, 2013).",4.1 Evaluation on RIT-Twitter,[0],[0]
Figure 3 shows the visualization results.,4.1 Evaluation on RIT-Twitter,[0],[0]
"From the figure, we can see that the adversary in our method makes the two distributions of features much more similar, which means that the outputs of bi-LSTM are domain-invariant.",4.1 Evaluation on RIT-Twitter,[0],[0]
"Hence, the PTB training data can provide much more help than directly combining PTB and RIT-Train together.",4.1 Evaluation on RIT-Twitter,[0],[0]
"IRC, which contains Internet relay room messages from 2006, is a medium of online conversational text.",4.2 Evaluation on NPSChat,[0],[0]
Its content is very similar to tweets.,4.2 Evaluation on NPSChat,[0],[0]
"We evaluate the proposed method on the NPSChat corpus (Forsyth, 2007), a PTB-part-of-speech annotated dataset of IRC.
",4.2 Evaluation on NPSChat,[0],[0]
"We compared our method with a tagger in the same setup as experiments with (Forsyth, 2007).",4.2 Evaluation on NPSChat,[0],[0]
The training part contains 90% of the data.,4.2 Evaluation on NPSChat,[0],[0]
The testing part contains the other 10%.,4.2 Evaluation on NPSChat,[0],[0]
Table 3 shows the results of the ARK Tagger and our method.,4.2 Evaluation on NPSChat,[0],[0]
"We used PTB, unlabeled Twitter, and the training part of NPSChat to train our model.",4.2 Evaluation on NPSChat,[0],[0]
"From the results, we can see that our model achieved 94.1% accuracy.",4.2 Evaluation on NPSChat,[0],[0]
"This is significantly better than the result Forsyth (2007) reported, which was 90.8%.",4.2 Evaluation on NPSChat,[0],[0]
"They trained their tagger with a mix of several POS-annotated corpora (12K from Twitter, 40K from IRC, and 50K from PTB).",4.2 Evaluation on NPSChat,[0],[0]
"Our method also outperforms state-of-the-art results 93.4%±0.3%, which was achieved by the ARK Tagger with various external corpus and features, e.g., Brown clustering, PTB, Freebase lists of celebrities, and video games.",4.2 Evaluation on NPSChat,[0],[0]
"ARK-Twitter data contains an entire dataset consisting of a number of tweets sampled from one particular day (October 27, 2010) described in (Gimpel et al., 2011).",4.3 Evaluation on ARK-Twitter,[0],[0]
This part is used for training.,4.3 Evaluation on ARK-Twitter,[0],[0]
"They also created another dataset, which consists of 547 tweets, for evaluation (DAILY547).",4.3 Evaluation on ARK-Twitter,[0],[0]
"This dataset consists of one random English tweet from every day between January 1, 2011 and June 30, 2012.",4.3 Evaluation on ARK-Twitter,[0],[0]
"The distribution of training data may be slightly different from the testing data, for example a substantial fraction of the messages in the training data are about a basketball game.",4.3 Evaluation on ARK-Twitter,[0],[0]
"Since ARK-Twitter uses a different tagset with PTB, we manually construct a table to link tags for the two datasets.
",4.3 Evaluation on ARK-Twitter,[0],[0]
Table 4 shows the results of different methods on this dataset.,4.3 Evaluation on ARK-Twitter,[0],[0]
"From the results, we can see that our method can achieve a better result than (Gimpel et al., 2011).",4.3 Evaluation on ARK-Twitter,[0],[0]
"However, the performance of our method is worse than the ARK Tagger.",4.3 Evaluation on ARK-Twitter,[0],[0]
"Through analyzing the errors, we find that 16.7% errors occurr between nouns and proper nouns.",4.3 Evaluation on ARK-Twitter,[0],[0]
"Since our method do not include any ontology or knowledge, proper nouns can not be easily detected.",4.3 Evaluation on ARK-Twitter,[0],[0]
"However, the ATK Tagge add a tokenlevel name list feature.",4.3 Evaluation on ARK-Twitter,[0],[0]
"The name list is useful for proper nouns recognition, which fires on names from many sources, such as Freebase lists of celebrities, the Moby Words list of US Locations, proper names from Mark Kantrowitz’s name corpus and so on.",4.3 Evaluation on ARK-Twitter,[0],[0]
"So, our model is also competitive when lacking of manual feature knowledge.",4.3 Evaluation on ARK-Twitter,[0],[0]
Part-of-Speech tagging is an important preprocessing step and can provide valuable information for various natural language processing tasks.,5 Related Work,[0],[0]
"In recent years, deep learning algorithms have been successfully used for POS tagging.",5 Related Work,[0],[0]
A number of approaches have been proposed and have achieved some progress.,5 Related Work,[0],[0]
"Santos and Guimaraes (2015) proposed
using a character-based convolutional neural network to perform the POS tagging problem.",5 Related Work,[0],[0]
"Bi-LSTMs with word, character or unicode byte embedding were also introduced to achieve the POS tagging and named entity recognition tasks (Plank et al., 2016; Chiu and Nichols, 2015; Ma and Hovy, 2016).",5 Related Work,[0],[0]
"In this work, we study the problem from a domain adaption perspective.",5 Related Work,[0],[0]
"Inspired by these works, we also propose to use character-level methods to handle out-ofvocabulary words and bi-LSTMs to model the sequence relations.
",5 Related Work,[0],[0]
"Adversarial networks were successfully used for image generation (Goodfellow et al., 2014; Dosovitskiy et al., 2015; Denton et al., 2015), domain adaption (Tzeng et al., 2014; Ganin et al., 2016), and semi-supervised learning (Denton et al., 2016).",5 Related Work,[0],[0]
"The key idea of adversarial networks for domain adaption is to construct invariant features by optimizing the feature extractor as an adversary against the domain classifier (Zhang et al., 2017).
",5 Related Work,[0],[0]
Sequence autoencoder reads the input sequence into a vector and then tries to reconstruct it.,5 Related Work,[0],[0]
Dai and Le (2015) used the model on a number of different tasks and verified its validity.,5 Related Work,[0],[0]
"Li et al. (2015) introduced the model to hierarchically build an embedding for a paragraph, showing that the model was able to encode texts to preserve syntactic, semantic, and discourse coherence.
",5 Related Work,[0],[0]
"In this work, we incorporate adversarial networks with autoencoder to obtain domaininvariant features and keep domain-specific features.",5 Related Work,[0],[0]
Our model is more suitable for target domain tasks.,5 Related Work,[0],[0]
"In this work, we propose a novel adversarial neural network to address the POS tagging problem.",6 Conclusion,[0],[0]
"Besides learning common representations between source domain and target domain, it can simultaneously preserve specific features of target domain.",6 Conclusion,[0],[0]
"The proposed method leverages newswire resources and large scale in-domain unlabeled data to help POS tagging classification on Twitter, which has a few of labeled data.",6 Conclusion,[0],[0]
We evaluate the proposed method and several state-ofthe-art methods on three different corpora.,6 Conclusion,[0],[0]
"In most of the cases, the proposed method can achieve better performance than previous methods.",6 Conclusion,[0],[0]
"Experimental results demonstrate that the proposed
method can make full use of these resources, which can be easily obtained.",6 Conclusion,[0],[0]
The authors wish to thank the anonymous reviewers for their helpful comments.,Acknowledgments,[0],[0]
"This work was partially funded by National Natural Science Foundation of China (No. 61532011, 61473092, and 61472088) and STCSM (No.16JC1420401).",Acknowledgments,[0],[0]
"In this work, we study the problem of partof-speech tagging for Tweets.",abstractText,[0],[0]
"In contrast to newswire articles, Tweets are usually informal and contain numerous out-ofvocabulary words.",abstractText,[0],[0]
"Moreover, there is a lack of large scale labeled datasets for this domain.",abstractText,[0],[0]
"To tackle these challenges, we propose a novel neural network to make use of out-of-domain labeled data, unlabeled in-domain data, and labeled indomain data.",abstractText,[0],[0]
"Inspired by adversarial neural networks, the proposed method tries to learn common features through adversarial discriminator.",abstractText,[0],[0]
"In addition, we hypothesize that domain-specific features of target domain should be preserved in some degree.",abstractText,[0],[0]
"Hence, the proposed method adopts a sequence-to-sequence autoencoder to perform this task.",abstractText,[0],[0]
Experimental results on three different datasets show that our method achieves better performance than state-of-the-art methods.,abstractText,[0],[0]
Part-of-Speech Tagging for Twitter with Adversarial Neural Networks,title,[0],[0]
"This paper is about weighted correlation clustering (Bansal et al., 2004), a combinatorial optimization problem whose feasible solutions are all clusterings of a graph, and whose objective function is a sum of weights w0, w1 : E → R+0 defined on the edgesE of the graph.",1. Introduction,[0],[0]
"The weightw0e is added to the sum if the nodes {u, v} = e ∈",1. Introduction,[0],[0]
"E are in the same cluster, and the weight w1e is added to the sum if these nodes are in distinct clusters.",1. Introduction,[0],[0]
"The problem consists in finding a clustering of minimum weight.
",1. Introduction,[0],[0]
"Weighted correlation clustering has found applications in the fields of network analysis (Cesa-Bianchi et al., 2012) and, more recently, computer vision (Kappes et al., 2011; Keuper et al., 2015; Insafutdinov et al., 2016; Beier et al., 2017; Tang et al., 2017), partly due to its key property that the number of clusters is not fixed, constrained or penalized in the problem statement but is instead defined by the (any)
1Max Planck Institute for Informatics, Saarbrücken, Germany 2Saarland University, Saarbrücken, Germany 3Bosch Center for AI, Renningen, Germany 4University of Tübingen, Germany.",1. Introduction,[0],[0]
"Correspondence to: Jan-Hendrik Lange <jlange@mpi-inf.mpg.de>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
solution.",1. Introduction,[0],[0]
"Weighted correlation clustering in general graphs is hard to solve exactly and hard to approximate (Demaine et al., 2006).",1. Introduction,[0],[0]
Remarkable progress has been made toward algorithms that find feasible solutions by approximations or heuristics (cf. Section 2).,1. Introduction,[0],[0]
"Yet, the computation of lower bounds remains challenging for large instances (Swoboda & Andres, 2017).
",1. Introduction,[0],[0]
"We make the following contributions: Firstly, in order to reduce instances in size, we establish partial optimality conditions on the graph and weights that can be checked combinatorially in polynomial time and determine the values of some variables in an optimal solution.",1. Introduction,[0],[0]
"By applying these conditions recursively, we reduce an instance in size without restricting the quality of solutions.",1. Introduction,[0],[0]
"For series-parallel graphs, our algorithm solves weighted correlation clustering exactly and in linear time, as we show.",1. Introduction,[0],[0]
"For general graphs, we demonstrate its effectiveness empirically.
",1. Introduction,[0],[0]
"Secondly, in order to compute lower bounds to the optimal objective value efficiently, we define an algorithm that outputs a heuristic solution to a packing problem that is the dual of a reformulation of weighted correlation clustering.",1. Introduction,[0],[0]
"Empirically, this algorithm is shown to exhibit a run-time/tightness trade-off that is different from both the cutting plane algorithm of Kappes et al. (2015) and the message passing algorithm of Swoboda & Andres (2017), both of which solve a canonical linear program relaxation of weighted correlation clustering.
",1. Introduction,[0],[0]
"Thirdly, toward the goal of obtaining primal feasible solutions, we define a transformation of the weights w.r.t.",1. Introduction,[0],[0]
our heuristic solution to the dual problem.,1. Introduction,[0],[0]
This transformation is again a heuristic and is motivated by complementary slackness.,1. Introduction,[0],[0]
"Empirically, local search algorithms are shown to find feasible solutions of lower original weight when applied to instances with transformed weights.
",1. Introduction,[0],[0]
"In the supplementary material, we provide additional results that were omitted from the main paper for the sake of space.",1. Introduction,[0],[0]
Implementations of our algorithms are provided on GitHub.,1. Introduction,[0],[0]
Weighted correlation clustering has a long history in the field of combinatorial optimization.,2. Related Work,[0],[0]
"Grötschel & Wakabayashi (1989) state an equivalent problem for complete graphs and
devise a branch-and-cut algorithm for solving this problem exactly.",2. Related Work,[0],[0]
"The polyhedral geometry of its feasible set is studied by Grötschel & Wakabayashi (1990); Deza et al. (1990; 1992), in the case of general graphs by Chopra & Rao (1993); Chopra (1994) and, for a more general problem, by Horňáková et al. (2017).",2. Related Work,[0],[0]
"For uniform absolute edge costs, Bansal et al. (2004) coined the name correlation clustering, established NP-hardness and the first approximation results.",2. Related Work,[0],[0]
The connection between correlation clustering in general weighted graphs and weighted multicuts was made by Demaine et al. (2006) who thus established APX-hardness and obtained anO(log|V |) approximation algorithm for the problem.,2. Related Work,[0],[0]
"Further hardness results and improved approximation algorithms for particular classes of graphs are due to Charikar et al. (2005); Chawla et al. (2006; 2015); Ailon et al. (2012); Klein et al. (2015).
",2. Related Work,[0],[0]
Approximation algorithms are typically based on rounding the solution of a linear or semi-definite program relaxation.,2. Related Work,[0],[0]
"Due to its importance, tailored algorithms for solving the linear program relaxation more efficiently than standard methods have been proposed by Yarkony et al. (2012; 2015); Swoboda & Andres (2017).",2. Related Work,[0],[0]
"Complementary to these lower bounds, a variety of fast primal heuristics have been developed to tackle large instances (Beier et al., 2014; Pan et al., 2015; Levinkov et al., 2017).",2. Related Work,[0],[0]
"Although it has been observed that, in practice, heuristic solutions are often good, it remains difficult for large instances to determine non-trivial bounds on their optimality gap.
",2. Related Work,[0],[0]
"Partial optimality concepts have been developed and exploited successfully for node labeling problems that arise from pseudo-Boolean optimization and from maximum a-posteriori inference in Markov Random Fields, cf.",2. Related Work,[0],[0]
"(Shekhovtsov, 2014; Swoboda et al., 2016).",2. Related Work,[0],[0]
"Transferring this knowledge to weighted correlation clustering is nontrivial, due to the different nature of the problem.",2. Related Work,[0],[0]
Two partial optimality conditions for weighted correlation clustering are established by Alush & Goldberger (2012) and are here generalized.,2. Related Work,[0],[0]
"Weighted correlation clustering is a combinatorial optimization problem whose feasible solutions are all clusterings of a graph.
",3.1. Weighted Correlation Clustering,[0],[0]
"Let G = (V,E) be a simple graph.",3.1. Weighted Correlation Clustering,[0],[0]
"We call a partition Π of V a clustering if every S ∈ Π induces a connected subgraph (cluster) of G. For any clustering Π of G, we denote by E0Π the set of those edges whose nodes are in the same cluster, and by E1Π the (complementary) set of those edges whose
nodes are in distinct clusters:
E0Π = {uv ∈ E",3.1. Weighted Correlation Clustering,[0],[0]
"| ∃S ∈ Π : u ∈ S and v ∈ S}, (1) E1Π = E",3.1. Weighted Correlation Clustering,[0],[0]
\,3.1. Weighted Correlation Clustering,[0],[0]
"E0Π. (2)
",3.1. Weighted Correlation Clustering,[0],[0]
"The set of edges E1Π is known as the multicut of G that corresponds to the clustering Π.
Definition 1.",3.1. Weighted Correlation Clustering,[0],[0]
"For any graph G = (V,E) and any w0, w1 : E → R+0 , the instance of weighted correlation clustering w.r.t.",3.1. Weighted Correlation Clustering,[0],[0]
"G, w0 and w1 is the optimization problem
min Π ∑ e∈E0Π w0e + ∑ e∈E1Π w1e .",3.1. Weighted Correlation Clustering,[0],[0]
(3),3.1. Weighted Correlation Clustering,[0],[0]
Weighted correlation clustering is commonly stated in the form of a binary program whose feasible solutions are the incidence vectors of the multicuts of the graph.,3.2. Minimum Cost Multicut,[0],[0]
The incidence vector xΠ ∈,3.2. Minimum Cost Multicut,[0],[0]
"{0, 1}E corresponding to the multicut induced by Π is defined as
xΠe = { 1 if e ∈",3.2. Minimum Cost Multicut,[0],[0]
"E1Π 0 else.
",3.2. Minimum Cost Multicut,[0],[0]
"(4)
Definition 2.",3.2. Minimum Cost Multicut,[0],[0]
"For any graph G = (V,E) and any c : E → R, the instance of the minimum cost multicut problem w.r.t.",3.2. Minimum Cost Multicut,[0],[0]
"G and c is the binary program
min Π ∑ e∈E ce x Π e .",3.2. Minimum Cost Multicut,[0],[0]
"(5)
The minimizers of an instance of weighted correlation clustering (Def. 1) coincide with the minimizers of the instance of minimum cost multicut (Def. 2) with c = w1−w0, since
min Π ∑ e∈E0Π w0e + ∑ e∈E1Π w1e (6)
",3.2. Minimum Cost Multicut,[0],[0]
= min Π ∑ e∈E ( w0e (1− xΠe ) +,3.2. Minimum Cost Multicut,[0],[0]
"w1e xΠe ) (7)
= ∑ e∈E
w0e︸ ︷︷ ︸ const.",3.2. Minimum Cost Multicut,[0],[0]
"+ min Π
∑ e∈E (w1e − w0e)︸ ︷︷ ︸ ce xΠe .",3.2. Minimum Cost Multicut,[0],[0]
(8),3.2. Minimum Cost Multicut,[0],[0]
"By taking the convex hull of multicut incidence vectors
MC(G) := conv{xΠ | Π clustering of G}, (9)
the minimum cost multicut problem (Def. 2) can be written as the integer linear programming problem
min x∈MC(G) ∑ e∈E ce xe.",3.3. Linear Program Relaxation,[0],[0]
"(PMC)
The set MC(G) is called multicut polytope of G (Chopra & Rao, 1993).",3.3. Linear Program Relaxation,[0],[0]
"As the minimum cost multicut problem is NP-hard, a full description of the multicut polytope in terms of its facets is impractical.",3.3. Linear Program Relaxation,[0],[0]
"For practical purposes a linear programming (LP) relaxation of PMC is derived as follows.
",3.3. Linear Program Relaxation,[0],[0]
"Denote by C(G) the set of all simple cycles of G. For any cycle C ∈ C(G), we write EC for the edge set of C. It is straight-forward to check the fact that any multicut incidence vector xΠ satisfies the system of linear inequalities
∀C ∈ C(G) ∀f",3.3. Linear Program Relaxation,[0],[0]
"∈ EC : xf ≤ ∑
e∈EC\{f}
xe , (10)
the so-called cycle inequalities (Chopra & Rao, 1993).",3.3. Linear Program Relaxation,[0],[0]
"Therefore, the standard linear programming relaxation is given by the program
min x∈CYC(G) ∑ e∈E ce xe (PCYC)
",3.3. Linear Program Relaxation,[0],[0]
"whose feasible set
CYC(G) :",3.3. Linear Program Relaxation,[0],[0]
= { x ∈,3.3. Linear Program Relaxation,[0],[0]
"[0, 1]E ∣∣x satisfies (10)} (11) is also known as the cycle relaxation of MC(G).",3.3. Linear Program Relaxation,[0],[0]
"The problem PCYC is practical, because the cycle inequalities in (10) can be separated in polynomial time.",3.3. Linear Program Relaxation,[0],[0]
"The lower bounds thus obtained can serve to solve (small) instances of the minimum cost multicut problem by branch-and-cut because the cycle relaxation has no integer vertices except the incidence vectors of multicuts, according to Lemma 1.
",3.3. Linear Program Relaxation,[0],[0]
Lemma 1 (Chopra & Rao (1993)).,3.3. Linear Program Relaxation,[0],[0]
"For any graph G = (V,E), it holds that MC(G) = CYC(G) ∩ ZE .
",3.3. Linear Program Relaxation,[0],[0]
A reference algorithm that we use for the experiments in Section 7 further exploits the fact that a cycle inequality in (10) defines a facet of MC(G) iff the associated cycle is chordless.,3.3. Linear Program Relaxation,[0],[0]
"For the presentation of this paper, we employ an alternative (integer) linear programming formulation in terms of covering cycles, which was similarly considered, e.g., by Demaine et al. (2006) for the combinatorial problem and by Charikar et al. (2005) in connection with the LP relaxation for complete graphs.",3.4. Cycle Covering Formulation,[0],[0]
"We rewrite the feasible set of the general LP relaxation relative to the cost vector c. Therefore, let G and c be fixed.
",3.4. Cycle Covering Formulation,[0],[0]
We call an edge e ∈ E repulsive if ce < 0,3.4. Cycle Covering Formulation,[0],[0]
and we call it attractive if ce > 0.,3.4. Cycle Covering Formulation,[0],[0]
"Note that we may w.l.o.g. remove all edges e ∈ E with ce = 0, since the choice of xe is irrelevant to the objective.",3.4. Cycle Covering Formulation,[0],[0]
"We write E = E+ ∪ E− where E+, E− collect all attractive and repulsive edges, respectively.
",3.4. Cycle Covering Formulation,[0],[0]
We call a cycle of G conflicted w.r.t.,3.4. Cycle Covering Formulation,[0],[0]
"(G, c) if it contains precisely one repulsive edge.",3.4. Cycle Covering Formulation,[0],[0]
"We denote by C−(G, c) ⊆ C(G) the set of all such cycles.
",3.4. Cycle Covering Formulation,[0],[0]
We consider the relaxation of CYC(G) that is constrained only by conflicted cycles.,3.4. Cycle Covering Formulation,[0],[0]
"More specifically, we consider the system
∀C ∈ C−(G, c), f ∈ EC ∩",3.4. Cycle Covering Formulation,[0],[0]
E− :,3.4. Cycle Covering Formulation,[0],[0]
"xf ≤ ∑
e∈EC\{f}
xe
(12)
of only those linear inequalities of (10) for which the edge on the left-hand side is repulsive and all other edges are attractive.",3.4. Cycle Covering Formulation,[0],[0]
"Defining
CYC−(G, c) := { x ∈",3.4. Cycle Covering Formulation,[0],[0]
"[0, 1]E ∣∣ x satisfies (12)} (13) and replacing CYC(G) by CYC−(G, c) in PCYC has no effect on the solutions, due to the following lemma, a weaker form of which was also given by Yarkony et al. (2015).",3.4. Cycle Covering Formulation,[0],[0]
Lemma 2.,3.4. Cycle Covering Formulation,[0],[0]
"For any c : E → R it holds that
min x∈CYC(G)",3.4. Cycle Covering Formulation,[0],[0]
"c>x = min x∈CYC−(G,c) c",3.4. Cycle Covering Formulation,[0],[0]
">x (14)
and
min x∈MC(G)",3.4. Cycle Covering Formulation,[0],[0]
"c>x = min x∈CYC−(G,c)∩ZE c>x. (15)
",3.4. Cycle Covering Formulation,[0],[0]
Proof.,3.4. Cycle Covering Formulation,[0],[0]
Let x∗ be an optimal solution to the right-hand side of (14).,3.4. Cycle Covering Formulation,[0],[0]
We show that x∗ satisfies all cycle inequalities (10) by contradiction.,3.4. Cycle Covering Formulation,[0],[0]
"To this end, suppose there exists a cycle C ∈ C(G) and f ∈ EC such that
x∗f > ∑
e∈EC\{f}
x∗e.
",3.4. Cycle Covering Formulation,[0],[0]
"If any edge g ∈ EC \ {f} is repulsive, then increasing x∗g would lower the objective.",3.4. Cycle Covering Formulation,[0],[0]
"Since x
∗ is optimal, there must be a conflicted cycle C ′",3.4. Cycle Covering Formulation,[0],[0]
"with g ∈ EC′ such that x∗g = ∑ e∈EC′\{g}
x∗e .",3.4. Cycle Covering Formulation,[0],[0]
Note that this means f /∈ EC′ .,3.4. Cycle Covering Formulation,[0],[0]
We write C4C ′ for the cycle obtained from the symmetric difference of EC and EC′ .,3.4. Cycle Covering Formulation,[0],[0]
"Apparently, the cycle C4C ′ has one repulsive edge less and f ∈ EC4C′ .",3.4. Cycle Covering Formulation,[0],[0]
"Therefore, by repeating the argument, we may w.l.o.g. assume that all edges in EC \ {f} are attractive.",3.4. Cycle Covering Formulation,[0],[0]
"Now assume that f is attractive as well, then decreasing x∗f would lower the objective.",3.4. Cycle Covering Formulation,[0],[0]
"Therefore, since x
∗ is optimal, there is a conflicted cycle C ′ with f ∈ EC′ and g ∈ EC′ ∩ E− such that
x∗g = x ∗",3.4. Cycle Covering Formulation,[0],[0]
"f + ∑ e∈EC′\{f,g} x∗e
> ∑
e∈EC\{f}
x∗e + ∑
e∈EC′\{f,g}
x∗e
≥ ∑
e∈EC4C′\{g}
x∗e.
",3.4. Cycle Covering Formulation,[0],[0]
Note that C4C ′ is a conflicted cycle.,3.4. Cycle Covering Formulation,[0],[0]
"Thus, we conclude that x∗ violates an inequality of (12) and hence cannot be feasible.",3.4. Cycle Covering Formulation,[0],[0]
"This concludes the proof of (14), the argument for (15) is analogous.
",3.4. Cycle Covering Formulation,[0],[0]
"With the help of Lemma 2, we formulate PMC as a set covering problem: Definition 3.",3.4. Cycle Covering Formulation,[0],[0]
"For any graph G = (V,E) and any c ∈ RE , we call
min x̂∈SC(G,c) ∑ e∈E |ce| x̂e (PSC)
with SC(G, c) the convex hull of all x̂ ∈ ZE that satisfy the system
∀C ∈ C−(G, c) : ∑ e∈EC",3.4. Cycle Covering Formulation,[0],[0]
"x̂e ≥ 1 (16)
∀e ∈ E : x̂e ≥ 0 (17)
the set covering problem w.r.t.",3.4. Cycle Covering Formulation,[0],[0]
"conflicted cycles, and we call SC(G, c) the set covering polyhedron w.r.t.",3.4. Cycle Covering Formulation,[0],[0]
conflicted cycles.,3.4. Cycle Covering Formulation,[0],[0]
Lemma 3.,3.4. Cycle Covering Formulation,[0],[0]
"For any graph G = (V,E) and any c ∈ RE , we have
min x∈CYC−(G,c)∩ZE ∑ e∈E ce xe
= Ltriv + min x̂∈SC(G,c) ∑ e∈E |ce| x̂e (18)
with
Ltriv = ∑ e∈E− ce (19)
the sum of negative edge costs (a trivial lower bound to the optimal value of PMC).
",3.4. Cycle Covering Formulation,[0],[0]
Proof.,3.4. Cycle Covering Formulation,[0],[0]
We define x̂ via x̂e,3.4. Cycle Covering Formulation,[0],[0]
:= xe for any attractive edge e ∈ E+,3.4. Cycle Covering Formulation,[0],[0]
and x̂e,3.4. Cycle Covering Formulation,[0],[0]
:= 1− xe for any repulsive edge e ∈,3.4. Cycle Covering Formulation,[0],[0]
"E−. Since any conflicted cycle C ∈ C−(G, c) has precisely one repulsive edge, all conflicted cycle inequalities (12) become covering inequalities.",3.4. Cycle Covering Formulation,[0],[0]
"In this section, we study partial optimality for PMC.",4. Partial Optimality,[0],[0]
"More precisely, we establish conditions on an edge e ∈ E which guarantee that xe assumes one particular value, either 0 or 1, in at least one optimal solution (weak persistency).",4. Partial Optimality,[0],[0]
"Fixations to 0 are of particular interest as they can be implemented as edge contractions (with subsequent merging of parallel edges), which effectively reduce the size of a given instance of the problem.",4. Partial Optimality,[0],[0]
"As a corollary, we obtain an algorithm that solves weighted correlation clustering problems on seriesparallel graphs in linear time.",4. Partial Optimality,[0],[0]
A direct consequence from Lemma 3 is that we may disregard all edges that are not contained in any conflicted cycle.,4.1. Basic Conditions,[0],[0]
There are (at least) two ways this can happen: 1.,4.1. Basic Conditions,[0],[0]
"An edge e ∈ E is not contained in any cycle at all, that is, e is a bridge.",4.1. Basic Conditions,[0],[0]
2.,4.1. Basic Conditions,[0],[0]
"The endpoints of a repulsive edge e = {u, v} ∈",4.1. Basic Conditions,[0],[0]
"E− belong to different components of G+ = (V,E+).",4.1. Basic Conditions,[0],[0]
"In both cases, for any optimal solution x∗ of PMC, it holds that x∗e = 0",4.1. Basic Conditions,[0],[0]
"if e is attractive, and x ∗ e",4.1. Basic Conditions,[0],[0]
= 1 if e is repulsive.,4.1. Basic Conditions,[0],[0]
"Thus, we can restrict the instance of the problem to the maximal components ofG that are connected in G+ and biconnected in G. This was also observed by Alush & Goldberger (2012).
",4.1. Basic Conditions,[0],[0]
"Below, we establish more general partial optimality conditions.",4.1. Basic Conditions,[0],[0]
"To this end, we need the following notation.",4.1. Basic Conditions,[0],[0]
"A cut of G is a bipartition B = (S1, S2) of the nodes V , i.e. V = S1 ∪̇S2.",4.1. Basic Conditions,[0],[0]
The edge set of the cut B is denoted by EB = {uv ∈ E,4.1. Basic Conditions,[0],[0]
"| u ∈ S1, v ∈ S2}.",4.1. Basic Conditions,[0],[0]
Definition 4.,4.2. Dominant Edges,[0],[0]
"Let G = (V,E) be any graph and let c ∈ RE .",4.2. Dominant Edges,[0],[0]
An edge f ∈ E is called dominant attractive iff cf > 0,4.2. Dominant Edges,[0],[0]
"and there exists a cut B with f ∈ EB such that
cf ≥ ∑
e∈EB\{f}
|ce| .",4.2. Dominant Edges,[0],[0]
"(20)
An edge f ∈",4.2. Dominant Edges,[0],[0]
E− is called dominant repulsive iff cf < 0,4.2. Dominant Edges,[0],[0]
"and there exists a cut B with f ∈ EB such that
|cf | ≥ ∑
e∈EB∩E+ ce.",4.2. Dominant Edges,[0],[0]
"(21)
",4.2. Dominant Edges,[0],[0]
"An edge is called dominant iff it is dominant attractive or dominant repulsive.
",4.2. Dominant Edges,[0],[0]
Lemma 4.,4.2. Dominant Edges,[0],[0]
"Let G = (V,E) be any graph and let c ∈ RE .
",4.2. Dominant Edges,[0],[0]
(i),4.2. Dominant Edges,[0],[0]
"If f ∈ E is dominant attractive, then x∗f = 0 in at least one optimal solution x∗ of PMC.
(ii) If f ∈ E is dominant repulsive, then x∗f = 1 in at least one optimal solution x∗ of PMC.
",4.2. Dominant Edges,[0],[0]
Proof.,4.2. Dominant Edges,[0],[0]
(i),4.2. Dominant Edges,[0],[0]
We use the set covering formulation of PMC.,4.2. Dominant Edges,[0],[0]
Suppose f ∈ E+ is dominant and x̂∗f = 1 in an optimal solution x̂∗ of PSC.,4.2. Dominant Edges,[0],[0]
"Every conflicted cycle that contains f also contains some edge e ∈ EB , since B is a cut.",4.2. Dominant Edges,[0],[0]
"Therefore, the vector x̂ ∈ {0, 1}E defined by
x̂e =  0",4.2. Dominant Edges,[0],[0]
"if e = f 1 if e ∈ EB , e 6= f x̂∗e else
is a feasible solution to PSC.",4.2. Dominant Edges,[0],[0]
"It has the same objective value as x̂∗, since f is dominant and x̂∗ is optimal.
",4.2. Dominant Edges,[0],[0]
(ii) Suppose f ∈,4.2. Dominant Edges,[0],[0]
E− is dominant and x̂∗f,4.2. Dominant Edges,[0],[0]
= 1 in an optimal solution x̂∗ of PSC,4.2. Dominant Edges,[0],[0]
.,4.2. Dominant Edges,[0],[0]
Every conflicted cycle that contains f also contains some edge e ∈ EB ∩,4.2. Dominant Edges,[0],[0]
"E+, since B is a cut and every conflicted cycle contains only one repulsive edge.",4.2. Dominant Edges,[0],[0]
"Then the vector x̂ ∈ {0, 1}E defined by x̂f = 0, x̂e = 1 for all e ∈ EB ∩ E+ and x̂e = x̂∗e elsewhere is a feasible solution to PSC.",4.2. Dominant Edges,[0],[0]
"It has the same objective value as x̂∗, since f is dominant and x̂∗ is optimal.
",4.2. Dominant Edges,[0],[0]
"Lemma 4 generalizes the basic conditions discussed in Section 4.1, since each edge f ∈ E that is not contained in any conflicted cycle is also dominant.",4.2. Dominant Edges,[0],[0]
"Dominance of edges can be decided in polynomial time, by computing minimum st-cuts in G for a suitable choice of capacities.",4.2. Dominant Edges,[0],[0]
"In practice, the required computational effort may be mitigated by constructing a cut tree of G (Gomory & Hu, 1961).",4.2. Dominant Edges,[0],[0]
"The practically most relevant cuts can even be checked in linear time, which we discuss in the following section.",4.2. Dominant Edges,[0],[0]
"In practice, it is expected that dominant edges are more likely to be found in cuts that are relatively sparse.",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"We discuss two special cases of sparse cuts that are of particular interest, due to the following reasons.",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"First, they can be checked in linear time, which gives rise to a fast preprocessing algorithm.",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"Second, we show that our techniques solve PMC to optimality if G is series-parallel.
",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
Two-edge cuts.,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"Suppose B is a two-edge cut of G, i.e. EB = {e, f} for two edges e, f ∈",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
E.,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"Apparently, according to (20) and (21), at least one of them must be dominant.",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"Further, it is guaranteed that we can simplify the instance by edge deletions or contractions.",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"To see this, distinguish the following cases.",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"If both e and f are repulsive, then both of them are dominant and we can delete them, as they are not contained in any conflicted cycle.",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"If f is dominant attractive, we can contract f .",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"Finally, if f is dominant repulsive and e is attractive, then we can switch the signs of their coefficients and redefine xf := 1 − xf as well as xe := 1 − xe.",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"Since |EB | = 2, this operation does not change the set of conflicted cycles of G and thus is valid (while only adding a constant to the objective).",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"Afterwards, the edge f is dominant attractive and we can contract f .",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"The two-edge cuts of G can be found in linear time, by computing the 3-edge-connected components of G, cf.",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"(Mehlhorn et al., 2017).
",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
Single-node cuts.,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"For any v ∈ V , let Bv = ({v}, V \ {v}) denote the cut that is induced by v. Whether EBv contains a dominant edge is easily decided by considering all edges incident to v. Moreover, if deg v = 2, then Bv is also a two-edge cut and we can apply the operation described in
Algorithm 1 Single-Node Cut Preprocessing input G = (V,E), c :",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"E → R
1: Initialize objective value offset ∆ = 0.",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
2: Initialize a queue Q = V .,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
3: while Q 6= ∅,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
do 4: Extract a vertex v ∈ Q. 5: if deg v = 1 then 6: Get neighbor u ∈ V .,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"7: if cuv ≥ 0 then 8: Set xuv = 0 and contract uv ∈ E. 9: else
10: Set xuv = 1, ∆ = ∆ + cuv and delete uv ∈ E. 11: end if 12: else if deg v = 2 then 13: Get neighbors u,w ∈ V with |cuv| ≥ |cwv|.",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
14: if uv ∈ E+ then 15: Set xuv = 0 and contract uv ∈ E. 16: else if uv ∈,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
E− and wv ∈,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
E− then 17: Adjust offset ∆ = ∆ + cuv + cwv .,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"18: Set xuv = xwv = 1 and delete uv,wv ∈",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
E. 19: else if uv ∈,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
E− and wv ∈ E+ then 20:,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
Adjust offset ∆ = ∆ + cuv + cwv .,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"21: Redefine xuv = 1− xuv , xwv = 1− xwv and cuv = −cuv , cwv = −cwv .",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
22: Set xuv = 0 and contract uv ∈ E. 23: end if 24: else if ∃f ∈ Bv dominant attractive then 25: Set xf = 0 and contract f ∈,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
E. 26: end if 27:,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
Add to Q all vertices u /∈,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
Q whose neighborhood was changed.,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"28: end while 29: return (G, c), x,∆
the last paragraph.",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
Updating the graph and applying these techniques recursively as specified in Algorithm 1 takes linear time.,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"This has the following theoretical consequence.
",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
Corollary 1.,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"If G has treewidth at most 2, then Algorithm 1 can be implemented to solve PMC exactly in O(|V |) time.
",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
Proof.,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
Place the vertices of G into buckets of ascending degree and always pick a vertex of minimal degree.,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
Every graph of treewidth 2 has a vertex v with deg v ≤ 2.,4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"Since Algorithm 1 only contracts or deletes edges, fixing the variables according to Lemma 4, the updated graph still has treewidth at most 2.",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"The number of nodes decreases by 1 in every iteration, hence the algorithm terminates in time O(|E|) = O(|V |) and outputs an optimal solution.",4.3. Two-Edge Cuts & Single-Node Cuts,[0],[0]
"In this section, we define an algorithm for computing lower bounds for PMC.",5. Dual Lower Bounds,[0],[0]
"This algorithm exploits the structure of
Algorithm 2 Iterative Cycle Packing (ICP) input G = (V,E), c :",5. Dual Lower Bounds,[0],[0]
"E → R
1: Initialize we = |ce| for all e ∈ E and y = 0, L = Ltriv. 2: for ` = 3 . . .",5. Dual Lower Bounds,[0],[0]
"|E| do 3: while ∃C ∈ C−(G, c) : |EC | ≤ ` do 4: Pick C ∈ C−(G, c) such that |EC | ≤",5. Dual Lower Bounds,[0],[0]
"`. 5: Compute yC = mine∈EC we.
6: Redefine we = { we − yC",5. Dual Lower Bounds,[0],[0]
if e ∈ EC we else.,5. Dual Lower Bounds,[0],[0]
7: Increase lower bound L = L+ yC .,5. Dual Lower Bounds,[0],[0]
"8: Remove all edges e ∈ E with we = 0 from G. 9: end while
10: if C−(G, c) = ∅ then 11: return y, L 12: end if 13: end for
the reformulation PSC.",5. Dual Lower Bounds,[0],[0]
"It computes a heuristic solution to the dual of its LP relaxation.
",5. Dual Lower Bounds,[0],[0]
"The LP relaxation (up to the constant Ltriv) of problem PSC is given by
min ∑ e∈E |ce|x̂e (22)
subject to ∑ e∈EC x̂e ≥ 1 ∀C ∈ C−(G, c) (23)
",5. Dual Lower Bounds,[0],[0]
"x̂e ≥ 0 ∀e ∈ E .
",5. Dual Lower Bounds,[0],[0]
"The corresponding dual program reads
max ∑
C∈C−(G,c)
yC (24)
subject to ∑
C: e∈EC
yC ≤ |ce| ∀e ∈ E (25)
yC ≥ 0 ∀C ∈ C−(G, c) .
",5. Dual Lower Bounds,[0],[0]
"A heuristic solution of (24), and thus a lower bound for (22), is found by Algorithm 2 that we call Iterative Cycle Packing (ICP).",5. Dual Lower Bounds,[0],[0]
It works as follows:,5. Dual Lower Bounds,[0],[0]
"Firstly, it chooses a conflicted cycle C and increases yC as much as possible.",5. Dual Lower Bounds,[0],[0]
"Secondly, it decreases the weights we (initially |ce|) of all edges e ∈ EC by yC and removes all edges of zero weight.",5. Dual Lower Bounds,[0],[0]
"These steps are repeated until there are no conflicted cycles left.
",5. Dual Lower Bounds,[0],[0]
Implementation details.,5. Dual Lower Bounds,[0],[0]
"The absolute running time of ICP as well as the quality of the output lower bounds depends on the choice of cycles C. We pursue the following strategy that we found to perform well empirically in both aspects: In each iteration of the main loop, we choose a repulsive edge e = uv ∈",5. Dual Lower Bounds,[0],[0]
"E− such that u and v are in the same connected component of G+ = (V,E+).",5. Dual Lower Bounds,[0],[0]
"Then, we find a conflicted cycle containing e by searching for a shortest path
(in terms of hop distance) from u to v in G+.",5. Dual Lower Bounds,[0],[0]
"We apply this search for conflicted cycles in rounds of increasing cycle length, using breadth-first search with an early termination criterion based on the hop distance.",5. Dual Lower Bounds,[0],[0]
We also maintain and periodically update a component labeling of G+ in order to to reduce the number of redundant shortest path searches.,5. Dual Lower Bounds,[0],[0]
"In this section, we exploit the dual solution in primal algorithms.",6. Re-weighting for Primal Algorithms,[0],[0]
"The motivation is due to complementary slackness, which is made explicit in the following lemma.
",6. Re-weighting for Primal Algorithms,[0],[0]
Lemma 5.,6. Re-weighting for Primal Algorithms,[0],[0]
"Assume the primal LP (22) is tight, i.e., its optimal solution x̂∗ also solves PSC, and the solution output by ICP solves the dual (24) optimally.",6. Re-weighting for Primal Algorithms,[0],[0]
"Then, for every e ∈ E with positive residual weight we > 0, it holds that x̂∗e = 0.
",6. Re-weighting for Primal Algorithms,[0],[0]
Proof.,6. Re-weighting for Primal Algorithms,[0],[0]
"If we > 0, the constraint (25) at e ∈ E is inactive at the optimal dual solution.",6. Re-weighting for Primal Algorithms,[0],[0]
"Thus, x̂∗e = 0 in the optimal primal solution, by complementary slackness.
",6. Re-weighting for Primal Algorithms,[0],[0]
"Of course, the assumption of Lemma 5 is too strong for practical purposes.",6. Re-weighting for Primal Algorithms,[0],[0]
"However, the intuition is that if the LP relaxation is fairly tight and the obtained dual solution is close to optimal, it can still provide useful information about the primal problem.",6. Re-weighting for Primal Algorithms,[0],[0]
"More specifically, the weights we output by ICP can be interpreted as an indication of how likely the primal variable x̂e is zero in an optimal solution.",6. Re-weighting for Primal Algorithms,[0],[0]
"In order to make use of this information, we propose to shift the weights of the primal problem to a convex combination λ|ce|+ (1− λ)we of the original and residual weights, for a suitable choice of λ ∈ (0, 1).",6. Re-weighting for Primal Algorithms,[0],[0]
Experiments in Section 7 show that this shift can guide primal heuristics toward better feasible solutions to the original problem.,6. Re-weighting for Primal Algorithms,[0],[0]
"In this section, we study partial optimality, dual lower bounds and re-weightings empirically, for all instances of
the weighted correlation clustering problem from Kappes et al. (2015) and Leskovec et al. (2010).
Instances.",7. Experiments,[0],[0]
"From Kappes et al. (2015), we consider all three collections of instances: Image Segmentation contains instances w.r.t.",7. Experiments,[0],[0]
planar superpixel adjacency graphs of photographs.,7. Experiments,[0],[0]
Knott-3D contains instances w.r.t.,7. Experiments,[0],[0]
non-planar supervoxel adjacency graphs of volume images taken by a serial sectioning electron microscope.,7. Experiments,[0],[0]
Modularity Clustering contains instances w.r.t.,7. Experiments,[0],[0]
complete graphs.,7. Experiments,[0],[0]
"In all three collections, the edge costs ce are fractional and non-uniform.",7. Experiments,[0],[0]
"For all these instances, except one in the collection Modularity Clustering, optimal solutions are accessible and are computed here as a reference.",7. Experiments,[0],[0]
"From Leskovec et al. (2010), we consider directed graphs of the social networks Epinions and Slashdot, each with more than half a million edges labeled either +1 or −1.",7. Experiments,[0],[0]
"Instances of the minimum cost multicut problem are defined here by removing the orientation of edges, by deleting all self-loops, and by replacing parallel edges by a single edge with the sum of their costs1.",7. Experiments,[0],[0]
"In order to study the partial optimality conditions of Section 4 empirically, we process the above instances as follows: First, we remove all edges of cost 0, all bridges, as well as all repulsive edges whose nodes belong to distinct connected components of G+.",7.1. Partial Optimality,[0],[0]
"Second, we check for every v ∈ V whether the cut Bv = ({v}, V \ {v}) induces dominant edges.",7.1. Partial Optimality,[0],[0]
"If we find dominant attractive edges or vertices of degree ≤ 2, we perform contractions and deletions according to Alg. 1.",7.1. Partial Optimality,[0],[0]
"Both steps are repeated until no further edges can be removed or contracted.
",7.1. Partial Optimality,[0],[0]
"After the main reduction step, which takes linear time and is thus very fast, we further check all remaining edges uv ∈ E for dominance in any (general) uv-cut.",7.1. Partial Optimality,[0],[0]
"To this end, we construct a cut tree of G with the help of Gusfield’s algorithm (Gusfield, 1990), which takes |V | − 1 max-flow computations.",7.1. Partial Optimality,[0],[0]
"Despite the increased computational effort, we only found a small number of additional dominant attractive edges and thus could only perform few further contractions.",7.1. Partial Optimality,[0],[0]
"However, we found a significant number of additional dom-
1This results in 2703 edges of cost 0 for Epinions, and 1949 such edges for Slashdot.
inant repulsive edges.
",7.1. Partial Optimality,[0],[0]
The effect of our method in the total number of nodes and edges is shown in Table 1.,7.1. Partial Optimality,[0],[0]
We also report the number of remaining edges that are not dominant repulsive.,7.1. Partial Optimality,[0],[0]
It can be seen from this table that the numbers are effectively reduced.,7.1. Partial Optimality,[0],[0]
"This is explained, firstly, by the sparsity of the graphs and, secondly, by the non-uniformity of the costs.",7.1. Partial Optimality,[0],[0]
"From the comparison to the number of remaining non-persistent variables when only the criteria of Alush & Goldberger (2012) are applied, it can be seen that our more general criteria reveal considerably more persistency.
",7.1. Partial Optimality,[0],[0]
It may be expected that optimization methods benefit in terms of runtime from the reduced size of the instances.,7.1. Partial Optimality,[0],[0]
"On the instances of Kappes et al. (2015), we found the effect to be insignificant due to their small original size.",7.1. Partial Optimality,[0],[0]
"On Epinions and Slashdot, however, the runtime of the local search algorithm GAEC+KLj (cf. Section 7.3) decreased by more than 70%.",7.1. Partial Optimality,[0],[0]
"For completeness, we provide the numbers in the supplements.",7.1. Partial Optimality,[0],[0]
"In order to put into perspective the dual lower bounds output by Iterative Cycle Packing (ICP) as described in Section 5, we compare this algorithm, firstly, to the cutting plane algorithm for PCYC of Kappes et al. (2015), with Gurobi for solving the LPs (denoted here by LP) and, secondly, to the message passing algorithm of Swoboda & Andres (2017), applied to PCYC, with code and parameter settings kindly provided by the authors (denoted here by MPC).
",7.2. Dual Lower Bounds,[0],[0]
Results are shown in Figure 1 and Table 2.,7.2. Dual Lower Bounds,[0],[0]
"It can be seen from the figure and the table that, for the large and hard instances Epinions and Slashdot, ICP converges at under 102 seconds, outputting lower bounds that are matched and exceeded by MPC at around 103 seconds.",7.2. Dual Lower Bounds,[0],[0]
"It can be seen from Table 2 that the situation is similar for the smaller instances: The lower bounds output by ICP are a bit worse than those output by LP or MPC (here compared to the best optimal solution known) but are obtained faster (by as much as three orders of magnitude for Knott-3D-450).
",7.2. Dual Lower Bounds,[0],[0]
"It is known from Kappes et al. (2015) that their instances can be solved faster than their LP relaxations by means of branch-and-cut, separating only integer infeasible points
·105
Epinions
·105
Slashdot
by cycle inequalities using BFS (instead of Dijkstra’s algorithm), and resorting to the strong (undisclosed) cuts of Gurobi for cutting off fractional solutions.",7.2. Dual Lower Bounds,[0],[0]
We restrict our comparison here to algorithms that seek to solve the LP relaxation PCYC.,7.2. Dual Lower Bounds,[0],[0]
This is justified by the fact that size ultimately renders integer linear programming intractable.,7.2. Dual Lower Bounds,[0],[0]
We conclude that ICP is capable of computing non-trivial lower bounds fast.,7.2. Dual Lower Bounds,[0],[0]
"In order to study the re-weighting described in Section 6, we measure its effect on heuristic algorithms for finding feasible solutions.",7.3. Re-weighting,[0],[0]
"To this end, we employ the implementations of Levinkov et al. (2017) of Greedy Additive Edge Contraction (GAEC), an algorithm that starts from singleton clusters and greedily contracts attractive edges with maximum nonnegative cost, and of KLj, the well-known Kernighan-Lin heuristic for graph partitioning that recursively improves an initial clustering by splitting, merging or exchanging nodes between neighboring clusters.
",7.3. Re-weighting,[0],[0]
"A comparison between the feasible solutions found by applying the heuristics GAEC and GAEC+KLj to original instances, on the one hand, and to instances re-weighted by ICP with λ = 12 , on the other hand, can be found in Table
3.",7.3. Re-weighting,[0],[0]
"Note that we only re-weight the input to GAEC and let KLj run with original weights, starting from the solution returned by GAEC, as we found this approach to be advantageous.",7.3. Re-weighting,[0],[0]
It can be seen from Table 3 that our re-weighting consistently improves the gap.,7.3. Re-weighting,[0],[0]
"On average, it is slightly less effective than the reparameterization with the more accurate dual solutions obtained from MPC, as proposed by Swoboda & Andres (2017).",7.3. Re-weighting,[0],[0]
A more detailed comparison is provided in the supplements.,7.3. Re-weighting,[0],[0]
"We have established partial optimality conditions, a heuristic lower bound and a heuristic re-weighting for instances of the weighted correlation clustering problem.",8. Conclusion,[0],[0]
We have shown advantages of each of these constructions empirically.,8. Conclusion,[0],[0]
Checking a subset of our partial optimality conditions recursively gives a fast combinatorial algorithm that efficiently reduces the size of problem instances.,8. Conclusion,[0],[0]
"Conceptually, it solves the problem for series-parallel graphs to optimality, in linear time.",8. Conclusion,[0],[0]
Our dual heuristic algorithm provides nontrivial lower bounds and valuable dual information fast.,8. Conclusion,[0],[0]
"For future work, it is relevant to examine if more sophisticated dual solvers such as MPC benefit from a “warm-start” that transforms and exploits the heuristic dual solution.",8. Conclusion,[0],[0]
Weighted correlation clustering is hard to solve and hard to approximate for general graphs.,abstractText,[0],[0]
Its applications in network analysis and computer vision call for efficient algorithms.,abstractText,[0],[0]
"To this end, we make three contributions: We establish partial optimality conditions that can be checked efficiently, and doing so recursively solves the problem for series-parallel graphs to optimality, in linear time.",abstractText,[0],[0]
"We exploit the packing dual of the problem to compute a heuristic, but non-trivial lower bound faster than that of a canonical linear program relaxation.",abstractText,[0],[0]
We introduce a re-weighting with the dual solution by which efficient local search algorithms converge to better feasible solutions.,abstractText,[0],[0]
The effectiveness of our methods is demonstrated empirically on a number of benchmark instances.,abstractText,[0],[0]
Partial Optimality and Fast Lower Bounds for Weighted Correlation Clustering,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3719–3728 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
3719",text,[0],[0]
Many interpretation methods for neural networks explain the model’s prediction as a counterfactual: how does the prediction change when the input is modified?,1 Introduction,[0],[0]
"Adversarial examples (Szegedy et al., 2014; Goodfellow et al., 2015) highlight the instability of neural network predictions by showing how small perturbations to the input dramatically change the output.
",1 Introduction,[0],[0]
"A common, non-adversarial form of model interpretation is feature attribution: features that are crucial for predictions are highlighted in a heatmap.",1 Introduction,[0],[0]
One can measure a feature’s importance by input perturbation.,1 Introduction,[0],[0]
"Given an input for text classification, a word’s importance can be measured by the difference in model confidence before and after that word is removed from the input—the word is important if confidence decreases significantly.",1 Introduction,[0],[0]
"This is the leave-one-out method (Li et al., 2016b).",1 Introduction,[0],[0]
"Gradients can also measure feature importance; for example, a feature is influential to the prediction if its gradient is a large positive value.",1 Introduction,[0],[0]
"Both perturbation and gradient-based methods can generate heatmaps, implying that the model’s prediction is highly influenced by the highlighted, important words.
",1 Introduction,[0],[0]
"Instead, we study how the model’s prediction is influenced by the unimportant words.",1 Introduction,[0],[0]
"We use input reduction, a process that iteratively removes the unimportant words from the input while maintaining the model’s prediction.",1 Introduction,[0],[0]
"Intuitively, the words remaining after input reduction should be important for prediction.",1 Introduction,[0],[0]
"Moreover, the words
should match the leave-one-out method’s selections, which closely align with human perception (Li et al., 2016b; Murdoch et al., 2018).",1 Introduction,[0],[0]
"However, rather than providing explanations of the original prediction, our reduced examples more closely resemble adversarial examples.",1 Introduction,[0],[0]
"In Figure 1, the reduced input is meaningless to a human but retains the same model prediction with high confidence.",1 Introduction,[0],[0]
"Gradient-based input reduction exposes pathological model behaviors that contradict what one expects based on existing interpretation methods.
",1 Introduction,[0],[0]
"In Section 2, we construct more of these counterintuitive examples by augmenting input reduction with beam search and experiment with three tasks: SQUAD (Rajpurkar et al., 2016) for reading comprehension, SNLI (Bowman et al., 2015) for textual entailment, and VQA (Antol et al., 2015) for visual question answering.",1 Introduction,[0],[0]
Input reduction with beam search consistently reduces the input sentence to very short lengths—often only one or two words—without lowering model confidence on its original prediction.,1 Introduction,[0],[0]
"The reduced examples appear nonsensical to humans, which we verify with crowdsourced experiments.",1 Introduction,[0],[0]
"In Section 3, we draw connections to adversarial examples and confidence calibration; we explain why the observed pathologies are a consequence of the overconfidence of neural models.",1 Introduction,[0],[0]
This elucidates limitations of interpretation methods that rely on model confidence.,1 Introduction,[0],[0]
"In Section 4, we encourage high model uncertainty on reduced examples with entropy regularization.",1 Introduction,[0],[0]
"The pathological model behavior under input reduction is mitigated, leading to more reasonable reduced examples.",1 Introduction,[0],[0]
"To explain model predictions using a set of important words, we must first define importance.",2 Input Reduction,[0],[0]
"After defining input perturbation and gradient-based approximation, we describe input reduction with these importance metrics.",2 Input Reduction,[0],[0]
Input reduction drastically shortens inputs without causing the model to change its prediction or significantly decrease its confidence.,2 Input Reduction,[0],[0]
Crowdsourced experiments confirm that reduced examples appear nonsensical to humans: input reduction uncovers pathological model behaviors.,2 Input Reduction,[0],[0]
"Ribeiro et al. (2016) and Li et al. (2016b) define importance by seeing how confidence changes when a feature is removed; a natural approximation is to use the gradient (Baehrens et al., 2010; Simonyan et al., 2014).",2.1 Importance from Input Gradient,[0],[0]
We formally define these importance metrics in natural language contexts and introduce the efficient gradient-based approximation.,2.1 Importance from Input Gradient,[0],[0]
"For each word in an input sentence, we measure its importance by the change in the confidence of the original prediction when we remove that word from the sentence.",2.1 Importance from Input Gradient,[0],[0]
"We switch the sign so that when the confidence decreases, the importance value is positive.
",2.1 Importance from Input Gradient,[0],[0]
"Formally, let x = 〈x1, x2, . . .",2.1 Importance from Input Gradient,[0],[0]
"xn〉 denote the input sentence, f(y |x)",2.1 Importance from Input Gradient,[0],[0]
"the predicted probability of label y, and y = argmaxy′ f(y
′ |x)",2.1 Importance from Input Gradient,[0],[0]
the original predicted label.,2.1 Importance from Input Gradient,[0],[0]
"The importance is then
g(xi",2.1 Importance from Input Gradient,[0],[0]
| x) = f(y |x)− f(y |x−i).,2.1 Importance from Input Gradient,[0],[0]
"(1)
To calculate the importance of each word in a sentence with n words, we need n forward passes of the model, each time with one of the words left out.",2.1 Importance from Input Gradient,[0],[0]
"This is highly inefficient, especially for longer sentences.",2.1 Importance from Input Gradient,[0],[0]
"Instead, we approximate the importance value with the input gradient.",2.1 Importance from Input Gradient,[0],[0]
"For each word in the sentence, we calculate the dot product of its word embedding and the gradient of the output with respect to the embedding.",2.1 Importance from Input Gradient,[0],[0]
The importance of n words can thus be computed with a single forward-backward pass.,2.1 Importance from Input Gradient,[0],[0]
"This gradient approximation has been used for various interpretation methods for natural language classification models (Li et al., 2016a; Arras et al., 2016); see Ebrahimi et al. (2017) for further details on the derivation.",2.1 Importance from Input Gradient,[0],[0]
We use this approximation in all our experiments as it selects the same words for removal as an exhaustive search (no approximation).,2.1 Importance from Input Gradient,[0],[0]
Instead of looking at the words with high importance values—what interpretation methods commonly do—we take a complementary approach and study how the model behaves when the supposedly unimportant words are removed.,2.2 Removing Unimportant Words,[0],[0]
"Intuitively, the important words should remain after the unimportant ones are removed.
",2.2 Removing Unimportant Words,[0],[0]
Our input reduction process iteratively removes the unimportant words.,2.2 Removing Unimportant Words,[0],[0]
"At each step, we remove the word with the lowest importance value until the model changes its prediction.",2.2 Removing Unimportant Words,[0],[0]
"We experi-
ment with three popular datasets: SQUAD (Rajpurkar et al., 2016) for reading comprehension, SNLI (Bowman et al., 2015) for textual entailment, and VQA (Antol et al., 2015) for visual question answering.",2.2 Removing Unimportant Words,[0],[0]
"We describe each of these tasks and the model we use below, providing full details in the Supplement.
",2.2 Removing Unimportant Words,[0],[0]
"In SQUAD, each example is a context paragraph and a question.",2.2 Removing Unimportant Words,[0],[0]
The task is to predict a span in the paragraph as the answer.,2.2 Removing Unimportant Words,[0],[0]
We reduce only the question while keeping the context paragraph unchanged.,2.2 Removing Unimportant Words,[0],[0]
"The model we use is the DRQA Document Reader (Chen et al., 2017).
",2.2 Removing Unimportant Words,[0],[0]
"In SNLI, each example consists of two sentences: a premise and a hypothesis.",2.2 Removing Unimportant Words,[0],[0]
"The task is to predict one of three relationships: entailment, neutral, or contradiction.",2.2 Removing Unimportant Words,[0],[0]
We reduce only the hypothesis while keeping the premise unchanged.,2.2 Removing Unimportant Words,[0],[0]
"The model we use is Bilateral Multi-Perspective Matching (BIMPM) (Wang et al., 2017).
",2.2 Removing Unimportant Words,[0],[0]
"In VQA, each example consists of an image and a natural language question.",2.2 Removing Unimportant Words,[0],[0]
We reduce only the question while keeping the image unchanged.,2.2 Removing Unimportant Words,[0],[0]
"The model we use is Show, Ask, Attend, and Answer (Kazemi and Elqursh, 2017).
",2.2 Removing Unimportant Words,[0],[0]
"During the iterative reduction process, we ensure that the prediction does not change (exact same span for SQUAD); consequently, the model accuracy on the reduced examples is identical to the original.",2.2 Removing Unimportant Words,[0],[0]
The predicted label is used for input reduction and the ground-truth is never revealed.,2.2 Removing Unimportant Words,[0],[0]
"We use the validation set for all three tasks.
",2.2 Removing Unimportant Words,[0],[0]
Most reduced inputs are nonsensical to humans (Figure 2) as they lack information for any reasonable human prediction.,2.2 Removing Unimportant Words,[0],[0]
"However, models make confident predictions, at times even more confident than the original.
",2.2 Removing Unimportant Words,[0],[0]
"To find the shortest possible reduced inputs (potentially the most meaningless), we relax the requirement of removing only the least important word and augment input reduction with beam search.",2.2 Removing Unimportant Words,[0],[0]
"We limit the removal to the k least important words, where k is the beam size, and decrease the beam size as the remaining input is shortened.1",2.2 Removing Unimportant Words,[0],[0]
We empirically select beam size five as it produces comparable results to larger beam sizes with reasonable computation cost.,2.2 Removing Unimportant Words,[0],[0]
"The requirement of maintaining model prediction is unchanged.
",2.2 Removing Unimportant Words,[0],[0]
"1We set beam size to max(1,min(k, L − 3))",2.2 Removing Unimportant Words,[0],[0]
"where k is maximum beam size and L is the current length of the input sentence.
",2.2 Removing Unimportant Words,[0],[0]
"With beam search, input reduction finds extremely short reduced examples with little to no decrease in the model’s confidence on its original predictions.",2.2 Removing Unimportant Words,[0],[0]
Figure 3 compares the length of input sentences before and after the reduction.,2.2 Removing Unimportant Words,[0],[0]
"For all three tasks, we can often reduce the sentence to only one word.",2.2 Removing Unimportant Words,[0],[0]
Figure 4 compares the model’s confidence on original and reduced inputs.,2.2 Removing Unimportant Words,[0],[0]
"On SQUAD and SNLI the confidence decreases slightly, and on VQA the confidence even increases.",2.2 Removing Unimportant Words,[0],[0]
"On the reduced examples, the models retain their original predictions despite short input lengths.",2.3 Humans Confused by Reduced Inputs,[0],[0]
"The following experiments examine whether these predictions are justified or pathological, based on how humans react to the reduced inputs.
",2.3 Humans Confused by Reduced Inputs,[0],[0]
"For each task, we sample 200 examples that are correctly classified by the model and generate their reduced examples.",2.3 Humans Confused by Reduced Inputs,[0],[0]
"In the first setting, we compare the human accuracy on original and reduced examples.",2.3 Humans Confused by Reduced Inputs,[0],[0]
"We recruit two groups of crowd workers and task them with textual entailment, reading comprehension, or visual question answering.",2.3 Humans Confused by Reduced Inputs,[0],[0]
We show one group the original inputs and the other the reduced.,2.3 Humans Confused by Reduced Inputs,[0],[0]
"Humans are no longer able to give
the correct answer, showing a significant accuracy loss on all three tasks (compare Original and Reduced in Table 1).
",2.3 Humans Confused by Reduced Inputs,[0],[0]
The second setting examines how random the reduced examples appear to humans.,2.3 Humans Confused by Reduced Inputs,[0],[0]
"For each of the original examples, we generate a version where words are randomly removed until the length matches the one generated by input reduction.",2.3 Humans Confused by Reduced Inputs,[0],[0]
We present the original example along with the two reduced examples and ask crowd workers their preference between the two reduced ones.,2.3 Humans Confused by Reduced Inputs,[0],[0]
"The workers’ choice is almost fifty-fifty (the vs. Random in Table 1): the reduced examples appear almost random to humans.
",2.3 Humans Confused by Reduced Inputs,[0],[0]
These results leave us with two puzzles: why are the models highly confident on the nonsensical reduced examples?,2.3 Humans Confused by Reduced Inputs,[0],[0]
"And why, when the leave-oneout method selects important words that appear reasonable to humans, the input reduction process selects ones that are nonsensical?",2.3 Humans Confused by Reduced Inputs,[0],[0]
"Having established the incongruity of our definition of importance vis-à-vis human judgements, we now investigate possible explanations for these results.",3 Making Sense of Reduced Inputs,[0],[0]
We explain why model confidence can empower methods such as leave-one-out to generate reasonable interpretations but also lead to pathologies under input reduction.,3 Making Sense of Reduced Inputs,[0],[0]
We attribute these results to two issues of neural models.,3 Making Sense of Reduced Inputs,[0],[0]
"Neural models are overconfident in their predictions (Guo et al., 2017).",3.1 Model Overconfidence,[0],[0]
One explanation for overconfidence is overfitting: the model overfits the negative log-likelihood loss during training by learning to output low-entropy distributions over classes.,3.1 Model Overconfidence,[0],[0]
Neural models are also overconfident on examples outside the training data distribution.,3.1 Model Overconfidence,[0],[0]
"As Goodfellow et al. (2015) observe for image classification, samples from pure noise can sometimes trigger highly confident predictions.",3.1 Model Overconfidence,[0],[0]
"These socalled rubbish examples are degenerate inputs that
a human would trivially classify as not belonging to any class but for which the model predicts with high confidence.",3.1 Model Overconfidence,[0],[0]
Goodfellow et al. (2015) argue that the rubbish examples exist for the same reason that adversarial examples do: the surprising linear nature of neural models.,3.1 Model Overconfidence,[0],[0]
"In short, the confidence of a neural model is not a robust estimate of its prediction uncertainty.
",3.1 Model Overconfidence,[0],[0]
"Our reduced inputs satisfy the definition of rubbish examples: humans have a hard time making predictions based on the reduced inputs (Table 1), but models make predictions with high confidence (Figure 4).",3.1 Model Overconfidence,[0],[0]
"Starting from a valid example, input reduction transforms it into a rubbish example.
",3.1 Model Overconfidence,[0],[0]
"The nonsensical, almost random results are best explained by looking at a complete reduction path (Figure 5).",3.1 Model Overconfidence,[0],[0]
"In this example, the transition from valid to rubbish happens immediately after the first step: following the removal of “Broncos”, humans can no longer determine which team the question is asking about, but model confidence remains high.",3.1 Model Overconfidence,[0],[0]
"Not being able to lower its confidence on rubbish examples—as it is not trained to do so— the model neglects “Broncos” and eventually the process generates nonsensical results.
",3.1 Model Overconfidence,[0],[0]
"In this example, the leave-one-out method will not highlight “Broncos”.",3.1 Model Overconfidence,[0],[0]
"However, this is not a failure of the interpretation method but of the model itself.",3.1 Model Overconfidence,[0],[0]
"The model assigns a low importance to “Broncos” in the first step, causing it to be removed—leave-one-out would be able to expose this particular issue by not highlighting “Broncos”.",3.1 Model Overconfidence,[0],[0]
"However, in cases where a similar issue only appear after a few unimportant words are removed, the leave-one-out method would fail to expose the unreasonable model behavior.
",3.1 Model Overconfidence,[0],[0]
Input reduction can expose deeper issues of model overconfidence and stress test a model’s uncertainty estimation and interpretability.,3.1 Model Overconfidence,[0],[0]
"So far, we have seen that the output of a neural model is sensitive to small changes in its input.",3.2 Second-order Sensitivity,[0],[0]
"We call this first-order sensitivity, because interpretation based on input gradient is a first-order Taylor expansion of the model near the input (Simonyan et al., 2014).",3.2 Second-order Sensitivity,[0],[0]
"However, the interpretation also shifts drastically with small input changes (Figure 6).",3.2 Second-order Sensitivity,[0],[0]
"We call this second-order sensitivity.
",3.2 Second-order Sensitivity,[0],[0]
"The shifting heatmap suggests a mismatch between the model’s first- and second-order sensi-
tivities.",3.2 Second-order Sensitivity,[0],[0]
"The heatmap shifts when, with respect to the removed word, the model has low first-order sensitivity but high second-order sensitivity.
",3.2 Second-order Sensitivity,[0],[0]
Similar issues complicate comparable interpretation methods for image classification models.,3.2 Second-order Sensitivity,[0],[0]
"For example, Ghorbani et al. (2017) modify image inputs so the highlighted features in the interpretation change while maintaining the same prediction.",3.2 Second-order Sensitivity,[0],[0]
"To achieve this, they iteratively modify the input to maximize changes in the distribution of feature importance.",3.2 Second-order Sensitivity,[0],[0]
"In contrast, the shifting heatmap we observe occurs by only removing the least impactful features without a targeted optimization.",3.2 Second-order Sensitivity,[0],[0]
They also speculate that the steepest gradient direction for the first- and secondorder sensitivity values are generally orthogonal.,3.2 Second-order Sensitivity,[0],[0]
"Loosely speaking, the shifting heatmap suggests that the direction of the smallest gradient value can sometimes align with very steep changes in second-order sensitivity.
",3.2 Second-order Sensitivity,[0],[0]
"When explaining individual model predictions, the heatmap suggests that the prediction is made based on a weighted combination of words, as in a linear model, which is not true unless the model is indeed taking a weighted sum such as in a DAN (Iyyer et al., 2015).",3.2 Second-order Sensitivity,[0],[0]
"When the model composes representations by a non-linear combination of words, a linear interpretation oblivious to second-order sensitivity can be misleading.",3.2 Second-order Sensitivity,[0],[0]
The previous section explains the observed pathologies from the perspective of overconfidence: models are too certain on rubbish examples when they should not make any prediction.,4 Mitigating Model Pathologies,[0],[0]
Human experiments in Section 2.3 confirm that the reduced examples fit the definition of rubbish examples.,4 Mitigating Model Pathologies,[0],[0]
"Hence, a natural way to mitigate the pathologies is to maximize model uncertainty on the reduced examples.",4 Mitigating Model Pathologies,[0],[0]
"To maximize model uncertainty on reduced examples, we use the entropy of the output distribution as an objective.",4.1 Regularization on Reduced Inputs,[0],[0]
"Given a model f trained on a dataset (X ,Y), we generate reduced examples using input reduction for all training examples X .",4.1 Regularization on Reduced Inputs,[0],[0]
"Beam search often yields multiple reduced versions with the same minimum length for each input x, and we collect all of these versions together to form X̃ as the “negative” example set.
",4.1 Regularization on Reduced Inputs,[0],[0]
Let H (·) denote the entropy and f(y |x) denote the probability of the model predicting y given x.,4.1 Regularization on Reduced Inputs,[0],[0]
"We fine-tune the existing model to simultaneously maximize the log-likelihood on regular examples and the entropy on reduced examples:∑ (x,y)∈(X ,Y) log(f(y |x))",4.1 Regularization on Reduced Inputs,[0],[0]
+ λ,4.1 Regularization on Reduced Inputs,[0],[0]
"∑ x̃∈X̃ H (f(y | x̃)) , (2) where hyperparameter λ controls the trade-off between the two terms.",4.1 Regularization on Reduced Inputs,[0],[0]
"Similar entropy regularization is used by Pereyra et al. (2017), but not in
combination with input reduction; their entropy term is calculated on regular examples rather than reduced examples.",4.1 Regularization on Reduced Inputs,[0],[0]
"On regular examples, entropy regularization does no harm to model accuracy, with a slight increase for SQUAD (Accuracy in Table 2).
",4.2 Regularization Mitigates Pathologies,[0],[0]
"After entropy regularization, input reduction produces more reasonable reduced inputs (Figure 7).",4.2 Regularization Mitigates Pathologies,[0],[0]
"In the SQUAD example from Figure 1, the reduced question changed from “did” to “spend Astor money on ?”",4.2 Regularization Mitigates Pathologies,[0],[0]
after fine-tuning.,4.2 Regularization Mitigates Pathologies,[0],[0]
The average length of reduced examples also increases across all tasks (Reduced length in Table 2).,4.2 Regularization Mitigates Pathologies,[0],[0]
"To verify that model overconfidence is indeed mitigated— that the reduced examples are less “rubbish” compared to before fine-tuning—we repeat the human experiments from Section 2.3.
",4.2 Regularization Mitigates Pathologies,[0],[0]
Human accuracy increases across all three tasks (Table 3).,4.2 Regularization Mitigates Pathologies,[0],[0]
"We also repeat the vs. Random experiment: we re-generate the random examples to match the lengths of the new reduced examples from input reduction, and find humans now prefer the reduced examples to random ones.",4.2 Regularization Mitigates Pathologies,[0],[0]
"The increase in both human performance and preference suggests that the reduced examples are more reasonable; model pathologies have been mitigated.
",4.2 Regularization Mitigates Pathologies,[0],[0]
"While these results are promising, it is not clear whether our input reduction method is necessary to achieve them.",4.2 Regularization Mitigates Pathologies,[0],[0]
"To provide a baseline, we finetune models using inputs randomly reduced to the same lengths as the ones generated by input reduction.",4.2 Regularization Mitigates Pathologies,[0],[0]
This baseline improves neither the model accuracy on regular examples nor interpretability under input reduction (judged by lengths of reduced examples).,4.2 Regularization Mitigates Pathologies,[0],[0]
Input reduction is effective in generating negative examples to counter model overconfidence.,4.2 Regularization Mitigates Pathologies,[0],[0]
"Rubbish examples have been studied in the image domain (Goodfellow et al., 2015; Nguyen et al., 2015), but to our knowledge not for NLP.",5 Discussion,[0],[0]
Our input reduction process gradually transforms a valid input into a rubbish example.,5 Discussion,[0],[0]
"We can often determine which word’s removal causes the transition to occur—for example, removing “Broncos” in Figure 5.",5 Discussion,[0],[0]
"These rubbish examples are particularly interesting, as they are also adversarial: the difference from a valid example is small, unlike image rubbish examples generated from pure noise which are far outside the training data distribution.
",5 Discussion,[0],[0]
"The robustness of NLP models has been studied extensively (Papernot et al., 2016; Jia and Liang, 2017; Iyyer et al., 2018; Ribeiro et al., 2018), and most studies define adversarial examples similar to the image domain: small perturbations to the input lead to large changes in the output.",5 Discussion,[0],[0]
"HotFlip (Ebrahimi et al., 2017) uses a gradient-based approach, similar to image adversarial examples, to flip the model prediction by perturbing a few characters or words.",5 Discussion,[0],[0]
"Our work and Belinkov and Bisk (2018) both identify cases where noisy
user inputs become adversarial by accident: common misspellings break neural machine translation models; we show that incomplete user input can lead to unreasonably high model confidence.
",5 Discussion,[0],[0]
Other failures of interpretation methods have been explored in the image domain.,5 Discussion,[0],[0]
"The sensitivity issue of gradient-based interpretation methods, similar to our shifting heatmaps, are observed by Ghorbani et al. (2017) and Kindermans et al. (2017).",5 Discussion,[0],[0]
They show that various forms of input perturbation—from adversarial changes to simple constant shifts in the image input—cause significant changes in the interpretation.,5 Discussion,[0],[0]
"Ghorbani et al. (2017) make a similar observation about secondorder sensitivity, that “the fragility of interpretation is orthogonal to fragility of the prediction”.
",5 Discussion,[0],[0]
Previous work studies biases in the annotation process that lead to datasets easier than desired or expected which eventually induce pathological models.,5 Discussion,[0],[0]
We attribute our observed pathologies primarily to the lack of accurate uncertainty estimates in neural models trained with maximum likelihood.,5 Discussion,[0],[0]
"SNLI hypotheses contain artifacts that allow training a model without the premises (Gururangan et al., 2018); we apply input reduction at test time to the hypothesis.",5 Discussion,[0],[0]
"Similarly, VQA images are surprisingly unimportant for training a model; we reduce the question.",5 Discussion,[0],[0]
"The recent SQUAD 2.0 (Rajpurkar et al., 2018) augments the original reading comprehension task with an uncertainty modeling requirement, the goal being to make the task more realistic and challenging.
",5 Discussion,[0],[0]
Section 3.1 explains the pathologies from the overconfidence perspective.,5 Discussion,[0],[0]
"One explanation for overconfidence is overfitting: Guo et al. (2017) show that, late in maximum likelihood training,
the model learns to minimize loss by outputting low-entropy distributions without improving validation accuracy.",5 Discussion,[0],[0]
"To examine if overfitting can explain the input reduction results, we run input reduction using DRQA model checkpoints from every training epoch.",5 Discussion,[0],[0]
"Input reduction still achieves similar results on earlier checkpoints, suggesting that better convergence in maximum likelihood training cannot fix the issues by itself—we need new training objectives with uncertainty estimation in mind.",5 Discussion,[0],[0]
We use the reduced examples generated by input reduction to regularize the model and improve its interpretability.,5.1 Methods for Mitigating Pathologies,[0],[0]
"This resembles adversarial training (Goodfellow et al., 2015), where adversarial examples are added to the training set to improve model robustness.",5.1 Methods for Mitigating Pathologies,[0],[0]
"The objectives are different: entropy regularization encourages high uncertainty on rubbish examples, while adversarial training makes the model less sensitive to adversarial perturbations.
",5.1 Methods for Mitigating Pathologies,[0],[0]
Pereyra et al. (2017) apply entropy regularization on regular examples from the start of training to improve model generalization.,5.1 Methods for Mitigating Pathologies,[0],[0]
"A similar method is label smoothing (Szegedy et al., 2016).",5.1 Methods for Mitigating Pathologies,[0],[0]
"In comparison, we fine-tune a model with entropy regularization on the reduced examples for better uncertainty estimates and interpretations.
",5.1 Methods for Mitigating Pathologies,[0],[0]
"To mitigate overconfidence, Guo et al. (2017) propose post-hoc fine-tuning a model’s confidence with Platt scaling.",5.1 Methods for Mitigating Pathologies,[0],[0]
This method adjusts the softmax function’s temperature parameter using a small held-out dataset to align confidence with accuracy.,5.1 Methods for Mitigating Pathologies,[0],[0]
"However, because the output is calibrated using the entire confidence distribution, not individual values, this does not reduce overconfidence on specific inputs, such as the reduced examples.",5.1 Methods for Mitigating Pathologies,[0],[0]
"To highlight the erratic model predictions on short examples and provide a more intuitive demonstration, we present paired-input tasks.",5.2 Generalizability of Findings,[0],[0]
"On these tasks, the short lengths of reduced questions and hypotheses obviously contradict the necessary number of words for a human prediction (further supported by our human studies).",5.2 Generalizability of Findings,[0],[0]
"We also apply input reduction to single-input tasks including sentiment analysis (Maas et al., 2011) and Quizbowl (BoydGraber et al., 2012), achieving similar results.
",5.2 Generalizability of Findings,[0],[0]
"Interestingly, the reduced examples transfer to other architectures.",5.2 Generalizability of Findings,[0],[0]
"In particular, when we feed fifty reduced SNLI inputs from each class—generated with the BIMPM model (Wang et al., 2017)—through the Decomposable Attention Model (Parikh et al., 2016),2 the same prediction is triggered 81.3% of the time.",5.2 Generalizability of Findings,[0],[0]
"We introduce input reduction, a process that iteratively removes unimportant words from an input while maintaining a model’s prediction.",6 Conclusion,[0],[0]
"Combined with gradient-based importance estimates often used for interpretations, we expose pathological behaviors of neural models.",6 Conclusion,[0],[0]
"Without lowering model confidence on its original prediction, an input sentence can be reduced to the point where it appears nonsensical, often consisting of one or two words.",6 Conclusion,[0],[0]
"Human accuracy degrades when shown the reduced examples instead of the original, in contrast to neural models which maintain their original predictions.
",6 Conclusion,[0],[0]
We explain these pathologies with known issues of neural models: overconfidence and sensitivity to small input changes.,6 Conclusion,[0],[0]
The nonsensical reduced examples are caused by inaccurate uncertainty estimates—the model is not able to lower its confidence on inputs that do not belong to any label.,6 Conclusion,[0],[0]
"The second-order sensitivity is another issue why gradient-based interpretation methods may fail to align with human perception: a small change in the input can cause, at the same time, a minor change in the prediction but a large change in the interpretation.",6 Conclusion,[0],[0]
Input reduction perturbs the input multiple times and can expose deeper issues of model overconfidence and oversensitivity that other methods cannot.,6 Conclusion,[0],[0]
"Therefore, it can be used to stress test the interpretability of a model.
",6 Conclusion,[0],[0]
"Finally, we fine-tune the models by maximizing entropy on reduced examples to mitigate the deficiencies.",6 Conclusion,[0],[0]
"This improves interpretability without sacrificing model accuracy on regular examples.
",6 Conclusion,[0],[0]
"To properly interpret neural models, it is important to understand their fundamental characteristics: the nature of their decision surfaces, robustness against adversaries, and limitations of their training objectives.",6 Conclusion,[0],[0]
We explain fundamental difficulties of interpretation due to pathologies in neural models trained with maximum likelihood.,6 Conclusion,[0],[0]
"Our
2http://demo.allennlp.org/ textual-entailment
work suggests several future directions to improve interpretability: more thorough evaluation of interpretation methods, better uncertainty and confidence estimates, and interpretation beyond bagof-word heatmap.",6 Conclusion,[0],[0]
Feng was supported under subcontract to Raytheon BBN Technologies by DARPA award HR0011-15-C-0113.,Acknowledgments,[0],[0]
JBG is supported by NSF Grant IIS1652666.,Acknowledgments,[0],[0]
"Any opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the view of the sponsor.",Acknowledgments,[0],[0]
"The authors would like to thank Hal Daumé III, Alexander M. Rush, Nicolas Papernot, members of the CLIP lab at the University of Maryland, and the anonymous reviewers for their feedback.",Acknowledgments,[0],[0]
"One way to interpret neural model predictions is to highlight the most important input features—for example, a heatmap visualization over the words in an input sentence.",abstractText,[0],[0]
"In existing interpretation methods for NLP, a word’s importance is determined by either input perturbation—measuring the decrease in model confidence when that word is removed—or by the gradient with respect to that word.",abstractText,[0],[0]
"To understand the limitations of these methods, we use input reduction, which iteratively removes the least important word from the input.",abstractText,[0],[0]
This exposes pathological behaviors of neural models: the remaining words appear nonsensical to humans and are not the ones determined as important by interpretation methods.,abstractText,[0],[0]
"As we confirm with human experiments, the reduced examples lack information to support the prediction of any label, but models still make the same predictions with high confidence.",abstractText,[0],[0]
"To explain these counterintuitive results, we draw connections to adversarial examples and confidence calibration: pathological behaviors reveal difficulties in interpreting neural models trained with maximum likelihood.",abstractText,[0],[0]
"To mitigate their deficiencies, we fine-tune the models by encouraging high entropy outputs on reduced examples.",abstractText,[0],[0]
Fine-tuned models become more interpretable under input reduction without accuracy loss on regular examples.,abstractText,[0],[0]
Pathologies of Neural Models Make Interpretations Difficult,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1805–1816, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
"Recent progress in NLP has given rise to the field of personality profiling - automated classification of personality traits based on written, verbal and multimodal behavior of an individual.",1 Introduction,[0],[0]
"This research builds upon findings from classical personality psychology and has applications in a wide range of areas from medicine (suicide prevention) across security (forensics, paedophile detection, cyberbullying) to marketing and sales (recommendation systems, target group profiles).",1 Introduction,[0],[0]
"The gold standard labels for an objective evaluation of personality are mostly obtained by means of personality tests of the Five Factor Model (FFM) (McCrae and Costa, 1987; Goldberg, 1990), which is wellknown and widely accepted in psychology and other research fields.",1 Introduction,[0],[0]
"The FFM defines personality
along five bipolar scales: Extraversion (sociable vs. reserved), Emotional stability (secure vs. neurotic), Agreeableness (friendly vs. unsympathic), Conscientiousness (organized vs. careless) and Openness to experience (insightful vs. unimaginative).",1 Introduction,[0],[0]
"Psychologists have shown that these five personality traits are stable across individual lifespan, demographical and cultural differences (John and Srivastava, 1999) and affect many life aspects.",1 Introduction,[0],[0]
"(Terracciano et al., 2008; Rentfrow et al., 2011).
",1 Introduction,[0],[0]
"It has been shown that the personality traits of readers impact their literature preferences (Tirre and Dixit, 1995; Mar et al., 2009).",1 Introduction,[0],[0]
"Psychology researchers also found that perceived similarity is predictive of interpersonal attraction (Montoya et al., 2008; Byrne, 1961; Chartrand and Bargh, 1999).",1 Introduction,[0],[0]
"More explicitly, recent research (Kaufman and Libby, 2012) shows that readers of a narrative develop more favorable attitudes and less stereotype application towards a character, if his difference (e.g. racial) is revealed only later in the story.",1 Introduction,[0],[0]
We therefore hypothesize that readers might have a preference for reading novels depicting fictional characters that are similar to themselves.,1 Introduction,[0],[0]
Finding a direct link between reader’s and protagonist’s personality traits would advance the development of content-based recommendation systems.,1 Introduction,[0],[0]
"As a first step to explore this hypothesis further, it needs to be determined if we are able to construct a personality profile of a fictional character in a similar way as it is done for humans, and which aspects of personality profiling can be exploited to automatize such procedure.
",1 Introduction,[0.9609627723879526],"['To alleviate this problem, we presented the original personas we collected to a new set of crowdworkers and asked them to rewrite the sentences so that a new sentence is about “a related characteristic that the same person may have”, hence the revisions could be rephrases, generalizations or specializations.']"
"In this paper, we open this research topic by presenting a novel collaboratively built dataset of fictional character personality in Section 3, which we make available on our website.1 Framing the personality prediction as a text classification task, we incorporate features of both lexical-
1https://www.ukp.tu-darmstadt.de/data/ personality-profiling/
1805
resource-based and vector space semantics, including WordNet and VerbNet sense-level information and vectorial word representations.",1 Introduction,[0],[0]
"We evaluate three machine learning models based on the speech (Section 4), actions (Section 5) and predicatives (Section 6) of the protagonists, and show that especially on the direct speech and action data the lexical-semantic features significantly outperform the baselines.",1 Introduction,[0],[0]
Qualitative analysis reveals that the most predictive features correspond to reported findings in psychology and NLP.,1 Introduction,[0],[0]
"Research in the the area of content-based recommendation systems have shown that incorporating semantic information is valuable for the user and leads to measurable improvements (Passant, 2010; Di Noia et al., 2012; Heitmann and Hayes, 2010).",2 Related work,[0],[0]
De Clercq et al. (2014) incorporated semantic frames from FrameNet into the recommendation system for books.,2 Related work,[0],[0]
They represent the plot of each book with a sequence of ca.,2 Related work,[0],[0]
200 semantic frames,2 Related work,[0],[0]
and has shown that the frame information (such as Killing - Revenge - Death) outperforms the bag-of-words approach.,2 Related work,[0],[0]
Recent NLP experiments begin to reveal the importance of entitycentric models in a variety of tasks.,2 Related work,[0],[0]
"Chambers (2013) show improvement in event schema induction by learning entity-centric rules (e.g., a victim is likely to be a person).",2 Related work,[0],[0]
"Bamman et al. (2014) and Smith et al. (2013) present latent variable models for unsupervised learning of latent character types in movie plot summaries and in English novels, taking authorial style into account.",2 Related work,[0],[0]
"However, even the state-of-the-art NLP work rather describes personas of fictional characters by their role in the story - e.g., action hero, valley girl, best friend, villain etc. - or by their relations to other characters, such as mother or daughter (Elson et al., 2010; Kokkinakis and Malm, 2011), rather than by their inner preferences and motivations.",2 Related work,[0],[0]
It is important to note here that determining a personality of a character is a very different task from determining its role in the story.,2 Related work,[0],[0]
"Psychological understanding of personality, in contrast to role attribution requires a certain detached objectivity - even outright villains may have traits considered desirable in real life.",2 Related work,[0],[0]
"For example, the devil has in many tales a very high aspiration level, appearing highly conscientious and agreeable.",2 Related work,[0],[0]
"We hypothesize that these deeper personality aspects are
those which drive reader’s affiliation to the character, thus deserve to be examined closer.
",2 Related work,[0],[0]
"Also literary scholars formulate ad hoc personality descriptions for their experiments, for example to test hypotheses from evolutionary psychology (Johnson et al., 2011) or examine fictional portrayals of physicists (Dotson, 2009).",2 Related work,[0],[0]
"These descriptions are usually adjusted to the experiment focus (e.g. emotions, relationships, ambitions).",2 Related work,[0],[0]
"As McCrae et al. () point out, a standard set of personality traits, that encompass the full range of characteristics found in all characters in literature (p.77), is needed for a better comparison.
",2 Related work,[0],[0]
Hence we base our present study primarily on the previous NLP research on personality prediction of human individuals.,2 Related work,[0],[0]
"Correlations between lexical and stylistic aspects of text and the five FFM personality traits of the author have been found in numerous experiments, with extraversion receiving the most attention (Pennebaker and King, 1999; Dewaele and Furnham, 1999; Gill and Oberlander, 2002; Mehl et al., 2006; Aran and Gatica-Perez, 2013; Lepri et al., 2010).",2 Related work,[0],[0]
"The LIWC lexicon (Pennebaker et al., 2001) established its position as a powerful mean of such analysis.
",2 Related work,[0],[0]
"The first machine learning experiments in this area were conducted by Argamon et al. (2005), Oberlander and Nowson (2006) and Mairesse et al. (2007).",2 Related work,[0],[0]
"Researchers predicted the five personality traits of the authors of stream-ofconscientiousness essays, blog posts and recorded conversation snippets.",2 Related work,[0],[0]
"Given balanced data sets, Mairesse et al. (2007) report binary classification accuracy of 50-56% on extraversion in text and 47-57% in speech, using word ngrams, LIWC, MRC psycholinguistic database (Coltheart, 1981) and prosodic features.",2 Related work,[0],[0]
Additional improvement is reported when the extraversion was labeled by external judges rather than by self-testing.,2 Related work,[0],[0]
"Extended studies on larger datasets achieve accuracies around 55% (Nowson, 2007; Estival et al., 2007).",2 Related work,[0],[0]
"More recent work in this area focuses on the personality prediction in social networks (Kosinski et al., 2013; Kosinski et al., 2014) and multimodal personality prediction (Biel and Gatica-Perez, 2013; Aran and Gatica-Perez, 2013).",2 Related work,[0],[0]
"These trends emphasized the correlation of network features and audiovisual features with extraversion, giving rise to the Workshop on Computational Personality Recognition (for an overview see (Celli et al., 2013; Celli et al., 2014).",2 Related work,[0],[0]
"Traditionally, the gold standard for this supervised classification task is obtained by the means of personality questionnaires, used for the Five-Factor Model, taken by each of the individuals assessed.",3 Data set construction,[0],[0]
This poses a challenge for fictional characters.,3 Data set construction,[0],[0]
"However, strong correlations have been found between the self-reported and perceived personality traits (Mehl et al., 2006).",3 Data set construction,[0],[0]
Our gold standard benefits from the fact that readers enjoy discussing the personality of their favourite book character online.,3 Data set construction,[0],[0]
"A popular layman instrument for personality classification is the Myers-Brigggs Type Indicator (Myers et al., 1985), shortly MBTI, which sorts personal preferences into four opposite pairs, or dichotomies, such as Thinking vs. Feeling or Judging vs. Perceiving.",3 Data set construction,[0],[0]
"While the MBTI validity has been questioned by the research community (Pittenger, 2005), the Extraversion scale is showing rather strong validity and correlation to similar trait in the Five-Factor Model (McCrae and Costa, 1989; MacDonald et al., 1994).",3 Data set construction,[0],[0]
"Our study hence focuses on the Extraversion scale.
",3 Data set construction,[0],[0]
"Our data was collected from the collaboratively constructed Personality Databank2 where the readers can vote if a book character is, among other aspects, introverted or extraverted.",3 Data set construction,[0],[0]
"While the readers used codes based on the MBTI typology, they did not apply the MBTI assessment strategies.",3 Data set construction,[0],[0]
There was no explicit annotation guideline and the interpretation was left to readers’ intuition and knowledge.3,3 Data set construction,[0],[0]
This approach of gold standard collection has several obvious drawbacks.,3 Data set construction,[0],[0]
"First, the question is posed as dichotomic, while in reality the extraversion is a normally distributed trait in human population (Goldberg, 1990).",3 Data set construction,[0],[0]
"Second, users can view the vote of previous participants, which may influence their decision.",3 Data set construction,[0],[0]
"While we address both of these issues in our ongoing data collection project based on the Five-Factor Model, we consider them acceptable for this study due to the exploratory character of our pilot research.
",3 Data set construction,[0],[0]
"We have collected extraversion ratings for 298 book characters, of which 129 (43%) are rather extraverted and 166 (56%) rather introverted.",3 Data set construction,[0],[0]
"Rated
2http://www.mbti-databank.com/ 3MBTI defines extraversion as “getting energy from active involvement in events, having a lot of different activities, enjoying being around people.”",3 Data set construction,[0],[0]
"In the NEO Five-Factor Inventory (Costa and McCrae, 1992), underlying facets of extraversion are warmth, gregariousness, assertiveness, activity, excitement seeking and positive emotion.
",3 Data set construction,[0],[0]
"characters come from a wide range of novels that the online users are familiar with, often covering classical literature which is part of the high school syllabus, as well as the most popular modern fiction, such as the Harry Potter series, Twilight, Star Wars or A Game of Thrones.",3 Data set construction,[0],[0]
A sample of the most rated introverts and extraverts is given in table 1.,3 Data set construction,[0],[0]
The rating distribution in our data is strongly Ushaped.,3 Data set construction,[0],[0]
"The percentage agreement of voters in our data is 84.9%, calculated as:
P = 1 N N∑ i=1",3 Data set construction,[0],[0]
k∑ j=1 nij(nij,3 Data set construction,[0],[0]
"− 1) n(n− 1)
where k = 2 (introvert, extravert), N is the number of book characters and n the number of votes per character.",3 Data set construction,[0],[0]
Voters on the website were anonymous and cannot be uniquely identified for additional corrections.,3 Data set construction,[0],[0]
"There is no correlation between the extraversion and the gender of the character.
",3 Data set construction,[0],[0]
Our set of English e-books covered 220 of the characters from our gold standard.,3 Data set construction,[0],[0]
"We have built three systems to assess the following:
1.",3 Data set construction,[0],[0]
Direct speech: Does the style and content of character’s utterances predict his extraversion in a similar way as it was shown for living individuals?,3 Data set construction,[0],[0]
"2. Actions: Is the behavior, of which a character is an agent, predictive for extraversion?",3 Data set construction,[0],[0]
3.,3 Data set construction,[0],[0]
"Predicatives and adverbs: Are the explicit (John was an exhibitionist) or implicit (John shouted abruptly) descriptions of the character in the book predictive for extraversion?
",3 Data set construction,[0],[0]
In the next three sections we present the experimental settings and results for each of the systems.,3 Data set construction,[0],[0]
"The system for the direct speech resembles the most to the previous systems developed for author personality profiling, e.g. on stream of consciousness essays (Mairesse et al., 2007) or social media posts (Celli et al., 2013) and therefore provides the best opportunity for comparison between human individuals and fictional characters.",4 Direct speech of fictional characters,[0],[0]
"On top of the comparison to previous research, we exploit the sense links between WordNet and VerbNet to extract additional features - an approach which is novel for this type of task.",4 Direct speech of fictional characters,[0],[0]
"We process the book text using freely available components of the DKPro framework (Gurevych et al., 2007).",4.1 Extraction and assignment of speech,[0],[0]
The most challenging task in building the direct speech data set is assigning to the direct speech utterance the correct speaker.,4.1 Extraction and assignment of speech,[0],[0]
"We benefit from the epub format of the e-books which defines a paragraph structure in such a way, that only the indirect speech chunk immediately surrounding the direct speech can be considered:
<p> John turned to Harry.",4.1 Extraction and assignment of speech,[0],[0]
"""Let’s go,"" he said.</p>
Given the large amount of text available in the books we focus on precision and discard all utterances with no explicit speaker (i.e., 30-70% of the utterances, dependent on the book), as the performance of current systems on such utterance types is still fairly low (O’Keefe et al., 2012; He et al., 2013; Iosif and Mishra, 2014).",4.1 Extraction and assignment of speech,[0],[0]
"Similarly, conventional coreference resolution systems did not perform well on this type of data and were therefore not used in the final setup.",4.1 Extraction and assignment of speech,[0],[0]
"We adapt the Stanford Named Entity Recognizer(Finkel et al., 2005) to consider titles (Mr., Mrs., Sir...) as a part of the name and to treat the first person I as a named entity.",4.1 Extraction and assignment of speech,[0],[0]
"However, identifying only the named entity PERSON in this way is not sufficient.",4.1 Extraction and assignment of speech,[0],[0]
"On our evaluation sample consisting of A Game of Thrones and Pride and Prejudice books (the former annotated by us, the latter by He et al. (2013)), 20% of utterances with explicit named speaker were not recognized.",4.1 Extraction and assignment of speech,[0],[0]
"Of those correctly identified as a Person in the adjacent indirect speech, 17% were not the speakers.",4.1 Extraction and assignment of speech,[0],[0]
"Therefore we implemented a
custom heuristics (Algorithm 1), which additionally benefits from the WordNet semantic classes of verbs, enriching the speaker detection by grabbing the nouns .",4.1 Extraction and assignment of speech,[0],[0]
"With this method we retrieve 89% of known speakers, of which 92% is assigned correctly.",4.1 Extraction and assignment of speech,[0],[0]
"Retrieved names are grouped based on string overlap (e.g. Ser Jaime and Jaime Lannister), excluding the match on last name, and corrected for non-obvious groupings (such as Margaret and Peggy).",4.1 Extraction and assignment of speech,[0],[0]
"Algorithm 1 Assign speaker 1: nsubj← subjects in adjacent indirect speech 2: if count(nsubj(i) = PERSON) = 1 then speaker ←
nsubj 3: else if count(nsubj(i) = PERSON) ≥ 1 then
speaker ← the nearest one to directSpeech 4: else if directSpeech preceded by
VERB.COMMUNICATION then speaker ← the preceding noun(s) 5: else if directSpeech followed by VERB.COMMUNICATION then speaker ← the following noun(s) 6: else if directSpeech followed by gap & VERB.COMMUNICATION then speaker ← the noun(s) in gap 7: else if directSpeech preceded by gap & VERB.COMMUNICATION then speaker ← the noun(s) in gap return speaker
Our experimental data consists of usable direct speech sets of 175 characters - 80 extraverts (E) and 95 introverts (I) - containing 289 274 words in 21 857 utterances (on average 111 utterances for E and 136 for I, as I are often central in books).4",4.1 Extraction and assignment of speech,[0],[0]
All speech utterances of one book character are represented as one instance in our system.,4.2 Classification approach for direct speech,[0],[0]
"We use the leave-one-out classification setup due to the relatively small dataset size, using the support vector machines (SVM-SMO) classifier, which performs well on comparable tasks (Celli et al., 2013).",4.2 Classification approach for direct speech,[0],[0]
"The classification is performed through the DKPro TC Framework (Daxenberger et al., 2014).
",4.2 Classification approach for direct speech,[0],[0]
"Lexical features As a bottom-up approach we use the 1000 most frequent word uni-, bi- and trigrams, 1000 dependency word pairs, 1000 character trigrams and 500 most frequent verbs, adverbs, adjectives and interjections as binary features.
",4.2 Classification approach for direct speech,[0],[0]
"Semantic features Since the top-down approach, i.e. not focusing on individual words, has
4The data set size is comparable to ongoing personality profiling challenges - see http://pan.webis.de
been found more suitable for the personality profiling task on smaller data sets (Celli et al., 2013), we aim on capturing additional phenomena on a higher level of abstraction.",4.2 Classification approach for direct speech,[0],[0]
The main part of our features is extracted on sense level.,4.2 Classification approach for direct speech,[0],[0]
"We use the most frequent sense of WordNet (Miller, 1995) to annotate all verbs in the direct speech (a simple but well performing approach for books).",4.2 Classification approach for direct speech,[0],[0]
"We then label the disambiguated verbs with their semantic field given in WordNet (WordNet defines 14 semantic classes of verbs which group verbs by their semantic field) and we measure frequency and occurence of each of these classes (e.g. cognition, communication, motion, perception)5.",4.2 Classification approach for direct speech,[0],[0]
"Additionally, we use the lexical-semantic resource UBY (Gurevych et al., 2012) to access the WordNet and VerbNet information, and to exploit the VerbNet sense-level links which connects WordNet senses with the corresponding 273 main VerbNet classes (Kipper-Schuler, 2005).",4.2 Classification approach for direct speech,[0],[0]
"These are more fine-grained (e.g. pay, conspire, neglect, discover) than the WordNet semantic fields.",4.2 Classification approach for direct speech,[0],[0]
"WordNet covered 90% and VerbNet 86% of all the verb occurences.
",4.2 Classification approach for direct speech,[0],[0]
"On word level, we extract 81 additional features using the Linguistic Inquiry and Word Count (LIWC) tools (Pennebaker et al., 2001), which consists of lexicons related to psychological processes (cognitive, perceptual, social, biological, affective) and personal concerns (achievement, religion, death...) and other categories such as fillers, disfluencies or swear words6.",4.2 Classification approach for direct speech,[0],[0]
"Additionally, since emotion detection has been found predictive in previous personality work (Mohammad and Kiritchenko, 2013), we measure overall positive and negative sentiment expressed per character, using SentiWordNet (Esuli and Sebastiani, 2006) and NRC Emotion Lexicon (Mohammad and Turney, 2010) for the word lookup, inverting sentiment scores for negated dependency sub-tree given by the Stanford Parser.
Stylistic features Features of this group capture the syntactic and stylistic properties of the utterances of a character, disregarding the content.",4.2 Classification approach for direct speech,[0],[0]
"Starting from the surfacial properties, we measure the sentence, utterance and word length, including the proportion of words shorter than 4 or longer than 6 letters, frequency of each punctuation mark,
5https://wordnet.princeton.edu/man/ lexnames.5WN.html
6For complete overview refer to www.liwc.net
and endings of each adjective as per Corney et al. (2002).",4.2 Classification approach for direct speech,[0],[0]
"On the syntax level we measure the frequency of each part of speech as well as the 500 most frequent part-of-speech bi-, tri- and quadrigrams, and the frequency of each dependency obtained from the Stanford Parser.",4.2 Classification approach for direct speech,[0],[0]
"We additionally capture the frequency of superlatives, comparatives and modal verbs, the proportion of verbs in present, past and future tense, and the formality of the language as per the part-of-speech-based formality coefficient (Heylighen and Dewaele, 2002), and measure the average depth of the parse trees.
",4.2 Classification approach for direct speech,[0],[0]
"Word embeddings as features Since vector space semantics has been beneficial for predicting author’s personality in previous work (Neuman and Cohen, 2014), we use a pre-trained word vector model created by the GloVe algorithm (Pennington et al., 2014) on English Wikipedia.",4.2 Classification approach for direct speech,[0],[0]
GloVe employs a global log-bilinear regression model that combines the advantages of the global matrix factorization and local context window methods.,4.2 Classification approach for direct speech,[0],[0]
"We assign the resulting 300-dimensional vectors to the words in character’s direct speech, excluding stopwords, and calculate an average vector for each character.",4.2 Classification approach for direct speech,[0],[0]
"We calculate for each test character the cosine similarity to the mean vector of extravert, resp.",4.2 Classification approach for direct speech,[0],[0]
"introvert, in the training data, and to each character in the training set individually using the DL4J NLP package7.",4.2 Classification approach for direct speech,[0],[0]
We consider both the final scalar outcome and the difference of each of the individual vector dimensions as features.,4.2 Classification approach for direct speech,[0],[0]
"Table 2 shows the precision, recall, F1-score and accuracy for extraversion and introversion as a weighted average of the two class values.
",4.3 Classification results on direct speech,[0],[0]
"7http://deeplearning4j.org/
Similarly to previous research (Mairesse et al., 2007; Celli et al., 2013), the bottom-up word based approach is outperformed by top-down semantic approaches which employ a more abstract feature representation.",4.3 Classification results on direct speech,[0],[0]
"As in previous work, LIWC features exhibit good performance.",4.3 Classification results on direct speech,[0],[0]
"However, the highest performance is achieved employing the VerbNet verb classes with WordNet wordsense disambiguation.",4.3 Classification results on direct speech,[0],[0]
Also stylistic features contribute substantially to the classification despite the mixture of genres in our book corpus - especially frequencies of modal verbs and part-ofspeech ratios were particularly informative.,4.3 Classification results on direct speech,[0],[0]
"The most predictive features from each group are listed in Table 3 together with their correlation merit (Hall, 1999), and compared with previous work in Table 4.
",4.3 Classification results on direct speech,[0],[0]
"In accordance with the experiments of Pennebaker and King (1999), we observe more frequent exclusions (e.g. without, but), hedging and negation expressed by introverts, and inclusion (e.g. with, and) by extraverts.",4.3 Classification results on direct speech,[0],[0]
"Extraverts talk more in first person plural, use more back-channels and interjections, and talk more about aspects related to their body.",4.3 Classification results on direct speech,[0],[0]
"Introverts show more rationalization through insight words and more factual speech using less pronouns.
",4.3 Classification results on direct speech,[0],[0]
"Additionally, the semantic features in Table 3 confirm the broad psychological characteristics of both types in general, i.e., for introverts the rationalization, uncertainty and preference for individual or rather static activities, and for extraverts their spontaneity, talkativeness and preference for motion.",4.3 Classification results on direct speech,[0],[0]
"Furthermore, we observe certain directness in extraverts’ speech - note the predictive words fat and dirty and frequent descriptions of body functions.
",4.3 Classification results on direct speech,[0],[0]
Discussion Exploiting the links between lexicalsemantic resources (performing WordNet wordsense disambiguation and using VerbNet verb classes linked to the disambiguated senses) was particularly beneficial for this task.,4.3 Classification results on direct speech,[0],[0]
"WordNet semantic fields for verbs alone are too coarsegrained to capture the nuances in direct speech, and experiments with fine-grained VerbNet classes without WSD resulted in noisy labels.",4.3 Classification results on direct speech,[0],[0]
"We did not confirm the previously reported findings on emotional polarity - we observe that the genre of the books (e.g. love romance vs horror story) have blurred the subtle differences between individual characters, unfortunately the dataset size did not allow for genre distinctions.",4.3 Classification results on direct speech,[0],[0]
"Furthermore, a perceived extravert in our case can be a pure villain (Draco Malfoy, Joffrey Baratheon...) as well as a friendly companion (Gimli, Ron Weasley...), while the evil extravert types are possibly rarer in the experiments on human writing, or are more likely to fit under the MBTI definition of extraversion than FFM facets.",4.3 Classification results on direct speech,[0],[0]
"Another potential cause, based on the error analysis, is the different target of the same sentiment for extraverts and introverts.",4.3 Classification results on direct speech,[0],[0]
"For example, the ngram ”I fear” is highly predictive for an introvert in our data while extraverts would rather use formulations to imply that others should fear.",4.3 Classification results on direct speech,[0],[0]
"Similarly to Nowson et al. (2005), we did not find any difference in the formality measure of Heylighen and Dewaele (2002).",4.3 Classification results on direct speech,[0],[0]
"Neither we did in the complexity of sentences as per the parse tree depth
and sentence length.",4.3 Classification results on direct speech,[0],[0]
It is probable that these aspects were also impacted by our broad variety of author style (F. Dostoyevsky vs J. K. Rowling).,4.3 Classification results on direct speech,[0],[0]
"Our basic vector-based features carried no useful information in our case, in contrast to the personality research of Neuman and Cohen (2014).",4.3 Classification results on direct speech,[0],[0]
We observed that the factual content of the stories contributed to the character similarity measure more than the subtle personality differences.,4.3 Classification results on direct speech,[0],[0]
"While psycholinguists and consequenlty NLP researchers analyzed the relation between speech, resp.",5 Actions of fictional characters,[0],[0]
"writing, and personality of an individual, psychologists often evaluate extraversion through behavioral personality questionnaries (Costa and McCrae, 1992; Goldberg et al., 2006).",5 Actions of fictional characters,[0],[0]
We hypothesize that similar behavior shall be predictive for extraversion of fictional characters as perceived by the readers.,5 Actions of fictional characters,[0],[0]
"For our purpose we define actions as the subject, verb and context of a sentence, where the subject is a named entity Person and the context is either a direct object in relation dobj to the verb or a first child of the adjacent verb phrase in a parse tree.",5.1 Action extraction,[0],[0]
"After grouping the actions per character, the subject name is removed.",5.1 Action extraction,[0],[0]
"For example, a sample of actions of the character Eddard Stark of Game of Thrones would be: X paused a moment, X studied his face, X changed his mind, X unrolled the paper, X said etc., visualized in Figure 1.",5.1 Action extraction,[0],[0]
"We obtained 22 030 actions for 205 characters (102 E, 116 I), with on average 100 actions for E and 101 for I. Note that also actions for those characters who do not talk enough in the books (often first-person perspectives) could be used.",5.1 Action extraction,[0],[0]
In the system based on actions we use only a subset of the features described in 4.2.,5.2 Action classification setup,[0],[0]
From the lexical features we focus on the 500 most frequent verbs and dependency word pairs.,5.2 Action classification setup,[0],[0]
"Semantic features are used the same way as in 4.2, profiting from LIWC, WordNet, Verbnet and the sentiment lexicons.",5.2 Action classification setup,[0],[0]
Word embedding vectors for book characters are in this case computed by taking only the verbs into account rather than all content words.,5.2 Action classification setup,[0],[0]
"From the stylistic features we use the part-ofspeech bigrams and trigrams, verb modality and verb tense.",5.2 Action classification setup,[0],[0]
"Table 5 shows the performance of the classification models based on the protagonists’ actions, using different feature groups.",5.3 Classification results on actions,[0],[0]
"The overall performance is higher than for the direct speech model.
",5.3 Classification results on actions,[0],[0]
"Due to the lack of previous NLP experiments on this task, we compare our features to the actions measured in the International Personality Item Pool (Goldberg et al., 2006), frequently used personality assesment questionnaire (Table 6).
",5.3 Classification results on actions,[0],[0]
The most predictive features of this model capture the activity and excitement seeking facets of extraversion.,5.3 Classification results on actions,[0],[0]
"Stylistic features reflect the complexity difference of the verb phrases (John jumped vs. John thought about it), extraverts being characterized by plain verbs.",5.3 Classification results on actions,[0],[0]
Semantic features exhibit higher precision than stylistic ones.,5.3 Classification results on actions,[0],[0]
"Sense-linked semantic classes of VerbNet demonstrate the preference of extraverts for being active and expressing themselves - they jump, fight, shout, run in and run out, eat and drink, see and hear and get easily bored.",5.3 Classification results on actions,[0],[0]
"Extraverts in books also
often bring or hold something.",5.3 Classification results on actions,[0],[0]
"Introverts, on the other hand, seem to favor slow movements - while they are thinking, reflecting, creating, looking for explanations and find out solutions, they tend to lie down, sit or walk, eventually even sleep or snooze.",5.3 Classification results on actions,[0],[0]
"The uncertainty typical for introverts is also notable in their actions, as they often hope or wish for something they might like to do.",5.3 Classification results on actions,[0],[0]
"Additionally, semantic classes Social and Family, reported as correlated to extraversion by Pennebaker and King (1999) and not confirmed in our first model, became predictive in protaonists’ actions.",5.3 Classification results on actions,[0],[0]
"Also in this task, the VerbNet classes brought significant improvement in performance.",5.4 Discussion,[0],[0]
"The classification model based on actions outperforms not only the direct speech model, but also the state-of-the-art systems predicting authors’ extraversion from the stream-of-consciousness essays (Mairesse et al., 2007; Celli et al., 2013; Neuman and Cohen, 2014).",5.4 Discussion,[0],[0]
"While surely not directly comparable, this result hints to the fact that the personality is easier to detect from behavior than from person’s verbal expression.",5.4 Discussion,[0],[0]
"This would correspond to the findings of Mairesse et al. (2007), Biel and Gatica-Perez (2013) and Aran and Gatica-Perez (2013) on multimodal data sets.",5.4 Discussion,[0],[0]
Our third extraversion prediction system is subordinate to how fictional characters are described and to the manners in which they behave.,6 Predicatives of fictional characters,[0],[0]
We are not aware of a previous NLP work predicting extraversion using descriptive adjectives of the persons in question.,6 Predicatives of fictional characters,[0],[0]
We thus juxtapose the most predictive features of our system to the adjectival extraversion markers developed by Goldberg (1992).,6 Predicatives of fictional characters,[0],[0]
In this setup we extract predicatives of the named entities PERSON in the books - relations amod (angry John) and cop (John was smart).,6.1 Extraction of descriptive properties,[0],[0]
"As these explicit statements are very sparse in modern novels, we additionally include adverbial modifiers (advmod) related to person’s actions (John said angrily).",6.1 Extraction of descriptive properties,[0],[0]
"We extract data for 205 characters, with on average 43 words per character.",6.1 Extraction of descriptive properties,[0],[0]
"This system uses similar set of lexical, semantic and vectorial features similarly as in 5.2, this time with the focus on adjectives, nouns and adverbs instead of verbs.",6.2 Classification setup,[0],[0]
"Stylistic and VerbNet features are hence not included, word vectors are as in 4.2.",6.2 Classification setup,[0],[0]
Table 7 reports on the performance of individual feature groups.,6.3 Classification results on descriptions,[0],[0]
"With only few words per character semantic lexicons are less powerful than ngrams.
",6.3 Classification results on descriptions,[0],[0]
Table 8 displays the most predictive features in our system contrasted to the adjectival markers.,6.3 Classification results on descriptions,[0],[0]
All our systems had issues with characters rated by less than five readers and with protagonists with low agreement.,6.4 Discussion on errors,[0],[0]
"Other challenges arise from authorial style, age of the novel and speech individuality of characters (e.g. Yoda).",6.4 Discussion on errors,[0],[0]
"Varied length of information for different characters poses issues in measuring normally distributed features (e.g. ratio of jumping verbs), being in shorter texts less reliable.",6.4 Discussion on errors,[0],[0]
"Ongoing and future work on this task addresses the limitations of these initial experiments, especially the data set size and the gold standard quality.",6.4 Discussion on errors,[0],[0]
Extending the data will also enable us to examine different book genres as variables for the personality distribution and feature impact.,6.4 Discussion on errors,[0],[0]
"It will be worth examining the relations between characters, since we observed certain patterns in our data, such as the main introvert character supported by his best friend extravert.",6.4 Discussion on errors,[0],[0]
"Additionally, we want to verify if the system in Section 6 is overly optimistic due to the data size.",6.4 Discussion on errors,[0],[0]
"Automated personality profiling of fictional characters, based on rigorous models from personality psychology, has a potential to impact numerous domains.",7 Conclusion and future work,[0],[0]
We framed it as a text classification problem and presented a novel collaboratively built dataset of fictional personality.,7 Conclusion and future work,[0],[0]
"We incor-
porate features of both lexical resource-based and vectorial semantics, including WordNet and VerbNet sense-level information and vectorial word representations.",7 Conclusion and future work,[0],[0]
"In models based on the speech and actions of the protagonists, we demonstrated that the sense-linked lexical-semantic features significantly outperform the baselines.",7 Conclusion and future work,[0],[0]
The most predictive features correspond to the reported findings in personality psychology and NLP experiments on human personality.,7 Conclusion and future work,[0],[0]
"Our systems based on actions and appearance of characters demonstrate higher performance than systems based on direct speech, which is in accordance with recent research on personality in social networks (Kosinski et al., 2014; Biel and Gatica-Perez, 2013), revealing the importance of the metadata.",7 Conclusion and future work,[0],[0]
"We have shown that exploiting the links between lexical resources to leverage more accurate semantic information can be beneficial for this type of tasks, oriented to actions performed by the entity.",7 Conclusion and future work,[0],[0]
"However, the human annotator agreement in our task stays high above the performance achieved.",7 Conclusion and future work,[0],[0]
"Considering that most of the sucessful novels were produced as movies, we cannot exclude that our annotators based their decision on the multimodal representation of the protagonists.",7 Conclusion and future work,[0],[0]
"In the future we aim on collecting a more detail and rigorous gold standard through gamification and expanding our work on all five personality traits from the FiveFactor Model and their facets, and ultimately extend our system to a semi-supervised model dealing with notably larger amount of data.",7 Conclusion and future work,[0.9527377492217823],"['Firstly, we see a difference in fluency, engagingness and consistency between all PERSONACHAT models and the models trained on OpenSubtitles and Twitter.']"
"We also plan to examine closer the differences between perceived human and fictional personality, and the relationship between the personality of the reader and the characters.",7 Conclusion and future work,[0],[0]
This work has been supported by the Volkswagen Foundation as part of the Lichtenberg Professorship Program under grant No. I/82806 and by the German Research Foundation under grant No. GU 798/14-1.,Acknowledgments,[0],[0]
Additional support was provided by the German Federal Ministry of Education and Research (BMBF) as a part of the Software Campus program under the promotional reference 01-S12054 and by the German Institute for Educational Research (DIPF).,Acknowledgments,[0],[0]
"We also warmly thank Holtzbrinck Digital GmbH for providing a substantial part of the e-book resources, and the EMNLP reviewers for their helpful comments.",Acknowledgments,[0],[0]
This study focuses on personality prediction of protagonists in novels based on the Five-Factor Model of personality.,abstractText,[0],[0]
We present and publish a novel collaboratively built dataset of fictional character personality and design our task as a text classification problem.,abstractText,[0],[0]
"We incorporate a range of semantic features, including WordNet and VerbNet sense-level information and word vector representations.",abstractText,[0],[0]
"We evaluate three machine learning models based on the speech, actions and predicatives of the main characters, and show that especially the lexical-semantic features significantly outperform the baselines.",abstractText,[0],[0]
The most predictive features correspond to reported findings in personality psychology.,abstractText,[0],[0]
Personality Profiling of Fictional Characters using Sense-Level Links between Lexical Resources,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 700–705 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
700",text,[0],[0]
Query auto-completion (QAC) is a feature used by search engines that provides a list of suggested queries for the user as they are typing.,1 Introduction,[0],[0]
"For instance, if the user types the prefix “mete” then the system might suggest “meters” or “meteorite” as completions.",1 Introduction,[0],[0]
"This feature can save the user time and reduce cognitive load (Cai et al., 2016).
",1 Introduction,[0],[0]
"Most approaches to QAC are extensions of the Most Popular Completion (MPC) algorithm (BarYossef and Kraus, 2011).",1 Introduction,[0],[0]
MPC suggests completions based on the most popular queries in the training data that match the specified prefix.,1 Introduction,[0],[0]
"One way to improve MPC is to consider additional signals such as temporal information (Shokouhi and Radinsky, 2012; Whiting and Jose, 2014) or information gleaned from a users’ past queries (Shokouhi, 2013).",1 Introduction,[0],[0]
"This paper deals with the latter of those two signals, i.e. personalization.",1 Introduction,[0],[0]
"Personalization relies on the fact that query likelihoods are drastically different among different people depending on their needs and interests.
",1 Introduction,[0],[0]
"Recently, Park and Chiba (2017) suggested a significantly different approach to QAC.",1 Introduction,[0],[0]
"In their
work, completions are generated from a character LSTM language model instead of by ranking completions retrieved from a database, as in the MPC algorithm.",1 Introduction,[0],[0]
"This approach is able to complete queries whose prefixes were not seen during training and has significant memory savings over having to store a large query database.
",1 Introduction,[0],[0]
"Building on this work, we consider the task of personalized QAC, advancing current methods by combining the obvious advantages of personalization with the effectiveness of a language model in handling rare and previously unseen prefixes.",1 Introduction,[0],[0]
The model must learn how to extract information from a user’s past queries and use it to adapt the generative model for that person’s future queries.,1 Introduction,[0],[0]
"To do this, we leverage recent advances in contextadaptive neural language modeling.",1 Introduction,[0],[0]
"In particular, we make use of the recently introduced FactorCell model that uses an embedding vector to additively transform the weights of the language model’s recurrent layer with a low-rank matrix (Jaech and Ostendorf, 2017).",1 Introduction,[0],[0]
"By allowing a greater fraction of the weights to change during personalization, the FactorCell model has advantages over the traditional approach to adaptation of concatenating a context vector to the input of the LSTM (Mikolov and Zweig, 2012).
",1 Introduction,[0],[0]
"Table 1 provides an anecdotal example from
the trained FactorCell model to demonstrate the intended behavior.",1 Introduction,[0],[0]
The table shows the top five completions for the prefix “ba” in a cold start scenario and again after the user has completed five sports related queries.,1 Introduction,[0],[0]
"In the warm start scenario, the “baby names” and “babiesrus” completions no longer appear in the top five and have been replaced with “basketball” and “baseball”.
",1 Introduction,[0],[0]
The novel aspects of this work are the application of an adaptive language model to the task of QAC personalization and the demonstration of how RNN language models can be adapted to contexts (users) not seen during training.,1 Introduction,[0],[0]
An additional contribution is showing that a richer adaptation framework gives added gains with added data.,1 Introduction,[0],[0]
"Adaptation depends on learning an embedding for each user, which we discuss in Section 2.1, and then using that embedding to adjust the weights of the recurrent layer, discussed in Section 2.2.",2 Model,[0],[0]
"During training, we learn an embedding for each of the users.",2.1 Learning User Embeddings,[0],[0]
We think of these embeddings as holding latent demographic factors for each user.,2.1 Learning User Embeddings,[0],[0]
"Users who have less than 15 queries in the training data (around half the users but less than 13% of the queries) are grouped together as a single entity, user1, leaving k users.",2.1 Learning User Embeddings,[0],[0]
"The user embeddings matrix Uk×m, wherem is the user embedding size, is learned via back-propagation as part of the end-toend model.",2.1 Learning User Embeddings,[0],[0]
"The embedding for an individual user is the ith row of U and is denoted by ui.
",2.1 Learning User Embeddings,[0],[0]
It is important to be able to apply the model to users that are not seen during training.,2.1 Learning User Embeddings,[0],[0]
This is done by online updating of the user embeddings during evaluation.,2.1 Learning User Embeddings,[0],[0]
"When a new person, userk+1 is seen, a new row is added to U and initialized to u1.",2.1 Learning User Embeddings,[0],[0]
Each person’s user embedding is updated via back-propagation every time they select a query.,2.1 Learning User Embeddings,[0],[0]
"When doing online updating of the user embeddings, the rest of the model parameters (everything except U) are frozen.",2.1 Learning User Embeddings,[0],[0]
We consider three model architectures which differ only in the method for adapting the recurrent layer.,2.2 Recurrent Layer Adaptation,[0],[0]
"First is the unadapted LM, analogous to the model from Park and Chiba (2017), which does no personalization.",2.2 Recurrent Layer Adaptation,[0],[0]
"The second architecture was
introduced by Mikolov and Zweig (2012) and has been used multiple times for LM personalization (Wen et al., 2013; Huang et al., 2014; Li et al., 2016).",2.2 Recurrent Layer Adaptation,[0],[0]
It works by concatenating a user embedding to the character embedding at every step of the input to the recurrent layer.,2.2 Recurrent Layer Adaptation,[0],[0]
Jaech and Ostendorf (2017) refer to this model as the ConcatCell and show that it is equivalent to adding a term Vu to adjust the bias of the recurrent layer.,2.2 Recurrent Layer Adaptation,[0],[0]
"The hidden state of a ConcatCell with embedding size e and hidden state size h is given in Equation 1 where σ is the activation function, wt is the character embedding, ht−1 is the previous hidden state, and W ∈ Re+h×h and b ∈",2.2 Recurrent Layer Adaptation,[0],[0]
"Rh are the recurrent layer weight matrix and bias vector.
",2.2 Recurrent Layer Adaptation,[0],[0]
"ht = σ([wt, ht−1]W + b+Vu) (1)
",2.2 Recurrent Layer Adaptation,[0],[0]
Adapting just the bias vector is a significant limitation.,2.2 Recurrent Layer Adaptation,[0],[0]
"The FactorCell model, (Jaech and Ostendorf, 2017), remedies this by letting the user embedding transform the weights of the recurrent layer via the use of a low-rank adaptation matrix.",2.2 Recurrent Layer Adaptation,[0],[0]
"The FactorCell uses a weight matrix W′ = W +A that has been additively transformed by a personalized low-rank matrix A. Because the FactorCell weight matrix W′ is different for each user (See Equation 2), it allows for a much stronger adaptation than what is possible using the more standard ConcatCell model.1
ht = σ([wt, ht−1]W ′ + b) (2)
The low-rank adaptation matrix A is generated by taking the product between a user’s m dimensional embedding and left and right bases tensors, ZL ∈ Rm×e+h×r and ZR ∈ Rr×h×m",2.2 Recurrent Layer Adaptation,[0],[0]
"as so,
A = (ui ×1 ZL)(ZR ×3 ui) (3)
where ×i denotes the mode-i tensor product.",2.2 Recurrent Layer Adaptation,[0],[0]
The above product selects a user specific adaptation matrix by taking a weighted combination of the m rank r matrices held between ZL and ZR.,2.2 Recurrent Layer Adaptation,[0],[0]
"The rank, r, is a hyperparameter which controls the degree of personalization.",2.2 Recurrent Layer Adaptation,[0],[0]
"Our experiments make use of the AOL Query data collected over three months in 2006 (Pass et al., 2006).",3 Data,[0],[0]
"The first six of the ten files were used for
1In the case of an LSTM, W′ is extended to incorporate all of the gates.
training.",3 Data,[0],[0]
"This contains approximately 12 million queries from 173,000 users for an average of 70 queries per user (median 15).",3 Data,[0],[0]
"A set of 240,000 queries from those same users (2% of the data) was reserved for tuning and validation.",3 Data,[0],[0]
"From the remaining files, one million queries from 30,000 users are used to test the models on a disjoint set of users.",3 Data,[0],[0]
The vocabulary consists of 79 characters including special start and stop tokens.,4.1 Implementation Details,[0],[0]
Models were trained for six epochs.,4.1 Implementation Details,[0],[0]
"The Adam optimizer is used during training with a learning rate of 10−3 (Kingma and Ba, 2014).",4.1 Implementation Details,[0],[0]
"When updating the user embeddings during evaluation, we found that it is easier to use an optimizer without momentum.",4.1 Implementation Details,[0],[0]
"We use Adadelta (Zeiler, 2012) and tune the online learning rate to give the best perplexity on a held-out set of 12,000 queries, having previously verified that perplexity is a good indicator of performance on the QAC task.2
",4.1 Implementation Details,[0],[0]
"The language model is a single-layer characterlevel LSTM with coupled input and forget gates and layer normalization (Melis et al., 2018; Ba et al., 2016).",4.1 Implementation Details,[0],[0]
We do experiments on two model configurations: small and large.,4.1 Implementation Details,[0],[0]
The small models use an LSTM hidden state size of 300 and 20 dimensional user embeddings.,4.1 Implementation Details,[0],[0]
The large models use a hidden state size of 600 and 40 dimensional user embeddings.,4.1 Implementation Details,[0],[0]
Both sizes use 24 dimensional character embeddings.,4.1 Implementation Details,[0],[0]
"For the small sized models, we experimented with different values of the FactorCell rank hyperparameter between 30 and 50 dimensions finding that bigger rank is better.",4.1 Implementation Details,[0],[0]
The large sized models used a fixed value of 60 for the rank hyperparemeter.,4.1 Implementation Details,[0],[0]
"During training only and due to limited computational resources, queries are truncated to a length of 40 characters.
",4.1 Implementation Details,[0],[0]
Prefixes are selected uniformly at random with the constraint that they contain at least two characters in the prefix and that there is at least one character in the completion.,4.1 Implementation Details,[0],[0]
"To generate completions using beam search, we use a beam width of 100 and a branching factor of 4.",4.1 Implementation Details,[0],[0]
"Results are reported using mean reciprocal rank (MRR), the standard method of evaluating QAC systems.",4.1 Implementation Details,[0],[0]
"It is the mean of the reciprocal rank of the true completion in the
2Code at http://github.com/ajaech/query completion
top ten proposed completions.",4.1 Implementation Details,[0],[0]
"The reciprocal rank is zero if the true completion is not in the top ten.
",4.1 Implementation Details,[0],[0]
Neural models are compared against an MPC baseline.,4.1 Implementation Details,[0],[0]
"Following Park and Chiba (2017), we remove queries seen less than three times from the MPC training data.",4.1 Implementation Details,[0],[0]
Table 2 compares the performance of the different models against the MPC baseline on a test set of one million queries from a user population that is disjoint with the training set.,4.2 Results,[0],[0]
Results are presented separately for prefixes that are seen or unseen in the training data.,4.2 Results,[0],[0]
"Consistent with prior work, the neural models do better than the MPC baseline.",4.2 Results,[0],[0]
The personalized models are both better than the unadapted one.,4.2 Results,[0],[0]
"The FactorCell model is the best overall in both the big and small sized experiments, but the gain is mainly for the seen prefixes.
",4.2 Results,[0],[0]
Figure 1 shows the relative improvement in MRR over an unpersonalized model versus the number of queries seen per user.,4.2 Results,[0],[0]
"Both the Factor-
Cell and the ConcatCell show continued improvement as more queries from each user are seen, and the FactorCell outperforms the ConcatCell by an increasing margin over time.",4.2 Results,[0],[0]
"In the long run, we expect that the system will have seen many queries from most users.",4.2 Results,[0],[0]
"Therefore, the right side of Figure 1, where the relative gain of FactorCell is up to 2% better than that of the ConcatCell, is more indicative of the potential of these models for active users.",4.2 Results,[0],[0]
"Since the data was collected over a limited time frame and half of all users have fifteen or fewer queries, the results in Table 2 do not reflect the full benefit of personalization.
",4.2 Results,[0],[0]
Figure 2 shows the MRR for different prefix and query lengths.,4.2 Results,[0],[0]
We find that longer prefixes help the model make longer completions and (more obviously) shorter completions have higher MRR.,4.2 Results,[0],[0]
"Comparing the personalized model against the unpersonalized baseline, we see that the biggest gains are for short queries and prefixes of length one or two.
",4.2 Results,[0],[0]
We found that one reason why the FactorCell outperforms the ConcatCell is that it is able to pick up sooner on the repetitive search behaviors that some users have.,4.2 Results,[0],[0]
This commonly happens for navigational queries where someone searches for the name of their favorite website once or more per day.,4.2 Results,[0],[0]
At the extreme tail there are users who search for nothing but free online poker.,4.2 Results,[0],[0]
"Both models do well on these highly predictable users but the FactorCell is generally a bit quicker to adapt.
",4.2 Results,[0],[0]
We conducted case studies to better understand what information is represented in the user embeddings and what makes the FactorCell different from the ConcatCell.,4.2 Results,[0],[0]
From a cold start user embedding we ran two queries and allowed the model to update the user embedding.,4.2 Results,[0],[0]
"Then, we ranked
the most frequent 1,500 queries based on the ratio of their likelihood from before and after updating the user embeddings.
",4.2 Results,[0],[0]
"Tables 3 and 4 show the queries with the highest relative likelihood of the adapted vs. unadapted models after two related search queries: “high school softball” and “math homework help” for Table 3, and “Prada handbags” and “Versace eyewear” for Table 4.",4.2 Results,[0],[0]
"In both cases, the FactorCell model examples are more semantically coherent than the ConcatCell examples.",4.2 Results,[0],[0]
"In the first case, the FactorCell model identifies queries that a high school student might make, including entertainment sources and a celebrity entertainer popular with that demographic.",4.2 Results,[0],[0]
"In the second case, the FactorCell model chooses retailers that carry woman’s apparel and those that sell home goods.",4.2 Results,[0],[0]
"While these companies’ brands are not as luxurious as Prada or Versace, most of the top luxury brand names do not appear in the top 1,500 queries and our model may not be capable of being that specific.",4.2 Results,[0],[0]
There is no obvious semantic connection between the highest likelihood ratio phrases for the ConcatCell; it seems to be focusing more on orthography than semantics (e.g. “home” in the first example)..,4.2 Results,[0],[0]
Not shown are the queries which experienced the greatest decrease in likelihood.,4.2 Results,[0],[0]
"For the “high school” case, these included searches for travel agencies and airline tickets— websites not targeted towards the high school age demographic.",4.2 Results,[0],[0]
"While the standard implementation of MPC can not handle unseen prefixes, there are variants which do have that ability.",5 Related Work,[0],[0]
"Park and Chiba (2017) find that the neural LM outperforms MPC even when MPC has been augmented with the approach from Mitra and Craswell (2015) for handling rare
prefixes.",5 Related Work,[0],[0]
"There has also been work on personalizing MPC (Shokouhi, 2013; Cai et al., 2014).",5 Related Work,[0],[0]
We did not compare against these specific models because our goal was to show how personalization can improve the already-proven generative neural model approach.,5 Related Work,[0],[0]
"RNN’s have also previously been used for the related task of next query suggestion (Sordoni et al., 2015).
",5 Related Work,[0],[0]
Our results are not directly comparable to Park and Chiba (2017) or Mitra and Craswell (2015) due to differences in the partitioning of the data and the method for selecting random prefixes.,5 Related Work,[0],[0]
Prior work partitions the data by time instead of by user.,5 Related Work,[0],[0]
"Splitting by users is necessary in order to properly test personalization over longer time ranges.
",5 Related Work,[0],[0]
Wang et al. (2018) show how spelling correction can be integrated into an RNN language model query auto-completion system and how the completions can be generated in real time using a GPU.,5 Related Work,[0],[0]
"Our method of updating the model during evaluation resembles work on dynamic evaluation for language modeling (Krause et al., 2017), but differs in that only the user embeddings (latent demographic factors) are updated.",5 Related Work,[0],[0]
Our experiments show that the LSTM model can be improved using personalization.,6 Conclusion and Future Work,[0],[0]
The method of adapting the recurrent layer clearly matters and we obtained an advantage by using the FactorCell model.,6 Conclusion and Future Work,[0],[0]
The reason the FactorCell does better is in part attributable to having two to three times as many parameters in the recurrent layer as either the ConcatCell or the unadapted models.,6 Conclusion and Future Work,[0],[0]
"By design, the adapted weight matrix W′ only needs to be computed at most once per query and is reused many thousands of times during beam search.",6 Conclusion and Future Work,[0],[0]
"As a result, for a given latency budget, the FactorCell
model outperforms the Mikolov and Zweig (2012) model for LSTM adaptation.
",6 Conclusion and Future Work,[0],[0]
"The cost for updating the user embeddings is similar to the cost of the forward pass and depends on the size of the user embedding, hidden state size, FactorCell rank, and query length.",6 Conclusion and Future Work,[0],[0]
"In most cases there will be time between queries for updates, but updates can be less frequent to reduce computational costs.
",6 Conclusion and Future Work,[0],[0]
We also showed that language model personalization can be effective even on users who are not seen during training.,6 Conclusion and Future Work,[0],[0]
The benefits of personalization are immediate and increase over time as the system continues to leverage the incoming data to build better user representations.,6 Conclusion and Future Work,[0],[0]
The approach can easily be extended to include time as an additional conditioning factor.,6 Conclusion and Future Work,[0],[0]
We leave the question of whether the results can be improved by combining the language model with MPC for future work.,6 Conclusion and Future Work,[0],[0]
Query auto-completion is a search engine feature whereby the system suggests completed queries as the user types.,abstractText,[0],[0]
"Recently, the use of a recurrent neural network language model was suggested as a method of generating query completions.",abstractText,[0],[0]
We show how an adaptable language model can be used to generate personalized completions and how the model can use online updating to make predictions for users not seen during training.,abstractText,[0],[0]
The personalized predictions are significantly better than a baseline that uses no user information.,abstractText,[0],[0]
Personalized Language Model for Query Auto-Completion,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2019–2025, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
"Technologies are increasingly personalized, accommodating their behavior for each user.",1 Introduction,[0],[0]
Such personalization is done through user modeling where the goal is to “get to know” the user.,1 Introduction,[0],[0]
"To that end, personalization is based on users’ attributes, such as demographics (gender, age etc.), personalities, and preferences.",1 Introduction,[0],[0]
"For example, in Information Retrieval, results are customized according to the user’s information and search history (Speretta and Gauch, 2005), performance of Automatic Speech Recognition substantially improves when adapted to a specific speaker (Neumeyer et al., 1995), and Targeted Advertising makes use of the user’s location and prior purchases (Kölmel and Alexakis, 2002).
",1 Introduction,[0],[0]
Personalization in machine translation has a somewhat different nature.,1 Introduction,[0],[0]
"Providers of MT tools and services offer means to “customize” or “personalize” the translation engine for each client, mostly through domain adaptation techniques, and a great deal of effort is made to make the human-involved translation process more efficient (see Section 2.2).",1 Introduction,[0],[0]
"Most of the focus, though, goes to customization for companies or professional translators.",1 Introduction,[0],[0]
"We argue that Personalized Machine Translation (PMT below) should and can take the next step and directly address individual end-users.
∗This work was done while the first author was at Xerox Research Centre Europe.
",1 Introduction,[0],[0]
The difficulty to objectively determine whether one (automatic) translation is better than another has been repeatedly revealed in the MT literature.,1 Introduction,[0],[0]
"Our conjecture is that one reason is individual preferences, to which we refer as Translational Preferences (TP).",1 Introduction,[0],[0]
"TP come into play both when the alternative translations are all correct, and when each of them is wrong in a different way.",1 Introduction,[0],[0]
"In the former case, a preference may be a stylistic choice, and in the latter, a matter of comprehension or a selection of the least intolerable error in one’s opinion.",1 Introduction,[0],[0]
"For instance, one user may prefer shorter sentences than others; she may favor a more formal style, while another would rather have it casual.",1 Introduction,[0],[0]
A user could be fine with some reordering errors but be more picky concerning punctuations.,1 Introduction,[0],[0]
"One user will not be bothered if some words are left untranslated (perhaps because the source language belongs to the same language family as the target language that he speaks), while another will find it utterly displeasing.",1 Introduction,[0],[0]
"Such differences may be the result of the type of translation system being employed (e.g. syntax- vs. phrased-based), the specific training data or many other factors.",1 Introduction,[0],[0]
"On the user’s side, a preference may be attributed, for example, to her mother tongue, her age or her personality.
",1 Introduction,[0],[0]
"Two aspects of end-user PMT may be considered: (i) Personalized translation of texts written by a specific user, and (ii) PMT to provide better translations for a specific reader.",1 Introduction,[0],[0]
"In this work we address the second task, aiming to identify translations each user is more likely to prefer.1 Specifically, we consider a setting where at least two MT systems are available, and the goal is to predict which of the translation systems the user would choose, assuming we have no knowledge about her preference between them.",1 Introduction,[0],[0]
"Benchmarking the systems in advance with respect to a reference set, or estimating the quality of the translations (Specia et al., 2009) are viable alternatives for translation selection; these, however, are not personalized to the target user.",1 Introduction,[0],[0]
"Instead, we employ a user-user Collaborative Filtering approach, common in Recommender Systems, which we map to the TP prediction task.
",1 Introduction,[0],[0]
"We assess this approach using a collection of user rankings of MT systems from a shared translation task
1In (Mirkin et al., 2015)",1 Introduction,[0],[0]
"we investigate the first task, assessing whether the author’s demographic and personality traits are preserved over machine translation.
2019
(see Section 3).",1 Introduction,[0],[0]
"Our results show that the personalized method modestly, but consistently, outperforms several other approaches that rank the systems in general, disregarding the specific user.",1 Introduction,[0],[0]
We consider this as an indication that user feedback can be employed towards a more personalized approach to machine translation.,1 Introduction,[0],[0]
"Collaborative filtering (CF) is a common approach employed by recommender systems for suggesting to users items, such as books or movies.",2.1 Collaborative filtering,[0],[0]
"A recommender system may simply suggest to all users the most popular items; often, however, the recommendations are personalized for each individual user to fit her taste or preferences.",2.1 Collaborative filtering,[0],[0]
User-user CF relies on community preferences.,2.1 Collaborative filtering,[0],[0]
"The idea is to recommend to the user items that are liked by users similar to her, as manifested, for example, by high rating.",2.1 Collaborative filtering,[0],[0]
Similar users are those that agree with the current user on previously-rated items.,2.1 Collaborative filtering,[0],[0]
"In k-nearest-neighbors CF, a user is typically represented by a vector of her preferences, where each entry of the vector is, e.g., a rating of a movie.",2.1 Collaborative filtering,[0],[0]
k similar users are then identified by measuring the similarity between the users’ vectors.,2.1 Collaborative filtering,[0],[0]
"Cosine similarity is a popular function for that purpose, and we also use it in our work (Resnick et al., 1994; Sarwar et al., 2001; Ricci et al., 2011).",2.1 Collaborative filtering,[0],[0]
"An alternative to cosine, Pearson’s correlation coefficient (Pearson, 1895), allows addressing different rating patterns across users.",2.1 Collaborative filtering,[0],[0]
"In comparison to cosine, here vector entries are normalized with respect to the user’s average rating.",2.1 Collaborative filtering,[0],[0]
"In our case, such normalization is not very meaningful since the entries of the users vectors represent comparisons rather than absolute ratings, as will be made clear in Section 4.",2.1 Collaborative filtering,[0],[0]
"Nevertheless, we have experimented with Pearson correlation as well, and found no advantage in using it instead of cosine.",2.1 Collaborative filtering,[0],[0]
"Various means of customization and personalization are available, in both academic and commercial MT.","2.2 Customization, personalization and adaptation in MT",[0],[0]
"Many of them target the company, rather than the individual user, and much of the effort is invested in designing tools for professional translators, aiming to improve their productivity, through intelligent Computer Aided Translation (CAT).
","2.2 Customization, personalization and adaptation in MT",[0],[0]
"Domain adaptation methods are commonly used to adapt to the topic, the genre and even the style of the translated material.","2.2 Customization, personalization and adaptation in MT",[0],[0]
"Using the company’s own corpora is one of the simplest techniques to do so, but many more approaches have been proposed, including dataselection (Axelrod et al., 2011; Gascó","2.2 Customization, personalization and adaptation in MT",[0],[0]
"et al., 2012; Mirkin and Besacier, 2014), mixture models (Foster and Kuhn, 2007) and table fill-up (Bisazza et al., 2011).","2.2 Customization, personalization and adaptation in MT",[0],[0]
"Clients can utilize their own glossaries (Federico et al., 2014), corpora (parallel or monolingual) and translation memories (TM), either shared or private ones
(Caskey and Maskey, 2014; Federico et al., 2014).","2.2 Customization, personalization and adaptation in MT",[0],[0]
"Through Adaptive and Interactive MT (Nepveu et al., 2004), the system learns from the translator’s edits, in order to avoid repeating errors that have already been corrected.","2.2 Customization, personalization and adaptation in MT",[0],[0]
"Post-editions can continuously be added to the translator’s TM or be used as additional training material, for tighter adaptation to the domain of interest, through batch or incremental training.","2.2 Customization, personalization and adaptation in MT",[0],[0]
Many tasks that require annotation by humans are affected by the annotator and not only by the item being judged.,2.3 User preferences in MT,[0],[0]
"Metrics for inter-rater reliability or interannotator agreement, such as Cohen’s Kappa (Cohen, 1960), help measuring the extent to which annotators disagree.",2.3 User preferences in MT,[0],[0]
"Disagreement may be due to untrained or inattentive annotators, a result of a task that is not well defined, or when there is no obvious “truth”.",2.3 User preferences in MT,[0],[0]
Such is the case with the evaluation of translation quality – it is not always straightforward to tell whether one translation is better than another.,2.3 User preferences in MT,[0],[0]
A single sentence can be translated in multiple correct ways.,2.3 User preferences in MT,[0],[0]
The decision becomes even harder when the translations are automatically produced and are imperfect: Is one error worse than another?,2.3 User preferences in MT,[0],[0]
The answer is in the eye of the beholder.,2.3 User preferences in MT,[0],[0]
"MT papers regularly report rather low Kappa levels, even when measured on simpler tasks, such as short segments (Macháček and Bojar, 2015).
",2.3 User preferences in MT,[0],[0]
Turchi et al. (2013) refer to the issue of “subjectivity” of human annotators.,2.3 User preferences in MT,[0],[0]
"They address the task of binary classification of “good” vs. “bad” translations, and show that relying on human annotation for training a binary quality estimator is less effective than using automatically-generated labels.",2.3 User preferences in MT,[0],[0]
This subjectivity is exactly what we are after.,2.3 User preferences in MT,[0],[0]
"We treat it as a preference, trying to identify the systems or specific translations which the user subjectively prefers.
Kichhoff et al. (2012) analyze user preferences with respect to MT errors.",2.3 User preferences in MT,[0],[0]
"They show that some types, e.g. word order errors, are the most dis-preferred by users, and that this is a more important factor than the number of errors.",2.3 User preferences in MT,[0],[0]
"While very relevant for our research, their analysis is aggregated over all users participating in the study, and is not focusing on individuals’ preferences.",2.3 User preferences in MT,[0],[0]
"In this work we used the data provided for the MT Shared Task in the 2013 Workshop on Statistical Machine Translation (WMT) (Bojar et al., 2013).2",3 Data,[0],[0]
"This data was of a particularly large scale, with crowdsourced human judges, either volunteer researchers or paid Amazon Turkers.",3 Data,[0],[0]
"For each source sentence, a judge was presented with the source sentence itself, a reference translation, and the outputs of five machine translation systems.",3 Data,[0],[0]
"The five systems were randomly selected from the pool of participating systems,
2http://www.statmt.org/wmt13/ translation-task.html
and were anonymized and randomly-ordered when presented to the judge.",3 Data,[0],[0]
"The judge had to rank the translations, with ties allowed (i.e. two system can receive the same ranking).",3 Data,[0],[0]
"Hence, each annotation point provided with 10 pairwise rankings between systems.",3 Data,[0],[0]
"Translations of 10 language pairs were assessed, with 11 to 19 systems for each pair.",3 Data,[0],[0]
"In total, over 900K non-tied pairwise rankings were collected.",3 Data,[0],[0]
"The Turkers’ annotation included a control task for quality assurance, rejecting Turkers failing more than 50% of the control points.",3 Data,[0],[0]
The inter-annotator score showed on average a fair to moderate level of agreement.,3 Data,[0],[0]
"Our method, denoted CTP (Collaborative Translational Preferences), is based on a k-nearest-neighbors approach for user-user CF.",4 Translational preferences with collaborative filtering,[0],[0]
"That is, we predict the translational preferences of a user based on those of similar users.",4 Translational preferences with collaborative filtering,[0],[0]
"In our setting, a user preference is the choice between two translation systems – which system’s translations does the user prefer.",4 Translational preferences with collaborative filtering,[0],[0]
"Given two systems (or models of the same system) we wish to predict which one the user would prefer, without assuming the user has ever expressed her preference between these two specific systems.",4 Translational preferences with collaborative filtering,[0],[0]
"It is important to emphasize that the method presented here considers the users’ overall preferences of systems, and does not regard the specific sentence that is being translated.",4 Translational preferences with collaborative filtering,[0],[0]
In future work we intend to make use of this information as well.,4 Translational preferences with collaborative filtering,[0],[0]
"As mentioned in Section 3, each annotation consists of a ranking of five systems.",4.1 Representation,[0],[0]
"From that, we extract pairwise rankings for every pair of systems that were ranked for a given language pair.",4.1 Representation,[0],[0]
"For each user u ∈ U (where U are all users who annotated the language pair), we create a user-preference vector, pu, that contains an entry for each pair of translation systems.",4.1 Representation,[0],[0]
"Denoting the set of systems with S, we have |S|·(|S|−1)2 system pairs.",4.1 Representation,[0],[0]
"E.g., for Czech-English, with 11 participating systems, the user vector size is 55.",4.1 Representation,[0],[0]
"Each entry (i, j) of the vector is assigned the following value:
pu (i,j) =
w",4.1 Representation,[0],[0]
"(i,j) u − l(i,j)u w",4.1 Representation,[0],[0]
"(i,j) u +",4.1 Representation,[0],[0]
"l (i,j) u
(1)
where w(i,j)u and l (i,j) u are the number of wins and loses of system si vs. system sj as judged by user u.3
With this representation, a user vector contains values between −1 (if si always lost to sj) and 1 (if si always won).",4.1 Representation,[0],[0]
"If the user always ranked the two systems identically, the value is 0, and if she has never evaluated the pair, the entry is regarded as a missing value (NA).",4.1 Representation,[0],[0]
"Altogether, we have a matrix of users by system pairs, as depicted in Figure 1.
",4.1 Representation,[0],[0]
3We have also considered including ties in the denominator of the equation; discarding them was found superior.,4.1 Representation,[0],[0]
"Given a user preference to predict for a pair of systems (si, sj), we compute the similarity between pu and each one of pu′ for all other u′ ∈ U .",4.2 Finding similar users,[0],[0]
In our experiments we used cosine as the similarity measure.,4.2 Finding similar users,[0],[0]
The k most-similar-users (MSU ) are then selected.,4.2 Finding similar users,[0],[0]
"To be included in MSU (u), we require that u and u′ have judged at least 2 common system pairs.",4.2 Finding similar users,[0],[0]
"Given the similarity scores, to predict the user’s preference for the target system pair, we compute a weighted average of the predictions of the users in MSU (u).
",4.3 Preference prediction,[0],[0]
We include in the average only users with similarity scores above a certain positive threshold (0.05).,4.3 Preference prediction,[0],[0]
We then require that a minimum number of users meet the above criteria of common annotations and minimum similarity (we used 5).,4.3 Preference prediction,[0],[0]
"If not enough such similar users are found, we turn to a fallback, where we use the non-weighted average preference across all users (AVPF presented in Section 5).4 The prediction is then the sign of the weighted average.",4.3 Preference prediction,[0],[0]
"A positive value means si is the preferred system; a negative one means it is sj , and a zero is a draw.",4.3 Preference prediction,[0],[0]
"In our evaluation we compare this prediction to the sign of the actual preference of the user, pu(i,j).",4.3 Preference prediction,[0],[0]
"Formally, CTP computes the following prediction function f for a given user u and a system pair (si, sj):
fCTP(u)(i,j) = sign( ∑ u′ pu′",4.3 Preference prediction,[0],[0]
"(i,j) · sim(u, u′)∑
u′ sim(u, u′) )",4.3 Preference prediction,[0],[0]
"(2)
where u′ ∈ MSU (u) are the most similar users (the nearest neighbors) of u; pu′ (i,j) are the preferences of user u′ for (si, sj) and sim(u, u′) is the similarity score between the two users.5",4.3 Preference prediction,[0],[0]
"In our experiments we try to predict which one of two translation systems would be preferred by a given user.
4The fallback was used 0.1% of the times.",5.1 Evaluation methodology,[0],[0]
5The denominator is not required as long as we predict only the sign since all used similarity scores are positive.,5.1 Evaluation methodology,[0],[0]
"We keep it in order to obtain a normalized score that can be used for other decisions, e.g. ranking multiple systems.
",5.1 Evaluation methodology,[0],[0]
"We evaluate our method, as well as several other prediction functions, when compared with the user’s pairwise system preference according to the annotation – pu
(i,j), shown in Equation 1.",5.1 Evaluation methodology,[0],[0]
"For each user this is an aggregated figure over all her pairwise rankings for the pair, determining the preferred system as the one chosen by the user (i.e. ranked higher) more times.
",5.1 Evaluation methodology,[0],[0]
We conduct a leave-one-out experiment.,5.1 Evaluation methodology,[0],[0]
"For each language pair, we iterate over all non-NA entries in the user-preferences matrix, remove the entry and try to predict it.",5.1 Evaluation methodology,[0],[0]
"User similarity scores are re-computed for each evaluation point, to ensure they do not consider the target pair.",5.1 Evaluation methodology,[0],[0]
"The “gold” preference is positive when the user prefers si, negative when she prefers sj and 0 when she has no preference between them.",5.1 Evaluation methodology,[0],[0]
"Hence, each of the assessed methods is measured by the accuracy of predicting the sign of the preference.",5.1 Evaluation methodology,[0],[0]
"We compare CTP to the following prediction methods:
Always i (ALI)",5.2 Non-personalized methods,[0],[0]
This is a naı̈ve baseline showing the score when always predicting that system i wins.,5.2 Non-personalized methods,[0],[0]
"Note that the baseline is not simply 50% due to ties.
",5.2 Non-personalized methods,[0],[0]
Average rank (RANK),5.2 Non-personalized methods,[0],[0]
"Here, two systems are compared by the average of their rankings across all annotations (r ∈ {1, 2, 3, 4, 5}):
fRANK(u)(i,j) = sign(rj − ri) (3) rj and ri are the average ranks of sj and si respectively.",5.2 Non-personalized methods,[0],[0]
"Since a smaller value of r corresponds to a higher rank, we subtract the rank of si from sj and not the other way around.",5.2 Non-personalized methods,[0],[0]
"This way, if for instance, si is ranked on averaged higher than sj , the prediction would be positive, as desired.
",5.2 Non-personalized methods,[0],[0]
Expected (EXPT),5.2 Non-personalized methods,[0],[0]
"This metric, proposed by Koehn (2012) and used by Bojar et al. (2013) in order to rank the participating systems in the WMT benchmark, compares the expected wins of the two systems.",5.2 Non-personalized methods,[0],[0]
"Its intuition is explained as follows: “If the system is compared against a randomly picked opposing system, on a randomly picked sentence, by a randomly picked judge, what is the probability that its translation is ranked higher?”",5.2 Non-personalized methods,[0],[0]
"The expected wins of si, e(si), is the probability of si to win when compared to another system, estimated as the total number of wins of si relative to the total number of comparisons involving it, excluding ties, and normalized by the total number of systems excluding si, |{sk}|:
e(i)",5.2 Non-personalized methods,[0],[0]
"= 1 |{sk}| ∑ k 6=i
w(i,k)
w(i,k) + l(i,k) (4)
where w(i,k) and l(i,k) are summed over all users.",5.2 Non-personalized methods,[0],[0]
"The preference prediction is therefore:
fEXPT(u)(i,j) = sign(e(i)− e(j)) (5)
RANK and EXPT predict preferences based on a system’s performance in general, when compared to all other systems.",5.2 Non-personalized methods,[0],[0]
"We propose an additional prediction function for comparison which uses only the information concerning the system pair under consideration.
",5.2 Non-personalized methods,[0],[0]
Average user preference (AVPF),5.2 Non-personalized methods,[0],[0]
This method takes into account only the specific system pair and averages the user preferences for the pair.,5.2 Non-personalized methods,[0],[0]
"Formally:
fAVPF(u)(i,j) = sign( ∑ u′ p (i,j) u′
|{u′}| ) (6)
where u′ 6= u, and {u′} are all users except u.",5.2 Non-personalized methods,[0],[0]
"This method can be viewed as a non-personalized version of CTP, with two differences:
(1) It considers all users, and not only similar ones.",5.2 Non-personalized methods,[0],[0]
"(2) It does not weight the preferences of the other
users by their similarity to the target user.",5.2 Non-personalized methods,[0],[0]
Table 1 shows the results of an experiment comparing the performance of the various methods in terms of prediction accuracy.,5.3 Results,[0],[0]
"Figure 2 shows the micro-average scores, when giving each of the 97,412 test points an equal weight in the average.",5.3 Results,[0],[0]
"CTP outperforms all others for 9 out of 10 language pairs, and in the overall microaveraged results.",5.3 Results,[0],[0]
"The difference between CTP and each of the other metrics was found statistically significance with p < 5 · 10−6 at worse, as measured with a paired Wilcoxon signed rank test (Wilcoxon, 1945) on the predictions of the two methods.",5.3 Results,[0],[0]
"The significance test captures in this case the fact that the methods disagreed in many more cases than is visible by the score difference.
",5.3 Results,[0],[0]
"Our method was found superior to all others also when computing macro-average, taking the average of the scores of each language pair, as well as when the ties are included in the computation of pu.
",5.3 Results,[0],[0]
The parameters with which the above results were obtained are found within the method’s description in Section 4.,5.3 Results,[0],[0]
"Yet, in our experiments, CTP turned out to be rather insensitive to their values.",5.3 Results,[0],[0]
In this experiment we used a global set of parameters and did not tune them for each language pair separately.,5.3 Results,[0],[0]
It is reasonable to assume that such tuning would improve results.,5.3 Results,[0],[0]
"For instance, choosing k, the number of users to include in the average, depends on the total number of users.",5.3 Results,[0],[0]
"E.g., for en-es, where there are only 57 users in total, reducing k’s value from 50 to 25, improves results of CTP from 62.6% to 63.2%, higher than all other methods (whose scores are not affected).
",5.3 Results,[0],[0]
"Specifically in comparison to AVPF, weighting by the similarity scores was found to be a more significant factor than selecting a small subset of the users.",5.3 Results,[0],[0]
"This may not come as a surprise, since less similar users that are added to MSU (u) have a smaller impact on the final decision since their weight in the average is smaller.
",5.3 Results,[0],[0]
"One weakness of CTP, as well as of other methods, is that it poorly predict ties.",5.3 Results,[0],[0]
"In the above experiment, approximately 13.5% of the preferences were 0, none of them was correctly identified.",5.3 Results,[0],[0]
Our analysis showed that numerical accuracy is not the main cause; setting any prediction that is smaller than some values of |ε| to 0 was not found helpful.,5.3 Results,[0],[0]
"Arguably, ties need not be predicted, since if the user has no preference between two systems, any choice is just as good.",5.3 Results,[0],[0]
"Still, we believe that better ties prediction could lead to general improvement of our method and we wish to address it in future work.",5.3 Results,[0],[0]
We addressed the task of predicting user preference with respect to MT output via a collaborative filtering approach whose prediction is based on preferences of similar users.,6 Discussion,[0],[0]
This method predicts TP better than a set of non-personalized methods.,6 Discussion,[0],[0]
"The gain is modest in absolute numbers, but the results are highly statistically significant and stable over parameter values.
",6 Discussion,[0],[0]
We consider this work as a step towards more personalized MT.,6 Discussion,[0],[0]
This line of research can be extended in multiple ways.,6 Discussion,[0],[0]
"First and foremost, as mentioned, we did not consider the actual content of the sentences, but rather identified a general preference for one system over another.",6 Discussion,[0],[0]
"It is plausible, however, that one system is better – from the user’s perspective – at translating one type of text, while another is preferred for other texts.",6 Discussion,[0],[0]
"Taking the actual texts into account seems therefore es-
sential.",6 Discussion,[0],[0]
Content-based methods for recommender systems may be useful for this purpose.,6 Discussion,[0],[0]
"Another factor that may be affecting preferences is translation quality: when compared translations are all poor, preferences play a less significant role.",6 Discussion,[0],[0]
"Hence, it may be informative to assess TP prediction separately across different levels of translation quality.
",6 Discussion,[0],[0]
Large parallel corpora are typically required for training reasonable statistical translation models.,6 Discussion,[0],[0]
"Yet, parallel corpora, and even more so in-domain ones, are hard to gather.",6 Discussion,[0],[0]
"It is virtually impossible to find a user-specific parallel corpus, and methods for monolingual domain adaptation are easier to envisage if one wishes to address author-aware PMT (the first PMT task mentioned in Section 1).",6 Discussion,[0],[0]
"Collecting user feedback is another challenge, especially since most endusers do not speak the source language.",6 Discussion,[0],[0]
"For that and other reasons, it currently seems more feasible to collect preference information from professional translators, explicitly or implicitly.",6 Discussion,[0],[0]
"Yet, in this research we aim at end-users rather than translators whose preferences are often driven by the ease of correction more than anything else.",6 Discussion,[0],[0]
"We believe that one way to tackle this issue is to exploit other kinds of feedback, from which we can infer user preferences and similarity.",6 Discussion,[0],[0]
Online MT providers are recently collecting end-user feedback for their proposed translations which may be useful for TP prediction.,6 Discussion,[0],[0]
"For instance, in early 2015 Facebook introduced a feature letting users rate (Bing) translations, and Google Translate asks for suggested improvements.",6 Discussion,[0],[0]
We are hopeful that such data becomes publicly available.,6 Discussion,[0],[0]
"Nevertheless, it remains unlikely to obtain feedback from each and every user.",6 Discussion,[0],[0]
"A potential direction for both corpora and feedback collection is personalizing models and identifying preferences for groups of users based on socio-demographic traits, such as gender, age or mother tongue, or based on (e.g. Big 5) personality traits.",6 Discussion,[0],[0]
These can even be inferred by automatically analyzing user texts.,6 Discussion,[0],[0]
We wish to thank Hervé Déjean and the EMNLP reviewers for their valuable feedback on this work.,Acknowledgments,[0],[0]
"Machine Translation (MT) has advanced in recent years to produce better translations for clients’ specific domains, and sophisticated tools allow professional translators to obtain translations according to their prior edits.",abstractText,[0],[0]
We suggest that MT should be further personalized to the end-user level – the receiver or the author of the text – as done in other applications.,abstractText,[0],[0]
"As a step in that direction, we propose a method based on a recommender systems approach where the user’s preferred translation is predicted based on preferences of similar users.",abstractText,[0],[0]
"In our experiments, this method outperforms a set of non-personalized methods, suggesting that user preference information can be employed to provide better-suited translations for each user.",abstractText,[0],[0]
Personalized Machine Translation: Predicting Translational Preferences,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 208–215 New Orleans, Louisiana, June 1 - 6, 2018. c©2017 Association for Computational Linguistics",text,[0],[0]
"Predicting the next characters or words following a prefix has had multiple uses from helping handicapped people (Swiffin et al., 1987) to, more recently, helping search engine users (Cai et al., 2016).",1 Introduction,[0],[0]
"In practice, most search engines today use query auto completion (QAC) systems, consisting of suggesting queries as users type in the search box (Fiorini et al., 2017).",1 Introduction,[0],[0]
"The task suffers from high dimensionality, because the number of possible solutions increases as the length of the target query increases.",1 Introduction,[0],[0]
"Historically, the query prediction task has been addressed by relying on query logs, particularly the popularity of past queries (BarYossef and Kraus, 2011; Lu et al., 2009).",1 Introduction,[0],[0]
"The idea is to rely on the wisdom of the crowd, as popular
queries matching a typed prefix are more likely to be the user’s intent.
",1 Introduction,[0],[0]
"This traditional approach is usually referred to as MostPopularCompletion (MPC)(Bar-Yossef and Kraus, 2011).",1 Introduction,[0],[0]
"However, the performance of MPC is skewed: it is very high for popular queries and very low for rare queries.",1 Introduction,[0],[0]
"At the extreme, MPC simply cannot predict a query it has never seen.",1 Introduction,[0],[0]
"This becomes a bigger problem in academic search (Lankinen et al., 2016), where systems are typically less used, with a wider range of possible queries.",1 Introduction,[0],[0]
"Recent advances in deep learning, particularly in semantic modeling (Mitra and Craswell, 2015) and neural language modeling (Park and Chiba, 2017) showed promising results for predicting rare queries.",1 Introduction,[0],[0]
"In this work, we propose to improve the state-of-the-art approaches in neural QAC by integrating personalization and time sensitivity information as well as addressing current MPC limitations by diversifying the suggestions, thus approaching a production-ready architecture.",1 Introduction,[0],[0]
"While QAC has been well studied, the field has recently started to shift towards deep learningbased models, which can be categorized into two main classes: semantic models (using Convolutional Neural Nets, or CNNs) (Mitra and Craswell, 2015) and language models (using Recurrent Neural Nets, or RNNs) (Park and Chiba, 2017).",2.1 Neural query auto completion,[0],[0]
"Both approaches are frequently used in natural language processing in general (Kim et al., 2016) and tend to capture different features.",2.1 Neural query auto completion,[0],[0]
"In this work, we focus on RNNs as they provide a flexible solution to generate text, even when it is not previously seen in the training data.
",2.1 Neural query auto completion,[0],[0]
"Yet, recent work in this field (Park and Chiba, 2017) suffers from some limitations.",2.1 Neural query auto completion,[0],[0]
"Most importantly, the probability estimates for full queries
208
are directly correlated to the length of the suggestions, consequently favoring shorter queries in some cases and hampering some predictions (Park and Chiba, 2017).",2.1 Neural query auto completion,[0],[0]
"By appending these results to MPC’s and re-ranking the list with LambdaMART (Burges, 2010) in another step as suggested in previous work (Mitra and Craswell, 2015), they achieve state-of-the-art performance in neural query auto completion at the cost of a higher complexity and more computation time.",2.1 Neural query auto completion,[0],[0]
"Still, these preliminary approaches have yet to integrate standards in QAC, e.g. query personalization (Koutrika and Ioannidis, 2005; Margaris et al., 2018) and time sensitivity (Cai et al., 2014).",2.2 Context information,[0],[0]
This integration has to differ from traditional approaches by taking full advantage of neural language modeling.,2.2 Context information,[0],[0]
"For example, neural language models could be refined to capture interests of some users as well as their actual language or query formulation.",2.2 Context information,[0],[0]
"The same can apply to timesensitivity, where the probability of queries might change over time (e.g. for queries such as “tv guide”, or “weather”).",2.2 Context information,[0],[0]
"Furthermore, the feasibility of these approaches in real-world settings has not been demonstrated, even more so on specialized domains.
",2.2 Context information,[0],[0]
"By addressing these issues, we make the following contributions in this work compared to the previous approaches:
• We propose a more straightforward architecture with improved scalability;
• Our method integrates user information when available as well as time-sensitivity;
• We propose to use a balanced beam search for ensuring diversity;
• We test on a second dataset and compare the generalizability of different methods in a specialized domain;
•",2.2 Context information,[0],[0]
"Our method achieves stronger performance than the state of the art on both datasets.
",2.2 Context information,[0],[0]
"Finally, our source code is made available in a public repository1.",2.2 Context information,[0],[0]
"This allows complete reproducibility of our results and future comparisons.
1https://github.com/ncbi-nlp/NQAC",2.2 Context information,[0],[0]
"The justification of using a neural language model for the task of predicting queries is that it has been proven to perform well to generate text that has never been seen in the training data (Sutskever et al., 2011).",3.1 Personalized neural Language Model,[0],[0]
"Particularly, character-level models work with a finer granularity.",3.1 Personalized neural Language Model,[0],[0]
"That is, if a given prefix has not been seen in the training data (e.g. a novel or incomplete word), the model can use the information shared across similar prefixes to make a prediction nonetheless.
",3.1 Personalized neural Language Model,[0],[0]
Recurrent Neural Network,3.1 Personalized neural Language Model,[0],[0]
The difficulty of predicting queries given a prefix is that the number of candidates explodes as the query becomes longer.,3.1 Personalized neural Language Model,[0],[0]
"RNNs allow to represent each character (or word) of a sequence as a cell state, therefore reducing the dimensionality of the task.",3.1 Personalized neural Language Model,[0],[0]
"However, they also introduce the vanishing gradient problem during backpropagation, preventing them from learning long-term dependencies.",3.1 Personalized neural Language Model,[0],[0]
"Both gated recurrent units (GRU) (Cho et al., 2014) and long-short term memory cells (LSTMs) solve this limitation — albeit with a different approach — and are increasingly used.",3.1 Personalized neural Language Model,[0],[0]
"In preliminary experiments, we tried various forms of RNNs: vanilla RNNs, GRUs and LSTMs.",3.1 Personalized neural Language Model,[0],[0]
"GRUs performed similarly to LSTM with a smaller computational complexity due to fewer parameters to learn as was previously observed (Jozefowicz et al., 2015).
",3.1 Personalized neural Language Model,[0],[0]
Word embedded character-level Neural Language Model,3.1 Personalized neural Language Model,[0],[0]
"The main novelty in (Park and Chiba, 2017) is to combine a character-level neural language model with a word-embedded space character.",3.1 Personalized neural Language Model,[0],[0]
"The incentive is that character-level neural language models benefit from a finer granularity for predictions but they lack the semantic understanding words-level models provide, and vice versa.",3.1 Personalized neural Language Model,[0],[0]
"Therefore, they encode text sequences using one-hot encoding of characters, character embedding and pre-trained word embedding (using word2vec (Mikolov et al., 2013)) of the previous word when a space character is encountered.",3.1 Personalized neural Language Model,[0],[0]
"Our preliminary results showed that the character embedding does not bring much to the learning, so we traded it with the context feature vectors below to save some computation time while enriching the model with additional, diverse information.
",3.1 Personalized neural Language Model,[0],[0]
"User representation We make the assumption that the way a user types a query is a function of their actual language/vocabulary, but also a function of their interests.",3.1 Personalized neural Language Model,[0],[0]
"Therefore, a language model could capture these user characteristics to better predict the query, if we feed the learner with the information.",3.1 Personalized neural Language Model,[0],[0]
"Each query qi is a set of words such that qi = {w1, ..., wn}.",3.1 Personalized neural Language Model,[0],[0]
"U is a column matrix and a user u ∈ U is characterized by the union of words in their k past queries, i.e. Qu = ∪ki=1qi.",3.1 Personalized neural Language Model,[0],[0]
"The objective is to reduce, for each user, the vocabulary used in their queries to a vector of a dimensionality d of choice, or Qu → Rd.",3.1 Personalized neural Language Model,[0],[0]
"We chose d = 30, in order to stay in the same computation order of previous work using character embedding (Park and Chiba, 2017).",3.1 Personalized neural Language Model,[0],[0]
"To this end, we adapted the approach PV-DBOW detailed in (Le and Mikolov, 2014).",3.1 Personalized neural Language Model,[0],[0]
"That is, at each training iteration, a random word wi is sampled from Qu.",3.1 Personalized neural Language Model,[0],[0]
"The model is trained by maximizing the probability of predicting the user u given the word wi, i.e.:
1 |U | ∑
u∈U
∑
wi∈Qu log P (u|wi).",3.1 Personalized neural Language Model,[0],[0]
"(1)
The resulting vectors are stored for each user ID and are used as input for the neural net (NN) (see Architecture section).
",3.1 Personalized neural Language Model,[0],[0]
"Time representation As an example, in the background data (see Section 4.1), the query “tv guide” appears 1,682 times and it is vastly represented in evening and nights.",3.1 Personalized neural Language Model,[0],[0]
"For this reason, we propose to integrate time features in the language model.",3.1 Personalized neural Language Model,[0],[0]
"While there has been more elaborated approaches to model it in the past (Shokouhi and Radinsky, 2012), we instead propose a straightforward encoding and leave the rest of the work to the neural net.",3.1 Personalized neural Language Model,[0],[0]
"For each query, we look at the time it was issued, consisting of hour x , minute y and second z, and we derive the following features:
sin
( 2π(3600x+ 60y + z)
86400
) ,
cos
( 2π(3600x+ 60y + z)
86400
) .
",3.1 Personalized neural Language Model,[0],[0]
"(2)
This encoding has the benefit of belonging to [−1, 1], which is a range comparable to the rest of the features.",3.1 Personalized neural Language Model,[0],[0]
"It is also capable to model cyclic data, which is important particularly around boundaries (e.g. considering a query at 11:55PM
and another at 00:05AM).",3.1 Personalized neural Language Model,[0],[0]
"We proceed the same way to encode weekdays and we end up with four time features.
",3.1 Personalized neural Language Model,[0],[0]
Overall architecture An overview of the architecture is proposed in Figure 1.,3.1 Personalized neural Language Model,[0],[0]
"The input of our neural language model is a concatenation of the vectors defined above, for each character and for each query in the training set.",3.1 Personalized neural Language Model,[0],[0]
"We use zeropadding after the “\n” character to keep the sequence length consistent, and the NN learns to recognize it.",3.1 Personalized neural Language Model,[0],[0]
"We feed this input vector into 2 layers of 1024 GRUs2, each followed by a dropout layer (with a dropout rate of 50%) to prevent overfitting.",3.1 Personalized neural Language Model,[0],[0]
Each GRU cell is activated with ReLu(x) =,3.1 Personalized neural Language Model,[0],[0]
x+ and gradients are clipped to a norm of 0.5 to avoid gradient exploding problems.,3.1 Personalized neural Language Model,[0],[0]
"The output of the second dropout layer is fed to a temporal softmax layer, which allows to make predictions at each state.",3.1 Personalized neural Language Model,[0],[0]
"The softmax function returns the probability P (ci|c1, ..., ci−1) of the character ci given the previous characters of the sequence, which is then used to calculate the loss function by comparing it to the next character in the target query.",3.1 Personalized neural Language Model,[0],[0]
"Instead of using the objective denoted in (Park and Chiba, 2017), we minimize the loss L defined as the average cross entropy of this probability with the reference probability P̂ (ci) across all queries, that is
L =
− 1|Q| ∑
q∈Q
|q|−1∑
i=1
P̂ (ci+1)× log P (ci+1|c1, ..., ci).
",3.1 Personalized neural Language Model,[0],[0]
"(3)
Q is the set of queries in the training dataset, |Q| is the total number of queries in the set and |q| is the number of characters in the query q. Convergence stabilizes around 5-10 epochs for the AOL dataset (depending on the model) and 15-20 epochs for the biomedical specialized dataset (see Section 4.1).",3.1 Personalized neural Language Model,[0],[0]
"The straightforward approach for decoding the most likely output sequence — in this case, a suffix given a prefix — is to use a greedy approach.",3.2 Balanced diverse beam search,[0],[0]
"That is, we feed the prefix into the trained NN and pick the most likely output at every step, until the sequence is complete.",3.2 Balanced diverse beam search,[0],[0]
"This approach has a high
2It was reported that using more cells may not help the prediction while hurting computation (Park and Chiba, 2017).
chance to output a locally optimal sequence and a common alternative is to use a beam search instead.",3.2 Balanced diverse beam search,[0],[0]
"We propose to improve the beam search by adding a greedy heuristic within it, in order to account for the diversity in the results.",3.2 Balanced diverse beam search,[0],[0]
"A similar suggestion has been made in (Vijayakumar et al., 2016), and our proposition differs by rebalancing the probabilities after diversity was introduced.",3.2 Balanced diverse beam search,[0],[0]
"In (Vijayakumar et al., 2016), at every step the most likely prediction is not weighted while all others are, by greedily comparing them.",3.2 Balanced diverse beam search,[0],[0]
This approach effectively always prefers the most likely character over all other alternatives at each step.,3.2 Balanced diverse beam search,[0],[0]
"The first result will thus be the same as the local optimum using a greedy approach, which becomes problematic for QAC where order is critical.",3.2 Balanced diverse beam search,[0],[0]
"By rebalancing the probability of the most likely suggestion with the average diversity weight given to other suggestions, we make sure probabilities stay uniform yet suggestions are diverse.",3.2 Balanced diverse beam search,[0],[0]
We use a normalized Levenshtein distance to assess the diversity.,3.2 Balanced diverse beam search,[0],[0]
"The AOL query logs (Pass et al., 2006) are commonly used to evaluate the quality of QAC systems.",4.1 Dataset,[0],[0]
"We rely on a background dataset for the
NN; training and validation datasets for lambdaMART integrations; and a test dataset for evaluations.",4.1 Dataset,[0],[0]
"Some adaptations are done to the AOL background dataset as in (Park and Chiba, 2017), such as removing the queries appearing less than 3 times or longer that 100 characters.",4.1 Dataset,[0],[0]
"For each query in the training, validation and test datasets, we use all possible prefixes starting after the first word as in (Shokouhi, 2013).",4.1 Dataset,[0],[0]
"We use the sets from (Park and Chiba, 2017) available online, enriched with user and time information provided in the original AOL dataset.",4.1 Dataset,[0],[0]
"In addition, we evaluate the systems on a second real-world dataset from a production search engine in the biomedical domain, PubMed (Fiorini et al., 2017; Lu, 2011; Mohan et al., 2018), that was created in the same manner.",4.1 Dataset,[0],[0]
"The biomedical dataset consists of 8,490,317 queries.",4.1 Dataset,[0],[0]
"The sizes of training, validation and test sets are comparable to those used for the AOL dataset.",4.1 Dataset,[0],[0]
Systems are evaluated using the traditional Mean Reciprocal Rank (MRR) metric.,4.2 Evaluation,[0],[0]
This metric assesses the quality of suggestions by identifying the rank of the real query in the suggestions given one of its prefixes.,4.2 Evaluation,[0],[0]
"We also tested PMRR as introduced in (Park and Chiba, 2017) and observed the same trends in results as MRR, so we do not show them due to space limitation.",4.2 Evaluation,[0],[0]
"Given the set of prefixes
P in the test dataset, MRR is defined as follows:
MRR = 1 |Q| ∑
r∈P
1
rp , (4)
where rp represent the rank of the match.",4.2 Evaluation,[0],[0]
Paired t-tests measure the significance of score variations among systems and are reported in the Results section.,4.2 Evaluation,[0],[0]
We also evaluate prediction time as this is an important parameter for building production systems.,4.2 Evaluation,[0],[0]
"The prediction time is averaged over 10 runs on the test set, on the same hardware for all models.",4.2 Evaluation,[0],[0]
We do not evaluate throughput but rather compare the time required by all approaches to process one prefix.,4.2 Evaluation,[0],[0]
"We implemented the method in (Park and Chiba, 2017) and used their best-performing model as a baseline.",4.3 Systems and setups,[0],[0]
"We also compare our results to the standard MPC (Bar-Yossef and Kraus, 2011).",4.3 Systems and setups,[0],[0]
"For our method, we evaluate several incremental versions, starting with NQAC which follows the architecture detailed above but with the word embeddings and the one-hot encoding of characters only.",4.3 Systems and setups,[0],[0]
We add the subscript U when the language model is enriched with user vectors and T when it integrates time features.,4.3 Systems and setups,[0],[0]
We append +D to indicate the use of the diverse beam search to predict queries instead of a standard beam search.,4.3 Systems and setups,[0],[0]
"Finally, we also study the impact of adding MPC and LambdaMART (+MPC, +λMART).",4.3 Systems and setups,[0],[0]
A summary of the results is presented in Table 1.,5 Results,[0],[0]
"Interestingly, our simple NQAC model performs similarly to the state-of-the-art on this dataset, called Neural Query Language Model (NQLM), on all queries.",5 Results,[0],[0]
It is significantly less good for seen queries (-5.6%) and significantly better for unseen queries (+4.2%).,5 Results,[0],[0]
"Although GRUs have less expressive power than LSTMs, their smaller number of parameters to train allowed them to better converge than all LSTM models we tested, including that of (Park and Chiba, 2017).",5 Results,[0],[0]
NQAC also benefits from a significantly better scalability (28% faster than NQLM) and thus seems more appropriate for production systems.,5 Results,[0],[0]
"When we enrich the language model with user information, it becomes better for seen queries (+1.9%) while being about as fast.",5 Results,[0],[0]
"Adding time sensitivity does not yield significant improvements on this
dataset overall, but improves significantly the performance for seen queries (+1.7%).",5 Results,[0],[0]
Relying on the diverse beam search significantly hurts the processing time (39% longer) while not providing significantly better performance.,5 Results,[0],[0]
Our integration of MPC differs from previous studies.,5 Results,[0],[0]
"We noticed that for Web search, MPC performs extremely well and is computationally cheap (0.24 seconds).",5 Results,[0],[0]
"On the other hand, all neural QAC systems are better for unseen queries but struggle to stay under a second of processing time.",5 Results,[0],[0]
"Since identifying if a query has been seen or not is done in constant time, we route the query either to MPC or to NQACUT and we note the overall performance as NQACUT+MPC.",5 Results,[0],[0]
This method provides a significant improvement over NQLM (+6.7%) overall while being faster on average.,5 Results,[0],[0]
"Finally, appending NQACUT ’s results to MPC’s and reranking the list with LambdaMART provides the best results on this dataset, but at the expense of greater computational cost (+60%).
",5 Results,[0],[0]
"While NQACUT+MPC appears clearly as the best compromise between performance and quality for the AOL dataset, the landscape changes drastically on the biomedical dataset and the quality drops significantly for all systems.",5 Results,[0],[0]
"This shows the potential difficulties associated with real-world systems, which particularly occur in specialized domains.",5 Results,[0],[0]
"In this case, the drop in performance is mostly due to the fact that biomedical queries are longer and it becomes more difficult for models to predict the entire query accurately only with the first keywords.",5 Results,[0],[0]
"While the generated queries make sense and are relevant candidates, the chance for generative models to predict the exact target query diminishes as the target query is longer because of combinatorial explosion.",5 Results,[0],[0]
"This is even more true when the target queries are diverse as in specialized domains (Islamaj Dogan et al., 2009; Névéol et al., 2011).",5 Results,[0],[0]
"For example, for the prefix “breast cancer”, there are 1169 diverse suffixes in a single day of logs used for training.",5 Results,[0],[0]
"These include “local recurrence”, “nodular prognosis”, “hormone receptor”, “circulating cells”, “family history”, “chromosome 4p16” or “herceptin review”, to cite only a few.",5 Results,[0],[0]
"Hence, while the model predicts plausible queries, it is a lot more difficult to predict the one the user intended.",5 Results,[0],[0]
"The target query length also has an impact on prediction time, as roughly twice the time is needed for Web searches.",5 Results,[0],[0]
"MPC is the exception, however, it per-
forms poorly even on seen queries (0.165).",5 Results,[0],[0]
This observation suggests that more elaborate models are specifically needed for specialized domains.,5 Results,[0],[0]
"On this dataset, NQAC does not perform as well as NQLM",5 Results,[0],[0]
and it seems this time that the higher number of parameters in NQLM is more appropriate for the task.,5 Results,[0],[0]
"Still, user information helps significantly for seen queries (+23%), probably because some users frequently check the same queries to keep up-to-date.",5 Results,[0],[0]
Time sensitivity seems to help significantly unseen queries (+21%) while significantly hurting the quality for seen queries (-47%).,5 Results,[0],[0]
Diversity is significantly helpful on this dataset (+19%) and provides a balance in performance for both seen and unseen queries.,5 Results,[0],[0]
"NQACUT+MPC yields the best overall MRR score for this dataset, and LambdaMART is unable to learn how to rerank the suggestions, thus decreasing the score.",5 Results,[0],[0]
"From these results, we draw several conclusions.",5 Results,[0],[0]
"First, MPC performs very well on seen queries for Web searches and it should be used on them.",5 Results,[0],[0]
"For unseen queries, the NQACUT model we propose achieves a sub-second state-of-the-art performance.",5 Results,[0],[0]
"Second, it is clear that the field of application will affect many of the decisions when designing a QAC system.",5 Results,[0],[0]
"On a specialized domain, the task is more challenging: fast approaches like MPC perform too poorly while more elaborate approaches do not meet production requirements.",5 Results,[0],[0]
"NQACU performs best on seen queries, NQACUT on unseen queries.",5 Results,[0],[0]
"Finally, NQACUT+D provides an equilibrium between the two at a greater computational cost.",5 Results,[0],[0]
Its overall MRR is similar to that of NQACUT+MPC but it is less redundant (see Table 2).,5 Results,[0],[0]
"Particularly, the system seems not to be limited anymore by the higher probability associ-
ated with shorter suggestions (e.g. “www google”, a form of “www google com”), thus bringing more diversity.",5 Results,[0],[0]
This aspect can be more useful for specialized domains where the range of possible queries is broader.,5 Results,[0],[0]
"Finally, we found that a lot more data was needed for the biomedical domain than for general Web search.",5 Results,[0],[0]
"After about a million queries, NQAC suggests meaningful and plausible queries for both datasets.",5 Results,[0],[0]
"However, for the biomedical dataset, the loss needs more epochs to stabilize than for the AOL dataset, mainly due to the combinatorial explosion mentioned above.",5 Results,[0],[0]
"To the best of our knowledge, we proposed the first neural language model that integrates user information and time sensitivity for query auto completion with a focus on scalability for real-world systems.",6 Conclusions and future work,[0],[0]
Personalization is provided through pretrained user vectors based on their past queries.,6 Conclusions and future work,[0],[0]
"By incorporating this information and by adapting the architecture, we were able to achieve stateof-the-art performance in neural query auto completion without relying on re-ranking, making this approach significantly more scalable in practice.
",6 Conclusions and future work,[0],[0]
"We studied multiple variants, their benefits and drawbacks for various use cases.",6 Conclusions and future work,[0],[0]
"We also demonstrate the utility of this method for specialized domains such as biomedicine, where the query diversity and vocabulary are broader and MPC fails to provide the same performance as in Web search.",6 Conclusions and future work,[0],[0]
We also found that user information and diversity improve the performance significantly more than for Web search engines.,6 Conclusions and future work,[0],[0]
"To allow readers to easily reproduce, evaluate and improve our models, we provide all the code on a public repository.",6 Conclusions and future work,[0],[0]
"The handling of time-sensitivity may benefit from a more elaborate integration, for example sessionbased rather than absolute time.",6 Conclusions and future work,[0],[0]
"Also, we evaluated our approaches on a general search setup for both datasets, while searches in the biomedical domain commonly contain fields (i.e. authors, title, abstract, etc.) which adds to the difficulty.",6 Conclusions and future work,[0.950109186376194],"['Here, we apply this model to dialogue, and consider the keys as dialog histories (from the training set), and the values as the next dialogue utterances, i.e., the replies from the speaking partner.']"
"The choice of a diversity metric is also important and could be faster or more efficient (e.g., using word embeddings to diversify the semantics of the suggestions).",6 Conclusions and future work,[0],[0]
These limitations warrant further work and we leave them as perspectives.,6 Conclusions and future work,[0],[0]
"This research was supported by the Intramural Research Program of the NIH, National Library of Medicine.",Acknowledgement,[0],[0]
"Query auto completion (QAC) systems are a standard part of search engines in industry, helping users formulate their query.",abstractText,[0],[0]
"Such systems update their suggestions after the user types each character, predicting the user’s intent using various signals — one of the most common being popularity.",abstractText,[0],[0]
"Recently, deep learning approaches have been proposed for the QAC task, to specifically address the main limitation of previous popularity-based methods: the inability to predict unseen queries.",abstractText,[0],[0]
"In this work we improve previous methods based on neural language modeling, with the goal of building an end-to-end system.",abstractText,[0],[0]
We particularly focus on using real-world data by integrating user information for personalized suggestions when possible.,abstractText,[0],[0]
We also make use of time information and study how to increase diversity in the suggestions while studying the impact on scalability.,abstractText,[0],[0]
"Our empirical results demonstrate a marked improvement on two separate datasets over previous best methods in both accuracy and scalability, making a step towards neural query auto-completion in production search engines.",abstractText,[0],[0]
Personalized neural language models for real-world query auto completion,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 706–711 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
706",text,[0],[0]
"Contextual, or ‘data-to-text’ natural language generation is one of the core tasks in natural language processing and has a considerable impact on various fields (Gatt and Krahmer, 2017).",1 Introduction,[0],[0]
"Within the field of recommender systems, a promising application is to estimate (or generate) personalized reviews that a user would write about a product, i.e., to discover their nuanced opinions about each of its individual aspects.",1 Introduction,[0],[0]
"A successful model could work (for instance) as (a) a highly-nuanced recommender system that tells users their likely reaction to a product in the form of text fragments; (b) a writing tool that helps users ‘brainstorm’ the review-writing process; or (c) a querying system that facilitates personalized natural lan-
guage queries (i.e., to find items about which a user would be most likely to write a particular phrase).",1 Introduction,[0],[0]
"Some recent works have explored the review generation task and shown success in generating cohesive reviews (Dong et al., 2017; Ni et al., 2017; Zang and Wan, 2017).",1 Introduction,[0],[0]
"Most of these works treat the user and item identity as input; we seek a system with more nuance and more precision by allowing users to ‘guide’ the model via short phrases, or auxiliary data such as item specifications.",1 Introduction,[0],[0]
"For example, a review writing assistant might allow users to write short phrases and expand these key points into a plausible review.
",1 Introduction,[0],[0]
"Review text has been widely studied in traditional tasks such as aspect extraction (Mukherjee and Liu, 2012; He et al., 2017), extraction of sentiment lexicons (Zhang et al., 2014), and aspectaware sentiment analysis (Wang et al., 2016; McAuley et al., 2012).",1 Introduction,[0],[0]
These works are related to review generation since they can provide prior knowledge to supervise the generative process.,1 Introduction,[0],[0]
"We are interested in exploring how such knowledge (e.g. extracted aspects) can be used in the review generation task.
",1 Introduction,[0],[0]
"In this paper, we focus on designing a review generation model that is able to leverage both user and item information as well as auxiliary, textual input and aspect-aware knowledge.",1 Introduction,[0],[0]
"Specifically, we study the task of expanding short phrases into complete, coherent reviews that accurately reflect the opinions and knowledge learned from those phrases.
",1 Introduction,[0],[0]
"These short phrases could include snippets provided by the user, or manifest aspects about the items themselves (e.g. brand words, technical specifications, etc.).",1 Introduction,[0],[0]
"We propose an encoderdecoder framework that takes into consideration three encoders (a sequence encoder, an attribute encoder, and an aspect encoder), and one decoder.",1 Introduction,[0],[0]
"The sequence encoder uses a gated recurrent unit
(GRU) network to encode text information; the attribute encoder learns a latent representation of user and item identity; finally, the aspect encoder finds an aspect-aware representation of users and items, which reflects user-aspect preferences and item-aspect relationships.",1 Introduction,[0],[0]
The aspect-aware representation is helpful to discover what each user is likely to discuss about each item.,1 Introduction,[0],[0]
"Finally, the output of these encoders is passed to the sequence decoder with an attention fusion layer.",1 Introduction,[0],[0]
The decoder attends on the encoded information and biases the model to generate words that are consistent with the input phrases and words belonging to the most relevant aspects.,1 Introduction,[0],[0]
"Review generation belongs to a large body of work on data-to-text natural language generation (Gatt and Krahmer, 2017), which has applications including summarization (See et al., 2017), image captioning (Vinyals et al., 2015), and dialogue response generation (Xing et al., 2017; Li et al., 2016; Ghosh et al., 2017), among others.",2 Related Work,[0],[0]
"Among these, review generation is characterized by the need to generate long sequences and estimate high-order interactions between users and items.
",2 Related Work,[0],[0]
Several approaches have been recently proposed to tackle these problems.,2 Related Work,[0],[0]
Dong et al. (2017) proposed an attribute-to-sequence (Attr2Seq) method to encode user and item identities as well as rating information with a multi-layer perceptron and a decoder then generates reviews conditioned on this information.,2 Related Work,[0],[0]
"They also used an attention mechanism to strengthen the alignment between
output and input attributes.",2 Related Work,[0],[0]
Ni et al. (2017) trained a collaborative-filtering generative concatenative network to jointly learn the tasks of review generation and item recommendation.,2 Related Work,[0],[0]
"Zang and Wan (2017) proposed a hierarchical structure to generate long reviews; they assume each sentence is associated with an aspect score, and learn the attention between aspect scores and sentences during training.",2 Related Work,[0],[0]
"Our approach differs from these mainly in our goal of incorporating auxiliary textual information (short phrases, product specifications, etc.) into the generative process, which facilitates the generation of higher-fidelity reviews.
",2 Related Work,[0],[0]
"Another line of work related to review generation is aspect extraction and opinion mining (Park et al., 2015; Qiu et al., 2017; He et al., 2017; Chen et al., 2014).",2 Related Work,[0],[0]
"In this paper, we argue that the extra aspect (opinion) information extracted using these previous works can effectively improve the quality of generated reviews.",2 Related Work,[0],[0]
We propose a simple but effective way to combine aspect information into the generative model.,2 Related Work,[0],[0]
We describe the review generation task as follows.,3 Approach,[0],[0]
"Given a user u, item i, several short phrases {d1, d2, ..., dM}, and a group of extracted aspects {A1, A2, ..., Ak}, our goal is to generate a review (w1, w2, ..., wT) that maximizes the probability P (w1:T|u, i, d1:M).",3 Approach,[0],[0]
"To solve this task, we propose a method called ExpansionNet which contains two parts: 1) three encoders to leverage the input phrases and aspect information; and 2) a decoder with an attention fusion layer to generate sequences and align the generation with the input
sources.",3 Approach,[0],[0]
The model structure is shown in Figure 1.,3 Approach,[0],[0]
"Our sequence encoder is a two-layer bi-directional GRU, as is commonly used in sequence-tosequence (Seq2Seq) models (Cho et al., 2014).","3.1 Sequence encoder, attribute encoder and aspect encoder",[0],[0]
"Input phrases first pass a word embedding layer, then go through the GRU one-by-one and finally yield a sequence of hidden states {e1, e2..., eL}.","3.1 Sequence encoder, attribute encoder and aspect encoder",[0],[0]
"In the case of multiple phrases, these share the same sequence encoder and have different lengths L. To simplify notation, we only consider one input phrase in this section.
","3.1 Sequence encoder, attribute encoder and aspect encoder",[0],[0]
The attribute encoder and aspect encoder both consist of two embedding layers and a projection layer.,"3.1 Sequence encoder, attribute encoder and aspect encoder",[0],[0]
"For the attribute encoder, we define two general embedding layers Eu ∈ R|U|×m and Ei ∈ R|I|×m to obtain the attribute latent factors γu and γi; for the aspect encoder, we use two aspect-aware embedding layers E ′","3.1 Sequence encoder, attribute encoder and aspect encoder",[0],[0]
u ∈ R|U|×k,"3.1 Sequence encoder, attribute encoder and aspect encoder",[0],[0]
and E ′,"3.1 Sequence encoder, attribute encoder and aspect encoder",[0],[0]
i ∈ R|I|×k to obtain aspect-aware latent factors βu,"3.1 Sequence encoder, attribute encoder and aspect encoder",[0],[0]
and βi.,"3.1 Sequence encoder, attribute encoder and aspect encoder",[0],[0]
"Here |U|, |I|, m and k are the number of users, number of items, the dimension of attributes, and the number of aspects, respectively.","3.1 Sequence encoder, attribute encoder and aspect encoder",[0],[0]
"After the embedding layers, the attribute and aspect-aware latent factors are concatenated and fed into a projection layer with tanh activation.","3.1 Sequence encoder, attribute encoder and aspect encoder",[0],[0]
"The outputs are calculated as:
γu = Eu(u), γi = Ei(i) (1) βu = E ′ u(u), βi = E ′ i(i) (2)
u = tanh(Wu[γu; γi] + bu) (3)
v = tanh(Wv[βu;βi] + bv) (4)
where Wu ∈","3.1 Sequence encoder, attribute encoder and aspect encoder",[0],[0]
"Rn×2m,","3.1 Sequence encoder, attribute encoder and aspect encoder",[0],[0]
"bu ∈ Rn, Wv ∈ Rn×2k, bv ∈ Rn are learnable parameters and n is the dimensionality of the hidden units in the decoder.","3.1 Sequence encoder, attribute encoder and aspect encoder",[0],[0]
The decoder is a two-layer GRU that predicts the target words given the start token.,3.2 Decoder with attention fusion layer,[0],[0]
The hidden state of the decoder is initialized using the sum of the three encoders’ outputs.,3.2 Decoder with attention fusion layer,[0],[0]
The hidden state at time-step t is updated via the GRU unit based on the previous hidden state and the input word.,3.2 Decoder with attention fusion layer,[0],[0]
"Specifically:
h0 = eL + u+ v (5)
ht = GRU(wt,ht−1), (6)
where h0 ∈ Rn is the decoder’s initial hidden state and ht ∈",3.2 Decoder with attention fusion layer,[0],[0]
"Rn is the hidden state at time-step t.
To fully exploit the encoder-side information, we apply an attention fusion layer to summarize the output of each encoder and jointly determine the final word distribution.",3.2 Decoder with attention fusion layer,[0],[0]
"For the sequence encoder, the attention vector is defined as in many other applications (Bahdanau et al., 2014; Luong et al., 2015):
a1t = L∑ j=1 α1tjej (7)
α1tj = exp(tanh(v 1 α >",3.2 Decoder with attention fusion layer,[0],[0]
"(W 1α[ej ;ht] + b 1 α)))/Z,
(8)
where a1t ∈",3.2 Decoder with attention fusion layer,[0],[0]
"Rn is the attention vector on the sequence encoder at time-step t, α1tj is the attention score over the encoder hidden state ej and decoder hidden state ht, and Z is a normalization term.
",3.2 Decoder with attention fusion layer,[0],[0]
"For the attribute encoder, the attention vector is calculated as:
a2t = ∑ j∈u,i α2tjγj (9)
α2tj = exp(tanh(v 2 α >",3.2 Decoder with attention fusion layer,[0],[0]
"(W 2α[γj ;ht] + b 2 α)))/Z,
(10)
where a2t ∈",3.2 Decoder with attention fusion layer,[0],[0]
"Rn is the attention vector on the attribute encoder, and α2tj is the attention score between the attribute latent factor γj and decoder hidden state ht.
",3.2 Decoder with attention fusion layer,[0],[0]
"Inspired by the copy mechanism (Gu et al., 2016; See et al., 2017), we design an attention vector that estimates the probability that each aspect will be discussed in the next time-step:
sui =Ws[βu;βi] + bs (11) a3t = tanh(W 3 α[sui; et;ht] + b 3 α), (12)
where sui ∈ Rk is the aspect importance considering the interaction between u and i, et is the decoder input after embedding layer at time-step t, and a3t ∈",3.2 Decoder with attention fusion layer,[0],[0]
"Rk is a probability vector to bias each aspect at time-step t. Finally, the first two attention vectors are concatenated with the decoder hidden state at time-step t and projected to obtain the output word distribution Pv.",3.2 Decoder with attention fusion layer,[0],[0]
The attention scores from the aspect encoder are then directly added to the aspect words in the final word distribution.,3.2 Decoder with attention fusion layer,[0],[0]
"The output probability for word w at time-step t is given by:
Pv(wt) =",3.2 Decoder with attention fusion layer,[0],[0]
tanh(W,3.2 Decoder with attention fusion layer,[0],[0]
[ht;a 1 t ;a 2 t ] + b),3.2 Decoder with attention fusion layer,[0],[0]
"(13)
P (wt) = Pv(wt) + a 3 t [k] · 1wt∈Ak , (14)
where wt is the target word at time-step t, a3t",3.2 Decoder with attention fusion layer,[0],[0]
"[k] is the probability that aspect k will be discussed at time-step t, Ak represents all words belonging to aspect k and 1wt∈Ak is a binary variable indicating whether wt belongs to aspect k.
During inference, we use greedy decoding by choosing the word with maximum probability, denoted as yt = argmaxwtsoftmax(P (wt)).",3.2 Decoder with attention fusion layer,[0],[0]
Decoding finishes when an end token is encountered.,3.2 Decoder with attention fusion layer,[0],[0]
"We consider a real world dataset from Amazon Electronics (McAuley et al., 2015) to evaluate our model.",4 Experiments,[0],[0]
"We convert all text into lowercase, add start and end tokens to each review, and perform tokenization using NLTK.1",4 Experiments,[0],[0]
"We discard reviews with length greater than 100 tokens and consider a vocabulary of 30,000 tokens.",4 Experiments,[0],[0]
"After preprocessing, the dataset contains 182,850 users, 59,043 items, and 992,172 reviews (sparsity 99.993%), which is much sparser than the datasets used in previous works (Dong et al., 2017; Ni et al., 2017).",4 Experiments,[0],[0]
"On average, each review contains 49.32 tokens as well as a short-text summary of 4.52 tokens.",4 Experiments,[0],[0]
"In our experiments, the basic ExpansionNet uses these summaries as input phrases.",4 Experiments,[0],[0]
"We split the dataset into training (80%), validation (10%) and test sets (10%).",4 Experiments,[0],[0]
All results are reported on the test set.,4 Experiments,[0],[0]
"We use the method2 in (He et al., 2017) to extract 15 aspects and consider the top 100 words from each aspect.",4.1 Aspect Extraction,[0],[0]
Table 2 shows 10 inferred aspects and representative words (inferred aspects are manually labeled).,4.1 Aspect Extraction,[0],[0]
"ExpansionNet calculates an attention score based on the user and item aspect-aware representation, then determines how much these representative words are biased in the output word distribution.
",4.1 Aspect Extraction,[0],[0]
1 https://www.nltk.org/ 2 https://github.com/ruidan/ Unsupervised-Aspect-Extraction,4.1 Aspect Extraction,[0],[0]
We use PyTorch3 to implement our model.4 Parameter settings are shown in Table 1.,4.2 Experiment Details,[0],[0]
"For the attribute encoder and aspect encoder, we set the dimensionality to 64 and 15 respectively.",4.2 Experiment Details,[0],[0]
"For both the sequence encoder and decoder, we use a 2- layer GRU with hidden size 512.",4.2 Experiment Details,[0],[0]
We also add dropout layers before and after the GRUs.,4.2 Experiment Details,[0],[0]
The dropout rate is set to 0.1.,4.2 Experiment Details,[0],[0]
"During training, the input sequences of the same source (e.g. review, summary) inside each batch are padded to the same length.",4.2 Experiment Details,[0],[0]
"We evaluate the model on six automatic metrics (Table 3): Perplexity, BLEU-1/BLEU-4, ROUGEL and Distinct-1/2 (percentage of distinct unigrams and bi-grams) (Li et al., 2016).",4.3 Performance Evaluation,[0],[0]
"We compare
3 http://pytorch.org/docs/master/index.html 4 https://github.com/nijianmo/textExpansion
against three baselines: Rand (randomly choose a review from the training set), GRU-LM (the GRU decoder works alone as a language model) and a state-of-the-art model Attr2Seq that only considers user and item attribute (Dong et al., 2017).",4.3 Performance Evaluation,[0],[0]
"ExpansionNet (with summary, item title, attribute and aspect as input) achieves significant improvements over Attr2Seq on all metrics.",4.3 Performance Evaluation,[0],[0]
"As we add more input information, the model continues to obtain better results, except for the ROUGE-L metric.",4.3 Performance Evaluation,[0],[0]
"This proves that our model can effectively learn from short input phrases and aspect information and improve the correctness and diversity of generated results.
",4.3 Performance Evaluation,[0],[0]
Figure 2 presents a sample generation result.,4.3 Performance Evaluation,[0],[0]
"ExpansionNet captures fine-grained item information (e.g. that the item is a tablet), which Attr2Seq fails to recognize.",4.3 Performance Evaluation,[0],[0]
"Moreover, given a phrase like “easy to use” in the summary, ExpansionNet generates reviews containing the same text.",4.3 Performance Evaluation,[0],[0]
This demonstrates the possibility of using our model in an assistive review generation scenario.,4.3 Performance Evaluation,[0],[0]
"Finally, given extra aspect information, the model successfully estimates that the screen would be an important aspect (i.e., for the current user and item); it generates phrases such as “screen is very respon-
sive” about the aspect “screen” which is also covered in the real (ground-truth) review (“display is beautiful”).
",4.3 Performance Evaluation,[0],[0]
We are also interested in seeing how the aspectaware representation can find related aspects and bias the generation to discuss more about those aspects.,4.3 Performance Evaluation,[0],[0]
We analyze the average number of aspects in real and generated reviews and show on average how many aspects in real reviews are covered in generated reviews.,4.3 Performance Evaluation,[0],[0]
We consider a review as covering an aspect if any of the aspect’s representative words exists in the review.,4.3 Performance Evaluation,[0],[0]
"As shown in Table 4, Attr2Seq tends to cover more aspects in generation, many of which are not discussed in real reviews.",4.3 Performance Evaluation,[0],[0]
"On the other hand, ExpansionNet better captures the distribution of aspects that are discussed in real reviews.",4.3 Performance Evaluation,[0],[0]
"In this paper, we focus on the problem of building assistive systems that can help users to write reviews.",abstractText,[0],[0]
"We cast this problem using an encoder-decoder framework that generates personalized reviews by expanding short phrases (e.g. review summaries, product titles) provided as input to the system.",abstractText,[0],[0]
We incorporate aspect-level information via an aspect encoder that learns ‘aspect-aware’ user and item representations.,abstractText,[0],[0]
An attention fusion layer is applied to control generation by attending on the outputs of multiple encoders.,abstractText,[0],[0]
Experimental results show that our model is capable of generating coherent and diverse reviews that expand the contents of input phrases.,abstractText,[0],[0]
"In addition, the learned aspectaware representations discover those aspects that users are more inclined to discuss and bias the generated text toward their personalized aspect preferences.",abstractText,[0],[0]
Personalized Review Generation by Expanding Phrases and Attending on Aspect-Aware Representations,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2204–2213 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
2204",text,[0],[0]
"Despite much recent success in natural language processing and dialogue research, communication between a human and a machine is still in its infancy.",1 Introduction,[0],[0]
It is only recently that neural models have had sufficient capacity and access to sufficiently large datasets that they appear to generate meaningful responses in a chit-chat setting.,1 Introduction,[0],[0]
"Still, conversing with such generic chit-chat models for even a short amount of time quickly exposes their weaknesses (Serban et al., 2016; Vinyals and Le, 2015).
",1 Introduction,[0],[0]
"Common issues with chit-chat models include: (i) the lack of a consistent personality (Li et al., 2016a) as they are typically trained over many dialogs each with different speakers, (ii) the lack of an explicit long-term memory as they are typically trained to produce an utterance given only the recent dialogue history (Vinyals and Le, 2015);
1Work done while at Facebook AI Research.
and (iii) a tendency to produce non-specific answers like “I don’t know” (Li et al., 2015).",1 Introduction,[0.99715090041032],"['Common issues with chit-chat models include: (i) the lack of a consistent personality (Li et al., 2016a) as they are typically trained over many dialogs each with different speakers, (ii) the lack of an explicit long-term memory as they are typically trained to produce an utterance given only the recent dialogue history (Vinyals and Le, 2015); and (iii) a tendency to produce non-specific answers like “I don’t know” (Li et al., 2015).']"
Those three problems combine to produce an unsatisfying overall experience for a human to engage with.,1 Introduction,[0],[0]
"We believe some of those problems are due to there being no good publicly available dataset for general chit-chat.
",1 Introduction,[0],[0]
"Because of the low quality of current conversational models, and because of the difficulty in evaluating these models, chit-chat is often ignored as an end-application.",1 Introduction,[0],[0]
"Instead, the research community has focused on task-oriented communication, such as airline or restaurant booking (Bordes and Weston, 2016), or else single-turn information seeking, i.e. question answering (Rajpurkar et al., 2016).",1 Introduction,[0],[0]
"Despite the success of the latter, simpler, domain, it is well-known that a large quantity of human dialogue centers on socialization, personal interests and chit-chat (Dunbar et al., 1997).",1 Introduction,[0],[0]
"For example, less than 5% of posts on Twitter are questions, whereas around 80% are about personal emotional state, thoughts or activities, authored by so called “Meformers” (Naaman et al., 2010).
",1 Introduction,[0],[0]
"In this work we make a step towards more engaging chit-chat dialogue agents by endowing them with a configurable, but persistent persona, encoded by multiple sentences of textual description, termed a profile.",1 Introduction,[1.0],"['In this work we make a step towards more engaging chit-chat dialogue agents by endowing them with a configurable, but persistent persona, encoded by multiple sentences of textual description, termed a profile.']"
"This profile can be stored in a memory-augmented neural network and then used to produce more personal, specific, consistent and engaging responses than a persona-free model, thus alleviating some of the common issues in chit-chat models.",1 Introduction,[0],[0]
"Using the same mechanism, any existing information about the persona of the dialogue partner can also be used in the same way.",1 Introduction,[1.0],"['Using the same mechanism, any existing information about the persona of the dialogue partner can also be used in the same way.']"
"Our models are thus trained to both ask and answer questions about personal topics, and the resulting dialogue can be used to build a model of the persona of the speaking partner.
",1 Introduction,[0],[0]
"To support the training of such models, we
present the PERSONA-CHAT dataset, a new dialogue dataset consisting of 164,356 utterances between crowdworkers who were randomly paired and each asked to act the part of a given provided persona (randomly assigned, and created by another set of crowdworkers).",1 Introduction,[0],[0]
The paired workers were asked to chat naturally and to get to know each other during the conversation.,1 Introduction,[0],[0]
"This produces interesting and engaging conversations that our agents can try to learn to mimic.
",1 Introduction,[0],[0]
"Studying the next utterance prediction task during dialogue, we compare a range of models: both generative and ranking models, including Seq2Seq models and Memory Networks (Sukhbaatar et al., 2015) as well as other standard retrieval baselines.",1 Introduction,[0],[0]
We show experimentally that in either the generative or ranking case conditioning the agent with persona information gives improved prediction of the next dialogue utterance.,1 Introduction,[1.0],['We show experimentally that in either the generative or ranking case conditioning the agent with persona information gives improved prediction of the next dialogue utterance.']
"The PERSONA-CHAT dataset is designed to facilitate research into alleviating some of the issues that traditional chitchat models face, and with the aim of making such models more consistent and engaging, by endowing them with a persona.",1 Introduction,[0],[0]
"By comparing against chit-chat models built using the OpenSubtitles and Twitter datasets, human evaluations show that our dataset provides more engaging models, that are simultaneously capable of being fluent and consistent via conditioning on a persistent, recognizable profile.",1 Introduction,[0],[0]
"Traditional dialogue systems consist of building blocks, such as dialogue state tracking components and response generators, and have typically been applied to tasks with labeled internal dialogue state and precisely defined user intent (i.e., goal-oriented dialogue), see e.g. (Young, 2000).",2 Related Work,[0],[0]
"The most successful goal-oriented dialogue systems model conversation as partially observable Markov decision processes (POMDPs) (Young et al., 2013).",2 Related Work,[0],[0]
All those methods typically do not consider the chit-chat setting and are more concerned with achieving functional goals (e.g. booking an airline flight) than displaying a personality.,2 Related Work,[0],[0]
"In particular, many of the tasks and datasets available are constrained to narrow domains (Serban et al., 2015).
",2 Related Work,[0],[0]
"Non-goal driven dialogue systems go back to Weizenbaum’s famous program ELIZA (Weizenbaum, 1966), and hand-coded systems have con-
tinued to be used in applications to this day.",2 Related Work,[0],[0]
"For example, modern solutions that build an openended dialogue system to the Alexa challenge combine hand-coded and machine-learned elements (Serban et al., 2017a).",2 Related Work,[0],[0]
"Amongst the simplest of statistical systems that can be used in this domain, that are based on data rather than handcoding, are information retrieval models (Sordoni et al., 2015), which retrieve and rank responses based on their matching score with the recent dialogue history.",2 Related Work,[0],[0]
"We use IR systems as a baseline in this work.
",2 Related Work,[0],[0]
End-to-end neural approaches are a class of models which have seen growing recent interest.,2 Related Work,[0],[0]
"A popular class of methods are generative recurrent systems like seq2seq applied to dialogue (Sutskever et al., 2014; Vinyals and Le, 2015; Sordoni et al., 2015; Li et al., 2016b; Serban et al., 2017b).",2 Related Work,[0],[0]
"Rooted in language modeling, they are able to produce syntactically coherent novel responses, but their memory-free approach means they lack long-term coherence and a persistent personality, as discussed before.",2 Related Work,[0],[0]
"A promising direction, that is still in its infancy, to fix this issue is to use a memory-augmented network instead (Sukhbaatar et al., 2015; Dodge et al., 2015) by providing or learning appropriate memories.
",2 Related Work,[0],[0]
Serban et al. (2015) list available corpora for training dialogue systems.,2 Related Work,[0],[0]
"Perhaps the most relevant to learning chit-chat models are ones based on movie scripts such as OpenSubtitles and Cornell Movie-Dialogue Corpus, and dialogue from web platforms such as Reddit and Twitter, all of which have been used for training neural approaches (Vinyals and Le, 2015; Dodge et al., 2015; Li et al., 2016b; Serban et al., 2017b).",2 Related Work,[0],[0]
Naively training on these datasets leads to models with the lack of a consistent personality as they will learn a model averaged over many different speakers.,2 Related Work,[0],[0]
"Moreover, the data does little to encourage the model to engage in understanding and maintaining knowledge of the dialogue partner’s personality and topic interests.
",2 Related Work,[0],[0]
"According to Serban et al. (2015)’s survey, personalization of dialogue systems is “an important task, which so far has not received much attention”.",2 Related Work,[0],[0]
"In the case of goal-oriented dialogue some work has focused on the agent being aware of the human’s profile and adjusting the dialogue accordingly, but without a personality to the agent itself (Lucas et al., 2009; Joshi et al., 2017).",2 Related Work,[0],[0]
"For
the chit-chat setting, the most relevant work is (Li et al., 2016a).",2 Related Work,[0],[0]
"For each user in the Twitter corpus, personas were captured via distributed embeddings (one per speaker) to encapsulate individual characteristics such as background information and speaking style, and they then showed using those vectors improved the output of their seq2seq model for the same speaker.",2 Related Work,[0],[0]
"Their work does not focus on attempting to engage the other speaker by getting to know them, as we do here.",2 Related Work,[0],[0]
"For that reason, our focus is on explicit profile information, not hard-to-interpret latent variables.
",2 Related Work,[0],[0]
3,2 Related Work,[0],[0]
"The PERSONA-CHAT Dataset
The aim of this work is to facilitate more engaging and more personal chit-chat dialogue.",2 Related Work,[0],[0]
"The PERSONA-CHAT dataset is a crowd-sourced dataset, collected via Amazon Mechanical Turk, where each of the pair of speakers condition their dialogue on a given profile, which is provided.
",2 Related Work,[0],[0]
"The data collection consists of three stages: (i) Personas: we crowdsource a set of 1155 possible personas, each consisting of at least 5 profile sentences, setting aside 100 never seen before personas for validation, and 100 for test.
",2 Related Work,[0],[0]
"(ii) Revised personas: to avoid modeling that takes advantage of trivial word overlap, we crowdsource additional rewritten sets of the same 1155 personas, with related sentences that are rephrases, generalizations or specializations, rendering the task much more challenging.
",2 Related Work,[0],[0]
"(iii) Persona chat: we pair two Turkers and assign them each a random (original) persona from the pool, and ask them to chat.",2 Related Work,[0],[0]
"This resulted in a dataset of 164,356 utterances over 10,981 dialogs, 15,705 utterances (968 dialogs) of which are set aside for validation, and 15,119 utterances (1000 dialogs) for test.
",2 Related Work,[0],[0]
"The final dataset and its corresponding data collection source code, as well as models trained on the data, are all available open source in ParlAI2.
",2 Related Work,[0],[0]
"In the following, we describe each data collection stage and the resulting tasks in more detail.",2 Related Work,[0],[0]
"We asked the crowdsourced workers to create a character (persona) description using 5 sentences, providing them only a single example:
2https://github.com/facebookresearch/ ParlAI/tree/master/projects/personachat
“I am a vegetarian.",3.1 Personas,[0],[0]
I like swimming.,3.1 Personas,[0],[0]
My father used to work for Ford.,3.1 Personas,[0],[0]
My favorite band is Maroon5.,3.1 Personas,[0],[0]
"I got a new job last month, which is about advertising design.”
",3.1 Personas,[0],[0]
"Our aim was to create profiles that are natural and descriptive, and contain typical topics of human interest that the speaker can bring up in conversation.",3.1 Personas,[0],[0]
"Because the personas are not the real profiles of the Turkers, the dataset does not contain personal information (and they are told specifically not to use any).",3.1 Personas,[0],[0]
"We asked the workers to make each sentence short, with a maximum of 15 words per sentence.",3.1 Personas,[0],[0]
"This is advantageous both for humans and machines: if they are too long, crowdsourced workers are likely to lose interest, and for machines the task could become more difficult.
",3.1 Personas,[0],[0]
Some examples of the personas collected are given in Table 1 (left).,3.1 Personas,[0],[0]
"A difficulty when constructing dialogue datasets, or text datasets in general, is that in order to encourage research progress, the task must be carefully constructed so that is neither too easy nor too difficult for the current technology (Voorhees et al., 1999).",3.2 Revised Personas,[0],[0]
"One issue with conditioning on textual personas is that there is a danger that humans will, even if asked not to, unwittingly repeat profile information either verbatim or with significant word overlap.",3.2 Revised Personas,[0],[0]
"This may make any subsequent machine learning tasks less challenging, and the solutions will not generalize to more difficult tasks.",3.2 Revised Personas,[0],[0]
"This has been a problem in some recent datasets: for example, the dataset curation technique used for the well-known SQuAD dataset suffers from this word overlap problem to a certain extent (Chen et al., 2017).
",3.2 Revised Personas,[0],[0]
"To alleviate this problem, we presented the original personas we collected to a new set of crowdworkers and asked them to rewrite the sentences so that a new sentence is about “a related characteristic that the same person may have”, hence the revisions could be rephrases, generalizations or specializations.",3.2 Revised Personas,[0],[0]
"For example “I like basketball” can be revised as “I am a big fan of Michael Jordan” not because they mean the same thing but because the same persona could contain both.
",3.2 Revised Personas,[0.9999999693944218],['For example “I like basketball” can be revised as “I am a big fan of Michael Jordan” not because they mean the same thing but because the same persona could contain both.']
"In the revision task, workers are instructed not to trivially rephrase the sentence by copying the original words.",3.2 Revised Personas,[0],[0]
"However, during the entry stage if a non-stop word is copied we issue a warning,
and ask them to rephrase, guaranteeing that the instructions are followed.",3.2 Revised Personas,[0],[0]
"For example, “My father worked for Ford.”",3.2 Revised Personas,[0],[0]
"can be revised to “My dad worked in the car industry”, but not “My dad was employed by Ford.”",3.2 Revised Personas,[0.9798979615425253],"['For example, “My father worked for Ford.” can be revised to “My dad worked in the car industry”, but not “My dad was employed by Ford.” due to word overlap.']"
"due to word overlap.
",3.2 Revised Personas,[0],[0]
Some examples of the revised personas collected are given in Table 1 (right).,3.2 Revised Personas,[0],[0]
"After collecting personas, we then collected the dialogues themselves, conditioned on the personas.",3.3 Persona Chat,[1.0],"['After collecting personas, we then collected the dialogues themselves, conditioned on the personas.']"
"For each dialogue, we paired two random crowdworkers, and gave them the instruction that they will chit-chat with another worker, while playing the part of a given character.",3.3 Persona Chat,[0],[0]
"We then provide them with a randomly chosen persona from our pool, different to their partners.",3.3 Persona Chat,[0],[0]
"The instructions are on
purpose quite terse and simply ask them to “chat with the other person naturally and try to get to know each other”.",3.3 Persona Chat,[0],[0]
"In an early study we noticed the crowdworkers tending to talk about themselves (their own persona) too much, so we also added the instructions “both ask questions and answer questions of your chat partner” which seemed to help.",3.3 Persona Chat,[0],[0]
We also gave a bonus for high quality dialogs.,3.3 Persona Chat,[0],[0]
"The dialog is turn-based, with a maximum of 15 words per message.",3.3 Persona Chat,[0],[0]
"We again gave instructions to not trivially copy the character descriptions into the messages, but also wrote explicit code sending them an error if they tried to do so, using simple string matching.",3.3 Persona Chat,[0],[0]
We define a minimum dialogue length which is randomly between 6 and 8 turns each for each dialogue.,3.3 Persona Chat,[0],[0]
An example dialogue from the dataset is given in Table 2.,3.3 Persona Chat,[0],[0]
"We focus on the standard dialogue task of predicting the next utterance given the dialogue history, but consider this task both with and without the profile information being given to the learning agent.",3.4 Evaluation,[0],[0]
"Our goal is to enable interesting directions for future research, where chatbots can for instance have personalities, or imputed personas could be used to make dialogue more engaging to the user.
",3.4 Evaluation,[0],[0]
"We consider this in four possible scenarios: conditioning on no persona, your own persona, their persona, or both.",3.4 Evaluation,[0],[0]
"These scenarios can be tried using either the original personas, or the revised ones.",3.4 Evaluation,[0],[0]
"We then evaluate the task using three metrics: (i) the log likelihood of the correct sequence, measured via perplexity, (ii) F1 score, and (iii) next utterance classification loss, following Lowe et al. (2015).",3.4 Evaluation,[0],[0]
"The latter consists of choosing N random distractor responses from other dialogues (in our setting, N=19) and the model selecting the best response among them, resulting in a score of one if the model chooses the correct response, and zero otherwise (called hits@1 in the experiments).",3.4 Evaluation,[0],[0]
We consider two classes of model for next utterance prediction: ranking models and generative models.,4 Models,[0],[0]
Ranking models produce a next utterance by considering any utterance in the training set as a possible candidate reply.,4 Models,[0],[0]
"Generative models generate novel sentences by conditioning on the dialogue history (and possibly, the persona), and then generating the response word-by-word.",4 Models,[0],[0]
"Note one can still evaluate the latter as ranking models by computing the probability of generating a given candidate, and ranking candidates by those scores.",4 Models,[1.0],"['Note one can still evaluate the latter as ranking models by computing the probability of generating a given candidate, and ranking candidates by those scores.']"
"We first consider two baseline models, an IR baseline (Sordoni et al., 2015) and a supervised embedding model, Starspace (Wu et al., 2017)3.",4.1 Baseline ranking models,[0],[0]
"While there are many IR variants, we adopt the simplest one: find the most similar message in the (training) dataset and output the response from that exchange.",4.1 Baseline ranking models,[0],[0]
Similarity is measured by the tfidf weighted cosine similarity between the bags of words.,4.1 Baseline ranking models,[0],[0]
"Starspace is a recent model that also performs information retrieval but by learning the
3github.com/facebookresearch/StarSpace
similarity between the dialog and the next utterance by optimizing the embeddings directly for that task using the margin ranking loss and k-negative sampling.",4.1 Baseline ranking models,[0],[0]
"The similarity function sim(q, c′) is the cosine similarity of the sum of word embeddings of the query q and candidate c′.",4.1 Baseline ranking models,[0],[0]
"Denoting the dictionary of D word embeddings as W which is a D× d matrix, where Wi indexes the ith word (row), yielding its d-dimensional embedding, it embeds the sequences q and c′.
In both methods, IR and StarSpace, to incorporate the profile we simply concatenate it to the query vector bag of words.",4.1 Baseline ranking models,[0],[0]
"Both the previous models use the profile information by combining it with the dialogue history, which means those models cannot differentiate between the two when deciding on the next utterance.",4.2 Ranking Profile Memory Network,[0],[0]
"In this model we instead use a memory network with the dialogue history as input, which then performs attention over the profile to find relevant lines from the profile to combine with the input, and then finally predicts the next utterance.",4.2 Ranking Profile Memory Network,[0],[0]
"We use the same representation and loss as in the Starspace model, so without the profile, the two models are identical.",4.2 Ranking Profile Memory Network,[0],[0]
"When the profile is available attention is performed by computing the similarity of the input q with the profile sentences pi, computing the softmax, and taking the weighted sum:
q+ = q+ ∑ sipi, si = Softmax(sim(q, pi))",4.2 Ranking Profile Memory Network,[0],[0]
where Softmax(zi) = ezi/ ∑ j e zj .,4.2 Ranking Profile Memory Network,[0],[0]
"One can then rank the candidates c′ using sim(q+, c′).",4.2 Ranking Profile Memory Network,[1.0],"['One can then rank the candidates c′ using sim(q+, c′).']"
"One can also perform multiple “hops” of attention over the profile rather than one, as shown here, although that did not bring significant gains in our parameter sweeps.",4.2 Ranking Profile Memory Network,[1.0],"['One can also perform multiple “hops” of attention over the profile rather than one, as shown here, although that did not bring significant gains in our parameter sweeps.']"
"The key-value (KV) memory network (Miller et al., 2016) was proposed as an improvement to the memory network by performing attention over keys and outputting the values (instead of the same keys as in the original), which can outperform memory networks dependent on the task and definition of the key-value pairs.",4.3 Key-Value Profile Memory Network,[0],[0]
"Here, we apply this model to dialogue, and consider the keys as dialog histories (from the training set), and the values as the next dialogue utterances, i.e., the replies from the speaking partner.",4.3 Key-Value Profile Memory Network,[0],[0]
"This allows the model
to have a memory of past dialogues that it can directly use to help influence its prediction for the current conversation.",4.3 Key-Value Profile Memory Network,[0],[0]
"The model we choose is identical to the profile memory network just described in the first hop over profiles, while in the second hop, q+ is used to attend over the keys and output a weighted sum of values as before, producing q++.",4.3 Key-Value Profile Memory Network,[0],[0]
"This is then used to rank the candidates c′ using sim(q++, c′) as before.",4.3 Key-Value Profile Memory Network,[0],[0]
As the set of (key-value) pairs is large this would make training very slow.,4.3 Key-Value Profile Memory Network,[0],[0]
In our experiments we simply trained the profile memory network and used the same weights from that model and applied this architecture at test time instead.,4.3 Key-Value Profile Memory Network,[0],[0]
"Training the model directly would presumably give better results, however this heuristic already proved beneficial compared to the original network.",4.3 Key-Value Profile Memory Network,[0],[0]
The input sequence x is encoded by applying het = LSTMenc(xt | het−1).,4.4 Seq2Seq,[0],[0]
"We use GloVe (Pennington et al., 2014) for our word embeddings.",4.4 Seq2Seq,[0],[0]
"The final hidden state, het , is fed into the decoder LSTMdec as the initial state hd0.",4.4 Seq2Seq,[0],[0]
"For each time step t, the decoder then produces the probability of a word j occurring in that place via the softmax, i.e.,
p(yt,j = 1 | yt−1, . . .",4.4 Seq2Seq,[0],[0]
", y1) = exp(wjh d t )",4.4 Seq2Seq,[0],[0]
"∑K
j′=1 exp(wj′h d t ) .
",4.4 Seq2Seq,[0],[0]
The model is trained via negative log likelihood.,4.4 Seq2Seq,[0],[0]
"The basic model can be extended to include persona information, in which case we simply prepend it to the input sequence x, i.e., x = ∀p ∈ P ||",4.4 Seq2Seq,[0],[0]
"x, where || denotes concatenation.",4.4 Seq2Seq,[0],[0]
"For the OpenSubtitles and Twitter datasets trained in Section 5.2 we found training a language model (LM), essentially just the decoder part of this model, worked better and we report that instead.",4.4 Seq2Seq,[0],[0]
"Finally, we introduce a generative model that encodes each of the profile entries as individual memory representations in a memory network.",4.5 Generative Profile Memory Network,[0],[0]
"As before, the dialogue history is encoded via LSTMenc, the final state of which is used as the initial hidden state of the decoder.",4.5 Generative Profile Memory Network,[0],[0]
"Each entry pi = 〈pi,1, . . .",4.5 Generative Profile Memory Network,[0],[0]
", pi,n〉 ∈ P is then encoded via f(pi)",4.5 Generative Profile Memory Network,[0],[0]
"=∑|pi|
j αipi,j .",4.5 Generative Profile Memory Network,[0],[0]
"That is, we weight words by their inverse term frequency: αi = 1/(1 + log(1 + tf)) where tf is computed from the GloVe index via
Zipf’s law4.",4.5 Generative Profile Memory Network,[0],[0]
Let F be the set of encoded memories.,4.5 Generative Profile Memory Network,[0],[0]
"The decoder now attends over the encoded profile entries, i.e., we compute the mask at, context ct and next input x̂t as:
at = softmax(FWah d t ),
ct = a ᵀ tF ; x̂t = tanh(Wc[ct−1, xt]).
",4.5 Generative Profile Memory Network,[0],[0]
"If the model has no profile information, and hence no memory, it becomes equivalent to the Seq2Seq model.",4.5 Generative Profile Memory Network,[1.0],"['If the model has no profile information, and hence no memory, it becomes equivalent to the Seq2Seq model.']"
"We first report results using automated evaluation metrics, and subsequently perform an extrinsic evaluation where crowdsourced workers perform a human evaluation of our models.",5 Experiments,[0],[0]
The main results are reported in Table 3.,5.1 Automated metrics,[0],[0]
"Overall, the results show the following key points:
Persona Conditioning Most models improve significantly when conditioning prediction on their own persona at least for the original (non-revised) versions, which is an easier task than the revised ones which have no word overlap.",5.1 Automated metrics,[0],[0]
"For example, the Profile Memory generation model has improved perplexity and hits@1 compared to Seq2Seq, and all the ranking algorithms (IR baseline, Starspace and Profile Memory Networks) obtain improved hits@1.
",5.1 Automated metrics,[0],[0]
Ranking vs. Generative.,5.1 Automated metrics,[0],[0]
Ranking models are far better than generative models at ranking.,5.1 Automated metrics,[0],[0]
"This is perhaps obvious as that is the metric they are optimizing, but still the performance difference is quite stark.",5.1 Automated metrics,[0],[0]
"It may be that the word-based probability which generative models use works well, but is not calibrated well enough to give a sentencebased probability which ranking requires.",5.1 Automated metrics,[0],[0]
"Human evaluation is also used to compare these methods, which we perform in Sec. 5.2.
",5.1 Automated metrics,[0],[0]
Ranking Models.,5.1 Automated metrics,[0],[0]
"For the ranking models, the IR baseline is outperformed by Starspace due to its learnt similarity metric, which in turn is outperformed by Profile Memory networks due to the attention mechanism over the profiles (as all other parts of the models are the same).",5.1 Automated metrics,[0],[0]
"Finally KV Profile Memory networks outperform Profile Memory Networks in the no persona case due to the ability to consider neighboring dialogue history and next
4tf = 1e6 ∗ 1/(idx1.07)
utterance pairs in the training set that are similar to the current dialogue, however when using persona information the performance is similar.
",5.1 Automated metrics,[0],[0]
Revised Personas.,5.1 Automated metrics,[0],[0]
Revised personas are much harder to use.,5.1 Automated metrics,[0],[0]
We do however still see some gain for the Profile Memory networks compared to none (0.354 vs. 0.318 hits@1).,5.1 Automated metrics,[0],[0]
"We also tried two variants of training: with the original personas in the training set or the revised ones, a comparison of which is shown in Table 6 of the Appendix.",5.1 Automated metrics,[0],[0]
"Training on revised personas helps, both for test examples that are in original form or revised form, likely due to the model be forced to learn more than simple word overlap, forcing the model to generalize more (i.e., learn semantic similarity of differing phrases).
",5.1 Automated metrics,[0],[0]
Their Persona.,5.1 Automated metrics,[0],[0]
"We can also condition a model on the other speaker’s persona, or both personas at once, the results of which are in Tables 5 and 6 in the Appendix.",5.1 Automated metrics,[0],[0]
Using “Their persona” has less impact on this dataset.,5.1 Automated metrics,[0],[0]
We believe this is because most speakers tend to focus on themselves when it comes to their interests.,5.1 Automated metrics,[0],[0]
It would be interesting how often this is the case in other datasets.,5.1 Automated metrics,[0],[0]
Certainly this is skewed by the particular instructions one could give to the crowdworkers.,5.1 Automated metrics,[0],[0]
"For example if we gave the instructions “try not to talk about yourself, but about the other’s interests’ likely these metrics would change.",5.1 Automated metrics,[0],[0]
"As automated metrics are notoriously poor for evaluating dialogue (Liu et al., 2016)",5.2 Human Evaluation,[0],[0]
we also perform human evaluation using crowdsourced workers.,5.2 Human Evaluation,[0],[0]
The procedure is as follows.,5.2 Human Evaluation,[0],[0]
We perform almost exactly the same setup as in the dataset collection process itself as in Section 3.3.,5.2 Human Evaluation,[0],[0]
"In that setup, we paired two Turkers and assigned them each a random (original) persona from the collected pool, and asked them to chat.",5.2 Human Evaluation,[0],[0]
"Here, from the Turker’s point of view everything looks the same except instead of being paired with a Turker they are paired with one of our models instead (they do not know this).",5.2 Human Evaluation,[1.0],"['Here, from the Turker’s point of view everything looks the same except instead of being paired with a Turker they are paired with one of our models instead (they do not know this).']"
"In this setting, for both the Turker and the model, the personas come from the test set pool.
",5.2 Human Evaluation,[0],[0]
"After the dialogue, we then ask the Turker some additional questions in order to evaluate the quality of the model.",5.2 Human Evaluation,[0],[0]
"We ask them to evaluate fluency, engagingness and consistency (scored between 1- 5).",5.2 Human Evaluation,[1.0],"['We ask them to evaluate fluency, engagingness and consistency (scored between 1- 5).']"
"Finally, we measure the ability to detect the other speaker’s profile by displaying two possible profiles, and ask which is more likely to be the profile of the person the Turker just spoke to.",5.2 Human Evaluation,[0],[0]
"More details of these measures are given in the Appendix.
",5.2 Human Evaluation,[0],[0]
"The results are reported in Table 4 for the best performing generative and ranking models, in both the No Persona and Self Persona categories, 100 dialogues each.",5.2 Human Evaluation,[1.0],"['The results are reported in Table 4 for the best performing generative and ranking models, in both the No Persona and Self Persona categories, 100 dialogues each.']"
We also evaluate the scores of human performance by replacing the chatbot with a human (another Turker).,5.2 Human Evaluation,[1.0],['We also evaluate the scores of human performance by replacing the chatbot with a human (another Turker).']
This effectively gives us upper bound scores which we can aim for with our models.,5.2 Human Evaluation,[0],[0]
"Finally, and importantly, we compare our models trained on PERSONA-CHAT with chit-chat models trained with the Twitter and OpenSubtitles datasets (2009 and 2018 versions) instead, following Vinyals and Le (2015).",5.2 Human Evaluation,[0],[0]
"Example chats from a few of the models are shown in the Appendix in Tables 7, 8, 9, 10, 11 and 12.
",5.2 Human Evaluation,[0.9999999878342959],"['Example chats from a few of the models are shown in the Appendix in Tables 7, 8, 9, 10, 11 and 12.']"
"Firstly, we see a difference in fluency, engagingness and consistency between all PERSONACHAT models and the models trained on OpenSubtitles and Twitter.",5.2 Human Evaluation,[0],[0]
"PERSONA-CHAT is a resource that is particularly strong at providing training data for the beginning of conversations, when the two speakers do not know each other, focusing on asking and answering questions, in contrast to other resources.",5.2 Human Evaluation,[0],[0]
"We also see suggestions of more subtle differences between the models, although these differences are obscured by the high variance of
the human raters’ evaluations.",5.2 Human Evaluation,[1.0000000426493962],"['We also see suggestions of more subtle differences between the models, although these differences are obscured by the high variance of the human raters’ evaluations.']"
"For example, in both the generative and ranking model cases, models endowed with a persona can be detected by the human conversation partner, as evidenced by the persona detection accuracies, whilst maintaining fluency and consistency compared to their nonpersona driven counterparts.
",5.2 Human Evaluation,[0],[0]
"Finding the balance between fluency, engagement, consistency, and a persistent persona remains a strong challenge for future research.",5.2 Human Evaluation,[1.0],"['Finding the balance between fluency, engagement, consistency, and a persistent persona remains a strong challenge for future research.']"
"Two tasks could naturally be considered using PERSONACHAT: (1) next utterance prediction during dialogue, and (2) profile prediction given dialogue history.",5.3 Profile Prediction,[1.0],"['Two tasks could naturally be considered using PERSONACHAT: (1) next utterance prediction during dialogue, and (2) profile prediction given dialogue history.']"
"The main study of this work has been Task 1, where we have shown the use of profile information.",5.3 Profile Prediction,[0],[0]
"Task 2, however, can be used to extract such information.",5.3 Profile Prediction,[0],[0]
"While a full study is beyond the scope of this paper, we conducted some preliminary experiments, the details of which are in Appendix D. They show (i) human speaker’s profiles can be predicted from their dialogue with high accuracy (94.3%, similar to human performance in Table 4) or even from the model’s dialogue (23% with KV Profile Memory) showing the model is paying attention to the human’s interests.",5.3 Profile Prediction,[1.0],"['While a full study is beyond the scope of this paper, we conducted some preliminary experiments, the details of which are in Appendix D. They show (i) human speaker’s profiles can be predicted from their dialogue with high accuracy (94.3%, similar to human performance in Table 4) or even from the model’s dialogue (23% with KV Profile Memory) showing the model is paying attention to the human’s interests.']"
"Further, the accuracies clearly improve with further dialogue, as shown in Table 14.",5.3 Profile Prediction,[0],[0]
Combining Task 1 and Task 2 into a full system is an exciting area of future research.,5.3 Profile Prediction,[0],[0]
"In this work we have introduced the PERSONACHAT dataset, which consists of crowd-sourced dialogues where each participant plays the part of an assigned persona; and each (crowd-sourced) persona has a word-distinct paraphrase.",6 Conclusion & Discussion,[0],[0]
"We test various baseline models on this dataset, and show that models that have access to their own personas in addition to the state of the dialogue are scored as more consistent by annotators, although not more engaging.",6 Conclusion & Discussion,[0],[0]
"On the other hand, we show that models trained on PERSONA-CHAT (with or without personas) are more engaging than models trained on dialogue from other resources (movies, Twitter).
",6 Conclusion & Discussion,[0],[0]
We believe PERSONA-CHAT will be a useful resource for training components of future dialogue systems.,6 Conclusion & Discussion,[0],[0]
"Because we have paired human generated profiles and conversations, the data aids the construction of agents that have consistent per-
sonalities and viewpoints.",6 Conclusion & Discussion,[0],[0]
"Furthermore, predicting the profiles from a conversation moves chitchat tasks in the direction of goal-directed dialogue, which has metrics for success.",6 Conclusion & Discussion,[0],[0]
"Because we collect paraphrases of the profiles, they cannot be trivially matched; indeed, we believe the original and rephrased profiles are interesting as a semantic similarity dataset in their own right.",6 Conclusion & Discussion,[0],[0]
"We hope that the data will aid training agents that can ask questions about users’ profiles, remember the answers, and use them naturally in conversation.",6 Conclusion & Discussion,[0],[0]
"Chit-chat models are known to have several problems: they lack specificity, do not display a consistent personality and are often not very captivating.",abstractText,[0],[0]
In this work we present the task of making chit-chat more engaging by conditioning on profile information.,abstractText,[0],[0]
"We collect data and train models to (i) condition on their given profile information; and (ii) information about the person they are talking to, resulting in improved dialogues, as measured by next utterance prediction.",abstractText,[0],[0]
"Since (ii) is initially unknown, our model is trained to engage its partner with personal topics, and we show the resulting dialogue can be used to predict profile information about the interlocutors.",abstractText,[0],[0]
"Personalizing Dialogue Agents: I have a dog, do you have pets too?",title,[0],[0]
