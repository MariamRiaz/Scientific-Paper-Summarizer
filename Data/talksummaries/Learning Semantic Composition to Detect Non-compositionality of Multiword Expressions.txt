6	18	Collocations constitute the largest subset of all kinds of MWEs, however, non-compositional ones cause more problems in various NLP tasks, for example word sense disambiguation (McCarthy et al., 2003) and machine translation (Lin, 1999).
9	61	In this paper, we capture non-compositionality of English Noun Compounds (NCs)2 based on the assumption that the majority of the compounds are compositional, for which a composition function can be learned.
11	31	In previous work on vector-space models of distributional semantics, semantic composition has been commonly assumed to be a trivial predetermined function such as addition, multiplication, 1733 and their weighted variations (Mitchell and Lapata, 2008; Reddy et al., 2011; Salehi et al., 2015).
18	69	An overly powerful composition function memorizes all compositional and non-compositional compounds, resulting in overfitting and low learning error that hinders discrimination between compositional and non-compositional compounds.
66	27	Word embeddings have proven to be effective models of semantic representation of words and outperform the count-based models in various NLP tasks (Baroni et al., 2014; Collobert et al., 2011; Collobert and Weston, 2008; Yazdani and Popescu-Belis, 2013; Huang et al., 2012; Mikolov et al., 2013c).
68	21	In this work we use word embeddings of Mikolov et al. (2013a) to represent the semantics of words and compounds.
69	17	We chose an English Wikipedia dump as our corpus.
72	16	We also learn the embeddings of the compounds of the evaluation set, plus the embeddings of all the compounds’ component words.
75	57	After the unsupervised learning of word embeddings and candidate compound embeddings (see section 3), we use these embeddings as supervised signals in order to train our composition functions.
82	18	In other words, we learn a composition function (with several models) and identify non-compositional expressions as those for which the error of this composition function is high.
84	36	We want a composition function that is powerful enough to learn composition for compositional compounds, but simple enough that it fails to learn composition for non-compositional compounds.
89	17	min θ ‖[φ(wi), φ(wj)]θ2d×d − φ(wi, wj)‖ As mentioned earlier, a composition function that doesn’t overfit the training data and induces a more meaningful error is more suitable for our purpose.
96	33	The two diagonals of the matrices correspond to the sum of the two embeddings, which we can see are the main component of the sparse function, and play an important role in the non-sparse one.
99	15	If ψ shows the polynomial transformation then we have: f(φ(wi), φ(wj)) = ψ([φ(wi), φ(wj)])θ We couldn’t successfully apply any polynomial beyond quadratic transformation without overfitting.
100	27	The case of a quadratic ψ transformation is: ψ(x) = x21, · · ·x2n︸ ︷︷ ︸ Pure quadratic , x1x2, · · ·xn−1xn︸ ︷︷ ︸ interaction terms , x1, · · ·xn︸ ︷︷ ︸ linear terms Similar to the linear case we can have sparse version of the polynomial regression in which we allow the presence of only a few non-zero elements in the θ matrix.
118	28	For these three scores we consider the problem of predicting noncompositional NCs a problem with a binary solution where we assume compounds (of the evaluation set) with at least two non-compositionality votes are non-compositional.
125	28	In general, we can see that more complex functions tend to learn compositionality in a more effective way.
127	24	Sparsity seems to address this issue by reducing the number of non-zero parameters while the function can still keep the complex terms if needed.
128	22	In general sparse models show improvement over their non-sparse counterparts, specifically for more powerful models.
135	18	Given the semantics of this compound it is much easier to predict each of its component words independently.
136	53	In the previous section, the training signals came from the embeddings of the candidate compounds and their component words.
139	15	We thus add this hypothesis to the learning process: a good composition function not only builds the semantics of the compound from the semantics of its component words, but it also allows the independent prediction of the semantics of its component words from the compound semantics.
140	45	In the following sections we add this assumption to both linear projection (which encompasses polynomial) and Neural Network models.
165	34	There is almost no improvement in the case of linear model because this model does not have enough learning capacity to benefit from a higher number of training signals.
169	163	The best (optimum) composition function is the one that fits well all the compositional compounds and does not fit the non-compositional ones.
181	227	In other words we assume the compounds with big error are presumably non-compositional according to what we know until that iteration.
182	35	We continue alternating between training the composition function and annotating high error points until the algorithm reaches convergence.
185	28	This is expected since at training time we consider a model that returns B noncompositional compounds and therefore precision at 100 is optimized.
186	27	The latent annotations do not improve the linear model since the model is simple and there is not much room to improve its learning.
191	63	We further showed that enforcing sparsity is an effective way of learning a complex composition function while avoiding overfitting and producing meaningful learning errors.
