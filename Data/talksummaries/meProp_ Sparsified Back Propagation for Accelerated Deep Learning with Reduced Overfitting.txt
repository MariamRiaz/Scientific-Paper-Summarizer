0	18	Neural network learning is typically slow, where back propagation usually dominates the computational cost during the learning process.
1	14	Back propagation entails a high computational cost because it needs to compute full gradients and update all model parameters in each learning step.
2	62	It is not uncommon for a neural network to have a massive number of model parameters.
5	35	This leads to sparsified gradients, such that only highly relevant parameters are updated and other parameters stay untouched.
9	26	We propose a top-k search method to find the most important parameters.
13	17	The second question is whether or not this minimal effort back propagation strategy will hurt the accuracy of the trained models.
18	12	Experimental results demonstrate that we can update only 1–4% of the weights at each back propagation pass.
30	72	We can see that the computational cost of back propagation is directly proportional to the dimension of output vector n. The proposed meProp uses approximate gradients by keeping only top-k elements based on the magnitude values.
32	62	For example, suppose a vector v = 〈1, 2, 3,−4〉, then top2(v) = 〈0, 0, 3,−4〉.
33	47	We denote the indices of vector σ ′ (y)’s top-k values as {t1, t2, ..., tk}(1 ≤ k ≤ n), and the approximate gradient of the parameter matrix W and input vector x is: ∂z ∂Wij ← σ ′ ix T j if i ∈ {t1, t2, ..., tk} else 0 (5) ∂z ∂xi ← ∑ j WTijσ ′ j if j ∈ {t1, t2, ..., tk} else 0 (6) As a result, only k rows or columns (depending on the layout) of the weight matrix are modified, leading to a linear reduction (k divided by the vector dimension) in the computational cost.
35	38	The original back propagation uses the full gradient of the output vectors to compute the gradient of the parameters.
37	38	As for a complete neural network framework with a loss L, the original back propagation computes the gradient of the parameter matrix W as: ∂L ∂W = ∂L ∂y · ∂y ∂W (7) while the gradient of the input vector x is: ∂L ∂x = ∂y ∂x · ∂L ∂y (8) The proposed meProp selects top-k elements of the gradient ∂L∂y to approximate the original gradient, and passes them through the gradient computation graph according to the chain rule.
49	16	We also have an implementation based on the PyTorch framework for GPU based experiments.
54	22	If there are multiple hidden layers, the top-k sparsification needs to be applied to every hidden layer, because the sparsified gradient will again be dense from one layer to another.
57	11	For example, there are 10 tags in the MNIST task, so the dimension of the output layer is 10, and we use an MLP with the hidden dimension of 500.
60	21	The algorithm has a time complexity of O(n log k) and a space complexity of O(k).
61	13	Riedmiller and Braun (1993) proposed a direct adaptive method for fast learning, which performs a local adaptation of the weight update according to the behavior of the error function.
62	29	Tollenaere (1990) also proposed an adaptive acceleration strategy for back propagation.
71	15	To demonstrate that the proposed method is generalpurpose, we perform experiments on different models (LSTM/MLP), various training methods (Adam/AdaGrad), and diverse tasks.
83	50	The evaluation metric is per-image accuracy.
84	43	We use the MLP model as the baseline.
85	11	We set the dimension of the hidden layers to 500 for all the tasks.
87	28	For Parsing, the input dimension is 48 (features) × 50 (dim per feature) = 2400, and the output dimension is 25.
108	19	Then, the optimal number of iterations is decided based on the optimal score on development data, and the model of this iteration is used upon the test data to obtain the test scores.
109	36	As we can see, applying meProp can substantially speed up the back propagation.
111	14	Surprisingly, results demonstrate that we can update only 1–4% of the weights at each back propagation pass.
112	53	This does not result in a larger number of training iterations.
117	33	The results are consistent among AdaGrad and Adam.
119	49	For simplicity, in the following experiments the optimizer is based on Adam.
122	103	As we can see, meProp achieves consistently better accuracy than the baseline.
