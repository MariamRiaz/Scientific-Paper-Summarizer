0	34	An engineering discipline should be able to predict the cost of a project before the project is started.
1	45	Because training data is often the most expensive part of an NLP or ML project, it is important to estimate how much training data required for a system to achieve a target accuracy.
2	61	Unfortunately our field only offers fairly impractical advice, e.g., that more data increases accuracy (Banko and Brill, 2001); we currently have no practical methods for estimating how much data or what quality of data is required to achieve a target accuracy goal.
3	80	Imagine if bridge construction was planned the way we build our systems!
6	121	These extrapolations allow us to estimate how much training data a system will require to achieve a target accuracy.
7	12	We focus on a specific task (document classification) using a specific system (the fastText classifier of Joulin et al. (2016)), and leave to future work to determine if our approach and results generalise to other tasks and systems.
8	50	We introduce an accuracy extrapolation task that can be used to evaluate different extrapolation models.
9	31	We describe three well-known extrapolation models and evaluate them on a document classification dataset.
10	29	On our development data the biased power-law method with binomial item weighting performs best, so we propose it should be a baseline for future research.
11	33	We demonstrate the importance of hyperparameter optimisation on each different-sized data subset (rather than just optimising on the largest data subset) and item weighting, and show that these can have a dramatic impact on extrapolation, especially from small pilot data sets.
22	21	We train the system on different-sized subsets of the pilot dataset, and use the results of those training runs to estimate how the system’s accuracy varies as a function of training data size.
24	16	We investigate three different extrapolation models of e(n) in this paper: • Power law: ê(n) = bnc • Inverse square-root: ê(n) = a+ bn−1/2 • Biased power law: ê(n) = a+ bnc Here ê(n) is the estimate of e(n), and a, b and c are adjustable parameters that are estimated based on the system’s performance on the pilot dataset.
26	18	We fit these models using weighted least squares regression.
36	12	We investigated three item weighting functions in regression: • constant weights (1), • linear weights (n), and • binomial weights (n/e(1− e)) Linear weights are motivated by the assumption that the item variance follows the Central Limit Theorem, while the binomial weights are motivated by the assumption that item variance follows a binomial distribution (see the Supplemental Materials for further discussion).
41	13	These corpora contain labelled documents for a document classification task, and come randomised and divided into training and test sections.
50	27	When extrapolating from subsets of a smaller pilot set (we explored pilot sets consisting of 0.1 and 0.5 of the full training data) there are two plausible ways of performing hyperparameter optimisation.
51	53	Ideally, one would optimise the hyperparameters for each subset of the pilot data considered (we selected the best-performing hyperparameters using grid search).
52	19	However, if one is not working with computationally efficient algorithms like fastText, one might be tempted to only optimise the hyperparameters once on all the pilot data, and use the hyperparameters optimised on all the pilot data when calculating the error rate on subsets of that pilot data.
53	27	As figure 2 and table 2 make clear, selecting the optimal hyperparameters for each subset of the pilot data generally produces better extrapolation results.
54	43	Figure 1 shows how different ways of choosing hyperparameters can affect extrapolation.
55	56	As that figure shows, hyperparameters optimised on 50% of the training data perform very badly on 1% of the training data.
57	22	Interestingly, more complex extrapolation models, such as the extended power-law model, often do much better.
58	26	Based on the development corpora results presented in Figures 1 and 2, we choose the biased power law model (ê(n) = a+ bnc) with binomial item weights (n/e(1− e)) as the model to evaluate on the evaluation corpora.
59	41	We evaluate an extrapolation by calculating the root-mean-square (RMS) of the relative residuals ê/e − 1, where e is the minimum error achieved by the classifier with any hyperparameter setting when trained on the full training set, and ê is the predicted error made by the extrapolation model from the pilot dataset.1 Unsurprisingly, Table 2 shows that extrapolation is more accurate from larger pilot datasets; increasing the size of the pilot dataset 5 times reduces the RMS relative residuals by a factor of 10.
60	25	It also clearly shows that it valuable to perform hyperparameter optimisation on all subsets of the pilot dataset, not just on the whole pilot data.
62	14	This makes sense; the hyperparameters that are optimal on a large pilot dataset may be far from optimal on a very small subset (this is clearly visible in Figure 1, where the items deviating most are those for the = 0.5 pilot data and hyperparameter choice).
64	20	This only scratches the surface of performance extrapolation tasks.
65	26	We hope that teams with greater computational resources will study the extrapolation task for computationally more-demanding systems, including popular deep learning models.
66	127	The power-law models should be considered baselines for more sophisticated extrapolation models, which might exploit more information than just accuracy on subsets of the pilot data.
67	43	We hope this work will spur the development of better methods for estimating the resources needed to build an NLP or ML system to meet a specification, as we believe this is essential for any mature engineering field.
