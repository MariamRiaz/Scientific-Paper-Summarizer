42	6	We are interested in the symmetries of this structure.
43	68	The set of permutations (πN, πM) ∈ SN × SM of nodes (within each part of the bipartite graph) that preserve all edgecolors define the Automorphism Group Aut(Ω) ≤ SN × SM – that is ∀(n,m) ∈ N ×M (πN, πM) ∈Aut(Ω) ⇔ α(n,m) = α((πNn,πMm)) (3) Alternatively, to facilitate the notation, we define the same structure (colored multi-edged bipartite graph) as a set of binary relations between N and M – that is Ω = (N,M,{∆c}1≤c≤C) where each relation is associated with one color ∆c = {(n,m) ∣ c ∈ α(n,m)∀(n,m) ∈ N ×M}.
44	14	This definition of structure, gives an alternative expression for Aut(Ω) (πN, πM) ∈Aut(Ω) ⇔ (4) ((n,m) ∈ ∆c ⇔ (πNn,πMm) ∈ ∆c) ∀c, n,m The significance of this structure is in that, it defines a parameter-sharing scheme in a neural layer, where the same edge-colors correspond to the same parameters.
45	17	, φM ] ∶ RN → RM φm(x;w,Ω) ≐ σ(∑ n ∑ c∈α(n,m) wcxn) ∀m (5) where σ ∶ R → R is a strictly monotonic nonlinearity and w = [w1, .
46	11	The following key theorem relates the equivariances of φ(⋅;w,Ω) to the symmetries of Ω. Theorem 2.1 For any w ∈ RC s.t., wc ≠ wc′∀c, c′, the function φ(⋅;w,Ω) is uniquely Aut(Ω)-equivariant.
47	31	Corollary 2.2 For any HN,M ≤ Aut(Ω), the function φ(⋅;w,Ω) is HN,M-equivariant.
48	29	The implication is that to achieve unique equivariance for a given group-action, we need to define the parametersharing using the structure Ω with symmetry group GN,M.
49	18	Example 2.1 (Reverse Convolution) Revisiting Example 1.2 we can show that the condition of Theorem 2.1 holds.
50	97	In this case σ(x) = x and the parameter-sharing of the matrix W is visualized below, where we used two different line styles for a, b ∈ R. In this figure, the circular shift of variables at the output and input level to the left and right respectively, does not change the edge-colors.
52	26	Six repetitions of this action produces different permutations corresponding to six members of GN,M.
58	14	In other words we can rewrite φ using W ∈ RM×N φ(x;w; Ω) = σ(Wx) Wm,n = ∑ c∈α(n,m) wc (6) Using this notation, and due to strict monotonicity of the nonlinearity σ(⋅), Theorem 2.1 simply states that for all (gN,gM) ∈Aut(Ω), x ∈ RN and W given by (6) GMWx =WGNx.
59	16	(7) Example 2.2 (Permutation-Equivariant Layer) Consider all permutations of indices N and M = N. We want to define a neural layer such that all permutations of the input gN ∈ GN = SN result in the same permutation of the output gM = gN.
64	17	The function (5) for this Ω is φ(x;w = [w1,w2],Ω) = σ(w1Ix +w211Tx).
74	23	regularity The group action is free or semi-regular iff ∀n1, n2 ∈ N, there is at most one g ∈ G such at gn1 = n2, and the action is regular iff it is both transitive and free – i.e., for any pair n1, n2 ∈ N, there is uniquely one g ∈ G such that gn1 = n2.
81	64	Example 3.1 (Mirror Symmetry) Consider G = Z2 = {e = 0,1} (1 + 1 = 0) acting on N, where the only non-trivial action is defined as flipping the input: 1N[1, .
83	17	If N is even, then G-action is semi-regular.
87	9	In the following, Section 3.1 proposes a procedure for parameter-sharing in a fully connected layer.
92	21	The action of GN,M partitions the edges into orbits {GN,M(np,mq)}np,mq , where (np,mq) is a representative edge from an orbit.
94	15	(9) Therefore two edges (n,m) and (n′,m′) have the same color iff an action in GN,M moves one edge to the other.
102	8	The group of our interest is the wreath product Sd ≀ SD.
108	6	Recall Our objective is to define parameter-sharing so that φW ∶ RdD → RdD is equivariant to the action of G = Sd ≀ SD – i.e., permutations within sets at two levels.
115	21	While this achieves the desirable equivariance, it is inefficient and does not generalize as well as a convolution layer with small filters.
116	7	Moreover, the dense design does not guarantee “unique” equivariance.
120	9	• The set A ⊆ G is called the generating set of G (< A >= G), iff every member of G can be expressed as a combination of members of A.
132	6	The symmetric generating set is the generating set that is used in the Cayley diagram, with the addition of inverse shift (inverse of the blue arrow).
139	22	Here, we want to see how to achieve GN,M-equivariance for φ ∶ RN×K → RM×K ′ , where K and K ′ are the number of input and output channels.
140	7	First, we extend the action of G on N and M to NK = [N, .
148	14	Example 3.5 (Group Convolution) The idea of groupconvolution is studied by Cohen & Welling (2016a); see also (Olah, 2014).
161	18	In this case, the resulting neural layer has the desired equivariance (right).
162	22	However, it is equivariant to the action of a larger group GN,M ≅ Z2 ×Z2 > Z2, in which 1 in the second Z2 group exchanges variables across the orbits on N (left in figure above).
