0	86	Given the numerous applications of distributed optimization in machine learning, many algorithms have recently emerged, that allow the minimization of objective functions f defined as the average 1n ∑n i=1 fi of functions fi which are respectively accessible by separate nodes in a network (Nedic & Ozdaglar, 2009; Boyd et al., 2011; Duchi et al., 2012; Shi et al., 2015).
1	109	These algorithms typically alternate local incremental improvement steps (such as gradient steps) with communication steps between nodes in the network, and come with a variety of convergence rates (see for example Shi et al. (2014; 2015); Jakovetić et al. (2015); Nedich et al. (2016)).
2	51	Two main regimes have been looked at: (a) centralized where communications are precisely scheduled and (b) decentralized where communications may not exhibit a precise schedule.
6	42	Indeed, (a) for a single machine the optimal number of gradient steps to optimize a function is proportional to the square root of the condition number (Nesterov, 2004), and (b) for mean estimation, the optimal number of communication steps is proportional to the diameter of the network in centralized problems or to the square root of the eigengap of the Laplacian matrix in decentralized problems (Boyd et al., 2006).
26	40	We also denote by αg , βg and κg , respectively, the strong convexity, smoothness and condition number of the average (global) function f̄ .
30	22	A large body of literature considers a decentralized approach to distributed optimization based on the gossip algorithm (Boyd et al., 2006; Nedic & Ozdaglar, 2009; Duchi et al., 2012; Wei & Ozdaglar, 2012).
42	19	In this section, we prove oracle complexity lower bounds for distributed optimization in two settings: strongly convex and smooth functions for centralized (i.e. master/slave) and decentralized algorithms based on a gossip matrix W .
57	13	In this section, we show that, for any black-box optimization procedure, at least Ω(√κg ln(1/ε)) gradient steps and Ω(∆ √ κg ln(1/ε)) communication steps are necessary to achieve a precision ε > 0, where κg is the global condition number and ∆ is the diameter of the network.
67	58	For any graph of diameter ∆ and any blackbox procedure, there exists functions fi such that the time to reach a precision ε > 0 is lower bounded by Ω ( √ κg ( 1 + ∆τ ) ln ( 1 ε )) , (7) This optimal convergence rate is achieved by distributing Nesterov’s accelerated gradient descent on the global function.
68	109	Computing the gradient of f̄ is performed by sending all the local gradients ∇fi to a single node (denoted as master node) in ∆ communication steps (which may involve several simultaneous messages), and then returning the new parameter θt+1 to every node in the network (which requires another ∆ communication steps).
69	19	In practice, summing the gradients can be distributed by computing a spanning tree (with the root as master node), and asking for each node to perform the sum of its children’s gradients before sending it to its parent.
71	20	This algorithm has three limitations: first, the algorithm is not robust to machine failures, and the central role played by the master node also means that a failure of this particular machine may completely freeze the procedure.
73	20	Finally, the algorithm requires every node to complete its gradient computation before aggregating them on the master node, and the efficiency of the algorithm thus depends on the slowest of all machines.
79	31	There exists a gossip matrix W of eigengap γ(W ) = γ, and α-strongly convex and β-smooth functions fi : `2 → R such that, for any t ≥ 0 and any black-box procedure using W one has, for all i ∈ {1, ..., n}, f̄(θi,t)− f̄(θ∗) ≥ 3α 2 ( 1− 16√ κl )1+ t 1+ τ 5 √ γ ‖θi,0 − θ∗‖2, (8) where κl = β/α is the local condition number.
83	27	For any γ > 0, there exists a gossip matrix W of eigengap γ and α-strongly convex, β-smooth functions such that, with κl = β/α, for any black-box procedure using W the time to reach a precision ε > 0 is lower bounded by Ω ( √ κl ( 1 + τ √ γ ) ln ( 1 ε )) .
84	26	(9) We will see in the next section that this lower bound is met for a novel decentralized algorithm called multi-step dual accelerated (MSDA) and based on the dual formulation of the optimization problem.
85	27	Note that these results provide optimal convergence rates with respect to κl and γ, but do not imply that γ is the right quantity to consider on general graphs.
87	26	For example, for linear graphs, ∆ = n − 1 and 1/√γ ≈ 2n/π, for totally connected networks, ∆ = 1 and 1/ √ γ = 1, and for regular networks, 1/ √ γ ≥ ∆ 2 √ 2 ln2 n (Alon & Milman, 1985).
90	12	(1) in a decentralized setting, from which we will derive several variants, including a synchronized algorithm whose convergence rate matches the lower bound in Corollary 2.
96	23	, θn) and W is a gossip matrix verifying the assumptions described Algorithm 1 Single-Step Dual Accelerated method Input: number of iterations T > 0, gossip matrix W ∈ Rn×n, η = αλ1(W ) , µ = √ κl− √ γ√ κl+ √ γ Output: θi,T , for i = 1, ..., n 1: x0 = 0, y0 = 0 2: for t = 0 to T − 1 do 3: θi,t = ∇f∗i (xi,t), for all i = 1, ..., n 4: yt+1 = xt − ηΘtW 5: xt+1 = (1 + µ)yt+1 − µyt 6: end for in Section 2.
100	50	(11) is a convex problem, it is equivalent to its dual optimization problem: max λ∈Rd×n −F ∗(λ √ W ), (12) where F ∗(y) = supx∈Rd×n〈y, x〉 − F (x) is the Fenchel conjugate of F , and 〈y, x〉 = tr(y>x) is the standard scalar product between matrices.
105	15	The algorithm is derived by noting that a gradient step of size η > 0 for Eq.
106	32	(12) is λt+1 = λt − η∇F ∗(λt √ W ) √ W, (13) and the change of variable yt = λt √ W leads to yt+1 = yt − η∇F ∗(yt)W. (14) This equation can be interpreted as gossiping the gradients of the local conjugate functions ∇f∗i (yi,t), since ∇F ∗(yt)ij = ∇f∗j (yj,t)i. Theorem 3.
109	52	(15) This theorem relies on proving that the condition number of the dual objective function is upper bounded by κlγ , and noting that the convergence rate for accelerated gradient descent depends on the square root of the condition number (see, e.g., Bubeck (2015)).
124	24	Algorithm 2 Multi-Step Dual Accelerated method Input: number of iterations T > 0, gossip matrix W ∈ Rn×n, c1 = 1−√γ 1+ √ γ , c2 = 1+γ 1−γ , c3 = 2 (1+γ)λ1(W ) , K = ⌊ 1√ γ ⌋ , η = α(1+c 2K 1 ) (1+cK1 ) 2 , µ = (1+cK1 ) √ κl−1+cK1 (1+cK1 ) √ κl+1−cK1 Output: θi,T , for i = 1, ..., n 1: x0 = 0, y0 = 0 2: for t = 0 to T − 1 do 3: θi,t = ∇f∗i (xi,t), for all i = 1, ..., n 4: yt+1 = xt − η ACCELERATEDGOSSIP(Θt,W ,K) 5: xt+1 = (1 + µ)yt+1 − µyt 6: end for 7: procedure ACCELERATEDGOSSIP(x,W ,K) 8: a0 = 1, a1 = c2 9: x0 = x, x1 = c2x(I − c3W ) 10: for k = 1 to K − 1 do 11: ak+1 = 2c2ak − ak−1 12: xk+1 = 2c2xk(I − c3W )− xk−1 13: end for 14: return x0 − xKaK 15: end procedure Computation of ∇f∗i (xi,t).
161	19	In all our experiments, MSDA outperforms SSDA, indicating that performing several communication rounds per gradient iteration never degrades the efficiency of the algorithm, while significantly improving it when τ 1.
172	28	In this paper, we derived optimal convergence rates for strongly convex and smooth distributed optimization in two settings: centralized and decentralized communications in a network.
173	21	For the decentralized setting, we introduced the multi-step dual accelerated (MSDA) algorithm with a provable optimal linear convergence rate, and showed its high efficiency compared to other state-of-the-art methods, including distributed ADMM and EXTRA.
174	36	The simplicity of the approach makes the algorithm extremely flexible, and allows for future extensions, including time-varying networks and an analysis for non-strongly-convex functions.
175	61	Finally, extending our complexity lower bounds to time delays, variable computational speeds of local systems, or machine failures would be a notable addition to this work.
