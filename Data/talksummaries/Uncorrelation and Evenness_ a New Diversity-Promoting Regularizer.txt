0	18	A fundamental task in machine learning (ML) is to discover latent patterns underlying data, for instance, extracting topics from documents and communities from social networks.
1	30	Latent space models (Bishop, 1998; Knott & Bartholomew, 1999; Blei, 2014) are effective tools to accomplish this task.
2	29	An LSM contains a collection of learnable components such as hidden units in neural networks and factors in factor analysis (Harman, 1960).
6	34	First, under many circumstances, the frequency of patterns is highly imbalanced.
8	12	As a typical example, in a news corpus, politics and economics are frequent topics (patterns) while furniture and gardening are infrequent.
9	20	Classic LSMs are sensitive to the skewness of pattern frequency and less capable of capturing the infrequent patterns (Wang et al., 2014).
10	29	Second, when using LSMs, one needs to carefully balance the tradeoff between model size (precisely, the number of components) and modeling power (Xie, 2015).
11	17	Larger-sized LSMs are more expressive, but incur higher computational complexity.
14	62	They conjecture that: (1) through “diversification”, some components that are originally aggregated around frequent patterns can be pushed apart to cover infrequent patterns; (2) “diversified” components bear less redundancy and are mutually complementary; a small number of such components are sufficient to model data well.
15	32	Along this line of research, several diversity-promoting regularizers have been proposed, based upon determinantal point process (Kulesza & Taskar, 2012; Zou & Adams, 2012), cosine similarity (Yu et al., 2011; Bao et al., 2013; Xie et al., 2015) and covariance (Malkin & Bilmes, 2008; Cogswell et al., 2015).
16	16	While these regularizers demonstrate notable efficacy, they have certain limitations, such as sensitivity to vector scaling (Zou & Adams, 2012; Malkin & Bilmes, 2008), inability to measure diversity in a global manner (Yu et al., 2011; Bao et al., 2013; Xie et al., 2015) and computational inefficiency (Cogswell et al., 2015).
65	15	Evenness is borrowed from biological diversity (Magurran, 2013), which measures how equally important different species are in maintaining the ecological balance within an ecosystem.
67	12	Likewise, in latent space modeling, we desire the components to play equally important roles and no one dominates another, such that each component contributes significantly to the modeling of data.
68	15	We characterize the uncorrelation among components from a statistical perspective: treating the components as random variables and measuring their covariance which is proportional to their correlation.
69	21	Let A ∈ Rd×m denote the component matrix where in the k-th column is the parameter vector ak of component k. Alternatively, we can take a row view (Figure 1(b)) of A: each component is treated as a random variable and each row vector ã>i can be seen as a sample drawn from the random vector formed by the m components.
87	12	As shown in Figure 3(a), component 1 has a larger eigenvalue λ1 and accordingly larger variability, hence is more important than component 2.
90	24	To sum up, we desire to encourage the eigenvalues to be even in both cases: (1) when the eigenvectors are not aligned with the coordinate axis, they are preferred to be even to reduce the correlation of components; (2) when the eigenvectors are aligned with the coordinate axis, they are encouraged to be even such that different components contribute equally in modeling data.
93	20	The basic idea is: we normalize the eigenvalues into a probability simplex and encourage the discrete distribution parameterized by the normalized eigenvalues to have small Kullback-Leibler (KL) divergence with the uniform distribution.
107	12	Second, unlike the regularizers proposed in (Bao et al., 2013; Xie et al., 2015) that are non-smooth, UER is a smooth function.
115	20	A matrix M is referred to as a density matrix (Bengtsson & Zyczkowski, 2007) if its eigenvalues are strictly positive and sum to one, equivalently, M 0 and tr(M) = 1.
120	55	This new UER is a special case of the previous one (Eq.(2)).
128	19	Due to space limit, the results of the latter two are deferred to the supplements.
129	20	Distance Metric Learning (DML) Given data pairs either labeled as “similar” or “dissimilar”, DML (Xing et al., 2002; Davis et al., 2007; Guillaumin et al., 2009) aims to learn a distance metric under which similar pairs would be placed close to each other and dissimilar pairs are separated apart.
170	12	Experimental Setup In DML experiments, two samples are labeled as similar if belonging to the same class and dissimilar otherwise.
186	57	Results Table 2 shows the retrieval precision (K = 10) on three datasets, where we observe: (1) DML-UE achieves much better precision than DML, proving that UER is an effective regularizer in improving generalization performance; (2) UER outperforms other diversity-promoting regularizers possibly due to its capability to capture global relations among all components and insensitivity to vector scaling; (3) diversity-promoting regularizers perform better than other types of regularizers such as L2, L1, low rank and Dropout, corroborating the efficacy of inducing diversity; (4) DML-UE outperforms other popular distance learning methods such as ITML, LDML and GMML.
190	18	In contrast, with more components (300), DML achieves a much lower precision (53.1%).
191	17	This demonstrates that by encouraging the components to be diverse, UER is able to reduce model size without sacrificing modeling power.
195	15	Next, we verify whether “diversifying” the components in DML can better capture infrequent patterns.
222	14	We propose a new diversity-promoting regularizer from the perspectives of uncorrelation which prefers the components in LSMs to be uncorrelated and evenness which encourages the components to contribute equally to the modeling of data.
225	20	Experimental studies reveal that UER greatly boosts the performance of LSMs, better captures infrequent patterns, reduces model size without compromising modeling power and outperforms other diversity-promoting regularizers.
