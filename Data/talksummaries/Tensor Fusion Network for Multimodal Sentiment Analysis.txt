4	69	The utterance “This movie is sick” can be ambiguous (either positive or negative) by itself, but if the speaker is also smiling at the same time, then it will be perceived as positive.
6	28	A person speaking loudly “This movie is sick” would still be ambiguous.
9	28	The complexity of inter-modality dynamics is shown in the second trimodal example where the utterance “This movie is fair” is still weakly positive, given the strong influence of the word “fair”.
12	119	A spoken opinion such as “I think it was alright .
14	45	This volatile nature of spoken opinions, where proper language structure is often ignored, complicates sentiment analysis.
15	21	Visual and acoustic modalities also contain their own intra-modality dynamics which are expressed through both space and time.
21	81	This prevents the model from learning inter-modality dynamics in an efficient way by assuming that simple weighted averaging is a proper fusion approach.
23	37	Inter-modality dynamics are modeled with a new multimodal fusion approach, named Tensor Fusion, which explicitly aggregates unimodal, bimodal and trimodal interactions.
36	56	Multimodal Opinion Sentiment Intensity (CMUMOSI) dataset is an annotated dataset of video 0 100 200 300 400 500 600 Highly Negative Negative Weakly Negative Neutral Weakly Positive Positive Highly Positive 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% 5 10 15 20 25 30 35 Pe rc en ta ge o f S en tim en t D eg re es Utterance Size Highly Positive Positive Weakly Positive Neutral Weakly Negative Negative Highly Negative N um be r o f O pi ni on S eg m en ts Figure 2: Distribution of sentiment across different opinions (left) and opinion sizes (right) in CMU-MOSI.
37	21	opinions from YouTube movie reviews (Zadeh et al., 2016a).
44	36	These utterance are annotated by five Mechanical Turk annotators for sentiment.
50	66	2) Tensor Fusion Layer explicitly models the unimodal, bimodal and trimodal interactions using a 3-fold Cartesian product from modality embeddings.
55	23	Spoken Language Embedding Subnetwork: Spoken text is different than written text (reviews, tweets) in compositionality and grammar.
74	65	A set of 20 Facial Action Units (Ekman et al., 1980), indicating detailed muscle movements on the face, are also extracted using FACET.
112	27	In this paper, we devise three sets of experiments each addressing a different research question: Experiment 1: We compare our TFN with previous state-of-the-art approaches in multimodal sentiment analysis.
118	28	In this section, we compare the performance of TFN model with previously proposed multimodal sentiment analysis models.
119	43	We compare to the following baselines: C-MKL (Poria et al., 2015) Convolutional MKL-based model is a multimodal sentiment classification model which uses a CNN to extract textual features and uses multiple kernel learning for sentiment analysis.
120	120	It is current SOTA (state of the art) on CMU-MOSI.
121	80	SAL-CNN (Wang et al., 2016) Select-Additive Learning is a multimodal sentiment analysis model that attempts to prevent identity-dependent information from being learned in a deep neural network.
123	49	SVM-MD (Zadeh et al., 2016b) is a SVM model trained on multimodal features using early fusion.
125	29	We also present the results of Random Forest RF-MD to compare to another non-neural approach.
136	23	We also perform a comparison with the early fusion approach (TFNearly) by simply concatenating all three modality embeddings < zl, za, zv > and passing it directly as input to Us.
138	91	When looking at Table 4 results, we see that our TFN approach outperforms the early fusion approach2.
140	20	We selected the following state-of-the-art approaches to include variety in their techniques, based on dependency parsing (RNTN), distributional representation of text (DAN), and convolutional approaches (DynamicCNN).
148	116	We suspect that this underperformance is due to: RNTN and similar approaches rely heavily on dependency structure, which may not be present in spoken language; DAN and similar sentence embeddings approaches can easily be diluted by words that may not relate directly to sentiment or meaning; D-CNN and similar convolutional approaches rely on spatial proximity of related words, which may not always be present in spoken language.
155	41	We use the same features extracted for Ua averaged over time slices of every 200 intervals.
165	48	We analyze the impact of our proposed TFN multimodal fusion approach by comparing it with the early fusion approach TFNearly and the three unimodal models.
170	55	We can see this trend by comparing it with the language unimodal model TFNlanguage.
172	24	Specifically, in the first example, the utterance is weakly negative where the speaker is referring to lack of funny jokes in the movie.
173	23	This example contains a bimodal interaction where the visual modality shows a negative expression (frowning) which is correctly captured by our TFN approach.
