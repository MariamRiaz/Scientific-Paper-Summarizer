0,1,label2,summary_sentences
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1948–1958 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1948",text,[0],[0]
"With the recent surge of interest in deep learning, one question that is being asked across a number of fronts is: can deep learning techniques be harnessed for creative purposes?",1 Introduction,[0],[0]
"Creative applications where such research exists include the composition of music (Humphrey et al., 2013; Sturm et al., 2016; Choi et al., 2016), the design of sculptures (Lehman et al., 2016), and automatic choreography (Crnkovic-Friis and Crnkovic-Friis, 2016).",1 Introduction,[0],[0]
"In this paper, we focus on a creative textual task: automatic poetry composition.
",1 Introduction,[0],[0]
"A distinguishing feature of poetry is its aesthetic forms, e.g. rhyme and rhythm/",1 Introduction,[0],[0]
"meter.1 In this work, we treat the task of poem generation as a constrained language modelling task, such that lines of a given poem rhyme, and each line follows a canonical meter and has a fixed number
1Noting that there are many notable divergences from this in the work of particular poets (e.g. Walt Whitman) and poetry types (such as free verse or haiku).
",1 Introduction,[0],[0]
Shall I compare thee to a summer’s day?,1 Introduction,[0],[0]
"Thou art more lovely and more temperate: Rough winds do shake the darling buds of May, And summer’s lease hath all too short a date:
",1 Introduction,[0],[0]
"Figure 1: 1st quatrain of Shakespeare’s Sonnet 18.
of stresses.",1 Introduction,[0],[0]
"Specifically, we focus on sonnets and generate quatrains in iambic pentameter (e.g. see Figure 1), based on an unsupervised model of language, rhyme and meter trained on a novel corpus of sonnets.
",1 Introduction,[0],[0]
"Our findings are as follows:
• our proposed stress and rhyme models work very well, generating sonnet quatrains with stress and rhyme patterns that are indistinguishable from human-written poems and rated highly by an expert; • a vanilla language model trained over our son-
net corpus, surprisingly, captures meter implicitly at human-level performance; • while crowd workers rate the poems generated
by our best model as nearly indistinguishable from published poems by humans, an expert annotator found the machine-generated poems to lack readability and emotion, and our best model to be only comparable to a vanilla language model on these dimensions; • most work on poetry generation focuses on me-
ter (Greene et al., 2010; Ghazvininejad et al., 2016; Hopkins and Kiela, 2017); our results suggest that future research should look beyond meter and focus on improving readability.
",1 Introduction,[0],[0]
"In this, we develop a new annotation framework for the evaluation of machine-generated poems, and release both a novel data of sonnets and the full source code associated with this research.2
2https://github.com/jhlau/deepspeare",1 Introduction,[0],[0]
"Early poetry generation systems were generally rule-based, and based on rhyming/TTS dictionaries and syllable counting (Gervás, 2000; Wu et al., 2009; Netzer et al., 2009; Colton et al., 2012; Toivanen et al., 2013).",2 Related Work,[0],[0]
"The earliest attempt at using statistical modelling for poetry generation was Greene et al. (2010), based on a language model paired with a stress model.
",2 Related Work,[0],[0]
Neural networks have dominated recent research.,2 Related Work,[0],[0]
"Zhang and Lapata (2014) use a combination of convolutional and recurrent networks for modelling Chinese poetry, which Wang et al. (2016) later simplified by incorporating an attention mechanism and training at the character level.",2 Related Work,[0],[0]
"For English poetry, Ghazvininejad et al. (2016) introduced a finite-state acceptor to explicitly model rhythm in conjunction with a recurrent neural language model for generation.",2 Related Work,[0],[0]
"Hopkins and Kiela (2017) improve rhythm modelling with a cascade of weighted state transducers, and demonstrate the use of character-level language model for English poetry.",2 Related Work,[0],[0]
"A critical difference over our work is that we jointly model both poetry content and forms, and unlike previous work which use dictionaries (Ghazvininejad et al., 2016) or heuristics (Greene et al., 2010) for rhyme, we learn it automatically.",2 Related Work,[0],[0]
"The sonnet is a poem type popularised by Shakespeare, made up of 14 lines structured as 3 quatrains (4 lines) and a couplet (2 lines);3 an example quatrain is presented in Figure 1.",3 Sonnet Structure and Dataset,[0],[0]
"It follows a number of aesthetic forms, of which two are particularly salient: stress and rhyme.
",3 Sonnet Structure and Dataset,[0],[0]
"A sonnet line obeys an alternating stress pattern, called the iambic pentameter, e.g.:
S− S+ S− S+ S− S+ S− S+ S− S+
Shall I compare thee to a summer’s day?",3 Sonnet Structure and Dataset,[0],[0]
"where S− and S+ denote unstressed and stressed syllables, respectively.
",3 Sonnet Structure and Dataset,[0],[0]
"A sonnet also rhymes, with a typical rhyming scheme being ABAB CDCD EFEF GG.",3 Sonnet Structure and Dataset,[0],[0]
"There are a number of variants, however, mostly seen in the quatrains; e.g. AABB or ABBA are also common.
",3 Sonnet Structure and Dataset,[0],[0]
"We build our sonnet dataset from the latest image of Project Gutenberg.4 We first create a
3There are other forms of sonnets, but the Shakespearean sonnet is the dominant one.",3 Sonnet Structure and Dataset,[0],[0]
"Hereinafter “sonnet” is used to specifically mean Shakespearean sonnets.
",3 Sonnet Structure and Dataset,[0],[0]
"4https://www.gutenberg.org/.
(generic) poetry document collection using the GutenTag tool (Brooke et al., 2015), based on its inbuilt poetry classifier and rule-based structural tagging of individual poems.
",3 Sonnet Structure and Dataset,[0],[0]
"Given the poems, we use word and character statistics derived from Shakespeare’s 154 sonnets to filter out all non-sonnet poems (to form the “BACKGROUND” dataset), leaving the sonnet corpus (“SONNET”).5 Based on a small-scale manual analysis of SONNET, we find that the approach is sufficient for extracting sonnets with high precision.",3 Sonnet Structure and Dataset,[0],[0]
"BACKGROUND serves as a large corpus (34M words) for pre-training word embeddings, and SONNET is further partitioned into training, development and testing sets.",3 Sonnet Structure and Dataset,[0],[0]
Statistics of SONNET are given in Table 1.6,3 Sonnet Structure and Dataset,[0],[0]
"We propose modelling both content and forms jointly with a neural architecture, composed of 3 components: (1) a language model; (2) a pentameter model for capturing iambic pentameter; and (3) a rhyme model for learning rhyming words.
",4 Architecture,[0],[0]
"Given a sonnet line, the language model uses standard categorical cross-entropy to predict the next word, and the pentameter model is similarly trained to learn the alternating iambic stress patterns.7 The rhyme model, on the other hand, uses a margin-based loss to separate rhyming word pairs from non-rhyming word pairs in a quatrain.",4 Architecture,[0],[0]
"For generation we use the language model to generate one word at a time, while applying the pentame-
5The following constraints were used to select sonnets: 8.0 6 mean words per line 6 11.5; 40 6 mean characters per line 6 51.0; min/max number of words per line of 6/15; min/max number of characters per line of 32/60; and min letter ratio per line > 0.59.
6The sonnets in our collection are largely in Modern English, with possibly a small number of poetry in Early Modern English.",4 Architecture,[0],[0]
"The potentially mixed-language dialect data might add noise to our system, and given more data it would be worthwhile to include time period as a factor in the model.
",4 Architecture,[0],[0]
"7There are a number of variations in addition to the standard pattern (Greene et al., 2010), but our model uses only the standard pattern as it is the dominant one.
",4 Architecture,[0],[0]
ter model to sample meter-conforming sentences and the rhyme model to enforce rhyme.,4 Architecture,[0],[0]
The architecture of the joint model is illustrated in Figure 2.,4 Architecture,[0],[0]
We train all the components together by treating each component as a sub-task in a multitask learning setting.8,4 Architecture,[0],[0]
"The language model is a variant of an LSTM encoder–decoder model with attention (Bahdanau et al., 2015), where the encoder encodes the preceding context (i.e. all sonnet lines before the current line) and the decoder decodes one word at a time for the current line, while attending to the preceding context.
",4.1 Language Model,[0],[0]
"In the encoder, we embed context words zi using embedding matrix Wwrd to yield wi, and feed them to a biLSTM9 to produce a sequence of encoder hidden states",4.1 Language Model,[0],[0]
hi =,4.1 Language Model,[0],[0]
[~hi; ~hi].,4.1 Language Model,[0],[0]
"Next we apply
8We stress that although the components appear to be disjointed, the shared parameters allow the components to mutually influence each other during joint training.",4.1 Language Model,[0],[0]
"To exemplify this, we found that the pentameter model performs very poorly when we train each component separately.
",4.1 Language Model,[0],[0]
"9We use a single layer for all LSTMs.
",4.1 Language Model,[0],[0]
"a selective mechanism (Zhou et al., 2017) to each hi.",4.1 Language Model,[0],[0]
"By defining the representation of the whole context h = [~hC ; ~h1] (where C is the number of words in the context), the selective mechanism filters the hidden states hi using h as follows:
h′i = hi σ(Wahi",4.1 Language Model,[0],[0]
"+Uah+ ba)
where denotes element-wise product.",4.1 Language Model,[0],[0]
"Hereinafter W, U and b are used to refer to model parameters.",4.1 Language Model,[0],[0]
"The intuition behind this procedure is to selectively filter less useful elements from the context words.
",4.1 Language Model,[0],[0]
"In the decoder, we embed words xt in the current line using the encoder-shared embedding matrix (Wwrd) to produce wt.",4.1 Language Model,[0],[0]
"In addition to the word embeddings, we also embed the characters of a word using embedding matrix Wchr to produce ct,i, and feed them to a bidirectional (character-level) LSTM:
~ut,i = LSTMf (ct,i, ~ut,i−1) ~ut,i = LSTMb(ct,i, ~ut,i+1)
(1)
We represent the character encoding of a word by concatenating the last forward and first back-
ward hidden states ut =",4.1 Language Model,[0],[0]
"[~ut,L; ~ut,1], where L is the length of the word.",4.1 Language Model,[0],[0]
"We incorporate character encodings because they provide orthographic information, improve representations of unknown words, and are shared with the pentameter model (Section 4.2).10 The rationale for sharing the parameters is that we see word stress and language model information as complementary.
",4.1 Language Model,[0],[0]
"Given the word embedding wt and character encoding ut, we concatenate them together and feed them to a unidirectional (word-level) LSTM to produce the decoding states:
st = LSTM([wt;ut], st−1) (2)
We attend st to encoder hidden states h′i and compute the weighted sum of h′i as follows:
eti = v ᵀ b tanh(Wbh ′",4.1 Language Model,[0],[0]
"i +Ubst + bb) at = softmax(et)
h∗t",4.1 Language Model,[0],[0]
= ∑ i atih ′,4.1 Language Model,[0],[0]
"i
To combine st and h∗t , we use a gating unit similar to a GRU (Cho et al., 2014; Chung et al., 2014): s′t = GRU(st,h ∗ t ).",4.1 Language Model,[0],[0]
"We then feed s ′ t to a linear layer with softmax activation to produce the vocabulary distribution (i.e. softmax(Wouts′t + bout), and optimise the model with standard categorical cross-entropy loss.",4.1 Language Model,[0],[0]
"We use dropout as regularisation (Srivastava et al., 2014), and apply it to the encoder/decoder LSTM outputs and word embedding lookup.",4.1 Language Model,[0],[0]
"The same regularisation method is used for the pentameter and rhyme models.
",4.1 Language Model,[0],[0]
"As our sonnet data is relatively small for training a neural language model (367K words; see Table 1), we pre-train word embeddings and reduce parameters further by introducing weight-sharing between output matrix Wout and embedding matrix Wwrd via a projection matrix",4.1 Language Model,[0],[0]
"Wprj (Inan et al., 2016; Paulus et al., 2017; Press and Wolf, 2017):
Wout = tanh(WwrdWprj)",4.1 Language Model,[0],[0]
This component is designed to capture the alternating iambic stress pattern.,4.2 Pentameter Model,[0],[0]
"Given a sonnet line,
10We initially shared the character encodings with the rhyme model as well, but found sub-par performance for the rhyme model.",4.2 Pentameter Model,[0],[0]
"This is perhaps unsurprising, as rhyme and stress are qualitatively very different aspects of forms.
",4.2 Pentameter Model,[0],[0]
"the pentameter model learns to attend to the appropriate characters to predict the 10 binary stress symbols sequentially.11 As punctuation is not pronounced, we preprocess each sonnet line to remove all punctuation, leaving only spaces and letters.",4.2 Pentameter Model,[0],[0]
"Like the language model, the pentameter model is fashioned as an encoder–decoder network.
",4.2 Pentameter Model,[0],[0]
"In the encoder, we embed the characters using the shared embedding matrix Wchr and feed them to the shared bidirectional character-level LSTM (Equation (1)) to produce the character encodings for the sentence: uj = [~uj ; ~uj ].
",4.2 Pentameter Model,[0],[0]
"In the decoder, it attends to the characters to predict the stresses sequentially with an LSTM:
gt = LSTM(u∗t−1,gt−1)
where u∗t−1 is the weighted sum of character encodings from the previous time step, produced by an attention network which we describe next,12 and gt is fed to a linear layer with softmax activation to compute the stress distribution.
",4.2 Pentameter Model,[0],[0]
"The attention network is designed to focus on stress-producing characters, whose positions are monotonically increasing (as stress is predicted sequentially).",4.2 Pentameter Model,[0],[0]
"We first compute µt, the mean position of focus:
µ′t = σ(v ᵀ c tanh(Wcgt +Ucµt−1 + bc)) µt =M ×min(µ′t + µt−1, 1.0)
where M is the number of characters in the sonnet line.",4.2 Pentameter Model,[0],[0]
"Given µt, we can compute the (unnormalised) probability for each character position:
ptj = exp",4.2 Pentameter Model,[0],[0]
"( −(j − µt)2
2T 2 ) where standard deviation T is a hyper-parameter.",4.2 Pentameter Model,[0],[0]
"We incorporate this position information when computing u∗t : 13
u′j = p t juj",4.2 Pentameter Model,[0],[0]
dtj = v ᵀ d tanh(Wdu ′,4.2 Pentameter Model,[0],[0]
j,4.2 Pentameter Model,[0],[0]
"+Udgt + bd)
f t = softmax(dt + logpt) u∗t = ∑ j btjuj
11That is, given the input line Shall I compare thee to a summer’s day?",4.2 Pentameter Model,[0],[0]
the model is required to output S− S+ S− S+ S− S+ S− S+,4.2 Pentameter Model,[0],[0]
"S− S+, based on the syllable boundaries from Section 3.
",4.2 Pentameter Model,[0],[0]
"12Initial input (u∗0) and state (g0) is a trainable vector and zero vector respectively.
",4.2 Pentameter Model,[0],[0]
"13Spaces are masked out, so they always yield zero attention weights.
",4.2 Pentameter Model,[0],[0]
"Intuitively, the attention network incorporates the position information at two points, when computing: (1) dtj by weighting the character encodings; and (2) f t by adding the position log probabilities.",4.2 Pentameter Model,[0],[0]
"This may appear excessive, but preliminary experiments found that this formulation produces the best performance.
",4.2 Pentameter Model,[0],[0]
"In a typical encoder–decoder model, the attended encoder vector u∗t would be combined with the decoder state gt to compute the output probability distribution.",4.2 Pentameter Model,[0],[0]
"Doing so, however, would result in a zero-loss model as it will quickly learn that it can simply ignore u∗t to predict the alternating stresses based on gt.",4.2 Pentameter Model,[0],[0]
"For this reason we use only u∗t to compute the stress probability:
P (S−) = σ(Weu ∗ t + be)
which gives the loss Lent = ∑
t− logP (S?t ) for the whole sequence, where S?t is the target stress at time step t.
We find the decoder still has the tendency to attend to the same characters, despite the incorporation of position information.",4.2 Pentameter Model,[0],[0]
"To regularise the model further, we introduce two loss penalties: repeat and coverage loss.
",4.2 Pentameter Model,[0],[0]
"The repeat loss penalises the model when it attends to previously attended characters (See et al., 2017), and is computed as follows:
Lrep = ∑ t ∑ j min(f tj , t−1∑ t=1 f tj )
By keeping a sum of attention weights over all previous time steps, we penalise the model when it focuses on characters that have non-zero history weights.
",4.2 Pentameter Model,[0],[0]
"The repeat loss discourages the model from focussing on the same characters, but does not assure that the appropriate characters receive attention.",4.2 Pentameter Model,[0],[0]
"Observing that stresses are aligned with the vowels of a syllable, we therefore penalise the model when vowels are ignored:
Lcov = ∑ j∈V ReLU(C − 10∑ t=1 f tj )
where V is a set of positions containing vowel characters, and C is a hyper-parameter that defines the minimum attention threshold that avoids penalty.
",4.2 Pentameter Model,[0],[0]
"To summarise, the pentameter model is optimised with the following loss:
Lpm = Lent + αLrep + βLcov (3)
where α and β are hyper-parameters for weighting the additional loss terms.",4.2 Pentameter Model,[0],[0]
"Two reasons motivate us to learn rhyme in an unsupervised manner: (1) we intend to extend the current model to poetry in other languages (which may not have pronunciation dictionaries); and (2) the language in our SONNET data is not Modern English, and so contemporary dictionaries may not accurately reflect the rhyme of the data.
",4.3 Rhyme Model,[0],[0]
"Exploiting the fact that rhyme exists in a quatrain, we feed sentence-ending word pairs of a quatrain as input to the rhyme model and train it to learn how to separate rhyming word pairs from non-rhyming ones.",4.3 Rhyme Model,[0],[0]
"Note that the model does not assume any particular rhyming scheme — it works as long as quatrains have rhyme.
",4.3 Rhyme Model,[0],[0]
"A training example consists of a number of word pairs, generated by pairing one target word with 3 other reference words in the quatrain, i.e. {(xt, xr), (xt, xr+1), (xt, xr+2)}, where xt is the target word and xr+i are the reference words.14",4.3 Rhyme Model,[0],[0]
We assume that in these 3 pairs there should be one rhyming and 2 non-rhyming pairs.,4.3 Rhyme Model,[0],[0]
From preliminary experiments we found that we can improve the model by introducing additional non-rhyming or negative reference words.,4.3 Rhyme Model,[0],[0]
"Negative reference words are sampled uniform randomly from the vocabulary, and the number of additional negative words is a hyper-parameter.
",4.3 Rhyme Model,[0],[0]
For each word x in the word pairs we embed the characters using the shared embedding matrix Wchr and feed them to an LSTM to produce the character states uj,4.3 Rhyme Model,[0],[0]
.15,4.3 Rhyme Model,[0],[0]
"Unlike the language and pentameter models, we use a unidirectional forward LSTM here (as rhyme is largely determined by the final characters), and the LSTM parameters are not shared.",4.3 Rhyme Model,[0],[0]
"We represent the encoding of the whole word by taking the last state u = uL, where L is the character length of the word.
",4.3 Rhyme Model,[0],[0]
"Given the character encodings, we use a
14E.g.",4.3 Rhyme Model,[0],[0]
"for the quatrain in Figure 1, a training example is {(day, temperate), (day, may), (day, date)}.
",4.3 Rhyme Model,[0],[0]
"15The character embeddings are the only shared parameters in this model.
margin-based loss to optimise the model:
Q = {cos(ut,ur), cos(ut,ur+1), ...}",4.3 Rhyme Model,[0],[0]
"Lrm = max(0, δ − top(Q, 1) + top(Q, 2))
where top(Q, k) returns the k-th largest element in Q, and δ is a margin hyper-parameter.
",4.3 Rhyme Model,[0],[0]
"Intuitively, the model is trained to learn a sufficient margin (defined by δ) that separates the best pair with all others, with the second-best being used to quantify all others.",4.3 Rhyme Model,[0],[0]
"This is the justification used in the multi-class SVM literature for a similar objective (Wang and Xue, 2014).
",4.3 Rhyme Model,[0],[0]
"With this network we can estimate whether two words rhyme by computing the cosine similarity score during generation, and resample words as necessary to enforce rhyme.",4.3 Rhyme Model,[0],[0]
"We focus on quatrain generation in this work, and so the aim is to generate 4 lines of poetry.",4.4 Generation Procedure,[0],[0]
During generation we feed the hidden state from the previous time step to the language model’s decoder to compute the vocabulary distribution for the current time step.,4.4 Generation Procedure,[0],[0]
"Words are sampled using a temperature between 0.6 and 0.8, and they are resampled if the following set of words is generated: (1) UNK token; (2) non-stopwords that were generated before;16 (3) any generated words with a frequency > 2; (4) the preceding 3 words; and (5) a number of symbols including parentheses, single and double quotes.17",4.4 Generation Procedure,[0],[0]
"The first sonnet line is generated without using any preceding context.
",4.4 Generation Procedure,[0],[0]
We next describe how to incorporate the pentameter model for generation.,4.4 Generation Procedure,[0],[0]
"Given a sonnet line, the pentameter model computes a loss Lpm (Equation (3))",4.4 Generation Procedure,[0],[0]
that indicates how well the line conforms to the iambic pentameter.,4.4 Generation Procedure,[0],[0]
"We first generate 10 candidate lines (all initialised with the same hidden state), and then sample one line from the candidate lines based on the pentameter loss values (Lpm).",4.4 Generation Procedure,[0],[0]
"We convert the losses into probabilities by taking the softmax, and a sentence is sampled with temperature = 0.1.
",4.4 Generation Procedure,[0],[0]
"To enforce rhyme, we randomly select one of the rhyming schemes (AABB, ABAB or ABBA) and resample sentence-ending words as necessary.",4.4 Generation Procedure,[0],[0]
"Given a pair of words, the rhyme model produces a cosine similarity score that estimates how well the
16We use the NLTK stopword list (Bird et al., 2009).",4.4 Generation Procedure,[0],[0]
"17We add these constraints to prevent the model from being
too repetitive, in generating the same words.
",4.4 Generation Procedure,[0],[0]
two words rhyme.,4.4 Generation Procedure,[0],[0]
We resample the second word of a rhyming pair (e.g. when generating the second A in AABB) until it produces a cosine similarity > 0.9.,4.4 Generation Procedure,[0],[0]
"We also resample the second word of a nonrhyming pair (e.g. when generating the first B in AABB) by requiring a cosine similarity 6 0.7.18
When generating in the forward direction we can never be sure that any particular word is the last word of a line, which creates a problem for resampling to produce good rhymes.",4.4 Generation Procedure,[0],[0]
"This problem is resolved in our model by reversing the direction of the language model, i.e. generating the last word of each line first.",4.4 Generation Procedure,[0],[0]
We apply this inversion trick at the word level (character order of a word is not modified) and only to the language model; the pentameter model receives the original word order as input.,4.4 Generation Procedure,[0],[0]
"We assess our sonnet model in two ways: (1) component evaluation of the language, pentameter and rhyme models; and (2) poetry generation evaluation, by crowd workers and an English literature expert.",5 Experiments,[0],[0]
"A sample of machine-generated sonnets are included in the supplementary material.
",5 Experiments,[0],[0]
We tune the hyper-parameters of the model over the development data (optimal configuration in the supplementary material).,5 Experiments,[0],[0]
"Word embeddings are initialised with pre-trained skip-gram embeddings (Mikolov et al., 2013a,b) on the BACKGROUND dataset, and are updated during training.",5 Experiments,[0],[0]
"For optimisers, we use Adagrad (Duchi et al., 2011) for the language model, and Adam (Kingma and Ba, 2014) for the pentameter and rhyme models.",5 Experiments,[0],[0]
"We truncate backpropagation through time after 2 sonnet lines, and train using 30 epochs, resetting the network weights to the weights from the previous epoch whenever development loss worsens.",5 Experiments,[0],[0]
We use standard perplexity for evaluating the language model.,5.1.1 Language Model,[0],[0]
"In terms of model variants, we have:19 • LM: Vanilla LSTM language model; • LM∗: LSTM language model that incorporates
character encodings (Equation (2)); 18Maximum number of resampling steps is capped at 1000.",5.1.1 Language Model,[0],[0]
"If the threshold is exceeded the model is reset to generate from scratch again.
",5.1.1 Language Model,[0],[0]
"19All models use the same (applicable) hyper-parameter configurations.
",5.1.1 Language Model,[0],[0]
• LM∗∗: LSTM language model that incorporates both character encodings and preceding context; • LM∗∗-C:,5.1.1 Language Model,[0],[0]
"Similar to LM∗∗, but preceding con-
text is encoded using convolutional networks, inspired by the poetry model of Zhang and Lapata (2014);20 • LM∗∗+PM+RM: the full model, with joint training of the language, pentameter and rhyme models.",5.1.1 Language Model,[0],[0]
Perplexity on the test partition is detailed in Table 2.,5.1.1 Language Model,[0],[0]
"Encouragingly, we see that the incorporation of character encodings and preceding context improves performance substantially, reducing perplexity by almost 10 points from LM to LM∗∗.",5.1.1 Language Model,[0],[0]
The inferior performance of LM∗∗-C compared to LM∗∗ demonstrates that our approach of processing context with recurrent networks with selective encoding is more effective than convolutional networks.,5.1.1 Language Model,[0],[0]
"The full model LM∗∗+PM+RM, which learns stress
20In Zhang and Lapata (2014), the authors use a series of convolutional networks with a width of 2 words to convert 5/7 poetry lines into a fixed size vector; here we use a standard convolutional network with max-pooling operation (Kim, 2014) to process the context.
and rhyme patterns simultaneously, also appears to improve the language model slightly.",5.1.1 Language Model,[0],[0]
"To assess the pentameter model, we use the attention weights to predict stress patterns for words in the test data, and compare them against stress patterns in the CMU pronunciation dictionary.21 Words that have no coverage or have nonalternating patterns given by the dictionary are discarded.",5.1.2 Pentameter Model,[0],[0]
"We use accuracy as the metric, and a predicted stress pattern is judged to be correct if it matches any of the dictionary stress patterns.
",5.1.2 Pentameter Model,[0],[0]
"To extract a stress pattern for a word from the model, we iterate through the pentameter (10 time steps), and append the appropriate stress (e.g. 1st time step = S−) to the word if any of its characters receives an attention > 0.20.
",5.1.2 Pentameter Model,[0],[0]
For the baseline (Stress-BL) we use the pretrained weighted finite state transducer (WFST) provided by Hopkins and Kiela (2017).22 The WFST maps a sequence word to a sequence of stresses by assuming each word has 1–5 stresses and the full word sequence produces iambic pentameter.,5.1.2 Pentameter Model,[0],[0]
"It is trained using the EM algorithm on a sonnet corpus developed by the authors.
",5.1.2 Pentameter Model,[0],[0]
We present stress accuracy in Table 2.,5.1.2 Pentameter Model,[0],[0]
"LM∗∗+PM+RM performs competitively, and informal inspection reveals that a number of mistakes are due to dictionary errors.",5.1.2 Pentameter Model,[0],[0]
"To understand the predicted stresses qualitatively, we display attention heatmaps for the the first quatrain of Shakespeare’s Sonnet 18 in Figure 3.",5.1.2 Pentameter Model,[0],[0]
"The y-axis represents the ten stresses of the iambic pentameter, and
21http://www.speech.cs.cmu.edu/cgi-bin/ cmudict.",5.1.2 Pentameter Model,[0],[0]
"Note that the dictionary provides 3 levels of stresses: 0, 1 and 2; we collapse 1 and 2 to S+.
22https://github.com/JackHopkins/ ACLPoetry
x-axis the characters of the sonnet line (punctuation removed).",5.1.2 Pentameter Model,[0],[0]
"The attention network appears to perform very well, without any noticeable errors.",5.1.2 Pentameter Model,[0],[0]
"The only minor exception is lovely in the second line, where it predicts 2 stresses but the second stress focuses incorrectly on the character e rather than y. Additional heatmaps for the full sonnet are provided in the supplementary material.",5.1.2 Pentameter Model,[0],[0]
"We follow a similar approach to evaluate the rhyme model against the CMU dictionary, but score based on F1 score.",5.1.3 Rhyme Model,[0],[0]
Word pairs that are not included in the dictionary are discarded.,5.1.3 Rhyme Model,[0],[0]
"Rhyme is determined by extracting the final stressed phoneme for the paired words, and testing if their phoneme patterns match.
",5.1.3 Rhyme Model,[0],[0]
"We predict rhyme for a word pair by feeding them to the rhyme model and computing cosine similarity; if a word pair is assigned a score > 0.8,23 it is considered to rhyme.",5.1.3 Rhyme Model,[0],[0]
"As a baseline (Rhyme-BL), we first extract for each word the last vowel and all following consonants, and predict a word pair as rhyming if their extracted sequences match.",5.1.3 Rhyme Model,[0],[0]
"The extracted sequence can be interpreted as a proxy for the last syllable of a word.
",5.1.3 Rhyme Model,[0],[0]
Reddy and Knight (2011) propose an unsupervised model for learning rhyme schemes in poems via EM.,5.1.3 Rhyme Model,[0],[0]
"There are two latent variables: φ specifies the distribution of rhyme schemes, and θ defines
230.8 is empirically found to be the best threshold based on development data.
",5.1.3 Rhyme Model,[0],[0]
the pairwise rhyme strength between two words.,5.1.3 Rhyme Model,[0],[0]
The model’s objective is to maximise poem likelihood over all possible rhyme scheme assignments under the latent variables φ and θ.,5.1.3 Rhyme Model,[0],[0]
"We train this model (Rhyme-EM) on our data24 and use the learnt θ to decide whether two words rhyme.25
Table 2 details the rhyming results.",5.1.3 Rhyme Model,[0],[0]
"The rhyme model performs very strongly at F1 > 0.90, well above both baselines.",5.1.3 Rhyme Model,[0],[0]
"Rhyme-EM performs poorly because it operates at the word level (i.e. it ignores character/orthographic information) and hence does not generalise well to unseen words and word pairs.26
To better understand the errors qualitatively, we present a list of word pairs with their predicted cosine similarity in Table 3.",5.1.3 Rhyme Model,[0],[0]
Examples on the left side are rhyming word pairs as determined by the CMU dictionary; right are non-rhyming pairs.,5.1.3 Rhyme Model,[0],[0]
"Looking at the rhyming word pairs (left), it appears that these words tend not to share any wordending characters.",5.1.3 Rhyme Model,[0],[0]
"For the non-rhyming pairs, we spot several CMU errors: (sire, ire) and (queen, been) clearly rhyme.",5.1.3 Rhyme Model,[0],[0]
"Following Hopkins and Kiela (2017), we present a pair of quatrains (one machine-generated and one human-written, in random order) to crowd workers on CrowdFlower, and ask them to guess which is the human-written poem.",5.2.1 Crowdworker Evaluation,[0],[0]
"Generation quality is estimated by computing the accuracy of workers at correctly identifying the human-written poem (with lower values indicate better results for the model).
",5.2.1 Crowdworker Evaluation,[0],[0]
"We generate 50 quatrains each for LM, LM∗∗ and LM∗∗+PM+RM (150 in total), and as a control, generate 30 quatrains with LM trained for one epoch.",5.2.1 Crowdworker Evaluation,[0],[0]
An equal number of human-written quatrains was sampled from the training partition.,5.2.1 Crowdworker Evaluation,[0],[0]
"A HIT contained 5 pairs of poems (of which one is a control), and workers were paid $0.05 for each HIT.",5.2.1 Crowdworker Evaluation,[0],[0]
"Workers who failed to identify the human-written poem in the control pair reliably (minimum accuracy = 70%) were removed by CrowdFlower automati-
24We use the original authors’ implementation: https: //github.com/jvamvas/rhymediscovery.
",5.2.1 Crowdworker Evaluation,[0],[0]
"25A word pair is judged to rhyme if θw1,w2 > 0.02; the threshold (0.02) is selected based on development performance.
",5.2.1 Crowdworker Evaluation,[0],[0]
"26Word pairs that did not co-occur in a poem in the training data have rhyme strength of zero.
cally, and they were restricted to do a maximum of 3 HITs.",5.2.1 Crowdworker Evaluation,[0],[0]
"To dissuade workers from using search engines to identify real poems, we presented the quatrains as images.
",5.2.1 Crowdworker Evaluation,[0],[0]
Accuracy is presented in Table 4.,5.2.1 Crowdworker Evaluation,[0],[0]
"We see a steady decrease in accuracy (= improvement in model quality) from LM to LM∗∗ to LM∗∗+PM+RM, indicating that each model generates quatrains that are less distinguishable from human-written ones.",5.2.1 Crowdworker Evaluation,[0],[0]
"Based on the suspicion that workers were using rhyme to judge the poems, we tested a second model, LM∗∗+RM, which is the full model without the pentameter component.",5.2.1 Crowdworker Evaluation,[0],[0]
"We found identical accuracy (0.532), confirming our suspicion that crowd workers depend on only rhyme in their judgements.",5.2.1 Crowdworker Evaluation,[0],[0]
These observations demonstrate that meter is largely ignored by lay persons in poetry evaluation.,5.2.1 Crowdworker Evaluation,[0],[0]
"To better understand the qualitative aspects of our generated quatrains, we asked an English literature expert (a Professor of English literature at a major English-speaking university; the last author of this paper) to directly rate 4 aspects: meter, rhyme, readability and emotion (i.e. amount of emotion the poem evokes).",5.2.2 Expert Judgement,[0],[0]
All are rated on an ordinal scale between 1 to 5 (1 = worst; 5 = best).,5.2.2 Expert Judgement,[0],[0]
"In total, 120 quatrains were annotated, 30 each for LM, LM∗∗, LM∗∗+PM+RM, and human-written poems (Human).",5.2.2 Expert Judgement,[0],[0]
The expert was blind to the source of each poem.,5.2.2 Expert Judgement,[0],[0]
"The mean and standard deviation of the ratings are presented in Table 5.
",5.2.2 Expert Judgement,[0],[0]
"We found that our full model has the highest ratings for both rhyme and meter, even higher than
human poets.",5.2.2 Expert Judgement,[0],[0]
"This might seem surprising, but in fact it is well established that real poets regularly break rules of form to create other effects (Adams, 1997).",5.2.2 Expert Judgement,[0],[0]
"Despite excellent form, the output of our model can easily be distinguished from humanwritten poetry due to its lower emotional impact and readability.",5.2.2 Expert Judgement,[0],[0]
"In particular, there is evidence here that our focus on form actually hurts the readability of the resulting poems, relative even to the simpler language models.",5.2.2 Expert Judgement,[0],[0]
"Another surprise is how well simple language models do in terms of their grasp of meter: in this expert evaluation, we see only marginal benefit as we increase the sophistication of the model.",5.2.2 Expert Judgement,[0],[0]
"Taken as a whole, this evaluation suggests that future research should look beyond forms, towards the substance of good poetry.",5.2.2 Expert Judgement,[0],[0]
"We propose a joint model of language, meter and rhyme that captures language and form for modelling sonnets.",6 Conclusion,[0],[0]
"We provide quantitative analyses for each component, and assess the quality of generated poems using judgements from crowdworkers and a literature expert.",6 Conclusion,[0],[0]
"Our research reveals that vanilla LSTM language model captures meter implicitly, and our proposed rhyme model performs exceptionally well.",6 Conclusion,[0],[0]
"Machine-generated generated poems, however, still underperform in terms of readability and emotion.",6 Conclusion,[0],[0]
"In this paper, we propose a joint architecture that captures language, rhyme and meter for sonnet modelling.",abstractText,[0],[0]
We assess the quality of generated poems using crowd and expert judgements.,abstractText,[0],[0]
"The stress and rhyme models perform very well, as generated poems are largely indistinguishable from human-written poems.",abstractText,[0],[0]
"Expert evaluation, however, reveals that a vanilla language model captures meter implicitly, and that machine-generated poems still underperform in terms of readability and emotion.",abstractText,[0],[0]
"Our research shows the importance expert evaluation for poetry generation, and that future research should look beyond rhyme/meter and focus on poetic language.",abstractText,[0],[0]
"Deep-speare: A joint neural model of poetic language, meter and rhyme",title,[0],[0]
The composition of polyphonic chorale music in the style of J.S. Bach has represented a major challenge in automatic music composition over the last decades.,1. Introduction,[0],[0]
"The corpus of the chorale harmonizations by Johann Sebastian Bach is remarkable by its homogeneity and its size (389 chorales in (Bach, 1985)).",1. Introduction,[0],[0]
"All these short pieces (approximately one minute long) are written for a four-part chorus (soprano, alto, tenor and bass) using similar compositional principles: the composer takes a well-known (at that time) melody from a Lutheran hymn and harmonizes it i.e. the three lower parts (alto, tenor and bass) accompanying the soprano (the highest part) are composed, see Fig.1 for an example.
",1. Introduction,[0],[0]
"1LIP6, Université Pierre et Marie Curie 2Sony CSL, Paris 3Sony CSL, Japan.",1. Introduction,[0],[0]
"Correspondence to: Gaëtan Hadjeres <gaetan.hadjeres@etu.upmc.fr>, François",1. Introduction,[0],[0]
"Pachet <pachetcsl@gmail.com>, Frank Nielsen <Frank.Nielsen@acm.org>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"Moreover, since the aim of reharmonizing a melody is to give more power or new insights to its text, the lyrics have to be understood clearly.",1. Introduction,[0],[0]
"We say that voices are in homophony, i.e. they articulate syllables simultaneously.",1. Introduction,[0],[0]
"This implies characteristic rhythms, variety of harmonic ideas as well as characteristic melodic movements which make the style of these chorale compositions easily distinguishable, even for non experts.
",1. Introduction,[0],[0]
"The difficulty, from a compositional point of view comes from the intricate interplay between harmony (notes sounding at the same time) and voice movements (how a single voice evolves through time).",1. Introduction,[0],[0]
"Furthermore, each voice has its own “style” and its own coherence.",1. Introduction,[0],[0]
"Finding a chorale-like reharmonization which combines Bach-like harmonic progressions with musically interesting melodic movements is a problem which often takes years of practice for musicians.
",1. Introduction,[0],[0]
"From the point of view of automatic music generation, the first solution to this apparently highly combinatorial problem was proposed by (Ebcioglu, 1988) in 1988.",1. Introduction,[0],[0]
"This problem is seen as a constraint satisfaction problem, where the system must fulfill numerous hand-crafted constraints characterizing the style of Bach.",1. Introduction,[0],[0]
It is a rule-based expert system which contains no less than 300 rules and tries to reharmonize a given melody with a generate-and-test method and intelligent backtracking.,1. Introduction,[0],[0]
"Among the short examples presented at the end of the paper, some are flawless.",1. Introduction,[0],[0]
"The drawbacks of this method are, as stated by the author, the considerable effort to generate the rule base and the fact that the harmonizations produced “do not sound like Bach, except for occasional Bachian patterns and cadence formulas.”",1. Introduction,[0],[0]
"In our opinion, the requirement of an expert knowledge implies a lot of subjective choices.
",1. Introduction,[0],[0]
"A neural-network-based solution was later developed by (Hild et al., 1992).",1. Introduction,[0],[0]
"This method relies on several neural networks, each one trained for solving a specific task: a harmonic skeleton is first computed then refined and ornamented.",1. Introduction,[0],[0]
"A similar approach is adopted in (Allan & Williams, 2005), but uses Hidden Markov Models (HMMs) instead of neural networks.",1. Introduction,[0],[0]
"Chords are represented as lists of intervals and form the states of the Markov mod-
2https://www.youtube.com/watch?v=",1. Introduction,[0],[0]
"73WF0M99vlg
els.",1. Introduction,[0],[0]
These approaches produce interesting results even if they both use expert knowledge and bias the generation by imposing their compositional process.,1. Introduction,[0],[0]
"In (Whorley et al., 2013; Whorley & Conklin, 2016), authors elaborate on those methods by introducing multiple viewpoints and variations on the sampling method (generated sequences which violate “rules of harmony” are put aside for instance).",1. Introduction,[0],[0]
"However, this approach does not produce a convincing chorale-like texture, rhythmically as well as harmonically and the resort to hand-crafted criteria to assess the quality of the generated sequences might rule out many musically-interesting solutions.
",1. Introduction,[0],[0]
"Recently, agnostic approaches (requiring no knowledge about harmony, Bach’s style or music) using neural networks have been investigated with promising results.",1. Introduction,[0],[0]
"In (Boulanger-Lewandowski et al., 2012), chords are modeled with Restricted Boltzmann Machines (RBMs).",1. Introduction,[0],[0]
Their temporal dependencies are learned using Recurrent Neural Networks (RNNs).,1. Introduction,[0],[0]
"Variations of these architectures based on Long Short-Term Memory (LSTM) units ((Hochreiter & Schmidhuber, 1997; Mikolov et al., 2014)) or GRUs (Gated Recurrent Units) have been developed by (Lyu et al., 2015) and (Chung et al., 2014) respectively.",1. Introduction,[0],[0]
"However, these models which work on piano roll representations of the music are too general to capture the specificity of Bach chorales.",1. Introduction,[0],[0]
"Also, a major drawback is their lack of flexibility.",1. Introduction,[0],[0]
Generation is performed from left to right.,1. Introduction,[0],[0]
A user cannot interact with the system: it is impossible to do reharmonization for instance which is the essentially how the corpus of Bach chorales was composed.,1. Introduction,[0],[0]
"Moreover, their invention capacity and non-plagiarism abilities are not demonstrated.
",1. Introduction,[0],[0]
"A method that addresses the rigidity of sequential generation in music was first proposed in (Sakellariou et al., 2015; Sakellariou et al., 2016) for monophonic music and later generalized to polyphony in (Hadjeres et al., 2016).",1. Introduction,[0],[0]
"These approaches advocate for the use of Gibbs sampling as a generation process in automatic music composition.
",1. Introduction,[0],[0]
"The most recent advances in chorale harmonization is arguably the BachBot model (Liang, 2016), a LSTMbased approach specifically designed to deal with Bach
chorales.",1. Introduction,[0],[0]
This approach relies on little musical knowledge (all chorales are transposed in a common key) and is able to produce high-quality chorale harmonizations.,1. Introduction,[0],[0]
"However, compared to our approach, this model is less general (produced chorales are all in the C key for instance) and less flexible (only the soprano can be fixed).",1. Introduction,[0],[0]
"Similarly to our work, the authors evaluate their model with an online Turing test to assess the efficiency of their model.",1. Introduction,[0],[0]
"They also take into account the fermata symbols (Fig. 2) which are indicators of the structure of the chorales.
",1. Introduction,[0],[0]
"In this paper we introduce DeepBach, a dependency network (Heckerman et al., 2000) capable of producing musically convincing four-part chorales in the style of Bach by using a Gibbs-like sampling procedure.",1. Introduction,[0],[0]
"Contrary to models based on RNNs, we do not sample from left to right which allows us to enforce positional, unary user-defined constraints such as rhythm, notes, parts, chords and cadences.",1. Introduction,[0],[0]
"DeepBach is able to generate coherent musical phrases and provides, for instance, varied reharmonizations of melodies without plagiarism.",1. Introduction,[0],[0]
"Its core features are its speed, the possible interaction with users and the richness of harmonic ideas it proposes.",1. Introduction,[0],[0]
"Its efficiency opens up new ways of composing Bach-like chorales for non experts in an interactive manner similarly to what is proposed in (Papadopoulos et al., 2016) for leadsheets.
",1. Introduction,[0],[0]
In Sect.,1. Introduction,[0],[0]
2 we present the DeepBach model for four-part chorale generation.,1. Introduction,[0],[0]
We discuss in Sect.,1. Introduction,[0],[0]
3 the results of an experimental study we conducted to assess the quality of our model.,1. Introduction,[0],[0]
"Finally, we provide generated examples in Sect.",1. Introduction,[0],[0]
4.3 and elaborate on the possibilities offered by our interactive music composition editor in Sect.,1. Introduction,[0],[0]
4.,1. Introduction,[0],[0]
All examples can be heard on the accompanying web page3 and the code of our implementation is available on GitHub4.,1. Introduction,[0],[0]
"Even if our presentation focuses on Bach chorales, this model has been successfully applied to other styles and composers including Monteverdi five-voice madrigals to Palestrina masses.
",1. Introduction,[0],[0]
"3https://sites.google.com/site/ deepbachexamples/
4https://github.com/Ghadjeres/DeepBach",1. Introduction,[0],[0]
In this paper we introduce a generative model which takes into account the distinction between voices.,2. DeepBach,[0],[0]
Sect.,2. DeepBach,[0],[0]
2.1 presents the data representation we used.,2. DeepBach,[0],[0]
This representation is both fitted for our sampling procedure and more accurate than many data representation commonly used in automatic music composition.,2. DeepBach,[0],[0]
Sect.,2. DeepBach,[0],[0]
2.2 presents the model’s architecture and Sect.,2. DeepBach,[0],[0]
2.3 our generation method.,2. DeepBach,[0],[0]
"Finally, Sect. 2.4 provides implementation details and indicates how we preprocessed the corpus of Bach chorale harmonizations.",2. DeepBach,[0],[0]
We use MIDI pitches to encode notes and choose to model voices separately.,2.1.1. NOTES AND VOICES,[0],[0]
"We consider that only one note can be sung at a given time and discard chorales with voice divisions.
",2.1.1. NOTES AND VOICES,[0],[0]
"Since Bach chorales only contain simple time signatures, we discretize time with sixteenth notes, which means that each beat is subdivided into four equal parts.",2.1.1. NOTES AND VOICES,[0],[0]
"Since there is no smaller subdivision in Bach chorales, there is no loss of information in this process.
",2.1.1. NOTES AND VOICES,[0],[0]
"In this setting, a voice Vi = {Vti }t is a list of notes indexed by t ∈",2.1.1. NOTES AND VOICES,[0],[0]
"[T ]5, where T is the duration piece (in sixteenth notes).",2.1.1. NOTES AND VOICES,[0],[0]
We choose to model rhythm by simply adding a hold symbol “ ” coding whether or not the preceding note is held to the list of existing notes.,2.1.2. RHYTHM,[0],[0]
"This representation is thus unambiguous, compact and well-suited to our sampling method (see Sect.",2.1.2. RHYTHM,[0],[0]
2.3.4).,2.1.2. RHYTHM,[0],[0]
The music sheet (Fig. 1b) conveys more information than only the notes played.,2.1.3. METADATA,[0],[0]
"We can cite:
• the lyrics,
• the key signature,
• the time signature,
• the beat index,
• an implicit metronome (on which subdivision of the beat the note is played),
• the fermata symbols (see Fig. 2), 5We adopt the standard notation [N ] to denote the set of inte-
gers {1, . . .",2.1.3. METADATA,[0],[0]
", N} for any integer N .
",2.1.3. METADATA,[0],[0]
"In the following, we will only take into account the fermata symbols, the subdivision indexes and the current key signature.",2.1.3. METADATA,[0],[0]
"To this end, we introduce:
•",2.1.3. METADATA,[0],[0]
"The fermata list F that indicates if there is a fermata symbol, see Fig. 2, over the current note, it is a Boolean value.",2.1.3. METADATA,[0],[0]
"If a fermata is placed over a note on the music sheet, we consider that it is active for all time indexes within the duration of the note.
",2.1.3. METADATA,[0],[0]
•,2.1.3. METADATA,[0],[0]
The subdivision list S that contains the subdivision indexes of the beat.,2.1.3. METADATA,[0],[0]
It is an integer between 1 and 4: there is no distinction between beats in a bar so that our model is able to deal with chorales with three and four beats per measure.,2.1.3. METADATA,[0],[0]
"We represent a chorale as a couple
(V,M) (1)
composed of voices and metadata.",2.1.4. CHORALE,[0],[0]
"For Bach chorales, V is a list of 4 voices Vi for i ∈",2.1.4. CHORALE,[0],[0]
"[4] (soprano, alto, tenor and bass) andM a collection of metadata lists (F and S).
",2.1.4. CHORALE,[0],[0]
Our choices are very general and do not involve expert knowledge about harmony or scales but are only mere observations of the corpus.,2.1.4. CHORALE,[0],[0]
The list S acts as a metronome.,2.1.4. CHORALE,[0],[0]
The list F is added since fermatas in Bach chorales indicate the end of each musical phrase.,2.1.4. CHORALE,[0],[0]
The use of fermata to this end is a specificity of Bach chorales that we want to take advantage of.,2.1.4. CHORALE,[0],[0]
We choose to consider the metadata sequences in M as given.,2.2. Model Architecture,[0],[0]
"For clarity, we suppose in this section that our dataset is composed of only one chorale written as in Eq. 1 of size T .",2.2. Model Architecture,[0],[0]
"We define a dependency network on the finite set of variables V = {V ti } by specifying a set of conditional probability distributions (parametrized by parameter θi,t){
pi,t(V t i |V\i,t,M, θi,t) } i∈[4],t∈[T ] , (2)
where Vti indicates the note of voice i at time index t and V\i,t all variables in V except from the variable Vti .",2.2. Model Architecture,[0],[0]
"As we want our model to be time invariant so that we can apply it to sequences of any size, we share the parameters between all conditional probability distributions on variables lying in the same voice, i.e.
θi := θi,t, pi := pi,t ∀t ∈",2.2. Model Architecture,[0],[0]
"[T ].
Finally, we fit each of these conditional probability distributions on the data by maximizing the log-likelihood.",2.2. Model Architecture,[0],[0]
"Due to weight sharing, this amounts to solving four classification problems of the form:
max θi ∑ t log pi(Vti |V\i,t,M, θi), for i ∈",2.2. Model Architecture,[0],[0]
"[4], (3)
where the aim is to predict a note knowing the value of its neighboring notes, the subdivision of the beat it is on and the presence of fermatas.",2.2. Model Architecture,[0],[0]
"The advantage with this formulation is that each classifier has to make predictions within a small range of notes whose ranges correspond to the notes within the usual voice ranges (see 2.4).
",2.2. Model Architecture,[0],[0]
"For accurate predictions and in order to take into account the sequential aspect of the data, each classifier is modeled using four neural networks: two Deep Recurrent Neural Networks (Pascanu et al., 2013), one summing up past information and another summing up information coming from the future together with a non-recurrent neural network for notes occurring at the same time.",2.2. Model Architecture,[0],[0]
Only the last output from the uppermost RNN layer is kept.,2.2. Model Architecture,[0],[0]
"These three outputs are then merged and passed as the input of a fourth neural network whose output is pi(Vti |V\i,t,M, θ).",2.2. Model Architecture,[0],[0]
Figure 4 shows a graphical representation for one of these models.,2.2. Model Architecture,[0],[0]
Details are provided in Sect.,2.2. Model Architecture,[0],[0]
2.4.,2.2. Model Architecture,[0],[0]
These choices of architecture somehow match real compositional practice on Bach chorales.,2.2. Model Architecture,[0],[0]
"Indeed, when reharmonizing a given melody, it is often simpler to start from the cadence and write music “backwards.”",2.2. Model Architecture,[0],[0]
Generation in dependency networks is performed using the pseudo-Gibbs sampling procedure.,2.3.1. ALGORITHM,[0],[0]
"This Markov Chain
Monte Carlo (MCMC) algorithm is described in Alg.1.",2.3.1. ALGORITHM,[0],[0]
"It is similar to the classical Gibbs sampling procedure (Geman & Geman, 1984) on the difference that the conditional distributions are potentially incompatible (Chen & Ip, 2015).",2.3.1. ALGORITHM,[0],[0]
This means that the conditional distributions of Eq.,2.3.1. ALGORITHM,[0],[0]
(2) do not necessarily comes from a joint distribution p(V) and that the theoretical guarantees that the MCMC converges to this stationary joint distribution vanish.,2.3.1. ALGORITHM,[0],[0]
"We experimentally verified that it was indeed the case by checking that the Markov Chain of Alg.1 violates Kolmogorov’s criterion (Kelly, 2011): it is thus not reversible and cannot converge to a joint distribution whose conditional distributions match the ones used for sampling.
",2.3.1. ALGORITHM,[0],[0]
"However, this Markov chain converges to another stationary distribution and applications on real data demonstrated that this method yielded accurate joint probabilities, especially when the inconsistent probability distributions are learned from data (Heckerman et al., 2000).",2.3.1. ALGORITHM,[0],[0]
"Furthermore, nonreversible MCMC algorithms can in particular cases be better at sampling that reversible Markov Chains (Vucelja, 2014).",2.3.1. ALGORITHM,[0],[0]
The advantage of this method is that we can enforce userdefined constraints by tweaking Alg.,2.3.2. FLEXIBILITY OF THE SAMPLING PROCEDURE,[0],[0]
"1:
• instead of choosing voice i from 1 to 4 we can choose to fix the soprano and only resample voices from 2, 3
Algorithm 1 Pseudo-Gibbs sampling 1: Input: Chorale length L, metadataM containing lists
of length L, probability distributions (p1, p2, p3, p4), maximum number of iterations M 2: Create four lists V = (V1,V2,V3,V4) of length L 3: {The lists are initialized with random notes drawn from
the ranges of the corresponding voices (sampled uniformly or from the marginal distributions of the notes)}
4: for m from 1 to M do 5: Choose voice i uniformly between 1 and 4 6: Choose time t uniformly between 1 and L 7: Re-sample Vti from pi(Vti |V\i,t,M, θi) 8: end for 9: Output: V = (V1,V2,V3,V4)
and 4 in step (3) in order to provide reharmonizations of the fixed melody
• we can choose the fermata list F in order to impose end of musical phrases at some places
• more generally, we can impose any metadata
• for any t and any i, we can fix specific subsets Rti of notes within the range of voice i.",2.3.2. FLEXIBILITY OF THE SAMPLING PROCEDURE,[0],[0]
"We then restrict ourselves to some specific chorales by re-sampling Vti from
pi(Vti |V\i,t,M, θi,Vti ∈",2.3.2. FLEXIBILITY OF THE SAMPLING PROCEDURE,[0],[0]
"Rti)
at step (5).",2.3.2. FLEXIBILITY OF THE SAMPLING PROCEDURE,[0],[0]
"This allows us for instance to fix rhythm (since the hold symbol is considered as a note), impose some chords in a soft manner or restrict the vocal ranges.",2.3.2. FLEXIBILITY OF THE SAMPLING PROCEDURE,[0],[0]
Note that it is possible to make generation faster by making parallel Gibbs updates on GPU.,2.3.3. PERFORMANCE,[0],[0]
Steps (3) to (5) from Alg. 1 can be run simultaneously to provide significant speedups.,2.3.3. PERFORMANCE,[0],[0]
"Even if it is known that this approach is biased (De Sa et al., 2016) (since we can update simultaneously variables which are not conditionally independent), we experimentally observed that for small batch sizes (16 or 32), DeepBach still generates samples of great musicality while running ten times faster than the sequential version.",2.3.3. PERFORMANCE,[0],[0]
"This allows DeepBach to generate chorales in a few seconds.
",2.3.3. PERFORMANCE,[0],[0]
"It is also possible to use the hard-disk-configurations generation algorithm (Alg.2.9 in (Krauth, 2006)) to appropriately choose all the time indexes at which we parallelly resample so that:
• every time index is at distance at least δ from the other time indexes
• configurations of time indexes satisfying the relation above are equally sampled.
",2.3.3. PERFORMANCE,[0],[0]
This trick allows to assert that we do not update simultaneously a variable and its local context.,2.3.3. PERFORMANCE,[0],[0]
We emphasize on this section the importance of our particular choice of data representation with respect to our sampling procedure.,2.3.4. IMPORTANCE OF THE DATA REPRESENTATION,[0],[0]
"The fact that we obtain great results using pseudo-Gibbs sampling relies exclusively on our choice to integrate the hold symbol into the list of notes.
",2.3.4. IMPORTANCE OF THE DATA REPRESENTATION,[0],[0]
"Indeed, Gibbs sampling fails to sample the true joint distribution p(V|M, θ) when variables are highly correlated, creating isolated regions of high probability states in which the MCMC chain can be trapped.",2.3.4. IMPORTANCE OF THE DATA REPRESENTATION,[0],[0]
"However, many data representations used in music modeling such as
• the piano-roll representation,
• the couple (pitch, articulation) representation where articulation is a Boolean value indicating whether or not the note is played or held,
tend to make the musical data suffer from this drawback.
",2.3.4. IMPORTANCE OF THE DATA REPRESENTATION,[0],[0]
"As an example, in the piano-roll representation, a long note is represented as the repetition of the same value over many variables.",2.3.4. IMPORTANCE OF THE DATA REPRESENTATION,[0],[0]
"In order to only change its pitch, one needs to change simultaneously a large number of variables (which is exponentially rare) while this is achievable with only one variable change with our representation.",2.3.4. IMPORTANCE OF THE DATA REPRESENTATION,[0],[0]
"We implemented DeepBach using Keras (Chollet, 2015) with the Tensorflow (Abadi et al., 2015) backend.",2.4. Implementation Details,[0],[0]
"We used the database of chorale harmonizations by J.S. Bach included in the music21 toolkit (Cuthbert & Ariza, 2010).",2.4. Implementation Details,[0],[0]
"After removing chorales with instrumental parts and chorales containing parts with two simultaneous notes (bass parts sometimes divide for the last chord), we ended up with 352 pieces.",2.4. Implementation Details,[0],[0]
"Contrary to other approaches which transpose all chorales to the same key (usually in C major or A minor), we choose to augment our dataset by adding all chorale transpositions which fit within the vocal ranges defined by the initial corpus.",2.4. Implementation Details,[0],[0]
This gives us a corpus of 2503 chorales and split it between a training set (80%) and a validation set (20%).,2.4. Implementation Details,[0],[0]
"The vocal ranges contains less than 30 different pitches for each voice (21, 21, 21, 28) for the soprano, alto, tenor and bass parts respectively.
",2.4. Implementation Details,[0],[0]
"As shown in Fig. 4, we model only local interactions between a note Vti and its context (V\i,t, M) i.e. only elements with time index t between t − ∆t and t + ∆t are
taken as inputs of our model for some scope ∆t.",2.4. Implementation Details,[0],[0]
"This approximation appears to be accurate since musical analysis reveals that Bach chorales do not exhibit clear long-term dependencies.
",2.4. Implementation Details,[0],[0]
The reported results in Sect.,2.4. Implementation Details,[0],[0]
3 and examples in Sect.,2.4. Implementation Details,[0],[0]
4.3 were obtained with ∆t = 16.,2.4. Implementation Details,[0],[0]
"We chose as the “neural network brick” in Fig. 4 a neural network with one hidden layer of size 200 and ReLU (Nair & Hinton, 2010)",2.4. Implementation Details,[0],[0]
"nonlinearity and as the “Deep RNN brick” two stacked LSTMs (Hochreiter & Schmidhuber, 1997; Mikolov et al., 2014), each one being of size 200 (see Fig. 2 (f) in (Li & Wu, 2015)).",2.4. Implementation Details,[0],[0]
"The “embedding brick” applies the same neural network to each time slice (Vt,Mt).",2.4. Implementation Details,[0],[0]
"There are 20% dropout on input and 50% dropout after each layer.
",2.4. Implementation Details,[0],[0]
We experimentally found that sharing weights between the left and right embedding layers improved neither validation accuracy nor the musical quality of our generated chorales.,2.4. Implementation Details,[0],[0]
We evaluated the quality of our model with an online test conducted on human listeners.,3. Experimental Results,[0],[0]
"For the parameters used in our experiment, see Sect 2.4.",3.1. Setup,[0],[0]
"We compared our model with two other models: a Maximum Entropy model (MaxEnt) as in (Hadjeres et al., 2016) and a Multilayer Perceptron (MLP) model.
",3.1. Setup,[0],[0]
The Maximum Entropy model is a neural network with no hidden layer.,3.1. Setup,[0],[0]
"It is given by:
pi(Vti |V\i,t,M, Ai, bi) = Softmax(AX + b) (4)
where X is a vector containing the elements in V\i,t ∪Mt, Ai a (ni,mi) matrix and bi a vector of size mi with mi being the size of X , ni the number of notes in the voice range i and Softmax the softmax function given by
Softmax(z)j = ezj∑K k=1 e zk for j ∈",3.1. Setup,[0],[0]
"[K],
for a vector z = (z1, . . .",3.1. Setup,[0],[0]
", zK).
",3.1. Setup,[0],[0]
"The Multilayer Perceptron model we chose takes as input elements in V\i,t∪M, is a neural network with one hidden layer of size 500 and uses a ReLU (Nair & Hinton, 2010)",3.1. Setup,[0],[0]
"nonlinearity.
",3.1. Setup,[0],[0]
"All models are local and have the same scope ∆t, see Sect. 2.4.
",3.1. Setup,[0],[0]
Subjects were asked to give information about their musical expertise.,3.1. Setup,[0],[0]
"They could choose what category fits them best between:
1.",3.1. Setup,[0],[0]
"I seldom listen to classical music
2.",3.1. Setup,[0],[0]
"Music lover or musician
3.",3.1. Setup,[0],[0]
"Student in music composition or professional musician.
",3.1. Setup,[0],[0]
"The musical extracts have been obtained by reharmonizing 50 chorales from the validation set by each of the three models (MaxEnt, MLP, DeepBach).",3.1. Setup,[0],[0]
"We rendered the MIDI files using the Leeds Town Hall Organ soundfont6 and cut two extracts of 12 seconds from each chorale, which gives us 400 musical extracts for our test: 4 versions for each of the 100 melody chunks.",3.1. Setup,[0],[0]
"We chose our rendering so that the generated parts (alto, tenor and bass) can be distinctly heard and differentiated from the soprano part (which is fixed and identical for all models): in our mix, dissonances are easily heard, the velocity is the same for all notes as in a real organ performance and the sound does not decay, which is important when evaluating the reharmonization of long notes.",3.1. Setup,[0],[0]
Subjects were presented series of only one musical extract together with the binary choice “Bach” or “Computer”.,3.2. Discrimination Test: “Bach or Computer” experiment,[0],[0]
Fig. 5 shows how the votes are distributed depending on the level of musical expertise of the subjects for each model.,3.2. Discrimination Test: “Bach or Computer” experiment,[0],[0]
"For this experiment, 1272 people took this test, 261 with musical expertise 1, 646 with musical expertise 2 and 365 with musical expertise 3.
",3.2. Discrimination Test: “Bach or Computer” experiment,[0],[0]
The results are quite clear: the percentage of “Bach” votes augment as the model’s complexity increase.,3.2. Discrimination Test: “Bach or Computer” experiment,[0],[0]
"Furthermore, the distinction between computer-generated extracts and Bach’s extracts is more accurate when the level of musical expertise is higher.",3.2. Discrimination Test: “Bach or Computer” experiment,[0],[0]
"When presented a DeepBach-generated
6https://www.samplephonics.com/products/ free/sampler-instruments/the-leeds-townhall-organ
extract, around 50% of the voters would judge it as composed by Bach.",3.2. Discrimination Test: “Bach or Computer” experiment,[0],[0]
"We consider this to be a good score knowing the complexity of Bach’s compositions and the facility to detect badly-sounding chords even for non musicians.
",3.2. Discrimination Test: “Bach or Computer” experiment,[0],[0]
We also plotted specific results for each of the 400 extracts.,3.2. Discrimination Test: “Bach or Computer” experiment,[0],[0]
Fig. 6 shows for each reharmonization extract the percentage of Bach votes it collected: more than half of the DeepBach’s automatically-composed extracts has a majority of votes considering them as being composed by J.S. Bach while it is only a third for the MLP model.,3.2. Discrimination Test: “Bach or Computer” experiment,[0],[0]
We developed a plugin on top of the MuseScore music editor allowing a user to call DeepBach on any rectangular region.,4.1. Description,[0],[0]
"Even if the interface is minimal (see Fig.7), the possibilities are numerous: we can generate a chorale from scratch, reharmonize a melody and regenerate a given chord, bar or part.",4.1. Description,[0],[0]
We believe that this interplay between a user and the system can boost creativity and can interest a wide range of audience.,4.1. Description,[0],[0]
We made two major changes between the model we described for the online test and the interactive composition tool.,4.2. Adapting the model,[0],[0]
We changed the MIDI encoding of the notes to a full name encoding of the notes.,4.2.1. NOTE ENCODING,[0],[0]
"Indeed, some information is lost when reducing a music sheet to its MIDI representation since we cannot differentiate between two enharmonic
notes (notes that sound the same but that are written differently e.g. F# and Gb).",4.2.1. NOTE ENCODING,[0],[0]
"This difference in Bach chorales is unambiguous and it is thus natural to consider the full name of the notes, like C#3, Db3 or E#4.",4.2.1. NOTE ENCODING,[0],[0]
"From a machine learning point of view, these notes would appear in totally different contexts.",4.2.1. NOTE ENCODING,[0],[0]
"This improvement enables the model to generate notes with the correct spelling, which is important when we focus on the music sheet rather than on its audio rendering.",4.2.1. NOTE ENCODING,[0],[0]
We added the current key signature list K to the metadataM. This allows users to impose modulations and key changes.,4.2.2. STEERING MODULATIONS,[0],[0]
Each element Kt of this list contains the number of sharps of the estimated key for the current bar.,4.2.2. STEERING MODULATIONS,[0],[0]
It is a integer between -7 and 7.,4.2.2. STEERING MODULATIONS,[0],[0]
The current key is computed using the key analyzer algorithm from music21.,4.2.2. STEERING MODULATIONS,[0],[0]
We now provide and comment on examples of chorales generated using the DeepBach plugin.,4.3. Generation examples,[0],[0]
Our aim is to show the quality of the solutions produced by DeepBach.,4.3. Generation examples,[0],[0]
"For these examples, no note was set by hand and we asked DeepBach to generate regions longer than one bar and covering all four voices.
",4.3. Generation examples,[0],[0]
"Despite some compositional errors like parallel octaves, the musical analysis reveals that the DeepBach compositions reproduce typical Bach-like patterns, from characteristic cadences to the expressive use of nonchord tones.",4.3. Generation examples,[0],[0]
As discussed in Sect.,4.3. Generation examples,[0],[0]
"4.2, DeepBach also learned the correct spelling of the notes.",4.3. Generation examples,[0],[0]
"Among examples in Fig. 8, examples (a) and (b) share the same metadata (S,F and K).",4.3. Generation examples,[0],[0]
"This demonstrates that even with fixed metadata it is possible to generate contrasting chorales.
",4.3. Generation examples,[0],[0]
"Since we aimed at producing music that could not be distinguished from actual Bach compositions, we had all provided extracts sung by the Wishful Singing choir.",4.3. Generation examples,[0],[0]
These audio files can be heard on the accompanying website.,4.3. Generation examples,[0],[0]
"We described DeepBach, a probabilistic model together with a sampling method which is flexible, efficient and provides musically convincing results even to the ears of professionals.",5. Discussion and future work,[0],[0]
"The strength of our method is the possibility to let users impose unary constraints, which is a feature often neglected in probabilistic models of music.",5. Discussion and future work,[0],[0]
"Through our graphical interface, the composition of polyphonic music becomes accessible to non-specialists.",5. Discussion and future work,[0],[0]
The playful interaction between the user and this system can boost creativity and help explore new ideas quickly.,5. Discussion and future work,[0],[0]
"We believe that this approach could form a starting point for a novel com-
positional process that could be described as a constructive dialogue between a human operator and the computer.",5. Discussion and future work,[0],[0]
This method is general and its implementation simple.,5. Discussion and future work,[0],[0]
"It is not only applicable to Bach chorales but embraces a wider range of polyphonic music.
",5. Discussion and future work,[0],[0]
"Future work aims at refining our interface, speeding up
generation and handling datasets with small corpora.",5. Discussion and future work,[0],[0]
"This paper introduces DeepBach, a graphical model aimed at modeling polyphonic music and specifically hymn-like pieces.",abstractText,[0],[0]
"We claim that, after being trained on the chorale harmonizations by Johann Sebastian Bach, our model is capable of generating highly convincing chorales in the style of Bach.",abstractText,[0],[0]
DeepBach’s strength comes from the use of pseudo-Gibbs sampling coupled with an adapted representation of musical data.,abstractText,[0],[0]
This is in contrast with many automatic music composition approaches which tend to compose music sequentially.,abstractText,[0],[0]
"Our model is also steerable in the sense that a user can constrain the generation by imposing positional constraints such as notes, rhythms or cadences in the generated score.",abstractText,[0],[0]
We also provide a plugin on top of the MuseScore music editor making the interaction with DeepBach easy to use.,abstractText,[0],[0]
DeepBach: a Steerable Model for Bach Chorales Generation ,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1125–1135 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
User comments play a central role in social media and online discussion fora.,1 Introduction,[0],[0]
"News portals and blogs often also allow their readers to comment to get feedback, engage their readers, and build customer loyalty.1 User comments, however, and more generally user content can also be abusive (e.g., bullying, profanity, hate speech) (Cheng et al., 2015).",1 Introduction,[0],[0]
"Social media are under pressure to combat abusive content, but so far rely mostly on user reports and tools that detect frequent words and phrases of reported posts.2 Wulczyn et al. (2017) estimated that only 17.9% of personal attacks in Wikipedia discussions were followed by moderator actions.",1 Introduction,[0],[0]
"News portals also
1 See, for example, http://niemanreports.org/ articles/the-future-of-comments/.
2 Consult, for example, https://www.facebook.",1 Introduction,[0],[0]
com/help/131671940241729 and https://www.,1 Introduction,[0],[0]
"theguardian.com/technology/2017/feb/07/ twitter-abuse-harassment-crackdown.
suffer from abusive user comments, which damage their reputations and make them liable to fines, e.g., when hosting comments encouraging illegal actions.",1 Introduction,[0],[0]
"They often employ moderators, who are frequently overwhelmed, however, by the volume and abusiveness of comments.3 Readers are disappointed when non-abusive comments do not appear quickly online because of moderation delays.",1 Introduction,[0],[0]
"Smaller news portals may be unable to employ moderators, and some are forced to shut down their comments sections entirely.
",1 Introduction,[0],[0]
"We examine how deep learning (Goodfellow et al., 2016; Goldberg, 2016, 2017) can be employed to moderate user comments.",1 Introduction,[0],[0]
We experiment with a new dataset of approx.,1 Introduction,[0],[0]
"1.6M manually moderated (accepted or rejected) user comments from a Greek sports news portal (called Gazzetta), which we make publicly available.4 This is one of the largest publicly available datasets of moderated user comments.",1 Introduction,[0],[0]
We also provide word embeddings pre-trained on 5.2M comments from the same portal.,1 Introduction,[0],[0]
"Furthermore, we experiment on the ‘attacks’ dataset of Wulczyn et al. (2017), approx.",1 Introduction,[0],[0]
"115K English Wikipedia talk page comments labeled as containing personal attacks or not.
",1 Introduction,[0],[0]
"In a fully automatic scenario, there is no moderator and a system accepts or rejects comments.",1 Introduction,[0],[0]
"Although this scenario may be the only available one, e.g., when news portals cannot afford moderators, it is unrealistic to expect that fully automatic moderation will be perfect, because abusive comments may involve irony, sarcasm, harassment without profane phrases etc., which are particularly difficult for a machine to detect.",1 Introduction,[0],[0]
"When moderators are available, it is more realistic to develop semi-
3See, e.g., https://www.wired.com/2017/04/ zerochaos-google-ads-quality-raters and https://goo.gl/89M2bI.
4The portal is http://www.gazzetta.gr/. Instructions to download the dataset will become available at http://nlp.cs.aueb.gr/software.html.
1125
automatic systems aiming to assist, rather than replace the moderators, a scenario that has not been considered in previous work.",1 Introduction,[0],[0]
"In this case, comments for which the system is uncertain (Fig. 1) are shown to a moderator to decide; all other comments are accepted or rejected by the system.",1 Introduction,[0],[0]
"We discuss how moderation systems can be tuned, depending on the availability and workload of the moderators.",1 Introduction,[0],[0]
"We also introduce additional evaluation measures for the semi-automatic scenario.
",1 Introduction,[0],[0]
"On both datasets (Gazzetta and Wikipedia comments) and for both scenarios (automatic, semiautomatic), we show that a recurrent neural network (RNN) outperforms the system of Wulczyn et al. (2017), the previous state of the art for comment moderation, which employed logistic regression or a multi-layer Perceptron (MLP), and represented each comment as a bag of (character or word) n",1 Introduction,[0],[0]
-grams.,1 Introduction,[0],[0]
We also propose an attention mechanism that improves the overall performance of the RNN.,1 Introduction,[0],[0]
"Our attention mechanism differs from most previous ones (Bahdanau et al., 2015; Luong et al., 2015) in that it is used in a classification setting, where there is no previously generated output subsequence to drive the attention, unlike sequence-to-sequence models (Sutskever et al., 2014).",1 Introduction,[0],[0]
"In that sense, our attention is similar to that of of Yang et al. (2016), but our attention mechanism is a deeper MLP and it is only applied to words, whereas Yang et al. also have a second attention mechanism that assigns attention scores to entire sentences.",1 Introduction,[0],[0]
"In effect, our attention detects the words of a comment that affect most the classification decision (accept, reject), by examining them in the context of the particular comment.
",1 Introduction,[0],[0]
"Although our attention mechanism does not always improve the performance of the RNN, it has the additional advantage of allowing the RNN to highlight suspicious words that a moderator could consider to decide more quickly if a comment should be accepted or rejected.",1 Introduction,[0],[0]
"The highlighting
comes for free, i.e., the training data do not contain highlighted words.",1 Introduction,[0],[0]
"We also show that words highlighted by the attention mechanism correlate well with words that moderators would highlight.
",1 Introduction,[0],[0]
Our main contributions are: (i),1 Introduction,[0],[0]
We release a dataset of 1.6M moderated user comments.,1 Introduction,[0],[0]
"(ii) We introduce a novel, deep, classification-specific attention mechanism and we show that an RNN with our attention mechanism outperforms the previous state of the art in user comment moderation.",1 Introduction,[0],[0]
"(iii) Unlike previous work, we also consider a semiautomatic scenario, along with threshold tuning and evaluation measures for it.",1 Introduction,[0],[0]
"(iv) We show that the attention mechanism can automatically highlight suspicious words for free, without manually highlighting words in the training data.",1 Introduction,[0],[0]
"We first discuss the datasets we used, to help acquaint the reader with the problem.",2 Datasets,[0],[0]
There are approx.,2.1 Gazzetta comments,[0],[0]
"1.45M training comments (covering Jan. 1, 2015 to Oct. 6, 2016) in the Gazzetta dataset; we call them G-TRAIN-L (Table 1).",2.1 Gazzetta comments,[0],[0]
"Some experiments use only the first 100K comments of G-TRAIN-L, called G-TRAIN-S. An additional set of 60,900 comments (Oct. 7 to Nov. 11, 2016) was split to development (G-DEV, 29,700 comments), large test (G-TEST-L, 29,700), and small test set (G-TEST-S, 1,500).",2.1 Gazzetta comments,[0],[0]
"Gazzetta’s moderators (2 full-time, plus journalists occasionally helping) are occasionally instructed to be stricter (e.g., during violent events).",2.1 Gazzetta comments,[0],[0]
"To get a more accurate view of performance in normal situtations, we manually re-moderated (labeled as ‘accept’ or ‘reject’)",2.1 Gazzetta comments,[0],[0]
"the comments of G-TEST-S, producing G-TEST-SR.",2.1 Gazzetta comments,[0],[0]
The reject ratio is approx.,2.1 Gazzetta comments,[0],[0]
"30% in all subsets, except for G-TEST-S-R where it drops to 22%, because there are no occasions where the moderators were instructed to be stricter in G-TEST-S-R.
Each G-TEST-S-R comment was re-moderated by five annotators.",2.1 Gazzetta comments,[0],[0]
"Krippendorff’s (2004) alpha was 0.4762, close to the value (0.45) reported by Wulczyn et al. (2017) for the Wikipedia ‘attacks’ dataset.",2.1 Gazzetta comments,[0],[0]
"Using Cohen’s Kappa (Cohen, 1960), the mean pairwise agreement was 0.4749.",2.1 Gazzetta comments,[0],[0]
The mean pairwise percentage of agreement (% of comments each pair of annotators agreed on) was 81.33%.,2.1 Gazzetta comments,[0],[0]
"Cohen’s Kappa and Krippendorff’s alpha lead to lower scores, because they account for agreement by chance, which is high when there is class imbalance (22% reject, 78% accept in G-TEST-S-R).
",2.1 Gazzetta comments,[0],[0]
"During the re-moderation of G-TEST-S-R, the annotators were also asked to highlight snippets they considered suspicious, i.e., words or phrases that could lead a moderator to consider rejecting each comment.5",2.1 Gazzetta comments,[0],[0]
"We also asked the annotators to classify each snippet into one of the following categories: calumniation (e.g., false accusations), discrimination (e.g., racism), disrespect (e.g., looking down at a profession), hooliganism (e.g., calling for violence), insult (e.g., making fun of appearance), irony, swearing, threat, other.",2.1 Gazzetta comments,[0],[0]
"Figure 2 shows how many comments of G-TEST-S-R contained at least one snippet of each category, according to the majority of annotators; e.g., a comment counts as containing irony if at least 3 annotators annotated it with an irony snippet (not necessarily the same).",2.1 Gazzetta comments,[0],[0]
The gold class of each comment (accept or reject) is determined by the majority of the annotators.,2.1 Gazzetta comments,[0],[0]
"Irony and disrespect are particularly frequent in both classes, followed by calumniation, swearing, hooliganism, insults.",2.1 Gazzetta comments,[0],[0]
"Notice that comments that contain irony, disrespect etc. are not necessarily rejected.",2.1 Gazzetta comments,[0],[0]
"They are, however, more likely in the rejected class, considering that the accepted comments are 2.5 times more
5Treating snippet overlaps as agreements, the mean pairwise Dice coefficient for snippet highlighting was 50.03%.
",2.1 Gazzetta comments,[0],[0]
than the rejected ones (78% vs. 22%).,2.1 Gazzetta comments,[0],[0]
"We also provide 300-dimensional word embeddings, pre-trained on approx.",2.1 Gazzetta comments,[0],[0]
"5.2M comments (268M tokens) from Gazzetta using WORD2VEC (Mikolov et al., 2013a,b).6 This larger dataset cannot be used to directly train classifiers, because most of its comments are from a period (before 2015) when Gazzetta did not employ moderators.",2.1 Gazzetta comments,[0],[0]
"The Wikipedia ‘attacks’ dataset (Wulczyn et al., 2017) contains approx.",2.2 Wikipedia comments,[0],[0]
"115K English Wikipedia talk page comments, which were labeled as containing personal attacks or not.",2.2 Wikipedia comments,[0],[0]
Each comment was labeled by at least 10 annotators.,2.2 Wikipedia comments,[0],[0]
"Inter-annotator agreement, measured on a random sample of 1K comments using Krippendorff’s (2004) alpha, was 0.45.",2.2 Wikipedia comments,[0],[0]
"The gold label of each comment is determined by the majority of annotators, leading to binary labels (accept, reject).",2.2 Wikipedia comments,[0],[0]
"Alternatively, the gold label is the percentage of annotators that labeled the comment as ‘accept’ (or ‘reject’), leading to probabilistic labels.7",2.2 Wikipedia comments,[0],[0]
"The dataset is split in three parts (Table 1): training (W-ATT-TRAIN, 69,526 comments), development (W-ATT-DEV, 23,160), and test (W-ATT-TEST, 23,178).",2.2 Wikipedia comments,[0],[0]
"In all three parts, the rejected comments are 12%, but this is an artificial ratio (Wulczyn et al. oversampled comments posted by banned users).",2.2 Wikipedia comments,[0],[0]
"By contrast, the ratio of rejected comments in all the Gazzetta subsets is the truly observed one.",2.2 Wikipedia comments,[0],[0]
"The Wikipedia comments are also longer (median length 38 tokens) compared to Gazzetta’s (median length 25 tokens).
",2.2 Wikipedia comments,[0],[0]
"Wulczyn et al. (2017) also provide two additional datasets of English Wikipedia talk page comments, which are not used in this paper.",2.2 Wikipedia comments,[0],[0]
"The first one, called ‘aggression’ dataset, contains the same comments as the ‘attacks’ dataset, now labeled as ‘aggressive’ or not.",2.2 Wikipedia comments,[0],[0]
"The (probabilistic) labels of the ‘attacks’ and ‘aggression’ datasets are very highly correlated (0.8992 Spearman, 0.9718 Pearson) and we did not consider the aggression dataset any further.",2.2 Wikipedia comments,[0],[0]
"The second additional dataset, called ‘toxicity’ dataset, contains approx.",2.2 Wikipedia comments,[0],[0]
160K comments labeled as being toxic or not.,2.2 Wikipedia comments,[0],[0]
"Experiments we reported elsewhere (Pavlopoulos et al., 2017) show that results on the ‘attacks’ and ‘toxicity’ datasets are very similar; we do not include
6We used CBOW, window size 5, min. term freq.",2.2 Wikipedia comments,[0],[0]
"5, negative sampling, obtaining a vocabulary size of approx.",2.2 Wikipedia comments,[0],[0]
"478K.
7 We also construct probabilistic labels for G-TEST-S-R, where there are five annotators.
results on the latter in this paper to save space.",2.2 Wikipedia comments,[0],[0]
"We experimented with an RNN operating on word embeddings, the same RNN enhanced with our attention mechanism (a-RNN), a vanilla convolutional neural network (CNN) also operating on word embeddings, the DETOX system of Wulczyn et al. (2017), and a baseline that uses word lists.",3 Methods,[0],[0]
"DETOX (Wulczyn et al., 2017) was the previous state of the art in comment moderation, in the sense that it had the best reported results on the Wikipedia datasets (Section 2.2), which were in turn the largest previous publicly available dataset of moderated user comments.8 DETOX represents each comment as a bag of word n-grams (n ≤ 2, each comment becomes a bag containing its 1- grams and 2-grams) or a bag of character n-grams (n ≤ 5, each comment becomes a bag containing character 1-grams, . . .",3.1 DETOX,[0],[0]
", 5-grams).",3.1 DETOX,[0],[0]
"DETOX can rely on a logistic regression (LR) or MLP classifier, and it can use binary or probabilistic gold labels (Section 2.2) during training.
",3.1 DETOX,[0],[0]
"We used the DETOX implementation provided by Wulczyn et al. and the same grid search (and code) to tune the hyper-parameters of DETOX that select word or character n-grams, classifier (LR or MLP), and gold labels (binary or probabilistic).",3.1 DETOX,[0],[0]
"For Gazzetta, only binary gold labels were possible, since G-TRAIN-L and G-TRAIN-S have a single gold label per comment.",3.1 DETOX,[0],[0]
"Unlike Wulczyn et al., we tuned the hyper-parameters by evaluating (computing AUC and Spearman, Section 4) on a random 2% of held-out comments of W-ATTTRAIN or G-TRAIN-S, instead of the development subsets, to be able to obtain more realistic results from the development sets while developing the methods.",3.1 DETOX,[0],[0]
"For both Wikipedia and Gazzetta, the tuning selected character n-grams, as in the work of Wulczyn et al.",3.1 DETOX,[0],[0]
"Also, for both Wikipedia and Gazzetta, it preferred LR to MLP, whereas Wulczyn et al. reported slightly higher performance
8Two of the co-authors of Wulczyn et al. (2017) are with Jigsaw, who recently announced Perspective, a system to detect ‘toxic’ comments.",3.1 DETOX,[0],[0]
"Perspective is not the same as DETOX (personal communication), but we were unable to obtain scientific articles describing it.",3.1 DETOX,[0],[0]
An API for Perspective is available at https://www.perspectiveapi.,3.1 DETOX,[0],[0]
"com/, but we did not have access to the API at the time the experiments of this paper were carried out.
for the MLP on W-ATT-DEV.9",3.1 DETOX,[0],[0]
"The tuning also selected probabilistic labels for Wikipedia, as in the work of Wulczyn et al.",3.1 DETOX,[0],[0]
RNN:,3.2 RNN-based methods,[0],[0]
"The RNN method is a chain of GRU cells (Cho et al., 2014) that transforms the tokens w1 . .",3.2 RNN-based methods,[0],[0]
.,3.2 RNN-based methods,[0],[0]
", wk of each comment to the hidden states h1 . . .",3.2 RNN-based methods,[0],[0]
", hk, followed by an LR layer that uses hk to classify the comment (accept, reject).",3.2 RNN-based methods,[0],[0]
"Formally, given the vocabulary V , a matrixE ∈ Rd×|V | containing d-dimensional word embeddings, an initial h0, and a comment c = 〈w1, . . .",3.2 RNN-based methods,[0],[0]
", wk〉, the RNN computes h1, . . .",3.2 RNN-based methods,[0],[0]
", hk as follows (ht ∈ Rm):
h̃t = tanh(Whxt + Uh(rt ht−1) + bh) ht = (1− zt) ht−1",3.2 RNN-based methods,[0],[0]
+ zt h̃t zt = σ(Wzxt + Uzht−1 + bz) rt = σ(Wrxt + Urht−1,3.2 RNN-based methods,[0],[0]
"+ br)
where h̃t ∈ Rm is the proposed hidden state at position t, obtained by considering the word embedding xt of token wt and the previous hidden state ht−1; denotes element-wise multiplication; rt ∈ Rm is the reset gate (for rt all zeros, it allows the RNN to forget the previous state ht−1); zt ∈ Rm is the update gate (for zt all zeros, it allows the RNN to ignore the new proposed h̃t, hence also xt, and copy ht−1 as ht); σ is the sigmoid function; Wh,Wz,Wr ∈ Rm×d; Uh, Uz, Ur ∈ Rm×m; bh, bz, br ∈ Rm.",3.2 RNN-based methods,[0],[0]
"Once hk has been computed, the LR layer estimates the probability that comment c should be rejected, with Wp ∈ R1×m, bp ∈ R:
PRNN(reject|c) = σ(Wphk + bp)
a-RNN: When the attention mechanism is added, the LR layer considers the weighted sum hsum of all the hidden states, instead of",3.2 RNN-based methods,[0],[0]
just hk (Fig.,3.2 RNN-based methods,[0],[0]
"3):10
hsum = k∑ t=1 atht (1)
Pa−RNN(reject|c) = σ(Wphsum + bp)
",3.2 RNN-based methods,[0],[0]
"The weights at are produced by an attention mech-
9We repeated the tuning by evaluating on W-ATT-DEV, and again character n-grams with LR were selected.
",3.2 RNN-based methods,[0],[0]
"10We tried replacing the LR layer by a deeper classification MLP, and the RNN chain by a bidirectional RNN (Schuster and Paliwal, 1997), but there were no improvements.
",3.2 RNN-based methods,[0],[0]
"anism, which is an MLP with l layers:
a (1) t = RELU(W (1)ht + b(1)) (2) . .",3.2 RNN-based methods,[0],[0]
".
",3.2 RNN-based methods,[0],[0]
a (l−1) t = RELU(W (l−1)a(l−2)t + b,3.2 RNN-based methods,[0],[0]
"(l−1))
",3.2 RNN-based methods,[0],[0]
a (l) t = W (l)a (l−1) t + b,3.2 RNN-based methods,[0],[0]
"(l)
at = softmax(a (l) t ; a (l) 1 , . . .",3.2 RNN-based methods,[0],[0]
", a (l) k ) (3)
where a(1)t , . . .",3.2 RNN-based methods,[0],[0]
", a (l−1) t ∈",3.2 RNN-based methods,[0],[0]
"Rr, a(l)t , at ∈ R, W (1) ∈",3.2 RNN-based methods,[0],[0]
"Rr×m, W (2), . . .",3.2 RNN-based methods,[0],[0]
",W (l−1) ∈",3.2 RNN-based methods,[0],[0]
"Rr×r, W (l) ∈",3.2 RNN-based methods,[0],[0]
"R1×r, b(1), . . .",3.2 RNN-based methods,[0],[0]
", b(l−1) ∈",3.2 RNN-based methods,[0],[0]
"Rr, b(l) ∈",3.2 RNN-based methods,[0],[0]
R.,3.2 RNN-based methods,[0],[0]
"The softmax operates across the a(l)t (t = 1, . . .",3.2 RNN-based methods,[0],[0]
", k), making the weights at sum to 1.",3.2 RNN-based methods,[0],[0]
"Our attention mechanism differs from most previous ones (Mnih et al., 2014; Bahdanau et al., 2015; Xu et al., 2015; Luong et al., 2015) in that it is used in a classification setting, where there is no previously generated output subsequence (e.g., partly generated translation) to drive the attention (e.g., assign more weight to source words to translate next), unlike seq2seq models (Sutskever et al., 2014).",3.2 RNN-based methods,[0],[0]
"It assigns larger weights at to hidden states ht corresponding to positions where there is more evidence that the comment should be accepted or rejected.
",3.2 RNN-based methods,[0],[0]
"Yang et al. (2016) use a similar attention mechanism, but ours is deeper.",3.2 RNN-based methods,[0],[0]
"In effect they always set l = 2, whereas we allow l to be larger (tuning selects l = 4).11",3.2 RNN-based methods,[0],[0]
"On the other hand, the attention mechanism of Yang et al. is part of a classification method for longer texts (e.g., product reviews).",3.2 RNN-based methods,[0],[0]
"Their method uses two GRU RNNs, both bidirectional (Schuster and Paliwal, 1997), one turning the word embeddings of each sentence to a sentence embedding, and one turning the sentence embeddings to a document embedding, which is then fed to an LR layer.",3.2 RNN-based methods,[0],[0]
"Yang et al. use their attention mechanism in both RNNs, to assign attention scores to words and sentences.",3.2 RNN-based methods,[0],[0]
"We consider shorter texts (comments), we have a single RNN, and we assign attention scores to words only.12
da-CENT: We also experiment with a variant of a-RNN, called da-CENT, which does not use the hidden states of the RNN.",3.2 RNN-based methods,[0],[0]
"The input to the first layer of the attention mechanism is now directly the embedding xt instead of ht (cf. Eq. 2), and
11Yang et al. use tanh instead of RELU in Eq. 2, which works worse in our case, and no bias b(l) in the l-th layer.
",3.2 RNN-based methods,[0],[0]
"12We tried a bidirectional instead of unidirectional GRU chain in our methods, also replacing the LR layer by a deeper classification MLP, but there were no improvements.
hsum is now the weighted sum (centroid) of word embeddings hsum = ∑k t=1 atxt (cf. Eq. 1).",3.2 RNN-based methods,[0],[0]
"13
We set l = 4, d = 300, r = m = 128, having tuned all hyper-parameters on the same 2% held-out comments of W-ATT-TRAIN or G-TRAINS that were used to tune DETOX.",3.2 RNN-based methods,[0],[0]
"We use Glorot initialization (Glorot and Bengio, 2010), categorical cross-entropy loss, and Adam (Kingma and Ba, 2015).14",3.2 RNN-based methods,[0],[0]
Early stopping evaluates on the same held-out subsets.,3.2 RNN-based methods,[0],[0]
"For Gazzetta, word embeddings are initialized to the WORD2VEC embeddings we provide (Section 2.1).",3.2 RNN-based methods,[0],[0]
"For Wikipedia, they are initialized to GLOVE embeddings (Pennington et al., 2014).15",3.2 RNN-based methods,[0],[0]
"In both cases, the embeddings are updated during backpropagation.",3.2 RNN-based methods,[0],[0]
"Out of vocabulary (OOV) words, meaning words for which we have no initial embeddings, are mapped to a single randomly initialized embedding, also updated.",3.2 RNN-based methods,[0],[0]
We also compare against a vanilla CNN operating on word embeddings.,3.3 CNN,[0],[0]
"We describe the CNN only briefly, because it is very similar to that of of Kim (2014); see also Goldberg (2016) for an introduction to CNNs, and Zhang and Wallace (2015).
",3.3 CNN,[0],[0]
"For Wikipedia comments, we use a ‘narrow’ convolution layer, with kernels sliding (stride 1) over (entire) embeddings of word n-grams of sizes n = 1, . . .",3.3 CNN,[0],[0]
", 4.",3.3 CNN,[0],[0]
"We use 300 kernels for each n value, a total of 1,200 kernels.",3.3 CNN,[0],[0]
"The outputs of each kernel, obtained by applying the kernel to the different n-grams of a comment c, are then
13 For experiments with additional variants of a-RNN, consult Pavlopoulos et al. (2017).
",3.3 CNN,[0],[0]
"14We implemented the methods of this sub-section using Keras (keras.io) and TensorFlow (tensorflow.org).
",3.3 CNN,[0],[0]
"15See https://nlp.stanford.edu/projects/ glove/. We use ‘Common Crawl’ (840B tokens).
",3.3 CNN,[0],[0]
"max-pooled, leading to a single output per kernel.",3.3 CNN,[0],[0]
"The resulting feature vector (1,200 maxpooled outputs) goes through a dropout layer (Hinton et al., 2012) (p = 0.5), and then to an LR layer, which provides PCNN(reject|c).",3.3 CNN,[0],[0]
"For Gazzetta, the CNN is the same, except that n = 1, . . .",3.3 CNN,[0],[0]
", 5, leading to 1,500 features per comment.",3.3 CNN,[0],[0]
All hyperparameters were tuned on the 2% held-out comments of W-ATT-TRAIN or G-TRAIN-S that were used to tune the other methods.,3.3 CNN,[0],[0]
"Again, we use 300-dimensional embeddings, which are now randomly initialized, since tuning indicated this was better than initializing to pre-trained embeddings.",3.3 CNN,[0],[0]
OOV words are treated as in the RNN-based methods.,3.3 CNN,[0],[0]
All embeddings are updated during backpropagation.,3.3 CNN,[0],[0]
Early stopping evaluates on the heldout subsets.,3.3 CNN,[0],[0]
"Again, we use Glorot initialization, categorical cross-entropy loss, and Adam.16",3.3 CNN,[0],[0]
"A baseline, called LIST, collects every word w that occurs in more than 10 (for W-ATT-TRAIN, G-TRAIN-S) or 100 comments (for G-TRAIN-L) in the training set, along with the precision of w, i.e., the ratio of rejected training comments containing w divided by the total number of training comments containing",3.4 LIST baseline,[0],[0]
w.,3.4 LIST baseline,[0],[0]
"The resulting lists contain 10,423, 16,864, and 21,940 word types, when using W-ATT-TRAIN, G-TRAIN-S, G-TRAIN-L, respectively.",3.4 LIST baseline,[0],[0]
"For a comment c, LIST returns as PLIST(reject|c) the maximum precision of all the words in c.",3.4 LIST baseline,[0],[0]
"All methods produce a p = P (reject|c) per comment c. In semi-automatic moderation (Fig. 1), a comment is directly rejected if its p is above a rejection theshold tr, it is directly accepted if p is below an acceptance threshold ta, and it is shown to a moderator if ta ≤ p ≤ tr (gray zone of Fig. 4).
",3.5 Tuning thresholds,[0],[0]
"In our experience, moderators (or their employers) can easily specify the approximate percentage of comments they can afford to check manually (e.g., 20% daily) or, equivalently, the approximate percentage of comments the system should
16We implemented the CNN directly in TensorFlow.
handle automatically.",3.5 Tuning thresholds,[0],[0]
"We call coverage the latter percentage; hence, 1 − coverage is the approximate percentage of comments to be checked manually.",3.5 Tuning thresholds,[0],[0]
"By contrast, moderators are baffled when asked to tune tr and ta directly.",3.5 Tuning thresholds,[0],[0]
"Consequently, we ask them to specify the approximate desired coverage.",3.5 Tuning thresholds,[0],[0]
"We then sort the comments of the development set (G-DEV or W-ATT-DEV) by p, and slide ta from 0.0 to 1.0 (Fig. 4).",3.5 Tuning thresholds,[0],[0]
"For each ta value, we set tr to the value that leaves a 1 − coverage percentage of development comments in the gray zone (ta ≤ p ≤ tr).",3.5 Tuning thresholds,[0],[0]
"We then select the ta (and tr) that maximizes the weighted harmonic mean Fβ(Preject, Paccept) on the development set:
Fβ(Preject, Paccept) =",3.5 Tuning thresholds,[0],[0]
"(1 + β2) · Preject · Paccept β2 · Preject + Paccept
where Preject is the rejection precision (correctly rejected comments divided by rejected comments) and Paccept is the acceptance precision (correctly accepted divided by accepted).",3.5 Tuning thresholds,[0],[0]
"Intuitively, coverage sets the width of the gray zone, whereas Preject and Paccept show how certain we can be that the red (reject) and green (accept) zones are free of misclassified comments.",3.5 Tuning thresholds,[0],[0]
"We set β = 2, emphasizing Paccept, because moderators are more worried about wrongly accepting abusive comments than wrongly rejecting non-abusive ones.17 The selected ta, tr (tuned on development data) are then used in experiments on test data.",3.5 Tuning thresholds,[0],[0]
"In fully automatic moderation, coverage = 100 and ta = tr; otherwise, threshold tuning is identical.",3.5 Tuning thresholds,[0],[0]
"Following Wulczyn et al. (2017), we report in Table 2 AUC scores (area under ROC curve), along with Spearman correlations between systemgenerated probabilities P (accept|c) and human probabilistic gold labels (Section 2.2) when probabilistic gold labels are available.18",4.1 Comment classification evaluation,[0],[0]
"Wulczyn et al. reported DETOX results only on W-ATT-DEV, shown in brackets.",4.1 Comment classification evaluation,[0],[0]
"Table 2 shows that RNN is
17More precisely, when computing Fβ , we reorder the development comments by time posted, and split them into batches of 100.",4.1 Comment classification evaluation,[0],[0]
"For each ta (and tr) value, we compute Fβ per batch and macro-average across batches.",4.1 Comment classification evaluation,[0],[0]
"The resulting thresholds lead to Fβ scores that are more stable over time.
",4.1 Comment classification evaluation,[0],[0]
"18When computing AUC, the gold label is the majority label of the annotators.",4.1 Comment classification evaluation,[0],[0]
"When computing Spearman, the gold label is probabilistic (% of annotators that accepted the comment).",4.1 Comment classification evaluation,[0],[0]
"The decisions of the systems are always probabilistic.
always better than CNN and DETOX; there is no clear winner between CNN and DETOX.",4.1 Comment classification evaluation,[0],[0]
"Furthermore, a-RNN is always better than RNN on Gazzetta comments, but not on Wikipedia comments, where RNN is overall slightly better according to Table 2.",4.1 Comment classification evaluation,[0],[0]
"Also, da-CENT is always worse than a-RNN and RNN, confirming that the hidden states (intuitively, context-aware word embeddings) of the RNN chain are important, even with the attention mechanism.",4.1 Comment classification evaluation,[0],[0]
Increasing the size of the Gazzetta training set (G-TRAIN-S to G-TRAINL) significantly improves the performance of all methods.,4.1 Comment classification evaluation,[0],[0]
"The implementation of DETOX could not handle the size of G-TRAIN-L, which is why we do not report DETOX results for G-TRAIN-L. Notice, also, that the Wikipedia dataset is easier than the Gazzetta one (all methods perform better on Wikipedia comments, compared to Gazzetta).
",4.1 Comment classification evaluation,[0],[0]
"Figure 5 shows F2(Preject, Paccept) on G-TESTL and W-ATT-TEST, when ta, tr are tuned on GDEV, W-ATT-DEV for varying coverage.",4.1 Comment classification evaluation,[0],[0]
"For GTEST-L, we show results training on G-TRAIN-S (solid lines) and G-TRAIN-L (dotted).",4.1 Comment classification evaluation,[0],[0]
"The differ-
ences between RNN and a-RNN are again small, but it is now easier to see that a-RNN is overall better.",4.1 Comment classification evaluation,[0],[0]
"Again, a-RNN and RNN are better than CNN and DETOX.",4.1 Comment classification evaluation,[0],[0]
All three deep learning methods benefit from the larger training set (dotted).,4.1 Comment classification evaluation,[0],[0]
"In Wikipedia, a-RNN obtains Paccept, Preject ≥ 0.94 for all coverages (Fig. 5, call-outs).",4.1 Comment classification evaluation,[0],[0]
"On the more difficult Gazzetta dataset, a-RNN still obtains Paccept, Preject ≥ 0.85 when tuned for 50% coverage.",4.1 Comment classification evaluation,[0],[0]
"When tuned for 100% coverage, comments for which the system is uncertain (gray zone) cannot be avoided and there are inevitably more misclassifications; the use of F2 during threshold tuning places more emphasis on avoiding wrongly accepted comments, leading to high Paccept (0.82), at the expense of wrongly rejected comments, i.e., sacrificing Preject (0.59).",4.1 Comment classification evaluation,[0],[0]
"On the re-moderated G-TEST-S-R (similar diagrams, not shown), Paccept, Preject become 0.96, 0.88 for coverage 50%, and 0.92, 0.48 for coverage 100%.
",4.1 Comment classification evaluation,[0],[0]
"We also repeated the annotator ensemble experiment of Wulczyn et al. (2017) on 8K randomly chosen comments of W-ATT-TEST (4K comments
from random users, 4K comments from banned users).19 The decisions of 10 randomly chosen annotators (possibly different per comment) were used to construct the gold label of each comment.",4.1 Comment classification evaluation,[0],[0]
"The gold labels were then compared to the decisions of the systems and the decisions of an ensemble of k other annotators, k ranging from 1 to 10.",4.1 Comment classification evaluation,[0],[0]
"Table 3 shows the mean AUC and Spearman scores, averaged over 25 runs of the experiment, along with standard errrors (in brackets).",4.1 Comment classification evaluation,[0],[0]
"We conclude that RNN and a-RNN are as good as an ensemble of 7 human annotators; CNN is as good as 4 annotators; DETOX is as good as 4 in AUC and 3 annotators in Spearman correlation, which is consistent with the results of Wulczyn et al. (2017).",4.1 Comment classification evaluation,[0],[0]
"To investigate if the attention scores of a-RNN can highlight suspicious words, we focused on GTEST-S-R, the only dataset with suspicious snippets annotated by humans.",4.2 Snippet highlighting evaluation,[0],[0]
"We removed comments with no human-annotated snippets, leaving 841 comments (515 accepted, 326 rejected), a total of 40,572 tokens, of which 13,146 were inside a suspicious snippet of at least one annotator.",4.2 Snippet highlighting evaluation,[0],[0]
"In each remaining comment, each token was assigned a gold suspiciousness score, defined as the percentage of annotators that included it in their snippets.
",4.2 Snippet highlighting evaluation,[0],[0]
We evaluated three methods that score each token wt of a comment c for suspiciousness.,4.2 Snippet highlighting evaluation,[0],[0]
"The first one assigns to each wt the attention score at
19We used the protocol, code, and data of Wulczyn et al.
",4.2 Snippet highlighting evaluation,[0],[0]
(Eq. 3) of a-RNN (trained on G-TRAIN-L).,4.2 Snippet highlighting evaluation,[0],[0]
"The second method assigns to each wt its precision, as computed by LIST (Section 3.4).",4.2 Snippet highlighting evaluation,[0],[0]
The third method (RAND) assigns to each wt a random (uniform distribution) score between 0 and 1.,4.2 Snippet highlighting evaluation,[0],[0]
"In the latter two methods, a softmax is applied to the scores of all the tokens per comment, as in a-RNN.",4.2 Snippet highlighting evaluation,[0],[0]
"Figure 6 shows three comments (from W-ATT-TEST) highlighted by a-RNN; heat corresponds to attention.20
We computed Pearson and Spearman correlations between the gold suspiciousness scores and the scores of the three methods on the 40,572 tokens.",4.2 Snippet highlighting evaluation,[0],[0]
Figure 7 shows the correlations on comments that were accepted (left) and rejected (right) by the majority of moderators.,4.2 Snippet highlighting evaluation,[0],[0]
"In both cases, a-RNN performs better than LIST and RAND by both Pearson and Spearman correlations.",4.2 Snippet highlighting evaluation,[0],[0]
The high Pearson correlations of a-RNN also show that its attention scores are to a large extent linearly related to the gold ones.,4.2 Snippet highlighting evaluation,[0],[0]
"By contrast, LIST performs reasonably well in terms of Spearman correlation, but much worse in terms of Pearson, indicating that its precision scores rank reasonably well the tokens from most to least suspicious ones, but are not linearly related to the gold scores.",4.2 Snippet highlighting evaluation,[0],[0]
"Djuric et al. (2015) experimented with 952K manually moderated comments from Yahoo Finance, but their dataset is not publicly available.",5 Related work,[0],[0]
"They convert each comment to a comment embedding using DOC2VEC (Le and Mikolov, 2014), which is then fed to an LR classifier.",5 Related work,[0],[0]
Nobata et al. (2016) experimented with approx.,5 Related work,[0],[0]
3.3M manually moderated comments from Yahoo Finance and News,5 Related work,[0],[0]
; their data are also not available.21,5 Related work,[0],[0]
"They used Vowpal Wabbit22 with character n-grams (n = 3, . . .",5 Related work,[0],[0]
", 5) and word n-grams (n = 1, 2), handcrafted features (e.g., number of capitalized or black-listed words), features based on dependency
20In innocent comments, a-RNN spreads its attention to all tokens, leading to quasi-uniform low color intensity.
21According to Nobata et al., their clean test dataset (2K comments) would be made available, but it is currently not.
22See http://hunch.net/˜vw/.
trees, averages of WORD2VEC embeddings, and DOC2VEC-like embeddings.",5 Related work,[0],[0]
Character n,5 Related work,[0],[0]
"-grams were the best, on their own outperforming Djuric et al. (2015).",5 Related work,[0],[0]
"The best results, however, were obtained using all features.",5 Related work,[0],[0]
"We use no hand-crafted features and parsers, making our methods more easily portable to other domains and languages.
",5 Related work,[0],[0]
"Mehdad et al. (2016) train a (token or characterbased) RNN language model per class (accept, reject), and use the probability ratio of the two models to accept or reject user comments.",5 Related work,[0],[0]
"Experiments on the dataset of Djuric et al. (2015), however, showed that their method (RNNLMs) performed worse than a combination of SVM and Naive Bayes classifiers (NBSVM) that used character and token n-grams.",5 Related work,[0],[0]
"An LR classifier operating on DOC2VEC-like comment embeddings (Le and Mikolov, 2014) also performed worse than NBSVM.",5 Related work,[0],[0]
"To surpass NBSVM, Mehdad et al. used an SVM to combine features from their three other methods (RNNLMs, LR with DOC2VEC, NBSVM).
",5 Related work,[0],[0]
Wulczyn et al. (2017) experimented with character and word n-grams.,5 Related work,[0],[0]
We included their dataset and moderation system (DETOX) in our experiments.,5 Related work,[0],[0]
Waseem et al. (2016) used approx.,5 Related work,[0],[0]
17K tweets annotated for hate speech.,5 Related work,[0],[0]
"Their best results were obtained using an LR classifier with character n-grams (n = 1, . . .",5 Related work,[0],[0]
", 4), plus gender.",5 Related work,[0],[0]
"Warner and Hirschberg (2012) aimed to detect anti-semitic speech, experimenting with 9K paragraphs and a linear SVM.",5 Related work,[0],[0]
"Their features consider windows of at most 5 tokens, examining the tokens of each window, their order, POS tags, Brown clusters etc., following Yarowsky (1994).
",5 Related work,[0],[0]
Cheng et al. (2015) aimed to predict which users would be banned from on-line communities.,5 Related work,[0],[0]
"Their best system used a random forest or LR classifier, with features examining readability, activity (e.g., number of posts daily), community and moderator reactions (e.g., up-votes, number of deleted posts).
",5 Related work,[0],[0]
"Sood et al. (2012a; 2012b) experimented with 6.5K comments from Yahoo Buzz, moderated via crowdsourcing.",5 Related work,[0],[0]
"They showed that a linear SVM, representing each comment as a bag of word bigrams and stems, performs better than word lists.",5 Related work,[0],[0]
"Their best results were obtained by combining the SVM with a word list and edit distance.
",5 Related work,[0],[0]
Yin et al. (2009) used posts from chat rooms and discussion fora (<15K posts in total) to train an SVM to detect online harassment.,5 Related work,[0],[0]
"They used TF-IDF, sentiment, and context features (e.g., sim-
ilarity to other posts in a thread).",5 Related work,[0],[0]
"Our methods might also benefit by considering threads, rather than individual comments.",5 Related work,[0],[0]
"Yin at al. point out that unlike other abusive content, spam in comments or dicsussion fora (Mishne et al., 2005; Niu et al., 2007) is off-topic and serves a commercial purpose.",5 Related work,[0],[0]
"Spam is unlikely in Wikipedia discussions and not an issue in the Gazzetta dataset (Fig. 2).
",5 Related work,[0],[0]
"For a more extensive discussion of related work, consult Pavlopoulos et al. (2017).",5 Related work,[0],[0]
We experimented with a new publicly available dataset of 1.6M moderated user comments from a Greek sports news portal and an existing dataset of 115K English Wikipedia talk page comments.,6 Conclusions,[0],[0]
"We showed that a GRU RNN operating on word embeddings outpeforms the previous state of the art, which used an LR or MLP classifier with character or word n-gram features, also outperforming a vanilla CNN operating on word embeddings, and a baseline that uses an automatically constructed word list with precision scores.",6 Conclusions,[0],[0]
"A novel, deep, classification-specific attention mechanism improves further the overall results of the RNN, and can also highlight suspicious words for free, without including highlighted words in the training data.",6 Conclusions,[0],[0]
"We considered both fully automatic and semi-automatic moderation, along with threshold tuning and evaluation measures for both.
",6 Conclusions,[0],[0]
"We plan to consider user-specific information (e.g., ratio of comments rejected in the past) (Cheng et al., 2015; Waseem and Hovy, 2016) and explore character-level RNNs or CNNs (Zhang et al., 2015), e.g., as a first layer to produce embeddings of unknown words from characters (dos Santos and Zadrozny, 2014; Ling et al., 2015), which would then be passed on to our current methods that operate on word embeddings.",6 Conclusions,[0],[0]
"This work was funded by Google’s Digital News Initiative (project ML2P, contract 362826).23 We are grateful to Gazzetta for the data they provided.",Acknowledgments,[0],[0]
"We also thank Gazzetta’s moderators for their feedback, insights, and advice.
23See https://digitalnewsinitiative.com/.",Acknowledgments,[0],[0]
"Experimenting with a new dataset of 1.6M user comments from a news portal and an existing dataset of 115K Wikipedia talk page comments, we show that an RNN operating on word embeddings outpeforms the previous state of the art in moderation, which used logistic regression or an MLP classifier with character or word n-grams.",abstractText,[0],[0]
"We also compare against a CNN operating on word embeddings, and a word-list baseline.",abstractText,[0],[0]
"A novel, deep, classificationspecific attention mechanism improves the performance of the RNN further, and can also highlight suspicious words for free, without including highlighted words in the training data.",abstractText,[0],[0]
We consider both fully automatic and semi-automatic moderation.,abstractText,[0],[0]
Deeper Attention to Abusive User Content Moderation,title,[0],[0]
"A fundamental challenge in artificial intelligence, robotics, and language processing is sequential prediction: to reason, plan, and make a sequence of predictions or decisions to minimize accumulated cost, achieve a long-term goal, or
1Robotics Institute, Carnegie Mellon University, USA 2Machine Learning Department, Carnegie Mellon University, USA 3College of Computing, Georgia Institute of Technology, USA.",1. Introduction,[0],[0]
"Correspondence to: Wen Sun <wensun@cs.cmu.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"optimize for a loss acquired only after many predictions.
",1. Introduction,[0],[0]
"Although conventional supervised learning of deep models has been pivotal in advancing performance in sequential prediction problems, researchers are beginning to utilize Reinforcement Learning (RL) methods to achieve even higher performance (Ranzato et al., 2015; Bahdanau et al., 2016; Li et al., 2016).",1. Introduction,[0],[0]
"In sequential prediction tasks, future predictions often depend on the history of previous predictions; thus, a poor prediction early in the sequence can lead to high loss (cost) for future predictions.",1. Introduction,[0],[0]
"Viewing the predictor as a policy ⇡, deep RL algorithms are able to reason about the future accumulated cost in sequential prediction problems.",1. Introduction,[0],[0]
"These approaches have dramatically advanced the state-of-the-art on a number of problems including high-dimensional robotics control tasks and video and board games (Schulman et al., 2015; Silver et al., 2016).
",1. Introduction,[0],[0]
"In contrast with general reinforcement learning methods, imitation learning and related sequential prediction algorithms such as SEARN (Daumé III et al., 2009), DaD (Venkatraman et al., 2015), AggreVaTe (Ross & Bagnell, 2014), and LOLS (Chang et al., 2015b) reduce the sequential prediction problems to supervised learning by leveraging a (near) optimal cost-to-go oracle that can be queried for the next (near)-best prediction at any point during training.",1. Introduction,[0],[0]
"Specifically, these methods assume access to an oracle that provides an optimal or near-optimal action and the future accumulated loss Q⇤, the so-called cost-to-go.",1. Introduction,[0],[0]
"For robotics control problems, this oracle may be a human expert guiding the robot during the training phase (Abbeel & Ng, 2004) or the policy from an optimal MDP solver (Ross et al., 2011; Kahn et al., 2016; Choudhury et al., 2017) that is either too slow to use at test time or leverages information unavailable at test time.",1. Introduction,[0],[0]
"For sequential prediction problems, an oracle can be constructed by optimization (e.g., beam search) or by a clairvoyant greedy algorithm (Daumé III et al., 2009; Ross et al., 2013; Rhinehart et al., 2015; Chang et al., 2015a) that, given the training data’s ground truth, is near-optimal on the task-specific performance metric (e.g., cumulative reward, IoU, Unlabeled Attachment Score, BLEU).
",1. Introduction,[0],[0]
"Expert, demonstrator, and oracle are used interchangeably.
",1. Introduction,[0],[0]
We stress that the oracle is only required to be available during training.,1. Introduction,[0],[0]
"Therefore, the goal of IL is to learn a policy ⇡̂ with the help of the oracle (⇡⇤, Q⇤) during the training session, such that ⇡̂ achieves similar or better performance at test time when the oracle is unavailable.",1. Introduction,[0],[0]
"In contrast to IL, reinforcement learning methods often initialize with a random policy ⇡
0 or cost-to-go estimate Q 0 that may be far from optimal.",1. Introduction,[0],[0]
"The optimal policy (or cost-to-go) must be found by exploring, often with random actions.
",1. Introduction,[0],[0]
A classic family of IL methods is to collect data from running the demonstrator or oracle and train a regressor or classifier via supervised learning.,1. Introduction,[0],[0]
"These methods (Abbeel & Ng, 2004; Syed et al., 2008; Ratliff et al., 2006; Ziebart et al., 2008; Finn et al., 2016; Ho & Ermon, 2016) learn either a policy ⇡̂⇤ or ˆQ⇤ from a fixed-size dataset precollected from the oracle.",1. Introduction,[0],[0]
"Unfortunately, these methods exhibit a pernicious problem: they require the training and test data to be sampled from the same distribution, despite the fact they explicitly change the sample policy during training.",1. Introduction,[0],[0]
"As a result, policies learned by these methods can fail spectacularly (Ross & Bagnell, 2010).",1. Introduction,[0],[0]
"Interactive approaches to IL such as SEARN (Daumé III et al., 2009), DAgger (Ross et al., 2011), and AggreVaTe (Ross & Bagnell, 2014) interleave learning and testing to overcome the data mismatch issue and, as a result, work well in practical applications.",1. Introduction,[0],[0]
"Furthermore, these interactive approaches can provide strong theoretical guarantees between training time loss and test time performance through a reduction to no-regret online learning.
",1. Introduction,[0],[0]
"In this work, we introduce AggreVaTeD, a differentiable version of AggreVaTe (Aggregate Values to Imitate (Ross & Bagnell, 2014)) which allows us to train policies with efficient gradient update procedures.",1. Introduction,[0],[0]
AggreVaTeD extends and scales interactive IL for use in sequential prediction and challenging continuous robot control tasks.,1. Introduction,[0],[0]
"We provide two gradient update procedures: a regular gradient update developed from Online Gradient Descent (OGD) (Zinkevich, 2003) and a natural gradient update (Kakade, 2002; Bagnell & Schneider, 2003), which is closely related to Weighted Majority (WM) (Littlestone & Warmuth, 1994), a popular no-regret algorithm that enjoys an almost dimension-free property (Bubeck et al., 2015).
",1. Introduction,[0],[0]
AggreVaTeD leverages the oracle to learn rich polices that can be represented by complicated non-linear function approximators.,1. Introduction,[0],[0]
"Our experiments with deep neural networks on various robotics control simulators and on a dependency parsing sequential prediction task show that AggreVaTeD can achieve expert-level performance and even super-expert performance when the oracle is sub-optimal, a result rarely achieved by non-interactive IL approaches.
",1. Introduction,[0],[0]
"i.e., the regret bound depends on poly-log of the dimension of parameter space.
",1. Introduction,[0],[0]
"The differentiable nature of AggreVaTeD additionally allows us to employ Recurrent Neural Network policies, e.g., Long Short-Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997), to handle partially observable settings (e.g., observe only partial robot state).",1. Introduction,[0],[0]
"Empirical results demonstrate that by leveraging an oracle, IL can learn much faster than RL.
",1. Introduction,[0],[0]
"In addition to providing a set of practical algorithms, we develop a comprehensive theoretical study of IL on discrete MDPs.",1. Introduction,[0],[0]
We construct an MDP that demonstrates exponentially better sample efficiency for IL than any RL algorithm.,1. Introduction,[0],[0]
"For general discrete MDPs, we provide a regret upper bound for AggreVaTeD with WM, which shows IL can learn dramatically faster than RL.",1. Introduction,[0],[0]
"We provide a regret lower bound for any IL algorithm, which demonstrates that AggreVaTeD with WM is near-optimal.
",1. Introduction,[0],[0]
"To summarize the contributions of this work: (1) AggreVaTeD allows us to handle continuous action spaces and employ recurrent neural network policies for Partially Observable Markov Decision Processes (POMDPs); (2) understanding IL from a perspective that is related to policy gradient allows us to leverage advances from the wellstudied RL policy gradient literature (e.g., gradient variance reduction techniques, efficient natural gradient computation); (3) we provide a new sample complexity study of IL and compare to RL, showing that we can expect up to exponentially lower sample complexity.",1. Introduction,[0],[0]
"Our experimental and theoretical results support the proposition:
Imitation Learning is a more effective strategy than Reinforcement Learning for sequential prediction with near-optimal cost-to-go oracles.",1. Introduction,[0],[0]
"A Markov Decision Process consists of a set of states, actions (that come from a policy), cost (loss), and a model that transitions states given actions.",2. Preliminaries,[0],[0]
"Interestingly, most sequential prediction problems can be framed in terms of MDPs (Daumé III et al., 2009).",2. Preliminaries,[0],[0]
"The actions are the learner’s (e.g., RNN’s) predictions.",2. Preliminaries,[0],[0]
"The state is then the result of all the predictions made so far (e.g., the dependency tree constructed so far or the words translated so far).",2. Preliminaries,[0],[0]
"The cumulative cost is the performance metric such as (negative) UAS, received at the end (horizon) or after the final prediction.",2. Preliminaries,[0],[0]
"For robotics control problems, the robot’s configuration is the state, the controls (e.g., joint torques) are the actions, and the cost is related to achieving a task (e.g., distance walked).
",2. Preliminaries,[0],[0]
"Formally, a finite-horizon Markov Decision Process (MDP) is defined as (S,A, P, C, ⇢
0 , H).",2. Preliminaries,[0],[0]
"Here, S is a set of S states and A is a set of A actions; at time step t, Pt is the transition dynamics such that for any st 2 S, st+1 2 S, at 2 A,
Pt(st+1|st, at) is the probability of transitioning to state st+1 from state st by taking action at at step t; C is the cost distribution such that a cost ct at step t is sampled from Ct(·|st, at).",2. Preliminaries,[0],[0]
"Finally, we denote c̄t(st, at) as the expected cost, ⇢
0",2. Preliminaries,[0],[0]
"as the initial distribution of states,",2. Preliminaries,[0],[0]
and,2. Preliminaries,[0],[0]
"H 2 N+ as the finite horizon (max length) of the MDP.
",2. Preliminaries,[0],[0]
"We define a stochastic policy ⇡ such that for any state s 2 S , ⇡(·|s) 2 (A), where (A) is a A-dimensional simplex, conditioned on state s. ⇡(a|s) 2",2. Preliminaries,[0],[0]
"[0, 1] outputs the probability of taking action a at state s.",2. Preliminaries,[0],[0]
"The distribution of trajectories ⌧ = (s
1 , a 1 , . . .",2. Preliminaries,[0],[0]
", aH 1, sH) is determined by ⇡ and the MDP, and is defined as
⇢⇡(⌧) = ⇢0(s1) HY
t=2
⇡(at 1|st 1)Pt 1(st|st 1, at 1).
",2. Preliminaries,[0],[0]
"The distribution of the states at time step t, induced by running the policy ⇡ until t, is defined 8st:
d⇡t (st) = X
{si,ai}it 1
⇢ 0",2. Preliminaries,[0],[0]
"(s 1 )
t 1Y
i=1
⇡(ai|si)Pi(si+1|si, ai).
",2. Preliminaries,[0],[0]
Note that the summation above can be replaced by an integral if the state or action space is continuous.,2. Preliminaries,[0],[0]
"The average state distribution ¯d⇡(s) = PH t=1 d ⇡ t (s)/H .
",2. Preliminaries,[0],[0]
"The expected average cost of a policy ⇡ can be defined with respect to ⇢⇡ or {d⇡t }:
µ(⇡) = E ⌧⇠⇢⇡
"" HX
t=1
c̄t(st, at) # = HX
t=1
E s⇠d⇡t (s),a⇠⇡(a|s)",2. Preliminaries,[0],[0]
"[c̄t(s, a)] .
We define the state-action value Q⇡t (s, a) (i.e., cost-to-go) for policy ⇡ at time step t as:
Q⇡t (st, at) = c̄t(st, at) + E s⇠Pt(·|st,at),a⇠⇡(·|s) Q⇡t+1(s, a),
where the expectation is taken over the randomness of the policy ⇡ and the MDP.
",2. Preliminaries,[0],[0]
"We define ⇡⇤ as the expert policy (e.g., human demonstrators, search algorithms equipped with ground-truth) and Q⇤t (s, a) as the expert’s cost-to-go oracle.",2. Preliminaries,[0],[0]
"We emphasize that ⇡⇤ may not be optimal, i.e., ⇡⇤ 62 argmin⇡ µ(⇡).",2. Preliminaries,[0],[0]
"Throughout the paper, we assume Q⇤t (s, a) is known or can be estimated without bias (e.g., by rolling out ⇡⇤: starting from state s, applying action a, and then following ⇡⇤ for H t steps).
",2. Preliminaries,[0],[0]
"When ⇡ is represented by a function approximator, we use the notation ⇡✓ to represent the policy parametrized by ✓ 2 Rd: ⇡(·|s; ✓).",2. Preliminaries,[0],[0]
In this work we specifically consider optimizing policies in which the parameter dimension d may be large.,2. Preliminaries,[0],[0]
"We also consider the partially observable setting in our experiments, where the policy ⇡(·|o
1 , a 1 , ..., ot; ✓)
is defined over the whole history of observations and actions (ot is generated from the hidden state st).",2. Preliminaries,[0],[0]
"We use both LSTM and Gated Recurrent Unit (GRU) (Chung et al., 2014) based policies where the RNN’s hidden states provide a compressed feature of the history.",2. Preliminaries,[0],[0]
"To our best knowledge, this is the first time RNNs are employed in an IL framework to handle partially observable environments.",2. Preliminaries,[0],[0]
Policy based imitation learning aims to learn a policy ⇡̂ that approaches the performance of the expert ⇡⇤ at test time when ⇡⇤ is no longer available.,3. Differentiable Imitation Learning,[0],[0]
"In order to learn rich policies such as LSTMs or deep networks (Schulman et al., 2015), we derive a method related to policy gradients for imitation learning and sequential prediction.",3. Differentiable Imitation Learning,[0],[0]
"To do this, we leverage the reduction of IL and sequential prediction to online learning as shown in (Ross & Bagnell, 2014) to learn policies represented by expressive differentiable function approximators.
",3. Differentiable Imitation Learning,[0],[0]
"The fundamental idea in Ross & Bagnell (2014) is to use a no-regret online learner to update policies using the following loss function at each episode n:
`n(⇡)",3. Differentiable Imitation Learning,[0],[0]
"= 1
H
HX
t=1
E st⇠d⇡nt
h E
a⇠⇡(·|st) [Q⇤t (st, a)]
i .",3. Differentiable Imitation Learning,[0],[0]
"(1)
",3. Differentiable Imitation Learning,[0],[0]
"The loss function intuitively encourages the learner to find a policy that minimize the expert’s cost-to-go under the state distribution resulting from the current learned policy ⇡n. Specifically, Ross & Bagnell (2014) suggest an algorithm named AggreVaTe (Aggregate Values to Imitate) that uses Follow-the-Leader (FTL) (ShalevShwartz et al., 2012) to update policies: ⇡n+1 = argmin⇡2⇧",3. Differentiable Imitation Learning,[0],[0]
Pn i=1,3. Differentiable Imitation Learning,[0],[0]
"`n(⇡), where ⇧ is a pre-defined convex policy set.",3. Differentiable Imitation Learning,[0],[0]
"When `n(⇡) is strongly convex with respect to ⇡ and ⇡⇤ 2 ⇧, after N iterations AggreVaTe with FTL can find a policy ⇡̂ with:
µ(⇡̂)  µ(⇡⇤) ✏N +O(ln(N)/N), (2)
where ✏N =",3. Differentiable Imitation Learning,[0],[0]
[ PN n=1 `n(⇡ ⇤ ) min⇡ PN n=1 `n(⇡)]/N .,3. Differentiable Imitation Learning,[0],[0]
"Note that ✏N 0 and the above inequality indicates that ⇡̂ can outperform ⇡⇤ when ⇡⇤ is not (locally) optimal (i.e., ✏n > 0).",3. Differentiable Imitation Learning,[0],[0]
"Our experimental results support this observation.
",3. Differentiable Imitation Learning,[0],[0]
A simple implementation of AggreVaTe that aggregates the values (as the name suggests) will require an exact solution to a batch optimization procedure in each episode.,3. Differentiable Imitation Learning,[0],[0]
"When ⇡ is represented by large, non-linear function approximators, the argmin procedure generally takes more and more computation time as n increases.",3. Differentiable Imitation Learning,[0],[0]
"Hence an efficient incremental update procedure is necessary for the method to scale.
",3. Differentiable Imitation Learning,[0],[0]
"To derive an incremental update procedure, we can take one of two routes.",3. Differentiable Imitation Learning,[0],[0]
"The first route, suggested already by (Ross & Bagnell, 2014), is to update our policy with an incremental no-regret algorithm such as weighted majority (Littlestone & Warmuth, 1994), instead of with a batch algorithm like FTRL.",3. Differentiable Imitation Learning,[0],[0]
"Unfortunately, for rich policy classes such as deep networks, no-regret learning algorithms may not be available (e.g., a deep network policy is non-convex with respect to its parameters).",3. Differentiable Imitation Learning,[0],[0]
"So instead we propose a novel second route: we directly differentiate Eq. 1, yielding an update related to policy gradient methods.",3. Differentiable Imitation Learning,[0],[0]
"We work out the details below, including a novel update rule for IL based on natural gradients.
",3. Differentiable Imitation Learning,[0],[0]
"Interestingly, the two routes described above yield almost identical algorithms if our policy class is simple enough: e.g., for a tabular policy, AggreVaTe with weighted majority yields the natural gradient version of AggreVaTeD described below.",3. Differentiable Imitation Learning,[0],[0]
"And, the two routes yield complementary theoretical guarantees: the first route yields a regret bound for simple-enough policy classes, while the second route yields convergence to a local optimum for extremely flexible policy classes.",3. Differentiable Imitation Learning,[0],[0]
"For discrete actions, the gradient of `n(⇡✓) (Eq. 1) with respect to the parameters ✓ of the policy is
r✓`n(✓) = 1
H
HX
t=1
E st⇠d ⇡✓n t
X
a
r✓⇡(a|st; ✓)Q⇤t (st, a).
",3.1. Online Gradient Descent,[0],[0]
"(3)
For continuous action spaces, we cannot simply replace the summation by integration since in practice it is hard to evaluate Q⇤t (s, a) for infinitely many a, so, instead, we use importance weighting to re-formulate `n (Eq. 1) as
`n(⇡✓)",3.1. Online Gradient Descent,[0],[0]
"= 1
H
HX
t=1
E s⇠d
⇡✓n t ,a⇠⇡(·|s;✓n)
⇡(a|s; ✓) ⇡(a|s; ✓n) Q⇤t (s, a)
=
1
H E
⌧⇠⇢⇡✓n
HX
t=1
⇡(at|st; ✓) ⇡(at|st; ✓n) Q⇤t (st, at).",3.1. Online Gradient Descent,[0],[0]
"(4)
See Appendix B for the derivation of the above equation.",3.1. Online Gradient Descent,[0],[0]
"With this reformulation, the gradient with respect to ✓ is
r✓`n(✓) = 1 H E
⌧⇠⇢⇡✓n
HX
t=1
r✓⇡(at|st; ✓) ⇡(at|st; ✓n) Q⇤t (st, at)
=
1 H E⌧⇠⇢⇡✓n
HX
t=1
r✓ ln(⇡(at|st; ✓n))Q⇤t (st, at).",3.1. Online Gradient Descent,[0],[0]
"(5)
The above gradient computation enables a very efficient update procedure with online gradient descent: ✓n+1 = ✓n ⌘nr✓`n(✓)|✓=✓n , where ⌘n is the learning rate.",3.1. Online Gradient Descent,[0],[0]
"We derive a natural gradient update procedure for imitation learning inspired by the success of natural gradient descent in RL (Kakade, 2002; Bagnell & Schneider, 2003; Schulman et al., 2015).",3.2. Policy Updates with Natural Gradient Descent,[0],[0]
"Following (Bagnell & Schneider, 2003), we define the Fisher information matrix I(✓n) using trajectory likelihood:
I(✓n) = 1 H2 E ⌧⇠⇢⇡✓n r✓n log(⇢⇡✓n (⌧))",3.2. Policy Updates with Natural Gradient Descent,[0],[0]
r,3.2. Policy Updates with Natural Gradient Descent,[0],[0]
✓n log(⇢⇡✓n (⌧)),3.2. Policy Updates with Natural Gradient Descent,[0],[0]
"T ,
(6)
where r✓ log(⇢⇡⌧ (⌧)) is the gradient of the log likelihood of the trajectory ⌧ which can be computed asPH
t=1 r✓ log(⇡✓(at|st)).",3.2. Policy Updates with Natural Gradient Descent,[0],[0]
"Note that this representation is equivalent to the original Fisher information matrix proposed by (Kakade, 2002).",3.2. Policy Updates with Natural Gradient Descent,[0],[0]
"Now, we can use Fisher information matrix together with the IL gradient derived in the previous section (Eq. 53) to compute the natural gradient as I(✓n)",3.2. Policy Updates with Natural Gradient Descent,[0],[0]
"1r✓`n(✓)|✓=✓n , which yields a natural gradient update: ✓n+1 = ✓n µnI(✓n)",3.2. Policy Updates with Natural Gradient Descent,[0],[0]
"1r✓`n(✓)|✓=✓n .
",3.2. Policy Updates with Natural Gradient Descent,[0],[0]
"Interesting, as we mentioned before, when the given MDP is discrete and the policy class is in a tabular representation, AggreVaTe with Weighted Majority (Littlestone & Warmuth, 1994) yields an extremely similar update procedure as AggreVaTeD with natural gradient.",3.2. Policy Updates with Natural Gradient Descent,[0],[0]
"Due to space limitation, we defer the detailed similarity between AggreVaTe with Weighted Majority and AggreVaTeD with natural gradient to Appendix A.",3.2. Policy Updates with Natural Gradient Descent,[0],[0]
"As Weighted Majority can speed up online learning (i.e., almost dimension free (Bubeck et al., 2015)) and AggreVaTe with Weighted Majority enjoys strong theoretical guarantees on the performance of the learned policy (Ross & Bagnell, 2014), this similarity provides an intuitive explanation why we can expect AggreVaTeD with natural gradient to speed up IL and learn a high quality policy.",3.2. Policy Updates with Natural Gradient Descent,[0],[0]
"In the previous section, we derived a regular gradient update procedure and a natural gradient update procedure for IL.",4. Sample-Based Practical Algorithms,[0],[0]
Note that all of the computations of gradients and Fisher information matrices assumed it was possible to exactly compute expectations including Es⇠d⇡ and Ea⇠⇡(a|s).,4. Sample-Based Practical Algorithms,[0],[0]
"In this section, we provide practical algorithms where we approximate the gradients and Fisher information matrices using finite samples collected during policy execution.",4. Sample-Based Practical Algorithms,[0],[0]
"We consider an episodic framework where given a policy ⇡n at episode n, we roll out ⇡n K times to collect K trajectories {⌧ni }, for i 2",4.1. Gradient Estimation and Variance Reduction,[0],[0]
"[K], ⌧ni = {s i,n 1 , ai,n 1
, ...}.",4.1. Gradient Estimation and Variance Reduction,[0],[0]
"For gradient r✓`n(✓)|✓=✓n we can compute an unbiased estimate
using {⌧ni }i2[K]:
˜r✓n = 1
HK
KX
i=1
HX
t=1
X
a
r✓n⇡✓n(a|s",4.1. Gradient Estimation and Variance Reduction,[0],[0]
"i,n t )Q ⇤ t (s i,n t , a),
(7)
˜r✓n = 1
HK
KX
i=1
HX
t=1
r✓n ln(⇡✓n(a i,n t |s",4.1. Gradient Estimation and Variance Reduction,[0],[0]
"i,n t ))",4.1. Gradient Estimation and Variance Reduction,[0],[0]
"Q ⇤ t (s i,n t , a i,n t ).
",4.1. Gradient Estimation and Variance Reduction,[0],[0]
"(8)
for discrete and continuous setting respectively.
",4.1. Gradient Estimation and Variance Reduction,[0],[0]
"When we can compute V ⇤t (s), we can replace Q⇤t (s i,n t , a) by the state-action advantage function A⇤t (s i,n t , a) = Q⇤t (s i,n t , a) V ⇤t (s i,n t ), which leads to the following unbiased and variance-reduced gradient estimation for continuous action setting (Greensmith et al., 2004):
˜r✓n = 1
HK
KX
i=1
HX
t=1
r✓n ln(⇡✓n(a i,n t |s",4.1. Gradient Estimation and Variance Reduction,[0],[0]
"i,n t ))",4.1. Gradient Estimation and Variance Reduction,[0],[0]
"A ⇤ t (s i,n t , a i,n t ),
(9)
",4.1. Gradient Estimation and Variance Reduction,[0],[0]
"In fact, we can use any baselines to reduce the variance by replacing Q⇤t (st, at) by Q⇤t (st, at) b(st), where b(st) : S !",4.1. Gradient Estimation and Variance Reduction,[0],[0]
R is a action-independent function.,4.1. Gradient Estimation and Variance Reduction,[0],[0]
Ideally b(st) should be some function approximator that approximates V ⇤(st).,4.1. Gradient Estimation and Variance Reduction,[0],[0]
"In our experiments, we test linear function approximator b(s) = wT s, which is online learned using ⇡⇤’s roll-out data.
",4.1. Gradient Estimation and Variance Reduction,[0],[0]
"The Fisher information matrix (Eq. 19) is approximated as:
˜I(✓n)",4.1. Gradient Estimation and Variance Reduction,[0],[0]
"= 1
H2K
KX
i=1
r✓n log(⇢⇡✓n (⌧i))r✓n log(⇢⇡✓n (⌧i))",4.1. Gradient Estimation and Variance Reduction,[0],[0]
"T
= SnS T n , (10)
where, for notation simplicity, we denote Sn as a d⇥K matrix where the i’s th column is r✓n log(⇢⇡✓n (⌧i))/(H p K).",4.1. Gradient Estimation and Variance Reduction,[0],[0]
Namely the Fisher information matrix is represented by a sum of K rank-one matrices.,4.1. Gradient Estimation and Variance Reduction,[0],[0]
"For large policies represented by neural networks, K ⌧ d, and hence ˜I(✓n) a low rank matrix.",4.1. Gradient Estimation and Variance Reduction,[0],[0]
"One can find the descent direction ✓n by solving the linear system SnSTn ✓n = ˜r✓n for ✓n using Conjugate Gradient (CG) with a fixed number of iterations, which is equivalent to solving the above linear systems using Partial Least Squares (Phatak & de Hoog, 2002).",4.1. Gradient Estimation and Variance Reduction,[0],[0]
"This approach is used in TRPO (Schulman et al., 2015).",4.1. Gradient Estimation and Variance Reduction,[0],[0]
The difference is that our representation of the Fisher matrix is in the form of SnSTn and in CG we never need to explicitly compute or store SnSTn which requires d2 space and time.,4.1. Gradient Estimation and Variance Reduction,[0],[0]
"Instead, we only compute and store Sn (O(Kd)) and the total computational time is still O(K2d).",4.1. Gradient Estimation and Variance Reduction,[0],[0]
"The learning-rate for natural gradient descent can be chosen as ⌘n = q KL/( ˜rT✓n ✓n), such that KL(⇢⇡✓n+1 (⌧)k⇢⇡✓n (⌧)) ⇡",4.1. Gradient Estimation and Variance Reduction,[0],[0]
"KL 2 R +
Algorithm 1",4.1. Gradient Estimation and Variance Reduction,[0],[0]
AggreVaTeD (Differentiable AggreVaTe) 1: Input: The given MDP and expert ⇡⇤.,4.1. Gradient Estimation and Variance Reduction,[0],[0]
"Learning rate
{⌘n}.",4.1. Gradient Estimation and Variance Reduction,[0],[0]
"Schedule rate {↵i}, ↵n ! 0, n !",4.1. Gradient Estimation and Variance Reduction,[0],[0]
"1. 2: Initialize policy ⇡✓1 (either random or supervised
learning).",4.1. Gradient Estimation and Variance Reduction,[0],[0]
3: for n = 1 to N do 4: Mixing policies: ⇡̂n = ↵n⇡⇤ + (1 ↵n)⇡✓n .,4.1. Gradient Estimation and Variance Reduction,[0],[0]
"5: Starting from ⇢
0 , roll out by executing ⇡̂n on the given MDP to generate K trajectories {⌧ni }.
6: Using Q⇤ and {⌧ni }i, compute the descent direction ✓n",4.1. Gradient Estimation and Variance Reduction,[0],[0]
"(Eq. 7, Eq. 8, Eq. 9, or CG).",4.1. Gradient Estimation and Variance Reduction,[0],[0]
7: Update: ✓n+1 = ✓n ⌘n ✓n .,4.1. Gradient Estimation and Variance Reduction,[0],[0]
8: end for 9: Return: the best hypothesis ⇡̂ 2 {⇡n}n on validation.,4.1. Gradient Estimation and Variance Reduction,[0],[0]
"Summarizing the above discussion, we present the differentiable imitation learning framework AggreVaTeD, in Alg. 1.",4.2. Differentiable Imitation Learning: AggreVaTeD,[0],[0]
"At every iteration n, the roll out policy ⇡̂n is a mix of the expert policy ⇡⇤ and the current policy ⇡✓n , with mixing rate ↵ (↵n !",4.2. Differentiable Imitation Learning: AggreVaTeD,[0],[0]
"0, n !",4.2. Differentiable Imitation Learning: AggreVaTeD,[0],[0]
"1): at every step, with probability ↵ ⇡̂n picks ⇡⇤ and picks ⇡✓n otherwise.",4.2. Differentiable Imitation Learning: AggreVaTeD,[0],[0]
"This mixing strategy with the decay rate was first introduced in (Ross et al., 2011) for IL, and later on was used in sequence prediction (Bengio et al., 2015).",4.2. Differentiable Imitation Learning: AggreVaTeD,[0],[0]
"In Line 6, one can either choose Eq. 8 or the corresponding variance reduced estimation Eq. 9 to perform regular gradient descent, and choose CG to perform natural gradient descent.",4.2. Differentiable Imitation Learning: AggreVaTeD,[0],[0]
"AggreVaTeD is extremely simple: we do not need to perform any data aggregation (i.e., we do not need to store all {⌧i}i from all previous iterations); the computational complexity of each policy update scales in O(d).
",4.2. Differentiable Imitation Learning: AggreVaTeD,[0],[0]
"When we use non-linear function approximators to represent the polices, the analysis of AggreVaTe from (Ross & Bagnell, 2014) will not hold, since the loss function `n(✓) is not convex with respect to parameters ✓.",4.2. Differentiable Imitation Learning: AggreVaTeD,[0],[0]
"Nevertheless, as we will show in experiments, in practice AggreVaTeD is still able to learn a policy that is competitive with, and sometimes superior to, the oracle’s performance.",4.2. Differentiable Imitation Learning: AggreVaTeD,[0],[0]
How much faster can IL learn a good policy than RL?,5. Quantify the Gap: An Analysis of IL vs RL,[0],[0]
In this section we quantify the gap on discrete MDPs when IL can (1) query for an optimal Q⇤ or (2) query for a noisy but unbiased estimate of Q⇤.,5. Quantify the Gap: An Analysis of IL vs RL,[0],[0]
"To measure the speed of learning, we look at the cumulative regret of the entire learning process, defined as RN = PN n=1(µ(⇡n) µ(⇡⇤)).",5. Quantify the Gap: An Analysis of IL vs RL,[0],[0]
A smaller regret rate indicates faster learning.,5. Quantify the Gap: An Analysis of IL vs RL,[0],[0]
"Throughout this section, we assume the expert ⇡⇤ is optimal.",5. Quantify the Gap: An Analysis of IL vs RL,[0],[0]
"We consider finite-horizon, episodic IL and RL algorithms.",5. Quantify the Gap: An Analysis of IL vs RL,[0],[0]
"We consider an MDP M shown in Fig. 1 which is a depthK binary tree-structure with S = 2K 1 states and two actions al, ar: go-left and go-right.",5.1. Exponential Gap,[0],[0]
"The transition is deterministic and the initial state s
0 (root) is fixed.",5.1. Exponential Gap,[0],[0]
The cost for each non-leaf state is zero; the cost for each leaf is i.i.d sampled from a given distribution (possibly different distributions per leaf).,5.1. Exponential Gap,[0],[0]
"Below we show that for M, IL can be exponentially more sample efficient than RL.
Theorem 5.1.",5.1. Exponential Gap,[0],[0]
"For M, the regret RN of any finite-horizon, episodic RL algorithm is at least:
E[RN ] ⌦( p SN).",5.1. Exponential Gap,[0],[0]
"(11)
",5.1. Exponential Gap,[0],[0]
The expectation is with respect to random generation of cost and internal randomness of the algorithm.,5.1. Exponential Gap,[0],[0]
"However, for the same MDP M, with the access to Q⇤, we show IL can learn exponentially faster:
Theorem 5.2.",5.1. Exponential Gap,[0],[0]
"For the MDP M, AggreVaTe with FTL can achieve the following regret bound:
RN  O(ln (S)).",5.1. Exponential Gap,[0],[0]
"(12)
Fig. 1 illustrates the intuition behind the theorem.",5.1. Exponential Gap,[0],[0]
"Assume during the first episode, the initial policy ⇡
1 picks the rightmost trajectory (bold black) to explore.",5.1. Exponential Gap,[0],[0]
"We query from the cost-to-go oracle Q⇤ at s
0 for al and ar, and learn that Q⇤(s
0 , al) <",5.1. Exponential Gap,[0],[0]
"Q⇤(s0, ar).",5.1. Exponential Gap,[0],[0]
"This immediately tells us that the optimal policy will go left (black arrow) at s
0 .",5.1. Exponential Gap,[0],[0]
"Hence the algorithm does not have to explore the right sub-tree (dotted circle).
",5.1. Exponential Gap,[0],[0]
"Next we consider a more difficult setting where one can only query for a noisy but unbiased estimate of Q⇤ (e.g., by rolling out ⇡⇤ finite number of times).",5.1. Exponential Gap,[0],[0]
The above halving argument will not apply since deterministically eliminating nodes based on noisy estimates might permanently remove good trajectories.,5.1. Exponential Gap,[0],[0]
"However, IL can still achieve a poly-log regret with respect to S, even in the noisy setting:
Theorem 5.3.",5.1. Exponential Gap,[0],[0]
"With only access to unbiased estimate of Q⇤, for the MDP M, AggreVaTeD with WM can achieve the
following regret with probability at least 1 :
RN  O ⇣ ln(S)( p ln(S)N + p ln(2/ )N)",5.1. Exponential Gap,[0],[0]
⌘ .,5.1. Exponential Gap,[0],[0]
"(13)
",5.1. Exponential Gap,[0],[0]
"The detailed proofs of the above three theorems can be found in Appendix E,F,G respectively.",5.1. Exponential Gap,[0],[0]
"In summary, for MDP M, IL is is exponentially faster than RL.",5.1. Exponential Gap,[0],[0]
We next quantify the gap in general discrete MDPs and also show that AggreVaTeD is near-optimal.,5.2. Polynomial Gap and Near-Optimality,[0],[0]
"We consider the harder case where we can only access an unbiased estimate of Q⇤t , for any t and state-action pair.",5.2. Polynomial Gap and Near-Optimality,[0],[0]
"The policy ⇡ is represented as a set of probability vectors ⇡s,t 2 (A), for all s 2 S and t 2",5.2. Polynomial Gap and Near-Optimality,[0],[0]
"[H]: ⇡ = {⇡s,t}s2S,t2[H].",5.2. Polynomial Gap and Near-Optimality,[0],[0]
Theorem 5.4.,5.2. Polynomial Gap and Near-Optimality,[0],[0]
"With access to unbiased estimates of Q⇤t , AggreVaTeD with WM achieves the regret upper bound:
RN  O HQe
max
p S ln(A)N .",5.2. Polynomial Gap and Near-Optimality,[0],[0]
"(14)
",5.2. Polynomial Gap and Near-Optimality,[0],[0]
Here Qe max is the maximum cost-to-go of the expert.,5.2. Polynomial Gap and Near-Optimality,[0],[0]
The total regret shown in Eq. 14 allows us to compare IL algorithms to RL algorithms.,5.2. Polynomial Gap and Near-Optimality,[0],[0]
"For example, the Upper Confidence Bound (UCB) based, near-optimal optimistic RL algorithms from (Jaksch et al., 2010), specifically designed for efficient exploration, admit regret ˜O(HS",5.2. Polynomial Gap and Near-Optimality,[0],[0]
"p HAN), leading to a gap of approximately p HAS compared to the regret bound of imitation learning shown in Eq. 14.
",5.2. Polynomial Gap and Near-Optimality,[0],[0]
"We also provide a lower bound on RN for the H = 1 case which shows the dependencies on N,A, S are tight:
Theorem 5.5.",5.2. Polynomial Gap and Near-Optimality,[0],[0]
"There exists an MDP (H=1) such that, with only access to unbiased estimates of Q⇤, any finite-horizon episodic imitation learning algorithm must have:
E[RN ] ⌦( p S ln(A)N).",5.2. Polynomial Gap and Near-Optimality,[0],[0]
"(15)
The proofs of the above two theorems regarding general MDPs can be found in Appendix H,I.",5.2. Polynomial Gap and Near-Optimality,[0],[0]
"In summary for discrete MDPs, one can expect at least a polynomial gap and a possible exponential gap between IL and RL.",5.2. Polynomial Gap and Near-Optimality,[0],[0]
"We evaluate our algorithms on robotics simulations from OpenAI Gym (Brockman et al., 2016) and on Handwritten Algebra Dependency Parsing (Duyck & Gordon, 2015).",6. Experiments,[0],[0]
"We report reward instead of cost, since OpenAI Gym by default uses reward and dependency parsing aims to maximize UAS score.",6. Experiments,[0],[0]
"As our approach only promises there
Here we assume Qe max is a constant compared to H .",6. Experiments,[0],[0]
"If Qe
max = ⇥(H), then the expert is no better than a random policy of which the cost-to-go is around ⇥(H).
exists a policy among all of the learned polices that can perform as well as the expert, we report the performance of the best policy so far: max{µ(⇡
1 ), ..., µ(⇡i)}.",6. Experiments,[0],[0]
"For regular gradient descent, we use ADAM (Kingma & Ba, 2014) which is a first-order no-regret algorithm, and for natural gradient, we use CG to compute the descent direction.",6. Experiments,[0],[0]
"For RL we use REINFORCE (Williams, 1992) and Truncated Natural Policy Gradient (TNPG)",6. Experiments,[0],[0]
"(Duan et al., 2016).",6. Experiments,[0],[0]
"We consider CartPole Balancing, Acrobot Swing-up, Hopper and Walker.",6.1. Robotics Simulations,[0],[0]
"For generating an expert, similar to previous work (Ho & Ermon, 2016), we used a Deep Q-Network (DQN) to generate Q⇤ for CartPole and Acrobot (e.g., to simulate the settings where Q⇤ is available), while using the publicly available TRPO implementation to generate ⇡⇤ for Hopper and Walker to simulate the settings where one has to estimate Q⇤ by Monte-Carlo roll outs ⇡⇤.
",6.1. Robotics Simulations,[0],[0]
Discrete Action Setting We use a one-layer (16 hidden units) neural network with ReLu activation functions to represent the policy ⇡ for the Cart-pole and Acrobot benchmarks.,6.1. Robotics Simulations,[0],[0]
"The value function Q⇤ is obtained from the DQN (Mnih et al., 2015) and represented by a multi-layer fully connected neural network.",6.1. Robotics Simulations,[0],[0]
The policy ⇡✓1 is initialized with common ReLu neural network initialization techniques.,6.1. Robotics Simulations,[0],[0]
"For the scheduling rate {↵i}, we set all ↵i = 0: namely we did not roll-in using the expert’s actions during training.",6.1. Robotics Simulations,[0],[0]
"We set the number of roll outs K = 50 and horizon H = 500 for CartPole and H = 200 for Acrobot.
",6.1. Robotics Simulations,[0],[0]
Fig.,6.1. Robotics Simulations,[0],[0]
2a and 2b shows the performance averaged over 10 random trials of AggreVaTeD with regular gradient descent and natural gradient descent.,6.1. Robotics Simulations,[0],[0]
Note that AggreVaTeD outperforms the experts’ performance significantly: Natural gradient surpasses the expert by 5.8% in Acrobot and 25% in Cart-pole.,6.1. Robotics Simulations,[0],[0]
"Also, for Acrobot swing-up, at horizon H = 200, with high probability a randomly initialized neural network policy won’t be able to collect any reward signals.",6.1. Robotics Simulations,[0],[0]
Hence the improvement rates of REINFORCE and TNPG are slow.,6.1. Robotics Simulations,[0],[0]
"In fact, we observed that for a short horizon such as H = 200, REINFORCE and Truncated Natural Gradient often even fail to improve the policy at all (failed
6 times among 10 trials).",6.1. Robotics Simulations,[0],[0]
"On the contrary, AggreVaTeD does not suffer from the delayed reward signal issue, since the expert will collect reward signals much faster than a randomly initialized policy.
",6.1. Robotics Simulations,[0],[0]
Fig. 2c shows the performance of AggreVaTeD with an LSTM policy (32 hidden states) in a partially observed setting where the expert has access to full states but the learner has access to partial observations (link positions).,6.1. Robotics Simulations,[0],[0]
RL algorithms did not achieve any improvement while AggreVaTeD still achieved 92% of the expert’s performance.,6.1. Robotics Simulations,[0],[0]
"In Appendix K, we provide extra experiments on partial observable CartPole with GRU-based policies, where we demonstrate that even in partial observable setting, AggreVaTeD can learn RNN polices that outperform experts.
",6.1. Robotics Simulations,[0],[0]
Continuous Action Setting We test our approaches on two robotics simulators with continuous actions: (1) the 2-d Walker and (2) the Hopper from the MuJoCo physics simulator.,6.1. Robotics Simulations,[0],[0]
"Following the neural network settings described in Schulman et al. (2015), the expert policy ⇡⇤ is obtained from TRPO with one hidden layer (64 hidden states), which is the same structure that we use to represent our policies ⇡✓.",6.1. Robotics Simulations,[0],[0]
We set K = 50 and H = 100.,6.1. Robotics Simulations,[0],[0]
"We initialize ⇡✓1 by collecting K expert demonstrations and then maximize the likelihood of these demonstrations (i.e., supervised learning).",6.1. Robotics Simulations,[0],[0]
"We use a linear baseline b(s) = wT s for RL and IL.
Fig.",6.1. Robotics Simulations,[0],[0]
2e and 2d show the performance averaged over 5 random trials.,6.1. Robotics Simulations,[0],[0]
Note that AggreVaTeD outperforms the expert in the Walker by 13.7% while achieving 97% of the expert’s performance in the Hopper problem.,6.1. Robotics Simulations,[0],[0]
"After 100 iterations, we see that by leveraging the help from experts, AggreVaTeD can achieve much faster improvement rate than the corresponding RL algorithms (though eventually we can expect RL to catch up).",6.1. Robotics Simulations,[0],[0]
"In Walker, we also tested AggreVaTeD without linear baseline, which still outperforms the expert but performed slightly worse than AggreVaTeD with baseline as expected.",6.1. Robotics Simulations,[0],[0]
"We consider a sequential prediction problem: transitionbased dependency parsing for handwritten algebra with raw image data (Duyck & Gordon, 2015).",6.2. Dependency Parsing on Handwritten Algebra,[0],[0]
"The parsing task
for algebra is similar to the classic dependency parsing for natural language (Chang et al., 2015a) where the problem is modelled in the IL setting and the state-of-the-art is achieved by AggreVaTe with FTRL (using Data Aggregation).",6.2. Dependency Parsing on Handwritten Algebra,[0],[0]
The additional challenge here is that the inputs are handwritten algebra symbols in raw images.,6.2. Dependency Parsing on Handwritten Algebra,[0],[0]
We directly learn to predict parse trees from low level image features (Histogram of Gradient features (HoG)).,6.2. Dependency Parsing on Handwritten Algebra,[0],[0]
"During training, the expert is constructed using the ground-truth dependencies in training data.",6.2. Dependency Parsing on Handwritten Algebra,[0],[0]
"The full state s during parsing consists of three data structures: Stack, Buffer and Arcs, which store raw images of the algebraic symbols.",6.2. Dependency Parsing on Handwritten Algebra,[0],[0]
"Since the sizes of stack, buffer and arcs change during parsing, a common approach is to featurize the state s by taking the features of the latest three symbols from stack, buffer and arcs (e.g., (Chang et al., 2015a)).",6.2. Dependency Parsing on Handwritten Algebra,[0],[0]
"Hence the problem falls into the partially observable setting, where the feature o is extracted from state s and only contains partial information about s. The dataset consists of 400 sets of handwritten algebra equations.",6.2. Dependency Parsing on Handwritten Algebra,[0],[0]
"We use 80% for training, 10% for validation, and 10% for testing.",6.2. Dependency Parsing on Handwritten Algebra,[0],[0]
"We include an example of handwritten algebra equations and its dependency tree in Appendix J. Note that different from robotics simulators where at every episode one can get fresh data from the simulators, the dataset is fixed and sample efficiency is critical.
",6.2. Dependency Parsing on Handwritten Algebra,[0],[0]
"The RNN policy follows the design from (Sutskever et al., 2014).",6.2. Dependency Parsing on Handwritten Algebra,[0],[0]
It consists of two LSTMs.,6.2. Dependency Parsing on Handwritten Algebra,[0],[0]
"Given a sequence of algebra symbols ⌧ , the first LSTM processes one symbol at a time and at the end outputs its hidden states and memory (i.e., a summary of ⌧ ).",6.2. Dependency Parsing on Handwritten Algebra,[0],[0]
The second LSTM initializes its own hidden states and memory using the outputs of the first LSTM.,6.2. Dependency Parsing on Handwritten Algebra,[0],[0]
"At every parsing step t, the second LSTM takes the current partial observation ot (ot consists of features of the most recent item from stack, buffer and arcs) as input, and uses its internal hidden state and memory to compute the action distribution ⇡(·|o
1 , ..., ot, ⌧) conditioned on history.",6.2. Dependency Parsing on Handwritten Algebra,[0],[0]
"We also tested reactive policies constructed as fully connected ReLu neural networks (NN) (one-layer with 1000 hidden states) that directly maps from observation ot to action a, where ot uses the most three recent items.",6.2. Dependency Parsing on Handwritten Algebra,[0],[0]
"We use variance reduced gradient estimations, which give better performance in practice.",6.2. Dependency Parsing on Handwritten Algebra,[0],[0]
The performance is summarised in Table 1.,6.2. Dependency Parsing on Handwritten Algebra,[0],[0]
"Due to the partial observability of the problem, AggreVaTeD with a LSTM policy achieves significantly better UAS scores compared to the NN reactive pol-
icy and DAgger with a Kernelized SVM (Duyck & Gordon, 2015).",6.2. Dependency Parsing on Handwritten Algebra,[0],[0]
Also AggreVaTeD with a LSTM policy achieves 97% of optimal expert’s performance.,6.2. Dependency Parsing on Handwritten Algebra,[0],[0]
Fig. 3 shows the improvement rate of regular gradient and natural gradient on both validation set and test set.,6.2. Dependency Parsing on Handwritten Algebra,[0],[0]
Overall we observe that both methods have similar performance.,6.2. Dependency Parsing on Handwritten Algebra,[0],[0]
Natural gradient achieves a better UAS score in validation and converges slightly faster on the test set but also achieves a lower UAS score on test set.,6.2. Dependency Parsing on Handwritten Algebra,[0],[0]
"We introduced AggreVaTeD, a differentiable imitation learning algorithm which trains neural network policies for sequential prediction tasks such as continuous robot control and dependency parsing on raw image data.",7. Conclusion,[0],[0]
We showed that in theory and in practice IL can learn much faster than RL with access to optimal cost-to-go oracles.,7. Conclusion,[0],[0]
The IL learned policies were able to achieve expert and sometimes super-expert levels of performance in both fully observable and partially observable settings.,7. Conclusion,[0],[0]
The theoretical and experimental results suggest that IL is significantly more effective than RL for sequential prediction with near optimal cost-to-go oracles.,7. Conclusion,[0],[0]
This research was supported in part by ONR 36060-1b1141268.,Acknowledgement,[0],[0]
"Recently, researchers have demonstrated stateof-the-art performance on sequential prediction problems using deep neural networks and Reinforcement Learning (RL).",abstractText,[0],[0]
"For some of these problems, oracles that can demonstrate good performance may be available during training, but are not used by plain RL methods.",abstractText,[0],[0]
"To take advantage of this extra information, we propose AggreVaTeD, an extension of the Imitation Learning (IL) approach of Ross & Bagnell (2014).",abstractText,[0],[0]
"AggreVaTeD allows us to use expressive differentiable policy representations such as deep networks, while leveraging training-time oracles to achieve faster and more accurate solutions with less training data.",abstractText,[0],[0]
"Specifically, we present two gradient procedures that can learn neural network policies for several problems, including a sequential prediction task and several high-dimensional robotics control problems.",abstractText,[0],[0]
We also provide a comprehensive theoretical study of IL that demonstrates that we can expect up to exponentially-lower sample complexity for learning with AggreVaTeD than with plain RL algorithms.,abstractText,[0],[0]
Our results and theory indicate that IL (and AggreVaTeD in particular) can be a more effective strategy for sequential prediction than plain RL.,abstractText,[0],[0]
Deeply AggreVaTeD: Differentiable Imitation Learning for Sequential Prediction,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 564–573 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"In recent years, deep learning techniques have obtained many state-of-the-art results in various classification and recognition problems (Krizhevsky et al., 2012; Hinton et al., 2012; Kim, 2014).",1 Introduction,[0],[0]
"However, complex natural language processing problems often require multiple inter-related decisions, and empowering deep learning models with the ability of learning to reason is still a challenging issue.",1 Introduction,[0],[0]
"To handle complex queries where there are no obvious answers, intelligent machines must be able to reason with existing resources, and learn to infer an unknown answer.
",1 Introduction,[0],[0]
"More specifically, we situate our study in the context of multi-hop reasoning, which is the task of learning explicit inference formulas, given a large KG.",1 Introduction,[0],[0]
"For example, if the KG includes the
1Code and the NELL dataset are available at https:// github.com/xwhan/DeepPath.
",1 Introduction,[0],[0]
"beliefs such as Neymar plays for Barcelona, and Barcelona are in the La Liga league, then machines should be able to learn the following formula: playerPlaysForTeam(P,T)",1 Introduction,[0],[0]
"∧ teamPlaysInLeague(T,L) ⇒ playerPlaysInLeague(P,L).",1 Introduction,[0],[0]
"In the testing time, by plugging in the learned formulas, the system should be able to automatically infer the missing link between a pair of entities.",1 Introduction,[0],[0]
"This kind of reasoning machine will potentially serve as an essential components of complex QA systems.
",1 Introduction,[0],[0]
"In recent years, the Path-Ranking Algorithm (PRA) (Lao et al., 2010, 2011a) emerges as a promising method for learning inference paths in large KGs.",1 Introduction,[0],[0]
PRA uses a random-walk with restarts based inference mechanism to perform multiple bounded depth-first search processes to find relational paths.,1 Introduction,[0],[0]
"Coupled with elastic-net based learning, PRA then picks more plausible paths using supervised learning.",1 Introduction,[0],[0]
"However, PRA operates in a fully discrete space, which makes it difficult to evaluate and compare similar entities and relations in a KG.
",1 Introduction,[0],[0]
"In this work, we propose a novel approach for controllable multi-hop reasoning: we frame the path learning process as reinforcement learning (RL).",1 Introduction,[0],[0]
"In contrast to PRA, we use translationbased knowledge based embedding method (Bordes et al., 2013) to encode the continuous state of our RL agent, which reasons in the vector space environment of the knowledge graph.",1 Introduction,[0],[0]
The agent takes incremental steps by sampling a relation to extend its path.,1 Introduction,[0],[0]
"To better guide the RL agent for learning relational paths, we use policy gradient training (Mnih et al., 2015) with a novel reward function that jointly encourages accuracy, diversity, and efficiency.",1 Introduction,[0],[0]
"Empirically, we show that our method outperforms PRA and embedding based methods on a Freebase and a Never-Ending Language Learning (Carlson et al., 2010a) dataset.
564
Our contributions are three-fold:
• We are the first to consider reinforcement learning (RL) methods for learning relational paths in knowledge graphs;
• Our learning method uses a complex reward function that considers accuracy, efficiency, and path diversity simultaneously, offering better control and more flexibility in the pathfinding process;
• We show that our method can scale up to large scale knowledge graphs, outperforming PRA and KG embedding methods in two tasks.
",1 Introduction,[0],[0]
"In the next section, we outline related work in path-finding and embedding methods in KGs.",1 Introduction,[0],[0]
We describe the proposed method in Section 3.,1 Introduction,[0],[0]
We show experimental results in Section 4.,1 Introduction,[0],[0]
"Finally, we conclude in Section 5.",1 Introduction,[0],[0]
"The Path-Ranking Algorithm (PRA) method (Lao et al., 2011b) is a primary path-finding approach that uses random walk with restart strategies for multi-hop reasoning.",2 Related Work,[0],[0]
Gardner et al. (2013; 2014) propose a modification to PRA that computes feature similarity in the vector space.,2 Related Work,[0],[0]
Wang and Cohen (2015) introduce a recursive random walk approach for integrating the background KG and text—the method performs structure learning of logic programs and information extraction from text at the same time.,2 Related Work,[0],[0]
"A potential bottleneck for random walk inference is that supernodes connecting to large amount of formulas will create huge fan-out areas that significantly slow down the inference and affect the accuracy.
",2 Related Work,[0],[0]
Toutanova et al. (2015) provide a convolutional neural network solution to multi-hop reasoning.,2 Related Work,[0],[0]
"They build a CNN model based on lexicalized dependency paths, which suffers from the error propagation issue due to parse errors.",2 Related Work,[0],[0]
Guu et al. (2015) uses KG embeddings to answer path queries.,2 Related Work,[0],[0]
"Zeng et al. (2014) described a CNN model for relational extraction, but it does not explicitly model the relational paths.",2 Related Work,[0],[0]
"Neelakantan et al. (2015) propose a recurrent neural networks model for modeling relational paths in knowledge base completion (KBC), but it trains too many separate models, and therefore it does not scale.",2 Related Work,[0],[0]
"Note that many of the recent KG reasoning methods (Neelakantan et al.,
2015; Das et al., 2017) still rely on first learning the PRA paths, which only operates in a discrete space.",2 Related Work,[0],[0]
"Comparing to PRA, our method reasons in a continuous space, and by incorporating various criteria in the reward function, our reinforcement learning (RL) framework has better control and more flexibility over the path-finding process.
",2 Related Work,[0],[0]
"Neural symbolic machine (Liang et al., 2016) is a more recent work on KG reasoning, which also applies reinforcement learning but has a different flavor from our work.",2 Related Work,[0],[0]
"NSM learns to compose programs that can find answers to natural language questions, while our RL model tries to add new facts to knowledge graph (KG) by reasoning on existing KG triples.",2 Related Work,[0],[0]
"In order to get answers, NSM learns to generate a sequence of actions that can be combined as a executable program.",2 Related Work,[0],[0]
The action space in NSM is a set of predefined tokens.,2 Related Work,[0],[0]
"In our framework, the goal is to find reasoning paths, thus the action space is relation space in the KG.",2 Related Work,[0],[0]
"A similar framework (Johnson et al., 2017) has also been applied to visual reasoning tasks.",2 Related Work,[0],[0]
"In this section, we describe in detail our RL-based framework for multi-hop relation reasoning.",3 Methodology,[0],[0]
The specific task of relation reasoning is to find reliable predictive paths between entity pairs.,3 Methodology,[0],[0]
We formulate the path finding problem as a sequential decision making problem which can be solved with a RL agent.,3 Methodology,[0],[0]
We first describe the environment and the policy-based RL agent.,3 Methodology,[0],[0]
"By interacting with the environment designed around the KG, the agent learns to pick the promising reasoning paths.",3 Methodology,[0],[0]
Then we describe the training procedure of our RL model.,3 Methodology,[0],[0]
"After that, we describe an efficient path-constrained search algorithm for relation reasoning with the paths found by the RL agent.",3 Methodology,[0],[0]
The RL system consists of two parts (see Figure 1).,3.1 Reinforcement Learning for Relation Reasoning,[0],[0]
The first part is the external environment E which specifies the dynamics of the interaction between the agent and the KG.,3.1 Reinforcement Learning for Relation Reasoning,[0],[0]
This environment is modeled as a Markov decision process (MDP).,3.1 Reinforcement Learning for Relation Reasoning,[0],[0]
"A tuple < S,A,P,R > is defined to represent the MDP, where S is the continuous state space, A = {a1, a2, ..., an} is the set of all available actions, P(St+1 = s′",3.1 Reinforcement Learning for Relation Reasoning,[0],[0]
"|St = s,At = a) is the transition probability matrix, and R(s, a) is the reward
function of every (s, a) pairs.
",3.1 Reinforcement Learning for Relation Reasoning,[0],[0]
"The second part of the system, the RL agent, is represented as a policy network πθ(s, a) = p(a|s; θ) which maps the state vector to a stochastic policy.",3.1 Reinforcement Learning for Relation Reasoning,[0],[0]
The neural network parameters θ are updated using stochastic gradient descent.,3.1 Reinforcement Learning for Relation Reasoning,[0],[0]
"Compared to Deep Q Network (DQN) (Mnih et al., 2013), policy-based RL methods turn out to be more appropriate for our knowledge graph scenario.",3.1 Reinforcement Learning for Relation Reasoning,[0],[0]
"One reason is that for the path finding problem in KG, the action space can be very large due to complexity of the relation graph.",3.1 Reinforcement Learning for Relation Reasoning,[0],[0]
This can lead to poor convergence properties for DQN.,3.1 Reinforcement Learning for Relation Reasoning,[0],[0]
"Besides, instead of learning a greedy policy which is common in value-based methods like DQN, the policy network is able to learn a stochastic policy which prevent the agent from getting stuck at an intermediate state.",3.1 Reinforcement Learning for Relation Reasoning,[0],[0]
"Before we describe the structure of our policy network, we first describe the components (actions, states, rewards) of the RL environment.
",3.1 Reinforcement Learning for Relation Reasoning,[0],[0]
"Actions Given the entity pairs (es, et) with relation r, we want the agent to find the most informative paths linking these entity pairs.",3.1 Reinforcement Learning for Relation Reasoning,[0],[0]
"Beginning with the source entity es, the agent use the policy network to pick the most promising
relation to extend its path at each step until it reaches the target entity et.",3.1 Reinforcement Learning for Relation Reasoning,[0],[0]
"To keep the output dimension of the policy network consistent, the action space is defined as all the relations in the KG.
",3.1 Reinforcement Learning for Relation Reasoning,[0],[0]
States,3.1 Reinforcement Learning for Relation Reasoning,[0],[0]
The entities and relations in a KG are naturally discrete atomic symbols.,3.1 Reinforcement Learning for Relation Reasoning,[0],[0]
"Since existing practical KGs like Freebase (Bollacker et al., 2008) and NELL (Carlson et al., 2010b) often have huge amounts of triples.",3.1 Reinforcement Learning for Relation Reasoning,[0],[0]
It is impossible to directly model all the symbolic atoms in states.,3.1 Reinforcement Learning for Relation Reasoning,[0],[0]
"To capture the semantic information of these symbols, we use translation-based embeddings such as TransE (Bordes et al., 2013) and TransH (Wang et al., 2014) to represent the entities and relations.",3.1 Reinforcement Learning for Relation Reasoning,[0],[0]
These embeddings map all the symbols to a lowdimensional vector space.,3.1 Reinforcement Learning for Relation Reasoning,[0],[0]
"In our framework, each state captures the agent’s position in the KG.",3.1 Reinforcement Learning for Relation Reasoning,[0],[0]
"After taking an action, the agent will move from one entity to another.",3.1 Reinforcement Learning for Relation Reasoning,[0],[0]
These two are linked by the action (relation) just taken by the agent.,3.1 Reinforcement Learning for Relation Reasoning,[0],[0]
"The state vector at step t is given as follows:
st = (et, etarget − et)
where et denotes the embeddings of the current entity node and etarget denotes the embeddings of
the target entity.",3.1 Reinforcement Learning for Relation Reasoning,[0],[0]
"At the initial state, et = esource.",3.1 Reinforcement Learning for Relation Reasoning,[0],[0]
"We do not incorporate the reasoning relation in the state, because the embedding of the reasoning relation remain constant during path finding, which is not helpful in training.",3.1 Reinforcement Learning for Relation Reasoning,[0],[0]
"However, we find out that by training the RL agent using a set of positive samples for one particular relation, the agent can successfully discover the relation semantics.
",3.1 Reinforcement Learning for Relation Reasoning,[0],[0]
Rewards There are a few factors that contribute to the quality of the paths found by the RL agent.,3.1 Reinforcement Learning for Relation Reasoning,[0],[0]
"To encourage the agent to find predictive paths, our reward functions include the following scoring criteria: Global accuracy: For our environment settings, the number of actions that can be taken by the agent can be very large.",3.1 Reinforcement Learning for Relation Reasoning,[0],[0]
"In other words, there are much more incorrect sequential decisions than the correct ones.",3.1 Reinforcement Learning for Relation Reasoning,[0],[0]
The number of these incorrect decision sequences can increase exponentially with the length of the path.,3.1 Reinforcement Learning for Relation Reasoning,[0],[0]
"In view of this challenge, the first reward function we add to the RL model is defined as follows:
rGLOBAL = { +1, if the path reaches etarget −1, otherwise
the agent is given an offline positive reward +1 if it reaches the target after a sequence of actions.",3.1 Reinforcement Learning for Relation Reasoning,[0],[0]
"Path efficiency: For the relation reasoning task, we observe that short paths tend to provide more reliable reasoning evidence than longer paths.",3.1 Reinforcement Learning for Relation Reasoning,[0],[0]
Shorter chains of relations can also improve the efficiency of the reasoning by limiting the length of the RL’s interactions with the environment.,3.1 Reinforcement Learning for Relation Reasoning,[0],[0]
"The efficiency reward is defined as follows:
rEFFICIENCY = 1
length(p)
where path p is defined as a sequence of relations r1 → r2 → ...→ rn.",3.1 Reinforcement Learning for Relation Reasoning,[0],[0]
Path diversity: We train the agent to find paths using positive samples for each relation.,3.1 Reinforcement Learning for Relation Reasoning,[0],[0]
"These training sample (esource, etarget) have similar state representations in the vector space.",3.1 Reinforcement Learning for Relation Reasoning,[0],[0]
The agent tends to find paths with similar syntax and semantics.,3.1 Reinforcement Learning for Relation Reasoning,[0],[0]
These paths often contains redundant information since some of them may be correlated.,3.1 Reinforcement Learning for Relation Reasoning,[0],[0]
"To encourage the agent to find diverse paths, we define a diversity reward function using the cosine similarity
between the current path and the existing ones:
rDIVERSITY =",3.1 Reinforcement Learning for Relation Reasoning,[0],[0]
− 1|F | |F |∑ i=1,3.1 Reinforcement Learning for Relation Reasoning,[0],[0]
"cos(p,pi)
",3.1 Reinforcement Learning for Relation Reasoning,[0],[0]
"where p = ∑n
i=1",3.1 Reinforcement Learning for Relation Reasoning,[0],[0]
ri represents the path embedding for the relation chain r1 → r2 → ...→ rn. Policy Network,3.1 Reinforcement Learning for Relation Reasoning,[0],[0]
We use a fully-connected neural network to parameterize the policy function π(s; θ) that maps the state vector s to a probability distribution over all possible actions.,3.1 Reinforcement Learning for Relation Reasoning,[0],[0]
"The neural network consists of two hidden layers, each followed by a rectifier nonlinearity layer (ReLU).",3.1 Reinforcement Learning for Relation Reasoning,[0],[0]
The output layer is normalized using a softmax function (see Figure 1).,3.1 Reinforcement Learning for Relation Reasoning,[0],[0]
"In practice, one big challenge of KG reasoning is that the relation set can be quite large.",3.2 Training Pipeline,[0],[0]
"For a typical KG, the RL agent is often faced with hundreds (thousands) of possible actions.",3.2 Training Pipeline,[0],[0]
"In other words, the output layer of the policy network often has a large dimension.",3.2 Training Pipeline,[0],[0]
"Due to the complexity of the relation graph and the large action space, if we directly train the RL model by trial and errors, which is typical for RL algorithms, the RL model will show very poor convergence properties.",3.2 Training Pipeline,[0],[0]
"After a long-time training, the agents fails to find any valuable path.",3.2 Training Pipeline,[0],[0]
"To tackle this problem, we start our training with a supervised policy which is inspired by the imitation learning pipeline used by AlphaGo (Silver et al., 2016).",3.2 Training Pipeline,[0],[0]
"In the Go game, the player is facing nearly 250 possible legal moves at each step.",3.2 Training Pipeline,[0],[0]
Directly training the agent to pick actions from the original action space can be a difficult task.,3.2 Training Pipeline,[0],[0]
AlphaGo first train a supervised policy network using experts moves.,3.2 Training Pipeline,[0],[0]
"In our case, the supervised policy is trained with a randomized breadth-first search (BFS).
",3.2 Training Pipeline,[0],[0]
"Supervised Policy Learning For each relation, we use a subset of all the positive samples (entity pairs) to learn the supervised policy.",3.2 Training Pipeline,[0],[0]
"For each positive sample (esource, etarget), a two-side BFS is conducted to find same correct paths between the entities.",3.2 Training Pipeline,[0],[0]
For each path p with a sequence of relations r1 → r2 → ...,3.2 Training Pipeline,[0],[0]
"→ rn, we update the parameters θ to maximize the expected cumulative reward using Monte-Carlo Policy Gradient (RE-
INFORCE)",3.2 Training Pipeline,[0],[0]
"(Williams, 1992): J(θ) = Ea∼π(a|s;θ)( ∑ t Rst,at)
= ∑ t ∑ a∈A π(a|st; θ)Rst,at (1)
where J(θ) is the expected total rewards for one episode.",3.2 Training Pipeline,[0],[0]
"For supervised learning, we give a reward of +1 for each step of a successful episode.",3.2 Training Pipeline,[0],[0]
"By plugging in the paths found by the BFS, the approximated gradient used to update the policy network is shown below:
∇θJ(θ)",3.2 Training Pipeline,[0],[0]
"= ∑ t ∑ a∈A π(a|st; θ)∇θ log π(a|st; θ)
≈",3.2 Training Pipeline,[0],[0]
"∇θ ∑ t log π(a = rt|st; θ) (2)
where rt belongs to the path",3.2 Training Pipeline,[0],[0]
p.,3.2 Training Pipeline,[0],[0]
"However, the vanilla BFS is a biased search algorithm which prefers short paths.",3.2 Training Pipeline,[0],[0]
"When plugging in these biased paths, it becomes difficult for the agent to find longer paths which may potentially be useful.",3.2 Training Pipeline,[0],[0]
We want the paths to be controlled only by the defined reward functions.,3.2 Training Pipeline,[0],[0]
"To prevent the biased search, we adopt a simple trick to add some random mechanisms to the BFS.",3.2 Training Pipeline,[0],[0]
"Instead of directly searching the path between esource and etarget, we randomly pick a intermediate node einter and then conduct two BFS between (esource, einter) and (einter, etarget).",3.2 Training Pipeline,[0],[0]
The concatenated paths are used to train the agent.,3.2 Training Pipeline,[0],[0]
The supervised learning saves the agent great efforts learning from failed actions.,3.2 Training Pipeline,[0],[0]
"With the learned experience, we then train the agent to find desirable paths.",3.2 Training Pipeline,[0],[0]
"Retraining with Rewards To find the reasoning paths controlled by the reward functions, we use reward functions to retrain the supervised policy network.",3.2 Training Pipeline,[0],[0]
"For each relation, the reasoning with one entity pair is treated as one episode.",3.2 Training Pipeline,[0],[0]
"Starting with the source node esource, the agent picks a relation according to the stochastic policy π(a|s), which is a probability distribution over all relations, to extend its reasoning path.",3.2 Training Pipeline,[0],[0]
"This relation link may lead to a new entity, or it may lead to nothing.",3.2 Training Pipeline,[0],[0]
These failed steps will cause the agent to receive negative rewards.,3.2 Training Pipeline,[0],[0]
The agent will stay at the same state after these failed steps.,3.2 Training Pipeline,[0],[0]
"Since the agent is following a stochastic policy, the agent will not get stuck by repeating a wrong step.",3.2 Training Pipeline,[0],[0]
"To improve the training efficiency, we limit the episode length with an upper
Algorithm 1: Retraining Procedure with reward functions
1 Restore parameters θ from supervised policy; 2 for episode← 1 to N do 3 Initialize state vector st ← s0 4 Initialize episode length steps← 0 5 while num steps < max length do 6 Randomly sample action a ∼ π(a|st) 7 Observe rewardRt, next state st+1 //",3.2 Training Pipeline,[0],[0]
if the step fails 8 ifRt = −1 then 9,3.2 Training Pipeline,[0],[0]
"Save < st, a > toMneg
10 if success or steps = max length then 11 break 12 Increment num steps
// penalize failed steps 13 Update θ using
g ∝",3.2 Training Pipeline,[0],[0]
∇θ,3.2 Training Pipeline,[0],[0]
"∑ Mneg log π(a = rt|st; θ)(−1)
",3.2 Training Pipeline,[0],[0]
if success then 14 Rtotal ← λ1rGLOBAL + λ2rEFFICIENCY,3.2 Training Pipeline,[0],[0]
"+ λ3rDIVERSITY 15 Update θ using
g ∝",3.2 Training Pipeline,[0],[0]
∇θ,3.2 Training Pipeline,[0],[0]
∑,3.2 Training Pipeline,[0],[0]
"t log π(a = rt|st; θ)Rtotal
boundmax length.",3.2 Training Pipeline,[0],[0]
The episode ends if the agent fails to reach the target entity within max length steps.,3.2 Training Pipeline,[0],[0]
"After each episode, the policy network is updated using the following gradient:
∇θJ(θ) =",3.2 Training Pipeline,[0],[0]
"∇θ ∑ t log π(a = rt|st; θ)Rtotal (3)
where Rtotal is the linear combination of the defined reward functions.",3.2 Training Pipeline,[0],[0]
The detail of the retrain process is shown in Algorithm 1.,3.2 Training Pipeline,[0],[0]
"In practice, θ is updated using the Adam Optimizer (Kingma and Ba, 2014) with L2 regularization.",3.2 Training Pipeline,[0],[0]
"Given an entity pair, the reasoning paths learned by the RL agent can be used as logical formulas to predict the relation link.",3.3 Bi-directional Path-constrained Search,[0],[0]
Each formula is verified using a bi-directional search.,3.3 Bi-directional Path-constrained Search,[0],[0]
"In a typical KG, one entity node can be linked to a large number of neighbors with the same relation link.",3.3 Bi-directional Path-constrained Search,[0],[0]
"A simple example is the relation personNationality−1, which denotes the inverse of personNationality.",3.3 Bi-directional Path-constrained Search,[0],[0]
"Following this link, the entity United States can reach numerous neighboring entities.",3.3 Bi-directional Path-constrained Search,[0],[0]
"If the for-
Algorithm 2: Bi-directional search for path verification
1",3.3 Bi-directional Path-constrained Search,[0],[0]
"Given a reasoning path p : r1 → r2 → ...→ rn 2 for (ei, ej) in test set D do 3 start← 0; end← n 4 left← ∅; right← ∅ 5 while start < end do 6 leftEx← ∅; rightEx← ∅ 7 if len(left) < len(right) then 8 Extend path on the left side 9 Add connected nodes to leftEx
10 left← leftEx 11 else 12 Extend path on the right side 13 Add connected nodes to rightEx 14 right← rightEx 15 if left ∩ right 6= ∅",3.3 Bi-directional Path-constrained Search,[0],[0]
"then 16 return True 17 else 18 return False
mula consists of such links, the number of intermediate entities can exponentially increase as we follow the reasoning formula.",3.3 Bi-directional Path-constrained Search,[0],[0]
"However, we observe that for these formulas, if we verify the formula from the inverse direction.",3.3 Bi-directional Path-constrained Search,[0],[0]
The number of intermediate nodes can be tremendously decreased.,3.3 Bi-directional Path-constrained Search,[0],[0]
Algorithm 2 shows a detailed description of the proposed bi-directional search.,3.3 Bi-directional Path-constrained Search,[0],[0]
"To evaluate the reasoning formulas found by our RL agent, we explore two standard KG reasoning tasks: link prediction (predicting target entities) and fact prediction (predicting whether an unknown fact holds or not).",4 Experiments,[0],[0]
We compare our method with both path-based methods and embedding based methods.,4 Experiments,[0],[0]
"After that, we further analyze the reasoning paths found by our RL agent.",4 Experiments,[0],[0]
These highly predictive paths validate the effectiveness of the reward functions.,4 Experiments,[0],[0]
"Finally, we conduct a experiment to investigate the effect of the supervised learning procedure.",4 Experiments,[0],[0]
Table 1 shows the statistics of the two datasets we conduct our experiments on.,4.1 Dataset and Settings,[0],[0]
"Both of them
are subsets of larger datasets.",4.1 Dataset and Settings,[0],[0]
"The triples in FB15K-237 (Toutanova et al., 2015) are sampled from FB15K (Bordes et al., 2013) with redundant relations removed.",4.1 Dataset and Settings,[0],[0]
We perform the reasoning tasks on 20 relations which have enough reasoning paths.,4.1 Dataset and Settings,[0],[0]
"These tasks consists of relations from different domains like Sports, People, Locations, Film, etc.",4.1 Dataset and Settings,[0],[0]
"Besides, we present a new NELL subset that is suitable for multi-hop reasoning from the 995th iteration of the NELL system.",4.1 Dataset and Settings,[0],[0]
We first remove the triples with relation generalizations or haswikipediaurl.,4.1 Dataset and Settings,[0],[0]
"These two relations appear more than 2M times in the NELL dataset, but they have no reasoning values.",4.1 Dataset and Settings,[0],[0]
"After this step, we only select the triples with Top-200 relations.",4.1 Dataset and Settings,[0],[0]
"To facilitate path finding, we also add the inverse triples.",4.1 Dataset and Settings,[0],[0]
"For each triple (h, r, t), we append (t, r−1, h) to the datasets.",4.1 Dataset and Settings,[0],[0]
"With these inverse triples, the agent is able to step backward in the KG.
",4.1 Dataset and Settings,[0],[0]
"For each reasoning task ri, we remove all the triples with ri or r−1i from the KG.",4.1 Dataset and Settings,[0],[0]
These removed triples are split into train and test samples.,4.1 Dataset and Settings,[0],[0]
"For the link prediction task, each h in the test triples {(h, r, t)} is considered as one query.",4.1 Dataset and Settings,[0],[0]
A set of candidate target entities are ranked using different methods.,4.1 Dataset and Settings,[0],[0]
"For fact prediction, the true test triples are ranked with some generated false triples.",4.1 Dataset and Settings,[0],[0]
Most KG reasoning methods are based on either path formulas or KG embeddings.,4.2 Baselines and Implementation Details,[0],[0]
we explore methods from both of these two classes in our experiments.,4.2 Baselines and Implementation Details,[0],[0]
"For path based methods, we compare our RL model with the PRA (Lao et al., 2011a) algorithm, which has been used in a couple of reasoning methods (Gardner et al., 2013; Neelakantan et al., 2015).",4.2 Baselines and Implementation Details,[0],[0]
PRA is a data-driven algorithm using random walks (RW) to find paths and obtain path features.,4.2 Baselines and Implementation Details,[0],[0]
"For embedding based methods, we evaluate several state-of-the-art embeddings designed for knowledge base completion, such as TransE (Bordes et al., 2013), TransH (Wang et al., 2014), TransR",4.2 Baselines and Implementation Details,[0],[0]
"(Lin et al., 2015) and TransD (Ji et al., 2015) .
",4.2 Baselines and Implementation Details,[0],[0]
"The implementation of PRA is based on the
code released by (Lao et al., 2011a).",4.2 Baselines and Implementation Details,[0],[0]
We use the TopK negative mode to generate negative samples for both train and test samples.,4.2 Baselines and Implementation Details,[0],[0]
"For each positive samples, there are approximately 10 corresponding negative samples.",4.2 Baselines and Implementation Details,[0],[0]
Each negative sample is generated by replacing the true target entity t with a faked one t ′,4.2 Baselines and Implementation Details,[0],[0]
"in each triple (h, r, t).",4.2 Baselines and Implementation Details,[0],[0]
These positive and negative test pairs generated by PRA make up the test set for all methods evaluated in this paper.,4.2 Baselines and Implementation Details,[0],[0]
"For TransE,R,H,D, we learn a separate embedding matrix for each reasoning task using the positive training entity pairs.",4.2 Baselines and Implementation Details,[0],[0]
"All these embeddings are trained for 1,000 epochs.",4.2 Baselines and Implementation Details,[0],[0]
"2
Our RL model make use of TransE to get the continuous representation of the entities and relations.",4.2 Baselines and Implementation Details,[0],[0]
"We use the same dimension as TransE, R to embed the entities.",4.2 Baselines and Implementation Details,[0],[0]
"Specifically, the state vector we use has a dimension of 200, which is also the input size of the policy network.",4.2 Baselines and Implementation Details,[0],[0]
"To reason using the path formulas, we adopt a similar linear regression approach as in PRA to re-rank the paths.",4.2 Baselines and Implementation Details,[0],[0]
"However, instead of using the random walk probabilities as path features, which can be computationally expensive, we simply use binary path features obtained by the bi-directional search.",4.2 Baselines and Implementation Details,[0],[0]
"We observe that with only a few mined path formulas, our method can achieve better results than PRA’s data-driven approach.",4.2 Baselines and Implementation Details,[0],[0]
Link Prediction This task is to rank the target entities given a query entity.,4.3.1 Quantitative Results,[0],[0]
"Table 2 shows the mean average precision (MAP) results on two datasets.
2The implementation we used can be found at https: //github.com/thunlp/Fast-TransX",4.3.1 Quantitative Results,[0],[0]
"Since path-based methods generally work better than embedding methods for this task, we do not include the other two embedding baselines in this table.",RL 0.311 0.493,[0],[0]
"Instead, we spare the room to show the detailed results on each relation reasoning task.
",RL 0.311 0.493,[0],[0]
"For the overall MAP shown in the last row of the table, our approach significantly outperforms both the path-based method and embedding methods on two datasets, which validates the strong reasoning ability of our RL model.",RL 0.311 0.493,[0],[0]
"For most relations, since the embedding methods fail to use the path infor-
mation in the KG, they generally perform worse than our RL model or PRA.",RL 0.311 0.493,[0],[0]
"However, when there are not enough paths between entities, our model and PRA can give poor results.",RL 0.311 0.493,[0],[0]
"For example, for the relation filmWrittenBy, our RL model only finds 4 unique reasoning paths, which means there is actually not enough reasoning evidence existing in the KG.",RL 0.311 0.493,[0],[0]
Another observation is that we always get better performance on the NELL dataset.,RL 0.311 0.493,[0],[0]
"By analyzing the paths found from the KGs, we believe the potential reason is that the NELL dataset has more short paths than FB15K-237 and some of them are simply synonyms of the reasoning relations.",RL 0.311 0.493,[0],[0]
"Fact Prediction Instead of ranking the target entities, this task directly ranks all the positive and negative samples for a particular relation.",RL 0.311 0.493,[0],[0]
"The PRA is not included as a baseline here, since the PRA code only gives a target entity ranking for each query node instead of a ranking of all triples.",RL 0.311 0.493,[0],[0]
Table 3 shows the overall results of all the methods.,RL 0.311 0.493,[0],[0]
Our RL model gets even better results on this task.,RL 0.311 0.493,[0],[0]
We also observe that the RL model beats all the embedding baselines on most reasoning tasks.,RL 0.311 0.493,[0],[0]
"To analyze the properties of reasoning paths, we show a few reasoning paths found by the agent in Table 5.",4.3.2 Qualitative Analysis of Reasoning Paths,[0],[0]
"To illustrate the effect of the efficiency reward function, we show the path length distributions in Figure 2.",4.3.2 Qualitative Analysis of Reasoning Paths,[0],[0]
"To interpret these paths, take the personNationality relation for example, the first reasoning path indicates that if we know facts placeOfBirth(x,y) and locationContains(z,y) then it is highly possible that person x has nationality z.",4.3.2 Qualitative Analysis of Reasoning Paths,[0],[0]
These short but predictive paths indicate the effectiveness of the RL model.,4.3.2 Qualitative Analysis of Reasoning Paths,[0],[0]
"Another important observation is that our model use much
fewer reasoning paths than PRA, which indicates that our model can actually extract the most reliable reasoning evidence from KG.",4.3.2 Qualitative Analysis of Reasoning Paths,[0],[0]
Table 4 shows some comparisons about the number of reasoning paths.,4.3.2 Qualitative Analysis of Reasoning Paths,[0],[0]
"We can see that, with the pre-defined reward functions, the RL agent is capable of picking the strong ones and filter out similar or irrelevant ones.",4.3.2 Qualitative Analysis of Reasoning Paths,[0],[0]
"As mentioned in Section 3.2, one major challenge for applying RL to KG reasoning is the large action space.",4.3.3 Effect of Supervised Learning,[0],[0]
We address this issue by applying supervised learning before the reward retraining step.,4.3.3 Effect of Supervised Learning,[0],[0]
"To show the effect of the supervised training, we evaluate the agent’s success ratio of reaching the target within 10 steps (succ10) after different number of training episodes.",4.3.3 Effect of Supervised Learning,[0],[0]
"For each training episode, one pair of entities (esource, etarget) in the train set is used to find paths.",4.3.3 Effect of Supervised Learning,[0],[0]
All the correct paths linking the entities will get a +1 global reward.,4.3.3 Effect of Supervised Learning,[0],[0]
We then plug in some true paths for training.,4.3.3 Effect of Supervised Learning,[0],[0]
The succ10 is calculated on a held-out test set that consists of 100 entity pairs.,4.3.3 Effect of Supervised Learning,[0],[0]
"For the NELL995 dataset, since we have 200 unique relations, the dimension of the action space will be 400 after we add the backward actions.",4.3.3 Effect of Supervised Learning,[0],[0]
This means that random walks will get very low succ10 since there may be nearly 40010 invalid paths.,4.3.3 Effect of Supervised Learning,[0],[0]
Figure 3 shows the succ10 during training.,4.3.3 Effect of Supervised Learning,[0],[0]
"We see that even the agent has not seen the entity before, it can actually pick the promising relation to extend its path.",4.3.3 Effect of Supervised Learning,[0],[0]
"This also validates the effectiveness of our state representations.
",4.3.3 Effect of Supervised Learning,[0],[0]
3The confidence band is generated using 50 different runs.,4.3.3 Effect of Supervised Learning,[0],[0]
"In this paper, we propose a reinforcement learning framework to improve the performance of relation reasoning in KGs.",5 Conclusion and Future Work,[0],[0]
"Specifically, we train a RL agent to find reasoning paths in the knowledge base.",5 Conclusion and Future Work,[0],[0]
"Unlike previous path finding models that are based on random walks, the RL model allows us to control the properties of the found paths.",5 Conclusion and Future Work,[0],[0]
These effective paths can also be used as an alternative to PRA in many path-based reasoning methods.,5 Conclusion and Future Work,[0],[0]
"For two standard reasoning tasks, using the RL paths as reasoning formulas, our approach generally outperforms two classes of baselines.
",5 Conclusion and Future Work,[0],[0]
"For future studies, we plan to investigate the possibility of incorporating adversarial learning (Goodfellow et al., 2014) to give better rewards than the human-defined reward functions used in this work.",5 Conclusion and Future Work,[0],[0]
"Instead of designing rewards according to path characteristics, a discriminative model can be trained to give rewards.",5 Conclusion and Future Work,[0],[0]
"Also, to address the problematic scenario when the KG does not have enough reasoning paths, we are interested in applying our RL framework to joint reasoning with KG triples and text mentions.",5 Conclusion and Future Work,[0],[0]
We study the problem of learning to reason in large scale knowledge graphs (KGs).,abstractText,[0],[0]
"More specifically, we describe a novel reinforcement learning framework for learning multi-hop relational paths: we use a policy-based agent with continuous states based on knowledge graph embeddings, which reasons in a KG vector space by sampling the most promising relation to extend its path.",abstractText,[0],[0]
"In contrast to prior work, our approach includes a reward function that takes the accuracy, diversity, and efficiency into consideration.",abstractText,[0],[0]
"Experimentally, we show that our proposed method outperforms a path-ranking based algorithm and knowledge graph embedding methods on Freebase and Never-Ending Language Learning datasets.1",abstractText,[0],[0]
DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 433–438 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,[0],[0]
"Models tackling vision-to-language (V2L) tasks, for example Image Captioning (IC) and Visual Question Answering (VQA), have demonstrated impressive results in recent years in terms of automatic metric scores.",1 Introduction,[0],[0]
"However, whether or not these models are actually learning to address the tasks they are designed for is questionable.",1 Introduction,[0],[0]
"For example, Hodosh and Hockenmaier (2016) showed that IC models do not understand images sufficiently, as reflected by the generated captions.",1 Introduction,[0],[0]
"As a consequence, in the last few years many diagnostic tasks and datasets have been proposed aiming at investigating the capabilities of such models in more detail to determine whether and how these models are capable of exploiting visual and/or linguistic information (Shekhar et al., 2017b; John-
son et al., 2017; Antol et al., 2015; Chen et al., 2015; Gao et al., 2015; Yu et al., 2015; Zhu et al., 2016).
FOIL (Shekhar et al., 2017b) is one such dataset.",1 Introduction,[0],[0]
It was proposed to evaluate the ability of V2L models in understanding the interplay of objects and their attributes in the images and their relations in an image captioning framework.,1 Introduction,[0],[0]
"This is done by replacing a word in MSCOCO (Lin et al., 2014) captions with a ‘foiled’ word that is semantically similar or related to the original word (substituting dog with cat), thus rendering the image caption unfaithful to the image content, while yet linguistically valid.",1 Introduction,[0],[0]
Shekhar et al. (2017b) report poor performance for V2L models in classifying captions as foiled (or not).,1 Introduction,[0],[0]
"They suggested that their models (using image embeddings as input) are very poor at encoding structured visuallinguistic information to spot the mismatch between a foiled caption and the corresponding content depicted in the image.
",1 Introduction,[0],[0]
"In this paper, we focus on the foiled captions classification task (Section 2), and propose the use of explicit object detections as salient image cues for solving the task.",1 Introduction,[0],[0]
"In contrast to methods from previous work that make use of word based information extracted from captions (Heuer et al., 2016; Yao et al., 2016; Wu et al., 2018), we use explicit object category information directly extracted from the images.",1 Introduction,[0],[0]
"More specifically, we use an interpretable bag of objects as image representation for the classifier.",1 Introduction,[0],[0]
"Our hypothesis is that, to truly ‘understand’ the image, V2L models should exploit information about objects and their relations in the image and not just global, low-level image embeddings as used by most V2L models.
",1 Introduction,[0],[0]
"Our main contributions are:
1.",1 Introduction,[0],[0]
"A model (Section 3) for foiled captions classification using a simple and interpretable
433
object-based representation, which leads to the best performance in the task (Section 4);
2.",1 Introduction,[0],[0]
"Insights on upper-bound performance for foiled captions classification using gold standard object annotations (Section 4);
3.",1 Introduction,[0],[0]
"An analysis of the models, providing insights into the reasons for their strong performance (Section 5).
",1 Introduction,[0],[0]
"Our results reveal that the FOIL dataset has a very strong linguistic bias, and that the proposed simple object-based models are capable of finding salient patterns to solve the task.",1 Introduction,[0],[0]
"In this section we describe the foiled caption classification task and dataset.
",2 Background,[0],[0]
We combine the tasks and data from Shekhar et al. (2017b) and Shekhar et al. (2017a).,2 Background,[0],[0]
"Given an image and a caption, in both cases the task is to learn a model that can distinguish between a REAL caption that describes the image, and a FOILed caption where a word from the original caption is swapped such that it no longer describes the image accurately.",2 Background,[0],[0]
"There are several sets of ‘foiled captions’ where words from specific parts of speech are swapped:
• Foiled Noun: In this case a noun word in the original caption is replaced with another similar noun, such that the resultant caption is not the correct description for the image.",2 Background,[0],[0]
"The foiled noun is obtained from list of object annotations from MSCOCO (Lin et al., 2014) and nouns are constrained to the same supercategory;
",2 Background,[0],[0]
"• Foiled Verb: Here, verb is foiled with a similar verb.",2 Background,[0],[0]
"The similar verb is extracted using external resources;
• Foiled Adjective and Adverb: Adjectives and adverbs are replaced with similar adjectives and adverbs.",2 Background,[0],[0]
"Here, the notion of similarity again is obtained from external resources;
• Foiled Preposition: Prepositions are directly replaced with functionally similar prepositions.
",2 Background,[0],[0]
"The Verb, Adjective, Adverb and Preposition subsets were obtained using a slightly different
methodology (see Shekhar et al. (2017a))",2 Background,[0],[0]
"than that used for Nouns (Shekhar et al., 2017b).",2 Background,[0],[0]
"Therefore, we evaluate these two groups separately.",2 Background,[0],[0]
"For the foiled caption classification task (Section 3.1), our proposed model uses information from explicit object detections as an object-based image representation along with textual representations (Section 3.2) as input to several different classifiers (Section 3.3).",3 Proposed Model,[0],[0]
"Let y ∈ {REAL, FOIL} denote binary class labels.",3.1 Model definition,[0],[0]
"The objective is to learn a model that computes P (y|I;C), where I and C correspond to the image and caption respectively.",3.1 Model definition,[0],[0]
"Our model seeks to maximize a scoring function θ:
y = argmax θ(I;C)",3.1 Model definition,[0],[0]
(1),3.1 Model definition,[0],[0]
Our scoring function θ takes in image features and text features (from captions) and concatenates them.,3.2 Representations,[0],[0]
"We experiment with various types of features.
",3.2 Representations,[0],[0]
"For the image side, we propose a bag of objects representation for 80 pre-defined MSCOCO categories.",3.2 Representations,[0],[0]
"We consider two variants: (a) Object Mention: A binary vector where we encode the presence/absence of instances of each object category for a given image; (b) Object Frequency: A histogram vector where we encode the number of instances of each object category in a given image.
",3.2 Representations,[0],[0]
"For both features, we use Gold MSCOCO object annotations as well as Predicted object detections using YOLO (Redmon and Farhadi, 2017) pre-trained on MSCOCO to detect instances of the 80 categories.
",3.2 Representations,[0],[0]
"As comparison, we also compute a standard CNN-based image representation, using the POOL5 layer of a ResNet-152 (He et al., 2016)",3.2 Representations,[0],[0]
CNN pre-trained on ImageNet.,3.2 Representations,[0],[0]
"We posit that our object-based representation will better capture semantic information corresponding to the text compared to the CNN embeddings used directly as a feature by most V2L models.
",3.2 Representations,[0],[0]
"For the language side, we explore two features: (a) a simple bag of words (BOW) representation for each caption; (b) an LSTM classifier based model trained on the training part of the dataset.
",3.2 Representations,[0],[0]
"Our intuition is that an image description/caption is essentially a result of the interaction between important objects in the image (this includes spatial relations, co-occurrences, etc.).",3.2 Representations,[0],[0]
"Thus, representations explicitly encoding objectlevel information are better suited for the foiled caption classification task.",3.2 Representations,[0],[0]
Three types of classifiers are explored: (a) Multilayer Perceptron (MLP):,3.3 Classifiers,[0],[0]
"For BOW-based text representations, a two 100-dimensional hidden layer MLP with ReLU activation function is used with cross-entropy loss, and is optimized with Adam (learning rate 0.001); (b) LSTM Classifier: For LSTM-based text representations, a uni-directional LSTM classifier is used with 100-dimensional word embeddings and 200- dimensional hidden representations.",3.3 Classifiers,[0],[0]
We train it using cross-entropy loss and optimize it using Adam (learning rate 0.001).,3.3 Classifiers,[0],[0]
Image representations are appended to the final hidden state of the LSTM; (c) Multimodal LSTM (MM-LSTM),3.3 Classifiers,[0],[0]
"Classifier: As above, except that we initialize the LSTM with the image representation instead of appending it to its output.",3.3 Classifiers,[0],[0]
This can also be seen as am image grounded LSTM based classifier.,3.3 Classifiers,[0],[0]
Data:,4 Experiments,[0],[0]
We use the dataset for nouns from Shekhar et al. (2017b)1 and the datasets for other parts of speech from Shekhar et al. (2017a) 2.,4 Experiments,[0],[0]
Statistics about the dataset are given in Table 1.,4 Experiments,[0],[0]
"The evaluation metric is accuracy per class and the average (overall) accuracy over the two classes.
",4 Experiments,[0],[0]
Performance on nouns: The results of our experiments with foiled nouns are summarized in Table 2.,4 Experiments,[0],[0]
"First, we note that the models that use Gold
1https://foilunitn.github.io/ 2The authors have kindly provided us the datasets.
bag of objects information are the best performing models across classifiers.",4 Experiments,[0],[0]
We also note that the performance is better than human performance.,4 Experiments,[0],[0]
"We hypothesize the following reasons for this: (a) human responses were crowd-sourced, which could have resulted in some noisy annotations; (b) our gold object-based features closely resembles the information used for data-generation as described in Shekhar et al. (2017b) for the foil noun dataset.",4 Experiments,[0],[0]
The models using Predicted bag of objects from a detector are very close to the performance of Gold.,4 Experiments,[0],[0]
The performance of models using simple bag of words (BOW) sentence representations and an MLP is better than that of models that use LSTMs.,4 Experiments,[0],[0]
"Also, the accuracy of the bag of objects model with Frequency counts is higher than with the binary Mention vector, which only encodes the presence of objects.",4 Experiments,[0],[0]
The Multimodal LSTM (MM-LSTM) has a slightly better performance than LSTM classifiers.,4 Experiments,[0],[0]
"In all cases, we observe that the performance is on par with human-level accuracy.",4 Experiments,[0],[0]
Our overall accuracy is substantially higher than that reported in Shekhar et al. (2017b).,4 Experiments,[0],[0]
"Interestingly, our implementation of CNN+LSTM produced better results than their equivalent model (they reported 61.07% vs. our 87.45%).",4 Experiments,[0],[0]
"We investigate this further in Section 5.
",4 Experiments,[0],[0]
"Performance on other parts of speech: For other parts of speech, we fix the image representation to Gold Frequency, and compare results using the BOW-based MLP and MM-LSTM.",4 Experiments,[0],[0]
We also compare the scores to the state of the art reported in Shekhar et al. (2017a).,4 Experiments,[0],[0]
"Note that this
model does not use gold object information and may thus not be directly comparable – we however recall that only a slight drop in accuracy was found for our models when using predicted object detections rather than gold ones.",4 Experiments,[0],[0]
Our findings are summarized in Table 3.,4 Experiments,[0],[0]
The classification performance is not as high as it was for the nouns dataset.,4 Experiments,[0],[0]
"Noteworthy is the performance on adverbs, which is significantly lower than the performance across other parts of speech.",4 Experiments,[0],[0]
We hypothesize that this is because of the imbalanced distribution of foiled and real captions in the dataset.,4 Experiments,[0],[0]
"We also found that the performance of LSTM-based models on other parts of speech datasets are almost always better than BOW-based models, indicating the necessity of more sophisticated features.",4 Experiments,[0],[0]
"In this section, we attempt to better understand why our models achieve such a high accuracy.",5 Analysis,[0],[0]
We first perform ablation experiments with our proposed models over the Nouns dataset (FOIL).,5.1 Ablation Analysis,[0],[0]
"We compute image-only models (CNN or Gold Frequency) and text-only models (BOW or LSTM), and investigate which components of our model (text or image/objects) contribute to the strong classification performance (Table 4).",5.1 Ablation Analysis,[0],[0]
"As expected, we cannot classify foiled captions given only image information (global or object-level), resulting in chance-level performance.
",5.1 Ablation Analysis,[0],[0]
"On the other hand, text-only models achieve a
very high accuracy.",5.1 Ablation Analysis,[0],[0]
"This is a central finding, suggesting that foiled captions are easy to detect even without image information.",5.1 Ablation Analysis,[0],[0]
"We also observe that the performance of BOW improves by adding object Frequency image information, but not CNN image embeddings.",5.1 Ablation Analysis,[0],[0]
We posit that this is because there is a tighter correspondence between the bag of objects and bag of word models.,5.1 Ablation Analysis,[0],[0]
"In the case of LSTMs, adding either image information helps slightly.",5.1 Ablation Analysis,[0],[0]
"The accuracy of our models is substantially higher than that reported in Shekhar et al. (2017b), even for equivalent models.
",5.1 Ablation Analysis,[0],[0]
"We note, however, that while the trends of image information is similar for other parts of speech datasets, the performance of BOW based models are lower than the performance of LSTM based models.",5.1 Ablation Analysis,[0],[0]
The anomaly of improved performance of BOW based models seems heavily pronounced in the nouns dataset.,5.1 Ablation Analysis,[0],[0]
"Thus, we further analyze our model in the next section to shed light on whether the high performance is due to the models or the dataset itself.",5.1 Ablation Analysis,[0],[0]
"We apply Local Interpretable Model-agnostic Explanations (Ribeiro et al., 2016) to further understand the strong performance of our simple classifier on the Nouns dataset (FOIL) without any image information.",5.2 Feature Importance Analysis,[0],[0]
We present an example in Figure 1.,5.2 Feature Importance Analysis,[0],[0]
We use MLP with BOW only (no image information) as our classifier.,5.2 Feature Importance Analysis,[0],[0]
"As the caption is correctly predicted to be foiled, we observe that the most important feature for classification is the information on the word ball, which also happens to be the foiled word.",5.2 Feature Importance Analysis,[0],[0]
We further analyzed the chances of this happening on the entire test set.,5.2 Feature Importance Analysis,[0],[0]
We found that 96.56% of the time the most important classification feature happens to be the foiled word.,5.2 Feature Importance Analysis,[0],[0]
"This firmly indicates that there is a very strong linguistic bias in the training data, despite
the claim in Shekhar et al. (2017b) that special attention was paid to avoid linguistic biases in the dataset.3",5.2 Feature Importance Analysis,[0],[0]
We note that we were not able to detect the linguistic bias in the other parts of speech datasets.,5.2 Feature Importance Analysis,[0],[0]
We presented an object-based image representation derived from explicit object detectors/gold annotations to tackle the task of classifying foiled captions.,6 Conclusions,[0],[0]
"The hypothesis was that such models provide the necessary semantic information for the task, while this informaiton is not explicitly present in CNN image embeddings commonly used in V2L tasks.",6 Conclusions,[0],[0]
"We achieved stateof-the-art performance on the task, and also provided a strong upper-bound using gold annotations.",6 Conclusions,[0],[0]
"A significant finding is that our simple models, especially for the foiled noun dataset, perform well even without image information.",6 Conclusions,[0],[0]
"This could be partly due to the strong linguistic bias in the foiled noun dataset, which was revealed by our analysis on our interpretable object-based models.",6 Conclusions,[0],[0]
We release our analysis and source code at https://github.com/ sheffieldnlp/foildataset.git.,6 Conclusions,[0],[0]
This work is supported by the MultiMT project (H2020 ERC Starting Grant,Acknowledgments,[0],[0]
No. 678017).,Acknowledgments,[0],[0]
"The authors also thank the anonymous reviewers for their valuable feedback on an earlier draft of the paper.
",Acknowledgments,[0],[0]
3Shekhar et al. (2017b) have acknowledged about the bias in our personal communications and are currently working on a fix,Acknowledgments,[0],[0]
"We address the task of detecting foiled image captions, i.e. identifying whether a caption contains a word that has been deliberately replaced by a semantically similar word, thus rendering it inaccurate with respect to the image being described.",abstractText,[0],[0]
Solving this problem should in principle require a fine-grained understanding of images to detect linguistically valid perturbations in captions.,abstractText,[0],[0]
"In such contexts, encoding sufficiently descriptive image information becomes a key challenge.",abstractText,[0],[0]
"In this paper, we demonstrate that it is possible to solve this task using simple, interpretable yet powerful representations based on explicit object information.",abstractText,[0],[0]
"Our models achieve stateof-the-art performance on a standard dataset, with scores exceeding those achieved by humans on the task.",abstractText,[0],[0]
We also measure the upperbound performance of our models using gold standard annotations.,abstractText,[0],[0]
"Our analysis reveals that the simpler model performs well even without image information, suggesting that the dataset contains strong linguistic bias.",abstractText,[0],[0]
Defoiling Foiled Image Captions,title,[0],[0]
"Machine learning commonly considers static objectives defined on a snapshot of the population at one instant in time; consequential decisions, in contrast, reshape the population over time.",1 Introduction,[0],[0]
"Lending practices, for example, can shift the distribution of debt and wealth in the population.",1 Introduction,[0],[0]
Job advertisements allocate opportunity.,1 Introduction,[0],[0]
"School admissions shape the level of education in a community.
",1 Introduction,[0],[0]
Existing scholarship on fairness in automated decisionmaking criticizes unconstrained machine learning for its potential to harm historically underrepresented or disadvantaged groups in the population,1 Introduction,[0],[0]
"[Executive Office of the President, 2016; Barocas and Selbst, 2016].",1 Introduction,[0],[0]
"Consequently, a variety of fairness criteria have been proposed as constraints on standard learning objectives.",1 Introduction,[0],[0]
"Even though, in each case, these constraints are clearly intended to protect the disadvantaged group by an appeal to intuition, a rigorous argument to that effect is often lacking.
",1 Introduction,[0],[0]
"In this work, we formally examine under what circumstances fairness criteria do indeed promote the long-term well-being of disadvantaged groups measured in terms of a temporal variable of interest.",1 Introduction,[0],[0]
"Going beyond the standard classification setting, we introduce a one-step feedback model of
∗This paper is an abridged version of the paper of the same name which appeared at the 35th International Conference of Machine Learning",1 Introduction,[0],[0]
"[Liu et al., 2018].",1 Introduction,[0],[0]
"The interested reader is referred to the full version for extended results and discussion.
decision-making that exposes how decisions change the underlying population over time.
",1 Introduction,[0],[0]
Our running example is a hypothetical lending scenario.,1 Introduction,[0],[0]
"There are two groups in the population with features described by a summary statistic, such as a credit score, whose distribution differs between the two groups.",1 Introduction,[0],[0]
The bank can choose thresholds for each group at which loans are offered.,1 Introduction,[0],[0]
"While group-dependent thresholds may face legal challenges [Ross and Yinger, 2006], they are generally inevitable for some of the criteria we examine.",1 Introduction,[0],[0]
The impact of a lending decision has multiple facets.,1 Introduction,[0],[0]
"A default event not only diminishes profit for the bank, it also worsens the financial situation of the borrower as reflected in a subsequent decline in credit score.",1 Introduction,[0],[0]
"A successful lending outcome leads to profit for the bank and also to an increase in credit score for the borrower.
",1 Introduction,[0],[0]
"When thinking of one of the two groups as disadvantaged, it makes sense to ask what lending policies (choices of thresholds) lead to an expected improvement in the score distribution within that group.",1 Introduction,[0],[0]
"An unconstrained bank would maximize profit, choosing thresholds that meet a break-even point above which it is profitable to give out loans.",1 Introduction,[0],[0]
"One frequently proposed fairness criterion, sometimes called demographic parity, requires the bank to lend to both groups at an equal rate.",1 Introduction,[0],[0]
Subject to this requirement the bank would continue to maximize profit to the extent possible.,1 Introduction,[0],[0]
"Another criterion, originally called equality of opportunity, equalizes the true positive rates between the two groups, thus requiring the bank to lend in both groups at an equal rate among individuals who repay their loan.",1 Introduction,[0],[0]
"Other criteria are natural, but for clarity we restrict our attention to these three.
",1 Introduction,[0],[0]
Do these fairness criteria benefit the disadvantaged group?,1 Introduction,[0],[0]
When do they show a clear advantage over unconstrained classification?,1 Introduction,[0],[0]
Under what circumstances does profit maximization work in the interest of the individual?,1 Introduction,[0],[0]
These are important questions that we begin to address in this work.,1 Introduction,[0],[0]
We introduce a one-step feedback model that allows for the quantification of the long-term impact of classification on different groups in the population.,2 Problem Setting,[0],[0]
"Individuals are assigned scores in X := {1, . . .",2 Problem Setting,[0],[0]
", C}, where a score highlights one variable of interest in a specific domain such that higher score values correspond to a higher probability of a positive outcome.",2 Problem Setting,[0],[0]
"This score is used by an institution, which makes a
binary decision for each individual in each group.",2 Problem Setting,[0],[0]
The institution designs selection policies τ :,2 Problem Setting,[0],[0]
"X → [0, 1] that assign to each possible score a number representing the rate of selection for that value.",2 Problem Setting,[0],[0]
"In our example, these policies specify the lending rate at a given credit score.",2 Problem Setting,[0],[0]
"We consider policies designed to maximize the utility of the institution, potentially subject to fairness constraints.
",2 Problem Setting,[0],[0]
"To measure the impact of decisions, we assume the availability of a function ∆ : X → R that provides the expected change in score for a selected individual at a given score.",2 Problem Setting,[0],[0]
The central quantity we study is the expected difference ∆µ in the mean score that results from the selection policy.,2 Problem Setting,[0],[0]
"When modeling the problem, the expected mean difference can also absorb external factors so long as they are mean-preserving.
",2 Problem Setting,[0],[0]
We focus on the impact of a selection policy over a single epoch.,2 Problem Setting,[0],[0]
The motivation is that the designer of a system usually has an understanding of the time horizon after which the system is evaluated and possibly redesigned.,2 Problem Setting,[0],[0]
"Formally, nothing prevents the repeated application of our model and to trace changes over multiple epochs.",2 Problem Setting,[0],[0]
"In reality, however, it is plausible that over greater time periods, economic background variables might dominate the effect of selection.
",2 Problem Setting,[0],[0]
"To compare the impact of classification for different groups, we consider two groups A and B, which comprise a gA and gB = 1 − gA fraction of the total population.",2 Problem Setting,[0],[0]
"We use subscripts on previously defined quantities to denote the group-specific values, e.g. πA denotes the distribution of A over scores.",2 Problem Setting,[0],[0]
"We assume that that there exists a function u : X → R, such that the institution’s expected utility for a policy τ is additive over individuals:
U(τ ) = ∑ j∈{A,B} gj ∑",2 Problem Setting,[0],[0]
x∈X τ j(x)πj(x)u(x).,2 Problem Setting,[0],[0]
"(1)
Then we consider how the outcome of the decision differs between groups.",2 Problem Setting,[0],[0]
"The average change of the mean score µj for group j is given by
∆µj(τ )",2 Problem Setting,[0],[0]
:= ∑ x∈X πj(x)τ j(x)∆(x) .,2 Problem Setting,[0],[0]
"(2)
We remark that many of our results also go through if ∆µj(τ ) simply refers to an abstract change in group well-being, not necessarily a change in the mean score.",2 Problem Setting,[0],[0]
"Lastly, we assume that the success of an individual is independent of their group given the score; that is, the score summarizes all relevant information about the success event, so there exists a function ρ :",2 Problem Setting,[0],[0]
"X → [0, 1] such that individuals of score x succeed with probability ρ(x).
",2 Problem Setting,[0],[0]
Example 2.1 (Credit scores).,2 Problem Setting,[0],[0]
"In the setting of loans, scores x ∈",2 Problem Setting,[0],[0]
"[C] represent credit scores, and the bank serves as the institution.",2 Problem Setting,[0],[0]
The bank chooses to grant or refuse loans to individuals according to a policy τ .,2 Problem Setting,[0],[0]
"Both the profit and the change in credit score are given as functions of loan repayment, and therefore depend on the success probabilities ρ(x), representing the probability that any individual with credit score x can repay a loan within a fixed time frame.",2 Problem Setting,[0],[0]
"The expected utility to the bank is given by the expected return from a loan, which can be modeled as an affine function of ρ(x): u(x) = u+ρ(x) + u−(1 − ρ(x)), where u+ denotes the profit when loans are repaid and u− the loss when they are defaulted on.",2 Problem Setting,[0],[0]
"Individual outcomes of being granted a loan
OUTCOME CURVE
are based on whether or not an individual repays the loan, and a simple model for ∆(x) may also be affine in ρ(x): ∆(x) = c+ρ(x) + c−(1 − ρ(x)), modified accordingly at boundary states.",2 Problem Setting,[0],[0]
The constant c+ > 0 denotes the gain in credit score if loans are repaid and c− < 0,2 Problem Setting,[0],[0]
is the score penalty in case of default.,2 Problem Setting,[0],[0]
"We now introduce important outcome regimes, stated in terms of the change in average group score.",2.1 The Outcome Curve,[0],[0]
"In particular, we focus on these outcomes for a disadvantaged group, and from this point forward, we take A to be the disadvantaged or protected group.",2.1 The Outcome Curve,[0],[0]
We denote the policy that maximizes the institution’s utility in the absence of constraints as MaxUtil.,2.1 The Outcome Curve,[0],[0]
"Under our model, MaxUtil policies can be chosen in a standard fashion which applies the same threshold τ",2.1 The Outcome Curve,[0],[0]
"MaxUtil for both groups, and is agnostic to the distributions πA and πB.",2.1 The Outcome Curve,[0],[0]
"Hence, if we define
∆µMaxUtilj := ∆µj(τ MaxUtil) (3)
we say that a policy causes relative harm to the protected group if ∆µA(τA)",2.1 The Outcome Curve,[0],[0]
<,2.1 The Outcome Curve,[0],[0]
"∆µ MaxUtil A , relative improvement if ∆µA(τA) >",2.1 The Outcome Curve,[0],[0]
"∆µ MaxUtil A , and active harm if ∆µA(τA)",2.1 The Outcome Curve,[0],[0]
"< 0.
",2.1 The Outcome Curve,[0],[0]
Figure 1 displays the important outcome regimes in terms of selection rates βj := ∑ x∈X πj(x)τ j(x).,2.1 The Outcome Curve,[0],[0]
"This succinct characterization is possible when considering decision rules based on score thresholding, in which all individuals with scores above a threshold are selected.",2.1 The Outcome Curve,[0],[0]
"To explicitly connect selection rates to decision policies, we define the rate function rπj(τ j) which returns the proportion of group j selected by the policy.",2.1 The Outcome Curve,[0],[0]
"In the following, we will abuse notation to abbreviate ∆µj(r −1",2.1 The Outcome Curve,[0],[0]
πj (β)) as ∆µj(β).,2.1 The Outcome Curve,[0],[0]
Now we define the values of β that mark boundaries of the outcome regions: Definition 2.1 (Selection rates of interest).,2.1 The Outcome Curve,[0],[0]
"Given the protected group A, the following selection rates are of interest in distinguishing between qualitatively different classes of outcomes (Figure 1): βMaxUtil is the selection rate for A under MaxUtil; β0 is the harm threshold, such that ∆µA(β0) = 0; β∗ is the selection rate such that ∆µA is maximized; β
is the outcome-complement of the MaxUtil selection rate, ∆µA(β) = ∆µA(β MaxUtil) with β ≥ βMaxUtil.",2.1 The Outcome Curve,[0],[0]
"We will consider policies that maximize the institution’s total expected utility, potentially subject to a constraint set C which enforces some notion of “fairness”.",2.2 Decision Rules and Fairness Criteria,[0],[0]
"Formally, the institution selects τ∗ ∈ argmax U(τ ) s.t. τ ∈ C.",2.2 Decision Rules and Fairness Criteria,[0],[0]
We consider the three following constraints: Definition 2.2 (Fairness criteria).,2.2 Decision Rules and Fairness Criteria,[0],[0]
"The maximum utility (MaxUtil) policy corresponds to the null-constraint, so that the institution is free to focus solely on utility.",2.2 Decision Rules and Fairness Criteria,[0],[0]
The demographic parity (DemParity) policy results in equal selection rates between both groups.,2.2 Decision Rules and Fairness Criteria,[0],[0]
"Formally, the constraint is C = { (τA, τB) : ∑ x∈X πA(x)τA = ∑ x∈X πB(x)τB } .",2.2 Decision Rules and Fairness Criteria,[0],[0]
"The equal opportunity (EqOpt) policy results in equal true positive rates (TPR) between both group, where TPR is defined as TPRj(τ )",2.2 Decision Rules and Fairness Criteria,[0],[0]
:= ∑ x∈X πj(x)ρ(x)τ,2.2 Decision Rules and Fairness Criteria,[0],[0]
"(x)∑
x∈X πj(x)ρ(x) .",2.2 Decision Rules and Fairness Criteria,[0],[0]
"EqOpt en-
sures that the conditional probability of selection given that the individual will be successful is independent of the population, formally enforced by the constraint C = {(τA, τB) : TPRA(τA) = TPRB(τB)} .
",2.2 Decision Rules and Fairness Criteria,[0],[0]
"Just as the expected outcome ∆µ can be expressed in terms of selection rate for threshold policies, so can the total utility U .",2.2 Decision Rules and Fairness Criteria,[0],[0]
"In the unconstrained case, U varies independently over the selection rates for group A and B; however, in the presence of fairness constraints the selection rate for one group determines the allowable selection rate for the other.",2.2 Decision Rules and Fairness Criteria,[0],[0]
"The selection rates must be equal for DemParity, and for EqOpt there is a one-to-one mapping.",2.2 Decision Rules and Fairness Criteria,[0],[0]
"Therefore, when considering threshold policies, decision rules amount to maximizing functions of single parameters.",2.2 Decision Rules and Fairness Criteria,[0],[0]
"This idea is expressed in Figure 2, and underpins the results to follow.",2.2 Decision Rules and Fairness Criteria,[0],[0]
"In order to clearly characterize the outcome of applying fairness constraints, we make the following assumption.
",3 Results,[0],[0]
Assumption 1 (Institution utilities).,3 Results,[0],[0]
"The institution’s individual utility function is more stringent than the expected score changes, u(x)",3 Results,[0],[0]
> 0,3 Results,[0],[0]
=⇒ ∆(x) > 0.,3 Results,[0],[0]
"(For the linear form presented in Example 2.1, u−u+ < c− c+
is necessary and sufficient.)
",3 Results,[0],[0]
This simplifying assumption quantifies the intuitive notion that institutions take a greater risk by accepting than the individual does by applying.,3 Results,[0],[0]
"For example, in the credit setting, a bank loses the amount loaned in the case of a default, but makes only interest in case of a payback.",3 Results,[0],[0]
"Using Assumption 1, we can restrict the position of MaxUtil on the outcome curve in the following sense.",3 Results,[0],[0]
Proposition 3.1 (MaxUtil does not cause active harm).,3 Results,[0],[0]
"Under Assumption 1, 0 ≤ ∆µMaxUtil ≤ ∆µ∗.
We direct the reader to the full version of this paper",3 Results,[0],[0]
"[Liu et al., 2018] for the proof of the above proposition, and all subsequent theorems presented in this section.",3 Results,[0],[0]
We begin by characterizing general settings under which fairness criteria act to improve outcomes over unconstrained MaxUtil strategies.,3.1 Prospects and Pitfalls of Fairness Criteria,[0],[0]
Proposition 3.2 (Fairness criteria can cause relative improvement).,3.1 Prospects and Pitfalls of Fairness Criteria,[0],[0]
Assume that group A is disadvantaged in the sense that the MaxUtil acceptance rate for B is large compared to relevant acceptance rates for A.,3.1 Prospects and Pitfalls of Fairness Criteria,[0],[0]
"Then there are general settings under which g0, g1, g2, g3 exist such that (a) DemParity causes relative improvement as long as gA ∈",3.1 Prospects and Pitfalls of Fairness Criteria,[0],[0]
"[g0, g1], and (b) EqOpt causes relative improvement as long as gA ∈",3.1 Prospects and Pitfalls of Fairness Criteria,[0],[0]
"[g2, g3].
",3.1 Prospects and Pitfalls of Fairness Criteria,[0],[0]
"A full description of conditions under which we can guarantee that fairness criteria cause improvement relative to MaxUtil is given in [Liu et al., 2018].",3.1 Prospects and Pitfalls of Fairness Criteria,[0],[0]
The result follows from comparing the position of optima on the utility curve to the outcome curve.,3.1 Prospects and Pitfalls of Fairness Criteria,[0],[0]
Figure 2 displays an illustrative example of both the outcome curve and the institution’s utility U as a function of the selection rates in group A.,3.1 Prospects and Pitfalls of Fairness Criteria,[0],[0]
"In the utility function (1), the contributions of each group are weighted by their population proportions gj, and thus the resulting selection rates are sensitive to these proportions.",3.1 Prospects and Pitfalls of Fairness Criteria,[0],[0]
"As we see in the remainder of this section, fairness criteria can achieve nearly any position along the outcome curve under the right conditions.",3.1 Prospects and Pitfalls of Fairness Criteria,[0],[0]
"This fact comes from the potential mismatch between the outcomes, controlled by ∆, and the institution’s utility u.
The next theorem implies that DemParity can be bad for long term well-being of the protected group by being overgenerous.",3.1 Prospects and Pitfalls of Fairness Criteria,[0],[0]
Proposition 3.3 (DemParity can cause harm by being over-eager).,3.1 Prospects and Pitfalls of Fairness Criteria,[0],[0]
Assume that ∆µA(βMaxUtilB ),3.1 Prospects and Pitfalls of Fairness Criteria,[0],[0]
< 0.,3.1 Prospects and Pitfalls of Fairness Criteria,[0],[0]
Then there are general settings under which a g0 exists such that DemParity cases active or relative harm as long as gA ∈,3.1 Prospects and Pitfalls of Fairness Criteria,[0],[0]
"[0, g0].
",3.1 Prospects and Pitfalls of Fairness Criteria,[0],[0]
Notice that both the assumption and the condition encode notions that could be taken to mean ‘disadvantage:’,3.1 Prospects and Pitfalls of Fairness Criteria,[0],[0]
The assumption says that a policy which selects individuals from group A at the selection rate that MaxUtil would have used for group B necessarily lowers average score in A.,3.1 Prospects and Pitfalls of Fairness Criteria,[0],[0]
"The condition requires that gA is small enough.
",3.1 Prospects and Pitfalls of Fairness Criteria,[0],[0]
"Using credit scores as an example, Theorem 3.3 tells us that an overly aggressive fairness criterion will give too many loans to people in a protected group who cannot pay them back, hurting the group’s credit scores on average.",3.1 Prospects and Pitfalls of Fairness Criteria,[0],[0]
"An analogous result holds for EqOpt, and is stated in [Liu et al., 2018].
3.2",3.1 Prospects and Pitfalls of Fairness Criteria,[0],[0]
Comparing EqOpt and DemParity,3.1 Prospects and Pitfalls of Fairness Criteria,[0],[0]
It is difficult to compare DemParity and EqOpt on general terms.,3.1 Prospects and Pitfalls of Fairness Criteria,[0],[0]
"In fact, we have found that settings exist both in which DemParity causes harm while EqOpt causes improvement and in which DemParity causes improvement while EqOpt causes harm.",3.1 Prospects and Pitfalls of Fairness Criteria,[0],[0]
Proposition 3.4 (EqOpt may avoid active harm where DemParity fails).,3.1 Prospects and Pitfalls of Fairness Criteria,[0],[0]
"For a simple example of distributions, there exists g0, g1 such that for gA ∈",3.1 Prospects and Pitfalls of Fairness Criteria,[0],[0]
"[g0, g1], DemParity causes active harm while EqOpt causes improvement.
",3.1 Prospects and Pitfalls of Fairness Criteria,[0],[0]
"In the simple geometry of the example for the above result, EqOpt is better than DemParity at avoiding active harm because it is more conservative.",3.1 Prospects and Pitfalls of Fairness Criteria,[0],[0]
A natural question then is: can EqOpt cause relative harm by being too stingy?,3.1 Prospects and Pitfalls of Fairness Criteria,[0],[0]
"Theorem 3.5 (DemParity never loans less than MaxUtil, but EqOpt might).",3.1 Prospects and Pitfalls of Fairness Criteria,[0],[0]
Suppose that the MaxUtil policy is such that βMaxUtilA,3.1 Prospects and Pitfalls of Fairness Criteria,[0],[0]
"< β MaxUtil B and TPRA(τ
MaxUtil) >",3.1 Prospects and Pitfalls of Fairness Criteria,[0],[0]
"TPRB(τ
MaxUtil).",3.1 Prospects and Pitfalls of Fairness Criteria,[0],[0]
Then EqOpt causes relative harm by selecting at a rate lower than MaxUtil.,3.1 Prospects and Pitfalls of Fairness Criteria,[0],[0]
We examine the outcomes induced by fairness constraints in the context of FICO scores for two race groups.,4 Simulations,[0],[0]
FICO scores are a proprietary classifier widely used in the United States to predict credit worthiness.,4 Simulations,[0],[0]
"Our FICO data is based on a sample of 301,536 TransUnion TransRisk scores from 2003 [US Federal Reserve, 2007], preprocessed by [Hardt et al., 2016].",4 Simulations,[0],[0]
"Empirical data labeled by race allows us to estimate the distributions πj, where j represents race, which is restricted to two values: white non-Hispanic (labeled “white” in figures), and black.",4 Simulations,[0],[0]
"We use the outcome and profit models from Example 2.1, with individual penalties as a score drop of c− = −150 in the case of a default, and in increase of c+ = 75 in the case of successful repayment.",4 Simulations,[0],[0]
We also model the utility ratio of the bank as u−u+ = −4.,4 Simulations,[0],[0]
"Further details of the presented simulations are in [Liu et al., 2018].
",4 Simulations,[0],[0]
Figure 3 displays the outcome and utility curves for both the white and the black group.,4 Simulations,[0],[0]
"In this figure, the top panel corresponds to the average simulated change in credit scores for each group under different loaning rates β; the bottom panels shows the corresponding total utility U (summed over both groups and weighted by group population sizes) for the bank.",4 Simulations,[0],[0]
"Although one might hope for decisions made under fairness constraints to positively affect the black group, we observe the opposite behavior for DemParity, which causes a decrease in the average credit score.",4 Simulations,[0],[0]
This behavior stems from a discrepancy in the outcome and profit curves for the black population.,4 Simulations,[0],[0]
"Reflecting on our findings, we argue that careful temporal modeling is necessary in order to accurately evaluate the im-
pact of different fairness criteria on the population.",5 Conclusion,[0],[0]
The nuances of our characterization underline how intuition may be a poor guide in judging the long-term impact of fairness constraints.,5 Conclusion,[0],[0]
"Our formal framework exposes a concise, yet expressive way to model outcomes via the expected change in a variable of interest caused by an institutional decision.",5 Conclusion,[0],[0]
This leads to the natural concept of an outcome curve that allows us to interpret and compare solutions effectively.,5 Conclusion,[0],[0]
"In essence, the formalism we propose requires us to understand the twovariable causal mechanism that translates decisions to outcomes.",5 Conclusion,[0],[0]
"Depending on the application, such an understanding might necessitate greater domain knowledge and additional research into the specifics of the application.",5 Conclusion,[0],[0]
"This is consistent with much scholarship that points to the context-sensitive nature of fairness in machine learning [Green and Hu, 2018].",5 Conclusion,[0],[0]
"We thank Lily Hu, Aaron Roth, and Cathy O’Neil for discussions and feedback on an earlier version of the manuscript.",Acknowledgements,[0],[0]
"We thank the students of CS294: Fairness in Machine Learning (Fall 2017, University of California, Berkeley) for inspiring class discussions and comments on a presentation that was a precursor of this work.",Acknowledgements,[0],[0]
This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No. DGE 1752814.,Acknowledgements,[0],[0]
Static classification has been the predominant focus of the study of fairness in machine learning.,abstractText,[0],[0]
"While most models do not consider how decisions change populations over time, it is conventional wisdom that fairness criteria promote the long-term wellbeing of groups they aim to protect.",abstractText,[0],[0]
This work studies the interaction of static fairness criteria with temporal indicators of well-being.,abstractText,[0],[0]
"We show a simple one-step feedback model in which common criteria do not generally promote improvement over time, and may in fact cause harm.",abstractText,[0],[0]
"Our results highlight the importance of temporal modeling in the evaluation of fairness criteria, suggesting a range of new challenges and trade-offs.",abstractText,[0],[0]
Delayed Impact of Fair Machine Learning,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 1865–1874 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,[0],[0]
"The success of natural language generation (NLG) systems depends on their ability to carefully control not only the topic of produced utterances, but also attributes such as sentiment and style.",1 Introduction,[0],[0]
"The desire for more sophisticated, controllable NLG has led to increased interest in text attribute transfer— the task of editing a sentence to alter specific attributes, such as style, sentiment, and tense (Hu
∗Work done while the author was a visiting researcher at Stanford University.
great food but horrible staff and very very rude workers !
target=positivegreat food staff and very workers !
",1 Introduction,[0],[0]
"Delete attribute markers
(b) Attribute transfer
neg pos
pos pos
pos neg
neg neg
worst very disappointed won't be back ...
",1 Introduction,[0],[0]
"delicious great place for well worth ...
(a) Extracting attribute markers
et al., 2017; Shen et al., 2017; Fu et al., 2018).",1 Introduction,[0],[0]
"In each of these cases, the goal is to convert a sentence with one attribute (e.g., negative sentiment) to one with a different attribute (e.g., positive sentiment), while preserving all attribute-independent content1 (e.g., what properties of a restaurant are being discussed).",1 Introduction,[0],[0]
"Typically, aligned sentences with the same content but different attributes are not available; systems must learn to disentangle attributes and content given only unaligned sentences labeled with attributes.
",1 Introduction,[0],[0]
"Previous work has attempted to use adversarial 1 Henceforth, we refer to attribute-independent content as
simply content, for simplicity.
1865
networks (Shen et al., 2017; Fu et al., 2018) for this task, but—as we demonstrate—their outputs tend to be low-quality, as judged by human raters.",1 Introduction,[0],[0]
"These models are also difficult to train (Salimans et al., 2016; Arjovsky and Bottou, 2017; Bousmalis et al., 2017).
",1 Introduction,[0],[0]
"In this work, we propose a set of simpler, easierto-train systems that leverage an important observation: attribute transfer can often be accomplished by changing a few attribute markers— words or phrases in the sentence that are indicative of a particular attribute—while leaving the rest of the sentence largely unchanged.",1 Introduction,[0],[0]
"Figure 1 shows an example in which the sentiment of a sentence can be altered by changing a few sentiment-specific phrases but keeping other words fixed.
",1 Introduction,[0],[0]
"With this intuition, we first propose a simple baseline that already outperforms prior adversarial approaches.",1 Introduction,[0],[0]
Consider a sentiment transfer (negative to positive) task.,1 Introduction,[0],[0]
"First, from unaligned corpora of positive and negative sentences, we identify attribute markers by finding phrases that occur much more often within sentences of one attribute than the other (e.g., “worst” and “very disppointed” are negative markers).",1 Introduction,[0],[0]
"Second, given a sentence, we delete any negative markers in it, and regard the remaining words as its content.",1 Introduction,[0],[0]
"Third, we retrieve a sentence with similar content from the positive corpus.
",1 Introduction,[0],[0]
"We further improve upon this baseline by incorporating a neural generative model, as shown in Figure 1.",1 Introduction,[0],[0]
"Our neural system extracts content words in the same way as our baseline, then generates the final output with an RNN decoder that conditions on the extracted content and the target attribute.",1 Introduction,[0],[0]
"This approach has significant benefits at training time, compared to adversarial networks: having already separated content and attribute, we simply train our neural model to reconstruct sentences in the training data as an auto-encoder.
",1 Introduction,[0],[0]
"We test our methods on three text attribute transfer datasets: altering sentiment of Yelp reviews, altering sentiment of Amazon reviews, and altering image captions to be more romantic or humorous.",1 Introduction,[0],[0]
"Averaged across these three datasets, our simple baseline generated grammatical sentences with appropriate content and attribute 23% of the time, according to human raters; in contrast, the best adversarial method achieved only 12%.",1 Introduction,[0],[0]
"Our best neural system in turn outperformed our baseline, achieving an average
success rate of 34%.",1 Introduction,[0],[0]
"Our code and data, including newly collected human reference outputs for the Yelp and Amazon domains, can be found at https://github.com/lijuncen/ Sentiment-and-Style-Transfer.",1 Introduction,[0],[0]
"We assume access to a corpus of labeled sentences D = {(x1, v1), . . .",2 Problem Statement,[0],[0]
", (xm, vm)}, where xi is a sentence and vi ∈ V , the set of possible attributes (e.g., for sentiment, V = {“positive”, “negative”}).",2 Problem Statement,[0],[0]
"We define Dv = {x : (x, v) ∈ D}, the set of sentences in the corpus with attribute v. Crucially, we do not assume access to a parallel corpus that pairs sentences with different attributes and the same content.
",2 Problem Statement,[0],[0]
"Our goal is to learn a model that takes as input (x, vtgt) where x is a sentence exhibiting source (original) attribute vsrc, and vtgt is the target attribute, and outputs a sentence y that retains the content of x while exhibiting vtgt.",2 Problem Statement,[0],[0]
"As a motivating example, suppose we wanted to change the sentiment of “The chicken was delicious.”",3 Approach,[0],[0]
from positive to negative.,3 Approach,[0],[0]
"Here the word “delicious” is the only sentiment-bearing word, so we just need to replace it with an appropriate negative sentiment word.",3 Approach,[0],[0]
"More generally, we find that the attribute is often localized to a small fraction of the words, an inductive bias not captured by previous work.
",3 Approach,[0],[0]
How do we know which negative sentiment word to insert?,3 Approach,[0],[0]
The key observation is that the remaining content words provide strong cues: given “The chicken was . . .,3 Approach,[0],[0]
"”, one can infer that a tasterelated word like “bland” fits, but a word like “rude” does not, even though both have negative sentiment.",3 Approach,[0],[0]
"In other words, while the deleted sentiment words do contain non-sentiment information too, this information can often be recovered using the other content words.
",3 Approach,[0],[0]
"In the rest of this section, we describe our four systems: two baselines (RETRIEVEONLY and TEMPLATEBASED) and two neural models (DELETEONLY and DELETEANDRETRIEVE).",3 Approach,[0],[0]
An overview of all four systems is shown in Figure 2.,3 Approach,[0],[0]
"Formally, the main components of these systems are as follows:
1.",3 Approach,[0],[0]
"Delete: All 4 systems use the same procedure to separate the words in x into a set of
attribute markers a(x, vsrc) and a sequence of content words c(x, vsrc).
",3 Approach,[0],[0]
2.,3 Approach,[0],[0]
"Retrieve: 3 of the 4 systems look through the corpus and retrieve a sentence xtgt that has the target attribute vtgt and whose content is similar to that of x.
3.",3 Approach,[0],[0]
Generate:,3 Approach,[0],[0]
"Given the content c(x, vsrc), target attribute vtgt, and (optionally) the retrieved sentence xtgt, each system generates y, either in a rule-based fashion or with a neural sequence-to-sequence model.
",3 Approach,[0],[0]
We describe each component in detail below.,3 Approach,[0],[0]
We propose a simple method to delete attribute markers (n-grams) that have the most discriminative power.,3.1 Delete,[0],[0]
"Formally, for any v ∈ V , we define the salience of an n-gram u with respect to v by its (smoothed) relative frequency in Dv:
s(u, v) =",3.1 Delete,[0],[0]
"count(u,Dv) + λ(∑
v′∈V,v′ 6=v count(u,Dv′) )",3.1 Delete,[0],[0]
"+ λ ,
(1) where count(u,Dv) denotes the number of times an n-gram u appears inDv, and λ is the smoothing parameter.",3.1 Delete,[0],[0]
"We declare u to be an attribute marker for v if s(u, v) is larger than a specified threshold γ.",3.1 Delete,[0],[0]
"The attributed markers can be viewed as discriminative features for a Naive Bayes classifier.
",3.1 Delete,[0],[0]
"We define a(x, vsrc) to be the set of all source attribute markers in x, and define c(x, vsrc) as the sequence of words after deleting all markers in a(x, vsrc) from x.",3.1 Delete,[0],[0]
"For example, for “The chicken was delicious,” we would delete “delicious” and consider “The chicken was. . . ”",3.1 Delete,[0],[0]
"to be the content (Figure 2, Step 1).",3.1 Delete,[0],[0]
"To decide what words to insert into c(x, vsrc), one useful strategy is to look at similar sentences with the target attribute.",3.2 Retrieve,[0],[0]
"For example, negative sentences that use phrases similar to “The chicken was. . . ” are more likely to contain “bland” than “rude.”",3.2 Retrieve,[0],[0]
"Therefore, we retrieve sentences of similar content and use target attribute markers in them for insertion.
",3.2 Retrieve,[0],[0]
"Formally, we retrieve xtgt according to:
xtgt = argmin x′∈Dvtgt d(c(x, vsrc), c(x′, vtgt)), (2)
where d may be any distance metric comparing two sequences of words.",3.2 Retrieve,[0],[0]
"We experiment with two options: (i) TF-IDF weighted word overlap and (ii) Euclidean distance using the content embeddings in Section 3.3 (Figure 2, Step 2).",3.2 Retrieve,[0],[0]
"Finally, we describe how each system generates y (Figure 2, Step 3).
",3.3 Generate,[0],[0]
RETRIEVEONLY returns the retrieved sentence xtgt verbatim.,3.3 Generate,[0],[0]
"This is guaranteed to produce a grammatical sentence with the target attribute, but its content might not be similar to x.
TEMPLATEBASED replaces the attribute markers deleted from the source sentence a(x, vsrc) with those of the target sentence a(xtgt,",3.3 Generate,[0],[0]
"vtgt).2 This strategy relies on the assumption that if two attribute markers appear in similar contexts , they are roughly syntactically exchangeable.",3.3 Generate,[0],[0]
"For example, “love” and “don’t like” appear in similar contexts (e.g., “i love this place.”",3.3 Generate,[0],[0]
"and “i don’t like this place.”), and exchanging them is syntactically valid.",3.3 Generate,[0],[0]
"However, this naive swapping of attribute markers can result in ungrammatical outputs.
",3.3 Generate,[0],[0]
"DELETEONLY first embeds the content c(x, vsrc) into a vector using an RNN.",3.3 Generate,[0],[0]
"It then concatenates the final hidden state with a learned embedding for vtgt, and feeds this into an RNN decoder to generate y.",3.3 Generate,[0],[0]
"The decoder attempts to produce words indicative of the source content and target attribute, while remaining fluent.
",3.3 Generate,[0],[0]
"DELETEANDRETRIEVE is similar to DELETEONLY, but uses the attribute markers of the retrieved sentence xtgt rather than the target attribute vtgt.",3.3 Generate,[0],[0]
"Like DELETEONLY, it encodes c(x, vsrc) with an RNN.",3.3 Generate,[0],[0]
"It then encodes the sequence of attribute markers a(xtgt, vtgt) with another RNN.",3.3 Generate,[0],[0]
"The RNN decoder uses the concatenation of this vector and the content embedding to generate y.
DELETEANDRETRIEVE combines the advantages of TEMPLATEBASED and DELETEONLY.",3.3 Generate,[0],[0]
"Unlike TEMPLATEBASED, DELETEANDRETRIEVE can pick a better place to insert the given attribute markers, and can add or remove function words to ensure grammaticality.",3.3 Generate,[0],[0]
"Compared to DELETEONLY, DELETEANDRETRIEVE has a stronger inductive bias towards using target attribute markers that are likely to fit in the current context.",3.3 Generate,[0],[0]
Guu et al. (2018) showed that retrieval strategies like ours can help neural generative models.,3.3 Generate,[0],[0]
"Finally, DELETEANDRETRIEVE gives us finer control over the output; for example, we can control the degree of sentiment by deciding whether to add “good” or “fantastic” based on the retrieved sentence xtgt.
",3.3 Generate,[0],[0]
"2 Markers are replaced from left to right, in order.",3.3 Generate,[0],[0]
"If there are not enough markers in xtgt, we use an empty string.",3.3 Generate,[0],[0]
We now describe how to train DELETEANDRETRIEVE and DELETEONLY.,3.4 Training,[0],[0]
"Recall that at training time, we do not have access to ground truth outputs that express the target attribute.",3.4 Training,[0],[0]
"Instead, we train DELETEONLY to reconstruct the sentences in the training corpus given their content and original attribute value by maximizing:
L(θ) = ∑
(x,vsrc)∈D log p(x | c(x, vsrc), vsrc); θ).
(3) For DELETEANDRETRIEVE, we could similarly learn an auto-encoder that reconstructs x from c(x, vsrc) and a(x, vsrc).",3.4 Training,[0],[0]
"However, this results in a trivial solution: because a(x, vsrc) and c(x, vsrc) were known to come from the same sentence, the model merely learns to stitch the two sequences together without any smoothing.",3.4 Training,[0],[0]
"Such a model would fare poorly at test time, when we may need to alter some words to fluently combine a(xtgt, vtgt) with c(x, vsrc).",3.4 Training,[0],[0]
"To address this train/test mismatch, we adopt a denoising method similar to the denoising auto-encoder (Vincent et al., 2008).",3.4 Training,[0],[0]
"During training, we apply some noise to a(x, vsrc) by randomly altering each attribute marker in it independently with probability 0.1.",3.4 Training,[0],[0]
"Specifically, we replace an attribute marker with another randomly selected attribute marker of the same attribute and word-level edit distance 1 if such a noising marker exists, e.g., “was very rude” to “very rude”, which produces a′(x, vsrc).
",3.4 Training,[0],[0]
"Therefore, the training objective for DELETEANDRETRIEVE is to maximize:
L(θ) = ∑
(x,vsrc)∈D log p(x | c(x, vsrc), a′(x, vsrc); θ).
(4)",3.4 Training,[0],[0]
"We evaluated our approach on three domains: flipping sentiment of Yelp reviews (YELP) and Amazon reviews (AMAZON), and changing image captions to be romantic or humorous (CAPTIONS).",4 Experiments,[0],[0]
We compared our four systems to human references and three previously published adversarial approaches.,4 Experiments,[0],[0]
"As judged by human raters, both of our two baselines outperform all three adversarial methods.",4 Experiments,[0],[0]
"Moreover, DELETEANDRETRIEVE outperforms all other automatic approaches.",4 Experiments,[0],[0]
"First, we describe the three datasets we use, which are commonly used in prior works too.",4.1 Datasets,[0],[0]
"All datasets are randomly split into train, development, and test sets (Table 1).
",4.1 Datasets,[0],[0]
YELP,4.1 Datasets,[0],[0]
"Each example is a sentence from a business review on Yelp, and is labeled as having either positive or negative sentiment.
",4.1 Datasets,[0],[0]
"AMAZON Similar to YELP, each example is a sentence from a product review on Amazon, and is labeled as having either positive or negative sentiment (He and McAuley, 2016).
",4.1 Datasets,[0],[0]
CAPTIONS,4.1 Datasets,[0],[0]
"In the CAPTIONS dataset (Gan et al., 2017), each example is a sentence that describes an image, and is labeled as either factual, romantic, or humorous.",4.1 Datasets,[0],[0]
We focus on the task of converting factual sentences into romantic and humorous ones.,4.1 Datasets,[0],[0]
"Unlike YELP and AMAZON, CAPTIONS is actually an aligned corpus—it contains captions for the same image in different styles.",4.1 Datasets,[0],[0]
"Our systems do not use these alignments, but we use them as gold references for evaluation.
",4.1 Datasets,[0],[0]
"CAPTIONS is also unique in that we reconstruct romantic and humorous sentences during training, whereas at test time we are given factual captions.",4.1 Datasets,[0],[0]
"We assume these factual captions carry only content, and therefore do not look for and delete factual attribute markers; The model essentially only inserts romantic or humorous attribute markers as appropriate.",4.1 Datasets,[0],[0]
"To supply human reference outputs to which we could compare the system outputs for YELP and AMAZON, we hired crowdworkers on Amazon Mechanical Turk to write gold outputs for all test sentences.",4.2 Human References,[0],[0]
"Workers were instructed to edit a sentence to flip its sentiment while preserving its content.
",4.2 Human References,[0],[0]
"Our delete-retrieve-generate approach relies on the prior knowledge that to accomplish attribute
transfer, a small number of attribute markers should be changed, and most other words should be kept the same.",4.2 Human References,[0],[0]
We analyzed our human reference data to understand the extent to which humans follow this pattern.,4.2 Human References,[0],[0]
"We measured whether humans preserved words our system marks as content, and changed words our system marks as attribute-related (Section 3.1).",4.2 Human References,[0],[0]
"We define the content word preservation rate Sc as the average fraction of words our system marks as content that were preserved by humans, and the attributerelated word change rate Sa as the average fraction of words our system marks as attribute-related that were changed by humans:
",4.2 Human References,[0],[0]
"Sc = 1 |Dtest| ∑
(x,vsrc,y∗)∈Dtest
|c(x, vsrc) ∩ y∗| |c(x, vsrc)|
Sa = 1− 1 |Dtest| ∑
(x,vsrc,y∗)∈Dtest
|a(x, vsrc) ∩",4.2 Human References,[0],[0]
"y∗| |a(x, vsrc)| ,
(5) where Dtest is the test set, y∗ is the human reference sentence, and | · | denotes the number of nonstopwords.",4.2 Human References,[0],[0]
"Higher values of Sc and Sa indicate that humans preserve content words and change attribute-related words, in line with the inductive bias of our model.",4.2 Human References,[0],[0]
"Sc is 0.61, 0.71, and 0.50 on YELP, AMAZON, and CAPTIONS, respectively; Sa is 0.72 on YELP and 0.54 on AMAZON (not applicable on CAPTIONS).
",4.2 Human References,[0],[0]
"To understand why humans sometimes deviated from the inductive bias of our model, we randomly sampled 50 cases from YELP where humans changed a content word or preserved an attribute-related word.",4.2 Human References,[0],[0]
"70% of changed content words were unimportant words (e.g., “whole” was deleted from “whole experience”), and another 18% were paraphrases (e.g., “charge” became “price”); the remaining 12% were errors where the system mislabeled an attribute-related word as a content word (e.g., “old” became “new”).",4.2 Human References,[0],[0]
"84% of preserved attribute-related words did pertain to sentiment but remained fixed due to changes in the surrounding context (e.g., “don’t like” became “like”, and “below average” became “above average”); the remaining 16% were mistagged by our system as being attribute-related (e.g., “walked out”).",4.2 Human References,[0],[0]
"We compare with three previous models, all of which use adversarial training.",4.3 Previous Methods,[0],[0]
"STYLEEMBED-
DING (Fu et al., 2018) learns an vector encoding of the source sentence such that a decoder can use it to reconstruct the sentence, but a discriminator, which tries to identify the source attribute using this encoding, fails.",4.3 Previous Methods,[0],[0]
"They use a basic MLP discriminator and an LSTM decoder. MULTIDECODER (Fu et al., 2018) is similar to STYLEEMBEDDING, except that it uses a different decoder for each attribute value.",4.3 Previous Methods,[0],[0]
"CROSSALIGNED (Shen et al., 2017) also encodes the source sentence into a vector, but the discriminator looks at the hidden states of the RNN decoder instead.",4.3 Previous Methods,[0],[0]
The system is trained so that the discriminator cannot distinguish these hidden states from those obtained by forcing the decoder to output real sentences from the target domain; this objective encourages the real and generated target sentences to look similar at a population level.,4.3 Previous Methods,[0],[0]
"For our methods, we use 128-dimensional word vectors and a single-layer GRU with 512 hidden units for both encoders and the decoder.",4.4 Experimental Details,[0],[0]
"We use the maxout activation function (Goodfellow et al., 2013).",4.4 Experimental Details,[0],[0]
All parameters are initialized by sampling from a uniform distribution between−0.1 and 0.1.,4.4 Experimental Details,[0],[0]
"For optimization, we use Adadelta (Zeiler, 2012) with a minibatch size of 256.
",4.4 Experimental Details,[0],[0]
"For attribute marker extraction, we consider spans up to 4 words, and the smoothing parameter λ is set to 1.",4.4 Experimental Details,[0],[0]
"We set the attribute marker threshold γ, which controls the precision and recall of our attribute markers, to 15, 5.5 and 5 for YELP, AMAZON, and CAPTIONS.",4.4 Experimental Details,[0],[0]
These values were set by manual inspection of the resulting markers and tuning slightly on the dev set.,4.4 Experimental Details,[0],[0]
"For retrieval, we used the TF-IDF weighted word overlap score for DELETEANDRETRIEVE and TEMPLATEBASED,
and the Euclidean distance of content embeddings for RETRIEVEONLY.",4.4 Experimental Details,[0],[0]
"We find the two scoring functions give similar results.
",4.4 Experimental Details,[0],[0]
"For all neural models, we do beam search with a beam size of 10.",4.4 Experimental Details,[0],[0]
"For DELETEANDRETRIEVE, similar to Guu et al. (2018), we retrieve the top10 sentences and generate results using markers from each sentence.",4.4 Experimental Details,[0],[0]
We then select the output with the lowest perplexity given by a separately-trained neural language model on the target-domain training data.,4.4 Experimental Details,[0],[0]
We hired workers on Amazon Mechanical Turk to rate the outputs of all systems.,4.5 Human Evaluation,[0],[0]
"For each source sentence and target attribute, the same worker was shown the output of each tested system.",4.5 Human Evaluation,[0],[0]
"Workers were asked to rate each output on three criteria on a Likert scale from 1 to 5: grammaticality, similarity to the target attribute, and preservation of the source content.",4.5 Human Evaluation,[0],[0]
"Finally, we consider a generated output “successful” if it is rated 4 or 5 on all three criteria.",4.5 Human Evaluation,[0],[0]
"For each dataset, we evaluated 400 randomly sampled examples (200 for each target attribute).
",4.5 Human Evaluation,[0],[0]
Table 2 shows the human evaluation results.,4.5 Human Evaluation,[0],[0]
"On all three datasets, both of our baselines have a higher success rate than the previously published models, and DELETEANDRETRIEVE achieves the best performance among all systems.",4.5 Human Evaluation,[0],[0]
"Additionally, we see that human raters strongly preferred the human references to all systems, suggesting there is still significant room for improvement on this task.
",4.5 Human Evaluation,[0],[0]
We find that a human evaluator’s judgment of a sentence is largely relative to other sentences being evaluated together and examples given in the instruction (different for each dataset/task).,4.5 Human Evaluation,[0],[0]
"There-
fore, evaluating all system outputs in one batch is important and results on different datasets are not directly comparable.",4.5 Human Evaluation,[0],[0]
We analyze the strengths and weaknesses of the different systems.,4.6 Analysis,[0],[0]
"Table 3 show typical outputs of each system on the YELP and CAPTIONS dataset.
",4.6 Analysis,[0],[0]
We first analyze the adversarial methods.,4.6 Analysis,[0],[0]
"CROSSALIGNED and MULTIDECODER tend to lose the content of the source sentence, as seen in both the example outputs and the overall human ratings.",4.6 Analysis,[0],[0]
The decoder tends to generate a frequent but only weakly related sentence with the target attribute.,4.6 Analysis,[0],[0]
"On the other hand, STYLEEMBEDDING almost always generates a paraphrase of the input sentence, implying that the encoder preserves some attribute information.",4.6 Analysis,[0],[0]
"We conclude that there is a delicate balance between preserving the original content and dropping the original attribute, and existing adversarial models tend to sacrifice one or the other.
",4.6 Analysis,[0],[0]
"Next, we analyze our baselines.",4.6 Analysis,[0],[0]
"RETRIEVEONLY scores well on grammaticality and having the target attribute, since it retrieves sentences with the desired attribute directly from the corpus.",4.6 Analysis,[0],[0]
"However, it is likely to change the content when there is no perfectly aligned sentence in the target domain.",4.6 Analysis,[0],[0]
"In contrast, TEMPLATEBASED is good at preserving the content because the content words are guaranteed to be kept.",4.6 Analysis,[0],[0]
"However, it makes grammatical mistakes due to the unsmoothed combination of content and attribute words.
DELETEANDRETRIEVE and DELETEONLY achieve a good balance among grammaticality, preserving content, and changing the attribute.",4.6 Analysis,[0],[0]
"Both have strong inductive bias on what words should be changed, but still have the flexibility to smooth out the sentence.",4.6 Analysis,[0],[0]
"The main difference is that DELETEONLY fills in attribute words based on only the target attribute, whereas DELETEANDRETRIEVE conditions on retrieved attribute words.",4.6 Analysis,[0],[0]
When there is a diverse set of phrases to fill in—for example in CAPTIONS— conditioning on retrieved attribute words helps generate longer sentences with more specific attribute descriptions.,4.6 Analysis,[0],[0]
"Following previous work (Hu et al., 2017; Shen et al., 2017), we also compute automatic evalua-
tion metrics, and compare these numbers to our human evaluation results.
",4.7 Automatic Evaluation,[0],[0]
"We use an attribute classifier to assess whether outputs have the desired attribute (Hu et al., 2017; Shen et al., 2017).",4.7 Automatic Evaluation,[0],[0]
We define the classifier score as the fraction of outputs classified as having the target attribute.,4.7 Automatic Evaluation,[0],[0]
"For each dataset, we train an attribute classifier on the same training data.",4.7 Automatic Evaluation,[0],[0]
"Specifically, we encode the sentence into a vector by a bidirectional LSTM with an average pooling layer over the outputs, and train the classifier by minimizing the logistic loss.
",4.7 Automatic Evaluation,[0],[0]
"We also compute BLEU between the output and the human references, similar to Gan et al. (2017).",4.7 Automatic Evaluation,[0],[0]
A high BLEU score primarily indicates that the system can correctly preserve content by retaining the same words from the source sentence as the reference.,4.7 Automatic Evaluation,[0],[0]
"One might also hope that it has some correlation with fluency, though we expect this correlation to be much weaker.
",4.7 Automatic Evaluation,[0],[0]
Table 4 shows the classifier and BLEU scores.,4.7 Automatic Evaluation,[0],[0]
"In Table 5, we compute the system-level correlation between classifier score and human judgments of attribute transfer, and between BLEU and human judgments of content preservation and grammaticality.",4.7 Automatic Evaluation,[0],[0]
We also plot scores given by the automatic metrics and humans in Figure 4.,4.7 Automatic Evaluation,[0],[0]
"While the scores are sometimes well-correlated, the results vary significantly between datasets; on AMAZON, there is no correlation between the classifier score and the human evaluation.",4.7 Automatic Evaluation,[0],[0]
"Manual inspection shows that on AMAZON, some product genres are associated with either mostly positive or mostly negative reviews.",4.7 Automatic Evaluation,[0],[0]
"However, our systems produce, for example, negative reviews about products that are mostly discussed positively in the training set.",4.7 Automatic Evaluation,[0],[0]
"Therefore, the classifier often gives unreliable predictions on system outputs.",4.7 Automatic Evaluation,[0],[0]
"As expected, BLEU does not correlate well with human grammaticality ratings.",4.7 Automatic Evaluation,[0],[0]
"The lack of automatic fluency evaluation artificially favors systems like TEMPLATEBASED, which make more grammatical mistakes.",4.7 Automatic Evaluation,[0],[0]
"We conclude that while these automatic evaluation methods are useful for model development, they cannot replace human evaluation.",4.7 Automatic Evaluation,[0],[0]
One advantage of our methods is that we can control the trade-off between matching the target attribute and preserving the source content.,4.8 Trading off Content versus Attribute,[0],[0]
"To achieve different points along this trade-off curve,
we simply vary the threshold γ (Section 3.1) at test time to control how many attribute markers we delete from the source sentence.",4.8 Trading off Content versus Attribute,[0],[0]
"In contrast, other methods (Shen et al., 2017; Fu et al., 2018) would require retraining the model with different hyperparameters to achieve this effect.
",4.8 Trading off Content versus Attribute,[0],[0]
"Figure 3 shows this trade-off curve for DELETEANDRETRIEVE, DELETEONLY, and TEMPLATEBASED on YELP, where target attribute match is measured by the classifier score and content preservation is measured by BLEU.3",4.8 Trading off Content versus Attribute,[0],[0]
"We see a clear trade-off between changing the attribute and retaining the content.
",4.8 Trading off Content versus Attribute,[0],[0]
"3 RETRIEVEONLY is less affected by what content words are preserved, especially when no good output sentence exists in the target corpus.",4.8 Trading off Content versus Attribute,[0],[0]
"Therefore, we found that it did not exhibit a clear content-attribute trade-off.",4.8 Trading off Content versus Attribute,[0],[0]
"Our work is closely related to the recent body of work on text attribute transfer with unaligned data, where the key challenge to disentangle attribute and content in an unsupervised way.",5 Related Work and Discussion,[0],[0]
"Most existing work (Shen et al., 2017; Zhao et al., 2018; Fu et al., 2018; Melnyk et al., 2017) uses adversarial training to separate attribute and content: the content encoder aims to fool the attribute discriminator by removing attribute information from the content embedding.",5 Related Work and Discussion,[0],[0]
"However, we find that empirically it is often easy to fool the discriminator without actually removing the attribute information.",5 Related Work and Discussion,[0],[0]
"Therefore, we explicitly separate attribute and content by taking advantage of the prior knowledge that the attribute is localized to parts of the sentence.
",5 Related Work and Discussion,[0],[0]
"To address the problem of unaligned data, Hu
et al. (2017) relies on an attribute classifier to guide the generator to produce sentences with a desired attribute (e.g. sentiment, tense) in the Variational Autoencoder (VAE) framework.",5 Related Work and Discussion,[0],[0]
"Similarly, Zhao et al. (2018) used a regularized autoencoder in the adversarial training framework; however, they also find that these models require extensive hyperparameter tuning and the content tends to be changed during the transfer.",5 Related Work and Discussion,[0],[0]
Shen et al. (2017) used a discriminator to align target sentences and sentences transfered to the target domain from the source domain.,5 Related Work and Discussion,[0],[0]
"More recently, unsupervised machine translation models (Artetxe et al., 2017; Lample et al., 2017) used a cycle loss similar to Jun-Yan et al. (2017) to ensure that the content is preserved during the transformation.",5 Related Work and Discussion,[0],[0]
"These methods often rely on bilinguial word vectors to provide word-for-word translations, which are then finetune by back-translation.",5 Related Work and Discussion,[0],[0]
"Thus they can be used to further improve our results.
",5 Related Work and Discussion,[0],[0]
"Our method of detecting attribute markers is reminiscent of Naive Bayes, which is a strong baseline for tasks like sentiment classification (Wang and Manning, 2012).",5 Related Work and Discussion,[0],[0]
"Deleting these at-
tribute markers can be viewed as attacking a Naive Bayes classifier by deleting the most informative features (Globerson and Roweis, 2006), similarly to how adversarial methods are trained to fool an attribute classifier.",5 Related Work and Discussion,[0],[0]
"One difference is that our classifier is fixed, not jointly trained with the model.
",5 Related Work and Discussion,[0],[0]
"To conclude, we have described a simple method for text attribute transfer that outperforms previous models based on adversarial training.",5 Related Work and Discussion,[0],[0]
The main leverage comes from the inductive bias that attributes are usually manifested in localized discriminative phrases.,5 Related Work and Discussion,[0],[0]
"While many prior works on linguistic style analysis confirm our observation that attributes often manifest in idiosyncratic phrases (Recasens et al., 2013; Schwartz et al., 2017; Newman et al., 2003), we recognize the fact that in some problems (e.g., Pavlick and Tetreault (2017)), content and attribute cannot be so cleanly separated along phrase boundaries.",5 Related Work and Discussion,[0],[0]
"Looking forward, a fruitful direction is to develop a notion of attributes more general than n-grams, but with more inductive bias than arbitrary latent vectors.
",5 Related Work and Discussion,[0],[0]
Reproducibility.,5 Related Work and Discussion,[0],[0]
"All code, data, and experiments for this paper are available on the CodaLab platform at https://worksheets.",5 Related Work and Discussion,[0],[0]
"codalab.org/worksheets/ 0xe3eb416773ed4883bb737662b31b4948/.
Acknowledgements.",5 Related Work and Discussion,[0],[0]
This work is supported by the DARPA Communicating with Computers (CwC) program under ARO prime contract no.,5 Related Work and Discussion,[0],[0]
W911NF- 15-1-0462.,5 Related Work and Discussion,[0],[0]
J.L. is supported by Tencent.,5 Related Work and Discussion,[0],[0]
R.J. is supported by an NSF Graduate Research Fellowship under Grant No.,5 Related Work and Discussion,[0],[0]
DGE-114747.,5 Related Work and Discussion,[0],[0]
"We consider the task of text attribute transfer: transforming a sentence to alter a specific attribute (e.g., sentiment) while preserving its attribute-independent content (e.g., changing “screen is just the right size” to “screen is too small”).",abstractText,[0],[0]
"Our training data includes only sentences labeled with their attribute (e.g., positive or negative), but not pairs of sentences that differ only in their attributes, so we must learn to disentangle attributes from attributeindependent content in an unsupervised way.",abstractText,[0],[0]
Previous work using adversarial methods has struggled to produce high-quality outputs.,abstractText,[0],[0]
"In this paper, we propose simpler methods motivated by the observation that text attributes are often marked by distinctive phrases (e.g., “too small”).",abstractText,[0],[0]
"Our strongest method extracts content words by deleting phrases associated with the sentence’s original attribute value, retrieves new phrases associated with the target attribute, and uses a neural model to fluently combine these into a final output.",abstractText,[0],[0]
"On human evaluation, our best method generates grammatical and appropriate responses on 22% more inputs than the best previous system, averaged over three attribute transfer datasets: altering sentiment of reviews on Yelp, altering sentiment of reviews on Amazon, and altering image captions to be more romantic or humorous.",abstractText,[0],[0]
"Delete, Retrieve, Generate: a Simple Approach to Sentiment and Style Transfer",title,[0],[0]
"Streams of data of massive and increasing volume are generated every second, and demand fast analysis and efficient storage, including massive clickstreams, stock market data, image and video streams, sensor data for environmental or health monitoring, to name a few.",1. Introduction,[0],[0]
To make efficient and reliable decisions we usually need to react in real-time to the data.,1. Introduction,[0],[0]
"However, big and fast data makes it difficult to store, analyze, or make predictions.",1. Introduction,[0],[0]
"Therefore, data summarization – mining and extracting useful information from large data sets – has become a central topic in machine learning and information retrieval.
",1. Introduction,[0],[0]
A recent body of research on data summarization relies on utility/scoring functions that are submodular.,1. Introduction,[0],[0]
"Intuitively, submodularity (Krause & Golovin, 2013) states that select-
1ETH Zurich, Switzerland 2Yale University, New Haven, USA.",1. Introduction,[0],[0]
"Correspondence to: Baharan Mirzasoleiman <baharanm@inf.ethz.ch>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
ing any given data point earlier helps more than selecting it later.,1. Introduction,[0],[0]
"Hence, submodular functions can score both diversity and representativeness of a subset w.r.t.",1. Introduction,[0],[0]
the entire dataset.,1. Introduction,[0],[0]
"Thus, many problems in data summarization require maximizing submodular set functions subject to cardinality (or more complicated hereditary constraints).",1. Introduction,[0],[0]
"Numerous examples include exemplar-based clustering (Dueck & Frey, 2007), document (Lin & Bilmes, 2011) and corpus summarization (Sipos et al., 2012), recommender systems (El-Arini & Guestrin, 2011), search result diversification (Rakesh Agrawal, 2009), data subset selection (Wei et al., 2015), and social networks analysis (Kempe et al., 2003).
",1. Introduction,[0],[0]
"Classical methods, such as the celebrated greedy algorithm (Nemhauser et al., 1978) or its accelerated versions (Mirzasoleiman et al., 2015; Badanidiyuru & Vondrák, 2014) require random access to the entire data, make multiple passes, and select elements sequentially in order to produce near optimal solutions.",1. Introduction,[0],[0]
"Naturally, such solutions cannot scale to large instances.",1. Introduction,[0],[0]
"The limitations of centralized methods inspired the design of streaming algorithms that are able to gain insights from data as it is being collected (Badanidiyuru et al., 2014; Chakrabarti & Kale, 2014; Chekuri et al., 2015; Mirzasoleiman et al., 2017).
",1. Introduction,[0],[0]
"While extracting useful information from big data in realtime promises many benefits, the development of more sophisticated methods for extracting, analyzing and using personal information has made privacy a major public issue.",1. Introduction,[0],[0]
Various web services rely on the collection and combination of data about individuals from a wide variety of sources.,1. Introduction,[0],[0]
"At the same time, the ability to control the information an individual can reveal about herself in online applications has become a growing concern.
",1. Introduction,[0],[0]
"The “right to be forgotten” (with a specific mandate for protection in the European Data Protection Regulation (2012), and concrete guidelines released in 2014) allows individuals to claim the ownership of their personal information and gives them the authority to their online activities (videos, photos, tweets, etc).",1. Introduction,[0],[0]
"As an example, consider a road traffic information system that monitors traffic speeds, travel times and incidents in real time.",1. Introduction,[0],[0]
"It combines the massive amount of control messages available at the cellular network with their geo-coordinates in order to gen-
erate the area-wide traffic information service.",1. Introduction,[0],[0]
"Some consumers, while using the service and providing data, may not be willing to share information about specific locations in order to protect their own privacy.",1. Introduction,[0],[0]
"With the right to be forgotten, an individual can have certain data deleted from online database records so that third parties (e.g., search engines) can no longer trace them (Weber, 2011).",1. Introduction,[0],[0]
"Note that the data could be in many forms, including a) user’s posts to an online social media, b) visual data shared by wearable cameras (e.g., Google Glass), c) behavioral patterns or feedback obtained from clicking on advertisement or news.
",1. Introduction,[0],[0]
"In this paper, we propose the first framework that offers instantaneous data summarization while preserving the right of an individual to be forgotten.",1. Introduction,[1.0],"['In this paper, we propose the first framework that offers instantaneous data summarization while preserving the right of an individual to be forgotten.']"
We cast this problem as an instance of robust streaming submodular maximization where the goal is to produce a concise real-time summary in the face of data deletion requested by users.,1. Introduction,[1.0],['We cast this problem as an instance of robust streaming submodular maximization where the goal is to produce a concise real-time summary in the face of data deletion requested by users.']
"We develop ROBUST-STREAMING, a method that for a generic streaming algorithm STREAMINGALG with approximation guarantee α, ROBUST-STREAMING outputs a robust solution, against any m deletions from the summary at any given time, while preserving the same approximation guarantee.",1. Introduction,[0],[0]
"To the best of our knowledge, ROBUST-STREAMING is the first algorithm with such strong theoretical guarantees.",1. Introduction,[0],[0]
Our experimental results also demonstrate the effectiveness of ROBUST-STREAMING on several practical applications.,1. Introduction,[0],[0]
Several streaming algorithms for submodular maximization have been recently developed.,2. Background and Related Work,[0],[0]
"For monotone functions, Gomes & Krause (2010) first developed a multipass algorithm with 1/2− approximation guarantee subject to a cardinality constraint k, using O(k) memory, under strong assumptions on the way the data is generated.",2. Background and Related Work,[0],[0]
"Later, Badanidiyuru et al. (2014) proposed the first single pass streaming algorithm with 1/2 − approximation under a cardinality constraint.",2. Background and Related Work,[0],[0]
"They made no assumptions on the order of receiving data points, and only require O(k log k/ ) memory.",2. Background and Related Work,[0],[0]
"Following the same line of inquiry, Chakrabarti & Kale (2014) developed a single pass algorithm with 1/4p approximation guarantee for handling more general constraints such as intersections of p matroids.",2. Background and Related Work,[0],[0]
The required memory is unbounded and increases polylogarithmically with the size of the data.,2. Background and Related Work,[0],[0]
"For general submodular functions, Chekuri et al. (2015) presented a randomized algorithm subject to a broader range of constraints, namely p-matchoids.",2. Background and Related Work,[0],[0]
Their method gives a (2 − o(1))/(8+e)p approximation usingO(k log k/ 2) memory (k is the size of the largest feasible solution).,2. Background and Related Work,[0],[0]
"Very recently, Mirzasoleiman et al. (2017) introduced a (4p−1)/4p(8p+ 2d − 1)-approximation algorithm under a p-system and d knapsack constraints, using O(pk log2(k)/ 2) memory.
",2. Background and Related Work,[0],[0]
"An important requirement, which frequently arises in practice, is robustness.",2. Background and Related Work,[0],[0]
"Krause et al. (2008) proposed the problem of robust submodular observation selection, where we want to solve max|A|≤k mini∈[`] fi(A), for normalized monotonic fi.",2. Background and Related Work,[0],[0]
Submodular maximization of f robust against m deletions can be cast as an instance of the above problem: max|A|≤k min|B|≤m f(A\B).,2. Background and Related Work,[0],[0]
"The running time, however, will be exponential in m. Recently, Orlin et al. (2016) developed an algorithm with an asymptotic guarantee 0.285 for deletion-robust submodular maximization under up to m =",2. Background and Related Work,[0],[0]
o( √ k) deletions.,2. Background and Related Work,[0],[0]
"The results can be improved for only 1 or 2 deletions.
",2. Background and Related Work,[0],[0]
"The aforementioned approaches aim to construct solutions that are robust against deletions in a batch mode way, without being able to update the solution set after each deletion.",2. Background and Related Work,[0],[0]
"To the best of our knowledge, this is the first to address the general deletion-robust submodular maximization problem in the streaming setting.",2. Background and Related Work,[0],[0]
"We also highlight the fact that our method does not require m, the number of deletions, to be bounded by k, the size of the largest feasible solution.
",2. Background and Related Work,[0],[0]
"Very recently, submodular optimization over sliding windows has been considered, where we want to maintain a solution that considers only the last W items (Epasto et al., 2017; Jiecao et al., 2017).",2. Background and Related Work,[0],[0]
"This is in contrast to our setting, where the guarantee is with respect to all the elements received from the stream, except those that have been deleted.",2. Background and Related Work,[0],[0]
The sliding window model can be easily incorporated into our solution to get a robust sliding window streaming algorithm with the possibility of m deletions in the window.,2. Background and Related Work,[0],[0]
We review the static submodular data summarization problem.,3. Deletion-Robust Model,[0],[0]
"We then formalize a novel dynamic variant, and constraints on time and memory that algorithms need to obey.",3. Deletion-Robust Model,[1.0],"['We then formalize a novel dynamic variant, and constraints on time and memory that algorithms need to obey.']"
"In static data summarization, we have a large but fixed dataset V of size n, and we are interested in finding a summary that best represents the data.",3.1. Static Submodular Data Summarization,[1.0],"['In static data summarization, we have a large but fixed dataset V of size n, and we are interested in finding a summary that best represents the data.']"
The representativeness of a subset is defined based on a utility function f :,3.1. Static Submodular Data Summarization,[0],[0]
2V → R+ where for any A ⊂ V the function f(A) quantifies how well A represents V .,3.1. Static Submodular Data Summarization,[0],[0]
We define the marginal gain of adding an element e ∈ V to a summary A ⊂ V by ∆(e|A) = f(A ∪ {e}),3.1. Static Submodular Data Summarization,[0],[0]
− f(A),3.1. Static Submodular Data Summarization,[0],[0]
.,3.1. Static Submodular Data Summarization,[0],[0]
"In many data summarization applications, the utility function f satisfies submodularity, i.e., for all A ⊆ B ⊆ V and e ∈ V \B,
∆(e|A) ≥ ∆(e|B).
",3.1. Static Submodular Data Summarization,[0.9999999726661944],"['In many data summarization applications, the utility function f satisfies submodularity, i.e., for all A ⊆ B ⊆ V and e ∈ V \\B, ∆(e|A) ≥ ∆(e|B).']"
"Many data summarization applications can be cast as an instance of a constrained submodular maximization:
OPT = max A∈I f(A), (1)
where I ⊂ 2V is a given family of feasible solutions.",3.1. Static Submodular Data Summarization,[0],[0]
"We will denote by A∗ the optimal solution, i.e. A∗ = arg maxA∈I f(A).",3.1. Static Submodular Data Summarization,[0],[0]
"A common type of constraint is a cardinality constraint, i.e., I = {A ⊆ 2V , s.t., |A| ≤ k}.",3.1. Static Submodular Data Summarization,[0],[0]
"Finding A∗ even under cardinality constraint is NP-hard, for many classes of submodular functions (Feige, 1998).",3.1. Static Submodular Data Summarization,[0],[0]
"However, a seminal result by Nemhauser et al. (1978) states that for a non-negative and monotone submodular function a simple greedy algorithm that starts with the empty set S0 = ∅, and at each iteration augments the solution with the element with highest marginal gain, obtains a (1−1/e) approximation to the optimum solution.",3.1. Static Submodular Data Summarization,[0],[0]
"For small, static data, the centralized greedy algorithm or its accelerated variants produce near-optimal solutions.",3.1. Static Submodular Data Summarization,[0],[0]
"However, such methods fail to scale to truly large problems.",3.1. Static Submodular Data Summarization,[0],[0]
"In dynamic deletion-robust submodular maximization problem, the data V is generated at a fast pace and in realtime, such that at any point t in time, a subset Vt ⊆ V of the data has arrived.",3.2. Dynamic Data: Additions and Deletions,[0],[0]
"Naturally, we assume that V1 ⊆ V2 ⊆ · · · ⊆ Vn, with no assumption made on the order or the size of the datastream.",3.2. Dynamic Data: Additions and Deletions,[0],[0]
"Importantly, we allow data to be deleted dynamically as well.",3.2. Dynamic Data: Additions and Deletions,[0],[0]
"We use Dt to refer to data deleted by time t, where again D1 ⊆ D2 ⊆ · · · ⊆ Dn.",3.2. Dynamic Data: Additions and Deletions,[0],[0]
"Without loss of generality, below we assume that at every time step t exactly one element et ∈ V is either added or deleted, i.e., |Dt \",3.2. Dynamic Data: Additions and Deletions,[0],[0]
Dt−1|,3.2. Dynamic Data: Additions and Deletions,[0],[0]
+,3.2. Dynamic Data: Additions and Deletions,[0],[0]
|Vt \ Vt−1| = 1.,3.2. Dynamic Data: Additions and Deletions,[0],[0]
"We now seek to solve a dynamic variant of Problem (1) OPTt = max
At∈It f(At) s.t.",3.2. Dynamic Data: Additions and Deletions,[0],[0]
"It = {A : A ∈ I∧A ⊆ Vt \Dt}.
(2) Note that in general a feasible solution at time t might not be a feasible solution at a later time t′.",3.2. Dynamic Data: Additions and Deletions,[0],[0]
This is particularly important in practical situations where a subset of the elements Dt should be removed from the solution.,3.2. Dynamic Data: Additions and Deletions,[0],[0]
"We do not make any assumptions on the order or the size of the data stream V , but we assume that the total number of deletions is limited to m , i.e., |Dn| ≤ m.",3.2. Dynamic Data: Additions and Deletions,[0],[0]
"In principle, we could solve Problem (2) by repeatedly – at every time t – solving a static Problem (1) by restricting the ground set V to Vt \Dt.",3.3. Dealing with Limited Time and Memory,[0],[0]
This is impractical even for moderate problem sizes.,3.3. Dealing with Limited Time and Memory,[0],[0]
"For large problems, we may not even be able to fit Vt into the main memory of the computing device (space constraints).",3.3. Dealing with Limited Time and Memory,[0],[0]
"Moreover, in real-time applications, one needs to make decisions in a timely manner while the data is continuously arriving (time constraints).
",3.3. Dealing with Limited Time and Memory,[0],[0]
We hence focus on streaming algorithms which may maintain a limited memory Mt ⊂,3.3. Dealing with Limited Time and Memory,[0],[0]
"Vt \ Dt, and must have an updated feasible solution {At |At ⊆Mt, At ∈",3.3. Dealing with Limited Time and Memory,[0],[0]
"It} to output at any given time t. Ideally, the capacity of the memory
|Mt| should not depend on t and Vt.",3.3. Dealing with Limited Time and Memory,[0],[0]
"Whenever a new element is received, the algorithm can choose 1) to insert it into its memory, provided that the memory does not exceed a pre-specified capacity bound, 2) to replace it with one or a subset of elements in the memory (in the preemptive model), or otherwise 3) the element gets discarded and cannot be used later by the algorithm.",3.3. Dealing with Limited Time and Memory,[0],[0]
If the algorithm receives a deletion request for a subset Dt ⊂ Vt at time t (in which case It will be updated to accommodate this request) it has to drop Dt from Mt in addition to updating At to make sure that the current solution is feasible (all subsetsA′t ⊂,3.3. Dealing with Limited Time and Memory,[0],[0]
"Vt that contain an element fromDt are infeasible, i.e., A′t /∈",3.3. Dealing with Limited Time and Memory,[0],[0]
It).,3.3. Dealing with Limited Time and Memory,[0],[0]
"To account for such losses, the streaming algorithm can only use other elements maintained in its memory in order to produce a feasible candidate solution, i.e. At ⊆ Mt ⊆ ((Vt \ Vt−1) ∪Mt−1) \Dt.",3.3. Dealing with Limited Time and Memory,[0],[0]
"We say that the streaming algorithm is robust against m deletions, if it can provide a feasible solution At ∈",3.3. Dealing with Limited Time and Memory,[0],[0]
It at any given time t such that f(At) ≥ τOPTt for some constant τ > 0.,3.3. Dealing with Limited Time and Memory,[0],[0]
"Later, we show how robust streaming algorithms can be obtained by carefully increasing the memory and running multiple instances of existing streaming methods simultaneously.",3.3. Dealing with Limited Time and Memory,[0],[0]
"We now discuss three concrete applications, with their submodular objective functions f , where the size of the datasets and the nature of the problem often require a deletion-robust streaming solution.",4. Example Applications,[0],[0]
There exists a tremendous opportunity of harnessing prevalent activity logs and sensing resources.,4.1. Summarizing Click-stream and Geolocation Data,[0],[0]
"For instance, GPS traces of mobile phones can be used by road traffic information systems (such as Google traffic, TrafficSense, Navigon) to monitor travel times and incidents in real time.",4.1. Summarizing Click-stream and Geolocation Data,[0],[0]
"In another example, stream of user activity logs is recorded while users click on various parts of a webpage such as ads and news while browsing the web, or using social media.",4.1. Summarizing Click-stream and Geolocation Data,[0],[0]
Continuously sharing all collected data is problematic for several reasons.,4.1. Summarizing Click-stream and Geolocation Data,[0],[0]
"First, memory and communication constraints may limit the amount of data that can be stored and transmitted across the network.",4.1. Summarizing Click-stream and Geolocation Data,[0],[0]
"Second, reasonable privacy concerns may prohibit continuous tracking of users.
",4.1. Summarizing Click-stream and Geolocation Data,[0],[0]
"In many such applications, the data can be described in terms of a kernel matrix K which encodes the similarity between different data elements.",4.1. Summarizing Click-stream and Geolocation Data,[0],[0]
The goal is to select a small subset (active set) of elements while maintaining a certain diversity.,4.1. Summarizing Click-stream and Geolocation Data,[0],[0]
"Very often, the utility function boils down to the following monotone submodular function (Krause & Golovin, 2013) where α > 0",4.1. Summarizing Click-stream and Geolocation Data,[0],[0]
"andKS,S is the principal submatrix of K indexed by the set S.
f(S) = log det(I + αKS,S) (3)
",4.1. Summarizing Click-stream and Geolocation Data,[0],[0]
"In light of privacy concerns, it is natural to consider participatory models that empower users to decide what portion of their data could be made available.",4.1. Summarizing Click-stream and Geolocation Data,[0],[0]
"If a user decides not to share, or to revoke information about parts of their activity, the monitoring system should be able to update the summary to comply with users’ preferences.",4.1. Summarizing Click-stream and Geolocation Data,[1.0],"['If a user decides not to share, or to revoke information about parts of their activity, the monitoring system should be able to update the summary to comply with users’ preferences.']"
"Therefore, we use ROBUST-STREAMING to identify a robust set of the k most informative data points by maximizing Eq.",4.1. Summarizing Click-stream and Geolocation Data,[1.0],"['Therefore, we use ROBUST-STREAMING to identify a robust set of the k most informative data points by maximizing Eq.']"
(3).,4.1. Summarizing Click-stream and Geolocation Data,[0],[0]
"Given a collection of images, one might be interested in finding a subset that best summarizes and represents the collection.",4.2. Summarizing Image Collections,[0],[0]
This problem has recently been addressed via submodular maximization.,4.2. Summarizing Image Collections,[1.0],['This problem has recently been addressed via submodular maximization.']
"More concretely, Tschiatschek et al. (2014) designed several submodular objectives f1, . . .",4.2. Summarizing Image Collections,[0],[0]
", fl, which quantify different characteristics that good summaries should have, e.g., being representative w.r.t.",4.2. Summarizing Image Collections,[0],[0]
commonly reoccurring motives.,4.2. Summarizing Image Collections,[0],[0]
"Each function either captures coverage (including facility location, sumcoverage, and truncated graph cut, or rewards diversity (such as clustered facility location, and clustered diversity).",4.2. Summarizing Image Collections,[0],[0]
"Then, they optimize a weighted combination of such functions
fw(A) = l∑ i=1",4.2. Summarizing Image Collections,[0],[0]
"wifi(A), (4)
where weights are non-negative, i.e., wi ≥ 0, and learned via a large-margin structured prediction.",4.2. Summarizing Image Collections,[0],[0]
We use their learned mixtures of submodular functions in our image summarization experiments.,4.2. Summarizing Image Collections,[0],[0]
"Now, consider a situation where a user wants to summarize a large collection of her photos.",4.2. Summarizing Image Collections,[0],[0]
"If she decides to delete some of the selected photos in the summary, she should be able to update the result without processing the whole collection from scratch.",4.2. Summarizing Image Collections,[0],[0]
ROBUST-STREAMING can be used as an appealing method.,4.2. Summarizing Image Collections,[0],[0]
"In this section, we first elaborate on why naively increasing the solution size does not help.",5. Robust-Streaming Algorithm,[0],[0]
"Then, we present our main algorithm, ROBUST-STREAMING, for deletion-robust streaming submodular maximization.",5. Robust-Streaming Algorithm,[0],[0]
"Our approach builds on the following key ideas: 1) simultaneously constructing non-overlapping solutions, and 2) appropriately merging solutions upon deleting an element from the memory.",5. Robust-Streaming Algorithm,[0],[0]
One of the main challenges in designing streaming solutions is to immediately discover whether an element received from the data stream at time t is good enough to be added to the memory Mt.,5.1. Increasing the Solution Size Does Not Help,[0],[0]
"This decision is usually made based on the added value or marginal gain of the new element which in turn depends on the previously chosen elements in the memory, i.e., Mt−1.",5.1. Increasing the Solution Size Does Not Help,[0],[0]
"Now, let us consider
the opposite scenario when an element e should be deleted from the memory at time t. Since now we have a smaller context, submodularity guarantees that the marginal gains of the elements added to the memory after e was added, could have only increased if e was not part of the stream (diminishing returns).",5.1. Increasing the Solution Size Does Not Help,[0],[0]
"Hence, if some elements had large marginal values to be included in the memory before the deletion, they still do after the deletion.",5.1. Increasing the Solution Size Does Not Help,[0],[0]
"Based on this intuition, a natural idea is to keep a solution of a bigger size, saym+k (rather than k) for at mostm deletions.",5.1. Increasing the Solution Size Does Not Help,[0],[0]
"However, this idea does not work as shown by the following example.
",5.1. Increasing the Solution Size Does Not Help,[0],[0]
"Bad Example (Coverage): Consider a collection of n subsets V = {B1, . . .",5.1. Increasing the Solution Size Does Not Help,[0],[0]
", Bn}, where Bi ⊆ {1, . . .",5.1. Increasing the Solution Size Does Not Help,[0],[0]
",",5.1. Increasing the Solution Size Does Not Help,[0],[0]
"n}, and",5.1. Increasing the Solution Size Does Not Help,[0],[0]
a coverage function f(A),5.1. Increasing the Solution Size Does Not Help,[0],[0]
"= |∪i∈ABi|,A ⊆ V .",5.1. Increasing the Solution Size Does Not Help,[0],[0]
"Suppose we receive B1 = {1, . . .",5.1. Increasing the Solution Size Does Not Help,[0],[0]
", n}, and then Bi = {i} for 2≤ i ≤ n from the stream.",5.1. Increasing the Solution Size Does Not Help,[0],[0]
"Streaming algorithms that select elements according to their marginal gain and are allowed to pick k + m elements, will only pick up B1 upon encounter (as other elements provide no gain), and returnAn = {B1} after processing the stream.",5.1. Increasing the Solution Size Does Not Help,[0],[0]
"Hence, if B1 is deleted after the stream is received, these algorithms return the empty set An = ∅ (with f(An) = 0).",5.1. Increasing the Solution Size Does Not Help,[0],[0]
"An optimal algorithm which knows that elementB1 will be deleted, however, will return set An = {B2, . . .",5.1. Increasing the Solution Size Does Not Help,[0],[0]
", Bk+2}, with value f(An) =",5.1. Increasing the Solution Size Does Not Help,[0],[0]
k + 1.,5.1. Increasing the Solution Size Does Not Help,[0],[0]
"Hence, standard streaming algorithms fail arbitrarily badly even under a single deletion (i.e., m = 1), even when we allow them to pick sets larger than k.
In the following we show how we can solve the above issue by carefully constructing not one but multiple solutions.",5.1. Increasing the Solution Size Does Not Help,[0],[0]
"As stated earlier, the existing one-pass streaming algorithms for submodular maximization work by identifying elements with marginal gains above a carefully chosen threshold.",5.2. Building Multiple Solutions,[0],[0]
This ensures that any element received from the stream which is fairly similar to the elements of the solution set is discarded by the algorithm.,5.2. Building Multiple Solutions,[0],[0]
"Since elements are chosen as diverse as possible, the solution may suffer dramatically in case of a deletion.
",5.2. Building Multiple Solutions,[0],[0]
"One simple idea is to try to findm (near) duplicates for each element e in the memory, i.e., find e′ such that f(e′) = f(e) and ∆(e′|e) = 0",5.2. Building Multiple Solutions,[0],[0]
"(Orlin et al., 2016).",5.2. Building Multiple Solutions,[0],[0]
This way if we facem deletions we can still find a good solution.,5.2. Building Multiple Solutions,[0],[0]
"The drawback is that even one duplicate may not exist in the data stream (see the bad example above), and we may not be able to recover for the deleted element.",5.2. Building Multiple Solutions,[0],[0]
"Instead, what we will do is to construct non-overlapping solutions such that once we experience a deletion, only one solution gets affected.
",5.2. Building Multiple Solutions,[0],[0]
"In order to be robust against m deletions, we run a cascading chain of r instances of STREAMINGALGs as follows.",5.2. Building Multiple Solutions,[1.0],"['In order to be robust against m deletions, we run a cascading chain of r instances of STREAMINGALGs as follows.']"
"Let Mt = M (1)t ,M (2) t , . . .",5.2. Building Multiple Solutions,[0],[0]
",M (r) t denote the con-
tent of their memories at time t. When we receive a new element e ∈ Vt from the data stream at time t, we pass it to the first instance of STREAMINGALG(1).",5.2. Building Multiple Solutions,[0.9999999843857116],"[',M (r) t denote the con- tent of their memories at time t. When we receive a new element e ∈ Vt from the data stream at time t, we pass it to the first instance of STREAMINGALG(1).']"
"If STREAMINGALG(1) discards e, the discarded element is cascaded in the chain and is passed to its successive algorithm, i.e. STREAMINGALG(2).",5.2. Building Multiple Solutions,[0],[0]
"If e is discarded by STREAMINGALG(2), the cascade continues and e is passed to STREAMINGALG(3).",5.2. Building Multiple Solutions,[0],[0]
This process continues until either e is accepted by one of the instances or discarded for good.,5.2. Building Multiple Solutions,[1.0],['This process continues until either e is accepted by one of the instances or discarded for good.']
"Now, let us consider the case where e is accepted by the i-th instance, SIEVE-STREAMING(i), in the chain.",5.2. Building Multiple Solutions,[0],[0]
"As discussed in Section 3.3, STREAMINGALG may choose to discard a set of points R(i)t ⊂ M (i) t from its memory before inserting e, i.e., M (i)t ←M (i) t ∪ {e} \R (i) t .",5.2. Building Multiple Solutions,[0],[0]
"Note that R (i) t is empty, if e is inserted and no element is discarded from M
(i) t .",5.2. Building Multiple Solutions,[0],[0]
"For every discarded element r ∈ R (i) t , we start a new
cascade from (i+ 1)-th instance, STREAMINGALG (i+1).
",5.2. Building Multiple Solutions,[0],[0]
"Note that in the worst case, every element of the stream can go once through the whole chain during the execution of the algorithm, and thus the processing time for each element scales linearly by r. An important observation is that at any given time t, all the memories M (1)t ,M (2) t , · · · ,M (r) t contain disjoint sets of elements.",5.2. Building Multiple Solutions,[0],[0]
"Next, we show how this data structure leads to a deletion-robust streaming algorithm.",5.2. Building Multiple Solutions,[0],[0]
"Equipped with the above data structure shown in Fig. 1, we now demonstrate how deletions can be treated.",5.3. Dealing with Deletions,[0],[0]
"Assume an element ed is being deleted from the memory of the j-th instance of STREAMINGALG(j) at time t, i.e., M
(j) t ← M (j) t \ {ed}.",5.3. Dealing with Deletions,[0],[0]
"As discussed in Section 5.1, the solution of the streaming algorithm can suffer dramatically from a deletion, and we may not be able to restore the quality of the solution by substituting similar elements.",5.3. Dealing with Deletions,[0],[0]
"Since there is no guarantee for the quality of the solution after a deletion, we remove STREAMINGALG (j) from the chain by makingR(j)t =null and for all the remaining elements in
its memory M (j)t , namely, R (j) t ←M (j) t \ {ed}, we start a new cascade from j+1-th instance, STREAMINGALG(j+1).
",5.3. Dealing with Deletions,[0],[0]
The key reason why the above algorithm works is that the guarantee provided by the streaming algorithm is independent of the order of receiving the data elements.,5.3. Dealing with Deletions,[0],[0]
"Note that at any point in time, the first instance i of the algorithm with M (i)t 6= null has processed all the elements from the stream Vt (not necessarily in the order the stream is originally received) except the ones deleted by time t, i.e., Dt.",5.3. Dealing with Deletions,[0],[0]
"Therefore, we can guarantee that STREAMINGALG (i) provides us with its inherent α-approximation guarantee for reading Vt \Dt.",5.3. Dealing with Deletions,[0],[0]
"More precisely, f(S(i)t ) ≥ αOPTt, where OPTt is the optimum solution for the constrained optimization problem (2) when we have m deletions.
",5.3. Dealing with Deletions,[0],[0]
"In case of adversary deletions, there will be one deletion from the solution of m instances of STREAMINGALG in the chain.",5.3. Dealing with Deletions,[0],[0]
"Therefore, having r = m+ 1 instances, we will remain with only one STREAMINGALG that gives us the desired result.",5.3. Dealing with Deletions,[0],[0]
"However, as shown later in this section, if the deletions are i.i.d.",5.3. Dealing with Deletions,[0],[0]
"(which is often the case in practice), and we have m deletions in expectation, we need r to be much smaller than m+1.",5.3. Dealing with Deletions,[0],[0]
"Finally, note that we do not need to assume that m ≤ k where k is the size of the largest feasible solution.",5.3. Dealing with Deletions,[0],[0]
"The above idea works for arbitrarym≤n.
",5.3. Dealing with Deletions,[0],[0]
The pseudocode of ROBUST-STREAMING is given in Algorithm 1.,5.3. Dealing with Deletions,[0],[0]
It uses r ≤,5.3. Dealing with Deletions,[0],[0]
m+ 1 instances of STREAMINGALG as subroutines in order to produce r solutions.,5.3. Dealing with Deletions,[0],[0]
"We denote by S(1)t , S (1) t , . . .",5.3. Dealing with Deletions,[0],[0]
", S (r) t the solutions of the r STREAMINGALGs at any given time t.",5.3. Dealing with Deletions,[0],[0]
We assume that an instance i of STREAMINGALG(i) receive an input element and produces a solution S(i)t based on the input.,5.3. Dealing with Deletions,[0],[0]
"It may also change its memory content M (i)t , and discard a set R(i)t .",5.3. Dealing with Deletions,[0],[0]
"Among all the remained solutions (i.e., the ones that are not ”null”), it returns the first solution in the chain, i.e. the one with the lowest index.
",5.3. Dealing with Deletions,[0],[0]
"Theorem 1 Let STREAMINGALG be a 1-pass streaming algorithm that achieves an α-approximation guarantee for the constrained maximization problem (2) with an update time of T , and a memory of size M when there is no deletion.",5.3. Dealing with Deletions,[1.0],"['Theorem 1 Let STREAMINGALG be a 1-pass streaming algorithm that achieves an α-approximation guarantee for the constrained maximization problem (2) with an update time of T , and a memory of size M when there is no deletion.']"
Then ROBUST-STREAMING uses r ≤ m + 1 instances of STREAMINGALGs to produce a feasible solution St ∈,5.3. Dealing with Deletions,[0],[0]
It (now It encodes deletions in addition to constraints) such that f(St) = αOPTt as long as no more than m elements are deleted from the data stream.,5.3. Dealing with Deletions,[0],[0]
"Moreover, ROBUST-STREAMING uses a memory of size rM , and has worst case update time of O(r2MT ), and average update time of O(rT ).
",5.3. Dealing with Deletions,[0.9999999589933645],"['Moreover, ROBUST-STREAMING uses a memory of size rM , and has worst case update time of O(r2MT ), and average update time of O(rT ).']"
The proofs can be found in the appendix.,5.3. Dealing with Deletions,[0],[0]
"In Table 1 we combine the result of Theorem 1 with the existing streaming algorithms that satisfy our requirements.
",5.3. Dealing with Deletions,[0],[0]
Algorithm 1 ROBUST-STREAMING Input: data stream,5.3. Dealing with Deletions,[0],[0]
"Vt, deletion set",5.3. Dealing with Deletions,[0],[0]
"Dt, r ≤ m+1.",5.3. Dealing with Deletions,[0],[0]
"Output: solution St at any time t.
1: t = 1, M (i)t = 0, S (i) t = ∅",5.3. Dealing with Deletions,[0],[0]
∀i ∈,5.3. Dealing with Deletions,[0],[0]
[1 · · · r] 2: while ({Vt \ Vt−1} ∪ {Dt \Dt−1} 6= ∅) do 3: if {Dt \Dt−1} 6= ∅,5.3. Dealing with Deletions,[0],[0]
"then 4: ed ← {Dt \Dt−1} 5: Delete(ed) 6: else 7: et ← {Vt \ Vt−1} 8: Add(1, et) 9: end if
10: t = t+ 1 11: St = { S (i) t",5.3. Dealing with Deletions,[0],[0]
| i = min{j ∈,5.3. Dealing with Deletions,[0],[0]
"[1 · · · r], M (j) t 6= null} } 12: end while
13: function Add(i, R) 14: for e ∈ R do 15:",5.3. Dealing with Deletions,[0],[0]
"[R(i)t ,M (i) t , S (i) t ] =STREAMINGALG (i)(e) 16: if R(i)t 6= ∅",5.3. Dealing with Deletions,[0],[0]
"and i < r then 17: Add(i+ 1, R(i)t ) 18: end if 19: end for 20: end function
21: function Delete(e) 22: for i = 1 to r do 23: if e ∈M (i)t then 24: R(i)t = M (i) t",5.3. Dealing with Deletions,[0],[0]
\,5.3. Dealing with Deletions,[0],[0]
"{e} 25: M (i)t ← null 26: Add(i+ 1, R(i)t ) 27: return 28: end if 29: end for 30: end function
Theorem 2",5.3. Dealing with Deletions,[0],[0]
Assume each element of the stream is deleted with equal probability p,5.3. Dealing with Deletions,[0],[0]
"= m/n, i.e., in expectation we have m deletions from the stream.",5.3. Dealing with Deletions,[0],[0]
"Then, with probability 1− δ, ROBUST-STREAMING provides an α-approximation as long as
r ≥",5.3. Dealing with Deletions,[0],[0]
"( 1
1− p
)k log ( 1/δ ) .
",5.3. Dealing with Deletions,[0],[0]
"Theorem 2 shows that for fixed k, δ and p, a constant number r of STREAMINGALGs is sufficient to support m = pn (expected) deletions independently of n.",5.3. Dealing with Deletions,[0],[0]
"In contrast, for adversarial deletions, as analyzed in Theorem 1, pn + 1 copies of STREAMINGALG are required, which grows linearly in n. Hence, the required dependence of r on m is much milder for random than adversarial deletions.",5.3. Dealing with Deletions,[0],[0]
This is also verified by our experiments in Section 6.,5.3. Dealing with Deletions,[0],[0]
We address the following questions: 1) How much can ROBUST-STREAMING recover and possibly improve the performance of STREAMINGALG in case of deletions?,6. Experiments,[0],[0]
2) How much does the time of deletions affect the performance?,6. Experiments,[0],[0]
3) To what extent does deleting representative vs. random data points affect the performance?,6. Experiments,[0],[0]
"To this end, we run ROBUST-STREAMING on the applications we described in Section 4, namely, image collection summarization, summarizing stream of geolocation sensor data, as well as summarizing a clickstream of size 45 million.
",6. Experiments,[0],[0]
"Throughout this section we consider the following streaming algorithms: SIEVE-STREAMING (Badanidiyuru et al., 2014), STREAM-GREEDY (Gomes & Krause, 2010), and STREAMING-GREEDY (Chekuri et al., 2015).",6. Experiments,[0],[0]
"We allow all streaming algorithms, including the non-preemptive SIEVE-STREAMING, to update their solution after each deletion.",6. Experiments,[0],[0]
"We also consider a stronger variant of SIEVESTREAMING, called EXTSIEVE, that aims to pick k ·r elements to protect for deletions, i.e., is allowed the same memory as ROBUST-STREAMING.",6. Experiments,[0],[0]
"After the deletions, the remaining solution is pruned to k elements.
",6. Experiments,[0],[0]
"To compare the effect of deleting representative elements to the that of deleting random elements from the stream, we use two stochastic variants of the greedy algorithm, namely, STOCHASTIC-GREEDY (Mirzasoleiman et al., 2015) and RANDOM-GREEDY (Buchbinder et al., 2014).",6. Experiments,[0],[0]
This way we introduce randomness into the deletion process in a principled way.,6. Experiments,[0],[0]
"Hence, we have:
STOCHASTIC-GREEDY (SG): Similar to the the greedy algorithm, STOCHASTIC-GREEDY starts with an empty set and adds one element at each iteration until obtains a solution of sizem.",6. Experiments,[0],[0]
"But in each step it first samples a random set R of size (n/m) log(1/ ) and then adds an element from R to the solution which maximizes the marginal gain.
",6. Experiments,[1.0000000077483948],['But in each step it first samples a random set R of size (n/m) log(1/ ) and then adds an element from R to the solution which maximizes the marginal gain.']
"RANDOM-GREEDY (RG): RANDOM-GREEDY iteratively selects a random element from the top m elements with the highest marginal gains, until finds a solution of size m.
For each deletion method, the m data points are deleted either while receiving the data (where the steaming algorithms have the chance to update their solutions by selecting new elements) or after receiving the data (where there is no chance of updating the solution with new elements).",6. Experiments,[0.9999999497153759],"['RANDOM-GREEDY (RG): RANDOM-GREEDY iteratively selects a random element from the top m elements with the highest marginal gains, until finds a solution of size m. For each deletion method, the m data points are deleted either while receiving the data (where the steaming algorithms have the chance to update their solutions by selecting new elements) or after receiving the data (where there is no chance of updating the solution with new elements).']"
"Finally, the performance of all algorithms are normalized against the utility obtained by the centralized algorithm that knows the set of deleted elements in advance.",6. Experiments,[1.0],"['Finally, the performance of all algorithms are normalized against the utility obtained by the centralized algorithm that knows the set of deleted elements in advance.']"
We first apply ROBUST-STREAMING to a collection of 100 images from Tschiatschek et al. (2014).,6.1. Image Collection Summarization,[1.0],['We first apply ROBUST-STREAMING to a collection of 100 images from Tschiatschek et al. (2014).']
"We used
the weighted combination of 594 submodular functions either capturing coverage or rewarding diversity (c.f. Section 4.2).",6.1. Image Collection Summarization,[0],[0]
"Here, despite the small size of the dataset, computing the weighted combination of 594 functions makes the function evaluation considerably expensive.
",6.1. Image Collection Summarization,[0],[0]
Fig.,6.1. Image Collection Summarization,[0],[0]
2a compares the performance of SIEVE-STREAMING with its robust version ROBUST-STREAMING for r = 3 and solution size k=5.,6.1. Image Collection Summarization,[0],[0]
"Here, we vary the numberm of deletions from 1 to 20 after the whole stream is received.",6.1. Image Collection Summarization,[0],[0]
We see that ROBUST-STREAMING maintains its performance by updating the solution after deleting subsets of data points imposed by different deletion strategies.,6.1. Image Collection Summarization,[1.0],['We see that ROBUST-STREAMING maintains its performance by updating the solution after deleting subsets of data points imposed by different deletion strategies.']
"It can be seen that, even for a larger number m of deletions, ROBUSTSTREAMING, run with parameter r < m, is able to return a solution competitive with the strong centralized benchmark that knows the deleted elements beforehand.",6.1. Image Collection Summarization,[1.0],"['It can be seen that, even for a larger number m of deletions, ROBUSTSTREAMING, run with parameter r < m, is able to return a solution competitive with the strong centralized benchmark that knows the deleted elements beforehand.']"
"For the image collection, we were not able to compare the performance of STREAM-GREEDY with its robust version due to the prohibitive running time.",6.1. Image Collection Summarization,[0],[0]
Fig. 2b shows an example of an updated image summary returned by ROBUST-STREAMING after deleting the first image from the summary.,6.1. Image Collection Summarization,[0],[0]
Next we apply ROBUST-STREAMING to the active set selection objective described in Section 4.1.,6.2. Summarizing a stream of geolocation data,[0],[0]
"Our dataset con-
sists of 3,607 geolocations, collected during a one hour bike ride around Zurich (Fatio, 2015).",6.2. Summarizing a stream of geolocation data,[0],[0]
"For each pair of points i and j we used the corresponding (latitude, longitude) coordinates to calculate their distance in meters di,j and chose a Gaussian kernel Ki,j = exp(−d2i,j/h2) with h=1500.",6.2. Summarizing a stream of geolocation data,[0],[0]
Fig.,6.2. Summarizing a stream of geolocation data,[0],[0]
"3e shows the dataset where red and green triangles show a summary of size 10 found by SIEVESTREAMING, and the updated summary provided by ROBUST-STREAMING with r = 5 after deleting m= 70% of the datapoints.",6.2. Summarizing a stream of geolocation data,[1.0],"['3e shows the dataset where red and green triangles show a summary of size 10 found by SIEVESTREAMING, and the updated summary provided by ROBUST-STREAMING with r = 5 after deleting m= 70% of the datapoints.']"
Fig.,6.2. Summarizing a stream of geolocation data,[0],[0]
"3a and 3c compare the performance of SIEVE-STREAMING with its robust version when the data is deleted after or during the stream, respectively.",6.2. Summarizing a stream of geolocation data,[1.0],"['3a and 3c compare the performance of SIEVE-STREAMING with its robust version when the data is deleted after or during the stream, respectively.']"
"As we see, ROBUST-STREAMING provides a solution very close to the hindsight centralized method.",6.2. Summarizing a stream of geolocation data,[1.0],"['As we see, ROBUST-STREAMING provides a solution very close to the hindsight centralized method.']"
Fig.,6.2. Summarizing a stream of geolocation data,[0],[0]
3b and 3d show similar behavior for STREAM-GREEDY.,6.2. Summarizing a stream of geolocation data,[0],[0]
Note that deleting data points via STOCHASTIC-GREEDY or RANDOM-GREEDY are much more harmful on the quality of the solution provided by STREAM-GREEDY.,6.2. Summarizing a stream of geolocation data,[0],[0]
We repeated the same experiment by dividing the map into grids of length 2km.,6.2. Summarizing a stream of geolocation data,[0],[0]
We then considered a partition matroid by restricting the number of points selected from each grid to be 1.,6.2. Summarizing a stream of geolocation data,[0],[0]
The red and green triangles in Fig.,6.2. Summarizing a stream of geolocation data,[0],[0]
3f are the summary found by STREAMING-GREEDY and the updated summary provided by ROBUST-STREAMING after deleting the shaded area in the figure.,6.2. Summarizing a stream of geolocation data,[0],[0]
"For our large-scale experiment we consider again the active set selection objective, described in Section 4.1.",6.3. Large scale click through prediction,[0],[0]
We used Yahoo!,6.3. Large scale click through prediction,[0],[0]
"Webscope data set containing 45,811,883 user click logs for news articles displayed in the Featured Tab of the Today Module on Yahoo!",6.3. Large scale click through prediction,[0],[0]
"Front Page during the first ten days in May 2009 (Yahoo, 2012).",6.3. Large scale click through prediction,[0],[0]
"For each visit, both the user and shown articles are associated with a feature vector of dimension 6.",6.3. Large scale click through prediction,[0],[0]
"We take their outer product, resulting in a feature vector of size 36.
",6.3. Large scale click through prediction,[0],[0]
The goal was to predict the user behavior for each displayed article based on historical clicks.,6.3. Large scale click through prediction,[0],[0]
"To do so, we considered the first 80% of the data (for the fist 8 days) as our training set, and the last 20% (for the last 2 days) as our test set.",6.3. Large scale click through prediction,[0],[0]
"We used Vowpal-Wabbit (Langford et al., 2007) to train a linear classifier on the full training set.",6.3. Large scale click through prediction,[0],[0]
"Since only 4% of the data points are clicked, we assign a weight of 10 to each
clicked vector.",6.3. Large scale click through prediction,[0],[0]
The AUC score of the trained classifier on the test set was 65%.,6.3. Large scale click through prediction,[0],[0]
We then used ROBUST-STREAMING and SIEVE-STREAMING to find a representative subset of size k consisting of k/2 clicked and k/2 not-clicked examples from the training data.,6.3. Large scale click through prediction,[0],[0]
"Due to the massive size of the dataset, we used Spark on a cluster of 15 quad-core machines with 32GB of memory each.",6.3. Large scale click through prediction,[0],[0]
We partitioned the training data to the machines keeping its original order.,6.3. Large scale click through prediction,[0],[0]
"We
ran ROBUST-STREAMING on each machine to find a summary of size k/15, and merged the results to obtain the final summary of size",6.3. Large scale click through prediction,[0],[0]
"k. We then start deleting the data uniformly at random until we left with only 1% of the data, and trained another classifier on the remaining elements from the summary.
",6.3. Large scale click through prediction,[0],[0]
"Fig. 4a compares the performance of ROBUSTSTREAMING for a fixed active set of size k = 10, 000, and r = 2 with random selection, randomly selecting equal numbers of clicked and not-clicked vectors, and using SIEVE-STREAMING for selecting equal numbers of clicked and not-clicked data points.",6.3. Large scale click through prediction,[0.999077824186758],"['4a compares the performance of ROBUSTSTREAMING for a fixed active set of size k = 10, 000, and r = 2 with random selection, randomly selecting equal numbers of clicked and not-clicked vectors, and using SIEVE-STREAMING for selecting equal numbers of clicked and not-clicked data points.']"
"The y-axis shows the improvement in AUC score of the classifier trained on a summary obtained by different algorithms over random guessing (AUC=0.5), normalized by the AUC score of the classifier trained on the whole training data.",6.3. Large scale click through prediction,[0],[0]
"To maximize fairness, we let other baselines select a subset of r.k elements before deletions.",6.3. Large scale click through prediction,[0],[0]
Fig.,6.3. Large scale click through prediction,[0],[0]
4b shows the same quantity for r = 5.,6.3. Large scale click through prediction,[0],[0]
It can be seen that a slight increase in the amount of memory helps boosting the performance for all the algorithms.,6.3. Large scale click through prediction,[0],[0]
"However, ROBUST-STREAMING benefits from the additional memory the most, and can almost recover the performance of the classifier trained on the full training data, even after 99% deletion.",6.3. Large scale click through prediction,[1.0],"['However, ROBUST-STREAMING benefits from the additional memory the most, and can almost recover the performance of the classifier trained on the full training data, even after 99% deletion.']"
We have developed the first deletion-robust streaming algorithm – ROBUST-STREAMING – for constrained submodular maximization.,7. Conclusion,[1.0],['We have developed the first deletion-robust streaming algorithm – ROBUST-STREAMING – for constrained submodular maximization.']
"Given any single-pass streaming algorithm STREAMINGALG with α-approximation guarantee, ROBUST-STREAMING outputs a solution that is robust against m deletions.",7. Conclusion,[0],[0]
The returned solution also satisfies an α-approximation guarantee w.r.t.,7. Conclusion,[0],[0]
to the solution of the optimum centralized algorithm that knows the set of m deletions in advance.,7. Conclusion,[1.0],['to the solution of the optimum centralized algorithm that knows the set of m deletions in advance.']
We have demonstrated the effectiveness of our approach through an extensive set of experiments.,7. Conclusion,[1.0],['We have demonstrated the effectiveness of our approach through an extensive set of experiments.']
"This research was supported by ERC StG 307036, a Microsoft Faculty Fellowship, DARPA Young Faculty Award (D16AP00046), Simons-Berkeley fellowship and an ETH Fellowship.",Acknowledgements,[0],[0]
"This work was done in part while Amin Karbasi, and Andreas Krause were visiting the Simons Institute for the Theory of Computing.",Acknowledgements,[0],[0]
How can we summarize a dynamic data stream when elements selected for the summary can be deleted at any time?,abstractText,[0],[0]
"This is an important challenge in online services, where the users generating the data may decide to exercise their right to restrict the service provider from using (part of) their data due to privacy concerns.",abstractText,[0],[0]
"Motivated by this challenge, we introduce the dynamic deletion-robust submodular maximization problem.",abstractText,[0],[0]
"We develop the first resilient streaming algorithm, called ROBUST-STREAMING, with a constant factor approximation guarantee to the optimum solution.",abstractText,[0],[0]
"We evaluate the effectiveness of our approach on several real-world applications, including summarizing (1) streams of geocoordinates (2); streams of images; and (3) clickstream log data, consisting of 45 million feature vectors from a news recommendation task.",abstractText,[0],[0]
Deletion-Robust Submodular Maximization: Data Summarization with ``the Right to be Forgotten'',title,[0],[0]
