0,1,label2,summary_sentences
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2832–2838 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"Bidirectional Long Short-Term Memory (BLSTM) based models (Graves and Schmidhuber, 2005), along with word embeddings and character embeddings, have shown competitive performance on Part-of-Speech (POS) tagging given sufficient amount of training examples (Ling et al., 2015; Lample et al., 2016; Plank et al., 2016; Yang et al., 2017).
",1 Introduction,[0],[0]
"Given insufficient training examples, we can improve the POS tagging performance by cross-
lingual POS tagging, which exploits affluent POS tagging corpora from other source languages.",1 Introduction,[0],[0]
"This approach usually requires linguistic knowledge or resources about the relation between the source language and the target language such as parallel corpora (Täckström et al., 2013; Duong et al., 2013; Kim et al., 2015a; Zhang et al., 2016), morphological analyses (Hana et al., 2004), dictionaries (Wisniewski et al., 2014), and gaze features (Barrett et al., 2016).
",1 Introduction,[0],[0]
"Given no linguistic resources between the source language and the target language, transfer learning methods can be utilized instead.",1 Introduction,[0],[0]
"Transfer learning for cross-lingual cases is a type of transductive transfer learning, where the input domains of the source and the target are different (Pan and Yang, 2010) since each language has its own vocabulary space.",1 Introduction,[0],[0]
"When the input space is the same, lower layers of hierarchical models can be shared for knowledge transfer (Collobert et al., 2011; Kim et al., 2015b; Yang et al., 2017), but that approach is not directly applicable when the input spaces differ.
",1 Introduction,[0],[0]
Yang et al. (2017) used shared character embeddings for different languages as a cross-lingual transfer method while using different word embeddings for different languages.,1 Introduction,[0],[0]
"Although the approach showed improved performance on Named Entity Recognition, it is limited to character-level representation transfer and it is not applicable for knowledge transfer between languages without overlapped alphabets.
",1 Introduction,[0],[0]
"In this work, we introduce a cross-lingual transfer learning model for POS tagging requiring no cross-lingual resources, where knowledge transfer is made in the BLSTM layers on top of word embeddings and character embeddings.",1 Introduction,[0],[0]
"Inspired by Kim et al. (2016)’s multi-task slot-filling model, our model utilizes a common BLSTM for representing language-generic information, which al-
2832
lows knowledge transfer from other languages, and private BLSTMs for representing languagespecific information.",1 Introduction,[0],[0]
"The common BLSTM is additionally encouraged to be language-agnostic with language-adversarial training (Chen et al., 2016) so that the language-general representations to be more compatible among different languages.
",1 Introduction,[0],[0]
"Evaluating on POS datasets from 14 different target languages with English as the source language in the Universal Dependencies corpus 1.4 (Nivre et al., 2016), the proposed model showed significantly better performance when the source language and the target language are in the same language family, and competitive performance when the language families are different.",1 Introduction,[0],[0]
Cross-Lingual Training Figure 1 shows the overall architecture of the proposed model.,2 Model,[0],[0]
"The baseline POS tagging model is similar to Plank et al. (2016)’s model, and it corresponds to having only word+char embeddings, common BLSTM, and Softmax Output in Figure 1.",2 Model,[0],[0]
"Given an input
word sequence, a BLSTM is used for the character sequence of each word, where the outputs of the ends of the character sequences from the forward LSTM and the backward LSTM are concatenated to the word vector of the current word to supplement the word representation.",2 Model,[0],[0]
"These serve as an input to a BLSTM, and an output layer are used for POS tag prediction.
",2 Model,[0],[0]
"For the cross-lingual transfer learning, the character embedding, the BLSTM with the character embedding (Yang et al., 2017),1 and the common BLSTM are shared for all the given languages while word embeddings and private BLSTMs have different parameters for different languages.
",2 Model,[0],[0]
The outputs of the common BLSTM and the private BLSTM of the current language are summed to be used as the input to the softmax layer to predict the POS tags of given word sequences.,2 Model,[0],[0]
"The loss function of the POS tagging can be formulate as:
Lp = − S∑
i=1",2 Model,[0],[0]
"N∑ j=1 pi,j log (p̂i,j) , (1)
where S is the number of sentences in the current minibatch,N is the number of words in the current sentence, pi,j is the label of the j-th tag of the i-th sentence in the minibatch, and p̂i,j is the predicted tag.",2 Model,[0],[0]
"In addition to this main objective, two more objectives for improving the transfer learning are described in the following subsections.
",2 Model,[0],[0]
"Language-Adversarial Training We encourage the outputs of the common BLSTM to be language-agnostic by using language-adversarial training (Chen et al., 2016) inspired by domainadversarial training (Ganin et al., 2016; Bousmalis et al., 2016).",2 Model,[0],[0]
"First, we encode a BLSTM output sequence as a single vector using a CNN/MaxPool encoder, which is implemented the same as a CNN for text classification (Kim, 2014).",2 Model,[0],[0]
"The encoder is with three convolution filters whose sizes are 3, 4, and 5.",2 Model,[0],[0]
"For each filter, we pass the BLSTM output sequence as the input sequence and obtain a single vector from the filter output by using max pooling, and then tanh activation function is used for transforming the vector.",2 Model,[0],[0]
"Then, the vector outputs of the three filters are concatenated and forwarded to the language discriminator through the gradient reversal layer.",2 Model,[0],[0]
"The discriminator is implemented
1We also tried isolated character-level modules but the overall performance was worse.
as a fully-connected neural network with a single hidden layer, whose activation function is Leaky ReLU (Maas et al., 2013), where we multiply 0.2 to negative input values as the outputs.
",2 Model,[0],[0]
"Since the gradient reversal layer is below the language classifier, the gradients minimizing language classification errors are passed back with opposed sign to the sentence encoder, which adversarially encourages the sentence encoder to be language-agnostic.",2 Model,[0],[0]
"The loss function of the language classifier is formulated as:
La = − S∑
i=1
li log l̂i, (2)
where S is the number of sentences, li is the language of the i-th sentence, and l̂i is the softmax output of the tagging.",2 Model,[0],[0]
"Note that though the language classifier is optimized to minimize the language classification error, the gradient from the language classifier is negated so that the bottom layers are trained to be language-agnostic.
",2 Model,[0],[0]
"Bidirectional Language Modeling Rei (2017) showed the effectiveness of the bidirectional language modeling objective, where each time step of the forward LSTM outputs predicts the word of the next time step, and each of the backward LSTM outputs predicts the previous word.",2 Model,[0],[0]
"For example, if the current sentence is “I am happy”, the forward LSTM predicts “am happy <eos>” and the backward LSTM predicts “<bos> I am”.",2 Model,[0],[0]
"This objective encourages the BLSTM layers and the embedding layers to learn linguistically general-purpose representations, which are also useful for specific downstream tasks (Rei, 2017).",2 Model,[0],[0]
"We adopted the bidirectional language modeling objective, where the sum of the common BLSTM and the private BLSTM is used as the input to the language modeling module.",2 Model,[0],[0]
"It can be formulated as:
Ll = − S∑
i=1",2 Model,[0],[0]
N∑ j=1 log (P (wj+1|fj)),2 Model,[0],[0]
"+
log (P (wj−1|bj)) , (3)
where fj and bj represent the j-th outputs of the forward direction and the backward direction, respectively, given the output sum of the common BLSTM and the private BLSTM.
",2 Model,[0],[0]
"All the three loss functions are added to be optimized altogether as:
L = ws",2 Model,[0],[0]
"(Lp + λLa + λLl) , (4)
where λ is gradually increased from 0 to 1 as epoch increases so that the model is stably trained with auxiliary objectives (Ganin et al., 2016).",2 Model,[0],[0]
ws is used to give different weights to the source language and the target language.,2 Model,[0],[0]
"Since the source language has a larger train set and we are focusing on improving the performance of the target language, ws is set to 1 when training the target language.",2 Model,[0],[0]
"For the source language, instead, it is set as the size of the target train set divided by the size of the source train set.",2 Model,[0],[0]
"For the evaluation, we used the POS datasets from 14 different languages in Universal Dependencies corpus 1.4 (Nivre et al., 2016).",3 Experiments,[0],[0]
"We used English as the source language, which is with 12,543 training sentences.2",3 Experiments,[0],[0]
We chose datasets with 1k to 14k training sentences.,3 Experiments,[0],[0]
"The number of tag labels differs for each language from 15 to 18 though most of them are overlapped within the languages.
",3 Experiments,[0],[0]
"Table 1 shows the POS tagging accuracies of different transfer learning models when we limited the number of training sentences of the target languages to be the same as 1,280 for fair comparison among different languages.",3 Experiments,[0],[0]
The remainder training examples of the target languages are still used for both language-adversarial training and bidirectional language modeling since the objectives do not require tag labels.,3 Experiments,[0],[0]
Training with only the train sets in the target languages (c) showed 91.61% on average.,3 Experiments,[0],[0]
"When bidirectional language modeling objective is used (c, l), the accuracies were significantly increased to 92.82% on average.",3 Experiments,[0],[0]
"Therefore, we used the bidirectional language modeling for all the transfer learning evaluations.
",3 Experiments,[0],[0]
"With transfer learning, the three cases of using only the common BLSTM (c), using only the private BLSTMs (p), and using both (c, p) were evaluated.",3 Experiments,[0],[0]
"They showed better average accuracies than target only cases, but they showed mixed results.",3 Experiments,[0],[0]
"However, our proposed model (c, p, l + a), which utilizes both the common BLSTM with language-adversarial training and the private BLSTMs, showed the highest average score, 93.26%.",3 Experiments,[0],[0]
"For all the Germanic languages, where the source language also belongs to, the accuracies are significantly higher than those of
2The accuracies of English POS tagging are 94.01 and 94.33 for models without the bidirectional language modeling and with it, respectively.
",3 Experiments,[0],[0]
other transfer learning models.,3 Experiments,[0],[0]
"For the languages belonging to Slavic, Romance, or Indo-Iranian, our model shows competitive performance with the highest average accuracies among the compared models.",3 Experiments,[0],[0]
"Since languages in the same family are more likely to be similar and compatible, it is expected that the gain from the knowledge transfer to the languages in the same family to be higher than transferring to the languages in different families, which was shown in the results.",3 Experiments,[0],[0]
"This shows that utilizing both language-general representations that are encouraged to be more language-agnostic and language-specific representations effectively helps improve the POS tagging performance with transfer learning.
",3 Experiments,[0],[0]
Table 2 shows the results when using 320 taglabeled training sentences.,3 Experiments,[0],[0]
"In this case, transfer learning methods still show better accuracies than target-only approaches on average.",3 Experiments,[0],[0]
"However, the performance gain is weakened compared to using 1,280 labeled training sentences and there are some mixed results.",3 Experiments,[0],[0]
"In several cases, just utilizing private BLSTMs without the common BLSTM showed better accuracies than utilizing the common BLSTM.
",3 Experiments,[0],[0]
"When training with only 32 tag-labeled sentences, which is an extremely low-resourced setting, transfer learning methods still showed better accuracies than target-only methods on average.",3 Experiments,[0],[0]
"However, not using the common BLSTM
in transfer learning models showed better performance than using it on average.3",3 Experiments,[0],[0]
The main reason would be that we are not given a sufficient number of labeled training sentences to train both the common BLSTM and the private BLSTMs.,3 Experiments,[0],[0]
"In this case, just having private BLSTMs without the common BLSTM can show better performance.",3 Experiments,[0],[0]
"We also evaluated the opposite cases, which use all the tag-labeled training sentences in the target languages, and they showed mixed results.",3 Experiments,[0],[0]
"For example, the accuracy of German with the target only model is 93.31% while that of the proposed model is 93.04%.",3 Experiments,[0],[0]
"This is expected since transfer learning is effective when the target train set is small.
",3 Experiments,[0],[0]
An extension of this work is utilizing multiple languages as the source languages.,3 Experiments,[0],[0]
"Since we have four languages for each of Germanic, Slavic, and Romance language families, we evaluated the performance of those languages using the other languages in the same families as the source languages expecting that languages in the same language family are more likely to be helpful each other.",3 Experiments,[0],[0]
"For the efficiency, we performed multi-task learning for multiple languages rather than differentiating the targets from sources.",3 Experiments,[0],[0]
"When we tried to use 1,280, 320, and 32 tag-labeled training sentences for each language in the multi-source settings, the results showed noticeably better per-
3The results in detail are shown in the first authors dissertation Kim (2017).
formance than the results of using English as a single source language.",3 Experiments,[0],[0]
"Considering that utilizing 1,280*3=3,840, 320*3=960, or 32*3=96 tag labels from three other languages showed better results than using 12,543 English tag labels as the source, we can see that the knowledge transfer from multiple languages can be more helpful than that from single resource-rich source language.",3 Experiments,[0],[0]
"We also tried to use Wasserstein distance (Arjovsky et al., 2017) for the adversarial training in the multi-source settings, but there were no significant differences on average.4
Implementation Details All the models were optimized using ADAM (Kingma and Ba, 2015)5 with minibatch size 32 for total 100 epochs and we picked the parameters showing the best accuracy on the development set to report the score on the test set.",3 Experiments,[0],[0]
The dimensionalites of all the BLSTM related layers follow Plank et al. (2016)’s model.,3 Experiments,[0],[0]
Each word vector is 128 dimensional and each character vector is 100 dimensional.,3 Experiments,[0],[0]
"They are randomly initialized with Xavier initialization (Glorot and Bengio, 2010).",3 Experiments,[0],[0]
"For stable training, we use gradient clipping, where the threshold is set to 5.",3 Experiments,[0],[0]
"The dimensionality of each hidden output of LSTMs is 100, and the hidden outputs of both forward LSTM and backward LSTM are concatenated, thereby the output of each BLSTM for each time step is 200.",3 Experiments,[0],[0]
"Therefore, the input to the common BLSTM and the private BLSTM is 128+200=328
4The extended work in detail are shown in Kim (2017).",3 Experiments,[0],[0]
"5learning rate=0.001, β1 = 0.9, β2 = 0.999, = 1e− 8.
dimensional.",3 Experiments,[0],[0]
"The inputs and the outputs of the BLSTMs are regularized with dropout rate 0.5 (Pham et al., 2014).",3 Experiments,[0],[0]
"For the consistent dropout usages, we let the dropout masks to be identical for all the time steps of each sentence (Gal and Ghahramani, 2016).",3 Experiments,[0],[0]
"For all the BLSTMs, forget biases are initialized with 1 (Jozefowicz et al., 2015) and the other biases are initialized with 0.",3 Experiments,[0],[0]
"Each convolution filter output for the sentence encoding is 64 dimensional, and the three filter outputs are concatenated to represent each sentence with a 192 dimensional vector.",3 Experiments,[0],[0]
We introduced a cross-lingual transfer learning model for POS tagging which uses separate BLSTMs for language-general and languagespecific representations.,4 Conclusion,[0],[0]
"Evaluating on 14 different languages, including the source language improved tagging accuracies in almost all the cases.",4 Conclusion,[0],[0]
"Specifically, our model showed noticeably better performance when the source language and the target languages belong to the same language family, and competitively performed with the highest average accuracies for target languages in different families.",4 Conclusion,[0],[0]
We thank the anonymous reviewers for their helpful comments.,Acknowledgments,[0],[0]
All the experiments in this work were conducted with machines at Ohio Supercomputer Center (1987).,Acknowledgments,[0],[0]
Training a POS tagging model with crosslingual transfer learning usually requires linguistic knowledge and resources about the relation between the source language and the target language.,abstractText,[0],[0]
"In this paper, we introduce a cross-lingual transfer learning model for POS tagging without ancillary resources such as parallel corpora.",abstractText,[0],[0]
"The proposed cross-lingual model utilizes a common BLSTM that enables knowledge transfer from other languages, and private BLSTMs for language-specific representations.",abstractText,[0],[0]
The cross-lingual model is trained with language-adversarial training and bidirectional language modeling as auxiliary objectives to better represent language-general information while not losing the information about a specific target language.,abstractText,[0],[0]
"Evaluating on POS datasets from 14 languages in the Universal Dependencies corpus, we show that the proposed transfer learning model improves the POS tagging performance of the target languages without exploiting any linguistic knowledge between the source language and the target language.",abstractText,[0],[0]
Cross-Lingual Transfer Learning for POS Tagging without Cross-Lingual Resources,title,[0],[0]
"Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 998–1008, Baltimore, Maryland, USA, June 23-25 2014. c©2014 Association for Computational Linguistics",text,[0],[0]
"Discourse structure, logical flow of sentences, and context play a large part in ordering medical events based on temporal relations within a clinical narrative.",1 Introduction,[0],[0]
"However, cross-narrative temporal relation ordering is a challenging task as it is difficult to learn temporal relations among medical events which are not part of the logically coherent discourse of a single narrative.",1 Introduction,[0],[0]
"Resolving crossnarrative temporal relationships between medical events is essential to the task of generating an event timeline from across unstructured clinical narratives such as admission notes, radiology reports, history and physical reports and discharge summaries.",1 Introduction,[0],[0]
"Such a timeline has multiple applications in clinical trial recruitment (Luo et al., 2011), medical document summarization (Bramsen et al.,
2006, Reichert et al., 2010) and clinical decision making (Demner-Fushman et al., 2009).
",1 Introduction,[0],[0]
"Given multiple temporally ordered medical event sequences generated from each clinical narrative in a patient record, how can we combine the events to create a timeline across all the narratives?",1 Introduction,[0],[0]
"The tendency to copy-paste text and summarize past information in newly generated clinical narratives leads to multiple mentions of the same medical event across narratives (Cohen et al., 2013).",1 Introduction,[0],[0]
These cross-narrative coreferences act as important anchors for reasoning with information across narratives.,1 Introduction,[0],[0]
We leverage crossnarrative coreference information along with confident cross-narrative temporal relation predictions and learn to align and temporally order medical event sequences across longitudinal clinical narratives.,1 Introduction,[0],[0]
We model the problem as a sequence alignment task and propose solving this using two approaches.,1 Introduction,[0],[0]
"First, we use weighted finite state machines to represent medical events sequences, thus enabling composition and search to obtain the most probable combined sequence of medical events.",1 Introduction,[0],[0]
"As a contrast, we adapt dynamic programming algorithms (Needleman et al., 1970, Smith and Waterman, 1981) used to produce global and local alignments for aligning sequences of medical events across narratives.",1 Introduction,[0],[0]
"We also compare the proposed methods with an Integer Linear Programming (ILP) based method for timeline construction (Do et al., 2012).",1 Introduction,[0],[0]
"The cross-narrative coreference and temporal relation scores used in both these approaches are learned from a corpus of patient narratives from The Ohio State University Wexner Medical Center.
",1 Introduction,[0],[0]
The main contribution of this paper is a general framework that allows aligning multiple event sequences using cascaded weighted finite state transducers (WFSTs) with the help of efficient composition and decoding.,1 Introduction,[0],[0]
"Moreover, we demonstrate that this method can be used for more accurate multiple sequence alignment when compared to
998
dynamic programming or other ILP-based methods proposed in literature.",1 Introduction,[0],[0]
"In the areas of summarization and text-to-text generation, there has been prior work on several ordering strategies to order pieces of information extracted from different input documents (Barzilay et al., 2002, Lapata, 2003, Bollegala et al., 2010).",2 Related Work,[0],[0]
"In this paper, we focus on temporal ordering of information, as discussed next.
",2 Related Work,[0],[0]
"Recent state-of-the art research has focused on the problem of temporal relation learning within the same document, and in many cases within the same sentence (Mani et al., 2006, Verhagen et al., 2009, Lapata and Lascarides, 2011).",2 Related Work,[0],[0]
"Chambers and Jurafsky (2009) describe a process to induce a partially ordered set of events related by a common protagonist by using an unsupervised distributional method to learn relations between events sharing coreferring arguments, followed by temporal classification to induce partial order.",2 Related Work,[0],[0]
"The task was carried out on the Timebank newswire corpus, but was limited to an intra-document setting.",2 Related Work,[0],[0]
"More recently, (Do et al., 2012) proposed an ILP-based method to combine the outputs of an event-interval and an event-event classifier for timeline construction on the ACE 2005 corpus.",2 Related Work,[0],[0]
"However, this approach is also restricted to events within documents and requires annotations for event intervals.",2 Related Work,[0],[0]
We empirically compare our methods for timeline creation from longitudinal clinical narratives to such an ILP-based approach in Section 7.,2 Related Work,[0],[0]
"While a lot of this work has been done in the news domain, there is also some recent work in rule-based algorithms (Zhou et al., 2006) and machine learning (Roberts et al., 2008) applied to temporal relations between medical events in clinical text.",2 Related Work,[0],[0]
"Clinical narratives are written in a distinct sub-language with domain specific terminology and temporal characteristics, making them markedly different from newswire text.
",2 Related Work,[0],[0]
There is limited prior work in learning relations across documents.,2 Related Work,[0],[0]
"Ji and Grishman (2008) extended the one sense per discourse idea (Yarowsky, 1995) to multiple topically related documents and propagate consistent event arguments across sentences and documents.",2 Related Work,[0],[0]
Barzilay and McKeown (2005) propose a text-to-text generation technique for synthesizing common information across documents using sentence fusion.,2 Related Work,[0],[0]
"This involves multisequence dependency tree alignment to identify phrases conveying sim-
ilar information and statistical generation to combine common phrases into a sentence.",2 Related Work,[0],[0]
"Along with syntactic features, they combine knowledge from resources like WordNet to find similar sentences.",2 Related Work,[0],[0]
"In case of clinical narratives and medical event alignment, the objective is to identify a unique sequence of temporally ordered medical events from across longitudinal clinical data.
",2 Related Work,[0],[0]
"To the best of our knowledge, there is no prior work on cross-document alignment of event sequences.",2 Related Work,[0],[0]
"Multiple sequence alignment is a problem that arises in a variety of domains including gene/protein alignments in bioinformatics (Notredame, 2002), word alignments in machine translation (Kumar and Byrne, 2003), and sentence alignments for summarization (Lacatusu et al., 2004).",2 Related Work,[0],[0]
"Dynamic programming algorithms have been popularly leveraged to produce pairwise and global genetic alignments, where edit distance based metrics are used to compute the cost of insertions, deletions and substitutions.",2 Related Work,[0],[0]
"We use dynamic programming to compute the best alignment, given the temporal and coreference information between medical events across these sequences.",2 Related Work,[0],[0]
"More importantly, we propose a cascaded WFST-based framework for crossdocument temporal ordering of medical event sequences.",2 Related Work,[0],[0]
"Composition and search operations can be used to build a single transducer that integrates these components, directly mapping from input states to desired outputs, and obtain the best alignment (Mohri et al., 2000).",2 Related Work,[0],[0]
"In natural language processing, WFSTs have seen varied applications in machine translation (Kumar and Byrne, 2003), morphology (Sproat, 2006), named entity recognition (Krstev et al., 2011) and biological sequence alignment / generation (Whelan et al., 2010) among others.",2 Related Work,[0],[0]
We demonstrate that the WFST-based approach outperforms popularly used dynamic programming algorithms for multiple sequence alignment.,2 Related Work,[0],[0]
"Medical events are temporally-associated concepts in clinical text that describe a medical condition affecting the patient’s health, or procedures performed on a patient.",3 Problem Description,[0],[0]
We represent medical events by splitting each event into a start and a stop.,3 Problem Description,[0],[0]
"When there is insufficient information to discern the start or stop of an event, it is represented as a single concept.",3 Problem Description,[0],[0]
"If only the start is known then the stop is set to +∞, whereas when only the stop is known , the start is set to the date of birth of the
patient.1 Often, for chronic ailments like hypertension, we would only associate a start with the medical event and set the stop to +∞. The start of hypertension may be associated with the temporal expression history of in the narrative.",3 Problem Description,[0],[0]
"This, when considered along with the admission date, allows us to relatively order hypertension with respect to other medical events.",3 Problem Description,[0],[0]
"A medical event occurrence like chest pain may be associated with a start and a stop, where the start may be determined by the mention of “patient was complaining of chest pain yesterday” in the narrative text.",3 Problem Description,[0],[0]
"Further, the narrative may state that “he continued to have chest pain on admission, but currently he is chest pain free”; this may be used to infer the relative stop of chest pain.",3 Problem Description,[0],[0]
"Medical events may also be instantaneous, for e.g., injected with antibiotic.",3 Problem Description,[0],[0]
Such events are represented with the start and stop as being the same.,3 Problem Description,[0],[0]
Temporal relations exist between the start and stop of events as shown in Figure 1.,3 Problem Description,[0],[0]
"Learning temporal relations before, after and simultaneous between the medical event starts and stops corresponds to learning all of Allen’s temporal relations (Allen, 1981) between the medical events.",3 Problem Description,[0],[0]
"Following our previous work (Raghavan et al., 2012c), such a representation allows us to temporally order the event starts and stops within each clinical narrative by learning to rank them in relative order of time.",3 Problem Description,[0],[0]
"The problem definition is as follows:
1Patient date of birth, admission/ discharge date are usually available in the metadata associated with a clinical narrative.
",3 Problem Description,[0],[0]
Input: Sequences of temporally ordered medical event starts and stops.,3 Problem Description,[0],[0]
"This corresponds to N1, N2, and N3 in Figure 2.",3 Problem Description,[0],[0]
Each sequence corresponds to a clinical narrative.,3 Problem Description,[0],[0]
"The total number of sequences correspond to the number of clinical narratives for a patient.
",3 Problem Description,[0],[0]
Problem:,3 Problem Description,[0],[0]
"Combine medical events across these sequences to generate a timeline i.e., a single comprehensive sequence of medical events over all clinical narratives of the patient.
",3 Problem Description,[0],[0]
Expected Output:,3 Problem Description,[0],[0]
"In the example shown in Figure 2, the output would be as follows:",3 Problem Description,[0],[0]
"Timeline (N1, N2, N3)= {cocaine usestart < hypertensionstart = hypertensionstart < admission1 < chest painstart ∼ palpitationsstart < chest painstop < heart attackstart = myocardial infarctionstart <",3 Problem Description,[0],[0]
"admission2 < infectionstart < MRSAstart < admission3 < woundsstart}.
",3 Problem Description,[0],[0]
The goal of multiple sequence alignment is to find an alignment that maximizes some overall alignment score.,3 Problem Description,[0],[0]
"Thus, in order to align event sequences, we need to compute scores corresponding to cross-narrative medical event coreference resolution and cross-narrative temporal relations.",3 Problem Description,[0],[0]
"The first approach to learning a temporal ordering of medical events across all clinical narratives is to consider all pairs of events across all narratives and learn to classify them as sharing one of Allen’s temporal relations (Allen, 1981) using a single learning model.",4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[0],[0]
"Alternatively, a ranking ap-
proach, similar to the one used to generate intranarrative temporal ordering, can also be extended to the cross-narrative case.",4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[0],[0]
"However, the features related to narrative structure and relative and implicit temporal expressions used for temporal ordering within a clinical narrative may not be applicable across narratives.",4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[0],[0]
"For instance, a history and physical report may have sections like “past medical history”, “history of present illness”, “assessment and plan”, and a certain logical pattern to the flow of text within and across these sections.",4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[0],[0]
"Further, temporal cues like “thereafter”, “subsequently”, follow from the context around an event mention.",4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[0],[0]
"The absence of such features in the cross-narrative case does not allow such a model to generate accurate temporal relation predictions.
",4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[0],[0]
"Thus, for use in our sequence alignment models, we learn two independent classifiers for medical event coreference and temporal relation learning across narratives.",4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[0],[0]
We train a classifier to resolve cross-narrative coreferences by extracting semantic and temporal relatedness feature sets for each pair of medical concepts.,4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[0],[0]
"Extracting these feature sets helps us train a classifier to predict medical event coreferences (Raghavan et al., 2012a).",4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[0],[0]
"Another classifier is then trained to classify pairs of medical event starts and stops across narratives as sharing temporal relations {before, after, overlaps}.",4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[0],[0]
The learned cross-narrative coreference predictions can then be used along with confident temporal relation predictions to derive a joint probability to enable cross-narrative temporal ordering.,4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[0],[0]
Sequence alignment algorithms have been developed and popularly used in bioinformatics.,5 Narrative Sequence Alignment for Cross-narrative Temporal Ordering,[0],[0]
"However, multiple sequence alignment (MSA) has been shown to be NP complete (Wang and Jiang, 1994) and various heuristic algorithms have been proposed to solve this problem (Notredame, 2002).",5 Narrative Sequence Alignment for Cross-narrative Temporal Ordering,[0],[0]
"We propose a novel WFST-based representation that enables accurate decoding for MSA when compared to popularly used dynamic programming algorithms (Needleman et al., 1970, Smith and Waterman, 1981) or other state of the art methods (Do et al., 2012).
",5 Narrative Sequence Alignment for Cross-narrative Temporal Ordering,[0],[0]
"In the problem of aligning events across multiple narrative sequences, we want to align temporally ordered medical events corresponding to clinical narratives of a patient.",5 Narrative Sequence Alignment for Cross-narrative Temporal Ordering,[0],[0]
"Unlike problems in biological sequence alignment where the sym-
bols to be aligned across sequences are restricted to a fixed set, our symbol set is not fixed or certain because the symbols correspond to medical events in clinical narratives.",5 Narrative Sequence Alignment for Cross-narrative Temporal Ordering,[0],[0]
"Moreover, we cannot have fixed scores for symbol transformations since our transformations correspond to coreference and temporal relations between the medical events across sequences.",5 Narrative Sequence Alignment for Cross-narrative Temporal Ordering,[0],[0]
The computation of these scores is described next.,5 Narrative Sequence Alignment for Cross-narrative Temporal Ordering,[0],[0]
"Let us assume a, b are medical events in the first clinical narrative and have been temporally ordered so a < b.",5.1 Scoring Scheme,[0],[0]
"Similarly, x, y are medical events in the second clinical narrative such that x",5.1 Scoring Scheme,[0],[0]
< y.,5.1 Scoring Scheme,[0],[0]
"There exists a match or an alignment between a pair of medical events, across the sequences, in the following cases:
1.",5.1 Scoring Scheme,[0],[0]
"If the medical events are simultaneous and coreferring, denoted as a = x.
2.",5.1 Scoring Scheme,[0],[0]
"If the medical events are simultaneous and non-coreferring, denoted as a ∼ x.
3.",5.1 Scoring Scheme,[0],[0]
"If the a medical event from one sequence is before a medical event from another sequence, denoted as a < x.
4.",5.1 Scoring Scheme,[0],[0]
"If the a medical event from one sequence is after a medical event from another sequence, denoted as a > x.
We now illustrate how the scores for candidate aligned sequences are computed using the learned cross-narrative coreference and temporal probabilities for the following three scenarios:
• The medical events across sequences are simultaneous and corefer as illustrated in Figure 3.",5.1 Scoring Scheme,[0],[0]
"The joint score considers the probability of event temporal relations simultaneous conditioned on coreference.
",5.1 Scoring Scheme,[0],[0]
•,5.1 Scoring Scheme,[0],[0]
Some medical events across sequences are simultaneous but do not corefer as illustrated in Figure 4.,5.1 Scoring Scheme,[0],[0]
"Here, the joint score considers the joint probability of temporal relations simultaneous or before and no-coreference.
",5.1 Scoring Scheme,[0],[0]
•,5.1 Scoring Scheme,[0],[0]
The medical events across sequences are not simultaneous and do not corefer as illustrated in Figure 5.,5.1 Scoring Scheme,[0],[0]
"In this case, the joint score considers the probability of the temporal relation before and no coreference.
",5.1 Scoring Scheme,[0],[0]
"Thus, the coreference and temporal relation scores can be leveraged for aligning sequences of medical events.",5.1 Scoring Scheme,[0],[0]
"These scores are used in both the WFSTbased representation and decoding, as well as for dynamic programming.",5.1 Scoring Scheme,[0],[0]
"A weighted finite-state transducer (WFST) is an automaton in which each transition between states
is associated with an input symbol, an output symbol, and a weight (Mohri et al., 2005).",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
WFSTs can be used to efficiently represent and combine sequences of medical events based coreference and temporal relation information.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
The WFST representation gives us the ability to talk about the global joint probability derived from coreference and temporal relation scores described in Section 5.1.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
It allows us to build a weighted lattice of sequences that can be searched for the most probable sequence of medical events from across all clinical narratives of a patient.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"We use unweighted FSAs to represent the input described in Section 3, i.e. temporally ordered sequences of medical events corresponding to clinical narratives.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"This corresponds to N1 and N2 in Figure 6.
",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"Based on whether we want to align the sequences purely based on coreference scores or both coreference and temporal relation scores, the arc weights for the WFST can be determined.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
M c12 is a WFST that maps input symbols from N1 to output symbols inN2 and is weighted by the probability of coreference or no-coreference between medical events across N1 and N2.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
The representation in WFST M c+t12 shown in Figure 7 allows us to align N1 and N2 based on both coreference as well as temporal relation probabilities.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
The WFST has transitions to accommodate insertion and deletion of medical events when combining the sequences.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
Deletions correspond to the case when an event in the first sequence does not map to any event in the second sequence; similarly insertions correspond to the case where an event in the second sequence does not map to any event in the first sequence.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
The WFST composition operation allows the outputs of one WFST to be fed to the inputs of a second WFST or FSA.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"Thus, we build our final machine by composing the three sub-machines as,
D = N1 ◦M i12 ◦N2.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
(1) where i = c or i = c + t.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
This gives us a combined weighted graph by mapping the output symbols of the first medical event sequence to the input symbols of the second medical event sequence.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"The scores on the decoding graph are derived from only the coreference probabilities if i = c and both coreference and temporal relation probabilities if i = c+ t.
In the medical event sequence alignment problem, we want to align multiple sequences of medical events that correspond to multiple clinical narratives of a patient.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"Since we want to now combine
all narrative chains belonging to the same patient, the composition cascade to build the final combined sequence will be as,
Df = N1◦M i12◦N2◦M i23◦N3◦M i34...◦",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"Nn (2)
where i = c",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
or i = c + t and n is the number of medical event sequences corresponding to clinical narratives for a patient.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"During composition we retain intermediate paths like M i23 utilizing the ability to do lazy composition (Mohri and Pereira, 1998) in order to facilitate beam search through the multi-alignment.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
The best hypothesis corresponds to the highest scoring path which can be obtained using shortest path algorithms like Djikstra’s algorithm.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"The best path corresponds to the best alignment across all medical event sequences based on the joint probability of cross-narrative medical event coreferences and temporal relations across the narrative sequences.
",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"The complexity of decoding increases exponentially with the number of narrative sequences in
the composition, and exact decoding becomes infeasible.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"One solution to this problem is to do the alignment greedily pairwise, starting from the most recent medical event sequences, finding the best path, and iteratively moving on to the next sequence, and proceeding until the oldest medical event sequence.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"The disadvantage of such a method is that it does not take into account constraints between medical events across multiple event sequences and may lead to a less accurate solution.
",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
An alternative method is to use lazy composition to perform more efficient composition as it allows practical memory usage.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"We also use beam search to make for an efficient approximation to the best-path computation (Mohri et al., 2005).",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
This allows accommodating constraints from across multiple sequences and generates a more accurate best path.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"Thus, this method generates more accurate alignments when we have more than two sequences to be aligned.
",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"For instance, instance say a, b ∈ N1, x, y ∈ N2, and m,",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"n ∈ N3 are temporally medical event sequences corresponding to narratives N1, N2 and N3.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"Based on the learned pairwise temporal relations, if we have the following constraints a < x, m > x, m < a.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"Aligning N1 and N2 greedily pairwise may give us the best combined sequence as a, x, b, y ∈ N12.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"Now in aligning N12 with N3, we won’t be able to accommodate m > x",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
and m < a.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"However, performing a beam search over the composed WFST in equation 2 allows us to accommodate such constraints across multiple sequences.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"The complexity of composing two transducers is O(V1V2D1(logD2 + M2)) where each edge from the first sequence matches every edge in the second sequence and Vi is the number of states, Di is the maximum out-degree and Mi maximum multiplicity for the ith FST (Mohri et al., 2005).
",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"We also use popular dynamic programming algorithms (Needleman et al., 1970, Smith and Waterman, 1981) for sequence alignment of medical events across narratives and compare it to the WFST-based representation and decoding.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"As a contrast, we adapt two dynamic programming algorithms for sequence alignment: global alignment using the Needleman Wunsch algorithm (NW) (Needleman et al., 1970) and local alignment using the Smith-Waterman algorithm (SW) (Smith and Waterman, 1981).",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
NW allows us to align all events in one sequence with all events in another sequence.,5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
A drawback of NW is that short and highly similar sequences maybe missed because they get overweighted by the rest of the sequence.,5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
NW is suitable when the two sequences are of similar length with significant degree of similarity throughout.,5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"On the other hand, SW gives the longest sub-sequence pair that yields maximum degree of similarity between the two original sequences.",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
It does not force all events in a sequence to align with another sequence.,5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
SW is useful in aligning sequences that differ in length and have short patches of similarity.,5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"The time complexity of these methods for sequences of length m and n are O(mn).
",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
The scoring scheme described earlier is used to update the scoring matrix for dynamic programming.,5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"In order to accommodate the temporal relations before and after, we insert a null symbol after every medical event in each sequence in the scoring matrix.",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"A vertical or horizontal gap arises when cases 1, 2, 3 and 4 in Section 5.1 mentioned
above are not true.",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"If the medical events are not simultaneous, not before or not after, the medical events will not align.",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"Thus, the value of each cell in the scoring matrix is determined by computing the maximum score at each position C(i, j) as,
max{(C(i−1, j−1)+Sij), (C(i, j−1)+w), (C(i− 1, j) + w)} (3)
where, Sij = max{P (i = j), P (i < j), P (i > j)}, and w = max{(1",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"− P (i = j)), (1 − P (i < j)), (1 − P (i > j))}.",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"Here, C(i − 1, j − 1) corresponds to a match, whereas C(i, j − 1) and C(i − 1, j) correspond to a gaps in sequence one and two.
",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"In case of the SW algorithm, the negative scoring matrix cells are set to zero, thus making the positively scoring local alignments visible.",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"Backtracking starts at the highest scoring matrix cell and proceeds until a cell with score zero is encountered, yielding the highest scoring local alignment.
",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
The time and space complexity grows exponentially with the number of sequences to be aligned and finding the global optimum has been shown to be a NP-complete problem.,5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"The time complexity of aligning N sequences of length L is O(2NLN ) (Wang and Jiang, 1994).",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"Thus, for MSA using dynamic programming, we use a heuristic method where we combine pairwise alignments iteratively starting with the latest narrative and progressing towards the oldest narrative.",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
Corpus Description.,6 Experiments and Evaluation,[0],[0]
The corpus consists of a dataset of clinical narratives obtained from the [redacted] medical center.,6 Experiments and Evaluation,[0],[0]
"The corpus has a total of 2060 patients, and 100704 clinical narratives.",6 Experiments and Evaluation,[0],[0]
"We gathered a gold standard set of seven patients (80 clinical narratives overall) with manual annotation of all medical events mentioned in the narratives, coreferences, and medical event sequence information.",6 Experiments and Evaluation,[0],[0]
"The annotation agreement across annotators is high, with 89.5% agreement corresponding to inter-annotator Cohen’s kappa statistic of 0.86 (Raghavan et al., 2012b).",6 Experiments and Evaluation,[0],[0]
"The types of clinical narratives included 27 discharge summaries, 30 history and physical reports, 15 radiology reports and 8 pathology reports.",6 Experiments and Evaluation,[0],[0]
The distribution of the number of medical event sequences and unique medical events across patients is shown in Table 1.,6 Experiments and Evaluation,[0],[0]
"The annotated dataset is used to crossvalidate and train our coreference and temporal relation learning models and to evaluate our crossnarrative medical event timeline.
",6 Experiments and Evaluation,[0],[0]
Evaluation Metric.,6 Experiments and Evaluation,[0],[0]
"For each patient and each method (WFST or dynamic programming), the output timeline to evaluate is the highest scoring candidate hypothesis derived as described above.",6 Experiments and Evaluation,[0],[0]
Accuracy of the timeline is calculated as the number of transformations required to obtain the reference sequence in the annotated gold-standard from the one generated by our system.,6 Experiments and Evaluation,[0],[0]
"Transformations are measured in terms of the minimum edit distance, insertions, deletions, and substitutions of medical events.
",6 Experiments and Evaluation,[0],[0]
Experiments and Results.,6 Experiments and Evaluation,[0],[0]
"We first temporally order medical events within each clinical narrative by learning to rank them in relative order of occurence as described in our previous work (Raghavan et al., 2012c).",6 Experiments and Evaluation,[0],[0]
The overall accuracy of ranking medical events using leave-one-out cross validation is 82.1%.,6 Experiments and Evaluation,[0],[0]
"The resulting medical event sequences serve as the input to the problem of crossnarrative sequence alignment.
",6 Experiments and Evaluation,[0],[0]
The cross-narrative coreference and temporal relation pairwise classification models described in Section 4 are trained using a Maximum entropy classifier.,6 Experiments and Evaluation,[0],[0]
The coreference resolution performs with 71.5% precision and 82.3% recall.,6 Experiments and Evaluation,[0],[0]
The temporal relation classifier performs with 60.2% precision and 76.3% recall.,6 Experiments and Evaluation,[0],[0]
"The learned pairwise coreference and temporal relation probabilities are now used to derive the score for the WFST and dynamic programming approaches.
WFST representation and decoding.",6 Experiments and Evaluation,[0],[0]
We build finite-state machines using the open source OpenFST,6 Experiments and Evaluation,[0],[0]
library.2,6 Experiments and Evaluation,[0],[0]
We use a tropical semi-ring weighted using the negative log-likelihood of the computed scores.,6 Experiments and Evaluation,[0],[0]
"OpenFST provides tools that can search for the highest scoring sequences accepted by the machine, and can sample from highscoring sequences probabilistically, by treating the
2www.openfst.org
scores of each transition within the machine as a negative log probability.",6 Experiments and Evaluation,[0],[0]
The decoding process to compute the most likely combined medical event sequence can be defined as searching for the best path in the combined graph representation (Equation 2).,6 Experiments and Evaluation,[0],[0]
The best path is the one that minimizes the total weight on a path (since the arcs are negative log probabilities).,6 Experiments and Evaluation,[0],[0]
"In searching for the best path, the beam size is set to 5.",6 Experiments and Evaluation,[0],[0]
"The accuracy of the WFST-based representation and beam search across all sequences using the coreference and temporal relation scores to obtain the combined aligned sequence is 78.9%.
",6 Experiments and Evaluation,[0],[0]
Dynamic Programming.,6 Experiments and Evaluation,[0],[0]
We use the NW and SW algorithms described in Section 5.3 to produce local and global alignments respectively.,6 Experiments and Evaluation,[0],[0]
We use the scoring scheme described in Section 5.1 to update the cost matrix for dynamic programming and implement the algorithms as described in Section 5.3.,6 Experiments and Evaluation,[0],[0]
The overall accuracy of sequence alignment with both coreference and temporal relation scores using NW is 68.7% whereas SW gives an accuracy of 72.1%.,6 Experiments and Evaluation,[0],[0]
"In case of aligning just two sequences, both methods yield the same results.",6 Experiments and Evaluation,[0],[0]
"The accuracy of cross-narrative MSA for each patient, for each method, using cross validation, is shown in Table 1.",6 Experiments and Evaluation,[0],[0]
Results indicate that the WFSTbased method outperforms the dynamic programming approach for multi-sequence alignment (statistical significance p<0.05).,6 Experiments and Evaluation,[0],[0]
"Morever, the results using both coreference and temporal realtion scores for alignment outperform using only coreference scores for alignment using all approaches.",6 Experiments and Evaluation,[0],[0]
This indicates that cross-narrative temporal relations are important for accurately aligning medical event sequences across narratives.,6 Experiments and Evaluation,[0],[0]
"We propose and evaluate different approaches to multiple sequence alignment of medical events.
",7 Discussion,[0],[0]
Approaches to multi-alignment.,7 Discussion,[0],[0]
We address the problem of aligning medical event sequences using a novel WFST-based framework and empirically demonstrate that it outperforms pairwise progressive alignment using dynamic programming.,7 Discussion,[0],[0]
"This is mainly because the WFST-based allows us to consider temporal constraints from across multiple sequences when performing the alignment.
",7 Discussion,[0],[0]
"Moreover, it also outperforms the integer linear programming (ILP) method for timeline construction proposed in (Do et al., 2012).",7 Discussion,[0],[0]
We implemented the proposed method that also allows combining the output of classifiers subject to some constraints.,7 Discussion,[0],[0]
We derive intervals from event starts and stops and learn two perceptron classifiers for classifying the temporal relations between events and assigning events to intervals.,7 Discussion,[0],[0]
The classifier probabilities are then used to solve the optimization problem using the lpsolve solver.3,7 Discussion,[0],[0]
We also use intra-document coreference information to resolve coreference before performing the global optimization.,7 Discussion,[0],[0]
"We observe that in case of MSA, the optimal solution using ILP is still intractable as the number of constraints increases exponentially with the number of sequences.",7 Discussion,[0],[0]
Aligning pairwise iteratively gives us an overall average accuracy of 68.2% similar to dynamic programming.,7 Discussion,[0],[0]
"While this is comparable to the dynamic programming performance, the WFST-based method significantly outperforms this in case of multialignments for cross-narrative temporal ordering.
",7 Discussion,[0],[0]
Performance and error analysis.,7 Discussion,[0],[0]
"We perform multi-alignments over medical event sequences for a patient, where each sequence corresponds to temporally ordered medical events in a clinical narrative generated using the ranking model described in (Raghavan et al., 2012c).",7 Discussion,[0],[0]
The accuracy of intra-narrative temporal ordering is 82.1%.,7 Discussion,[0],[0]
The errors in performing this intra-narrative ordering may propagate to the cross-narrative model resulting in reduced accuracy.,7 Discussion,[0],[0]
"This may be addressed by considering n-best temporally ordered medical event sequences, generated by the ranking process, and aligning the n-best sequences using the WFST-based framework.",7 Discussion,[0],[0]
"This could be feasible as, practically, the WFST-based method for multialignment takes only a few secs to align a pair of medical event sequences with average length 40.
",7 Discussion,[0],[0]
The accuracy of alignments across multiple medical event sequences is also affected by the error induced by the coreference and temporal relation scores.,7 Discussion,[0],[0]
"Often, insufficient temporal cues leads
3http://lpsolve.sourceforge.net/5.5/
to misclassification of events incorrectly as sharing the “simultaneous” temporal relation and often as coreferring.",7 Discussion,[0],[0]
This induces errors in the score calculation and hence the alignments.,7 Discussion,[0],[0]
"Better methods to address the challenging problem of crossdocument temporal relation learning, perhaps with the help of structured data from the patient record, could improve the accuracy of alignments.
",7 Discussion,[0],[0]
"There is no clear trend with respect to the number of medical events and narratives for a patient (Table 1.), and the alignment accuracy.",7 Discussion,[0],[0]
"In future work, it would be interesting to examine any such correlation and also study the scalability of the WFST-based method for sequence alignment on longer medical event sequences and a larger dataset of patients.",7 Discussion,[0],[0]
"Further, the WFST-based method may be used to model multi-alignment tasks in other speech and language problems as well.",7 Discussion,[0],[0]
We propose a novel framework for aligning medical event sequences across clinical narratives based on coreference and temporal relation information using cascaded WFSTs.,8 Conclusion,[0],[0]
FSTs provide a convenient and flexible framework to model sequences of temporally ordered medical events and compose them into a combined graph representation.,8 Conclusion,[0],[0]
Decoding this graph allows us to jointly maximize coreference as well as temporal relation probabilities to derive a timeline of the most likely temporal ordering of medical events.,8 Conclusion,[0],[0]
This approach to aligning multiple sequences of medical events significantly outperforms other approaches such as dynamic programming.,8 Conclusion,[0],[0]
"Moreover, we demonstrate the importance of learning temporal relations for the task timeline generation from across multiple clinical narratives by empirically proving that decoding using both coreference and temporal relation scores is far more accurate than decoding with only coreference scores.",8 Conclusion,[0],[0]
The project was supported by Award Number Grant R01LM011116 from the National Library of Medicine.,Acknowledgments,[0],[0]
The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Library of Medicine or the National Institutes of Health.,Acknowledgments,[0],[0]
The authors would like to thank Yanzhang He for his input on the WFST-based model.,Acknowledgments,[0],[0]
Cross-narrative temporal ordering of medical events is essential to the task of generating a comprehensive timeline over a patient’s history.,abstractText,[0],[0]
"We address the problem of aligning multiple medical event sequences, corresponding to different clinical narratives, comparing the following approaches: (1) A novel weighted finite state transducer representation of medical event sequences that enables composition and search for decoding, and (2) Dynamic programming with iterative pairwise alignment of multiple sequences using global and local alignment algorithms.",abstractText,[0],[0]
The cross-narrative coreference and temporal relation weights used in both these approaches are learned from a corpus of clinical narratives.,abstractText,[0],[0]
We present results using both approaches and observe that the finite state transducer approach performs performs significantly better than the dynamic programming one by 6.8% for the problem of multiple-sequence alignment.,abstractText,[0],[0]
Cross-narrative temporal ordering of medical events,title,[0],[0]
Relation extraction has made great strides in newswire and Web domains.,1 Introduction,[0],[0]
"Recently, there has
∗",1 Introduction,[0],[0]
"This research was conducted when the authors were at Microsoft Research.
been increasing interest in applying relation extraction to high-value domains such as biomedicine.",1 Introduction,[0],[0]
"The advent of $1000 human genome1 heralds the dawn of precision medicine, but progress in personalized cancer treatment has been hindered by the arduous task of interpreting genomic data using prior knowledge.",1 Introduction,[0],[0]
"For example, given a tumor sequence, a molecular tumor board needs to determine which genes and mutations are important, and what drugs are available to treat them.",1 Introduction,[0],[0]
"Already the research literature has a wealth of relevant knowledge, and it is growing at an astonishing rate.",1 Introduction,[0],[0]
"PubMed2, the online repository of biomedical articles, adds two new papers per minute, or one million each year.",1 Introduction,[0],[0]
"It is thus imperative to advance relation extraction for machine reading.
",1 Introduction,[0],[0]
"In the vast literature on relation extraction, past work focused primarily on binary relations in single sentences, limiting the available information.",1 Introduction,[0],[0]
"Consider the following example: “The deletion mutation on exon-19 of EGFR gene was present in 16 patients, while the L858E point mutation on exon-21 was noted in 10.",1 Introduction,[0],[0]
All patients were treated with gefitinib and showed a partial response.”.,1 Introduction,[0],[0]
"Collectively, the two sentences convey the fact that there is a ternary interaction between the three entities in bold, which is not expressed in either sentence alone.",1 Introduction,[0],[0]
"Namely, tumors with L858E mutation in EGFR gene can be treated with gefitinib.",1 Introduction,[0],[0]
Extracting such knowledge clearly requires moving beyond binary relations and single sentences.,1 Introduction,[0],[0]
N -ary relations and cross-sentence extraction have received relatively little attention in the past.,1 Introduction,[0],[0]
"Prior
1http://www.illumina.com/systems/ hiseq-x-sequencing-system.html
2https://www.ncbi.nlm.nih.gov/pubmed
ar X
iv :1
70 8.
03 74
3v 1
[ cs
.C",1 Introduction,[0],[0]
"L
] 1
2 A
ug 2
work on n-ary relation extraction focused on single sentences (Palmer et al., 2005; McDonald et al., 2005) or entity-centric attributes that can be extracted largely independently (Chinchor, 1998; Surdeanu and Heng, 2014).",1 Introduction,[0],[0]
"Prior work on cross-sentence extraction often used coreference to gain access to arguments in a different sentence (Gerber and Chai, 2010; Yoshikawa et al., 2011), without truly modeling inter-sentential relational patterns.",1 Introduction,[0],[0]
(See Section 7 for a more detailed discussion.),1 Introduction,[0],[0]
"A notable exception is Quirk and Poon (2017), which applied distant supervision to general cross-sentence relation extraction, but was limited to binary relations.
",1 Introduction,[0],[0]
"In this paper, we explore a general framework for cross-sentence n-ary relation extraction, based on graph long short-term memory networks (graph LSTMs).",1 Introduction,[0],[0]
"By adopting the graph formulation, our framework subsumes prior approaches based on chain or tree LSTMs, and can incorporate a rich set of linguistic analyses to aid relation extraction.",1 Introduction,[0],[0]
"Relation classification takes as input the entity representations learned from the entire text, and can be easily extended for arbitrary relation arity n. This approach also facilitates joint learning with kindred relations where the supervision signal is more abundant.
",1 Introduction,[0],[0]
We conducted extensive experiments on two important domains in precision medicine.,1 Introduction,[0],[0]
"In both distant supervision and supervised learning settings, graph LSTMs that encode rich linguistic knowledge outperformed other neural network variants, as well as a well-engineered feature-based classifier.",1 Introduction,[0],[0]
Multitask learning with sub-relations led to further improvement.,1 Introduction,[0],[0]
"Syntactic analysis conferred a significant benefit to the performance of graph LSTMs, especially when syntax accuracy was high.
",1 Introduction,[0],[0]
"In the molecular tumor board domain, PubMedscale extraction using distant supervision from a
small set of known interactions produced orders of magnitude more knowledge, and cross-sentence extraction tripled the yield compared to single-sentence extraction.",1 Introduction,[0],[0]
Manual evaluation verified that the accuracy is high despite the lack of annotated examples.,1 Introduction,[0],[0]
"Let e1, · · · , em be entity mentions in text T .",2 Cross-sentence n-ary relation extraction,[0],[0]
"Relation extraction can be formulated as a classification problem of determining whether a relation R holds for e1, · · · , em in T .",2 Cross-sentence n-ary relation extraction,[0],[0]
"For example, given a cancer patient with mutation v in gene g, a molecular tumor board seeks to find if this type of cancer would respond to drug d. Literature with such knowledge has been growing rapidly; we can help the tumor board by checking if the Respond relation holds for the (d, g, v) triple.
",2 Cross-sentence n-ary relation extraction,[0],[0]
"Traditional relation extraction methods focus on binary relations where all entities occur in the same sentence (i.e., m = 2 and T is a sentence), and cannot handle the aforementioned ternary relations.",2 Cross-sentence n-ary relation extraction,[0],[0]
"Moreover, as we focus on more complex relations and n increases, it becomes increasingly rare that the related entities will be contained entirely in a single sentence.",2 Cross-sentence n-ary relation extraction,[0],[0]
"In this paper, we generalize extraction to cross-sentence, n-ary relations, where m > 2 and T can contain multiple sentences.",2 Cross-sentence n-ary relation extraction,[0],[0]
"As will be shown in our experiments section, n-ary relations are crucial for high-value domains such as biomedicine, and expanding beyond the sentence boundary enables the extraction of more knowledge.
",2 Cross-sentence n-ary relation extraction,[0],[0]
"In the standard binary-relation setting, the dominant approaches are generally defined in terms of the shortest dependency path between the two entities in question, either by deriving rich features from the path or by modeling it using deep neural
networks.",2 Cross-sentence n-ary relation extraction,[0],[0]
"Generalizing this paradigm to the n-ary setting is challenging, as there are ( n 2 ) paths.",2 Cross-sentence n-ary relation extraction,[0],[0]
"One apparent solution is inspired by Davidsonian semantics: first, identify a single trigger phrase that signifies the whole relation, then reduce the n-ary relation to n binary relations between the trigger and an argument.",2 Cross-sentence n-ary relation extraction,[0],[0]
"However, challenges remain.",2 Cross-sentence n-ary relation extraction,[0],[0]
"It is often hard to specify a single trigger, as the relation is manifested by several words, often not contiguous.",2 Cross-sentence n-ary relation extraction,[0],[0]
"Moreover, it is expensive and time-consuming to annotate training examples, especially if triggers are required, as is evident in prior annotation efforts such as GENIA (Kim et al., 2009).",2 Cross-sentence n-ary relation extraction,[0],[0]
"The realistic and widely adopted paradigm is to leverage indirect supervision, such as distant supervision (Craven and Kumlien, 1999; Mintz et al., 2009), where triggers are not available.
",2 Cross-sentence n-ary relation extraction,[0],[0]
"Additionally, lexical and syntactic patterns signifying the relation will be sparse.",2 Cross-sentence n-ary relation extraction,[0],[0]
"To handle such sparsity, traditional feature-based approaches require extensive engineering and large data.",2 Cross-sentence n-ary relation extraction,[0],[0]
"Unfortunately, this challenge becomes much more severe in crosssentence extraction when the text spans multiple sentences.
",2 Cross-sentence n-ary relation extraction,[0],[0]
"To overcome these challenges, we explore a general relation extraction framework based on graph LSTMs.",2 Cross-sentence n-ary relation extraction,[0],[0]
"By learning a continuous representation for words and entities, LSTMs can handle sparsity effectively without requiring intense feature engineering.",2 Cross-sentence n-ary relation extraction,[0],[0]
"The graph formulation subsumes prior LSTM approaches based on chains or trees, and can incorporate rich linguistic analyses.
",2 Cross-sentence n-ary relation extraction,[0],[0]
This approach also opens up opportunities for joint learning with related relations.,2 Cross-sentence n-ary relation extraction,[0],[0]
"For example, the Response relation over d, g, v also implies a binary sub-relation over drug d and mutation v, with the gene underspecified.",2 Cross-sentence n-ary relation extraction,[0],[0]
"Even with distant supervision, the supervision signal for n-ary relations will likely be sparser than their binary sub-relations.",2 Cross-sentence n-ary relation extraction,[0],[0]
Our approach makes it very easy to use multi-task learning over both the n-ary relations and their sub-relations.,2 Cross-sentence n-ary relation extraction,[0],[0]
Learning a continuous representation can be effective for dealing with lexical and syntactic sparsity.,3 Graph LSTMs,[0],[0]
"For sequential data such as text, recurrent neural networks (RNNs) are quite popular.",3 Graph LSTMs,[0],[0]
"They resemble hidden
Markov models (HMMs), except that discrete hidden states are replaced with continuous vectors, and emission and transition probabilities with neural networks.",3 Graph LSTMs,[0],[0]
"Conventional RNNs with sigmoid units suffer from gradient diffusion or explosion, making training very difficult (Bengio et al., 1994; Pascanu et al., 2013).",3 Graph LSTMs,[0],[0]
"Long short-term memory (LSTMs) (Hochreiter and Schmidhuber, 1997) combats these problems by using a series of gates (input, forget and output) to avoid amplifying or suppressing gradients during backpropagation.",3 Graph LSTMs,[0],[0]
"Consequently, LSTMs are much more effective in capturing long-distance dependencies, and have been applied to a variety of NLP tasks.",3 Graph LSTMs,[0],[0]
"However, most approaches are based on linear chains and only explicitly model the linear context, which ignores a variety of linguistic analyses, such as syntactic and discourse dependencies.
",3 Graph LSTMs,[0],[0]
"In this section, we propose a general framework that generalizes LSTMs to graphs.",3 Graph LSTMs,[0],[0]
"While there is some prior work on learning tree LSTMs (Tai et al., 2015; Miwa and Bansal, 2016), to the best of our knowledge, graph LSTMs have not been applied to any NLP task yet.",3 Graph LSTMs,[0],[0]
Figure 2 shows the architecture of this approach.,3 Graph LSTMs,[0],[0]
The input layer is the word embedding of input text.,3 Graph LSTMs,[0],[0]
Next is the graph LSTM which learns a contextual representation for each word.,3 Graph LSTMs,[0],[0]
"For the entities in question, their contextual representations are concatenated and become the input to the relation classifiers.",3 Graph LSTMs,[0],[0]
"For a multi-word entity, we simply used the average of its word representations and leave the exploration of more sophisticated aggregation approaches to future work.",3 Graph LSTMs,[0],[0]
The layers are trained jointly with backpropagation.,3 Graph LSTMs,[0],[0]
"This framework is
agnostic to the choice of classifiers.",3 Graph LSTMs,[0],[0]
"Jointly designing classifiers with graph LSTMs would be interesting future work.
",3 Graph LSTMs,[0],[0]
At the core of the graph LSTM is a document graph that captures various dependencies among the input words.,3 Graph LSTMs,[0],[0]
"By choosing what dependencies to include in the document graph, graph LSTMs naturally subsumes linear-chain or tree LSTMs.
",3 Graph LSTMs,[0],[0]
"Compared to conventional LSTMs, the graph formulation presents new challenges.",3 Graph LSTMs,[0],[0]
"Due to potential cycles in the graph, a straightforward implementation of backpropagation might require many iterations to reach a fixed point.",3 Graph LSTMs,[0],[0]
"Moreover, in the presence of a potentially large number of edge types (adjacent-word, syntactic dependency, etc.), parametrization becomes a key problem.
",3 Graph LSTMs,[0],[0]
"In the remainder of this section, we first introduce the document graph and show how to conduct backpropagation in graph LSTMs.",3 Graph LSTMs,[0],[0]
We then discuss two strategies for parametrizing the recurrent units.,3 Graph LSTMs,[0],[0]
"Finally, we show how to conduct multi-task learning with this framework.",3 Graph LSTMs,[0],[0]
"To model various dependencies from linguistic analysis at our disposal, we follow Quirk and Poon (2017) and introduce a document graph to capture intra- and inter-sentential dependencies.",3.1 Document Graph,[0],[0]
"A document graph consists of nodes that represent words and edges that represent various dependencies such as linear context (adjacent words), syntactic dependencies, and discourse relations (Lee et al., 2013; Xue et al., 2015).",3.1 Document Graph,[0],[0]
"Figure 1 shows the document graph for our running example; this instance suggests that tumors with L858E mutation in EGFR gene responds to the drug gefitinib.
",3.1 Document Graph,[0],[0]
This document graph acts as the backbone upon which a graph LSTM is constructed.,3.1 Document Graph,[0],[0]
"If it con-
tains only edges between adjacent words, we recover linear-chain LSTMs.",3.1 Document Graph,[0],[0]
"Similarly, other prior LSTM approaches can be captured in this framework by restricting edges to those in the shortest dependency path or the parse tree.",3.1 Document Graph,[0],[0]
Conventional LSTMs are essentially very deep feedforward neural networks.,3.2 Backpropagation in Graph LSTMs,[0],[0]
"For example, a left-to-right linear LSTM has one hidden vector for each word.",3.2 Backpropagation in Graph LSTMs,[0],[0]
This vector is generated by a neural network (recurrent unit) that takes as input the embedding of the given word and the hidden vector of the previous word.,3.2 Backpropagation in Graph LSTMs,[0],[0]
"In discriminative learning, these hidden vectors then serve as input for the end classifiers, from which gradients are backpropagated through the whole network.
",3.2 Backpropagation in Graph LSTMs,[0],[0]
"Generalizing such a strategy to graphs with cycles typically requires unrolling recurrence for a number of steps (Scarselli et al., 2009; Li et al., 2016; Liang et al., 2016).",3.2 Backpropagation in Graph LSTMs,[0],[0]
"Essentially, a copy of the graph is created for each step that serves as input for the next.",3.2 Backpropagation in Graph LSTMs,[0],[0]
"The result is a feed-forward neural network through time, and backpropagation is conducted accordingly.
",3.2 Backpropagation in Graph LSTMs,[0],[0]
"In principle, we could adopt the same strategy.",3.2 Backpropagation in Graph LSTMs,[0],[0]
"Effectively, gradients are backpropagated in a manner similar to loopy belief propagation (LBP).",3.2 Backpropagation in Graph LSTMs,[0],[0]
"However, this makes learning much more expensive as each update step requires multiple iterations of backpropagation.",3.2 Backpropagation in Graph LSTMs,[0],[0]
"Moreover, loopy backpropagation could suffer from the same problems encountered to in LBP, such as oscillation or failure to converge.
",3.2 Backpropagation in Graph LSTMs,[0],[0]
"We observe that dependencies such as coreference and discourse relations are generally sparse, so the backbone of a document graph consists of the linear chain and the syntactic dependency tree.",3.2 Backpropagation in Graph LSTMs,[0],[0]
"As in belief propagation, such structures can be leveraged to make backpropagation more efficient by replac-
ing synchronous updates, as in the unrolling strategy, with asynchronous updates, as in linear-chain LSTMs.",3.2 Backpropagation in Graph LSTMs,[0],[0]
"This opens up opportunities for a variety of strategies in ordering backpropagation updates.
",3.2 Backpropagation in Graph LSTMs,[0],[0]
"In this paper, we adopt a simple strategy that performed quite well in preliminary experiments, and leave further exploration to future work.",3.2 Backpropagation in Graph LSTMs,[0],[0]
"Specifically, we partition the document graph into two directed acyclic graphs (DAGs).",3.2 Backpropagation in Graph LSTMs,[0],[0]
"One DAG contains the left-to-right linear chain, as well as other forwardpointing dependencies.",3.2 Backpropagation in Graph LSTMs,[0],[0]
The other DAG covers the right-to-left linear chain and the backward-pointing dependencies.,3.2 Backpropagation in Graph LSTMs,[0],[0]
Figure 3 illustrates this strategy.,3.2 Backpropagation in Graph LSTMs,[0],[0]
"Effectively, we partition the original graph into the forward pass (left-to-right), followed by the backward pass (right-to-left), and construct the LSTMs accordingly.",3.2 Backpropagation in Graph LSTMs,[0],[0]
"When the document graph only contains linear chain edges, the graph LSTMs is exactly a bi-directional LSTMs (BiLSTMs).",3.2 Backpropagation in Graph LSTMs,[0],[0]
"A standard LSTM unit consists of an input vector (word embedding), a memory cell and an output vector (contextual representation), as well as several gates.",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"The input gate and output gate control the information flowing into and out of the cell, whereas the forget gate can optionally remove information from the recurrent connection to a precedent unit.
",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"In linear-chain LSTMs, each unit contains only one forget gate, as it has only one direct precedent (i.e., the adjacent-word edge pointing to the previous word).",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"In graph LSTMs, however, a unit may have several precedents, including connections to the same word via different edges.",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"We thus introduce a forget gate for each precedent, similar to the approach taken by Tai et al. (2015) for tree LSTMs.
",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"Encoding rich linguistic analysis introduces many distinct edge types besides word adjacency, such as syntactic dependencies, which opens up many possibilities for parametrization.",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"This was not considered in prior syntax-aware LSTM approaches (Tai et al., 2015; Miwa and Bansal, 2016).",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"In this paper, we explore two schemes that introduce more fined-grained parameters based on the edge types.
",3.3 The Basic Recurrent Propagation Unit,[0],[0]
Full Parametrization,3.3 The Basic Recurrent Propagation Unit,[0],[0]
"Our first proposal simply introduces a different set of parameters for each edge type, with computation specified below.
",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"it = σ(Wixt + ∑
j∈P (t) U
m(t,j) i hj + bi)
ot = σ(Woxt + ∑
j∈P (t) Um(t,j)o hj + bo) c̃t = tanh(Wcxt + ∑
j∈P (t) Um(t,j)c hj + bc)
ftj = σ(Wfxt",3.3 The Basic Recurrent Propagation Unit,[0],[0]
+,3.3 The Basic Recurrent Propagation Unit,[0],[0]
"U m(t,j) f hj + bf )",3.3 The Basic Recurrent Propagation Unit,[0],[0]
ct = it c̃t,3.3 The Basic Recurrent Propagation Unit,[0],[0]
"+ ∑
j∈P (t) ftj",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"cj
ht = ot tanh(ct)
",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"As in standard chain LSTMs, xt is the input word vector for node t, ht is the hidden state vector for node t, W ’s are the input weight matrices, and b’s are the bias vectors.",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"σ, tanh, and represent the sigmoid function, the hyperbolic tangent function, and the Hadamard product (pointwise multiplication), respectively.",3.3 The Basic Recurrent Propagation Unit,[0],[0]
The main differences lie in the recurrence terms.,3.3 The Basic Recurrent Propagation Unit,[0],[0]
"In graph LSTMs, a unit might have multiple predecessors (P (t)), for each of which (j) there is a forget gate ftj , and a typed weight matrix Um(t,j), where m(t, j) signifies the connection type between t and j. The input and output gates (it, ot) depend on all predecessors, whereas the forget gate (ftj) only depends on the predecessor with which the gate is associated.",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"ct and c̃t represent intermediate computation results within the memory cell, which take into account the input and forget gates, and will be combined with output gate to produce the hidden representation ht.
",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"Full parameterization is straightforward, but it requires a large number of parameters when there are many edge types.",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"For example, there are dozens of syntactic edge types, each corresponding to a Stanford dependency label.",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"As a result, in our experiments we resort to using only the coarse-grained types: word adjacency, syntactic dependency, etc.",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"Next, we will consider a more fine-grained approach by learning an edge-type embedding.
",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"Edge-Type Embedding To reduce the number of parameters and leverage potential correlation among fine-grained edge types, we learned a lowdimensional embedding of the edge types, and conducted an outer product of the predecessor’s hidden vector and the edge-type embedding to generate a “typed hidden representation”, which is a matrix.",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"The new computation is as follows:
it = σ(Wixt + ∑
j∈P (t) Ui ×T (hj ⊗ ej) + bi)
ftj = σ(Wfxt +",3.3 The Basic Recurrent Propagation Unit,[0],[0]
Uf ×T (hj ⊗ ej) + bf ),3.3 The Basic Recurrent Propagation Unit,[0],[0]
"ot = σ(Woxt + ∑
j∈P (t)",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"Uo ×T (hj ⊗ ej) + bo) c̃t = tanh(Wcxt + ∑
j∈P (t) Uc ×T (hj ⊗ ej) + bc) ct",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"= it c̃t + ∑
j∈P (t) ftj cj
ht = ot tanh(ct)
",3.3 The Basic Recurrent Propagation Unit,[0],[0]
U ’s are now l ×,3.3 The Basic Recurrent Propagation Unit,[0],[0]
"l × d tensors (l is the dimension of the hidden vector and d is the dimension for edgetype embedding), and hj ⊗ ej is a tensor product that produces an l × d matrix.",3.3 The Basic Recurrent Propagation Unit,[0],[0]
×T denotes a tensor dot product defined as T ×T,3.3 The Basic Recurrent Propagation Unit,[0],[0]
"A = ∑ d(T:,:,d · A:,d), which produces an l-dimensional vector.",3.3 The Basic Recurrent Propagation Unit,[0],[0]
The edgetype embedding ej is jointly trained with the other parameters.,3.3 The Basic Recurrent Propagation Unit,[0],[0]
The main advantages of a graph formulation are its generality and flexibility.,3.4 Comparison with Prior LSTM Approaches,[0],[0]
"As seen in Section 3.1, linear-chain LSTMs are a special case when the document graph is the linear chain of adjacent words.",3.4 Comparison with Prior LSTM Approaches,[0],[0]
"Similarly, Tree LSTMs (Tai et al., 2015) are a special case when the document graph is the parse tree.
",3.4 Comparison with Prior LSTM Approaches,[0],[0]
"In graph LSTMs, the encoding of linguistic knowledge is factored from the backpropagation strategy (Section 3.2), making it much more flexible, including introducing cycles.",3.4 Comparison with Prior LSTM Approaches,[0],[0]
"For example, Miwa and Bansal (2016) conducted joint entity and binary relation extraction by stacking a LSTM for relation extraction on top of another LSTM for entity recognition.",3.4 Comparison with Prior LSTM Approaches,[0],[0]
"In graph LSTMs, the two can be combined seamlessly using a document graph comprising both the word-adjacency chain and the dependency path between the two entities.
",3.4 Comparison with Prior LSTM Approaches,[0],[0]
The document graph can also incorporate other linguistic information.,3.4 Comparison with Prior LSTM Approaches,[0],[0]
"For example, coreference and discourse parsing are intuitively relevant for cross-sentence relation extraction.",3.4 Comparison with Prior LSTM Approaches,[0],[0]
"Although existing systems have not yet been shown to improve crosssentence relation extraction (Quirk and Poon, 2017), it remains an important future direction to explore incorporating such analyses, especially after adapting them to the biomedical domains (Bell et al., 2016).",3.4 Comparison with Prior LSTM Approaches,[0],[0]
"Multi-task learning has been shown to be beneficial in training neural networks (Caruana, 1998; Collobert and Weston, 2008; Peng and Dredze, 2016).",3.5 Multi-task Learning with Sub-relations,[0],[0]
"By learning contextual entity representations, our framework makes it straightforward to conduct multi-task learning.",3.5 Multi-task Learning with Sub-relations,[0],[0]
The only change is to add a separate classifier for each related auxiliary relation.,3.5 Multi-task Learning with Sub-relations,[0],[0]
"All classifiers share the same graph LSTMs representation learner and word embeddings, and can potentially help each other by pooling their supervision signals.
",3.5 Multi-task Learning with Sub-relations,[0],[0]
"In the molecular tumor board domain, we applied this paradigm to joint learning of both the ternary relation (drug-gene-mutation) and its binary sub-relation (drug-mutation).",3.5 Multi-task Learning with Sub-relations,[0],[0]
Experiment results show that this provides significant gains in both tasks.,3.5 Multi-task Learning with Sub-relations,[0],[0]
"We implemented our methods using the Theano library (Theano Development Team, 2016).",4 Implementation Details,[0],[0]
We used logistic regression for our relation classifiers.,4 Implementation Details,[0],[0]
Hyper parameters were set based on preliminary experiments on a small development dataset.,4 Implementation Details,[0],[0]
Training was done using mini-batched stochastic gradient descent (SGD) with batch size 8.,4 Implementation Details,[0],[0]
"We used a learning rate of 0.02 and trained for at most 30 epochs, with early stopping based on development data (Caruana et al., 2001; Graves et al., 2013).",4 Implementation Details,[0],[0]
"The dimension for the hidden vectors in LSTM units was set to 150, and the dimension for the edge-type embedding was set to 3.",4 Implementation Details,[0],[0]
"The word embeddings were initialized with the publicly available 100-dimensional GloVe word vectors trained on 6 billion words from Wikipedia and web text3 (Pennington et al., 2014).",4 Implementation Details,[0],[0]
"Other model parameters were initialized with random samples drawn uniformly from the range [−1, 1].
",4 Implementation Details,[0],[0]
"In multi-task training, we alternated among all tasks, each time passing through all data for one task4, and updating the parameters accordingly.",4 Implementation Details,[0],[0]
"This was repeated for 30 epochs.
3http://nlp.stanford.edu/projects/glove/ 4However, drug-gene pairs have much more data, so we subsampled the instances down to the same size as the main n-ary relation task.",4 Implementation Details,[0],[0]
"Our main experiments focus on extracting ternary interactions over drugs, genes and mutations, which is important for molecular tumor boards.",5 Domain: Molecular Tumor Boards,[0],[0]
A druggene-mutation interaction is broadly construed as an association between the drug efficacy and the mutation in the given gene.,5 Domain: Molecular Tumor Boards,[0],[0]
There is no annotated dataset for this problem.,5 Domain: Molecular Tumor Boards,[0],[0]
"However, due to the importance of such knowledge, oncologists have been painstakingly curating known relations from reading papers.",5 Domain: Molecular Tumor Boards,[0],[0]
"Such a manual approach cannot keep up with the rapid growth of the research literature, and the coverage is generally sparse and not up to date.",5 Domain: Molecular Tumor Boards,[0],[0]
"However, the curated knowledge can be used for distant supervision.",5 Domain: Molecular Tumor Boards,[0],[0]
"We obtained biomedical literature from PubMed Central5, consisting of approximately one million fulltext articles as of 2015.",5.1 Datasets,[0],[0]
Note that only a fraction of papers contain knowledge about drug-gene-mutation interactions.,5.1 Datasets,[0],[0]
Extracting such knowledge from the vast body of biomedical papers is exactly the challenge.,5.1 Datasets,[0],[0]
"As we will see in later subsections, distant supervision enables us to generate a sizable training set from a small number of manually curated facts, and the learned model was able to extract orders of magnitude more facts.",5.1 Datasets,[0],[0]
"In future work, we will explore incorporating more known facts for distant supervision and extracting from more full-text articles.
",5.1 Datasets,[0],[0]
"We conducted tokenization, part-of-speech tagging, and syntactic parsing using SPLAT (Quirk et al., 2012), and obtained Stanford dependencies (de Marneffe et al., 2006) using Stanford CoreNLP",5.1 Datasets,[0],[0]
"(Manning et al., 2014).",5.1 Datasets,[0],[0]
"We used the entity taggers from Literome (Poon et al., 2014) to identify drug, gene and mutation mentions.
",5.1 Datasets,[0],[0]
"We used the Gene Drug Knowledge Database (GDKD) (Dienstmann et al., 2015) and the Clinical Interpretations of Variants In Cancer (CIVIC) knowledge base6 for distant supervision.",5.1 Datasets,[0],[0]
"The knowledge bases distinguish fine-grained interaction types, which we do not use in this paper.
",5.1 Datasets,[0],[0]
5http://www.ncbi.nlm.nih.gov/pmc/ 6http://civic.genome.wustl.edu,5.1 Datasets,[0],[0]
"After identifying drug, gene and mutation mentions in the text, co-occurring triples with known interactions were chosen as positive examples.",5.2 Distant Supervision,[0],[0]
"However, unlike the single-sentence setting in standard distant supervision, care must be taken in selecting the candidates.",5.2 Distant Supervision,[0],[0]
"Since the triples can reside in different sentences, an unrestricted selection of text spans would risk introducing many obviously wrong examples.",5.2 Distant Supervision,[0],[0]
"We thus followed Quirk and Poon (2017) in restricting the candidates to those occurring in a minimal span, i.e., we retain a candidate only if is no other co-occurrence of the same entities in an overlapping text span with a smaller number of consecutive sentences.",5.2 Distant Supervision,[0],[0]
"Furthermore, we avoid picking unlikely candidates where the triples are far apart in the document.",5.2 Distant Supervision,[0],[0]
"Specifically, we considered entity triples within K consecutive sentences, ignoring paragraph boundaries.",5.2 Distant Supervision,[0],[0]
K = 1 corresponds to the baseline of extraction within single sentences.,5.2 Distant Supervision,[0],[0]
"We explored K ≤ 3, which captured a large fraction of candidates without introducing many unlikely ones.
",5.2 Distant Supervision,[0],[0]
Only 59 distinct drug-gene-mutation triples from the knowledge bases were matched in the text.,5.2 Distant Supervision,[0],[0]
"Even from such a small set of unique triples, we obtained 3,462 ternary relation instances that can serve as positive examples.",5.2 Distant Supervision,[0],[0]
"For multi-task learning, we also considered drug-gene and drug-mutation sub-relations, which yielded 137,469 drug-gene and 3,192 drugmutation relation instances as positive examples.
",5.2 Distant Supervision,[0],[0]
"We generate negative examples by randomly sampling co-occurring entity triples without known interactions, subject to the same restrictions above.",5.2 Distant Supervision,[0],[0]
We sampled the same number as positive examples to obtain a balanced dataset7.,5.2 Distant Supervision,[0],[0]
"To compare the various models in our proposed framework, we conducted five-fold cross-validation, treating the positive and negative examples from distant supervision as gold annotation.",5.3 Automatic Evaluation,[0],[0]
"To avoid traintest contamination, all examples from a document were assigned to the same fold.",5.3 Automatic Evaluation,[0],[0]
"Since our datasets are balanced by construction, we simply report average test accuracy on held-out folds.",5.3 Automatic Evaluation,[0],[0]
"Obviously, the
7We will release the dataset at http://hanover.azurewebsites.net.
results could be noisy (e.g., entity triples not known to have an interaction might actually have one), but this evaluation is automatic and can quickly evaluate the impact of various design choices.
",5.3 Automatic Evaluation,[0],[0]
We evaluated two variants of graph LSTMs: “Graph LSTM-FULL” with full parametrization and “Graph LSTM-EMBED” with edge-type embedding.,5.3 Automatic Evaluation,[0],[0]
"We compared graph LSTMs with three strong baseline systems: a well-engineered feature-based classifier (Quirk and Poon, 2017), a convolutional neural network (CNN)",5.3 Automatic Evaluation,[0],[0]
"(Zeng et al., 2014; Santos et al., 2015; Wang et al., 2016), and a bi-directional LSTM (BiLSTM).",5.3 Automatic Evaluation,[0],[0]
"Following Wang et al. (2016), we used input attention for the CNN and a input window size of 5.",5.3 Automatic Evaluation,[0],[0]
Quirk and Poon (2017) only extracted binary relations.,5.3 Automatic Evaluation,[0],[0]
"We extended it to ternary relations by deriving features for each entity pair (with added annotation to signify the two entity types), and pooling the features
from all pairs.",5.3 Automatic Evaluation,[0],[0]
"For binary relation extraction, prior syntax-aware approaches are directly applicable.",5.3 Automatic Evaluation,[0],[0]
"So we also compared with a state-of-the-art tree LSTM system (Miwa and Bansal, 2016) and a BiLSTM on the shortest dependency path between the two entities (BiLSTM-Shortest-Path) (Xu et al., 2015b).
",5.3 Automatic Evaluation,[0],[0]
"Table 1 shows the results for cross-sentence, ternary relation extraction.",5.3 Automatic Evaluation,[0],[0]
"All neural-network based models outperformed the feature-based classifier, illustrating their advantage in handling sparse linguistic patterns without requiring intense feature engineering.",5.3 Automatic Evaluation,[0],[0]
"All LSTMs significantly outperformed CNN in the cross-sentence setting, verifying the importance in capturing long-distance dependencies.
",5.3 Automatic Evaluation,[0],[0]
"The two variants of graph LSTMs perform on par with each other, though Graph LSTM-FULL has a small advantage, suggesting that further exploration of parametrization schemes could be beneficial.",5.3 Automatic Evaluation,[0],[0]
"In particular, the edge-type embedding might improve by pretraining on unlabeled text with syntactic parses.
",5.3 Automatic Evaluation,[0],[0]
"Both graph variants significantly outperformed BiLSTMs (p < 0.05 by McNemar’s chi-square test), though the difference is small.",5.3 Automatic Evaluation,[0],[0]
This result is intriguing.,5.3 Automatic Evaluation,[0],[0]
"In Quirk and Poon (2017), the best system incorporated syntactic dependencies and outperformed the linear-chain variant (Base) by a large margin.",5.3 Automatic Evaluation,[0],[0]
"So why didn’t graph LSTMs make an equally substantial gain by modeling syntactic dependencies?
",5.3 Automatic Evaluation,[0],[0]
One reason is that linear-chain LSTMs can already captured some of the long-distance dependencies available in syntactic parses.,5.3 Automatic Evaluation,[0],[0]
"BiLSTMs substantially outperformed the feature-based classifier, even without explicit modeling of syntactic dependencies.",5.3 Automatic Evaluation,[0],[0]
"The gain cannot be entirely attributed to word embedding as LSTMs also outperformed CNNs.
",5.3 Automatic Evaluation,[0],[0]
Another reason is that syntactic parsing is less accurate in the biomedical domain.,5.3 Automatic Evaluation,[0],[0]
"Parse errors confuse the graph LSM learner, limiting the potential for gain.",5.3 Automatic Evaluation,[0],[0]
"In Section 6, we show supporting evidence in a domain when gold parses are available.
",5.3 Automatic Evaluation,[0],[0]
"We also reported accuracy on instances within single sentences, which exhibited a broadly similar set of trends.",5.3 Automatic Evaluation,[0],[0]
"Note that single-sentence and crosssentence accuracies are not directly comparable, as the test sets are different (one subsumes the other).
",5.3 Automatic Evaluation,[0],[0]
We conducted the same experiments on the binary sub-relation between drug-mutation pairs.,5.3 Automatic Evaluation,[0],[0]
"Table 2
shows the results, which are similar to the ternary case: Graph LSTM-FULL consistently performed the best for both single sentence and cross-sentence instances.",5.3 Automatic Evaluation,[0],[0]
"BiLSTMs on the shortest path substantially underperformed BiLSTMs or graph LSTMs, losing between 4-5 absolute points in accuracy, which could be attributed to the lower parsing quality in the biomedical domain.",5.3 Automatic Evaluation,[0],[0]
"Interestingly, the state-of-the-art tree LSTMs (Miwa and Bansal, 2016) also underperformed graph LSTMs, even though they encoded essentially the same linguistic structures (word adjacency and syntactic dependency).",5.3 Automatic Evaluation,[0],[0]
"We attributed the gain to the fact that Miwa and Bansal (2016) used separate LSTMs for the linear chain and the dependency tree, whereas graph LSTMs learned a single representation for both.
",5.3 Automatic Evaluation,[0],[0]
"To evaluate whether joint learning with subrelations can help, we conducted multi-task learning using Graph LSTM-FULL to jointly train extractors for both the ternary interaction and the drug-mutation, drug-gene sub-relations.",5.3 Automatic Evaluation,[0],[0]
Table 3 shows the results.,5.3 Automatic Evaluation,[0],[0]
Multi-task learning resulted in a significant gain for both the ternary interaction and the drug-mutation interaction.,5.3 Automatic Evaluation,[0],[0]
"Interestingly, the advantage of graph LSTMs over BiLSTMs is reduced with multi-task learning, suggesting that with more supervision signal, even linear-chain LSTMs can learn to capture long-range dependencies that are were made evident by parse features in graph LSTMs.",5.3 Automatic Evaluation,[0],[0]
"Note that there are many more instances for drug-gene interaction than others, so we only sampled a subset of comparable size.",5.3 Automatic Evaluation,[0],[0]
"Therefore, we do not evaluate the performance gain for drug-gene interaction, as in practice, one would simply learn from all available data, and the sub-sampled results are not competitive.
",5.3 Automatic Evaluation,[0],[0]
We included coreference and discourse relations in our document graph.,5.3 Automatic Evaluation,[0],[0]
"However, we didn’t observe any significant gains, similar to the observation in
Quirk and Poon (2017).",5.3 Automatic Evaluation,[0],[0]
We leave further exploration to future work.,5.3 Automatic Evaluation,[0],[0]
Our ultimate goal is to extract all knowledge from available text.,5.4 PubMed-Scale Extraction,[0],[0]
"We thus retrained our model using the best system from automatic evaluation (i.e., Graph LSTM-FULL) on all available data.",5.4 PubMed-Scale Extraction,[0],[0]
"The resulting model was then used to extract relations from all PubMed Central articles.
",5.4 PubMed-Scale Extraction,[0],[0]
Table 4 shows the number of candidates and extracted interactions.,5.4 PubMed-Scale Extraction,[0],[0]
"With as little as 59 unique druggene-mutation triples from the two databases8, we learned to extract orders of magnitude more unique interactions.",5.4 PubMed-Scale Extraction,[0],[0]
"The results also highlight the benefit of cross-sentence extraction, which yields 3 to 5 times more relations than single-sentence extraction.
",5.4 PubMed-Scale Extraction,[0],[0]
"Table 5 conducts a similar comparison on unique number of drugs, genes, and mutations.",5.4 PubMed-Scale Extraction,[0],[0]
"Again, machine reading covers far more unique entities, especially with cross-sentence extraction.",5.4 PubMed-Scale Extraction,[0],[0]
"Our automatic evaluations are useful for comparing competing approaches, but may not reflect the true classifier precision as the labels are noisy.",5.5 Manual Evaluation,[0],[0]
"Therefore, we randomly sampled extracted relation instances and asked three researchers knowledgeable in precision medicine to evaluate their correctness.",5.5 Manual Evaluation,[0],[0]
"For each instance, the annotators were presented with the provenance: sentences with the drug, gene, and mutation highlighted.",5.5 Manual Evaluation,[0],[0]
"The annotators determined in
8There are more in the databases, but these are the only ones for which we found matching instances in the text.",5.5 Manual Evaluation,[0],[0]
"In future work, we will explore various ways to increase the number, e.g., by matching underspecified drug classes to specific drugs.
",5.5 Manual Evaluation,[0],[0]
each case whether this instance implied that the given entities were related.,5.5 Manual Evaluation,[0],[0]
"Note that evaluation does not attempt to identify whether the relationships are true or replicated in follow-up papers; rather, it focuses on whether the relationships are entailed by the text.
",5.5 Manual Evaluation,[0],[0]
We focused our evaluation efforts on the crosssentence ternary-relation setting.,5.5 Manual Evaluation,[0],[0]
"We considered three probability thresholds: 0.9 for a high-precision but potentially low-recall setting, 0.5, and a random sample of all candidates.",5.5 Manual Evaluation,[0],[0]
"In each case, 150 instances were selected for a total of 450 annotations.",5.5 Manual Evaluation,[0],[0]
"A subset of 150 instances were reviewed by two annotators, and the inter-annotator agreement was 88%.
",5.5 Manual Evaluation,[0],[0]
"Table 6 shows that the classifier indeed filters out a large portion of potential candidates, with estimated instance accuracy of 64% at the threshold of 0.5, and 75% at 0.9.",5.5 Manual Evaluation,[0],[0]
"Interestingly, LSTMs are effective at screening out many entity mention errors, presumably because they include broad contextual features.",5.5 Manual Evaluation,[0],[0]
"We also conducted experiments on extracting genetic pathway interactions using the GENIA Event Extraction dataset (Kim et al., 2009).",6 Domain: Genetic Pathways,[0],[0]
"This dataset contains gold syntactic parses for the sentences, which offered a unique opportunity to investigate the impact of syntactic analysis on graph LSTMs.",6 Domain: Genetic Pathways,[0],[0]
"It also allowed us to test our framework in supervised learning.
",6 Domain: Genetic Pathways,[0],[0]
"The original shared task evaluated on complex, nested events for nine event types, many of which are unary relations (Kim et al., 2009).",6 Domain: Genetic Pathways,[0],[0]
"Following Poon et al. (2015), we focused on gene regulation and reduced it to binary-relation classification for headto-head comparison.",6 Domain: Genetic Pathways,[0],[0]
"We followed their experimental protocol by sub-sampling negative examples to be about three times of positive examples.
",6 Domain: Genetic Pathways,[0],[0]
"Since the dataset is not entirely balanced, we reported precision, recall, and F1.",6 Domain: Genetic Pathways,[0],[0]
We used our best performing graph LSTM from the previous experiments.,6 Domain: Genetic Pathways,[0],[0]
"By default, automatic parses were used in the document graphs, whereas in Graph LSTM (GOLD), gold parses were used instead.",6 Domain: Genetic Pathways,[0],[0]
Table 7 shows the results.,6 Domain: Genetic Pathways,[0],[0]
"Once again, despite the lack of intense feature engineering, linear-chain LSTMs performed on par with the feature-based classifier (Poon et al., 2015).",6 Domain: Genetic Pathways,[0],[0]
"Graph LSTMs exhibited a more commanding advantage over linear-chain LSTMs in this domain, substantially outperforming the latter (p < 0.01 by McNemar’s chi-square test).",6 Domain: Genetic Pathways,[0],[0]
"Most interestingly, graph LSTMs using gold parses significantly outperformed that using automatic parses, suggesting that encoding high-quality analysis is particularly beneficial.",6 Domain: Genetic Pathways,[0],[0]
Most work on relation extraction has been applied to binary relations of entities in a single sentence.,7 Related Work,[0],[0]
"We first review relevant work on the single-sentence bi-
nary relation extraction task, and then review related work on n-ary and cross-sentence relation extraction.
",7 Related Work,[0],[0]
"Binary relation extraction The traditional featurebased methods rely on carefully designed features to learn good models, and often integrate diverse sources of evidence such as word sequences and syntax context (Kambhatla, 2004; GuoDong et al., 2005; Boschee et al., 2005; Suchanek et al., 2006; Chan and Roth, 2010; Nguyen and Grishman, 2014).",7 Related Work,[0],[0]
"The kernel-based methods design various subsequence or tree kernels (Mooney and Bunescu, 2005; Bunescu and Mooney, 2005; Qian et al., 2008) to capture structured information.",7 Related Work,[0],[0]
"Recently, models based on neural networks have advanced the state of the art by automatically learning powerful feature representations (Xu et al., 2015a; Zhang et al., 2015; Santos et al., 2015; Xu et al., 2015b; Xu et al., 2016).
",7 Related Work,[0],[0]
"Most neural architectures resemble Figure 2, where there is a core representation learner (blue) that takes word embeddings as input and produces contextual entity representations.",7 Related Work,[0],[0]
Such representations are then taken by relation classifiers to produce the final predictions.,7 Related Work,[0],[0]
"Effectively representing sequences of words, both convolutional (Zeng et al., 2014; Wang et al., 2016; Santos et al., 2015) and RNN-based architectures (Zhang et al., 2015; Socher et al., 2012; Cai et al., 2016) have been successful.",7 Related Work,[0],[0]
Most of these have focused on modeling either the surface word sequences or the hierarchical syntactic structure.,7 Related Work,[0],[0]
"Miwa and Bansal (2016) proposed an architecture that benefits from both types of information, using a surface sequence layer, followed by a dependency-tree sequence layer.
",7 Related Work,[0],[0]
"N -ary relation extraction Early work on extracting relations between more than two arguments has been done in MUC-7, with a focus on fact/event extraction from news articles (Chinchor, 1998).",7 Related Work,[0],[0]
"Semantic role labeling in the Propbank (Palmer et al., 2005) or FrameNet (Baker et al., 1998) style are also instances of n-ary relation extraction, with extraction of events expressed in a single sentence.",7 Related Work,[0],[0]
"McDonald et al. (2005) extract n-ary relations in a biomedical domain, by first factoring the n-ary relation into pair-wise relations between all entity pairs, and then constructing maximal cliques of related entities.",7 Related Work,[0],[0]
"Recently, neural models have been applied to semantic role labeling (FitzGerald et al., 2015; Roth
and Lapata, 2016).",7 Related Work,[0],[0]
"These works learned neural representations by effectively decomposing the n-ary relation into binary relations between the predicate and each argument, by embedding the dependency path between each pair, or by combining features of the two using a feed-forward network.",7 Related Work,[0],[0]
"Although some re-ranking or joint inference models have been employed, the representations of the individual arguments do not influence each other.",7 Related Work,[0],[0]
"In contrast, we propose a neural architecture that jointly represents n entity mentions, taking into account long-distance dependencies and inter-sentential information.
",7 Related Work,[0],[0]
"Cross-sentence relation extraction Several relation extraction tasks have benefited from crosssentence extraction, including MUC fact and event extraction (Swampillai and Stevenson, 2011), record extraction from web pages (Wick et al., 2006), extraction of facts for biomedical domains (Yoshikawa et al., 2011), and extensions of semantic role labeling to cover implicit inter-sentential arguments (Gerber and Chai, 2010).",7 Related Work,[0],[0]
"These prior works have either relied on explicit co-reference annotation, or on the assumption that the whole document refers to a single coherent event, to simplify the problem and reduce the need for powerful representations of multi-sentential contexts of entity mentions.",7 Related Work,[0],[0]
"Recently, cross-sentence relation extraction models have been learned with distant supervision, and used integrated contextual evidence of diverse types without reliance on these assumptions (Quirk and Poon, 2017), but that work focused on binary relations only and explicitly engineered sparse indicator features.
",7 Related Work,[0],[0]
"Relation extraction using distant supervision Distant supervision has been applied to extraction of binary (Mintz et al., 2009; Poon et al., 2015) and n-ary (Reschke et al., 2014;",7 Related Work,[0],[0]
"Li et al., 2015) relations, traditionally using hand-engineered features.",7 Related Work,[0],[0]
"Neural architectures have recently been applied to distantly supervised extraction of binary relations (Zeng et al., 2015).",7 Related Work,[0],[0]
"Our work is the first to propose a neural architecture for n-ary relation extraction, where the representation of a tuple of entities is not decomposable into independent representations of the individual entities or entity pairs, and which integrates diverse information from multi-sentential context.",7 Related Work,[0],[0]
"To utilize training data more effectively, we show how multitask learning for component binary sub-relations can
improve performance.",7 Related Work,[0],[0]
"Our learned representation combines information sources within a single sentence in a more integrated and generalizable fashion than prior approaches, and can also improve performance on single-sentence binary relation extraction.",7 Related Work,[0],[0]
We explore a general framework for cross-sentence nary relation extraction based on graph LSTMs.,8 Conclusion,[0],[0]
The graph formulation subsumes linear-chain and tree LSTMs and makes it easy to incorporate rich linguistic analysis.,8 Conclusion,[0],[0]
"Experiments on biomedical domains showed that extraction beyond the sentence boundary produced far more knowledge, and encoding rich linguistic knowledge provided consistent gain.
",8 Conclusion,[0],[0]
"While there is much room to improve in both recall and precision, our results indicate that machine reading can already be useful in precision medicine.",8 Conclusion,[0],[0]
"In particular, automatically extracted facts (Section 5.4) can serve as candidates for manual curation.",8 Conclusion,[0],[0]
"Instead of scanning millions of articles to curate from scratch, human curators would just quickly vet thousands of extractions.",8 Conclusion,[0],[0]
The errors identified by curators offer direct supervision to the machine reading system for continuous improvement.,8 Conclusion,[0],[0]
"Therefore, the most important goal is to attain high recall and reasonable precision.",8 Conclusion,[0],[0]
"Our current models are already quite capable.
",8 Conclusion,[0],[0]
Future directions include: interactive learning with user feedback; improving discourse modeling in graph LSTMs; exploring other backpropagation strategies; joint learning with entity linking; applications to other domains.,8 Conclusion,[0],[0]
"We thank Daniel Fried and Ming-Wei Chang for useful discussions, as well as the anonymous reviewers and editor-in-chief Mark Johnson for their helpful comments.",Acknowledgements,[0],[0]
Past work in relation extraction has focused on binary relations in single sentences.,abstractText,[0],[0]
Recent NLP inroads in high-value domains have sparked interest in the more general setting of extracting n-ary relations that span multiple sentences.,abstractText,[0],[0]
"In this paper, we explore a general relation extraction framework based on graph long short-term memory networks (graph LSTMs) that can be easily extended to cross-sentence n-ary relation extraction.",abstractText,[0],[0]
"The graph formulation provides a unified way of exploring different LSTM approaches and incorporating various intra-sentential and intersentential dependencies, such as sequential, syntactic, and discourse relations.",abstractText,[0],[0]
"A robust contextual representation is learned for the entities, which serves as input to the relation classifier.",abstractText,[0],[0]
"This simplifies handling of relations with arbitrary arity, and enables multi-task learning with related relations.",abstractText,[0],[0]
"We evaluate this framework in two important precision medicine settings, demonstrating its effectiveness with both conventional supervised learning and distant supervision.",abstractText,[0],[0]
Cross-sentence extraction produced larger knowledge bases.,abstractText,[0],[0]
and multi-task learning significantly improved extraction accuracy.,abstractText,[0],[0]
A thorough analysis of various LSTM approaches yielded useful insight the impact of linguistic analysis on extraction accuracy.,abstractText,[0],[0]
Cross-Sentence N -ary Relation Extraction with Graph LSTMs,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 778–783 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
778",text,[0],[0]
"Stance classification is the task of automatically identifying users’ positions about a specific target from text (Mohammad et al., 2017).",1 Introduction,[0],[0]
"Table 1 shows an example of this task, where the stance of the sentence is recognized as favorable on the target climate change is concern.",1 Introduction,[0],[0]
"Traditionally, this task is approached by learning a target-specific classifier that is trained for prediction on the same target of interest (Hasan and Ng, 2013; Mohammad et al., 2016; Ebrahimi et al., 2016).",1 Introduction,[0],[0]
"This implies that a new classifier has to be built from scratch on a well-prepared set of ground-truth data whenever predictions are needed for an unseen target.
",1 Introduction,[0],[0]
"An alternative to this approach is to conduct a cross-target classification, where the classifier is adapted from different but related targets (Augenstein et al., 2016), which allows benefiting from the knowledge of existing targets.",1 Introduction,[0],[0]
"For example, in our project we are interested in online users’ stances on the approvals of particular mining projects in the country.",1 Introduction,[0],[0]
"It might be useful to start with a classifier that is adapted from a related target such as climate change is concern (presumably available and annotated), as in both cases
users could discuss the impacts from the targets to some common issues, such as the environment or communities.
",1 Introduction,[0],[0]
Cross-target stance classification is a more challenging task simply because the language models may not be compatible between different targets.,1 Introduction,[0],[0]
"However, for some targets that can be recognized as being related to the same and more general domains, it could be possible to generalize through certain aspects of the domains that reflect users’ major concerns.",1 Introduction,[0],[0]
"For example, from the following sentence, whose stance is against the approval of a mining project, “Environmentalists warn the $16 billion coal facility will damage the Great Barrier Reef”, it can be seen that both this sentence and the one in Table 1 mention the same aspect “reef destruction/damage”, which is closely related to the “environment” domain.
",1 Introduction,[0],[0]
"In this paper, we focus on cross-target stance classification and explore the limits of generalizing models between different but domain-related targets1.",1 Introduction,[0],[0]
"The basic idea is to learn a set of domainspecific aspects from a source target, and then apply them to prediction on a destination target.",1 Introduction,[0],[0]
"To this end, we propose CrossNet, a novel neural model that implements the above idea based on the self-attention mechanism.",1 Introduction,[0],[0]
"Our preliminary analysis shows that the proposed model can find useful domain-specific information from a stancebearing sentence and that the classification performance is improved in certain domains.
",1 Introduction,[0],[0]
"1In this work, the source target is chosen based on common sense.",1 Introduction,[0],[0]
Exploring more sophisticated source target selection methods will be our future work.,1 Introduction,[0],[0]
"In this section, we introduce the proposed model, CrossNet, for cross-target stance classification.",2 Model,[0],[0]
Figure 1 shows the architecture of CrossNet.,2 Model,[0],[0]
It consists of four layers from the Embedding Layer (bottom) to the Prediction Layer (top).,2 Model,[0],[0]
It works by taking a stance-bearing sentence and a target as input and yielding the predicted stance label as output.,2 Model,[0],[0]
"In the following, we present the implementation of each layer in CrossNet.",2 Model,[0],[0]
"There are two inputs in CrossNet: a stance-bearing sentence P and a descriptive target T (e.g, climate change is concern in Table 1).",2.1 Embedding Layer,[0],[0]
"We use word embeddings (Mikolov et al., 2013) to represent each word in the input as a dense vector.",2.1 Embedding Layer,[0],[0]
"The output of this layer are two sequences of vectors P = {p1, ...,p|P |} and T = {t1, ..., t|T |}, where p, t are word vectors.",2.1 Embedding Layer,[0],[0]
"In this layer, we encode the contextual information in the input sentence and target.",2.2 Context Encoding Layer,[0],[0]
"We use a bi-directional Long Short-Term Memory Network (BiLSTM) (Hochreiter and Schmidhuber, 1997) to capture the left and right contexts of each word in the input.",2.2 Context Encoding Layer,[0],[0]
"Moreover, to account for the impact of the target on stance inference, we borrow the idea of conditional encoding (Augenstein et al., 2016) to model the dependency of the sentence on the target.",2.2 Context Encoding Layer,[0],[0]
"Formally, we first use a BiLSTMT to encode the target:
",2.2 Context Encoding Layer,[0],[0]
[ −→ h Ti −→c,2.2 Context Encoding Layer,[0],[0]
"Ti ] = −−−−→ LSTMT (ti, −→ h Ti−1, −→c",2.2 Context Encoding Layer,[0],[0]
Ti−1),2.2 Context Encoding Layer,[0],[0]
"[ ←− h Ti ←−c Ti ] = ←−−−− LSTMT (ti, ←− h Ti+1, ←−c Ti+1)",2.2 Context Encoding Layer,[0],[0]
"(1)
where h ∈ Rh and c ∈",2.2 Context Encoding Layer,[0],[0]
Rh are the hidden state and cell state of LSTM.,2.2 Context Encoding Layer,[0],[0]
The symbol −→(←−) indicates the forward (backward) pass.,2.2 Context Encoding Layer,[0],[0]
"ti is the input word vector at time step i.
",2.2 Context Encoding Layer,[0],[0]
"Then, we learn a conditional encoding of the sentence P , by initializing BiLSTMP (a different BiLSTM) with the final states of BiLSTMT :
",2.2 Context Encoding Layer,[0],[0]
"[ −→ h P1 −→c P1 ] = −−−−→ LSTMP (p1, −→ h",2.2 Context Encoding Layer,[0],[0]
T|T,2.2 Context Encoding Layer,[0],[0]
"|, −→c T|T",2.2 Context Encoding Layer,[0],[0]
"|)
",2.2 Context Encoding Layer,[0],[0]
"[ ←− h P|P | ←−c P|P |] = ←−−−− LSTMP (p|P |, ←− h T1 , ←−c T1 )
(2)
It can be seen that the initialization is done by aligning the forward (backward) pass of the two BiLSTMs.",2.2 Context Encoding Layer,[0],[0]
"The output is a contextually-encoded sequence, HP = {hP1 , ...,hP|P |}, where h =",2.2 Context Encoding Layer,[0],[0]
[ −→ h ; ←− h ] ∈ R2h with [; ] as the vector concatenation operation.,2.2 Context Encoding Layer,[0],[0]
"In this layer, we implement the idea of discovering domain-specific aspects for cross-target stance inference.",2.3 Aspect Attention Layer,[0],[0]
"In particular, the key observation we make is that the domain aspects that reflect users’ major concerns are usually the core of understanding their stances, and could be mentioned by multiple users in a discussion.",2.3 Aspect Attention Layer,[0],[0]
"For example, we find that many users in our corpus mention the aspect “reef” to express their concerns about the impact of a mining project on the Great Barrier Reef.",2.3 Aspect Attention Layer,[0],[0]
"Based on this observation, the perception of the domain aspects can be boiled down to finding the sentence parts that not only carry the core idea of a stance-bearing sentence but also tend to be recurring in the corpus.
",2.3 Aspect Attention Layer,[0],[0]
"First, to capture the recurrences of the domain aspects, a simple way is to make every input sentence be consumed by this layer (see Figure 1), so that the layer parameters are shared across the corpus for being stimulated by all appearances of the domain aspects.
",2.3 Aspect Attention Layer,[0],[0]
"Then, we utilize self-attention to signal the core parts of a stance-bearing sentence.",2.3 Aspect Attention Layer,[0],[0]
"Self-attention is an attention mechanism for selecting specific parts of a sequence by relating its elements at different positions (Vaswani et al., 2017; Cheng et al., 2016).",2.3 Aspect Attention Layer,[0],[0]
"In our case, the self-attention process is based on the assumption that the core parts of a sentence are those that are compatible with the semantics of the entire sentence.",2.3 Aspect Attention Layer,[0],[0]
"To this end, we introduce a compatibility function to score the semantic compatibility between the encoded se-
quence HP and each of its hidden states hP :
ci = w > 2 σ(W1h P i + b1) + b2 (3)
where W1 ∈ Rd×2h, w2 ∈ Rd, b1 ∈ Rd, and b2 ∈ R are trainable parameters, and σ is the activation function.",2.3 Aspect Attention Layer,[0],[0]
Note that all the above parameters are shared by every hidden state in HP .,2.3 Aspect Attention Layer,[0],[0]
"Next, we compute the attention weight ai for each hPi based on its compatibility score via softmax operation:
ai = exp(ci)∑|P | j=1 exp(cj)
(4)
",2.3 Aspect Attention Layer,[0],[0]
"Finally, we can obtain the domain aspect encoded representation based on the attention weights:
AP = |P |∑ i=1",2.3 Aspect Attention Layer,[0],[0]
"aih P i (5)
where AP ∈ R2h is the domain aspect encoding for sentence P and also the output of this layer.",2.3 Aspect Attention Layer,[0],[0]
"We predict the stance label of the sentence based on its domain aspect encoding:
ŷ = softmax(MLP(AP ))",2.4 Prediction Layer,[0],[0]
"(6)
where we use a multilayer perceptron (MLP) to consume the domain aspect encoding AP and apply the softmax to get the predicted probability for each of the C classes, ŷ = {y1, ..., yC}.",2.4 Prediction Layer,[0],[0]
"For model training, we use multi-class crossentropy loss,
J (θ) =",2.5 Model Training,[0],[0]
− N∑ i C∑ j y,2.5 Model Training,[0],[0]
(i) j log ŷ,2.5 Model Training,[0],[0]
(i) j,2.5 Model Training,[0],[0]
"+ λ‖Θ‖ (7)
",2.5 Model Training,[0],[0]
whereN is the size of training set.,2.5 Model Training,[0],[0]
"y is the groundtruth label indicator for each class, and ŷ is the predicted probability.",2.5 Model Training,[0],[0]
λ is the coefficient for L2regularization.,2.5 Model Training,[0],[0]
Θ denotes the set of all trainable parameters in our model.,2.5 Model Training,[0],[0]
This section reports the results of quantitative and qualitative evaluations of the proposed model.,3 Experiments,[0],[0]
SemEval-2016:,3.1 Datasets,[0],[0]
the first dataset is from SemEval2016,3.1 Datasets,[0],[0]
"Task 6 on Twitter stance detection, which contains stance-bearing tweets on different targets.",3.1 Datasets,[0],[0]
"We use the following five targets for our experiments: Climate Change is Concern (CC), Feminist Movement (FM), Hillary Clinton (HC), Legalization of Abortion (LA), and Donald Trump (DT).",3.1 Datasets,[0],[0]
"The class labels are favor, against, and neither, and their distributions are shown in Table 2.",3.1 Datasets,[0],[0]
Tweets on an Australian mining project (AM): the second is our collection of tweets on a mining project in Australia obtained using Twitter API.,3.1 Datasets,[0],[0]
"It includes 220,067 tweets posted from January 2016 to June 2017 that contain the project name in the text.",3.1 Datasets,[0],[0]
"We remove all URL-only tweets and duplicate tweets, and obtain a set of 40,852 (unlabeled) tweets.",3.1 Datasets,[0],[0]
"Due to the lack of annotation, this dataset is only used for our qualitative evaluation.
",3.1 Datasets,[0],[0]
"To align with our scenario, the above targets can be categorized into three different domains: Women’s Rights (FM, LA), American Politics (HC, DT), and Environments (CC, AM).",3.1 Datasets,[0],[0]
We use F1-score to measure the classification performance.,3.2 Metric,[0],[0]
"Due to the imbalanced class distributions of the SemEval dataset, we compute both micro-averaged (large classes dominate) and macro-averaged (small classes dominate) F1scores (Manning et al., 2008), and use their average as the metric, i.e., F = 12(Fmicro + Fmacro).
",3.2 Metric,[0],[0]
"To evaluate the effectiveness of target adaptation, we use the metric transfer ratio (Glorot et al., 2011) to compare the cross-target and in-target performance of a model: Q = F (S,D)Fb(D,D) , where F (S,D) is the cross-target F1-score of a model trained on the source target S and tested on the destination target D, and Fb(D,D) is the in-target F1-score of a baseline model trained and tested on the same target D, which serves as the performance calibration for target adaptation.",3.2 Metric,[0],[0]
"The word embeddings are initialized with the pretrained 200d GloVe word vectors on the 27B Twitter corpus (Pennington et al., 2014), and fixed during training.",3.3 Training setup,[0],[0]
"The model is trained (90%) and validated (10%) on a source target, and tested on a destination target.",3.3 Training setup,[0],[0]
"The following model settings are selected based on a small grid search on the validation set: the LSTM hidden size of 60, the MLP layer size of 60, and dropout 0.1.",3.3 Training setup,[0],[0]
The L2-regularization coefficient λ in the loss is 0.01.,3.3 Training setup,[0],[0]
"ADAM (Kingma and Ba, 2014) is used as the optimizer, with a learning rate of 10−3.",3.3 Training setup,[0],[0]
Stratified 10-fold cross-validation is conducted to produce averaged results.,3.3 Training setup,[0],[0]
This section reports the results of our model and two baseline approaches on cross-target stance classification.,3.4 Classification Performance,[0],[0]
BiLSTM:,3.4 Classification Performance,[0],[0]
this is a base model for our task.,3.4 Classification Performance,[0],[0]
It has two BiLSTMs for encoding the sentence and target separately.,3.4 Classification Performance,[0],[0]
"Then, the concatenation of the resulting encodings is fed into the final Prediction Layer to generate predicted stance labels.",3.4 Classification Performance,[0],[0]
"In our evaluation, this model is treated as the baseline model for deriving the in-target performance calibration Fb(D,D).",3.4 Classification Performance,[0],[0]
"MITRE (Augenstein et al., 2016):",3.4 Classification Performance,[0],[0]
"this is the
best system in SemEval-2016 Task 6.",3.4 Classification Performance,[0],[0]
It utilizes the conditional encoding to learn a targetdependent representation for the input sentence.,3.4 Classification Performance,[0],[0]
"The conditional encoding is realized in the same way as the Context Encoding Layer does in our model, namely by using the hidden states of the target-encoding BiLSTM to initialize the sentence-encoding BiLSTM.
Table 3 shows the results (in-target and crosstarget) on the two domains: Women’s Rights and American Politics.",3.4 Classification Performance,[0],[0]
"First, it is observed that MITRE outperforms BiLSTM over all target configurations, suggesting that, compared to simple concatenation, the conditional encoding of the target information could be more helpful to capture the dependency of the sentence on the target.
",3.4 Classification Performance,[0],[0]
"Second, our model is shown to achieve better results than the two baselines in almost all cases (only slightly worse than MITRE on LA under the in-target setting, and the difference is not statistically significant), which implies that the aspect attention mechanism adopted in our model could benefit target-level generalization while it does not hurt the in-target performance.",3.4 Classification Performance,[0],[0]
"Moreover, by comparing the performance of our model under different target configurations, we see that the improvements brought by our model are more significant on the cross-target task than they are on the intarget task, with an average improvement of 6.6% (cross-target) vs. 3.0% (in-target) over MITRE in F1-score, which demonstrates a greater advantage of our model in the cross-target task.
",3.4 Classification Performance,[0],[0]
"Finally, according to the transfer ratio results, the general drop from the in-target to cross-target performance (26% averaged over all cases) could imply that while the target-independent information (i.e., the domain-specific aspects) is shown to benefit generalization, it could be important to also consider the information that is specific to the destination target for model building (which has not yet been explored in this work).",3.4 Classification Performance,[0],[0]
"To show that our model can select sentence parts that are related to domain aspects, we visualize the self-attention results on some tweet examples that are correctly classified by our model in Table 4.
",3.5 Visualization of Attention,[0],[0]
We can see that the most highlighted parts in each example are relevant to the respective domain.,3.5 Visualization of Attention,[0],[0]
"For example, “feminist”, “rights”, and “equality” are commonly used when talking about women’s rights, and “president” and “dreams” of-
ten appear in text about politics.",3.5 Visualization of Attention,[0],[0]
"It is also interesting to note that words that are specific to the destination target may not be captured by the model learned from the source target, such as “abortion” in sentence 1 and “trumps” in sentence 3.",3.5 Visualization of Attention,[0],[0]
"This makes sense because those words are rare in the source target corpus and thus not well noticed by the model.
",3.5 Visualization of Attention,[0],[0]
"Finally, for our project, we can see from the last two sentences that the model learned from climate change is concern is able to concentrate on words that are central to understanding the authors’ stances on the approval of the mining project, such as “reef”, “destroy”, “environmental”, and “disaster”.",3.5 Visualization of Attention,[0],[0]
"Overall, the above visualization demonstrates that our model could benefit stance inference across related targets through capturing domain-specific information.",3.5 Visualization of Attention,[0],[0]
"Finally, it is also possible to show the learned domain aspects by extracting all sentence parts in a corpus that are highly attended by our model.",3.6 Learned Domain-Specific Aspects,[0],[0]
Table 5 presents a number of samples from the intersections between the sets of highly-attended words on the respective targets in the three domains.,3.6 Learned Domain-Specific Aspects,[0],[0]
"Again, we see that these highly-attended words are specific to the respective domains.",3.6 Learned Domain-Specific Aspects,[0],[0]
"We
also notice that besides the domain-aspect words, our model can find words that carry sentiments as well, such as “great”, “crazy”, and “beautiful”, which contribute to stance prediction.",3.6 Learned Domain-Specific Aspects,[0],[0]
"In this work, we study cross-target stance classification and propose a novel self-attention neural model that can extract target-independent information for model generalization.",4 Conclusion and Future Work,[0],[0]
Experimental results show that the proposed model can perceive high-level domain-specific information in a sentence and achieves superior results over a number of baselines in certain domains.,4 Conclusion and Future Work,[0],[0]
"In the future, there are several ways of extending our model.
",4 Conclusion and Future Work,[0],[0]
"First, selecting the effective source targets to generalize from is crucial for achieving satisfying results on the destination targets.",4 Conclusion and Future Work,[0],[0]
"One possibility could be to learn certain correlations between target closeness and generalization performance, which could further be used for guiding the target selection process.",4 Conclusion and Future Work,[0],[0]
"Second, our current model for identifying users’ stances on mining projects only generalizes from one source target (i.e., Climate Change is Concern).",4 Conclusion and Future Work,[0],[0]
"However, a mining project in general could affect other aspects of our society such as community and economics.",4 Conclusion and Future Work,[0],[0]
It could be useful to also consider other related sources for knowledge transfer.,4 Conclusion and Future Work,[0],[0]
"Finally, it would be interesting to evaluate our model in a multilingual scenario (Taulé et al., 2017), in order to examine its generalization ability (whether it can attend to useful domain-specific information in a new language) and multilingual scope.",4 Conclusion and Future Work,[0],[0]
We thank all anonymous reviewers for their valuable comments.,Acknowledgments,[0],[0]
We would also like to thank Keith Vander Linden for his helpful comments on drafts of this paper.,Acknowledgments,[0],[0]
"In stance classification, the target on which the stance is made defines the boundary of the task, and a classifier is usually trained for prediction on the same target.",abstractText,[0],[0]
"In this work, we explore the potential for generalizing classifiers between different targets, and propose a neural model that can apply what has been learned from a source target to a destination target.",abstractText,[0],[0]
We show that our model can find useful information shared between relevant targets which improves generalization in certain scenarios.,abstractText,[0],[0]
Cross-Target Stance Classification with Self-Attention Networks,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3664–3674 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
3664",text,[0],[0]
Information retrieval and question answering are by now mature technologies that excel at answering factual queries on noncontroversial topics.,1 Introduction,[0],[0]
"However, they provide no specialized support for queries where there is no single canonical answer, as with topics that are controversial or opinion-based.",1 Introduction,[0],[0]
"For such queries, the user may need to carefully assess the stance, source, and supportability for each of the answers.",1 Introduction,[0],[0]
"These processes can be supported by argument mining (AM), a nascent area of natural language processing concerned with the automatic recognition and interpretation of arguments.",1 Introduction,[0],[0]
"In this paper, we apply AM to the task of argument search—that is, searching a large document collection for arguments relevant to a given topic.",1 Introduction,[0],[0]
"Searching for and classifying relevant arguments plays an important role in decision making (Svenson, 1979), legal reasoning (Wyner et al., 2010), and
the critical reading, writing, and summarization of persuasive texts (Kobayashi, 2009; Wingate, 2012).",1 Introduction,[0],[0]
"Automating the argument search process could ease much of the manual effort involved in these tasks, particularly if it can be made to robustly handle arguments from different text types and topics.",1 Introduction,[0],[0]
"But despite its obvious usefulness, this sort of argument search has attracted little attention in the research community.",1 Introduction,[0],[0]
"This may be due in part to the limitations of the underlying models and training resources, particularly as they relate to heterogeneous sources.",1 Introduction,[0],[0]
"That is, most current approaches to AM are designed for use with particular text types, faring poorly when applied to new data (Daxenberger et al., 2017).",1 Introduction,[0],[0]
"Indeed, as Habernal et al. (2014) observe, while there is a great diversity of perspectives on how arguments can be best characterized and modelled, there is no “one-size-fits-all” argumentation theory that applies to the variety of text sources found on the Web.",1 Introduction,[0],[0]
"To approach these challenges, we propose the novel task of topic-based sentential argument mining.",1 Introduction,[0],[0]
Our contributions are as follows: (1) We propose a new argument annotation scheme applicable to the information-seeking perspective of argument search.,1 Introduction,[0],[0]
"We show it to be general enough for use on heterogeneous data sources, and simple enough to be applied manually by untrained annotators at a reasonable cost.",1 Introduction,[0],[0]
"(2) We introduce a novel corpus of heterogeneous text types annotated with topic-based arguments.1 The corpus includes over 25,000 instances covering eight controversial topics.",1 Introduction,[0],[0]
This is the first known resource that can be used to evaluate the performance of argument mining methods across topics in heterogeneous sources.,1 Introduction,[0],[0]
"(3) We investigate different approaches for incorporating topic information into neural networks and
1https://www.ukp.tu-darmstadt.de/sent_am
show that including the topic vector into the i- and c-gates of the LSTM cell outperforms common attention-based approaches in two- and three-label cross-topic experiments.",1 Introduction,[0],[0]
(4) We further improve the performance of the modified LSTM cell by leveraging additional data for topic relevance in a multi-task learning setup.,1 Introduction,[0],[0]
"(5) In the more challenging setup of cross-topic experiments, we show that our models yield considerably better performance than common BiLSTM models when little data of the target topic is available.",1 Introduction,[0],[0]
"Most existing approaches treat argument mining at the discourse level, focusing on tasks such as segmenting argumentative discourse units (Ajjour et al., 2017; Goudas et al., 2014), classifying the function of argumentative discourse units (for example, as claims or premises) (Mochales-Palau and Moens, 2009; Stab and Gurevych, 2014), and recognizing argumentative discourse relations (Eger et al., 2017; Stab and Gurevych, 2017; Nguyen and Litman, 2016).",2 Related work,[0],[0]
"These discourse-level approaches address the identification of argumentative structures within a single document but do not consider relevance to externally defined topics.
",2 Related work,[0],[0]
"To date, there has been little research on the identification of topic-relevant arguments for argument search.",2 Related work,[0],[0]
Wachsmuth et al. (2017) present a generic argument search framework.,2 Related work,[0],[0]
"However, it relies on already-structured arguments from debate portals and is not yet able to retrieve arguments from arbitrary texts.",2 Related work,[0],[0]
"Levy et al. (2014) investigate the identification of topic-relevant claims, an approach that was later extended with evidence extraction to mine supporting statements for claims (Rinott et al., 2015).",2 Related work,[0],[0]
"However, both approaches are designed to mine arguments from Wikipedia articles; it is unclear whether their annotation scheme is applicable to other text types.",2 Related work,[0],[0]
"It is also uncertain that it can be easily and accurately applied by untrained annotators, since it requires unitizing (i.e., finding the boundaries of argument components at the token level).",2 Related work,[0],[0]
Hua and Wang (2017) identify sentences in cited documents that have been used by an editor to formulate an argument.,2 Related work,[0],[0]
"By contrast, we do not limit our approach to the identification of sentences related to a given argument, but rather focus on the retrieval of any argument relevant to a given topic.",2 Related work,[0],[0]
"The fact that we are concerned with retrieval of arguments also sets our work apart from
the discourse-agnostic stance detection task of Mohammad et al. (2016), which is concerned with the identification of sentences expressing support or opposition to a given topic, irrespective of whether those sentences contain supporting evidence (as opposed to mere statements of opinion).
",2 Related work,[0],[0]
"Cross-domain AM experiments have so far been conducted only for discourse-level tasks such as claim identification (Daxenberger et al., 2017), argumentative segment identification (Al-Khatib et al., 2016), and argumentative unit segmentation (Ajjour et al., 2017).",2 Related work,[0],[0]
"However, the discourse-level argumentation models these studies employ seem to be highly dependent on the text types for which they were designed; they do not work well when applied to other text types (Daxenberger et al., 2017).",2 Related work,[0],[0]
The crucial difference between our own work and prior cross-domain experiments is that we investigate AM from heterogeneous texts across different topics instead of studying specific discourse-level AM tasks across restricted text types of existing corpora.,2 Related work,[0],[0]
"There exists a great diversity in models of argumentation, which differ in their perspective, complexity, terminology, and intended applications (Bentahar et al., 2010).",3 Corpus creation,[0],[0]
"For the present study, we propose a model which, though simplistic, is nonetheless well-suited to the argument search scenario.",3 Corpus creation,[0],[0]
We define an argument as a span of text expressing evidence or reasoning that can be used to either support or oppose a given topic.,3 Corpus creation,[0],[0]
"An argument need not be “direct” or self-contained—it may presuppose some common or domain knowledge, or the application of commonsense reasoning—but it must be unambiguous in its orientation to the topic.",3 Corpus creation,[0],[0]
"A topic, in turn, is some matter of controversy for which there is an obvious polarity to the possible outcomes—that is, a question of being either for or against the use or adoption of something, the commitment to some course of action, etc.",3 Corpus creation,[0],[0]
"In some graph-based models of argumentation (Stab, 2017, Ch. 2), what we refer to as a topic would be part of a (major) claim expressing a positive or negative stance, and our arguments would be premises with supporting/attacking consequence relations to the claim.",3 Corpus creation,[0],[0]
"However, unlike these models, which are typically used to represent (potentially deep or complex) argument structures at the discourse level, ours is a flat model that considers arguments in isolation from their surrounding context.",3 Corpus creation,[0],[0]
"A great
advantage of this approach is that it allows annotators to classify text spans without having to read large amounts of context and without having to consider relations to other topics or arguments.",3 Corpus creation,[0],[0]
"In this work, we consider only those topics that can be concisely and implicitly expressed through keywords, and those arguments that consist of individual sentences.",3 Corpus creation,[0],[0]
"Some examples, drawn from our dataset, are shown in Table 1.",3 Corpus creation,[0],[0]
"Note that while the fourth example expresses opposition to the topic, under our definition it is properly classified as a non-argument because it is a mere statement of stance that provides no evidence or reasoning.
",3 Corpus creation,[0],[0]
Data.,3 Corpus creation,[0],[0]
For our experiments we gathered a large collection of manually annotated arguments that cover a variety of topics and that come from a variety of text types.,3 Corpus creation,[0],[0]
"We started by randomly selecting eight topics (see Table 2) from online lists of controversial topics.2 For each topic, we made a Google query for the topic name, removed results not archived by the Wayback Machine,3 and truncated the list to the top 50 results.",3 Corpus creation,[0],[0]
"This resulted in a set of persistent, topic-relevant, largely polemical Web documents representing a range of genres and text types, including news reports, editorials, blogs, debate forums, and encyclopedia articles.",3 Corpus creation,[0],[0]
"We preprocessed each document with Apache Tika (Mattmann and Zitting, 2011) to remove boilerplate text.",3 Corpus creation,[0],[0]
"We then used the Stanford CoreNLP tools (Manning et al., 2014) to perform tokenization, sentence segmentation, and part-ofspeech tagging on the remaining text, and removed all sentences without verbs or with less than three tokens.",3 Corpus creation,[0],[0]
"This left us with a raw dataset of 27,520 sentences (about 2,700 to 4,400 per topic).",3 Corpus creation,[0],[0]
"Annotators classified the sentences using a browser-based interface that presents a set of in2https://www.questia.com/library/ controversial-topics, https://www.procon.org/ 3https://web.archive.org/
structions, a topic, a list of sentences, and amultiplechoice form for specifying whether each sentence is a supporting argument, an opposing argument, or not an argument with respect to the topic.",3 Corpus creation,[0],[0]
"(In preliminary experiments, we presented annotators with a fourth option for sentences that are ambiguous or incomprehensible.",3 Corpus creation,[0],[0]
"However, we found that these constituted less than 1% of the distribution and so mapped all such answers to the “no argument” class.)
",3 Corpus creation,[0],[0]
Annotation experiments.,3 Corpus creation,[0],[0]
"We tested the applicability of our annotation scheme by untrained annotators by performing an experiment where we had a group of “expert” annotators and a group of untrained annotators classify the same set of sentences, and then compared the two groups’ classifications.",3 Corpus creation,[0],[0]
The data for this experiment consisted of 200 sentences randomly selected from each of our eight topics.,3 Corpus creation,[0],[0]
Our expert annotators were two graduatelevel language technology researchers who were fully briefed on the nature and purpose of the argument model.,3 Corpus creation,[0],[0]
Our untrained annotators were anonymous American workers from the Amazon Mechanical Turk (AMT) crowdsourcing platform.,3 Corpus creation,[0],[0]
"Each sentence was independently annotated by the two expert annotators and ten crowd workers.
",3 Corpus creation,[0],[0]
"Inter-annotator agreement for our two experts, as measured by Cohen’s κ, was 0.721; this exceeds the commonly used threshold of 0.7 for assuming the results are reliable (Carletta, 1996).",3 Corpus creation,[0],[0]
"We proceeded by having the two experts resolve their disagreements, resulting in a set of “expert” gold-standard annotations.",3 Corpus creation,[0],[0]
"Similar gold standards were produced for the crowd annotations by applying the MACE denoising tool (Hovy et al., 2013); we tested various thresholds (1.0, 0.9, and 0.8) to discard instances that could be confidently assigned a gold label.",3 Corpus creation,[0],[0]
We then calculated κ between the remaining instances in the expert and crowd gold standards.,3 Corpus creation,[0],[0]
"In order to
determine the relationship between inter-annotator agreement and the number of crowd workers, we performed this procedure with successively lower numbers of crowd workers, going from the original ten annotators per instance down to two.",3 Corpus creation,[0],[0]
The results are visualized in Fig. 1.,3 Corpus creation,[0],[0]
We found that using seven annotators and a MACE threshold of 0.9 results in κ = 0.723; this gives us similar reliability as with the expert annotators without sacrificing much coverage.,3 Corpus creation,[0],[0]
"Table 3 shows the κ and percentage agreement for this setup, as well as the agreement between our expert annotators, broken down by topic.",3 Corpus creation,[0],[0]
"We proceeded with annotating the remaining instances in our dataset using seven crowd workers each, paying a rate corresponding to the US federal minimum wage of $7.25/hour.",3 Corpus creation,[0],[0]
"Our total expenditure, including AMT processing fees, was $2,774.02.",3 Corpus creation,[0],[0]
"After MACE denoising, we were left with 25,492 gold-standard annotations.",3 Corpus creation,[0],[0]
Table 2 provides statistics on the size and class distribution of the final corpus.,3 Corpus creation,[0],[0]
"We are releasing the gold-standard annotations for this dataset, and code for retrieving
the original sentences from the Wayback Machine, under a Creative Commons licence.",3 Corpus creation,[0],[0]
We model the identification of arguments as a sentence-level classification task.,4 Approaches for identifying arguments,[0],[0]
"In particular, given a sentence ς with words u1, . . .",4 Approaches for identifying arguments,[0],[0]
",unς and a topic τ of words v1, . . .",4 Approaches for identifying arguments,[0],[0]
", vnτ (e.g., “gun control” or “school uniforms”), we aim to classify ς",4 Approaches for identifying arguments,[0],[0]
"as a “supporting argument” or “opposing argument” if it includes a relevant reason for supporting or opposing the τ , or as a “non-argument” if it does not include a reason or is not relevant to τ .",4 Approaches for identifying arguments,[0],[0]
We also investigate a two-label classification where we combine supporting and opposing arguments into a single category; this allows us to evaluate argument classification independent of stance.,4 Approaches for identifying arguments,[0],[0]
"We focus on the challenging task of cross-topic experiments, where one topic is withheld from the training data and used for testing.",4 Approaches for identifying arguments,[0],[0]
"Here, we denote scalars by italic lowercase letters (e.g., t), vector representations by italic bold lowercase letters (e.g., c), and matrices as italic bold uppercase letters (e.g.,W ).",4 Approaches for identifying arguments,[0],[0]
"Since arguments need to be relevant to the given topic, we posit that providing topic information to the learner results in a more robust prediction capability in cross-topic setups.",4.1 Integrating topic information,[0],[0]
"Below, we present two models that integrate the topic, one that uses an attention mechanism and another that includes the topic vector directly in the LSTM cell.
",4.1 Integrating topic information,[0],[0]
Outer-attention BiLSTM (outer-att).,4.1 Integrating topic information,[0],[0]
"To let the model learn which parts of the sentence are relevant (or irrelevant) to the given topic, we use an attentionbased neural network (Bahdanau et al., 2014) that learns an importance weighting of the input words depending on the given topic.",4.1 Integrating topic information,[0],[0]
"In particular, we adopt an outer-attention mechanism similar to the one proposed by Hermann et al. (2015), which has achieved state-of-the-art results in related tasks such as natural language inference and recognizing textual entailment (Rocktäschel et al., 2015; Wang and Jiang, 2016).",4.1 Integrating topic information,[0],[0]
"We combine the attention mechanism with a common BiLSTM model and, at time step t, determine the importance weighting for each hidden state h(t) as
m(t) = tanh(W hh(t)",4.1 Integrating topic information,[0],[0]
"+W pp) (1)
fattention(h(t),p) = exp(wTmm(t))∑ t exp(wTmm(t))
",4.1 Integrating topic information,[0],[0]
"(2)
whereW h,W p, andwm are trainable parameters of the attention mechanism and p is the average of all word embeddings of topic words v1, . .",4.1 Integrating topic information,[0],[0]
.,4.1 Integrating topic information,[0],[0]
", vnτ .",4.1 Integrating topic information,[0],[0]
"Using the importance weighting, we determine the final, weighted hidden output state s as
αt ∝ fattention(h(t),p) (3) s = n∑ t=1",4.1 Integrating topic information,[0],[0]
h(t)αt .,4.1 Integrating topic information,[0],[0]
"(4)
Finally, we feed s into a dense layer with a softmax activation function to get predictions for our twoor three-label setups.
",4.1 Integrating topic information,[0],[0]
Contextual BiLSTM (biclstm).,4.1 Integrating topic information,[0],[0]
"A more direct approach to integrating an argument’s topic is the contextual LSTM (CLSTM) architecture (Ghosh et al., 2016), where topic information is added as another term to all four gates of an LSTM cell.",4.1 Integrating topic information,[0],[0]
"We, however, hypothesize that topic information is more relevant at the i- and c-gates, the former because it has the biggest impact on how a new token is processed and the latter because it is closely linked
to how the sequence seen so far is to be interpreted and stored.",4.1 Integrating topic information,[0],[0]
"To this end, we experimented with severalmodifications to the original CLSTMsuch as removing peepholes—i.e., removing gates’ access to the cell state c",4.1 Integrating topic information,[0],[0]
"(Gers and Schmidhuber, 2000)— and removing topic information from one or more gates.",4.1 Integrating topic information,[0],[0]
"Empirical results on the validation set show that topic integration at the i- and c-gates only, and removal of all peephole connections, does indeed outperform the original CLSTM on our task by 1 percentage point.",4.1 Integrating topic information,[0],[0]
"Our modified CLSTM (Fig. 2) is defined as
i t = σ(W xi x t",4.1 Integrating topic information,[0],[0]
+,4.1 Integrating topic information,[0],[0]
"W hiht−1 + bi + Wpi p ) (5)
f t = σ(W x f x t",4.1 Integrating topic information,[0],[0]
+W h f ht−1 + b f ),4.1 Integrating topic information,[0],[0]
"(6) ct = f tct−1 + i tσc(W xc x t +W hcht−1
+bc + Wpc p ) (7)
ot = σ(W xox t",4.1 Integrating topic information,[0],[0]
+,4.1 Integrating topic information,[0],[0]
W hoht−1 + bo) (8) ht = otσc(ct ).,4.1 Integrating topic information,[0],[0]
"(9)
Here i , f , and o represent the input, forget, and output gates; c the cell memory; x t the embedded token of a sentence at timestep t; ht−1 the previous hidden state; and b the bias.",4.1 Integrating topic information,[0],[0]
σ,4.1 Integrating topic information,[0],[0]
"and σc are the activation and recurrent activation functions, respectively.",4.1 Integrating topic information,[0],[0]
The novel terms for topic integration are outlined.,4.1 Integrating topic information,[0],[0]
"We use this model bidirectionally, as we did with our BiLSTM network, and hence refer to it as biclstm.",4.1 Integrating topic information,[0],[0]
"As we want to classify arguments related to specific topics, leveraging information that supports the classifier in the decision of topic-relation is crucial.",4.2 Leveraging additional data,[0],[0]
The multi-task learning (mtl) and transfer learning (trl) models are able to make use of auxiliary data that can potentially improve the results on the main task.,4.2 Leveraging additional data,[0],[0]
"Thus, we extend our previously described models by integrating them into mtl and trl setups.",4.2 Leveraging additional data,[0],[0]
"We also choose to integrate two corpora
from which we expect to learn (a) topic-relevance and (b) the capability to distinguish between supporting and attacking arguments.",4.2 Leveraging additional data,[0],[0]
"The first corpus, DIP2016 (Habernal et al., 2016), consists of 49 queries from the educational domain and 100 documents for each query.",4.2 Leveraging additional data,[0],[0]
Each document has its sentences annotated for relevance (true/false) to the query.4,4.2 Leveraging additional data,[0],[0]
"The second corpus, from SemEval-2016 Task 6 (Mohammad et al., 2016), consists of around 5000 multi-sentence tweets, a corresponding topic (e.g., “atheism”), and the author’s stance on the topic (for/against/neither).
",4.2 Leveraging additional data,[0],[0]
"For our mtl and trl approaches, we consider every possible pairing of a model (biclstm, outer-att, and the bilstm baseline we introduce in §5) with an auxiliary corpus (DIP2016, SemEval).",4.2 Leveraging additional data,[0],[0]
"We formalize our datasets as Sk = {(xki ,pki , yki )|i = 0, . . .",4.2 Leveraging additional data,[0],[0]
", |Sk |}, where k can be either our main dataset or an auxiliary dataset, xki denotes a single sentence as a sequence of word embeddings and yki its corresponding label in k, and pki represents the corresponding averaged topic vector.
",4.2 Leveraging additional data,[0],[0]
Transfer learning (trl).,4.2 Leveraging additional data,[0],[0]
"For trl, we use the approach of parameter transfer (Pan andYang, 2010)— i.e., we do not modify the model used.",4.2 Leveraging additional data,[0],[0]
"Instead, we train the model twice: the first time, we train the model on the chosen auxiliary corpus, and the second time, we keep the trained model’s weights and train it with our own corpus.",4.2 Leveraging additional data,[0],[0]
"For the threelabel setting, we have to modify the transfer model slightly for the DIP2016 corpus, since it provides only two labels for each training sample.",4.2 Leveraging additional data,[0],[0]
"In this case, we simply add a layer with two neurons on top of the layer with three neurons for training with the DIP2016 corpus and remove it afterwards for training with our corpus.
4We only use 300K of the corpus’s 600K samples to ease hyperparameter tuning for our computation-heavy models.
",4.2 Leveraging additional data,[0],[0]
Multi-task learning (mtl).,4.2 Leveraging additional data,[0],[0]
"For mtl, we use a shared–private model (Liu et al., 2017), which showed promising results for text classification and word segmentation (Chen et al., 2017).",4.2 Leveraging additional data,[0],[0]
"(We also experimented with their adversarial approach to learn topic-invariant features, but abandoned this due to low scores.)",4.2 Leveraging additional data,[0],[0]
"The mtl base model consists of a private recurrent neural network (RNN) for both the auxiliary dataset and our dataset, plus a shared RNN that both datasets use (Fig. 3).",4.2 Leveraging additional data,[0],[0]
The last hidden states of the RNNs are concatenated and fed through a dense layer and a softmax activation function.,4.2 Leveraging additional data,[0],[0]
"The model is trained in an alternating fashion—i.e., after each epoch the loss for the other dataset is minimized until each dataset has run for the set number of epochs, where the last epoch is always executed on our dataset.",4.2 Leveraging additional data,[0],[0]
"At prediction time, only the private RNN trained on our dataset and the shared RNN are used.",4.2 Leveraging additional data,[0],[0]
"The core idea is that the shared RNN learns what is relevant for both tasks, while the private ones learn only the task-specific knowledge.
",4.2 Leveraging additional data,[0],[0]
"For the cases of mtl+bilstm+corpus, mtl+biclstm+ corpus, and mtl+outer-att+corpus, we simply switch the RNN with our bilstm, biclstm, and outer-att, respectively.",4.2 Leveraging additional data,[0],[0]
"For mtl+outer-att+corpus, we add the outer attention mechanism (see §4.1), modified for use with the mtl model, after each of the private RNNs, while additionally feeding it a second topic vector—the last hidden state of the shared RNN:
m(t) =",4.2 Leveraging additional data,[0],[0]
tanh(W rhr (t) +,4.2 Leveraging additional data,[0],[0]
W shs,4.2 Leveraging additional data,[0],[0]
"+W pp) (10) fattention(hr (t),hs,p) = exp(wTmm(t))∑ t exp(wTmm(t))",4.2 Leveraging additional data,[0],[0]
"(11)
",4.2 Leveraging additional data,[0],[0]
αt ∝,4.2 Leveraging additional data,[0],[0]
"fattention(hr (t),hs,p) (12)
s = n∑ t=1 hr (t)αt (13)
",4.2 Leveraging additional data,[0],[0]
"whereW r ,W s, andW p are trainable weight matrices, hr (t) is the hidden state of the private bilstm at timestep t, hs is the last hidden state of the shared model, and p is the average of all word embeddings of topic words v1, . . .",4.2 Leveraging additional data,[0],[0]
", vnτ .",4.2 Leveraging additional data,[0],[0]
"To evaluate the robustness of the models, we conduct cross-topic experiments to evaluate how well the models generalize to an unknown topic.",5 Evaluation,[0],[0]
"To this end, we combine training (70%) and validation
data (10%) of seven topics for training and parameter tuning, and use the test data (20%) of the eighth topic for testing.",5 Evaluation,[0],[0]
"For encoding the words of sentence ς and topic τ , we use 300-dimensional word embeddings trained on the Google News dataset by Mikolov et al. (2013).",5 Evaluation,[0],[0]
"To handle out-of-vocabulary words, we create separate random word vectors for each.5 Since reporting single performance scores is insufficient to compare non-deterministic learning approaches like neural networks (Reimers and Gurevych, 2017), we report all results as averages over ten runs with different random seeds.",5 Evaluation,[0],[0]
"As evaluation measures, we report the average macro F1, as well as the precision and the recall for the argument class (Parg, Rarg).",5 Evaluation,[0],[0]
"For the three-label approach, we split the precision and recall for predicting supporting (Parg+, Rarg+) and attacking arguments (Parg−, Rarg−).",5 Evaluation,[0],[0]
"As baselines, we use a simple bidirectional LSTM (Hochreiter and Schmidhuber, 1997), as well as a logistic regression model with lowercased unigram features, which has been shown to be a strong baseline for various other AM tasks (Daxenberger et al., 2017; Stab and Gurevych, 2017).",5 Evaluation,[0],[0]
"We refer to these models as bilstm and lr-uni, respectively.",5 Evaluation,[0],[0]
"All neural networks are trained using the Adam optimizer (Kingma and Ba, 2015) and cross-entropy loss function.",5 Evaluation,[0],[0]
"For finding the best model, we run each for ten epochs and take the best model based on the lowest validation loss.",5 Evaluation,[0],[0]
"In addition to that, we tune the hyperparameters of all
5Each dimension is set to a random number between −0.01 and 0.01.",5 Evaluation,[0],[0]
"Digits are mapped to the same random word vector.
neural networks (see Appendix A).",5 Evaluation,[0],[0]
"To accelerate training, we truncate sentences at 60 words.6",5 Evaluation,[0],[0]
Two-label setup.,5.1 Results,[0],[0]
The results in Table 4 show that all our models outperform the baselines for two-label prediction.7 F1 for biclstm improves by 3.5 percentage points over the bilstm baseline and by 5.6 over lr-uni.,5.1 Results,[0],[0]
A main reason for this proves to be the substantial increase in recall for our topic-integrating models—outer-att and especially biclstm—in comparison to our baselines.,5.1 Results,[0],[0]
These results show that knowledge of the argument’s topic has a strong impact on argument prediction capability.,5.1 Results,[0],[0]
"Further, we observe that integrating biclstm in a multi-task learning setup in order to draw knowledge about topic relevance from the DIP2016 corpus (mtl+biclstm+dip2016) improves F1 by an additional 2.5 percentage points.",5.1 Results,[0],[0]
"It achieves an F1 of 0.6662, which is 19.48 percentage points less than the human upper bound of 0.861.",5.1 Results,[0],[0]
"When using the SemEval corpus, which holds less task-relevant knowledge for our two-label approach, we are able to gain only 1 percentage point when integrating it into mtl+biclstm+corpus.
",5.1 Results,[0],[0]
"For the transfer learning models that integrate the topic (tr+biclstm+corpus and tr+outer-att+corpus), the parameter transfer is mostly ineffective.",5.1 Results,[0],[0]
"If no topic is provided (tr+bilstm+corpus), the transfer learning models are able to improve over the baseline bilstm.",5.1 Results,[0],[0]
"This shows that the parameter transfer
6Only 244 of our sentences (<1%) exceed this length.",5.1 Results,[0],[0]
"7Detailed results per topic are given in Appendix B.
itself can be of use, but confuses the model when combined with topic integration.
",5.1 Results,[0],[0]
"In general, we observe an overall lower score for trl models that use the DIP2016 corpus compared to those using the SemEval corpus.",5.1 Results,[0],[0]
"In contrast to the mtl model, for trl models all parameters are transferred to the main task, not just parameters that represent shared knowledge.",5.1 Results,[0],[0]
"Thus, we suspect the lower scores of the trl models with DIP2016 are due to overfitting on the vast number of samples which shape the parameters much more than the comparatively small SemEval corpus could.
",5.1 Results,[0],[0]
Three-label setup.,5.1 Results,[0],[0]
"For the three-label approach, we observe overall lower scores due to the additional difficulty in distinguishing supporting from opposing arguments.",5.1 Results,[0],[0]
"As already observed in the two-label setup, biclstm outperforms both the bilstm and lr-uni baselines; here, the former by 4.5 and the latter by 4.2 percentage points in F1.",5.1 Results,[0],[0]
"Again, this is caused by a substantial increase in recall and shows the impact that the available topic information has on the classifier’s predictive power.",5.1 Results,[0],[0]
"For transfer learning, we see similar results as for the two-label approach; both the DIP2016 and SemEval corpora have a generally negative impact when compared to the respective base models.",5.1 Results,[0],[0]
The SemEval corpus does not provide the knowledge required to distinguish supporting from attacking arguments.,5.1 Results,[0],[0]
"We conclude that the original purpose of the SemEval task, stance recognition, is too different from our own.",5.1 Results,[0],[0]
"But in multi-task learning, where only the shared parameters are taken, we observe slight improvements when using biclstm with DIP2016; this correlates with the same model in the two-label setup.",5.1 Results,[0],[0]
"To understand the errors of our best model, mtlbiclstm-dip, and the nature of this task, we manually analyzed 100 sentences randomly sampled from the false positive and false negative arguments of the three-label experiments (combining supporting and attacking arguments).",5.2 Error analysis,[0],[0]
"Among the false positives, we found 48 off-topic sentences that were wrongly classified as arguments.",5.2 Error analysis,[0],[0]
The 52 on-topic false positives consist of non-argumentative background information or mere opinions without evidence (as with the first and fourth examples of Table 1) and questions about the topic.,5.2 Error analysis,[0],[0]
"Among the false negatives, we found 65 arguments that did not explicitly refer to the topic but to related aspects that
depend on background knowledge.",5.2 Error analysis,[0],[0]
"For instance, the model fails to establish an argumentative link between the topic “gun control” and the Second Amendment to the US Constitution.",5.2 Error analysis,[0],[0]
"Lastly, we inspected arguments that are incorrectly classified as supporting and/or opposing a topic.",5.2 Error analysis,[0],[0]
We found several samples in which the term “against” is not correctly interpreted and the argument is classified as supporting a topic.,5.2 Error analysis,[0],[0]
"Similarly, for arguments incorrectly classified as attacking, we find various samples where the word “oppose” is used not to oppose the topic but to strengthen a supporting argument, as in “There is reason even for people who oppose the use of marijuana to support its legalization. . . ”",5.2 Error analysis,[0],[0]
"To evaluate the performance of the models in datascarce scenarios, we gradually add target topic data to the training data and analyze the model performance on the target test set.",5.3 Adapting to new topics,[0],[0]
"Figure 4 shows model performance (F1, Parg, and Rarg) on the “marijuana legalization” topicwhen adding different amounts of randomly sampled topic-specific data to the training data (x-axes).8",5.3 Adapting to new topics,[0],[0]
"As the results show, the models that integrate the topic achieve higher recall when adding target topic data to the training data.",5.3 Adapting to new topics,[0],[0]
"For bilstm, we observe a drastic difference when compared to the other models; the recall for arguments stays at around 30% and rises only when integrating more than 60% target topic data.",5.3 Adapting to new topics,[0],[0]
"In strong contrast, topic-integrating models retrieve a much higher number of actual arguments at target topic augmentation levels as low as 20%.",5.3 Adapting to new topics,[0],[0]
"Further, and equally important, this does not come at the cost of precision; on the contrary, the precision is mostly steady and slowly rising after around 20% of target topic integration, leading to an overall higher F1 for these models.",5.3 Adapting to new topics,[0],[0]
"Finally, in comparing F1 between topic-integrating models and bilstm, we conclude that the former need much less target topic data to substantially improve their score, making them more robust in situations of data scarcity.",5.3 Adapting to new topics,[0],[0]
We have presented a new approach for searching a document collection for arguments relevant to a given topic.,6 Conclusion,[0],[0]
"First, we introduced an annotation scheme suited to the information-seeking perspec-
8Each data point in the plot is the average score of ten runs with different random samples of target topic data.
",6 Conclusion,[0],[0]
tive of argument search and showed that it is cheaply but reliably applicable by untrained annotators to arbitrary Web texts.,6 Conclusion,[0],[0]
"Second, we presented a new corpus, including over 25,000 instances over eight topics, that allows for cross-topic experiments using heterogeneous text types.",6 Conclusion,[0],[0]
"Third, we conducted cross-topic experiments and showed that integrating topic information of arguments with our contextual BiLSTM leads to better generalization to unknown topics.",6 Conclusion,[0],[0]
"Fourth, by leveraging knowledge from similar datasets and integrating our contextual BiLSTM into a multi-task learning setup, we were able to gain an improvement over our strongest baseline of 5.9 percentage points in F1 in the two-label setup and 4.6 in the three-label setup.",6 Conclusion,[0],[0]
"Finally, by gradually adding target topic data to our training set, we showed that, when available, even small amounts of target topic data (20%) have a strong positive influence on the recall of arguments.
",6 Conclusion,[0],[0]
"In a separate, simultaneously written paper (Stab et al., 2018) we evaluate our models in real-world application scenarios by applying them to a large document collection and comparing the results to a manually produced gold standard.",6 Conclusion,[0],[0]
"An online argument search engine implementing our approach is now available for noncommercial use at https://www.argumentsearch.com/. Furthermore, we are experimenting with language adaptation and plan to extend the tool to the German language.",6 Conclusion,[0],[0]
Preliminary results are presented in Stahlhut (2018).,6 Conclusion,[0],[0]
We also intend to investigate methods for grouping similar arguments.,6 Conclusion,[0],[0]
This work has been supported by the German Federal Ministry of Education and Research (BMBF) under the promotional reference 03VP02540,Acknowledgements,[0],[0]
"(Ar-
gumenText) and the DFG-funded research training group “Adaptive Preparation of Information form Heterogeneous Sources” (AIPHES, GRK 1994/1).",Acknowledgements,[0],[0]
Argument mining is a core technology for automating argument search in large document collections.,abstractText,[0],[0]
"Despite its usefulness for this task, most current approaches are designed for use onlywith specific text types and fall short when applied to heterogeneous texts.",abstractText,[0],[0]
"In this paper, we propose a new sentential annotation scheme that is reliably applicable by crowd workers to arbitrary Web texts.",abstractText,[0],[0]
"We source annotations for over 25,000 instances covering eight controversial topics.",abstractText,[0],[0]
We show that integrating topic information into bidirectional long short-termmemory networks outperforms vanilla BiLSTMs by more than 3 percentage points in F1 in twoand three-label cross-topic settings.,abstractText,[0],[0]
We also show that these results can be further improved by leveraging additional data for topic relevance using multi-task learning.,abstractText,[0],[0]
Cross-topic Argument Mining from Heterogeneous Sources,title,[0],[0]
"Crowdsourcing is an omnipresent phenomenon: it has emerged as an integral part of the machine learning pipeline in recent years, and one reason for the great advances in deep learning is the presence of large data sets that have been labeled by the crowd (e.g., Deng et al., 2009; Krizhevsky, 2009).",1. Introduction,[0],[0]
"Crowdsourcing is also at the heart of peer grading systems (e.g., Alfaro & Shavlovsky, 2014), which help with rising enrollment at universities, and online rating systems (e.g., Liao et al., 2014), which many of us rely on when choosing the next restaurant, to provide just a few examples.
",1. Introduction,[0],[0]
A crowdsourcing scenario consists of a set of workers and a set of tasks that need to be solved.,1. Introduction,[0],[0]
A data curator utilizing crowdsourcing can aim at estimating various quantities of interest.,1. Introduction,[0],[0]
The first goal might be to estimate the true labels or answers for the tasks at hand.,1. Introduction,[0],[0]
"Typically, additional constraints are involved here such as a worker not being willing
1Department of Computer Science, Rutgers University, Piscataway Township, New Jersey, USA.",1. Introduction,[0],[0]
Correspondence to: Matthäus,1. Introduction,[0],[0]
"Kleindessner <matthaeus.kleindessner@rutgers.edu>, Pranjal Awasthi <pranjal.awasthi@rutgers.edu>.
Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
to solve too many tasks and the data curator wanting to get high-quality labels at a low price.",1. Introduction,[0],[0]
The canonical example of this case is the Amazon Mechanical TurkTM.,1. Introduction,[0],[0]
There one cannot track specific workers as they are fleeting.,1. Introduction,[0],[0]
"However, in scenarios such as peer grading or online rating systems, a second goal might be to estimate worker qualities, especially if workers can be reused at a later time.
",1. Introduction,[0],[0]
"In a seminal paper, Dawid & Skene (1979) proposed a formal model that involves worker quality parameters for crowdsourcing scenarios in the context of classification.",1. Introduction,[0],[0]
"The Dawid-Skene model has become a standard theoretical framework and has led to a flurry of research over the past few years (Liu et al., 2012; Raykar & Yu, 2012; Li et al., 2013; Gao et al., 2016; Zhang et al., 2016; Khetan et al., 2017), in particular in its special symmetric form usually referred to as one-coin model (Ghosh et al., 2011; Karger et al., 2011a;b; Dalvi et al., 2013; Gao & Zhou, 2013; Karger et al., 2014; Bonald & Combes, 2017; Ma et al., 2017).",1. Introduction,[0],[0]
"In its general form for binary classification problems, the DawidSkene model assumes that for each worker, the probability of providing the wrong label only depends on the true label of the task, but not on the task itself.",1. Introduction,[0],[0]
"Moreover, given the true label, the responses provided by different workers are independent.",1. Introduction,[0],[0]
"The one-coin model additionally assumes that for each worker, the probability of providing the wrong label is the same for both classes.",1. Introduction,[0],[0]
We will formally introduce the one-coin model in Section 2.,1. Introduction,[0],[0]
"A discussion of prior work work is provided in Section 5 and Appendix A.
The crucial limitation of the Dawid-Skene and one-coin model is the assumption that workers’ error probabilities are task-independent.",1. Introduction,[0],[0]
"In particular, this excludes the possibility of colluding adversaries (other than those that provide the wrong label all of the time), which might make these models a poor approximation of the real world encountered in such applications as peer grading or online rating.",1. Introduction,[0],[0]
"In this paper, we study a significant extension of the one-coin model that allows for arbitrary, highly colluding adversaries.",1. Introduction,[0],[0]
We provide an algorithm for estimating the workers’ error probabilities and prove that it asymptotically recovers the true error probabilities.,1. Introduction,[0],[0]
"Using our estimates of the error probabilities in weighted majority votes, we also provide strategies to estimate ground-truth labels of the tasks.",1. Introduction,[0],[0]
Experiments on both synthetic and real data show that our approach clearly outperforms existing methods in the presence of adversaries.,1. Introduction,[0],[0]
"We first describe a general model for crowdsourcing with non-adaptive workers and binary classification tasks: there are n workers w1, . . .",2. Setup and problem formulation,[0],[0]
", wn and an i.i.d. sample of m tasklabel pairs ((xi, yi))mi=1 ∼",2. Setup and problem formulation,[0],[0]
"Dm, where D is a joint probability distribution over tasks x ∈ X and corresponding labels y ∈ {−1,+1}.",2. Setup and problem formulation,[0],[0]
"There is a variable gij ∈ {0, 1},",2. Setup and problem formulation,[0],[0]
i ∈,2. Setup and problem formulation,[0],[0]
"[m], j ∈",2. Setup and problem formulation,[0],[0]
"[n], indicating whether worker wj is presented with task xi (for k ∈ N, we use [k] to denote the set {1, . . .",2. Setup and problem formulation,[0],[0]
", k}).",2. Setup and problem formulation,[0],[0]
"If wj is presented with xi, that is gij = 1, wj provides an estimate wj(xi) ∈ {−1,+1} of the ground-truth label yi.",2. Setup and problem formulation,[0],[0]
"Let A ∈ {−1, 0,+1}m×n be a matrix that stores all the responses collected from the workers: Aij = wj(xi) if gij = 1 and Aij = 0",2. Setup and problem formulation,[0],[0]
"if gij = 0.
",2. Setup and problem formulation,[0],[0]
We assume that each worker wj follows some (probabilistic or deterministic) strategy such that wj(xi) only depends on xi.,2. Setup and problem formulation,[0],[0]
"In particular, given xi, any two different workers’ responses wj(xi) and wk(xi) and the ground-truth label yi are independent.",2. Setup and problem formulation,[0],[0]
"Let εwj (x, y) ∈",2. Setup and problem formulation,[0],[0]
"[0, 1] be the conditional error probability that, given x and y, wj(x) does not equal y, that is
εwj (x, y)",2. Setup and problem formulation,[0],[0]
":= Prwj |(x,y)[wj(x) 6=",2. Setup and problem formulation,[0],[0]
"y | (x, y)].",2. Setup and problem formulation,[0],[0]
"(1)
Note that the unconditional probability of wj(x) being incorrect, before seeing x and y, is given by
Pr(x,y)∼D,wj [wj(x) 6=",2. Setup and problem formulation,[0],[0]
"y] = E(x,y)∼D[εwj (x, y)]",2. Setup and problem formulation,[0],[0]
"=: εwj .
",2. Setup and problem formulation,[0],[0]
"Now one may study the following questions:
(i)",2. Setup and problem formulation,[0],[0]
"Given only the matrix A, how can we estimate the ground-truth labels y1, . . .",2. Setup and problem formulation,[0],[0]
", ym?
(ii)",2. Setup and problem formulation,[0],[0]
"Given only the matrix A, how can we estimate the workers’ unconditional error probabilities εw1 , . . .",2. Setup and problem formulation,[0],[0]
", εwn?
(iii)",2. Setup and problem formulation,[0],[0]
"If we can choose gij (either in advance of collecting workers’ responses or adaptively while doing so), how should we choose it such that we can achieve (i) or (ii) with a minimum number of collected responses?
",2. Setup and problem formulation,[0],[0]
"In case of εwj (x, y) as defined in (1) being constant on X × {−1,+1}, that is εwj (x, y) ≡ εwj , for all j ∈",2. Setup and problem formulation,[0],[0]
"[n], our model boils down to what is usually referred to as the one-coin model (e.g., Szepesvari, 2015), for which (i) to (iii) have been studied extensively (see Section 5 and Appendix A for references and a detailed discussion).",2. Setup and problem formulation,[0],[0]
With this paper we initiate the study of a significant extension of the one-coin model.,2. Setup and problem formulation,[0],[0]
"We will allow almost half of the workers to deviate from the one-coin model and for such a worker wj , the conditional error probability εwj (x, y) to be a completely arbitrary random variable.",2. Setup and problem formulation,[0],[0]
"In other words, we will allow for arbitrary adversaries, for which not only error
probabilities can be high, but for which error probabilities can be arbitrarily correlated.",2. Setup and problem formulation,[0],[0]
We mainly study (ii) in this scenario.,2. Setup and problem formulation,[0],[0]
We then make use of existing results for the onecoin model to answer (i) satisfactorily for our purposes.,2. Setup and problem formulation,[0],[0]
"We do not deal with (iii), but instead assume that gij has been specified in advance.",2. Setup and problem formulation,[0],[0]
In this section we want to present the general outline of our approach.,3. General outline of our approach,[0],[0]
"A key insight is that the unconditional probability of workers wj and wk being agreeing is given by
Pr(x,y)∼D,wj ,wk [wj(x) = wk(x)]",3. General outline of our approach,[0],[0]
"= 1− εwj − εwk+ 2εwjεwk + 2 Cov(x,y)∼D[εwj (x, y), εwk(x, y)].
(2)
Cov(x,y)∼D[εwj",3. General outline of our approach,[0],[0]
"(x, y), εwk(x, y)] denotes the covariance between random variables εwj",3. General outline of our approach,[0],[0]
"(x, y) and εwk(x, y), that is
Cov(x,y)∼D[εwj",3. General outline of our approach,[0],[0]
"(x, y), εwk(x, y)]",3. General outline of our approach,[0],[0]
"= E(x,y)∼D[(εwj (x, y)− εwj ) · (εwk(x, y)− εwk)].
",3. General outline of our approach,[0],[0]
"A proof of (2) can be found in Appendix B. The probability on the left-hand side of (2) can be easily estimated from A by the ratio of the number of tasks that wj and wk agreed on to the number of tasks they were both presented with:
Pr[wj(x) = wk(x)]",3. General outline of our approach,[0],[0]
≈ ∑m i=1,3. General outline of our approach,[0],[0]
"gijgik1{Aij = Aik}∑m
i=1",3. General outline of our approach,[0],[0]
"gijgik =: pjk.
(3)
",3. General outline of our approach,[0],[0]
"This suggests to solve the system of equations
1− εj",3. General outline of our approach,[0],[0]
"− εk + 2εjεk + 2cjk = pjk, 1 ≤ j < k ≤ n, (4)
in the unknowns εl, l ∈",3. General outline of our approach,[0],[0]
"[n], and cjk, 1 ≤ j",3. General outline of our approach,[0],[0]
"< k ≤ n, in order to obtain estimates of the workers’ unconditional error probabilities εw1 , . . .",3. General outline of our approach,[0],[0]
", εwn .",3. General outline of our approach,[0],[0]
"However, there is a catch: in general, the system (4) is not identifiable and has several solutions.",3. General outline of our approach,[0],[0]
We will assume that at least n2 + 2 of the workers follow the one-coin model and have error probabilities smaller than one half.,3. General outline of our approach,[0],[0]
"A worker wj following the one-coin model implies
Cov(x,y)∼D[εwj",3. General outline of our approach,[0],[0]
"(x, y), εwk(x, y)] = 0, ∀k 6=",3. General outline of our approach,[0],[0]
"j, (5)
and hence under this assumption we can restrict the search for solutions of (4) to εl, l ∈",3. General outline of our approach,[0],[0]
"[n], and cjk, 1 ≤ j",3. General outline of our approach,[0],[0]
"< k ≤ n, with the property that1
∃L ⊆",3. General outline of our approach,[0],[0]
[n] with |L| ≥ n/2 + 2 such that ∀j ∈ L : (εj < 1/2,3. General outline of our approach,[0],[0]
∧,3. General outline of our approach,[0],[0]
[∀k 6= j : cjk = 0]) .,3. General outline of our approach,[0],[0]
"(6)
1Throughout the paper, we set cjk = ckj if",3. General outline of our approach,[0],[0]
j > k.,3. General outline of our approach,[0],[0]
"We also assume pjk = pkj .
",3. General outline of our approach,[0],[0]
"Note that we never assume to know which workers follow the one-coin model, which corresponds to using the existential quantifier for the set L in (6) rather than considering a “fixed” L. We can show that the system (4) has at most one solution with property (6).",3. General outline of our approach,[0],[0]
We also provide evidence that our assumption of n2 + 2 of the workers following the one-coin model and having error probabilities smaller than one half is a necessary condition for guaranteeing the identifiability of system (4).,3. General outline of our approach,[0],[0]
"If the workers satisfy our assumption and pjk on the right-hand side of (4) are actually true agreement probabilities, then εl = εwl and cjk = Cov[εwj",3. General outline of our approach,[0],[0]
"(x, y), εwk(x, y)] is the unique solution of (4) that satisfies (6).",3. General outline of our approach,[0],[0]
"But if pjk are not exactly true agreement probabilities, there might be no solution of (4) with property (6) at all.",3. General outline of our approach,[0],[0]
"We prove that if estimates pjk are not too bad, we can solve (4) together with (6) approximately, and our approximate solution is guaranteed to be close to true error probabilities εw1 , . . .",3. General outline of our approach,[0],[0]
", εwn and covariances Cov[εwj",3. General outline of our approach,[0],[0]
"(x, y), εwk(x, y)], j < k.",3. General outline of our approach,[0],[0]
This answers (ii) from Section 2 and is the main contribution of our paper: Main result.,3. General outline of our approach,[0],[0]
Assume that at least n2 + 2 of the workers follow the one-coin model and have error probabilities not greater than γTR < 12 .,3. General outline of our approach,[0],[0]
If |Pr[wj(x) = wk(x)],3. General outline of our approach,[0],[0]
"− pjk| ≤ β for all j 6= k and β sufficiently small, we can compute estimates ε̂w1 , . . .",3. General outline of our approach,[0],[0]
", ε̂wn of εw1 , . .",3. General outline of our approach,[0],[0]
.,3. General outline of our approach,[0],[0]
", εwn such that
|εwi",3. General outline of our approach,[0],[0]
"− ε̂wi | ≤ C(γTR) · β1/4.
",3. General outline of our approach,[0],[0]
"We answer (i) from Section 2 and provide two ways to predict ground-truth labels y1, . . .",3. General outline of our approach,[0],[0]
", ym by taking weighted majority votes over the responses provided by the workers.",3. General outline of our approach,[0],[0]
"In these majority votes, the weights depend on our estimates of true error probabilities εw1 , . . .",3. General outline of our approach,[0],[0]
", εwn .",3. General outline of our approach,[0],[0]
"If gij has been specified in advance, we have the following guarantee on the quality of the estimates pjk (see (3)):",4.1. Estimating agreement probabilities,[0],[0]
Lemma 1.,4.1. Estimating agreement probabilities,[0],[0]
Assume ∑m i=1,4.1. Estimating agreement probabilities,[0],[0]
"gijgik > 0, j 6= k.",4.1. Estimating agreement probabilities,[0],[0]
Let δ > 0,4.1. Estimating agreement probabilities,[0],[0]
"and
βjk",4.1. Estimating agreement probabilities,[0],[0]
"= min
{ 1, [ ln(2n2/δ)/ ( 2 ∑m
i=1",4.1. Estimating agreement probabilities,[0],[0]
"gijgik
)]1/2} .
",4.1. Estimating agreement probabilities,[0],[0]
"Then we have with probability at least 1− δ over the sample ((xi, yi))",4.1. Estimating agreement probabilities,[0],[0]
m i=1,4.1. Estimating agreement probabilities,[0],[0]
"and the randomness in workers’ strategies that
|Pr[wj(x) = wk(x)]− pjk| ≤ βjk, 1 ≤ j < k ≤",4.1. Estimating agreement probabilities,[0],[0]
"n.
Proof.",4.1. Estimating agreement probabilities,[0],[0]
A straightforward application of Hoeffding’s inequality and the union bound yields the result.,4.1. Estimating agreement probabilities,[0],[0]
"If all workers follow the one-coin model, that is εwj (x, y) ≡",4.2. Identifiability and approximate solution,[0],[0]
εwj for all j ∈,4.2. Identifiability and approximate solution,[0],[0]
"[n], we have
Cov(x,y)∼D[εwj",4.2. Identifiability and approximate solution,[0],[0]
"(x, y), εwk(x, y)]",4.2. Identifiability and approximate solution,[0],[0]
"= 0, 1 ≤ j < k ≤ n, and system (4) reduces to
1− εj − εk + 2εjεk = pjk, 1 ≤ j < k ≤ n, (7)
in the unknowns εl, l ∈",4.2. Identifiability and approximate solution,[0],[0]
[n].,4.2. Identifiability and approximate solution,[0],[0]
"It is well known that, in general, even (7) is not identifiable.",4.2. Identifiability and approximate solution,[0],[0]
"For example, if pjk = 1 for all 1 ≤ j < k ≤ n, there are the two solutions εl = 0, l ∈",4.2. Identifiability and approximate solution,[0],[0]
"[n], and εl = 1, l ∈",4.2. Identifiability and approximate solution,[0],[0]
"[n], corresponding to either all perfect or all completely erroneous workers.",4.2. Identifiability and approximate solution,[0],[0]
"On the other hand, the system (7) is identifiable if we assume that on average workers are better than random guessing, that is 1 n ∑n j=1 εwj < 1 2 , and there are at least three informative workers with εwj",4.2. Identifiability and approximate solution,[0],[0]
6= 12,4.2. Identifiability and approximate solution,[0],[0]
(,4.2. Identifiability and approximate solution,[0],[0]
"Bonald & Combes, 2017).
",4.2. Identifiability and approximate solution,[0],[0]
"Clearly, these two conditions do not guarantee identifiability of the general system (4).",4.2. Identifiability and approximate solution,[0],[0]
"The next lemma shows that even if we additionally assume half of the workers to follow the one-coin model, the system (4) is not identifiable.",4.2. Identifiability and approximate solution,[0],[0]
Here we only state an informal version of the lemma.,4.2. Identifiability and approximate solution,[0],[0]
"A detailed version and its proof can be found in Appendix B.
Lemma 2.",4.2. Identifiability and approximate solution,[0],[0]
"There exists an instance of the system (4), where n is even, that has two different solutions.",4.2. Identifiability and approximate solution,[0],[0]
"In both solutions, it holds that εl < 12 , l ∈",4.2. Identifiability and approximate solution,[0],[0]
[n].,4.2. Identifiability and approximate solution,[0],[0]
"Furthermore:
(a) In the first solution, cjk = 0 for all j ∈",4.2. Identifiability and approximate solution,[0],[0]
"[n2 ] and k 6= j, and εl is small",4.2. Identifiability and approximate solution,[0],[0]
if l ∈,4.2. Identifiability and approximate solution,[0],[0]
[n2 ] and,4.2. Identifiability and approximate solution,[0],[0]
big if l ∈,4.2. Identifiability and approximate solution,[0],[0]
[n] \,4.2. Identifiability and approximate solution,[0],[0]
[ n 2 ].,4.2. Identifiability and approximate solution,[0],[0]
"(b) In the second solution, cjk = 0 for all j ∈",4.2. Identifiability and approximate solution,[0],[0]
"[n]\ [n2 ] and k 6= j, and εl is small if l ∈",4.2. Identifiability and approximate solution,[0],[0]
[n] \,4.2. Identifiability and approximate solution,[0],[0]
"[n2 ] and big if l ∈ [ n 2 ].
We want to mention that a solution of (4) does not necessarily correspond to actual workers, that is given εl, l ∈",4.2. Identifiability and approximate solution,[0],[0]
"[n], and cjk, 1 ≤ j",4.2. Identifiability and approximate solution,[0],[0]
"< k ≤ n, there might be no collection of workers w1, . . .",4.2. Identifiability and approximate solution,[0],[0]
", wn such that εwl = εl and Cov[εwj",4.2. Identifiability and approximate solution,[0],[0]
"(x, y), εwk(x, y)] = cjk.",4.2. Identifiability and approximate solution,[0],[0]
"By the BhatiaDavis inequality (Bhatia & Davis, 2010) it holds that Var[εwj (x, y)] ≤ εwj − ε 2wj .",4.2. Identifiability and approximate solution,[0],[0]
"Hence, a necessary condition for a solution to correspond to actual workers is that |cjk| ≤ (εj−ε 2j )1/2(εk−ε 2k )1/2",4.2. Identifiability and approximate solution,[0],[0]
(in addition to εl ∈,4.2. Identifiability and approximate solution,[0],[0]
"[0, 1]).",4.2. Identifiability and approximate solution,[0],[0]
"The two solutions in Lemma 2 correspond to actual workers.
",4.2. Identifiability and approximate solution,[0],[0]
"From now on we assume that at least n2 + 2 workers follow the one-coin model and have error probabilities smaller than one half:2
Assumption A.",4.2. Identifiability and approximate solution,[0],[0]
There exists L ⊆,4.2. Identifiability and approximate solution,[0],[0]
"[n] with |L| ≥ n/2 + 2 such that for all j ∈ L, the worker wj follows the one-coin model with error probability εwj < 1/2.
",4.2. Identifiability and approximate solution,[0],[0]
This corresponds to considering (4) together with the constraint (6).,4.2. Identifiability and approximate solution,[0],[0]
"The system (4) together with (6) is identifiable:
Proposition 1.",4.2. Identifiability and approximate solution,[0],[0]
"There exists at most one solution of system (4) that has property (6).
",4.2. Identifiability and approximate solution,[0],[0]
"2All results of Section 4.2 hold true if we assume, more generally, the existence of L ⊆",4.2. Identifiability and approximate solution,[0],[0]
"[n] with |L| ≥ n
2 + 2 such that (5)
together with εwj < 1 2 holds for all j ∈ L.
Proof.",4.2. Identifiability and approximate solution,[0],[0]
"Assuming there are two solutions (εS1l )l∈[n], (c S1jk )",4.2. Identifiability and approximate solution,[0],[0]
"1≤j<k≤n and (ε S2 l )l∈[n], (c S2 jk )1≤j<k≤n with L1 and L2 satisfying (6), there have to be pairwise different i1, i2, i3 ∈ L1 ∩ L2.",4.2. Identifiability and approximate solution,[0],[0]
"It is easy to see that (εS1i1 , ε S1 i2 , εS1i3 ) and (εS2i1 , ε S2 i2 , εS2i3 ) and consequently also all the other components of the two solutions have to coincide.",4.2. Identifiability and approximate solution,[0],[0]
"Details can be found in Appendix B.
If pjk at the right-hand side of (4) are true agreement probabilities, the true error probabilities εw1 , . . .",4.2. Identifiability and approximate solution,[0],[0]
", εwn and covariances Cov[εwj",4.2. Identifiability and approximate solution,[0],[0]
"(x, y), εwk(x, y)], j < k, make up the unique solution of (4) that satisfies (6), but if pjk are not exactly true agreement probabilities, there might be no solution of (4) that satisfies (6) at all.",4.2. Identifiability and approximate solution,[0],[0]
"Our goal is then to find a solution of (4) that satisfies (6) approximately and to show that our approximate solution has to be close to εw1 , . . .",4.2. Identifiability and approximate solution,[0],[0]
", εwn and Cov[εwj",4.2. Identifiability and approximate solution,[0],[0]
"(x, y), εwk(x, y)], j < k.",4.2. Identifiability and approximate solution,[0],[0]
"As a first step towards this goal we need a generalization of Proposition 1:
Proposition 2.",4.2. Identifiability and approximate solution,[0],[0]
Let γ < 1/2 and ν < 1/8− γ/2 +,4.2. Identifiability and approximate solution,[0],[0]
γ2/2.,4.2. Identifiability and approximate solution,[0],[0]
"If there exist two solutions (εSil )l∈[n], (c Si jk )1≤j<k≤n, i ∈ {1, 2}, of system (4) (where pjk ∈",4.2. Identifiability and approximate solution,[0],[0]
"[0, 1]) with the property that εSil ∈",4.2. Identifiability and approximate solution,[0],[0]
"[0, 1], l ∈",4.2. Identifiability and approximate solution,[0],[0]
"[n], and
∃Li ⊆",4.2. Identifiability and approximate solution,[0],[0]
[n] with |Li| ≥ n/2 + 2 such that ∀j ∈ Li : ( εSij ≤,4.2. Identifiability and approximate solution,[0],[0]
γ,4.2. Identifiability and approximate solution,[0],[0]
∧,4.2. Identifiability and approximate solution,[0],[0]
[ ∀k 6=,4.2. Identifiability and approximate solution,[0],[0]
"j : |c Sijk | ≤ ν ]) , (8)
then∣∣εS1l − εS2l ∣∣ ≤ G(γ, ν)√ν, ∣∣c S1jk − c S2jk ∣∣ ≤ 3G(γ, ν)√ν for l ∈",4.2. Identifiability and approximate solution,[0],[0]
"[n], j < k, where G(γ, ν)→ G(γ)",4.2. Identifiability and approximate solution,[0],[0]
> 0,4.2. Identifiability and approximate solution,[0],[0]
"as ν → 0.
",4.2. Identifiability and approximate solution,[0],[0]
"The proof of Proposition 2, which provides an explicit expression for G(γ, ν), can be found in Appendix B.
In a next step, we assume that we are given pairwise different i1, i2, i3 ∈",4.2. Identifiability and approximate solution,[0],[0]
"[n] such that wi1 , wi2 , wi3 follow the onecoin model with εwi1 , εwi2 , εwi3 < 1/2.",4.2. Identifiability and approximate solution,[0],[0]
"In this case, assuming that estimates pjk are close to true agreement probabilities, we can construct a solution of (4) that is guaranteed to be close to the true error probabilities and covariances (and hence approximately satisfies (6)).",4.2. Identifiability and approximate solution,[0],[0]
"This is made precise in the next lemma (its proof can be found in Appendix B).
",4.2. Identifiability and approximate solution,[0],[0]
Lemma 3.,4.2. Identifiability and approximate solution,[0],[0]
Let γTR < 1/2 and consider the system (4) with p TRjk ∈,4.2. Identifiability and approximate solution,[0],[0]
"[0, 1] as right-hand side.",4.2. Identifiability and approximate solution,[0],[0]
"Assume there exists a solution3 (εTRl )l∈[n], (c TR jk )1≤j<k≤n with the property that",4.2. Identifiability and approximate solution,[0],[0]
εTRl ∈,4.2. Identifiability and approximate solution,[0],[0]
"[0, 1] and
∃LTR ⊆",4.2. Identifiability and approximate solution,[0],[0]
[n] with |LTR| ≥ n/2 + 2 such that ∀j ∈ LTR : ( εTRj ≤ γTR,4.2. Identifiability and approximate solution,[0],[0]
∧,4.2. Identifiability and approximate solution,[0],[0]
[ ∀k 6=,4.2. Identifiability and approximate solution,[0],[0]
j :,4.2. Identifiability and approximate solution,[0],[0]
c TRjk = 0 ]) .,4.2. Identifiability and approximate solution,[0],[0]
"(9)
Now consider the system (4) with pjk ∈",4.2. Identifiability and approximate solution,[0],[0]
"[0, 1] as right-hand side.",4.2. Identifiability and approximate solution,[0],[0]
"Assume that |p TRjk − pjk| ≤ β for all j 6= k, where
3By Proposition 1, this solution is unique.
",4.2. Identifiability and approximate solution,[0],[0]
β satisfies β < 1/2− 2γTR + 2γ2TR.,4.2. Identifiability and approximate solution,[0],[0]
"Let i1, i2, i3 ∈",4.2. Identifiability and approximate solution,[0],[0]
"[n] be pairwise different and set
B := −2",4.2. Identifiability and approximate solution,[0],[0]
"+ 4pi1i3 , C := 1 + 2pi1i2pi2i3",4.2. Identifiability and approximate solution,[0],[0]
− pi1i2,4.2. Identifiability and approximate solution,[0],[0]
"− pi1i3 − pi2i3 ,
εRi2 := 1 2 − √ B + 4C 2 √ B , εSi2 := min(γTR,max(0, ε R i2))
(10)
and for all l 6= i2 and for all 1 ≤ j < k ≤ n
εRl := pi2l − 1 + εSi2
2εSi2 − 1 ,
εSl :=
{ min(γTR,max(0, ε R l ))",4.2. Identifiability and approximate solution,[0],[0]
"if l ∈ {i1, i3}
min(1,max(0, εRl ))",4.2. Identifiability and approximate solution,[0],[0]
"if l /∈ {i1, i3} ,
c Sjk := pjk − (1− εSj − εSk + 2εSj εSk )
2 .
(11)
",4.2. Identifiability and approximate solution,[0],[0]
"If all expressions are defined (i.e., B > 0, B + 4C ≥ 0 and εSi2 6= 1 2 ), then (ε S l )l∈[n], (c S jk)1≤j<k≤n is a solution of (4) with pjk as right-hand side.",4.2. Identifiability and approximate solution,[0],[0]
"If i1, i2, i3 ∈ LTR, then all expressions are defined and∣∣εTRl",4.2. Identifiability and approximate solution,[0],[0]
"− εSl ∣∣ ≤ H(γTR, β)√β, l ∈",4.2. Identifiability and approximate solution,[0],[0]
"[n],∣∣c TRjk − c Sjk∣∣ ≤",4.2. Identifiability and approximate solution,[0],[0]
"3H(γTR, β)√β + β/2, j < k, (12) where H(γTR, β)→ H(γTR)",4.2. Identifiability and approximate solution,[0],[0]
> 0,4.2. Identifiability and approximate solution,[0],[0]
"as β → 0.
",4.2. Identifiability and approximate solution,[0],[0]
"In Lemma 3, for constructing the solution (εSl )l∈[n], (c Sjk)1≤j<k≤n as defined in (10) and (11) we need to know γTR < 1/2, which is an upper bound on the error probabilities of at least n2 + 2 workers that follow the one-coin model.",4.2. Identifiability and approximate solution,[0],[0]
"In practice, we might choose γTR depending on the difficulty of the tasks or simply set it conservatively, for example as γTR = 0.45.",4.2. Identifiability and approximate solution,[0],[0]
"If i1, i2, i3 ∈ LTR, then (12) implies that (εSl )l∈[n], (c S jk)1≤j<k≤n satisfies (8) with
γ = γTR +H(γTR, β) √ β, ν = 3H(γTR, β) √ β + β/2.
",4.2. Identifiability and approximate solution,[0],[0]
"(13)
If we know the value of β (using Lemma 1, we easily obtain an upper bound β that holds with high probability), we can compute these quantities.",4.2. Identifiability and approximate solution,[0],[0]
"This suggests the following strategy for obtaining estimates of εw1 , . . .",4.2. Identifiability and approximate solution,[0],[0]
", εwn and Cov[εwj",4.2. Identifiability and approximate solution,[0],[0]
"(x, y), εwk(x, y)], j < k: we sample pairwise different i1, i2, i3 ∈",4.2. Identifiability and approximate solution,[0],[0]
"[n] uniformly at random and construct (εSl )l∈[n], (c S jk)1≤j<k≤n as defined in (10) and (11).",4.2. Identifiability and approximate solution,[0],[0]
"If one of the expressions is not defined, we can immediately discard (i1, i2, i3).",4.2. Identifiability and approximate solution,[0],[0]
"Otherwise, we check whether (εSl )l∈[n], (c Sjk)1≤j<k≤n satisfies (8) with γ and ν as specified in (13).",4.2. Identifiability and approximate solution,[0],[0]
"If it does, since (εTRl )l∈[n], (c TR jk + (pjk− p TRjk )/2)1≤j<k≤n is a solution of (4) with pjk as right-hand side that satisfies
property (8) too, Proposition 2 guarantees that ∣∣εTRl",4.2. Identifiability and approximate solution,[0],[0]
"− εSl ∣∣ ≤√3H(γTR, β)√β",4.2. Identifiability and approximate solution,[0],[0]
"+ β2 · G ( γTR +H(γTR, β)",4.2. Identifiability and approximate solution,[0],[0]
"√ β, 3H(γTR, β)",4.2. Identifiability and approximate solution,[0],[0]
"√ β + β
2
) ∼ β1/4
(14)
for all l ∈",4.2. Identifiability and approximate solution,[0],[0]
"[n] and a similar bound on |c TRjk − c Sjk|, j < k.",4.2. Identifiability and approximate solution,[0],[0]
"If (εSl )l∈[n], (c S jk)1≤j<k≤n does not satisfy (8), we discard (i1, i2, i3) and start anew.",4.2. Identifiability and approximate solution,[0],[0]
"Note that under our Assumption A, the probability of choosing i1, i2, i3 such that i1, i2, i3 ∈ LTR is greater than 1/8.",4.2. Identifiability and approximate solution,[0],[0]
"In expectation we have to discard (i1, i2, i3) for not more than eight times before finding a solution that satisfies (8) and hence (14).
",4.2. Identifiability and approximate solution,[0],[0]
"Assuming that every worker is presented with every task, that is gij",4.2. Identifiability and approximate solution,[0],[0]
= 1,4.2. Identifiability and approximate solution,[0],[0]
for all i ∈,4.2. Identifiability and approximate solution,[0],[0]
[m] and j ∈,4.2. Identifiability and approximate solution,[0],[0]
"[n], it follows from Lemma 1 and (14) that m has to scale as ln(n2/δ)/ρ8 in order that the described strategy is guaranteed to yield, with probability at least 1 − δ, estimates εS1 , . . .",4.2. Identifiability and approximate solution,[0],[0]
", εSn satisfying |εTRl − εSl
∣∣ ≤ ρ,",4.2. Identifiability and approximate solution,[0],[0]
l ∈,4.2. Identifiability and approximate solution,[0],[0]
[n].,4.2. Identifiability and approximate solution,[0],[0]
"This is significantly larger than the rate m ∼ ln(n2/δ)/ρ2 required by the TE algorithm, which solves the estimation problem for the error probabilities in the one-coin model and is claimed to be minimax optimal (Bonald & Combes, 2017).",4.2. Identifiability and approximate solution,[0],[0]
"We suspect that our rate with its dependence on ρ−8 is not optimal and consider it to be an interesting follow-up question to study the minimax rate for our extension of the one-coin model.
",4.2. Identifiability and approximate solution,[0],[0]
"Although the convergence rate that we can guarantee for the described strategy is slow, we might still hope that the strategy performs better in practice.",4.2. Identifiability and approximate solution,[0],[0]
"However, there is an issue that we have to overcome.",4.2. Identifiability and approximate solution,[0],[0]
"Unless β is very small, γ and ν as specified in (13) are too big for being meaningful, that is any solution (εSl )l∈[n], (c S jk)1≤j<k≤n as defined in (10) and (11) will satisfy (8) with these values.",4.2. Identifiability and approximate solution,[0],[0]
"We will not discard any (i1, i2, i3), regardless of whether i1, i2, i3 ∈ LTR holds or not.",4.2. Identifiability and approximate solution,[0],[0]
"We deal with this issue by adapting the strategy as follows: let P ⊆ {(i1, i2, i3) : i1, i2, i3 ∈",4.2. Identifiability and approximate solution,[0],[0]
[n] pairwise different}.,4.2. Identifiability and approximate solution,[0],[0]
"For every p = (i1, i2, i3) ∈ P , we construct (εSl (p))l∈[n], (c Sjk(p))1≤j<k≤n as defined in (10) and (11).",4.2. Identifiability and approximate solution,[0],[0]
We set Qp =,4.2. Identifiability and approximate solution,[0],[0]
"[n] unless γ as specified in (13) is smaller than one, in which case we set Qp = {l ∈",4.2. Identifiability and approximate solution,[0],[0]
"[n] : εSl (p) ≤ γ} and discard any solution (εSl (p))l∈[n], (c S jk(p))1≤j<k≤n for which |Qp| < n2 + 2.",4.2. Identifiability and approximate solution,[0],[0]
Let ν,4.2. Identifiability and approximate solution,[0],[0]
p be the dn2 + 2e-th smallest element of {maxk∈[n]\{l} |c Slk (p)| : l ∈ Qp}.,4.2. Identifiability and approximate solution,[0],[0]
"Then we finally return the solution (εSl (p0))l∈[n], (c S jk(p0)))1≤j<k≤n for which νp is smallest, that is p0 = argminp ν",4.2. Identifiability and approximate solution,[0],[0]
"p.
",4.2. Identifiability and approximate solution,[0],[0]
"If γ is small enough, it follows from Proposition 2 that∣∣εTRl",4.2. Identifiability and approximate solution,[0],[0]
"− εSl (p0)∣∣ ≤√max{νp0 , β/2} · G ( γTR +H(γTR, β) √ β,max{νp0 , β/2} ) .",4.2. Identifiability and approximate solution,[0],[0]
"(15)
Note that if P contains at least one triple of indices i1, i2, i3 ∈ LTR, then νp0 ≤ 3H(γTR, β) √ β + β2 , so that the guarantee (15) is at least as good as (14).",4.2. Identifiability and approximate solution,[0],[0]
We also expect νp0 to be smaller the larger P is.,4.2. Identifiability and approximate solution,[0],[0]
"Hence, we should choose P as large as we can afford due to computational reasons, but in practice, there is one more aspect that we have to consider.",4.2. Identifiability and approximate solution,[0],[0]
"Depending on how gij has been chosen, there might be workers wj and wk that were presented with only a few common tasks or no common tasks at all.",4.2. Identifiability and approximate solution,[0],[0]
"In this case, the estimate pjk of the agreement probability between wj and wk is only poor and there is no uniform bound β on |p TRjk −pjk| (where p TRjk are true agreement probabilities).",4.2. Identifiability and approximate solution,[0],[0]
"We can deal with this aspect by choosing P in a way such that for all p ∈ P , all estimates pjk that are involved in the computation of (εSl (p))l∈[n] are somewhat reliable.",4.2. Identifiability and approximate solution,[0],[0]
We present a concrete implementation of this in Algorithm 1 below.,4.2. Identifiability and approximate solution,[0],[0]
"Once we have estimates ε̂w1 , . . .",4.3. Predicting ground-truth labels,[0],[0]
", ε̂wn of the true error probabilities εw1 , . . .",4.3. Predicting ground-truth labels,[0],[0]
", εwn , we predict ground-truth labels yi by taking a weighted majority vote over the responses collected for the task xi.",4.3. Predicting ground-truth labels,[0],[0]
"Our estimate for yi is given by
ŷi = sign {∑n
l=1 f(ε̂wl) ·Ail
} , (16)
where f :",4.3. Predicting ground-truth labels,[0],[0]
"[0, 1]→",4.3. Predicting ground-truth labels,[0],[0]
"[−∞,+∞].",4.3. Predicting ground-truth labels,[0],[0]
Ties are broken uniformly at random.,4.3. Predicting ground-truth labels,[0],[0]
"We consider two choices for the function f .
",4.3. Predicting ground-truth labels,[0],[0]
"It is well-known that if all workers follow the one-coin model with known error probabilities εw1 , . . .",4.3. Predicting ground-truth labels,[0],[0]
", εwn , groundtruth labels are balanced, that is Pr(x,y)∼D[y = +1] = Pr(x,y)∼D[y = −1], and gij are independent Bernoulli random variables with common success probability α > 0, then the optimal estimator for the ground-truth label yi is given by the weighted majority vote (16) with f(ε̂wl) replaced by f(εwl) =",4.3. Predicting ground-truth labels,[0],[0]
ln,4.3. Predicting ground-truth labels,[0],[0]
"((1− εwl)/εwl) (Nitzan & Paroush, 1982; Berend & Kontorovich, 2015; Bonald & Combes, 2017).",4.3. Predicting ground-truth labels,[0],[0]
"Hence, a common approach for the one-coin model is to first estimate the true error probabilities and then to estimate ground-truth labels by using the majority vote (16) with f(ε̂wl) = ln",4.3. Predicting ground-truth labels,[0],[0]
"((1− ε̂wl)/ε̂wl) (Bonald & Combes, 2017; Ma et al., 2017).",4.3. Predicting ground-truth labels,[0],[0]
"We propose to use the same majority vote, but restricted to answers from workers that we believe to follow the one-coin model.",4.3. Predicting ground-truth labels,[0],[0]
"Using the notation from Section 4.2, this means that we set f(ε̂wl) = ln ((1− ε̂wl)/ε̂wl) for l ∈ Qp0 with maxk∈[n]\{l} |c Slk (p0)| ≤ νp0 and f(ε̂wl) = 0 otherwise.
",4.3. Predicting ground-truth labels,[0],[0]
"Alternatively, we suggest to set f(ε̂wl) = 1 − 2ε̂wl for l ∈",4.3. Predicting ground-truth labels,[0],[0]
[n].,4.3. Predicting ground-truth labels,[0],[0]
With this choice of f we make use of the responses provided by all workers.,4.3. Predicting ground-truth labels,[0],[0]
"The same choice has been used for the one-coin model too (Dalvi et al., 2013).",4.3. Predicting ground-truth labels,[0],[0]
A third option would be to set f(ε̂wl) = 1− 2ε̂wl for l ∈ Qp0 with maxk∈[n]\{l} |c Slk (p0)| ≤ νp0 and f(ε̂wl) = 0,4.3. Predicting ground-truth labels,[0],[0]
"otherwise, but we do not consider this choice any further.",4.3. Predicting ground-truth labels,[0],[0]
"In the interests of clarity, we present our approach as self contained Algorithm 1.",4.4. Algorithm,[0],[0]
Choosing P as the set of triples such that involved pairs of workers have been provided with at least ten or three common tasks might seem somewhat arbitrary here.,4.4. Algorithm,[0],[0]
"Indeed, one could introduce two parameters to the algorithm instead.",4.4. Algorithm,[0],[0]
"Without optimizing for these parameters, we chose them as ten and three in all our experiments on real data, and hence we state Algorithm 1 as is.
",4.4. Algorithm,[0],[0]
"Our analysis best applies to the setting of a full matrix A (or variables gij that are independent Bernoulli random variables with common success probability, as it is assumed by Bonald & Combes, 2017, for example).",4.4. Algorithm,[0],[0]
"In this case, which we consider in our experiments on synthetic data, choosing P as stated in Algorithm 1 reduces to choosing P as the set of all triples of pairwise different indices.",4.4. Algorithm,[0],[0]
"If the number of workers n is small, this is the best one can do.",4.4. Algorithm,[0],[0]
"If n is large, it is infeasible to choose P as the set of all triples though since the running time of Algorithm 1 is in O(n2(m + |P",4.4. Algorithm,[0],[0]
|)).,4.4. Algorithm,[0],[0]
"If n is large and A full, one should sample P uniformly at random.",4.4. Algorithm,[0],[0]
For |P | ≥ ln δ/ ln(7/8),4.4. Algorithm,[0],[0]
our error guarantee (14) holds with probability at least 1− δ then (compare with Section 4.2).,4.4. Algorithm,[0],[0]
We briefly survey related work here.,5. Related work,[0],[0]
A complete discussion can be found in Appendix A.,5. Related work,[0],[0]
"As discussed in Sections 1 and 2, in crowdsourcing one might be interested in estimating ground-truth labels and/or worker qualities given the response matrix A, but also in optimal task assignment.",5. Related work,[0],[0]
"In their seminal paper, Dawid & Skene (1979) proposed an EM based algorithm to address the first two goals.",5. Related work,[0],[0]
"Since then numerous works have followed addressing all three goals for the Dawid-Skene and one-coin model (Ghosh et al., 2011; Karger et al., 2011a;b; 2013; 2014; Dalvi et al., 2013; Gao & Zhou, 2013; Gao et al., 2016; Zhang et al., 2016; Bonald & Combes, 2017; Ma et al., 2017).",5. Related work,[0],[0]
"There have also been efforts to study generalizations of the Dawid-Skene model (Jaffe et al., 2016; Khetan & Oh, 2016; Shah et al., 2016) as well as to explicitly deal with adversaries (Raykar & Yu, 2012; Jagabathula et al., 2017).",5. Related work,[0],[0]
"However, none of the prior work can handle a number of arbitrary adversaries almost as large as the number of reliable workers as we do.",5. Related work,[0],[0]
"On both synthetic and real data, we compared our proposed Algorithm 1 to straightforward majority voting for predicting labels (referred to as Maj) and the following methods from the literature: the spectral algorithms by Ghosh et al. (2011) (GKM), Dalvi et al. (2013) (RoE and EoR) and Karger et al. (2013) (KOS), the two-stage procedure by
Algorithm 1",6. Experiments,[0],[0]
"Input: crowdsourced labels stored in A ∈ {−1, 0,+1}m×n, upper bound 0 < γTR < 12 on the error probabilities of dn2 + 2e workers that follow the one-coin model, confidence parameter 0",6. Experiments,[0],[0]
< δ < 1,6. Experiments,[0],[0]
"Output: estimates (εFl )l∈[n], (c Fjk )j<k, (ŷi)i∈[m] of error probabilities, covariances and ground-truth labels
I Estimating agreement probabilities set gij = 1{Aij 6= 0}, i ∈",6. Experiments,[0],[0]
"[m], j ∈",6. Experiments,[0],[0]
[n] set qjk = ∑m i=1,6. Experiments,[0],[0]
"gijgik, j, k ∈",6. Experiments,[0],[0]
"[n] set pjk as in (3), j, k ∈",6. Experiments,[0],[0]
"[n] (pjk = NaN if qjk = 0)
",6. Experiments,[0],[0]
I Estimating error probabilities and covariances set β =,6. Experiments,[0],[0]
"[ ln(2n2/δ)/ ( 2 minj,k∈[n] qjk )]1/2 ∈ (0,+Inf ] set γ as in (13)",6. Experiments,[0],[0]
if γ /∈,6. Experiments,[0],[0]
"[0, 1] then
set γ = 1 end if set P = { (i1, i2, i3) :",6. Experiments,[0],[0]
"i1, i2, i3 ∈",6. Experiments,[0],[0]
"[n] pairwise different
and qjk ≥ 10, j, k ∈ {i1, i2, i3}, and qi2j ≥ 3, j 6= i2 }
set νold = Inf, (εFl )l∈[n] = 0, (c F jk )1≤j<k≤n = 0, L = ∅ for (i1, i2,",6. Experiments,[0],[0]
"i3) ∈ P do if not all expressions in (10) or (11) are defined then
break end if compute (εSl )l∈[n], (c S jk)1≤j<k≤n as in (10) and (11) set Q = {l ∈",6. Experiments,[0],[0]
[n] : εSl ≤ γ} set ν = dn2 +,6. Experiments,[0],[0]
2e-th smallest element of {maxk∈[n]\{l} |c Slk,6. Experiments,[0],[0]
| : l ∈ Q} (ν = NaN ifQ = ∅) if |Q| ≥ n2 + 2 AND ν <,6. Experiments,[0],[0]
νold then set (εFl )l∈[n] =,6. Experiments,[0],[0]
"(ε S l )l∈[n], (c F jk )",6. Experiments,[0],[0]
j,6. Experiments,[0],[0]
"<k = (c S jk)j<k
set L = {l ∈ Q :",6. Experiments,[0],[0]
"maxk∈[n]\{l} |c Slk | ≤ ν} set νold = ν
end if end for
I Estimating ground-truth labels set f(ε̂wl) =",6. Experiments,[0],[0]
ln ((1− ε̂wl)/ε̂wl) ∈,6. Experiments,[0],[0]
"[−Inf,+Inf], l ∈ L,
and f(ε̂wl) = 0, l ∈",6. Experiments,[0],[0]
"[n] \ L (alternatively set f(ε̂wl) = 1− 2ε̂wl , l ∈",6. Experiments,[0],[0]
"[n]) set ŷi as in (16), i ∈",6. Experiments,[0],[0]
"[m]
Zhang et al. (2016) (S-EM1 and S-EM10, where we run one or ten iterations of the EM algorithm) and the recent method by Bonald & Combes (2017) (TE).",6. Experiments,[0],[0]
"We used the Matlab implementation of KOS, S-EM1 and S-EM10 made available by Zhang et al. (2016).",6. Experiments,[0],[0]
"In our implementations of the other methods, we adapted GKM, RoE and EoR as to assume that the average error of the workers is smaller than one half rather than assuming that the error of the first worker is.",6. Experiments,[0],[0]
"We always called Algorithm 1 with parameters γTR = 0.4 and δ = 0.1, which resulted in γ being set to 1
in the execution of the algorithm in all our experiments.",6. Experiments,[0],[0]
We refer to Algorithm 1 with the logarithmic weights in (16) as Alg. 1 and and with the linear weights as Alt-Alg. 1.,6. Experiments,[0],[0]
"In the following, all results are average results obtained from running an experiment for 100 times.",6. Experiments,[0],[0]
"In our first experiment, we consider n = 50 workers and m = 5000 tasks with balanced ground-truth labels.",6.1. Synthetic data,[0],[0]
Every worker is presented with every task.,6.1. Synthetic data,[0],[0]
"For 0 ≤ t ≤ 25, we choose t workers at random.",6.1. Synthetic data,[0],[0]
"These workers are corrupted workers that all provide the same random response to every task, which is incorrect with error probability 0.5.",6.1. Synthetic data,[0],[0]
"The remaining n − t workers provide responses according to the one-coin model, where the error probability of each of these workers is 0.4.",6.1. Synthetic data,[0],[0]
Figure 1 shows the prediction error for estimating ground-truth labels and the estimation error for estimating error probabilities in both the maximum norm and the normalized 1-norm for the various methods as a function of t.,6.1. Synthetic data,[0],[0]
The prediction error is given by 1m ∑m i=1,6.1. Synthetic data,[0],[0]
1{yi 6= ŷi} for ground-truth labels yi and estimates ŷi and the estimation error is given by maxl∈[n] |εwl,6.1. Synthetic data,[0],[0]
− ε̂wl | or 1n,6.1. Synthetic data,[0],[0]
∑n l=1 |εwl − ε̂wl | for true error probabilities εwl and estimates ε̂wl .,6.1. Synthetic data,[0],[0]
"The methods Maj and KOS, by default, do not provide estimates of the workers’ error probabilities.",6.1. Synthetic data,[0],[0]
"We adapt these two methods in order to return estimates of the error probabilities too as follows: if the method returns label estimates ŷ1, . . .",6.1. Synthetic data,[0],[0]
", ŷm and worker wl provides responses A1l, . . .",6.1. Synthetic data,[0],[0]
", Aml 6= 0, then the method
returns 1m ∑m i=1",6.1. Synthetic data,[0],[0]
1{ŷi 6=,6.1. Synthetic data,[0],[0]
"Ail} as estimate ε̂wl of εwl .
",6.1. Synthetic data,[0],[0]
Our Algorithm 1 is the only method that can handle up to 23 = n2,6.1. Synthetic data,[0],[0]
− 2 corrupted workers (in accordance with our theoretical results).,6.1. Synthetic data,[0],[0]
Its estimation error is constant as the number of corrupted workers increases from 0 to 23.,6.1. Synthetic data,[0],[0]
"Its prediction error depends on which weights we use in (16): the prediction error of Alg. 1 is constant in this range too, the one of Alt-Alg. 1 is slightly increasing.",6.1. Synthetic data,[0],[0]
"If only a few workers are corrupted, Alt-Alg.1 performs better than Alg. 1, while it is the other way round if more than 13 workers are corrupted.",6.1. Synthetic data,[0],[0]
The methods from the literature predict ground-truth labels as badly as random guessing already in the presence of only six corrupted workers.,6.1. Synthetic data,[0],[0]
All these methods are outperformed by majority voting.,6.1. Synthetic data,[0],[0]
We do not have an explanation for the non-monotonic behavior of the estimation error of SEM10 in the maximum norm.,6.1. Synthetic data,[0],[0]
"In Appendix C we present similar experiments, in which the error probability of the workers following the one-coin model is smaller or the error probabilities of the corrupted workers are less correlated.",6.1. Synthetic data,[0],[0]
"Still, the overall picture there is the same.
",6.1. Synthetic data,[0],[0]
One might wonder whether one can combine the considered methods from the literature with one of the algorithms by Jagabathula et al. (2017) in order to first sort the corrupted workers out and then apply the method only to the remaining workers and their responses.,6.1. Synthetic data,[0],[0]
"However, those algorithms cannot deal with the corrupted workers considered in this experiment, which are perfectly colluding, at all.",6.1. Synthetic data,[0],[0]
"Even though provided with the correct number t of corrupted workers as input, when t ≥ 3, the soft-penalty algorithm by Jagabathula et al. (2017) was not able to identify any of the corrupted workers in any of the 100 runs of the experiment.
",6.1. Synthetic data,[0],[0]
"In our next experiment, we study the convergence rate of Algorithm 1.",6.1. Synthetic data,[0],[0]
"We consider n = 50 workers, out of which t = 23 are corrupted in the same way as above.",6.1. Synthetic data,[0],[0]
Figure 2 shows the prediction and estimation error of Algorithm 1 as a function of the number of tasks m varying from 5000 to 20000.,6.1. Synthetic data,[0],[0]
"The prediction error of Alg. 1 decreases only slightly as m increases, the prediction error of Alt-Alg. 1 decreases more significantly.",6.1. Synthetic data,[0],[0]
Most interesting is the decay of the estimation error.,6.1. Synthetic data,[0],[0]
"Apparently, in this experiment it
decreases at a rate ofm−1/2 rather than at a rate ofm−1/8 as suggested by our upper bound (compare with Section 4.2).",6.1. Synthetic data,[0],[0]
We performed experiments on six publicly available data sets that are are commonly used in the literature (cf.,6.2. Real data,[0],[0]
"Snow et al., 2008, Zhang et al., 2016, and Bonald & Combes, 2017).",6.2. Real data,[0],[0]
All six data sets come with ground truth labels for each task.,6.2. Real data,[0],[0]
"For most of the data sets the matrix A, which stores the collected responses, is highly sparse.",6.2. Real data,[0],[0]
"In order to reduce sparseness, we removed workers that provided fewer than 50 labels.",6.2. Real data,[0],[0]
"For two of the data sets, we merged classes in order to end up with binary classification problems in the same way as Bonald & Combes (2017) did (Dog: {0, 2} vs {1, 3}; Web: {0, 1, 2} vs {3, 4}).",6.2. Real data,[0],[0]
Table 2 in Appendix C provides the characteristic values of the data sets.,6.2. Real data,[0],[0]
Note that only for the Bird data set every worker provided a label for every task whereas for the other ones A is still rather sparse.,6.2. Real data,[0],[0]
Figure 5 in Appendix C shows for each data set a histogram of the error probabilities of the workers (computed over those tasks that a worker was presented with).,6.2. Real data,[0],[0]
"Figure 6 shows a heat map of the matrix (|Cov[εwj (x, y), εwk(x, y)]|)nj,k=1 (computed over those tasks that two workers were jointly presented with).
",6.2. Real data,[0],[0]
Table 1 shows the prediction error for the various methods and data sets.,6.2. Real data,[0],[0]
There is no method that performs best on all data sets.,6.2. Real data,[0],[0]
"Overall, S-EM10 seems to be the method of choice.",6.2. Real data,[0],[0]
"Our Algorithm 1 can compete with the other methods, and on four out of the six data sets, the prediction error of Alt-Alg. 1 is smaller or larger only by 0.01 than the prediction error of S-EM10.",6.2. Real data,[0],[0]
Alg. 1 performs slightly worse than Alt-Alg. 1.,6.2. Real data,[0],[0]
"The poor performance of our method on
the Bird data set might be explained by the fact that there the workers clearly deviate from our model: as Figure 6 shows, there are no n2 + 2 workers that follow the one-coin model.
",6.2. Real data,[0],[0]
We performed another experiments on these data sets by corrupting some of the workers (chosen at random).,6.2. Real data,[0],[0]
"Like in the experiments of Section 6.1, the corrupted workers provide the same random response to every task.",6.2. Real data,[0],[0]
Figure 3 shows the prediction errors for the various methods and the first three data sets as functions of the number of corrupted workers.,6.2. Real data,[0],[0]
"Similar plots for the other data sets are shown in Figure 7 in Appendix C. On none of the data sets, any method can handle more corrupted workers than Alt-Alg. 1.",6.2. Real data,[0],[0]
"In this work, we studied an extension of the well-known one-coin model for crowdsourcing that allows for colluding adversaries.",7. Discussion,[0],[0]
"Our results show that even if almost half of the workers are adversarial, one can consistently estimate the workers’ error probabilities with an efficient algorithm.
",7. Discussion,[0],[0]
"For future work, it would be interesting to relax the assumption that the reliable workers follow the one-coin model and to allow for task-dependent error probabilities also for them.",7. Discussion,[0],[0]
It would also be interesting to see whether our approach can be extended to multiclass classification problems.,7. Discussion,[0],[0]
"Another direction concerns improving the sufficient rate m ∼ ρ−8 , which we obtained for our algorithm for recovering worker qualities up to error ρ.",7. Discussion,[0],[0]
"In the absence of adversaries one can achieve a rate m ∼ ρ−2, and we would like to understand whether this gap is inherent or an artifact of our algorithm/proof.",7. Discussion,[0],[0]
"Finally, we wonder about the role of adaptive task assignment in our extension of the one-coin model.",7. Discussion,[0],[0]
This research is supported by a Rutgers Research Council Grant and a Center for Discrete Mathematics and Theoretical Computer Science (DIMACS) postdoctoral fellowship.,Acknowledgements,[0],[0]
"Most existing works on crowdsourcing assume that the workers follow the Dawid-Skene model, or the one-coin model as its special case, where every worker makes mistakes independently of other workers and with the same error probability for every task.",abstractText,[0],[0]
We study a significant extension of this restricted model.,abstractText,[0],[0]
"We allow almost half of the workers to deviate from the one-coin model and for those workers, their probabilities of making an error to be task-dependent and to be arbitrarily correlated.",abstractText,[0],[0]
"In other words, we allow for arbitrary adversaries, for which not only error probabilities can be high, but which can also perfectly collude.",abstractText,[0],[0]
"In this adversarial scenario, we design an efficient algorithm to consistently estimate the workers’ error probabilities.",abstractText,[0],[0]
Crowdsourcing with Arbitrary Adversaries,title,[0],[0]
Reinforcement learning algorithms aim at learning policies for achieving target tasks by maximizing rewards provided by the environment.,1. Introduction,[0],[0]
"In some scenarios, these rewards are supplied to the agent continuously, e.g. the running score in an Atari game (Mnih et al., 2015), or the distance between a robot arm and an object in a reaching task (Lillicrap et al., 2016).",1. Introduction,[0],[0]
"However, in many real-world scenarios, rewards extrinsic to the agent are extremely sparse or miss-
1University of California, Berkeley.",1. Introduction,[0],[0]
"Correspondence to: Deepak Pathak <pathak@berkeley.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, 2017.",1. Introduction,[0],[0]
JMLR: W&CP.,1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"(a) learn to explore in Level-1 (b) explore faster in Level-2
Figure 1.",1. Introduction,[0],[0]
Discovering how to play Super Mario Bros without rewards.,1. Introduction,[0],[0]
"(a) Using only curiosity-driven exploration, the agent makes significant progress in Level-1.",1. Introduction,[0],[0]
(b) The gained knowledge helps the agent explore subsequent levels much faster than when starting from scratch.,1. Introduction,[0],[0]
"Watch the video at http://pathak22. github.io/noreward-rl/
ing altogether, and it is not possible to construct a shaped reward function.",1. Introduction,[0],[0]
This is a problem as the agent receives reinforcement for updating its policy only if it succeeds in reaching a pre-specified goal state.,1. Introduction,[0],[0]
"Hoping to stumble into a goal state by chance (i.e. random exploration) is likely to be futile for all but the simplest of environments.
",1. Introduction,[0],[0]
"As human agents, we are accustomed to operating with rewards that are so sparse that we only experience them once or twice in a lifetime, if at all.",1. Introduction,[0],[0]
"To a three-year-old enjoying a sunny Sunday afternoon on a playground, most trappings of modern life – college, good job, a house, a family – are so far into the future, they provide no useful reinforcement signal.",1. Introduction,[0],[0]
"Yet, the three-year-old has no trouble entertaining herself in that playground using what psychologists call intrinsic motivation (Ryan, 2000) or curiosity (Silvia, 2012).",1. Introduction,[0],[0]
Motivation/curiosity have been used to explain the need to explore the environment and discover novel states.,1. Introduction,[0],[0]
"The French word flâneur perfectly captures the notion of a curiosity-driven observer, the “deliberately aimless pedestrian, unencumbered by any obligation or sense of urgency” (Cornelia Otis Skinner).",1. Introduction,[0],[0]
"More generally, curiosity is a way of learning new skills which might come handy for pursuing rewards in the future.
",1. Introduction,[0],[0]
"Similarly, in reinforcement learning, intrinsic motivation/rewards become critical whenever extrinsic rewards are sparse.",1. Introduction,[0],[0]
"Most formulations of intrinsic reward can be grouped into two broad classes: 1) encourage the agent to explore “novel” states (Bellemare et al., 2016; Lopes
ar X
iv :1
70 5.
05 36
3v 1
[ cs
.L",1. Introduction,[0],[0]
"G
] 1
5 M
ay 2
01 7
et al., 2012; Poupart et al., 2006) or, 2) encourage the agent to perform actions that reduce the error/uncertainty in the agent’s ability to predict the consequence of its own actions (i.e. its knowledge about the environment) (Houthooft et al., 2016; Mohamed & Rezende, 2015; Schmidhuber, 1991; 2010; Singh et al., 2005; Stadie et al., 2015).
",1. Introduction,[0],[0]
"Measuring “novelty” requires a statistical model of the distribution of the environmental states, whereas measuring prediction error/uncertainty requires building a model of environmental dynamics that predicts the next state (st+1) given the current state (st) and the action (at) executed at time t. Both these models are hard to build in highdimensional continuous state spaces such as images.",1. Introduction,[0],[0]
"An additional challenge lies in dealing with the stochasticity of the agent-environment system, both due to the noise in the agent’s actuation, which causes its end-effectors to move in a stochastic manner, and, more fundamentally, due to the inherent stochasticity in the environment.",1. Introduction,[0],[0]
"To give the example from (Schmidhuber, 2010), if the agent receiving images as state inputs is observing a television screen displaying white noise, every state will be novel and it would be impossible to predict the value of any pixel in the future.",1. Introduction,[0],[0]
"Other examples of such stochasticity include appearance changes due to shadows from other moving entities, presence of distractor objects, or other agents in the environment whose motion is not only hard to predict but is also irrelevant to the agent’s goals.",1. Introduction,[0],[0]
"Somewhat different, but related, is the challenge of generalization across physically (and perhaps also visually) distinct but functionally similar parts of an environment, which is crucial for largescale problems.",1. Introduction,[0],[0]
"One proposed solution to all these problems is to only reward the agent when it encounters states that are hard to predict but are “learnable” (Schmidhuber, 1991).",1. Introduction,[0],[0]
"However, estimating learnability is a non-trivial problem (Lopes et al., 2012).
",1. Introduction,[0],[0]
"This work belongs to the broad category of methods that generate an intrinsic reward signal based on how hard it is for the agent to predict the consequences of its own actions, i.e. predict the next state given the current state and the executed action.",1. Introduction,[0],[0]
"However, we manage to escape most pitfalls of previous prediction approaches with the following key insight: we only predict those changes in the environment that could possibly be due to the actions of our agent or affect the agent, and ignore the rest.",1. Introduction,[0],[0]
"That is, instead of making predictions in the raw sensory space (e.g. pixels), we transform the sensory input into a feature space where only the information relevant to the action performed by the agent is represented.",1. Introduction,[0],[0]
We learn this feature space using self-supervision – training a neural network on a proxy inverse dynamics task of predicting the agent’s action given its current and next states.,1. Introduction,[0],[0]
"Since the neural network is only required to predict the action, it has no incentive to represent within its feature embedding space the factors of vari-
ation in the environment that do not affect the agent itself.",1. Introduction,[0],[0]
"We then use this feature space to train a forward dynamics model that predicts the feature representation of the next state, given the feature representation of the current state and the action.",1. Introduction,[0],[0]
"We provide the prediction error of the forward dynamics model to the agent as an intrinsic reward to encourage its curiosity.
",1. Introduction,[0],[0]
The role of curiosity has been widely studied in the context of solving tasks with sparse rewards.,1. Introduction,[0],[0]
"In our opinion, curiosity has two other fundamental uses.",1. Introduction,[0],[0]
Curiosity helps an agent explore its environment in the quest for new knowledge (a desirable characteristic of exploratory behavior is that it should improve as the agent gains more knowledge).,1. Introduction,[0],[0]
"Further, curiosity is a mechanism for an agent to learn skills that might be helpful in future scenarios.",1. Introduction,[0],[0]
"In this paper, we evaluate the effectiveness of our curiosity formulation in all three of these roles.
",1. Introduction,[0],[0]
"We first compare the performance of an A3C agent (Mnih et al., 2016) with and without the curiosity signal on 3-D navigation tasks with sparse extrinsic reward in the VizDoom environment.",1. Introduction,[0],[0]
We show that a curiosity-driven intrinsic reward is crucial in accomplishing these tasks (see Section 4.1).,1. Introduction,[0],[0]
"Next, we show that even in the absence of any extrinsic rewards, a curious agent learns good exploration policies.",1. Introduction,[0],[0]
"For instance, an agent trained only with curiosity as its reward is able to cross a significant portion of Level-1 in Super Mario Bros. Similarly in VizDoom, the agent learns to walk intelligently along the corridors instead of bumping into walls or getting stuck in corners (see Section 4.2).",1. Introduction,[0],[0]
"A question that naturally follows is whether the learned exploratory behavior is specific to the physical space that the agent trained itself on, or if it enables the agent to perform better in unseen scenarios too?",1. Introduction,[0],[0]
"We show that the exploration policy learned in the first level of Mario helps the agent explore subsequent levels faster (shown in Figure 1), while the intelligent walking behavior learned by the curious VizDoom agent transfers to a completely new map with new textures (see Section 4.3).",1. Introduction,[0],[0]
These results suggest that the proposed method enables an agent to learn generalizable skills even in the absence of an explicit goal.,1. Introduction,[0],[0]
Our agent is composed of two subsystems: a reward generator that outputs a curiosity-driven intrinsic reward signal and a policy that outputs a sequence of actions to maximize that reward signal.,2. Curiosity-Driven Exploration,[0],[0]
"In addition to intrinsic rewards, the agent optionally may also receive some extrinsic reward from the environment.",2. Curiosity-Driven Exploration,[0],[0]
Let the intrinsic curiosity reward generated by the agent at time t be rit and the extrinsic reward be ret .,2. Curiosity-Driven Exploration,[0],[0]
"The policy sub-system is trained to maximize the sum of these two rewards rt = rit + r e t , with r e t mostly (if not always) zero.
",2. Curiosity-Driven Exploration,[0],[0]
We represent the policy π(st; θP ) by a deep neural network with parameters θP .,2. Curiosity-Driven Exploration,[0],[0]
"Given the agent in state st, it executes the action at ∼ π(st; θP ) sampled from the policy.",2. Curiosity-Driven Exploration,[0],[0]
"θP is optimized to maximize the expected sum of rewards,
max θP Eπ(st;θP )",2. Curiosity-Driven Exploration,[0],[0]
"[Σtrt] (1)
Unless specified otherwise, we use the notation π(s) to denote the parameterized policy π(s; θP ).",2. Curiosity-Driven Exploration,[0],[0]
"Our curiosity reward model can potentially be used with a range of policy learning methods; in the experiments discussed here, we use the asynchronous advantage actor critic policy gradient (A3C) (Mnih et al., 2016) for policy learning.",2. Curiosity-Driven Exploration,[0],[0]
"Our main contribution is in designing an intrinsic reward signal based on prediction error of the agent’s knowledge about its environment that scales to high-dimensional continuous state spaces like images, bypasses the hard problem of predicting pixels and is unaffected by the unpredictable aspects of the environment that do not affect the agent.",2. Curiosity-Driven Exploration,[0],[0]
"Making predictions in the raw sensory space (e.g. when st corresponds to images) is undesirable not only because it is hard to predict pixels directly, but also because it is unclear if predicting pixels is even the right objective to optimize.",2.1. Prediction error as curiosity reward,[0],[0]
"To see why, consider using prediction error in the pixel space as the curiosity reward.",2.1. Prediction error as curiosity reward,[0],[0]
Imagine a scenario where the agent is observing the movement of tree leaves in a breeze.,2.1. Prediction error as curiosity reward,[0],[0]
"Since it is inherently hard to model breeze, it is even harder to predict the pixel location of each leaf.
",2.1. Prediction error as curiosity reward,[0],[0]
This implies that the pixel prediction error will remain high and the agent will always remain curious about the leaves.,2.1. Prediction error as curiosity reward,[0],[0]
But the motion of the leaves is inconsequential to the agent and therefore its continued curiosity about them is undesirable.,2.1. Prediction error as curiosity reward,[0],[0]
The underlying problem is that the agent is unaware that some parts of the state space simply cannot be modeled and thus the agent can fall into an artificial curiosity trap and stall its exploration.,2.1. Prediction error as curiosity reward,[0],[0]
Novelty-seeking exploration schemes that record the counts of visited states in a tabular form (or their extensions to continuous state spaces) also suffer from this issue.,2.1. Prediction error as curiosity reward,[0],[0]
"Measuring learning progress instead of prediction error has been proposed in the past as one solution (Schmidhuber, 1991).",2.1. Prediction error as curiosity reward,[0],[0]
"Unfortunately, there are currently no known computationally feasible mechanisms for measuring learning progress.
",2.1. Prediction error as curiosity reward,[0],[0]
"If not the raw observation space, then what is the right feature space for making predictions so that the prediction error provides a good measure of curiosity?",2.1. Prediction error as curiosity reward,[0],[0]
"To answer this question, let us divide all sources that can modify the agent’s observations into three cases: (1) things that can be controlled by the agent; (2) things that the agent cannot control but that can affect the agent (e.g. a vehicle driven by another agent), and (3) things out of the agent’s control and not affecting the agent (e.g. moving leaves).",2.1. Prediction error as curiosity reward,[0],[0]
A good feature space for curiosity should model (1) and (2) and be unaffected by (3).,2.1. Prediction error as curiosity reward,[0],[0]
"This latter is because, if there is a source of variation that is inconsequential for the agent, then the agent has no incentive to know about it.",2.1. Prediction error as curiosity reward,[0],[0]
"Instead of hand-designing a feature representation for every environment, our aim is to come up with a general mechanism for learning feature representations such that the prediction error in the learned feature space provides a good intrinsic reward signal.",2.2. Self-supervised prediction for exploration,[0],[0]
"We propose that such a feature space can be learned by training a deep neural network with two sub-modules: the first sub-module encodes the raw state (st) into a feature vector φ(st) and the second submodule takes as inputs the feature encoding φ(st), φ(st+1) of two consequent states and predicts the action (at) taken by the agent to move from state st to st+1.",2.2. Self-supervised prediction for exploration,[0],[0]
"Training this neural network amounts to learning function g defined as:
ât = g ( st, st+1; θI )",2.2. Self-supervised prediction for exploration,[0],[0]
"(2)
where, ât is the predicted estimate of the action at and the the neural network parameters θI are trained to optimize,
min θI",2.2. Self-supervised prediction for exploration,[0],[0]
"LI(ât, at) (3)
where, LI is the loss function that measures the discrepancy between the predicted and actual actions.",2.2. Self-supervised prediction for exploration,[0],[0]
"In case at is discrete, the output of g is a soft-max distribution across all possible actions and minimizing LI amounts to maximum likelihood estimation of θI under a multinomial distribution.",2.2. Self-supervised prediction for exploration,[0],[0]
"The learned function g is also known as the inverse dynamics model and the tuple (st, at, st+1) required to learn g is obtained while the agent interacts with the environment using its current policy π(s).
",2.2. Self-supervised prediction for exploration,[0],[0]
"In addition to inverse dynamics model, we train another neural network that takes as inputs at and φ(st) and predicts the feature encoding of the state at time step t+ 1,
φ̂(st+1)",2.2. Self-supervised prediction for exploration,[0],[0]
"= f ( φ(st), at; θF ) (4)
where φ̂(st+1) is the predicted estimate of φ(st+1) and the neural network parameters θF are optimized by minimizing the loss function LF :
LF ( φ(st), φ̂(st+1) )",2.2. Self-supervised prediction for exploration,[0],[0]
"= 1
2 ‖φ̂(st+1)− φ(st+1)‖22 (5)
The learned function f is also known as the forward dynamics model.",2.2. Self-supervised prediction for exploration,[0],[0]
"The intrinsic reward signal rit is computed as,
rit = η
2 ‖φ̂(st+1)− φ(st+1)‖22 (6)
where η > 0 is a scaling factor.",2.2. Self-supervised prediction for exploration,[0],[0]
"In order to generate the curiosity based intrinsic reward signal, we jointly optimize the forward and inverse dynamics loss described in equations 3 and 5 respectively.",2.2. Self-supervised prediction for exploration,[0],[0]
The inverse model learns a feature space that encodes information relevant for predicting the agent’s actions only and the forward model makes predictions in this feature space.,2.2. Self-supervised prediction for exploration,[0],[0]
"We refer to this proposed
curiosity formulation as Intrinsic Curiosity Module (ICM).",2.2. Self-supervised prediction for exploration,[0],[0]
"As there is no incentive for this feature space to encode any environmental features that are not influenced by the agent’s actions, our agent will receive no rewards for reaching environmental states that are inherently unpredictable and its exploration strategy will be robust to the presence of distractor objects, changes in illumination, or other nuisance sources of variation in the environment.",2.2. Self-supervised prediction for exploration,[0],[0]
"See Figure 2 for illustration of the formulation.
",2.2. Self-supervised prediction for exploration,[0],[0]
"The use of inverse models has been investigated to learn features for recognition tasks (Agrawal et al., 2015; Jayaraman & Grauman, 2015).",2.2. Self-supervised prediction for exploration,[0],[0]
Agrawal et al. (2016) constructed a joint inverse-forward model to learn feature representation for the task of pushing objects.,2.2. Self-supervised prediction for exploration,[0],[0]
"However, they only used the forward model as a regularizer for training the inverse model features, while we make use of the error in the forward model predictions as the curiosity reward for training our agent’s policy.
",2.2. Self-supervised prediction for exploration,[0],[0]
"The overall optimization problem that is solved for learning the agent is a composition of equations 1, 3 and 5 and can be written as,
min θP ,θI ,θF
[ − λEπ(st;θP )",2.2. Self-supervised prediction for exploration,[0],[0]
"[Σtrt] + (1− β)LI + βLF ] (7)
where 0 ≤ β ≤ 1 is a scalar that weighs the inverse model loss against the forward model loss and λ > 0 is a scalar that weighs the importance of the policy gradient loss against the importance of learning the intrinsic reward signal.",2.2. Self-supervised prediction for exploration,[0],[0]
"To evaluate our curiosity module on its ability to improve exploration and provide generalization to novel scenarios, we will use two simulated environments.",3. Experimental Setup,[0],[0]
"This section describes the details of the environments and the experimental setup.
",3. Experimental Setup,[0],[0]
"Environments The first environment we evaluate on is the VizDoom (Kempka et al., 2016) game.",3. Experimental Setup,[0],[0]
"We consider the Doom 3-D navigation task where the action space of the agent consists of four discrete actions – move forward, move left, move right and no-action.",3. Experimental Setup,[0],[0]
"Our testing setup in all the experiments is the ‘DoomMyWayHome-v0’ environment which is available as part of OpenAI Gym (Brockman et al., 2016).",3. Experimental Setup,[0],[0]
Episodes are terminated either when the agent finds the vest or if the agent exceeds a maximum of 2100 time steps.,3. Experimental Setup,[0],[0]
The map consists of 9 rooms connected by corridors and the agent is tasked to reach some fixed goal location from its spawning location.,3. Experimental Setup,[0],[0]
The agent is only provided a sparse terminal reward of +1 if it finds the vest and zero otherwise.,3. Experimental Setup,[0],[0]
"For generalization experiments, we pre-train on
a different map with different random textures from (Dosovitskiy & Koltun, 2016) and each episode lasts for 2100 time steps.",3. Experimental Setup,[0],[0]
"Sample frames from VizDoom are shown in Figure 3a, and maps are explained in Figure 4.",3. Experimental Setup,[0],[0]
"It takes approximately 350 steps for an optimal policy to reach the vest location from the farthest room in this map (sparse reward).
",3. Experimental Setup,[0],[0]
"Our second environment is the classic Nintendo game Super Mario Bros (Paquette, 2016).",3. Experimental Setup,[0],[0]
We consider four levels of the game: pre-training on the first level and showing generalization on the subsequent levels.,3. Experimental Setup,[0],[0]
"In this setup, we reparametrize the action space of the agent into 14 unique actions following (Paquette, 2016).",3. Experimental Setup,[0],[0]
"This game is played using a joystick allowing for multiple simultaneous button presses, where the duration of the press affects what action is being taken.",3. Experimental Setup,[0],[0]
"This property makes the game particularly hard, e.g. to make a long jump over tall pipes or wide gaps, the agent needs to predict the same action up to 12 times in a row, introducing long-range dependencies.",3. Experimental Setup,[0],[0]
"All our experiments on Mario are trained using curiosity signal only, without any reward from the game.
",3. Experimental Setup,[0],[0]
Training details,3. Experimental Setup,[0],[0]
"All agents in this work are trained using visual inputs that are pre-processed in manner similar to (Mnih et al., 2016).",3. Experimental Setup,[0],[0]
The input RGB images are converted into gray-scale and re-sized to 42 × 42.,3. Experimental Setup,[0],[0]
"In order to model temporal dependencies, the state representation (st) of the environment is constructed by concatenating the current frame with the three previous frames.",3. Experimental Setup,[0],[0]
"Closely following (Mnih et al., 2015; 2016), we use action repeat of four during training time in VizDoom and action repeat of six in Mario.",3. Experimental Setup,[0],[0]
"However, we sample the policy without any action repeat during inference.",3. Experimental Setup,[0],[0]
"Following the asynchronous training protocol in A3C, all the agents were trained asynchronously with twenty workers using stochastic gradient descent.",3. Experimental Setup,[0],[0]
"We used ADAM optimizer with its parameters not shared across the workers.
A3C architecture The input state st is passed through a sequence of four convolution layers with 32 filters each,
kernel size of 3x3, stride of 2 and padding of 1.",3. Experimental Setup,[0],[0]
"An exponential linear unit (ELU; (Clevert et al., 2015)) is used after each convolution layer.",3. Experimental Setup,[0],[0]
The output of the last convolution layer is fed into a LSTM with 256 units.,3. Experimental Setup,[0],[0]
"Two seperate fully connected layers are used to predict the value function and the action from the LSTM feature representation.
",3. Experimental Setup,[0],[0]
Intrinsic Curiosity Module (ICM) architecture The intrinsic curiosity module consists of the forward and the inverse model.,3. Experimental Setup,[0],[0]
"The inverse model first maps the input state (st) into a feature vector φ(st) using a series of four convolution layers, each with 32 filters, kernel size 3x3, stride of 2 and padding of 1.",3. Experimental Setup,[0],[0]
ELU non-linearity is used after each convolution layer.,3. Experimental Setup,[0],[0]
The dimensionality of φ(st) (i.e. the output of the fourth convolution layer) is 288.,3. Experimental Setup,[0],[0]
"For the inverse model, φ(st) and φ(st+1) are concatenated into a single feature vector and passed as inputs into a fully connected layer of 256 units followed by an output fully connected layer with 4 units to predict one of the four possible actions.",3. Experimental Setup,[0],[0]
The forward model is constructed by concatenating φ(st) with at and passing it into a sequence of two fully connected layers with 256 and 288 units respectively.,3. Experimental Setup,[0],[0]
"The value of β is 0.2, and λ is 0.1.",3. Experimental Setup,[0],[0]
"The Equation (7) is minimized with learning rate of 1e− 3.
",3. Experimental Setup,[0],[0]
"Baseline Methods ‘ICM + A3C’ denotes our full algorithm which combines intrinsic curiosity model with A3C. Across different experiments, we compare our approach with three baselines.",3. Experimental Setup,[0],[0]
First is the vanilla ‘A3C’ algorithm with -greedy exploration.,3. Experimental Setup,[0],[0]
"Second is ‘ICM-pixels + A3C’, which is a variant of our ICM without the inverse model, and has curiosity reward dependent only on the forward model loss in predicting next observation in pixel space.",3. Experimental Setup,[0],[0]
"To design this, we remove the inverse model layers and append
deconvolution layers to the forward model.",3. Experimental Setup,[0],[0]
ICM-pixels is close to ICM in architecture but incapable of learning embedding that is invariant to the uncontrollable part of environment.,3. Experimental Setup,[0],[0]
"Note that ICM-pixels is representative of previous methods which compute information gain by directly using the observation space (Schmidhuber, 2010; Stadie et al., 2015).",3. Experimental Setup,[0],[0]
We show that directly using observation space for computing curiosity is significantly worse than learning an embedding as in ICM.,3. Experimental Setup,[0],[0]
"Finally, we include comparison with state-of-the-art exploration methods based on variational information maximization (VIME) (Houthooft et al., 2016) which is trained with TRPO.",3. Experimental Setup,[0],[0]
"We qualitatively and quantitatively evaluate the performance of the learned policy with and without the proposed intrinsic curiosity signal in two environments, VizDoom and Super Mario Bros. Three broad settings are evaluated: a) sparse extrinsic reward on reaching a goal (Section 4.1); b) exploration with no extrinsic reward (Section 4.2); and c) generalization to novel scenarios (Section 4.3).",4. Experiments,[0],[0]
"In VizDoom generalization is evaluated on a novel map with novel textures, while in Mario it is evaluated on subsequent game levels.",4. Experiments,[0],[0]
We perform extrinsic reward experiments on VizDoom using ‘DoomMyWayHome-v0’ setup described in Section 3.,4.1. Sparse Extrinsic Reward Setting,[0],[0]
The extrinsic reward is sparse and only provided when the agent finds the goal (a vest) located at a fixed location in the map.,4.1. Sparse Extrinsic Reward Setting,[0],[0]
"We systematically varied the difficulty of this goaldirected exploration task by varying the distance between
the initial spawning location of the agent and the location of the goal.",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"A larger distance means that the chances of reaching the goal location by random exploration is lower and consequently the reward is said to be sparser.
",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"Varying the degree of reward sparsity: We consider three setups with “dense”, “sparse” and “very-sparse” rewards (see Figure 4b).",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"In these settings, the reward is always terminal and the episode terminates upon reaching goal or after a maximum of 2100 steps.",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"In the “dense” reward case, the agent is randomly spawned in any of the 17 possible spawning locations uniformly distributed across the map.",4.1. Sparse Extrinsic Reward Setting,[0],[0]
This is not a hard exploration task because sometimes the agent is randomly initialized close to the goal and therefore by random -greedy exploration it can reach the goal with reasonably high probability.,4.1. Sparse Extrinsic Reward Setting,[0],[0]
"In the “sparse” and “very sparse” reward cases, the agent is always spawned in Room-13 and Room-17 respectively which are 270 and 350 steps away from the goal under an optimal policy.",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"A long sequence of directed actions is required to reach the goals from these rooms, making these settings hard goal directed exploration problems.
",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"Results shown in Figure 5 indicate that while the performance of the baseline A3C degrades with sparser rewards, curious A3C agents are superior in all cases.",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"In the “dense” reward case, curious agents learn much faster indicating more efficient exploration of the environment as compared to -greedy exploration of the baseline agent.",4.1. Sparse Extrinsic Reward Setting,[0],[0]
One possible explanation of the inferior performance of ICM-pixels in comparison to ICM is that in every episode the agent is spawned in one out of seventeen rooms with different textures.,4.1. Sparse Extrinsic Reward Setting,[0],[0]
"It is hard to learn a pixel-prediction model as the number of textures increases.
",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"In the “sparse” reward case, as expected, the baseline A3C agent fails to solve the task, while the curious A3C agent is able to learn the task quickly.",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"Note that ICM-pixels and ICM have similar convergence because, with a fixed spawning location of the agent, the ICM-pixels encounters the same textures at the starting of each episode which makes learning the pixel-prediction model easier as compared to the “dense” reward case.",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"Finally, in the “very sparse” reward case, both the A3C agent and ICM-pixels never succeed, while the ICM agent achieves a perfect score in 66% of the random runs.",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"This indicates that ICM is better suited than ICM-pixels and vanilla A3C for hard goal directed exploration tasks.
",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"Robustness to uncontrollable dynamics For testing the robustness of the proposed ICM formulation to changes in the environment that do not affect the agent, we augmented the agent’s observation with a fixed region of white noise which made up 40% of the image (see Figure 3b).",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"In VizDoom 3-D navigation, ideally the agent should be unaffected by this noise as the noise does not affect the agent in anyway and is merely a nuisance.",4.1. Sparse Extrinsic Reward Setting,[0],[0]
Figure 6 compares the performance of ICM against some baseline methods on the “sparse” reward setup described above.,4.1. Sparse Extrinsic Reward Setting,[0],[0]
"While, the proposed ICM agent achieves a perfect score, ICM-pixels suffers significantly despite having succeeded at the “sparse reward” task when the inputs were not augmented with any noise (see Figure 5b).",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"This indicates that in contrast to ICM-pixels, ICM is insensitive to nuisance changes in the environment.
",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"Comparison to TRPO-VIME We now compare our curious agent against variational information maximization agent trained with TRPO (Houthooft et al., 2016) for the
VizDoom “sparse” reward setup described above.",4.1. Sparse Extrinsic Reward Setting,[0],[0]
TRPO is in general more sample efficient than A3C but takes a lot more wall-clock time.,4.1. Sparse Extrinsic Reward Setting,[0],[0]
We do not show these results in plots because TRPO and A3C have different setups.,4.1. Sparse Extrinsic Reward Setting,[0],[0]
"The hyperparameters and accuracy for the TRPO and VIME results follow from the concurrent work (Fu et al., 2017).",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"Despite the sample efficiency of TRPO, we see that our ICM agents work significantly better than TRPO and TRPO-VIME, both in terms of convergence rate and accuracy.",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"Results are shown in the Table below:
Method Mean (Median) Score (“sparse” reward setup) (at convergence)
TRPO 26.0 % ( 0.0 %) A3C 0.0 % ( 0.0 %) VIME + TRPO 46.1 % ( 27.1 %)
",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"ICM + A3C 100.0 % (100.0 %)
",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"As a sanity check, we replaced the curiosity network with random noise sources and used them as the curiosity reward.",4.1. Sparse Extrinsic Reward Setting,[0],[0]
"We performed systematic sweep across different distribution parameters in the “sparse” reward case: uniform, Gaussian and Laplacian.",4.1. Sparse Extrinsic Reward Setting,[0],[0]
We found that none of these agents were able to reach the goal showing that our curiosity module does not learn degenerate solutions.,4.1. Sparse Extrinsic Reward Setting,[0],[0]
A good exploration policy is one which allows the agent to visit as many states as possible even without any goals.,4.2. No Reward Setting,[0],[0]
"In the case of 3-D navigation, we expect a good exploration policy to cover as much of the map as possible; in the case of playing a game, we expect it to visit as many game states as possible.",4.2. No Reward Setting,[0],[0]
"In order to test if our agent can learn a good exploration policy, we trained it on VizDoom and Mario without any rewards from the environment.",4.2. No Reward Setting,[0],[0]
"We then evaluated what portion of the map was explore (for VizDoom), and how much progress it made (for Mario) in this setting.",4.2. No Reward Setting,[0],[0]
"To our surprise, we have found that in both cases, the noreward agent was able to perform quote well (see video at http://pathak22.github.io/noreward_rl/).
",4.2. No Reward Setting,[0],[0]
VizDoom: Coverage during Exploration.,4.2. No Reward Setting,[0],[0]
"An agent trained with no extrinsic rewards was able to learn to navigate corridors, walk between rooms and explore many rooms in the 3-D Doom environment.",4.2. No Reward Setting,[0],[0]
On many occasions the agent traversed the entire map and reached rooms that were farthest away from the room it was initialized in.,4.2. No Reward Setting,[0],[0]
"Given that the episode terminates in 2100 steps and farthest rooms are over 250 steps away (for an optimally-moving agent), this result is quite remarkable, demonstrating that it is possible to learn useful skills without the requirement of any external supervision of rewards.",4.2. No Reward Setting,[0],[0]
Example explorations are shown in Figure 7.,4.2. No Reward Setting,[0],[0]
"The first 3 maps show our agent ex-
plore a much larger state space without any extrinsic signal, compared to a random exploration agent (last two maps), which often has hard time getting around local minima of state spaces, e.g. getting stuck against a wall and not able to move (see video).
",4.2. No Reward Setting,[0],[0]
Mario: Learning to play with no rewards.,4.2. No Reward Setting,[0],[0]
We train our agent in the Super Mario World using only curiosity based signal.,4.2. No Reward Setting,[0],[0]
"Without any extrinsic reward from environment, our Mario agent can learn to cross over 30% of Level-1.",4.2. No Reward Setting,[0],[0]
"The agent received no reward for killing or dodging enemies or avoiding fatal events, yet it automatically discovered these behaviors (see video).",4.2. No Reward Setting,[0],[0]
"One possible reason is because getting killed by the enemy will result in only seeing a small part of the game space, making its curiosity saturate.",4.2. No Reward Setting,[0],[0]
"In order to remain curious, it is in the agent’s interest to learn how to kill and dodge enemies so that it can reach new parts of the game space.",4.2. No Reward Setting,[0],[0]
"This suggests that curiosity provides indirect supervision for learning interesting behaviors.
",4.2. No Reward Setting,[0],[0]
"To the best of our knowledge, this is the first demonstration where the agent learns to navigate in a 3D environment and discovers how to play a game by making use of relatively complex visual imagery directly from pixels, without any extrinsic rewards.",4.2. No Reward Setting,[0],[0]
"There are several prior works that use reinforcement learning to navigate in 3D environments from pixel inputs or playing ATARI games such as (Mirowski et al., 2017; Mnih et al., 2015; 2016), but they rely on intermediate external rewards provided by the environment.",4.2. No Reward Setting,[0],[0]
In the previous section we showed that our agent learns to explore large parts of the space where its curiosity-driven exploration policy was trained.,4.3. Generalization to Novel Scenarios,[0],[0]
"However, it remains unclear whether the agent has done this by learning “generalized skills” for efficiently exploring its environment, or if it simply memorized the training set.",4.3. Generalization to Novel Scenarios,[0],[0]
"In other words we would like to know, when exploring a space, how much of the learned behavior is specific to that particular space and how much is general enough to be useful in novel scenar-
ios?",4.3. Generalization to Novel Scenarios,[0],[0]
"To investigate this question, we train a no reward exploratory behavior in one scenario (e.g. Level-1 of Mario) and then evaluate the resulting exploration policy in three different ways: a) apply the learned policy “as is” to a new scenario; b) adapt the policy by fine-tuning with curiosity reward only; c) adapt the policy to maximize some extrinsic reward.",4.3. Generalization to Novel Scenarios,[0],[0]
"Happily, in all three cases, we observe some promising generalization results:
Evaluate “as is”: We evaluate the policy trained by maximizing curiosity on Level-1 of Mario on subsequent levels without adapting the learned policy in any way.",4.3. Generalization to Novel Scenarios,[0],[0]
"We measure the distance covered by the agent as a result of executing this policy on Levels 1, 2, and 3, as shown in Table 1.",4.3. Generalization to Novel Scenarios,[0],[0]
"We note that the policy performs surprisingly well on Level 3, suggesting good generalization, despite the fact that Level-3 has different structures and enemies compared to Level-1.",4.3. Generalization to Novel Scenarios,[0],[0]
"However, note that the running “as is” on Level2 does not do well.",4.3. Generalization to Novel Scenarios,[0],[0]
"At first, this seems to contradict the generalization results on Level-3.",4.3. Generalization to Novel Scenarios,[0],[0]
"However, note that Level-3 has similar global visual appearance (day world with sunlight) to Level-1, whereas Level-2 is significantly different (night world).",4.3. Generalization to Novel Scenarios,[0],[0]
"If this is indeed the issue, then it should be possible to quickly adapt the exploration policy to Level-2 with a little bit of “fine-tuning”.",4.3. Generalization to Novel Scenarios,[0],[0]
"We will explore this below.
",4.3. Generalization to Novel Scenarios,[0],[0]
Fine-tuning with curiosity only: From Table 1 we see that when the agent pre-trained (using only curiosity as reward) on Level-1 is fine-tuned (using only curiosity as reward) on Level-2 it quickly overcomes the mismatch in global visual appearance and achieves a higher score than training from scratch with the same number of iterations.,4.3. Generalization to Novel Scenarios,[0],[0]
"Interestingly, training “from scratch” on Level-2 is worse than the fine-tuned policy, even when training for more iterations than pre-training + fine-tuning combined.",4.3. Generalization to Novel Scenarios,[0],[0]
"One possible reason is that Level-2 is more difficult than Level1, so learning the basic skills such as moving, jumping, and killing enemies from scratch is much more dangerous than in the relative “safety” of Level-1.",4.3. Generalization to Novel Scenarios,[0],[0]
"This result, therefore might suggest that first pre-training on an earlier level
and then fine-tuning on a later one produces a form of curriculum which aids learning and generalization.",4.3. Generalization to Novel Scenarios,[0],[0]
"In other words, the agent is able to use the knowledge it acquired by playing Level-1 to better explore the subsequent levels.",4.3. Generalization to Novel Scenarios,[0],[0]
"Of course, the game designers do this on purpose to allow the human players to gradually learn to play the game.
",4.3. Generalization to Novel Scenarios,[0],[0]
"However, interestingly, fine-tuning the exploration policy pre-trained on Level-1 to Level-3 deteriorates the performance, compared to running “as is”.",4.3. Generalization to Novel Scenarios,[0],[0]
This is because Level3 is very hard for the agent to cross beyond a certain point – the agent hits a curiosity blockade and is unable to make any progress.,4.3. Generalization to Novel Scenarios,[0],[0]
"As the agent has already learned about parts of the environment before the hard point, it receives almost no curiosity reward and as a result it attempts to update its policy with almost zero intrinsic rewards and the policy slowly degenerates.",4.3. Generalization to Novel Scenarios,[0],[0]
"This behavior is vaguely analogous to boredom, where if the agent is unable to make progress it gets bored and stops exploring.
",4.3. Generalization to Novel Scenarios,[0],[0]
"Fine-tuning with extrinsic rewards: If it is the case that the agent has actually learned useful exploratory behavior, then it should be able to learn quicker than starting from scratch even when external rewards are provided by environment.",4.3. Generalization to Novel Scenarios,[0],[0]
We perform this evaluation on VizDoom where we pre-train the agent using curiosity reward on a map showed in Figure 4a.,4.3. Generalization to Novel Scenarios,[0],[0]
"We then test on the “very sparse” reward setting of ‘DoomMyWayHome-v0’ environment which uses a different map with novel textures (see Figure 4b) as described earlier in Section 4.1.
",4.3. Generalization to Novel Scenarios,[0],[0]
Results in Figure 8 show that the ICM agent pre-trained only with curiosity and then fine-tuned with external reward learns faster and achieves higher reward than an ICM agent trained from scratch to jointly maximize curiosity and the external rewards.,4.3. Generalization to Novel Scenarios,[0],[0]
This result confirms that the learned exploratory behavior is also useful when the agent is required to achieve goals specified by the environment.,4.3. Generalization to Novel Scenarios,[0],[0]
It is also worth noting that ICM-pixels does not generalize to this test environment.,4.3. Generalization to Novel Scenarios,[0],[0]
This indicates that the proposed mechanism of measuring curiosity is significantly better for learning skills that generalize as compared to measuring curiosity in the raw sensory space.,4.3. Generalization to Novel Scenarios,[0],[0]
"Curiosity-driven exploration is a well studied topic in the reinforcement learning literature and a good summary can be found in (Oudeyer & Kaplan, 2009; Oudeyer et al., 2007).",5. Related Work,[0],[0]
Schmidhuber (1991; 2010) and Sun et al. (2011) use surprise and compression progress as intrinsic rewards.,5. Related Work,[0],[0]
Classic work of Kearns et al. (1999) and Brafman et al. (2002) propose exploration algorithms polynomial in the number of state space parameters.,5. Related Work,[0],[0]
"Others have used empowerment, which is the information gain based on entropy of actions, as intrinsic rewards (Klyubin et al., 2005; Mohamed & Rezende, 2015).",5. Related Work,[0],[0]
Stadie et al. (2015) use prediction error in the feature space of an auto-encoder as a measure of interesting states to explore.,5. Related Work,[0],[0]
"State visitation counts have also been investigated for exploration (Bellemare et al., 2016; Oh et al., 2015; Tang et al., 2016).",5. Related Work,[0],[0]
"Osband et al. (2016) train multiple value functions and makes
use of bootstrapping and Thompson sampling for exploration.",5. Related Work,[0],[0]
"Many approaches measure information gain for exploration (Little & Sommer, 2014; Still & Precup, 2012; Storck et al., 1995).",5. Related Work,[0],[0]
Houthooft et al. (2016) use an exploration strategy that maximizes information gain about the agent’s belief of the environment’s dynamics.,5. Related Work,[0],[0]
"Our approach of jointly training forward and inverse models for learning a feature space has similarities to (Agrawal et al., 2016; Jordan & Rumelhart, 1992; Wolpert et al., 1995), but these works use the learned models of dynamics for planning a sequence of actions instead of exploration.",5. Related Work,[0],[0]
"The idea of using a proxy task to learn a semantic feature embedding has been used in a number of works on self-supervised learning in computer vision (Agrawal et al., 2015; Doersch et al., 2015; Goroshin et al., 2015; Jayaraman & Grauman, 2015; Pathak et al., 2016; Wang & Gupta, 2015).
",5. Related Work,[0],[0]
Concurrent work: A number of interesting related papers have appeared on Arxiv while the present work was in submission.,5. Related Work,[0],[0]
Sukhbaatar et al. (2017) generates supervision for pre-training via asymmetric self-play between two agents to improve data efficiency during fine-tuning.,5. Related Work,[0],[0]
"Several methods propose improving data efficiency of RL algorithms using self-supervised prediction based auxiliary tasks (Jaderberg et al., 2017; Shelhamer et al., 2017).",5. Related Work,[0],[0]
"Fu et al. (2017) learn discriminative models, and Gregor et al. (2017) use empowerment based measure to tackle exploration in sparse reward setups.",5. Related Work,[0],[0]
"In this work we propose a mechanism for generating curiosity-driven intrinsic reward signal that scales to high dimensional visual inputs, bypasses the difficult problem of predicting pixels and ensures that the exploration strategy of the agent is unaffected by nuisance factors in the environment.",6. Discussion,[0],[0]
"We demonstrate that our agent significantly outperforms the baseline A3C with no curiosity, a recently proposed VIME (Houthooft et al., 2016) formulation for exploration, and a baseline pixel-predicting formulation.
",6. Discussion,[0],[0]
In VizDoom our agent learns the exploration behavior of moving along corridors and across rooms without any rewards from the environment.,6. Discussion,[0],[0]
In Mario our agent crosses more than 30% of Level-1 without any rewards from the game.,6. Discussion,[0],[0]
One reason why our agent is unable to go beyond this limit is the presence of a pit at 38% of the game that requires a very specific sequence of 15-20 key presses in order to jump across it.,6. Discussion,[0],[0]
"If the agent is unable to execute this sequence, it falls in the pit and dies, receiving no further rewards from the environment.",6. Discussion,[0],[0]
Therefore it receives no gradient information indicating that there is a world beyond the pit that could potentially be explored.,6. Discussion,[0],[0]
"This issue is somewhat orthogonal to developing models of curiosity, but presents a challenging problem for policy learning.
",6. Discussion,[0],[0]
It is common practice to evaluate reinforcement learning approaches in the same environment that was used for training.,6. Discussion,[0],[0]
"However, we feel that it is also important to evaluate on a separate “testing set” as well.",6. Discussion,[0],[0]
"This allows us to gauge how much of what has been learned is specific to the training environment (i.e. memorized), and how much might constitute “generalizable skills” that could be applied to new settings.",6. Discussion,[0],[0]
"In this paper, we evaluate generalization in two ways: 1) by applying the learned policy to a new scenario “as is” (no further learning), and 2) by finetuning the learned policy on a new scenario (we borrow the pre-training/fine-tuning nomenclature from the deep feature learning literature).",6. Discussion,[0],[0]
We believe that evaluating generalization is a valuable tool and will allow the community to better understand the performance of various reinforcement learning algorithms.,6. Discussion,[0],[0]
"To further aid in this effort, we will make the code for our algorithm, as well as testing and environment setups freely available online.
",6. Discussion,[0],[0]
"An interesting direction of future research is to use the learned exploration behavior/skill as a motor primitive/lowlevel policy in a more complex, hierarchical system.",6. Discussion,[0],[0]
"For example, our VizDoom agent learns to walk along corridors instead of bumping into walls.",6. Discussion,[0],[0]
"This could be a useful primitive for a navigation system.
",6. Discussion,[0],[0]
"While the rich and diverse real world provides ample opportunities for interaction, reward signals are sparse.",6. Discussion,[0],[0]
Our approach excels in this setting and converts unexpected interactions that affect the agent into intrinsic rewards.,6. Discussion,[0],[0]
However our approach does not directly extend to the scenarios where “opportunities for interactions” are also rare.,6. Discussion,[0],[0]
"In theory, one could save such events in a replay memory and use them to guide exploration.",6. Discussion,[0],[0]
"However, we leave this extension for future work.
",6. Discussion,[0],[0]
"Acknowledgements: We would like to thank Sergey Levine, Evan Shelhamer, Saurabh Gupta, Phillip Isola and other members of the BAIR lab for fruitful discussions and comments.",6. Discussion,[0],[0]
We thank Jacob Huh for help with Figure 2 and Alexey Dosovitskiy for VizDoom maps.,6. Discussion,[0],[0]
"This work was supported in part by NSF IIS-1212798, IIS-1427425, IIS1536003, IIS-1633310, ONR MURI N00014-14-1-0671, Berkeley DeepDrive, equipment grant from Nvidia, and the Valrhona Reinforcement Learning Fellowship.",6. Discussion,[0],[0]
"In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether.",abstractText,[0],[0]
"In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life.",abstractText,[0],[0]
We formulate curiosity as the error in an agent’s ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model.,abstractText,[0],[0]
"Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent.",abstractText,[0],[0]
"The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch.",abstractText,[0],[0]
Curiosity-driven Exploration by Self-supervised Prediction,title,[0],[0]
"Biological organisms can learn to perform tasks (and often do) by observing a sequence of labeled events, just like supervised machine learning.",1. Introduction,[0],[0]
"But unlike machine learning, in human learning supervision is often accompanied by a curriculum.",1. Introduction,[0],[0]
Thus the order of presented examples is rarely random when a human teacher teaches another human.,1. Introduction,[0],[0]
"Likewise, the task may be divided by the teacher into smaller sub-tasks, a process sometimes called shaping (Krueger & Dayan, 2009) and typically studied in the context of rein-
1School of Computer Science and Engineering, The Hebrew University of Jerusalem, Jerusalem 91904, Israel.",1. Introduction,[0],[0]
"Correspondence to: Daphna Weinshall <daphna@mail.huji.ac.il>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"forcement learning (e.g. Graves et al., 2017).",1. Introduction,[0],[0]
"Although it remained for the most part in the fringes of machine learning research, curriculum learning has been identified as a key challenge for machine learning throughout (e.g., Mitchell, 1980; 2006; Wang & Cottrell, 2015).
",1. Introduction,[0],[0]
"We focus here on curriculum learning based on ranking (or weighting as in (Bengio et al., 2009)) of the training examples, which is used to guide the order of presentation of examples to the learner.",1. Introduction,[0],[0]
"Risking over simplification, the idea is to first present the learner primarily with examples of higher weight or rank, later to be followed by examples with lower weight or rank.",1. Introduction,[0],[0]
"Ranking may be based on the difficulty of each training example as evaluated by the teacher, from easiest to the most difficult.
",1. Introduction,[0],[0]
"In Section 2 we investigate this strict definition of curriculum learning theoretically, in the context of stochastic gradient descent used to optimize the convex linear regression loss function.",1. Introduction,[0],[0]
We first define the (ideal) difficulty of a training point as its loss with respect to the optimal classifier.,1. Introduction,[0],[0]
"We then prove that curriculum learning, when given the ranking of training points by their difficulty thus defined, is expected (probabilistically) to significantly speed up learning especially at the beginning of training.",1. Introduction,[0],[0]
"This theoretical result is supported by empirical evidence obtained in the deep learning scenario of curriculum learning described in Section 3, where similar behavior is observed.",1. Introduction,[0],[0]
"We also show that when the difficulty of the sampled training points is fixed, convergence is faster when sampling points that incur higher loss with respect to the current hypothesis as suggested in (Shrivastava et al., 2016).",1. Introduction,[0],[0]
"This result is not always true when the difficulty of the sampled training points is not fixed.
",1. Introduction,[0],[0]
But such ideal ranking is rarely available.,1. Introduction,[0],[0]
"In fact, the need for such supervision has rendered curriculum learning less useful in machine learning, since ranking by difficulty is hard to obtain.",1. Introduction,[0],[0]
"Moreover, even when it is provided by a human teacher, it may not reflect the true difficulty as it affects the machine learner.",1. Introduction,[0],[0]
"For example, in visual object recognition it has been demonstrated that what makes an image difficult to a neural network classifier may not always match whatever makes it difficult to a human observer, an observation that has been taken advantage of in the recent
work on adversarial examples (Szegedy et al., 2013).",1. Introduction,[0],[0]
"Possibly, this is one of the reasons why curriculum learning is rarely used in practice (but see, e.g., Zaremba & Sutskever, 2014; Amodei et al., 2016; Jesson et al., 2017).
",1. Introduction,[0],[0]
In the second part of this paper we focus on this question - how to rank (or weight) the training examples without the aid of a human teacher.,1. Introduction,[0],[0]
"This is paramount when a human teacher cannot provide a reliable difficulty score for the task at hand, or when obtaining such a score by human teachers is too costly.",1. Introduction,[0],[0]
This question is also closely related to transfer learning: here we investigate the use of another classifier to provide the ranking of the training examples by their presumed difficulty.,1. Introduction,[0],[0]
"This form of transfer should not be confused with the notion of transfer discussed in (Bengio et al., 2009) in the context of multi-task and lifelong learning (Thrun & Pratt, 2012), where knowledge is transferred from earlier tasks (e.g. the discrimination of easy examples) to later tasks (e.g. the discrimination of difficult examples).",1. Introduction,[0],[0]
"Rather, we investigate the transfer of knowledge from one classifier to another, as in teacher classifier to student classifier.",1. Introduction,[0],[0]
"In this form curriculum learning has not been studied in the context of deep learning, and hardly ever in the context of other classification paradigms.
",1. Introduction,[0],[0]
"Differently from previous work, it is not the instance representation which is being transferred but rather the ranking of training examples.",1. Introduction,[0],[0]
Why is this a good idea?,1. Introduction,[0],[0]
"This kind of transfer assumes that a powerful pre-trained network is only available at train time, and cannot be used at test time even for the computation of a test point’s representation.",1. Introduction,[0],[0]
"This may be the case, for example, when the powerful network is too big to run on the target device.",1. Introduction,[0],[0]
"One can no longer expect to have access to the transferred representation at test time, while ranking can be used at train time in order to improve the learning of the target smaller network (see related discussion of network compression in (Chen et al., 2015; Kim et al., 2015), for example).
",1. Introduction,[0],[0]
"In Section 3 we describe our method, an algorithm which uses the ranking to construct a schedule for the order of presentation of training examples.",1. Introduction,[0],[0]
"In subsequent empirical evaluations we compare the performance of the method when using a curriculum which is based on different scheduling options, including 2 control conditions where difficult examples are presented first or when using arbitrary scheduling.",1. Introduction,[0],[0]
"The main results of this empirical study can be summarized as follows: (i) Learning rate is always faster with curriculum learning, especially at the beginning of training.",1. Introduction,[0],[0]
"(ii) Final generalization is sometimes improved with curriculum learning, especially when the conditions for learning are hard: the task is difficult, the network is small, or when strong regularization is enforced.",1. Introduction,[0],[0]
"These results are consistent with prior art (see e.g. Bengio et al., 2009).",1. Introduction,[0],[0]
"We start with some notations in Section 2.1, followed in Sections 2.2 by the rigorous analysis of curriculum learning when used to optimize the linear regression loss.",2. Theoretical analysis,[0],[0]
"In Section 2.3 we report supporting empirical evidence for the main theoretical results, obtained using the deep learning setup described later in Section 3.",2. Theoretical analysis,[0],[0]
"Let X = {(xi, yi)}ni=1 denote the training data, where xi ∈ Rd denotes the i-th data point and yi its corresponding label.",2.1. Notations and definitions,[0],[0]
"In general, the goal is to find a hypothesis h̄(x) ∈ H that minimizes the risk function (the expected loss).",2.1. Notations and definitions,[0],[0]
"In order to minimize this objective, Stochastic Gradient Descent (SGD) is often used with various extensions and regularization.
",2.1. Notations and definitions,[0],[0]
"We start with two definitions:
Definition 1 (Ideal Difficulty Score).",2.1. Notations and definitions,[0],[0]
"The difficulty of point x is measured by its minimal loss with respect to the set of optimal hypotheses {L(h̄(xi), yi)}.",2.1. Notations and definitions,[0],[0]
Definition 2 (Stochastic Curriculum Learning).,2.1. Notations and definitions,[0],[0]
"SCL is a variation on Stochastic Gradient Descent (SGD), where the learner is exposed to the data gradually based on the difficulty score of the training points.
",2.1. Notations and definitions,[0],[0]
"In vanilla SGD training, at each iteration the learner is presented with a new datapoint (or mini-batch) sampled from the training data based on some probability function D(X).",2.1. Notations and definitions,[0],[0]
"In SCL, the sampling is biased to favor easier examples at the beginning of the training.",2.1. Notations and definitions,[0],[0]
This bias is decreased following some scheduling procedure.,2.1. Notations and definitions,[0],[0]
"At the end of training, points are sampled according to D(X) as in vanilla SGD.
",2.1. Notations and definitions,[0],[0]
"In practice, an SCL algorithm should solve two problems: (i) Score the training points by difficulty; in prior art this score was typically provided by the teacher in a supervised manner.",2.1. Notations and definitions,[0],[0]
(ii) Define the scheduling procedure.,2.1. Notations and definitions,[0],[0]
Here we analyze SCL when used to minimize the linear regression model.,2.2. The linear regression loss,[0],[0]
"Specifically, we investigate the differential effect of a point’s Difficulty Score on convergence towards the global minimum of the expected least squares loss, when the family of hypotheses H includes the linear functions h(x) = atx + b and y ∈ R.
",2.2. The linear regression loss,[0],[0]
"The risk function of the regression model is the following
R(X,w) = ED(X)L(hw(x), y) L(hw(xi), yi) =",2.2. The linear regression loss,[0],[0]
"(h(xi)− yi)2 = (atxi + b− yi)2
, (xtiw",2.2. The linear regression loss,[0],[0]
"− yi)2 , L(Xi,w) (1)
",2.2. The linear regression loss,[0],[0]
"In the last transition above, w =",2.2. The linear regression loss,[0],[0]
"[
a b
] ∈ Rd+1.",2.2. The linear regression loss,[0],[0]
"With some
abuse of notation, xi denotes the vector [ xi 1 ] .",2.2. The linear regression loss,[0],[0]
Xi denotes the vector,2.2. The linear regression loss,[0],[0]
"[xi, yi], with Difficulty Score L(Xi, w̄).
",2.2. The linear regression loss,[0],[0]
"In general the output hypothesis hw(x) = xtiw is determined by minimizing R(X,w) with respect to w. The global minimum w̄ of the empirical loss can be computed in closed form from the training data.",2.2. The linear regression loss,[0],[0]
"However, gradient descent can be used to find w̄ with guaranteed convergence, which is efficient when n is very large.
",2.2. The linear regression loss,[0],[0]
Recall that SCL computes a sequence of estimators {wt}Tt=1 for the parameters of the optimal hypothesis w̄.,2.2. The linear regression loss,[0],[0]
"This is based on a sequence of training points {Xt = [xt, yt]}Tt=1, sampled from the training data while favoring easy points at the beginning of training.",2.2. The linear regression loss,[0],[0]
"Other than sampling probability, the update step at time t follows SGD:
wt+1 =",2.2. The linear regression loss,[0],[0]
"wt − η ∂L(Xt,w)
∂w",2.2. The linear regression loss,[0],[0]
|w,2.2. The linear regression loss,[0],[0]
=wt (2),2.2. The linear regression loss,[0],[0]
The main theorem in this sub-section states that the expected rate of convergence of gradient descent is monotonically decreasing with the Difficulty Score of the sample,CONVERGENCE RATE DECREASES WITH DIFFICULTY,[0],[0]
Xt.,CONVERGENCE RATE DECREASES WITH DIFFICULTY,[0],[0]
We prove it below for the gradient step as defined in (2).,CONVERGENCE RATE DECREASES WITH DIFFICULTY,[0],[0]
"If the size of the gradient step is fixed at η, a somewhat stronger theorem can be obtained where the constraint on the step size being small is not required.
",CONVERGENCE RATE DECREASES WITH DIFFICULTY,[0],[0]
"We first derive the gradient step at time t:
s = −η ∂L(Xi,w) ∂w",CONVERGENCE RATE DECREASES WITH DIFFICULTY,[0],[0]
= −2η(xtiw,CONVERGENCE RATE DECREASES WITH DIFFICULTY,[0],[0]
"− yi)xi (3)
",CONVERGENCE RATE DECREASES WITH DIFFICULTY,[0],[0]
"Let Ωi denote the hyperplane on which this gradient vanishes ∂L(Xi,w)∂w = 0.",CONVERGENCE RATE DECREASES WITH DIFFICULTY,[0],[0]
"This hyperplane is defined by x t iw = yi, namely, xi defines its normal direction.",CONVERGENCE RATE DECREASES WITH DIFFICULTY,[0],[0]
Thus (3) implies that the gradient step at time t is perpendicular to Ωi.,CONVERGENCE RATE DECREASES WITH DIFFICULTY,[0],[0]
Let z̄ denote the projection of w̄ on Ωi.,CONVERGENCE RATE DECREASES WITH DIFFICULTY,[0],[0]
"Let Ψ2 = L(Xi, w̄) denote the Difficulty Score of Xi.
",CONVERGENCE RATE DECREASES WITH DIFFICULTY,[0],[0]
Lemma 1.,CONVERGENCE RATE DECREASES WITH DIFFICULTY,[0],[0]
Fix the training point Xi.,CONVERGENCE RATE DECREASES WITH DIFFICULTY,[0],[0]
The Difficulty Score of Xi is Ψ2 = r2‖w̄,CONVERGENCE RATE DECREASES WITH DIFFICULTY,[0],[0]
− z̄‖2.,CONVERGENCE RATE DECREASES WITH DIFFICULTY,[0],[0]
"Ψ2 = L(Xi, w̄) = L(Xi, z̄ + (w̄ − z̄))",Proof.,[0],[0]
=,Proof.,[0],[0]
[xtiz̄ + x t i(w̄,Proof.,[0],[0]
"− z̄)− yi]2
=",Proof.,[0],[0]
"[xti(w̄ − z̄)]2 = ‖xi‖2‖w̄ − z̄‖2 (4)
",Proof.,[0],[0]
"Recall that xi,w ∈ Rd+1.",Proof.,[0],[0]
"We continue the analysis in the parameter space w ∈ Rd+1, where parameter vector w corresponds to a point, and data vector xi describes a hyperplane.",Proof.,[0],[0]
"In this space we represent each vector
xi in a hyperspherical coordinate system [r, ϑ,Φ], with pole (origin) fixed at w̄ and polar axis (zenith direction) ~O = w̄",Proof.,[0],[0]
− wt (see Fig. 1).,Proof.,[0],[0]
"r denotes the vector’s length, while 0 ≤ ϑ ≤ π denotes the polar angle with respect to ~O.",Proof.,[0],[0]
Let Φ = [ϕ1 . . .,Proof.,[0],[0]
", ϕd−1] denote the remaining polar angles.
",Proof.,[0],[0]
"To illustrate, Fig. 1 shows a planar section of the parameter space, the 2D plane formed by the two intersecting lines ~O and z̄− w̄.",Proof.,[0],[0]
The gradient step s points from wt towards Ωi.,Proof.,[0],[0]
"Ωi is perpendicular to xi, which is parallel to z̄− w̄ and to s, and therefore Ωi is projected onto a line in this plane.",Proof.,[0],[0]
"We introduce the notation λ = ‖w̄ −wt‖.
Let sO denote the projection of the gradient vector s on the polar axis ~O, and let s⊥ denote the perpendicular component.",Proof.,[0],[0]
"From (3) and the definition of Ψ
s = −2ηxi(xtiwt",Proof.,[0],[0]
− yi) =,Proof.,[0],[0]
−2ηxi[xti(wt − w̄)±Ψ] sO = s · w̄,Proof.,[0],[0]
"−wt
λ = 2
η λ",Proof.,[0],[0]
[r2λ2 cos2 ϑ∓Ψrλ,Proof.,[0],[0]
"cosϑ]
(5)
",Proof.,[0],[0]
"Let x = (r, ϑ,Φ).",Proof.,[0],[0]
"Let fD(X) = f(r, ϑ,Φ)fY (|y − xtw̄|) denote the density function of the data X.",Proof.,[0],[0]
"This choice assumes that the density of the label y only depends on the absolute error |y − xtw̄|.
",Proof.,[0],[0]
"For the subsequent derivations we need the conditional distribution of the data X given difficulty score Ψ. Fixing the difficulty score determines one of two labels y(x,Ψ) = xtw̄",Proof.,[0],[0]
± Ψ.,Proof.,[0],[0]
"We further assume that both labels are equally likely1, and therefore fD(X)/Ψ ([x, y]) = 12f(r, ϑ,Φ).
",Proof.,[0],[0]
"Let ∆(Ψ) denote the expected convergence rate at time t, given fixed difficulty score Ψ.
∆(Ψ) =",Proof.,[0],[0]
E[‖wt − w̄‖2 − ‖wt+1,Proof.,[0],[0]
"− w̄‖2/Ψ] (6)
Lemma 2.
∆(Ψ) = 2λE[sO/Ψ]− E[s 2/Ψ] (7) 1This assumption can be somewhat relaxed, but the strict form is used to simplify the exposition.
",Proof.,[0],[0]
Proof.,Proof.,[0],[0]
"From (6)
E[∆] = (−λ)2 − E[(−λ+ sO)2 + s2⊥] = λ2 − (λ2 − 2λE[sO] + E[s2O])− E[s2⊥] = 2λE[sO]− E[s2]
From Lemma 2 and (5)2
1 4 ∆(Ψ) = ηE[r2λ2 cos2 ϑ]− η2E[r4λ2 cos2 ϑ]
−η2Ψ2E[r2] (8) −ηE[(±Ψ)rλ cosϑ]− 2η2E[(±Ψ)r3λ cosϑ]
Lemma 3.
E[(±Ψ)rλ cosϑ] = E[(±Ψ)r3λ cosϑ] = 0
Proof.",Proof.,[0],[0]
"The lemma follows from the assumed symmetry of D(X) with respect to the sign of yi − xtiw̄.
",Proof.,[0],[0]
"It follows from Lemma 3 that 1
4 ∆(Ψ) =ηE[r2λ2 cos2 ϑ]− η2E[r4λ2 cos2 ϑ]
− η2Ψ2E[r2] (9)
We can now state the main theorem of this section.",Proof.,[0],[0]
Theorem 1.,Proof.,[0],[0]
At time t the expected convergence rate for training point x is monotonically decreasing with the Difficulty Score Ψ of x.,Proof.,[0],[0]
If the step size coefficient is sufficiently small so that η ≤,Proof.,[0],[0]
"E[r
2 cos2 ϑ] E[r4 cos2 ϑ] , it is likewise monotonically
increasing with the distance λ between the current estimate of the hypothesis wt and the optimal hypothesis w̄.
Proof.",Proof.,[0],[0]
"From (9)
∂∆(Ψ)
∂Ψ = −8η2E[r2]Ψ ≤ 0
which proves the first statement.",Proof.,[0],[0]
"In addition,
∂∆(Ψ)
∂λ = 8ηλ
( E[r2 cos2 ϑ]− ηE[r4 cos2 ϑ] )",Proof.,[0],[0]
"If η ≤ E[r
2 cos2 ϑ] E[r4 cos2 ϑ] then ∂∆(Ψ) ∂λ ≥ 0, and the second statement
follows.
",Proof.,[0],[0]
Corollary 1.,Proof.,[0],[0]
"Although E[∆(Ψ)] may be negative, wt always converges faster to w̄ when the training points are sampled from easier examples with smaller Ψ. Corollary 2.",Proof.,[0],[0]
If the step size coefficient η is small enough so that η ≤,Proof.,[0],[0]
"E[r
2 cos2 ϑ] E[r4 cos2 ϑ] , we should expect faster convergence
at the beginning of SCL.",Proof.,[0],[0]
"2Below the short-hand notation E[(±Ψ)] implies that the 2 cases of y(x,Ψ) = xtw̄",Proof.,[0],[0]
"± Ψ should be considered, with equal probability 1
2 by assumption.",Proof.,[0],[0]
"The main theorem in this sub-section states that for a fixed difficulty score Ψ, when the gradient step is small enough, convergence is monotonically increasing with the loss of the point with respect to the current hypothesis.",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
This is not true in general.,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"The second theorem in this section shows that when the difficulty score is not fixed, there exist hypotheses w ∈ H for which the convergence rate is decreasing with the current loss.
",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"Let Υ2 = L(Xi,wt) denote the loss of Xi with respect to the current hypothesis wt.",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
Define the angle β ∈,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"[0, π2 ) as follows (see Fig. 1)
β = β(r,Ψ, λ) = arccos(min",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"( Ψ
λr , 1))",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"(10)
Lemma 4.",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"The relation between Υ,Ψ, r, ϑ can be written separately in 4 regions as follows (see Fig. 1):
A1 0 ≤ ϑ ≤",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"π−β, yi = xtiw̄+ Ψ =⇒ yi",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"= xtiwt+ Υ, λr cosϑ = xti(w̄ −wt) = −Ψ",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"+ Υ
A2 π−β ≤ ϑ ≤ π, yi = xtiw̄+Ψ =⇒ yi = xtiwt−Υ, λr cosϑ = −Ψ−Υ
A3 0 ≤ ϑ ≤ β, yi",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
= xtiw̄ −Ψ =⇒ yi = xtiwt,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"+ Υ, λr cosϑ = Ψ + Υ
A4 β ≤ ϑ ≤ π,",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"yi = xtiw̄ −Ψ =⇒ yi = xtiwt −Υ, λr cosϑ = Ψ−Υ
Proof.",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"We keep in mind that ∀xi and Ψ, there are 2 possible yi with equal probability.",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
Recall that z̄ denotes the projection of w̄ on Ωi.,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"In the planar section shown in Fig. 1,
z̄ lies in the upper half space ⇐⇒ yi =",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"xtiw̄ + Ψ
z̄ lies in the lower half space ⇐⇒ yi =",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"xtiw̄ −Ψ
",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"This follows from 3 observations: x̄i lies in the upper half space by the definition of the polar coordinate system, xtiw̄",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"− yi = ±Ψ, and
0 = xtiz̄− yi = xti(z̄− w̄) +",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
xtiw̄,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"− yi
Next, let zt denote the projection of wt on Ωi.",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"Then
0 = xtizt",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
− yi = xti(zt −wt) +,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
xtiwt,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"− yi
When z̄ lies in the upper half space, the following can be verified geometrically from Fig. 1:
0 ≤ ϑ ≤ π−β ⇒ xti(zt−wt)",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
≥ 0 ⇒ yi = xtiwt,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
+,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"Υ
π−β ≤ ϑ ≤ π ⇒",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"xti(zt−wt) ≤ 0 ⇒ yi = xtiwt−Υ
When z̄ lies in the lower half space
0 ≤ ϑ ≤ β =⇒ xti(zt −wt) ≥ 0",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"=⇒ yi = xtiwt + Υ
β ≤ ϑ ≤ π =⇒ xti(zt −wt) ≤ 0",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"=⇒ yi = xtiwt −Υ
Next we analyze how the convergence rate at xi changes with Υ. Let ∆(Ψ,Υ) denote the expected convergence rate at time t, given fixed difficulty score Ψ and fixed loss Υ. From (9) ∆(Ψ,Υ)",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"= 4ηE[r2λ2 cos2 ϑ/Υ] +O(η 2).
",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"It is easier to analyze ∆(Ψ,Υ) when using the Cartesian coordinates, rather than polar, in the 2D plane defined by the vectors ~O = w̄−wt and z̄− w̄",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"(see Fig. 1); thus we define u = r cosϑ, v = r sinϑ.",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"The 4 cases listed in Lemma 4 can be readily transformed to this coordinate system as follows {0 ≤ ϑ ≤ β} ⇔ {λu ≥ Ψ}, {β ≤ ϑ ≤ π",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"− β} ⇔ {−Ψ ≤ λu ≤ Ψ}, and {π − β ≤ ϑ ≤ π} ⇔ {λu ≤ −Ψ}:
A1 λu ≥ −Ψ : λu = −Ψ + Υ
A2 λu ≤ −Ψ",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
": λu = −Ψ−Υ
A3 λu ≥ Ψ : λu = Ψ + Υ
A4 λu ≤ Ψ : λu = Ψ−Υ
Define
∇ = f(ψ+Υλ )",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
− f( ψ−Υ λ ),CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"− f( −ψ+Υ λ ) + f( −ψ−Υ λ )
f(ψ+Υλ )",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
+,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
f( ψ−Υ λ ) +,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
f( −ψ+Υ λ ) +,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"f( −ψ−Υ λ )
",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
Clearly −1 ≤ ∇ ≤ 1.,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
Theorem 2.,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"Assume that the gradient step size is small enough so that we can neglect second order terms O(η2), and",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
that ∂∇∂Υ ≥ ψ Υ,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"− Υ ψ ∀Υ. Fix the difficulty score at Ψ. At time t the expected convergence rate is monotonically increasing with the loss Υ of the training point x.
Proof.",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"In the coordinate system defined above ∆(Ψ,Υ) = 4ηE[λ2u2/Υ]",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"+ O(η
2)",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
.,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"We compute ∆(Ψ,Υ) separately in each region, marginalizing out v based on the following∫ ∫ ∞
0
λ2u2vd−1f(u, v)dvdu = ∫",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"λ2u2f(u)du
where f(u) denotes the marginal distribution of u.
Let ui denote the value of u corresponding to loss Υ in each region A1-A4, and 12f(ui) its density.",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"∆(Ψ,Υ) takes 4 discrete values, one in each region, and its expected value is therefore ∆(Ψ,Υ) =",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
4η ∑4 i=1,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
λ 2u2i f(ui)∑4 i=1,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"f(ui)
.",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"It can readily be shown that
1
4η ∆(ψ,Υ) = ψ2 +",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"Υ2 + 2ψΥ∇ (11)
and subsequently
1
4η
∂∆(ψ,Υ)
∂Υ",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
= 2Υ + 2ψΥ ∂∇ ∂Υ,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"+ 2ψ ∇
≥ 2Υ + 2ψΥ ∂∇ ∂Υ",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"− 2ψ
(12)
",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
Using the assumption that ∂∇∂Υ ≥ ψ Υ,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"− Υ ψ ∀Υ, we have that
1
8η
∂∆(ψ,Υ) ∂Υ",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
≥ Υ + ψΥ ψ,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
−Υ ψΥ,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
− ψ,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"= 0
Corollary 3.",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"For any c ∈ R+, if∇ is (c− 1c )-lipschitz",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"then ∂∆(ψ,Υ) ∂Υ",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
≥ 0 for any Υ ≥ c ψ.,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
Corollary 4.,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"If D(X/Ψ) = k(Ψ) over a compact region and η small enough, then ∂∆(ψ,Υ)∂Υ ≥ 0 for all Υ excluding the boundaries of the compact region.",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"If in addition Υ > Ψ, then ∂∆(ψ,Υ)∂Υ ≥ 0 almost surely.",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
Theorem 3.,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
Assume that D(X) is continuous and that w̄ is realizable.,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"Then there are always hypotheses w ∈ H for which the expected convergence rate under D(X) is monotonically decreasing with the loss Υ of the sampled points.
",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
Proof.,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"We shift to a hyperspherical coordinate system in Rd+1 similar as before, but now the pole (origin) is fixed at wt.",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"For the gradient step s, it can be shown that:
s = − sgn (xtiwt",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"− yi)2ηxiΥ
sO = s · w̄ −wt",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"λ = ±2η λ rλ cosϑ Υ
(13)
Let ∆(Υ) denote the expected convergence rate at time t, given a fixed loss Υ. From Lemma 2
∆(Υ) = 2ηΥ",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"( E[r cosϑ/xtiwt − yi = −Υ ]−
E[r cosϑ/xtiwt − yi = Υ ]
)",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"− E[(2ηrΥ)2]
, 2ηΥQ(r, ϑ,wt)− 4η2Υ2E[r2]
If w = w̄, then Q(r, ϑ,w) = 0 from the symmetry of D(X) with respect to Ψ. From the continuity of D(X), there exists δ > 0",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"such that if ‖w − w̄‖2 < δ, then ‖Q(r, ϑ,w)−Q(r, ϑ, w̄)‖2 < ηΥE[r2], which implies that ∆(Υ) <",CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
−2η2Υ2E[r2] < 0.,CONVERGENCE RATE INCREASES WITH CURRENT LOSS,[0],[0]
"While the corollaries above apply to a rather simple situation, when using the Difficulty Score to guide SGD while
minimizing the convex regression loss, their predictions can be empirically tested with the deep learning architecture and loss which are described in Section 3.",2.3. Deep learning: simulation results,[0],[0]
"There an additional challenge is posed by the fact that the empirical ranking is not based on the ideal definition given in Def. 1, but rather on an estimate derived from another classifier.
",2.3. Deep learning: simulation results,[0],[0]
"Still, the empirical results as shown in Fig. 2 demonstrate agreement with the theoretical analysis of the linear regression loss.",2.3. Deep learning: simulation results,[0],[0]
"Specifically, in epoch 0",2.3. Deep learning: simulation results,[0],[0]
"there is a big difference between the average errors in estimating the gradient direction, which is smallest for the easiest examples and highest for the most difficult examples as predicted by Corollary 1.",2.3. Deep learning: simulation results,[0],[0]
"This difference in significantly reduced after 10 epochs, and becomes insignificant after 20 epochs, in agreement with Corollary 2.
",2.3. Deep learning: simulation results,[0],[0]
Discussion.,2.3. Deep learning: simulation results,[0],[0]
"Fig. 2 shows that the variance in the direction of the gradient step defined by easier points is significantly smaller than that defined by difficult points, especially at the beginning of training.",2.3. Deep learning: simulation results,[0],[0]
"This is advantageous when the initial point w0 does not lie in the basin of attraction of the desired global minimum w̄, and if, in agreement with Lemma 1, the pronounced shared component of the easy gradient steps points in the direction of the global minimum, or a more favorable local minimum; then the likelihood of escaping the local minimum decreases with a point’s Difficulty Score.",2.3. Deep learning: simulation results,[0],[0]
This scenario suggests another possible advantage for curriculum learning at the initial stages of training.,2.3. Deep learning: simulation results,[0],[0]
"As discussed in the introduction, a practical curriculum learning method should address two main questions: how to rank the training examples, and how to modify the sampling procedure based on this ranking.",3. Curriculum learning in deep networks,[0],[0]
Solutions to these issues are discussed in Section 3.1.,3. Curriculum learning in deep networks,[0],[0]
In Section 3.2 we discuss the empirical evaluation of our method.,3. Curriculum learning in deep networks,[0],[0]
"The main novelty of our proposed method lies in this step, where we rank the training examples by estimated difficulty in the absence of human supervision.",RANKING EXAMPLES BY KNOWLEDGE TRANSFER,[0],[0]
Difficulty is estimated based on knowledge transfer from another classifier.,RANKING EXAMPLES BY KNOWLEDGE TRANSFER,[0],[0]
"Here we investigate transfer from a more powerful learner.
",RANKING EXAMPLES BY KNOWLEDGE TRANSFER,[0],[0]
It is a common practice now to treat one of the upstream layers of a pre-trained network as a representation (or embedding) layer.,RANKING EXAMPLES BY KNOWLEDGE TRANSFER,[0],[0]
"This layer activation is then used for representing similar objects and train a simpler classifier (such as SVM, or shallower NNs) to perform a different task, related but not identical to the original task the network had been trained on.",RANKING EXAMPLES BY KNOWLEDGE TRANSFER,[0],[0]
"In computer vision such embeddings are commonly obtained by training a deep network on the recognition of a very large database such as ImageNet (Deng et al., 2009).",RANKING EXAMPLES BY KNOWLEDGE TRANSFER,[0],[0]
"These embeddings have been shown to provide better semantic representations of images (as compared to more traditional image features) in a number of related tasks, including the classification of small datasets (Sharif Razavian et al., 2014), image annotation (Donahue et al., 2015) and structured predictions (Hu et al., 2016).
",RANKING EXAMPLES BY KNOWLEDGE TRANSFER,[0],[0]
"Following this practice, the activation in the penultimate layer of a large and powerful pre-trained network is the loci of knowledge transfer from one network to another.",RANKING EXAMPLES BY KNOWLEDGE TRANSFER,[0],[0]
"Repeatedly, as in (Sharif Razavian et al., 2014), it has been shown that competitive performance can be obtained by training a shallow classifier on this representation in a new related task.",RANKING EXAMPLES BY KNOWLEDGE TRANSFER,[0],[0]
"Here we propose to use the confidence of such a classifier, e.g. the margin of an SVM classifier, as the estimator for the difficulty of each training example.",RANKING EXAMPLES BY KNOWLEDGE TRANSFER,[0],[0]
This measure is then used to sort the training data.,RANKING EXAMPLES BY KNOWLEDGE TRANSFER,[0],[0]
"We note that unlike the traditional practice of reusing a pre-trained network, here we only transfer information from one learner to another.",RANKING EXAMPLES BY KNOWLEDGE TRANSFER,[0],[0]
"The goal is to achieve a smaller classifier that can conceivably be used with simpler hardware, without depending on access to the powerful learner at test time.",RANKING EXAMPLES BY KNOWLEDGE TRANSFER,[0],[0]
"In agreement with prior art, e.g. the definition of curriculum in (Bengio et al., 2009), we investigate curriculum learning
where the scheduling of examples changes with time, giving priority to easier examples at the beginning of training.",SCHEDULING THE APPEARANCE OF TRAINING EXAMPLES,[0],[0]
"We explored two variants of the basic scheduling idea:
Fixed.",SCHEDULING THE APPEARANCE OF TRAINING EXAMPLES,[0],[0]
The distribution used to sample examples from the training data is gradually changed in fixed steps.,SCHEDULING THE APPEARANCE OF TRAINING EXAMPLES,[0],[0]
Initially all the weight is put on the easiest examples.,SCHEDULING THE APPEARANCE OF TRAINING EXAMPLES,[0],[0]
"In subsequent steps the weight of more difficult examples is gradually increased, until the final step in which the training data is sampled uniformly (or based on some prior distribution on the training set).
",SCHEDULING THE APPEARANCE OF TRAINING EXAMPLES,[0],[0]
Adaptive.,SCHEDULING THE APPEARANCE OF TRAINING EXAMPLES,[0],[0]
"Similar to the previous mode, but where the length of each step is not fixed, but is being determined adaptively based on the current loss of the training data.",SCHEDULING THE APPEARANCE OF TRAINING EXAMPLES,[0],[0]
Datasets.,EXPERIMENTAL SETUP,[0],[0]
"For evaluation we used 2 data sets: CIFAR-100 (Krizhevsky & Hinton, 2009) and STL-10 (Coates et al., 2010).",EXPERIMENTAL SETUP,[0],[0]
"In all cases, as is commonly done, the data was pre-processed using global contrast normalization; cropping and flipping were used for STL-10.
Network architecture.",EXPERIMENTAL SETUP,[0],[0]
We used convolutional Neural Networks (CNN) which excel at image classification tasks.,EXPERIMENTAL SETUP,[0],[0]
"Specifically, we used two architectures which are henceforth denoted Large and Small, in accordance with the number of parameters.",EXPERIMENTAL SETUP,[0],[0]
"The Large network is comprised of four blocks, each with two convolutional layers, ELU activation, and max-pooling.",EXPERIMENTAL SETUP,[0],[0]
"This is followed by a fully connected layer, for a total of 1,208,101 parameters.",EXPERIMENTAL SETUP,[0],[0]
"The Small network consists of only three hidden layers, for a total of 4,557 parameters.",EXPERIMENTAL SETUP,[0],[0]
"During training, we applied dropout and l2 regularization on the weights, and used either SGD or ADAM to optimize the cross-entropy loss.
",EXPERIMENTAL SETUP,[0],[0]
Scheduling mechanisms: control.,EXPERIMENTAL SETUP,[0],[0]
"As described above, our method is based on a scheduling design which favors the presentation of easier examples at the beginning of training.",EXPERIMENTAL SETUP,[0],[0]
"In order to isolate the contribution of scheduling by increasing level of difficulty as against other spurious consequences of data scheduling, we compared performance with the following control conditions: control-curriculum, identical scheduling mechanism but where the underlying ranking of the training examples is random and unrelated to estimated difficulty; and anti-curriculum, identical scheduling mechanism but favoring the more difficult examples at the beginning of training.",EXPERIMENTAL SETUP,[0],[0]
"Evidence from prior art is conflicting regarding where the benefits of curriculum learning lie, which is to be expected given the variability in the unknown sources of the curricu-
lum supervision information and its quality.",CONTROLLING FOR TASK DIFFICULTY,[0],[0]
We observed in our empirical study that the benefits depended to a large extent on the difficulty of the task.,CONTROLLING FOR TASK DIFFICULTY,[0],[0]
"We always saw faster learning at the beginning of the training process, while lower generalization error was seen only when the task was relatively difficult.",CONTROLLING FOR TASK DIFFICULTY,[0],[0]
"We therefore employed controls for the following 3 sources of task difficulty:
Inherent task difficulty.",CONTROLLING FOR TASK DIFFICULTY,[0],[0]
"To investigate this factor, we take advantage of the fact that CIFAR-100 is a hierarchical dataset with 100 classes and 20 super-classes, each including 5 member classes.",CONTROLLING FOR TASK DIFFICULTY,[0],[0]
We therefore trained a network to discriminate the 5 member classes of 2 super-classes as 2 separate tasks: ‘small mammals’ (task 1) and ‘aquatic mammals’ (task 2).,CONTROLLING FOR TASK DIFFICULTY,[0],[0]
These are expected to be relatively hard learning tasks.,CONTROLLING FOR TASK DIFFICULTY,[0],[0]
"We also trained a network to discriminate 5 random well separated classes: ‘camel’, ‘clock’, ‘bus’, ‘dolphin’ and ‘orchid’ (task 3).",CONTROLLING FOR TASK DIFFICULTY,[0],[0]
"This task is expected to be relatively easy.
",CONTROLLING FOR TASK DIFFICULTY,[0],[0]
Size of classification network.,CONTROLLING FOR TASK DIFFICULTY,[0],[0]
"For a given task, classification performance is significantly affected by the size of the network and its architecture.",CONTROLLING FOR TASK DIFFICULTY,[0],[0]
"We assume, of course, that we operate in the domain where the number of model parameters is smaller than can be justified by the training data (i.e., there is no overfit).",CONTROLLING FOR TASK DIFFICULTY,[0],[0]
We therefore used networks of different sizes in order to evaluate how curriculum learning is affected by task difficulty as determined by the network’s strength (see Fig. 3a-b).,CONTROLLING FOR TASK DIFFICULTY,[0],[0]
"In this comparative evaluation, the smaller the network is, the more difficult the task is likely to be (clearly, many other factors participate in the determination of task difficulty).
",CONTROLLING FOR TASK DIFFICULTY,[0],[0]
Regularization and optimization.,CONTROLLING FOR TASK DIFFICULTY,[0],[0]
"Regularization is used to constrain the family of hypotheses, or models, so that they possess such desirable properties as smoothness.",CONTROLLING FOR TASK DIFFICULTY,[0],[0]
Regularization effectively decreases the number of degrees of freedom in the model.,CONTROLLING FOR TASK DIFFICULTY,[0],[0]
"In fact, most optimization methods, other then vanilla stochastic gradient descent, incorporate some form of regularization and smoothing, among other inherent properties.",CONTROLLING FOR TASK DIFFICULTY,[0],[0]
Therefore the selection of optimization method also plays a role in determining the effective size of the final network.,CONTROLLING FOR TASK DIFFICULTY,[0],[0]
"Fig. 3a shows typical results when training the Large CNN (see network’s details above) to classify a subset of 5 CIFAR100 images (task 1 as defined above), using slow learning rate and Adam optimization.",RESULTS,[0],[0]
"In this setup we see that curriculum learning speeds up the learning rate at the beginning of the training, but converges to the same performance as regular training.",RESULTS,[0],[0]
"When we make learning more difficulty by using the Small network, performance naturally decreases, but now we see that curriculum learning also improves the final generalization performance (Fig. 3b).
",RESULTS,[0],[0]
"Similar results are shown for the STL-10 dataset (Fig. 3c).
",RESULTS,[0],[0]
"Fig. 4 shows comparative results when controlling for inherent task difficulty in the 3 tasks described above, using faster learning rate and SGD optimization.",RESULTS,[0],[0]
Task difficulty can be evaluated in retrospect from the final performance seen in each plot.,RESULTS,[0],[0]
"As can be clearly seen in the figure, the improvement in final accuracy with curriculum learning is larger when the task is more difficult.",RESULTS,[0],[0]
"When manipulating the level of regularization, we see that while too much regularization always harms performance, curriculum learning is least affected by this degradation (results are omitted).",RESULTS,[0],[0]
"We investigated curriculum learning, an extension of stochastic gradient descent in which easy examples are more frequently sampled at the beginning of training.",4. Summary and Discussion,[0],[0]
"We started
with the theoretical investigation of this strict definition in the context of linear regression, showing that curriculum learning accelerates the learning rate in agreement with prior empirical evidence.",4. Summary and Discussion,[0],[0]
"While not shedding light on its affect on the classifier’s final performance, our analysis suggests that the direction of a gradient step based on ”easy” examples may be more effective in traversing the input space towards the ideal minimum of the loss function.",4. Summary and Discussion,[0],[0]
"Specifically, we have empirically shown that the variance in the gradient direction of points increases with their difficulty when optimizing a non-convex loss function.",4. Summary and Discussion,[0],[0]
"Over-sampling the more coherent easier examples may therefore increase the likelihood to escape the basin of attraction of a low quality local minimum in favor of higher quality local minima even in the general non-convex case.
",4. Summary and Discussion,[0],[0]
"We also showed theoretically that when the difficulty score of the training points is fixed, convergence is faster if the loss with respect to the current hypothesis is higher.",4. Summary and Discussion,[0],[0]
"This seems to be a very intuitive result, an intuition that underlies the boosting method for example.",4. Summary and Discussion,[0],[0]
"However, as intuitive as it might be, this is not always true when the prior data density is assumed to be continuous and when the optimal hypothesis is realizable.",4. Summary and Discussion,[0],[0]
"Thus the requirement that the difficulty score is fixed is necessary.
",4. Summary and Discussion,[0],[0]
In the second part of this paper we described a curriculum learning method for deep networks.,4. Summary and Discussion,[0],[0]
The method relies on knowledge transfer from other (pre-trained) networks in order to rank the training examples by difficulty.,4. Summary and Discussion,[0],[0]
We described extensive experiments where we evaluated our proposed method under different task difficulty conditions and against a variety of control conditions.,4. Summary and Discussion,[0],[0]
"In all cases curriculum learning has been shown to increase the rate of convergence at the beginning of training, in agreement with the theoretical results.",4. Summary and Discussion,[0],[0]
"With more difficult tasks, curriculum learning improved generalization performance.",4. Summary and Discussion,[0],[0]
This work was supported in part by a grant from the Israel Science Foundation (ISF) and by the Gatsby Charitable Foundations.,Acknowledgements,[0],[0]
We provide theoretical investigation of curriculum learning in the context of stochastic gradient descent when optimizing the convex linear regression loss.,abstractText,[0],[0]
We prove that the rate of convergence of an ideal curriculum learning method is monotonically increasing with the difficulty of the examples.,abstractText,[0],[0]
"Moreover, among all equally difficult points, convergence is faster when using points which incur higher loss with respect to the current hypothesis.",abstractText,[0],[0]
We then analyze curriculum learning in the context of training a CNN.,abstractText,[0],[0]
"We describe a method which infers the curriculum by way of transfer learning from another network, pre-trained on a different task.",abstractText,[0],[0]
"While this approach can only approximate the ideal curriculum, we observe empirically similar behavior to the one predicted by the theory, namely, a significant boost in convergence speed at the beginning of training.",abstractText,[0],[0]
"When the task is made more difficult, improvement in generalization performance is also observed.",abstractText,[0],[0]
"Finally, curriculum learning exhibits robustness against unfavorable conditions such as excessive regularization.",abstractText,[0],[0]
Curriculum Learning by Transfer Learning: Theory and Experiments with Deep Networks,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 570–575 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
570",text,[0],[0]
Building Artificial Intelligence (AI) algorithms to teach machines to read and to comprehend text is a long-standing challenge in Natural Language Processing (NLP).,1 Introduction,[0],[0]
A common strategy for assessing these AI algorithms is by treating them as RC tasks.,1 Introduction,[0],[0]
This can be formulated as finding an answer to a question given the document(s) as evidence.,1 Introduction,[0],[0]
"Recently, many deep-learning based models (Seo et al., 2017; Xiong et al., 2017; Wang et al., 2017; Shen et al., 2017; Clark and Gardner, 2017) have been proposed to solve RC tasks based on the SQuAD (Rajpurkar et al., 2016) and TriviaQA (Joshi et al., 2017) datasets, reaching human level performance.",1 Introduction,[0],[0]
"A common approach in these models is to score and/or extract candidate spans conditioned on a given question-document pair.
",1 Introduction,[0],[0]
Most of these models have limited applicability to real problems for the following reasons.,1 Introduction,[0],[0]
"They do not generalize well to scenarios where the answer is not present as a span, or where several discontinuous parts of the document are required to
∗ To whom correspondence should be addressed.
",1 Introduction,[0],[0]
form the answer.,1 Introduction,[0],[0]
"In addition, unlike humans, they can not easily skip through irrelevant parts to comprehend long documents (Masson, 1983).
",1 Introduction,[0],[0]
"To address the issues above we develop a novel context zoom-in network (ConZNet) for RC tasks, which can skip through irrelevant parts of a document and generate an answer using only the relevant regions of text.",1 Introduction,[0],[0]
The ConZNet architecture consists of two phases.,1 Introduction,[0],[0]
In the first phase we identify the relevant regions of text by employing a reinforcement learning algorithm.,1 Introduction,[0],[0]
"These relevant regions are not only useful to generate the answer, but can also be presented to the user as supporting information along with the answer.",1 Introduction,[0],[0]
"The second phase is based on an encoder-decoder architecture, which comprehends the identified regions of text and generates the answer by using a residual self-attention network as encoder and a RNNbased sequence generator along with a pointer network (Vinyals et al., 2015) as the decoder.",1 Introduction,[0],[0]
"It has the ability to generate better well-formed answers not verbatim present in the document than span prediction models.
",1 Introduction,[0],[0]
"Recently, there have been several attempts to adopt condensing documents in RC tasks.",1 Introduction,[0],[0]
Wang et al. (2018) retrieve a relevant paragraph based on the question and predict the answer span.,1 Introduction,[0],[0]
Choi et al. (2017) select sentence(s) to make a summary of the entire document with a feed-forward network and generate an answer based on the summary.,1 Introduction,[0],[0]
"Unlike existing approaches, our method has the ability to select relevant regions of text not just based on the question but also on how well regions are related to each other.",1 Introduction,[0],[0]
"Moreover, our decoder combines span prediction and sequence generation.",1 Introduction,[0],[0]
"This allows the decoder to copy words from the relevant regions of text as well as to generate words from a fixed vocabulary.
",1 Introduction,[0],[0]
"We evaluate our model using one of the challenging RC datasets, called ‘NarrativeQA’, which
was released recently by Kočiskỳ et al. (2017).",1 Introduction,[0],[0]
Experimental results show the usefulness of our framework for RC tasks and we outperform stateof-the-art results on this dataset.,1 Introduction,[0],[0]
"An overview of our architecture is shown in Figure 1, which consists of two phases.",2 Proposed Architecture,[0],[0]
"First, the identification of relevant regions of text is computed by the Co-attention and Context Zoom layers as explained in Sections 2.1 and 2.2.",2 Proposed Architecture,[0],[0]
"Second, the comprehension of identified regions of text and output generation is computed by Answer Generation block as explained in Section 2.3.",2 Proposed Architecture,[0],[0]
"The words in the document, question and answer are represented using pre-trained word embeddings (Pennington et al., 2014).",2.1 Co-attention layer,[0],[0]
These wordbased embeddings are concatenated with their corresponding char embeddings.,2.1 Co-attention layer,[0],[0]
"The char embeddings are learned by feeding all the characters of a word into a Convolutional Neural Network (CNN) (Kim, 2014).",2.1 Co-attention layer,[0],[0]
"We further encode the document and question embeddings using a shared bi-directional GRU (Cho et al., 2014) to get context-aware representations.
",2.1 Co-attention layer,[0],[0]
We compute the co-attention between document and question to get question-aware representations for the document by using tri-linear attention as proposed by Seo et al. (2017).,2.1 Co-attention layer,[0],[0]
"Let di be the vector representation for the document word i, qj be the vector for the question word j, and ld and lq be the lengths of the document and question respectively.",2.1 Co-attention layer,[0],[0]
"The tri-linear attention is calculated as
aij = wddi + wqqj + wdq(di qj), (1)
where wd, wq, and wdq are learnable parameters and denotes the element-wise multiplication.
",2.1 Co-attention layer,[0],[0]
We compute the attended document word d̃i by first computing λi = softmax(ai:) and followed by d̃i = ∑lq j=1 λijqj .,2.1 Co-attention layer,[0],[0]
"Similarly, we compute a question to document attention vector q̃ by first computing b = softmax(max(ai:)) and followed by q̃",2.1 Co-attention layer,[0],[0]
= ∑ld i=i dibi.,2.1 Co-attention layer,[0],[0]
"Finally, di, d̃i, di d̃i, d̃i q̃ are concatenated to yield a query-aware contextual representation for each word in the document.",2.1 Co-attention layer,[0],[0]
This layer finds relevant regions of text.,2.2 Context Zoom Layer,[0],[0]
"We use reinforcement learning to do that, with the goal of improving answer generation accuracy – see Section 2.4.
",2.2 Context Zoom Layer,[0],[0]
The Split Context operation splits the attended document vectors into sentences or fixed size chunks (useful when sentence tokenization is not available for a particular language).,2.2 Context Zoom Layer,[0],[0]
"This results in n text regions with each having length lk,",2.2 Context Zoom Layer,[0],[0]
where ld = ∑n k=1 lk.,2.2 Context Zoom Layer,[0],[0]
"We then get the representations, denoted as zk, for each text region by running a BiGRU and concatenating the last states of the forward and backward GRUs.
",2.2 Context Zoom Layer,[0],[0]
"The text region representations, zk, encode how well they are related to the question, and their surrounding context.",2.2 Context Zoom Layer,[0],[0]
"Generating an answer may depend on multiple regions, and it is important for
each text region to collect cues from other regions which are outside of their surroundings.",2.2 Context Zoom Layer,[0],[0]
We can compute this by using a Self-Attention layer.,2.2 Context Zoom Layer,[0],[0]
"It is a special case of co-attention where both operands (di and qj) are the text fragment itself, computed by setting aij = −∞ when i = j in Eq. 1.
",2.2 Context Zoom Layer,[0],[0]
"These further self-attended text region representations, z̃k, are passed through a linear layer with tanh activation and softmax layer as follows:
u = tanh(Wc[z̃1, · · · , z̃n] + bc), (2) ψ = softmax(u), (3)
where ψ is the probability distribution of text regions, which is the evidence used to generate the answer.",2.2 Context Zoom Layer,[0],[0]
"The policy of the reinforcement learner is defined as π(r|u; θz) = ψr, where ψr is the probability of a text region r (agent’s action) being selected, u is the environment state as defined in Eq. 2, and θz are the learnable parameters.",2.2 Context Zoom Layer,[0],[0]
"During the training time we sample text regions using ψ, in inference time we follow greedy evaluation by selecting most probable region(s).",2.2 Context Zoom Layer,[0],[0]
"This component is implemented based on the encoder-decoder architecture of (Sutskever et al., 2014).",2.3 Answer Generation,[0],[0]
"The selected text regions from the Context Zoom layer are given as input to the encoder, where its output is given to the decoder in order to generate the answer.
",2.3 Answer Generation,[0],[0]
The encoder block uses residual connected selfattention layer followed by a BiGRU.,2.3 Answer Generation,[0],[0]
"The selected relevant text regions (∈ ψr) are first passed through a separate BiGRU, then we apply a selfattention mechanism similar to the Context Zoom layer followed by a linear layer with ReLU activations.",2.3 Answer Generation,[0],[0]
"The encoder’s output consists of representations of the relevant text regions, denoted by ei.
",2.3 Answer Generation,[0],[0]
"The decoder block is based on an attention mechanism (Bahdanau et al., 2015) and a copy mechanism by using a pointer network similar to (See et al., 2017).",2.3 Answer Generation,[0],[0]
This allows the decoder to predict words from the relevant regions as well as from the fixed vocabulary.,2.3 Answer Generation,[0],[0]
"At time step t, the decoder predicts the next word in the answer using the attention distribution, context vector and current word embedding.",2.3 Answer Generation,[0],[0]
"The attention distribution and context vector are obtained as follows:
oti = v T tanh(Weei +Whht + bo), (4)
γt = softmax(oti), (5)
where ht is hidden state of the decoder, v, We, Wh, bo are learnable parameters.",2.3 Answer Generation,[0],[0]
The γt represents a probability distribution over words of relevant regions ei.,2.3 Answer Generation,[0],[0]
"The context vector is given by ct = ∑ i γ t iei.
",2.3 Answer Generation,[0],[0]
"The probability distribution to predict word wt from the fixed vocabulary (Pfv) is computed by passing state ht and context vector ct to a linear layer followed by a softmax function denoted as
Pfv = softmax(Wv(Xv[ht, ct] + bp)+ bq).",2.3 Answer Generation,[0],[0]
"(6)
To allow decoder to copy words from the encoder sequence, we compute a soft gate (Pcopy), which helps the decoder to generate a word by sampling from the fixed vocabulary or by copying from a selected text regions (ψr).",2.3 Answer Generation,[0],[0]
"The soft gate is calculated as
Pcopy = σ(w T p ct + v T h",2.3 Answer Generation,[0],[0]
"ht + w T x xt + bc), (7)
where xt is current word embedding, ht is hidden state of the decoder, ct is the context vector, and wp, vh, wx, and bc are learnable parameters.",2.3 Answer Generation,[0],[0]
We maintain a list of out-of-vocabulary (OOV) words for each document.,2.3 Answer Generation,[0],[0]
The fixed vocabulary along with this OOV list acts as an extended vocabulary for each document.,2.3 Answer Generation,[0],[0]
"The final probability distribution (unnormalized) over this extended vocabulary (Pev) is given by
Pev(wt) =",2.3 Answer Generation,[0],[0]
"(1−Pcopy)Pfv(wt)+Pcopy ∑
i:wi=wt
γti .
(8)",2.3 Answer Generation,[0],[0]
"We jointly estimate the parameters of our model coming from the Co-attention, Context Zoom, and Answer Generation layers, which are denoted as θa, θz , and θg respectively.",2.4 Training,[0],[0]
"Estimating θa and θg is straight-forward by using the cross-entropy objective J1({θa, θg}) and the backpropagation algorithm.",2.4 Training,[0],[0]
"However, selecting text regions in the Context Zoom layer makes it difficult to estimate θz
given their discrete nature.",2.4 Training,[0],[0]
We therefore formulate the estimation of θz as a reinforcement learning problem via a policy gradient method.,2.4 Training,[0],[0]
"Specifically, we design a reward function over θz .
",2.4 Training,[0],[0]
"We use mean F-score of ROUGE-1, ROUGE-2, and ROUGE-L (Lin and Hovy, 2003) as our reward function R.",2.4 Training,[0],[0]
"The objective function to maximize is the expected reward under the probability distribution of current text regions ψr, i.e., J2(θz) = Ep(r|θz)[R].",2.4 Training,[0],[0]
"We approximate the gradient ∇θzJ2(θz) by following the REINFORCE (Williams, 1992) algorithm.",2.4 Training,[0],[0]
To reduce the high variance in estimating∇θzJ2(θz) one widely used mechanism is to subtract a baseline value from the reward.,2.4 Training,[0],[0]
"It is shown that any number will reduce the variance (Williams, 1992; Zaremba and Sutskever, 2015), here we used the mean of the mini-batch reward b as our baseline.",2.4 Training,[0],[0]
"The final objective is to minimize the following equation:
J(θ) = J1({θa, θg})−J2(θz)+ B∑ i=1",2.4 Training,[0],[0]
"(Ri−b), (9)
where, B is the size of mini-batch, and Ri is the reward of example i ∈",2.4 Training,[0],[0]
B. J(θ) is now fully differentiable and we use backpropagation to estimate θ.,2.4 Training,[0],[0]
"The NarrativeQA dataset (Kočiskỳ et al., 2017) consists of fictional stories gathered from books and movie scripts, where corresponding summaries and question-answer pairs are generated with the help of human experts and Wikipedia articles.",3.1 Dataset,[0],[0]
The summaries in NarrativeQA are 4-5 times longer than documents in the SQuAD dataset.,3.1 Dataset,[0],[0]
"Moreover, answers are well-formed by human experts and are not verbatim in the story, thus making this dataset ideal for testing our model.",3.1 Dataset,[0],[0]
The statistics of NarrativeQA are available in Table 11.,3.1 Dataset,[0],[0]
"We compare our model against reported models in Kočiskỳ et al. (2017) (Seq2Seq, ASR, BiDAF) and the Multi-range Reasoning Unit (MRU) in Tay et al. (2018).",3.2 Baselines,[0],[0]
"We implemented two baseline models (Baseline 1, Baseline 2) with Context Zoom layer similar to Wang et al. (2018).",3.2 Baselines,[0],[0]
In both baselines we replace the span prediction layer with an answer generation layer.,3.2 Baselines,[0],[0]
"In Baseline 1 we use an
1please refer Kočiskỳ et al. (2017) for more details
attention based seq2seq layer without using copy mechanism in the answer generation unit similar to Choi et al. (2017).",3.2 Baselines,[0],[0]
In Baseline 2 the answer generation unit is similar to our ConZNet architecture.,3.2 Baselines,[0],[0]
"We split each document into sentences using the sentence tokenizer of the NLTK toolkit (Bird and Loper, 2004).",3.3 Implementation Details,[0],[0]
"Similarly, we further tokenize each sentence, corresponding question and answer using the word tokenizer of NLTK.",3.3 Implementation Details,[0],[0]
"The model is implemented using Python and Tensorflow (Abadi et al., 2015).",3.3 Implementation Details,[0],[0]
"All the weights of the model are initialized by Glorot Initialization (Glorot et al., 2011) and biases are initialized with zeros.",3.3 Implementation Details,[0],[0]
"We use a 300 dimensional word vectors from GloVe (Pennington et al., 2014) (with 840 billion pre-trained vectors) to initialize the word embeddings, which we kept constant during training.",3.3 Implementation Details,[0],[0]
"All the words that do not appear in Glove are initialized by sampling from a uniform random distribution between [-0.05, 0.05].",3.3 Implementation Details,[0],[0]
"We apply dropout (Srivastava et al., 2014) between the layers with keep probability of 0.8 (i.e dropout=0.2).",3.3 Implementation Details,[0],[0]
The number of hidden units are set to 100.,3.3 Implementation Details,[0],[0]
"We trained our model with the AdaDelta (Zeiler, 2012) optimizer for 50 epochs, an initial learning rate of 0.1, and a minibatch size of 32.",3.3 Implementation Details,[0],[0]
The hyperparameter ‘sample size’ (number of relevant sentences) is chosen based on the model performance on the devset.,3.3 Implementation Details,[0],[0]
Table 2 shows the performance of various models on NarrativeQA.,3.4 Results,[0],[0]
It can be noted that our model with sample size 5 (choosing 5 relevant sentences) outperforms the best ROUGE-L score available so far by 12.62% compared to Tay et al. (2018).,3.4 Results,[0],[0]
"The low performance of Baseline 1 shows that the hybrid approach (ConZNet) for generating words from a fixed vocabulary as well as copying words from the document is better suited than span prediction models (Seq2Seq, ASR, BiDAF, MRU).
",3.4 Results,[0],[0]
"To validate the importance of finding relevant sentences in contrast to using an entire document for answer generation, we experimented with sample sizes beyond 5.",3.4 Results,[0],[0]
The performance of our model gradually dropped from sample size 7 onwards.,3.4 Results,[0],[0]
"This result shows evidence that only a few relevant sentences are sufficient to answer a question.
",3.4 Results,[0],[0]
"We also experimented with various sample sizes to see the effect of intra sentence relations for an-
swer generation.",3.4 Results,[0],[0]
The performance of the model improved dramatically with sample sizes 3 and 5 compared to the sample size of 1.,3.4 Results,[0],[0]
These results show that the importance of selecting multiple relevant sentences for generating an answer.,3.4 Results,[0],[0]
"In addition, the low performance of Baseline 2 indicates that just selecting multiple sentences is not enough, they should also be related to each other.",3.4 Results,[0],[0]
This result points out that the self-attention mechanism in the Context zoom layer is an important component to identify related relevant sentences.,3.4 Results,[0],[0]
We have proposed a new neural-based architecture which condenses an original document to facilitate fast comprehension in order to generate better well-formed answers than span based prediction models.,4 Conclusion,[0],[0]
Our model achieved the best performance on the challenging NarrativeQA dataset.,4 Conclusion,[0],[0]
"Future work can focus for example on designing an inexpensive preprocess layer, and other strategies for improved performance on answer generation.",4 Conclusion,[0],[0]
In recent years many deep neural networks have been proposed to solve Reading Comprehension (RC) tasks.,abstractText,[0],[0]
Most of these models suffer from reasoning over long documents and do not trivially generalize to cases where the answer is not present as a span in a given document.,abstractText,[0],[0]
We present a novel neural-based architecture that is capable of extracting relevant regions based on a given question-document pair and generating a well-formed answer.,abstractText,[0],[0]
"To show the effectiveness of our architecture, we conducted several experiments on the recently proposed and challenging RC dataset ‘NarrativeQA’.",abstractText,[0],[0]
"The proposed architecture outperforms state-of-the-art results (Tay et al., 2018) by 12.62% (ROUGE-L) relative improvement.",abstractText,[0],[0]
Cut to the Chase: A Context Zoom-in Network for Reading Comprehension,title,[0],[0]
"1 INTRODUCTION
Dance Dance Revolution (DDR) is a popular rhythm-based video game with millions of players worldwide (Hoysniemi, 2006). Players perform steps atop a dance platform, containing four buttons, each labeled with an arrow. An on-screen step chart prompts players to step on the buttons at specific, musically salient points in time. Scores depend upon both hitting the right buttons and hitting them at the right time. Step charts vary in difficulty with harder charts containing more steps and more complex sequences.
Despite the game’s popularity, players have some reasonable complaints: For one, packs are limited to songs with favorable licenses, meaning players may be unable to dance to their favorite songs. Even when charts are available, players may tire of repeatedly performing the same charts. Although players can produce their own charts, the process is painstaking and requires significant expertise.
This paper introduces learning to choreograph, the task of producing a step chart from raw audio. We break the problem into two subtasks: First, step placement consists of identifying a set of timestamps in the song at which to place steps. This process can be conditioned on a user-specified difficulty level. Second, step selection consists of choosing which steps to place at each timestamp. Running these two steps in sequence yields a playable step chart (Figure 1). 1
For both prediction stages of learning to choreograph, we demonstrate the superior performance of neural networks over strong alternatives. Our best model for step placement jointly learns convolutional neural network (CNN) representations and a recurrent neural network (RNN), which
1 Demonstration video showing human choreography and the output of Dance Dance Convolution side-byside: https://youtu.be/yUc3O237p9M
integrates information across consecutive time slices. Our best model for step selection consists of a conditional LSTM generative model which receives high-level rhythm features as auxiliary information.",text,[0],[0]
"Before applying our step placement algorithms, we transform raw audio samples into perceptuallyinformed representations.",2 METHODS,[0],[0]
Music files arrive as lossy encodings at 44.1kHz .,2 METHODS,[0],[0]
We decode the audio files into stereo PCM and average the two channels to produce a monophonic representation.,2 METHODS,[0],[0]
"We then compute a multiple-timescale short-time Fourier transform (STFT) using window lengths of 23ms , 46ms , and 93ms and a stride of 10ms .",2 METHODS,[0],[0]
We reduce the dimensionality of the STFT magnitude spectrum by applying a Mel-scale filterbank yielding 80 frequency bands.,2 METHODS,[0],[0]
Then we scale the filter outputs logarithmically in accordance with human perception of loudness.,2 METHODS,[0],[0]
"Finally, we prepend and append seven frames of past and future context to each frame.
",2 METHODS,[0],[0]
"2.1 STEP PLACEMENT
We consider several models to address the step placement task.",2 METHODS,[0],[0]
Each model’s output consists of a single sigmoid unit which estimates the probability that a step is placed.,2 METHODS,[0],[0]
"For all models, we augment the audio features with a one-hot representation of difficulty.
",2 METHODS,[0],[0]
"Following state-of-the-art work on musical onset detection (Schlüter & Böck, 2014), we adopt a convolutional neural network (CNN) architecture.",2 METHODS,[0],[0]
This model consists of two convolutional layers followed by two fully connected layers.,2 METHODS,[0],[0]
Our first convolutional layer has 10 filter kernels that are 7-wide in time and 3-wide in frequency.,2 METHODS,[0],[0]
The second layer has 20 filter kernels that are 3-wide in time and 3-wide in frequency.,2 METHODS,[0],[0]
"We apply 1D max-pooling after each convolutional layer, only in the frequency dimension, with a width and stride of 3.",2 METHODS,[0],[0]
Both convolutional layers use rectified linear units (ReLU) (Glorot et al.).,2 METHODS,[0],[0]
"Following the convolutional layers, we add two fully connected layers with ReLU activation functions and 256 and 128 nodes respectively.
",2 METHODS,[0],[0]
"To improve upon the CNN, we propose a C-LSTM model (Figure 2), combining a convolutional encoding with an LSTM-RNN (Hochreiter & Schmidhuber, 1997) that integrates information across longer windows of time.",2 METHODS,[0],[0]
Our C-LSTM contains two convolutional layers (of the same shape as the CNN) applied across the full unrolling length.,2 METHODS,[0],[0]
"The output of the second convolutional layer is a 3D tensor, which we flatten along the channel and frequency axes (preserving the temporal dimension).",2 METHODS,[0],[0]
The flattened features at each time step then become the inputs to a two-layer LSTM with 200 nodes per layer.,2 METHODS,[0],[0]
We train this model using 100 unrollings for backpropagation through time.,2 METHODS,[0],[0]
We treat the step selection task as a sequence generation problem.,2.2 STEP SELECTION,[0],[0]
"Our approach follows related work in language modeling where RNNs are well-known to produce coherent text that captures long-range relationships (Mikolov et al., 2010; Sutskever et al., 2011; Sundermeyer et al., 2012).
",2.2 STEP SELECTION,[0],[0]
Our LSTM model passes over the ground truth step placements and predicts the next token given the previous sequence of tokens.,2.2 STEP SELECTION,[0],[0]
The output is a softmax distribution over the game’s 256 possible steps.,2.2 STEP SELECTION,[0],[0]
"As inputs, we use a more compact bag-of-arrows representation containing 16 features (4 per
arrow) to depict the previous step.",2.2 STEP SELECTION,[0],[0]
"For each arrow, the 4 corresponding features represent the states on, off, hold, and release.",2.2 STEP SELECTION,[0],[0]
We add an additional feature that functions as a start token to denote the first step of a chart.,2.2 STEP SELECTION,[0],[0]
"For this task, our LSTM consists of 2 layers of 128 cells each.",2.2 STEP SELECTION,[0],[0]
"We use 64 steps of unrolling, an average of 100 seconds for the easiest charts and 9 seconds for the hardest.
To inform our LSTM of the non-uniform rhythmic spacing of the step placements, we provide the following two pieces of auxiliary information: (1) ∆-beat adds two features representing the number of beats since the previous and until the next step; (2) beat phase adds four features representing which sixteenth note subdivision of the beat the current step most closely aligns to.",2.2 STEP SELECTION,[0],[0]
"We collected a dataset consisting of 203 songs, labeled by 9 annotators.",3 EXPERIMENTS,[0],[0]
"One particularly prolific annotator, Fraxtil, annotated 90 of these songs for all five difficulty levels.",3 EXPERIMENTS,[0],[0]
The remaining songs are from a large multi-author collection called In The Groove (ITG).,3 EXPERIMENTS,[0],[0]
"In total, across all five difficulty settings, we obtain around 35 hours of annotated audio and 350, 000 steps.",3 EXPERIMENTS,[0],[0]
"2 We augment our dataset for step selection by synthesizing mirror images of each chart (i.e., interchanging left and right) which we found to improve performance for all models
For step placement, we compare the performance of our proposed CNN and C-LSTM models against a logistic regressor (LogReg) and a 2-layer MLP.",3 EXPERIMENTS,[0],[0]
"For step selection, we compare our proposed LSTM model against a fixed-window MLP and an n-gram model using modified Kneser-Ney smoothing (Chen & Goodman, 1998) with backoff.",3 EXPERIMENTS,[0],[0]
Both the MLP (MLP5) and n-gram model (KN5) predict the next step from four steps of history and the MLP received the same auxiliary information as the LSTM.,3 EXPERIMENTS,[0],[0]
"We also show the performance of an LSTM model trained with only 5 steps of unrolling (LSTM5) to demonstrate the advantage of longer context.
",3 EXPERIMENTS,[0],[0]
"Model Dataset PPL AUC F-score
LogReg Fraxtil 1.205 0.601 0.609 MLP Fraxtil 1.097 0.659 0.665 CNN Fraxtil 1.082 0.671 0.678 C-LSTM Fraxtil 1.070 0.682 0.681
LogReg ITG 1.123 0.599 0.634 MLP ITG 1.090 0.637 0.671 CNN ITG 1.083 0.677 0.689 C-LSTM ITG 1.072 0.680 0.697
Table 1: Perplexity, area under curve and Fscore assessed for the step placement task.
",3 EXPERIMENTS,[0],[0]
"Model Dataset PPL Acc.
",3 EXPERIMENTS,[0],[0]
KN5 Fraxtil 3.681 0.528 MLP5 Fraxtil 3.428 0.557 LSTM5 Fraxtil 3.185 0.581 LSTM64,3 EXPERIMENTS,[0],[0]
"Fraxtil 3.011 0.613
KN5 ITG 5.847 0.356",3 EXPERIMENTS,[0],[0]
"MLP5 ITG 4.786 0.401 LSTM5 ITG 4.447 0.441 LSTM64 ITG 4.342 0.444
Table 2: Perplexity and per-token accuracy assessed for the step selection task.
",3 EXPERIMENTS,[0],[0]
"Our experiments demonstrate that on both the step placement (Table 1) and the step selection (Table 2) tasks, deep neural network models outperform traditional baselines.",3 EXPERIMENTS,[0],[0]
"For the step placement task, the best performing method by all metrics is the C-LSTM.",3 EXPERIMENTS,[0],[0]
"For the step selection task, LSTMs outperform other models.
",3 EXPERIMENTS,[0],[0]
Data augmentation and the inclusion of ∆-beat and beat phase side information give a significant increase in performance to both the MLP and LSTMs for step selection.,3 EXPERIMENTS,[0],[0]
"For example, the LSTM64 model trained on the Fraxtil dataset without side information or data augmentation only achieves a PPL of 3.526 and accuracy of 0.562.",3 EXPERIMENTS,[0],[0]
Step selection models perform better on the single-author Fraxtil dataset in comparison to the multi-author ITG.,3 EXPERIMENTS,[0],[0]
Author style tends to be distinctive and thus a collection of single-author sequences is more predictable.,3 EXPERIMENTS,[0],[0]
"A few prior systems attempt automatic synthesis of step charts (O’Keeffe, 2003; Nogaj, 2005), however neither establishes a reproducible evaluation methodology or learns the semantics of steps from data.",4 RELATED WORK,[0],[0]
"The most closely related work to our step placement task is concerned with onset detection
2All data shall be made available at publication time
(Bello et al., 2005; Dixon, 2006), which has previously been attempted with deep neural networks (Eyben et al., 2010; Schlüter & Böck, 2014).",4 RELATED WORK,[0],[0]
Our step selection task most closely resembles conditional language modeling.,4 RELATED WORK,[0],[0]
"A recent wave of work in RNNs for language modeling began with (Mikolov et al., 2010; Sutskever et al., 2011).",4 RELATED WORK,[0],[0]
"Inspired by this work, several recent papers extend the methods to polyphonic music generation and transcription (Boulanger-Lewandowski et al., 2012; Chu et al., 2016; Sigtia et al., 2016).",4 RELATED WORK,[0],[0]
"To our knowledge, ours is the first paper to attempt end-to-end DDR choreography from raw audio with deep learning.",4 RELATED WORK,[0],[0]
Dance Dance Revolution (DDR) is a popular rhythm-based video game.,abstractText,[0],[0]
Players perform steps on a dance platform in synchronization with music as directed by on-screen step charts.,abstractText,[0],[0]
"While many step charts are available in standardized packs, users may grow tired of existing charts, or wish to dance to a song for which no chart exists.",abstractText,[0],[0]
We introduce the task of learning to choreograph.,abstractText,[0],[0]
"Given a raw audio track, the goal is to produce a new step chart.",abstractText,[0],[0]
This task decomposes naturally into two subtasks: deciding when to place steps and deciding which steps to select.,abstractText,[0],[0]
We demonstrate deep learning solutions for both tasks and establish strong benchmarks for future work.,abstractText,[0],[0]
