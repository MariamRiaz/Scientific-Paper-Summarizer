63	32	The resulting filled-in Mad Lib is usually funny, because players fill in the blanks with no knowledge of the story (except for its title).
65	19	Figure 1 shows part of a filled-in Mad Lib created from a story describing the theft of the Mona Lisa.
67	52	The title and words sur- rounding the blanks in a Mad Lib provide a contextual scaffolding that an algorithm can exploit to choose appropriate words for the blanks that make the resulting story humorous.
70	56	Without looking at the story, our algorithm would be reduced to one that chooses only a priori funny words.
71	33	Mad Libs are copyrighted, and therefore it is difficult to release a data set by using stories from Mad Libs books.
81	35	Next, we created our dataset of 50 Fun Libs using simple Wikipedia articles because, similar to Mad Libs, these articles have a title and text.
92	21	Finding good judges is challenging, because humor is subjective.
98	28	Turkers were allowed to select, for each story, a grade from {0,1,2,3} (a scale which we used throughout our work) described as follows: 0 - Not funny 2 - Moderately funny 1 - Somewhat funny 3 - Funny We marked the ground-truth grade of the Wikipedia excerpts as 0, and we used volunteers from our research group to decide the groundtruth grade of the other four stories between one and three.
131	53	We also announced bonuses for the top 10 judges selected based on other judges’ agreements with them and the top 10 players based on how funny their filled-in stories were, as graded by the judges.
151	21	To predict whether a filled-in word is funny or not, our classifier uses the following ten features extracted from the (word, story) pair: 1.
152	31	Language model’s joint probability for the containing sentence with the word filled-in.
155	63	The purpose of these features is to capture the phonetic funniness of words (e.g., “whacking” instead of “fighting”).
160	20	We split the data randomly by story titles, keeping 30 for training and 10 in a validation set.
163	36	Further, we assigned labels using a vast majority vote, i.e., a filled-in word having, out of nine judges’ votes: - six or more “funny votes” is funny - three or fewer funny votes is not funny Otherwise the word was discarded.
171	19	We show results from three baseline classifiers in Table 3.
172	32	Among these, the “Chance” classifiers always predict the most frequent class found in the training set, and “Chance Hint” predicts the most frequent class for each hint type.
173	24	The other baseline is a Linear SVM classifier trained using the three most important features of the trained random forest as shown in Figure 2.
174	25	The SVM’s learned weights for the similarity features between the word and the containing sentence, the entire story, and the title, respectively, were −0.449, −0.883 and 1.344, showing that funnier filled-in words are similar to the title but not to the body of the story.
178	39	LMC (Language Model + Multiple Choice): for each blank, players choose a word from up to 20 candidate words generated by the language model and sorted by their joint probability score4.
179	19	Libitum: Similar to the LMC method, except here we rank all the words up to the top 500 words generated by the language model using our classifier, keeping up to the top 20 “humorous” candidates4.
182	24	For evaluation, we used our ten test Fun Libs, and for each of our three approaches, each of the ten stories was completed by three players.
186	19	In only one story (ID = 21), the Libitum model receives a lower mean grade than the LMC model, suggesting that adding the machine learning to the language model helps generate significantly more humor than the language model alone.
189	19	Interestingly, the two stories that received the highest mean grade (“Batman” and “Ducks”) are from the FreeText format.
210	80	The “Beauty Contest” story shows the outstanding skills of humans in generating humor when two blanks are directly connected to each other (i.e., “brawler” and “deadly”).
212	27	With the computer aided approaches, it is quite difficult to suggest candidates for pairs of (or more) blanks such that the choices are coherent.
219	22	We also tested an automated Mad Lib humor generation system, where we filled-in each test blank with the most funny candidate word from Libitum.
227	139	For each blank, Libitum supplied a list of potentially funny words from which a human could choose.
228	110	As judged by humans, the Libitum-aided words easily worked better than words from a simple language model and were usually better than even words generated by human players who could fill in the blanks in whichever way they liked.
229	45	Our three contributions, the benchmark, Libitum, and the analysis of what makes it funny, advance the state of the art in computer humor by demonstrating a successful computer aided humor technique and quantitatively analyzing what makes for funny fill-in-the-blank words for Mad Libs.
230	113	These analyses show that coherent stories have tremendous potential in making a Mad Lib humorous, a promising direction for future work.
