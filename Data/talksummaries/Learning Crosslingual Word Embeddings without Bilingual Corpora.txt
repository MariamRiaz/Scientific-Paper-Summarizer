0	39	Monolingual word embeddings have had widespread success in many NLP tasks including sentiment analysis (Socher et al., 2013), dependency parsing (Dyer et al., 2015), machine translation (Bahdanau et al., 2014).
1	31	Crosslingual word embeddings are a natural extension facilitating various crosslingual tasks, e.g. through transfer learning.
2	70	A model built in a source resource-rich language can then applied to the target resource poor languages (Yarowsky and Ngai, 2001; Das and Petrov, 2011; Täckström et al., 2012; Duong et al., 2015).
4	27	Crosslingual word embeddings are a natural remedy where both source and target language lexicon are presented as dense vectors in the same vector space (Klementiev et al., 2012).
8	29	Søgaard et al. (2015) impose a less onerous data condition in the form of linked Wikipedia entries across several languages, however this approach tends to underperform other methods.
12	18	Our model uses a bilingual dictionary from Panlex (Kamholz et al., 2014) as the source of bilingual signal.
15	33	In addition to the dictionary, 1285 our method only requires monolingual data.
16	77	Our approach is an extension of the continuous bag-ofwords (CBOW) model (Mikolov et al., 2013b) to inject multilingual training signal based on dictionary translations.
17	38	We experiment with several variations of our model, whereby we predict only the translation or both word and its translation and consider different ways of using the different learned center-word versus context embeddings in application tasks.
21	21	Notably, our embedding combining techniques are general, yielding improvements also for monolingual word embedding.
68	20	The negative examples are drawn from combined vocabulary unigram distribution calculated from combined data De ∪Df .
71	18	The use of U, on the other hand, is understudied (Levy and Goldberg, 2014) with the exception of Pennington et al. (2014) who use a linear combination U + V, with minor improvement over V alone.
80	17	The simple question is, how to combine both V and U to produce a better representation.
86	21	For each word in the combined dictionary Ve ∪ Vf , we encourage the model to learn similar representation in both V and U by adding a regularization term to the objective function in equation (3) during training.
101	19	The translations in PanLex come from various sources such as glossaries, dictionaries, automatic inference from other languages, etc.
103	37	Given a word in a source language, the bilingual lexicon induction (BLI) task is to predict its translation in the target language.
109	44	Qualitative evaluation We jointly train the model to predict both wi and the translation w̄i, combine V and U during training for each language pair.
110	24	Table 2 shows the top 10 closest words in both source and target languages according to cosine similarity.
111	22	Note that the model correctly identifies the translation in en as the top candidate, and the top 10 words in both source and target languages are highly related.
122	27	Our joint model, as described in equation (3) which predicts both target word and the translation, further improves the performance, especially for nl-en.
142	23	Our combined model out-performed both Luong et al. (2015) and Gouws and Søgaard (2015)6 which represent the best published crosslingual embeddings trained on bitext and monolingual data respectively.
151	18	Other improvements can be explained by the observation that a dictionary can improve monolingual accuracy through linking synonyms (Faruqui and Dyer, 2014).
152	21	For example, since plane, airplane and aircraft have the same Italian translation aereo, the model will encourage those words to be closer in the embedding space.
169	24	This is convenient for a target low-resource language where we do not have document annotations.
177	22	Despite its simplicity, our model achieves competitive performance.
192	39	Previous CLWE methods often impose high resource requirements yet have low accuracy.
194	35	We model polysemy using EM translation selection during training to learn bilingual correspondences from monolingual corpora.
195	30	Our algorithm allows to train on massive amount of monolingual data efficiently, representing monolingual and bilingual properties of language.
196	43	This allows us to achieve state-of-the-art performance on bilingual lexicon induction task, competitive result on monolingual word similarity and crosslingual document classification task.
197	157	Our combination techniques during training, especially using regularization, are highly effective and could be used to improve monolingual word embeddings.
