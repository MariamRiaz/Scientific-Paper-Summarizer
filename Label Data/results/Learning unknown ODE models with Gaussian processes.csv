0,1,label2,summary_sentences
"Decades of research have been dedicated to heuristics for speeding up inference in natural language processing tasks, such as constituency parsing (Pauls and Klein, 2009; Caraballo and Charniak, 1998) and machine translation (Petrov et al., 2008; Xu et al., 2013).",1 Introduction,[0],[0]
"Such research is necessary because of a trend toward richer models, which improve accuracy at the cost of slower inference.",1 Introduction,[0],[0]
"For example, state-of-theart constituency parsers use grammars with millions of rules, while dependency parsers routinely use millions of features.",1 Introduction,[0],[0]
"Without heuristics, these parsers take minutes to process a single sentence.
",1 Introduction,[0],[0]
"To speed up inference, we will learn a pruning policy.",1 Introduction,[0],[0]
"During inference, the pruning policy is invoked to decide whether to keep or prune various parts of the search space, based on features of the input and (potentially) the state of the inference process.
",1 Introduction,[0],[0]
"Our approach searches for a policy with maximum end-to-end performance (reward) on training data, where the reward is a linear combination of problemspecific measures of accuracy and runtime, namely reward = accuracy−λ · runtime.",1 Introduction,[0],[0]
"The parameter λ ≥ 0
specifies the relative importance of runtime and accuracy.",1 Introduction,[0],[0]
"By adjusting λ, we obtain policies with different speed-accuracy tradeoffs.
",1 Introduction,[0],[0]
"For learning, we use Locally Optimal Learning to Search (LOLS) (Chang et al., 2015b), an algorithm for learning sequential decision-making policies, which accounts for the end-to-end performance of the entire decision sequence jointly.",1 Introduction,[0],[0]
"Unfortunately, executing LOLS naively in our setting is prohibitive because it would run inference from scratch millions of times under different policies, training examples, and variations of the decision sequence.",1 Introduction,[0],[0]
"Thus, this paper presents efficient algorithms for repeated inference, which are applicable to a wide variety of NLP tasks, including parsing, machine translation and sequence tagging.",1 Introduction,[0],[0]
"These algorithms, based on change propagation and dynamic programming, dramatically reduce time spent evaluating similar decision sequences by leveraging problem structure and sharing work among evaluations.
",1 Introduction,[0],[0]
We evaluate our approach by learning pruning heuristics for constituency parsing.,1 Introduction,[0],[0]
"In this setting, our approach is the first to account for end-to-end performance of the pruning policy, without making independence assumptions about the reward function, as in prior work (Bodenstab et al., 2011).",1 Introduction,[0],[0]
"In the larger context of learning-to-search for structured prediction, our work is unusual in that it learns to control a dynamic programming algorithm (i.e., graphbased parsing) rather than a greedy algorithm (e.g., transition-based parsing).",1 Introduction,[0],[0]
Our experiments show that accounting for end-to-end performance in training leads to better policies along the entire Pareto frontier of accuracy and runtime.,1 Introduction,[0],[0]
"A simple yet effective approach to speeding up parsing was proposed by Bodenstab et al. (2011), who trained a pruning policy π to classify whether or not spans of the input sentence w1 · · ·wn form plausible
263
Transactions of the Association for Computational Linguistics, vol. 5, pp.",2 Weighted CKY with pruning,[0],[0]
"263–278, 2017.",2 Weighted CKY with pruning,[0],[0]
Action Editor: Marco Kuhlmann.,2 Weighted CKY with pruning,[0],[0]
"Submission batch: 5/2016; Revision batch: 9/2016; Published 8/2017.
",2 Weighted CKY with pruning,[0],[0]
c©2017 Association for Computational Linguistics.,2 Weighted CKY with pruning,[0],[0]
"Distributed under a CC-BY 4.0 license.
constituents based on features of the input sentence.",2 Weighted CKY with pruning,[0],[0]
"These predictions enable a parsing algorithm, such as CKY, to skip expensive steps during its execution: unlikely constituents are pruned.",2 Weighted CKY with pruning,[0],[0]
"Only plausible constituents are kept, and the parser assembles the highest-scoring parse from the available constituents.
",2 Weighted CKY with pruning,[0],[0]
Alg. 1 provides pseudocode for weighted CKY with pruning.,2 Weighted CKY with pruning,[0],[0]
"Weighted CKY aims to find the highestscoring derivation (parse tree) of a given sentence, where a given grammar specifies a non-negative score for each derivation rule and a derivation’s score is the product of the scores of the rules it uses.1 CKY uses a dynamic programming strategy to fill in a three-dimensional array β, known as the chart.",2 Weighted CKY with pruning,[0],[0]
The score βikx is the score of the highest-scoring subderivation with fringe wi+1 . . .,2 Weighted CKY with pruning,[0],[0]
wk and root,2 Weighted CKY with pruning,[0],[0]
x.,2 Weighted CKY with pruning,[0],[0]
This value is computed by looping over the possible ways to assemble such a subderivation from smaller subderivations with scores βijy and βjkz (lines 17–22).,2 Weighted CKY with pruning,[0],[0]
"Additionally, we track a witness (backpointer) for each βikx, so that we can easily reconstruct the corresponding subderivation at line 23.",2 Weighted CKY with pruning,[0],[0]
"The chart is initialized with lexical grammar rules (lines 3–9), which derive words from grammar symbols.
",2 Weighted CKY with pruning,[0],[0]
"The key difference between pruned and unpruned CKY is an additional “if” statement (line 14), which queries the pruning policy π to decide whether to compute the several values βikx associated with a span (i, k).",2 Weighted CKY with pruning,[0],[0]
Note that width-1 and width-n spans are always kept because all valid parses require them.,2 Weighted CKY with pruning,[0],[0]
Bodenstab et al. (2011) train their pruning policy as a supervised classifier of spans.,3 End-to-end training,[0],[0]
"They derive direct supervision as follows: try to keep a span if it appears in the gold-standard parse, and prune it otherwise.",3 End-to-end training,[0],[0]
They found that using an asymmetric weighting scheme helped find the right balance between false positives and false negatives.,3 End-to-end training,[0],[0]
"Intuitively, failing to prune is only a slight slowdown, whereas pruning a good item can ruin the accuracy of the parse.
",3 End-to-end training,[0],[0]
"1As is common practice, we assume the grammar has been binarized.",3 End-to-end training,[0],[0]
"We focus on pre-trained grammars, leaving coadaptation of the grammar and pruning policy to future work.",3 End-to-end training,[0],[0]
"As indicated at lines 6 and 19, a rule’s score may be made to depend on the context in which that rule is applied (Finkel et al., 2008), although the pre-trained grammars in our present experiments are ordinary PCFGs for which this is not the case.
",3 End-to-end training,[0],[0]
"Algorithm 1 PARSE: Weighted CKY with pruning 1: Input: grammar G, sentence w, policy π
Output: completed chart β, derivation d 2: .",3 End-to-end training,[0],[0]
"Initialize chart 3: β := 0 4: for k := 1 to n : 5: for x such that (x→ wk) ∈ rules(G) : 6: s := G(x→ wk | w, k) 7: if s > βk−1,k,x : 8: βk−1,k,x := s 9: witness(k−1, k, x) := (k−1, k, wk)
10: for width := 2 to n : 11: for i := 0 to n− width : 12: k := i+ width .",3 End-to-end training,[0],[0]
"Current span is (i, k) 13: .",3 End-to-end training,[0],[0]
"Policy determines whether to fill in this span 14: if π(w, i, k) = prune : 15: continue 16: .",3 End-to-end training,[0],[0]
Fill in span by considering each split point j 17: for j := i+ 1 to k,3 End-to-end training,[0],[0]
"− 1 : 18: for (x→ y z) ∈ rules(G) : 19: s := βijy ·βjkz ·G(x→ y z | w, i, j, k) 20: if s > βikx : 21: βikx := s 22: witness(i, k, x) := (j, y, z) 23: d̂ := follow backpointers from (0, n,ROOT) 24: return (β, d̂)
",3 End-to-end training,[0],[0]
"Our end-to-end training approach improves upon asymmetric weighting by jointly evaluating the sequence of pruning decisions, measuring its effect on the test-time evaluation metric by actually running pruned CKY (Alg. 1).",3 End-to-end training,[0],[0]
"To estimate the value of a pruning policy π, we call PARSE(G,w(i), π) on each training sentence w(i), and apply the reward function, r = accuracy−λ · runtime.",3 End-to-end training,[0],[0]
"The empirical value of a policy is its average reward on the training set:
R(π) = 1 m
m∑
i=1
E",3 End-to-end training,[0],[0]
"[ r(PARSE(G,w(i), π)) ]",3 End-to-end training,[0],[0]
"(1)
The expectation in the definition may be dropped if PARSE, π, and r are all deterministic, as in our setting.2 Our definition of r depends on the user parameter λ ≥ 0, which specifies the amount of accuracy the user would sacrifice to save one unit of
2Parsers may break ties randomly or use Monte Carlo methods.",3 End-to-end training,[0],[0]
"The reward function r can be nondeterministic when it involves wallclock time or human judgments.
runtime.",3 End-to-end training,[0],[0]
"Training under a range of values for λ gives rise to policies covering a number of operating points along the Pareto frontier of accuracy and runtime.
",3 End-to-end training,[0],[0]
End-to-end training gives us a principled way to decide what to prune.,3 End-to-end training,[0],[0]
"Rather than artificially labeling each pruning decision as inherently good or bad, we evaluate its effect in the context of the particular sentence and the other pruning decisions.",3 End-to-end training,[0],[0]
"Actions that prune a gold constituent are not equally bad—some cause cascading errors, while others are “worked around” in the sense that the grammar still selects a mostly-gold parse.",3 End-to-end training,[0],[0]
"Similarly, actions that prune a non-gold constituent are not equally good—some provide more overall speedup (e.g., pruning narrow constituents prevents wider ones from being built), and some even improve accuracy by suppressing an incorrect but high-scoring parse.
",3 End-to-end training,[0],[0]
"More generally, the gold vs. non-gold distinction is not even available in NLP tasks where one is pruning potential elements of a latent structure, such as an alignment (Xu et al., 2013) or a finer-grained parse (Matsuzaki et al., 2005).",3 End-to-end training,[0.9593816498006676],"['Furthermore, an interesting future avenue is the study of various vector field kernels, such as divergence-free, curl-free or spectral kernels (Remes et al., 2017).']"
"Yet our approach can still be used in such settings, by evaluating the reward on the downstream task that the latent structure serves.
",3 End-to-end training,[0],[0]
Past work on optimizing end-to-end performance is discussed in §8.,3 End-to-end training,[0],[0]
"One might try to scale these techniques to learning to prune, but in this work we take a different approach.",3 End-to-end training,[0],[0]
"Given a policy, we can easily find small ways to improve it on specific sentences by varying individual pruning actions (e.g., if π currently prunes a span then try keeping it instead).",3 End-to-end training,[0],[0]
"Given a batch of improved action sequences (trajectories), the remaining step is to search for a policy which produces the improved trajectories.",3 End-to-end training,[0],[0]
"Conveniently, this can be reduced to a classification problem, much like the asymmetric weighting approach, except that the supervised labels and misclassification costs are not fixed across iterations, but rather are derived from interaction with the environment (i.e., PARSE and the reward function).",3 End-to-end training,[0],[0]
"This idea is formalized as a learning algorithm called Locally Optimal Learning to Search (Chang et al., 2015b), described in §4.
",3 End-to-end training,[0],[0]
The counterfactual interventions we require— evaluating how reward would change if we changed one action—can be computed more efficiently using our novel algorithms (§5) than by the default strategy of running the parser repeatedly from scratch.,3 End-to-end training,[0],[0]
"The key is to reuse work among evaluations, which is
possible because LOLS only makes tiny changes.",3 End-to-end training,[0],[0]
Pruned inference is a sequential decision process.,4 Learning algorithm,[0],[0]
The process begins in an initial state s0.,4 Learning algorithm,[0],[0]
"In pruned CKY, s0 specifies the state of Alg.",4 Learning algorithm,[0],[0]
"1 at line 10, after the chart has been initialized from some selected sentence.",4 Learning algorithm,[0],[0]
"Next, the policy is invoked to choose action a0 = π(s0)—in",4 Learning algorithm,[0],[0]
our case at line 14—which affects what the parser does next.,4 Learning algorithm,[0],[0]
"Eventually the parser reaches some state s1 from which it calls the policy to choose action a1 = π(s1), and so on.",4 Learning algorithm,[0],[0]
"When the policy is invoked at state st, it selects action at based on features extracted from the current state st—a snapshot of the input sentence, grammar and parse chart at time t.3",4 Learning algorithm,[0],[0]
"We call the state-action sequence s0 a0 s1 a1 · · · sT a trajectory, where T is the trajectory length.",4 Learning algorithm,[0],[0]
"At the final state, the reward function is evaluated, r(sT ).
",4 Learning algorithm,[0],[0]
The LOLS algorithm for learning a policy is given in Alg.,4 Learning algorithm,[0],[0]
"2,4 with a graphical illustration in Fig. 1.",4 Learning algorithm,[0],[0]
"At a high level, LOLS alternates between evaluating and improving the current policy πi.
",4 Learning algorithm,[0],[0]
"The evaluation phase first samples a trajectory from πi, called a roll-in: s0 a0 s1 a1 · · · sT ∼ ROLL-IN(πi).",4 Learning algorithm,[0],[0]
"In our setting, s0 is derived from a randomly sampled training sentence, but the rest of the trajectory is then deterministically computed by πi given s0.",4 Learning algorithm,[0],[0]
"Then we revisit each state s in the roll-in (line 7), and try each available action ā∈A(s)",4 Learning algorithm,[0],[0]
"(line 9), executing πi thereafter—a rollout—to measure the resulting reward r̂[ā] (line 10).",4 Learning algorithm,[0],[0]
"Our parser is deterministic, so a single rollout is an unbiased, 0-variance estimate of the expected reward.",4 Learning algorithm,[0],[0]
"This process is repeated many times, yielding a large list Q̂i of pairs 〈s, r̂〉, where s is a state that was encountered in some roll-in and r̂ maps the possible actions A(s) in that state to their measured rewards.
",4 Learning algorithm,[0],[0]
"The improvement phase now trains a new policy πi+1 to try to choose high-reward actions, seeking a policy that will “on average” get high rewards r[πi+1(s)].",4 Learning algorithm,[0],[0]
"Good generalization is important: the policy must select high-reward actions even in states s that are not represented in Q̂i, in case they are
3Our experiments do not make use of the current state of the chart.",4 Learning algorithm,[0],[0]
"We discuss this decision in §8.
4Alg.",4 Learning algorithm,[0],[0]
"2 is simpler than in Chang et al. (2015b) because it omits oracle rollouts, which we do not use in our experiments.
",4 Learning algorithm,[0],[0]
Algorithm 2 LOLS algorithm for learning to prune.,4 Learning algorithm,[0],[0]
1: π1 := INITIALIZEPOLICY(. . . ),4 Learning algorithm,[0],[0]
2: for i := 1 to number of iterations : 3: .,4 Learning algorithm,[0],[0]
Evaluate: Collect dataset for πi 4: Q̂i := ∅ 5: for j := 1 to minibatch size : 6: s0 a0 s1 a1 · · · sT ∼ ROLL-IN(πi) .,4 Learning algorithm,[0],[0]
Sample 7: for t := 0 to T−1 : 8: .,4 Learning algorithm,[0],[0]
Intervene: Evaluate each action at st 9: for āt ∈ A(st) : .,4 Learning algorithm,[0],[0]
"Possible actions
10: r̂t[āt] ∼ ROLLOUT(πi, st, āt) 11: Q̂i.append(〈st, r̂t 〉) 12: .",4 Learning algorithm,[0],[0]
"Improve: Train with dataset aggregation
13: πi+1 ← TRAIN",4 Learning algorithm,[0],[0]
(,4 Learning algorithm,[0],[0]
"⋃i k=1 Q̂k )
14: .",4 Learning algorithm,[0],[0]
Finalize: Pick the best policy over all iterations 15: return argmaxi′ R(πi′) encountered when running the new policy πi+1 (or when parsing test sentences).,4 Learning algorithm,[0],[0]
"Thus, beyond just regularizing the training objective, we apply dataset aggregation (Ross et al., 2011): we take the training set to include not just Q̂i but also the examples from previous iterations (line 13).",4 Learning algorithm,[0],[0]
"This also ensures that the sequence of policies π1, π2, . .",4 Learning algorithm,[0],[0]
".will be “stable” (Ross and Bagnell, 2011) and will eventually converge.
",4 Learning algorithm,[0],[0]
"So line 13 seeks to find a good classifier πi+1 using a training set: a possible classifier π would receive from each training example 〈s, r̂〉 a reward of r̂[π(s)].",4 Learning algorithm,[0],[0]
"In our case, where A(s) = {keep, prune}, this cost-sensitive classification problem is equivalent to training an ordinary binary classifier, after converting each training example 〈s, r̂〉 to 〈s, argmaxa",4 Learning algorithm,[0],[0]
"r̂[a]〉 and giving this example a weight of |r̂t,keep− r̂t,prune|.",4 Learning algorithm,[0],[0]
"Our specific classifier is described in §6.
",4 Learning algorithm,[0],[0]
"In summary, the evaluation phase of LOLS collects training data for a cost-sensitive classifier, where the
inputs (states), outputs (actions), and costs are obtained by interacting with the environment.",4 Learning algorithm,[0],[0]
"LOLS concocts a training set and repeatedly revises it, similar to the well-known Expectation-Maximization algorithm.",4 Learning algorithm,[0],[0]
This enables end-to-end training of systems with discrete decisions and nondecomposable reward functions.,4 Learning algorithm,[0],[0]
LOLS gives us a principled framework for deriving (nonstationary) “supervision” even in tricky cases such as latent-variable inference (mentioned in §3).,4 Learning algorithm,[0],[0]
"LOLS has strong theoretical guarantees, though in pathological cases, it may take exponential time to converge (Chang et al., 2015b).
",4 Learning algorithm,[0],[0]
"The inner loop of the evaluation phase performs roll-ins, interventions and rollouts.",4 Learning algorithm,[0],[0]
Roll-ins ensure that the policy is (eventually) trained under the distribution of states it tends to encounter at test time.,4 Learning algorithm,[0],[0]
Interventions and rollouts force πi to explore the effect of currently disfavored actions.,4 Learning algorithm,[0],[0]
"Unlike most applications of LOLS and related algorithms, such as SEARN (Daumé III, 2006) and DAGGER (Ross et al., 2011), executing the policy is a major bottleneck in training.",5 Efficient rollouts,[0],[0]
"Because our dynamic programming parser explores many possibilities (unlike a greedy, transition-based decoder) its trajectories are quite long.",5 Efficient rollouts,[0],[0]
"This not only slows down each rollout: it means we must do more rollouts.
",5 Efficient rollouts,[0],[0]
"In our case, the trajectory has length T = n·(n+1)
2",5 Efficient rollouts,[0],[0]
− 1− n,5 Efficient rollouts,[0],[0]
"for a sentence of length n, where T is also the number of pruning decisions: one for each span other than the root and width-1 spans.",5 Efficient rollouts,[0],[0]
LOLS must then perform T rollouts on this example.,5 Efficient rollouts,[0],[0]
"This means that to evaluate policy πi, we must parse each sentence in the minibatch hundreds of times (e.g., 189 for n=20, 434 for n=30, and 779 for n=40).
",5 Efficient rollouts,[0],[0]
"We can regard each policy π as defining a pruning
mask m, an array that maps each of the T spans (i, k) to a decision mik (1 = keep, 0 = prune).",5 Efficient rollouts,[0],[0]
"Each rollout tries flipping a different bit in this mask.
",5 Efficient rollouts,[0],[0]
We could spend less time on each sentence by sampling only some of its T rollouts (see §6).,5 Efficient rollouts,[0],[0]
"Regardless, the rollouts we do on a given sentence are related: in this section we show how to get further speedups by sharing work among them.",5 Efficient rollouts,[0],[0]
"In §5.2, we leverage the fact that rollouts will be similar to one another (differing by a single pruning decision).",5 Efficient rollouts,[0],[0]
"In §5.3, we show that the reward of all T rollouts can be computed simultaneously by dynamic programming under some assumptions about the structure of the reward function (described later).",5 Efficient rollouts,[0],[0]
We found these algorithms to be crucial to training in a “reasonable” amount of time (see the empirical comparison in §7.2).,5 Efficient rollouts,[0],[0]
"It is convenient to present our efficient rollout algorithms in terms of the hypergraph structure of Alg. 1 (Klein and Manning, 2001; Huang, 2008; Li and Eisner, 2009; Eisner and Blatz, 2007).",5.1 Background: Parsing as hypergraphs,[0],[0]
A hypergraph describes the information flow among related quantities in a dynamic programming algorithm.,5.1 Background: Parsing as hypergraphs,[0],[0]
"Many computational tricks apply generically to hypergraphs.
",5.1 Background: Parsing as hypergraphs,[0],[0]
A hypergraph edge e (or hyperedge) is a “generalized arrow” e.head ≺ e.Tail with one output and a list of inputs.,5.1 Background: Parsing as hypergraphs,[0],[0]
"We regard each quantity βikx,mik, or G(. . .)",5.1 Background: Parsing as hypergraphs,[0],[0]
"in Alg. 1 as the value of a corresponding hypergraph vertex β̇ikx, ṁik, or Ġ(. . .).",5.1 Background: Parsing as hypergraphs,[0],[0]
"Thus, value(v̇) = v for any vertex v̇. Each ṁik’s value is computed by the policy π or chosen by a rollout intervention.",5.1 Background: Parsing as hypergraphs,[0],[0]
"Each Ġ’s value is given by the grammar.
",5.1 Background: Parsing as hypergraphs,[0],[0]
"Values of β̇ikx, by contrast, are computed at line 19 if k − i > 1.",5.1 Background: Parsing as hypergraphs,[0],[0]
"To record the dependence of βikx on other quantities, our hypergraph includes the hyperedge β̇ikx ≺",5.1 Background: Parsing as hypergraphs,[0],[0]
"(β̇ijy, β̇jkz, ṁik, ġ) for each 0 ≤",5.1 Background: Parsing as hypergraphs,[0],[0]
i < j < k ≤ n,5.1 Background: Parsing as hypergraphs,[0],[0]
"and (x→ y z) ∈ rules(G), where ġ denotes the vertex Ġ(x→ y z | w, i, j, k).
",5.1 Background: Parsing as hypergraphs,[0],[0]
"If k − i = 1, then values of βikx are instead computed at line 6, which does not access any other β values or the pruning mask.",5.1 Background: Parsing as hypergraphs,[0],[0]
"Thus our hypergraph includes the hyperedge vikx ≺(ġ) whenever i = k−1, 0 ≤",5.1 Background: Parsing as hypergraphs,[0],[0]
"i < k ≤ n, and (x→ wk) ∈ rules(G), with ġ = Ġ(x→ wk | w, k).
",5.1 Background: Parsing as hypergraphs,[0],[0]
"With this setup, the value βikx is the maximum score of any derivation of vertex β̇ikx (a tree rooted at β̇ikx, representing a subderivation), where the score
of a derivation is the product of its leaf values.",5.1 Background: Parsing as hypergraphs,[0],[0]
Alg. 1 computes it by considering hyperedges β̇ikx ≺ T and the previously computed values of the vertices in the tail T .,5.1 Background: Parsing as hypergraphs,[0],[0]
"For a vertex v̇, we write In(v̇) and Out(v̇) for its sets of incoming and outgoing hyperedges.",5.1 Background: Parsing as hypergraphs,[0],[0]
"Our algorithms follow these hyperedges implicitly, without the overhead of materializing or storing them.",5.1 Background: Parsing as hypergraphs,[0],[0]
"Change propagation is an efficient method for incrementally re-evaluating a computation under a change to its inputs (Acar and Ley-Wild, 2008; Filardo and Eisner, 2012).",5.2 Change propagation (CP),[0],[0]
"In our setting, each roll-in at Alg. 2 line 6 evaluates the reward r(PARSE(G, xi, π)) from (1), which involves computing an entire parse chart via Alg. 1.",5.2 Change propagation (CP),[0],[0]
"The inner loop at line 10 performs T interventions per roll-in, which ask how reward would have changed if one bit in the pruning maskm had been different.",5.2 Change propagation (CP),[0],[0]
"Rather than reparsing from scratch (T times) to determine this, we can simply adjust the initial roll-in computation (T times).
",5.2 Change propagation (CP),[0],[0]
CP is efficient when only a small fraction of the computation needs to be adjusted.,5.2 Change propagation (CP),[0],[0]
"In principle, flipping a single pruning bit can change up to 50% of the chart, so one might expect the bookkeeping overhead of CP to outweigh the gains.",5.2 Change propagation (CP),[0],[0]
"In practice, however, 90% of the interventions change < 10% of the β values in the chart.",5.2 Change propagation (CP),[0],[0]
"The reason is that βikx is a maximum over many quantities, only one of which “wins.”",5.2 Change propagation (CP),[0],[0]
"Changing a given βijy rarely affects this maximum, and so changes are unlikely to propagate from vertex β̇ijy to β̇ikx.",5.2 Change propagation (CP),[0],[0]
"Since changes are not very contagious, the “epidemic of changes” does not spread far.
",5.2 Change propagation (CP),[0],[0]
Alg. 3 provides pseudocode for updating the highest-scoring derivation found by Alg. 1.,5.2 Change propagation (CP),[0],[0]
"We remark that the RECOMPUTE is called only when we flip a bit from keep to prune, which removes hyperedges and potentially decreases vertex values.",5.2 Change propagation (CP),[0],[0]
"The reverse flip only adds hyperedges, which increases vertex values via a running max (lines 12–14).
",5.2 Change propagation (CP),[0],[0]
"After determining the effect of flipping a bit, we must restore the original chart before trying a different bit (the next rollout).",5.2 Change propagation (CP),[0],[0]
The simplest approach is to call Alg. 3 again to flip the bit,5.2 Change propagation (CP),[0],[0]
"back.5
5Our implementation uses a slightly faster method which accumulates an “undo list” of changes that it makes to the chart to quickly revert the modified chart to the original roll-in state.
",5.2 Change propagation (CP),[0],[0]
Algorithm 3 Change propagation algorithm 1: Global: Alg.,5.2 Change propagation (CP),[0],[0]
"1’s vertex values/witnesses (roll-in) 2: procedure CHANGE(v̇, v) 3: .",5.2 Change propagation (CP),[0],[0]
Change the value of a leaf vertex v̇ to v 4: value(v̇) := v ; witness(v̇) = LEAF 5: Q := ∅; Q.push(v̇) .,5.2 Change propagation (CP),[0],[0]
Work queue (“agenda”) 6: while Q 6= ∅ : .,5.2 Change propagation (CP),[0],[0]
Propagate until convergence 7: u̇,5.2 Change propagation (CP),[0],[0]
:= Q.pop() .,5.2 Change propagation (CP),[0],[0]
Narrower constituents first 8: if witness(u̇) = NULL : .,5.2 Change propagation (CP),[0],[0]
Value is unknown 9: RECOMPUTE(u̇) .,5.2 Change propagation (CP),[0],[0]
"Get value & witness
10: for e ∈ Out(u̇) : .",5.2 Change propagation (CP),[0],[0]
Propagate new value of u̇ 11: ṡ := e.head; s := ∏ u̇′∈e.,5.2 Change propagation (CP),[0],[0]
"Tail value(u̇
′) 12: if s > value(ṡ) : .",5.2 Change propagation (CP),[0],[0]
Increase value 13: value(ṡ) := s; witness(ṡ) := e 14: Q.push(ṡ) 15: else if witness(ṡ) = e and s < value(ṡ): 16: witness(ṡ) := NULL .Value,5.2 Change propagation (CP),[0],[0]
may decrease 17: Q.push(ṡ) .,5.2 Change propagation (CP),[0],[0]
"so, recompute upon pop 18: procedure RECOMPUTE(ṡ) 19: for e ∈ In(ṡ) : .",5.2 Change propagation (CP),[0],[0]
Max over incoming hyperedges 20: s := ∏ u̇∈e.,5.2 Change propagation (CP),[0],[0]
Tail value(u̇) 21: if s > value(ṡ) : 22: value(ṡ) = s; witness(ṡ) =,5.2 Change propagation (CP),[0],[0]
e,5.2 Change propagation (CP),[0],[0]
The naive rollout algorithm runs the parser T times— once for each variation of the pruning mask.,5.3 Dynamic programming (DP),[0],[0]
"The reader may be reminded of the finite difference approximation to the gradient of a function, which also measures the effects from perturbing each input value individually.",5.3 Dynamic programming (DP),[0.9660425216814622],"['The sensitivity equation based approach is superior to the finite differences approximation because we have exact formulation for the gradients of state over inducing points, which can be solved up to the numerical accuracy of the ODE solver.']"
"In fact, for certain reward functions, the naive algorithm can be precisely regarded as computing a gradient—and thus we can use a more efficient algorithm, back-propagation, which finds the entire gradient vector of reward as fast (in the big-O sense) as computing the reward once.",5.3 Dynamic programming (DP),[0],[0]
"The overall algorithm is O(|E| + T ) where |E| is the total number of hyperedges, whereas the naive algorithm is O(|E′|·T ) where |E′| ≤ |E| is the maximum number of hyperedges actually visited on any rollout.
",5.3 Dynamic programming (DP),[0],[0]
What accuracy measure must we use?,5.3 Dynamic programming (DP),[0],[0]
Let r(d) denote the recall of a derivation d—the fraction of gold constituents that appear as vertices in the derivation.,5.3 Dynamic programming (DP),[0],[0]
"A simple accuracy metric would be 1-best recall, the recall r(d̂) of the highest-scoring derivation d̂ that was not pruned.",5.3 Dynamic programming (DP),[0],[0]
"In this section, we relax that to ex-
pected recall,6 r̄= ∑
d p(d)r(d).",5.3 Dynamic programming (DP),[0],[0]
"Here we interpret the pruned hypergraph’s values as an unnormalized probability distribution over derivations, where the probability p(d) =",5.3 Dynamic programming (DP),[0],[0]
p̃(d)/Z of a derivation is proportional to its score p̃(d) =,5.3 Dynamic programming (DP),[0],[0]
"∏ u̇∈leaves(d) value(u̇).
",5.3 Dynamic programming (DP),[0],[0]
"Though r̄ is not quite our evaluation metric, it captures more information about the parse forest, and so may offer some regularizing effect when used in a training criterion (see §7.1).",5.3 Dynamic programming (DP),[0],[0]
"In any case, r̄ is close to r(d̂) when probability mass is concentrated on a few derivations, which is common with heavy pruning.
",5.3 Dynamic programming (DP),[0],[0]
"We can re-express r̄ as r̃/Z, where
r̃ = ∑
d
p̃(d)r(d) Z = ∑
d
p̃(d) (2)
These can be efficiently computed by dynamic programming (DP), specifically by a variant of the inside algorithm (Li and Eisner, 2009).",5.3 Dynamic programming (DP),[0],[0]
"Since p̃(d) is a product of rule weights and pruning mask bits at d’s leaves (§5.1), each appearing at most once, both r̃ and Z vary linearly in any one of these inputs provided that all other inputs are held constant.",5.3 Dynamic programming (DP),[0],[0]
"Thus, the exact effect on r̃ or Z of changing an input mik can be found from the partial derivatives with respect to it.",5.3 Dynamic programming (DP),[0],[0]
"In particular, if we increased mik by ∆ ∈ {−1, 1} (to flip this bit), the new value of r̄ would be exactly
r̃ + ∆ · ∂r̃/∂mik",5.3 Dynamic programming (DP),[0],[0]
"Z + ∆ · ∂Z/∂mik
(3)
",5.3 Dynamic programming (DP),[0],[0]
It remains to compute these partial derivatives.,5.3 Dynamic programming (DP),[0],[0]
"All partials can be jointly computed by back-propagation, which equivalent to another dynamic program known as the outside algorithm (Eisner, 2016).
",5.3 Dynamic programming (DP),[0],[0]
"The inside algorithm only needs to visit the |E′| unpruned edges, but the outside algorithm must also visit some pruned edges, to determine the effect of “unpruning” them (changing their mik input from 0 to 1) by finding ∂r̃/∂mik and ∂Z/∂mik.",5.3 Dynamic programming (DP),[0],[0]
"On the other hand, these partials are 0 when some other input to the hyperedge is 0.",5.3 Dynamic programming (DP),[0],[0]
"This case is common when the hypergraph is heavily pruned (|E′| |E|), and means that back-propagation need not descend further through that hyperedge.
",5.3 Dynamic programming (DP),[0],[0]
"6In theory, we could anneal from expected to 1-best recall (Smith and Eisner, 2006).",5.3 Dynamic programming (DP),[0],[0]
"We experimented extensively with annealing but found it to be too numerically unstable for our purposes, even with high-precision arithmetic libraries.
",5.3 Dynamic programming (DP),[0],[0]
Note that the DP method computes only the accuracies of rollouts—not the runtimes.,5.3 Dynamic programming (DP),[0],[0]
"In this paper, we will combine DP with a very simple runtime measure that is trivial to roll out (see §7).",5.3 Dynamic programming (DP),[0],[0]
An alternative would be to use CP to roll out the runtimes.,5.3 Dynamic programming (DP),[0],[0]
"This is very efficient: to measure just runtime, CP only needs to update the record of which constituents or edges are built, and not their scores, so the changes are easier to compute than in §5.2, and peter out more quickly.
6 Parser details7
Setup: We use the standard English parsing setup: the Penn Treebank (Marcus et al., 1993) with the standard train/dev/test split, and standard tree normalization.8 For efficiency during training, we restrict the length of sentences to ≤ 40.",5.3 Dynamic programming (DP),[0],[0]
We do not restrict the length of test sentences.,5.3 Dynamic programming (DP),[0],[0]
"We experiment with two grammars: coarse, the “no frills” left-binarized treebank grammar, and fine, a variant of the Berkeley split-merge level-6 grammar (Petrov et al., 2006) as provided by Dunlop (2014, ch. 5).",5.3 Dynamic programming (DP),[0],[0]
The parsing algorithms used during training are described in §5.,5.3 Dynamic programming (DP),[0],[0]
"Our test-time parsing algorithm uses the left-child loop implementation of CKY (Dunlop et al., 2010).",5.3 Dynamic programming (DP),[0],[0]
All algorithms allow unary rules (though not chains).,5.3 Dynamic programming (DP),[0],[0]
"We evaluate accuracy at test time with the F1 score from the official EVALB script (Sekine and Collins, 1997).
",5.3 Dynamic programming (DP),[0],[0]
Training:,5.3 Dynamic programming (DP),[0],[0]
Note that we never retrain the grammar weights—we train only the pruning policy.,5.3 Dynamic programming (DP),[0],[0]
"To TRAIN our classifiers (Alg. 2 line 13), we use L2-regularized logistic regression, trained with L-BFGS optimization.",5.3 Dynamic programming (DP),[0],[0]
"We always rescale the example weights in the training set to sum to 1 (otherwise as LOLS proceeds, dataset aggregation overwhelms the regularizer).",5.3 Dynamic programming (DP),[0],[0]
"For the baseline (defined in next section), we determine the regularization coefficient by sweeping {2−11, 2−12, 2−13, 2−14, 2−15} and picking the best value (2−13) based on the dev frontier.",5.3 Dynamic programming (DP),[0],[0]
We re-used this regularization parameter for LOLS.,5.3 Dynamic programming (DP),[0],[0]
"The number of LOLS iterations is determined by a 6-day training-time limit9 (meaning some jobs run many
7Code for experiments is available at http://github.",5.3 Dynamic programming (DP),[0],[0]
"com/timvieira/learning-to-prune.
",5.3 Dynamic programming (DP),[0],[0]
8Data train/dev/test split (by section) 2–21 / 22 / 23.,5.3 Dynamic programming (DP),[0],[0]
"Normalization operations: Remove function tags, traces, spurious unary edges (X → X), and empty subtrees left by other operations.",5.3 Dynamic programming (DP),[0],[0]
"Relabel ADVP and PRT|ADVP tags to PRT.
",5.3 Dynamic programming (DP),[0],[0]
"9On the 7th day, LOLS rested and performance was good.
",5.3 Dynamic programming (DP),[0],[0]
fewer iterations than others).,5.3 Dynamic programming (DP),[0],[0]
For LOLS minibatch size we use 10K on the coarse grammar and 5K on the fine grammar.,5.3 Dynamic programming (DP),[0],[0]
"At line 15 of Alg. 2, we return the policy that maximized reward on development data, using the reward function from training.
",5.3 Dynamic programming (DP),[0],[0]
"Features: We use similar features to Bodenstab et al. (2011), but we have removed features that depend on part-of-speech tags.",5.3 Dynamic programming (DP),[0],[0]
"We use the following 16 feature templates for span (i, k) with 1 < k−i < N : bias, sentence length, boundary words, conjunctions of boundary words, conjunctions of word shapes, span shape, width bucket.",5.3 Dynamic programming (DP),[0],[0]
"Shape features map a word or phrase into a string of character classes (uppercase, lowercase, numeric, spaces); we truncate substrings of identical classes to length two; punctuation chars are never modified in any way.",5.3 Dynamic programming (DP),[0],[0]
"Width buckets use the following partition: 2, 3, 4, 5, [6, 10], [11, 20], [21,∞).",5.3 Dynamic programming (DP),[0],[0]
"We use feature hashing (Weinberger et al., 2009) with MurmurHash3 (Appleby, 2008) and project to 222 features.",5.3 Dynamic programming (DP),[0],[0]
"Conjunctions are taken at positions (i−1, i), (k, k+1), (i−1, k+1) and (i, k).",5.3 Dynamic programming (DP),[0],[0]
"We use special begin and end symbols when a template accesses positions beyond the sentence boundary.
",5.3 Dynamic programming (DP),[0],[0]
Hall et al. (2014) give examples motivating our feature templates and show experimentally that they are effective in multiple languages.,5.3 Dynamic programming (DP),[0],[0]
Boundary words are strong surface cues for phrase boundaries.,5.3 Dynamic programming (DP),[0],[0]
Span shape features are also useful as they (minimally) check for matched parentheses and quotation marks.,5.3 Dynamic programming (DP),[0],[0]
Reward functions and surrogates: Each user has a personal reward function.,7 Experimental design and results,[0],[0]
"In this paper, we choose to specify our true reward as accuracy − λ · runtime, where accuracy is given by labeled F1 percentage and runtime by mega-pushes (mpush), millions of calls per sentence to lines 6 and 19 of Alg. 1, which is in practice proportional to seconds per sentence (correlation > 0.95) and is more replicable.",7 Experimental design and results,[0],[0]
We evaluate accordingly (on test data)—but during LOLS training we approximate these metrics.,7 Experimental design and results,[0],[0]
"We compare:
• rCP (fast): Use change propagation (§5.2) to compute accuracy on a sentence as F1 of just that sentence, and to approximate runtime as ||β||0,
the number of constituents that were built.10
• rDP (faster): Use dynamic programming (§5.3) to approximate accuracy on a sentence as expected recall.11 This time we approximate runtime more crudely as ||m||0, the number of nonzeros in the pruning mask for the sentence (i.e., the number of spans whose constituents the policy would be willing to keep if they were built).
",7 Experimental design and results,[0],[0]
We use these surrogates because they admit efficient rollout algorithms.,7 Experimental design and results,[0],[0]
"Less important, they preserve the training objective (1) as an average over sentences.",7 Experimental design and results,[0],[0]
"(Our true F1 metric on a corpus cannot be computed in this way, though it could reasonably be estimated by averaging over minibatches of sentences in (1).)
",7 Experimental design and results,[0],[0]
"Controlled experimental design: Our baseline system is an adaptation of Bodenstab et al. (2011) to learning-to-prune, as described in §3 and §6.",7 Experimental design and results,[0],[0]
Our goal is to determine whether such systems can be improved by LOLS training.,7 Experimental design and results,[0],[0]
"We repeat the following design for both reward surrogates (rCP and rDP) and for both grammars (coarse and fine).
",7 Experimental design and results,[0],[0]
¬ We start by training a number of baseline models by sweeping the asymmetric weighting parameter.,7 Experimental design and results,[0],[0]
"For the coarse grammar we train 8 such models, and for the fine grammar 12.
 ",7 Experimental design and results,[0],[0]
"For each baseline policy, we estimate a value of λ for which that policy is optimal (among baseline policies) according to surrogate reward.12
10When using rCP, we speed up LOLS by doing≤ 2n rollouts per sentence of length n. We sample these uniformly without replacement from the T possible rollouts (§5), and compensate by upweighting the resulting training examples by T/(2n).
",7 Experimental design and results,[0],[0]
"11Considering all nodes in the binarized tree, except for the root, width-1 constituents, and children of unary rules.
",7 Experimental design and results,[0],[0]
"12We estimate λ by first fitting a parametric model yi = h(xi) , ymax · sigmoid(a · log(xi + c) + b) to the baseline runtime-accuracy measurements on dev data (shown in green in Fig. 2) by minimizing mean squared error.",7 Experimental design and results,[0],[0]
"We then use the fitted curve’s slope h′ to estimate each λi = h′(xi), where xi is the runtime of baseline i. The resulting choice of reward function y−λi",7 Experimental design and results,[0],[0]
"·x increases along the green arrow in Fig. 2, and is indeed maximized (subject to y ≤ h(x), and in the region where h is concave) at x = xi.",7 Experimental design and results,[0],[0]
"As a sanity check, notice since λi is a derivative of the function y = h(x), its units are in units of y (accuracy) per unit of x (runtime), as appropriate for use in the expression",7 Experimental design and results,[0],[0]
y,7 Experimental design and results,[0],[0]
− λi · x.,7 Experimental design and results,[0],[0]
"Indeed, this procedure will construct the same reward function regardless of the units we use to express x.",7 Experimental design and results,[0],[0]
"Our specific parametric model h is a sigmoidal curve, with
® For each baseline policy, we run LOLS with the same surrogate reward function (defined by λ) for which that baseline policy was optimal.",7 Experimental design and results,[0],[0]
We initialize LOLS by setting π0 to the baseline policy.,7 Experimental design and results,[0],[0]
"Furthermore, we include the baseline policy’s weighted training set Q̂0 in the ⋃ at line 13.
",7 Experimental design and results,[0],[0]
"Fig. 2 shows that LOLS learns to improve on the baseline, as evaluated on development data.
¯",7 Experimental design and results,[0],[0]
But do these surrogate reward improvements also improve our true reward?,7 Experimental design and results,[0],[0]
"For each baseline policy, we use dev data to estimate a value of λ for which that policy is optimal according to our true reward function.",7 Experimental design and results,[0],[0]
"We use blind test data to compare the baseline policy to its corresponding LOLS policy on this true reward function, testing significance with a paired permutation test.",7 Experimental design and results,[0],[0]
"The improvements hold up, as shown in Fig. 3.
",7 Experimental design and results,[0],[0]
"The rationale behind this design is that a user who actually wishes to maximize accuracy−λ·runtime, for some specific λ, could reasonably start by choosing the best baseline policy for this reward function, and then try to improve that baseline by running LOLS with the same reward function.",7 Experimental design and results,[0],[0]
"Our experiments show this procedure works for a range of λ values.
",7 Experimental design and results,[0],[0]
"In the real world, a user’s true objective might instead be some nonlinear function of runtime and accuracy.",7 Experimental design and results,[0],[0]
"For example, when accuracy is “good enough,” it may be more important to improve runtime, and vice-versa.",7 Experimental design and results,[0],[0]
LOLS could be used with such a nonlinear reward function as well.,7 Experimental design and results,[0],[0]
"In fact, a user does not even have to quantify their global preferences by writing down such a function.",7 Experimental design and results,[0],[0]
"Rather, they could select manually among the baseline policies, choosing one with an attractive speed-accuracy tradeoff, and then specify λ to indicate a local direction of desired improvement (like the green arrows in Fig. 2), modifying this direction periodically as LOLS runs.",7 Experimental design and results,[0],[0]
"As previous work has shown, learning to prune gives us excellent parsers with less than < 2% overhead
accuracy → ymax asymptotically as runtime → ∞. It obtains an excellent fit by placing accuracy and runtime on the loglogit scale—that is, log(xi + c) and logit(yi/ymax) transforms are used to convert our bounded random variables xi and yi to unbounded ones—and then assuming they are linearly related.
for deciding what to prune (i.e., pruning feature extraction and span classification).",7.1 Discussion,[0],[0]
"Even the baseline pruner has access to features unavailable to the grammar, and so it learns to override the grammar, improving an unpruned coarse parser’s accuracy from 61.1 to as high as 70.1% F1 on test data (i.e., beneficial search error).",7.1 Discussion,[0],[0]
"It is also 8.1x faster!13 LOLS simply does a better job at figuring out where to prune, raising accuracy 2.1 points to 72.2 (while maintaining a 7.4x speedup).",7.1 Discussion,[0],[0]
"Where pruning is more aggressive,
13We measure runtime as best of 10 runs (recommended by Dunlop (2014)).",7.1 Discussion,[0],[0]
"All parser timing experiments were performed on a Linux laptop with the following specs: Intel® Core™ i5-2540M 2.60GHz CPU, 8GB memory, 32K/256K/3072K L1/L2/L3 cache.",7.1 Discussion,[0],[0]
"Code is written in the Cython language.
",7.1 Discussion,[0],[0]
"LOLS has even more impact on accuracy.
",7.1 Discussion,[0],[0]
"Even on the fine grammar, where there is less room to improve accuracy, the most accurate LOLS system improves an unpruned parser by +0.16% F1 with a 8.6x speedup.",7.1 Discussion,[0],[0]
"For comparison, the most accurate baseline drops −0.03% F1 with a 9.7x speedup.
",7.1 Discussion,[0],[0]
"With the fine grammar, we do not see much improvement over the baseline in the accuracy > 85 regions.",7.1 Discussion,[0],[0]
This is because the supervision specified by asymmetric weighting is similar to what LOLS surmises via rollouts.,7.1 Discussion,[0],[0]
"However, in lower-accuracy regions we see that LOLS can significantly improve reward over its baseline policy.",7.1 Discussion,[0],[0]
"This is because the baseline supervision does not teach which plausible
constituents are “safest” to prune, nor can it learn strategies such as “skip all long sentences.”",7.1 Discussion,[0],[0]
"We discuss why LOLS does not help as much in the high accuracy regions further in §7.3.
",7.1 Discussion,[0],[0]
"In a few cases in Fig. 2, LOLS finds no policy that improves surrogate reward on dev data.",7.1 Discussion,[0],[0]
"In these cases, surrogate reward does improve slightly on training data (not shown), but early stopping just keeps the initial (baseline) policy since it is just as good on dev data.",7.1 Discussion,[0],[0]
"Adding a bit of additional random exploration might help break out of this initialization.
",7.1 Discussion,[0],[0]
"Interestingly, the rDP LOLS policies find higheraccuracy policies than the corresponding rCP policies, despite a greater mismatch in surrogate accuracy definitions.",7.1 Discussion,[0],[0]
"We suspect that rDP’s approach of trying to improve expected accuracy may provide a useful regularizing effect, which smooths out the reward signal and provides a useful bias (§5.3).
",7.1 Discussion,[0],[0]
"The most pronounced qualitative difference due to LOLS training is substantially lower rates of parse failure in the mid- to high- λ-range on both grammars
(not shown).",7.1 Discussion,[0],[0]
"Since LOLS does end-to-end training, it can advise the learner that a certain pruning decision catastrophically results in no parse being found.",7.1 Discussion,[0],[0]
Part of the contribution of this paper is faster algorithms for performing LOLS rollouts during training (§5).,7.2 Training speed and convergence,[0],[0]
"Compared to the naive strategy of running the parser from scratch T times, rCP achieves speedups of 4.9–6.6x on the coarse grammar and 1.9–2.4x on the fine grammar.",7.2 Training speed and convergence,[0],[0]
"rDP is even faster, 10.4–11.9x on coarse and 10.5–13.8x on fine.",7.2 Training speed and convergence,[0],[0]
"Most of the speedup comes from longer sentences, which take up most of the runtime for all methods.",7.2 Training speed and convergence,[0],[0]
Our new algorithms enable us to train on fairly long sentences (≤ 40).,7.2 Training speed and convergence,[0],[0]
"We note that our implementations of rCP and rDP are not as highly optimized as our test-time parser, so there may be room for improvement.
",7.2 Training speed and convergence,[0],[0]
Orthogonal to the cost per rollout is the number of training iterations.,7.2 Training speed and convergence,[0],[0]
"LOLS may take many steps to converge if trajectories are long (i.e., T is large)
because each iteration of LOLS training attempts to improve the current policy by a single action.",7.2 Training speed and convergence,[0],[0]
"In our setting, T is quite large (discussed extensively in §5), but we are able to circumvent slow convergence by initializing the policy (via the baseline method).",7.2 Training speed and convergence,[0],[0]
This means that LOLS can focus on fine-tuning a policy which is already quite good.,7.2 Training speed and convergence,[0],[0]
"In fact, in 4 cases, LOLS did not improve from its initial policy.
",7.2 Training speed and convergence,[0],[0]
We find that when λ is large—the cases where we get meaningful improvements because the initial policy is far from locally optimal—LOLS steadily and smoothly improves the surrogate reward on both training and development data.,7.2 Training speed and convergence,[0],[0]
"Because these are fast parsers, LOLS was able to run on the order of 10 (fine grammar) or 100 (coarse grammar) epochs within our 6-day limit; usually it was still improving when we terminated it.",7.2 Training speed and convergence,[0],[0]
"By contrast, for the slower and more accurate small-λ parsers (which completed fewer training epochs), LOLS still improves surrogate reward on training data, but without systematically improving on development data—often the reward on development fluctuates, and early stopping simply picks the best of this small set of “random” variants.",7.2 Training speed and convergence,[0],[0]
"In §3, we argued that LOLS gives a more appropriate training signal for pruning than the baseline method of consulting the gold parse, because it uses rollouts to measure the full effect of each pruning decision in the context of the other decisions made by the policy.
",7.3 Understanding the LOLS training signal,[0],[0]
"To better understand the results of our previous experiments, we analyze how often a rollout does determine that the baseline supervision for a span is suboptimal, and how suboptimal it is in those cases.
",7.3 Understanding the LOLS training signal,[0],[0]
We specifically consider LOLS rollouts that evaluate the rCP surrogate (because rDP is a cruder approximation to true reward).,7.3 Understanding the LOLS training signal,[0],[0]
"These rollouts Q̂i tell us what actions LOLS is trying to improve in its current policy πi for a given λ, although there is no guarantee that the learner in §4 will succeed at classifying Q̂i correctly (due to limited features, regularization, and the effects of dataset aggregation).
",7.3 Understanding the LOLS training signal,[0],[0]
We define regret of the baseline oracle.,7.3 Understanding the LOLS training signal,[0],[0]
"Let best(s) , argmaxaROLLOUT(π, s, a) and regret(s) , (ROLLOUT(π, s, best(s) − ROLLOUT(π, s, gold(s)))).",7.3 Understanding the LOLS training signal,[0],[0]
"Note that regret(s)≥0 for all s, and let diff(s) be the event that regret(s) > 0 strictly.",7.3 Understanding the LOLS training signal,[0],[0]
"We are interested in analyzing the expected regret over all gold and
non-gold spans, which we break down as
E[regret] = p(diff) (4) · ( p(gold | diff) · E[regret | gold, diff] + p(¬ gold | diff) · E[regret | ¬ gold, diff] )
where expectations are taken over s ∼ ROLL-IN(π).",7.3 Understanding the LOLS training signal,[0],[0]
"Empirical analysis of regret: To show where the benefit of the LOLS oracle comes from, Fig. 4 graphs the various quantities that enter into the definition (4) of baseline regret, for different π, λ, and grammar.",7.3 Understanding the LOLS training signal,[0],[0]
"The LOLS oracle evolves along with the policy π, since it identifies the best action given π.",7.3 Understanding the LOLS training signal,[0],[0]
"We thus evaluate the oracle baseline against two LOLS oracles: the one used at the start of LOLS training (derived from the initial policy π1 that was trained on baseline supervision), and the one obtained at the end (derived from the LOLS-trained policy π∗ selected by early stopping).",7.3 Understanding the LOLS training signal,[0],[0]
"These comparisons are shown by solid and dashed lines respectively.
",7.3 Understanding the LOLS training signal,[0],[0]
"Class imbalance (black curves): In all graphs, the aggregate curves primarily reflect the non-gold spans, since only 8% of spans are gold.
",7.3 Understanding the LOLS training signal,[0],[0]
"Gold spans (gold curves): The top graphs show that a substantial fraction of the gold spans should be pruned (whereas the baseline tries to keep them all), although the middle row shows that the benefit of pruning them is small.",7.3 Understanding the LOLS training signal,[0],[0]
"In most of these cases, pruning a gold span improves speed but leaves accuracy unchanged—because that gold span was missed anyway by the highest-scoring parse.",7.3 Understanding the LOLS training signal,[0],[0]
Such cases become both more frequent and more beneficial as λ increases and we prune more heavily.,7.3 Understanding the LOLS training signal,[0],[0]
"In a minority of cases, however, pruning a gold span also improves accuracy (through beneficial search error).
",7.3 Understanding the LOLS training signal,[0],[0]
"Non-gold spans (purple curves): Conversely, the top graphs show that a few non-gold spans should be kept (whereas the baseline tries to prune them all), and the middle row shows a large benefit from keeping them.",7.3 Understanding the LOLS training signal,[0],[0]
"They are needed to recover from catastrophic errors and get a mostly-correct parse.
",7.3 Understanding the LOLS training signal,[0],[0]
Coarse vs. fine (left vs. right):,7.3 Understanding the LOLS training signal,[0],[0]
"The two grammars differ mainly for small λ, and this difference comes especially from the top row.",7.3 Understanding the LOLS training signal,[0],[0]
"With a fine grammar and small λ, the baseline parses are more accurate, so LOLS has less room for improvement: fewer
gold spans go unused, and fewer non-gold spans are needed for recovery.
",7.3 Understanding the LOLS training signal,[0],[0]
"Effect of λ: Aggressive pruning (large λ) reduces accuracy, so its effect on the top row is similar to that of using a coarse grammar.",7.3 Understanding the LOLS training signal,[0],[0]
"Aggressive pruning also has an effect on the middle row: there is more benefit to be derived from pruning unused gold spans (surprisingly), and especially from keeping those non-gold spans that are helpful (presumably they enable recovery from more severe parse errors).",7.3 Understanding the LOLS training signal,[0],[0]
"These effects are considerably sharper with rDP reward (not shown here), which more smoothly evaluates the entire weighted pruned parse forest rather than trying to coordinate actions to ensure a good single 1-best tree; the baseline oracle is excellent at choosing the action that gets the better forest when the forest is mostly present (small λ) but not when it is mostly pruned (large λ).
",7.3 Understanding the LOLS training signal,[0],[0]
Effect on retraining the policy: The black lines in the bottom graphs show the overall regret (on training data) if we were to perfectly follow the baseline oracle rather than the LOLS oracle.,7.3 Understanding the LOLS training signal,[0],[0]
"In practice, retraining the policy to match the oracle will not match it perfectly in either case.",7.3 Understanding the LOLS training signal,[0],[0]
"Thus the baseline method has a further disadvantage: when it trains a policy, its training objective weights all gold or all non-gold examples equally, whereas LOLS invests greater effort in matching the oracle on those states where doing so would give greater downstream reward.",7.3 Understanding the LOLS training signal,[0],[0]
Our experiments have focused on using LOLS to improve a reasonable baseline.,8 Related work,[0],[0]
Fig. 5 shows that our resulting parser fits reasonably among state-of-the-art constituency parsers trained and tested on the Penn Treebank.,8 Related work,[0],[0]
These parsers include a variety of techniques that improve speed or accuracy.,8 Related work,[0],[0]
"Many are quite orthogonal to our work here—e.g., the SpMV method (which is necessary for Bodenstab’s parser to beat ours) is a set of cache-efficient optimizations (Dunlop, 2014) that could be added to our parser (just as it was added to Bodenstab’s), while Hall et al. (2014) and Fernández-González and Martins (2015) replace the grammar with faster scoring models that have more conditional independence.",8 Related work,[0],[0]
"Overall, other fast parsers could also be trained using LOLS, so that
they quickly find parses that are accurate, or at least helpful to the accuracy of some downstream task.
",8 Related work,[0],[0]
"Pruning methods14 can use classifiers not only to select spans but also to prune at other granularities (Roark and Hollingshead, 2008; Bodenstab et al., 2011).",8 Related work,[0],[0]
"Prioritization methods do not prune substructures, but instead delay their processing until they are needed—if ever (Caraballo and Charniak, 1998).
",8 Related work,[0],[0]
This paper focuses on learning pruning heuristics that have trainable parameters.,8 Related work,[0],[0]
"In the same way, Stoyanov and Eisner (2012) learn to turn off unneeded factors in a graphical model, and Jiang et al. (2012) and Berant and Liang (2015) train prioritization heuristics (using policy gradient).",8 Related work,[0],[0]
"In both of those 2012 papers, we explicitly sought to maximize accuracy − λ · runtime as we do here.",8 Related work,[0],[0]
"Some previous “coarse-to-fine” work does not optimize heuris-
14We focus here on parsing, but pruning is generally useful in structured prediction.",8 Related work,[0],[0]
"E.g., Xu et al. (2013) train a classifier to prune (latent) alignments in a machine translation system.
tics directly but rather derives heuristics for pruning (Charniak et al., 2006; Petrov and Klein, 2007; Weiss and Taskar, 2010; Rush and Petrov, 2012) or prioritization (Klein and Manning, 2003; Pauls and Klein, 2009) from a coarser version of the model.",8 Related work,[0.9537096475252922],"['Conventional ODE models have also been considered from the stochastic perspective with stochastic differential equation (SDE) models that commonly model the deterministic system drift and diffusion processes separately leading to a distribution of trajectories p(x(t)) (Archambeau et al., 2007; Garcı́a et al., 2017).']"
"Combining these automatic methods with LOLS would require first enriching their heuristics with trainable parameters, or parameterizing the coarse-to-fine hierarchy itself as in the “feature pruning” work of He et al. (2013) and Strubell et al. (2015).
",8 Related work,[0],[0]
Dynamic features are ones that depend on previous actions.,8 Related work,[0],[0]
"In our setting, a policy could in principle benefit from considering the full state of the chart at Alg. 1 line 14.",8 Related work,[0],[0]
"While coarse-to-fine methods implicitly use certain dynamic features, training with dynamic features is a fairly new goal that is challenging to treat efficiently.",8 Related work,[0],[0]
"It has usually been treated with some form of simple imitation learning, using a heuristic training signal much as in our baseline (Jiang, 2014; He et al., 2013).",8 Related work,[0],[0]
"LOLS would be a more principled way to train such features, but for efficiency, our present paper restricts to static features that only access the state via π(w, i, k).",8 Related work,[0],[0]
This permits our fast CP and DP rollout algorithms.,8 Related work,[0],[0]
"It also reduces the time and space cost of dataset aggregation.15
LOLS attempts to do end-to-end training of a sequential decision-making system, without falling back on black-box optimization tools (Och, 2003; Chung and Galley, 2012) that ignore the sequential structure.",8 Related work,[0],[0]
"In NLP, sequential decisions are more commonly trained with step-by-step supervision
15LOLS repeatedly evaluates actions given (w, i, k).",8 Related work,[0],[0]
"We consolidate the resulting training examples by summing their reward vectors r̂, so the aggregated dataset does not grow over time.
",8 Related work,[0],[0]
"(Kuhlmann et al., 2011), using methods such as local classification (Punyakanok and Roth, 2001) or beam search with early update (Collins and Roark, 2004).",8 Related work,[0],[0]
LOLS tackles the harder setting where the only training signal is a joint assessment of the entire sequence of actions.,8 Related work,[0],[0]
"It is an alternative to policy gradient, which does not scale well to our long trajectories because of high variance in the estimated gradient and because random exploration around (even good) pruning policies most often results in no parse at all.",8 Related work,[0],[0]
"LOLS uses controlled comparisons, resulting in more precise “credit assignment” and tighter exploration.
",8 Related work,[0],[0]
"We would be remiss not to note that current transition-based parsers—for constituency parsing (Zhu et al., 2013; Crabbé, 2015) as well as dependency parsing (Chen and Manning, 2014)—are both incredibly fast and surprisingly accurate.",8 Related work,[0],[0]
"This may appear to undermine the motivation for our work, or at least for its application to fast parsing.16",8 Related work,[0],[0]
"However, transition-based parsers do not produce marginal probabilities of substructures, which can be useful features for downstream tasks.",8 Related work,[0],[0]
"Indeed, the transitionbased approach is essentially greedy and so it may fail on tasks with more ambiguity than parsing.",8 Related work,[0],[0]
"Current transition-based parsers also require step-by-step supervision, whereas our method can also be used to train in the presence of incomplete supervision, latent structure, or indirect feedback.",8 Related work,[0],[0]
"Our method could also be used immediately to speed up dynamic programming methods for MT, synchronous parsing, parsing with non-context-free grammar formalisms, and other structured prediction problems for which transition systems have not (yet) been designed.",8 Related work,[0.9558705878280859],"['Recently, initial work to handle unknown or non-parametric ODE models have been proposed, although with various limiting approximations.']"
We presented an approach to learning pruning policies that optimizes end-to-end performance on a userspecified speed-accuracy tradeoff.,9 Conclusions,[0],[0]
We developed two novel algorithms for efficiently measuring how varying policy actions affects reward.,9 Conclusions,[0],[0]
"In the case of parsing, given a performance criterion and a good baseline policy for that criterion, the learner consistently manages to find a higher-reward policy.",9 Conclusions,[0],[0]
"We hope this work inspires a new generation of fast and accurate structured prediction models with tunable runtimes.
16Of course, LOLS can also train transition-based parsers (Chang et al., 2015a), or even vary their beam width dynamically.",9 Conclusions,[0],[0]
This material is based in part on research sponsored by the National Science Foundation under Grant No. 0964681 and DARPA under agreement number FA8750-13-2-0017 (DEFT program).,Acknowledgments,[0],[0]
"We’d like to thank Nathaniel Wesley Filardo, Adam Teichert, Matt Gormley and Hal Daumé III for helpful discussions.",Acknowledgments,[0],[0]
"Finally, we thank TACL action editor Marco Kuhlmann and the anonymous reviewers and copy editor for suggestions that improved this paper.",Acknowledgments,[0],[0]
Pruning hypotheses during dynamic programming is commonly used to speed up inference in settings such as parsing.,abstractText,[0],[0]
"Unlike prior work, we train a pruning policy under an objective that measures end-to-end performance: we search for a fast and accurate policy.",abstractText,[0],[0]
"This poses a difficult machine learning problem, which we tackle with the LOLS algorithm.",abstractText,[0],[0]
"LOLS training must continually compute the effects of changing pruning decisions: we show how to make this efficient in the constituency parsing setting, via dynamic programming and change propagation algorithms.",abstractText,[0],[0]
"We find that optimizing end-to-end performance in this way leads to a better Pareto frontier—i.e., parsers which are more accurate for a given runtime.",abstractText,[0],[0]
Learning to Prune: Exploring the Frontier of Fast and Accurate Parsing,title,[0],[0]
Deep neural networks (DNNs) have been widely used for machine learning applications due to their powerful capacity for modeling complex input patterns.,1. Introduction,[0],[0]
"Despite their success, it has been shown that DNNs are prone to training set biases, i.e. the training set is drawn from a joint distribution p(x, y) that is different from the distribution p(xv, yv) of the evaluation set.",1. Introduction,[0],[0]
"This distribution mismatch could have many
1Uber Advanced Technologies Group, Toronto ON, CANADA 2Department of Computer Science, University of Toronto, Toronto ON, CANADA.",1. Introduction,[0],[0]
"Correspondence to: Mengye Ren <mren3@uber.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
different forms.",1. Introduction,[0],[0]
Class imbalance in the training set is a very common example.,1. Introduction,[0],[0]
"In applications such as object detection in the context of autonomous driving, the vast majority of the training data is composed of standard vehicles but models also need to recognize rarely seen classes such as emergency vehicles or animals with very high accuracy.",1. Introduction,[0],[0]
"This will sometime lead to biased training models that do not perform well in practice.
",1. Introduction,[0],[0]
Another popular type of training set bias is label noise.,1. Introduction,[0],[0]
"To train a reasonable supervised deep model, we ideally need a large dataset with high-quality labels, which require many passes of expensive human quality assurance (QA).",1. Introduction,[0],[0]
"Although coarse labels are cheap and of high availability, the presence of noise will hurt the model performance, e.g. Zhang et al. (2017) has shown that a standard CNN can fit any ratio of label flipping noise in the training set and eventually leads to poor generalization performance.
",1. Introduction,[0],[0]
"Training set biases and misspecification can sometimes be addressed with dataset resampling (Chawla et al., 2002), i.e. choosing the correct proportion of labels to train a network on, or more generally by assigning a weight to each example and minimizing a weighted training loss.",1. Introduction,[0],[0]
"The example weights are typically calculated based on the training loss, as in many classical algorithms such as AdaBoost (Freund & Schapire, 1997), hard negative mining (Malisiewicz et al., 2011), self-paced learning (Kumar et al., 2010), and other more recent work (Chang et al., 2017; Jiang et al., 2017).
",1. Introduction,[0],[0]
"However, there exist two contradicting ideas in training loss based approaches.",1. Introduction,[0],[0]
"In noisy label problems, we prefer examples with smaller training losses as they are more likely to be clean images; yet in class imbalance problems, algorithms such as hard negative mining (Malisiewicz et al., 2011) prioritize examples with higher training loss since they are more likely to be the minority class.",1. Introduction,[0],[0]
"In cases when the training set is both imbalanced and noisy, these existing methods would have the wrong model assumptions.",1. Introduction,[0],[0]
"In fact, without a proper definition of an unbiased test set, solving the training set bias problem is inherently ill-defined.",1. Introduction,[0],[0]
"As the model cannot distinguish the right from the wrong, stronger regularization can usually work surprisingly well in certain synthetic noise settings.",1. Introduction,[0],[0]
"Here we argue that in order to learn general forms of training set biases, it is necessary to have a small unbiased validation to guide training.",1. Introduction,[0],[0]
"It is actually
not uncommon to construct a dataset with two parts - one relatively small but very accurately labeled, and another massive but coarsely labeled.",1. Introduction,[0],[0]
"Coarse labels can come from inexpensive crowdsourcing services or weakly supervised data (Cordts et al., 2016; Russakovsky et al., 2015; Chen & Gupta, 2015).
",1. Introduction,[0],[0]
"Different from existing training loss based approaches, we follow a meta-learning paradigm and model the most basic assumption instead: the best example weighting should minimize the loss of a set of unbiased clean validation examples that are consistent with the evaluation procedure.",1. Introduction,[0],[0]
"Traditionally, validation is performed at the end of training, which can be prohibitively expensive if we treat the example weights as some hyperparameters to optimize; to circumvent this, we perform validation at every training iteration to dynamically determine the example weights of the current batch.",1. Introduction,[0],[0]
"Towards this goal, we propose an online reweighting method that leverages an additional small validation set and adaptively assigns importance weights to examples in every iteration.",1. Introduction,[0],[0]
We experiment with both class imbalance and corrupted label problems and find that our approach significantly increases the robustness to training set biases.,1. Introduction,[0],[0]
The idea of weighting each training example has been well studied in the literature.,2. Related Work,[0],[0]
"Importance sampling (Kahn & Marshall, 1953), a classical method in statistics, assigns weights to samples in order to match one distribution to another.",2. Related Work,[0],[0]
"Boosting algorithms such as AdaBoost (Freund & Schapire, 1997), select harder examples to train subsequent classifiers.",2. Related Work,[0],[0]
"Similarly, hard example mining (Malisiewicz et al., 2011), downsamples the majority class and exploits the most difficult examples.",2. Related Work,[0],[0]
"Focal loss (Lin et al., 2017) adds a soft weighting scheme that emphasizes harder examples.
",2. Related Work,[0],[0]
Hard examples are not always preferred in the presence of outliers and noise processes.,2. Related Work,[0],[0]
Robust loss estimators typically downweigh examples with high loss.,2. Related Work,[0],[0]
"In selfpaced learning (Kumar et al., 2010), example weights are obtained through optimizing the weighted training loss encouraging learning easier examples first.",2. Related Work,[0],[0]
"In each step, the learning algorithm jointly solves a mixed integer program that iterates optimizing over model parameters and binary example weights.",2. Related Work,[0],[0]
"Various regularization terms on the example weights have since been proposed to prevent overfitting and trivial solutions of assigning weights to be all zeros (Kumar et al., 2010; Ma et al., 2017; Jiang et al., 2015).",2. Related Work,[0],[0]
Wang et al. (2017) proposed a Bayesian method that infers the example weights as latent variables.,2. Related Work,[0],[0]
"More recently, Jiang et al. (2017) proposed to use a meta-learning LSTM to output the weights of the examples based on the training loss.",2. Related Work,[0],[0]
"Reweighting examples is also related to curriculum learning (Bengio et al., 2009), where the model reweights
among many available tasks.",2. Related Work,[0],[0]
"Similar to self-paced learning, typically it is beneficial to start with easier examples.
",2. Related Work,[0],[0]
One crucial advantage of reweighting examples is robustness against training set bias.,2. Related Work,[0],[0]
"There has also been a multitude of prior studies on class imbalance problems, including using dataset resampling (Chawla et al., 2002; Dong et al., 2017), cost-sensitive weighting (Ting, 2000; Khan et al., 2015), and structured margin based objectives (Huang et al., 2016).",2. Related Work,[0],[0]
"Meanwhile, the noisy label problem has been thoroughly studied by the learning theory community (Natarajan et al., 2013; Angluin & Laird, 1988) and practical methods have also been proposed (Reed et al., 2014; Sukhbaatar & Fergus, 2014; Xiao et al., 2015; Azadi et al., 2016; Goldberger & Ben-Reuven, 2017; Li et al., 2017; Jiang et al., 2017; Vahdat, 2017; Hendrycks et al., 2018).",2. Related Work,[0],[0]
"In addition to corrupted data, Koh & Liang (2017); Muñoz-González et al. (2017) demonstrate the possibility of a dataset adversarial attack (i.e. dataset poisoning).
",2. Related Work,[0],[0]
"Our method improves the training objective through a weighted loss rather than an average loss and is an instantiation of meta-learning (Thrun & Pratt, 1998; Lake et al., 2017; Andrychowicz et al., 2016), i.e. learning to learn better.",2. Related Work,[0],[0]
"Using validation loss as the meta-objective has been explored in recent meta-learning literature for few-shot learning (Ravi & Larochelle, 2017; Ren et al., 2018; Lorraine & Duvenaud, 2018), where only a handful of examples are available for each class.",2. Related Work,[0],[0]
"Our algorithm also resembles MAML (Finn et al., 2017) by taking one gradient descent step on the meta-objective for each iteration.",2. Related Work,[0],[0]
"However, different from these meta-learning approaches, our reweighting method does not have any additional hyperparameters and circumvents an expensive offline training stage.",2. Related Work,[0],[0]
"Hence, our method can work in an online fashion during regular training.",2. Related Work,[0],[0]
"In this section, we derive our model from a meta-learning objective towards an online approximation that can fit into any regular supervised training.",3. Learning to Reweight Examples,[0],[0]
We give a practical implementation suitable for any deep network type and provide theoretical guarantees under mild conditions that our algorithm has a convergence rate of O(1/ 2).,3. Learning to Reweight Examples,[0],[0]
Note that this is the same as that of stochastic gradient descent (SGD).,3. Learning to Reweight Examples,[0],[0]
"Let (x, y) be an input-target pair, and {(xi, yi), 1 ≤",3.1. From a meta-learning objective to an online approximation,[0],[0]
i ≤ N} be the training set.,3.1. From a meta-learning objective to an online approximation,[0],[0]
"We assume that there is a small unbiased and clean validation set {(xvi , yvi ), 1 ≤ i ≤M}, and M N .",3.1. From a meta-learning objective to an online approximation,[0],[0]
"Hereafter, we will use superscript v to denote validation set and subscript i to denote the ith data.",3.1. From a meta-learning objective to an online approximation,[0],[0]
"We also assume
that the training set contains the validation set; otherwise, we can always add this small validation set into the training set and leverage more information during training.
",3.1. From a meta-learning objective to an online approximation,[0],[0]
"Let Φ(x, θ) be our neural network model, and θ be the model parameters.",3.1. From a meta-learning objective to an online approximation,[0],[0]
"We consider a loss function C(ŷ, y) to minimize during training, where ŷ = Φ(x, θ).
",3.1. From a meta-learning objective to an online approximation,[0],[0]
"In standard training, we aim to minimize the expected loss for the training set: 1N ∑N i=1",3.1. From a meta-learning objective to an online approximation,[0],[0]
"C(ŷi, yi) = 1 N ∑N i=1 fi(θ), where each input example is weighted equally, and fi(θ) stands for the loss function associating with data xi.",3.1. From a meta-learning objective to an online approximation,[0],[0]
"Here we aim to learn a reweighting of the inputs, where we minimize a weighted loss:
θ∗(w) = arg min θ N∑ i=1",3.1. From a meta-learning objective to an online approximation,[0],[0]
"wifi(θ), (1)
with wi unknown upon beginning.",3.1. From a meta-learning objective to an online approximation,[0],[0]
"Note that {wi}Ni=1 can be understood as training hyperparameters, and the optimal selection of w is based on its validation performance:
w∗ = arg min w,w≥0
1
M M∑ i=1",3.1. From a meta-learning objective to an online approximation,[0],[0]
fvi,3.1. From a meta-learning objective to an online approximation,[0],[0]
(θ ∗(w)).,3.1. From a meta-learning objective to an online approximation,[0],[0]
"(2)
It is necessary that wi ≥ 0 for all i, since minimizing the negative training loss can usually result in unstable behavior.
",3.1. From a meta-learning objective to an online approximation,[0],[0]
"Online approximation Calculating the optimal wi requires two nested loops of optimization, and every single loop can be very expensive.",3.1. From a meta-learning objective to an online approximation,[0],[0]
The motivation of our approach is to adapt online w through a single optimization loop.,3.1. From a meta-learning objective to an online approximation,[0],[0]
"For each training iteration, we inspect the descent direction of some training examples locally on the training loss surface and reweight them according to their similarity to the descent direction of the validation loss surface.
",3.1. From a meta-learning objective to an online approximation,[0],[0]
"For most training of deep neural networks, SGD or its variants are used to optimize such loss functions.",3.1. From a meta-learning objective to an online approximation,[0],[0]
"At every step t of training, a mini-batch of training examples {(xi, yi), 1 ≤ i ≤ n} is sampled, where n is the mini-batch size, n N .",3.1. From a meta-learning objective to an online approximation,[0],[0]
Then the parameters are adjusted according to the descent direction of the expected loss on the mini-batch.,3.1. From a meta-learning objective to an online approximation,[0],[0]
"Let’s consider vanilla SGD:
θt+1 = θt − α∇
( 1
n n∑ i=1",3.1. From a meta-learning objective to an online approximation,[0],[0]
"fi(θt)
) , (3)
where α is the step size.
",3.1. From a meta-learning objective to an online approximation,[0],[0]
We want to understand what would be the impact of training example,3.1. From a meta-learning objective to an online approximation,[0],[0]
"i towards the performance of the validation set at training step t. Following a similar analysis to Koh & Liang (2017), we consider perturbing the weighting by i for each
training example in the mini- batch,
fi, (θ) = ifi(θ), (4)
θ̂t+1( ) = θt − α∇ n∑ i=1",3.1. From a meta-learning objective to an online approximation,[0],[0]
"fi, (θ) ∣∣∣ θ=θt .",3.1. From a meta-learning objective to an online approximation,[0],[0]
"(5)
We can then look for the optimal ∗ that minimizes the validation loss fv locally at step t:
∗t = arg min
1
M M∑ i=1",3.1. From a meta-learning objective to an online approximation,[0],[0]
fvi (θt+1( )).,3.1. From a meta-learning objective to an online approximation,[0],[0]
"(6)
Unfortunately, this can still be quite time-consuming.",3.1. From a meta-learning objective to an online approximation,[0],[0]
"To get a cheap estimate of wi at step t, we take a single gradient descent step on a mini-batch of validation samples wrt. t, and then rectify the output to get a non-negative weighting:
ui,t = −η ∂
∂",3.1. From a meta-learning objective to an online approximation,[0],[0]
"i,t
1
m m∑ j=1 fvj (θt+1( )) ∣∣∣",3.1. From a meta-learning objective to an online approximation,[0],[0]
"i,t=0 , (7)
w̃i,t = max(ui,t, 0).",3.1. From a meta-learning objective to an online approximation,[0],[0]
"(8)
where η is the descent step size on .
",3.1. From a meta-learning objective to an online approximation,[0],[0]
"To match the original training step size, in practice, we can consider normalizing the weights of all examples in a training batch so that they sum up to one.",3.1. From a meta-learning objective to an online approximation,[0],[0]
"In other words, we choose to have a hard constraint within the set {w : ‖w‖1 = 1} ∪ {0}.
wi,t = w̃i,t ( ∑ j w̃j,t) + δ",3.1. From a meta-learning objective to an online approximation,[0],[0]
"( ∑ j w̃j,t) , (9)
where δ(·) is to prevent the degenerate case when all wi’s in a mini-batch are zeros, i.e. δ(a) = 1 if a = 0, and equals to 0 otherwise.",3.1. From a meta-learning objective to an online approximation,[0],[0]
"Without the batch-normalization step, it is possible that the algorithm modifies its effective learning rate of the training progress, and our one-step look ahead may be too conservative in terms of the choice of learning rate (Wu et al., 2018).",3.1. From a meta-learning objective to an online approximation,[0],[0]
"Moreover, with batch normalization, we effectively cancel the meta learning rate parameter η.",3.1. From a meta-learning objective to an online approximation,[0],[0]
"In this section, we study how to compute wi,t in a multilayer perceptron (MLP) network.",3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
One of the core steps is to compute the gradients of the validation loss wrt.,3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
"the local perturbation , We can consider a multi-layered network where we have parameters for each layer θ = {θl}Ll=1, and at every layer, we first compute zl the pre-activation, a weighted sum of inputs to the layer, and afterwards we apply a non-linear activation function σ to obtain z̃l the post-activation:
zl = θ > l z̃l−1, (10)
z̃l = σ(zl).",3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
"(11)
During backpropagation, let gl be the gradients of loss wrt. zl, and the gradients wrt.",3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
θl is given by z̃l−1g>l .,3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
"We can further express the gradients towards as a sum of local dot products.
∂ ∂",3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
"i,t E [ fv(θt+1( )) ∣∣∣",3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
"i,t=0 ] ∝− 1
m m∑ j=1 ∂fvj (θ) ∂θ ∣∣∣",3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
>,3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
θ=θt ∂fi(θ) ∂θ ∣∣∣ θ,3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
"=θt
=− 1 m m∑ j=1 L∑ l=1",3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
"(z̃vj,l−1 >z̃i,l−1)(g v j,l >gi,l).
",3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
"(12)
Detailed derivations can be found in Supplementary Materials.",3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
Eq. 12 suggests that the meta-gradient on is composed of the sum of the products of two terms: z>zv and g>gv.,3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
"The first dot product computes the similarity between the training and validation inputs to the layer, while the second computes the similarity between the training and validation gradient directions.",3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
"In other words, suppose that a pair of training and validation examples are very similar, and they also provide similar gradient directions, then this training example is helpful and should be up-weighted, and conversely, if they provide opposite gradient directions, this training example is harmful and should be downweighed.",3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
"In an MLP and a CNN, the unnormalized weights can be calculated based on the sum of the correlations of layerwise activation gradients and input activations.",3.3. Implementation using automatic differentiation,[0],[0]
"In more general networks, we can leverage automatic differentiation techniques to compute the gradient of the validation loss wrt.",3.3. Implementation using automatic differentiation,[0],[0]
the example weights of the current batch.,3.3. Implementation using automatic differentiation,[0],[0]
"As shown in Figure 1, to get the gradients of the example weights, one needs to first unroll the gradient graph of the training batch, and then use backward-on-backward automatic differentiation to take a second order gradient
pass (see Step 5 in Figure 1).",3.3. Implementation using automatic differentiation,[0],[0]
We list detailed step-bystep pseudo-code in Algorithm 1.,3.3. Implementation using automatic differentiation,[0],[0]
"This implementation can be generalized to any deep learning architectures and can be very easily implemented using popular deep learning frameworks such as TensorFlow (Abadi et al., 2016).
",3.3. Implementation using automatic differentiation,[0],[0]
"Algorithm 1 Learning to Reweight Examples using Automatic Differentiation Require: θ0, Df , Dg , n, m Ensure: θT
1: for t = 0 ...",3.3. Implementation using automatic differentiation,[0],[0]
T,3.3. Implementation using automatic differentiation,[0],[0]
"− 1 do 2: {Xf , yf} ← SampleMiniBatch(Df , n) 3: {Xg, yg} ← SampleMiniBatch(Dg , m) 4: ŷf ← Forward(Xf , yf , θt) 5: ← 0; lf ← ∑n i=1",3.3. Implementation using automatic differentiation,[0],[0]
"iC(yf,i, ŷf,i) 6: ∇θt ← BackwardAD(lf , θt) 7: θ̂t ← θt − α∇θt 8: ŷg",3.3. Implementation using automatic differentiation,[0],[0]
"← Forward(Xg, yg, θ̂t) 9: lg ← 1m ∑m i=1",3.3. Implementation using automatic differentiation,[0],[0]
"C(yg,i, ŷg,i)
10: ∇ ← BackwardAD(lg, ) 11:",3.3. Implementation using automatic differentiation,[0],[0]
"w̃ ← max(−∇ , 0); w ← w̃∑
j w̃+δ( ∑ j w̃)
12: l̂f ← ∑n i=1",3.3. Implementation using automatic differentiation,[0],[0]
"wiC(yi, ŷf,i) 13: ∇θt",3.3. Implementation using automatic differentiation,[0],[0]
"← BackwardAD(l̂f , θt) 14: θt+1 ← OptimizerStep(θt,∇θt) 15: end for
Training time Our automatic reweighting method will introduce a constant factor of overhead.",3.3. Implementation using automatic differentiation,[0],[0]
"First, it requires two full forward and backward passes of the network on training and validation respectively, and then another backward on backward pass (Step 5 in Figure 1), to get the gradients to the example weights, and finally a backward pass to minimize the reweighted objective.",3.3. Implementation using automatic differentiation,[0],[0]
"In modern networks, a backwardon-backward pass usually takes about the same time as a forward pass, and therefore compared to regular training, our method needs approximately 3× training time; it is also possible to reduce the batch size of the validation pass for speedup.",3.3. Implementation using automatic differentiation,[0],[0]
"We expect that it is worthwhile to spend the extra time to avoid the irritation of choosing early stopping, finetuning schedules, and other hyperparameters.",3.3. Implementation using automatic differentiation,[0],[0]
"Convergence results of SGD based optimization methods are well-known (Reddi et al., 2016).",3.4. Analysis: convergence of the reweighted training,[0],[0]
"However it is still meaningful to establish a convergence result about our method since it involves optimization of two-level objectives (Eq. 1, 2) rather than one, and we further make some firstorder approximation by introducing Eq. 7.",3.4. Analysis: convergence of the reweighted training,[0],[0]
"Here, we show theoretically that our method converges to the critical point of the validation loss function under some mild conditions, and we also give its convergence rate.",3.4. Analysis: convergence of the reweighted training,[0],[0]
"More detailed proofs can be found in the Supplementary Materials.
",3.4. Analysis: convergence of the reweighted training,[0],[0]
Definition 1.,3.4. Analysis: convergence of the reweighted training,[0],[0]
"A function f(x) : Rd → R is said to be Lipschitz-smooth with constant L if
‖∇f(x)−∇f(y)‖ ≤",3.4. Analysis: convergence of the reweighted training,[0],[0]
"L‖x− y‖,∀x, y ∈ Rd.
Definition 2. f(x) has σ-bounded gradients if ‖∇f(x)‖ ≤ σ for all x ∈ Rd.
",3.4. Analysis: convergence of the reweighted training,[0],[0]
"In most real-world cases, the high-quality validation set is really small, and thus we could set the mini-batch size m to be the same as the size of the validation set M .",3.4. Analysis: convergence of the reweighted training,[0],[0]
"Under this condition, the following lemma shows that our algorithm always converges to a critical point of the validation loss.",3.4. Analysis: convergence of the reweighted training,[0],[0]
"However, our method is not equivalent to training a model only on this small validation set.",3.4. Analysis: convergence of the reweighted training,[0],[0]
Because directly training a model on a small validation set will lead to severe overfitting issues.,3.4. Analysis: convergence of the reweighted training,[0],[0]
"On the contrary, our method can leverage useful information from a larger training set, and still converge to an appropriate distribution favored by this clean and balanced validation dataset.",3.4. Analysis: convergence of the reweighted training,[0],[0]
"This helps both generalization and robustness to biases in the training set, which will be shown in our experiments.
",3.4. Analysis: convergence of the reweighted training,[0],[0]
Lemma 1.,3.4. Analysis: convergence of the reweighted training,[0],[0]
"Suppose the validation loss function is Lipschitzsmooth with constant L, and the train loss function fi of training data xi have σ-bounded gradients.",3.4. Analysis: convergence of the reweighted training,[0],[0]
"Let the learning rate αt satisfies αt ≤ 2nLσ2 , where n is the training batch size.",3.4. Analysis: convergence of the reweighted training,[0],[0]
"Then, following our algorithm, the validation loss always monotonically decreases for any sequence of training batches, namely,
G(θt+1) ≤ G(θt), (13)
where G(θ) is the total validation loss
G(θ) = 1
M M∑ i=1",3.4. Analysis: convergence of the reweighted training,[0],[0]
fvi (θt+1( )).,3.4. Analysis: convergence of the reweighted training,[0],[0]
"(14)
Furthermore, in expectation, the equality in Eq. 13 holds only when the gradient of validation loss becomes 0 at some time step t, namely Et [G(θt+1)]",3.4. Analysis: convergence of the reweighted training,[0],[0]
"= G(θt) if and only if ∇G(θt) = 0, where the expectation is taking over possible training batches at time step t.
Moreover, we can prove the convergence rate of our method to be O(1/ 2).
",3.4. Analysis: convergence of the reweighted training,[0],[0]
Theorem 2.,3.4. Analysis: convergence of the reweighted training,[0],[0]
"Suppose G, fi and αt satisfy the aforementioned conditions, then Algorithm 1 achieves E",3.4. Analysis: convergence of the reweighted training,[0],[0]
[ ‖∇G(θt)‖2 ] ≤ in O(1/ 2) steps.,3.4. Analysis: convergence of the reweighted training,[0],[0]
"More specifically,
min 0<t<T
E [ ‖∇G(θt)‖2 ] ≤ C√
T , (15)
where C is some constant independent of the convergence process.",3.4. Analysis: convergence of the reweighted training,[0],[0]
"To test the effectiveness of our reweighting algorithm, we designed both class imbalance and noisy label settings, and a combination of both, on standard MNIST and CIFAR benchmarks for image classification using deep CNNs.",4. Experiments,[0],[0]
We use the standard MNIST handwritten digit classification dataset and subsample the dataset to generate a class imbalance binary classification task.,4.1. MNIST data imbalance experiments,[0],[0]
"We select a total of 5,000 images of size 28×28 on class 4 and 9, where 9 dominates the training data distribution.",4.1. MNIST data imbalance experiments,[0],[0]
We train a standard LeNet on this task and we compare our method with a suite of commonly used tricks for class imbalance: 1) PROPORTION weights each example by the inverse frequency 2),4.1. MNIST data imbalance experiments,[0],[0]
"RESAMPLE samples a class-balanced minibatch for each iteration 3) HARD MINING selects the highest loss examples from the majority class and 4) RANDOM is a random example weight baseline that assigns weights based on a rectified Gaussian distribution:
wrndi = max(zi, 0)∑",4.1. MNIST data imbalance experiments,[0],[0]
"i max(zi, 0) , where zi ∼ N (0, 1).",4.1. MNIST data imbalance experiments,[0],[0]
"(16)
To make sure that our method does not have the privilege of training on more data, we split the balanced validation set of 10 images directly from the training set.",4.1. MNIST data imbalance experiments,[0],[0]
"The network is trained with SGD with a learning rate of 1e-3 and mini-batch size of 100 for a total of 8,000 steps.
",4.1. MNIST data imbalance experiments,[0],[0]
Figure 2 plots the test error rate across various imbalance ratios averaged from 10 runs with random splits.,4.1. MNIST data imbalance experiments,[0],[0]
Note that our method significantly outperforms all the baselines.,4.1. MNIST data imbalance experiments,[0],[0]
"With class imbalance ratio of 200:1, our method only reports a small increase of error rate around 2%, whereas other methods suffer terribly under this setting.",4.1. MNIST data imbalance experiments,[0],[0]
"Compared with resampling and hard negative mining baselines, our approach does not throw away samples based on its class or training loss - as long as a sample is helpful towards the validation loss, it will be included as a part of the training loss.",4.1. MNIST data imbalance experiments,[0],[0]
Reweighting algorithm can also be useful on datasets where the labels are noisy.,4.2. CIFAR noisy label experiments,[0],[0]
"We study two settings of label noise here:
• UNIFORMFLIP: All label classes can uniformly flip to any other label classes, which is the most studied in the literature.",4.2. CIFAR noisy label experiments,[0],[0]
• BACKGROUNDFLIP:,4.2. CIFAR noisy label experiments,[0],[0]
All label classes can flip to a single background class.,4.2. CIFAR noisy label experiments,[0],[0]
This noise setting is very realistic.,4.2. CIFAR noisy label experiments,[0],[0]
"For instance, human annotators may not have recognized all the positive instances, while the
rest remain in the background class.",4.2. CIFAR noisy label experiments,[0],[0]
"This is also a combination of label imbalance and label noise since the background class usually dominates the label distribution.
",4.2. CIFAR noisy label experiments,[0],[0]
"We compare our method with prior work on the noisy label problem.
",4.2. CIFAR noisy label experiments,[0],[0]
"• REED, proposed by Reed et al. (2014), is a bootstrapping technique where the training target is a convex combination of the model prediction and the label.
",4.2. CIFAR noisy label experiments,[0],[0]
"• S-MODEL, proposed by Goldberger & Ben-Reuven (2017), adds a fully connected softmax layer after the regular classification output layer to model the noise transition matrix.
",4.2. CIFAR noisy label experiments,[0],[0]
"• MENTORNET, proposed by Jiang et al. (2017), is an RNN-based meta-learning model that takes in a sequence of loss values and outputs the example weights.",4.2. CIFAR noisy label experiments,[0],[0]
"We compare numbers reported in their paper with a base model that achieves similar test accuracy under 0% noise.
",4.2. CIFAR noisy label experiments,[0],[0]
"In addition, we propose two simple baselines: 1) RANDOM, which assigns weights according to a rectified Gaussian (see Eq. 16); 2) WEIGHTED, designed for BACKGROUNDFLIP, where the model knows the oracle noise ratio for each class and reweights the training loss proportional to the percentage of clean images of that label class.
",4.2. CIFAR noisy label experiments,[0],[0]
"Clean validation set For UNIFORMFLIP, we use 1,000 clean images in the validation set; for BACKGROUNDFLIP, we use 10 clean images per label class.",4.2. CIFAR noisy label experiments,[0],[0]
"Since our method uses information from the clean validation, for a fair comparison, we conduct an additional finetuning on the clean data based on the pre-trained baselines.",4.2. CIFAR noisy label experiments,[0],[0]
"We also study the effect on the size of the clean validation set in an ablation study.
",4.2. CIFAR noisy label experiments,[0],[0]
"Hyper-validation set For monitoring training progress and tuning baseline hyperparameters, we split out another
5,000 hyper-validation set from the 50,000 training images.",4.2. CIFAR noisy label experiments,[0],[0]
"We also corrupt the hyper-validation set with the same noise type.
",4.2. CIFAR noisy label experiments,[0],[0]
"Experimental details For REED model, we use the best β reported in Reed et al. (2014) (β = 0.8 for hard bootstrapping and β = 0.95 for soft bootstrapping).",4.2. CIFAR noisy label experiments,[0],[0]
"For the S-MODEL, we explore two versions to initialize the transition weights: 1) a smoothed identity matrix; 2) in background flip experiments we consider initializing the transition matrix with the confusion matrix of a pre-trained baseline model (S-MODEL +CONF).",4.2. CIFAR noisy label experiments,[0],[0]
"We find baselines can easily overfit the training noise, and therefore we also study early stopped versions of the baselines to provide a stronger comparison.",4.2. CIFAR noisy label experiments,[0],[0]
"In contrast, we find early stopping not necessary for our method.
",4.2. CIFAR noisy label experiments,[0],[0]
"To make our results comparable with the ones reported in MENTORNET and to save computation time, we exchange their Wide ResNet-101-10 with a Wide ResNet28-10 (WRN-28-10) (Zagoruyko & Komodakis, 2016) with dropout 0.3 as our base model in the UNIFORMFLIP experiments.",4.2. CIFAR noisy label experiments,[0],[0]
We find that test accuracy differences between the two base models are within 0.5% on CIFAR datasets under 0% noise.,4.2. CIFAR noisy label experiments,[0],[0]
"In the BACKGROUNDFLIP experiments, we use a ResNet-32 (He et al., 2016) as our base model.
",4.2. CIFAR noisy label experiments,[0],[0]
"We train the models with SGD with momentum, at an initial learning rate 0.1 and a momentum 0.9 with mini-batch size 100.",4.2. CIFAR noisy label experiments,[0],[0]
"For ResNet-32 models, the learning rate decays×0.1 at 40K and 60K steps, for a total of 80K steps.",4.2. CIFAR noisy label experiments,[0],[0]
"For WRN and early stopped versions of ResNet-32 models, the learning rate decays at 40K and 50K steps, for a total of 60K steps.",4.2. CIFAR noisy label experiments,[0],[0]
"Under regular 0% noise settings, our base ResNet-32 gets 92.5% and 68.1% classification accuracy on CIFAR-10 and 100, and the WRN-28-10 gets 95.5% and 78.2%.",4.2. CIFAR noisy label experiments,[0],[0]
"For the finetuning stage, we run extra 5K steps of training on the
CLEAN ONLY 15.90 ± 3.32 8.06 ± 0.76 BASELINE +FT 82.82 ± 0.93 54.23 ± 1.75 BASELINE +ES +FT 85.19 ± 0.46 55.22 ± 1.40 WEIGHTED +FT 85.98 ± 0.47 53.99 ± 1.62 S-MODEL +CONF +FT 81.90 ± 0.85 53.11 ± 1.33 S-MODEL +CONF +ES +FT 85.86 ± 0.63 55.75 ± 1.26
OURS 86.73 ± 0.48 59.30 ± 0.60
limited clean data.
",4.2. CIFAR noisy label experiments,[0],[0]
"We report the average test accuracy for 5 different random splits of clean and noisy labels, with 95% confidence interval in Table 1 and 2.",4.2. CIFAR noisy label experiments,[0],[0]
"The background classes for the 5 trials are [0, 1, 3, 5, 7] (CIFAR-10) and [7, 12, 41, 62, 85] (CIFAR-100).",4.2. CIFAR noisy label experiments,[0],[0]
"The first result that draws our attention is that “Random” performs surprisingly well on the UNIFORMFLIP benchmark, outperforming all historical methods that we compared.",4.3. Results and Discussion,[0],[0]
"Given that its performance is comparable with Baseline on BACKGROUNDFLIP and MNIST class imbalance, we hypothesize that random example weights act as a strong regularizer and under which the learning objective on UNIFORMFLIP is still consistent.
",4.3. Results and Discussion,[0],[0]
"Regardless of the strong baseline, our method ranks the top on both UNIFORMFLIP and BACKGROUNDFLIP, showing our method is less affected by the changes in the noise type.",4.3. Results and Discussion,[0],[0]
"On CIFAR-100, our method wins more than 3% compared to the state-of-the-art method.
",4.3. Results and Discussion,[0],[0]
Understanding the reweighting mechanism It is beneficial to understand how our reweighting algorithm contributes to learning more robust models during training.,4.3. Results and Discussion,[0],[0]
"First, we use a pre-trained model (trained at half of the total iterations without learning rate decay) and measure the example weight distribution of a randomly sampled batch of validation images, which the model has never seen.",4.3. Results and Discussion,[0],[0]
"As shown in the left figure of Figure 3, our model correctly
pushes most noisy images to zero weights.",4.3. Results and Discussion,[0],[0]
"Secondly, we conditioned the input mini-batch to be a single nonbackground class and randomly flip 40% of the images to the background, and we would like to see how well our model can distinguish clean and noisy images.",4.3. Results and Discussion,[0],[0]
"As shown in Figure 3 right, the model is able to reliably detect images that are flipped to the background class.
",4.3. Results and Discussion,[0],[0]
"Robustness to overfitting noise Throughout experimentation, we find baseline models can easily overfit to the noise in the training set.",4.3. Results and Discussion,[0],[0]
"For example, shown in Table 2, applying early stopping (“ES”) helps the classification performance of “S-Model” by over 10% on CIFAR-10.",4.3. Results and Discussion,[0],[0]
"Figure 6 compares the final confusion matrices of the baseline and the proposed algorithm, where a large proportion of noise transition probability is cleared in the final prediction.",4.3. Results and Discussion,[0],[0]
Figure 7 shows training curves on the BACKGROUNDFLIP experiments.,4.3. Results and Discussion,[0],[0]
"After the first learning rate decay, both “Baseline” and “SModel” quickly degrade their validation performance due to overfitting, while our model remains the same validation accuracy until termination.",4.3. Results and Discussion,[0],[0]
"Note that here “S-Model” knows the oracle noise ratio in each class, and this information is
not available in our method.
",4.3. Results and Discussion,[0],[0]
Impact of the noise level We would like to investigate how strongly our method can perform on a variety of noise levels.,4.3. Results and Discussion,[0],[0]
"Shown in Figure 5, our method only drops 6% accuracy when the noise ratio increased from 0% to 50%;
whereas the baseline has dropped more than 40%.",4.3. Results and Discussion,[0],[0]
"At 0% noise, our method only slightly underperforms baseline.",4.3. Results and Discussion,[0],[0]
"This is reasonable since we are optimizing on the validation set, which is strictly a subset of the full training set, and therefore suffers from its own subsample bias.
",4.3. Results and Discussion,[0],[0]
"Size of the clean validation set When the size of the clean validation set grows larger, fine-tuning on the validation set will be a reasonble approach.",4.3. Results and Discussion,[0],[0]
"Here, we make an attempt to explore the tradeoff and understand when fine-tuning becomes beneficial.",4.3. Results and Discussion,[0],[0]
Figure 4 plots the classification performance when we varied the size of the clean validation on BACKGROUNDFLIP.,4.3. Results and Discussion,[0],[0]
"Surprisingly, using 15 validation images for all classes only results in a 2% drop in performance, and the overall classification performance does not grow after having more than 100 validation images.",4.3. Results and Discussion,[0],[0]
"In comparison, we observe a significant drop in performance when only fine-tuning on these 15 validation images for the baselines, and the performance catches up around using 1,000 validation images (100 per class).",4.3. Results and Discussion,[0],[0]
"This phenomenon suggests that in our method the clean validation acts more like a regularizer rather than a data source for parameter finetuning, and potentially our method can be complementary with fine-tuning based method when the size of the clean set grows larger.",4.3. Results and Discussion,[0],[0]
"In this work, we propose an online meta-learning algorithm for reweighting training examples and training more robust deep learning models.",5. Conclusion,[0],[0]
"While various types of training set biases exist and manually designed reweighting objectives have their own bias, our automatic reweighting algorithm shows superior performance dealing with class imbalance, noisy labels, and both.",5. Conclusion,[0],[0]
Our method can be directly applied to any deep learning architecture and is expected to train end-to-end without any additional hyperparameter search.,5. Conclusion,[0],[0]
"Validating on every training step is a novel setting and we show that it has links with model regularization, which can be a fruitful future research direction.",5. Conclusion,[0],[0]
Deep neural networks have been shown to be very powerful modeling tools for many supervised learning tasks involving complex input patterns.,abstractText,[0],[0]
"However, they can also easily overfit to training set biases and label noises.",abstractText,[0],[0]
"In addition to various regularizers, example reweighting algorithms are popular solutions to these problems, but they require careful tuning of additional hyperparameters, such as example mining schedules and regularization hyperparameters.",abstractText,[0],[0]
"In contrast to past reweighting methods, which typically consist of functions of the cost value of each example, in this work we propose a novel meta-learning algorithm that learns to assign weights to training examples based on their gradient directions.",abstractText,[0],[0]
"To determine the example weights, our method performs a meta gradient descent step on the current mini-batch example weights (which are initialized from zero) to minimize the loss on a clean unbiased validation set.",abstractText,[0],[0]
"Our proposed method can be easily implemented on any type of deep network, does not require any additional hyperparameter tuning, and achieves impressive performance on class imbalance and corrupted label problems where only a small amount of clean validation data is available.",abstractText,[0],[0]
Learning to Reweight Examples for Robust Deep Learning,title,[0],[0]
"Many natural language processing (NLP) and computer vision problems necessitate predicting structured outputs such as labeled sequences, trees or general graphs (Smith, 2010; Nowozin & Lampert, 2011).",1. Introduction,[0],[0]
Such tasks require modeling both input-output relationships and the interactions between predicted outputs to capture correlations.,1. Introduction,[0],[0]
"Across the various structured prediction formulations (Lafferty et al., 2001; Taskar et al., 2003; Chang et al., 2012), prediction requires solving inference problems by searching for scoremaximizing output structures.",1. Introduction,[0],[0]
"The search space for inference is typically large (e.g., all parse trees), and grows with input size.",1. Introduction,[0],[0]
"Exhaustive search can be prohibitive and standard alternatives are either: (a) perform exact inference with a large computational cost or, (b) approximate inference to sacrifice accuracy in favor of time.
",1. Introduction,[0],[0]
"1School of Computing, University of Utah, Salt Lake City, Utah, USA.",1. Introduction,[0],[0]
"Correspondence to: Xingyuan Pan <xpan@cs.utah.edu>, Vivek Srikumar <svivek@cs.utah.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"In this paper, we focus on the computational cost of inference.",1. Introduction,[0],[0]
"We argue that naturally occurring problems have remarkable regularities across both inputs and outputs, and traditional formulations of inference ignore them.",1. Introduction,[0],[0]
"For example, parsing an n-word sentence will cost a standard head-driven lexical parser O(n5) time.",1. Introduction,[0],[0]
Current practice in NLP is to treat each new sentence as a fresh discrete optimization problem and pay the computational price each time.,1. Introduction,[0],[0]
"However, this practice is not only expensive, but also wasteful!",1. Introduction,[0],[0]
"We ignore the fact that slight changes to inputs often do not change the output, or even the sequence of steps taken to produce it.",1. Introduction,[0],[0]
"Moreover, not all outputs are linguistically meaningful structures; as we make more predictions, we should be able to learn to prune the output space.
",1. Introduction,[0],[0]
The motivating question that drives our work is: Can we design inference schemes that learn to make a trained structured predictor faster without sacrificing output quality?,1. Introduction,[0],[0]
"After training, the structured classifier can be thought as a black-box.",1. Introduction,[0],[0]
"Typically, once deployed, it is never modified over its lifetime of classifying new examples.",1. Introduction,[0],[0]
"Subsequently, we can view each prediction of the black-box classifier as an opportunity to learn how to navigate the output space more efficiently.",1. Introduction,[0],[0]
"Thus, if the classifier sees a previously encountered situation, it could make some decisions without needless computations.
",1. Introduction,[0],[0]
We formalize this intuition by considering the trained models as solving arbitrary integer linear programs (ILPs) for combinatorial inference.,1. Introduction,[0],[0]
"We train a second, inexpensive speedup classifier which acts as a heuristic for a searchbased inference algorithm that mimics the more expensive black-box classifier.",1. Introduction,[0],[0]
The speedup heuristic is a function that learns regularities among predicted structures.,1. Introduction,[0],[0]
"We present a mistake bound algorithm that, over the classifier’s lifetime, learns to navigate the feasible regions of the ILPs.",1. Introduction,[0],[0]
"By doing so, we can achieve a reduction in inference time.
",1. Introduction,[0],[0]
We further identify inference situations where the learned speedup heuristic alone can correctly label parts of the outputs without computing the corresponding input features.,1. Introduction,[0],[0]
"In such situations, the search algorithm can safely ignore parts of inputs if the corresponding outputs can be decided based on the sub-structures constructed so far.",1. Introduction,[0],[0]
"Seen this way, the speedup classifier can be seen as a statistical cache of past decisions made by the black-box classifier.
",1. Introduction,[0],[0]
We instantiate our strategy to the task of predicting entities and relations from sentences.,1. Introduction,[0],[0]
"Using an ILP based black-box classifier, we show that the trained speedup classifier mimics the reference inference algorithm to obtain improvements in running time, and also recovers its accuracy.",1. Introduction,[0],[0]
"Indeed, by learning to ignore input components when they will not change the prediction, we show that learned search strategy outperforms even greedy search in terms of speed.
",1. Introduction,[0],[0]
"To summarize, the main contribution of this paper is the formalization of the problem of learning to make structured output classifiers faster without sacrificing accuracy.",1. Introduction,[0],[0]
We develop a learning-to-search framework to train a speedup classifier with a mistake-bound guarantee and a sufficient condition to safely avoid computing input-based features.,1. Introduction,[0],[0]
"We show empirically on an entity-relation extraction task that we can learn a speedup classifier that is (a) faster than both the state-of-the-art Gurobi optimizer and greedy search, and (b) does not incur a loss in output quality.",1. Introduction,[0],[0]
"First, we will define the notation used in this paper with a running example that requires of identifying entity types and their relationships in text.",2. Notation and Preliminaries,[0],[0]
"The input to the problem consists of sentences such as:
Colin went back home in Ordon Village.
",2. Notation and Preliminaries,[0],[0]
"These inputs are typically preprocessed — here, we are given spans of text (underlined) corresponding to entities.",2. Notation and Preliminaries,[0],[0]
"We will denote such preprocessed inputs to the structured prediction problem as x.
We seek to produce a structure y ∈",2. Notation and Preliminaries,[0],[0]
"Yx (e.g., labeled trees, graphs) associated with these inputs.",2. Notation and Preliminaries,[0],[0]
"Here, Yx is the set of all possible output structures for the input x.",2. Notation and Preliminaries,[0],[0]
"In the example problem, our goal is to assign types to the entities and also label the relationships between them.",2. Notation and Preliminaries,[0],[0]
"Suppose our task has three types of entities: person, location and organization.",2. Notation and Preliminaries,[0],[0]
"A pair of entities can participate in one of five possible directed relations: Kill, LiveIn, WorkFor, LocatedAt and OrgBasedIn.",2. Notation and Preliminaries,[0],[0]
"Additionally, there is a special entity label NoEnt meaning a text span is not an entity, and a special relation label NoRel indicating that two spans are unrelated.",2. Notation and Preliminaries,[0],[0]
"Figure 1 shows a plausible structure for the example sentence as per this scheme.
",2. Notation and Preliminaries,[0],[0]
A standard way to model the prediction problem requires learning a model that scores all structures in Yx and searching for the score-maximizing structure.,2. Notation and Preliminaries,[0],[0]
"Linear models are commonly used as scoring functions, and require a feature vector characterizing input-output relationships Φ (x,y).",2. Notation and Preliminaries,[0],[0]
We will represent the model by a weight vector α.,2. Notation and Preliminaries,[0],[0]
"Every structure y associated with an input x is scored as the dot product α · Φ (x,y).",2. Notation and Preliminaries,[0],[0]
"The goal of prediction is to find the
structure y∗ that maximizes this score.",2. Notation and Preliminaries,[0],[0]
"That is,
y∗ = arg max y∈Yx α ·",2. Notation and Preliminaries,[0],[0]
"Φ (x,y) .",2. Notation and Preliminaries,[0],[0]
"(1)
Learning involves using training data to find the best weight vector α.
",2. Notation and Preliminaries,[0],[0]
"In general, the output structure y is a set of K categorical inference variables {y1, y2, · · · , yK} , each of which can take a value from a predefined set of n labels.",2. Notation and Preliminaries,[0],[0]
"That is, each yk ∈ y takes a value from {l1, l2, · · · , ln}.1 In our running example, the inference variables correspond to the four decisions that define the structure: the labels for the two entities, and the relations in each direction.",2. Notation and Preliminaries,[0],[0]
"The feature function Φ decomposes into a sum of features over each yk, each denoted by Φk, giving us the inference problem:
y∗ = arg max y∈Yx K∑",2. Notation and Preliminaries,[0],[0]
k=1 α ·,2. Notation and Preliminaries,[0],[0]
"Φk ( x, yk ) .",2. Notation and Preliminaries,[0],[0]
"(2)
The dependencies between the yk’s specify the nature of the output space.",2. Notation and Preliminaries,[0],[0]
Determining each yk in isolation greedily does not typically represent a viable inference strategy because constraints connecting the variables are ignored.,2. Notation and Preliminaries,[0],[0]
"In this spirit, the problem of finding the best structure can be viewed as a combinatorial optimization problem.
",2. Notation and Preliminaries,[0],[0]
"In this paper, we consider the scenario in which we have already trained a model α.",2. Notation and Preliminaries,[0],[0]
"We focus on solving the inference problem (i.e.,Eq.",2. Notation and Preliminaries,[0],[0]
(2)) efficiently.,2. Notation and Preliminaries,[0],[0]
We conjecture that it should be possible to observe a black-box inference algorithm over its lifetime to learn to predict faster without losing accuracy.,2. Notation and Preliminaries,[0],[0]
One common way to solve inference is by designing efficient dynamic programming algorithms that exploit problem structure.,2.1. Black-box Inference Mechanisms,[0],[0]
"While effective, this approach is limited to special cases where the problem admits efficient decoding, thus placing restrictions on factorization and feature design.
",2.1. Black-box Inference Mechanisms,[0],[0]
"In this paper, we seek to reason about the problem of predicting structures in the general case.",2.1. Black-box Inference Mechanisms,[0],[0]
"Since inference is essentially a combinatorial optimization problem, without loss
1We make this choice for simplicity of notation.",2.1. Black-box Inference Mechanisms,[0],[0]
"In general, K depends on the size of the input x, and categorical variables may take values from different label sets.
of generality, we can represent any inference problem as an integer linear programming (ILP) instance (Schrijver, 1998).",2.1. Black-box Inference Mechanisms,[0],[0]
To represent the inference task in Eq.,2.1. Black-box Inference Mechanisms,[0],[0]
"(2) as an ILP instance, we will define indicator variables of the form zki ∈ {0, 1}, which stands for the decision that the categorical variable yk is assigned the ith label among the n labels.",2.1. Black-box Inference Mechanisms,[0],[0]
"That is, zki = 1 if yk = li, and 0 otherwise.",2.1. Black-box Inference Mechanisms,[0],[0]
"Using this notation, we can write the cost of any structure y in terms of the indicators as
K∑ k=1 n∑ i=1",2.1. Black-box Inference Mechanisms,[0],[0]
cki,2.1. Black-box Inference Mechanisms,[0],[0]
z,2.1. Black-box Inference Mechanisms,[0],[0]
k,2.1. Black-box Inference Mechanisms,[0],[0]
i .,2.1. Black-box Inference Mechanisms,[0],[0]
"(3)
Here, cki is a stand in for −α ·",2.1. Black-box Inference Mechanisms,[0],[0]
"Φk (x, li), namely the cost (negative score) associated with this decision.2",2.1. Black-box Inference Mechanisms,[0],[0]
"In our example, suppose the first categorical variable y1 corresponds to the entity Colin, and it has possible labels {person,location, . . .",2.1. Black-box Inference Mechanisms,[0],[0]
}.,2.1. Black-box Inference Mechanisms,[0],[0]
"Then, assigning person to Colin would correspond to setting z11 = 1, and z 1 i = 0 for all i 6= 1.",2.1. Black-box Inference Mechanisms,[0],[0]
"Using the labels enumerated in §2, there will be 20 indicators for the four categorical decisions.
",2.1. Black-box Inference Mechanisms,[0],[0]
"Of course, arbitrary assignments to the indicators is not allowed.",2.1. Black-box Inference Mechanisms,[0],[0]
We can define the set of feasible structures using linear constraints.,2.1. Black-box Inference Mechanisms,[0],[0]
"Clearly, each categorical variable can take exactly one label, which can be expressed via:
n∑ i=1 zki = 1, for all k. (4)
",2.1. Black-box Inference Mechanisms,[0],[0]
"In addition, we can define the set of valid structures Yx using a collection of m linear constraints, the jth one of which can be written as
K∑ k=1 n∑ i=1",2.1. Black-box Inference Mechanisms,[0],[0]
"Akjiz k i = bj , for all j. (5)
",2.1. Black-box Inference Mechanisms,[0],[0]
These structural constraints characterize the interactions between the categorical variables.,2.1. Black-box Inference Mechanisms,[0],[0]
"For example, if a directed edge in our running example is labeled as LiveIn, then, its source and target must be a person and a location respectively.",2.1. Black-box Inference Mechanisms,[0],[0]
"While Eq.(5) only shows equality constraints, in practice, inequality constraints can also be included.
",2.1. Black-box Inference Mechanisms,[0],[0]
The inference problem in Eq.,2.1. Black-box Inference Mechanisms,[0],[0]
(2) is equivalent to the problem of minimizing the objective in Eq.,2.1. Black-box Inference Mechanisms,[0],[0]
(3) over the 0-1 indicator variables subject to the constraints in Eqs.,2.1. Black-box Inference Mechanisms,[0],[0]
"(4) and (5).
",2.1. Black-box Inference Mechanisms,[0],[0]
We should note the difference between the ability to write an inference problem as an ILP instance and actually solving it as one.,2.1. Black-box Inference Mechanisms,[0],[0]
"The former gives us the ability to reason about inference in general, and perhaps using other methods (such as Lagrangian relaxation (Lemaréchal, 2001)) for inference.",2.1. Black-box Inference Mechanisms,[0],[0]
"However, solving problems with industrial strength ILP
2The negation defines an equivalent minimization problem and makes subsequent description of the search framework easier.
solvers such as the Gurobi solver3 is competitive with other approaches in terms of inference time, even though they may not directly exploit problem structure.
",2.1. Black-box Inference Mechanisms,[0],[0]
"In this work, we use the general structure of the ILP inference formulation to develop the theory for speeding up inference.",2.1. Black-box Inference Mechanisms,[0],[0]
"In addition, because of its general applicability and fast inference speed, we use the Gurobi ILP solver as our black-box classifier, and learn a speedup heuristic to make even faster inference.",2.1. Black-box Inference Mechanisms,[0],[0]
Directly applying the black-box solver for the large output spaces may be impractical.,2.2. Inference as Search,[0],[0]
An alternative general purpose strategy for inference involves framing the maximization in Eq.,2.2. Inference as Search,[0],[0]
"(2) as a graph search problem.
",2.2. Inference as Search,[0],[0]
"Following Russell & Norvig (2003); Xu et al. (2009), a general graph search problem requires defining an initial search node I , a successor function s(·), and a goal test.",2.2. Inference as Search,[0],[0]
The successor function s(·) maps a search node to its successors.,2.2. Inference as Search,[0],[0]
The goal test determines whether a node is a goal node.,2.2. Inference as Search,[0],[0]
"Usually, each search step is associated with a cost function, and we seek to find a goal node with the least total cost.
",2.2. Inference as Search,[0],[0]
We can define the search problem corresponding to inference as follows.,2.2. Inference as Search,[0],[0]
"We will denote a generic search node in the graph as v, which corresponds to a set of partially assigned categorical variables.",2.2. Inference as Search,[0],[0]
"Specifically, we will define the search node v as a set of pairs {(k, i)}, each element of which specifies that the variable yk is assigned the ith label.",2.2. Inference as Search,[0],[0]
The initial search node I is the empty set since none of the variables has been assigned when the search begins.,2.2. Inference as Search,[0],[0]
"For a node v, its successors s(v) is a set of nodes, each containing one more assigned variable than v. A node is a goal node if all variables yk’s have been assigned.",2.2. Inference as Search,[0],[0]
"The size of any goal node is K, the number of categorical variables.
",2.2. Inference as Search,[0],[0]
"In our running example, at the start of search, we may choose to assign the first label l1 (person) to the variable y1 – the entity Colin – leading us to the successor {(1, 1)}.",2.2. Inference as Search,[0],[0]
Every search node specifies a partial or a full assignment to all the entities and relations.,2.2. Inference as Search,[0],[0]
"The goal test simply checks if we arrive at a full assignment, i.e., all the entity and relation candidates have been assigned a label.
",2.2. Inference as Search,[0],[0]
"Note that goal test does not test the quality of the node, it simply tests whether the search process is finished.",2.2. Inference as Search,[0],[0]
"The quality of the goal node is determined by the path cost from the initial node to the goal node, which is the accumulated cost of each step along the way.",2.2. Inference as Search,[0],[0]
The step cost for assigning label li to a variable yk is the same cki we defined for the ILP objective in Eq.,2.2. Inference as Search,[0],[0]
(3).,2.2. Inference as Search,[0],[0]
"Finding a shortest path in such a search space is equivalent to the original ILP problem
3http://www.gurobi.com
without the structural constraints in Eq.",2.2. Inference as Search,[0],[0]
(5).,2.2. Inference as Search,[0],[0]
The uniquelabel constraints in Eq.,2.2. Inference as Search,[0],[0]
"(4) are automatically satisfied by our formulation of the search process.
",2.2. Inference as Search,[0],[0]
"Indeed, solving inference without the constraints in Eq.(5) is trivial.",2.2. Inference as Search,[0],[0]
"For each categorical variable yk, we can pick the label li that has the lowest value of cki .",2.2. Inference as Search,[0],[0]
"This gives us two possible options for solving inference as search: We can (a) ignore the constraints that make inference slow to greedily predict all the labels, or, (b) enforce constraints at each step of the search, and only consider search nodes that satisfy all constraints.",2.2. Inference as Search,[0],[0]
"The first option is fast, but can give us outputs that are invalid.",2.2. Inference as Search,[0],[0]
"For example, we might get a structure that mandates that the person Colin lives in a person called Ordon Village.",2.2. Inference as Search,[0],[0]
"The second option will give us structurally valid outputs, but can be prohibitively slow.
",2.2. Inference as Search,[0],[0]
Various graph search algorithms can be used for performing inference.,2.2. Inference as Search,[0],[0]
"For efficiency, we can use beam search with a fixed beam width b.",2.2. Inference as Search,[0],[0]
When search begins the beam B0 contains only the initial node B0 =,2.2. Inference as Search,[0],[0]
[I].,2.2. Inference as Search,[0],[0]
"Following Collins & Roark (2004); Xu et al. (2009), we define the function BreadthExpand which takes the beam Bt at step t and generates the candidates Ct+1 for the next beam:
Ct+1 = BreadthExpand(Bt) = ∪v∈Bts(v)
The next beam is given by Bt+1 = Filter(Ct+1), where Filter takes top b nodes according to some priority function p(v).",2.2. Inference as Search,[0],[0]
"In the simplest case, the priority of a node v is the total path cost of reaching that node.",2.2. Inference as Search,[0],[0]
"More generally, the priority function can be informed not only by the path cost, but also by a heuristic function as in the popular A∗ algorithm.",2.2. Inference as Search,[0],[0]
"In the previous section, we saw that using a black-box ILP solver may be slower than greedy search which ignores constraints, but produces valid outputs.",3. Speeding up Structured Prediction,[0],[0]
"However, over its lifetime, a trained classifier predicts structures for a large number of inputs.",3. Speeding up Structured Prediction,[0],[0]
"While the number of unique inputs (e.g. sentences) may be large, the number of unique structures that actually occur among the predictions is not only finite, but also small.",3. Speeding up Structured Prediction,[0],[0]
"This observation was exploited by Srikumar et al. (2012); Kundu et al. (2013) for amortizing inference costs.
",3. Speeding up Structured Prediction,[0],[0]
"In this paper, we are driven by the need for an inference algorithm that learns regularities across outputs to become faster at producing structurally valid outputs.",3. Speeding up Structured Prediction,[0],[0]
"In order to do so, we will develop an inference-as-search scheme that inherits the speed of greedy search, but learns to produce structurally valid outputs.",3. Speeding up Structured Prediction,[0],[0]
"Before developing the algorithmic aspects of such an inference scheme, let us first see a proofof-concept for such a scheme.",3. Speeding up Structured Prediction,[0],[0]
Our goal is to incorporate the structural constraints from Eq.,3.1. Heuristics for Structural Validity,[0],[0]
(5) as a heuristic for greedy or beam search.,3.1. Heuristics for Structural Validity,[0],[0]
"To do so, at each step during search, we need to estimate how likely an assignment can lead to a constraint violation.",3.1. Heuristics for Structural Validity,[0],[0]
"This information can be characterized by using a heuristic function h(v), which will be used to evaluated a node v during search.
",3.1. Heuristics for Structural Validity,[0],[0]
The dual form the ILP in Eqs.,3.1. Heuristics for Structural Validity,[0],[0]
(3) to (5) help justify the idea of capturing constraint information using a heuristic function.,3.1. Heuristics for Structural Validity,[0],[0]
We treat the unique label constraints in Eq.,3.1. Heuristics for Structural Validity,[0],[0]
"(4) as defining the domain in which each 0-1 variable zki lives, and the only real constraints are given by Eq. (5).
Let uj represent the dual variable for the jth constraint.",3.1. Heuristics for Structural Validity,[0],[0]
"Thus, we obtain the Lagrangian4
L(z, u) =",3.1. Heuristics for Structural Validity,[0],[0]
K∑,3.1. Heuristics for Structural Validity,[0],[0]
k=1 n∑ i=1,3.1. Heuristics for Structural Validity,[0],[0]
cki z,3.1. Heuristics for Structural Validity,[0],[0]
k i,3.1. Heuristics for Structural Validity,[0],[0]
− m∑ j=1 uj (,3.1. Heuristics for Structural Validity,[0],[0]
K∑ k=1 n∑ i=1,3.1. Heuristics for Structural Validity,[0],[0]
Akjiz k i,3.1. Heuristics for Structural Validity,[0],[0]
"− bj )
",3.1. Heuristics for Structural Validity,[0],[0]
"= ∑ k,i cki",3.1. Heuristics for Structural Validity,[0],[0]
"−∑ j ujA k ji  zki +∑ j bjuj
",3.1. Heuristics for Structural Validity,[0],[0]
"The dual function θ(u) = minz L(z, u), where the minimization is over the domain of the z variables.
",3.1. Heuristics for Structural Validity,[0],[0]
Denote u∗ = arg max θ(u) as the solution to the dual problem.,3.1. Heuristics for Structural Validity,[0],[0]
"In the case of zero duality gap, the theory of Lagrangian relaxation (Lemaréchal, 2001) tells us that solving the following relaxed minimization problem will solve the original ILP:
min ∑ k,i cki",3.1. Heuristics for Structural Validity,[0],[0]
"−∑ j u∗jA k ji  zki (6)∑ i zki = 1, for all k (7) zki ∈ {0, 1}, for all k, i (8)
This new optimization problem does not have any structural constraints and can be solved greedily for each k if we know the optimal dual variables u∗.
To formulate the minimization in Eqs (6) to (8) as a search problem, we define the priority function p(v) for ranking the nodes as p(v) = g(v) + h∗(v), where the path cost g(v) and heuristic function h∗(v) are given by
g(v) = ∑
(k,i)∈v
cki , (9)
h∗(v) =",3.1. Heuristics for Structural Validity,[0],[0]
"− ∑
(k,i)∈v ∑ j Akjiu ∗ j (x).",3.1. Heuristics for Structural Validity,[0],[0]
"(10)
Since Eq. (6) is a minimization problem, smaller priority value p(v) means higher ranking during search.",3.1. Heuristics for Structural Validity,[0],[0]
"Note that
4We omit the ranges of the summation indices",3.1. Heuristics for Structural Validity,[0],[0]
"i, j, k hereafter.
even though heuristic function defined in this way is not always admissible, greedy search with ranking function p(v) will lead to the exact solution of Eqs.",3.1. Heuristics for Structural Validity,[0],[0]
(6) to (8).,3.1. Heuristics for Structural Validity,[0],[0]
"In practice, however, we do not have the optimal values for the dual variables u∗.",3.1. Heuristics for Structural Validity,[0],[0]
"Indeed, when Lagrangian relaxation is used for inference, the optmial dual variables are computed using subgradient optimization for each example because their value depends on the original input via the c’s.
",3.1. Heuristics for Structural Validity,[0],[0]
"Instead of performing expensive gradient based optimization for every input instance, we will approximate the heuristic function as a classifier that learns to prioritize structurally valid outputs.",3.1. Heuristics for Structural Validity,[0],[0]
"In this paper, we use a linear model based on a weight vector w to approximate the heuristic as
h(v) = −w · φ(v) (11)
",3.1. Heuristics for Structural Validity,[0],[0]
"For an appropriate choice of node features φ(v), the heuristic h(v) in Eq.(10) is indeed a linear function.5",3.1. Heuristics for Structural Validity,[0],[0]
"In other words, there exists a linear heuristic function that can guide graph search towards creating structurally valid outputs.
",3.1. Heuristics for Structural Validity,[0],[0]
"In this setting, the priority function p(v) for each node is determined by two components: the path cost g(v) from the initial node to the current node, and the learned heuristic cost h(v), which is an estimate of how good the current node is.",3.1. Heuristics for Structural Validity,[0],[0]
"Because the purpose of the heuristic is to help improve inference speed, we call φ(v) speedup features.",3.1. Heuristics for Structural Validity,[0],[0]
The speedup features can be different from the original model features in Eq.,3.1. Heuristics for Structural Validity,[0],[0]
(2).,3.1. Heuristics for Structural Validity,[0],[0]
In particular it can includes features for partial assignments made so far which were not available in the original model features.,3.1. Heuristics for Structural Validity,[0],[0]
"In this setting, the goal of speedup learning is to find suitable weight vector w over the black-box classifier’s lifetime.",3.1. Heuristics for Structural Validity,[0],[0]
"In this section, we will describe a mistake-bound algorithm to learn the weight vector w of the speedup classifier.",4. Learning the Speedup Classifier,[0],[0]
"The design of this algorithm is influenced by learning to search algorithms such as LaSO (Daumé III & Marcu, 2005; Xu et al., 2009).",4. Learning the Speedup Classifier,[0],[0]
"We assume that we have access to a trained black-box ILP solver called Solve, which can solve the structured prediction problems, and we have a large set of examples {xi}Ni=1.",4. Learning the Speedup Classifier,[0],[0]
Our goal is to use this set to train a speedup classifier to mimic the ILP solver while predicting structures for this set of examples.,4. Learning the Speedup Classifier,[0],[0]
"Subsequently, we can use the less expensive speedup influenced search procedure to replace the ILP solver.
",4. Learning the Speedup Classifier,[0],[0]
"To define the algorithm, we will need additional terminology.",4. Learning the Speedup Classifier,[0],[0]
"Given a reference solution y, we define a node v to be ygood, if it can possibly lead to the reference solution.",4. Learning the Speedup Classifier,[0],[0]
"If a node v is y-good, then the already assigned variables have the same labels as in the reference solution.",4. Learning the Speedup Classifier,[0],[0]
"We define a
5See supplementary material for an elaboration.
",4. Learning the Speedup Classifier,[0],[0]
"Algorithm 1 Learning a speedup classifier using examples {xi}Ni=1, and a black-box Solver Solve.
1: Initialize the speedup weight vector w← 0 2: for epoch = 1 . .",4. Learning the Speedup Classifier,[0],[0]
.M,4. Learning the Speedup Classifier,[0],[0]
do 3: for i = 1 . . .,4. Learning the Speedup Classifier,[0],[0]
N,4. Learning the Speedup Classifier,[0],[0]
do 4: y← Solve(xi) 5: Initialize the beam B ←,4. Learning the Speedup Classifier,[0],[0]
"[I] 6: while B is y-good and v̂ is not goal do 7: B ← Filter(BreadthExpand(B)) 8: end while 9: if B is not y-good then
10: v∗ ← SetGood(v̂) 11: w←",4. Learning the Speedup Classifier,[0],[0]
w + φ(v∗)− 1|B| ∑ v∈B φ(v) 12: else if v̂ is not y-good then 13: v∗ ← SetGood(v̂) 14: w←,4. Learning the Speedup Classifier,[0],[0]
"w + φ(v∗)− φ(v̂) 15: end if 16: end for 17: end for
beam B is y",4. Learning the Speedup Classifier,[0],[0]
-good if it contains at least one y-good node to represent the notion that search is still viable.,4. Learning the Speedup Classifier,[0],[0]
"We denote the first element (the highest ranked) in a beam by v̂. Finally, we define an operator SetGood, which takes a node that is not y-good, and return its corresponding y-good node by fixing the incorrect assignments according to the reference solution.",4. Learning the Speedup Classifier,[0],[0]
"The unassigned variables are still left unassigned by the SetGood operator.
",4. Learning the Speedup Classifier,[0],[0]
The speedup-learning algorithm is listed as Algorithm 1.,4. Learning the Speedup Classifier,[0],[0]
It begins by initializing the weight w to the zero vector.,4. Learning the Speedup Classifier,[0],[0]
We iterate over the examples for M epochs.,4. Learning the Speedup Classifier,[0],[0]
"For each example xi, we first solve inference using the ILP solver to obtain the reference structure y (line 4).",4. Learning the Speedup Classifier,[0],[0]
Next a breadth-expand search is performed (lines 5-8).,4. Learning the Speedup Classifier,[0],[0]
"Every time the beam B is updated, we check if the beam contains at least one ygood node that can possibly lead to the reference solution y. Search terminates if the beam is not y-good, or if the highest ranking node v̂ is a goal.",4. Learning the Speedup Classifier,[0],[0]
"If the beam is not y-good, we compute the corresponding y-good node v∗ from v̂, and perform a perceptron style update to the speedup weights (line 9-11).",4. Learning the Speedup Classifier,[0],[0]
"In other words, we update the weight vector by adding feature vector of φ(v∗), and subtracting the average feature vector of all the nodes in the beam.",4. Learning the Speedup Classifier,[0],[0]
Otherwise v̂ must be a goal node.,4. Learning the Speedup Classifier,[0],[0]
We then check if v̂ agrees with the reference solution (lines 12-15).,4. Learning the Speedup Classifier,[0],[0]
"If not, we perform a similar weight update, by adding the feature vector of φ(v∗), and subtracting φ(v̂).
",4. Learning the Speedup Classifier,[0],[0]
"Mistake bound Next, we show that the Algorithm 1 has a mistake bound.",4. Learning the Speedup Classifier,[0],[0]
"Let Rφ be a positive constant such that for every pair of nodes (v, v′), we have ‖φ(v)− φ(v′)‖ ≤ Rφ.",4. Learning the Speedup Classifier,[0],[0]
"Let Rg be a positive constant such that for every pair of
search nodes (v, v′), we have |g(v)− g(v′)| ≤",4. Learning the Speedup Classifier,[0],[0]
Rg .,4. Learning the Speedup Classifier,[0],[0]
"Finally we define the level margin of a weight vector w for a training set as
γ = min",4. Learning the Speedup Classifier,[0],[0]
"{(v,v′)}
",4. Learning the Speedup Classifier,[0],[0]
"w · ( φ(v)− φ(v′) ) (12)
",4. Learning the Speedup Classifier,[0],[0]
"Here, the set {(v, v′)} contains any pair such that v is ygood, v′ is not y-good, and v and v′ are at the same search level.",4. Learning the Speedup Classifier,[0],[0]
"The level margin denotes the minimum score gap between a y-good and a y-bad node at the same search level.
",4. Learning the Speedup Classifier,[0],[0]
The priority function used to rank the search nodes is defined as pw(v) = g(v)−w,4. Learning the Speedup Classifier,[0],[0]
·φ(v).,4. Learning the Speedup Classifier,[0],[0]
Smaller priority function value ranks higher during search.,4. Learning the Speedup Classifier,[0],[0]
With these definitions we have the following theorem: Theorem 1 (Speedup mistake bound).,4. Learning the Speedup Classifier,[0],[0]
"Given a training set such that there exists a weight vector w with level margin γ > 0 and ‖w‖ = 1, the speedup learning algorithm (Algorithm 1) will converge with a consistent weight vector after making no more than R2φ+2Rg
γ2 weight updates.
",4. Learning the Speedup Classifier,[0],[0]
Proof.,4. Learning the Speedup Classifier,[0],[0]
The complete proof is in the supplementary material of the paper.,4. Learning the Speedup Classifier,[0],[0]
"So far, we have shown that a structured prediction problem can be converted to a beam search problem.",4.1. Avoiding Computing the Input Features,[0],[0]
The priority function for ranking search nodes is determined by p(v) = g(v) + h(v).,4.1. Avoiding Computing the Input Features,[0],[0]
We have seen how the h function be trained to enforce structural constraints.,4.1. Avoiding Computing the Input Features,[0],[0]
"However, there are other opportunities for speeding up as well.
",4.1. Avoiding Computing the Input Features,[0],[0]
"Computing the path cost g(v) involves calculating the corresponding ILP coefficients, which in turn requires feature extraction using the original trained model.",4.1. Avoiding Computing the Input Features,[0],[0]
"This is usually a time-consuming step (Srikumar, 2017), thus motivating the question of whether we can avoid calculating them without losing accuracy.",4.1. Avoiding Computing the Input Features,[0],[0]
"If a search node is strongly preferred by the heuristic function, the path cost is unlikely to reverse the heuristic function’s decision.",4.1. Avoiding Computing the Input Features,[0],[0]
"In this case, we can rank the candidate search nodes with heuristic function only.
",4.1. Avoiding Computing the Input Features,[0],[0]
"Formally, given a fixed beam size b and the beam candidates Ct at step t from which we need to select the beam Bt, we can rank the nodes in Ct from smallest to largest according to the heuristic function value h(v).",4.1. Avoiding Computing the Input Features,[0],[0]
"Denote the bth smallest node as vb and the (b+1)th smallest node as vb+1, we define the heuristic gap ∆t as
∆t = h(vb+1)− h(vb).",4.1. Avoiding Computing the Input Features,[0],[0]
"(13)
If the beam Bt is selected from Ct only according to heuristic function, then ∆t is the gap between the last node in the beam and the first node outside the beam.",4.1. Avoiding Computing the Input Features,[0],[0]
"Next we define the path-cost gap δt as
δt = max v,v′∈Ct
(v − v′) (14)
",4.1. Avoiding Computing the Input Features,[0],[0]
With these definitions we immediately have the following theorem: Theorem 2.,4.1. Avoiding Computing the Input Features,[0],[0]
"Given the beam candidates Ct with heuristic gap ∆t and path-cost gap δt, if ∆t > δt, then using only heuristic function to select the beam Bt will have the same set of nodes selected as using the full priority function up to their ordering in the beam.
",4.1. Avoiding Computing the Input Features,[0],[0]
"If the condition of Theorem 2 holds, then we can rank the candidates using only heuristic function without calculating the path cost.",4.1. Avoiding Computing the Input Features,[0],[0]
This will further save computation time.,4.1. Avoiding Computing the Input Features,[0],[0]
"However, without actually calculating the path cost there is no way to determine the path-cost gap δt at each step.",4.1. Avoiding Computing the Input Features,[0],[0]
"In practice we can treat δt as an empirical parameter θ and define the following priority function
pθ(v) = { h(v), if ∆t > θ, g(v) + h(v), otherwise.
",4.1. Avoiding Computing the Input Features,[0],[0]
(15),4.1. Avoiding Computing the Input Features,[0],[0]
We empirically evaluate the speedup based inference scheme described in Section 4 on the problem of predicting entities and relations (i.e. our running example).,5. Experiments,[0],[0]
"In this task, we are asked to label each entity, and the relation between each pair of the entities.",5. Experiments,[0],[0]
"We assume the entity candidates are given, either from human annotators or from a preprocessing step.",5. Experiments,[0],[0]
"The goal of inference is to determine the types of the entity spans, and the relations between them, as opposed to identify entity candidates.",5. Experiments,[0],[0]
"The research questions we seek to resolve empirically are:
1.",5. Experiments,[0],[0]
Does using a learned speedup heuristic recover structurally valid outputs without paying the inference cost of the integer linear program solver?,5. Experiments,[0],[0]
2.,5. Experiments,[0],[0]
"Can we construct accurate outputs without always computing input features and using only the learned heuristic to guide search?
",5. Experiments,[0],[0]
The dataset we used is from the previous work by Roth & Yih (2004).,5. Experiments,[0],[0]
It contains 1441 sentences.,5. Experiments,[0],[0]
"Each sentence contains several entities with labels, and the labeled relations between every pair of entity.",5. Experiments,[0],[0]
"There are three types of entities, person, location and organization, and five types of relations, Kill, LiveIn, WorkFor, LocatedAt and OrgBasedIn.",5. Experiments,[0],[0]
"There are two constraints associated with each relation type, specifying the allowed source and target arguments.",5. Experiments,[0],[0]
"For example, if the relation label is LiveIn, the source entity must be person and the target entity must be location.",5. Experiments,[0],[0]
"There is also another kind of constraint which says for every pair of entities, they can not have a relation label in both directions between them, i.e., one of the direction must be labeled as NoRel.
",5. Experiments,[0],[0]
"We re-implemented the model from the original work using the same set of features as for the entity and relation scoring
functions.",5. Experiments,[0],[0]
"We used 70% of the labeled data to train an ILPbased inference scheme, which will become our black-box solver for learning the speedup classifier.",5. Experiments,[0],[0]
"The remaining 30% labeled data are held out for evaluations.
",5. Experiments,[0],[0]
"We use 29950 sentences from the Gigaword corpus (Graff et al., 2003) to train the speedup classifier.",5. Experiments,[0],[0]
"The entity candidates are extracted using the Stanford Named Entity Recognizer (Manning et al., 2014).",5. Experiments,[0],[0]
"We ignore the entity labels, however, since our task requires determining the type of the entities and relations.",5. Experiments,[0],[0]
"The features we use for the speedup classifiers are counts of the pairs of labels of the form (source label, relation label), (relation label, target label), and counts of the triples of labels of the form (source label, relation label, target label).",5. Experiments,[0],[0]
"We run Algorithm 1 over this unlabeled dataset, and evaluate the resulting speedup classifier on the held out test set.",5. Experiments,[0],[0]
"In all of our speedup search implementations, we first assign labels to the entities from left to right, then the relations among them.
",5. Experiments,[0],[0]
We evaluate the learned speedup classifier in terms of both accuracy and speed.,5. Experiments,[0],[0]
"The accuracy of the speedup classifier can be evaluated using three kinds of metrics: F-1 scores against gold labels, F-1 scores against the ILP solver’s prediction, and the validity ratio, which is the percentage of the predicted examples agreeing with all constraints.6",5. Experiments,[0],[0]
Our first set of experiments evaluates the impact of Algorithm 1.,5.1. Evaluation of Algorithm 1,[0],[0]
These results are shown in Table 1.,5.1. Evaluation of Algorithm 1,[0],[0]
We see the ILP solver achieves perfect entity and relation F-1 when compared with ILP model itself.,5.1. Evaluation of Algorithm 1,[0],[0]
It guarantees all constraints are satisfied.,5.1. Evaluation of Algorithm 1,[0],[0]
Its accuracy against gold label and its prediction time becomes the baselines of our speedup classifiers.,5.1. Evaluation of Algorithm 1,[0],[0]
We also provide two search baselines.,5.1. Evaluation of Algorithm 1,[0],[0]
The first search baseline just uses greedy search without any constraint considerations.,5.1. Evaluation of Algorithm 1,[0],[0]
"In this setting each label is assigned independently, since the step cost of assigning a label to an entity or a relation variable depends only on the corresponding coefficients in the ILP objectives.",5.1. Evaluation of Algorithm 1,[0],[0]
"In this case, a structured prediction problem becomes several independent multi-class classification problems.",5.1. Evaluation of Algorithm 1,[0],[0]
The prediction time is faster than ILP but the validity ratio is rather low (0.29).,5.1. Evaluation of Algorithm 1,[0],[0]
The second search baseline is greedy search with constraint satisfaction.,5.1. Evaluation of Algorithm 1,[0],[0]
The constraints are guaranteed to be satisfied by using the standard arc-consistency search.,5.1. Evaluation of Algorithm 1,[0],[0]
"The prediction takes much longer than the ILP solver (844 ms vs. 239 ms.).
",5.1. Evaluation of Algorithm 1,[0],[0]
We trained a speedup classifier with two different beam sizes.,5.1. Evaluation of Algorithm 1,[0],[0]
"Even with beam width b = 1, we are able to obtain > 95% validity ratio, and the prediction time is much faster
6All our experiments were conducted on a server with eight Intel i7 3.40 GHz cores and 16G memory.",5.1. Evaluation of Algorithm 1,[0],[0]
"We disabled multithreaded execution in all cases for a fair comparison.
than the ILP model.",5.1. Evaluation of Algorithm 1,[0],[0]
"Furthermore, we see that the F-1 score evaluated against gold labels is only slightly worse than ILP model.",5.1. Evaluation of Algorithm 1,[0],[0]
"With beam width b = 2, we recover the ILP model accuracy when evaluated against gold labels.",5.1. Evaluation of Algorithm 1,[0],[0]
The prediction time is still much less than the ILP solver.,5.1. Evaluation of Algorithm 1,[0],[0]
"In this section, we empirically verify the idea that we do not always need to compute the path cost, if the heuristic gap ∆t is large.",5.2. Experiments on Ignoring the Model Cost,[0],[0]
We use the evaluation function pθ(v) in Eq.,5.2. Experiments on Ignoring the Model Cost,[0],[0]
(15) with different values of θ to rank the search nodes.,5.2. Experiments on Ignoring the Model Cost,[0],[0]
"The results are given in Table 2.
",5.2. Experiments on Ignoring the Model Cost,[0],[0]
"For both beam widths, θ = 0 is the case in which the original model is completely ignored.",5.2. Experiments on Ignoring the Model Cost,[0],[0]
All the nodes are ranked using the speedup heuristic function only.,5.2. Experiments on Ignoring the Model Cost,[0],[0]
"Even though it has perfect validity ratio, the result is rather poor when evaluated on F-1 scores.",5.2. Experiments on Ignoring the Model Cost,[0],[0]
"When θ increases, the entity and relation F-1 scores quickly jump up, essentially getting back the same accuracy as the speedup classifiers in Table 1.",5.2. Experiments on Ignoring the Model Cost,[0],[0]
But the prediction time is lowered compared to the results from Table 1.,5.2. Experiments on Ignoring the Model Cost,[0],[0]
The idea of learning memo functions to make computation more efficient goes back to Michie (1968).,6. Discussion and Related Work,[0],[0]
"Speedup learning has been studied since the eighties in the context of general problem solving, where the goal is to learn a problem solver that becomes faster as opposed to becoming more accurate as it sees more data.",6. Discussion and Related Work,[0],[0]
Fern (2011) gives a broad survey of this area.,6. Discussion and Related Work,[0],[0]
"In this paper, we presented a variant of this idea that is more concretely applied to structured output prediction.
",6. Discussion and Related Work,[0],[0]
Efficient inference is a central topic in structured prediction.,6. Discussion and Related Work,[0],[0]
"In order to achieve efficiency, various strategies are adopted in the literature.",6. Discussion and Related Work,[0],[0]
Search based strategies are commonly used for this purpose and several variants abound.,6. Discussion and Related Work,[0],[0]
"The idea of framing a structured prediction problem as a search problem has been explored by several previous works (Collins & Roark, 2004; Daumé III & Marcu, 2005; Daumé III et al., 2009; Huang et al., 2012; Doppa et al., 2014).",6. Discussion and Related Work,[0],[0]
"It usually admits incorporating arbitrary features more easily than fully global structured prediction models like conditional random fields (Lafferty et al., 2001), structured perceptron (Collins, 2002), and structured support vector machines (Taskar et al., 2003; Tsochantaridis et al., 2004).",6. Discussion and Related Work,[0],[0]
"In such cases too, inference can be solved approximately using heuristic search.",6. Discussion and Related Work,[0],[0]
"Either a fixed beam size (Xu et al., 2009), or a dynamicallysized beam (Bodenstab et al., 2011) can be used.",6. Discussion and Related Work,[0],[0]
In our work we fix the beam size.,6. Discussion and Related Work,[0],[0]
The key difference from previous work is that our ranking function combines information from the trained model with the heuristic function which characterizes constraint information.,6. Discussion and Related Work,[0],[0]
"Closely related to the
work described in this paper are approaches that learn to prune the search space (He et al., 2014; Vieira & Eisner, 2016) and learn to select features (He et al., 2013).
",6. Discussion and Related Work,[0],[0]
Another line of recent related work focuses on discovering problem level regularities across the inference space.,6. Discussion and Related Work,[0],[0]
"These amortized inference schemes are designed using deterministic rules for discovering when a new inference problem can re-use previously computed solutions (Srikumar et al., 2012; Kundu et al., 2013) or in the context of a Bayesian network by learning a stochastic inverse network that generates outputs (Stuhlmüller et al., 2013).
",6. Discussion and Related Work,[0],[0]
"Our work is also related to the idea of imitation learning (Daumé III et al., 2009; Ross et al., 2011; Ross & Bagnell, 2014; Chang et al., 2015).",6. Discussion and Related Work,[0],[0]
"In this setting, we are given a reference policy, which may or may not be a good policy.",6. Discussion and Related Work,[0],[0]
"The goal of learning is to learn another policy to imitate the given policy, or even learn a better one.",6. Discussion and Related Work,[0],[0]
Learning usually proceeds in an online fashion.,6. Discussion and Related Work,[0],[0]
"However, imitation learning requires learning a new policy which is independent of the given reference policy, since during test time the reference policy is no longer available.",6. Discussion and Related Work,[0],[0]
"In our case, we can think of the black-box solver as a reference policy.",6. Discussion and Related Work,[0],[0]
"During prediction we always have this solver at our disposal, what we want is avoiding unnecessary calls to the solver.",6. Discussion and Related Work,[0],[0]
"Following recent successes in imitation learning, we expect that we can replace the linear heuristic function with a deep network to avoid feature design.
",6. Discussion and Related Work,[0],[0]
"Also related is the idea of knowledge distillation (Bucilă et al., 2006; Hinton et al., 2015; Kim & Rush, 2016), that seeks to train a student classifier (usually a neural network) to compress and mimic a larger teacher network, thus improve prediction speed.",6. Discussion and Related Work,[0.952939324092959],"['To overcome the computationally intensive forward solution, a family of methods denoted as gradient matching (Varah, 1982; Ellner et al., 2002; Ramsay et al., 2007) have proposed to replace the forward solution by matching f(yi) ≈ ẏi to empirical gradients ẏi of the data instead, which do not require the costly integration step.']"
The primary difference with the speedup idea of this paper is that our goal is to be more efficient at constructing internally self-consistent structures without explicitly searching over the combinatorially large output space with complex constraints.,6. Discussion and Related Work,[0],[0]
"In this paper, we asked whether we can learn to make inference faster over the lifetime of a structured output classifier.",7. Conclusions,[0],[0]
"To address this question, we developed a search-based strategy that learns to mimic a black-box inference engine but is substantially faster.",7. Conclusions,[0],[0]
We further extended this strategy by identifying cases where the learned search algorithm can avoid expensive input feature extraction to further improve speed without losing accuracy.,7. Conclusions,[0],[0]
We empirically evaluated our proposed algorithms on the problem of extracting entities and relations from text.,7. Conclusions,[0],[0]
"Despite using an object-heavy JVM-based implementation of search, we showed that by exploiting regularities across the output space, we can outperform the industrial strength Gurobi integer linear program solver in terms of speed, while matching its accuracy.
",7. Conclusions,[0],[0]
Acknowledgments We thank the Utah NLP group members and the anonymous reviewers for their valuable feedback.,7. Conclusions,[0],[0]
Predicting structured outputs can be computationally onerous due to the combinatorially large output spaces.,abstractText,[0],[0]
"In this paper, we focus on reducing the prediction time of a trained black-box structured classifier without losing accuracy.",abstractText,[0],[0]
"To do so, we train a speedup classifier that learns to mimic a black-box classifier under the learning-to-search approach.",abstractText,[0],[0]
"As the structured classifier predicts more examples, the speedup classifier will operate as a learned heuristic to guide search to favorable regions of the output space.",abstractText,[0],[0]
We present a mistake bound for the speedup classifier and identify inference situations where it can independently make correct judgments without input features.,abstractText,[0],[0]
We evaluate our method on the task of entity and relation extraction and show that the speedup classifier outperforms even greedy search in terms of speed without loss of accuracy.,abstractText,[0],[0]
Learning to Speed Up Structured Output Prediction,title,[0],[0]
"In conventional ODE modelling coefficients of an equation driving the system state forward in time are estimated. However, for many complex systems it is practically impossible to determine the equations or interactions governing the underlying dynamics. In these settings, parametric ODE model cannot be formulated. Here, we overcome this issue by introducing a novel paradigm of nonparametric ODE modelling that can learn the underlying dynamics of arbitrary continuous-time systems without prior knowledge. We propose to learn non-linear, unknown differential functions from state observations using Gaussian process vector fields within the exact ODE formalism. We demonstrate the model’s capabilities to infer dynamics from sparse data and to simulate the system forward into future.",text,[0.9574369345283791],"['We parameterise our model as an augmented Gaussian process vector field with inducing points, while we propose sensitivity equations to efficiently compute the gradients of the system.']"
Dynamical systems modelling is a cornerstone of experimental sciences.,1. Introduction,[0],[0]
"In biology, as well as in physics and chemistry, modelers attempt to capture the dynamical behavior of a given system or a phenomenon in order to improve its understanding and make predictions about its future state.",1. Introduction,[0],[0]
Systems of coupled ordinary differential equations (ODEs) are undoubtedly the most widely used models in science.,1. Introduction,[0],[0]
"Even simple ODE functions can describe complex dynamical behaviours (Hirsch et al., 2004).",1. Introduction,[0],[0]
"Typically, the dynamics are firmly grounded in physics with only a few parameters to be estimated from data.",1. Introduction,[0],[0]
"However, equally ubiquitous are the cases where the governing dynamics are partially or completely unknown.
",1. Introduction,[0],[0]
"We consider the dynamics of a system governed by multi-
*Equal contribution 1Aalto University, Finland 2Helsinki Institute of Information Technology HIIT, Finland.",1. Introduction,[0],[0]
Correspondence to: Markus Heinonen,1. Introduction,[0],[0]
<,1. Introduction,[0],[0]
"markus.o.heinonen@aalto.fi>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"variate ordinary differential functions:
ẋ(t) = dx(t)
dt = f(x(t)) (1)
where x(t) ∈ X",1. Introduction,[0],[0]
"= RD is the state vector of a Ddimensional dynamical system at time t, and the ẋ(t) ∈",1. Introduction,[0],[0]
Ẋ =,1. Introduction,[0],[0]
"RD is the first order time derivative of x(t) that drives the state x(t) forward, and where f :",1. Introduction,[0],[0]
RD → RD is the vector-valued derivative function.,1. Introduction,[0],[0]
"The ODE solution is determined by
x(t) = x0 + ∫",1. Introduction,[0],[0]
t 0,1. Introduction,[0],[0]
"f(x(τ))dτ, (2)
where we integrate the system state from an initial state x(0) = x0 for time t forward.",1. Introduction,[0],[0]
We assume that f(·) is completely unknown and we only observe one or several multivariate time series Y =,1. Introduction,[0.9790306747803237],"['We assume that f(·) is completely unknown and we only observe one or several multivariate time series Y = (y1, .']"
"(y1, . . .",1. Introduction,[0],[0]
",yN )",1. Introduction,[0],[0]
"T ∈ RN×D obtained from an additive noisy observation model at observation time points T = (t1, . . .",1. Introduction,[0],[0]
", tN ) ∈ RN ,
y(t)",1. Introduction,[0],[0]
= x(t) +,1. Introduction,[0],[0]
"εt, (3)
where εt ∼ N (0,Ω) follows a stationary zero-mean multivariate Gaussian distribution with diagonal noise variances Ω = diag(ω21 , . . .",1. Introduction,[0],[0]
", ω 2 D).",1. Introduction,[0],[0]
The observation time points do not need to be equally spaced.,1. Introduction,[0],[0]
"Our task is to learn the differential function f(·) given observations Y , with no prior knowledge of the ODE system.
",1. Introduction,[0],[0]
"There is a vast literature on conventional ODEs (Butcher, 2016) where a parametric form for function f(x;θ, t) is assumed to be known, and its parameters θ are subsequently optimised with least squares or Bayesian approach, where the expensive forward solution xθ(ti)",1. Introduction,[0],[0]
"=∫ ti 0
f(x(τ);θ, t)dτ is required to evaluate the system responses xθ(ti) from parameters θ against observations y(ti).",1. Introduction,[0],[0]
"To overcome the computationally intensive forward solution, a family of methods denoted as gradient matching (Varah, 1982; Ellner et al., 2002; Ramsay et al., 2007) have proposed to replace the forward solution by matching f(yi)",1. Introduction,[0.9572451861126029],"['In order to tackle the problem of dimensionality, we project the original dataset with PCA to a three dimensional latent space where the system is specified, following Damianou et al. (2011) and Wang et al. (2006).']"
"≈ ẏi to empirical gradients ẏi of the data instead, which do not require the costly integration step.",1. Introduction,[0],[0]
"Recently several authors have proposed embedding a parametric differential function within a Bayesian or Gaussian process (GP) framework (Graepel, 2003; Calderhead et al., 2008;
Dondelinger et al., 2013; Wang and Barber, 2014; Macdonald, 2017) (see Macdonald et al. (2015) for a review).",1. Introduction,[0],[0]
"GPs have been successfully applied to model linear differential equations as they are analytically tractable (Gao et al., 2008; Raissi et al., 2017).
",1. Introduction,[0],[0]
"However, conventional ODE modelling can only proceed if a parametric form of the driving function f(·) is known.",1. Introduction,[0],[0]
"Recently, initial work to handle unknown or non-parametric ODE models have been proposed, although with various limiting approximations.",1. Introduction,[0],[0]
"Early works include spline-based smoothing and additive functions ∑D j fj(xj) to infer gene regulatory networks (De Hoon et al., 2002; Henderson and Michailidis, 2014).",1. Introduction,[0],[0]
"Äijö and Lähdesmäki (2009) proposed estimating the unknown nonlinear function with GPs using either finite time differences, or analytically solving the derivative function as a function of only time, ẋ(t) = f(t) (Äijö et al., 2013).",1. Introduction,[0],[0]
"In a seminal technical report of Heinonen and d’Alche Buc (2014) a full vector-valued kernel model f(x) was proposed, however using a gradient matching approximation.",1. Introduction,[0],[0]
"To our knowledge, there exists no model that can learn non-linear ODE functions ẋ(t) = f(x(t)) over the state x against the true forward solutions x(ti).
",1. Introduction,[1.000000015403632],"['To our knowledge, there exists no model that can learn non-linear ODE functions ẋ(t) = f(x(t)) over the state x against the true forward solutions x(ti).']"
"In this work we propose NPODE1: the first ODE model for learning arbitrary, and a priori completely unknown nonparametric, non-linear differential functions f : X → Ẋ from data in a Bayesian way.",1. Introduction,[0],[0]
"We do not use gradient matching or other approximative models, but instead propose to directly optimise the exact ODE system with the fully forward simulated responses against data.",1. Introduction,[1.0],"['We do not use gradient matching or other approximative models, but instead propose to directly optimise the exact ODE system with the fully forward simulated responses against data.']"
"We parameterise our model as an augmented Gaussian process vector field with inducing points, while we propose sensitivity equations to efficiently compute the gradients of the system.",1. Introduction,[0],[0]
"Our model can forecast continuous-time systems arbitrary amounts to future, and we demonstrate the state-of-the-art performance in human motion datasets.",1. Introduction,[0],[0]
"The differential function f(x) to be learned defines a vector field2 f , that is, an assignment of a gradient vector f(x) ∈ RD to every state x ∈ RD.",2. Nonparametric ODE Model,[0],[0]
"We model the vector field as a vector-valued Gaussian process (Rasmussen and Williams, 2006)
f(x) ∼ GP(0,K(x,x′)), (4)
which defines a priori distribution over function values f(x) whose mean and covariances are
E[f(x)]",2. Nonparametric ODE Model,[0.9726892938245819],"['We model the vector field as a vector-valued Gaussian process (Rasmussen and Williams, 2006) f(x) ∼ GP(0,K(x,x′)), (4) which defines a priori distribution over function values f(x) whose mean and covariances are E[f(x)] = 0 (5) cov[f(x), f(x′)] = K(x,x′), (6) and where the kernel K(x,x′) ∈ RD×D is matrixvalued.']"
"= 0 (5) cov[f(x), f(x′)] = K(x,x′), (6)
1The implementation is publicly available in http://www. github.com/cagatayyildiz/npode
2We use vector field and differential function interchangeably.
and where the kernel K(x,x′) ∈ RD×D is matrixvalued.",2. Nonparametric ODE Model,[0],[0]
A GP prior defines that for any collection of states X =,2. Nonparametric ODE Model,[0],[0]
"(x1, . . .",2. Nonparametric ODE Model,[0],[0]
",xN )",2. Nonparametric ODE Model,[0],[0]
"T ∈ RN×D, the function values F = (f(x1), . . .",2. Nonparametric ODE Model,[0],[0]
", f(xN ))",2. Nonparametric ODE Model,[0],[0]
"T ∈ RN×D follow a matrixvalued normal distribution,
p(F ) = N (vec(F )|0,K(X,X)), (7)
where K(X,X) =",2. Nonparametric ODE Model,[0],[0]
"(K(xi,xj))Ni,j=1 ∈",2. Nonparametric ODE Model,[0],[0]
"RND×ND is a block matrix of matrix-valued kernels K(xi,xj).",2. Nonparametric ODE Model,[0],[0]
"The key property of Gaussian processes is that they encode functions where similar states x,x′ induce similar differentials f(x), f(x′), and where the state similarity is defined by the kernel K(x,x′).
",2. Nonparametric ODE Model,[0],[0]
"In standard GP regression we would obtain the posterior of the vector field by conditioning the GP prior with the data (Rasmussen and Williams, 2006).",2. Nonparametric ODE Model,[0],[0]
In ODE models the conditional f(x)|Y of a vector field is intractable due to the integral mapping (2) between observed states y(ti) and differentials f(x).,2. Nonparametric ODE Model,[0],[0]
"Instead, we resort to augmenting the Gaussian process with a set of M inducing points z ∈ X and u ∈",2. Nonparametric ODE Model,[0],[0]
"Ẋ , such that f(z) = u (Quiñonero-Candela and
Rasmussen, 2005).",2. Nonparametric ODE Model,[0],[0]
"We choose to interpolate the differential function between the inducing points as (See Figure 1)
f(x) , Kθ(x, Z)Kθ(Z,Z) −1vec(U), (8)
which supports the function f(x) with inducing locations Z = (z1, . . .",2. Nonparametric ODE Model,[0],[0]
", zM ), inducing vectors U = (u1, . . .",2. Nonparametric ODE Model,[0],[0]
",uM ), and θ are the kernel parameters.",2. Nonparametric ODE Model,[0],[0]
"The function above corresponds to a vector-valued kernel function (Alvarez et al., 2012), or to a multi-task Gaussian process conditional mean without the variance term (Rasmussen and Williams, 2006).",2. Nonparametric ODE Model,[0],[0]
This definition is then compatible with the deterministic nature of the ODE formalism.,2. Nonparametric ODE Model,[0],[0]
"Due to universality of several kernels and kernel functions (Shawe-Taylor and Cristianini, 2004), we can represent arbitrary vector fields with appropriate inducing point and kernel choices.",2. Nonparametric ODE Model,[0],[0]
"The vector-valued kernel function (8) uses operator-valued kernels, which result in matrix-valued kernels Kθ(z, z′) ∈ RD×D for real valued states x, z, while the kernel matrix over data points becomes Kθ = (K(zi, zj))Mi,j=1 ∈ RMD×MD (See Alvarez et al. (2012) for a review).",2.1. Operator-valued Kernels,[1.0],"['The vector-valued kernel function (8) uses operator-valued kernels, which result in matrix-valued kernels Kθ(z, z′) ∈ RD×D for real valued states x, z, while the kernel matrix over data points becomes Kθ = (K(zi, zj))Mi,j=1 ∈ RMD×MD (See Alvarez et al. (2012) for a review).']"
"Most straightforward operator-valued kernel is the identity decomposable kernel Kdec(z, z′) = k(z, z′) ·",2.1. Operator-valued Kernels,[0],[0]
"ID, where the scalar Gaussian kernel
Kθ(z, z ′) =",2.1. Operator-valued Kernels,[0],[0]
σ2f exp −1 2 D∑ j=1 (zj − z′j)2,2.1. Operator-valued Kernels,[0],[0]
"`2j  (9) with differential variance σ2f and dimension-specific lengthscales ` = (`1, . . .",2.1. Operator-valued Kernels,[0],[0]
", `D) are expanded into a diagonal matrix of size D × D. We collect the kernel parameters as θ = (σf , `).
",2.1. Operator-valued Kernels,[0],[0]
We note that more complex kernels can also be considered given prior information of the underlying system characteristics.,2.1. Operator-valued Kernels,[0],[0]
"The divergence-free matrix-valued kernel induces vector fields that have zero divergence (Wahlström et al., 2013; Solin et al., 2015).",2.1. Operator-valued Kernels,[0],[0]
"Intuitively, these vector fields do not have sinks or sources, and every state always finally returns to itself after sufficient amount of time.",2.1. Operator-valued Kernels,[0],[0]
"Similarly, curl-free kernels induce curl-free vector fields that can contain sources or sinks, that is, trajectories can accelerate or decelerate.",2.1. Operator-valued Kernels,[1.0],"['Similarly, curl-free kernels induce curl-free vector fields that can contain sources or sinks, that is, trajectories can accelerate or decelerate.']"
"For theoretical treatment of vector field kernels, see (Narcowich and Ward, 1994; Bhatia et al., 2013; Fuselier and Wright, 2017).",2.1. Operator-valued Kernels,[0],[0]
"Non-stationary vector fields can be modeled with input-dependent lengthscales (Heinonen et al., 2016), while spectral kernels can represent stationary (Wilson et al., 2013) or non-stationary (Remes et al., 2017) recurring patterns in the differential function.",2.1. Operator-valued Kernels,[0],[0]
"We assume a Gaussian likelihood over the observations yi and the corresponding simulated responses x(ti) of Equation (2),
p(Y |x0, U, Z,ω) = N∏ i=1 N",2.2. Joint Model,[0],[0]
"(yi|x(ti),Ω), (10)
where x(ti) are forward simulated responses using the integral Equation (2) and differential Equation (8), and Ω = diag(ω21 . .",2.2. Joint Model,[0],[0]
.,2.2. Joint Model,[0],[0]
", ω 2 D) collects the dimension-specific noise variances.
",2.2. Joint Model,[0],[0]
"The inducing vectors have a Gaussian process prior
p(U |Z,θ) = N",2.2. Joint Model,[0],[0]
"(vec(U)|0,Kθ(Z,Z)).",2.2. Joint Model,[0],[0]
"(11)
",2.2. Joint Model,[0],[0]
"The model posterior is then
p(U,x0,θ,ω|Y ) ∝",2.2. Joint Model,[0],[0]
"p(Y |x0, U,ω)p(U |θ) = L, (12)
where we have for brevity omitted the dependency on the locations of the inducing points Z and also the parameter hyperpriors p(θ) and p(ω) since we assume them to be uniform, unless there is specific domain knowledge of the priors.
",2.2. Joint Model,[0.9842206917751865],"['(11) The model posterior is then p(U,x0,θ,ω|Y ) ∝ p(Y |x0, U,ω)p(U |θ) = L, (12) where we have for brevity omitted the dependency on the locations of the inducing points Z and also the parameter hyperpriors p(θ) and p(ω) since we assume them to be uniform, unless there is specific domain knowledge of the priors.']"
"The model parameters are the initial state x03, the inducing vectors U , the noise standard deviations ω = (ω1, . . .",2.2. Joint Model,[0],[0]
", ωD), and the kernel hyperparameters θ = (σf , `1, . . .",2.2. Joint Model,[0],[0]
", `D).",2.2. Joint Model,[0],[0]
"We apply a latent parameterisation using Cholesky decomposition LθLTθ = Kθ(Z,Z), which maps the inducing vectors to whitened domain (Kuss and Rasmussen, 2005)
",2.3. Noncentral Parameterisation,[0],[0]
"U = LθŨ , Ũ = L −1 θ U. (13)
",2.3. Noncentral Parameterisation,[0],[0]
The latent variables Ũ are projected on the kernel manifold Lθ to obtain the inducing vectors U .,2.3. Noncentral Parameterisation,[0],[0]
"This non-centered parameterisation (NCP) transforms the hierarchical posterior L of Equation (12) into a reparameterised form
p(x0, Ũ ,θ,ω|Y ) ∝",2.3. Noncentral Parameterisation,[0],[0]
"p(Y |x0, Ũ ,ω,θ)p(Ũ), (14)
where all variables to be optimised are decoupled, with the latent inducing vectors having a standard normal prior Ũ ∼ N (0, I).",2.3. Noncentral Parameterisation,[0],[0]
Optimizing Ũ and θ is now more efficient since they have independent contributions to the vector field via U = LθŨ .,2.3. Noncentral Parameterisation,[0],[0]
"The gradients of the whitened posterior can be retrieved analytically as (Heinonen et al., 2016)
",2.3. Noncentral Parameterisation,[0],[0]
∇Ũ logL = L T,2.3. Noncentral Parameterisation,[0],[0]
"θ∇U logL. (15)
3In case of multiple time-series, we will use one initial state for each time-series.
",2.3. Noncentral Parameterisation,[0],[0]
"Finally, we find a maximum a posteriori (MAP) estimate for the initial state x0, latent vector field Ũ , kernel parameters θ and noise variances ω by gradient ascent,
x0,MAP, ŨMAP,θMAP,ωMAP = arg max x0,Ũ ,θ,ω
logL, (16)
while keeping the inducing locations Z fixed on a sufficiently dense grid (See Figure 1).",2.3. Noncentral Parameterisation,[0.9920125531271354],"['(15) Finally, we find a maximum a posteriori (MAP) estimate for the initial state x0, latent vector field Ũ , kernel parameters θ and noise variances ω by gradient ascent, x0,MAP, ŨMAP,θMAP,ωMAP = arg max x0,Ũ ,θ,ω logL, (16) while keeping the inducing locations Z fixed on a sufficiently dense grid (See Figure 1).']"
"The partial derivatives of the posterior with respect to noise parameters ω can be found analytically, while the derivative with respect to σf is approximated with finite differences.",2.3. Noncentral Parameterisation,[0],[0]
We select the optimal lengthscales ` by cross-validation.,2.3. Noncentral Parameterisation,[0],[0]
"The key term to carry out the MAP gradient ascent optimization is the likelihood
log p(Y |x0, Ũ ,ω)
that requires forward integration and computing the partial derivatives with respect to the whitened inducing vectors Ũ .",3. Sensitivity Equations,[0],[0]
Given Equation (15) we only need to compute the gradients with respect to the inducing vectors u = vec(U) ∈,3. Sensitivity Equations,[0],[0]
"RMD,
d log p(Y |x0,u,ω)",3. Sensitivity Equations,[0],[0]
"du
= N∑ s=1 d logN (ys|x(ts,u),Ω)",3. Sensitivity Equations,[0],[0]
"dx dx(ts,u) du .",3. Sensitivity Equations,[0],[0]
"(17)
",3. Sensitivity Equations,[0],[0]
"This requires computing the derivatives of the simulated system response x(t,u) against the vector field parameters u,
dx(t,u)
du ≡ S(t) ∈ RD×MD, (18)
which we denote by Sij(t)",3. Sensitivity Equations,[0],[0]
"= ∂x(t,u)i
∂uj , and expand the no-
tation to make the dependency of x on u explicit.",3. Sensitivity Equations,[0],[0]
"Approximating these with finite differences is possible in principle, but is highly inefficient and has been reported to cause unstability (Raue et al., 2013).",3. Sensitivity Equations,[0],[0]
"We instead turn to sensitivity equations for u and x0 that provide computationally efficient, analytical gradients S(t) (Kokotovic and Heller, 1967; Fröhlich et al., 2017).
",3. Sensitivity Equations,[0],[0]
"The solution for dx(t,u)du can be derived by differentiating the full nonparametric ODE system with respect to u by
d
du
dx(t,u)
dt =
d
du f(x(t,u)).",3. Sensitivity Equations,[0],[0]
"(19)
The sensitivity equation for the given system can be obtained by changing the order of differentiation on the left hand side and carrying out the differentiation on the right hand side.
",3. Sensitivity Equations,[0],[0]
"The resulting sensitivity equation can then be expressed in the form
Ṡ(t)︷ ︸︸ ︷",3. Sensitivity Equations,[0],[0]
"d
dt
dx(t,u)
du =
J(t)︷ ︸︸ ︷",3. Sensitivity Equations,[0],[0]
"∂f(x(t,u))
",3. Sensitivity Equations,[0],[0]
"∂x
S(t)︷ ︸︸ ︷",3. Sensitivity Equations,[0],[0]
"dx(t,u)
",3. Sensitivity Equations,[0],[0]
"du +
R(t)︷ ︸︸ ︷",3. Sensitivity Equations,[0],[0]
"∂f(x(t,u))
",3. Sensitivity Equations,[0],[0]
"∂u ,
(20)
",3. Sensitivity Equations,[0],[0]
"where J(t) ∈ RD×D, R(t), Ṡ(t) ∈ RD×MD (See Supplements for detailed specification).",3. Sensitivity Equations,[0],[0]
"For our nonparametric ODE system the sensitivity equation is fully determined by
J(t) = ∂K(x, Z)
∂x K(Z,Z)−1u",3. Sensitivity Equations,[0],[0]
"(21)
R(t) = K(x, Z)K(Z,Z)−1. (22)
",3. Sensitivity Equations,[0],[0]
The sensitivity equation provides us with an additional ODE system which describes the time evolution of the derivatives with respect to the inducing vectors S(t).,3. Sensitivity Equations,[0.9663934346106644],['(22) The sensitivity equation provides us with an additional ODE system which describes the time evolution of the derivatives with respect to the inducing vectors S(t).']
"The sensitivities are coupled with the actual ODE system and, thus both systems x(t) and S(t) are concatenated as the new augmented state that is solved jointly by Equation (2) driven by the differentials ẋ(t) and Ṡ(t) (Leis and Kramer, 1988).",3. Sensitivity Equations,[1.0],"['The sensitivities are coupled with the actual ODE system and, thus both systems x(t) and S(t) are concatenated as the new augmented state that is solved jointly by Equation (2) driven by the differentials ẋ(t) and Ṡ(t) (Leis and Kramer, 1988).']"
The initial sensitivities are computed as S(0) = dx0du .,3. Sensitivity Equations,[0],[0]
"In our implementation, we merge x0 with u for sensitivity analysis to obtain the partial derivatives with respect to the initial state which is estimated along with the other parameters.",3. Sensitivity Equations,[0],[0]
"We use the CVODES solver from the SUNDIALS package (Hindmarsh et al., 2005) to solve the nonparametric ODE models and the corresponding gradients numerically.",3. Sensitivity Equations,[0],[0]
"The sensitivity equation based approach is superior to the finite differences approximation because we have exact formulation for the gradients of state over inducing points, which can be solved up to the numerical accuracy of the ODE solver.",3. Sensitivity Equations,[0],[0]
"As first illustration of the proposed nonparametric ODE method we consider three simulated differential systems: the Van der Pol (VDP), FitzHugh-Nagumo (FHN) and Lotka-Volterra (LV) oscillators of form
VDP : ẋ1 = x2 ẋ2 =",4. Simple Simulated Dynamics,[0],[0]
(1− x21)x2,4. Simple Simulated Dynamics,[0],[0]
− x1 FHN :,4. Simple Simulated Dynamics,[0],[0]
ẋ1 = 3(x1 − x31 3 + x2),4. Simple Simulated Dynamics,[0],[0]
"ẋ2 = 0.2− 3x1 − 0.2x2
3 LV :",4. Simple Simulated Dynamics,[0],[0]
ẋ1 = 1.5x1,4. Simple Simulated Dynamics,[0],[0]
− x1x2,4. Simple Simulated Dynamics,[0],[0]
"ẋ2 = −3x2 + x1x2.
",4. Simple Simulated Dynamics,[0],[0]
"In the conventional ODE case the coefficients of these equations can be inferred using standard statistical techniques if sufficient amount of time series data is available (Girolami, 2008; Raue et al., 2013).",4. Simple Simulated Dynamics,[1.0],"['In the conventional ODE case the coefficients of these equations can be inferred using standard statistical techniques if sufficient amount of time series data is available (Girolami, 2008; Raue et al., 2013).']"
"Our main goal is to infer unknown dynamics, that is, when these equations are unavailable and we instead represent the dynamics with a nonparametric
vector field of Equation (8).",4. Simple Simulated Dynamics,[0],[0]
"We use these simulated models to only illustrate our model behavior against the true dynamics.
",4. Simple Simulated Dynamics,[0],[0]
"We employ 25 data points from one cycle of noisy observation data from VDP and FHN models, and 25 data points from 1.7 cycles from the LV model with a noise variance of σ2n = 0.1
2.",4. Simple Simulated Dynamics,[0.9999999490236947],"['We employ 25 data points from one cycle of noisy observation data from VDP and FHN models, and 25 data points from 1.7 cycles from the LV model with a noise variance of σ2n = 0.1 2.']"
"We learn the npODE model with five training sequences using M = 62 inducing locations on a fixed grid, and forecast between 4 and 8 future cycles starting from true initial state x0 at time 0.",4. Simple Simulated Dynamics,[1.0],"['We learn the npODE model with five training sequences using M = 62 inducing locations on a fixed grid, and forecast between 4 and 8 future cycles starting from true initial state x0 at time 0.']"
Training takes approximately 100 seconds per oscillator.,4. Simple Simulated Dynamics,[0],[0]
"Figure 2 (bottom) shows the training datasets (grey regions), initial states, true trajectories (black lines) and the forecasted trajectory likelihoods (colored regions).",4. Simple Simulated Dynamics,[0],[0]
"The model accurately learns the dynamics from less than two cycles of data and can reproduce them reliably into future.
",4. Simple Simulated Dynamics,[1.000000168370112],['The model accurately learns the dynamics from less than two cycles of data and can reproduce them reliably into future.']
Figure 2 (top) shows the corresponding true vector field (black arrows) and the estimated vector field (grey arrows).,4. Simple Simulated Dynamics,[0],[0]
"The vector field is a continuous function, which is plotted on a 8x8 grid for visualisation.",4. Simple Simulated Dynamics,[0],[0]
"In general the most difficult part of the system is learning the middle of the loop (as seen in the FHN model), and learning the most outermost regions (bottom left in the LV model).",4. Simple Simulated Dynamics,[0],[0]
"The model learns the
underlying differential f(x) accurately close to observed points, while making only few errors in the border regions with no data.",4. Simple Simulated Dynamics,[0],[0]
"Next, we illustrate how the model estimates realistic, unknown dynamics from noisy observations y(t1), . . .",5. Unknown System Estimation,[0],[0]
",y(tN ).",5. Unknown System Estimation,[0],[0]
"As in Section 4, we make no assumptions on the structure or form of the underlying system, and capture the underlying dynamics with the nonparameteric system alone.",5. Unknown System Estimation,[0],[0]
"We employ no subjective priors, and assume no inputs, controls or other sources of information.",5. Unknown System Estimation,[0],[0]
"The task is to infer the underlying dynamics f(x), and interpolate or extrapolate the state trajectory outside the observed data.
",5. Unknown System Estimation,[0],[0]
We use a benchmark dataset of human motion capture data from the Carnegie Mellon University motion capture (CMU mocap) database.,5. Unknown System Estimation,[1.0],['We use a benchmark dataset of human motion capture data from the Carnegie Mellon University motion capture (CMU mocap) database.']
"Our dataset contains 50-dimensional pose measurements y(ti) from humans walking, where each pose dimension records a measurement in different parts of the body during movement (Wang et al., 2008).",5. Unknown System Estimation,[0],[0]
We apply the preprocessing of Wang et al. (2008) by downsampling the datasets by a factor of four and centering the data.,5. Unknown System Estimation,[0],[0]
"This resulted in a total of 4303 datapoints spread across 43 trajec-
tories with on average 100 frames per trajectory.",5. Unknown System Estimation,[0],[0]
"In order to tackle the problem of dimensionality, we project the original dataset with PCA to a three dimensional latent space where the system is specified, following Damianou et al. (2011) and Wang et al. (2006).",5. Unknown System Estimation,[0],[0]
"We place M = 53 inducing vectors on a fixed grid, and optimize our model starting from 100 different initial values, which we set by perturbing the projected empirical differences y(ti)−y(ti−1) to the inducing vectors.",5. Unknown System Estimation,[0],[0]
We use an L-BFGS optimizer in Matlab.,5. Unknown System Estimation,[0],[0]
"The whole inference takes approximately few minutes per trajectory.
",5. Unknown System Estimation,[0],[0]
We evaluate the method with two types of experiments: imputing missing values and forecasting future cycles.,5. Unknown System Estimation,[0],[0]
"For the forecasting the first half of the trajectory is reserved for model training, and the second half is to be forecasted.",5. Unknown System Estimation,[0],[0]
"For imputation we remove roughly 20% of the frames from the middle of the trajectory, which are to be filled by the models.",5. Unknown System Estimation,[0],[0]
We perform model selection for lengthscales ` with crossvalidation split of 80/20.,5. Unknown System Estimation,[0],[0]
"We record the root mean square error (RMSE) over test points in the original feature space in both cases, where we reconstruct the original dimensions from the latent space trajectories.
",5. Unknown System Estimation,[0],[0]
"Due to the current lack of ODE methods suitable for this nonparametric inference task, we instead compare our method to the state-of-the-art state-space models where such problems have been previously considered (Wang et al., 2008).",5. Unknown System Estimation,[0],[0]
In a state-space or dynamical model a transition function x(tk+1) = g(x(tk)) moves the system forward in discrete steps.,5. Unknown System Estimation,[1.0],['In a state-space or dynamical model a transition function x(tk+1) = g(x(tk)) moves the system forward in discrete steps.']
"With sufficiently high sampling rate, such models can estimate and forecast finite approximations of smooth dynamics.",5. Unknown System Estimation,[0],[0]
"In Gaussian process dynamical model (Wang et al., 2006; Frigola et al., 2014; Svensson et al., 2016)",5. Unknown System Estimation,[0],[0]
"a GP transition function is inferred in a latent space, which can be inferred with a standard GPLVM (Lawrence, 2004) or with a dependent GPLVM (Zhao and Sun, 2016).",5. Unknown System Estimation,[0],[0]
"In dynamical systems the transition function is replaced by a GP interpolation (Damianou et al., 2011).",5. Unknown System Estimation,[0],[0]
"The discrete time state-space models emphasize inference of a low-dimensional manifold as an explanation of the high-dimensional measurement trajectories.
",5. Unknown System Estimation,[0],[0]
"We compare our method to the dynamical model GPDM of Wang et al. (2006) and to the dynamical system VGPLVM of Damianou et al. (2011), where we directly apply the implementations provided by the authors at inverseprobability.com/vargplvm and dgp.",5. Unknown System Estimation,[0],[0]
toronto.edu/˜jmwang/gpdm.,5. Unknown System Estimation,[0],[0]
"Both methods optimize their latent spaces separately, and they are thus not directly comparable.",5. Unknown System Estimation,[0],[0]
"In the forecasting task we train all models with the first half of the trajectory, while forecasting the second half starting from the first frame.",5.1. Forecasting,[1.0],"['In the forecasting task we train all models with the first half of the trajectory, while forecasting the second half starting from the first frame.']"
"The models are trained and forecasted
within a low-dimensional space, and subsequently projected back into the original space via inverting the PCA or with GPLVM mean predictions.",5.1. Forecasting,[0],[0]
"As all methods optimize their latent spaces separately, they are not directly comparable.",5.1. Forecasting,[0],[0]
"Thus, the mean errors are computed in the original highdimensional space.",5.1. Forecasting,[0],[0]
"Note that the low-dimensional representation necessarily causes some reconstruction errors.
",5.1. Forecasting,[0],[0]
Figure 3 illustrates the models on one of the trajectories 35 12.amc.,5.1. Forecasting,[0],[0]
"The top part (a) shows the training data in the PCA space for npODE, and optimized training data representation for GPDM and VGPLVM (black points).",5.1. Forecasting,[0],[0]
"The colored lines (npODE) and points (GPDM, VGPLVM) indicate the future forecast.",5.1. Forecasting,[0],[0]
The bottom part (b) shows the first 9 reconstructed original pose dimensions reconstructed from the latent forecasted trajectories.,5.1. Forecasting,[0],[0]
"The training data is shown in grey background, while test data is shown with circles.
",5.1. Forecasting,[0],[0]
"The VGPLVM has most trouble forecasting future points, and reverts quickly after training data to a value close to zero, failing to predict future points.",5.1. Forecasting,[0],[0]
"The GPDM model produces more realistic trajectories, but fails to predict any of the poses accurately.",5.1. Forecasting,[0],[0]
"Finally, npODE can accurately predict five poses, and still retains adequate performance on remaining poses, except for pose 2.
",5.1. Forecasting,[0],[0]
"Furthermore, Table 1 indicates that npODE is also best performing method on average over the whole dataset in the forecasting.",5.1. Forecasting,[0],[0]
In the imputation task we remove approximately 20% of the training data from the middle of the trajectory.,5.2. Imputation,[0],[0]
The goals are to learn a model with the remaining data and to forecast the missing values.,5.2. Imputation,[0],[0]
Figure 4 highlights the performance of the three models on the trajectory 07 07.amc.,5.2. Imputation,[0],[0]
"The top part (a) shows the training data (black points) in the PCA space (npODE) or optimized training locations in the latent space (GPDM, VGPLVM).",5.2. Imputation,[0],[0]
The middle part imputation is shown with colored points or lines.,5.2. Imputation,[0],[0]
"Interestingly both npODE and GPDM operate on cyclic representations, while VGPLVM is not cyclic.
",5.2. Imputation,[0],[0]
"The bottom panel (b) shows the first 9 reconstructed pose
dimensions from the three models.",5.2. Imputation,[0],[0]
"The missing values are shown in circles, while training points are shown with black dots.",5.2. Imputation,[0],[0]
"All models can accurately reproduce the overall trends, while npODE seems to fit slightly worse than the other methods.",5.2. Imputation,[0],[0]
The PCA projection causes the seemingly perfect fit of the npODE prediction (at the top) to lead to slightly warped reconstructions (at the bottom).,5.2. Imputation,[0],[0]
All methods mostly fit the missing parts as well.,5.2. Imputation,[0],[0]
Table 1 shows that on average the npODE and VGPLVM have approximately equal top performance on the imputing missing values task.,5.2. Imputation,[0],[0]
"We proposed the framework of nonparametric ODE model that can accurately learn arbitrary, nonlinear continuos-time dynamics from purely observational data without making assumptions of the underlying system dynamics.",6. Discussion,[0],[0]
We demonstrated that the model excels at learning dynamics that can be forecasted into the future.,6. Discussion,[0],[0]
"We consider this work as the
first in a line of studies of nonparametric ODE systems, and foresee several aspects as future work.",6. Discussion,[0],[0]
"Currently we do not handle non-stationary vector fields, that is time-dependent differentials ft(x).",6. Discussion,[0],[0]
"Furthermore, an interesting future avenue is the study of various vector field kernels, such as divergence-free, curl-free or spectral kernels (Remes et al., 2017).",6. Discussion,[0],[0]
"Finally, including inputs or controls to the system would allow precise modelling in interactive settings, such as robotics.
",6. Discussion,[0],[0]
"The proposed nonparametric ODE model operates along a continuous-time trajectory, while dynamic models such as hidden Markov models or state-space models are restricted to discrete time steps.",6. Discussion,[0],[0]
"These models are unable to consider system state at arbitrary times, for instance, between two successive timepoints.
",6. Discussion,[0],[0]
"Conventional ODE models have also been considered from the stochastic perspective with stochastic differential equation (SDE) models that commonly model the deterministic
system drift and diffusion processes separately leading to a distribution of trajectories p(x(t))",6. Discussion,[0],[0]
"(Archambeau et al., 2007; Garcı́a et al., 2017).",6. Discussion,[0],[0]
"As future work we will consider stochastic extensions of our nonparametric ODE model, as well as MCMC sampling of the inducing point posterior p(U |Y ), leading to trajectory distribution as well.
Acknowledgements.",6. Discussion,[0],[0]
The data used in this project was obtained from mocap.cs.cmu.edu.,6. Discussion,[0],[0]
The database was created with funding from NSF EIA-0196217.,6. Discussion,[0],[0]
"This work has been supported by the Academy of Finland Center of Excellence in Systems Immunology and Physiology, the Academy of Finland grants no. 284597, 311584, 313271, 299915.",6. Discussion,[0],[0]
In conventional ODE modelling coefficients of an equation driving the system state forward in time are estimated.,abstractText,[0],[0]
"However, for many complex systems it is practically impossible to determine the equations or interactions governing the underlying dynamics.",abstractText,[0],[0]
"In these settings, parametric ODE model cannot be formulated.",abstractText,[0],[0]
"Here, we overcome this issue by introducing a novel paradigm of nonparametric ODE modelling that can learn the underlying dynamics of arbitrary continuous-time systems without prior knowledge.",abstractText,[0],[0]
"We propose to learn non-linear, unknown differential functions from state observations using Gaussian process vector fields within the exact ODE formalism.",abstractText,[0],[0]
We demonstrate the model’s capabilities to infer dynamics from sparse data and to simulate the system forward into future.,abstractText,[0],[0]
Learning unknown ODE models with Gaussian processes,title,[0],[0]
