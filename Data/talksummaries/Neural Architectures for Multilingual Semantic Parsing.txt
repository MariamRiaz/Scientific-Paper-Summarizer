0	22	In this work, we address multilingual semantic parsing – the task of mapping natural language sentences coming from multiple different languages into their corresponding formal semantic representations.
1	58	We consider two multilingual scenarios: 1) the single-source setting, where the input consists of a single sentence in a single language, and 2) the multi-source setting, where the input consists of parallel sentences in multiple languages.
2	26	Previous work handled the former by means of monolingual models (Wong and Mooney, 2006; Lu et al., 2008; Jones et al., 2012), while the latter has only been explored by Jie and Lu (2014) who ensembled many monolingual models together.
3	8	Unfortunately, training a model for each language separately ignores the shared information among the source languages, which may be potentially beneficial for typologically related languages.
4	12	Practically, it is also inconvenient to train, tune, and configure a new model for each language, which can be a laborious process.
23	14	Unlike the mainstream approach that trains one monolingual parser per source language, our approach integrates N encoders, one for each language, into a single model.
28	23	Similar to previous multi-task frameworks, e.g., in neural MT (Firat et al., 2016; Zoph and Knight, 2016), we create one encoder per source language, i.e., {Ψnenc}Nn=1.
31	18	In the basic sequence-to-sequence model, the decoder generates each target token in a linear fashion.
34	35	At each depth in the tree, logical forms are generated sequentially until the end-ofsequence token is output.
35	36	Unlike in the single language setting, here we define a single, shared decoder Ψdec as opposed to one decoder per source language.
41	47	Figure 1 (a) depicts a scenario where the model is parsing Indonesian input, with English and Chinese being non-active.
47	5	In the first version, we define separate weight matrices for each language, i.e., {Un,Vn,Wn}Nn=1.
48	8	In the second version, the three weight matrices are shared across languages, essentially reducing the number of parameters by a factor of N .
51	24	Specifically, model parameters are updated after one batch from one language before moving to the next one.
52	13	Similar to Firat et al. (2016), this mechanism prevents excessive updates from a specific language.
53	15	In this setting, the input are semantically equivalent sentences in N languages.
54	38	Figure 1 (b) depicts a scenario where the model is parsing English, Indonesian, and Chinese simultaneously.
56	9	The decoder state at the first time step is initialized by first combining the N final states from each encoder, i.e., z0 = φinit(h1|X|, · · · ,hN|X|), where we implement φinit by max-pooling.
58	29	First, we consider word-level combination, where we combine N encoder states at every time step, as follows: αnk,t = exp(h̃nk · zt)∑N n′=1 ∑|X| k′=1 exp(h̃ n′ k′ · zt) (8) ct = N∑ n=1 |X|∑ k=1 αnk,th̃ n k (9) Alternatively, in sentence-level combination, we first compute the context vector for each language in the same way as Equation 6 and 7.
64	9	The GeoQuery (GEO) dataset is a standard benchmark evaluation for semantic parsing.
65	25	The multilingual version consists of 880 instances of natural language queries related to US geography facts in four languages (English, German, Greek, and Thai) (Jones et al., 2012).
75	7	In all experiments, following Dong and Lapata (2016), we use a one-layer LSTM with 200- dimensional cells and embeddings.
81	7	This is especially important due to the relatively small dataset.
84	24	See Appendix B for the accuracy of individual runs.
85	21	Table 1 compares the performance of the monolingual sequence-to-tree model (Dong and Lapata, 2016), SINGLE, and our multilingual model, MULTI, with separate and shared output parameters under the single-source setting as described in Section 3.1.
91	11	For RANKING, we combine the predictions from each language by selecting the one with the highest probability.
95	5	Regarding comparison to previous monolingual works, we want to highlight that there exist two different versions of the GeoQuery dataset annotated with completely different semantic representations: semantic tree and lambda calculus.
97	17	We use lambda calculus same as Dong and Lapata (2016).
102	67	Table 3 shows example output from the monolingual model, SINGLE, trained on the three languages in ATIS and the multilingual model, MULTI, with sentence-level combination.
108	12	In Table 4, we summarize the number of parameters in the baseline and our multilingual model.
