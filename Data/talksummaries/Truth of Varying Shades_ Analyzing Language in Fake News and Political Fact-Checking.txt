7	78	Importantly, like above examples, most factchecked statements on PolitiFact are rated as neither entirely true nor entirely false.
9	41	Compared to most prior work on deception literature that focused on binary categorization of truth and deception, political fact-checking poses a new challenge as it involves a graded notion of truthfulness.
10	34	While political fact-checking generally focuses on examining the accuracy of a single quoted statement by a public figure, the reliability of general news stories is also a concern (Connolly et al., 2016; Perrott, 2016).
12	31	2931 In this paper, we present an analytic study characterizing the language of political quotes and news media written with varying intents and degrees of truth.
13	57	We also investigate graded deception detection, determining the truthfulness on a 6-point scale using the political fact-checking database available at PolitiFact.2
14	50	News Corpus with Varying Reliability To analyze linguistic patterns across different types of articles, we sampled standard trusted news articles from the English Gigaword corpus and crawled articles from seven different unreliable news sites of differing types.
15	146	Table 1 displays sources identified under each type according to US News & World Report.3 These news types include: • Satire: mimics real news but still cues the reader that it is not meant to be taken seriously • Hoax: convinces readers of the validity of a paranoia-fueled story • Propaganda: misleads readers so that they be- lieve a particular political/social agenda Unlike hoaxes and propaganda, satire is intended to be notably different from real news so that audiences will recognize the humorous intent.
17	18	To characterize differences between news types, we applied various lexical resources to trusted and fake news articles.
20	32	First among these lexicons is the Linguistic Inquiry and Word Count (LIWC), a lexicon widely used in social science studies (Pennebaker et al., 2015).
23	36	We also use lexicons for hedging from (Hyland, 2015) because hedging can indicate vague, obscuring language.
25	9	We compiled five lists from Wiktionary of words that imply a degree a dramatization (comparatives, superlatives, action adverbs, manner adverbs, and modal adverbs) and measured their presence.
29	16	Our results show that first-person and secondperson pronouns are used more in less reliable or deceptive news types.
32	28	Editors at trustworthy sources are possibly more rigorous about removing language that seems too personal, which is one reason why this result differs from other lie detection domains.
44	20	In contrast, compared to other types of fake news, propaganda uses relatively more assertive verbs and superlatives.
46	45	We split our collected articles into balanced training (20k total articles from the Onion, American News, The Activist, and the Gigaword news excluding ‘APW’, ‘WPB’ sources) and test sets (3k articles from the remaining sources).
47	24	Because articles in the training and test set come from different sources, the models must classify articles without relying on author-specific cues.
50	23	This is a promising result as it is much higher than random, but still leaves room for improvement compared to the performance on the development set consisting of articles from in-domain sources.
56	11	Interestingly, “youtube” and “video” are highly weighted for the propaganda and hoax classes respectively; indicating that they often rely on video clips as sources.
58	16	Misleading statements can also have a variety of intents and levels of reliability depending on whom is making the statement.
61	68	This scale allows for distinction between categories like mostly true (the facts are correct but presented in an incomplete manner) or mostly false (the facts are not correct but are connected to a small kernel of truth).
62	9	We collected labelled statements from PolitiFact and its spin-off sites (PunditFact, etc.)
68	33	Given a statement, the model returns a rating for how reliable the statement is (Politifact ratings are used as gold labels).
69	79	We ran the experiment in two settings, one considering all 6 classes and the other considering only 2 (treating the top three truthful ratings as true and the lower three as false).
70	17	Model We trained an LSTM model (Hochreiter and Schmidhuber, 1997) that takes the sequence of words as the input and predicts the Politifact rating.
71	27	We also compared this model with Maximum Entropy (MaxEnt) and Naive Bayes models, frequently used for text categorization.
72	38	For input to the MaxEnt and Naive Bayes models, we tried two variants: one with the word tfidf vectors as input, and one with the LIWC measurements concatenated to the tf-idf vectors.
76	33	Training was done with ADAM to minimize categorical crossentropy loss over 10 epochs.
77	45	Classifier Results Table 5 summarizes the performance on the development set.
78	14	We report macro averaged F1 score in all tables.
83	37	set results, the LIWC features do not improve the LSTM’s performance, and even seem to hurt the performance slightly.
