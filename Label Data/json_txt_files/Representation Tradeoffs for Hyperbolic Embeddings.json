{"sections": [{"heading": "1. Introduction", "text": "Recently, hyperbolic embeddings have been proposed as a way to capture hierarchy information for network and natural language processing tasks (Nickel & Kiela, 2017; Chamberlain et al., 2017). This approach is an exciting way to fuse structural information (for example, from knowledge graphs or synonym hierarchies) with the continuous representations favored by modern machine learning methods.\nTo understand the intuition behind hyperbolic embeddings\u2019 superior capacity, note that trees can be embedded with arbitrarily low distortion into the Poincare\u0301 disk, a twodimensional model of hyperbolic space (Sarkar, 2011). In contrast, Bourgain\u2019s theorem (Linial et al., 1995) shows that Euclidean space cannot achieve comparably low distortion\n1Department of Computer Science, Stanford University 2Department of Computer Science, Cornell University. Correspondence to: Frederic Sala <fredsala@stanford.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nfor trees\u2014even using an unbounded number of dimensions.\nMany graphs, such as complex networks (Krioukov et al., 2010), the Internet (Krioukov et al., 2009), and social networks (Verbeek & Suri, 2016), are known to have tree-like or hyperbolic structure and thus befit hyperbolic embeddings. Indeed, recent works show that hyperbolic representations are suitable for many hierarchies (e.g. the question answering (Q/A) system HyperQA in Tay et al. (2018), vertex classifiers in Chamberlain et al. (2017), and link prediction (Nickel & Kiela, 2017)). However, the optimization problems underlying the embedding techniques in these works are challenging, motivating us to seek fundamental insights and to understand the subtle tradeoffs involved.\nWe begin by considering the case where we are given an input graph that is a tree or nearly tree-like, and our goal is to produce a low-dimensional hyperbolic embedding that preserves all distances. This leads to a simple combinatorial strategy that directly places points instead of minimizing a surrogate loss function. It is both fast (nearly linear time) and has formal quality guarantees. The approach proceeds in two phases: we (1) produce an embedding of a graph into a weighted tree, and (2) embed that tree into the hyperbolic disk. In particular, we consider an extension of an elegant embedding of trees into the Poincare\u0301 disk by Sarkar (2011) and work on low-distortion graph embeddings into tree metrics (Abraham et al., 2007). For trees, this approach has nearly perfect quality. On the WordNet hypernym graph reconstruction, it obtains a nearly perfect mean average precision (MAP) of 0.989 using just 2 dimensions. The best published numbers for WordNet in Nickel & Kiela (2017) range between 0.823 and 0.87 for 5 to 200 dimensions.\nWe analyze this construction to extract fundamental tradeoffs. One tradeoff involves the embedding dimension, the properties of the graph, and the number of bits of precision used to represent components of embedded points\u2014an important hidden cost. We show that for a fixed precision, the dimension required scales linearly with the length of the longest path. On the other hand, the dimension scales logarithmically with the maximum degree of the tree. This suggests that hyperbolic embeddings should have high quality on hierarchies like WordNet but require large dimensions or high precision on graphs with long chains.\nTo understand how hyperbolic embeddings perform for met-\nrics that are far from tree-like, we consider a more general problem: given a matrix of distances that arise from points that are embeddable in hyperbolic space of dimension d (not necessarily from a graph), find a set of points that produces these distances. In Euclidean space, the problem is known as multidimensional scaling (MDS) and is solvable using PCA. A key step is a transformation that effectively centers the points, without knowledge of their exact coordinates. It is not obvious how to center points in hyperbolic space, which is curved. We show that in hyperbolic space, a centering operation is still possible with respect to a non-standard mean. In turn, this allows us to reduce the hyperbolic MDS problem (h-MDS) to a standard eigenvalue problem that can be solved with power methods. We also extend classical PCA perturbation analysis (Sibson, 1978; 1979). When applied to distances from graphs induced by real data, h-MDS obtains low distortion on far from tree-like graphs. However, we observe that these solutions may require high precision, which is not surprising in light of our previous analysis.\nFinally, we handle increasing amounts of noise in the model, leading naturally into new SGD-based formulations. Like in traditional PCA, the underlying problem is nonconvex. In contrast to PCA, there are local minima that are not global minima\u2014an additional challenge. Our main technical result is that an SGD-based algorithm initialized with an h-MDS solution can recover the submanifold the data is on\u2014even in some cases in which the data is perturbed by noise that can be full dimensional. Our algorithm essentially provides new recovery results for convergence of Principal Geodesic Analysis (PGA) in hyperbolic space. We implemented the resulting SGD-based algorithm using PyTorch. Finally, we note that all of our algorithms can handle incomplete distance information through standard techniques."}, {"heading": "2. Background", "text": "We provide intuition connecting hyperbolic space and tree distances, discuss the metrics used to measure embedding fidelity, and discuss the relationship between the reconstruction and learning problems for graph embeddings.\nHyperbolic spaces The Poincare\u0301 disk H2 is a twodimensional model of hyperbolic geometry with points located in the interior of the unit disk, as shown in Figure 1. A natural generalization of H2 is the Poincare\u0301 ball Hr, with elements inside the unit ball. The Poincare\u0301 models offer several useful properties, chief among which is mapping conformally to Euclidean space. That is, angles are preserved between hyperbolic and Euclidean space. Distances, on the other hand, are not preserved, but are given by\ndH(x, y) = acosh ( 1 + 2 \u2016x\u2212 y\u20162\n(1\u2212 \u2016x\u20162)(1\u2212 \u2016y\u20162)\n) .\nThere are some potentially unexpected consequences of this formula, and a simple example gives intuition about a key technical property that allows hyperbolic space to embed trees. Consider three points inside the unit disk: the origin 0, and points x and y with \u2016x\u2016 = \u2016y\u2016 = t for some t > 0. As shown on the right of Figure 1, as t \u2192 1 (i.e., the points move towards the outside of the disk), in flat Euclidean space, the ratio dE(x,y)dE(x,0)+dE(0,y) is constant with respect to t (blue curve). In contrast, the ratio\ndH(x,y) dH(x,0)+dH(0,y) approaches 1, or, equivalently, the distance dH(x, y) approaches dH(x, 0) + dH(0, y) (red and pink curves). That is, the shortest path between x and y is almost the same as the path through the origin. This is analogous to the property of trees in which the shortest path between two sibling nodes is the path through their parent. This tree-like nature of hyperbolic space is the key property exploited by embeddings. Moreover, this property holds for arbitrarily small angles between x and y.\nLines and geodesics There are two types of geodesics (shortest paths) in the Poincare\u0301 disk model: segments of circles that are orthogonal to the disk surface, and disk diameters (Brannan et al., 2012). Our algorithms and proofs make use of a simple geometric fact: isometric reflection across geodesics (preserving hyperbolic distances) is represented in this Euclidean model as a circle inversion.\nEmbeddings and fidelity measures An embedding is a mapping f : U \u2192 V for spaces U, V with distances dU , dV . We measure the quality of embeddings with several fidelity measures, presented here from most local to most global.\nRecent work (Nickel & Kiela, 2017) proposes using the mean average precision (MAP). For a graph G = (V,E), let a \u2208 V have neighborhood Na = {b1, b2, . . . , bdeg(a)}, where deg(a) denotes the degree of a. In the embedding f , consider the points closest to f(a), and define Ra,bi to be the smallest set of such points that contains bi (that is, Ra,bi is the smallest set of nearest points required to retrieve the ith neighbor of a in f ). Then, the MAP is defined to be\nMAP(f) = 1 |V | \u2211 a\u2208V\n1\ndeg(a) |Na|\u2211 i=1 |Na \u2229Ra,bi | |Ra,bi | .\nWe have MAP(f) \u2264 1, with 1 as the best case. MAP is not concerned with explicit distances, but only ranks between the distances of immediate neighbors. It is a local metric.\nThe standard metric for graph embeddings is distortion D. For an n point embedding,\nD(f) = 1( n 2 )  \u2211 u,v\u2208U :u6=v |dV (f(u), f(v))\u2212 dU (u, v)| dU (u, v)  .\nThe best distortion isD(f) = 0, preserving the edge lengths exactly. This is a global metric, as it depends directly on the underlying distances rather than the local relationships between distances. A variant is the worst-case distortion Dwc, defined by\nDwc(f) = maxu,v\u2208U :u6=v dV (f(u), f(v))/dU (u, v)\nminu,v\u2208U :u6=v dV (f(u), f(v))/dU (u, v) .\nThat is, the wost-case distortion is the ratio of the maximal expansion and the minimal contraction of distances. Note that scaling the unit distance does not affect Dwc. The best worst-case distortion is Dwc(f) = 1.\nReconstruction and learning If we lack a full set of distances, we can either use the triangle inequality to recover the missing distances, or we can access the scaled Euclidean distances (the inside of the acosh in dH(x, y)), and apply standard matrix completion techniques (Candes & Tao, 2010). Then we compute an embedding using any of the approaches discussed in this paper. We quantify the error introduced by this process experimentally in Section 5."}, {"heading": "3. Combinatorial Constructions", "text": "We first focus on hyperbolic tree embeddings\u2014a natural approach considering the tree-like behavior of hyperbolic space. We review the embedding of Sarkar (2011). We then provide novel analysis on the precision, revealing fundamental limits of hyperbolic embeddings. In particular, we characterize the bits of precision needed for hyperbolic representations. We extend the construction to r dimensions, and propose to use Steiner nodes to better embed general graphs as trees, building on Abraham et al. (2007).\nEmbedding trees The nature of hyperbolic space lends itself towards excellent tree embeddings. In fact, it is possible to embed trees into the Poincare\u0301 disk H2 with arbitrarily low distortion (Sarkar, 2011). Remarkably, trees cannot be embedded into Euclidean space with arbitrarily low distortion for any number of dimensions. These notions motivate the following two-step process for embedding hierarchies\ninto hyperbolic space: (1) embed the graphG = (V,E) into a tree T , and (2) embed T into the Poincare\u0301 ball Hd. We refer to this process as the combinatorial construction. Note that we are not required to minimize a loss function. We begin by describing the second stage, where we extend an elegant construction from Sarkar (2011)."}, {"heading": "3.1. Sarkar\u2019s Construction", "text": "Algorithm 1 performs an embedding of trees into H2. The inputs are a scaling factor \u03c4 and a node a (of degree deg(a)) from the tree with parent node b. Say a and b have already been embedded into f(a) and f(b) in H2. The algorithm places the children c1, c2, . . . , cdeg(a)\u22121 into H2.\nA two-step process is used. First, f(a) and f(b) are reflected across a geodesic (using circle inversion) so that f(a) is mapped onto the origin 0 and f(b) is mapped onto some point z. Next, we place the children nodes to vectors y1, . . . , yd\u22121 equally spaced around a circle with radius e\u03c4\u22121 e\u03c4+1 (which is a circle of radius \u03c4 in the hyperbolic metric), and maximally separated from the reflected parent node embedding z. Lastly, we reflect all of the points back across the geodesic. The isometric properties of reflections imply that all children are now at hyperbolic distance exactly \u03c4 from f(a). To embed the entire tree, we place the root at the origin O and its children in a circle around it (as in Step 5 of Algorithm 1), then recursively place their children until all nodes have been placed. This process runs in linear time."}, {"heading": "3.2. Analyzing Sarkar\u2019s Construction", "text": "Sarkar\u2019s construction works by separating children sufficiently in hyperbolic space. A key technical idea is to scale all the edges by a factor \u03c4 before embedding. We can then recover the original distances by dividing by \u03c4 . This transformation exploits the fact that hyperbolic space is not scale invariant. Sarkar\u2019s construction always captures neighbors perfectly, but Figure 1 implies that increasing the scale preserves the distances between farther nodes better. Indeed, if one sets \u03c4 = 1+\u03b5\u03b5 ( 2 log degmax\u03c0/2 ) , then the worst-case distortion D of the resulting embedding is no more than\nAlgorithm 1 Sarkar\u2019s Construction 1: Input: Node a with parent b, children to place c1, c2, . . . , cdeg(a)\u22121, partial embedding f containing an embedding for a and b, scaling factor \u03c4\n2: (0, z)\u2190 reflectf(a)\u21920(f(a), f(b)) 3: \u03b8 \u2190 arg(z) {angle of z from x-axis in the plane} 4: for i \u2208 {1, . . . ,deg(a)\u2212 1} do 5: yi \u2190 e \u03c4\u22121 e\u03c4+1 \u00b7 ( cos ( \u03b8 + 2\u03c0ideg(a) ) , sin ( \u03b8 + 2\u03c0ideg(a)\n)) 6: (f(a), f(b), f(c1), . . . , f(cdeg(a)\u22121)) \u2190\nreflect0\u2192f(a)(0, z, y1, . . . , ydeg(x)\u22121) 7: Output: Embedded H2 vectors f(c1), f(c2), . . . , f(cdeg(a)\u22121)\n1 + \u03b5. For trees, Sarkar\u2019s construction has arbitrarily high fidelity. However, this comes at a cost: the scaling \u03c4 affects the bits of precision required. In fact, we will show that the precision scales logarithmically with the degree of the tree\u2014but linearly with the maximum path length.\nHow many bits of precision do we need to represent points in H2? If x \u2208 H2, then \u2016x\u2016 < 1, so we need sufficiently many bits so that 1\u2212 \u2016x\u2016 will not be rounded to zero. This requires roughly \u2212 log(1\u2212 \u2016x\u2016) = log 11\u2212\u2016x\u2016 bits. Say we are embedding two points x, y at distance d. As described in the background, there is an isometric reflection that takes a pair of points (x, y) in H2 to (0, z) while preserving their distance, so without loss of generality we have that\nd = dH(x, y) = dH(0, z) = acosh\n( 1 + 2 \u2016z\u20162\n1\u2212 \u2016z\u20162\n) .\nRearranging the terms, we have (cosh(d) + 1)/2 = (1 \u2212 \u2016z\u20162)\u22121 \u2265 (1 \u2212 \u2016z\u2016)\u22121/2. Thus, the number of bits we want so that 1 \u2212 \u2016z\u2016 will not be rounded to zero is log(cosh(d)+1). Since cosh(d) = (exp(d)+exp(\u2212d))/2, this is roughly d bits. That is, in hyperbolic space, we need about d bits to express distances of d (rather than log d in Euclidean space).1 This result will be of use below.\nConsider the largest distance in the embeddings produced by Algorithm 1. If the longest path length in the tree is `, and each edge has length \u03c4 = 1\u03b5 ( 2 log degmax \u03c0/2 ) , the largest distance is O( `\u03b5 log degmax), and we require this number of bits for the representation.\nLet us interpret this expression. Note that degmax is inside the log term, so that a bushy tree is not penalized much in precision. On the other hand, the longest path length ` is not, so that hyperbolic embeddings struggle with long paths. Moreover, by selecting an explicit graph, we derive a matching lower bound, concluding that to achieve a dis-\n1Although it is particularly easy to bound precision in the Poincare\u0301 model, this fact holds generally for hyperbolic space independent of model (shown in the appendix).\ntortion \u03b5, any construction requires \u2126 ( ` \u03b5 log(degmax) ) bits. The argument follows from selecting a graph consisting of m(degmax+1) nodes in a tree with a single root and degmax chains each of length m (shown in the appendix)."}, {"heading": "3.3. Improving the Construction", "text": "Our next contribution is a generalization of the construction from the disk H2 to the ball Hr. Our construction follows the same line as Algorithm 1, but since we have r dimensions, the step where we place children spaced out on a circle around their parent now uses a hypersphere.\nSpacing out points on the hypersphere is a classic problem known as spherical coding (Conway & Sloane, 1999). As we shall see, the number of children that we can place for a particular angle grows with the dimension. Since the required scaling factor \u03c4 gets larger as the angle decreases, we can reduce \u03c4 for a particular embedding by increasing the dimension. Note that increasing the dimension helps with bushy trees (large degmax), but has limited effect on tall trees with small degmax. We show\nProposition 3.1. The generalized Hr combinatorial construction has distortion at most 1 + \u03b5 and requires at most O( 1\u03b5 ` r log degmax) bits to represent a node component for r \u2264 (log degmax) + 1, and O( 1\u03b5 `) bits for r > (log degmax) + 1.\nTo generalize to Hr, we replace Step 5 in Algorithm 1 with a node placement step based on coding theory. The children are placed at the vertices of a hypercube inscribed into the unit hypersphere (and then scaled by \u03c4 ). Each component of a hypercube vertex has the form \u00b11\u221a\nr . We index these\npoints using binary sequences a \u2208 {0, 1}r in the following way: xa = ( (\u22121)a1\u221a r , (\u22121) a2 \u221a r , . . . , (\u22121) ar \u221a r ) . We space out the children by controlling the distances by selecting a set of binary sequences a with a prescribed minimum Hamming distance\u2014a binary error-correcting code\u2014and placing the children at the resulting hypercube vertices. We provide more details, including our choice of code in the appendix."}, {"heading": "3.4. Embedding into Trees", "text": "We revisit the first step of the construction: embedding graphs into trees. There are fundamental limits to how well graphs can be embedded into trees; in general, breaking long cycles inevitably adds distortion, as shown in Figure 2. We are inspired by a measure of this limit, the \u03b4-4 points condition introduced in Abraham et al. (2007). A graph on n nodes that satisfies the \u03b4-4 points condition has distortion at most (1 + \u03b4)c1 logn for some constant c1. This result enables our end-to-end embedding to achieve a distortion of at most D(f) \u2264 (1 + \u03b4)c1 logn(1 + \u03b5).\nThe result in Abraham et al. (2007) builds a tree with Steiner\nnodes. These additional nodes can help control the distances in the resulting weighted tree (Figure 2). Note that Algorithm 1 readily extends to the case of weighted trees.\nIn summary, the key takeaways of our analysis are:\n\u2022 There is a fundamental tension between precision and quality in hyperbolic embeddings.\n\u2022 Hyperbolic embeddings have an exponential advantage in space compared to Euclidean embeddings for short, bushy hierarchies, but will have less of an advantage for graphs that contain long paths.\n\u2022 Choosing an appropriate scaling factor \u03c4 is critical for quality. Later, we will propose to learn this scale factor automatically for computing embeddings in PyTorch.\n\u2022 Steiner nodes can help improve embeddings of graphs."}, {"heading": "4. Hyperbolic Multidimensional Scaling", "text": "In this section, we explore a fundamental and more general question than we did in the previous section: if we are given the pairwise distances arising from a set of points in hyperbolic space, can we recover the points? This enables us to produce an embedding for a desired distance metric. The equivalent problem for Euclidean distances is solved with multidimensional scaling (MDS). The goal of this section is to analyze the hyperbolic MDS (h-MDS) problem. We describe and overcome the additional technical challenges imposed by hyperbolic distances, and show that exact recovery is possible and interpretable. Afterwards we propose a technique for dimensionality reduction using principal geodesics analysis (PGA) that provides optimization guarantees. In particular, this addresses the shortcomings of h-MDS when recovering points that do not exactly lie on a hyperbolic manifold."}, {"heading": "4.1. Exact Hyperbolic MDS", "text": "Suppose that there is a set of hyperbolic points x1, . . . , xn \u2208 Hr, embedded in the Poincare\u0301 ball and written X \u2208 Rn\u00d7r in matrix form. We observe all the pairwise distances di,j = dH(xi, xj), but do not observe X: our goal is to use the observed di,j\u2019s to recover X (or some other set of points with the same pairwise distances di,j).\nThe MDS algorithm in the Euclidean setting makes an important centering2 assumption: the points have mean 0. If an exact embedding for the distances exists, it can be recovered from a matrix factorization. In other words, Euclidean MDS always recovers a centered embedding.\nIn hyperbolic space, the same algorithm does not work, but we show that it is possible to find an embedding centered at a different mean. More precisely, we introduce a new mean which we call the pseudo-Euclidean mean, that behaves like the Euclidean mean in that it enables recovery through matrix factorization. Once the points are recovered in hyperbolic space, they can be recentered around a more canonical mean by translating it to the origin.\nAlgorithm 2 is our complete algorithm, and for the remainder of this section we will describe how and why it works. We first describe the hyperboloid model, an alternate but equivalent model of hyperbolic geometry in which h-MDS is simpler. Of course, we can easily convert between the hyperboloid model and the Poincare\u0301 ball model. Next, we show how to reduce the problem to a standard PCA problem, which recovers an embedding centered at the points\u2019 pseudo-Euclidean mean. Finally, we discuss the meaning and implications of centering and prove that the algorithm preserves submanifolds as well\u2014that is, if there is an exact embedding in k < r dimensions centered at their canonical mean, then our algorithm will recover it.\nThe hyperboloid model Define Q to be the diagonal matrix in Rr+1 where Q00 = 1 and Qii = \u22121 for i > 0. For a vector x \u2208 Rr+1, xTQx is called the Minkowski quadratic form. The hyperboloid model is defined as\nMr = { x \u2208 Rr+1 \u2223\u2223xTQx = 1 \u2227 x0 > 0} , which is endowed with a distance measure dH(x, y) = acosh(xTQy). For convenience, for x \u2208Mr let x0 denote 0th coordinate eT0 x, and ~x \u2208 Rr denote the rest of the coordinates3. With this notation, the Minkowski bilinear form can be written xTQy = x0y0 \u2212 ~xT~y.\n2We say that points are centered at a particular mean if this mean is at 0. The act of centering refers to applying an isometry that makes the mean of the points 0.\n3Since x0 = \u221a\n1 + \u2016~x\u20162 is just a function of ~x, we can equivalently consider just ~x as being a member of a model of hyperbolic space: This representation is sometimes known as the Gans model.\nA new mean Given points x1, x2, . . . , xn \u2208Mr in hyperbolic space, define a variance term\n\u03a8(z;x1, x2, . . . , xn) = n\u2211 i=1 sinh2(dH(xi, z)).\nWe define a pseudo-Euclidean mean to be any local minimum of this expression. Notice that this is independent of any particular model of hyperbolic space, since it is defined only through the hyperbolic distance function dH . Lemma 4.1. Define X \u2208 Rn\u00d7r such that XT ei = ~xi and u \u2208 Rn such that ui = x0,i. Then\n\u2207~z\u03a8(z;x1, x2, . . . , xn)|~z=0 = \u22122 n\u2211 i=1 x0,i~xi = \u22122XTu.\nThis means that 0 is a pseudo-Euclidean mean if and only if 0 = XTu. Call some hyperbolic points x1, . . . , xn pseudoEuclidean centered if their average is 0 in this sense: i.e. if XTu = 0. We can always center a set of points without affecting their pairwise distances by simply finding their average, and then sending it to 0 through an isometry.\nRecovery via matrix factorization Suppose we observe the pairwise distances dH(xi, xj) of points x1, x2, . . . , xn \u2208Mr. This gives the matrix Y such that\nYi,j = cosh (dH(xi, xj)) = x0,ix0,j \u2212 ~xiT ~xj . (1)\nDefiningX and u as in Lemma 4.1, then in matrix form Y = uuT\u2212XXT . Without loss of generality, suppose that the xi are centered at their pseudo-Euclidean mean, so thatXTu = 0 by Lemma 4.1. This implies that u is an eigenvector of Y with positive eigenvalue, and the rest of Y \u2019s eigenvalues are negative. Therefore an eigendecomposition of Y will find u, X\u0302 such that Y = uuT \u2212 X\u0302X\u0302T , i.e. it will directly recover X up to rotation.\nIn fact, running PCA on \u2212Y = XTX \u2212 uuT to find the n most significant non-negative eigenvectors will recover X up to rotation, and then u can be found by leveraging the fact that x0 = \u221a 1 + \u2016~x\u20162. This leads to Algorithm 2, with optional post-processing steps for converting the embedding to the Poincare\u0301 ball model and for re-centering the points.\nA word on centering The MDS algorithm in Euclidean geometry returns points centered at their Karcher mean z, which is a point minimizing \u2211 d2(z, xi) (where d is the distance metric). The Karcher center is important for interpreting dimensionality reduction; we use the analogous hyperbolic Karcher mean for PGA in Section 4.2.\nAlthough Algorithm 2 returns points centered at their pseudo-Euclidean mean instead of their Karcher mean, they can be easily recentered by finding their Karcher mean and\nAlgorithm 2 1: Input: Distance matrix di,j and rank r 2: Compute scaled distance matrix Yi,j = cosh(di,j) 3: X \u2192 PCA(\u2212Y, r) 4: Project X from hyperboloid model to Poincare\u0301 model: x\u2192 x\n1+ \u221a 1+\u2016x\u20162\n5: If desired, centerX at a different mean (e.g. the Karcher mean) 6: return X\nreflecting it onto the origin. Furthermore, Algorithm 2 preserves the dimension of the embedding:\nLemma 4.2. If a set of points lie in a dimension-k geodesic submanifold, then both their Karcher mean and their pseudo-Euclidean mean lie in the same submanifold.\nThis implies that centering with the pseudo-Euclidean mean preserves geodesic submanifolds: If it is possible to embed distances in a dimension-k geodesic submanifold centered and rooted at a Karcher mean, then it is also possible to embed the distances in a dimension-k submanifold centered and rooted at a pseudo-Euclidean mean, and vice versa."}, {"heading": "4.2. Reducing Dimensionality with PGA", "text": "Given a high-rank embedding (resulting from h-MDS, for example), we may wish to find a lower-rank version. In Euclidean space, one can get the optimal lower rank embedding by simply discarding components. However, this may not be the case in hyperbolic space. Motivated by this, we study dimensionality reduction in hyperbolic space.\nAs hyperbolic space does not have a linear subspace structure like Euclidean space, we need to define what we mean by lower-dimensional. We follow Principal Geodesic Analysis (Fletcher et al., 2004), (Huckemann et al., 2010). Consider an initial embedding with points x1, . . . , xn \u2208 H2 and let dH : H2 \u00d7 H2 \u2192 R+ be the hyperbolic distance. Suppose we want to map this embedding onto a one-dimensional subspace. (Note that we are considering a two-dimensional embedding and one-dimensional subspace here for simplicity, and these results immediately extend to higher dimensions.) In this case, the goal of PGA is to find a geodesic \u03b3 : [0, 1] \u2192 H2 that passes through the mean of the points and that minimizes the squared error (or variance): f(\u03b3) = \u2211n i=1 mint\u2208[0,1] dH(\u03b3(t), xi) 2.\nThis expression can be simplified significantly and reduced to a minimization in Euclidean space. First, we find the mean of the points, the point x\u0304 which minimizes\u2211n i=1 dH(x\u0304, xi)\n2.4 Next, we reflect all the points xi so that their mean is 0 in the Poincare\u0301 disk model; we can\n4The derivative of the hyperbolic distance has a singularity, that is, limy\u2192x \u2202x|dH(x, y)| \u2192 \u221e for any x \u2208 H. This issue can\ndo this using a circle inversion that maps x\u0304 onto 0 Since reflections are isometric, if \u03b3 is a line through 0 and R\u03b3 is the reflection across \u03b3, we have that dH(\u03b3, x) = mint\u2208[0,1] dH(\u03b3(t), x) = 1 2dH(Rlx, x).\nCombining this with the Euclidean reflection formula and the hyperbolic metric produces\nf(\u03b3) = 1\n4 n\u2211 i=1 acosh2 ( 1 + 8dE(\u03b3, xi) 2 (1\u2212 \u2016xi\u20162)2 ) ,\nin which dE is the Euclidean distance from a point to a line. If we define wi = \u221a 8xi/(1 \u2212 \u2016xi\u20162) this reduces to the simplified expression f(\u03b3) = 1 4 \u2211n i=1 acosh 2 ( 1 + dE(\u03b3,wi) 2 ) .\nNotice that the loss function is not convex. We observe that there can be multiple local minima that are attractive and stable, in contrast to PCA. Figure 3 illustrates this nonconvexity on a simple dataset in H2 with only four examples. This makes globally optimizing the objective difficult.\nNevertheless, there will always be a region \u2126 containing a global optimum \u03b3\u2217 that is convex and admits an efficient projection, and where f is convex when restricted to \u2126. Thus it is possible to build a gradient descent-based algorithm to recover lower-dimensional subspaces: for example, we built a simple optimizer in PyTorch. We also give a sufficient condition on the data for f above to be convex.\nLemma 4.3. For hyperbolic PGA if for all i,\nacosh2 ( 1 + dE(\u03b3,wi) 2 ) < min ( 1, 1\n3 \u2016wi\u20162 ) then f is locally convex at \u03b3.\nbe mitigated by minimizing d2H , which does have a continuous derivative throughout H. The use of dH(x, y) is a minor instability in Nickel & Kiela (2017); Chamberlain et al. (2017)\u2019s formulation, necessitating guarding against NANs. We discuss this further in the appendix.\nAs a result, if we initialize in and optimize over a region that contains \u03b3\u2217 and where the condition of Lemma 4.3 holds, then gradient descent will be guaranteed to converge to \u03b3\u2217. We can turn this result around and read it as a recovery result: if the noise is bounded in this regime, then we are able to provably recover the correct low-dimensional embedding."}, {"heading": "5. Experiments", "text": "We evaluate the proposed approaches and compare against existing methods. We hypothesize that for tree-like data, the combinatorial construction offers the best performance. For general data, we expect h-MDS to produce the lowest distortion, while it may have low MAP due to precision limitations. We anticipate that dimension is a critical factor (outside of the combinatorial construction). In the appendix, we report on additional datasets, combinatorial construction parameters, and the effect of hyperparameters.\nDatasets We consider trees, tree-like hierarchies, and graphs that are not tree-like. Trees include fully-balanced and phylogenetic trees expressing genetic heritage (Hofbauer et al., 2016), available at Sanderson et al. (1994). Nearly tree-like hierarchies include the WordNet hypernym graph (the largest connected component from Nickel & Kiela (2017)) and a graph of Ph.D. advisor-advisee relationships (De Nooy et al., 2011). Also included are datasets\nthat vary in their tree nearness, such as disease relationships (Goh et al., 2007) and protein interactions (Jeong et al., 2001), both available from Rossi & Ahmed (2015). We also include the general relativity and quantum cosmology (GrQC) arXiv collaboration network (Leskovec et al., 2007).\nApproaches Combinatorial embeddings into H2 use the \u03b5 = 0.1 precision setting; others are considered in the Appendix. We performed h-MDS in floating point precision. We include results for our PyTorch implementation (PT) of an SGD-based algorithm (described later), and a warm start version (PWS) initialized with the high-dimensional combinatorial construction. We compare against classical MDS (i.e., PCA), and the optimization-based approach Nickel & Kiela (2017), which we call FB. The experiments for h-MDS, PyTorch SGD, PCA, and FB used dimensions of 2,5,10,50,100,200; we recorded the best resulting MAP and distortion. Due to the large scale, we did not replicate the best FB numbers on large graphs (i.e., Gr-QC and WordNet); we report their best published MAP numbers (their work does not report distortion). These entries are marked with an asterisk. For the WordNet graph, FB uses the transitive closure; a weighted version of the graph captures the ancestor relationships. The full details are in appendix.\nQuality In Table 3 (left), we report the distortion. As expected, for tree or tree-like graphs, the combinatorial construction has exceedingly low distortion. Because h-MDS is meant to recover points exactly, we hypothesized that h-MDS would offer very low distortion on these datasets. Table 3 confirms this: among h-MDS, PCA, and FB, hMDS consistently offers the lowest distortion, producing, for example, a distortion of 0.039 on the phylogenetic tree. We observe that floating point h-MDS struggles with MAP. We separately confirmed that this is due to precision (by\nusing a high-precision solver). The optimization-based approach is bolstered by appropriate initialization from the combinatorial construction.\nTable 3 (right) reports the MAP measure (we additionally include WordNet results in Table 2), which is a local measure. We confirm that the combinatorial construction performs well for tree-like hierarchies, where MAP is close to 1. The construction improves on approaches such as FB that rely on optimization. On larger graphs like WordNet, our approach yields a MAP of 0.989\u2014while their WordNet MAP result is 0.870 at 200 dimensions. This is exciting, as our approach is deterministic and linear-time.\nA refined understanding of hyperbolic embeddings may be used to improve the quality and runtime of extant algorithms. Indeed, we embedded WordNet entity-relationship-entity triples (Socher et al., 2013) using the combinatorial construction in 10 dimensions, accurately preserving relationship knowledge (Table 4). This suggests that hyperbolic embeddings are effective at compressing knowledge and may useful for knowledge base completion and Q/A tasks.\nSGD-Based Algorithm We built an SGD-based algorithm implemented in PyTorch. The loss function is equivalent to the PGA loss, and so is continuously differentiable.\nTo evaluate our algorithm\u2019s ability to deal with incomplete information, we sample the distance matrix at a ratio of nonedges to edges at 10 : 1 following Nickel & Kiela (2017). In Figure 4, we recover a good solution for the phylogenetic tree with a small fraction of the entries; for example, we sampled approximately 4% of the graph for a MAP of 0.74 and distortion of 0.6. We also considered learning the scale of the embedding (details in the appendix). Finally, all of our techniques scale to graphs with millions of nodes."}, {"heading": "6. Conclusion and Future Work", "text": "Hyperbolic embeddings embed hierarchical information with high fidelity and few dimensions. We explored the limits of this approach by describing scalable, high quality algorithms. We hope the techniques here encourage more follow-on work on the exciting techniques of Nickel & Kiela (2017); Chamberlain et al. (2017)."}, {"heading": "Acknowledgements", "text": "Thanks to Alex Ratner and Avner May for helpful discussion and to Beliz Gunel and Sen Wu for assistance with experiments. We gratefully acknowledge the support of DARPA under No. FA87501720095 and FA87501320039, ONR under No. N000141712266, the Moore Foundation, Okawa Research Grant, American Family Insurance, Accenture, Toshiba, the Secure Internet of Things Project, Google, VMware, Qualcomm, Ericsson, Analog Devices, and members of the Stanford DAWN project: Intel, Microsoft, Teradata, and VMware. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of DARPA, DOE, NIH, ONR, or the U.S. Government."}], "year": 2018, "references": [{"title": "Reconstructing approximate tree metrics", "authors": ["I. Abraham", "M. Balakrishnan", "F. Kuhn", "D. Malkhi", "V. Ramasubramanian", "K. Talwar"], "venue": "In Proc. of the 26th Annual ACM Symposium on Principles of Distributed Computing (PODC),", "year": 2007}, {"title": "The power of convex relaxation: Near-optimal matrix completion", "authors": ["E. Candes", "T. Tao"], "venue": "IEEE Transactions on Information Theory,", "year": 2010}, {"title": "Neural embeddings of graphs in hyperbolic space", "authors": ["B.P. Chamberlain", "J.R. Clough", "M.P. Deisenroth"], "venue": "arXiv preprint,", "year": 2017}, {"title": "Sphere Packings, Lattices and Groups", "authors": ["J. Conway", "N.J.A. Sloane"], "year": 1999}, {"title": "Exploratory social network analysis with Pajek, volume 27", "authors": ["W. De Nooy", "A. Mrvar", "V. Batagelj"], "year": 2011}, {"title": "Principal geodesic analysis for the study of nonlinear statistics of shape", "authors": ["P. Fletcher", "C. Lu", "S. Pizer", "S. Joshi"], "venue": "IEEE Transactions on Medical Imaging,", "year": 2004}, {"title": "The human disease", "authors": ["K. Goh", "M. Cusick", "D. Valle", "B. Childs", "M. Vidal", "A. Barab\u00e1si"], "venue": "network. Proceedings of the National Academy of Sciences,", "year": 2007}, {"title": "Intrinsic shape analysis: Geodesic PCA for Riemannian manifolds modulo isometric Lie group actions", "authors": ["S. Huckemann", "T. Hotz", "A. Munk"], "venue": "Statistica Sinica,", "year": 2010}, {"title": "Lethality and centrality in protein", "authors": ["H. Jeong", "S. Mason", "A. Barab\u00e1si", "Z. Oltvai"], "venue": "networks. Nature,", "year": 2001}, {"title": "Curvature and temperature of complex networks", "authors": ["D. Krioukov", "F. Papadopoulos", "A.V.M. Bogun\u00e1"], "venue": "Physical Review E,", "year": 2009}, {"title": "Hyperbolic geometry of complex networks", "authors": ["D. Krioukov", "F. Papadopoulos", "M. Kitsak", "A. Vahdat", "M. Bogun\u00e1"], "venue": "Physical Review E,", "year": 2010}, {"title": "Graph evolution: Densification and shrinking diameters", "authors": ["J. Leskovec", "J. Kleinberg", "C. Faloutsos"], "venue": "ACM Transactions on Knowledge Discovery from Data (TKDD),", "year": 2007}, {"title": "The geometry of graphs and some of its algorithmic applications", "authors": ["N. Linial", "E. London", "Y. Rabinovich"], "year": 1995}, {"title": "Poincar\u00e9 embeddings for learning hierarchical representations", "authors": ["M. Nickel", "D. Kiela"], "venue": "In Advances in Neural Information Processing Systems", "year": 2017}, {"title": "The network data repository with interactive graph analytics and visualization", "authors": ["R.A. Rossi", "N.K. Ahmed"], "venue": "In Proc. of the Twenty-Ninth AAAI Conference on Artificial Intelligence,", "year": 2015}, {"title": "TreeBASE: a prototype database of phylogenetic analyses and an interactive tool for browsing the phylogeny of life", "authors": ["M.J. Sanderson", "M.J. Donoghue", "W.H. Piel", "T. Eriksson"], "venue": "American Journal of Botany,", "year": 1994}, {"title": "Low distortion Delaunay embedding of trees in hyperbolic plane", "authors": ["R. Sarkar"], "venue": "In Proc. of the International Symposium on Graph Drawing (GD", "year": 2011}, {"title": "Studies in the robustness of multidimensional scaling: Procrustes statistics", "authors": ["R. Sibson"], "venue": "Journal of the Royal Statistical Society, Series B,", "year": 1978}, {"title": "Studies in the robustness of multidimensional scaling: Perturbational analysis of classical scaling", "authors": ["R. Sibson"], "venue": "Journal of the Royal Statistical Society, Series B,", "year": 1979}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "authors": ["R. Socher", "D. Chen", "C.D. Manning", "A. Ng"], "venue": "In Advances in Neural Information Processing Systems", "year": 2013}, {"title": "Hyperbolic representation learning for fast and efficient neural question answering", "authors": ["Y. Tay", "L.A. Tuan", "S.C. Hui"], "venue": "In Proc. of the Eleventh ACM International Conference on Web Search and Data Mining (WSDM", "year": 2018}, {"title": "Metric embedding, hyperbolic space, and social networks", "authors": ["K. Verbeek", "S. Suri"], "venue": "Computational Geometry,", "year": 2016}], "id": "SP:1d6423760041bd48cad1a46be5818f935de4a657", "authors": [{"name": "Frederic Sala", "affiliations": []}, {"name": "Christopher De Sa", "affiliations": []}, {"name": "Albert Gu", "affiliations": []}, {"name": "Christopher R\u00e9", "affiliations": []}], "abstractText": "Hyperbolic embeddings offer excellent quality with few dimensions when embedding hierarchical data structures. We give a combinatorial construction that embeds trees into hyperbolic space with arbitrarily low distortion without optimization. On WordNet, this algorithm obtains a meanaverage-precision of 0.989 with only two dimensions, outperforming existing work by 0.11 points. We provide bounds characterizing the precisiondimensionality tradeoff inherent in any hyperbolic embedding. To embed general metric spaces, we propose a hyperbolic generalization of multidimensional scaling (h-MDS). We show how to perform exact recovery of hyperbolic points from distances, provide a perturbation analysis, and give a recovery result that enables us to reduce dimensionality. Finally, we extract lessons from the algorithms and theory above to design a scalable PyTorch-based implementation that can handle incomplete information.", "title": "Representation Tradeoffs for Hyperbolic Embeddings"}