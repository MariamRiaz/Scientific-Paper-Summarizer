0	29	Language modeling is a fundamental task, used for example to predict the next word or character in a text sequence given the context.
3	86	RNNs are usually trained via back-propagation through time (Werbos, 1990), using stochastic optimization methods such as stochastic gradient descent (SGD) (Robbins and Monro, 1951); stochastic methods of this type are particularly important for training with large data sets.
7	26	To alleviate overfitting RNNs, good regularization is known as a key factor to successful applications.
8	47	In the neural network literature, Bayesian learning has been proposed as a principled method to impose regularization and incorporate model uncertainty (MacKay, 1992; Neal, 1995), by imposing prior distributions on model parameters.
10	20	Despite the elegant theoretical property of asymptotic convergence to the true posterior, HMC and other conventional Markov Chain Monte Carlo methods are not scalable to large training sets.
15	18	This procedure was also empirically found effective in Neelakantan et al. (2016).
33	14	Our goal is to learn model parameters θ to best characterize the relationship from Xn to Yn, with corresponding data likelihood p(D|θ) =∏N n=1 p(Dn|θ).
41	29	,yT } in language modeling or a discrete label in sentence classification.
48	30	The hidden units ht are updated as it = σ(Wixt +Uiht−1 + bi) , ft = σ(Wfxt +Ufht−1 + bf ) , ot = σ(Woxt +Uoht−1 + bo) , c̃t = tanh(Wcxt +Ucht−1 + bc) , ct = ft ct−1 + it c̃t , ht = ot tanh(ct) , where σ(·) denotes the logistic sigmoid function, and represents the element-wise matrix multiplication operator.
59	14	Specifically, for a length-T sequence, denote yt = xt+1 for t = 1, .
60	16	, T − 1. x1 and yT are always set to a special START and END token, respectively.
75	32	The evaluation of (4) is cheap even when N is large, allowing one to efficiently collect a sufficient number of samples in large-scale Bayesian learning, {θs}Ss=1, where S is the number of samples (this will be specified later).
76	19	These samples are used to construct a sample-based estimation to the expectation in (1): The finite-time estimation errors of SG-MCMC methods are bounded (Chen et al., 2015a), which guarantees (5) is an unbiased estimate of (1) asymptotically under appropriate decreasing stepsizes.
82	22	SGLD Stochastic Gradient Langevin Dynamics (SGLD) (Welling and Teh, 2011) draws posterior samples, with updates θt = θt−1 − ηtf̃t−1 + √ 2ηtξt , (6) where ηt is the learning rate, and ξt ∼ N (0, Ip) is a standard Gaussian random vector.
130	12	2(a) plots the perplexity of every single sample, Fig.
133	14	samples is a converged perplexity achieved in the thinned collection, while it requires 30 samples for forward collection or 60 samples for backward collection.
137	14	2(a) is also alleviated by model averaging.
159	17	Methods B-1 B-2 B-3 B-4 METEOR CIDEr ROUGE-L Perp.
160	32	Results on Flickr8k RMSprop 0.640 0.427 0.288 0.197 0.205 0.476 0.500 16.64 RMSprop + Dropout 0.647 0.444 0.305 0.209 0.208 0.514 0.510 15.72 RMSprop + Gal’s Dropout 0.651 0.443 0.305 0.209 0.206 0.501 0.509 14.70 pSGLD 0.669 0.463 0.321 0.224 0.214 0.535 0.522 14.29 pSGLD + Dropout 0.656 0.450 0.309 0.211 0.209 0.512 0.512 14.26 Results on Flickr30k RMSprop 0.644 0.422 0.279 0.184 0.180 0.372 0.476 17.80 RMSprop + Dropout 0.656 0.435 0.295 0.200 0.185 0.396 0.481 18.05 RMSprop + Gal’s Dropout 0.636 0.429 0.290 0.197 0.190 0.408 0.480 17.27 pSGLD 0.657 0.438 0.300 0.206 0.192 0.421 0.490 15.61 pSGLD + Dropout 0.666 0.448 0.308 0.209 0.189 0.419 0.487 17.05 with or without dropout.
161	23	In addition to (naive) dropout, we further compare pSGLD with the Gal’s dropout, recently proposed in Gal and Ghahramani (2016b), which is shown to be applicable to recurrent layers.
162	18	Consistent with the results in the basic language modeling, pSGLD yields improved performance compared to RMSprop.
174	20	10-fold cross-validation is used for evaluation on the first 4 datasets, while TREC has a pre-defined training/test split, and we run each algorithm 10 times on TREC.
178	14	5 plots the learning curves of different algorithms on the training, validation and testing sets of the TREC dataset.
187	23	One can leverage the uncertainty information to make decisions: either manually make a human judgement when uncertainty is high, or automatically choose the one with lower standard derivations when both types exhibits similar prediction means.
191	76	Let M1 denote only gradient noise, and M2 denote only model averaging.
193	22	This indicates that both gradient noise and model averaging are crucial for good performance in pSGLD.
194	13	Running Time We report the training and testing time for image captioning on the Flickr30k dataset in Table 8.
199	70	The learning framework is tested on several tasks, including language models, image caption generation and sentence classification.
200	54	Our algorithm outperforms stochastic optimization algorithms, indicating the importance of learning weight uncertainty in recurrent neural networks.
201	35	Our algorithm requires little additional computational overhead in training, and multiple times of forward-passing for model averaging in testing.
