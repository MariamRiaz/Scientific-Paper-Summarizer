0	31	Noun-compounds hold an implicit semantic relation between their constituents.
1	33	For example, a ‘birthday cake’ is a cake eaten on a birthday, while ‘apple cake’ is a cake made of apples.
4	78	Methods dedicated to paraphrasing nouncompounds usually rely on corpus co-occurrences of the compound’s constituents as a source of explicit relation paraphrases (e.g. Wubben, 2010; Versley, 2013).
6	65	Yet, most noun-compounds are very infrequent in text (Kim and Baldwin, 2007), and humans easily interpret the meaning of a new noun-compound by generalizing existing knowledge.
7	96	For example, consider interpreting parsley cake as a cake made of parsley vs. resignation cake as a cake eaten to celebrate quitting an unpleasant job.
22	20	As an alternative to the strict classification to predefined relation classes, Nakov and Hearst (2006) suggested that the semantics of a noun-compound could be expressed with multiple prepositional and verbal paraphrases.
25	55	SemEval 2010 task 9 (Butnariu et al., 2009) provided a list of plausible human-written paraphrases for each nouncompound, and systems had to rank them with the goal of high correlation with human judgments.
34	37	They represented each noun-compound using a compositional distributional vector (Mitchell and Lapata, 2010) and used it to predict paraphrases from the corpus.
36	102	For example, if the corpus does not contain paraphrases for plastic spoon, the model may predict the paraphrases of a similar compound such as steel knife.
52	46	Each training example consists of two constituents and a paraphrase (w2, p, w1), and we train the model on 3 subtasks: (1) predict p given w1 and w2, (2) predict w1 given p and w2, and (3) predict w2 given p and w1.
71	61	Similarly, for a word wi that did not occur in a paraphrase p we add (wi, p, UNK, sn) or (UNK, p, wi, sn), where UNK is the unknown word.
74	21	We use the 100-dimensional pretrained GloVe embeddings (Pennington et al., 2014), which are fixed during training.
75	32	In addition, we learn embeddings for the special words [w1], [w2], and [p], which are used to represent a missing component, as in “cake made of [w1]”, “[w2] made of apple”, and “cake [p] apple”.
78	17	We predict a distribution of the vocabulary of the missing component, i.e. to predict w1 correctly we need to predict its index in the word vocabulary Vw, while the prediction of p is from the vocabulary of paraphrases in the training set, Vp.
79	20	We predict the following distributions: p̂ = softmax(Wp · bLS( ~w2, [p], ~w1)2) ŵ1 = softmax(Ww · bLS( ~w2, ~p1:n, [w1])n+1) ŵ2 = softmax(Ww · bLS([w2], ~p1:n, ~w1)1) (1) where Ww ∈ R|Vw|×2d, Wp ∈ R|Vp|×2d, and d is the embeddings dimension.
94	29	This is a result of the model training objective (Section 3) which positions the vectors of semantically-similar paraphrases close to each other in the embedding space, based on similar constituents.
118	17	The overall score assigned to each system is calculated in two different ways.
120	51	The ‘non-isomorphic’ setting rewards only precision, and performing well on it requires accurately reproducing the top-ranked gold paraphrases, with no importance to order.
138	23	For each noun-compound, false positive errors are the top 10 predicted paraphrases which are not included in the gold paraphrases, while false negative errors are the top 10 gold paraphrases not found in the top k predictions made by the model.
141	21	Some are borderline valid with minor grammatical changes (error 6, “force of coalition forces”) or too specific (error 2, “life of women in community” instead of “life in community”).
142	19	Common prepositional paraphrases were often retrieved although they are incorrect (error 3).
143	24	We conjecture that this error often stem from an n-gram that does not respect the syntactic structure of the sentence, e.g. a sentence such as “rinse away the oil from baby ’s head” produces the n-gram “oil from baby”.
144	32	With respect to false negative examples, they consisted of many long paraphrases, while our model was restricted to 5 words due to the source of the training data (error 1, “holding done in the case of a share”).
146	19	Finally, in some paraphrases, the constituents in the gold paraphrase appear in inflectional forms (error 3, “holding of shares” instead of “holding of share”).
158	18	The lexical split better demonstrates the scenario in which a noun-compound whose constituents have not been observed needs to be interpreted based on similar observed noun-compounds, e.g. inferring the relation in pear tart based on apple cake and other similar compounds.
174	47	Our paraphrasing approach at its core assumes compositionality: only a noun-compound whose meaning is derived from the meanings of its constituent words can be rephrased using them.
176	20	We assumed that these compounds, more often than compositional ones would consist of unrelated constituents (spelling bee, sacred cow), and added instances of random unrelated nouns with ‘[w2] is unrelated to [w1]’.
177	74	Here, we assess whether our model succeeds to recognize non-compositional noun-compounds.
181	77	For example, it predicted that silver spoon is simply a spoon made of silver and that monkey business is a business that buys or raises monkeys.
184	46	We conclude that the model does not address compositionality and suggest to apply it only to compositional compounds, which may be recognized using compositionality prediction methods as in Reddy et al. (2011).
