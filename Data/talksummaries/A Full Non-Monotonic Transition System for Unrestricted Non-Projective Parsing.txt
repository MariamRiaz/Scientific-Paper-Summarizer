19	23	To our knowledge, the presented system is the first nonmonotonic parser that can produce non-projective dependency analyses.
23	23	The transition system that defines this parser is as follows: each parser configuration is of the form c = 〈λ1, λ2, B,A〉, such that λ1 and λ2 are lists of partially processed words, B is another list (called the buffer) containing currently unprocessed words, and A is the set of dependencies that have been built so far.
24	27	Suppose that our input is a string w1 · · ·wn, whose word occurrences will be identified with their indices 1 · · ·n for simplicity.
26	58	n], ∅〉, and execute transitions chosen from those in Figure 1 until a terminal configuration of the form {〈λ1, λ2, [], A〉 ∈ C} is reached.
34	14	While it runs in quadratic worst-case time, in theory worse than lineartime transition-based parsers (e.g. (Nivre, 2003; Gómez-Rodrı́guez and Nivre, 2013)), it has been shown to outspeed linear algorithms in practice, thanks to feature extraction optimizations that cannot be implemented in other parsers (Volokh and Neumann, 2012).
39	17	If this holds, then the loss of a configuration c equals the number of gold arcs that are not individually reachable from c, which is easy to compute in most parsers.
43	71	To calculate the first term, given a configuration cwith focus words i and j (i.e., c = 〈λ1|i, λ2, j|B,A〉), an arc x→ y will be in U(c, tG) if it is not in A, and at least one of the following holds: • j > max(x, y), (i.e., we have read too far in the string and can no longer get max(x, y) as right focus word), • j = max(x, y) ∧ i < min(x, y), (i.e., we have max(x, y) as the right focus word but the left focus word has already moved left past min(x, y), and we cannot go back), • there is some z 6= 0, z 6= x such that z → y ∈ A, (i.e., we cannot create x→ y because it would violate the single-head constraint), • x and y are on the same weakly connected component of A (i.e., we cannot create x → y due to the acyclicity constraint).
47	13	1: function LOSS(c = 〈λ1|i, λ2, j|B,A〉, tG) 2: U ← ∅ .
49	13	Variable I is for I(c, tG) 12: return |U |+ COUNTCYCLES(A ∪ I ) Algorithm 1 shows the resulting loss calculation algorithm, where COUNTCYCLES is a function that counts the number of cycles in the given graph and WEAKLYCONNECTED returns whether two given nodes are weakly connected in A.
52	33	If the node attached as dependent already had a previous head, the existing attachment is discarded in favor of the new one.
53	13	This allows the parser to correct erroneous attachments made in the past by assigning new heads, while still enforcing the single-head constraint, as only the most recent head assigned to each node is kept.
54	16	To enforce acyclicity, one possibility would be to keep the logic of the monotonic algorithm, forbidding the creation of arcs that would create cycles.
60	29	This not only enforces the acyclicity constraint while keeping the computation of U(c, tG) simple and efficient, but also produces a straightforward, coherent algorithm (arc transitions are always allowed, and both constraints are enforced by deleting a previous arc) and allows us to exploit non-monotonicity to the maximum (we can not only recover from assigning a node the wrong head, but also from situations where previous errors together with the acyclicity constraint prevent us from building a gold arc, keeping with the principle that later decisions override earlier ones).
62	13	To successfully train a non-monotonic system, we need a dynamic oracle with error exploration, so that the parser will be put in erroneous states and need to apply non-monotonic transitions in order to repair them.
71	15	This can be easily shown based on the fact that the non-monotonic parser does not forbid transitions at any configuration.
72	15	Thanks to this, we can can generate one such sequence by just applying the original Covington (2001) criteria (choose an arc transition whenever the focus words are linked in I(c, tG), and otherwise Shift or No-Arc depending on whether the left focus word is the first word in the sentence or not), although this sequence is not necessarily optimal in terms of loss.
73	14	In such a transition sequence, the gold arcs that are missed are (1) those in U(c, tG), and (2) those that are removed by the cycle-breaking in Left-Arc and Right-Arc transitions.
75	50	This reasoning also helps us calculate an up- per bound of the loss: in a transition sequence as described, if we only build the arcs in I(c, tG) and none else, the amount of arcs removed by breaking cycles (2) cannot be larger than the number of cycles in A ∪ I(c, tG).
76	14	This means that |U(c, tG)|+nc(A∪I(c, tG)) is an upper bound of the loss `(c).
77	75	Note that, contrary to the monotonic case, this expression does not always give us the exact loss, for several reasons: firstly, A∪I(c, tG) can have non-disjoint cycles (a node may have different heads in A and I since attachments are not permanent, contrary to the monotonic version) and thus removing a single arc may break more than one cycle; secondly, the removed arc can be a non-gold arc of A and therefore not incur loss; and thirdly, there may exist alternative transition sequences where a cycle in A∪I(c, tG) is broken early by non-monotonic configurations that change the head of a wrongly-attached node in A to a different (and also wrong) head,3 removing the cycle before the cycle-breaking mechanism needs to come into play without incurring in extra errors.
84	13	This implies that the calculation of the two non-monotonic upper bounds is less efficient than the linear loss computation in the monotonic scenario.
95	19	After those experiments, we conclude that the lower and the closer upper bounds are a tight approximation of the loss, with both bounds incurring relative errors below 0.8% in all datasets.
97	17	This means that the term npc(A∪I(c, tG)) provides a close approximation of the gold arcs missed by the presence of cycles in A.
105	23	As we can see, the novel non-monotonic oracle improves over the accuracy of the monotonic version on 14 out of 19 languages (0.32 in UAS on average) with the best loss calculation being |U(c, tG)| + nc(A ∪ I(c, tG)), where 6 of these improvements are statistically significant at the .05 level (Yeh, 2000).
109	11	This also leads us to hypothesize that, even if it were feasible to build an oracle with the exact loss, it would not provide practical improvements over these approximate oracles; as it appears difficult for a statistical model to learn the situations where replacing a wrong arc with another indirectly helps due to breaking prospective cycles.
112	59	In order to provide a broader contextualization of our approach, Table 4 presents a comparison of the average accuracy and parsing speed obtained by some well-known transition-based systems with dynamic oracles.
115	58	We also report the published accuracy of the non-projective Attardi algorithm (GómezRodrı́guez et al., 2014) on the nineteen datasets used in our experiments.
116	12	From Table 4 we can see that our approach achieves the best average UAS score, but is slightly slower at parsing time than the monotonic Covington algorithm.
131	27	While we used a perceptron classifier for our experiments, our oracle could also be used in neuralnetwork implementations of greedy transitionbased parsing (Chen and Manning, 2014; Dyer et al., 2015), providing an interesting avenue for future work.
132	36	We believe that gains from both techniques should be complementary, as they apply to orthogonal components of the parsing system (the scoring model vs. the transition system), although we might see a ”diminishing returns”effect.
