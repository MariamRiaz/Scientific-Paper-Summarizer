{"sections": [{"heading": "1 Introduction", "text": "Recent improvements to dependency parsing accuracy have been driven by higher-order features. Such a feature can look beyond just the parent and child words connected by a single edge to also consider siblings, grandparents, etc. By including increasingly global information, these features provide more information for the parser\u2014but they also complicate inference. The resulting higher-order parsers depend on approximate inference and decoding procedures, which may prevent them from predicting the best parse.\nFor example, consider the dependency parser we will train in this paper, which is based on the work\nof Smith and Eisner (2008). Ostensibly, this parser finds the minimum Bayes risk (MBR) parse under a probability distribution defined by a higher-order dependency parsing model. In reality, it achieves O(n3tmax) runtime by relying on three approximations during inference: (1) variational inference by loopy belief propagation (BP) on a factor graph, (2) truncating inference after tmax iterations prior to convergence, and (3) a first-order pruning model to limit the number of edges considered in the higherorder model. Such parsers are traditionally trained as if the inference had been exact.1\nIn contrast, we train the parser such that the approximate system performs well on the final evaluation function. We treat the entire parsing computation as a differentiable circuit, and backpropagate the evaluation function through our approximate inference and decoding methods to improve its parameters by gradient descent. The system also learns to cope with model misspecification, where the model couldn\u2019t perfectly fit the distribution even absent the approximations. For standard graphical models, Stoyanov and Eisner (2012) call this approach ERMA, for \u201cempirical risk minimization under approximations.\u201d For objectives besides empirical risk, Domke (2011) refers to it as \u201clearning with truncated message passing.\u201d\nOur primary contribution is the application of this approximation-aware learning method in the parsing setting, for which the graphical model involves a global constraint. Smith and Eisner (2008) previously showed how to run BP in this setting (by calling the inside-outside algorithm as a subroutine). We must backpropagate the downstream objective\n1For perceptron training, utilizing inexact inference as a drop-in replacement for exact inference can badly mislead the learner (Kulesza and Pereira, 2008; Huang et al., 2012).\n489\nTransactions of the Association for Computational Linguistics, vol. 3, pp. 489\u2013501, 2015. Action Editor: Sebastian Riedel. Submission batch: 4/2015; Published 8/2015.\nc\u00a92015 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license.\nfunction through their algorithm so that we can follow its gradient. We carefully define an empirical risk objective function (a\u0300 la ERMA) to be smooth and differentiable, yet equivalent to accuracy of the minimum Bayes risk (MBR) parse in the limit. Finding this difficult to optimize, we introduce a new simpler objective function based on the L2 distance between the approximate marginals and the \u201ctrue\u201d marginals from the gold data.\nThe goal of this work is to account for the approximations made by a system rooted in structured belief propagation. Taking such approximations into account during training enables us to improve the speed and accuracy of inference at test time. We compare our training method with the standard approach of conditional log-likelihood (CLL) training. We evaluate our parser on 19 languages from the CoNLL-2006 (Buchholz and Marsi, 2006) and CoNLL-2007 (Nivre et al., 2007) Shared Tasks as well as the English Penn Treebank (Marcus et al., 1993). On English, the resulting parser obtains higher accuracy with fewer iterations of BP than CLL. On the CoNLL languages, we find that on average it yields higher accuracy parsers than CLL, particularly when limited to few BP iterations."}, {"heading": "2 Dependency Parsing by Belief Propagation", "text": "This section describes the parser that we will train.\nModel A factor graph (Frey et al., 1997; Kschischang et al., 2001) defines the factorization of a probability distribution over a set of variables {Y1, Y2, . . .}. It is a bipartite graph between variables Yi and factors \u03b1. Edges connect each factor \u03b1 to a subset of the variables {Y\u03b11 , Y\u03b12 , . . .}, called its neighbors. Each factor defines a potential function \u03c8\u03b1, which assigns a nonnegative score to each configuration of its neighbors y\u03b1 = {y\u03b11 , y\u03b12 , . . .}. We define the probability of a given assignment y = {y1, y2, . . .} to be proportional to the product of all factors\u2019 potential functions: p(y) = 1Z \u220f \u03b1 \u03c8\u03b1(y\u03b1).\nSmith and Eisner (2008) define a factor graph for dependency parsing of a given n-word sentence: n2 binary variables indicate which of the directed arcs are included (yi = ON) or excluded (yi = OFF) in the dependency parse. One of the factors plays the role of a hard global constraint: \u03c8PTREE(y) is\n1 or 0 according to whether the assignment encodes a projective dependency tree. Another n2 factors (one per variable) evaluate the individual arcs given the sentence, so that p(y) describes a firstorder dependency parser. A higher-order parsing model is achieved by also including higher-order factors, each scoring configurations of two or more arcs, such as grandparent and sibling configurations. Higher-order factors tend to create cycles in the factor graph. See Figure 1 for an example factor graph.\nWe define each potential function to have a loglinear form: \u03c8\u03b1(y\u03b1) = exp(\u03b8 \u00b7 f\u03b1(y\u03b1,x)). Here x is the assignment to the observed variables such as the sentence and its POS tags; f\u03b1 extracts a vector of features; and \u03b8 is our vector of model parameters. We write the resulting probability distribution over parses as p\u03b8(y |x), to indicate that it depends on \u03b8. Loss For dependency parsing, our loss function is the number of missing edges in the predicted parse y\u0302, relative to the reference (or \u201cgold\u201d) parse y\u2217:\n`(y\u0302,y\u2217) = \u2211\ni: y\u0302i=OFF I(y\u2217i = ON) (1)\nI is the indicator function. Because y\u0302 and y\u2217 each specify exactly one parent per word token, `(y\u0302,y\u2217) equals the directed dependency error: the number of word tokens whose parent is predicted incorrectly.\nDecoder To obtain a single parse as output, we use a minimum Bayes risk (MBR) decoder, which returns the tree with minimum expected loss under the model\u2019s distribution (Bickel and Doksum, 1977; Goodman, 1996). Our ` gives the decision rule:\nh\u03b8(x) = argmin y\u0302 Ey\u223cp\u03b8(\u00b7 |x)[`(y\u0302,y)] (2)\n= argmax y\u0302\n\u2211\ni: y\u0302i=ON\np\u03b8(yi = ON |x) (3)\nHere y\u0302 ranges over well-formed parses. Thus, our parser seeks a well-formed parse h\u03b8(x) whose individual edges have a high probability of being correct according to p\u03b8 (since it lacks knowledge y\u2217 of which edges are truly correct). MBR is the principled way to take a loss function into account under a probabilistic model. By contrast, maximum a posteriori (MAP) decoding does not consider the loss function. It would return the single highestprobability parse even if that parse, and its individual edges, were unlikely to be correct.2\nAll systems in this paper use MBR decoding to consider the loss function at test time. This implies that the ideal training procedure would be to find the true p\u03b8 so that its marginals can be used in (3). Our baseline system attempts this. Yet in practice, we will not be able to find the true p\u03b8 (model misspecification) nor exactly compute the marginals of p\u03b8 (computational intractability). Thus, this paper proposes a training procedure that compensates for the system\u2019s approximations, adjusting \u03b8 to reduce the actual loss of h\u03b8(x) as measured at training time.\nTo find the MBR parse, we first run inference to compute the marginal probability p\u03b8(yi = ON |x) for each edge. Then we maximize (3) by running a first-order dependency parser with edge scores equal to those probabilities.3 When our inference algorithm is approximate, we replace the exact marginal with its approximation\u2014the belief from BP, given by bi(ON) in (6) below.\nInference Loopy belief propagation (BP) (Murphy et al., 1999) computes approximations to the variable marginals\n2If we used a simple 0-1 loss function within (2), then MBR decoding would reduce to MAP decoding.\n3Prior work (Smith and Eisner, 2008; Bansal et al., 2014) used the log-odds ratio log p\u03b8(yi=ON)\np\u03b8(yi=OFF) as the edge scores for\ndecoding, but this yields a parse different from the MBR parse.\np\u03b8(yi |x) = \u2211 y\u2032:y\u2032i=yi p\u03b8(y \u2032 |x), as needed by (3), as well as the factor marginals p\u03b8(y\u03b1 |x) = \u2211 y\u2032:y\u2032\u03b1=y\u03b1 p\u03b8(y \u2032 |x). The algorithm proceeds by iteratively sending messages from variables, yi, to factors, \u03b1:\nm (t) i\u2192\u03b1(yi) \u221d\n\u220f\n\u03b2\u2208N (i)\\\u03b1 m\n(t\u22121) \u03b2\u2192i (yi) (4)\nand from factors to variables:\nm (t) \u03b1\u2192i(yi) \u221d\n\u2211\ny\u03b1\u223cyi \u03c8\u03b1(y\u03b1)\n\u220f\nj\u2208N (\u03b1)\\i m\n(t\u22121) j\u2192\u03b1 (yi)\n(5)\nwhere N (i) and N (\u03b1) denote the neighbors of yi and \u03b1 respectively, and where y\u03b1 \u223c yi is standard notation to indicate that y\u03b1 ranges over all assignments to the variables participating in the factor \u03b1 provided that the ith variable has value yi. Note that the messages at time t are computed from those at time (t\u22121). Messages at the final time tmax are used to compute the beliefs at each factor and variable:\nbi(yi) \u221d \u220f\n\u03b1\u2208N (i) m\n(tmax) \u03b1\u2192i (yi) (6)\nb\u03b1(y\u03b1) \u221d \u03c8\u03b1(y\u03b1) \u220f\ni\u2208N (\u03b1) m\n(tmax) i\u2192\u03b1 (yi) (7)\nWe assume each of the messages and beliefs given in (4)\u2013(7) are scaled to sum-to-one. For example, bi is normalized such that \u2211 yi bi(yi) = 1 and approximates the marginal distribution over yi values. Messages continue to change indefinitely if the factor graph is cyclic, but in the limit, the messages may converge. Although the equations above update all messages in parallel, convergence is much faster if only one message is updated per timestep, in some well-chosen serial order.4\nFor the PTREE factor, the summation over variable assignments required for m(t)\u03b1\u2192i(yi) in Eq. (5) equates to a summation over exponentially many projective parse trees. However, we can use an inside-outside variant of Eisner (1996)\u2019s algorithm\n4Following Dreyer and Eisner (2009, footnote 22), we choose an arbitrary directed spanning tree of the factor graph rooted at the PTREE factor. We visit the nodes in topologically sorted order (from leaves to root) and update any message from the node being visited to a node that is later in the order. We then reverse this order and repeat, so that every message has been passed once. This constitutes one iteration of BP.\nto compute this in polynomial time (we describe this as hypergraph parsing in \u00a73). The resulting \u201cstructured BP\u201d inference procedure\u2014detailed by Smith and Eisner (2008)\u2014is exact for first-order dependency parsing. When higher-order factors are incorporated, it is approximate but remains fast, whereas exact inference would be slow.5"}, {"heading": "3 Approximation-aware Learning", "text": "We aim to find the parameters \u03b8\u2217 that minimize a regularized objective function over the training sample of (sentence, parse) pairs {(x(d),y(d))}Dd=1.\n\u03b8\u2217 = argmin \u03b8\n1\nD\n(( D\u2211\nd=1\nJ(\u03b8;x(d),y(d)) ) + \u03bb\n2 ||\u03b8||22\n)\n(8)\nwhere \u03bb > 0 is the regularization coefficient and J(\u03b8;x,y\u2217) is a given differentiable function, possibly nonconvex. We locally minimize this objective using `2-regularized AdaGrad with Composite Mirror Descent (Duchi et al., 2011)\u2014a variant of stochastic gradient descent that uses mini-batches, an adaptive learning rate per dimension, and sparse lazy updates from the regularizer.6\nObjective Functions The standard choice for J is the negative conditional log-likelihood (\u00a76). However, as in Stoyanov et al. (2011), our aim is to minimize expected loss on the true data distribution over sentence/parse pairs (X,Y ):\n\u03b8\u2217 = argmin\u03b8 E[`(h\u03b8(X), Y )] (9)\nSince the true data distribution is unknown, we substitute the expected loss over the training sample, and regularize our objective in order to reduce sampling variance. Specifically, we aim to minimize the regularized empirical risk, given by (8) with J(\u03b8;x(d),y(d)) set to `(h\u03b8(x(d)),y(d)). Note that\n5How slow is exact inference for dependency parsing? For certain choices of higher-order factors, polynomial time is possible via dynamic programming (McDonald et al., 2005; Carreras, 2007; Koo and Collins, 2010). However, BP will typically be asymptotically faster (for a fixed number of iterations) and faster in practice. In some other settings, exact inference is NPhard. In particular, non-projective parsing becomes NP-hard with even second-order factors (McDonald and Pereira, 2006). BP can handle this case in polynomial time by replacing the PTREE factor with a TREE factor that allows edges to cross.\n6\u03b8 is initialized to 0 when not otherwise specified.\nthis loss function would not be differentiable\u2014a key issue we will take up below. This is the \u201cERMA\u201d method of Stoyanov and Eisner (2012). We will also consider simpler choices of J\u2014akin to the loss functions used by Domke (2011).\nGradient Computation To compute the gradient \u2207\u03b8J(\u03b8;x,y\u2217) of the loss on a single sentence (x,y\u2217) = (x(d),y(d)), we apply automatic differentiation (AD) in the reverse mode (Griewank and Corliss, 1991). This yields the same type of \u201cbackpropagation\u201d algorithm that has long been used for training neural networks (Rumelhart et al., 1986). It is important to note that the resulting gradient computation algorithm is exact up to floating-point error, and has the same asymptotic complexity as the original decoding algorithm, requiring only about twice the computation. The AD method applies provided that the original function is indeed differentiable with respect to \u03b8. In principle, it is possible to compute the gradient with minimal additional coding. There exists AD software (some listed at autodiff.org) that could be used to derive the necessary code automatically. Another option would be to use the perturbation method of Domke (2010). However, we implemented the gradient computation directly, and we describe it here.\nInference, Decoding, and Loss as a Feedfoward Circuit The backpropagation algorithm is often applied to neural networks, where the topology of a feedforward circuit is statically specified and can be applied to any input. Our BP algorithm, decoder, and loss function similarly define a feedfoward circuit that computes our function J . The circuit\u2019s depth depends on the number of BP timesteps, tmax. Its topology is defined dynamically (per sentence x(d)) by \u201cunrolling\u201d the computation into a graph.\nFigure 2 shows this topology. The high level modules consist of (A) computing potential functions, (B) initializing messages, (C) sending messages, (D) computing beliefs, and (E) decoding and computing the loss. We zoom in on two submodules: the first computes messages from the PTREE factor efficiently (C.1\u2013C.3); the second computes a softened version of our loss function (E.1\u2013E.3). Both of these submodules are made efficient by the insideoutside algorithm.\nThe next two sections describe in greater detail\nhow we define the function J (the forward pass) and how we compute its gradient (the backward pass). Backpropagation through the circuit from Figure 2 poses several challenges. Eaton and Ghahramani (2009), Stoyanov et al. (2011), and Domke (2011) showed how to backpropagate through the basic BP algorithm, and we reiterate the key details below (\u00a75.2). The remaining challenges form the primary technical contribution of this paper:\n1. Our true loss function `(h\u03b8(x),y\u2217) by way of the decoder h\u03b8 contains an argmax (3) over trees and is therefore not differentiable. We show how to soften this decoder (by substituting a softmax), making it differentiable (\u00a74.1). 2. Empirically, we find the above objective difficult to optimize. To address this, we substitute a simpler L2 loss function (commonly used in neural networks). This is easier to optimize and yields our best parsers in practice (\u00a74.2). 3. We show how to run backprop through the inside-outside algorithm on a hypergraph (\u00a75.4) for use in two modules: the softened decoder (\u00a75.1) and computation of messages from the PTREE factor (\u00a75.3). This allows us to go beyond Stoyanov et al. (2011) and train structured BP in an approximation-aware and lossaware fashion."}, {"heading": "4 Differentiable Objective Functions", "text": ""}, {"heading": "4.1 Annealed Risk", "text": "Minimizing the test-time loss is the appropriate goal for training an approximate system like ours. That loss is estimated by the empirical risk on a large amount of in-domain supervised training data.\nAlas, this risk is nonconvex and piecewise constant, so we turn to deterministic annealing (Smith and Eisner, 2006) and clever initialization. Directed dependency error, `(h\u03b8(x),y\u2217), is not differentiable due to the argmax in the decoder h\u03b8. So we redefine J(\u03b8;x,y\u2217) to be a new differentiable loss function, the annealed risk R1/T\u03b8 (x,y\n\u2217), which approaches the loss `(h\u03b8(x),y\u2217) as the temperature T \u2192 0. Our first step is to define a distribution over parses, which takes the marginals p\u03b8(yi = ON |x) as input, or in practice, their BP approximations bi(ON):\nq 1/T \u03b8 (y\u0302 |x) \u221d exp (\u2211 i:y\u0302i=ON p\u03b8(yi=ON |x) T ) (10)\nUsing this distribution, we can replace our nondifferentiable decoder h\u03b8 with a differentiable one (at training time). Imagine that our new decoder stochastically returns a parse y\u0302 sampled from this distribution. We define the annealed risk as the expected loss of that decoder:\nR 1/T \u03b8 (x,y \u2217) = E y\u0302\u223cq1/T\u03b8 (\u00b7 |x) [`(y\u0302,y\u2217)] (11)\nAs T \u2192 0 (\u201cannealing\u201d), the decoder almost always chooses the MBR parse,7 so our risk approaches the loss of the actual MBR decoder that will be used at test time. However, as a function of \u03b8, it remains differentiable (though not convex) for any T > 0.\nTo compute the annealed risk, observe that it simplifies to R1/T\u03b8 (x,y \u2217) = \u2212\u2211i:y\u2217i=ON q 1/T \u03b8 (y\u0302i = ON |x). This is the negated expected recall of a parse y\u0302 \u223c q1/T\u03b8 . We obtain the required marginals q 1/T \u03b8 (y\u0302i = ON |x) from (10) by running inside-\n7Recall from (3) that the MBR parse is the tree y\u0302 that maximizes the sum\n\u2211 i:y\u0302i=ON\np\u03b8(yi = ON |x). As T \u2192 0, the right-hand side of (10) grows fastest for this y\u0302, so its probability under q1/T\u03b8 approaches 1 (or 1/k if there is a k-way tie for MBR parse).\noutside where the edge weight for edge i is given by exp(p\u03b8(yi = ON |x)/T ).\nWhether our test-time system computes the marginals of p\u03b8 exactly or does so approximately via BP, our new training objective approaches (as T \u2192 0) the true empirical risk of the test-time parser that performs MBR decoding from the computed marginals. Empirically, however, we will find that it is not the most effective training objective (\u00a77.2). Stoyanov et al. (2011) postulate that the nonconvexity of empirical risk may make it a difficult function to optimize, even with annealing. Our next two objectives provide alternatives."}, {"heading": "4.2 L2 Distance", "text": "We can view our inference, decoder, and loss as defining a form of deep neural network, whose topology is inspired by our linguistic knowledge of the problem (e.g., the edge variables should define a tree). This connection to deep learning allows us to consider training methods akin to supervised layer-wise training (Bengio et al., 2007). We temporarily remove the top layers of our network (i.e. the decoder and loss module, Fig. 2 (E)) so that the output layer of our \u201cdeep network\u201d consists of the variable beliefs bi(yi) from BP. We can then define a supervised loss function directly on these beliefs. We don\u2019t have supervised data for this layer of beliefs, but we can create it artificially. Use the supervised parse y\u2217 to define \u201ctarget beliefs\u201d by b\u2217i (yi) = I(yi = y\u2217i ) \u2208 {0, 1}. To find parameters \u03b8 that make BP\u2019s beliefs close to these targets, we can minimize an L2 distance loss function:\nJ(\u03b8;x,y\u2217) = \u2211\ni\n\u2211\nyi\n(bi(yi)\u2212 b\u2217i (yi))2 (12)\nWe can use this L2 distance objective function for training, adding the MBR decoder and loss evaluation back in only at test time."}, {"heading": "4.3 Layer-wise Training", "text": "Just as in layer-wise training of neural networks, we can take a two-stage approach to training. First, we train to minimize the L2 distance. Then, we use the resulting \u03b8 as initialization to optimize the annealed risk, which does consider the decoder and loss function (i.e. the top layers of Fig. 2). Stoyanov et al. (2011) found mean squared error (MSE) to give a\nsmoother training objective, though still nonconvex, and used it to initialize empirical risk. Though their variant of the L2 objective did not completely dispense with the decoder as ours does, it is a similar approach to our proposed layer-wise training."}, {"heading": "5 Gradients by Backpropagation", "text": "Backpropagation computes the derivative of any given function specified by an arbitrary circuit consisting of elementary differentiable operations (e.g. +,\u2212,\u00d7,\u00f7, log, exp). This is accomplished by repeated application of the chain rule. Backpropagating through an algorithm proceeds by similar application of the chain rule, where the intermediate quantities are determined by the topology of the circuit\u2014just as in Figure 2. Running backwards through the circuit, backprop computes the partial derivatives of the objective J(\u03b8;x,y\u2217) with respect to each intermediate quantity u\u2014or more concisely the adjoint of u: \u00f0u = \u2202J(\u03b8;x,y\n\u2217) \u2202u . This section\ngives a summary of the adjoint computations we require. Due to space constraints, we direct the reader to the extended version of this paper (Gormley et al., 2015a) for full details of all the adjoints."}, {"heading": "5.1 Backpropagation of Decoder / Loss", "text": "The adjoint of the objective itself \u00f0J(\u03b8;x,y\u2217) is always 1. So the first adjoints we must compute are those of the beliefs: \u00f0bi(yi) and \u00f0b\u03b1(y\u03b1). This corresponds to the backward pass through Figure 2 (E). Consider the simple case where J is L2 distance from (12): the variable belief adjoint is \u00f0bi(yi) = 2(bi(yi)\u2212 b\u2217i (yi)) and trivially \u00f0b\u03b1(y\u03b1) = 0. If J is annealed risk from (11), we compute \u00f0bi(yi) by applying backpropagation recursively to our algorithm for J from \u00a74.1. This sub-algorithm defines a subcircuit depicted in Figure 2 (E.1\u2013E.3). The computations of the annealed beliefs and the expected recall are easily differentiable. The main challenge is differentiating the function computed by the insideoutside algorithm; we address this in \u00a75.4."}, {"heading": "5.2 Backpropagation through Structured BP", "text": "Given the adjoints of the beliefs, we next backpropagate through structured BP\u2014extending prior work which did the same for regular BP (Eaton and Ghahramani, 2009; Stoyanov et al., 2011; Domke,\n2011). Except for the messages sent from the PTREE factor, each step of BP computes some value from earlier values using the update equations (4)\u2013(7). Backpropagation differentiates these elementary expressions. First, using the belief adjoints, we compute the adjoints of the final messages (\u00f0m(tmax)j\u2192\u03b1 (yj), \u00f0m (tmax) \u03b2\u2192i (yi)) by applying the chain rule to Eqs. (6) and (7). This is the backward pass through Fig. 2 (D). Recall that the messages at time t were computed from messages at time t \u2212 1 and the potential functions \u03c8\u03b1 in the forward pass via Eqs. (4) and (5). Backprop works in the opposite order, updating the adjoints of the messages at time t\u2212 1 and the potential functions (\u00f0m(t\u22121)j\u2192\u03b1 (yj), \u00f0m(t\u22121)\u03b2\u2192i (yi), \u00f0\u03c8\u03b1(y\u03b1)) only after it has computed the adjoints of the messages at time t. Repeating this through timesteps {t, t \u2212 1, . . . , 1} constitutes the backward pass through Fig. 2 (C). The backward pass through Fig. 2 (B) does nothing, since the messages were initialized to a constant. The final step of backprop uses \u00f0\u03c8\u03b1(y\u03b1) to compute \u00f0\u03b8j\u2014the backward pass through Fig. 2 (A). For the explicit formula of these adjoints, see Gormley et al. (2015a) or Appendix A.1 of Stoyanov et al. (2011). The next section handles the special case of \u00f0m(t)j\u2192PTREE(yj)."}, {"heading": "5.3 BP and Backpropagation with PTREE", "text": "The PTREE factor has a special structure that we exploit for efficiency during BP. Smith and Eisner (2008) give a more efficient way to implement Eq. (5), which computes the message from a factor \u03b1 to a variable yi, in the special case where \u03b1 = PTREE. They first run the inside-outside algorithm where the edge weights are given by the ratios of the messages to PTREE: m (t) i\u2192\u03b1(ON)\nm (t) i\u2192\u03b1(OFF)\n. Then\nthey multiply each resulting edge marginal given by inside-outside by the product of all the OFF messages \u220f im (t) i\u2192\u03b1(OFF) to get the marginal factor belief b\u03b1(yi). Finally they divide the belief by the incoming message m(t)i\u2192\u03b1(ON) to get the corresponding outgoing message m(t+1)\u03b1\u2192i (ON). These steps are shown in Figure 2 (C.1\u2013C.3), and are repeated each time we send a message from the PTree factor.\nSimilarly, we exploit the structure of this algorithm to compute the adjoints \u00f0m(t)j\u2192PTREE(yj). The derivatives of the message ratios and products men-\ntioned here are simple. In the next subsection, we explain how to backpropagate through the insideoutside algorithm. Though we focus here on projective dependency parsing, our techniques are also applicable to non-projective parsing and the TREE factor; we leave this to future work."}, {"heading": "5.4 Backprop of Hypergraph Inside-Outside", "text": "Both the annealed risk loss function (\u00a74.1) and the computation of messages from the PTREE factor (\u00a75.3) use the inside-outside algorithm for dependency parsing. Here we describe inside-outside and the accompanying backpropagation algorithm over a hypergraph. This general treatment (Klein and Manning, 2001; Li and Eisner, 2009) enables our method to be applied to other tasks such as constituency parsing, HMM forward-backward, and hierarchical machine translation. In the case of dependency parsing, the structure of the hypergraph is given by the dynamic programming algorithm of Eisner (1996).\nFor the forward pass of the inside-outside module, the input variables are the hyperedge weights we\u2200e and the outputs are the marginal probabilities pw(i)\u2200i of each node i in the hypergraph. The latter are a function of the inside \u03b2i and outside \u03b1j probabilities. We initialize \u03b1root = 1.\n\u03b2i = \u2211\ne\u2208I(i) we\n\u220f\nj\u2208T (e) \u03b2j (13)\n\u03b1j = \u2211\ne\u2208O(i) we \u03b1H(e)\n\u220f\nj\u2208T (e):j 6=i \u03b2j (14)\npw(i) = \u03b1i\u03b2i/\u03b2root (15)\nFor each node i, we define the set of incoming edges I(i) and outgoing edges O(i). The antecedents of the edge are T (e), the parent of the edge is H(e), and its weight is we.\nFor the backward pass of the inside-outside module, the inputs are \u00f0pw(i)\u2200i and the outputs are \u00f0we\u2200e. We also compute the adjoints of the intermediate quantities \u00f0\u03b2j , \u00f0\u03b1i. We first compute \u00f0\u03b1i bottom-up. Next \u00f0\u03b2j are computed top-down. The adjoints \u00f0we are then computed in any order.\n\u00f0\u03b1i = \u00f0pw(i)\u2202pw(i)\u2202\u03b1i + \u2211\ne\u2208I(i)\n\u2211\nj\u2208T (e) \u00f0\u03b1j \u2202\u03b1j\u2202\u03b1i (16)\n\u00f0\u03b2root = \u2211\ni 6=root \u00f0pw(i)\u2202pw(i)\u2202\u03b2root (17)\n\u00f0\u03b2j = \u00f0pw(j)\u2202pw(j)\u2202\u03b2j + \u2211\ne\u2208O(j) \u00f0\u03b2H(e) \u2202\u03b2H(e) \u2202\u03b2j\n+ \u2211\ne\u2208O(j)\n\u2211\nk\u2208T (e):k 6=j \u00f0\u03b1k \u2202\u03b1k\u2202\u03b2j \u2200j 6= root (18)\n\u00f0we = \u00f0\u03b2H(e) \u2202\u03b2H(e) \u2202we\n+ \u2211\nj\u2208T (e) \u00f0\u03b1j \u2202\u03b1j\u2202we (19)\nThe partial derivatives required for the above adjoints are given in the extended version of this paper (Gormley et al., 2015a). This backpropagation method is used for both Figure 2 (C.2) and (E.2)."}, {"heading": "6 Other Learning Settings", "text": "Loss-aware Training with Exact Inference Backpropagating through inference, decoder, and loss need not be restricted to approximate inference algorithms. Li and Eisner (2009) optimize Bayes risk with exact inference on a hypergraph for machine translation. Each of our differentiable loss functions (\u00a74) can also be coupled with exact inference. For a first-order parser, BP is exact. Yet, in place of modules (B), (C), and (D) in Figure 2, we can use a standard dynamic programming algorithm for dependency parsing, which is simply another instance of inside-outside on a hypergraph (\u00a75.4). The exact marginals from inside-outside (15) are then fed forward into the decoder/loss module (E).\nConditional and Surrogate Log-likelihood The standard approach to training is conditional loglikelihood (CLL) maximization (Smith and Eisner, 2008) without taking inexact inference into account: J(\u03b8;x,y\u2217) = \u2212 log p\u03b8(y |x). When inference is exact, this baseline computes the true gradient of CLL. When inference is approximate, this baseline uses the factor beliefs b\u03b1(y\u03b1) from BP in place of the exact marginals in the gradient. The literature refers to this approximation-unaware training method as surrogate likelihood training since it returns the \u201cwrong\u201d parameters even under the assumption of infinite training data drawn from the model being used (Wainwright, 2006). Despite this, the surrogate likelihood objective is commonly used to train CRFs. CLL and approximation-aware training are not mutually exclusive. Training a standard factor graph with ERMA and a log-likelihood objective recovers CLL exactly (Stoyanov et al., 2011)."}, {"heading": "7 Experiments", "text": ""}, {"heading": "7.1 Setup", "text": "Features As the focus of this work is on a novel approach to training, we look to prior work for model and feature design (\u00a72). We add O(n3) second-order grandparent and arbitrary-sibling factors as in Riedel and Smith (2010) and Martins et al. (2010). We use standard feature sets for first-order (McDonald et al., 2005) and second-order (Carreras, 2007) parsing. Following Rush and Petrov (2012), we also include a version of each part-of-speech (POS) tag feature, with the coarse tags from Petrov et al. (2012). We use feature hashing (Ganchev and Dredze, 2008; Weinberger et al., 2009) and restrict to at most 20 million features. We leave the incorporation of third-order features to future work.\nPruning To reduce the time spent on feature extraction, we enforce the type-specific dependency length bounds from Eisner and Smith (2005) as used by Rush and Petrov (2012): the maximum allowed dependency length for each tuple (parent tag, child tag, direction) is given by the maximum observed length for that tuple in the training data. Following Koo and Collins (2010), we train a first-order model with CLL and for each token prune any parents for which the marginal probability is less than 0.0001 times the maximum parent marginal for that token. On a per-token basis, we further restrict to the ten parents with highest marginal probability as in Martins et al. (2009) (but we avoid pruning the fully right-branching tree, so that some parse always exists).8 This lets us simplify the factor graph, removing variables yi corresponding to pruned edges and specializing their factors to assume yi = OFF. We train the full model\u2019s parameters to work well on this pruned graph.\nData We consider 19 languages from the CoNLL2006 (Buchholz and Marsi, 2006) and CoNLL-2007 (Nivre et al., 2007) Shared Tasks. We also convert the English Penn Treebank (PTB) (Marcus et al., 1993) to dependencies using the head rules from Yamada and Matsumoto (2003) (PTB-YM). We evaluate unlabeled attachment accuracy (UAS) using gold\n8The pruning model uses a simpler feature set as in Rush and Petrov (2012). Pruning is likely the least impactful of our approximations: it obtains 99.46% oracle UAS for English.\nPOS tags for the CoNLL languages, and predicted tags from TurboTagger (Martins et al., 2013) for the PTB. Unlike most prior work, we hold out 10% of each CoNLL training dataset as development data for regularization by early stopping.9\nSome of the CoNLL languages contain nonprojective edges, but our system is built using a probability distribution over projective trees only. ERMA can still be used with such a badly misspecified model\u2014one of its advantages\u2014but no amount of training can raise CLL\u2019s objective above \u2212\u221e, since any non-projective gold tree will always have probability 0. Thus, for CLL only, we replace each gold tree in training data with a minimum-loss projective tree (Carreras, 2007).10 This resembles ERMA\u2019s goal of training the system to find a lowloss projective tree. At test time, we always evaluate the system\u2019s projective output trees against the possibly non-projective gold trees, as in prior work.\nLearning Settings We compare three learning settings. The first, our baseline, is conditional log-\n9In dev experiments, we found L2 distance to be less sensitive to the `2-regularizer weight than CLL. So we added additional regularization by early stopping to improve CLL.\n10We also ran a controlled experiment with L2 and not just CLL trained on these projectivized trees: the average margin of improvement for our method widened very slightly.\nlikelihood training (CLL) (\u00a76). As is common in the literature, we conflate two distinct learning settings (conditional log-likelihood/surrogate loglikelihood) under the single name \u201cCLL,\u201d allowing the inference method (exact/inexact) to differentiate them. The second learning setting is approximationaware learning (\u00a73) with either our L2 distance objective (L2) (\u00a74.2) or our layer-wise training method (L2+AR) which takes the L2-trained model as an initializer for our annealed risk (\u00a74.3). The annealed risk objective requires an annealing schedule: over the course of training, we linearly anneal from initial temperature T = 0.1 to T = 0.0001, updating T at each step of stochastic optimization. The third learning setting uses the same two objectives, L2 and L2+AR, but with exact inference (\u00a76). The `2-regularizer weight in (8) is \u03bb = 1. Each method is trained by AdaGrad for 5 epochs with early stopping (i.e. the model with the highest score on dev data is returned). Across CoNLL, the average epoch chosen for CLL was 2.02 and for L2 was 3.42. The learning rate for each training run is dynamically tuned on a sample of the training data."}, {"heading": "7.2 Results", "text": "Our goal is to demonstrate that our approximationaware training method leads to improved parser accuracy as compared with the standard training approach of conditional log-likelihood (CLL) maximization (Smith and Eisner, 2008), which does not\ntake inexact inference into account. The two key findings of our experiments are that our learning approach is more robust to (1) decreasing the number of iterations of BP and (2) adding additional cycles to the factor graph in the form of higher-order factors. In short: our approach leads to faster inference and creates opportunities for more accurate parsers.\nSpeed-Accuracy Tradeoff Our first experiment is on English dependencies. For English PTB-YM, Figure 3 shows accuracy as a function of the number of BP iterations for our second-order model with both arbitrary sibling and grandparent factors on English. We find that our training methods (L2 and L2+AR) obtain higher accuracy than standard training (CLL), particularly when a small number of BP iterations are used and the inference is a worse approximation. Notice that with just two iterations of BP, the parsers trained by our approach obtain accuracy greater than or equal to those by CLL with any number of iterations (1 to 8). Contrasting the two objectives for our approximation-aware training, we find that our simple L2 objective performs very well. In fact, in only two cases, at 3 and 5 iterations, does risk annealing (L2+AR) further improve performance on test data. In our development experiments, we also evaluated AR without using L2 for initialization and we found that it performed worse than either of CLL and L2 alone. That AR performs only slightly better than L2 (and not worse) in the case of L2+AR is likely due to early stopping on dev data, which guards against selecting a worse model.\nIncreasingly Cyclic Models Figure 4 contrasts accuracy with the type of 2nd-order factors (grandparent, sibling, or both) included in the model for English, for a fixed budget of 4 BP iterations. Adding higher-order factors introduces more loops, making the loopy BP approximation more problematic for standard CLL training. By contrast, under approximation-aware training, enriching the model with more factors always helps performance, as desired, rather than hurting it.\nNotice that our advantage is not restricted to the case of loopy graphs. Even when we use a 1storder model, for which BP inference is exact, our approach yields higher-accuracy parsers than CLL training. We speculate that this improvement is due to our method\u2019s ability to better deal with model\nmisspecification\u2014a first-order model is quite misspecified! Note the following subtle point: when inference is exact, the CLL estimator is actually a special case of our approximation-aware learner\u2014 that is, CLL computes the same gradient that our training by backpropagation would if we used loglikelihood as the objective.\nExact Inference with Grandparents \u00a72 noted that since we always do MBR decoding, the ideal strategy is to fit the true distribution with a good model. Consider a \u201cgood model\u201d that includes unary and grandparent factors. Exact inference is possible here in O(n4) time by dynamic programming (Koo and Collins, 2010, Model 0). Table 1 shows that CLL training with exact inference indeed does well on test data\u2014but that accuracy falls if we substitute fast approximate inference (4 iterations of BP). Our proposed L2 training is able to close the gap, just as intended. That is, we succesfully train a few iterations of an approximate O(n3) algorithm to behave as well as an exact O(n4) algorithm.\nOther Languages Our final experiments train and test our parsers on 19 languages from CoNLL2006/2007 (Table 2). We find that, on average across languages, approximation-aware training with an L2 objective obtains higher UAS than CLL training. This result holds for both our poorest model (1storder) and our richest one (2nd-order with grandparent and sibling factors), using 1, 2, 4, or 8 iterations of BP. Notice that the approximation-aware training doesn\u2019t always outperform CLL training\u2014only in the aggregate. Again, we see the trend that our training approach yields larger gains when BP is restricted to a small number of maximum iterations. It is possible that larger training sets would also favor our approach, by providing a clearer signal of how to reduce the objective (8)."}, {"heading": "8 Discussion", "text": "The purpose of this work was to explore ERMA and related training methods for models which incorporate structured factors. We applied these methods to a basic higher-order dependency parsing model, because that was the simplest and first instance of structured BP (Smith and Eisner, 2008). In future work, we hope to explore further models with structured factors\u2014particularly those which jointly account for multiple linguistic strata (e.g. syntax, semantics, and topic). Another natural extension of this work is to explore other types of factors: here we considered only log-linear potential functions (commonly used in CRFs), but any differentiable function would be appropriate, such as a neural network (Durrett and Klein, 2015; Gormley et al., 2015b).\nOur primary contribution is approximation-aware training for structured BP. We have specifically presented message-passing formulas for any factor whose belief\u2019s partition function can be computed as the total weight of all hyperpaths in a weighted hypergraph. This would suffice to train the structured BP systems that have been built for projective\ndependency parsing (Smith and Eisner, 2008), CNF grammar parsing (Naradowsky et al., 2012), TAG (Auli and Lopez, 2011), ITG-constraints for phrase extraction (Burkett and Klein, 2012), and graphical models over strings (Dreyer and Eisner, 2009)."}, {"heading": "9 Conclusions", "text": "We introduce a new approximation-aware learning framework for belief propagation with structured factors. We present differentiable objectives for both empirical risk minimization (a\u0300 la ERMA) and a novel objective based on L2 distance between the inferred beliefs and the true edge indicator functions. Experiments on the English Penn Treebank and 19 languages from CoNLL-2006/2007 shows that our estimator is able to train more accurate dependency parsers with fewer iterations of belief propagation than standard conditional log-likelihood training, by taking approximations into account. For additional details, see the tech report version of this paper (Gormley et al., 2015a). Our code is available in a general-purpose library for structured BP, hypergraphs, and backprop (Gormley, 2015).\nAcknowledgments This research was funded by the Human Language Technology Center of Excellence at Johns Hopkins University. Thanks to the anonymous reviewers for their insightful comments."}], "year": 2015, "references": [{"title": "A comparison of loopy belief propagation and dual decomposition for integrated CCG supertagging and parsing", "authors": ["Michael Auli", "Adam Lopez."], "venue": "Proceedings of ACL.", "year": 2011}, {"title": "Structured learning for taxonomy induction with belief propagation", "authors": ["Mohit Bansal", "David Burkett", "Gerard de Melo", "Dan Klein."], "venue": "Proceedings of ACL.", "year": 2014}, {"title": "Greedy layer-wise training of deep networks", "authors": ["Yoshua Bengio", "Pascal Lamblin", "Dan Popovici", "Hugo Larochelle."], "venue": "B. Sch\u00f6lkopf, J.C. Platt, and T. Hoffman, editors, Advances in Neural Information Processing Systems 19.", "year": 2007}, {"title": "Mathematical Statistics: Basic Ideas and Selected Topics", "authors": ["Peter J. Bickel", "Kjell A. Doksum."], "venue": "Holden-Day Inc., Oakland, CA, USA.", "year": 1977}, {"title": "CoNLL-X shared task on multilingual dependency parsing", "authors": ["Sabine Buchholz", "Erwin Marsi."], "venue": "Proceedings of CoNLL.", "year": 2006}, {"title": "Fast inference in phrase extraction models with belief propagation", "authors": ["David Burkett", "Dan Klein."], "venue": "Proceedings of NAACL-HLT.", "year": 2012}, {"title": "Experiments with a higher-order projective dependency parser", "authors": ["Xavier Carreras."], "venue": "Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007.", "year": 2007}, {"title": "Implicit differentiation by perturbation", "authors": ["Justin Domke."], "venue": "Advances in Neural Information Processing Systems.", "year": 2010}, {"title": "Parameter learning with truncated message-passing", "authors": ["Justin Domke."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).", "year": 2011}, {"title": "Graphical models over multiple strings", "authors": ["Markus Dreyer", "Jason Eisner."], "venue": "Proceedings of EMNLP.", "year": 2009}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "authors": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "The Journal of Machine Learning Research.", "year": 2011}, {"title": "Neural CRF Parsing", "authors": ["Greg Durrett", "Dan Klein."], "venue": "Proceedings of ACL.", "year": 2015}, {"title": "Choosing a variable to clamp", "authors": ["Frederik Eaton", "Zoubin Ghahramani."], "venue": "Proceedings of AISTATS.", "year": 2009}, {"title": "Parsing with soft and hard constraints on dependency length", "authors": ["Jason Eisner", "Noah A. Smith."], "venue": "Proceedings of the International Workshop on Parsing Technologies (IWPT).", "year": 2005}, {"title": "Three new probabilistic models for dependency parsing: An exploration", "authors": ["Jason Eisner."], "venue": "Proceedings of COLING.", "year": 1996}, {"title": "Factor graphs and algorithms", "authors": ["Brendan J. Frey", "Frank R. Kschischang", "Hans-Andrea Loeliger", "Niclas Wiberg."], "venue": "Proceedings of the Annual Allerton Conference on Communication Control and Computing, volume 35.", "year": 1997}, {"title": "Small statistical models by random feature mixing", "authors": ["Kuzman Ganchev", "Mark Dredze."], "venue": "Proceedings of the ACL08 HLT Workshop on Mobile Language Processing.", "year": 2008}, {"title": "Efficient algorithms for parsing the DOP model", "authors": ["Joshua Goodman."], "venue": "Proceedings of EMNLP.", "year": 1996}, {"title": "Approximation-aware dependency parsing by belief propagation (extended version)", "authors": ["Matthew R. Gormley", "Mark Dredze", "Jason Eisner."], "venue": "Technical report available from arXiv.org as arXiv:1508.02375.", "year": 2015}, {"title": "Improved relation extraction with feature-rich compositional embedding models", "authors": ["Matthew R. Gormley", "Mo Yu", "Mark Dredze."], "venue": "Proceedings of EMNLP.", "year": 2015}, {"title": "Pacaya\u2014a graphical models and NLP library", "authors": ["Matthew R. Gormley."], "venue": "Available from https:// github.com/mgormley/pacaya.", "year": 2015}, {"title": "Structured perceptron with inexact search", "authors": ["Liang Huang", "Suphan Fayong", "Yang Guo."], "venue": "Proceedings of NAACL-HLT.", "year": 2012}, {"title": "Parsing and hypergraphs", "authors": ["Dan Klein", "Christopher D. Manning."], "venue": "Proceedings of the International Workshop on Parsing Technologies (IWPT).", "year": 2001}, {"title": "Efficient thirdorder dependency parsers", "authors": ["Terry Koo", "Michael Collins."], "venue": "Proceedings of ACL.", "year": 2010}, {"title": "Factor graphs and the sumproduct algorithm", "authors": ["Frank R. Kschischang", "Brendan J. Frey", "HansAndrea Loeliger."], "venue": "IEEE Transactions on Information Theory, 47(2).", "year": 2001}, {"title": "Structured Learning with Approximate Inference", "authors": ["Alex Kulesza", "Fernando Pereira."], "venue": "Advances in Neural Information Processing Systems.", "year": 2008}, {"title": "First- and second-order expectation semirings with applications to minimumrisk training on translation forests", "authors": ["Zhifei Li", "Jason Eisner."], "venue": "Proceedings of EMNLP.", "year": 2009}, {"title": "Building a large annotated corpus of English: The penn treebank", "authors": ["Mitchell P. Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini."], "venue": "Computational linguistics, 19(2).", "year": 1993}, {"title": "Concise integer linear programming formulations for dependency parsing", "authors": ["Andr\u00e9 F.T. Martins", "Noah A. Smith", "Eric P. Xing."], "venue": "Proceedings of ACLIJCNLP.", "year": 2009}, {"title": "Turbo parsers: Dependency parsing by approximate variational inference", "authors": ["Andr\u00e9 F.T. Martins", "Noah A. Smith", "Eric P. Xing", "Pedro M.Q. Aguiar", "M\u00e1rio A.T. Figueiredo."], "venue": "Proceedings of EMNLP.", "year": 2010}, {"title": "Turning on the turbo: Fast third-order non-projective turbo parsers", "authors": ["Andr\u00e9 F.T. Martins", "Miguel B. Almeida", "Noah A. Smith."], "venue": "Proceedings of ACL.", "year": 2013}, {"title": "Online learning of approximate dependency parsing algorithms", "authors": ["Ryan McDonald", "Fernando Pereira."], "venue": "Proceedings of EACL.", "year": 2006}, {"title": "Online large-margin training of dependency parsers", "authors": ["Ryan McDonald", "Koby Crammer", "Fernando Pereira."], "venue": "Proceedings of ACL.", "year": 2005}, {"title": "Loopy belief propagation for approximate inference: An empirical study", "authors": ["Kevin P. Murphy", "Yair Weiss", "Michael I. Jordan."], "venue": "Proceedings of UAI.", "year": 1999}, {"title": "Grammarless parsing for joint inference", "authors": ["Jason Naradowsky", "Tim Vieira", "David A. Smith."], "venue": "Proceedings of COLING.", "year": 2012}, {"title": "The CoNLL 2007 shared task on dependency parsing", "authors": ["Joakim Nivre", "Johan Hall", "Sandra K\u00fcbler", "Ryan McDonald", "Jens Nilsson", "Sebastian Riedel", "Deniz Yuret."], "venue": "Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007.", "year": 2007}, {"title": "A universal part-of-speech tagset", "authors": ["Slav Petrov", "Dipanjan Das", "Ryan McDonald."], "venue": "Proceedings of LREC.", "year": 2012}, {"title": "Relaxed marginal inference and its application to dependency parsing", "authors": ["Sebastian Riedel", "David A. Smith."], "venue": "Proceedings of NAACL-HLT.", "year": 2010}, {"title": "Learning internal representations by error propagation", "authors": ["David E. Rumelhart", "Geoffrey E. Hinton", "Ronald J. Williams."], "venue": "David E. Rumelhart and James L. McClelland, editors, Parallel Distributed Processing: Explorations in the Microstructure of", "year": 1986}, {"title": "Vine pruning for efficient multi-pass dependency parsing", "authors": ["Alexander M. Rush", "Slav Petrov."], "venue": "Proceedings of NAACL-HLT.", "year": 2012}, {"title": "Minimum-risk annealing for training log-linear models", "authors": ["David A. Smith", "Jason Eisner."], "venue": "Proceedings of COLING-ACL.", "year": 2006}, {"title": "Dependency parsing by belief propagation", "authors": ["David A. Smith", "Jason Eisner."], "venue": "Proceedings of EMNLP.", "year": 2008}, {"title": "Minimum-risk training of approximate CRF-Based NLP systems", "authors": ["Veselin Stoyanov", "Jason Eisner."], "venue": "Proceedings of NAACL-HLT.", "year": 2012}, {"title": "Empirical risk minimization of graphical model parameters given approximate inference, decoding, and model structure", "authors": ["Veselin Stoyanov", "Alexander Ropson", "Jason Eisner."], "venue": "Proceedings of AISTATS.", "year": 2011}, {"title": "Estimating the \u201cwrong\u201d graphical model: Benefits in the computation-limited setting", "authors": ["Martin J. Wainwright."], "venue": "The Journal of Machine Learning Research,", "year": 2006}, {"title": "Feature hashing for large scale multitask learning", "authors": ["Kilian Weinberger", "Anirban Dasgupta", "John Langford", "Alex Smola", "Josh Attenberg."], "venue": "Proceedings of ICML.", "year": 2009}, {"title": "Statistical dependency analysis with support vector machines", "authors": ["Hiroyasu Yamada", "Yuji Matsumoto."], "venue": "Proceedings of the International Workshop on Parsing Technologies (IWPT), volume 3.", "year": 2003}], "id": "SP:1abe41711155afe82222ac0f99b978b32b1e68b5", "authors": [{"name": "Matthew R. Gormley", "affiliations": []}, {"name": "Mark Dredze", "affiliations": []}, {"name": "Jason Eisner", "affiliations": []}], "abstractText": "We show how to train the fast dependency parser of Smith and Eisner (2008) for improved accuracy. This parser can consider higher-order interactions among edges while retaining O(n) runtime. It outputs the parse with maximum expected recall\u2014but for speed, this expectation is taken under a posterior distribution that is constructed only approximately, using loopy belief propagation through structured factors. We show how to adjust the model parameters to compensate for the errors introduced by this approximation, by following the gradient of the actual loss on training data. We find this gradient by backpropagation. That is, we treat the entire parser (approximations and all) as a differentiable circuit, as others have done for loopy CRFs (Domke, 2010; Stoyanov et al., 2011; Domke, 2011; Stoyanov and Eisner, 2012). The resulting parser obtains higher accuracy with fewer iterations of belief propagation than one trained by conditional log-likelihood.", "title": "Approximation-Aware Dependency Parsing by Belief Propagation"}