24	40	We consider both how to best aggregate sequential crowd labels (Task 1) and how to best predict sequences in unannotated text given a training set of crowd annotations (Task 2).
25	23	As part of this work, we propose novel models for working with noisy sequence labels from the crowd.
27	57	As noted in the Abstract, we have also shared our sourcecode and data online for use by the community.
62	42	Model: We first define a standard HMM with hidden states hi, observations vi, transition parameter vectors τ hi and emission parameter vectors Ωhi : hi+1|hi ∼ Discrete(τ hi) (1) vi|hi ∼ Discrete(Ωhi) (2) The discrete distributions here are governed by Multinomials.
64	45	For the crowd component, assume there are n classes, and let lij be the label for word i provided by worker j.
69	18	This is exacerbated when the label distribution is imbalanced, e.g., most words are not part of a named entity, concentrating the counts in a few confusion matrix entries.
84	62	Eh′|h is the expected number of times the HMM transitioned from state h to state h′, where the expectation is with respect to the posterior distribution p(hi, hi+1|V,L) that we infer in the E step: Eh′|h = ∑ i p(hi = h, hi+1 = h ′|V,L) (10) Similarly, Eh is the expected number of times the HMM is at state h: Eh = ∑ i p(hi = h|V,L) and Ev|h is the expected number of times the HMM emits the observation v from the state h: Ev|h =∑ i,vi=v p(hi = h|V,L).
85	9	For the crowd parameters C′(j), we use the (smoothed) maximum likelihood estimate: C ′(j) k = E (j) k|k + ac E (j) k + nac (11) where ac is the smoothing parameter and E (j) k|k is the expected number of times that worker j correctly labeled a word of true class k as k while E(j)k is the expected total number of words belonging to class k worker j has labeled.
87	10	For Task 2, we extend the LSTM architecture (Hochreiter and Schmidhuber, 1997) for NER (Lample et al., 2016) to account for noisy crowdsourced labels (this can be easily adapted to other sequence labeling tasks).
89	34	The LSTM block’s output then becomes input to a (fully connected) hidden layer, which produces a vector of tags scores for each word.
91	9	All the tags scores are then fed into a ‘CRF layer’ that ‘connects’ the word-level predictions in the sentence and produces the final output: the most likely sequence of tags.
97	28	Worker IDs are mapped to vector representations and incorporated into the LSTM architecture.
105	18	In this way, we are free to set the dimension of the crowd vectors and we have a more flexible model of worker noise.
108	10	We use the English portion of the CoNLL2003 dataset (Tjong Kim Sang and De Meulder, 2003), which includes over 21,000 annotated sentences from 1,393 news articles split into 3 sets: train, validation and test.
109	13	We also use crowd labels collected by Rodrigues et al. (2014) for 400 articles in the train set3.
110	20	For Task 1 (aggregating crowd labels), to avoid overfitting, we split these 400 articles into 50% validation and 50% test4.
120	26	Tuning: In all experiments, validation set results are used to tune the models hyper-parameters.
132	30	We also tried using a CRF or LSTM as the sequence labeler but found the HMM performed best.
142	8	To predict sequences on unannotated text when trained on crowd labels, we consider two broad approaches: (1) directly train the model on all individual crowd annotations; and (2) induce consensus labels via Task 1 and train on them.
156	8	For the non-sequential aggregation baselines, we see that Dawid and Skene (1979) outperforms both majority voting and MACE (Hovy et al., 2013).
158	23	However, we will see that this heuristic does not work as well for biomedical IR.
166	12	Adding a crowd component to the LSTM yields marked improvement of 2.5-3 points F1 vs. the LSTM trained on individual crowd annotations or consensus MV annotations.
170	18	For Task 1, Majority Vote achieves nearly 92% Precision but suffers from very low Recall.
174	11	For Task 2, as with NER, we again see that LSTM-Crowd (trained on individual labels) and ‘HMM-Crowd then LSTM’ (LSTM trained on HMM consensus labels) offer different paths to achieving fairly comparable results.
175	15	While LSTM-Crowd-cat again achieves slightly lower F1, simply training Lample et al. (2016)’s LSTM directly on all crowd labels performs much better than seen earlier with NER, likely due to the relatively larger size of this dataset (see Table 1).
180	17	HMM-Crowd is able to return the longer part of the correct span.
183	30	Results showed that our methods show improvement over strong baselines.
184	16	We expect our methods to be applicable to and similarly benefit other sequence labeling tasks, such as POS tagging and chunking (Hovy et al., 2014).
185	13	Our methods also provide an estimate of each worker’s label quality, which can be transfered between tasks and is useful for error analysis and intelligent task routing (Bragg et al., 2014).
186	35	We also plan to investigate extension of the crowd component in our HMM method with hierarchical models, as well as a fully-Bayesian approach.
