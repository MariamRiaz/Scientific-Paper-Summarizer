1	21	Currently the most successful formalisation of protecting user privacy is provided by differential privacy (Dwork & Roth, 2014), which is a definition that any algorithm operating on a database may or may not satisfy.
7	12	Although kernel mean embeddings are functions in an abstract Hilbert space, in practice they can be (at least approximately) represented using a possibly weighted set of data points in input space (i.e. a set of database rows).
9	11	As a result, our framework can be seen as leading to synthetic database algorithms.
21	9	This notion is sometimes called approximate differential privacy; an algorithm that is (ε, 0)-differentially private is simply said to be ε-differentially private.
35	14	In practice, the KME of a random variable X is approximated using a sample x1, .
40	18	The inner product induces a norm ‖ · ‖Hk , which can be used to measure distances ‖µkX − µkY ‖Hk between distributions of X and Y .
43	8	18), which replace a set of points S = {x1, .
45	8	, (zM , wM )} ⊆ X × R (of potentially smaller size), where the new points zm can, but need not equal any of the xns, such that the KME computed using the reduced setR is close to the KME computed using the original set S, as measured by the RKHS norm: ∥∥µkS − µkR∥∥Hk = ∥∥∥∥∥ 1N N∑ n=1 k(xn, ·)− M∑ m=1 wmk(zm, ·) ∥∥∥∥∥ Hk .
46	15	Reduced set methods are usually motivated by the computational savings arising when |R| < |S|; we will invoke them mainly to replace a collection S of private data points with a (possibly weighted) set R of synthetic data points.
78	14	Consistent estimation of population statistics: For any RKHS function h ∈ H, we have 〈µX , h〉H = E[h(X)], so a consistent estimator of µX yields a consistent estimator of the expectation of h(X).
83	20	Subsequent use of synthetic data: Since the output of the algorithm is a (possibly weighted) database, thirdparties are free to use this data for arbitrary purposes, such as training any machine learning model on this data.
84	11	Models trained purely on this data can be released with differential privacy guaranteed; however, the accuracy of such models on real data remains an empirical question that is beyond the scope of this work.
87	14	For example, one could fix the weights to uniform wm = 1 M to obtain an unweighted dataset, or to replace an expensive data type with a cheaper subset, such as requesting floats instead of doubles in the zm’s.
89	13	As a first illustrative example, we describe how a particular case of an existing, but inefficient synthetic database algorithm already fits into our framework.
90	31	The exponential mechanism (McSherry & Talwar, 2007) is a general mechanism for ensuring ε-differential privacy, and in our setting it operates as follows: given a similarity measure s : XN × XM → R between (private) databases of size N and (synthetic) databases of size M , output a random (synthetic) database R with probability proportional to exp( ε2∆1 s(D,R)), where D is the actual private database and ∆1 is the L1 sensitivity of s w.r.t.
91	25	D. This ensures ε-differential privacy (McSherry & Talwar, 2007).
92	12	To fit this into our framework, we can take s(D,R) = −‖µD − µR‖H to be the negative RKHS distance between the KMEs computed using D and R, and achieve ε-differential privacy by releasing R with probability proportional to exp(− ε2∆1 ‖µD − µR‖H).
94	18	The algorithm essentially corresponds to the SmallDB algorithm of Blum et al. (2008), except for choosing the RKHS distance as a well-studied similarity measure between two databases.
101	16	In this section we describe an instantiation of the framework proposed in Section 3 that achieves differential privacy of the KME by projecting it onto a finite-dimensional subspace of the RKHS spanned by feature maps k(zm, ·) of synthetic data points z1, .
109	11	, zM independently of the private data (only using the database size N ).
116	6	This theorem assures us that Algorithm 1 produces a consistent estimator of the true KME µX , if the synthetic data points are sampled from a distribution with sufficiently large support.
137	21	Unsurprisingly, the convergence rate deteriorates with input dimension D, since without prior information about the private data manifold it is increasingly difficult for randomly sampled synthetic points to capture patterns in the private data.
140	11	Say that a fixed proportion η of the private database can be published unmodified.
144	12	, wM that reweight the public data points using the information in the large private dataset, but respecting differential privacy.
146	9	This is encouraging, since obtaining permission to publish a larger subset of the private data unchanged will usually come at an increased cost.
149	9	, xN} ⊆ X , kernel k on X , privacy parameters ε > 0 and δ > 0 Output: (ε, δ)-differentially private, weighted synthetic database (representing an estimate of µX in the RKHSH of k) 1: J ← J(N) ∈ ω(1) ∩ o(N2), number of random features to use 2: φ← random feature map X 7→ RJ for kernel k with J features 3: µ̂φX ← 1 N ∑N n=1 φ(xn) ∈ RJ , empirical KME of X in RKHSHφ of the random features kernel kφ(·, ·) := φ(·)Tφ(·) 4: µ̃φX ← µ̂ φ X +N (0, 8 ln(1.25/δ) N2ε2 IJ), an (ε, δ)-differentially private version of the vector µ̂ φ X (Gaussian mechanism) 5: M ←M(N) ≥ N , number of synthetic expansion points to use for representing µ̃φX 6: (z1, w1), .
156	9	Lines 3-4 compute the empirical KME of X in the RKHS Hφ corresponding to the kernel induced by the random features, and then privacy-protect the resulting finite, realvalued vector.
167	9	The additional ability of Algorithm 2 to optimise the locations of the synthetic data points (rather than just the weights, as in Algorithm 1) seems to be more helpful in the higher-dimensional case D = 5, where the randomly sampled synthetic data points are less likely to land close to private data points.
186	8	To justify our framework, we presented two concrete algorithms and proved theoretical results guaranteeing their consistency and differential privacy.
188	16	We believe that exploring other instantiations of this framework, and comparing them theoretically and empirically, can be a fruitful direction for future research.
