27	47	We hope that our studies and findings will benefit the community to better understand and leverage reinforcement learning for developing strong NMT models, especially in real-world scenarios faced with deep models and large amount of training data (including both parallel and monolingual data).
29	38	In this section, we first introduce the attentionbased sequence-to-sequence learning framework for neural machine translation (NMT), and then introduce the basis of applying reinforcement learning to training NMT models.
30	15	Typical NMT models are based on the encoderdecoder framework with attention mechanism.
32	38	Given z, the decoder then generates a target sentence y = (y1, y2, ..., ym) of word tokens one by one.
36	19	The main difference between Transformer and previous RNNSearch (Bahdanau et al., 2015) or ConvS2S (Gehring et al., 2017) is that Transformer relies entirely on self-attention (Lin et al., 2017) to compute representations of source and target side sentences, without using recurrent or convolutional operations.
38	19	Specifically, NMT model can be viewed as an agent, which interacts with the environment (the previous words y<t and the context vector z available at each step t).
41	30	A terminal reward is observed once the agent generates a complete sequence ŷ.
42	26	The reward for machine translation is the BLEU (Papineni et al., 2002) score, denoted as R(ŷ, y), which is defined by comparing the generated ŷ with the ground-truth sentence y.
43	15	Note that here the reward R(ŷ, y) is the sentence-level reward, i.e., a scalar for each complete sentence ŷ.
44	15	The goal of the RL training is to maximize the expected reward: Lrl = N∑ i=1 Eŷ∼p(ŷ|xi)R(ŷ, y i) = N∑ i=1 ∑ ŷ∈Y p(ŷ|xi)R(ŷ, yi), (2) where Y is the space of all candidate translation sentences, which is exponentially large due to the large vocabulary size, making it impossible to exactly maximize Lrl.
45	14	In practice, REINFORCE (Williams, 1992) is usually leveraged to approximate the above expectation via sampling ŷ from the policy p(y|x), leading to the objective as maximizing: L̂rl = N∑ i=1 R(ŷi, yi), ŷi ∼ p(y|xi),∀i ∈ [N ].
49	41	To our best knowledge, currently there is no consensus, or even a systematic study on how to configure different setups for RL training to avoid such problems, especially for training deep NMT models on large scale datasets.
55	16	The first one is beam search (Sutskever et al., 2014), it is a breadth-first search method that maintains a “beam” of the top-K scoring candidates (prefix hypothesis sentences) at each generation step.
61	13	Beam search strategy generates more accurate ŷ by exploiting the probabilistic space output via current NMT model, while multinomial sampling pays more attention to explore more diverse candidates.
72	15	To reduce the variance, Ranzato et al. (2016) subtracts an average reward from the returned reward at each time step t, and the actual reward used to update the policy is R(ŷ, y)− r̂t, (4) where r̂t is the estimated average reward at step t, named as baseline reward (Weaver and Tao, 2001).
73	16	Together with reward shaping, the updated reward becomes ∑m τ=t rτ (ŷτ , y)− r̂t at step t. Intuitively speaking, a baseline reward r̂t is established, which either encourages a word choice ŷt if the induced reward R satisfies R > r̂t, or discourages it if R < r̂t.
81	33	(3)) objectives as follows: Lcom = α ∗ Lmle + (1− α) ∗ L̂rl, (5) where α is the hyperparamter controlling the tradeoff between MLE and RL objectives.
83	19	Previous works typically conduct RL training with only bilingual data for NMT.
88	26	We first provide a solution to RL training with source-side monolingual data.
99	37	We then pair the target monolingual data and its backtranslated sentence as a pseudo bilingual sentence pair, which can be used for RL training in the same way as the genuine bilingual sentence pairs.
106	24	Our goal is to investigate the model performance with different training data and find the best recipe of how to use these data in RL training.
145	20	Therefore, for the economic perspective, it is not necessary to add the additional steps of using baseline reward on RL training for NMT.
172	101	Taking the last three rows as an example, the BLEU score of the MLE model trained on the combination of bilingual data and target-side monolingual data is 25.24; based on this model, RL training using the source-side monolingual data further improves the model performance by 0.7 (ρ < 0.01) BLEU points.
173	78	From Table 6, we can observe on top of a quite strong MLE baseline (26.13), through the unified RL training, we can still improve the test set by 0.6 points to 26.73 (ρ < 0.01), which shows the effectiveness of combining source/target monolingual data and reinforcement learning.
175	34	The results clearly show that after combing both source-side and target-side monolingual data with RL training, we obtain the state-of-the-art BLEU score 26.73, even surpassing the best ensemble model in WMT17 Zh-En translation challenge.
190	56	Different RL strategies were evaluated in German-English, English-Chinese and ChineseEnglish translation tasks on large-scale bilingual datasets.
191	27	We found that (1) multinomial sampling is better than beam search, (2) several previous tricks such as reward shaping and baseline reward does not make significant difference, and (3) the combination of the MLE and RL objectives is important.
192	25	In addition, we explored the source/target monolingual data for RL training.
193	38	By combing the power of RL and monolingual data, we achieve the state-of-the-art BLEU score on WMT17 ChineseEnglish translation task.
194	56	We hope that our study and results can benefit the community and bring some insights on how to train deep NMT models with reinforcement learning and big data.
