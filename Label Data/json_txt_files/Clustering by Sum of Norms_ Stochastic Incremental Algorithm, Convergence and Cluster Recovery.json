{"sections": [{"heading": "1. Introduction", "text": "Clustering is perhaps the most fundamental problem in unsupervised learning. Many clustering algorithms have been proposed in the literature (Jain et al., 1999), including Kmeans, spectral clustering, Gaussian mixture models and hierarchical clustering, to solve problems with respect to a wide range of cluster shapes. However, much research has pointed out that these methods all suffer from instabilities. For example, the formulation of K-means is NP-hard and the typical way to solve it is the Lloyds method, which\n1ECE, North Carolina State University, Raleigh, NC 2CSE, Chalmers University of Technology, Go\u0308teborg, Sweden 3IMES, MIT, Cambridge, MA 4CSA, IISc, Bangalore, India. Correspondence to: Ashkan Panahi <panahi1986@gmail.com>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nrequires randomly initializing the clusters. However, one needs to know the number of clusters in advance and different initializations may lead to significantly different final cluster results.\nLindsten et al. (2011) and Hocking et al. (2011) proposed the following convex optimization procedure for clustering, called SON (\u201cSum of norms\u201d clustering) by the former and Clusterpath by the latter;\nmin {ui\u2208Rm}\n1\n2 n\u2211 i=1 \u2016xi \u2212 ui\u201622 + \u03bb \u2211 i<j \u2016ui \u2212 uj\u20162 (1)\nThe main idea of the formulation is that if input data points xi and xj belong to the same cluster, then their corresponding centroids ui and uj should be forced to be the same. Intuitively, this is due to the fact that the second term is a regularization term that enforces zeroes in the vector consisting of entries \u2016ui \u2212 uj\u2016 and can be seen as a generalization of the fused Lasso penalty. From another point of view, the regularization term can be seen as an `1,2 norm, i.e., the sum of `2 norms. Such a group norm is known to encourage block sparse solutions (Bach et al., 2012). Thus for many pairs (i, j), we expect to enforce ui = uj .\nLindsten et al. (2011) used an off\u2013the\u2013shelf convex solver, CVX to generate solution paths. Hocking et al. (2011) introduced three distinct algorithms for the three most commonly encountered norms. For the `1 norm, the objective function separates, and they solve the convex clustering problem by the exact path following method designed for the fused lasso. For the `1 and `2 norms, they employ subgradient descent in conjunction with active sets. Recently, Chi & Lange (2015); Chen et al. (2015) introduce two similar generic frameworks for minimizing the convex clustering objective function with an arbitrary norm. One approach solves the problem by the alternating direction method of multipliers (ADMM), while the other solves it by the alternating minimization algorithm (AMA). However both algorithms have issues with scalablity.\nMoreover, none of these papers provide any theoretical guarantees about the cluster recovery property of the algorithm. The first theoretical result on cluster recovery was shown by Zhu et al. (2010): if the samples are drawn from\ntwo cubes, each being one cluster, then SON can provably recover the clusters provided that the distance between the two cubes is larger than a threshold which depends (linearly) on the size of the cube and the ratio of numbers of samples in each cluster. Unfortunately, the conditions for recovery represent an extremely narrow special case: only two clusters which both have to be cubes. Moreover in their paper, there is no algorithm or analysis of the speed of convergence. No other theoretical guarantees for SON are known previously.\nHere we develop a new algorithm in the spirit of recent advances in stochastic methods for large scale optimization (Bottou et al., 2016) to solve the optimization problem (1). We give a convergence analysis and provide quite general cluster recovery guarantees.\nThere has been a flurry of advances (Johnson & Zhang, 2013; Defazio et al., 2014; Schmidt et al., 2016) in developing algorithms for solving optimization problems for the case when the objective consists of the sum of two convex functions: one is the average of a large number of smooth component functions, and the other is a general convex function that admits a proximal mapping (and the whole objective function is strongly convex). The optimization (1) is of this form but here we exploit the structure of (1) further by observing that the second function can also be split into component functions. This results in an incremental algorithm with proximal iterations consisting of very simple and natural steps. Our algorithm can be seen as a special case of the methods of Bertsekas (2011). We compute the proximal operator in closed form to yield very simple and cheap iterations. Using the fact that the proximal operator is non-expansive, we refine and strengthen Bertsekas\u2019 convergence results. The stochastic incremental nature of our algorithm makes it highly suited to large scale problems (Bottou et al., 2016) in contrast to the methods in Chi & Lange (2015); Chen et al. (2015).\nWe show that the SON formualation (1) provides strong cluster recovery properties that go far beyond the special case considered in Zhu et al. (2010). Our cluster recovery conditions are similar in spirit to the unifying general conditions recently formulated in A. Kumar (2010); P.Awasthi (2012) of the form that the means of the clusters are well\u2013 separated, i.e., the distance between the means of any two clusters is at least \u2126(k) standard deviations (the notion of standard deviations is based on the spectral norm of the matrix whose rows represent the difference between a point and the mean of the cluster to which it belongs). Besides containing the result of Zhu et al. (2010) as a special case, the condition essentially recovers the well known cluster recovery conditions for paradigm examples such as mixtures of Gaussians and planted partition models. The algorithms in A. Kumar (2010); P.Awasthi (2012) are based on\nan SVD-based initialization followed by applying Lloyd\u2019s K\u2013means algorithm, so K must be known in advance. Our method does not need to knowK and is independent of any initialization.\nA summary of our contributions are:\n\u2022 We develop a new incremental proximal algorithm for the SON optimization problem (1).\n\u2022 We give a convergence analysis for our algorithm that refines and strengthens the analysis in Bertsekas (2011).\n\u2022 We show that the SON formulation (1) provides strong cluster recovery guarantees that is far more general than previously known recovery results, essentially similar to the recently discovered unifying center separation conditions.\n\u2022 We give experimental results giving evidence that our algorithm produces clusters of comparable quality to previous methods but scales much better to large scale problems."}, {"heading": "2. Related Work", "text": "The SON formulation first appeared in (Lindsten et al., 2011) and in closely related forms in Hocking et al. (Hocking et al., 2011). Lindsten et al (Lindsten et al., 2011) used an off\u2013the\u2013shelf convex solver, CVX to generate solution paths. Hocking et al. (Hocking et al., 2011) introduced three distinct algorithms for the three most commonly encountered norms. For the `1 norm, the objective function separates, and they solve the convex clustering problem by the exact path following method designed for the fused lasso. For the `1 and `2 norms, they employ subgradient descent in conjunction with active sets. Neither provides any theoretical results on cluster recovery. Chi et al (Chi & Lange, 2015; Chen et al., 2015) introduce two similar generic frameworks for minimizing the convex clustering objective function with an arbitrary norm. One approach solves the problem by the alternating direction method of multipliers (ADMM), while the other solves it by the alternating minimization algorithm (AMA). The first (and only) theoretical results on cluster recovery are in (Zhu et al., 2010) but this is a very simple special case of exactly two cube shaped clusters that are well separated. This work also does not develop a specialized algorithm for the SON formulation."}, {"heading": "3. Cluster Recovery", "text": "To express our results, we first review few definitions: Definition 1. Take a finite set X = {x1, x2, . . . , xn} \u2282 Rm and its partitioning V = {V1, V2, . . . , VK}, where each\nVk is a subset of X . We say that a map \u03c6 on X perfectly recovers V when \u03c6(xi) = \u03c6(xj) is equivalent to xi, xj belonging to the same cluster, or in other words, there exist distinct vectors v1, v2, . . . , vK such that \u03c6(xi) = v\u03b1 holds whenever xi \u2208 V\u03b1. Definition 2. For any set S \u2282 Rm, its diameter is defined as\nD(S) = sup{\u2016x\u2212 y\u20162 | x, y \u2208 S}.\nMoreover, for any finite set T \u2282 Rm we define its separation as\nd(T ) = min{\u2016x\u2212 y\u20162 | x, y \u2208 S, x 6= y}\nand its Euclidean centroid as\nc(T ) =\n\u2211 x\u2208T x\n|V | .\nFinally, for any family of mutually disjoint finite sets T = {Ti \u2282 Rm}, we define C(T ) = {c(Ti)}. Definition 3. Take a finite set X = {x1, x2, . . . , xn} \u2282 Rm and its partitioning V = {V1, V2, . . . , VK}. We call a partitioning W = {W1,W2, . . . ,WL} of X a coarsening of V if each partition Wl is obtained by taking the union of a number of partitions Vk. Further, W is called the trivial coarsening of V if W has exactly one element, i.e. W = {X}. Otherwise, it is called a non-trivial coarsening.\nBased on the above definitions, our result can be explained as follows:\nTheorem 1. Consider a finite set X = {xi \u2208 Rm | i = 1, 2, ..., n} of vectors and its partitioning V = {V1, V2, . . . , VK}. Take the SON optimization in (1). Denote its optimal solution by {u\u0304i} and define the map \u03c6 : xi \u2192 \u03c6(xi) = u\u0304i.\n1. If\nmax V \u2208V\nD(V ) |V | \u2264 \u03bb \u2264 d(C(V)) 2n \u221a K ,\nthen the map \u03c6 perfectly recovers V .\n2. If\nmax V \u2208V\nD(V )\n|V | \u2264 \u03bb \u2264 max V \u2208V \u2016c(X)\u2212 c(V )\u20162 |X| \u2212 |V | ,\nthen the map \u03c6 perfectly recovers a non-trivial coarsening of V .\nProof. We introduce associated centroid optimization:\nmin {v\u03b1\u2208Rm}\n1\n2 K\u2211 i=1 \u2016v\u03b1 \u2212 c\u03b1\u201622n\u03b1 + \u03bb \u2211 \u03b1 6=\u03b2 n\u03b1n\u03b2\u2016c\u03b1 \u2212 c\u03b1\u20162\n(2)\nwhere\nc\u03b1 =\n\u2211 i\u2208V\u03b1 xi\nn\u03b1\nWe prove the following results, which clearly imply Theorem 1:\n1. Suppose that for every \u03b1 \u2208 [K],\nmax i,j\u2208V\u03b1\n\u2016xi \u2212 xj\u2016 n\u03b1 \u2264 \u03bb.\nThen, ui = v\u03b1 for i \u2208 V\u03b1 is a global solution of the SON clustering.\n2. If all c\u03b1s are distinct and d2n\u221aK \u2265 \u03bb where d = min \u03b1 6=\u03b2 \u2016c\u03b1 \u2212 c\u03b2\u2016, then all centroids v\u03b1 are distinct.\n3. If max \u03b1 \u2016c\u03b1\u2212c\u2016 n\u2212n\u03b1 \u2265 \u03bb where c = n\u2211 i=1 xi/n, then at least\ntwo centroids v\u03b1 are distinct.\nTo prove the above, notice that the solution of the centroid optimization satisfies\nc\u03b1 \u2212 v\u03b1 = \u03bb \u2211 \u03b2 n\u03b2z\u03b1,\u03b2\nwhere \u2016z\u03b1,\u03b2\u2016 \u2264 1, z\u03b1,\u03b2 = \u2212z\u03b2,\u03b1 and whenever v\u03b1 6= v\u03b2 , the relation z\u03b1,\u03b2 =\nv\u03b1\u2212v\u03b2 \u2016v\u03b1\u2212v\u03b2\u20162 holds. Now, for the solution\nui = v\u03b1 for i \u2208 V\u03b1, define\nz\u2032ij = { z\u03b1,\u03b2 \u03b1 6= \u03b2 xi\u2212xj \u03bbn\u03b1 \u03b1 = \u03b2 ,\nwhere i \u2208 V\u03b1, j \u2208 V\u03b2 . It is easy to see that \u2016z\u2032ij\u20162 \u2264 1, z\u2032ij = \u2212z\u2032ji and whenever ui 6= uj , we have that z\u2032ij = ui\u2212uj \u2016ui\u2212uj\u20162 . Further for each i,\n\u03bb \u2211 j z\u2032i,j = \u03bb \u2211 \u03b2 z\u03b1,\u03b2n\u03b2 + \u2211 j\u2208V\u03b1 xi \u2212 xj n\u03b1\n= c\u03b1 \u2212 v\u03b1 + xi \u2212 c\u03b1 = xi \u2212 v\u03b1 = xi \u2212 ui This shows that the local optimality conditions for the SON optimization holds and proves item a.\nFor item b, denote the solution of the centroid optimization by v\u03b1(\u03bb) and notice that the solution of SON consists of distinct elements v\u03b1 = c\u03b1 and is continuous at \u03bb = 0. Hence, v\u03b1:s remain distinct in an interval \u03bb \u2208 [0, \u03bb1). Take \u03bb0 as the supremum of all possible \u03bb1:s. Hence, the solution in \u03bb \u2208 [0, \u03bb0) contains distinct element and at \u03bb = \u03bb0 contains two equal elements (otherwise, one can extend [0, \u03bb0) to some [0, \u03bb0+ ), which is against \u03bb being supremum). Now, notice that for \u03bb \u2208 [0 \u03bb0) the objective\nfunction is smooth at the optimal point. Hence, v\u03b1(\u03bb) is differentiable and satisfies\n\u03b4 = [ dv\u03b1 d\u03bb ] \u03b1 = H\u22121 \u2202g \u2202\u03bb (3)\nwhere [. ]\u03b1 and [. ]\u03b1,\u03b2 denote block vectors and block matrices respectively. Moreover, H and g are the Hessian and the gradient of the objective function at the optimal point. It is possible, by explicitly expanding H and g, to show that \u2016\u03b4\u20162 \u2264 n \u221a K (see the supplementary material for more detailed derivations). Hence, \u2225\u2225\u2225\u2225dv\u03b1d\u03bb \u2225\u2225\u2225\u2225 2 \u2264 \u2016\u03b4\u20162 \u2264 \u221a Kn\nThis yields for \u03bb < \u03bb0 to\n\u2016v\u03b1(\u03bb)\u2212 v\u03b2(\u03bb)\u20162 = \u2225\u2225\u2225\u2225\u2225\u2225c\u03b1 \u2212 c\u03b2 + \u03bb\u222b\n0\n( dv\u03b1 d\u03bb \u2212 dv\u03b2 d\u03bb ) d\u03bb \u2225\u2225\u2225\u2225\u2225\u2225 2\n\u2265 \u2016c\u03b1 \u2212 c\u03b2\u20162 \u2212 \u03bb\u222b\n0\n\u2225\u2225\u2225\u2225dv\u03b1d\u03bb \u2212 dv\u03b2d\u03bb \u2225\u2225\u2225\u2225 2 d\u03bb\n\u2265 d\u2212 2n\u03bb \u221a K\nSince at \u03bb = \u03bb0, we have that v\u03b1 = v\u03b2 for some \u03b1 6= \u03b2, we get that d \u2212 2n\u03bb0 \u221a K \u2264 0 or \u03bb0 \u2265 d/2n \u221a K. this proves item b.\nFor item c, Take a value of \u03bb, where v1 = v2 = . . . = vK . It is simple to see that in this case v\u03b1 = c. The optimality condition leads to\nc\u2212 c\u03b1 = \u03bb \u2211 \u03b2 6=\u03b1 z\u03b1,\u03b2n\u03b2\nHence, \u2016c\u2212 c\u03b1\u20162 \u2264 \u03bb(n\u2212 n\u03b1). This proves item c.\nRemark 1. The study in Zhu et al. (2010) establishes some results for the special case of two clusters in rectangular boxes. In this special case, we observe that our result improves theirs.\nProof. Consider the notation in Zhu et al. (2010) with two clusters V1, V2 and notice that \u03bb = \u03b1/2 (\u03b1 denotes regularization parameter in Zhu et al. (2010)). Moreover, D(Vi) \u2264 \u2016si\u2016 as \u2016si\u2016 is the diameter of the rectangle surrounding Vi. We observe that\nw1,2 n \u2265 D(Vi)\n2n3\u2212i ( ni\u22121 n2i ) + 1\nn3\u2212i + ni \u2265 D(Vi) ni\nfor i = 1, 2, which shows that the condition \u03bb \u2265 w1,2n in Zhu et al. (2010) is tighter than \u03bb \u2265 maxD(V )/|V | in ours. On the other hand,\n\u2016c(Vi)\u2212 c(X)\u2016 n\u2212 ni = \u2016c(Vi)\u2212 c(V1)n1+c(V2)n2n1+n2 \u2016 n\u2212 ni\n= \u2016c(V1)\u2212 c(V2)\u20162\nn (4)\n= d(C(V))\nn\nHence, the condition \u03bb \u2264 d(C(V))n in Zhu et al. (2010) is the same as our condition \u03bb \u2264 \u2016c(Vi)\u2212c(X)\u2016n\u2212ni .\nRemark 2. The second result in Theorem 1 reflects a hierarchical structure in the SON clusters: Under weaker condition than the first part, SON may merge some clusters and provide larger clusters than the true ones. In a recursive way, SON clustering can be applied to each of these large clusters to refine them, which improves the guarantees in Theorem 1. We postpone careful study of this method to future work."}, {"heading": "3.1. Comparison with Center Separation Conditions", "text": "Recently, there have been a number of theoretical results of the form that if we have data points generated by a mixture of K probability distributions, then one can cluster the data points into the K clusters, one corresponding to each component, provided the means of the different components are well\u2013separated. There are different notions of well-separated, but mainly, the (best known) results can be qualitatively stated as: \u201cIf the means of every pair of densities are at least poly(K) standard deviations apart, then we can learn the mixture in polynomial time.\u201d. These results generally make heavy use of the generative model and particular properties of the distributions (Indeed, many of them specialize to Gaussians or independent Bernoulli trials).\nKumar and Kannan (A. Kumar, 2010) and Awasthi and Sheffet (P.Awasthi, 2012) unified these into a general deterministic condition which can be roughly stated as follows: \u201cIf the means of every pair of clusters are at least \u2126(K) times standard deviations apart, then we can learn the mixture in polynomial time.\u201d Here the spectral norm of the matrix A \u2212 C scaled by 1\u221a\nn plays the role of standard\ndeviation, whereA is the data matrix and C is the matrix of cluster centers. More formally, for any two distinct clusters \u03b1, \u03b2,\n\u2016c(V\u03b1)\u2212 c(V\u03b2)\u20162 \u2265 K (\n1 \u221a n\u03b1 + 1 \u221a n\u03b2\n) \u2016A\u2212 C\u2016 (5)\nOur condition is similar in spirit:\n\u2016c(V\u03b1)\u2212 c(V\u03b2)\u20162 \u2265 \u221a K\n( n\nn\u03b1 d(V\u03b1)\n) (6)\nIf n\u03b1 \u2265 wn for all clusters \u03b1, then this becomes\n\u2016c(V\u03b1)\u2212 c(V\u03b2)\u20162 \u2265 \u221a K\nw d(V\u03b1). (7)\nIn the sequel, we specialize the above discussion in a number of examples and provide an explicit comparison of our result with the center separation condition. In some cases, our condition is slightly tighter than the center separation guarantees, but we remind that the latter is obtained by applying K-means and a SVD-based initialization, which can be intractable in large problems, while our techniques scales with the problem size more suitably."}, {"heading": "3.1.1. MIXTURES OF GAUSSIANS", "text": "Suppose we have a mixture of K Gaussians in d dimensions with mixture weights w1, \u00b7 \u00b7 \u00b7 , wK , let w := mini wi and let \u00b51, \u00b7 \u00b7 \u00b7 , \u00b5K denote their means respectively. If we have n = \u2126(poly(d/w)) points sampled from this mixture distribution, then with high probability, the center separation condition is satisfied if:\n\u2016\u00b5r \u2212 \u00b5s\u2016 \u2265 cK\u03c3\u221a w polylog(d/w).\nHere \u03c3 is the maximum variance in any direction of any of the Gaussians. Our cluster recovery condition (7) is satisfied if:\n\u2016\u00b5r \u2212 \u00b5s\u2016 \u2265 cK\u03c3\nw polylog(n)."}, {"heading": "3.1.2. PLANTED PARTITION MODEL", "text": "In the planted partition model of McSherry, a set of n points is implicitly partitioned into K groups. There is an (unknown) K \u00d7K matrix of probabilities P . We are given a graph G on these n points, where an edge between two vertices from groups r and s is present with probability Pr,s. We can consider these n points x1, \u00b7 \u00b7 \u00b7 , xn \u2208 Rn where coordinate j in xi is 1 if (i, j) \u2208 G and 0 otherwise. The center \u00b5r of cluster r has in coordinate j the value Pr,\u03c8(j), where \u03c8(j) is the cluster vertex j belongs to. Kumar and Kannan show that the center separation condition holds with probability at least 1\u2212 \u03b4 if:\n\u2016\u00b5r \u2212 \u00b5s\u2016 \u2265 c\u03c32K( 1\nw + log\nn \u03b4 )\nwhere c is a large constant, w is such that every group has size at least w \u00b7 n and \u03c32 := maxr,s Pr,s. Our center separation condition (7) is satisfied if:\n\u2016\u00b5r \u2212 \u00b5s\u2016 \u2265 c \u03c32K\nw\n\u221a n"}, {"heading": "3.1.3. REGULAR AND DIRECTED CLUSTERS", "text": "Besides the stochastic models, we take a closer look at the result in A. Kumar (2010) and identify deterministic cases where the SON has better performance than the proved bounds for K-means. These cases essentially guarantee that the term \u2016A\u2212C\u2016 in (5) remains large and the bound therein becomes highly restrictive:\nDefinition 4. We say that a partition V = {V1, V2, . . . , VK} of X = {x1, x2, . . . , xn} is (\u03b4, \u03b3)\u2212expanded if\n|{x \u2208 V | \u2016x\u2212 c(V )\u20162 \u2265 \u03b4}| \u2265 \u03b3|V |.\nWe further say that this partition is (w,D, , \u03b3)-regular if for all V \u2208 V we have D(V ) \u2265 D, |V | \u2265 wn and it is ( D, \u03b3)\u2212expanded. Definition 5. We say that a set X = {x1, x2, . . . , xn} is \u03b8\u2212directed if there exists a unit vector v \u2208 Rm such that\u2211\nx\u2208X\\{c(X)}\n|vT (x\u2212 c(X))|2\n\u2016x\u2212 c(X)\u201622 \u2265 \u03b8|X|\nFor a (w,D, , \u03b3)-regular partition, the bound in (5) implies d(C(V)) \u2265 2cK D \u221a \u03b3n\u221a\nmwn . This is because\n\u2016A\u2212 C\u20162 = \u03c3max ( n\u2211 i=1 \u03b4i\u03b4 T i ) \u2265\nTr (\nn\u2211 i=1 \u03b4i\u03b4 T i ) m\n=\nn\u2211 i=1 \u2016\u03b4i\u201622\nm = \u03b3 2D2\nn\nm\n(8)\nwhere \u03b4i = xi \u2212 c(V\u03b1) for i \u2208 V\u03b1. Notice that our conditions can be implied by d(C(V)) \u2265 2nD \u221a K\nwn . Hence,SON can improve K-means if m \u2264 wKc2\u03b3 2, which means that the number of clusters K is large and the smallest fraction of cluster size w is \u2126(1).\nIf the (w,D, , \u03b3)-regular partition is further \u03b8\u2212directed we may improve the previous bounds as\n\u03c3max ( n\u2211 i=1 \u03b4i\u03b4 T i ) \u2265 \u2211 x\u2208X |vT \u03b4i|2 \u2265 \u03b3 2D2n\u03b8\nHence (5) implies d(C(V)) \u2265 2cK D \u221a \u03b3\u03b8n\u221a\nwn . This means that\nSON improves K-means if wK \u2265 1c2 2\u03b3\u03b8 , i.e. the number of clusters is higher than a fixed value."}, {"heading": "4. Stochastic Splitting Algorithm", "text": "Our implementation is identical to the so-called proximalbased incremental technique in Bertsekas (2011), which is\nperformed in a way that it requires little amount of calculations (precisely O(m) and independent of other parameters) in each iteration. The proximal-based incremental method is a variant of the stochastic gradient technique, in problems where many terms in the objective function are not differentiable, and the local gradient steps are replaced by local proximal operators. To perform the proximalbased incremental method, we first write the SON objective function as\n\u03a6(u1, u2, . . . , un) = \u2211 i<j \u03c6ij(ui, uj)\nwhere\n\u03c6ij(ui, uj) = 1\n2n \u2016xi\u2212ui\u201622+\n1\n2n \u2016xj\u2212uj\u201622+\u03bb\u2016ui\u2212uj\u2016.\nThen, we introduce and explicitly calculate the proximal operator \u03a0(\u00b5)ij of \u03c6ij with step size \u00b5 as\n\u03a0 (\u00b5) ij (ui, uj) =\narg min u\u2032i,u \u2032 j\n\u03c6ij(u \u2032 i, u \u2032 j) + 1 2\u00b5\u2016u \u2032 i \u2212 ui\u201622 + 12\u00b5\u2016u \u2032 j \u2212 uj\u201622\n= T\u03bb\u00b5(ui + \u00b5xi, uj + \u00b5xj), (9)\nwhere we also introduce the pairwise soft-thresholding operator T\u03b7(y, z) ={ (\ny + \u03b7 z\u2212y\u2016z\u2212y\u20162 , z + \u03b7 y\u2212z \u2016y\u2212z\u20162 ) \u2016y \u2212 z\u2016 \u2265 2\u03b7(\ny+z 2 , y+z 2\n) \u2016y \u2212 z\u2016 < 2\u03b7 ,\n(10) and the final equality is obtained by the local optimality conditions and straightforward calculations. Our algorithm simply consists in iteratively applying randomly selected proximal operators. This is depicted in Algorithm 1.\nAlgorithm 1 Stochastic Splitting Algorithm Input: The data vectors {xk}nk=1 and step sizes {\u00b5k}\u221ek=1 Initialization: Set u1, u2, . . . , un arbitrarily (we use u1 = u2 = . . . = un = 0) for k = 1, 2, . . . do\nSelect a pair (i, j) with i < j uniformly randomly. Update (ui, uj)\u2190 \u03a0(\u00b5k)ij (ui, uj)\nend for"}, {"heading": "4.1. Convergence Analysis", "text": "Convergence of proximal-based incremental method is discussed in Bertsekas (2011). We further elaborate on the convergence by further exploitation of the nonexpansiveness property of proximal operators. This allows us to complement the result in Bertsekas (2011) in the following two directions: First, we establish convergence in\nthe probability sense (uniform convergence), while the result in Bertsekas (2011) is pointwise. Second, we prove guaranteed speed of convergence with probability one. We present these results by the following theorem. In this theorem, we consider fixed data dimensionm and bounded data vectors (i.e. \u2016xk\u2016 \u2264 C for some absolute constant C). Theorem 2.\n1. Assume that {\u00b5k} is non-increasing \u221e\u2211 0 \u00b5k = \u221e and\n\u221e\u2211 0 \u00b52k < \u221e. Then, the sequence Uk converges to U\u0303 in the following strong probability sense:\n\u2200 > 0; lim k\u2192\u221e Pr ( sup l\u2265k \u2016Ul \u2212 U\u0303\u20162F > ) = 0 (11)\n2. Take \u00b5k = \u00b51k\u03b1 for k = 1, 2, . . . and 2 3 < \u03b1 < 1. For\nsufficiently small values of > 0 the relation \u2016Ul \u2212 U\u0303\u20162F = O ( n4\nl3\u03b1\u22122\u2212 ) holds for every l, n with probability 1.\nProof. We skip many steps in our proof for lack of space. These steps can be found in the supplement. Denote by Uk a matrix where the ith column is the value of ui at the kth iteration. Define\n\u03c8\u00b5(U) = E (Uk+1 | Uk = U, \u00b5k = \u00b5) , (12)\nStarting from U\u03040 = U0 (the initialization of the algorithm), we define the characteristic sequence {U\u0304k}\u221ek=0 by the following iteration:\nU\u0304k+1 = \u03c8\u00b5k(U\u0304k)\nOur proof is based on the following two results, which we prove in the supplementary material:\ni We have that\nPr ( sup k \u2016Uk \u2212 U\u0304k\u20162F + \u221e\u2211 l=k \u00b52l > \u03bb ) \u2264 \u221e\u2211 k=0 \u00b52k \u03bb\n(13)\nii Define U\u0303 as the unique optimal solution of the SON optimization and suppose that {\u00b5k} is a non-increasing sequence. There exists a universal constant a such that \u2016U\u0304k \u2212 U\u0303\u20162F is upper bounded by\na k\u22121\u2211 l=0 \u00b52l e \u2212 2 n2 k\u22121\u2211 s=l+1 \u00b5s + \u2016U0 \u2212 U\u0303\u20162Fe \u2212 2 n2 k\u22121\u2211 s=0 \u00b5s\nTo prove Theorem 2, define Uk = {U\u0304kl }\u221el=0 as the sequence obtained by starting from U\u0304k0 = Uk and applying\nU\u0304kl+1 = \u03c8\u00b5l+k(U\u0304 k l )\nTake arbitrary (non-zero) positive numbers , \u03b4. Take \u03bb such that \u03bb \u2265 2\u03b4 \u221e\u2211 l=0 \u00b52l . Take some values l0, k which we specialize later. Now, we define two outcomes H1 and H2:\nH1 : \u2200k \u2265 0; \u2016Uk \u2212 U\u0303\u20162F \u2264 \u03bb\nH2 : \u2200l \u2265 0; \u2016U\u0304kl \u2212Ul+k\u2016 \u2264\n4\nFrom item (i), it is simple to see that Pr(Hc1) and Pr(H c 2) are less than \u03b4/2. Furthermore we can show by (ii) that under H1 \u2229H2 and suitable l0, k:\n\u2200l > l0; \u2016Ul+k\u2212U\u0303\u201622 \u2264 2(\u2016Ul+k\u2212U\u0304kl \u20162F+\u2016U\u0304kl \u2212U\u0303\u20162F)\n\u2264 2( 4 + 4 ) =\nThis is detailed in the supplement. We conclude that\nPr( sup l>l0+k\n\u2016Ul \u2212 U\u0303\u201622 > ) \u2264 Pr(Hc1) + Pr(Hc2) \u2264 \u03b4\nwhich proves part (1) of Theorem.\nFor part (2), define kr = r\u03b3 , \u03bbr = r\u2212\u03b2 , where \u03b3 = 1\u2212 2 1\u2212\u03b1 , \u03b2 < \u03b3(2\u03b1\u2212 1)\u2212 1, and the outcomes:\nQr : sup l\u22650 \u2016Ul+kr \u2212 U\u0304 kr l \u2016 2 F > \u03bbr.\nBy item (i), we have that \u221e\u2211 r=1 Pr(Qr) < \u221e. Hence by Borel-Cantelli lemma, Qcr0 , Q c r0+1, Q c r0+2, . . . simultaneously hold for some r0 with probability 1. For simplicity and without loss of generality, we assume that r0 = 0 as it does not affect the asymptotic rate. Then for any r > 0, we have that\nsup l\u22650 \u2016Ul+kr \u2212 U\u0304 kr l \u2016 2 F \u2264 \u03bbr\nIn particular,\n\u2016Ukr+1 \u2212 U\u0304 kr lr \u20162F \u2264 \u03bbr\nwhere lr = kr+1 \u2212 kr. From item (ii), we also conclude that\n\u2016U\u0304krlr \u2212 U\u0303\u2016 2 F \u2264 A lr\u22121\u2211 t=0\n1 (t+ kr)2\u03b1 e \u22122a\nlr\u22121\u2211 s=t+1 1 (s+kr)\u03b1\n+\u2016Ukr \u2212 U\u0303\u20162Fe \u22122a\nlr\u22121\u2211 s=0 1 (s+kr)\u03b1\nwhere we introduce \u00b51 = bn2 and A = 4an4b2 for simplicity. This leads to\n\u2016Ukr+1 \u2212 U\u0303\u20162F \u2264 LeLk 1\u2212\u03b1 r \u2212Lk 1\u2212\u03b1 r+1 \u2016Ukr \u2212 U\u0303\u20162F+\n2\u03bbr +A lr\u2211 t=0\n1\n(t+ kr)2\u03b1 eL(kr+t)\n1\u2212\u03b1\u2212Lk1\u2212\u03b1r+1\nwhere L is a suitable constant with different values at different occurrences. Postponing few more steps to the supplementary material, we obtain that\n\u2016Ukr \u2212 U\u0303\u20162F \u2264 L log r r\u03b2\u2212 2 \u2264 L r\u03b2\u2212\nTake kr < l \u2264 kr+1. We obtain that\n\u2016Ul \u2212 U\u0303\u201622 \u2264 2(\u2016Ukr \u2212 U\u0303\u201622 + \u2016Ukr \u2212Ul\u201622)\n\u2264 2\u03bbr + L r\u03b2\u2212 \u2264 L r\u03b2\u2212 \u2264 L l \u03b2\u2212 \u03b3\nBy taking \u03b2 = \u03b3(2\u03b1\u2212 1)\u2212 1, we obtain part (2)."}, {"heading": "5. Experiments", "text": "We evaluate the proposed stochastic splitting algorithm in the task of clustering points generated by Gaussians mixture models. We compare the results to the exact algorithm proposed by Lindsten et al. (2011) in terms of a) the quality of the produced clustering and b) the time spent solving the optimization problem. The results of both algorithms are dense embeddings of the points that are then thresholded to form clusters. The clusters are the largest subsets of nodes such that the maximum pairwise distance within the subset is less than \u03c4 . The stochastic splitting algorithm is implemented as in Algorithm 1. We observed in practice\nthat a heuristic for adaptively setting the step-size improved robustness and rate of convergence. Specifically, the step size was reduced by a constant factor whenever the average change in the objective over successive rounds in a small window was positive. If the same average was negative, but small in absolute value, the step size was increased by a small constant factor.\nThe data is generated from Gaussian mixture models with two components in R2 where the means are separated by d = \u221a\n2 and the variance \u03c32 is varied. The number of samples is also varied, to illustrate the computational gains of the stochastic splitting method. As pointed out by Lindsten et al. (2011), the choice of the regularization parameter \u03bb is perhaps the most challenging hurdle in applying SON clustering. Choosing \u03bb too high might result in a single large cluster, and choosing it too low may cause each point to be represented by its own cluster. While this problem is of great importance in applications, we focus on the relative performance of the Lindsten et al. (2011) algorithm (CVX) and stochastic splitting (SS). We report the adjusted Rand index (Rand, 1971) as measure of cluster quality, and would like to emphasize that this does not rely on identifying the number of clusters beforehand.\nResults The results of the experiments are presented in Figures 1 and 2. We see in Figure 1 that the quality of the clustering produced by the stochastic splitting algorithm is comparable to that of the exact algorithm. This pattern is consistant across choices of \u03c3, where a high \u03c3 implies low sample separation between the clusters. We also note that the range of \u03bb for which the stochastic splitting algorithm achieves as good result as the exact algorithm is less wide than for the exact. We believe this is due to the stochastic nature of the algorithm which makes the resulting embedding clusters less separated than in the exact version. Deviations from the optimal embedding could be magnified by the thresholding step, effectively making the stochastic algorithm more sensitive to the choice of threshold, and in effect the quality more sensitive to the choice of \u03bb. In these\nexperiments, the same threshold was used for both algorithms, but tailored choices could be considered given an appropriate selection criterion.\nFurthermore, we see in Figure 2 that the running time of the stochastic splitting algorithm is lower than that of the exact algorithm, and grows significantly slower. While the stochastic splitting algorithm could in principle be implemented in time constant in the number of samples, and instead determined by the number of iterations, the adaptive stepsize used to improve performance requires evaluation of the objective value which scales with the number of samples. This could be improved by subsampling the terms in the objective function, but this was not done here."}, {"heading": "6. Conclusions", "text": "We developed a stochastic incremental algorithm based on proximal iterations for the SON convex relaxtion of clustering that is highly suited to large scale problems and gave an analysis of its convergence propertis. We also gave quite general theoretical guaranteees for exact recovery of clusters similar to the unifying proximity condition in approximation algorithms that covers paradigm models for clustering data.\nIt has not escaped our attention that our algorithm can easily be adapted to incorporate similarity weights as used in Chi & Lange (2015); Chen et al. (2015); Hocking et al. (2011) and that it is amenable to acceleration using variance reduction and other techniques. The cluster recovery conditions can also be extended to cover almost perfect recovery i.e. correctly clustering all except a small fraction of points. A more complete experimental evaluation of our algorithm and comparison to others will be included in a longer version of the paper."}, {"heading": "Acknowledgements", "text": "This work is supported in part by the Swedish Foundation for Strategic Research (SSF)."}], "year": 2017, "references": [{"title": "Clustering with spectral norm and the kmeans algorithm", "authors": ["A. Kumar", "R. Kannan"], "venue": "In FOCS,", "year": 2010}, {"title": "Optimization with sparsity-inducing penalties", "authors": ["F. Bach", "R. Jenatton", "J. Mairal", "G. Obozinski"], "venue": "Foundation and Trends in Machine Learning,", "year": 2012}, {"title": "Incremental proximal methods for large scale convex optimization", "authors": ["D. Bertsekas"], "venue": "Math. Program.,", "year": 2011}, {"title": "Optimization methods for large-scale machine learning", "authors": ["Bottou", "L\u00e9on", "Curtis", "Frank E", "Nocedal", "Jorge"], "venue": "Technical report,", "year": 2016}, {"title": "Convex clustering: An attractive alternative to hierarchical clustering", "authors": ["Chen", "Gary K", "Chi", "Eric C", "Ranola", "John M.O", "Lange", "Kenneth"], "venue": "PLoS Computational Biology,", "year": 2015}, {"title": "Splitting methods for convex clustering", "authors": ["Chi", "Eric C", "Lange", "Kenneth"], "venue": "Journal of Computational and Graphical Statistics,", "year": 2015}, {"title": "A fast incremental gradient method with support for non-strongly convex composite objectives", "authors": ["A. Defazio", "F.F. Bach", "S. Lacoste-Julien"], "venue": "In NIPS,", "year": 2014}, {"title": "Clusterpath: an algorithm for clustering using convex fusion penalties", "authors": ["T. Hocking", "Vert", "J-P", "F. Bach", "A. Joulin"], "venue": "In ICML,", "year": 2011}, {"title": "Data clustering: A review", "authors": ["A.K. Jain", "M.N. Murty", "P.J. Flynn"], "venue": "ACM Comput. Surv.,", "year": 1999}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "authors": ["R. Johnson", "T. Zhang"], "venue": "In NIPS,", "year": 2013}, {"title": "Clustering using sum-ofnorms regularization: With application to particle filter output computation", "authors": ["F. Lindsten", "H. Ohlsson", "L. Ljung"], "year": 2011}, {"title": "Improved spectral norm bounds for clustering", "authors": ["P.Awasthi", "O Sheffet"], "venue": "In APPROX-RANDOM,", "year": 2012}, {"title": "Objective criteria for the evaluation of clustering methods", "authors": ["Rand", "William M"], "venue": "Journal of the American Statistical association,", "year": 1971}, {"title": "Minimizing finite sums with the stochastic average gradient", "authors": ["M. Schmidt", "Roux", "N. Le", "F. Bach"], "venue": "Mathematical Programming,", "year": 2016}, {"title": "Convex optimization procedure for clustering: Theoretical revisit", "authors": ["C. Zhu", "H. Xu", "C. Leng", "S. Yan"], "venue": "In NIPS,", "year": 2010}], "id": "SP:c72b2e3ef1e5c7c8ea4ff45c61d265b62f03c3de", "authors": [{"name": "Ashkan Panahi", "affiliations": []}, {"name": "Devdatt Dubhashi", "affiliations": []}, {"name": "Fredrik D. Johansson", "affiliations": []}, {"name": "Chiranjib Bhattacharyya", "affiliations": []}], "abstractText": "Standard clustering methods such as K\u2013means, Gaussian mixture models, and hierarchical clustering, are beset by local minima, which are sometimes drastically suboptimal. Moreover the number of clustersK must be known in advance. The recently introduced sum\u2013of\u2013norms (SON) or Clusterpath convex relaxation of k-means and hierarchical clustering shrinks cluster centroids toward one another and ensure a unique global minimizer. We give a scalable stochastic incremental algorithm based on proximal iterations to solve the SON problem with convergence guarantees. We also show that the algorithm recovers clusters under quite general conditions which have a similar form to the unifying proximity condition introduced in the approximation algorithms community (that covers paradigm cases such as Gaussian mixtures and planted partition models). We give experimental results to confirm that our algorithm scales much better than previous methods while producing clusters of comparable quality.", "title": "Clustering by Sum of Norms: Stochastic Incremental Algorithm, Convergence and Cluster Recovery"}