18	12	This compact representation also improves generalization.
22	12	A neural network transforms an input vector o0 to an output vector o` through a series of ` hidden layers {oi}`i=1.
23	10	We assume each layer has identical dimension for the simplicity of notation i.e. ∀oi ∈ Rd×1.
25	11	As shown in Fig.1 (a), each fully-connected (fc) layer consists of a weight matrix, W i, and a set of hidden neurons, hi, each of which receives as input a weighted sum of outputs from the previous layer.
31	12	A popular choice is the rectified linear unit, relu(x) = max(0, x).
33	12	This section revisits whitened neural networks (WNN).
38	20	The input is decorrelated by P i−1 in the sense that its covariance matrix becomes an identity matrix, i.e. E[õi−1õi−1T ] = I .
43	20	Let L(o`, y; θ) denote a loss function of WNN, which measures the disagreement between a prediction o` made by the network, and a target y. WNN is trained by minimizing the loss function with respect to the parameter vector θ and two constraints min θ L(o`, y; θ) (2) s.t.
48	19	The second constraint, hi − E[hi] = ĥi, enforces that the centered hidden features are the same, before and after adapting a fc layer to WNN, as shown in Fig.1 (a) and (b).
68	12	As the distribution of the hidden representation changes after every update of the whitened weight matrix, to maintain good conditioning of FIM, the whitening matrix, P i−1, needs to be reconstructed frequently by performing eigen-decomposition on Σi−1, which is estimated using N samples.
72	11	How good is the conditioning of the FIM by using Al- Algorithm 1 Training WNN 1: Init: initial network parameters θ, αi, βi; whitening matrix P i−1 = I; iteration t = 0; Ŵ it = W i; ∀i ∈ {1...`}.
83	14	For example, although matrixes in (a) and (b) have different sizes, they have similar value of orthogonality when they are sampled from the same distribution.
88	21	However, ‘conv1’ uses image data as inputs, whose distribution is typically stable during training.
89	47	Its whitening matrix can be estimated once at the beginning and fixed in the entire training stage.
90	61	In the section below, we present generalized whitened neural networks to improve conditioning of FIM while reducing computation time.
91	17	We present two types of generalized WNN (GWNN), including pre-whitening and post-whitening GWNNs.
93	11	This section introduces pre-whitening GWNN, abbreviated as pre-GWNN, which performs whitening transformation before applying the weight matrix (i.e. whiten the input), as illustrated in Fig.1 (c).
94	11	When adapting a fc layer to pre-GWNN, the whitening matrix is truncated by removing those eigenvectors that have small eigenvalues, in order to learn compact representation.
144	11	Let a random variable x ∼ N (0, 1) and y = max(0, ax + b).
147	49	It generalizes (Arpit et al., 2016) that presented a special case when a = 1 and b = 0.
152	13	It does not have an analytical solution because there is a nonlinear activation function between the weight matrix and the whitening matrix (i.e. in the previous layer).
169	16	d) SVHN (Netzer et al., 2011) consists of color images of house numbers collected by Google Street View.
173	15	We didn’t train on validation, which is for tuning hyperparameters.
175	11	First, following (Desjardins et al., 2015), we compare the above three approaches on the task of minimizing reconstruction error of an autoencoder on MNIST.
180	15	We record the number of epochs and computation time, when training WNN, pre-, and post-GWNN on MNIST and CIFAR-100, respectively.
184	29	In particular, for WNN and pre-GWNN, the number of samples used to estimate the covariance matrix, N , is picked up from {103, 10 4 2 , 10 4}.
186	22	For a fair comparison, we report the best performance on validation set for each approach, and didn’t employ any data augmentation such as random image cropping and flipping.
220	35	Different from WNN that reduces computation time by whitening with a large period, leading to ill conditioning of FIM, GWNN learns compact internal representation, such that SVD is approximated by the top eigenvectors in an online manner, making GWNN not only reduces computations but also improves generalization.
221	51	By exploiting the knowledge of the hidden representation’s distribution, we showed that post-GWNN is able to compute the covariance matrix in a closed form, which can be also extended to the other activation function.
222	29	Extensive experiments demonstrated the effectiveness of GWNN.
