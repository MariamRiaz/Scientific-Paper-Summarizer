14	54	Much more complex inferences are routinely performed by speakers, down to estimating the cardinality of the entities involved in a particular situation.
15	159	Compare e.g. The cats are on the sofa (2 / a few cats?
18	22	22 Understanding how this process works would not only give us an insight into a complex cognitive process, but also make a crucial contribution to NLP tasks relying on inference (e.g. the Recognising Textual Entailment challenge, RTE: Dagan et al. (2009)).
19	54	Indeed, while systems have successfully been developed to model entailment between quantifiers, ranging from natural logic approaches (MacCartney and Manning, 2008) to distributional semantics solutions (Baroni et al., 2012), they rely on an explicit representation of quantification.
21	52	In this work, we assume the existence of a mapping between language (distributional models) and world (set-theoretic models), or to be more precise, between language and a shared set of beliefs about the world, as negotiated by a group of speakers.
22	149	To operationalise this mapping, we propose that set-theoretic models, like distributions, can be expressed in terms of vectors – giving us a common representation across formalisms.
61	29	The McRae norms (McRae et al., 2005) are a set of feature norms elicited from 725 human participants for 541 concepts covering living and non-living entities (e.g. alligator, chair, accordion).
68	46	A subset of the annotation layer is available for training computational models, corresponding to all instances with a majority label (i.e. those where two or three coders agreed on a label).
76	32	AD3 is a set of 72 animal concepts with quantification annotations along 54 features.
122	22	These values correspond to the weights giving the best inter-annotator agreement in Herbelot and Vecchi (2015), when calculating weighted Cohen’s kappa on QMR.
123	23	In each model-theoretic space, a concept is represented as a vector in which the dimensions are features (has buttons, is green), and the values of the vectors along each dimension are quantifiers (in numerical format).
127	22	Finally, we merge the two into a single space of 555 unique concepts (MTQMR+AD).
128	24	To map from one semantic representation to another, we learn a function f : DS → MT that transforms a distributional semantic vector for a concept to its model-theoretic equivalent.
133	29	We also include the number of quantified instances in the test set (i.e. the number of actual concept-feature pairs that were explicitly annotated in QMR/AD and that we can thus evaluate – this is a portion of each concept vector in the spaces including QMR data).
135	26	The results in Table 3 show the degree to which predicted values for each dimension in a model-theoretic space correlate with the gold annotations, operationalised as the Spearman ρ (rank-order correlation).
136	32	Wherever appropriate, we also report the mean Spearman correlation between the three human annotators for the particular test set under consideration, showing how much they agreed on their judgements.9 These figures provide an upper bound performance for the system, i.e. we will consider having reached human performance if the correlation between system and gold standard is in the same range as the agreement between humans.
139	25	The top section of the table reports results for the QMR and AD dataset taken separately, as well as their concatenation.
146	55	Finally, we quantify the specific improvement to the QMR animal concepts by comparing the correlation obtained on MTQMRanimals (a test set consisting only of QMR animal features) after training on a) the QMR data alone and b) the merged dataset (third section of Table 3).
160	22	In order to judge the performance of the system on the missing gold dimensions, we need a manual analysis to assess the quality of the whole vectors, which goes hand-in-hand with obtaining additional annotations for the missing dimensions.
185	21	In a last experiment, we attempt to map the settheoretic vectors obtained in §5 back to natural language quantifiers.
190	22	We set the tthresholds by a systematic search on a training set (see below).
191	59	To evaluate this step, we propose a function that calculates precision while taking into account the two following factors: a) some errors are worse than others: the system shouldn’t be overly penalised for classifying a property as MOST rather than ALL, but much more for classifying a gold standard ALL as SOME; b) errors that are conducive to false inferences should be strongly penalised, e.g. generating all dogs are black is more serious than some dogs are mammals, because the former might lead to incorrect inferences with respect to individual dogs while the latter is true, even though it is pragmatically odd.
209	53	In future work, we will however explore the effect of more powerful functions to learn the transformations from distributional to model-theoretic spaces.
214	36	Right now, many parsers give the same broad analysis to Mosquitoes are insects and Mosquitoes carry malaria, involving an underspecified/generic quantifier.
215	39	This prevents inferring, for instance, that Mandie the mosquito is definitely an insect but may or may not carry malaria.
216	26	In contrast, our system would attribute the most plausible quantifiers to those sentences (all/few), allowing us to produce correct inferences.
220	29	For instance, we should obtain a different quantifier for taxis are yellow depending on whether the sentence starts with In London... or In New York...
223	44	They do not represent the world per se, but rather some shared beliefs about the world, induced from an annotated dataset of feature norms.
225	164	While this would be a fundamental departure from the core philosophy of model theory, we feel that it may be a worthwhile endeavour, allowing us to preserve the immense benefits of the set-theoretic apparatus in a cognitively plausible fashion.
226	66	Following this aim, we hope to expand the preliminary framework presented here into a more expressive vector-based interpretation of set theory, catering for aspects not covered in this paper (e.g. cardinality, non-intersective modification) and refining our notion of a model, together with its relation to meaning.
