0	60	We present a model for describing scenes and objects by reasoning about context and listener behavior.
1	26	By incorporating standard neural modules for image retrieval and language modeling into a probabilistic framework for pragmatics, our model generates rich, contextually appropriate descriptions of structured world representations.
2	110	This paper focuses on a reference game RG played between a listener L and a speaker S. 1.
7	181	In order for the players to win, S’s description d must be pragmatic: it must be informative, fluent, concise, and must ultimately encode an understanding of L’s behavior.
10	43	Existing computational models of pragmatics can be divided into two broad lines of work, which we term the direct and derived approaches.
11	27	Direct models (see Section 2 for examples) are based on a representation of S. They learn pragmatic behavior by example.
19	31	Like direct approaches, we use machine learning to acquire a complete grounded generation model from data, without domain knowledge in the form of a hand-written grammar or hand-engineered listener model.
20	21	But like derived approaches, we use this learning to construct a base model, and embed it within a higher-order model that reasons about listener responses.
38	23	Specifically, given a target referent (e.g. scene or object) r and a distractor r′, the model must produce a description d that uniquely identifies r. For training, we have access to a set of non-contrastively captioned referents {(ri, di)}: each training description di is generated for its associated referent ri in isolation.
39	24	There is no guarantee that di would actually serve as a good referring expression for ri in any particular context.
44	63	Section 3.3 describes how to assemble two base models: a literal speaker, which maps from referents to strings, and a literal listener, which maps from strings to reference judgments.
50	42	Referent representations are similarly simple.
51	41	Because the model never generates referents—only conditions on them and scores them—a vectorvalued feature representation of referents suffices.
62	30	Referent describer The referent describer takes an image encoding and outputs a description using a (feedforward) conditional neural language model.
75	128	Literal listener Given a description d and a pair of candidate referents r1 and r2, the literal listener embeds both referents and passes them to the ranking module, producing a distribution over choices i. ed = Ed(d) e1 = Er(r1) e2 = Er(r2) pL0(i|d, r1, r2) = R(ei|e−i, ed) (4) That is, pL0(1|d, r1, r2) = R(e1|e2, ed) and viceversa.
77	42	For each training example (ri, di), this objective attempts to maximize the probability that the model chooses ri as the referent of di over a random distractor.
82	26	The neural encoding/decoding framework implemented by the modules in the previous subsection provides a simple way to map from referents to descriptions and descriptions to judgments without worrying too much about the details of syntax or semantics.
87	261	Given a target i and a pair of candidate referents r1 and r2, it is natural to specify the behavior of a reasoning speaker as simply: max d pL0(i|d, r1, r2) (7) At a first glance, the only thing necessary to implement this model is the representation of the literal listener itself.
89	134	For our purposes, however, we would like the model to be capable of producing arbitrary utterances.
92	51	The key ingredient here is a good proposal distribution from which to sample sentences likely to be assigned high weight by the model listener.
95	32	We can use it as a source of candidate descriptions, to be reweighted according to the expected behavior of L0.
97	40	Score samples: pk = pL0(i|dk, r1, r2).
99	45	Past work has re- stricted the set of utterances in a way that guarantees fluency.
100	63	But with an imperfect learned listener model, and a procedure that optimizes this listener’s judgments directly, the speaker model might accidentally discover the kinds of pathological optima that neural classification models are known to exhibit (Goodfellow et al., 2014)—in this case, sentences that cause exactly the right response from L0, but no longer bear any resemblance to human language use.
101	86	To correct this, we allow the model to consider two questions: as before, “how likely is it that a listener would interpret this sentence correctly?”, but additionally “how likely is it that a speaker would produce it?” Formally, we introduce a parameter λ that trades off between L0 and S0, and take the reasoning model score in step 2 above to be: pk = pS0(dk|ri)λ · pL0(i|dk, r1, r2)1−λ (8) This can be viewed as a weighted joint probability that a sentence is both uttered by the literal speaker and correctly interpreted by the literal listener, or alternatively in terms of Grice’s conversational maxims (Grice, 1970): L0 encodes the maxims of quality and relation, ensuring that the description contains enough information for L to make the right choice, while S0 encodes the maxim of manner, ensuring that the description conforms with patterns of human language use.
102	45	Responsibility for the maxim of quantity is shared: L0 ensures that the model doesn’t say too little, and S0 ensures that the model doesn’t say too much.
119	65	Accuracy is success rate at RG: as in Figure 1, humans are shown two images and a model-generated description, and asked to select the image matching the description.
120	95	In the remainder of this section, we measure the tradeoff between fluency and accuracy that results from different mixtures of the base models (Section 4.1), measure the number of samples needed to obtain good performance from the reasoning listener (Section 4.2), and attempt to approximate the reasoning listener with a monolithic “compiled” listener (Section 4.3).
128	24	However, by adding only a very small weight to the speaker model, it is possible to achieve near-perfect fluency without a substantial decrease in accuracy.
148	84	We evaluate on the test set, comparing this Reasoning model S1 to two baselines: Literal, an image captioning model trained normally on the abstract scene captions (corresponding to our L0), and Contrastive, a model trained with a soft contrastive objective, and previously used for visual referring expression generation (Mao et al., 2015).
