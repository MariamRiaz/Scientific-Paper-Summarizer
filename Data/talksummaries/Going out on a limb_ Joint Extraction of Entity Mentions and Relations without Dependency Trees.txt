0	109	Extraction of entities and their relations from text belongs to a very well-studied family of structured prediction tasks in NLP.
2	33	Several methods have been proposed for entity mention and relation extraction at the sentencelevel.
3	33	These can be broadly categorized into – 1) pipeline models that treat the identification of entity mentions (Nadeau and Sekine, 2007) and relation classification (Zhou et al., 2005) as two separate tasks; and 2) joint models, also the more recent, which simultaneously identify the entity mention and relations (Li and Ji, 2014; Miwa and Sasaki, 2014).
5	58	Recurrent networks (RNNs) (Elman, 1990) have recently become very popular for sequence tagging tasks such as entity extraction that involves a set of contiguous tokens.
6	20	However, their ability to identify relations between non-adjacent tokens in a sequence, e.g., the head nouns of two entities, is less explored.
7	8	For these tasks, RNNs that make use of tree structures have been deemed more suitable.
8	11	Miwa and Bansal (2016), for example, propose an RNN comprised of a sequencebased long short term memory (LSTM) for entity identification and a separate tree-based dependency LSTM layer for relation classification using shared parameters between the two components.
10	16	Also, their model does not jointly extract entities and relations; they first extract all entities and then perform relation classification on all pairs of entities in a sentence.
15	17	In this paper, we propose a novel RNN-based model for the joint extraction of entity mentions 917 and relations.
16	42	Unlike other models, our model does not depend on any dependency tree information.
17	11	Our RNN-based model is a multi-layer bidirectional LSTM over a sequence.
20	16	We also add an additional layer to our network to encode the output sequence from right-to-left and find significant improvement on the performance of relation identification using bi-directional encoding.
21	31	Our model significantly outperforms the feature-based structured perceptron model of Li and Ji (2014), showing improvements on both entity and relation extraction on the ACE05 dataset.
22	88	In comparison to the dependency treebased LSTM model of Miwa and Bansal (2016), our model performs within 1% on entities and 2% on relations on ACE05 dataset.
23	46	We also find that our model performs significantly better than their tree-based model on the AGENT-ARTIFACT relation, while their tree-based model performs better on PHYSICAL and PART-WHOLE relations; the two models perform comparably on all other relation types.
24	93	The very competitive performance of our non-tree-based model bodes well for relation extraction of non-adjacent entities in low-resource languages that lack good parsers.
25	48	In the sections that follow, we describe related work (Section 2); our bi-directional LSTM model with attention (Section 3); the training (Section 4); the experiments on ACE dataset (Section 5); results (Section 6); error analysis (Section 7) and conclusion (Section 8).
45	12	Our model comprises of a multi-layer bidirectional recurrent network which learns a representation for each token in the sequence.
47	41	For each token in the sequence, we output an entity tag and a relation tag.
48	8	The entity tag corresponds to the entity type, whereas the relation tag is a tuple of pointers to related entities and their respective relation types.
51	48	For example, we separately model the relation “ORG-AFF” for each token in the entity “ITV News”.
52	39	Thus, we model the relations between “ITV” and “Martin Geissler”, and “News” and “Martin Geissler” separately.
53	41	We employ a pointer-like network on top of the sequence layer in order to find the relation tag for each token as shown in Figure 2.
54	57	At each time step, the network utilizes the information available about all output tags from the previous time steps in order to output the entity tag and relation tag jointly for the current token.
55	65	We use multi-layer bi-directional LSTMs for sequence tagging because LSTMs are more capable of capturing long-term dependencies between tokens, making it ideal for both entity mention and relation extraction.
56	11	Using LSTMs, we can compute the hidden state−→ ht in the forward direction and ←− ht in the backward direction for every token as below: −→ h t = LSTM(xt, −→ h t−1) ←− h t = LSTM(xt, ←− h t+1) For every token t in the subsequent layer l, we combine the representations −→ h l−1t and ←− h l−1t from previous layer l-1 and feed it as an input.
59	20	We assign each token in the entity with the tag B appended with the entity type if it is the beginning of the entity, I for inside of an entity, L for the end of the entity or U if there is only one token in the entity.
74	36	For each token, we want to find the tokens in the past that the current token is related to along with its relation type.
76	8	For simplicity, let us assume that there is only one previous token the current token is related to when training, i.e., “Safwan” is related to “Geissler” via PHYS relation.
77	47	We can extend our approach to output multiple relations as explained in Section 4.
