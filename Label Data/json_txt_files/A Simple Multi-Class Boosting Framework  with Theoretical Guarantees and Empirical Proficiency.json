{"sections": [{"text": "learning systems that train quickly and with little data. To this end, we showcase REBEL, a multi-class boosting method, and present a novel family of weak learners called localized similarities. Our framework provably minimizes the training error of any dataset at an exponential rate. We carry out experiments on a variety of synthetic and real datasets, demonstrating a consistent tendency to avoid overfitting. We evaluate our method on MNIST and standard UCI datasets against other state-of-the-art methods, showing the empirical proficiency of our method."}, {"heading": "1. Motivation", "text": "The past couple of years have seen vast improvements in the performance of machine learning algorithms. Deep Nets of varying architectures reach almost (if not better than) human performance in many domains (LeCun et al., 2015). A key strength of these systems is their ability to transform the data using complex feature representations to facilitate classification. However, there are several considerable drawbacks to employing such networks.\nA first drawback is that validating through many architectures, each of which may have millions of parameters, requires a lot of data and time. In many fields (e.g. pathology of not-so-common diseases, expert curation of esoteric subjects, etc.), gathering large amounts of data is expensive or even impossible (Yu et al., 2015). Autonomous robots that need to learn on the fly may not be able to afford the large amount of processing power or time required to properly train more complex networks simply due to their hardware constraints. Moreover, most potential users (e.g. nonmachine-learning scientists, small business owners, hobby-\n1Caltech, Pasadena, USA. Correspondence to: Ron Appel <appel@vision.caltech.edu>, Pietro Perona <perona@vision.caltech.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nists, etc.) may not have the expertise or artistry required to hypothesize a set of appropriate models.\nA second drawback is that the complex representations achieved by these networks are difficult to interpret and to analyze. For many riskier applications (e.g. self-driving cars, robotic surgeries, military drones, etc.), a machine should only run autonomously if it is able to explain its every decision and action. Further, when used towards the scientific analysis of phenomena (e.g. understanding animal behavior, weather patterns, financial market trends, etc.), the goal is to extract a causal interpretation of the system in question; hence, to be useful, a machine should be able to provide a clear explanation of its internal logic.\nFor these reasons, it is desirable to have a simple white-box machine learning system that is able to train quickly and with little data. With these constraints in mind, we showcase a multi-class boosting algorithm called REBEL and a novel family of weak learners called similarity stumps, leading to much better generalization than decision stumps, as shown in Fig. 1. Our proposed framework is simple, efficient, and is able to perfectly train on any dataset (i.e. fully minimize the training error in a finite number of iterations).\nThe main contributions of our work are as follows:\n1. a simple multi-class boosting framework using local-\nized similarities as weak learners (see Sec. 3)\n2. a proof that the training error is fully minimized within\na finite number of iterations (see Sec. 5)\n3. a procedure for selecting an adequate learner at each\niteration (see Sec. 5.2)\n4. empirical demonstrations of state-of-the-art results on\na range of datasets (see Sec. 7)"}, {"heading": "2. Background", "text": "Boosting is a fairly mature method, originally formulated for binary classification (e.g. AdaBoost and similar variants) (Schapire, 1990; Freund, 1995; Freund & Schapire, 1996). Multi-class classification is more complex than its binary counterpart, however, many advances have been made in both performance and theory in the context of boosting. Since weak learners come in two flavors, binary and multi-class, two corresponding families of boosting methods have been explored.\nThe clever combination of multiple binary weak learners can result in a multi-class prediction. AdaBoost.MH reduces the K-class problem into a single binary problem with a K-fold augmented dataset (Schapire & Singer, 1999). AdaBoost.MO and similar methods reduce the K-class problem into C one-versus-all binary problems using Error-Correcting Output Codes to select the final hypothesized class (Allwein et al., 2001; Sun et al., 2005; Li, 2006). More recently, CD-MCBoost and CW-Boost return a K-dimensional vector of class scores, focusing each iteration on a (binary) problem of improving the margin of one class at a time (Saberian & Vasconcelos, 2011; Shen & Hao, 2011). REBEL also returns a vector of class scores, increasing the margin between dynamicallyselected binary groupings of the K classes at each iteration (Appel et al., 2016).\nWhen multi-class weak learners are acceptable (and available), a reduction to binary problems is unnecessary. AdaBoost.M1 is a straightforward extension of its binary counterpart (Freund & Schapire, 1996). AdaBoost.M2 and AdaBoost.MR make use of a K-fold augmented dataset to estimate output label probabilities or rankings for a given input (Freund & Schapire, 1996; Schapire & Singer, 1999). More recent methods such as SAMME, AOSOLogitBoost, and GD-MCBoost are based on linear combinations of a fixed set of codewords, outputting Kdimensional score vectors (Zhu et al., 2009; Sun et al., 2011; Saberian & Vasconcelos, 2011).\nIn the noteworthy paper \u201cA Theory of Multiclass Boosting\u201d (Mukherjee & Schapire, 2010), many of the existing boosting methods were shown to be inadequate at training; either\nbecause they require their weak learners to be too strong, or because their loss functions are unable to deal with some training data configurations. (Mukherjee & Schapire, 2010) outline the appropriate Weak Learning Condition that a boosting algorithm must require of its weak learners in order to guarantee training convergence. However, no method is prescribed with which to find an adequate set of weak learners.\nThe goal of our work is to propose a multi-class boosting framework with a simple family of binary weak learners that guarantee training convergence and are easily interpretable. Using REBEL (Appel et al., 2016) as the multiclass boosting method, our framework is meant to be as straightforward as possible so that it is accessible and practical to more users; outlining it in Sec. 3 below."}, {"heading": "3. Our Framework", "text": "In this section, we define our notation, introduce our boosting framework, and describe our training procedure.\nNotation\nscalars (regular), vectors (bold): x, x \u2261 [x1, x2, ...] constant vectors: 0 \u2261 [0,0, ...], 1 \u2261 [1,1, ...] indicator vector: \u03b4k (0 with a 1 in the kth entry) logical indicator function: 1(LOGICAL EXPRESSION) \u2208 {0,1} inner product: \u3008x,v\u3009 element-wise multiplication: x\u2299 v element-wise function: F[x] \u2261 [F(x1), F(x2), ...]\nIn the multi-class classification setting, a datapoint is represented as a feature vector x and is associated with a class label y. Each point is comprised of d features and belongs to one of K classes: x \u2208 X \u2286 Rd, y \u2208 Y \u2261 {1,2, ...,K} A good classifier reduces the training error while generalizing well to potentially-unseen data. We use REBEL (Appel et al., 2016) due to its support for binary weak learners, its mathematical simplicity (i.e. closed-form solution to loss minimization), and its strong empirical performance. REBEL returns a vector-valued output H, the sum of T {weak learner f, accumulation vector a} pairs, where ft : X \u2192 {\u00b11} and at \u2208 R K :\nH(x) \u2261 T\u2211\nt=1\nft(x)at\nThe hypothesized class is simply the index of the maximal entry in H:\nF(x) \u2261 argmax y\u2208Y {\u3008H(x), \u03b4y\u3009}\nThe average misclassification error \u03b5 can be expressed as:\n\u03b5 \u2261 1 N\nN\u2211\nn=1\n1(F(xn) 6=yn) (1)\nREBEL uses an exponential loss function to upper-bound the average training misclassification error:\n\u03b5 \u2264 L \u2261 1 2N\nN\u2211\nn=1\n\u3008exp[yn\u2299H(xn)],1\u3009 (2)\nwhere: yn \u2261 1\u22122\u03b4yn (i.e. all +1s with a \u22121 in the ythn index)\nBeing a greedy, additive model, all previously-trained parameters are fixed and each iteration amounts to jointly optimizing a new weak learner f and accumulation vector a. To this end, the loss at iteration I+1 can be expressed as:\nLI+1 = 1\nN\nN\u2211\nn=1\n\u3008wn, exp[f(xn)yn\u2299a]\u3009 (3)\nwhere: wn \u2261 1\n2 exp[yn\u2299HI(xn)]\nGiven a weak learner f, we define true and false (i.e. correct and incorrect) multi-class weight sums (sTf and s F f ) as:\nsTf \u2261 1\nN\nN\u2211\nn=1\n1[f(xn)yn<0]\u2299wn, s F f \u2261\n1\nN\nN\u2211\nn=1\n1[f(xn)yn>0]\u2299wn\nthus: sTf+s F f =\n1\nN\nN\u2211\nn=1\nwn, s T f\u2212sFf =\n1\nN\nN\u2211\nn=1\nf(xn)wn\u2299yn\nUsing these weight sums, the loss can be simplified to:\nLI+1 \u2261 Lf \u2261 \u3008sTf , exp[\u2212a]\u3009+ \u3008sFf , exp[a]\u3009 (4)\nIn this form, it is easily shown that with the optimal accumulation vector a\u2217, the loss has an explicit expression:\na\u2217= 1\n2\n( ln[sTf ]\u2212 ln[sFf ] ) \u2234 L\u2217f = 2\u3008\n\u221a\nsTf\u2299s F f ,1\u3009 (5)\nAt each iteration, growing decision trees requires an exhaustive search through a pool of decision stumps (which is tractable but time-consuming), storing the binary learner that best reduces the multi-class loss in Eq. 5. In some situations, axis-aligned trees are simply unable to reduce the loss any further, thereby stalling the training procedure.\nOur proposed framework circumvents such situations. At each iteration, instead of exhaustively searching for an adequate learner, we first determine an appropriate \u201cbinarization\u201d of the multi-class data (i.e. a separation of the K-class data into two distinct groups) and then find a weak learner with a guaranteed reduction in loss, foregoing the need for an exhaustive search."}, {"heading": "4. Binarizing Multi-Class Data", "text": "At each iteration, the first step in determining an adequate weak learner is binarizing the data, i.e. assigning a temporary binary label to each data point by placing it into one of two groups. The following manipulations result in a procedure for binarizing datapoints given their boosting weights.\nEq. 5 can be upper-bounded as follows:\nL\u2217f = 2\u3008 \u221a sTf\u2299s F f ,1\u3009 \u2264 \u3008sTf+sFf ,1\u3009 \u2212 1\n2 U \ufe37 \ufe38\ufe38 \ufe37 \u2329 [sTf\u2212sFf ]2 [sTf+s F f ] ,1 \u232a\n(6)\nsince: \u221a x(1\u2212x) \u2264 1 2 \u2212 (1 2 \u2212x )2 \u2200x, using: x= s T sT+sF\nBy expanding sTf \u00b1 sFf , U is expressed as a squared norm:\nU = \u2329 [ 1 N N\u2211 n=1 f(xn)wn\u2299yn ]2\n[ 1 N N\u2211\nn=1 wn\n] ,1\n\u232a\n= \u2225 \u2225 \u2225 N\u2211\nn=1\nf(xn)un \u2225 \u2225 \u2225 2\n(7)\nwhere: un \u2261 1\u221a N wn\u2299yn \u221a N\u2211\nn=1 wn\nEq. 7 can be written as a product of matrices by stacking all of the un as column vectors of a K\u00d7N matrix U and defining f as a row vector with elements f(xn):\nU = f [U\u22a4U] f\u22a4\nNote that the trace of U \u22a4 U can be lower-bounded:\ntr(U \u22a4 U) =\nN\u2211\nn=1\n\u2016un\u20162 = \u2329\nN\u2211\nn=1 [wn]\n2\nN [ N\u2211\nn=1\nwn\n] ,1\n\u232a\n\u2265 1 N2\nN\u2211\nn=1\n\u3008wn,1\u3009\nsince by Jensen\u2019s inequality:\nN\u2211\nn=1\nx2n \u2265 1\nN\n( N\u2211\nn=1\nxn\n)2\nFurthermore, U \u22a4\nU has N (not-necessarily unique) nonnegative eigenvalues, each associated with an independent eigenvector. Let v\u0302n be the eigenvector corresponding to the nth largest eigenvalue \u03bbn. Hence, f can be decomposed as:\nf = \u3008f ,v\u03021\u3009 v\u03021 + N\u2211\nn=2\n\u3008f ,v\u0302n\u3009 v\u0302n (8)\n\u2234 U = \u03bb1\u3008f ,v\u03021\u30092 + N\u2211\nn=2\n\u03bbn\u3008f ,v\u0302n\u30092 \u2265 \u03bb1\u3008f ,v\u03021\u30092\nSince the trace of a matrix is equal to the sum of its eigenvalues and U \u22a4\nU has at most K non-zero eigenvalues (\u03bb1 being the largest), hence:\n\u03bb1 \u2265 1\nK tr(U\n\u22a4 U) \u2265 L0 KN\n(9)\nsince: 1\nN\nN\u2211\nn=1\n\u3008wn,1\u3009 = L0\nBased on this formulation, binarization is achieved by setting the binarized class bn of each sample n as the sign of its corresponding element in v\u03021: bn \u2261 sign(\u3008v\u03021, \u03b4n\u3009) Accordingly, if b is the vector with elements bn, then:\n\u3008b,v\u03021\u30092 = \u3008sign[v\u03021],v\u03021\u30092 = \u3008|v\u03021|,1\u30092 \u2265 1 (10)\n(please refer to supplement for proof)\nFinally, by combining Eq. 6, Eq. 9, and Eq. 10, with perfect binarized classification (i.e. when the binary weak learner perfectly classifies the binarized data), the loss ratio at any iteration is bounded by:\nLf\u2217 L0 \u2264 1\u2212 1 2KN\nIn general, there is no guarantee that any weak learner can achieve perfect binarized classification. In the following section, we show that with the ability to isolate any single point in space (i.e. to classify an inner point as +1 and all outer points as \u22121), the loss decreases exponentially."}, {"heading": "5. Isolating Points", "text": "Assume that we have a weak learner fi that can isolate a single point xi in the input space X. Accordingly, denote fi = 2\u03b4i\u22121 as a vector of \u22121s with a +1 in the ith entry, corresponding to classification using the isolating learner fi(xn). If N \u2265 4, then for any unit vector v\u0302 \u2208 RN :\nmax i\n{\u3008fi,v\u0302\u30092} \u2265 4\nN (11)\n(please refer to supplement for proof)\nCombining Eq. 6, Eq. 9, and Eq. 11, the loss ratio at each iteration is upper-bounded as follows:\nmini{Lfi} L0 \u2264 1\u2212 2 KN2\nBefore the first iteration, the initial loss L0 = K/2. Each iteration decreases the loss exponentially. Since the training error is discrete and is upper bounded by the loss (as\nin Eq. 2), our framework attains minimal training error on any1 training set after a finite number of iterations:\ndefine: T0 \u2261 \u2308 ln(2/KN)\nln ( 1\u2212 2KN2 )\n\u2309\n\u2248 \u2308 KN2 2 ln (KN 2 )\u2309\n\u2234 T \u2265 T0 \u21d2 K\n2\n(\n1\u2212 2 KN2\n)T\n< 1\nN \u21d2 \u03b5 = 0\nAlthough this bound is too weak to be of practical use, it is a bound nonetheless (and can likely be improved). In the following section, we specify a suitable family of weak learners with the ability to isolate single points."}, {"heading": "5.1. One/Two-Point Localized Similarities", "text": "Classical decision stumps compare a single feature to a threshold, outputting +1 or \u22121. Instead, our proposed family of weak learners (called localized similarities) compare points in the input space using a similarity measure. Due to its simplicity and effectiveness, we use negative squared Euclidean distance \u2212\u2016xi\u2212xj\u20162 as a measure of similarity between points xi and xj . A localized similarity has two modes of operation:\n1. In one-point mode, given an anchor xi and a threshold\n\u03c4 , the input space is classified as positive if it is more similar to xi than \u03c4 , and negative otherwise; ranging between +1 and \u22121:\nfi(x) \u2261 \u03c4 \u2212 \u2016xi\u2212x\u20162 \u03c4 + \u2016xi\u2212x\u20162\n2. In two-point mode, given supports xi and xj , the input\nspace is classified as positive if it is more similar to xi than to xj (and vice-versa), with maximal absolute activations around xi and xj ; falling off radially away from the midpoint m:\nfij(x) \u2261 \u3008d,x\u2212m\u3009\n4\u2016d\u20164 + \u2016x\u2212m\u20164\nwhere: d \u2261 1 2 [xi\u2212xj ] and: m \u2261 1 2 [xi+xj ]\nOne-point mode enables the isolation of any single datapoint, guaranteeing a baseline reduction in loss. However, it essentially leads to pure memorization of the training data; mimicking a nearest-neighbor classifier. Two-point mode adds the capability to generalize better by providing margin-style functionality. The combination of these\n1 There may be situations in which multiple samples belonging to different classes are coincident in the input space. These cases can be dealt with (before or during training) either by assigning all such points as a special \u201cmixed\u201d class (to be dealt with at a later stage), or by setting the class labels of all coincident points to the single label that minimizes the error.\ntwo modes enables the flexibility to tackle a wide range of classification problems. Furthermore, in either mode, the functionality of a localized similarity is easily interpretable: \u201cwhich of these fixed training points is a given query point more similar to?\u201d"}, {"heading": "5.2. Finding Adequate Localized Similarities", "text": "Given a dataset with N samples, there are about N2 possible localized similarities. The following procedure efficiently selects an adequate localized similarity:\n0. Using Eq. 5, calculate the base loss L1 for the homogeneous stump f1 (i.e. the one-point stump with any xi and \u03c4 \u2261 \u221e, classifying all points as +1).\n1. Compute the eigenvector v\u03021 (as in Eq. 8); label the\npoints based on their binarized class labels bn.\n2. Find the optimal isolating localized similarity fi (i.e. with xi and appropriate \u03c4 , classifying point i as +1 and all other points as \u22121). 3. Using Eq. 5, calculate the corresponding loss Li. Of the two stumps f1 and fi, store the one with smaller loss as best-so-far. 4. Find point xj most similar 2 to xi among points of the\nopposite binarized class:\nxj = arg min bj=\u2212bi\n{\u2016xi\u2212xj\u20162}\n5. Calculate the loss achieved using the two-point local-\nized similarity fij . If it outperforms the previous best, store the newer learner and update the best-so-far loss.\n6. Find all points that are similar enough to xj and remove\nthem from consideration for the remainder of the current iteration. In our implementation, we remove all xn for which:\nfij(xn) \u2264 fij(xj)/2\nIf all points have been removed, return the best-so-far localized similarity; otherwise, loop back to step 4.\nUpon completion of this procedure, the best-so-far localized similarity is guaranteed to lead to an adequate reduction in loss, based on the derivation in Sec. 4 above."}, {"heading": "6. Generalization Experiments", "text": "Our boosting method provably reduces the loss well after the training error is minimized. In this section, we demonstrate that the continual reduction in loss serves only to improve the decision boundaries and not to overfit the data.\nWe generated 2-dimensional synthetic datasets in order to better visualize and get an intuition for what the classifiers\n2 \u201cmost similar\u201d need not be exact; approximate nearestneighbors also works with negligible differences.\nare doing. The results shown in this chapter are based on a dataset composed of 500 points belonging to one of three classes in a spiral formation, with a (2/3, 1/3) train/test split. Fig. 2 shows the hypothesized class using a classifier trained for 1000 iterations.\nOur classifier achieves perfect training (left) and test classification (right), producing a visually simple wellgeneralizing contour around the points. Training curves are given in Fig. 3, tracking the loss and classification errors per training iteration. Note that the test error does not increase even after the training error drops to zero.\nThe following experiments explore the functionality of our framework (i.e. REBEL using localized similarities) in two scenarios that commonly arise in practice: (1) varying sparsity of training data, and, (2) varying amounts of mislabeled training data."}, {"heading": "6.1. Sparse Training Data", "text": "In this section of experiments, classifiers were trained using varying amounts of data, from 4/5 to 1/5 of the total training set. Fig. 4 shows the classification boundaries learned by the classifier (ai,bi), and the training curves (ci). In all cases, the boundaries seem to aptly fit (and not overfit) the training data (i.e. being satisfied with isolated patches without overzealously trying to connect points of the same class together). This is more rigorously observed from the training curves; the test error does not increase after reaching its minimum (for hundreds of iterations)."}, {"heading": "6.2. Mislabeled Training Data", "text": "In this section of experiments, classifiers were trained with varying fractions of mislabeled data; from 1% to 30% of the training set. Fig. 5 shows the classification boundaries (ai,bi) and the training curves (ci). All classifiers seem to degenerate gracefully, isolating rogue points and otherwise maintaining smooth boundaries. Even the classifier trained on 30% mislabeled data (which we would consider to be unreasonably noisy) is able to maintain smooth boundaries.\nIn all cases, the training curves still show that the test error is fairly stable once reaching its minimum value. Moreover, test errors approximately equal the fraction of mislabeled data, further validating the generalization of our method."}, {"heading": "6.3. Real Data", "text": "Although the above observations are promising, they could result from the fact that the synthetic datasets are 2- dimensional. In order to rule out this possibility, we performed similar experiments on several UCI datasets (Bache & Lichman, 2013) of varying input dimensionalities (from 9 to 617). From the training curves in Fig. 6, we observe that once the test errors saturate, they no longer increase, even after hundreds of iterations.\nIn Fig. 7, we plot the training losses on a log-scaled y-axis. The linear trend signifies an exponential decrease in loss per iteration. Our proven bound predicts a much slower (exponential) rate than the actual trend observed during training. Note that within the initial \u223c10% of the iterations, the loss drops at an even faster rate, after which it settles down\nto a seemingly-constant rate of exponential decay. We have not yet determined the characteristics (i.e. the theoretically justified rates) of these observed trends, and relegate this endeavor to future work."}, {"heading": "7. Comparison with Other Methods", "text": "In Sec. 5 we proved that our framework adheres to theoretical guarantees, and in Sec. 6 above, we showed that it has promising empirical properties. In this section, we compete against several state-of-the-art boosting baselines. Specifically, we compared 1-vs-All AdaBoost and AdaBoost.MH (Schapire & Singer, 1999), AdaBoost.ECC (Dietterich & Bakiri, 1995), Struct-Boost (Shen et al., 2014), CW-Boost (Shen & Hao, 2011), AOSOLogitBoost (Sun et al., 2011), REBEL (Appel et al., 2016) using shallow decision trees, REBEL using only 1-point (isolating) similarities, and our full framework, REBEL using 2-point localized similarities.\nBased on the same experimental setup as in (Shen et al., 2014; Appel et al., 2016), competing methods are trained to a maximum of 200 decision stumps. For each dataset, five random splits are generated, with 50% of the samples for training, 25% for validation (i.e. for setting hyperparameters where needed), and the remaining 25% for testing.\nREBEL using localized similarities is the most accurate method on five of the six datasets tested. In the Vowel dataset, it achieves almost half of the error as the next best method. Note that although our framework uses REBEL as its boosting method, the localized similarities add an extra edge, beating REBEL with decision trees in all runs.\nFurther, when limited to only using 1-point (i.e. isolating) localized similarities, the performance is extremely poor, validating the need for 2-point localized similarities as prescribed in Sec. 5.2. Overall, these results demonstrate the\nGLASS VOWEL LANDSAT MNIST PENDIGITS SEGMENT 0\n10\n20\n30\n40\nP e rc\ne n t\nE rr\no r\n31 .7 32 .3 32 .7 35 .8 35 .4 34 .2 30 .4 35 .9 27 .4\n*\n21 .1 18 .8 20 .6 17 .5 22 .4 20 .6 17 .4 62 .3 9. 5* 15 .1 12 .7 12 .8 12 .1 11 .1 15 .4 10 .7 52 .3 10 .6\n*\n11 .0 13 .4 15 .8 12 .5 9. 3* 12 .5 10 .5 87 .1 9. 3* 7. 1 7. 4 8. 4 6. 9\n2. 5\n12 .8\n3. 2\n32 .8 1. 2* 7. 7\n3. 7\n2. 9 2. 9 2. 5*\n5. 6\n4. 6\n69 .7\n3. 3\n[0 0 1] Ada 1vsAll [0 0 0] Ada.MH [0 1 0] Ada.ECC [0 2 0] Struct-Boost [2 1 1] CW-Boost [0 0 0] A0S0-Logit [0 3 2] RBL-Stump [0 0 0] RBL-Iso.Sim [5 0 0] RBL-Loc.Sim\nFigure 8. Test errors of various state-of-the-art and baseline classification methods on MNIST and several UCI datasets. REBEL using localized similarities (shown in yellow) is the bestperforming method on all but one of the datasets shown. When constrained to use only 1-point (isolating) similarities (shown in red), the resulting classifier is completely inadequate.\nability of our framework to produce easily interpretable classifiers that are also empirically proficient."}, {"heading": "7.1. Comparison with Neural Networks and SVMs", "text": "Complex neural networks are able to achieve remarkable performance on large datasets, but they require an amount of training data proportional to their complexity. In the regime of small to medium amounts of data (within which UCI and MNIST datasets belong, i.e. 10 < N < 106 training samples), such networks cannot be too complex. Accordingly, in Fig. 9, we compare our method against fullyconnected neural networks.\nFour neural networks were implemented, each having one of the following architectures: [d\u22124d\u2212K], [d\u22124K\u2212K], [d\u22122d\u2212d\u2212K], [d\u22124K\u22122K\u2212K], where d is the number of input dimensions and K is the number of output classes. Only the one with the best test error is shown in the plot. A multi-class SVM (Chang & Lin, 2011) was validated using a 5\u00d7 6 parameter sweep for C and \u03b3. Our method was run until the training loss fell below 1/N . Overall, REBEL using localized similarities achieves the best results on eight of the ten datasets, decisively marking it as the method of choice for this range of data."}, {"heading": "8. Discussion", "text": "In Sec. 6, we observed that our classifiers tend to smoothen the decision boundaries in the iterations beyond zero training error. In Fig. 10, we see that this is not the case with the typically-used axis-aligned decision stumps. Why does this happen with our framework?\nFirstly, we note that the largest-margin boundary between two points is the hyperplane that bisects them. Every twopoint localized similarity acts as such a bisector. Therefore, it is not surprising that with only a pool of localized similarities, a classifier should have what it needs to place good boundaries. Further, not all pairs need to be separated (since many neighboring points belong to the same class); hence, only a small subset of the \u223c N2 possible learners will ever need to be selected.\nSecondly, we note that if some point (either an outlier or an unfortunately-placed point) continues to increase in weight until it can no-longer be ignored, it can simply be isolated and individually dealt with using a one-point localized similarity, there is no need to combine it with other \u201cinnocentbystander\u201d points. This phenomenon is observed in the mislabeled training experiments in Sec. 6.2.\nTogether, the two types of localized similarities complement each other. With the guarantee that every step reduces the loss, each iteration focuses on either further smoothening out an existing boundary, or reducing the weight of a single unfit point."}, {"heading": "9. Conclusions", "text": "We have presented a novel framework for multi-class boosting that makes use of a simple family of weak learners called localized similarities. Each of these learners has a clearly understandable functionality; a test of similarity between a query point and some pre-defined samples.\nWe have proven that the framework adheres to theoretical guarantees: the training loss is minimized at an exponential rate, and since the loss upper-bounds the training error (which can only assume discrete values), our framework is therefore able to achieve maximal accuracy on any dataset.\nWe further explored some of the empirical properties of our framework, noting that the combination of localized similarities and guaranteed loss reduction tend to lead to a non-overfitting regime, in which the classifier focuses on smoothing-out its decision boundaries. Finally, we compare our method against several state-of-the-art methods, outperforming all of the methods in most of the datasets.\nAltogether, we believe that we have achieved our goal of presenting a simple multi-class boosting framework with theoretical guarantees and empirical proficiency."}, {"heading": "Acknowledgements", "text": "The authors would like to thank anonymous reviewers for their feedback and Google Inc. and the Office of Naval Research MURI N00014-10-1-0933 for funding this work."}], "year": 2017, "references": [{"title": "Reducing multiclass to binary: a unifying approach for margin classifiers", "authors": ["E.L. Allwein", "R.E. Schapire", "Y. Singer"], "year": 2001}, {"title": "Improved multi-class cost-sensitive boosting via estimation of the minimum-risk", "authors": ["R. Appel", "X.P. Burgos-Artizzu", "P. Perona"], "venue": "class. arXiv,", "year": 2016}, {"title": "UCI machine learning repository", "authors": ["K. Bache", "M. Lichman"], "venue": "(uc irvine),", "year": 2013}, {"title": "LIBSVM: A library for support vector machines", "authors": ["C. Chang", "C. Lin"], "venue": "Transactions on Intelligent Systems and Technology,", "year": 2011}, {"title": "Solving multiclass learning problems via error-correcting output codes. arXiv", "authors": ["T.G. Dietterich", "G. Bakiri"], "year": 1995}, {"title": "Boosting a weak learning algorithm by majority", "authors": ["Y. Freund"], "venue": "Information and Computation,", "year": 1995}, {"title": "Experiments with a new boosting algorithm", "authors": ["Y. Freund", "R.E. Schapire"], "venue": "In Machine Learning International Workshop,", "year": 1996}, {"title": "Multiclass boosting with repartitioning", "authors": ["L. Li"], "venue": "In ICML,", "year": 2006}, {"title": "A theory of multiclass boosting", "authors": ["I. Mukherjee", "R.E. Schapire"], "venue": "In NIPS,", "year": 2010}, {"title": "Multiclass boosting: Theory and algorithms", "authors": ["M. Saberian", "N. Vasconcelos"], "venue": "In NIPS,", "year": 2011}, {"title": "The strength of weak learnability", "authors": ["R.E. Schapire"], "venue": "Machine Learning,", "year": 1990}, {"title": "Improved boosting algorithms using confidence-rated predictions", "authors": ["R.E. Schapire", "Y. Singer"], "venue": "In Conference on Computational Learning Theory,", "year": 1999}, {"title": "A direct formulation for totallycorrective multi-class boosting", "authors": ["C. Shen", "Z. Hao"], "venue": "In CVPR,", "year": 2011}, {"title": "Structboost: Boosting methods for predicting structured output variables", "authors": ["G. Shen", "G. Lin", "A. van den Hengel"], "year": 2014}, {"title": "Aoso-logitboost: Adaptive one-vs-one logitboost for multi-class problem", "authors": ["P. Sun", "M.D. Reid", "J. Zhou"], "year": 2011}, {"title": "Unifying the error-correcting and output-code adaboost within the margin framework", "authors": ["Y. Sun", "S. Todorovic", "J. Li", "D. Wu"], "venue": "In ICML,", "year": 2005}, {"title": "LSUN: construction of a large-scale image dataset using deep learning with humans", "authors": ["F. Yu", "Y. Zhang", "S. Song", "A. Seff", "J. Xiao"], "venue": "in the loop. arXiv,", "year": 2015}], "id": "SP:dff600a4fc3d93eb92603138b1473b84e39490ba", "authors": [{"name": "Ron Appel", "affiliations": []}, {"name": "Pietro Perona", "affiliations": []}], "abstractText": "There is a need for simple yet accurate white-box learning systems that train quickly and with little data. To this end, we showcase REBEL, a multi-class boosting method, and present a novel family of weak learners called localized similarities. Our framework provably minimizes the training error of any dataset at an exponential rate. We carry out experiments on a variety of synthetic and real datasets, demonstrating a consistent tendency to avoid overfitting. We evaluate our method on MNIST and standard UCI datasets against other state-of-the-art methods, showing the empirical proficiency of our method. 1. Motivation The past couple of years have seen vast improvements in the performance of machine learning algorithms. Deep Nets of varying architectures reach almost (if not better than) human performance in many domains (LeCun et al., 2015). A key strength of these systems is their ability to transform the data using complex feature representations to facilitate classification. However, there are several considerable drawbacks to employing such networks. A first drawback is that validating through many architectures, each of which may have millions of parameters, requires a lot of data and time. In many fields (e.g. pathology of not-so-common diseases, expert curation of esoteric subjects, etc.), gathering large amounts of data is expensive or even impossible (Yu et al., 2015). Autonomous robots that need to learn on the fly may not be able to afford the large amount of processing power or time required to properly train more complex networks simply due to their hardware constraints. Moreover, most potential users (e.g. nonmachine-learning scientists, small business owners, hobbyCaltech, Pasadena, USA. Correspondence to: Ron Appel <appel@vision.caltech.edu>, Pietro Perona <perona@vision.caltech.edu>. Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s). (a) Old: Decision Stumps (b) New: Localized Similarities Figure 1. (a) The typical decision stumps commonly used in boosting lead to classification boundaries that are axis aligned and not representative of the data. Although these methods can achieve perfect training accuracy, it is apparent that they heavily overfit. (b) Our method uses localized similarities, a novel family of simple weak learners (see Sec. 5.1). Paired with a procedure that provably guarantees exponential loss minimization, our classifiers focus on smooth, well-generalizing boundaries. ists, etc.) may not have the expertise or artistry required to hypothesize a set of appropriate models. A second drawback is that the complex representations achieved by these networks are difficult to interpret and to analyze. For many riskier applications (e.g. self-driving cars, robotic surgeries, military drones, etc.), a machine should only run autonomously if it is able to explain its every decision and action. Further, when used towards the scientific analysis of phenomena (e.g. understanding animal behavior, weather patterns, financial market trends, etc.), the goal is to extract a causal interpretation of the system in question; hence, to be useful, a machine should be able to provide a clear explanation of its internal logic. For these reasons, it is desirable to have a simple white-box machine learning system that is able to train quickly and with little data. With these constraints in mind, we showcase a multi-class boosting algorithm called REBEL and a novel family of weak learners called similarity stumps, leading to much better generalization than decision stumps, as shown in Fig. 1. Our proposed framework is simple, efficient, and is able to perfectly train on any dataset (i.e. fully minimize the training error in a finite number of iterations). A Simple Multi-class Boosting Framework The main contributions of our work are as follows: 1. a simple multi-class boosting framework using localized similarities as weak learners (see Sec. 3) 2. a proof that the training error is fully minimized within a finite number of iterations (see Sec. 5) 3. a procedure for selecting an adequate learner at each iteration (see Sec. 5.2) 4. empirical demonstrations of state-of-the-art results on a range of datasets (see Sec. 7) 2. Background Boosting is a fairly mature method, originally formulated for binary classification (e.g. AdaBoost and similar variants) (Schapire, 1990; Freund, 1995; Freund & Schapire, 1996). Multi-class classification is more complex than its binary counterpart, however, many advances have been made in both performance and theory in the context of boosting. Since weak learners come in two flavors, binary and multi-class, two corresponding families of boosting methods have been explored. The clever combination of multiple binary weak learners can result in a multi-class prediction. AdaBoost.MH reduces the K-class problem into a single binary problem with a K-fold augmented dataset (Schapire & Singer, 1999). AdaBoost.MO and similar methods reduce the K-class problem into C one-versus-all binary problems using Error-Correcting Output Codes to select the final hypothesized class (Allwein et al., 2001; Sun et al., 2005; Li, 2006). More recently, CD-MCBoost and CW-Boost return a K-dimensional vector of class scores, focusing each iteration on a (binary) problem of improving the margin of one class at a time (Saberian & Vasconcelos, 2011; Shen & Hao, 2011). REBEL also returns a vector of class scores, increasing the margin between dynamicallyselected binary groupings of the K classes at each iteration (Appel et al., 2016). When multi-class weak learners are acceptable (and available), a reduction to binary problems is unnecessary. AdaBoost.M1 is a straightforward extension of its binary counterpart (Freund & Schapire, 1996). AdaBoost.M2 and AdaBoost.MR make use of a K-fold augmented dataset to estimate output label probabilities or rankings for a given input (Freund & Schapire, 1996; Schapire & Singer, 1999). More recent methods such as SAMME, AOSOLogitBoost, and GD-MCBoost are based on linear combinations of a fixed set of codewords, outputting Kdimensional score vectors (Zhu et al., 2009; Sun et al., 2011; Saberian & Vasconcelos, 2011). In the noteworthy paper \u201cA Theory of Multiclass Boosting\u201d (Mukherjee & Schapire, 2010), many of the existing boosting methods were shown to be inadequate at training; either because they require their weak learners to be too strong, or because their loss functions are unable to deal with some training data configurations. (Mukherjee & Schapire, 2010) outline the appropriate Weak Learning Condition that a boosting algorithm must require of its weak learners in order to guarantee training convergence. However, no method is prescribed with which to find an adequate set of weak learners. The goal of our work is to propose a multi-class boosting framework with a simple family of binary weak learners that guarantee training convergence and are easily interpretable. Using REBEL (Appel et al., 2016) as the multiclass boosting method, our framework is meant to be as straightforward as possible so that it is accessible and practical to more users; outlining it in Sec. 3 below. 3. Our Framework In this section, we define our notation, introduce our boosting framework, and describe our training procedure. Notation scalars (regular), vectors (bold): x, x \u2261 [x1, x2, ...] constant vectors: 0 \u2261 [0,0, ...], 1 \u2261 [1,1, ...] indicator vector: \u03b4k (0 with a 1 in the k entry) logical indicator function: 1(LOGICAL EXPRESSION) \u2208 {0,1} inner product: \u3008x,v\u3009 element-wise multiplication: x\u2299 v element-wise function: F[x] \u2261 [F(x1), F(x2), ...] In the multi-class classification setting, a datapoint is represented as a feature vector x and is associated with a class label y. Each point is comprised of d features and belongs to one of K classes: x \u2208 X \u2286 R, y \u2208 Y \u2261 {1,2, ...,K} A good classifier reduces the training error while generalizing well to potentially-unseen data. We use REBEL (Appel et al., 2016) due to its support for binary weak learners, its mathematical simplicity (i.e. closed-form solution to loss minimization), and its strong empirical performance. REBEL returns a vector-valued output H, the sum of T {weak learner f, accumulation vector a} pairs, where ft : X \u2192 {\u00b11} and at \u2208 R K : H(x) \u2261 T \u2211", "title": "A Simple Multi-Class Boosting Framework  with Theoretical Guarantees and Empirical Proficiency"}