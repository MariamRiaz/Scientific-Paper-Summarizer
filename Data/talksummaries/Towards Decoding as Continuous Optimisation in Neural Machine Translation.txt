16	10	We present experimental results on Chinese-English and German-English translation tasks, confirming the effectiveness of our relaxed optimisation method for decoding (§5).
22	84	(2) The model parameters Θ include the weight matrix Wo ∈ R|VT |×nh and the bias bo ∈ R|VT | – with nH denoting the hidden dimension size – as well as the RNN encoder biRNNθenc / decoder RNNφdec parameters, word embedding matrices, and the parameters of the attention mechanism.
23	18	The model is trained end-to-end by optimising the training objective using stochastic gradient descent (SGD) or its variants.
27	19	In general, searching Yx to find the highest probability translation is intractable due to the recurrent nature of eqn (1) which prevents dynamic programming for efficient search.
29	12	We now formulate this discrete optimisation problem as a continuous one, and then use standard algorithms for continuous optimisation for decoding.
30	99	Let us assume that the maximum length of a possible translation for a source sentence is known and denote it as `.
32	31	where we allow the translation to be padded with sentinel symbols to the right, which are ignored in computing the model probability.
36	18	using back-propagation 4: For all i, w : update ŷti(w) ∝ ŷt−1i (w) · exp ( −η∇t−1i,w ) .
38	18	, ŷt`) We now convert the optimisation problem (5) to a continuous one by dropping the integrality constraints ỹi ∈ I|V | and require the variables to take values from the probability simplex: arg min ŷ1,...,ŷ` − ∑̀ i=1 ŷi · log softmax (f (Θ, ŷ<i,x)) s.t.
39	20	`} : ŷi ∈ ∆|VT | where ∆|VT | is the |VT |-dimensional probability simplex, i.e., {ŷi ∈ [0, 1]|VT | : ‖ŷi‖1 = 1}.
41	40	After solving the above constrained continuous optimisation problem, there is no guarantee that the resulting solution {ŷ∗i }`i=1 will comprise onehot vectors, i.,e., target language words.
42	21	Instead it can find fractional solutions, that require ‘rounding’ in order to to resolve them to lexical items.
44	155	We leave exploration of more elaborate projection techniques to the future work.
45	12	In the context of graphical models, the above relaxation technique gives rise to linear programming for approximate inference (Sontag, 2010; Belanger and McCallum, 2016).
46	11	However, our decoding problem is much harder due to the nonlinearity and non-convexity of the objective function operating on high dimensional space for deep models.
47	11	We now turn our attention to optimisation algorithms to effectively solve the decoding optimisation problem.
50	57	(6) EG is an iterative algorithm, which updates each distribution ŷti in the current time-step t based on the distributions of the previous time-step as follows: ∀w ∈ VT : ŷti(w) = 1 Zti ŷt−1i (w) exp ( −η∇t−1i,w ) where η is the step size, ∇t−1i,w = ∂Q(ŷt−11 ,...,ŷ t−1 ` ) ∂ŷi(w) and Zti is the normalisation constant Zti = ∑ w∈VT ŷt−1i (w) exp ( −η∇t−1i,w ) .
51	26	The partial derivatives ∇i,w are calculated using the back propagation algorithm treating {ŷi}`i=1 as parameters and the original parameters of the model Θ as constants.
52	10	Adapting EG to our decoding problem leads to Algorithm 1.
54	68	, ŷ`)− γ ∑̀ i=1 Entropy(ŷi) (7) In other words, the algorithm looks for the maximum entropy solution which also maximizes the log likelihood under the model.
57	134	To be able to apply SGD to our optimisation problem, we need to make sure that the simplex constraints are enforced.
58	105	One way to achieve this is by reparameterising using the softmax transformation, i.e. ŷi = softmax (r̂i).
59	33	The resulting unconstrained optimisation problem, now over r̂i, becomes arg min r̂1,...,r̂` − ∑̀ i=1 softmax (r̂i) · log softmax (f (Θ, ŷ<i,x)) where EyiT is replaced with the expected embedding of the target words under the distribution resulted from the Esoftmax(r̂i) [E w T ] in the model.
60	21	To apply SGD updates, we need the gradient of the objective function with respect to the new variables r̂i which can be derived with the backpropagation algorithm based on the chain rule: ∂Q ∂r̂i(w) = ∑ w′∈VT ∂Q(.)
61	39	∂ŷi(w′) ∂ŷi(w′) ∂r̂i(w) The resulting SGD algorithm is summarized in Algorithm 2.
63	41	This enables decoding for richer global models, for which there is no effective means of greedy decoding or beam search.
64	21	We outline several such models, and their corresponding relaxed objective functions for optimisation-based decoding.
65	32	Standard NMT generates the translation in a left-to-right manner, conditioning each target word on its left context.
66	56	However, the joint probability of the translation can be decomposed in a myriad of different orders; one compelling alternative would be to condition each target word on its right context, i.e., generating the target sentence from right-to-left.
67	52	We would not expect a right-to-left model to outperform a left-to-right, however, as the left-to-right ordering reflects the natural temporal order of spoken language.
