12	22	Most commonly, anchoring is manifested as an alignment towards the given values.
13	22	Focusing on the key NLP tasks of POS tagging and dependency parsing, we demonstrate that the standard approach of obtaining annotations via human correction of automatically generated POS tags and dependencies exhibits a clear anchoring effect â€“ a phenomenon we refer to as parser bias.
14	42	Given this evidence, we examine two potential adverse implications of this effect on parser-based gold standards.
15	21	First, we show that parser bias entails substantial overestimation of parser performance.
20	21	Furthermore, we conduct an experiment on interannotator agreement for POS tagging and dependency parsing which controls for parser bias.
79	28	We use the EWT UD corpus as the existing gold standard, and a sample of the FCE dataset as the new corpus.
80	59	Procedure Our experimental procedure, illustrated in figure 1(a) contains a set of 360 sentences (6,979 tokens) from the FCE, for which we generate three gold standards: one based on human annotations and two based on parser outputs.
94	15	The agreement between the Turbo parser and RBG parser based on the respective tagger outputs is 90.76 UAS, 91.6 LA and 87.34 LAS.
100	18	Second, we note that the performance of each of the parsers on the gold standard of the other parser is still higher than its performance on the human gold standard.
104	28	Taken together, the parser specific and parser shared effects lead to a dramatic overall average error reduction of 49.18% POS, 33.71% UAS, 34.9% LA and 35.61% LAS on the parser-based gold standards compared to the human-based gold standard.
106	44	In this section we extend our investigation to examine the impact of parser bias on the quality of parser-based gold standards.
111	34	In all three evaluation categories, human judges tend to prefer the human-based gold standard over both parser-based gold standards.
118	54	We observe that cases in which the parsers disagree are of substantially worse quality compared to humanbased annotations.
125	41	The initial quality of the provided annotations in combination with the parser bias effect observed in section 3 may influence the resulting agreement estimates.
128	61	We thus introduce a novel pipeline based on human annotation only, which eliminates parser bias from the agreement measurements.
130	29	Importantly, we include an additional review step for the initial annotations, designed to increase the precision of the agreement measurements by reducing the number of errors in the original annotations.
131	28	Sentence Scratch Scratch reviewed Figure 2: Experimental setup for the inter-annotator agreement experiment.
138	35	Two of the participants provide the sentence with annotations from scratch, while the remaining two participants provide reviews.
151	22	For example, Weiss et al. (2015) report 94.26 UAS and Andor et al. (2016) report 94.61 UAS on section 23 of the PTB-WSJ using an automatic conversion of the PTB phrase structure trees to Stanford dependencies (De Marneffe et al., 2006).
178	26	Such a strategy would largely maintain the annotation speed-ups of parser-based annotation schemes.
180	46	Further on, we obtain, to the best of our knowl- edge for the first time, syntactic inter-annotator agreement measurements on WSJ-PTB sentences.
181	32	Our experimental procedure reduces annotation errors and controls for parser bias.
183	40	We note that our results do not necessarily reflect an upper bound on the achievable syntactic inter-annotator agreement for English newswire.
184	16	Higher agreement rates could in principle be obtained through further annotator training, refinement and revision of annotation guidelines, as well as additional automatic validation tests for the annotations.
185	170	Nonetheless, we believe that our estimates reliably reflect a realistic scenario of expert syntactic annotation.
186	20	The obtained agreement rates call for a more extensive examination of annotator disagreements on parsing and tagging.
187	23	Recent work in this area has already proposed an analysis of expert annotator disagreements for POS tagging in the absence of annotation guidelines (Plank et al., 2014).
188	20	Our annotations will enable conducting such studies for annotation with guidelines, and support extending this line of investigation to annotations of syntactic dependencies.
189	63	As a first step towards this goal, we plan to carry out an in-depth analysis of disagreement in the collected data, characterize the main sources of inconsistent annotation and subsequently formulate further strategies for improving annotation accuracy.
190	164	We believe that better understanding of human disagreements and their relation to disagreements between humans and parsers will also contribute to advancing evaluation methodologies for POS tagging and syntactic parsing in NLP, an important topic that has received only limited attention thus far (Schwartz et al., 2011; Plank et al., 2015).
