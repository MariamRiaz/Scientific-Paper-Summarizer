{"sections": [{"heading": "1. Introduction", "text": "A widely used machine learning technique is the transfer of a representation learned from a source task, for which labeled data is abundant, to a target task, for which labeled data is scarce. This may be effective if the tasks approximately share an intermediate representation. For example:\n\u2022 features learned from an image of a human face to predict age may also be useful for predicting gender\n\u2022 word embeddings learned to predict word contexts may also be useful for part of speech tagging\n\u2022 features learned from financial data to predict loan default may also be useful for predicting insurance fraud.\nOften a representation is learned by a different organization that may have greater access to data, computational and human resources. Examples are the Google word2vec package (Mikolov et al., 2013), and downloadable pre-trained\n1The Australian National University and Data61, Canberra, ACT, Australia 2Carnegie Mellon University, Pittsburgh, PA, USA. Correspondence to: Daniel McNamara <daniel.mcnamara@anu.edu.au>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nneural networks.1 Under this \u2018representation-as-a-service\u2019 model, a user may expect to access the representation itself, as well as information about its performance on the source task data on which it was trained. We aim to convert this into a guarantee of the usefulness of the representation on other tasks, which is known in advance without the effort or cost of testing the representation on the target task(s). Our analysis also covers the case where the source task is constructed from unlabeled data, as in neural network unsupervised pre-training.\nWe consider two approaches to transferring a representation learned from a source task to a target task, as shown in Figure 1. We may either treat the representation as fixed, or we may narrow the class of representations considered on the target task, which we refer to as fine-tuning. The fixed option may be attractive when very little labeled target task data is available and hence overfitting is a strong concern, while the advantage of fine-tuning is relatively greater hypothesis class expressiveness.\nLet X,Y and Z be sets known as the input, output and feature spaces respectively. Let F be a class of representations, where f : X \u2192 Z for f \u2208 F . Let G be a class of specialized classifiers, where g : Z \u2192 Y for g \u2208 G. Let the hypothesis class H := {h : \u2203f \u2208 F, g \u2208 G such that h = g \u25e6 f}. Let hS , hT : X \u2192 Y be the labeling functions and PS , PT be the input distributions for source task S and target task T respectively. We consider the setting Y = {\u22121, 1}. Let the risk of a hypothesis h on S and T be RS(h) := Ex\u223cPS [hS(x) 6= h(x)] and RT (h) := Ex\u223cPT [hT (x) 6= h(x)] respectively. Let R\u0302S(h) and R\u0302T (h) be the corresponding empirical (i.e. training set) risks. We have mS labelled points for S and mT labelled points for T . Let dH be the VC dimension of H .\nThe remainder of the paper is structured as follows. In Section 2 we introduce related work. In Sections 3 and 4 we analyze the cases where the transferred representation is fixed and fine-tuned respectively. In Section 5 we apply the results and use them to motivate and test a practical approach to weight transfer in neural networks. We conclude in Section 6 and defer more involved proofs to Section 7.\n1See http://code.google.com/archive/p/ word2vec, http://caffe.berkeleyvision.org/ model_zoo and http://vlfeat.org/matconvnet/ pretrained for examples."}, {"heading": "2. Background", "text": "Empirical studies have shown the success of transferring representations between tasks (Donahue et al., 2014; Hoffman et al., 2014; Girshick et al., 2014; Socher et al., 2013; Bansal et al., 2014). Word embeddings learned on a source task have been shown (Qu et al., 2015) to perform better than unigram features on target tasks such as part of speech tagging, and comparably or better than embeddings finetuned on the target task. Yosinski et al. (2014) learned neural network weights using half of the ImageNet classes, and then learned the other classes with a neural network initialized with these weights, finding a benefit compared to random initialization only with target task fine-tuning. The transfer of representations, both with and without finetuning, is widely and successfully used.\nPrevious work on domain adaptation (Ben-David et al., 2010; Mansour et al., 2009; Germain et al., 2013) has considered learning a hypothesis h on S and re-using it on T , bounding RT (h) using RS(h) (measured with labeled source data) and some notion of similarity between PS and PT (measured with additional unlabeled target data). Such results motivate a joint optimization using labeled source and unlabeled target data (Ganin et al., 2016; Long et al., 2015) to learn separate mappings fS , fT : X \u2192 Z which make the induced distributions in the feature space Z similar, and a hypothesis g : Z \u2192 Y learned from the source labels which can be re-used on T . This approach assumes the tasks become the same if their input distributions can be aligned. We consider a relaxation where the tasks are more weakly related but some representation step can be transferred. We consider learning f : X \u2192 Z on S, re-using it on T , and then learning gT : Z \u2192 Y from a small amount of labeled target data. Given the widespread use of \u2018downloadable\u2019 representations, where f and gT are learned separately and there is no joint optimization over source and target data, this is a realistic setting.\nWork on lifelong learning relates the past performance of a representation over many tasks to its expected future performance. For a representation f \u2208 F we construct G \u25e6 f := {g \u25e6 f : g \u2208 G}. Suppose there is a distribution over tasks, known as an environment. Assume several tasks from this environment have been sampled, and that for each task some hypothesis in G \u25e6 f has been selected and its empirical risk evaluated. Previous work has provided bounds on the difference between the average empirical risk and the expected risk of the best hypothesis in G \u25e6 f for a new task drawn from the environment. Such bounds have been given by measuring the complexity of F and G using covering numbers (Baxter, 2000), a variant of the growth function (Galanti et al., 2016), and a distribution-dependent measure known as Gaussian complexity (Maurer et al., 2016). All of these bounds rely on\nknown past performance on a large number of tasks.2 In practice, however, representations such as neural network weights or word embeddings are often learned using only a single source task, which is the setting we consider."}, {"heading": "3. Representation Fixed by Source Task", "text": "Suppose labeled source data is abundant, labeled target data is scarce, and we believe the tasks share a representation. A natural approach to leveraging the source data is to learn g\u0302S \u25e6 f\u0302 \u2208 H on S, from which we assume we may extract f\u0302 \u2208 F ,3 then conduct empirical risk minimization over G \u25e6 f\u0302 := {g \u25e6 f\u0302 : g \u2208 G} on T yielding g\u0302T \u25e6 f\u0302 . Theorem 1 upper-bounds RT (g\u0302T \u25e6 f\u0302) using four terms: a function \u03c9 measuring a transferrability property obtained analytically from the problem setting, the empirical risk R\u0302S(g\u0302S \u25e6 f\u0302), the generalization error of a hypothesis in H learned from mS samples, and the generalization error of a hypothesis in G learned from mT samples. The value of the theorem is that if \u03c9(R) = O(R), R\u0302S(g\u0302S \u25e6 f\u0302) is a small constant, mS mT and dH dG,4 we improve on the VC dimension-based bound for learning T from scratch by avoiding the generalization error of a hypothesis in H learned from mT samples. Furthermore, we do not settle for bounding RT (g\u0302T \u25e6 f\u0302) in terms of R\u0302T (g\u0302T \u25e6 f\u0302), which may be large. The theorem can be used to select S given\n2Pentina & Lampert (2014) extend this analysis to stochastic hypotheses (i.e. distributions over deterministic hypotheses), where for each task we learn a posterior given a prior and training data. The quality of the prior affects the learner\u2019s performance. The study proposes using source tasks to learn a \u2018hyperposterior\u2019, a distribution over priors which is sampled to give a prior for each task. Such a hyperposterior may focus the learner on a representation shared across tasks. The study gives a PAC-Bayes bound on the expected risk of using a hyperposterior to learn a new task drawn from the environment, in terms of the average empirical risk obtained using the hyperposterior to learn the source tasks.\n3This is not possible with knowledge of g\u0302S \u25e6 f\u0302 alone, but in the case of feedforward neural networks which we focus on, f\u0302 is known if the weights learned on S are known.\n4We have mS mT if labeled source task data is abundant while labeled target task data is scarce, and dH dG if we simplify target task learning by substantially reducing the hypothesis space to be searched.\nseveral options. While we refer to \u03c9 in a general form, we give an example in Section 3.1 and expect that others exist.5\nTheorem 1. Let \u03c9 : R \u2192 R be a non-decreasing function. Suppose PS , PT , hS , hT , f\u0302 , G have the property that \u2200g\u0302S \u2208 G, min\ng\u2208G RT (g \u25e6 f\u0302) \u2264 \u03c9(RS(g\u0302S \u25e6 f\u0302)). Let g\u0302T :=\narg min g\u2208G\nR\u0302T (g \u25e6 f\u0302). Then with probability at least 1 \u2212 \u03b4\nover pairs of training sets for tasks S and T , RT (g\u0302T \u25e6 f\u0302) \u2264 \u03c9(R\u0302S(g\u0302S \u25e6 f\u0302) + 2\n\u221a 2dH log(2emS/dH)+2 log(8/\u03b4)\nmS ) + 4 \u221a\n2dG log(2emT /dG)+2 log(8/\u03b4) mT .\nProof. Let g\u2217T := arg min g\u2208G RT (g \u25e6 f\u0302). With probability at least 1\u2212 \u03b4,\nRT (g\u0302T \u25e6 f\u0302) \u2264 R\u0302T (g\u0302T \u25e6 f\u0302) + 2 \u221a\n2dG log(2emT /dG)+2 log(8/\u03b4) mT\n\u2264 R\u0302T (g\u2217T \u25e6 f\u0302) + 2 \u221a 2dG log(2emT /dG)+2 log(8/\u03b4) mT\n\u2264 RT (g\u2217T \u25e6 f\u0302) + 4 \u221a 2dG log(2emT /dG)+2 log(8/\u03b4) mT\n\u2264 \u03c9(RS(g\u0302S \u25e6 f\u0302)) + 4 \u221a\n2dG log(2emT /dG)+2 log(8/\u03b4) mT\n\u2264 \u03c9(R\u0302S(g\u0302S \u25e6 f\u0302) + 2 \u221a\n2dH log(2emS/dH)+2 log(8/\u03b4) mS ) + 4 \u221a\n2dG log(2emT /dG)+2 log(8/\u03b4) mT .\nUsing m training points and a hypothesis class of VC dimension d, with probability at least 1 \u2212 \u03b4, for all hypotheses h simultaneously, the riskR(h) and empirical risk R\u0302(h)\nsatisfy |R(h)\u2212R\u0302(h)| \u2264 2 \u221a\n2d log(2em/d)+2 log(4/\u03b4) m (Mohri\net al., 2012). ForG this yields the first and third inequalities with probability at least 1 \u2212 \u03b42 . For H , because \u03c9 is nondecreasing, this yields the fifth inequality with probability at least 1 \u2212 \u03b42 . Applying the union bound achieves the desired result. The second inequality is by the definition of g\u0302T and the fourth inequality follows from our assumption."}, {"heading": "3.1. Neural Network Example with Fixed Representation", "text": "In Theorem 2, we give an example of the property required by Theorem 1 which is specific to a particular problem setting. We consider a neural network with a single hidden layer (see Figure 2). We propose transferring the lowerlevel weights (corresponding to f\u0302 ) learned on S, so that only the upper-level weights (corresponding to G) have to be learned on T . We want to show f\u0302 is also useful for T .\n5We define \u03c9 by relating RS(g\u0302S \u25e6 f\u0302) to min g\u2208G RT (g \u25e6 f\u0302), since we expect this may be feasible analytically as in our example in Section 3.1. However, because we only observe R\u0302S(g\u0302S \u25e6 f\u0302), in Theorem 1 we use this to bound RS(g\u0302S \u25e6 f\u0302) and then apply \u03c9.\nTo do this, we assume that some lower-level weights perform well on both tasks, which is clearly a necessary condition for the specific f\u0302 we are transferring to perform well on both tasks. We also assume PS and PT have the relative rotation invariance property and that the upper-level weights have fixed magnitude. This is so that a point x for which f\u0302(x) contributes to the risk on T cannot be \u2018hidden\u2019 from the risk of using f\u0302 on S, either through low PS(x) or low magnitude upper-level weights. Hence RS(g\u0302S \u25e6 f\u0302) reliably indicates the usefulness of f\u0302 on T .\nLetX = Rn and Z = Rk. Let F be the function class such that f(x) = [a(w1 \u00b7 x), . . . , a(wk \u00b7 x)], where wi \u2208 Rn for 1 \u2264 i \u2264 k, a : R \u2192 R is an odd function6 and \u00b7 is the dot product. Let G be the function class such that g(z) = sign(v \u00b7 z), where v \u2208 {\u22121, 1}k. Suppose \u2203f \u2208 F, gS , gT \u2208 G such that max[RS(gS \u25e6f), RT (gT \u25e6f)] \u2264 . Let f\u0302(x) := [a(w\u03021 \u00b7 x), . . . , a(w\u0302k \u00b7 x)]. Given wi and w\u0302i, pick nonzero constants \u03b1i and \u03b2i such that ||wi|| = ||\u03b1iw\u0302i \u2212 \u03b2iwi|| and wi \u00b7 (\u03b1iw\u0302i \u2212 \u03b2iwi) = 0. Let M be a 2k\u00d7nmatrix with rowsw1, \u03b11w\u03021\u2212\u03b21w1, . . . , wk, \u03b1kw\u0302k\u2212 \u03b2kwk. Suppose M is full rank.7 Suppose \u2200x, x\u2032 such that ||Mx|| = ||Mx\u2032||, PT (x) \u2264 cPS(x\u2032) for some c \u2265 1, which we call relative rotation invariance and implies PS and PT have the same support. If M is an orthogonal matrix then \u2200x, x\u2032 such that ||x|| = ||x\u2032||, PT (x) \u2264 cPS(x\u2032).8\nTheorem 2. Let \u03c9(R) := cR + (1 + c). Then \u2200g\u0302S \u2208 G, min g\u2208G\nRT (g \u25e6 f\u0302) \u2264 \u03c9(RS(g\u0302S \u25e6 f\u0302)). 6i.e. a(\u2212x) = \u2212a(x). Examples are tanh, sign and identity. 7To see that this condition is necessary, consider the following example where M is not full rank. Let n = 4, k = 2, hS = sign(x1) and hT = sign(x2). For f(x) = [x1 + x2, x1 \u2212 x2], gS(z) = sign(z1 + z2) and gT (z) = sign(z1 \u2212 z2), we have RS(gS \u25e6 f) = RT (gT \u25e6 f) = 0. On S we learn f\u0302(x) = [x1 + x3, x1\u2212x3] and g\u0302S(z) = sign(z1 +z2), so thatRS(g\u0302S \u25e6 f\u0302) = 0 but in general min\ng\u2208G RT (g \u25e6 f\u0302) > 0 since f\u0302 ignores x2.\n8For example, PS and PT are spherical Gaussians. For a zeromean multivariate Gaussian distribution this is achieved by the whitening transformation x \u2192 \u039b\u22121/2UTx, where the columns of U and entries of the diagonal matrix \u039b are the eigenvectors and eigenvalues of the distribution\u2019s covariance matrix respectively."}, {"heading": "4. Representation Fine-Tuned on Target Task", "text": "Consider learning g\u0302S \u25e6 f\u0302 on S, and then using f\u0302 and RS(g\u0302S \u25e6 f\u0302) to find F\u0302 \u2286 F , as in Figure 1. Let h\u0303g\u25e6f be a stochastic hypothesis (i.e. a distribution over H) associated with g \u25e6 f (e.g. g \u25e6 f is the mode of h\u0303g\u25e6f ). We propose learning T with the hypothesis class H\u0303G\u25e6F\u0302 := {h\u0303g\u25e6f : f \u2208 F\u0302 , g \u2208 G} and the prior h\u0303g\u0302S\u25e6f\u0302 . Learning T from scratch we assume that we would instead use H\u0303G\u25e6F := {h\u0303g\u25e6f : f \u2208 F, g \u2208 G} and some fixed prior h\u03030 \u2208 H\u0303G\u25e6F . Let RT (h\u0303) := Ex\u223cPT ,h\u223ch\u0303[hT (x) 6= h(x)] and compute R\u0302T (h\u0303) on the training set distribution of T .\nIn Theorem 3 we show that if F\u0302 is \u2018small enough\u2019 so that all h\u0303 \u2208 H\u0303G\u25e6F\u0302 have a small KL divergence from h\u0303g\u0302S\u25e6f\u0302 , we may apply a PAC-Bayes bound to the generalization error of hypotheses in H\u0303G\u25e6F\u0302 involving four terms: a function \u03c9 measuring a transferrability property, the empirical risk R\u0302S(g\u0302S \u25e6 f\u0302), the generalization error of a hypothesis in H learned from mS points, and a weak dependence on mT . The value of the theorem is that if \u03c9(R) = O(R), R\u0302S(g\u0302S\u25e6f\u0302) is a small constant, andmS mT , we improve on the PAC-Bayes bound for H\u0303G\u25e6F and h\u03030.9 F\u0302 is useful if it is also \u2018large enough\u2019 in the sense that \u2203h\u0303gT \u25e6f \u2208 H\u0303G\u25e6F\u0302 such that RT (h\u0303gT \u25e6f ) \u2264 . Here \u03c9 quantifies how large the F\u0302 we search on T must be in order to be \u2018large enough\u2019, in terms of RS(g\u0302S \u25e6 f\u0302). While in general such an F\u0302 and \u03c9 may not exist, we give an example in Section 4.1.\nTheorem 3. Let \u03c9 : R \u2192 R be non-decreasing. Suppose given f\u0302 \u2208 F and RS(g\u0302S \u25e6 f\u0302) estimated from S, it is possible to construct F\u0302 with the property \u2200h\u0303 \u2208 H\u0303G\u25e6F\u0302 , KL(h\u0303||h\u0303g\u0302S\u25e6f\u0302 ) \u2264 \u03c9(RS(g\u0302S \u25e6 f\u0302)). Then with probability at least 1 \u2212 \u03b4 over pairs of training sets for tasks S and T , \u2200h\u0303 \u2208 H\u0303G\u25e6F\u0302 , RT (h\u0303) \u2264 R\u0302T (h\u0303) +\u221a\n\u03c9(R\u0302S(g\u0302S\u25e6f\u0302)+2 \u221a 2dH log(2emS/dH )+2 log(8/\u03b4)\nmS )+log 2mT /\u03b4\n2(mT\u22121) .\nProof. With probability at least 1\u2212 \u03b4,\nRT (h\u0303) \u2264 R\u0302T (h\u0303) + \u221a KL(h\u0303||h\u0303g\u0302S\u25e6f\u0302 )+log 2mT /\u03b4 2(mT\u22121)\n\u2264 R\u0302T (h\u0303) + \u221a\n\u03c9(RS(g\u0302S\u25e6f\u0302))+log 2mT /\u03b4 2(mT\u22121) .\nThe first inequality holds with probability at least 1 \u2212 \u03b42 (Shalev-Shwartz & Ben-David, 2014). The second inequality holds by assumption. Furthermore, RS(g\u0302S \u25e6 f\u0302) \u2264 R\u0302S(g\u0302S \u25e6 f\u0302) + 2 \u221a 2dH log(2emS/dH)+2 log(8/\u03b4)\nmS with prob-\nability at least 1 \u2212 \u03b42 (Mohri et al., 2012) and \u03c9 is nondecreasing. The result follows from the union bound.\n9Using the restricted deterministic hypothesis class G \u25e6 F\u0302 := {h : \u2203f \u2208 F\u0302 , g \u2208 G such that h = g \u25e6 f} and a VC dimensionbased bound may not improve on H , since possibly dG\u25e6F\u0302 = dH ."}, {"heading": "4.1. Neural Network Example with Fine-Tuning", "text": "We transfer and fine-tune weights in a feedforward neural network with one hidden layer to instantiate the property required by Theorem 3. We learn a deterministic hypothesis of this type on S and obtain k estimated lowerlevel weight vectors w\u0302i. Learning T we now consider only lower-level weights near w\u0302i, corresponding to F\u0302 . On T we learn a stochastic hypothesis formed by taking a deterministic network and adding independent sources of spherical Gaussian noise to the lower-level weights and sign-flipping noise to the upper-level weights. The KL divergence between two of the stochastic hypotheses is expressed using the angles between their lower-level weights10 and a quantity computable from their upper-level weights.\nWe want to prove that we can construct such an F\u0302 to successfully learn T . To do this, we assume some lower-level weights wi perform well on both S and T . We make F\u0302 \u2018small enough\u2019 by only including lower-level weights with small angles to w\u0302i, and \u2018large enough\u2019 by using the risk observed using w\u0302i on S to provide an upper bound on the angle between each pair wi and w\u0302i. Our assumptions ensure that poor w\u0302i cannot be \u2018hidden\u2019 from the risk on S, either through low PS density in the region of disagreement between wi and w\u0302i, or through low magnitude higher-level weights. Hence we know that searching F\u0302 will include wi.\nLet X = Rn and Z = Rk, where k is odd. Let F be the function class such that f(x) = [sign(w1 \u00b7 x), . . . , sign(wk \u00b7 x)], where wi \u2208 Rn for 1 \u2264 i \u2264 k. Let G be the function class such that g(z) = sign(v \u00b7 z), where v \u2208 {\u22121, 1}k. Let Bv be a distribution on {\u22121, 1}k such that for\nv\u2032 \u223c Bv , Pr(v\u2032) = k\u220f j=1 p1(v \u2032 j=vj)(1 \u2212 p)1(v \u2032 j=\u2212vj), where p \u2208 [0.5, 1]. Let h\u0303g\u25e6f := g\u2032 \u25e6f \u2032 such that v\u2032, w\u20321, . . . , w\u2032k \u223c\nBv k\u220f i=1 N (wi, \u03c32I). Suppose \u2203f \u2208 F, gS , gT \u2208 G such that max[RS(gS \u25e6 f), RT (h\u0303gT \u25e6f )] \u2264 . Let f\u0302(x) := [sign(w\u03021 \u00b7 x), . . . , sign(w\u0302k \u00b7 x)], \u03b8(wi, w\u0302i) be the angle between wi and w\u0302i, and assume \u2200i, ||w\u0302i|| = 1. Define M as in Section 3.1. Let PS have the rotation invariance property \u2200x, x\u2032 such that ||Mx|| = ||Mx\u2032||, PS(x) \u2264 cPS(x\u2032) for some c \u2265 1.\nTheorem 4. Given f\u0302 and RS(g\u0302S \u25e6 f\u0302) estimated from S, let \u03b8max := \u03c0 \u221a 2(k \u2212 1)c(RS(g\u0302S \u25e6 f\u0302) + ) and F\u0302 := {f \u2208 F : \u2200i, ||wi|| = 1 \u2227 |\u03b8(wi, w\u0302i)| \u2264 \u03b8max}. Let \u03c9(R) := k\u03c32 [1\u2212cos \u03b8max]+k[2p\u22121+(1\u2212p) k] log2 p 1\u2212p . Then \u2203h\u0303gT \u25e6f \u2208 H\u0303G\u25e6F\u0302 such that RT (h\u0303gT \u25e6f ) \u2264 and \u2200h\u0303 \u2208 H\u0303G\u25e6F\u0302 , KL(h\u0303||h\u0303g\u0302S\u25e6f\u0302 ) \u2264 \u03c9(RS(g\u0302S \u25e6 f\u0302)).\n10Assuming that the lower-level weight vectors are of fixed magnitude, which is no loss of model expressiveness since we use the sign activation function at the hidden layer."}, {"heading": "5. Applications", "text": "We show the utility of the risk bounds, and present a novel technique and experiments motivated by our theorems."}, {"heading": "5.1. Using the Risk Bounds", "text": "The results described yield tighter bounds on risk when transferring representations from S, compared to learning T from scratch. Examples are shown in Figure 3.11\nWe set \u03b4 = 0.05. For the top part, we use the example from Section 3.1 and set n = 10, k = 5. Learning T from scratch with H , we use the bound from Mohri et al. (2012) used previously. The VC dimension of a network of |E| edges using the sign activation is O(|E| log |E|) (Shalev-Shwartz & Ben-David, 2014), where in our case |E| = nk+k. We use dH = |E| log |E| in the chart. Transferring a representation from S to T without fine-tuning, we consider the limit \u2192 0, R\u0302S(g\u0302S \u25e6 f\u0302) \u2192 0, mS \u2192 \u221e, and hence \u03c9(\u00b7) \u2192 0 by Theorem 2. Furthermore, dG \u2264 k since G is finite and hence dG \u2264 log2 |G| (Shalev-Shwartz & Ben-David, 2014). We use the bound from Theorem 1.\nFor the bottom part, we use the example from Section 4.1 and set \u03c32 = 110 , k = 499, p = 2 3 . Learning T from scratch we use the stochastic hypothesis class {h\u0303g\u25e6f : f \u2208 F such that \u2200i||wi|| = 1, g \u2208 G} and a prior h\u03030 where \u2200i wi = 0 and v \u2208 {\u22121, 1}k is arbitrary.12 Hence we have the bound KL(h\u0303||h\u03030) \u2264 10k + k3 , which becomes tight for large k. We apply the PAC-Bayes bound (ShalevShwartz & Ben-David, 2014) used previously. Transferring a representation from S and fine-tuning on T , we consider the limit \u2192 0, R\u0302S(g\u0302S \u25e6 f\u0302) \u2192 0, mS \u2192 \u221e. We have KL(h\u0303||h\u0303g\u0302S\u25e6f\u0302 ) \u2264 k 3 by Theorem 4. We use the bound from Theorem 3."}, {"heading": "5.2. Fine-Tuning through Regularization", "text": "We relax the hard constraint on F\u0302 from Section 4.1 by using a modified loss function, which we find performs better in practice. Let yi and y\u0302i be the label and prediction respectively for the ith training point. In a fully-connected feedforward neural network with l layers of weights, let W (j) be the jth weight matrix, W\u0302 (j) be its estimate from S (excluding weights for bias units in both cases), and ||\u00b7||2 be the entry-wise 2 norm. A typical loss function (1) used for training is composed of the sum of training set log loss and L2 regularization on the weights.\n11Note that VC dimension risk bounds are known for being rather loose, while PAC-Bayesian bounds are tighter and hence yield non-trivial results in higher dimensions with fewer samples.\n12This class is as expressive as H\u0303G\u25e6F but by setting ||wi|| = 1 the KL divergence of all hypotheses from any prior is bounded, allowing a fair comparison to H\u0303G\u25e6F\u0302 . The choice of h\u03030 minimizes worst case KL divergence to a hypothesis in the class.\nm\u2211\ni=1 [\u2212yi log y\u0302i \u2212 (1\u2212 yi) log(1\u2212 y\u0302i)] + \u03bb 2\nl\u2211\nj=1 (||W (j)||22)\n(1)\nWe replace the regularization penalty with (2).13\nl\u2211 j=1 [ \u03bb1(j) 2 ||W (j) \u2212 W\u0302 (j)||22 + \u03bb2(j) 2 ||W (j)||22] (2) This penalizes estimates of W far from the representation learned on S. Since we expect the tasks to share a lowlevel representation (e.g. edge detectors for vision, word embeddings for text) but be distinct at higher levels (e.g. image components for vision, topics for text), we set \u03bb1(\u00b7) to be a decreasing function, while \u03bb2(\u00b7) controls standard L2 regularization. The technique is novel to our knowledge, although other approaches to transferring regularization between tasks exist (Evgeniou & Pontil, 2004; Raina et al., 2006; Argyriou et al., 2008; Ghifary et al., 2014)."}, {"heading": "5.3. Experiments", "text": "We experiment on basic image and text classification tasks.14 We show that learning algorithms motivated by our theoretical results can help to overcome a scarcity of labeled target task data. Note that we do not replicate the conditions specified in our theorems, nor do we attempt extensive tuning to achieve state-of-the-art performance.\n13Basing our approach on (1), we follow the convention that weights connected to bias units are excluded from the regularization penalty. However, the inclusion of these weights in the ||W (j) \u2212 W\u0302 (j)|| term of (2) is a plausible variant.\n14The MNIST and 20 Newgroups datasets are available at http://yann.lecun.com/exdb/mnist and http:// qwone.com/\u02dcjason/20Newsgroups respectively.\nWe randomly partition label classes into sets S+ and S\u2212, where |S+| = |S\u2212|.15 We construct T+ by randomly picking from S+ up to \u03b3 :=\n|S+\u2229T+| |S+| , then randomly picking\nfrom S\u2212 such that |T+| = |T\u2212|. We let S be the task of distinguishing between S+ and S\u2212 and T be that of distinguishing T+ and T\u2212. Constructing S+ and T+ as disjunctions of classes means that the class labels are a perfect representation shared between S and T .\nWe compare the accuracy on T of four options:\n\u2022 learn T from scratch (BASE)\n\u2022 transfer f\u0302 from S, fine-tune f and train g on T using (2) (FINE-TUNE f\u0302 )\n\u2022 transfer f\u0302 from S and fix, train g on T (FIX f\u0302 )16\n\u2022 transfer g\u0302S \u25e6 f\u0302 from S and fix (FIX g\u0302S \u25e6 f\u0302 ).17\nWe use \u03bb1(1) = \u03bb2(2) = \u03bb := 1,18 \u03bb1(2) = \u03bb2(1) = 0, mT = 500 and the sigmoid activation function. For MNIST we use raw pixel intensities, a 784 \u00d7 50 \u00d7 1 network andmS = 50000. For NEWSGROUPS we use TF-IDF weighted counts of most frequent words, a 2000 \u00d7 50 \u00d7 1 network and mS = 15000. We use conjugate gradient optimization with 200 iterations.\nThe results are shown in Table 1.19 When the tasks are nonidentical, FINE-TUNE f\u0302 is mostly the strongest but performs better on MNIST. FIX f\u0302 outperforms BASE when \u03b3 \u2265 0.8 and hence the tasks are similar. While FIX f\u0302 outperforms FIX g\u0302S \u25e6 f\u0302 when the tasks are non-identical on MNIST, on NEWSGROUPS there is no evidence of benefit. When the tasks are identical, FIX g\u0302S \u25e6 f\u0302 is the strongest.\nIt appears that learning an MNIST digit requires a dense weight vector and so W\u0302 (1) tends to encode single digits, which helps transferrability. However, it appears that since we may learn a newsgroup with a sparse weight vector, W\u0302 (1) tends to encode disjunctions of newsgroups which somewhat reduces transferrability. When transferring representations does work, fine-tuning using the regularization penalty proposed in (2) improves performance.\n15For MNIST there are 10 label classes and for 20 Newgroups there are 20. In both cases the classes are approximately balanced. Note that we ignore the hierarchical structure of the 20 Newsgroups classes, which likely contributes to the lower accuracies reported for all methods for this dataset relative to MNIST.\n16i.e. logistic regression with L2 regularization and f\u0302 fixed. 17Used to isolate the benefit of transferring f\u0302 rather than g\u0302S \u25e6 f\u0302 . 18We explored tuning \u03bb to lift the performance of BASE on MNIST, but found that the results did not materially improve. Potentially \u03bb1(j) and \u03bb2(j) in (2) could be tuned with cross validation on the target task.\n19For \u03b3 = 1, hS = hT . We do not consider \u03b3 < 0.5, since that is equivalent to 1\u2212\u03b3 with the definitions of T+ and T\u2212 swapped."}, {"heading": "6. Conclusion", "text": "We developed sufficient conditions for the successful transfer of representations both with and without fine-tuning. This is a step towards a principled explanation of the empirical success achieved by such techniques. A promising direction for future work is generalizing the neural network architectures considered (e.g. using multiple hidden layers) and relaxing the distributional assumptions required. Furthermore, in the fine-tuning case it may be possible to upper bound the target task generalization error of hypotheses in G \u25e6 F\u0302 := {h : \u2203f \u2208 F\u0302 , g \u2208 G such that h = g \u25e6 f} using another measure such as the Rademacher complexity of G \u25e6 F\u0302 , eliminating the need for stochastic hypotheses.\nWe proposed a novel form of regularization for neural network training motivated by our theoretical results, which penalizes divergence from source task weights and is stricter for lower-level weights. We validated this technique through applications to image and text classification. Future directions include experiments on more challenging tasks using deeper and more tailored network architectures (e.g. convolutional neural networks)."}, {"heading": "7. Additional Proofs", "text": "We provide complete proofs of Theorems 2 and 4. For brevity, we drop the explicit dependence of f , f\u0302 , hS and hT on x in our notation where the meaning is clear."}, {"heading": "7.1. Proof of Theorem 2", "text": "Proof. Let gS(z) := sign(vS \u00b7 z), gT (z) := sign(vT \u00b7 z), g\u0302S(z) := sign(v\u0302S \u00b7 z), g\u0302T (z) := sign(d \u2217 v\u0302S \u00b7 z), where d := vS \u2217vT \u2208 {\u22121, 1}k and \u2217 is the elementwise product. It is sufficient to showRT (g\u0302T \u25e6f\u0302) \u2264 cRS(g\u0302S\u25e6f\u0302)+ (1+c).\nRT (g\u0302T \u25e6 f\u0302)\n= Prx\u223cPT (hT d \u2217 v\u0302S \u00b7 f\u0302 \u2264 0)\n\u2264 Prx\u223cPT (hT d \u2217 vS \u00b7 f \u2264 0, d \u2217 vS \u00b7 fd \u2217 v\u0302S \u00b7 f\u0302 \u2265 0) + Prx\u223cPT (hT d \u2217 vS \u00b7 f \u2265 0, d \u2217 vS \u00b7 fd \u2217 v\u0302S \u00b7 f\u0302 \u2264 0)\n\u2264 Prx\u223cPT (hT d \u2217 vS \u00b7 f \u2264 0)+ Prx\u223cPT (d \u2217 vS \u00b7 fd \u2217 v\u0302S \u00b7 f\u0302 \u2264 0)\n\u2264 + Prx\u223cPT (d \u2217 vS \u00b7 fd \u2217 v\u0302S \u00b7 f\u0302 \u2264 0)\n\u2264 + cPrx\u223cPS (vS \u00b7 fv\u0302S \u00b7 f\u0302 \u2264 0)\n\u2264 + c[Prx\u223cPS (hS v\u0302S \u00b7 f\u0302 \u2264 0, hSvS \u00b7 f \u2265 0)+ Prx\u223cPS (hS v\u0302S \u00b7 f\u0302 \u2265 0, hSvS \u00b7 f \u2264 0)]\n\u2264 + c[Prx\u223cPS (hS v\u0302S \u00b7 f\u0302 \u2264 0) +Prx\u223cPS (hSvS \u00b7 f \u2264 0)]\n\u2264 cRS(g\u0302S \u25e6 f\u0302) + (1 + c).\nThe third and final inequalities are due to the shared representation assumption in the problem statement. The fourth inequality holds by Lemma 1. The remaining lines apply simple rules of probability.\nLemma 1. Suppose \u2200x, x\u2032 such that ||Mx|| = ||Mx\u2032||, PT (x) \u2264 cPS(x\u2032). Let f, f\u0302 \u2208 F , v, v\u0302, d \u2208 {\u22121, 1}k. Then Prx\u223cPT (d\u2217v \u00b7fd\u2217 v\u0302 \u00b7 f\u0302 \u2264 0) \u2264 cPrx\u223cPS (v \u00b7 fv\u0302 \u00b7 f\u0302 \u2264 0).\nProof. Suppose there is an invertible map Rn \u2192 Rn yielding x\u2032 on input x, such that \u2200x, ||Mx|| = ||Mx\u2032|| and d \u2217 v \u00b7 f(x)d \u2217 v\u0302 \u00b7 f\u0302(x) = v \u00b7 f(x\u2032)v\u0302 \u00b7 f\u0302(x\u2032). Then the result follows since PT (x) \u2264 cPS(x\u2032) by assumption. Furthermore, if M is an orthogonal matrix, ||x|| = ||x\u2032||.\nSuch a map is x\u2032 := (MTM)\u22121MT d\u0303 \u2217 (Mx), where d\u0303 := [d1, d1, . . . , dk, dk]. We have \u2200i, wi \u00b7 x\u2032 = diwi \u00b7 x and (\u03b1iw\u0302i\u2212\u03b2iwi)\u00b7x\u2032 = di(\u03b1iw\u0302i\u2212\u03b2iwi)\u00b7x, and hence w\u0302i \u00b7x\u2032 = diw\u0302i \u00b7 x for \u03b1i, \u03b2i 6= 0. Therefore:\nd \u2217 v \u00b7 f(x)d \u2217 v\u0302 \u00b7 f\u0302(x)\n= v \u00b7 d \u2217 f(x)v\u0302 \u00b7 d \u2217 f\u0302(x)\n= v \u00b7 f(x\u2032)v\u0302 \u00b7 d \u2217 f\u0302(x)\n= v \u00b7 f(x\u2032)v\u0302 \u00b7 f\u0302(x\u2032).\nThe first equality is a property of the elementwise and dot products. For the second equality, a(wi\u00b7x\u2032) = a(diwi\u00b7x) = dia(wi \u00b7 x) since a is an odd function. Similarly, for the third equality a(w\u0302i \u00b7 x\u2032) = a(diw\u0302i \u00b7 x) = dia(w\u0302i \u00b7 x)."}, {"heading": "7.2. Proof of Theorem 4", "text": "Proof of \u2203h\u0303gT \u25e6f \u2208 H\u0303G\u25e6F\u0302 such that RT (h\u0303gT \u25e6f ) \u2264 . Recall that wi are the weight vectors for f and w\u0302i are those for f\u0302 . Observe that for any wi such that wi \u00b7 w\u0302i < 0, we have \u2212wi \u00b7 w\u0302i > 0 and \u2212visign(\u2212wi \u00b7 x) = visign(wi \u00b7 x). Combining this with the assumption\nmin f\u2208F,gS ,gT\u2208G max[RS(gS \u25e6 f), RT (gT \u25e6 f)] \u2264 , we conclude \u2203f \u2208 F, gS , gT \u2208 G such that \u2200i, wi \u00b7 w\u0302i \u2265 0 and max[RS(gS \u25e6 f), RT (h\u0303gT \u25e6f )] \u2264 .\nLet gS(z) := sign(vS \u00b7 z) and g\u0302S(z) := sign(v\u0302S \u00b7 z). Let P be a rotation invariant distribution for c = 1. To prove h\u0303gT \u25e6f \u2208 H\u0303G\u25e6F\u0302 , by the definition of H\u0303G\u25e6F\u0302 it is sufficient to show \u2200i, |\u03b8(wi, w\u0302i)| \u2264 \u03c0 \u221a 2(k \u2212 1)c(RS(g\u0302S \u25e6 f\u0302) + ).\nmax i |\u03b8(wi,w\u0302i)|\n\u03c0 \u221a 2(k\u22121)\n\u2264 Prx\u223cP (vS \u00b7 fvS \u00b7 f\u0302 \u2264 0)\n\u2264 Prx\u223cP (vS \u00b7 fv\u0302S \u00b7 f\u0302 \u2264 0)\n\u2264 cPrx\u223cPS (vS \u00b7 fv\u0302S \u00b7 f\u0302 \u2264 0)\n\u2264 c[Prx\u223cPS (hSvS \u00b7 f \u2264 0, hS v\u0302S \u00b7 f\u0302 \u2265 0)+ Prx\u223cPS (hSvS \u00b7 f \u2265 0, hS v\u0302S \u00b7 f\u0302 \u2264 0)]\n\u2264 c[Prx\u223cPS (hSvS \u00b7 f \u2264 0) + Prx\u223cPS (hS v\u0302S \u00b7 f\u0302 \u2264 0)]\n\u2264 c[ +RS(g\u0302S \u25e6 f\u0302)].\nThe first inequality holds by Lemma 2. The second inequality holds by Lemma 3, using the fact \u2200i, wi \u00b7 w\u0302i \u2265 0. The third inequality uses the rotation invariance of PS . The following two lines use basic laws of probability. The final inequality uses the assumption RS(gS \u25e6 f) \u2264 .\nProof of \u2200h\u0303 \u2208 H\u0303G\u25e6F\u0302 ,KL(h\u0303||h\u0303g\u0302S\u25e6f\u0302 ) \u2264 \u03c9(RS(g\u0302S \u25e6 f\u0302)). For any h\u0303g\u25e6f \u2208 H\u0303G\u25e6F\u0302 , KL(h\u0303g\u25e6f ||h\u0303g\u0302S\u25e6f\u0302 ) = k\u2211 i=1 [KL(N (wi, \u03c32I)||N (w\u0302i, \u03c32I))] +KL(Bv||Bv\u0302S ).\nThe KL divergence of a product distribution is the sum of the KL divergences of its component distributions. We upper bound both terms and apply the definition of \u03c9. k\u2211 i=1 KL(N (wi, \u03c32I)||N (w\u0302i, \u03c32I)) = 12\u03c32 k\u2211 i=1 ||wi \u2212 w\u0302i||2 = 12\u03c32 k\u2211 i=1 (||wi||2 + ||w\u0302i||2 \u2212 2||wi||||w\u0302i|| cos |\u03b8(wi, w\u0302i)|) = 1\u03c32 k\u2211 i=1 (1\u2212 cos |\u03b8(wi, w\u0302i)|)\n\u2264 k\u03c32 [1\u2212 cos(\u03c0 \u221a 2(k \u2212 1)c(RS(g\u0302S \u25e6 f\u0302) + ))].\nThe first equality uses the KL divergence of Gaussian distributions. The second equality uses the law of cosines. The third equality is because \u2200i, ||wi|| = ||w\u0302i|| = 1 by construction. The inequality follows by the definition of F\u0302 and the fact that 1\u2212 cos |\u03b8| is non-decreasing for |\u03b8| \u2208 [0, \u03c0].\nKL(Bv||Bv\u0302S ) \u2264 k\u2211 i=1 ( k i ) pi(1\u2212 p)k\u2212i log2 pi(1\u2212p)k\u2212i (1\u2212p)ipk\u2212i\n= k[2p\u2212 1 + (1\u2212 p)k] log2 p 1\u2212p .\nThe first inequality uses the definition of Bv to express KL(Bv||Bv\u0302S ). The equality is a simplification.\nLemma 2. Suppose k is odd, v \u2208 {\u22121, 1}k, f, f\u0302 \u2208 F such that \u2200i, wi \u00b7 w\u0302i \u2265 0 and P is rotation invariant with c = 1. Then max i |\u03b8(wi,w\u0302i)|\n\u03c0 \u221a 2(k\u22121) \u2264 Prx\u223cP (v \u00b7 fv \u00b7 f\u0302 \u2264 0).\nProof. Let v\u2212j := [v1, . . . , vj\u22121, vj+1, . . . , vk] and define f\u2212j and f\u0302\u2212j similarly. Let Pr(\u00b7) := Prx\u223cP (\u00b7).\nPr(v \u00b7 fv \u00b7 f\u0302 \u2264 0)\n\u2265 Pr(v \u00b7 fv \u00b7 f\u0302 < 0)\n\u2265 Pr(v\u2212j \u00b7 f\u2212j = 0)Pr(v \u00b7 fv \u00b7 f\u0302 < 0|v\u2212j \u00b7 f\u2212j = 0)\n= Pr(v\u2212j \u00b7 f\u2212j = 0) Pr(vjfjv\u2212j \u00b7 f\u0302\u2212j + fj f\u0302j < 0|v\u2212j \u00b7 f\u2212j = 0)\n= Pr(v\u2212j \u00b7 f\u2212j = 0) [Pr(vjfjv\u2212j \u00b7 f\u0302\u2212j < \u22121, fj f\u0302j = 1|v\u2212j \u00b7 f\u2212j = 0)+ Pr(vjfjv\u2212j \u00b7 f\u0302\u2212j < 1, fj f\u0302j = \u22121|v\u2212j \u00b7 f\u2212j = 0)]\n\u2265 Pr(v\u2212j \u00b7 f\u2212j = 0) [Pr(vjfjv\u2212j \u00b7 f\u0302\u2212j < \u22121, fj f\u0302j = \u22121|v\u2212j \u00b7 f\u2212j = 0)+ Pr(vjfjv\u2212j \u00b7 f\u0302\u2212j < 1, fj f\u0302j = \u22121|v\u2212j \u00b7 f\u2212j = 0)]\n= Pr(v\u2212j \u00b7 f\u2212j = 0) [Pr(vjfjv\u2212j \u00b7 f\u0302\u2212j < \u22121, fj f\u0302j = \u22121|v\u2212j \u00b7 f\u2212j = 0)+ Pr(vjfjv\u2212j \u00b7 f\u0302\u2212j > \u22121, fj f\u0302j = \u22121|v\u2212j \u00b7 f\u2212j = 0)]\n= Pr(v\u2212j \u00b7 f\u2212j = 0)Pr(fj f\u0302j = \u22121|v\u2212j \u00b7 f\u2212j = 0)\n= Pr(v\u2212j \u00b7 f\u2212j = 0)Pr(fj f\u0302j = \u22121) = (k\u22121 k\u22121 2 ) ( 12 ) k\u22121 |\u03b8(wj ,w\u0302j)| \u03c0\n\u2265 2 k\u22121\u221a\n2(k\u22121) ( 12 ) k\u22121 |\u03b8(wj ,w\u0302j)| \u03c0\n\u2265 max i |\u03b8(wi,w\u0302i)|\n\u03c0 \u221a 2(k\u22121) .\nThe third inequality follows since P is rotation invariant and wj \u00b7 w\u0302j \u2265 0. The third and fifth equalities use rotation invariance. The final equality uses rotation invariance and the fact that k is odd. The fourth inequality is a standard lower bound for the central binomial coefficient. The other lines use basic simplifications and laws of probability.\nLemma 3. Suppose k is odd, v, v\u0302 \u2208 {\u22121, 1}k, f, f\u0302 \u2208 F such that \u2200i, wi \u00b7 w\u0302i \u2265 0 and P is rotation invariant with c = 1. Then Prx\u223cP (v \u00b7 fv \u00b7 f\u0302 \u2264 0) \u2264 Prx\u223cP (v \u00b7 fv\u0302 \u00b7 f\u0302 \u2264 0). Proof. Let Pr(\u00b7) := Prx\u223cP (\u00b7) and E[\u00b7] := Ex\u223cP [\u00b7]. Let Pr(f\u0303) := Prx\u223cP ([f1(x)f\u03021(x), . . . , fk(x)f\u0302k(x)] = f\u0303). Let d := v\u0302 \u2217 v and \u2206(x) := 1(v \u00b7 f(x)v\u0302 \u00b7 f\u0302(x) \u2264 0) \u2212 1(v \u00b7 f(x)v \u00b7 f\u0302(x) \u2264 0). Assume v\u0302 6= v (if v\u0302 = v then\nthe lemma clearly holds). Let a(f\u0303) := k\u2211 i=1 1(f\u0303i = 1) and let l := min i:di=\u22121 i. Let F\u0303 := {f\u0303 \u2208 {\u22121, 1}k : a(f\u0303) > a(d \u2217 f\u0303) \u2228 (a(f\u0303) = a(d \u2217 f\u0303) \u2227 f\u0303l = 1)}.\nLet \u03a6(a) := 1 2k\u22121 bk/2c\u2211 b=0 b\u2211 j=da/2+b/2\u2212k/4e ( a j )( k\u2212a b\u2212j ) . The\nterm b counts coordinates where vif\u0302i = sign(v \u00b7 f), while j counts those where vifi = sign(v \u00b7 f) and fi = f\u0302i.\nPr(v \u00b7 fv\u0302 \u00b7 f\u0302 \u2264 0)\u2212 Pr(v \u00b7 fv \u00b7 f\u0302 \u2264 0)\n= E[1(v \u00b7 fv\u0302 \u00b7 f\u0302 \u2264 0)]\u2212 E[1(v \u00b7 fv \u00b7 f\u0302 \u2264 0)]\n= E[\u2206] = \u2211\u0303 f\u2208F\u0303 Pr(f\u0303)E[\u2206|f\u0303 ] + Pr(d \u2217 f\u0303)E[\u2206|d \u2217 f\u0303 ]\n= \u2211\u0303 f\u2208F\u0303 [Pr(f\u0303)\u2212 Pr(d \u2217 f\u0303)]E[\u2206|f\u0303 ]\n= \u2211\u0303 f\u2208F\u0303 [Pr(f\u0303)\u2212 Pr(d \u2217 f\u0303)]\n[Pr(v \u00b7 fv \u00b7 f\u0302 \u2264 0|d \u2217 f\u0303)\u2212 Pr(v \u00b7 fv \u00b7 f\u0302 \u2264 0|f\u0303)] = \u2211\u0303 f\u2208F\u0303 [Pr(f\u0303)\u2212 Pr(d \u2217 f\u0303)][\u03a6(a(d \u2217 f\u0303))\u2212 \u03a6(a(f\u0303))]\n\u2265 0.\nThe second equality uses linearity of expectation. The third equality uses the law of total expectation and the definition of F\u0303 .\nThe fourth equality holds since E[\u2206|d \u2217 f\u0303 ] = \u2211 f\u2208{\u22121,1}k Pr(f |d \u2217 f\u0303)E[\u2206|d \u2217 f\u0303 , f ]\n= \u2212 \u2211\nf\u2208{\u22121,1}k Pr(f |d \u2217 f\u0303)E[\u2206|f\u0303 , f ]\n= \u2212 \u2211\nf\u2208{\u22121,1}k Pr(f |f\u0303)E[\u2206|f\u0303 , f ] = \u2212E[\u2206|f\u0303 ] due to the\nrotation invariance of P .\nThe fifth equality holds by expanding \u2206, linearity of expectation, and a similar argument to the previous equality to show Pr(v \u00b7 fv\u0302 \u00b7 f\u0302 \u2264 0|f\u0303) = Pr(v \u00b7 fv \u00b7 f\u0302 \u2264 0|d \u2217 f\u0303).\nThe sixth equality holds by the rotation invariance of P and the fact that k is odd.\nFor the final inequality, the right hand term is non-negative since a(f\u0303) \u2265 a(d \u2217 f\u0303) and \u03a6 is non-increasing. The left hand term is also non-negative due to the rotation invariance assumption and the fact that \u2200i, wi \u00b7 w\u0302i \u2265 0."}, {"heading": "Acknowledgements", "text": "Daniel McNamara was a visitor at Carnegie Mellon University during the period of this research, supported by a Fulbright Postgraduate Scholarship.\nThis work was supported in part by NSF grants CCF1422910, CCF-1535967, IIS-1618714, and a Microsoft Research Faculty Fellowship.\nWe thank the anonymous reviewers for their useful comments."}], "year": 2017, "references": [{"title": "Convex multi-task feature learning", "authors": ["Argyriou", "Andreas", "Evgeniou", "Theodoros", "Pontil", "Massimiliano"], "venue": "Machine Learning,", "year": 2008}, {"title": "Tailoring continuous word representations for dependency parsing", "authors": ["Bansal", "Mohit", "Gimpel", "Kevin", "Livescu", "Karen"], "venue": "In Association for Computational Linguistics,", "year": 2014}, {"title": "A model of inductive bias learning", "authors": ["Baxter", "Jonathan"], "venue": "Journal of Artificial Intelligence Research,", "year": 2000}, {"title": "A theory of learning from different domains", "authors": ["Ben-David", "Shai", "Blitzer", "John", "Crammer", "Koby", "Kulesza", "Alex", "Pereira", "Fernando", "Vaughan", "Jennifer Wortman"], "venue": "Machine Learning,", "year": 2010}, {"title": "DeCAF: a deep convolutional activation feature for generic visual recognition", "authors": ["Donahue", "Jeff", "Jia", "Yangqing", "Vinyals", "Oriol", "Hoffman", "Judy", "Zhang", "Ning", "Tzeng", "Eric", "Darrell", "Trevor"], "venue": "In International Conference on Machine Learning,", "year": 2014}, {"title": "Regularized multitask learning", "authors": ["Evgeniou", "Theodoros", "Pontil", "Massimiliano"], "venue": "In International Conference on Knowledge Discovery and Data Mining, pp", "year": 2004}, {"title": "A theoretical framework for deep transfer learning", "authors": ["Galanti", "Tomer", "Wolf", "Lior", "Hazan", "Tamir"], "venue": "Information and Inference,", "year": 2016}, {"title": "Domainadversarial training of neural networks", "authors": ["Ganin", "Yaroslav", "Ustinova", "Evgeniya", "Ajakan", "Hana", "Germain", "Pascal", "Larochelle", "Hugo", "Laviolette", "Fran\u00e7ois", "Marchand", "Mario", "Lempitsky", "Victor"], "venue": "Journal of Machine Learning Research,", "year": 2016}, {"title": "A PAC-Bayesian approach for domain adaptation with specialization to linear classifiers", "authors": ["Germain", "Pascal", "Habrard", "Amaury", "Laviolette", "Fran\u00e7ois", "Morvant", "Emilie"], "venue": "In International Conference on Machine Learning,", "year": 2013}, {"title": "Domain adaptive neural networks for object recognition", "authors": ["Ghifary", "Muhammad", "Kleijn", "W Bastiaan", "Zhang", "Mengjie"], "venue": "In Pacific Rim International Conference on Artificial Intelligence,", "year": 2014}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "authors": ["Girshick", "Ross", "Donahue", "Jeff", "Darrell", "Trevor", "Malik", "Jitendra"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "year": 2014}, {"title": "LSDA: Large scale detection through adaptation", "authors": ["Hoffman", "Judy", "Guadarrama", "Sergio", "Tzeng", "Eric S", "Hu", "Ronghang", "Donahue", "Jeff", "Girshick", "Ross", "Darrell", "Trevor", "Saenko", "Kate"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2014}, {"title": "Learning transferable features with deep adaptation networks", "authors": ["Long", "Mingsheng", "Cao", "Yue", "Wang", "Jianmin", "Jordan", "Michael I"], "venue": "In International Conference on Machine Learning,", "year": 2015}, {"title": "Domain adaptation: Learning bounds and algorithms", "authors": ["Mansour", "Yishay", "Mohri", "Mehryar", "Rostamizadeh", "Afshin"], "venue": "In Conference on Learning Theory,", "year": 2009}, {"title": "The benefit of multitask representation learning", "authors": ["Maurer", "Andreas", "Pontil", "Massimiliano", "RomeraParedes", "Bernardino"], "venue": "Journal of Machine Learning Research,", "year": 2016}, {"title": "Distributed representations of words and phrases and their compositionality", "authors": ["Mikolov", "Tomas", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg S", "Dean", "Jeff"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2013}, {"title": "Foundations of Machine Learning", "authors": ["Mohri", "Mehryar", "Rostamizadeh", "Afshin", "Talwalkar", "Ameet"], "year": 2012}, {"title": "A PACBayesian bound for Lifelong Learning", "authors": ["Pentina", "Anastasia", "Lampert", "Christoph H"], "venue": "In International Conference on Machine Learning,", "year": 2014}, {"title": "Constructing informative priors using transfer learning", "authors": ["Raina", "Rajat", "Ng", "Andrew Y", "Koller", "Daphne"], "venue": "In International Conference on Machine Learning,", "year": 2006}, {"title": "Understanding Machine Learning: From Theory to Algorithms", "authors": ["Shalev-Shwartz", "Shai", "Ben-David"], "year": 2014}, {"title": "Zero-shot learning through crossmodal transfer", "authors": ["Socher", "Richard", "Ganjoo", "Milind", "Manning", "Christopher D", "Ng", "Andrew"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2013}, {"title": "How transferable are features in deep neural networks", "authors": ["Yosinski", "Jason", "Clune", "Jeff", "Bengio", "Yoshua", "Lipson", "Hod"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2014}], "id": "SP:b10c1c81abb61b348f31d103f10d24bf096833da", "authors": [{"name": "Daniel McNamara", "affiliations": []}, {"name": "Maria-Florina Balcan", "affiliations": []}], "abstractText": "A popular machine learning strategy is the transfer of a representation (i.e. a feature extraction function) learned on a source task to a target task. Examples include the re-use of neural network weights or word embeddings. We develop sufficient conditions for the success of this approach. If the representation learned from the source task is fixed, we identify conditions on how the tasks relate to obtain an upper bound on target task risk via a VC dimension-based argument. We then consider using the representation from the source task to construct a prior, which is fine-tuned using target task data. We give a PAC-Bayes target task risk bound in this setting under suitable conditions. We show examples of our bounds using feedforward neural networks. Our results motivate a practical approach to weight transfer, which we validate with experiments.", "title": "Risk Bounds for Transferring Representations With and Without Fine-Tuning"}