0	51	Previous works on zero pronoun (ZP) resolution mainly focused on the supervised learning approaches (Han, 2006; Zhao and Ng, 2007; Iida et al., 2007; Kong and Zhou, 2010; Iida and Poesio, 2011; Chen and Ng, 2013).
1	87	However, a major obstacle for training the supervised learning models for ZP resolution is the lack of annotated data.
2	24	An important step is to organize the shared task on anaphora and coreference resolution, such as the ACE evaluations, SemEval-2010 shared task on Coreference Resolution in Multiple Languages (Marta Recasens, 2010) and CoNLL2012 shared task on Modeling Multilingual Unre- stricted Coreference in OntoNotes (Sameer Pradhan, 2012).
3	28	Following these shared tasks, the annotated evaluation data can be released for the following researches.
4	20	Despite the success and contributions of these shared tasks, it still faces the challenge of spending manpower on labeling the extended data for better training performance and domain adaptation.
5	48	To address the problem above, in this paper, we propose a simple but novel approach to automatically generate large-scale pseudo training data for zero pronoun resolution.
8	85	For the noun or pronoun in the document, which has the frequency equal to or greater than 2, we randomly choose one position where the noun or pronoun is located on, and replace it with a specific symbol 〈blank〉.
9	5	Let query Q and answer A denote the sentence that contains a 〈blank〉, and the noun or pronoun which is replaced by the 〈blank〉, respectively.
10	44	Thus, a pseudo training sample can be represented as a triple: 〈D,Q,A〉 (1) For the zero pronoun resolution task, a 〈blank〉 represents a zero pronoun (ZP) in query Q, and A indicates the corresponding antecedent of the ZP.
11	11	In this way, tremendous pseudo training samples can be generated from the various documents, such as news corpus.
12	24	Towards the shortcomings of the previous approaches that are based on feature engineering, we propose a neural network architecture, which is an attention-based neural network model, for zero pronoun resolution.
13	157	Also we propose a two-step 102 training method, which benefit from both largescale pseudo training data and task-specific data, showing promising performance.
15	32	• To our knowledge, this is the first time that utilizing reading comprehension neural network model into zero pronoun resolution task.
16	5	• We propose a two-step training approach, namely pre-training-then-adaptation, which benefits from both the large-scale automatically generated pseudo training data and taskspecific data.
19	6	First, we will describe our method of generating large-scale pseudo training data for zero pronoun resolution.
22	21	In order to get large quantities of training data for neural network model, we propose an approach, which is inspired by (Hermann et al., 2015), to automatically generate large-scale pseudo training data for zero pronoun resolution.
24	50	We will introduce the details of generating the pseudo training data for zero pronoun resolution as follows.
27	36	Given a certain document D, which is composed by a set of sentences D = {s1, s2, ..., sn}, we randomly choose an answer wordA in the document.
29	9	Second, after the answer word A is chosen, the sentence that contains A is defined as a queryQ, in which the answer wordA is replaced by a specific symbol 〈blank〉.
30	48	In this way, given the queryQ and documentD, the target of the prediction is to recover the answer A.
31	8	That is quite similar to the zero pronoun resolution task.
36	25	So we should do some adaptations to our model to deal with the zero pronoun resolution problems ideally.
