1	14	A large body of dailygenerated contents, such as tweets, web search snippets, news feeds, and forum messages, have far outpaced the reading and understanding capacity of individuals.
3	14	Among those techniques, text classification is a critical and fundamental one proven to be useful in various downstream applications, such as text summarization (Hu et al., 2015), recommendation (Zhang et al., 2012), and sentiment analysis (Chen et al., 2017).
4	21	Although many classification models like support vector machines (SVMs) (Wang and Manning, 2012) and neural networks (Kim, 2014; Xiao and Cho, 2016; Joulin et al., 2017) have demonstrated their success in processing formal and well-edited texts, such as news articles (Zhang ∗ This work was mainly conducted when Jichuan Zeng was an intern in Tencent AI Lab.
5	21	† Jing Li is the corresponding author.
7	13	This inferior performance is attributed to the severe data sparsity nature of short texts, which results in the limited features available for classifiers (Phan et al., 2008).
9	29	These approaches, however, rely on a large volume of high-quality external data, which may be unavailable to some specific domains or languages (Li et al., 2016a).
12	23	Without richer context, classifiers are likely to classify S into the same category as the training instance R1, which happens to share many words with S, in spite of the different categories they belong to,1 rather than R2, which only shares the word “wristbands” with S. Under this circumstance, how might we enrich the context of these short texts?
13	79	If looking at R2, we can observe that the semantic meaning of “wristbands” can be extended from its co- occurrence with “Bieber”, which is highly indicative of New.Music.Live.2 Such relation can further help in recognizing the word “wristbands” to be important when classifying the test instance S. Motivated by the above-mentioned observations, we present a novel neural framework, named as topic memory networks (TMN), for short text classification that does not rely on external knowledge.
14	85	Our model can identify the indicative words for classification, e.g., “wristbands” in S, via jointly exploiting the document-level word co-occurrence patterns, e.g., “wristbands” and “Bieber” in R2.
15	21	To be more specific, built upon the success of neural topic models (Srivastava and Sutton, 2017; Miao et al., 2017), our model is capable of discovering latent topics3, which can capture the co-occurrence of words in document level.
24	28	Softmax log Embedding Classifier Em bedding Neural Topic Model Topic Memory Mechanism Classifier N TM Encoder Decoder , Σ y Figure 1: The overall framework of our topic memory networks.
38	53	Similar to LDA-style topic models, we assume x having a topic mixture θ represented as a K-dimensional distribution, which is generated via Gaussian softmax construction (Miao et al., 2017).
39	26	Each topic k is represented by a word distribution φk over the vocabulary.
40	13	Specifically, the generation story for x is: • Draw latent variable z ∼ N (µ,σ2) • θ = softmax(fθ(z)) • For the n-th word in x: – Draw word wn ∼ softmax(fφ(θ)) where f∗(·) is a neural perceptron that linearly transforms inputs, activated by a non-linear transformation.
44	54	In NTM, we use variational inference (Blei et al., 2016) to approximate a posterior distribution over z given all the instances.
51	30	Assuming U as the embedded xSeq (word sequence form of x), in source memory, we compute the match between the k-th topic and the embedding of the l-th word in xSeq by P k,l = sigmoid(W s[Sk;U l] + b s) (3) where [x;y] denotes the merge of x and y, and we use concatenation operation here (Dou, 2017; Chen et al., 2017).
53	20	To further combine the instance-topic mixture θ with P , we define the integrated memory weights as ξk = θk + γ ∑ l P k,l (4) where γ is the pre-defined coefficient.
54	20	Then, in target memory, via weighting target memory matrix T with ξ, we obtain the output representation R of the topic memory mechanism: Rk = ξkT k (5) The concatenation of R and U (embedded xSeq) further serves as the features for classification.
55	65	In particular, similar to the memory networks in prior research (Sukhbaatar et al., 2015; Chen et al., 2017), our model can be extended to handle multiple computation layers (hops).
57	23	The entire TMN model integrates the three modules in Figure 1, i.e., the neural topic model, the topic memory mechanism, and the classifier, which can be updated simultaneously in one framework.
58	21	In doing so, we jointly tackle topic modeling and classification, and define the loss function of the overall framework to combine the two effects as following: L = LNTM + λLCLS (6) where LNTM represents the loss of NTM and LCLS is the cross entropy reflecting classification loss.
79	13	As to the Chinese Weibo dataset, we use FudanNLP toolkit (Qiu et al., 2013)9 for word segmentation.
115	33	By exploring topic representations in memory mechanisms, our TMN model, inferring topic models either separately or jointly with classification, significantly outperform the best comparison models on each of the four datasets.
137	28	To further analyze the quality of yielded topics, Table 5 shows the top 10 words of the sample latent topics reflecting “Egyptian revolution of 2011” discovered by various models.
152	18	To further understand why, in this section, we use the test instance S in Table 1 to analyze what the information captured by topic memory is indicative of class labels.
154	23	Figure 4 shows the heatmaps of the weight matrix P in topic memory and the topic mixture θ captured by NTM for instance S. As can be seen, the top 3 words for the latent topic with the largest value in θ are “bieber”, “justine”, and “tuesday”, which can effectively indicate the class label of S to be New.Music.Live because Justine Bieber was there on Tuesday.
166	54	Such errors can be reduced by enhancing our NTM to phrase discovery topic models (Lindsey et al., 2012; He, 2016), which is worthy exploring in future work.
182	34	We have presented topic memory networks that exploit corpus-level topic representations with a topic memory mechanism for short text classification.
183	49	The model alleviates data sparsity issues via jointly learning latent topics and text categories.
184	151	Empirical comparisons with state-of-the-art models on four benchmark datasets have demonstrated the validity and effectiveness of our model, where better results have been achieved on both short text classification and topic coherence evaluation.
