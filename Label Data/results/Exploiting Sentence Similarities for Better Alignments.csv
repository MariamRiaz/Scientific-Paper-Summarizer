0,1,label2,summary_sentences
A key to the success of probabilistic modeling is the pairing of rich probability models with fast and accurate inference algorithms.,1. Introduction,[0],[0]
Probabilistic graphical models enable this by providing a flexible class of probability distributions together with algorithms that exploit the graph structure for efficient inference.,1. Introduction,[0],[0]
"However, exact inference algorithms are only available when both the distributions involved and the graph structure are simple enough.",1. Introduction,[0],[0]
"How-
",1. Introduction,[0],[0]
"1College of Information and Computer Sciences, University of Massachusetts Amherst 2Department of Computer Science, Mount Holyoke College.",1. Introduction,[0],[0]
"Correspondence to: Kevin Winner <kwinner@cs.umass.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
ever, this situation is rare and consequently, much research today is devoted to general-purpose approximate inference techniques (e.g. Ranganath et al., 2014; Kingma & Welling, 2014; Carpenter et al., 2016).
",1. Introduction,[0],[0]
"Despite many advances in probabilistic inference, there remain relatively simple (and useful) models for which exact inference algorithms are not available.",1. Introduction,[0],[0]
This paper considers the case of graphical models with a simple structure but with (unbounded) latent count random variables.,1. Introduction,[0],[0]
"These are a natural modeling choice for many real world problems in ecology (Zonneveld, 1991; Royle, 2004; Dail & Madsen, 2011) and epidemiology (Farrington et al., 2003; Panaretos, 2007; Kvitkovicova & Panaretos, 2011).",1. Introduction,[0],[0]
"However, they pose a unique challenge for inference: even though algorithms like belief propagation (Pearl, 1986) or variable elimination (Zhang & Poole, 1994) are well defined mathematically, they cannot be implemented in an obvious way because factors have a countably infinite number of entries.",1. Introduction,[0],[0]
"As a result, approximations like truncating the support of the random variables or MCMC are applied (Royle, 2004; Gross et al., 2007; Chandler et al., 2011; Dail & Madsen, 2011; Zipkin et al., 2014; Winner et al., 2015).
",1. Introduction,[0],[0]
"Recently, Winner & Sheldon (2016) introduced a new technique for exact inference in models with latent count variables.",1. Introduction,[0],[0]
"Their approach executes the same operations as variable elimination, but with factors, which are infinite sequences of values, represented in a compact way using probability generating functions (PGFs).",1. Introduction,[0],[0]
"They developed an efficient exact inference algorithm for a specific class of Poisson hidden Markov models (HMMs) that represent a population undergoing mortality and immigration, and noisy observations of the population over time.
",1. Introduction,[0],[0]
A key open question is the extent to which PGF-based inference generalizes to a broader class of models.,1. Introduction,[0],[0]
There are two primary considerations.,1. Introduction,[0],[0]
"First, for what types of factors can the required operations (multiplication, marginalization, and conditioning) be “lifted” to PGF-based representations?",1. Introduction,[0],[0]
"Here, there is significant room for generalization: the mathematical PGF operations developed in (Winner & Sheldon, 2016) already apply to a broad class of nonPoisson immigration models, and we will generalize the models further to allow richer models of population survival and growth.",1. Introduction,[0],[0]
"Second, and more significantly, for what
types of PGFs can the requisite mathematical operations be implemented efficiently?",1. Introduction,[0],[0]
Winner & Sheldon (2016) manipulated PGFs symbolically.,1. Introduction,[0],[0]
"Their compact symbolic representation seems to rely crucially on properties of the Poisson distribution; it remains unclear whether symbolic PGF inference can be generalized beyond Poisson models.
",1. Introduction,[0],[0]
"This paper introduces a new algorithmic technique based on higher-order automatic differentiation (Griewank & Walther, 2008) for inference with PGFs.",1. Introduction,[0],[0]
A key insight is that most inference tasks do not require a full symbolic representation of the PGF.,1. Introduction,[0],[0]
"For example, the likelihood is computed by evaluating a PGF F (s) at s = 1.",1. Introduction,[0],[0]
Other probability queries can be posed in terms of derivatives F (k)(s) evaluated at either s = 0 or s = 1.,1. Introduction,[0],[0]
"In all cases, it suffices to evaluate F and its higher-order derivatives at particular values of s, as opposed to computing a compact symbolic representation of F .",1. Introduction,[0],[0]
"It may seem that this problem is then solved by standard techniques, such as higher-order forward-mode automatic differentiation (Griewank & Walther, 2008).",1. Introduction,[0],[0]
"However, the requisite PGF F is complex—it is defined recursively in terms of higher-order derivatives of other PGFs—and offthe-shelf automatic differentiation methods do not apply.",1. Introduction,[0],[0]
"We therefore develop a novel recursive procedure using building blocks of forward-mode automatic differentiation (generalized dual numbers and univariate Taylor polynomials; Griewank & Walther, 2008) to evaluate F and its derivatives.
",1. Introduction,[0],[0]
"Our algorithmic contribution leads to the first efficient exact algorithms for a class of HMMs that includes many well-known models as special cases, and has many applications.",1. Introduction,[0],[0]
"The hidden variables represent a population that undergoes three different processes: mortality (or emigration), immigration, and growth.",1. Introduction,[0],[0]
A variety of different distributional assumptions may be made about each process.,1. Introduction,[0],[0]
The models may also be viewed without this interpretation as a flexible class of models for integer-valued time series.,1. Introduction,[0],[0]
"Special cases include models from population ecology (Royle, 2004; Gross et al., 2007; Dail & Madsen, 2011), branching processes (Watson & Galton, 1875; Heathcote, 1965), queueing theory (Eick et al., 1993), and integer-valued autoregressive models (McKenzie, 2003).",1. Introduction,[0],[0]
Additional details about the relation to these models are given in Section 2.,1. Introduction,[0],[0]
"Our algorithms permit exact calculation of the likelihood for all of these models even when they are partially observed.
",1. Introduction,[0],[0]
"We demonstrate experimentally that our new exact inference algorithms are more scalable than competing approximate approaches, and support learning via exact likelihood calculations in a broad class of models for which this was not previously possible.",1. Introduction,[0],[0]
"We consider a hidden Markov model with integer latent variables N
1 , . . .",2. Model and Problem Statement,[0],[0]
", NK and integer observed variables Y 1
, . . .",2. Model and Problem Statement,[0],[0]
", YK .",2. Model and Problem Statement,[0],[0]
All variables are assumed to be non-negative.,2. Model and Problem Statement,[0],[0]
"The model is most easily understood in the context of its application to population ecology or branching processes (which are similar): in these cases, the variable Nk represents the size of a hidden population at time tk, and Yk represents the number of individuals that are observed at time tk.",2. Model and Problem Statement,[0],[0]
"However, the model is equally valid without this interpretation as a flexible class of autoregressive processes (McKenzie, 2003).
",2. Model and Problem Statement,[0],[0]
We introduce some notation to describe the model.,2. Model and Problem Statement,[0],[0]
"For an integer random variable N , write Y = ⇢ N to mean that Y ⇠ Binomial(N, ⇢).",2. Model and Problem Statement,[0],[0]
This operation is known as “binomial thinning”: the count Y is the number of “survivors” from the original count N .,2. Model and Problem Statement,[0],[0]
We can equivalently write Y = PN i=1,2. Model and Problem Statement,[0],[0]
Xi for iid Xi ⇠ Bernoulli(⇢) to highlight the fact that this is a compound distribution.,2. Model and Problem Statement,[0],[0]
"Indeed, compound distributions will play a key role: for independent integer random variables N and X , let Z = N X denote the compound random variable Z = PN i=1",2. Model and Problem Statement,[0],[0]
"Xi, where {Xi} are independent copies of X .",2. Model and Problem Statement,[0],[0]
"Now, we can describe our model as:
Nk = (Nk 1 Xk) +Mk, (1) Yk = ⇢k Nk. (2)
",2. Model and Problem Statement,[0],[0]
The variable Nk represents the population size at time tk.,2. Model and Problem Statement,[0],[0]
The random variable Nk 1 Xk 1 = PNk 1 i=1,2. Model and Problem Statement,[0],[0]
"Xk 1,i is the number of offspring of individuals from the previous time step, where Xk 1,i is the total number of individuals “caused by” the ith individual alive at time tk 1.",2. Model and Problem Statement,[0],[0]
"This definition of offspring is flexible enough to model immediate offspring, surviving individuals, and descendants of more than one generation.",2. Model and Problem Statement,[0],[0]
"The random variable Mk is the number of immigrants at time tk, and Yk is the number of individuals observed at time tk, with the assumption that each individual is observed independently with probability ⇢",2. Model and Problem Statement,[0],[0]
"k. We have left unspecified the distributions of Mk and Xk, which we term the immigration and offspring distributions, respectively.",2. Model and Problem Statement,[0],[0]
These may be arbitrary distributions over non-negative integers.,2. Model and Problem Statement,[0],[0]
"We will assume the initial condition N
0 = 0, though the model can easily be extended to accommodate arbitrary initial distributions.
",2. Model and Problem Statement,[0],[0]
Problem Statement We use lower case variables to denote specific settings of random variables.,2. Model and Problem Statement,[0],[0]
"Let yi:j = (yi, . . .",2. Model and Problem Statement,[0],[0]
", yj) and ni:j = (ni, . . .",2. Model and Problem Statement,[0],[0]
", nj).",2. Model and Problem Statement,[0],[0]
"The model above defines a joint probability mass function (pmf) p(n
1:K , y1:K ; ✓) where we introduce the vector ✓ containing parameters of all component distributions when necessary.",2. Model and Problem Statement,[0],[0]
"It is clear that the density factors according to a hidden Markov model: p(n
1:K , y1:K) =
QK k=1 p(nk |nk 1)p(yk |nk).",2. Model and Problem Statement,[0],[0]
"We will consider several inference problems that are standard for HMMs, but pose unique challenges when the hidden variables have countably infinite support.",2. Model and Problem Statement,[0],[0]
"Specifically, suppose y
1:K are observed, then we seek to:
• Compute the likelihood L(✓) = p(y 1:K ; ✓) for any ✓,
• Compute moments and values of the pmf of the filtered marginals p(nk | y1:k; ✓), for any k, ✓,
• Estimate parameters ✓ by maximizing the likelihood.
",2. Model and Problem Statement,[0],[0]
"We focus technically on the first two problems, which will enable numerical optimization to maximize the likelihood.",2. Model and Problem Statement,[0],[0]
Another standard problem is to compute smoothed marginals p(nk | y1:K ; ✓) given both past and future observations relative to time step k.,2. Model and Problem Statement,[0],[0]
"Although this is interesting, it is technically more difficult, and we defer it for future work.
",2. Model and Problem Statement,[0],[0]
Connections to Other Models This model specializes to capture many different models in the literature.,2. Model and Problem Statement,[0],[0]
The latent process of Eq.,2. Model and Problem Statement,[0],[0]
"(1) is a Galton-Watson branching process with immigration (Watson & Galton, 1875; Heathcote, 1965).",2. Model and Problem Statement,[0],[0]
"It also captures a number of different AR(1) (first-order autoregressive) processes for integer variables (McKenzie, 2003); these typically assume Xk ⇠ Bernoulli( k), i.e., that the offspring process is binomial thinning of the current individuals.",2. Model and Problem Statement,[0],[0]
"For clarity when describing this as an offspring distribution, we will refer to it as Bernoulli offspring.",2. Model and Problem Statement,[0],[0]
"With Bernoulli offspring and time-homogenous Poisson immigration, the model is an M/M/1 queue (McKenzie, 2003); with time-varying Poisson immigration it is an Mt/M/1 queue (Eick et al., 1993).",2. Model and Problem Statement,[0],[0]
"For each of these models, we contribute the first known algorithms for exact inference and likelihood calculations when the process is partially observed.",2. Model and Problem Statement,[0],[0]
"This allows estimation from data that is noisy and has variability that should not be modeled by the latent process.
",2. Model and Problem Statement,[0],[0]
Special cases of our model with noisy observations occur in statistical estimation problems in population ecology.,2. Model and Problem Statement,[0],[0]
"When immigration is zero after the first time step and Xk = 1, the population size is a fixed random variable, and we recover the N -mixture model of Royle (2004) for estimating the size of an animal population from repeated counts.",2. Model and Problem Statement,[0],[0]
"With Poisson immigration and Bernoulli offspring, we recover the basic model of Dail & Madsen (2011) for open metapopulations; extended versions with overdispersion and population growth also fall within our framework by using negative-binomial immigration and Poisson offspring.",2. Model and Problem Statement,[0],[0]
"Related models for insect populations also fall within our framework (Zonneveld, 1991; Gross et al., 2007; Winner et al., 2015).",2. Model and Problem Statement,[0],[0]
The main goal in most of this literature is parameter estimation.,2. Model and Problem Statement,[0],[0]
"Until very recently, no exact algorithms were known to compute the likelihood, so ap-
proximations such as truncating the support of the latent variables (Royle, 2004; Fiske & Chandler, 2011; Chandler et al., 2011; Dail & Madsen, 2011) or MCMC (Gross et al., 2007; Winner et al., 2015) were used.",2. Model and Problem Statement,[0],[0]
Winner & Sheldon (2016) introduced PGF-based exact algorithms for the restricted version of the model with Bernoulli offspring and Poisson immigration.,2. Model and Problem Statement,[0],[0]
We will build on that work to provide exact inference and likelihood algorithms for all of the aforementioned models.,2. Model and Problem Statement,[0],[0]
"The standard approach for inference in HMMs is the forward-backward algorithm (Rabiner, 1989), which is a special case of more general propagation or messagepassing algorithms (Pearl, 1986; Lauritzen & Spiegelhalter, 1988; Jensen et al., 1990; Shenoy & Shafer, 1990).",3. Methods,[0],[0]
"Winner & Sheldon (2016) showed how to implement the forward algorithm using PGFs for models with Bernoulli offspring and Poisson immigration.
",3. Methods,[0],[0]
Forward Algorithm,3. Methods,[0],[0]
"The forward algorithm recursively computes “messages”, which are unnormalized distributions of subsets of the variables.",3. Methods,[0],[0]
"Specifically, define ↵k(nk) :",3. Methods,[0],[0]
"= p(nk, y1:k) and k(nk)",3. Methods,[0],[0]
":= p(nk, y1:k 1).",3. Methods,[0],[0]
"These satisfy the recurrence:
k(nk) =",3. Methods,[0],[0]
"X
nk 1
↵k 1(nk 1)p(nk |nk 1), (3)
↵k(nk) = k(nk)p(yk |nk).",3. Methods,[0],[0]
"(4)
We will refer to Equation (3) as the prediction step (the value of nk is predicted based on the observations y1:k 1), and Equation (4) as the evidence step (the new evidence yk is incorporated).",3. Methods,[0],[0]
"In finite models, the forward algorithm can compute the ↵k messages for k = 1, . . .",3. Methods,[0],[0]
",K directly using Equations (3) and (4).",3. Methods,[0],[0]
"However, if nk is unbounded, this cannot be done directly; for example, ↵k(nk) is an infinite sequence, and Equation (3) contains an infinite sum.",3. Methods,[0],[0]
"Winner & Sheldon (2016) observed that, for some conditional distributions p(nk |nk 1) and p(yk |nk), the operations of the forward algorithm can be carried out using PGFs.",3.1. Forward Algorithm with PGFs,[0],[0]
"Specifically, define the PGFs k(uk) and Ak(sk) of k(nk) and ↵k(nk), respectively, as:
k(uk) := 1X
nk=0
k(nk)u nk k , (5)
Ak(sk) := 1X
nk=0
↵k(nk)s nk k .",3.1. Forward Algorithm with PGFs,[0],[0]
"(6)
The PGFs k and Ak are power series in the variables uk and sk with coefficients equal to the message entries.
",3.1. Forward Algorithm with PGFs,[0],[0]
These functions capture all relevant information about the associated distributions.,3.1. Forward Algorithm with PGFs,[0],[0]
"Technically, k and Ak are unnormalized PGFs because the coefficients do not sum to one.",3.1. Forward Algorithm with PGFs,[0],[0]
"However, the normalization constants are easily recovered by evaluating the PGF on input value 1: for example, Ak(1) =",3.1. Forward Algorithm with PGFs,[0],[0]
"P nk
↵k(nk) = p(y1:k).",3.1. Forward Algorithm with PGFs,[0],[0]
This also shows that we can recover the likelihood as AK(1) = p(y1:K).,3.1. Forward Algorithm with PGFs,[0],[0]
"After normalizing, the PGFs can be interpreted as expectations, for example Ak(sk)/Ak(1) =",3.1. Forward Algorithm with PGFs,[0],[0]
E[sNkk,3.1. Forward Algorithm with PGFs,[0],[0]
"| y1:k].
In general, it is well known that the PGF F (s) of a nonnegative integer-valued random variable X uniquely defines the entries of the probability mass function and the moments of X , which are recovered from (higher-order) derivatives of F evaluated at zero and one, respectively:
Pr(X = r)",3.1. Forward Algorithm with PGFs,[0],[0]
=,3.1. Forward Algorithm with PGFs,[0],[0]
"F (r)(0)/r!, (7)
E[X] = F (1)(1), (8)
Var(X) = F (2)(1)",3.1. Forward Algorithm with PGFs,[0],[0]
h F (1)(1),3.1. Forward Algorithm with PGFs,[0],[0]
i 2 + F (1)(1).,3.1. Forward Algorithm with PGFs,[0],[0]
"(9)
More generally, the first q moments are determined by the derivatives F (r)(1) for r  q.",3.1. Forward Algorithm with PGFs,[0],[0]
"Therefore, if we can evaluate the PGF Ak and its derivatives for sk 2 {0, 1}, we can answer arbitrary queries about the filtering distributions p(nk, y1:k), and, in particular, solve our three stated inference problems.
",3.1. Forward Algorithm with PGFs,[0],[0]
"But how can we compute values of Ak, k, and their derivatives?",3.1. Forward Algorithm with PGFs,[0],[0]
What form do these PGFs have?,3.1. Forward Algorithm with PGFs,[0],[0]
"One key result of Winner & Sheldon (2016), which we generalize here, is the fact that there is also a recurrence relation among the PGFs.",3.1. Forward Algorithm with PGFs,[0],[0]
Proposition 1.,3.1. Forward Algorithm with PGFs,[0],[0]
Consider the probability model defined in Equations (1) and (2).,3.1. Forward Algorithm with PGFs,[0],[0]
"Let Fk be the PGF of the offspring random variable Xk, and let Gk be the PGF of the immigration random variable Mk.",3.1. Forward Algorithm with PGFs,[0],[0]
"Then k and Ak satisfy the following recurrence:
k(uk) =",3.1. Forward Algorithm with PGFs,[0],[0]
"Ak 1 Fk(uk) ·Gk(uk) (10)
Ak(sk)",3.1. Forward Algorithm with PGFs,[0],[0]
"= (sk⇢k)yk
yk! · (yk)k sk(1 ⇢k)
",3.1. Forward Algorithm with PGFs,[0],[0]
"(11)
Proof.",3.1. Forward Algorithm with PGFs,[0],[0]
"A slightly less general version of Equation (10) appeared in Winner & Sheldon (2016); the general version appears in the literature on branching processes with immigration (Heathcote, 1965).",3.1. Forward Algorithm with PGFs,[0],[0]
"Equation (11) follows directly from general PGF operations outlined in (Winner & Sheldon, 2016).
",3.1. Forward Algorithm with PGFs,[0],[0]
The PGF recurrence has the same two elements as the pmf recurrence in equations (3) and (4).,3.1. Forward Algorithm with PGFs,[0],[0]
"Equation (10) is the prediction step: it describes the PGF of k(nk) = p(nk, y1:k 1) in terms of previous PGFs.",3.1. Forward Algorithm with PGFs,[0],[0]
"Equation (11) is the evidence step: it describes the PGF for ↵k(nk) =
p(nk, y1:k) in terms of the previous PGF and the new observation yk.",3.1. Forward Algorithm with PGFs,[0],[0]
"Note that the evidence step involves the ykth derivative of the PGF k from the prediction step, where yk is the observed count.",3.1. Forward Algorithm with PGFs,[0],[0]
These high-order derivatives complicate the calculation of the PGFs.,3.1. Forward Algorithm with PGFs,[0],[0]
The recurrence reveals structure about Ak and k but does not immediately imply an algorithm.,3.2. Evaluating Ak via Automatic Differentiation,[0],[0]
"Winner & Sheldon (2016) showed how to use the recurrence to compute symbolic representations of all PGFs in the special case of Bernoulli offspring and Poisson immigration: in this case, they proved that all PGFs have the form F (s) = f(s)",3.2. Evaluating Ak via Automatic Differentiation,[0],[0]
"exp(as + b), where f is a polynomial of bounded degree.",3.2. Evaluating Ak via Automatic Differentiation,[0],[0]
"Hence, they can be represented compactly and computed efficiently using the recurrence.",3.2. Evaluating Ak via Automatic Differentiation,[0],[0]
"The result is a symbolic representation, so, for example, one obtains a closed form representation of the final PGF AK , from which the likelihood, entries of the pmf, and moments can be calculated.",3.2. Evaluating Ak via Automatic Differentiation,[0],[0]
"However, the compact functional form f(s) exp(as + b) seems to rely crucially on properties of the Poisson distribution.",3.2. Evaluating Ak via Automatic Differentiation,[0],[0]
"When other distributions are used, the size of the symbolic PGF representation grows quickly with K. It is an open question whether the symbolic methods can be extended to other classes of PGFs.
",3.2. Evaluating Ak via Automatic Differentiation,[0],[0]
This motivates an alternate approach.,3.2. Evaluating Ak via Automatic Differentiation,[0],[0]
"Instead of computing Ak symbolically, we will evaluate Ak and its derivatives at particular values of sk corresponding to the queries we wish to make (cf. Equations (7)–(9)).",3.2. Evaluating Ak via Automatic Differentiation,[0],[0]
"To develop the approach, it is helpful to consider the feed-forward computation for evaluating Ak at a particular value sk.",3.2. Evaluating Ak via Automatic Differentiation,[0],[0]
"The circuit diagram in Figure 1 is a directed acyclic graph that describes this calculation; the nodes are intermediate quantities in the calculation, and the shaded rectangles illustrate the recursively nested PGFs.
",3.2. Evaluating Ak via Automatic Differentiation,[0],[0]
"Now, we can consider techniques from automatic differentiation (autodiff) to compute Ak and its derivatives.",3.2. Evaluating Ak via Automatic Differentiation,[0],[0]
"How-
ever, these will not apply directly.",3.2. Evaluating Ak via Automatic Differentiation,[0],[0]
"Note that Ak is defined in terms of higher-order derivatives of the function k, which depends on higher-order derivatives of k 1, and so forth.",3.2. Evaluating Ak via Automatic Differentiation,[0],[0]
Standard autodiff techniques cannot handle these recursively nested derivatives.,3.2. Evaluating Ak via Automatic Differentiation,[0],[0]
"Therefore, we will develop a novel algorithm.",3.2. Evaluating Ak via Automatic Differentiation,[0],[0]
We now develop basic notation and building blocks that we will assemble to construct our algorithm.,3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
"It is helpful to abstract from our particular setting and describe a general model for derivatives within a feed-forward computation, following Griewank & Walther (2008).",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
"We consider a procedure that assigns values to a sequence of variables v 0 , v 1
, . . .",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
", vn, where v0 is the input variable, vn is the output variable, and each intermediate variable vj is computed via a function 'j(vi)i j of some subset (vi)i j of the variables v
0:j 1.",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
Here the dependence relation,3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
"i j simply means that 'j depends directly on vi, and (vi)i j is the vector of variables for which that is true.",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
"Note that the dependence relation defines a directed acyclic graph G (e.g., the circuit in Figure 1), and v
0 , . . .",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
", vn is a topological ordering of G.
We will be concerned with the values of a variable v` and its derivatives with respect to some earlier variable vi.",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
"To represent this cleanly, we first introduce a notation to capture the partial computation between the assignment of vi and v`.",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
"For i  `, define fi`(v0:i) to be the value that is assigned to v` if the values of the first i variables are given by v 0:i (now treated as fixed input values).",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
"This can be defined formally in an inductive fashion:
fi`(v0:i) = '`(uij)j `, uij = ( vj if j  i fij(v0:i) if j >",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
"i
This can be interpreted as recursion with memoization for v 0:i. When '` “requests” the value of uij of vj : if j  i, this value was given as an input argument of fi`, so we just “look it up”; but if j > i, we recursively compute the correct value via the partial computation from i to j. Now, we define a notation to capture derivatives of a variable v` with respect to an earlier variable vi.",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
Definition 1 (Dual numbers).,3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
"The generalized dual number hv`, dviiq for 0  i  ` and q > 0 is the sequence consisting of v` and its first q derivatives with respect to vi:
hv`,",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
"dviiq = ✓ @p
@vpi fi`(v0:i)
◆q
p=0
We say that hv`, dviiq is a dual number of order q with respect to vi.",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
Let DRq be the set of dual numbers of order q.,3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
"We will commonly write dual numbers as:
hs, duiq = ⇣ s, ds
du , . . .",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
",
dqs
duq
⌘
in which case it is understood that s = v` and u = vi for some 0  i  `, and the function fi`(·) will be clear from context.
",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
Our treatment of dual numbers and partial computations is more explicit than what is standard.,3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
"In particular, we are explicit both about the variable v` we are differentiating and the variable vi with respect to which we are differentiating.",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
"This is important for our algorithm, and also helps distinguish our approach from traditional automatic differentiation approaches.",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
"Forward-mode autodiff computes derivatives of all variables with respect to v
0 , i.e., it computes hvj , dv0iq for j = 1, . . .",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
", n. Reverse-mode autodiff computes derivatives of vn with respect to all variables, i.e., it computes hvn, dviiq for i = n 1, . . .",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
", 0.",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
"In each case, one of the two variables is fixed, so the notation can be simplified.",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
The general idea of our algorithm will resemble forwardmode autodiff.,3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"Instead of sequentially calculating the values v
1 , . .",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
.,3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
", vn in our feed-forward computation, we will calculate dual numbers hv
1 , dvi1iq1 , . . .",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
", hvn, dviniqn , where we leave unspecified (for now) the variables with respect to which we differentiate, and the order of the dual numbers.",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
We will require three high-level operations on dual numbers.,3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"The first one is “lifting” a scalar function.
",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
Definition 2 (Lifted Function).,3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
Let f : Rm !,3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"R be a function of variables x
1 , . . .",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
", xm.",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
The qth-order lifted function Lqf : (DRq)m !,3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"DRq is the function that accepts as input dual numbers hx
1 , duiq, . . .",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
", hxm, duiq of order q with respect to the same variable u, and returns the value⌦ f(x
1 , . . .",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
", xm), du ↵ q .
",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
Lifting is the basic operation of higher-order forward mode autodiff.,3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"For functions f consisting only of “primitive operations”, the lifted function Lqf can be computed at a modest overhead relative to computing f .
",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"Proposition 2 (Griewank & Walther, 2008).",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
Let f : Rm !,3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"R be a function that consists only of the following primitive operations, where x and y are arbitrary input variables and all other numbers are constants: x",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"+ cy, x ⇤ y, x/y, xr, ln(x), exp(x), sin(x), cos(x).",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"Then Lqf can be computed in time O(q2) times the running time of f .
",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"Based on this proposition, we will write algebraic operations on dual numbers, e.g., hx, duiq⇥hy, duiq , and understand these to be lifted versions of the corresponding scalar operations.",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"The standard lifting approach is to represent dual numbers as univariate Taylor polynomials (UTPs), in which case many operations (e.g., multiplication, addition) translate directly to the corresponding operations on polynomials.",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"We will use UTPs in the proof of Theorem 1.
",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
The second operation we will require is composition.,3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
Say that variable vj separates vi from v` if all paths from vi to v` in G go through vj .,3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
Theorem 1 (Composition).,3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
Suppose vj separates vi from v`.,3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"In this case, the dual number hv`, dviiq depends only on the dual numbers hv`, dvjiq and hvj , dviiq , and we define the composition operation:
hv`, dvjiq hvj , dviiq := hv`, dviiq
If vj does not separate vi from v`, the written composition operation is undefined.",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"The composition operation can be performed in O(q2 log q) time by composing two UTPs.
",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
Proof.,3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"If all paths from vi to v` go through vj , then vj is a “bottleneck” in the partial computation fil.",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"Specifically, there exist functions F and H such that vj = F (vi) and v` = H(vj).",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"Here, the notation suppresses dependence on variables that either are not reachable from vi, or do not have a path to v`, and hence may be treated as constants because they they do not impact the dual number hv`, viiq .",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
A detailed justification of this is given in the supplementary material.,3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"Now, our goal is to compute the higher-order derivatives of v` = H(F (vi)).",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"Let ˆF and ˆH be infinite Taylor expansions about vi and vj , respectively, omitting the constant terms F (vi) and H(vj):
ˆF ("") := 1X
p=1
F (p)(vi)
p!",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"""p, ˆH("") :=
1X
p=1
H(p)(vj)
p!",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"""p.
These are polynomials in "", and the first q coefficients are given in the input dual numbers.",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"The coefficient of ""p in ˆU("") := ˆH( ˆF ("")) for p 1 is exactly dpv`/dvpi (see Wheeler, 1987, where the composition of Taylor polynomials is related directly to the higher-order chain rule known as Faà dı́ Bruno’s Formula).",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
So it suffices to compute the first q coefficients of ˆH( ˆF (✏)).,3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"This can be done by executing Horner’s method (Horner, 1819) in truncated Taylor polynomial arithmetic (Griewank & Walther, 2008), which keeps only the first q coefficients of all polynomials (i.e., it assumes ✏p = 0 for p > q).",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"After truncation, Horner’s method involves q additions and q multiplications of polynomials of degree at most q. Polynomial multiplication takes time O(q log q) using the FFT, so the overall running time is O(q2 log q).
",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
The final operation we will require is differentiation.,3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"This will support local functions '` that differentiate a previous value, e.g., v` = '`(vj) = dpvj/dvpi .",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
Definition 3 (Differential Operator).,3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"Let hs, duiq be a dual number.",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"For p  q, the differential operator Dp applied to hs, duiq returns the dual number of order q p given by:
Dphs, duiq := ⇣ dps dup , . . .",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
", dqs duq ⌘
The differential operator can be applied in O(q) time.
",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"This operation was defined in (Kalaba & Tesfatsion, 1986).",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"We will now use these operations to lift the function Ak to compute h↵k, skiq = LA hsk, dskiq), i.e., the output of Ak and its derivatives with respect to its input.",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
Algorithm 1 gives a sequence of mathematical operations to compute Ak(sk).,3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
Algorithm 2 shows the corresponding operations on dual numbers; we call this algorithm the generalized dual-number forward algorithm or GDUALFORWARD.,3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"Note that a dual number of a variable with respect to itself is simply hx, dxiq = (x, 1, 0, . . .",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
", 0); such expressions are used without explicit initialization in Algorithm 2.",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"Also, if the dual number hx, dyiq has been assigned, we will assume the scalar value x is also available, for example, to initialize a new dual variable hx, dxiq (cf.",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
the dual number on the RHS of Line 3).,3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
Note that Algorithm 1 contains a non-primitive operation on Line 5: the derivative dyk k/duykk .,3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"To evaluate this in Algorithm 2, we must manipulate the dual number of k to be taken with respect to uk, and not the original input value sk, as in forward-mode autodiff.",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"Our approach can be viewed as following a different recursive principle from either forward or reverse-mode autodiff: in the circuit diagram of Figure 1, we calculate derivatives of each nested circuit with respect to its own input, starting with the innermost circuit and working out.",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
Theorem 2.,3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"LAK computes h↵k, dskiq in time O K(q",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"+
Y )2 log(q + Y ) where Y = PK
k=1 yk is the sum of the observed counts and q is the requested number of derivatives.",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"Therefore, the likelihood can be computed in O(KY 2 log Y ) time, and the first q moments or the first q entries of the filtered marginals can be computed in time",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
O K(q,3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"+ Y )2 log(q + Y ) .
",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
Proof.,3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"To see that GDUAL-FORWARD is correct, note that it corresponds to Algorithm 1, but applies the three operations from the previous section to operate on dual numbers instead of scalars.",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
We will verify that the conditions for applying each operation are met.,3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
Lines 2–5 each use lifting of algebraic operations or the functions,3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"Fk and Gk, which are assumed to consist only of primitive operations.",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"Lines 4 and 5 apply the composition operation; here, we can verify from Figure 1 that sk 1 separates uk and ↵k 1 (Line 4) and that uk separates sk and k (Line 5).",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"The conditions for applying the differential operator on Line 5 are also met.
",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"For the running time, note that the total number of operations on dual numbers in LAK , including recursive calls, is O(K).",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"The order of the dual numbers is initially q, but increases by yk in each recursive call (Line 4).",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"Therefore, the maximum value is q + Y .",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"Each of the operations on
Algorithm 1 Ak(sk) if k = 0 then
1: return ↵k = 1 end if 2: uk = sk(1 ⇢k) 3: sk 1 = Fk(uk) 4: k = Ak 1(sk 1) ·Gk(uk) 5: ↵k = d yk
du yk k k · (sk⇢k)yk/yk! 6: return ↵k
Algorithm 2 LAk(hsk, dskiq) — GDUAL-FORWARD if k = 0 then
1: return h↵k, dskiq = (1, 0, . . .",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
", 0) end if 2: huk, dskiq = hsk, dskiq · (1 ⇢k) 3: hsk 1, dukiq+yk = LFk huk, dukiq+yk
4: h k, dukiq+yk = ⇥ LAk 1",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"hsk 1, dsk 1iq+yk hsk 1, dukiq+yk ⇤ ⇥",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"LGk huk, dukiq+yk 5: h↵k, dskiq = ⇥",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"Dyk h k, dukiq+yk huk, dskiq ⇤ ⇥ ⇢khsk, dskiq
yk/yk! 6: return h↵k, dskiq
dual numbers is O(p2 log p) for dual numbers of order p, so the total is O(K(q + Y )2 log(q + Y )).",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"In this section we describe simulation experiments to evaluate the running time of GDUAL-FORWARD against other algorithms, and to assess the ability to learn a wide variety of models for which exact likelihood calculations were not previously possible, by using GDUAL-FORWARD within a parameter estimation routine.
",4. Experiments,[0],[0]
Running time vs Y .,4. Experiments,[0],[0]
"We compared the running time of GDUAL-FORWARD with the PGF-FORWARD algorithm from (Winner & Sheldon, 2016) as well as TRUNC, the standard truncated forward algorithm (Dail & Madsen, 2011).",4. Experiments,[0],[0]
"PGF-FORWARD is only applicable to the Poisson HMM from (Winner & Sheldon, 2016), which, in our terminology, is a model with a Poisson immigration distribution and a Bernoulli offspring distribution.",4. Experiments,[0],[0]
"TRUNC applies to any choice of distributions, but is approximate.",4. Experiments,[0],[0]
"For these experiments, we restrict to Poisson HMMs for the sake of comparison with the less general PGF-FORWARD algorithm.
",4. Experiments,[0],[0]
A primary factor affecting running time is the magnitude of the counts.,4. Experiments,[0],[0]
We measured the running time for all algorithms to compute the likelihood p(y; ✓) for vectors y,4. Experiments,[0],[0]
:= y 1:K = c ⇥,4. Experiments,[0],[0]
"(1, 1, 1, 1, 1) with increasing c.",4. Experiments,[0],[0]
"In this case, Y = P k yk = 5c.",4. Experiments,[0],[0]
"PGF-FORWARD and GDUAL-FORWARD have running times O(KY 2) and O(KY 2 log Y ), respectively, which depend only on Y and not ✓.",4. Experiments,[0],[0]
"The running time of an FFT-based implementation of TRUNC is O(KN2
max
logN max ), where N max is the value used to truncate the support of each latent variable.",4. Experiments,[0],[0]
"A heuristic is required to choose N
max so that it captures most of the probability mass of p(y; ✓) but is not too big.",4. Experiments,[0],[0]
"The appropriate value depends strongly on ✓, which in practice may be unknown.",4. Experiments,[0],[0]
"In preliminary experiments with realistic immigration and offspring models (see below) and known parameters, we found that an excellent heuristic is N
max = 0.4Y/⇢, which we use here.",4. Experiments,[0],[0]
"With this heuristic, TRUNC’s running time is O(K⇢2Y 2 log Y ).
",4. Experiments,[0],[0]
"Figure 3 shows the results for ⇢ 2 {0.15, 0.85}, averaged over 20 trials with error bars showing 95% confidence intervals of the mean.",4. Experiments,[0],[0]
"GDUAL-FORWARD and TRUNC have the same asymptotic dependence on Y but GDUALFORWARD scales better empirically, and is exact.",4. Experiments,[0],[0]
"It is about 8x faster than TRUNC for the largest Y when ⇢ = 0.15, and 2x faster for ⇢ = 0.85.",4. Experiments,[0],[0]
"PGF-FORWARD is faster by a factor of log Y in theory and scales better in practice, but applies to fewer models.
",4. Experiments,[0],[0]
Running time for different ✓.,4. Experiments,[0],[0]
"We also conducted experiments where we varied parameters and used an oracle method to select N
max for TRUNC.",4. Experiments,[0],[0]
"This was done by running the algorithm for increasing values of N
max and selecting the smallest one such that the likelihood was within 10 6 of the true value (see Winner & Sheldon, 2016).
",4. Experiments,[0],[0]
"We simulated data from Poisson HMMs and measured the time to compute the likelihood p(y; ✓) for the true parameters ✓ = ( , , ⇢), where is a vector whose kth entry is the mean of the Poisson immigration distribution at time k, and and ⇢ are scalars representing the Bernoulli survival probability and detection probability, respectively, which are shared across time steps.",4. Experiments,[0],[0]
"We set and to mimic three different biological models; for each, we varied ⇢ from 0.05 to 0.95.",4. Experiments,[0],[0]
"The biological models were as follows: ‘PHMM’ follows a temporal model for insect populations (Zonneveld, 1991) with = (5.13, 23.26, 42.08, 30.09, 8.56) and = 0.26; ‘PHMM-peaked’ is similar, but sets = (0.04, 10.26, 74.93, 25.13, 4.14) so the immigration is temporally “peaked” at the middle time step; ‘NMix’ sets = (80, 0, 0, 0, 0) and = 0.4, which is similar to the N-mixture model (Royle, 2004), with no immigration following the first time step.
",4. Experiments,[0],[0]
Figure 2 shows the running time of all three methods versus ⇢.,4. Experiments,[0],[0]
"In these models, E[Y ] is proportional to ⇢, and the running times of GDUAL-FORWARD and PGF-FORWARD increase with ⇢ due to the corresponding increase in Y .",4. Experiments,[0],[0]
"PGFFORWARD is faster by a factor of log Y , but is applicable to fewer models.",4. Experiments,[0],[0]
"GDUAL-FORWARD perfoms best relative to PGF-FORWARD for the NMix model, because it is fastest when counts occur in early time steps.
",4. Experiments,[0],[0]
"Recall that the running time of TRUNC is O(N2
max
logN max ).",4. Experiments,[0],[0]
"For these models, the distribution of the hidden population depends only on and , and these are the primary factors determining N
max .",4. Experiments,[0],[0]
"Running time decreases slightly as ⇢ increases, because the observation model p(y |n; ⇢) exerts more influence restricting implausible settings of n when the detection probability is higher.
",4. Experiments,[0],[0]
Parameter Estimation.,4. Experiments,[0],[0]
"To demonstrate the flexibility of the method, we used GDUAL-FORWARD within an optimization routine to compute maximum likelihood estimates (MLEs) for models with different immigration and growth distributions.",4. Experiments,[0],[0]
"In each experiment, we generated 10 independent observation vectors for K = 7 time steps from the same model p(y; ✓), and then used the L-BFGS-B algorithm to numerically find ✓ to maximize the loglikelihood of the 10 replicates.",4. Experiments,[0],[0]
We varied the distributional forms of the immigration and offspring distributions as well as the mean R := E[Xk] of the offspring distribution.,4. Experiments,[0],[0]
"We fixed the mean immigration := E[Mk] = 6 and the de-
tection probability to ⇢ = 0.6 across all time steps.",4. Experiments,[0],[0]
"The quantity R is the “basic reproduction number”, or the average number of offspring produced by a single individual, and is of paramount importance for disease and population models.",4. Experiments,[0],[0]
"We varied R, which was also shared across time steps, between 0.2 and 1.2.",4. Experiments,[0],[0]
"The parameters and R were learned, and ⇢ was fixed to resolve ambiguity between population size and detection probability.",4. Experiments,[0],[0]
"Each experiment was repeated 50 times; a very small number of optimizer runs failed to converge after 10 random restarts and were excluded.
",4. Experiments,[0],[0]
Figure 4 shows the distribution of 50 MLE estimates for R vs. the true values for each model.,4. Experiments,[0],[0]
Results for two additional models appear in the supplementary material.,4. Experiments,[0],[0]
In all cases the distribution of the estimate is centered around the true parameter.,4. Experiments,[0],[0]
It is evident that GDUAL-FORWARD can be used effectively to produce parameter estimates across a variety of models for which exact likelihood computations were not previously possible.,4. Experiments,[0],[0]
This material is based upon work supported by the National Science Foundation under Grant No. 1617533.,Acknowledgments,[0],[0]
Graphical models with latent count variables arise in a number of areas.,abstractText,[0],[0]
"However, standard inference algorithms do not apply to these models due to the infinite support of the latent variables.",abstractText,[0],[0]
"Winner & Sheldon (2016) recently developed a new technique using probability generating functions (PGFs) to perform efficient, exact inference for certain Poisson latent variable models.",abstractText,[0],[0]
"However, the method relies on symbolic manipulation of PGFs, and it is unclear whether this can be extended to more general models.",abstractText,[0],[0]
"In this paper we introduce a new approach for inference with PGFs: instead of manipulating PGFs symbolically, we adapt techniques from the autodiff literature to compute the higher-order derivatives necessary for inference.",abstractText,[0],[0]
"This substantially generalizes the class of models for which efficient, exact inference algorithms are available.",abstractText,[0],[0]
"Specifically, our results apply to a class of models that includes branching processes, which are widely used in applied mathematics and population ecology, and autoregressive models for integer data.",abstractText,[0],[0]
Experiments show that our techniques are more scalable than existing approximate methods and enable new applications.,abstractText,[0],[0]
Exact Inference for Integer Latent-Variable Models,title,[0],[0]
"Given a graphical model, one essential problem is MAP inference, that is, finding the most likely configuration of states according to the model. Although this problem is NP-hard, large instances can be solved in practice and it is a major open question is to explain why this is true. We give a natural condition under which we can provably perform MAP inference in polynomial time—we require that the number of fractional vertices in the LP relaxation exceeding the optimal solution is bounded by a polynomial in the problem size. This resolves an open question by Dimakis, Gohari, and Wainwright. In contrast, for general LP relaxations of integer programs, known techniques can only handle a constant number of fractional vertices whose value exceeds the optimal solution. We experimentally verify this condition and demonstrate how efficient various integer programming methods are at removing fractional solutions.",text,[0],[0]
"Given a graphical model, one essential problem is MAP inference, that is, finding the most likely configuration of states according to the model.
",1. Introduction,[0],[0]
"Consider graphical models with binary random variables and pairwise interactions, also known as Ising models.",1. Introduction,[0],[0]
"For a graph G = (V,E) with node weights θ ∈ RV and edge weights W ∈ RE , the probability of a variable configura-
1Department of Electrical and Computer Engineering, University of Texas at Austin, USA 2Department of Computer Science, University of Texas at Austin, USA.",1. Introduction,[0],[0]
Correspondence to:,1. Introduction,[0],[0]
"Erik M. Lindgren <erikml@utexas.edu>, Alexandros G. Dimakis <dimakis@austin.utexas.edu>, Adam Klivans <klivans@cs.utexas.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
tion is given by
P(X = x) = 1
Z exp ∑ i∈V θixi + ∑ ij∈E",1. Introduction,[0],[0]
"Wijxixj  , (1) where Z is a normalization constant.
",1. Introduction,[0],[0]
"The MAP problem is to find the configuration x ∈ {0, 1}V that maximizes Equation (1).",1. Introduction,[0],[0]
"We can write this as an integer linear program (ILP) as follows:
max q∈RV ∪E ∑ i∈V θiqi + ∑ ij∈E",1. Introduction,[0],[0]
"Wijqij
s.t. qi ∈ {0, 1} ∀i ∈ V qij ≥ max{0, qi + qj",1. Introduction,[0],[0]
− 1} ∀ij ∈,1. Introduction,[0],[0]
E,1. Introduction,[0],[0]
qij ≤,1. Introduction,[0],[0]
"min{qi, qj} ∀ij ∈",1. Introduction,[0],[0]
"E. (2)
",1. Introduction,[0],[0]
"The MAP problem on binary, pairwise graphical models contains, as a special case, the Max-cut problem and is therefore NP-hard.",1. Introduction,[0],[0]
"For this reason, a significant amount of attention has focused on analyzing the LP relaxation of the ILP, which can be solved efficiently in practice.
",1. Introduction,[0],[0]
max q∈RV ∪E ∑ i∈V θiqi + ∑ ij∈E,1. Introduction,[0],[0]
"Wijqij
s.t. 0 ≤ qi ≤ 1 ∀i ∈ V qij ≥ max{0, qi + qj",1. Introduction,[0],[0]
− 1} ∀ij ∈,1. Introduction,[0],[0]
E,1. Introduction,[0],[0]
qij ≤,1. Introduction,[0],[0]
"min{qi, qj} ∀ij ∈ E (3)
",1. Introduction,[0],[0]
This relaxation has been an area of intense research in machine learning and statistics.,1. Introduction,[0],[0]
"In (Meshi et al., 2016), the authors state that a major open question is to identify why real world instances of Problem (2) can be solved efficiently despite the theoretical worst case complexity.
",1. Introduction,[0],[0]
"We make progress on this open problem by analyzing the fractional vertices of the LP relaxation, that is, the extreme points of the polytope with fractional coordinates.",1. Introduction,[0],[0]
Vertices of the relaxed polytope with fractional coordinates are called pseudomarginals for graphical models and pseudocodewords in coding theory.,1. Introduction,[0],[0]
"If a fractional vertex has higher objective value (i.e. likelihood) compared to the best integral one, the LP relaxation fails.",1. Introduction,[0],[0]
"We call fractional vertices with an objective value at least as good as the objective
value of the optimal integral vertex confounding vertices.",1. Introduction,[0],[0]
"Our main result is that it is possible to prune all confounding vertices efficiently when their number is polynomial.
",1. Introduction,[0],[0]
"Our contributions:
• Our first contribution is a general result on integer programs.",1. Introduction,[0],[0]
"We show that any 0-1 integer linear program (ILP) can be solved exactly in polynomial time, if the number confounding vertices is bounded by a polynomial.",1. Introduction,[0],[0]
This applies to MAP inference for a graphical model over any alphabet size and any order of connection.,1. Introduction,[0],[0]
"The same result (exact solution if the number of confounding vertices is bounded by a polynomial) was established by (Dimakis et al., 2009) for the special case of LP decoding of LDPC codes (Feldman et al., 2005).",1. Introduction,[0],[0]
"The algorithm from (Dimakis et al., 2009) relies on the special structure of the graphical models that correspond to LDPC codes.",1. Introduction,[0],[0]
In this paper we generalize this result for any ILP in the unit hypercube.,1. Introduction,[0],[0]
"Our results extend to finding all integral vertices among the M -best vertices.
",1. Introduction,[0],[0]
"• Given our condition, one may be tempted to think that we generate the top M -best vertices of a linear program (for M polynomial) and output the best integral one in this list.",1. Introduction,[0],[0]
We actually show that such an approach would be computationally intractable.,1. Introduction,[0],[0]
"Specifically, we show that it is NP-hard to produce a list of the M -best vertices if M = O(nε) for any fixed ε > 0.",1. Introduction,[0],[0]
This result holds even if the list is allowed to be approximate.,1. Introduction,[0],[0]
"This strengthens the previously known hardness result (Angulo et al., 2014) which was M = O(n) for the exact M -best vertices.",1. Introduction,[0],[0]
"In terms of achievability, the best previously known result (from (Angulo et al., 2014)) can only solve the ILP if there is at most a constant number of confounding vertices.
",1. Introduction,[0],[0]
"• We obtain a complete characterization of the fractional vertices of the local polytope for binary, pairwise graphical models.",1. Introduction,[0],[0]
We show that any variable in the fractional support must be connected to a frustrated cycle by other fractional variables in the graphical model.,1. Introduction,[0],[0]
"This is a complete structural characterization that was not previously known, to the best of our knowledge.
",1. Introduction,[0],[0]
• We develop an approach to estimate the number of confounding vertices of a half-integral polytope.,1. Introduction,[0],[0]
We use this method in an empirical evaluation of the number of confounding vertices of previously studied problems and analyze how well common integer programming techniques perform at pruning confounding vertices.,1. Introduction,[0],[0]
"For some classes of graphical models, it is possible to solve the MAP problem exactly.",2. Background and Related Work,[0],[0]
"For example see (Weller et al., 2016) for balanced and almost balanced models, (Jebara, 2009) for perfect graphs, and (Wainwright et al., 2008) for graphs with constant tree-width.
",2. Background and Related Work,[0],[0]
These conditions are often not true in practice and a wide variety of general purpose algorithms are able to solve the MAP problem for large inputs.,2. Background and Related Work,[0],[0]
"One class is belief propagation and its variants (Yedidia et al., 2000; Wainwright et al., 2003; Sontag et al., 2008).",2. Background and Related Work,[0],[0]
"Another class involves general ILP optimization methods (see e.g. (Nemhauser & Wolsey, 1999)).",2. Background and Related Work,[0],[0]
"Techniques specialized to graphical models include cutting-plane methods based on the cycle inequalities (Sontag & Jaakkola, 2007; Komodakis & Paragios, 2008; Sontag et al., 2012).",2. Background and Related Work,[0],[0]
"See also (Kappes et al., 2013) for a comparative survey of techniques.
",2. Background and Related Work,[0],[0]
"In (Weller et al., 2014), the authors investigate how pseudomarginals and relaxations relate to the success of the Bethe approximation of the partition function.
",2. Background and Related Work,[0],[0]
"There has been substantial prior work on improving inference building on these LP relaxations, especially for LDPC codes in the information theory community.",2. Background and Related Work,[0],[0]
"This work ranges from very fast solvers that exploit the special structure of the polytope (Burshtein, 2009), connections to unequal error protection (Dimakis et al., 2007), and graphical model covers (Koetter et al., 2007).",2. Background and Related Work,[0],[0]
"LP decoding currently provides the best known finite-length error-correction bounds for LDPC codes both for random (Daskalakis et al., 2008; Arora et al., 2009), and adversarial bit-flipping errors (Feldman et al., 2007).
",2. Background and Related Work,[0],[0]
"For binary graphical models, there is a body of work which tries to exploit the persistency of the LP relaxation, that is, the property that integer components in the solution of the relaxation must take the same value in the optimal solution, under some regularity assumptions (Boros & Hammer, 2002; Rother et al., 2007; Fix et al., 2012).
",2. Background and Related Work,[0],[0]
"Fast algorithms for solving large graphical models in practice include (Ihler et al., 2012; Dechter & Rish, 2003).
",2. Background and Related Work,[0],[0]
"The work most closely related to this paper involves eliminating fractional vertices (so-called pseudocodewords in coding theory) by changing the polytope or the objective function (Zhang & Siegel, 2012; Chertkov & Stepanov, 2008; Liu et al., 2012).",2. Background and Related Work,[0],[0]
"A binary integer linear program is an optimization problem of the following form.
",3. Provable Integer Programming,[0],[0]
"max x
cTx
subject to Ax ≤ b x ∈ {0, 1}n
which is relaxed to a linear program by replacing the x ∈ {0, 1}n constraint with 0 ≤ x ≤ 1.",3. Provable Integer Programming,[0],[0]
For binary integer programs with the box constraints 0 ≤,3. Provable Integer Programming,[0],[0]
"xi ≤ 1 for all i, every integral vector x is a vertex of the polytope described by the constraints of the LP relaxation.",3. Provable Integer Programming,[0],[0]
"However fraction vertices may also be in this polytope, and fractional solutions can potentially have an objective value larger than every integral vertex.
",3. Provable Integer Programming,[0],[0]
"If the optimal solution to the linear program happens to be integral, then this is the optimal solution to the original integer linear program.",3. Provable Integer Programming,[0],[0]
"If the optimal solution is fractional, then a variety of techniques are available to tighten the LP relaxation and eliminate the fractional solution.
",3. Provable Integer Programming,[0],[0]
"We establish a success condition for integer programming based on the number of confounding vertices, which to the best of our knowledge was unknown.",3. Provable Integer Programming,[0],[0]
"The algorithm used in proving Theorem 1 is a version of branch-and-bound, a classic technique in integer programming (Land & Doig, 1960) (see (Nemhauser & Wolsey, 1999) for a modern reference on integer programming).",3. Provable Integer Programming,[0],[0]
"This algorithm works by starting with a root node, then branching on a fractional coordinate by making two new linear programs with all the constraints of the parent node, with the constraint xi = 0 added to one new leaf and xi = 1 added to the other.",3. Provable Integer Programming,[0],[0]
The decision on which leaf of the tree to branch on next is based on which leaf has the best objective value.,3. Provable Integer Programming,[0],[0]
"When the best leaf is integral, we know that this is the best integral solution.",3. Provable Integer Programming,[0],[0]
"This algorithm is formally written in Algorithm 1.
",3. Provable Integer Programming,[0],[0]
Theorem 1.,3. Provable Integer Programming,[0],[0]
"Let x∗ be the optimal integral solution and let {v1, v2, . . .",3. Provable Integer Programming,[0],[0]
", vM} be the set of confounding vertices in the LP relaxation.",3. Provable Integer Programming,[0],[0]
"Algorithm 1 will find the optimal integral solution x∗ after 2M calls to an LP solver.
",3. Provable Integer Programming,[0],[0]
"Since MAP inference is a binary integer program regardless of the alphabet size of the variables and order of the clique potentials, we have the following corollary:
Corollary 2.",3. Provable Integer Programming,[0],[0]
"Given a graphical model such that the local polytope has M as cofounding variables, Algorithm 1 can find the optimal MAP configuration with 2M calls to an LP solver.
",3. Provable Integer Programming,[0],[0]
"Cutting-plane methods, which remove a fractional vertex by introducing a new constraint in the polytope may not have this property, since this cut may create new confound-
Algorithm 1 Branch and Bound test Input: an LP {min cTx :",3. Provable Integer Programming,[0],[0]
"Ax ≤ b, 0 ≤ x ≤ 1}
# branch (v, I0, I1) means v is optimal LP # with xI0 = 0 and xI1 = 1.",3. Provable Integer Programming,[0],[0]
"def LP(I0, I1): v∗ ← argmax cTx subject to:",3. Provable Integer Programming,[0],[0]
Ax ≤ b xI0 = 0,3. Provable Integer Programming,[0],[0]
"xI1 = 1
return v∗ if feasible, else return null
v ← LP(∅, ∅)",3. Provable Integer Programming,[0],[0]
"B ← {(v, ∅, ∅)} while optimal integral vertex not found:
(v, I0, I1)←",3. Provable Integer Programming,[0],[0]
"argmax(v,I0,I1)∈B c T v if v is integral: return v else: find a fractional coordinate i v(0)",3. Provable Integer Programming,[0],[0]
"← LP(I0 ∪ {i}, I1) v(1) ← LP(I0, I1 ∪ {i}) remove (v, I0, I1) from B add (v(0), I0 ∪ {i}, I1) to B if feasible add (v(1), I0, I1 ∪ {i}) to B if feasible
ing vertices.",3. Provable Integer Programming,[0],[0]
This branch-and-bound algorithm has the desirable property that it never creates a new fractional vertex.,3. Provable Integer Programming,[0],[0]
"We note that other branching algorithms, such as the algorithm presented by the authors in (Marinescu & Dechter, 2009), do not immediately allow us to prove our desired theorem.
",3. Provable Integer Programming,[0],[0]
Note that warm starting a linear program with slightly modified constraints allows subsequent calls to an LP solver to be much more efficient after the root LP has been solved.,3. Provable Integer Programming,[0],[0]
"The proof follows from the following invariants:
• At every iteration we remove at least one fractional vertex.
•",3.1. Proof of Theorem 1,[0],[0]
"Every integral vertex is in exactly one branch.
",3.1. Proof of Theorem 1,[0],[0]
•,3.1. Proof of Theorem 1,[0],[0]
"Every fractional vertex is in at most one branch.
",3.1. Proof of Theorem 1,[0],[0]
•,3.1. Proof of Theorem 1,[0],[0]
"No fractional vertices are created by the new constraints.
",3.1. Proof of Theorem 1,[0],[0]
"To see the last invariant, note that every vertex of a polytope can be identified by the set of inequality constraints that are satisfied with equality (see (Bertsimas & Tsitsiklis, 1997)).",3.1. Proof of Theorem 1,[0],[0]
"By forcing an inequality constraint to be tight, we cannot possibly introduce new vertices.",3.1. Proof of Theorem 1,[0],[0]
"As mentioned in the introduction, the algorithm used to prove Theorem 1 does not enumerate all the fractional vertices until it finds an integral vertex.",3.2. The M -Best LP Problem,[0],[0]
"Enumerating the M - best vertices of a linear program is theM -best LP problem.
",3.2. The M -Best LP Problem,[0],[0]
Definition.,3.2. The M -Best LP Problem,[0],[0]
"Given a linear program {min cTx : x ∈ P} over a polytope P and a positive integer M , the M -best LP problem is to optimize
max {v1,...,vM}⊆V (P ) M∑",3.2. The M -Best LP Problem,[0],[0]
"k=1 cT vk.
",3.2. The M -Best LP Problem,[0],[0]
"This was established by (Angulo et al., 2014) to be NP-hard when M = O(n).",3.2. The M -Best LP Problem,[0],[0]
"We strengthen this result to hardness of approximation even when M = nε for any ε > 0.
Theorem 3.",3.2. The M -Best LP Problem,[0],[0]
"It is NP-hard to approximate the M -best LP problem by a factor better than O(n ε
M ) for any fixed ε > 0.
",3.2. The M -Best LP Problem,[0],[0]
Proof.,3.2. The M -Best LP Problem,[0],[0]
"Consider the circulation polytope described in (Khachiyan et al., 2008), with the graph and weight vector w described in (Boros et al., 2011).",3.2. The M -Best LP Problem,[0],[0]
"By adding anO(logM) long series of 2×2 bipartite subgraphs, we can make it such that one long path in the original graph implies M long paths in the new graph, and thus it is NP-hard to find any of these long paths in the new graph.",3.2. The M -Best LP Problem,[0],[0]
"By adding the constraint vector wTx ≤ 0, and using the cost function −w, the vertices corresponding to the short paths have value 1/2, the vertices corresponding to the long paths have value O(1/n), and all other vertices have value 0.",3.2. The M -Best LP Problem,[0],[0]
Thus the optimal set has value O(n+ Mn ).,3.2. The M -Best LP Problem,[0],[0]
"However it is NP-hard to find a set of value greater than O(n) in polynomial time, which gives an O( nM ) approximation.",3.2. The M -Best LP Problem,[0],[0]
"Using a padding argument, we can replace n with nε.
",3.2. The M -Best LP Problem,[0],[0]
"The best known algorithm for the M -best LP problem is a generalization of the facet guessing algorithm (Dimakis et al., 2009) developed in (Angulo et al., 2014), which would require O(mM ) calls to an LP solver, where m is the number of constraints of the LP.",3.2. The M -Best LP Problem,[0],[0]
"Since we only care about integral solutions, we can find the single best integral vertex with O(M) calls to an LP solver, and if we want all of the K-best integral solutions among the top M vertices of the polytope, we can find these with O(nK",3.2. The M -Best LP Problem,[0],[0]
"+M) calls to an LP-solver, as we will see in the next section.
3.3.",3.2. The M -Best LP Problem,[0],[0]
"K-Best Integral Solutions
Finding the K-best solutions to general optimization problems has been uses in several machine learning applications.",3.2. The M -Best LP Problem,[0],[0]
Producing multiple high-value outputs can be naturally combined with post-processing algorithms that select the most desired solution using additional sideinformation.,3.2. The M -Best LP Problem,[0],[0]
"There is a significant volume of work in the general area, see (Fromer & Globerson, 2009; Batra et al., 2012) for MAP solutions in graphical models and (Eppstein, 2014) for a survey on M -best problems.
",3.2. The M -Best LP Problem,[0],[0]
We further generalize our theorem to find the K-best integral solutions.,3.2. The M -Best LP Problem,[0],[0]
Theorem 4.,3.2. The M -Best LP Problem,[0],[0]
"Under the assumption that there are less than M fractional vertices with objective value at least as good as the K-best integral solutions, we can find all of the Kbest integral solutions, O(nK",3.2. The M -Best LP Problem,[0],[0]
"+M) calls to an LP solver.
",3.2. The M -Best LP Problem,[0],[0]
The algorithm used in this theorem is Algorithm 2.,3.2. The M -Best LP Problem,[0],[0]
"It combines Algorithm 1 with the space partitioning technique used in (Murty, 1968; Lawler, 1972).",3.2. The M -Best LP Problem,[0],[0]
"If the current optimal solution in the solution tree is fractional, then we use the branching technique in Algorithm 1.",3.2. The M -Best LP Problem,[0],[0]
"If the current optimal solution in the solution tree x∗ is integral, then we branch by creating a new leaf for every i not currently constrained by the parent with the constraint",3.2. The M -Best LP Problem,[0],[0]
xi = ¬x∗i .,3.2. The M -Best LP Problem,[0],[0]
"We now describe the fractional vertices of the local polytope for binary, pairwise graphical models, which is described in Equation 3.",4. Fractional Vertices of the Local Polytope,[0],[0]
"It was shown in (Padberg, 1989) that all the vertices of this polytope are half-integral, that is, all coordinates have a value from {0, 12 , 1} (see (Weller et al., 2016) for a new proof of this).
",4. Fractional Vertices of the Local Polytope,[0],[0]
"Given a half-integral point q ∈ {0, 12 , 1} V ∪E in the local polytope, we say that a cycle C = (VC , EC) ⊆ G is frustrated if there is an odd number of edges ij ∈ EC such that qij = 0.",4. Fractional Vertices of the Local Polytope,[0],[0]
"If a point q has a frustrated cycle, then it is a pseudomarginal, as no probability distribution exists that has as its singleton and pairwise marginals the coordinates of q. Half-integral points q with a frustrated cycle do not satisfy the cycle inequalities (Sontag & Jaakkola, 2007; Wainwright et al., 2008), for all cycles C = (VC , EC), F = (VF , EF ) ⊆ C, |EF",4. Fractional Vertices of the Local Polytope,[0],[0]
| odd we must have∑ ij∈EF qi+qj−2qij− ∑ ij∈EC\EF qi+qj−2qij ≤ |FC |−1.,4. Fractional Vertices of the Local Polytope,[0],[0]
"(4)
Frustrated cycles allow a solution to be zero on negative weights in a way that is not possible for any integral solution.
",4. Fractional Vertices of the Local Polytope,[0],[0]
"We have the following theorem describing all the vertices of the local polytope for binary, pairwise graphical models.
",4. Fractional Vertices of the Local Polytope,[0],[0]
Algorithm 2 M -best Integral Input: an LP {max cTx :,4. Fractional Vertices of the Local Polytope,[0],[0]
"Ax ≤ b, 0 ≤ x ≤ 1} Input: number of solutions K
def LP(I0, I1): same as Algorithm 1
def SplitIntegral(v, I0, I1): P ← { } for i ∈",4. Fractional Vertices of the Local Polytope,[0],[0]
[n] if i /∈,4. Fractional Vertices of the Local Polytope,[0],[0]
"I0 ∪ I1: a← ¬vi I ′0, I ′",4. Fractional Vertices of the Local Polytope,[0],[0]
"1 ← copy(I0, I1)
add i to I ′a v′",4. Fractional Vertices of the Local Polytope,[0],[0]
"← LP(I ′0, I ′1) add (v′, I ′0, I ′ a) to P if feasible
return P
v ← LP(∅, ∅)",4. Fractional Vertices of the Local Polytope,[0],[0]
"B ← {(v, ∅, ∅)} results← { } while K integral vertices not found: (v, I0, I1)←",4. Fractional Vertices of the Local Polytope,[0],[0]
"argmax(v,I0,I1)∈B c
T v if v is integral:
add v to results add SplitIntegeral(v, I0, I1) to B remove (v, I0, I1) from B
else: find a fractional coordinate i v(0)",4. Fractional Vertices of the Local Polytope,[0],[0]
"← LP(I0 ∪ {i}, I1) v(1) ← LP(I0, I1 ∪ {i}) remove (v, I0, I1) from B add (v(0), I0 ∪ {i}, I1) to B if feasible add (v(1), I0, I1 ∪ {i}) to B if feasible
return results
Theorem 5.",4. Fractional Vertices of the Local Polytope,[0],[0]
"Given a point q in the local polytope, q is a vertex of this polytope if and only if q ∈ {0, 12 , 1}
V ∪E and the induced subgraph on the fractional nodes of q is such that every connected component of this subgraph contains a frustrated cycle.",4. Fractional Vertices of the Local Polytope,[0],[0]
"Every vertex q of an n-dimensional polytope is such that there are n constraints such that q satisfies them with equality, known as active constraints (see (Bertsimas & Tsitsiklis, 1997)).",4.1. Proof of Theorem 5,[0],[0]
Every integral q is thus a vertex of the local polytope.,4.1. Proof of Theorem 5,[0],[0]
"We now describe the fractional vertices of the local polytope.
Definition.",4.1. Proof of Theorem 5,[0],[0]
"Let q ∈ {0, 12 , 1} n+m be a point of the local polytope.",4.1. Proof of Theorem 5,[0],[0]
"Let GF = (VF , EF ) be an induced subgraph of points such that qi = 12 for all i ∈ VF .",4.1. Proof of Theorem 5,[0],[0]
"We say that GF is
full rank if the following system of equations is full rank.
",4.1. Proof of Theorem 5,[0],[0]
qi + qj,4.1. Proof of Theorem 5,[0],[0]
− qij = 1 ∀ij ∈ EF such that qij = 0 qij = 0,4.1. Proof of Theorem 5,[0],[0]
"∀ij ∈ EF such that qij = 0
qi",4.1. Proof of Theorem 5,[0],[0]
"− qij = 0 ∀ij ∈ EF such that qij = 1
2
qj",4.1. Proof of Theorem 5,[0],[0]
"− qij = 0 ∀ij ∈ EF such that qij = 1
2
(5)
Theorem 5 follows from the following lemmas.
",4.1. Proof of Theorem 5,[0],[0]
Lemma 6.,4.1. Proof of Theorem 5,[0],[0]
"Let q ∈ {0, 12 , 1} n+m be a point of the local polytope.",4.1. Proof of Theorem 5,[0],[0]
"Let GF = (VF , EF ) be the subgraph induced by the nodes i ∈ V such that qi = 12 .",4.1. Proof of Theorem 5,[0],[0]
"The point q is a vertex if and only if every connected component of GF is full rank.
",4.1. Proof of Theorem 5,[0],[0]
Lemma 7.,4.1. Proof of Theorem 5,[0],[0]
"Let q ∈ {0, 12 , 1} n+m be a point of the local polytope.",4.1. Proof of Theorem 5,[0],[0]
"Let GF = (VF , EF ) be a connected subgraph induced from nodes such that such that qi = 12 for all i ∈ VF .",4.1. Proof of Theorem 5,[0],[0]
"GF is full rank if and only if GF contains cycle that is full rank.
",4.1. Proof of Theorem 5,[0],[0]
Lemma 8.,4.1. Proof of Theorem 5,[0],[0]
"Let q ∈ {0, 12 , 1} n+m be a point of the local polytope.",4.1. Proof of Theorem 5,[0],[0]
"Let C = (VC , EC) be a cycle of G such that qi is fractional for all i ∈ VC .",4.1. Proof of Theorem 5,[0],[0]
"C is full rank if and only if C is a frustrated cycle.
",4.1. Proof of Theorem 5,[0],[0]
Proof of Lemma 6.,4.1. Proof of Theorem 5,[0],[0]
Suppose every connected component is full rank.,4.1. Proof of Theorem 5,[0],[0]
Then every fractional node and edge between fractional nodes is fully specified by their corresponding equations in Problem 3.,4.1. Proof of Theorem 5,[0],[0]
"It is easy to check that all integral nodes, edges between integral nodes, and edges between integral and fractional nodes is also fixed.",4.1. Proof of Theorem 5,[0],[0]
"Thus q is a vertex.
",4.1. Proof of Theorem 5,[0],[0]
Now suppose that there exists a connected component that is not full rank.,4.1. Proof of Theorem 5,[0],[0]
The only other constraints involving this connected component are those between fractional nodes and integral nodes.,4.1. Proof of Theorem 5,[0],[0]
"However, note that these constraints are always rank 1, and also introduce a new edge variable.",4.1. Proof of Theorem 5,[0],[0]
"Thus all the constraints where q is tight do not make a full rank system of equations.
",4.1. Proof of Theorem 5,[0],[0]
Proof of Lemma 7.,4.1. Proof of Theorem 5,[0],[0]
Suppose GF has a full rank cycle.,4.1. Proof of Theorem 5,[0],[0]
We will build the graph starting with the full rank cycle then adding one connected edge at a time.,4.1. Proof of Theorem 5,[0],[0]
"It is easy to see from Equations 5 that all new variables introduced to the system of equations have a fixed value, and thus the whole connected component is full rank.
",4.1. Proof of Theorem 5,[0],[0]
Now suppose GF has no full rank cycle.,4.1. Proof of Theorem 5,[0],[0]
We will again build the graph starting from the cycle then adding one connected edge at a time.,4.1. Proof of Theorem 5,[0],[0]
"If we add an edge that connects to a new node, then we added two variables and two equations, thus we did not make the graph full rank.",4.1. Proof of Theorem 5,[0],[0]
"If we add an edge between two existing nodes, then we have a cycle involving this edge.",4.1. Proof of Theorem 5,[0],[0]
"We introduce two new equations, however with
one of the equations and the other cycle equations, we can produce the other equation, thus we can increase the rank by one but we also introduced a new edge.",4.1. Proof of Theorem 5,[0],[0]
"Thus the whole graph cannot be full rank.
",4.1. Proof of Theorem 5,[0],[0]
"The proof of Lemma 8 from the following lemma.
",4.1. Proof of Theorem 5,[0],[0]
Lemma 9.,4.1. Proof of Theorem 5,[0],[0]
"Consider a collection of n vectors
v1 = (1, t1, 0, . . .",4.1. Proof of Theorem 5,[0],[0]
", 0)
",4.1. Proof of Theorem 5,[0],[0]
"v2 = (0, 1, t2, 0, . . .",4.1. Proof of Theorem 5,[0],[0]
", 0)
",4.1. Proof of Theorem 5,[0],[0]
"v3 = (0, 0, 1, t3, 0, . . .",4.1. Proof of Theorem 5,[0],[0]
", 0)
...
",4.1. Proof of Theorem 5,[0],[0]
"vn−1 = (0, . . .",4.1. Proof of Theorem 5,[0],[0]
", 0, 1, tn−1)
vn = (tn, 0, . . .",4.1. Proof of Theorem 5,[0],[0]
", 0, 1)
",4.1. Proof of Theorem 5,[0],[0]
"for ti ∈ {−1, 1}.",4.1. Proof of Theorem 5,[0],[0]
"We have rank(v1, v2, . . .",4.1. Proof of Theorem 5,[0],[0]
", vn) = n",4.1. Proof of Theorem 5,[0],[0]
"if and only if there is an odd number of vectors such that ti = 1.
",4.1. Proof of Theorem 5,[0],[0]
Proof of Lemma 9.,4.1. Proof of Theorem 5,[0],[0]
Let k be the number of vectors such that ti = 1.,4.1. Proof of Theorem 5,[0],[0]
"Let S1 = v1 and define
Si+1 = { Si − vi+1",4.1. Proof of Theorem 5,[0],[0]
"if Si(i+ 1) = 1 Si + vi+1 if Si(i+ 1) = −1
for i = 2, . .",4.1. Proof of Theorem 5,[0],[0]
.,4.1. Proof of Theorem 5,[0],[0]
", n− 1.
Note that if ti+1 = −1 then Si+1(i+2) = Si(i+1) and if ti+1 = 1 then Si+1(i+2) = −Si(i+1).",4.1. Proof of Theorem 5,[0],[0]
"Thus the number of times the sign changes is exactly the number of ti = 1 for i ∈ {2, . . .",4.1. Proof of Theorem 5,[0],[0]
", n− 1}.
",4.1. Proof of Theorem 5,[0],[0]
"Using the value of Sn−1 we can now we can check for all values of t1 and tn that the following is true.
",4.1. Proof of Theorem 5,[0],[0]
"• If k is odd then (1, 0, . . .",4.1. Proof of Theorem 5,[0],[0]
", 0) ∈ span(v1, v2, . . .",4.1. Proof of Theorem 5,[0],[0]
", vn), which allows us to create the entire standard basis, showing the vectors are full rank.
",4.1. Proof of Theorem 5,[0],[0]
"• If k is even then vn ∈ span(v1, v2, . . .",4.1. Proof of Theorem 5,[0],[0]
", vn−1) and thus the vectors are not full rank.",4.1. Proof of Theorem 5,[0],[0]
For this section we generalize generalize Theorem 1.,5. Estimating the number of Confounding Singleton Marginals,[0],[0]
We see after every iteration we potentially remove more than one confounding vertex—we remove all confounding vertices that agree with xI0 = 0 and xI1,5. Estimating the number of Confounding Singleton Marginals,[0],[0]
= 1 and are fractional with any value at coordinate i.,5. Estimating the number of Confounding Singleton Marginals,[0],[0]
"We also observe that we can
handle a mixed integer program (MIP) with the same algorithm.
",5. Estimating the number of Confounding Singleton Marginals,[0],[0]
"max x
cTx+ dT",5. Estimating the number of Confounding Singleton Marginals,[0],[0]
"z
subject to Ax+Bz ≤ b x ∈ {0, 1}n
",5. Estimating the number of Confounding Singleton Marginals,[0],[0]
"We will call a vertex (x, z) fractional if its x component is fractional.",5. Estimating the number of Confounding Singleton Marginals,[0],[0]
"For each fractional vertex (x, z), we create a half-integral vector S(x) such that
S(x)i =  0",5. Estimating the number of Confounding Singleton Marginals,[0],[0]
"if xi = 0 1 2 if xi is fractional 1 if xi = 1
For a set of vertices V , we define S(V ) to be the set {S(x) : (x, z) ∈ V }, i.e. we remove all duplicate entries.",5. Estimating the number of Confounding Singleton Marginals,[0],[0]
Theorem 10.,5. Estimating the number of Confounding Singleton Marginals,[0],[0]
"Let (x∗, z∗) be the optimal integral solution and let VC be the set of confounding vertices.",5. Estimating the number of Confounding Singleton Marginals,[0],[0]
"Algorithm 1 will find the optimal integral solution (x∗, z∗) after 2|S(VC)| calls to an LP solver.
",5. Estimating the number of Confounding Singleton Marginals,[0],[0]
"For MAP inference in graphical models, S(VC) refers to the fractional singleton marginals qV such that there exists a set of pairwise pseudomarginals qE such that (qV , qE) is a cofounding vertex.",5. Estimating the number of Confounding Singleton Marginals,[0],[0]
In this case we call qV a confounding singleton marginal.,5. Estimating the number of Confounding Singleton Marginals,[0],[0]
We develop Algorithm 3 to estimate the number of confounding singleton marginals for our experiments section.,5. Estimating the number of Confounding Singleton Marginals,[0],[0]
"It is based on the k-best enumeration method developed in (Murty, 1968; Lawler, 1972).
",5. Estimating the number of Confounding Singleton Marginals,[0],[0]
Algorithm 3 works by a branching argument.,5. Estimating the number of Confounding Singleton Marginals,[0],[0]
The root node is the original LP.,5. Estimating the number of Confounding Singleton Marginals,[0],[0]
"A leaf node is branched on by introducing a new leaf for every node in V and every element of {0, 12 , 1} such that qi 6=",5. Estimating the number of Confounding Singleton Marginals,[0],[0]
a in the parent node and the constraint {qi = a} is not in the constraints for the parent node.,5. Estimating the number of Confounding Singleton Marginals,[0],[0]
"For i ∈ V , a ∈ {0, 12 , 1}, we create the leaf such that it has all the constraints of its parents plus the constraint qi = a.
Note that Algorithm 3 actually generates a superset of the elements of S(VC), since the introduction of constraints of the type qi = 12 introduce vertices into the new polytope that were not in the original polytope.",5. Estimating the number of Confounding Singleton Marginals,[0],[0]
"This does not seem to be an issue for the experiments we consider, however this does occur for other graphs.",5. Estimating the number of Confounding Singleton Marginals,[0],[0]
An interesting question is if the vertices of the local polytope can be provably enumerated.,5. Estimating the number of Confounding Singleton Marginals,[0],[0]
"We consider a synthetic experiment on randomly created graphical models, which were also used in (Sontag & Jaakkola, 2007; Weller, 2016; Weller et al., 2014).",6. Experiments,[0],[0]
The graph topology used is the complete graph on 12 nodes.,6. Experiments,[0],[0]
"We first reparametrize the model to use the sufficient statistics
Algorithm 3 Estimate S(VC) for Binary, Pairwise Graphical Models
Input: a binary, pairwise graphical model LP
# branch (v, I0, I 1 2 , I1) means v is optimal LP # with xI0 = 0, xI 1 2 = 12 , and xI1 = 1.",6. Experiments,[0],[0]
"def LP(I0, I 1 2 , I1):
optimize LP with additional constraints: xI0 = 0 xI 1
2 = 12 xI1 = 1
return q∗ if feasible, else return null
q ← LP(∅, ∅, ∅)",6. Experiments,[0],[0]
"B ← {(q, ∅, ∅, ∅)}",6. Experiments,[0],[0]
"solution← { } while optimal integral vertex not found: (q, I0, I 1
2 , I1)←",6. Experiments,[0],[0]
"argmax(q,I0,I 1 2 ,I1)∈B objective val
add q to solution remove (q, I0, I 1
2 , I1) from B
for i ∈ V if i /∈",6. Experiments,[0],[0]
"I0 ∪ I 1 2 ∪ I1:
for a ∈ {0, 12 , 1} if qi 6=",6. Experiments,[0],[0]
"a: I ′0, I
′ 1 2 , I ′1 ← copy(I0, I 12 , I1)",6. Experiments,[0],[0]
I ′a ←,6. Experiments,[0],[0]
I ′a ∪ {i} q′,6. Experiments,[0],[0]
"← LP(I ′0, I ′1
2
, I ′1)
add (q′, I ′0, I ′ 1 2 , I ′1) to B if feasible return solution
1(xi = xj) and 1(xi = 1).",6. Experiments,[0],[0]
"The node weights are drawn θi ∼ Uniform(−1, 1) and the edge weights are drawn Wij ∼ Uniform(−w,w) for varying w.",6. Experiments,[0],[0]
The quantity w determines how strong the connections are between nodes.,6. Experiments,[0],[0]
"We do 100 draws for each choice of edge strength w.
For the complete graph, we observe that Algorithm 3 does not yield any points that do not correspond to vertices, however this does occur for other topologies.
",6. Experiments,[0],[0]
We first compare how the number of fractional singleton marginals |S(VC)| changes with the connection strengthw.,6. Experiments,[0],[0]
"In Figure 1, we plot the sample CDF of the probability that |S(VC)| is some given value.",6. Experiments,[0],[0]
We observe that |S(VC)| increases as the connection strength increases.,6. Experiments,[0],[0]
"Further we see that while most instances have a small number for |S(VC)|, there are rare instances where |S(VC)| is quite large.
",6. Experiments,[0],[0]
Now we compare how the number of cycle constraints from Equation (4) that need to be introduced to find the best integral solution changes with the number of confounding singleton marginals in Figure 2.,6. Experiments,[0],[0]
"We use the algorithm for finding the most frustrated cycle in (Sontag & Jaakkola, 2007) to introduce new constraints.",6. Experiments,[0],[0]
"We observe that each constraint seems to remove many confounding singleton
marginals.
",6. Experiments,[0],[0]
"We also observe the number of introduced confounding singleton marginals that are introduced by the cycle constraints increases with the number of confounding singleton marginals in Figure 3.
",6. Experiments,[0],[0]
Finally we compare the number of branches needed to find the optimal solution increases with the number of confounding singleton marginals in Figure 4.,6. Experiments,[0],[0]
A similar trend arises as with the number of cycle inequalities introduced.,6. Experiments,[0],[0]
"To compare the methods, note that branch-and-bound uses twice as many LP calls as there are branches.",6. Experiments,[0],[0]
"For this family of graphical models, branch-and-bound tends to require less calls to an LP solver than the cut constraints.",6. Experiments,[0],[0]
"Perhaps the most interesting follow-up question to our work is to determine when, in theory and practice, our condition on the number of confounding pseudomarginals in the LP relaxation is small.",7. Conclusion,[0],[0]
Another interesting question is to see if it is possible to prune the number of confounding pseudomarginals at a faster rate.,7. Conclusion,[0],[0]
The algorithm presented for our main theorem removes one pseudomarginal after two calls to an LP solver.,7. Conclusion,[0],[0]
Is it possible to do this at a faster rate?,7. Conclusion,[0],[0]
"From our experiments, this seems to be the case in practice.",7. Conclusion,[0],[0]
"This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No. DGE-1110007 as well as NSF Grants CCF 1344364, 1407278, 1422549, 1618689, 1018829 and ARO YIP W911NF-14-1-0258.",Acknowledgements,[0],[0]
"Given a graphical model, one essential problem is MAP inference, that is, finding the most likely configuration of states according to the model.",abstractText,[0],[0]
"Although this problem is NP-hard, large instances can be solved in practice and it is a major open question is to explain why this is true.",abstractText,[0],[0]
We give a natural condition under which we can provably perform MAP inference in polynomial time—we require that the number of fractional vertices in the LP relaxation exceeding the optimal solution is bounded by a polynomial in the problem size.,abstractText,[0],[0]
"This resolves an open question by Dimakis, Gohari, and Wainwright.",abstractText,[0],[0]
"In contrast, for general LP relaxations of integer programs, known techniques can only handle a constant number of fractional vertices whose value exceeds the optimal solution.",abstractText,[0],[0]
We experimentally verify this condition and demonstrate how efficient various integer programming methods are at removing fractional solutions.,abstractText,[0],[0]
Exact MAP Inference by Avoiding Fractional Vertices,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 694–699 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
694
Many corpora span broad periods of time. Language processing models trained during one time period may not work well in future time periods, and the best model may depend on specific times of year (e.g., people might describe hotels differently in reviews during the winter versus the summer). This study investigates how document classifiers trained on documents from certain time intervals perform on documents from other time intervals, considering both seasonal intervals (intervals that repeat across years, e.g., winter) and non-seasonal intervals (e.g., specific years). We show experimentally that classification performance varies over time, and that performance can be improved by using a standard domain adaptation approach to adjust for changes in time.",text,[0],[0]
"Language, and therefore data derived from language, changes over time (Ullmann, 1962).",1 Introduction,[0],[0]
"Word senses can shift over long periods of time (Wilkins, 1993; Wijaya and Yeniterzi, 2011; Hamilton et al., 2016), and written language can change rapidly in online platforms (Eisenstein et al., 2014; Goel et al., 2016).",1 Introduction,[0],[0]
"However, little is known about how shifts in text over time affect the performance of language processing systems.
",1 Introduction,[0],[0]
"This paper focuses on a standard text processing task, document classification, to provide insight into how classification performance varies with time.",1 Introduction,[0],[0]
We consider both long-term variations in text over time and seasonal variations which change throughout a year but repeat across years.,1 Introduction,[0],[0]
"Our empirical study considers corpora contain-
ing formal text spanning decades as well as usergenerated content spanning only a few years.
",1 Introduction,[0],[0]
"After describing the datasets and experiment design, this paper has two main sections, respectively addressing the following research questions:
1.",1 Introduction,[0],[0]
"In what ways does document classification depend on the timestamps of the documents?
2.",1 Introduction,[0],[0]
"Can document classifiers be adapted to perform better in time-varying corpora?
",1 Introduction,[0],[0]
"To address question 1, we train and test on data from different time periods, to understand how performance varies with time.",1 Introduction,[0],[0]
"To address question 2, we apply a domain adaptation approach, treating time intervals as domains.",1 Introduction,[0],[0]
"We show that in most cases this approach can lead to improvements in classification performance, even on future time intervals.",1 Introduction,[0],[0]
"Time is implicitly embedded in the classification process: classifiers are often built to be applied to future data that doesn’t yet exist, and performance on held-out data is measured to estimate performance on future data whose distribution may have changed.",1.1 Related Work,[0],[0]
"Methods exist to adjust for changes in the data distribution (covariate shift) (Shimodaira, 2000; Bickel et al., 2009), but time is not typically incorporated into such methods explicitly.
",1.1 Related Work,[0],[0]
"One line of work that explicitly studies the relationship between time and the distribution of data is work on classifying the time period in which a document was written (document dating) (Kanhabua and Nørvåg, 2008; Chambers, 2012; Kotsakos et al., 2014).",1.1 Related Work,[0],[0]
"However, this task is directed differently from our work: predicting timestamps given documents, rather than predicting information about documents given timestamps.",1.1 Related Work,[0],[0]
"Our study experiments with six corpora:
• Reviews: Three corpora containing reviews labeled with sentiment: music reviews from Amazon (He and McAuley, 2016), and hotel reviews and restaurant reviews from Yelp.1 We discarded reviews that had fewer than 10 tokens or a helpfulness/usefulness score of zero.",2 Datasets and Experimental Setup,[0],[0]
"The reviews with neutral scores were removed.
",2 Datasets and Experimental Setup,[0],[0]
"• Politics: Sentences from the American party platforms of Republicans and Democrats from 1948 to 2016, available every four years.2
• News: Newspaper articles from 1950-2014, labeled with whether the article is relevant to the US economy.3
• Twitter: Tweets labeled with whether they indicate that the user received an influenza vaccination (i.e., a flu shot) (Huang et al., 2017).
",2 Datasets and Experimental Setup,[0],[0]
Our experiments require documents to be grouped into time intervals.,2 Datasets and Experimental Setup,[0],[0]
Table 1 shows the intervals for each corpus.,2 Datasets and Experimental Setup,[0],[0]
Documents that fall outside of these time intervals were removed.,2 Datasets and Experimental Setup,[0],[0]
"We grouped documents into two types of intervals:
• Seasonal: Time intervals within a year (e.g., January through March) that may be repeated across years.
",2 Datasets and Experimental Setup,[0],[0]
"• Non-seasonal: Time intervals that do not repeat (e.g., 1997-1999).
",2 Datasets and Experimental Setup,[0],[0]
"For each dataset, we performed binary classification, implemented in sklearn (Pedregosa et al., 2011).",2 Datasets and Experimental Setup,[0],[0]
"We built logistic regression classifiers with TF-IDF weighted n-gram features (n ∈ {1, 2, 3}), removing features that appeared in less than 2 documents.",2 Datasets and Experimental Setup,[0],[0]
"Except when otherwise specified, we held out a random 10% of documents as
1https://www.yelp.com/dataset 2https://www.comparativeagendas.net/
datasets_codebooks 3https://www.crowdflower.com/ data-for-everyone/
validation data for each dataset.",2 Datasets and Experimental Setup,[0],[0]
"We used Elastic Net (combined `1 and `2) regularization (Zou and Hastie, 2005), and tuned the regularization parameters to maximize performance on the validation data.",2 Datasets and Experimental Setup,[0],[0]
We evaluated the performance using weighted F1 scores.,2 Datasets and Experimental Setup,[0],[0]
We first conduct an analysis of how classifier performance depends on the time intervals in which it is trained and applied.,3 How Does Classification Performance Vary with Time?,[0],[0]
"For each corpus, we train the classifier on each time interval and test on each time interval.",3 How Does Classification Performance Vary with Time?,[0],[0]
"We downsampled the training data within each time interval to match the number of documents in the smallest interval, so that differences in performance are not due to the size of the training data.
",3 How Does Classification Performance Vary with Time?,[0],[0]
"In all experiments, we train a classifier on a partition of 80% of the documents in the time interval, and repeat this five times on different partitions, averaging the five F1 scores to produce the final estimate.",3 How Does Classification Performance Vary with Time?,[0],[0]
"When training and testing on the same interval, we test on the held-out 20% of documents in that interval (standard cross-validation).",3 How Does Classification Performance Vary with Time?,[0],[0]
"When testing on different time intervals, we test on all documents, since they are all held-out from the training interval; however, we still train on five subsets of 80% of documents, so that the training data is identical across all experiments.
",3 How Does Classification Performance Vary with Time?,[0],[0]
"Finally, to understand why performance varies, we also qualitatively examined how the distribution of content changes across time intervals.",3 How Does Classification Performance Vary with Time?,[0],[0]
"To measure the distribution of content, we trained a topic model with 20 topics using gensim (Řehůřek and Sojka, 2010) with default parameters.",3 How Does Classification Performance Vary with Time?,[0],[0]
"We associated each document with one topic (the most probable topic in the document), and then calculated the proportion of each topic within a time period as the proportion of documents in that time period assigned to that topic.",3 How Does Classification Performance Vary with Time?,[0],[0]
"We can then visualize the extent to which the distribution of 20 topics varies by time.
",3 How Does Classification Performance Vary with Time?,[0],[0]
Jan-M ar Apr-Ju n,3 How Does Classification Performance Vary with Time?,[0],[0]
"Jul-Se p Oct-D ec
Train
JanMar",3 How Does Classification Performance Vary with Time?,[0],[0]
Apr -Jun,3 How Does Classification Performance Vary with Time?,[0],[0]
"Jul-S ep
Oct -De
c
Te st
0.948 0.912 0.913 0.910
0.916 0.949 0.914 0.909
0.916 0.912 0.952 0.910
0.916 0.914 0.918 0.945
Reviews data - music
Jan-M ar Apr-Ju n",3 How Does Classification Performance Vary with Time?,[0],[0]
"Jul-Se p Oct-D ec
Train
JanMar",3 How Does Classification Performance Vary with Time?,[0],[0]
Apr -Jun,3 How Does Classification Performance Vary with Time?,[0],[0]
"Jul-S ep
Oct -De
c
Te st
0.865 0.862 0.862 0.861
0.863 0.862 0.861 0.858
0.862 0.859 0.866 0.861
0.863 0.863 0.863 0.858
Reviews data - hotels
Jan-M ar Apr-Ju n",3 How Does Classification Performance Vary with Time?,[0],[0]
"Jul-Se p Oct-D ec
Train
JanMar",3 How Does Classification Performance Vary with Time?,[0],[0]
Apr -Jun,3 How Does Classification Performance Vary with Time?,[0],[0]
"Jul-S ep
Oct -De
c
Te st
0.898 0.806 0.750 0.769
0.795 0.876 0.745 0.787
0.794 0.795 0.900 0.767
0.791 0.790 0.731 0.891
News data - economy
Jan-M ar Apr-Ju n",3 How Does Classification Performance Vary with Time?,[0],[0]
"Jul-Se p Oct-D ec
Train
JanMar",3 How Does Classification Performance Vary with Time?,[0],[0]
Apr -Jun,3 How Does Classification Performance Vary with Time?,[0],[0]
"Jul-S ep
Oct -De
c
Te st
0.896 0.894 0.891 0.856
0.808 0.940 0.853 0.829
0.836 0.904 0.917 0.845
0.849 0.891 0.884 0.902
Twitter data - vaccine
2006 -08 2009 -11 2012 -14 2015 -17
Train
200 6-08 200 9-11 201 2-14 201 5-17 Te st
0.823 0.828 0.825 0.859
0.799 0.843 0.830 0.858
0.800 0.819 0.833 0.869
0.790 0.813 0.835 0.880
Reviews data - hotels
2006 -08 2009 -11 2012 -14 2015 -17
Train
200 6-08 200 9-11 201 2-14 201 5-17 Te st
0.829 0.838 0.869 0.883
0.814 0.856 0.870 0.883
0.815 0.842 0.884 0.894
0.814 0.839 0.875 0.902
Reviews data - restaurants
1948 -56 1960 -68 1972 -80 1984 -92",3 How Does Classification Performance Vary with Time?,[0],[0]
"1996 -20042008 -16
",3 How Does Classification Performance Vary with Time?,[0],[0]
"Train
194 8-56 196 0-68 197 2-80 198 4-92
199 6-20
04 200 8-16
Te st
0.659 0.567 0.518 0.544 0.525 0.532 0.551 0.800 0.529 0.477 0.474 0.495 0.545 0.506 0.678 0.635 0.573 0.523 0.515 0.473 0.565 0.866 0.594 0.569 0.435 0.404 0.490 0.618 0.848 0.684 0.435 0.416 0.480 0.606 0.674 0.819
Politics - US political data
1985 -89 1990 -94 1995 -99 2000 -04 2005 -09 2010 -14
",3 How Does Classification Performance Vary with Time?,[0],[0]
"Train
198 5-89 199 0-94 199 5-99 200 0-04 200 5-09 201 0-14",3 How Does Classification Performance Vary with Time?,[0],[0]
"Te st
0.876 0.758 0.783 0.794 0.777 0.756 0.764 0.883 0.771 0.802 0.789 0.748 0.759 0.760 0.905 0.798 0.806 0.763 0.760 0.756 0.770 0.926 0.805 0.771 0.773 0.767 0.783 0.826 0.900 0.778 0.773 0.750 0.778 0.810 0.786 0.897
News data - economy
Figure 1: Document classification performance when training and testing on different times of year (top) and different years (bottom).",3 How Does Classification Performance Vary with Time?,[0],[0]
Some corpora are omitted for space.,3 How Does Classification Performance Vary with Time?,[0],[0]
The top row of Figure 1 shows the test scores from training and testing on each pair of seasonal time intervals for four of the datasets.,3.1 Seasonal Variability,[0],[0]
"We observe very strong seasonal variations in the economic news corpus, with a drop in F1 score on the order of 10 when there is a mismatch in the season between training and testing.",3.1 Seasonal Variability,[0],[0]
"There is a similar, but weaker, effect on performance in the music reviews from Amazon and the vaccine tweets.",3.1 Seasonal Variability,[0],[0]
"There was virtually no difference in performance in any of the pairs in both review corpora from Yelp (restaurants, not pictured, and hotels).
",3.1 Seasonal Variability,[0],[0]
"To help understand why the performance varies, Figure 2 (left) shows the distribution of topics in each seasonal interval for two corpora: Amazon music reviews and Twitter.",3.1 Seasonal Variability,[0],[0]
"We observe very little variation in the topic distribution across seasons in the Amazon corpus, but some variation in the Twitter corpus, which may explain the large performance differences when testing on held-out seasons in the Twitter data as compared to the Amazon corpus.
",3.1 Seasonal Variability,[0],[0]
"For space, we do not show the descriptions of the topics, but instead only the shape of the distributions to show the degree of variability.",3.1 Seasonal Variability,[0],[0]
"We did qualitatively examine the differences in word features across the time periods, but had difficulty interpreting the observations and were unable to draw clear conclusions.",3.1 Seasonal Variability,[0],[0]
"Thus, characterizing the ways in which content distributions vary over time, and why this affects performance, is still an open question.",3.1 Seasonal Variability,[0],[0]
The bottom row of Figure 1 shows the test scores from training and testing on each pair of nonseasonal time intervals.,3.2 Non-seasonal Variability,[0],[0]
A strong pattern emerges in the political parties corpus: F1 scores can drop by as much as 40 points when testing on different time intervals.,3.2 Non-seasonal Variability,[0],[0]
"This is perhaps unsurprising, as this collection spans decades, and US party positions have substantially changed over time.",3.2 Non-seasonal Variability,[0],[0]
"The performance declines more when testing on time intervals that are further away in time from the training interval, suggesting that changes in party platforms shift gradually over time.",3.2 Non-seasonal Variability,[0],[0]
"In contrast, while there was a performance drop when testing outside the training interval in the economic news corpus, the drop was not gradual.",3.2 Non-seasonal Variability,[0],[0]
"In the Twitter dataset (not pictured), F1 dropped by an average of 4.9 points outside the training interval.
",3.2 Non-seasonal Variability,[0],[0]
"We observe an intriguing non-seasonal pattern that is consistent in both of the review corpora from Yelp, but not in the music review corpus from Amazon (not pictured), which is that the classification performance fairly consistently increases over time.",3.2 Non-seasonal Variability,[0],[0]
"Since we sampled the dataset so that the time intervals have the same number of reviews, this suggests something else changed over time about the way reviews are written that makes the sentiment easier to detect.
",3.2 Non-seasonal Variability,[0],[0]
The right side of Figure 2 shows the topic distribution in the Amazon and Twitter datasets across non-seasonal intervals.,3.2 Non-seasonal Variability,[0],[0]
We observe higher levels of variability across time in the non-seasonal intervals as compared to the seasonal intervals.,3.2 Non-seasonal Variability,[0],[0]
"Overall, it is clear that classifiers generally perform best when applied to the same time interval they were trained.",3.3 Discussion,[0],[0]
"Performance diminishes when applied to different time intervals, although different corpora exhibit differ patterns in the way in which the performance diminishes.",3.3 Discussion,[0],[0]
This kind of analysis can be applied to any corpus and could provide insights into characteristics of the corpus that may be helpful when designing a classifier.,3.3 Discussion,[0],[0]
We now consider how to improve classifiers when working with datasets that span different time intervals.,4 Making Classification Robust to Temporality,[0],[0]
We propose to treat this as a domain adaptation problem.,4 Making Classification Robust to Temporality,[0],[0]
"In domain adaptation, any partition of data that is expected to have a different distribution of features can be treated as a domain (Joshi et al., 2013).",4 Making Classification Robust to Temporality,[0],[0]
"Traditionally, domain adaptation is used to adapt models to a common task across rather different sets of data, e.g., a sentiment classifier for different types of products (Blitzer et al., 2007).",4 Making Classification Robust to Temporality,[0],[0]
"Recent work has also applied domain adaptation to adjust for potentially more subtle differences in data, such as adapting for differences in the demographics of authors (Volkova et al., 2013; Lynn et al., 2017).",4 Making Classification Robust to Temporality,[0],[0]
"We follow the same approach, treating time intervals as domains.
",4 Making Classification Robust to Temporality,[0],[0]
"In our experiments, we use the feature augmentation approach of Daumé III (2007) to perform domain adaptation.",4 Making Classification Robust to Temporality,[0],[0]
"Each feature is duplicated to have a specific version of the feature for every domain, as well as a domain-independent version of the feature.",4 Making Classification Robust to Temporality,[0],[0]
"In each instance, the domainindependent feature and the domain-specific feature for that instance’s domain have the same feature value, while the value is zeroed out for the domain-specific features for the other domains.
",4 Making Classification Robust to Temporality,[0],[0]
"This is equivalent to a model where the feature weights are domain specific but share a Gaussian prior across domains (Finkel and Manning, 2009).",4 Making Classification Robust to Temporality,[0],[0]
"This approach is widely used due to its simplicity, and derivatives of this approach have been used in similar work (e.g., (Lynn et al., 2017)).",4 Making Classification Robust to Temporality,[0],[0]
"Following Finkel and Manning (2009), we separately adjust the regularization strength for the domain-independent feature weights and the domain-specific feature weights.",4 Making Classification Robust to Temporality,[0],[0]
"We first examine classification performance on the datasets when grouping the seasonal time intervals (January-March, April-June, July-August, September-December) as domains and applying the feature augmentation approach for domain adaptation.",4.1 Seasonal Adaptation,[0],[0]
"As a baseline comparison, we apply the same classifier, but without domain adaptation.
",4.1 Seasonal Adaptation,[0],[0]
Results are shown in Table 2.,4.1 Seasonal Adaptation,[0],[0]
"We see that applying domain adaptation provides a small boost in three of the datasets, and has no effect on two of the datasets.",4.1 Seasonal Adaptation,[0],[0]
"If this pattern holds in other corpora, then this suggests that it does not hurt performance to apply domain adaptation across different times of year, and in some cases can lead to a small performance boost.",4.1 Seasonal Adaptation,[0],[0]
We now consider the non-seasonal time intervals (spans of years).,4.2 Non-seasonal Adaptation,[0],[0]
"In particular, we consider the scenario when one wants to apply a classifier trained on older data to future data.",4.2 Non-seasonal Adaptation,[0],[0]
"This requires a modification to the domain adaptation approach, because future data includes domains that did not exist in the training data, and thus we cannot learn domain-specific feature weights.",4.2 Non-seasonal Adaptation,[0],[0]
"To solve this, we train in the usual way, but when testing on future data, we only include the domain-independent features.",4.2 Non-seasonal Adaptation,[0],[0]
"The intuition is that the domain-independent parameters should be applicable to all domains, and so using only these features should lead to better generalizability to new domains.",4.2 Non-seasonal Adaptation,[0],[0]
"We test this hypothesis by training the classifiers on all but the last time interval, and testing on the final interval.",4.2 Non-seasonal Adaptation,[0],[0]
"For hyperparameter tuning, we used the final time interval of the training data (i.e., the penultimate interval) as the validation set.",4.2 Non-seasonal Adaptation,[0],[0]
"The intuition is that the penultimate interval is the closest to the test data and thus is expected to be most similar to it.
",4.2 Non-seasonal Adaptation,[0],[0]
Results are shown in the first three columns of Table 3.,4.2 Non-seasonal Adaptation,[0],[0]
We see that this approach leads to a small performance boost in all cases except the Twitter dataset.,4.2 Non-seasonal Adaptation,[0],[0]
"This means that this simple feature augmentation approach has the potential to make classifiers more robust to future changes in data.
",4.2 Non-seasonal Adaptation,[0],[0]
How to apply the feature augmentation technique to unseen domains is not well understood.,4.2 Non-seasonal Adaptation,[0],[0]
"By removing the domain-specific features, as we did here, the prediction model has changed, and so its behavior may be hard to predict.",4.2 Non-seasonal Adaptation,[0],[0]
"Nonetheless, this appears to be a successful approach.",4.2 Non-seasonal Adaptation,[0],[0]
We also experimented with including the seasonal features when performing non-seasonal adaptation.,4.2.1 Adding Seasonal Features,[0],[0]
"In this setting, we train the models with two domain-specific features in addition to the domain-independent features: one for the season,
and one for the non-seasonal interval.",4.2.1 Adding Seasonal Features,[0],[0]
"As above, we remove the non-seasonal features at test time; however, we retain the season-specific features in addition to the domain-independent features, as they can be reused in future years.
",4.2.1 Adding Seasonal Features,[0],[0]
The results of this approach are shown in the last column of Table 3.,4.2.1 Adding Seasonal Features,[0],[0]
We find that combining seasonal and non-seasonal features together leads to an additional performance gain in most cases.,4.2.1 Adding Seasonal Features,[0],[0]
"Our experiments suggest that time can substantially affect the performance of document classification, and practitioners should be cognizant of this variable when developing classifiers.",5 Conclusion,[0],[0]
"A simple analysis comparing pairs of time intervals can provide insights into how performance varies with time, which could be a good practice to do when initially working with a corpus.",5 Conclusion,[0],[0]
"Our experiments also suggest that simple domain adaptation techniques can help account for this variation.4
We make two practical recommendations following the insights from this work.",5 Conclusion,[0],[0]
"First, evaluation will be most accurate if the test data is as similar as possible to whatever future data the classifier will be applied to, and one way to achieve this is to select test data from the chronological end of the corpus, rather than randomly sampling data without regard to time.",5 Conclusion,[0],[0]
"Second, we observed that performance on future data tends to increase when hyperparameter tuning is conducted on later data; thus, we also recommend sampling validation data from the chronological end of the corpus.",5 Conclusion,[0],[0]
The authors thank the anonymous reviews for their insightful comments and suggestions.,Acknowledgements,[0],[0]
"This work was supported in part by the National Science Foundation under award number IIS-1657338.
4Our code is available at: https://github.com/ xiaoleihuang/Domain_Adaptation_ACL2018",Acknowledgements,[0],[0]
Many corpora span broad periods of time.,abstractText,[0],[0]
"Language processing models trained during one time period may not work well in future time periods, and the best model may depend on specific times of year (e.g., people might describe hotels differently in reviews during the winter versus the summer).",abstractText,[0],[0]
"This study investigates how document classifiers trained on documents from certain time intervals perform on documents from other time intervals, considering both seasonal intervals (intervals that repeat across years, e.g., winter) and non-seasonal intervals (e.g., specific years).",abstractText,[0],[0]
"We show experimentally that classification performance varies over time, and that performance can be improved by using a standard domain adaptation approach to adjust for changes in time.",abstractText,[0],[0]
Examining Temporality in Document Classification,title,[0],[0]
"Proceedings of the SIGDIAL 2015 Conference, pages 260–269, Prague, Czech Republic, 2-4 September 2015. c©2015 Association for Computational Linguistics",text,[0],[0]
"The amount of textual content that is produced and consumed each day all over the world, through news websites, social media, and other information sources, is constantly growing.",1 Introduction,[0],[0]
This makes the process of selecting the right content to read and quickly recognizing basic facts and topics in texts a core task for making content accessible to the users.,1 Introduction,[0],[0]
Automatic summarization strives to provide a means to this end.,1 Introduction,[0],[0]
"This paper describes our automatic summarization system, and its participation in the MultiLing 2015 summarization challenge.
",1 Introduction,[0],[0]
"Our focus has been on producing a largely language-independent solution for the MultiLing 2015 challenge that, in contrast to most attempts in this field, requires a strict minimum of languagespecific components and uses no language-specific materials for the core innovative elements.
",1 Introduction,[0],[0]
"Our motivation comes in part from Hong et al. (2014), who compares a number of single language summarization systems on the same standardized data set and shows that many complex, language-specific, highly optimized and trained
methods do not significantly out-perform simplistic algorithms that date back to the first summarization competitions in 2004.
",1 Introduction,[0],[0]
"Language-independent text summarization is generally based on sentence extractive methods: A subset of sentences in a text are identified and combined to form a summary, rather than performing more complex operations, and the primary task of summarization algorithms is to identify the set of sentences that form the best summary.",1 Introduction,[0],[0]
"In this case, algorithms differ mostly in how sentences are selected.
",1 Introduction,[0],[0]
One textual feature that has proven useful in identifying good summary sentences is the relative prominence of specific words in texts when contrasted to a reference distribution (like frequency in a large general corpus).,1 Introduction,[0],[0]
"For example, the “keyness” metric in El-Haj and Rayson (2013), singular value decomposition on a term-vector matrix (Steinberger, 2013) and neural network-derived transformations of term vectors (Kågebäck et al., 2014) have all produced significant results.",1 Introduction,[0],[0]
There are also a number of rule-based approaches like Anechitei and Ignat (2013).,1 Introduction,[0],[0]
"Hong et al. (2014) provides an overview of various current approaches, ranging from simple baseline algorithms to complex systems with many machine learning and rule-based components of various kinds.
",1 Introduction,[0],[0]
"One promising recent approach is graph theorybased schemes which construct sentence similarity graphs and use various graph techniques to determine the importance of specific sentences as a heuristic to identify good summary sentences (Barth, 2004; Li et al., 2013b; Mihalcea and Tarau, 2004).
",1 Introduction,[0],[0]
"In this paper, we describe ExB’s graphbased summarization approach and its results in two MultiLing 2015 tasks: Multilingual Singledocument Summarization and Multilingual Multidocument Summarization.",1 Introduction,[0],[0]
ExB’s submissions covered all languages in each task.,1 Introduction,[0],[0]
"Furthermore,
260
we summarize and discuss some unexpected negative experimental results, particularly in light of the problems posed by summarization tasks and their evaluation using ROUGE (Lin, 2004).",1 Introduction,[0],[0]
"The procedures used in both tasks start from similar assumptions and use a generalized framework for language-independent sentence selectionbased summarization.
",2 Process Overview,[0],[0]
We start from the same basic model as LDA approaches to text analysis: Every document contains a mixture of topics that are probabilistically indicative of the tokens present in it.,2 Process Overview,[0],[0]
"We select sentences in order to generate summaries whose topic mixtures most closely match that of the document as a whole (Blei et al., 2003).
",2 Process Overview,[0],[0]
"We construct a graph representation of the text in which each node corresponds to a sentence, and edges are weighted by a similarity metric for comparing them.",2 Process Overview,[0],[0]
"We then extract key sentences for use in summaries by applying the PageRank/TextRank algorithm, a well-studied algorithm for measuring graph centrality.",2 Process Overview,[0],[0]
"This technique has proven to be good model for similar extraction tasks in the past (Mihalcea and Tarau, 2004).
",2 Process Overview,[0],[0]
We deliberately chose not to optimize any parameters of our core algorithm for specific languages.,2 Process Overview,[0],[0]
Every parameter and design decision applied to all languages equally and was based on cross-linguistic performance.,2 Process Overview,[0],[0]
"Typically it is possible to increase evaluation performance by 2%-4% through fine tuning, but this tends to produce overfitting and the gains are lost when applied to any broader set of languages or domains.
",2 Process Overview,[0],[0]
"Our approach consists of three stages:
1.",2 Process Overview,[0],[0]
Preprocessing using common NLP tools.,2 Process Overview,[0],[0]
"This includes steps like tokenization and sentence identification, and in the multilingual summarization case, an extractor for time references like dates and specific times of day.",2 Process Overview,[0],[0]
"These tools are not entirely languageindependent.
2.",2 Process Overview,[0],[0]
"Sentence graph construction and sentence ranking as described in Sections 2.2 and 2.3 respectively.
3.",2 Process Overview,[0],[0]
Post-processing using simple and languageindependent rules for selecting the highest ranking sentences up to the desired length of text.,2 Process Overview,[0],[0]
Our processing pipeline starts with tokenization and sentence boundary detection.,2.1 Preprocessing,[0],[0]
For most languages we employ ExB’s proprietary languageindependent rule-based tokenizer.,2.1 Preprocessing,[0],[0]
"For Chinese, Japanese and Thai tokenization we use languagedependent approaches:
• Chinese is tokenized using a proprietary algorithm that relies on a small dictionary, the probability distribution of token lengths in Chinese, and a few handcrafted rules for special cases.
",2.1 Preprocessing,[0],[0]
"• For Thai, we use a dictionary containing data from NECTEC (2003) and Satayamas (2014) to calculate the optimal partition of Thai letter sequences based on a shortest path algorithm in a weighted, directed acyclic character graph using dictionary terms found in the text.
",2.1 Preprocessing,[0],[0]
"• For Japanese, we employ the CRF-based MeCab (Kudo et al., 2004; Kudo, 2013) morphological analyzer and tokenizer.",2.1 Preprocessing,[0],[0]
"MeCab is considered state-of-the-art and is currently being used in the construction of annotated reference corpora for Japanese by Maekawa et al. (2014).
",2.1 Preprocessing,[0],[0]
"Sentence boundary detection is rule-based and uses all sentence separators available in the Unicode range of the document’s main language, along with an abbreviation list and a few rules to correctly identify expressions like “p.ex.” or “...”
Finally, we use a proprietary SVM-based stemmer trained for a wide variety of languages on custom corpora.",2.1 Preprocessing,[0],[0]
"Given a set of tokenized sentences S, we construct a weighted undirected graph G = (V,E), where each vertex Vi ∈ V corresponds to a sentence in S. The weighted edges (Si, Sj , w) of the graph are defined as a subset of S × S where i",2.2 Graph construction,[0],[0]
"6= j and (w ← sim(Si, Sj))",2.2 Graph construction,[0],[0]
≥ t for a given similarity measure sim and a given threshold t.,2.2 Graph construction,[0],[0]
"We always assume a normalized similarity measure with a scale between 0 and 1.
",2.2 Graph construction,[0],[0]
"Sentence similarity is computed with the standard vector space model (Salton, 1989), where each sentence is defined by a vector of its tokens.
",2.2 Graph construction,[0],[0]
"We compared these vectors using a number of techniques:
• An unweighted bag-of-words model with sentence similarity computed using the Jacquard index.
",2.2 Graph construction,[0],[0]
"• Conventional cosine similarity of sentence vectors weighted by term frequency in the sentence.
",2.2 Graph construction,[0],[0]
"• TF-IDF weighted cosine similarity, where term frequencies in sentences are normalized with respect to the document collection.
",2.2 Graph construction,[0],[0]
"• Semantic similarity measured using the ExB Themis semantic approach described in Hänig et al. (2015).
",2.2 Graph construction,[0],[0]
"We also evaluated different settings for the threshold t. We did not optimize t separately for different languages, instead setting a single value for all languages.
",2.2 Graph construction,[0],[0]
"Surprisingly, when averaged over all 38 languages in the MSS training set, the simple bag-ofwords model with a threshold t = 0.3 produced the best result using the ROUGE-2 measure.",2.2 Graph construction,[0],[0]
"We then apply to the sentence similarity graph an iterative extension of the PageRank algorithm (Brin and Page, 1998) that we have called FairTextRank (FRank) to rank the sentences in the graph.",2.3 Sentence ranking,[0],[0]
"PageRank has been used as a ranker for an extractive summarizer before in Mihalcea and Tarau (2004), who named it TextRank when used for this purpose.",2.3 Sentence ranking,[0],[0]
"PageRank constitutes a measure of graph centrality, so intuitively we would expect it to select the most central, topical, and summarizing sentences in the text.
",2.3 Sentence ranking,[0],[0]
"Following our assumption that every document constitutes a mix of topics, we further assume that every topic corresponds to a cluster in the sentence graph.",2.3 Sentence ranking,[0],[0]
"However, PageRank is not a cluster sensitive algorithm and does not, by itself, ensure coverage of the different clusters present in any graph.",2.3 Sentence ranking,[0],[0]
"Therefore, our FRank algorithm invokes PageRank iteratively on the graph, at each step ranking all the sentences, then removing the top ranking sentence from the graph, and then running PageRank again to extract the next highest ranking sentence.",2.3 Sentence ranking,[0],[0]
"Because the most central sentence in the entire graph is also, by definition, the most central sentence in some cluster, removing it weakens
the centrality of the other sentences in that cluster and increases the likelihood that the next sentence selected will be the highest ranking sentence in another cluster.
",2.3 Sentence ranking,[0],[0]
"A similar method of removing selected sentences is used in the UWB Summarizer by Steinberger (2013), which was one of the top performing systems at MultiLing 2013.",2.3 Sentence ranking,[0],[0]
"However, the UWB Summarizer uses an LSA algorithm on a sentence-term matrix to identify representative sentences, where we have employed PageRank.
",2.3 Sentence ranking,[0],[0]
The complete algorithm is detailed in Algorithm 1.,2.3 Sentence ranking,[0],[0]
"The function adj returns the weighted adjacency matrix of the sentence graph G. An inner for-loop transforms the weighted adjacency matrix into a column-stochastic matrix where for each column c, where A[i, c] is the weight of the edge between sentence i and sentence c, the following expression holds: ∑ i∈|A|A[i, c] = 1.",2.3 Sentence ranking,[0],[0]
"Informally, each column is normalized at each iteration so that its values sum to 1.",2.3 Sentence ranking,[0],[0]
"pr is the PageRank-algorithm with the default parameters β = 0.85, a convergence threshold of 0.001 and allowed to run for at most 100 iterations as implemented in the JUNG API (O’Madadhain et al., 2010).
",2.3 Sentence ranking,[0],[0]
Algorithm 1,2.3 Sentence ranking,[0],[0]
FairTextRank 1: function FRANK(G) 2:,2.3 Sentence ranking,[0],[0]
R←,2.3 Sentence ranking,[0],[0]
[] 3: while |G| > 0,2.3 Sentence ranking,[0],[0]
"do 4: A← adj(G) 5: for (r, c)← |A|2 do 6: Anorm[r, c]← A[r,c],∑
i∈|A| A[i,c]
7: rank ← pr(Anorm) 8: v ← rank[0] 9: R← R+ v
10: G← G \ v return R",2.3 Sentence ranking,[0],[0]
The final step in processing is the production of a plain text summary.,2.4 Post-processing,[0],[0]
"Given a fixed maximum summary length, we selected the highest ranked sentences produced by the ranking algorithm until total text length was greater than the maximum allowed length, then truncated the last sentence to fit exactly the maximum allowed length.",2.4 Post-processing,[0],[0]
"Although this reduces the human readability of the summary - the last sentence is interrupted without any consideration of the reader at all - it can only increase
the score of an n-gram based evaluation metric like ROUGE.",2.4 Post-processing,[0],[0]
The Multilingual Single-document Summarization (MSS) task consisted of producing summaries for Wikipedia articles in 38 languages.,3 Single Document Summarizer,[0],[0]
All articles were provided as UTF-8 encoded plain-text files and as XML documents that mark sections and other elements of the text structure.,3 Single Document Summarizer,[0],[0]
"We took advantage of the availability of headers and section boundary information in performing this task.
",3 Single Document Summarizer,[0],[0]
There was no overlap between the training data and the evaluation data for the MSS task.,3 Single Document Summarizer,[0],[0]
The released training data consisted of the evaluation data set from MultiLing 2013 as described in Kubina et al. (2013).,3 Single Document Summarizer,[0],[0]
This training data contains 30 articles in each of 40 languages.,3 Single Document Summarizer,[0],[0]
"The MSS task itself at MultiLing 2015 used 30 articles in each of 38 languages, dropping two languages because there were not enough new articles not included in the training data.
",3 Single Document Summarizer,[0],[0]
"In addition to the preprocessing steps described in Section 2.1, for this task we applied a list of sentence filters developed specifically for Wikipedia texts:
• Skip all headers.",3 Single Document Summarizer,[0],[0]
•,3 Single Document Summarizer,[0],[0]
"Skip every sentence with with less than 2 to-
kens (mostly errors in sentence boundary detection).
",3 Single Document Summarizer,[0],[0]
•,3 Single Document Summarizer,[0],[0]
"Skip every sentence that contains double quotes.
",3 Single Document Summarizer,[0],[0]
"We then performed sentence graph construction and ranking as described in Sections 2.2 and 2.3
In the post-processing stage, we sorted the sentences selected to go into the summary in order of their position in the original article, before producing a plain text summary by concatenating them.",3 Single Document Summarizer,[0],[0]
The organizers of the MultiLing 2015 challenge measured the quality of our system’s output using five different versions of the ROUGE score.,3.1 Results,[0],[0]
We provide a summary of the results for all participants in Table 1.,3.1 Results,[0],[0]
"It shows the average ranking of each participating system over all the languages on which it was tested, as well as the number of languages on which each system was tested.",3.1 Results,[0],[0]
The systems labelled Lead and Oracles are special systems.,3.1 Results,[0],[0]
"Lead just uses the beginning of the article
as the summary and represents a very simple baseline.",3.1 Results,[0],[0]
"Oracles, on the other hand, is a cheating system that marks the upper bound for any extractive approach.
",3.1 Results,[0],[0]
Only three submissions - highlighted in bold - participated in more than 3 languages.,3.1 Results,[0],[0]
"We submitted only one run of our system, defined as a fixed set of parameters that are the same over all languages.",3.1 Results,[0],[0]
One of the other two systems that participated in all 38 languages submitted five runs.,3.1 Results,[0],[0]
"According to the frequently used ROUGE-1 and ROUGE-2 scores, our system achieved an average ranking of 3.2 and 3.3, respectively.",3.1 Results,[0],[0]
"This table shows that the CCS system performed better on average than our system, and the LCS-IESI system performed on average worse.
",3.1 Results,[0],[0]
"However, ROUGE-1 only measures matching single words, whereas ROUGE-2 measures matching bigrams.",3.1 Results,[0],[0]
More complex combinations of words are more indicative of topic matches between gold standard data and system output.,3.1 Results,[0],[0]
"We believe that ROUGE-SU4, which measures bigrams of words with some gaps as well as unigrams, would be a better measure of output quality.",3.1 Results,[0],[0]
"When manually inspecting the summaries, we have the strong impression that system runs in which our system scored well by ROUGESU4 measures, but poorly by ROUGE-2, did produce better summaries with greater readability and topic coverage.
",3.1 Results,[0],[0]
"Our system achieves a significantly better overall ranking using ROUGE-SU4 instead of ROUGE-2, even though the system was optimized to produce the highest ROUGE-2 scores.",3.1 Results,[0],[0]
Only two runs of the winning system CCS scored better than our system according to ROUGE-SU4.,3.1 Results,[0],[0]
"This underlines the robustness of our system’s underlying principles, despite the known problems with ROUGE evaluations.",3.1 Results,[0],[0]
The Multilingual Multi-document Summarization (MMS) task involves summarizing ten news articles on a single topic in a single language.,4 Multi Document Summarizer,[0],[0]
"For each language, the dataset consists of ten to fifteen topics, and ten languages were covered in all, including and expanding on the data used in the 2013 MMS task described by Li et al. (2013a).
",4 Multi Document Summarizer,[0],[0]
"The intuition guiding our approach to this task is the idea that if news articles on the same topic contain temporal references that are close together
or overlapping in time, then they are likely to describe the same event.",4 Multi Document Summarizer,[0],[0]
We therefore cluster the documents in each collection by the points in time referenced in the text rather than attempting to summarize the concatenation of the documents directly.,4 Multi Document Summarizer,[0],[0]
"This approach has the natural advantage that we can present summary information in chronological order, thereby often improving readability.",4 Multi Document Summarizer,[0],[0]
"Unfortunately, this improvement is not measurable using ROUGE-style metrics as employed in evaluating this task.
",4 Multi Document Summarizer,[0],[0]
"An official training data set with model summaries was released, but too late to inform our submission, which was not trained with any new 2015 data.",4 Multi Document Summarizer,[0],[0]
"We did, however, use data from the 2011 MultiLing Pilot including gold standard summaries (Giannakopoulos et al., 2011), which forms a part of the 2015 dataset.",4 Multi Document Summarizer,[0],[0]
"We used only the 700 documents and summaries from the 2011 task as training data, and did not use any Chinese, Spanish or Romanian materials in preparing our submission.
",4 Multi Document Summarizer,[0],[0]
"Our submission follows broadly the same procedure as for the single document summarization task, as described in Section 2 and Section 3, except for the final step, which relies on section information not present in the news articles that form the dataset for this task.",4 Multi Document Summarizer,[0],[0]
"Instead, a manual examination of the dataset revealed that the news articles all have a fixed structure: the first line is the headline, the second is the date, and the remaining lines form the main text.",4 Multi Document Summarizer,[0],[0]
"We used this underlying structure in preprocessing to identify the dateline of the news article, and we use this date to disambiguate relative time expressions in the text like “yesterday” or “next week”.",4 Multi Document Summarizer,[0],[0]
"Articles are also ordered in
time with respect to each other on the basis of the article date.
",4 Multi Document Summarizer,[0],[0]
"Furthermore, we remove in preprocessing any sentence that contains only time reference tokens because they are uninformative for summarization.
",4 Multi Document Summarizer,[0],[0]
"We then extract temporal references from the text, using ExB’s proprietary TimeRec framework described in Thomas (2012), which is available for all the languages used in this task.",4 Multi Document Summarizer,[0],[0]
"With the set of disambiguated time references in each document, we can provide a “timeframe” for each document that ranges from the earliest time referenced in the text to the latest.",4 Multi Document Summarizer,[0],[0]
"Note that this may not include the date of the document itself, if, for example, it is a retrospective article about an event that may have happened years in the past.",4 Multi Document Summarizer,[0],[0]
Ng et al. (2014) and,4.1 Time information processing,[0],[0]
Wan (2007) investigate using textural markers of time for multi-document summarization of news articles using very different algorithms.,4.1 Time information processing,[0],[0]
Our approach is more similar to Ng et al. in constructing a timeline for each document and for the collection as a whole based on references extracted from texts.,4.1 Time information processing,[0],[0]
"Once document timeframes are ordered chronologically, we organize them into groups based on their positions on a time line.",4.1 Time information processing,[0],[0]
"We explored two strategies to produce these groups:
• Least Variance Clustering (LVC):",4.1 Time information processing,[0],[0]
Grouping the documents iteratively by adding a new document to the group if the overall variance of the group doesn’t go over a threshold.,4.1 Time information processing,[0],[0]
"We set the standard deviation limit of the group
in 0.1.",4.1 Time information processing,[0],[0]
The algorithm is a divisive clustering algorithm based on the central time of the documents and the standard deviation.,4.1 Time information processing,[0],[0]
"At first the minimal central time of a document collection is subtracted from all other central times, then we compute mean, variance and standard deviation based on days as a unit and normalized by the mean.",4.1 Time information processing,[0],[0]
"Afterwards we recursively split the groups with the goal to minimize the variance of both splits until either a group consists only of one document or the recomputed standard deviation of a group is less than 0.1.
",4.1 Time information processing,[0],[0]
"• Overlapping Time Clustering (OTC): Grouping documents together if their timeframes overlap more than a certain amount, which we empirically set to 0.9 after experimenting with various values.",4.1 Time information processing,[0],[0]
"This means that if two texts A and B are grouped together, then either A’s timeframe includes at least 90% of B’s timeframe, or B’s timeframe includes 90% of A’s.",4.1 Time information processing,[0],[0]
"This approach proceeds iteratively, with each new addition to a group updating the timeframe of the group as a whole, and any text which overlaps more than 90% with this new interval is then grouped with it in the next iteration.
",4.1 Time information processing,[0],[0]
"In addition, we provide two baseline clusterings:
• One document per cluster (1PC): Each document is in a cluster by itself.
",4.1 Time information processing,[0],[0]
"• All in one cluster (AIO): All documents from one topic are clustered together.
",4.1 Time information processing,[0],[0]
"In the LVC and OTC cases, clustering is iterative and starts with the earliest document as determined by a fixed “central” date for each document.",4.1 Time information processing,[0],[0]
"We explored different ways of determining that “central” date: One was using the dateline found in preprocessing on the second line of each document, another was the median of the time references in the document.",4.1 Time information processing,[0],[0]
"Our best result used the dateline from each article and, as can be seen in Table 2, was produced by the OTC strategy.",4.1 Time information processing,[0],[0]
"This is a surprising result, as we expected LVC to perform better since variance is generally a better measure of clustering.",4.1 Time information processing,[0],[0]
"However, we found that LVC generally produced more clusters than OTC and we believe that to account for its poor performance.
",4.1 Time information processing,[0],[0]
"We experimented with a number of other ordering and clustering approaches, although they do not figure into our submission to the MMS task, but in all cases they failed to out-perform the OTC approach according to the ROUGE-2 recall measure.
",4.1 Time information processing,[0],[0]
"For all conditions, identical preprocessing was performed using ExB’s proprietary languagespecific tokenizer and sentence identifier.",4.1 Time information processing,[0],[0]
"ROUGE scores, because they are based on token n-grams, are very sensitive to discrepancies between tokenizers and stemmers.",4.1 Time information processing,[0],[0]
"In English, because most tokenizers perform very similarly, this causes fewer problems in scoring than for Arabic or other languages where tokenizers vary dramatically.",4.1 Time information processing,[0],[0]
"We used the results in Table 2 to decide which conditions to use in the competition, but we cannot be sure to what degree our results have been influenced by these kinds of ROUGE-related problems.
",4.1 Time information processing,[0],[0]
"After clustering, we perform graph-based sentence ranking as described in Sections 2.2 and 2.3 separately for each cluster.",4.1 Time information processing,[0],[0]
"We then select sentences from each cluster, ensuring that they are all represented in the final summary, so that the entire time span of the articles is covered.",4.1 Time information processing,[0],[0]
"We also order the selected sentences in the summary based on the temporal ordering of the clusters, so that summary presentation is in event order.",4.1 Time information processing,[0],[0]
"When experimenting with the challenge data we made several observations:
1.",4.2 Experimental results,[0],[0]
"Since the dataset of MMS is composed of news articles, just selecting the headlines and first sentences will produce a strong baseline with very high ROUGE scores.",4.2 Experimental results,[0],[0]
"It is difficult to beat this baseline using sentence extraction techniques.
",4.2 Experimental results,[0],[0]
2.,4.2 Experimental results,[0],[0]
The quality of the summaries varies a great deal between languages.,4.2 Experimental results,[0],[0]
"Instead of producing fine-tuned configurations for each lan-
guage that optimize ROUGE scores, we focused on increasing the performance in English - a language we can read and in which we can qualitatively evaluate the produced summaries.
",4.2 Experimental results,[0],[0]
3.,4.2 Experimental results,[0],[0]
All the results here of the time information processing are at document-level.,4.2 Experimental results,[0],[0]
"We also tried to apply the time grouping algorithms per sentence, but we noticed a drop of about 3% ROUGE-2 score on average.
",4.2 Experimental results,[0],[0]
"The most important finding is that using temporal expressions and chronological information does improve the performance of the summary system, and that the iterative FairTextRank algorithm shows a solid performance even for multiple documents.
",4.2 Experimental results,[0],[0]
"As can be seen in Table 3, our system gets ranked in middle position in the official scores of the challenge using the NPowER, MeMoG and AutoSummENG measures as described in Giannakopoulos and Karkaletsis (2013) and Giannakopoulos and Karkaletsis (2011).",4.2 Experimental results,[0],[0]
"We also note that our system out-performs all other participants in Chinese, a language for which we had no training data.",4.2 Experimental results,[0],[0]
"We feel that it is important not only to publish positive results, but also negative ones, to counter the strong publication bias identified in many areas in the natural and social sciences (Dickersin et al., 1987; Ioannidis, 2005).",5 Negative results,[0],[0]
"Since we conducted a large number of experiments in creating this system, we inevitably also came across a number of ideas that seemed good, but turned out to not improve our algorithm, at least as measured using ROUGE-2.
",5 Negative results,[0],[0]
In another challenge participation we developed a very powerful “semantic text similarity” (STS) toolkit.,5 Negative results,[0],[0]
"In SemEval 2015 Task 2 (Agirre et al., 2015), it achieved by far the highest scores for Spanish texts and the second best scores for English.",5 Negative results,[0],[0]
"Since our text summarization methodology is based on a sentence similarity graph, our intuitive hypothesis was that when using this module as opposed to simple matching-words strategies, performance should increase significantly.",5 Negative results,[0],[0]
"Matching-words strategies are used as the baseline in SemEval tasks, and it is easily out-performed by more sophisticated approaches.
",5 Negative results,[0],[0]
"Therefore, we tried out our STS module as a replacement for Jacquard and cosine similarity measures when constructing the sentence graph, while keeping all other parameters fixed.",5 Negative results,[0],[0]
"Surprisingly, it did not improve performance, and lowered ROUGE-2 scores by 2%.",5 Negative results,[0],[0]
"We also attempted to use word2vec embeddings precomputed on very large corpora (Mikolov et al., 2013) to represent words and hence compute a much finer-grained sentence similarity, but those results were 4% worse.",5 Negative results,[0],[0]
"It is possible that those systems were, in fact, better, but because ROUGE scoring focuses on word matches, any other improvement cannot be measured directly.",5 Negative results,[0],[0]
"We also attempted to include other factors such as sentence length, position, number of named entities, temporal expressions, and physical measurements into the sentence similarity score, all without seeing any increase in ROUGE scores.
",5 Negative results,[0],[0]
"Since identifying temporal expressions increases ROUGE scores, as this paper shows, we surmised that name recognition might also improve summarization.",5 Negative results,[0],[0]
"We applied our named entity recognition system, which is available in a number of different languages and won the Germeval 2014 (Benikova et al., 2014) NER challenge, and weighted more heavily sentences with detected names before extracting summary sentences.",5 Negative results,[0],[0]
"Interestingly, no matter how the weighting scheme was set up, the performance of the system always dropped by a few percent.",5 Negative results,[0],[0]
"Often, the system would select useless sentences that contain long lists of participating authors, or enumerations of entities participating in some reported event.",5 Negative results,[0],[0]
"Even when these kinds of sentences are explicitly removed, it still selects sentences that simply contain many names with little relevance to the topics of the news article.",5 Negative results,[0],[0]
"We conclude that sen-
tences describing central topics in documents are not strongly correlated with named entity usage.
",5 Negative results,[0],[0]
"Another very intuitive assumption is that filtering stop words, or down-weighting very frequent words, or using a TF-IDF based scheme with a similar effect, would improve the results.",5 Negative results,[0],[0]
"However, we did not observe any improvement by using these techniques.",5 Negative results,[0],[0]
"Nonetheless, there are strong indications that this is due to the limitations of ROUGE-2 scoring and we cannot conclude that these kinds of techniques are useless for summarization.",5 Negative results,[0],[0]
It is easy to achieve very competitive ROUGE-2 scores by just filling the summary with very frequent stop word combinations.,5 Negative results,[0],[0]
"A human would immediately recognize the uselessness of such a “summary”, but ROUGE-2 would count many bigram matches with a gold standard summary.
",5 Negative results,[0],[0]
"Finally, we considered the hypothesis that the summary system could be helped by explicitly removing very similar sentences presenting redundant information.",5 Negative results,[0],[0]
"Surprisingly, explicitly removing such sentences did not improve the performance of the system.",5 Negative results,[0],[0]
"Manually inspecting a number of summaries, we notice that very similar sentences recurring often in texts are rarely selected by the FRank algorithm.",5 Negative results,[0],[0]
We believe this is because our approach is sufficiently robust to discount these sentences on its own.,5 Negative results,[0],[0]
"In this paper we outline ExB’s largely languageindependent system for text summarization based on sentence selection, and show that it supports at least the 38 languages used in this completion without any language-specific fine-tuning.",6 Conclusions,[0],[0]
Sentences are selected using an iterative extension of PageRank calculation on a sentence similarity graph.,6 Conclusions,[0],[0]
"Our results in the MultiLing 2015 challenge have validated this approach by achieving the best scores for several languages and competitive scores for most of them, generally surpassed by only one other participating system.
",6 Conclusions,[0],[0]
"We also show that one basic summarization system can apply to different domains, different languages, and different tasks without special configuration, while retaining state-of-the-art performance.",6 Conclusions,[0],[0]
"Furthermore, for multi-document news summarization, we show that extracting temporal expressions is a useful feature for combining articles on the same topic.
",6 Conclusions,[0],[0]
"Our most relevant conclusion is that both the current evaluation methodology (based on various forms of ROUGE) as well as the current principal approach to language-independent text summarization (context-free, sentence selection based) are highly inadequate to model the vague requirements users associate with a text summarization product.
",6 Conclusions,[0],[0]
Participants in MultiLing 2015 did not receive the scripts and parameters used in producing evaluations.,6 Conclusions,[0],[0]
This made it difficult to optimize parameters and algorithms and has a significant impact on results using ROUGE measures and probably the other measures as well.,6 Conclusions,[0],[0]
"Hong et al. (2014), for example, notes values between 30.8% and 39.1% using ROUGE-1 for one well-known algorithm on one data set by different authors.",6 Conclusions,[0],[0]
It is not clear how the vastly different scores obtained for identical summaries using different ROUGE parameters correlate with the objective quality of a given summary.,6 Conclusions,[0],[0]
"We have no clear indication that ROUGE scores really capture the quality of a given summary at all.
",6 Conclusions,[0],[0]
"While it is possible to formulate summarization solutions based on sentence selection and even iteratively improve them using ROUGE scores, the actual achievable performance measured using ROUGE is very low.",6 Conclusions,[0],[0]
"We have noticed that stemming, stopword filtering and various tokenization strategies can have a very large influence on ROUGE scores, especially in morphologically richer languages than English.",6 Conclusions,[0],[0]
"More modern evaluation measures like MeMog or NPoweR might solve the problems inherent to ROUGE, however they currently lack widespread adoption in the research community.
",6 Conclusions,[0],[0]
"Nonetheless, even if these issues in evaluation can be addressed, we do not believe that summaries based on sentence selection will ever reach a quality where they could be accepted as comparable to a human written summary.",6 Conclusions,[0],[0]
We present our state of the art multilingual text summarizer capable of single as well as multi-document text summarization.,abstractText,[0],[0]
"The algorithm is based on repeated application of TextRank on a sentence similarity graph, a bag of words model for sentence similarity and a number of linguistic preand post-processing steps using standard NLP tools.",abstractText,[0],[0]
We submitted this algorithm for two different tasks of the MultiLing 2015 summarization challenge: Multilingual Singledocument Summarization and Multilingual Multi-document Summarization.,abstractText,[0],[0]
ExB Text Summarizer,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1329–1338 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1329
In this paper we present the Exemplar Encoder-Decoder network (EED), a novel conversation model that learns to utilize similar examples from training data to generate responses. Similar conversation examples (context-response pairs) from training data are retrieved using a traditional TF-IDF based retrieval model. The retrieved responses are used to create exemplar vectors that are used by the decoder to generate the response. The contribution of each retrieved response is weighed by the similarity of corresponding context with the input context. We present detailed experiments on two large data sets and find that our method outperforms state of the art sequence to sequence generative models on several recently proposed evaluation metrics. We also observe that the responses generated by the proposed EED model are more informative and diverse compared to existing state-of-the-art method.",text,[0],[0]
"With the availability of large datasets and the recent progress made by neural methods, variants of sequence to sequence learning (seq2seq) (Sutskever et al., 2014) architectures have been successfully applied for building conversational systems (Serban et al., 2016, 2017b).",1 Introduction,[0],[0]
"However, despite these methods being the stateof-the art frameworks for conversation generation, they suffer from problems such as lack of diversity in responses and generation of short, repetitive and uninteresting responses (Liu et al., 2016; Serban et al., 2016, 2017b).",1 Introduction,[0],[0]
"A large body of recent
literature has focused on overcoming such challenges (Li et al., 2016a; Lowe et al., 2017).
",1 Introduction,[0],[0]
"In part, such problems arise as all information required to generate responses needs to be captured as part of the model parameters learnt from the training data.",1 Introduction,[0],[0]
These model parameters alone may not be sufficient for generating natural conversations.,1 Introduction,[0],[0]
"Therefore, despite providing enormous amount of data, neural generative systems have been found to be ineffective for use in real world applications (Liu et al., 2016).
",1 Introduction,[0],[0]
"In this paper, we focus our attention on closed domain conversations.",1 Introduction,[0],[0]
"A characteristic feature of such conversations is that over a period of time, some conversation contexts1 are likely to have occurred previously (Lu et al., 2017b).",1 Introduction,[0],[0]
"For instance, Table 1 shows some contexts from the Ubuntu dialog corpus.",1 Introduction,[0],[0]
"Each row presents an input dialog context with its corresponding gold response followed by a similar context and response seen in training data – as can be seen, contexts for “installing dms”, “sharing files”, “blocking ufw ports” have all occurred in training data.",1 Introduction,[0],[0]
"We hypothesize that being able to refer to training responses for previously seen similar contexts could be a helpful signal to use while generating responses.
",1 Introduction,[0],[0]
"In order to exploit this aspect of closed domain conversations we build our neural encoderdecoder architecture called the Exemplar Encoder Decoder (EED), that learns to generate a response for a given context by exploiting similar contexts from training conversations.",1 Introduction,[0],[0]
"Thus, instead of having the seq2seq model learn patterns of language only from aligned parallel corpora, we assist the model by providing it closely related (similar) samples from the training data that it can refer to while generating text.
",1 Introduction,[0],[0]
"Specifically, given a context c, we retrieve a set 1We use the phrase “dialog context”, “conversation con-
text” and “context” interchangeably throughout the paper.
of context-response pairs (c(k), r(k)), 1 ≤ k ≤ K using an inverted index of training data.",1 Introduction,[0],[0]
We create an exemplar vector e(k) by encoding the response r(k) (also referred to as exemplar response) along with an encoded representation of the current context c. We then learn the importance of each exemplar vector e(k) based on the likelihood of it being able to generate the ground truth response.,1 Introduction,[0],[0]
We believe that e(k) may contain information that is helpful in generating the response.,1 Introduction,[0],[0]
"Table 1 highlights the words in exemplar responses that appear in the ground truth response as well.
",1 Introduction,[0],[0]
"Contributions: We present a novel Exemplar Encoder-Decoder (EED) architecture that makes use of similar conversations, fetched from an index of training data.",1 Introduction,[0],[0]
"The retrieved contextresponse pairs are used to create exemplar vectors which are used by the decoder in the EED model, to learn the importance of training context-response pairs, while generating responses.",1 Introduction,[0],[0]
"We present detailed experiments on the publicly benchmarked Ubuntu dialog corpus data set (Lowe et al., 2015) as well a large collection of more than 127,000 technical support conversations.",1 Introduction,[0],[0]
"We compare the performance of the EED model with the existing state of the art generative models such as HRED (Serban et al., 2016) and VHRED (Serban et al., 2017b).",1 Introduction,[0],[0]
"We find that our model out-performs these models on a wide variety of metrics such as the recently proposed Activity Entity metrics (Serban et al., 2017a) as well as Embedding-based metrics (Lowe et al., 2015).",1 Introduction,[0],[0]
"In addition, we present qualitative insights into our results and we find that exemplar based responses
are more informative and diverse.",1 Introduction,[0],[0]
The rest of the paper is organized as follows.,1 Introduction,[0],[0]
Section 2 briefly describes the recent works in neural dialogue generation The details of the proposed EED model for dialogue generation are described in detail in Section 3.,1 Introduction,[0],[0]
"In Section 4, we describe the datasets as well as the details of the models used during training.",1 Introduction,[0],[0]
We present quantitative and qualitative results of EED model in Section 5.,1 Introduction,[0],[0]
"In this section, we compare our work against other data-driven end-to-end conversation models.",2 Related Work,[0],[0]
"Endto-end conversation models can be further classified into two broad categories — generation based models and retrieval based models.
",2 Related Work,[0],[0]
Generation based models cast the problem of dialogue generation as a sequence to sequence learning problem.,2 Related Work,[0],[0]
"Initial works treat the entire context as a single long sentence and learn an encoder-decoder framework to generate response word by word (Shang et al., 2015; Vinyals and Le, 2015).",2 Related Work,[0],[0]
"This was followed by work that models context better by breaking it into conversation history and last utterance (Sordoni et al., 2015b).",2 Related Work,[0],[0]
"Context was further modeled effectively by using a hierarchical encoder decoder (HRED) model which first learns a vector representation of each utterance and then combines these representations to learn vector representation of context (Serban et al., 2016).",2 Related Work,[0],[0]
"Later, an alternative hierarchical model called VHRED (Serban et al., 2017b) was proposed, where generated responses were conditioned on latent variables.",2 Related Work,[0],[0]
"This leads to more in-
formative responses and adds diversity to response generation.",2 Related Work,[0],[0]
"Models that explicitly incorporate diversity in response generation have also been studied in literature (Li et al., 2016b; Vijayakumar et al., 2016; Cao and Clark, 2017; Zhao et al., 2017).
",2 Related Work,[0],[0]
"Our work differs from the above as none of these above approaches utilize similar conversation contexts observed in the training data explicitly.
",2 Related Work,[0],[0]
"Retrieval based models on the other hand treat the conversation context as a query and obtain a set of responses using information retrieval (IR) techniques from the conversation logs (Ji et al., 2014).",2 Related Work,[0],[0]
"There has been further work where the responses are further ranked using a deep learning based model (Yan et al., 2016a,b; Qiu et al., 2017).",2 Related Work,[0],[0]
"On the other hand of the spectrum, endto-end deep learning based rankers have also been employed to generate responses (Wu et al., 2017; Henderson et al., 2017).",2 Related Work,[0],[0]
"Recently a framework has also been proposed that uses a discriminative dialog network that ranks the candidate responses received from a response generator network and trains both the networks in an end to end manner (Lu et al., 2017a).
",2 Related Work,[0],[0]
"In contrast to the above models, we use the input contexts as well as the retrieved responses for generating the final responses.",2 Related Work,[0],[0]
"Contemporaneous to our work, a generative model for machine translation that employs retrieved translation pairs has also been proposed (Gu et al., 2017).",2 Related Work,[0],[0]
"We note that while the underlying premise of both the papers remains the same, the difference lies in the mechanism of incorporating the retrieved data.",2 Related Work,[0],[0]
A conversation consists of a sequence of utterances.,3.1 Overview,[0],[0]
"At a given point in the conversation, the utterances expressed prior to it are jointly referred to as the context.",3.1 Overview,[0],[0]
The utterance that immediately follows the context is referred to as the response.,3.1 Overview,[0],[0]
"As discussed in Section 1, given a conversational context, we wish to to generate a response by utilizing similar context-response pairs from the training data.",3.1 Overview,[0],[0]
We retrieve a set of K exemplar contextresponse pairs from an inverted index created using the training data in an off-line manner.,3.1 Overview,[0],[0]
"The input and the retrieved context-response pairs are then fed to the Exemplar Encoder Decoder (EED)
network.",3.1 Overview,[0],[0]
A schematic illustration of the EED network is presented in Figure 1.,3.1 Overview,[0],[0]
The EED encoder combines the input context and the retrieved responses to create a set of exemplar vectors.,3.1 Overview,[0],[0]
The EED decoder then uses the exemplar vectors based on the similarity between the input context and retrieved contexts to generate a response.,3.1 Overview,[0],[0]
We now provide details of each of these modules.,3.1 Overview,[0],[0]
"Given a large collection of conversations as (context, response) pairs, we index each response and its corresponding context in tf",3.2 Retrieval of Similar Context-Response Pairs,[0],[0]
− idf vector space.,3.2 Retrieval of Similar Context-Response Pairs,[0],[0]
"We further extract the last turn of a conversation and index it as an additional attribute of the context-response document pairs so as to allow directed queries based on it.
",3.2 Retrieval of Similar Context-Response Pairs,[0],[0]
"Given an input context c, we construct a query that weighs the last utterance in the context twice as much as the rest of the context and use it to retrieve the top-k similar context-response pairs from the index based on a BM25 (Robertson et al., 2009) retrieval model.",3.2 Retrieval of Similar Context-Response Pairs,[0],[0]
"These retrieved pairs form our exemplar context-response pairs (c(k), r(k)), 1 ≤ k ≤ K.",3.2 Retrieval of Similar Context-Response Pairs,[0],[0]
"Given the exemplar pairs (c(k), r(k)), 1 ≤ k ≤ K and an input context-response pair (c, r), we feed the input context c and the exemplar contexts c(1), . . .",3.3 Exemplar Encoder Network,[0],[0]
", c(K)",3.3 Exemplar Encoder Network,[0],[0]
"through an encoder to generate the embeddings as given below:
ce = Encodec(c)
c(k)e =",3.3 Exemplar Encoder Network,[0],[0]
"Encodec(c (k)), 1 ≤ k ≤",3.3 Exemplar Encoder Network,[0],[0]
"K
Note that we do not constrain our choice of encoder and that any parametrized differentiable architecture can be used as the encoder to generate the above embeddings.",3.3 Exemplar Encoder Network,[0],[0]
"Similarly, we feed the exemplar responses r(1), . . .",3.3 Exemplar Encoder Network,[0],[0]
", r(K) through a response encoder to generate response embeddings r (1) e , . . .",3.3 Exemplar Encoder Network,[0],[0]
", r (K) e , that is,
r(k)e =",3.3 Exemplar Encoder Network,[0],[0]
"Encoder(r (k)), 1 ≤ k ≤",3.3 Exemplar Encoder Network,[0],[0]
"K (1)
",3.3 Exemplar Encoder Network,[0],[0]
"Next, we concatenate the exemplar response encoding r(k)e with an encoded representation of current context ce as shown in equation 2 to create the exemplar vector e(k).",3.3 Exemplar Encoder Network,[0],[0]
"This allows us to include in-
formation about similar responses along with the encoded input context representation.
e(k) =",3.3 Exemplar Encoder Network,[0],[0]
"[ce; r (k) e ], 1 ≤ k ≤",3.3 Exemplar Encoder Network,[0],[0]
"K (2)
",3.3 Exemplar Encoder Network,[0],[0]
"The exemplar vectors e(k), 1 ≤ k ≤",3.3 Exemplar Encoder Network,[0],[0]
K are further used by the decoder for generating the ground truth response as described in the next section.,3.3 Exemplar Encoder Network,[0],[0]
Recall that we want the exemplar responses to help generate the responses based on how similar the corresponding contexts are with the input context.,3.4 Exemplar Decoder Network,[0],[0]
"More similar an exemplar context is to the input context, higher should be its effect in generating the response.",3.4 Exemplar Decoder Network,[0],[0]
"To this end, we compute the similarity scores s(k), 1 ≤ k ≤ K using the encodings computed in Section 3.3 as shown below.
s(k) =",3.4 Exemplar Decoder Network,[0],[0]
"exp(cTe c (k) e )∑K
l=1",3.4 Exemplar Decoder Network,[0],[0]
"exp(c T e c (l) e )
(3)
",3.4 Exemplar Decoder Network,[0],[0]
"Next, each exemplar vector e(k) computed in Section 3.3, is fed to a decoder, where the decoder is responsible for predicting the ground truth response from the exemplar vector.",3.4 Exemplar Decoder Network,[0],[0]
Let pdec(r|e(k)),3.4 Exemplar Decoder Network,[0],[0]
be the distribution of generating the ground truth response given the exemplar embedding.,3.4 Exemplar Decoder Network,[0],[0]
"The objective function to be maximized, is expressed as a
function of the scores s(k), the decoding distribution pdec and the exemplar vectors e(k) as shown below:
ll = K∑ k=1 s(k) log pdec(r|e(k))",3.4 Exemplar Decoder Network,[0],[0]
"(4)
Note that we weigh the contribution of each exemplar vector to the final objective based on how similar the corresponding context is to the input context.",3.4 Exemplar Decoder Network,[0],[0]
"Moreover, the similarities are differentiable function of the input and hence, trainable by back propagation.",3.4 Exemplar Decoder Network,[0],[0]
"The model should learn to assign higher similarities to the exemplar contexts, whose responses are helpful for generating the correct response.
",3.4 Exemplar Decoder Network,[0],[0]
The model description uses encoder and decoder networks that can be implemented using any differentiable parametrized architecture.,3.4 Exemplar Decoder Network,[0],[0]
We discuss our choices for the encoders and decoder in the next section.,3.4 Exemplar Decoder Network,[0],[0]
"In this section, we discuss the various encoders and the decoder used by our model.",3.5 The Encoders and Decoder,[0],[0]
The conversation context consists of an ordered sequence of utterances and each utterance can be further viewed as a sequence of words.,3.5 The Encoders and Decoder,[0],[0]
"Thus, context can be viewed as having multiple levels of
hierarchies—at the word level and then at the utterance (sentence) level.",3.5 The Encoders and Decoder,[0],[0]
"We use a hierarchical recurrent encoder—popularly employed as part of the HRED framework for generating responses and query suggestions (Sordoni et al., 2015a; Serban et al., 2016, 2017b).",3.5 The Encoders and Decoder,[0],[0]
The word-level encoder encodes the vector representations of words of an utterance to an utterance vector.,3.5 The Encoders and Decoder,[0],[0]
"Finally, the utterance-level encoder encodes the utterance vectors to a context vector.
",3.5 The Encoders and Decoder,[0],[0]
"Let (u1, . . .",3.5 The Encoders and Decoder,[0],[0]
",uN ) be the utterances present in the context.",3.5 The Encoders and Decoder,[0],[0]
"Furthermore, let (wn1, . . .",3.5 The Encoders and Decoder,[0],[0]
", wnMn) be the words present in the nth utterance for 1 ≤ n ≤",3.5 The Encoders and Decoder,[0],[0]
N .,3.5 The Encoders and Decoder,[0],[0]
"For each word in the utterance, we retrieve its corresponding embedding from an embedding matrix.",3.5 The Encoders and Decoder,[0],[0]
The word embedding for wnm will be denoted as wenm.,3.5 The Encoders and Decoder,[0],[0]
"The encoding of the nth utterance can be computed iteratively as follows:
hnm = f1(hnm−1, wenm), 1 ≤ m ≤Mn (5)
We use an LSTM (Hochreiter and Schmidhuber, 1997) to model the above equation.",3.5 The Encoders and Decoder,[0],[0]
"The last hidden state hnMn is referred to as the utterance encoding and will be denoted as hn.
",3.5 The Encoders and Decoder,[0],[0]
"The utterance-level encoder takes the utterance encodings h1, . . .",3.5 The Encoders and Decoder,[0],[0]
", hN as input and generates the encoding for the context as follows:
cen = f2(cen−1, hn), 1 ≤ n ≤",3.5 The Encoders and Decoder,[0],[0]
"N (6)
",3.5 The Encoders and Decoder,[0],[0]
"Again, we use an LSTM to model the above equation.",3.5 The Encoders and Decoder,[0],[0]
"The last hidden state ceN is referred to as the context embedding and is denoted as ce.
",3.5 The Encoders and Decoder,[0],[0]
A single level LSTM is used for embedding the response.,3.5 The Encoders and Decoder,[0],[0]
"In particular, let (w1, . . .",3.5 The Encoders and Decoder,[0],[0]
", wM ) be the sequence of words present in the response.",3.5 The Encoders and Decoder,[0],[0]
"For each word w, we retrieve the corresponding word embedding we from a word embedding matrix.",3.5 The Encoders and Decoder,[0],[0]
"The response embedding is computed from the word embeddings iteratively as follows:
rem = g(rem−1, wem), 1 ≤ m ≤M (7)
",3.5 The Encoders and Decoder,[0],[0]
"Again, we use an LSTM to model the above equation.",3.5 The Encoders and Decoder,[0],[0]
The last hidden state rem is referred to as the response embedding and is denoted as re.,3.5 The Encoders and Decoder,[0],[0]
"We conduct experiments on Ubuntu Dialogue Corpus (Lowe et al., 2015)(v2.0)2.",4.1.1 Ubuntu Dataset,[0],[0]
Ubuntu dialogue corpus has about 1M context response pairs along with a label.,4.1.1 Ubuntu Dataset,[0],[0]
The label value 1 indicates that the response associated with a context is the correct response and is incorrect otherwise.,4.1.1 Ubuntu Dataset,[0],[0]
As we are only interested in positive labeled data we work with label = 1.,4.1.1 Ubuntu Dataset,[0],[0]
Table 2 depicts some statistics for the dataset.,4.1.1 Ubuntu Dataset,[0],[0]
We also conduct our experiments on a large technical support dataset with more than 127K conversations.,4.1.2 Tech Support Dataset,[0],[0]
We will refer to this dataset as Tech Support dataset in the rest of the paper.,4.1.2 Tech Support Dataset,[0],[0]
"Tech Support dataset contains conversations pertaining to an employee seeking assistance from an agent (technical support) — to resolve problems such as password reset, software installation/licensing, and wireless access.",4.1.2 Tech Support Dataset,[0],[0]
"In contrast to Ubuntu dataset, this dataset has clearly two distinct users — employee and agent.",4.1.2 Tech Support Dataset,[0],[0]
"In our experiments we model the agent responses only.
",4.1.2 Tech Support Dataset,[0],[0]
"For each conversation in the tech support data, we sample context and response pairs to create a dataset similar to the Ubuntu dataset format.",4.1.2 Tech Support Dataset,[0],[0]
Note that multiple context-response pairs can be generated from a single conversation.,4.1.2 Tech Support Dataset,[0],[0]
"For each conversation, we sample 25% of the possible contextresponse pairs.",4.1.2 Tech Support Dataset,[0],[0]
We create validation pairs by selecting 5000 conversations randomly and sampling context response pairs).,4.1.2 Tech Support Dataset,[0],[0]
"Similarly, we create test pairs from a different subset of 5000 conversations.",4.1.2 Tech Support Dataset,[0],[0]
"The remaining conversations are used to
2https://github.com/rkadlec/ ubuntu-ranking-dataset-creator
create training context-response pairs.",4.1.2 Tech Support Dataset,[0],[0]
Table 3 depicts some statistics for this dataset:,4.1.2 Tech Support Dataset,[0],[0]
"The EED and HRED models were implemented using the PyTorch framework (Paszke et al., 2017).",4.2 Model and Training Details,[0],[0]
We initialize the word embedding matrix as well as the weights of context and response encoders from the standard normal distribution with mean 0 and variance 0.01.,4.2 Model and Training Details,[0],[0]
The biases of the encoders and decoder are initialized with 0.,4.2 Model and Training Details,[0],[0]
The word embedding matrix is shared by the context and response encoders.,4.2 Model and Training Details,[0],[0]
"For Ubuntu dataset, we use a word embedding size of 600, whereas the size of the hidden layers of the LSTMs in context and response encoders and the decoder is fixed at 1200.",4.2 Model and Training Details,[0],[0]
"For Tech support dataset, we use a word embedding size of 128.",4.2 Model and Training Details,[0],[0]
"Furthermore, the size of the hidden layers of the multiple LSTMs in context and response encoders and the decoder is fixed at 256.",4.2 Model and Training Details,[0],[0]
"A smaller embedding size was chosen for the Tech Support dataset since we observed much less diversity in the responses of the Tech Support dataset as compared to Ubuntu dataset.
",4.2 Model and Training Details,[0],[0]
Two different encoders are used for encoding the input context (not shown in Figure 1 for simplicity).,4.2 Model and Training Details,[0],[0]
The output of the first context encoder is concatenated with the exemplar response vectors to generate exemplar vectors as detailed in Section 3.3.,4.2 Model and Training Details,[0],[0]
The output of the second context encoder is used to compute the scoring function as detailed in Section 3.4.,4.2 Model and Training Details,[0],[0]
"For each input context, we retrieve 5 similar context-response pairs for Ubuntu dataset and 3 context-response pairs for Tech support dataset using the tf-idf mechanism discussed in Section 3.2.
",4.2 Model and Training Details,[0],[0]
"We use the Adam optimizer (Kingma and Ba, 2014) with a learning rate of 1e",4.2 Model and Training Details,[0],[0]
− 4 for training the model.,4.2 Model and Training Details,[0],[0]
"A batch size of 20 samples was used
during training.",4.2 Model and Training Details,[0],[0]
"In order to prevent overfitting, we use early stopping with log-likelihood on validation set as the stopping criteria.",4.2 Model and Training Details,[0],[0]
"In order to generate the samples using the proposed EED model, we identify the exemplar context that is most similar to the input context based on the learnt scoring function discussed in Section 3.4.",4.2 Model and Training Details,[0],[0]
The corresponding exemplar vector is fed to the decoder to generate the response.,4.2 Model and Training Details,[0],[0]
The samples are generated using a beam search with width 5.,4.2 Model and Training Details,[0],[0]
The average per-word log-likelihood is used to score the beams.,4.2 Model and Training Details,[0],[0]
"A traditional and popular metric used for comparing a generated sentence with a ground truth sentence is BLEU (Papineni et al., 2002) and is frequently used to evaluate machine translation.",5.1.1 Activity and Entity Metrics,[0],[0]
"The metric has also been applied to compute scores for predicted responses in conversations, but it has been found to be less indicative of actual performance (Liu et al., 2016; Sordoni et al., 2015a; Serban et al., 2017a), as it is extremely sensitive to the exact words in the ground truth response, and gives equal importance to stop words/phrases and informative words.
",5.1.1 Activity and Entity Metrics,[0],[0]
Serban et al. (2017a) recently proposed a new set of metrics for evaluating dialogue responses for the Ubuntu corpus.,5.1.1 Activity and Entity Metrics,[0],[0]
"It is important to highlight that these metrics have been specifically designed for the Ubuntu corpus and evaluate a generated response with the ground truth response by comparing the coarse level representation of an utterance (such as entities, activities, Ubuntu OS commands).",5.1.1 Activity and Entity Metrics,[0],[0]
"Here is a brief description of each metric:
• Activity: Activity metric compares the activities present in a predicted response with the ground truth response.",5.1.1 Activity and Entity Metrics,[0],[0]
Activity can be thought of as a verb.,5.1.1 Activity and Entity Metrics,[0],[0]
"Thus, all the verbs in a response are mapped to a set of manually identified list of 192 verbs.
",5.1.1 Activity and Entity Metrics,[0],[0]
•,5.1.1 Activity and Entity Metrics,[0],[0]
Entity:,5.1.1 Activity and Entity Metrics,[0],[0]
This compares the technical entities that overlap with the ground truth response.,5.1.1 Activity and Entity Metrics,[0],[0]
"A total of 3115 technical entities is identified using public resources such as Debian package manager APT.
",5.1.1 Activity and Entity Metrics,[0],[0]
•,5.1.1 Activity and Entity Metrics,[0],[0]
"Tense: This measure compares the time tense of ground truth with predicted response.
",5.1.1 Activity and Entity Metrics,[0],[0]
• Cmd:,5.1.1 Activity and Entity Metrics,[0],[0]
"This metric computes accuracy by comparing commands identified in ground truth utterance with a predicted response.
",5.1.1 Activity and Entity Metrics,[0],[0]
"Table 4 compares our model with other recent generative models (Serban et al., 2017a) — LSTM (Shang et al., 2015), HRED (Serban et al., 2016) & VHRED (Serban et al.,",5.1.1 Activity and Entity Metrics,[0],[0]
"2017b).We do not compare our model with Multi-Resolution RNN (MRNN) (Serban et al., 2017a), as MRNN explicitly utilizes the activities and entities during the generation process.",5.1.1 Activity and Entity Metrics,[0],[0]
"In contrast, the proposed EED model and the other models used for comparison are agnostic to the activity and entity information.",5.1.1 Activity and Entity Metrics,[0],[0]
"We use the standard script3 to compute the metrics.
",5.1.1 Activity and Entity Metrics,[0],[0]
"The EED model scores better than generative models on almost all of the metrics, indicating that we generate more informative responses than other state-of-the-art generative based approaches for Ubuntu corpus.",5.1.1 Activity and Entity Metrics,[0],[0]
"The results show that responses associated with similar contexts may contain the activities and entities present in the ground truth response, and thus help in response generation.",5.1.1 Activity and Entity Metrics,[0],[0]
This is discussed further in Section 5.2.,5.1.1 Activity and Entity Metrics,[0],[0]
"Additionally, we compared our proposed EED with a retrieval only baseline.",5.1.1 Activity and Entity Metrics,[0],[0]
"The retrieval baseline achieves an activity F1 score of 4.23 and entity F1 score of 2.72 compared to 4.87 and 2.99 respectively achieved by our method on the Ubuntu corpus.
",5.1.1 Activity and Entity Metrics,[0],[0]
"The Tech Support dataset is not evaluated using the above metrics, since activity and entity information is not available for this dataset.
",5.1.1 Activity and Entity Metrics,[0],[0]
3https://github.com/julianser/Ubuntu-MultiresolutionTools/blob/master/ActEntRepresentation/eval file.sh,5.1.1 Activity and Entity Metrics,[0],[0]
"Embedding metrics (Lowe et al., 2017) were proposed as an alternative to word by word comparison metrics such as BLEU.",5.1.2 Embedding Metrics,[0],[0]
"We use pre-trained Google news word embeddings4 similar to Serban et al. (2017b), for easy reproducibility as these metrics are sensitive to the word embeddings used.",5.1.2 Embedding Metrics,[0],[0]
"The three metrics of interest utilize the word vectors in ground truth response and a predicted response and are discussed below:
• Average: Average word embedding vectors are computed for the candidate response and ground truth.",5.1.2 Embedding Metrics,[0],[0]
The cosine similarity is computed between these averaged embeddings.,5.1.2 Embedding Metrics,[0],[0]
"High similarity gives as indication that ground truth and predicted response have similar words.
",5.1.2 Embedding Metrics,[0],[0]
"• Greedy: Greedy matching score finds the most similar word in predicted response to ground truth response using cosine similarity.
",5.1.2 Embedding Metrics,[0],[0]
"• Extrema: Vector extrema score computes the maximum or minimum value of each dimension of word vectors in candidate response and ground truth.
",5.1.2 Embedding Metrics,[0],[0]
"Of these, the embedding average metric is the most reflective of performance for our setup.",5.1.2 Embedding Metrics,[0],[0]
"The extrema representation, for instance, is very sensitive to text length and becomes ineffective beyond single length sentences(Forgues et al., 2014).",5.1.2 Embedding Metrics,[0],[0]
We use the publicly available script5 for all our computations.,5.1.2 Embedding Metrics,[0],[0]
"As the test outputs for HRED are not available for Technical Support dataset, we use our
4GoogleNews-vectors-negative300.bin from https:// code.google.com/archive/p/word2vec/
5https://github.com/julianser/ hed-dlg-truncated/blob/master/",5.1.2 Embedding Metrics,[0],[0]
"Evaluation/embedding_metrics.py
own implementation of HRED.",5.1.2 Embedding Metrics,[0],[0]
"Table 5 compares our model with HRED, and depicts that our model scores better on all metrics for Technical Support
dataset, and on majority of the metrics for Ubuntu dataset.
",5.1.2 Embedding Metrics,[0],[0]
"We note that the improvement achieved by the
EED model on activity and entity metrics are much more significant than those on embedding metrics.",5.1.2 Embedding Metrics,[0],[0]
"This suggests that the EED model is better able to capture the specific information (objects and actions) present in the conversations.
",5.1.2 Embedding Metrics,[0],[0]
"Finally, we evaluate the diversity of the generated responses for EED against HRED by counting the number of unique tokens, token-pairs and token-triplets present in the generated responses on Ubuntu and Tech Support dataset.",5.1.2 Embedding Metrics,[0],[0]
The results are shown in Table 6.,5.1.2 Embedding Metrics,[0],[0]
"As can be observed, the responses in EED have a larger number of distinct tokens, token-pairs and token-triplets than HRED, and hence, are arguably more diverse.",5.1.2 Embedding Metrics,[0],[0]
"Table 7 presents the responses generated by HRED, VHRED and the proposed EED for a few selected contexts along with the corresponding similar exemplar responses.",5.2 Qualitative Evaluation,[0],[0]
"As can be observed from the table, the responses generated by EED tend to be more specific to the input context as compared to the responses of HRED and VHRED.",5.2 Qualitative Evaluation,[0],[0]
"For example, in conversations 1 and 2 we find that both HRED and VHRED generate simple generic responses whereas EED generates responses with additional information such as the type of disk partition used or a command not working.",5.2 Qualitative Evaluation,[0],[0]
This is also confirmed by the quantitative results obtained using activity and entity metrics in the previous section.,5.2 Qualitative Evaluation,[0],[0]
We further observe that the exemplar responses contain informative words that are utilized by the EED model for generating the responses as highlighted in Table 7.,5.2 Qualitative Evaluation,[0],[0]
"In this work, we propose a deep learning method, Exemplar Encoder Decoder (EED), that given a conversation context uses similar contexts and corresponding responses from training data for generating a response.",6 Conclusions,[0],[0]
We show that by utilizing this information the system is able to outperform state of the art generative models on publicly available Ubuntu dataset.,6 Conclusions,[0],[0]
"We further show improvements achieved by the proposed method on a large collection of technical support conversations.
",6 Conclusions,[0],[0]
"While in this work, we apply the exemplar encoder decoder network on conversational task, the method is generic and could be used with other tasks such as question answering and machine translation.",6 Conclusions,[0],[0]
"In our future work we plan to extend
the proposed method to these other applications.",6 Conclusions,[0],[0]
We are grateful to the anonymous reviewers for their comments that helped in improving the paper.,Acknowledgements,[0],[0]
"In this paper we present the Exemplar Encoder-Decoder network (EED), a novel conversation model that learns to utilize similar examples from training data to generate responses.",abstractText,[0],[0]
Similar conversation examples (context-response pairs) from training data are retrieved using a traditional TF-IDF based retrieval model.,abstractText,[0],[0]
The retrieved responses are used to create exemplar vectors that are used by the decoder to generate the response.,abstractText,[0],[0]
The contribution of each retrieved response is weighed by the similarity of corresponding context with the input context.,abstractText,[0],[0]
We present detailed experiments on two large data sets and find that our method outperforms state of the art sequence to sequence generative models on several recently proposed evaluation metrics.,abstractText,[0],[0]
We also observe that the responses generated by the proposed EED model are more informative and diverse compared to existing state-of-the-art method.,abstractText,[0],[0]
Exemplar Encoder-Decoder for Neural Conversation Generation,title,[0],[0]
"Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 377–382, Baltimore, Maryland, USA, June 23-25 2014. c©2014 Association for Computational Linguistics",text,[0],[0]
"Training good predictive NLP models typically requires annotated data, but getting professional annotators to build useful data sets is often timeconsuming and expensive.",1 Introduction,[0],[0]
"Snow et al. (2008) showed, however, that crowdsourced annotations can produce similar results to annotations made by experts.",1 Introduction,[0],[0]
"Crowdsourcing services such as Amazon’s Mechanical Turk has since been successfully used for various annotation tasks in NLP (Jha et al., 2010; Callison-Burch and Dredze, 2010).
",1 Introduction,[0],[0]
"However, most applications of crowdsourcing in NLP have been concerned with classification problems, such as document classification and constructing lexica (Callison-Burch and Dredze, 2010).",1 Introduction,[0],[0]
"A large part of NLP problems, however, are structured prediction tasks.",1 Introduction,[0],[0]
"Typically, sequence labeling tasks employ a larger set of labels than classification problems, as well as complex interactions between the annotations.",1 Introduction,[0],[0]
"Disagreement among annotators is therefore potentially higher, and the task of annotating structured data thus harder.
",1 Introduction,[0],[0]
"Only a few recent studies have investigated crowdsourcing sequential tasks; specifically, named entity recognition (Finin et al., 2010; Rodrigues et al., 2013).",1 Introduction,[0],[0]
Results for this are good.,1 Introduction,[0],[0]
"However, named entities typically use only few labels (LOC, ORG, and PER), and the data contains mostly non-entities, so the complexity is manageable.",1 Introduction,[0],[0]
"The question of whether a more linguistically involved structured task like part-of-speech (POS) tagging can be crowdsourced has remained largely unaddressed.1
In this paper, we investigate how well lay annotators can produce POS labels for Twitter data.",1 Introduction,[0],[0]
"In our setup, we present annotators with one word at a time, with a minimal surrounding context (two words to each side).",1 Introduction,[0],[0]
"Our choice of annotating Twitter data is not coincidental: with the shortlived nature of Twitter messages, models quickly lose predictive power (Eisenstein, 2013), and retraining models on new samples of more representative data becomes necessary.",1 Introduction,[0],[0]
Expensive professional annotation may be prohibitive for keeping NLP models up-to-date with linguistic and topical changes on Twitter.,1 Introduction,[0],[0]
"We use a minimum of instructions and require few qualifications.
",1 Introduction,[0],[0]
"Obviously, lay annotation is generally less reliable than professional annotation.",1 Introduction,[0],[0]
It is therefore common to aggregate over multiple annotations for the same item to get more robust annotations.,1 Introduction,[0],[0]
"In this paper we compare two aggregation schemes, namely majority voting (MV) and MACE (Hovy et al., 2013).",1 Introduction,[0],[0]
"We also show how we can use Wiktionary, a crowdsourced lexicon, to filter crowdsourced annotations.",1 Introduction,[0],[0]
"We evaluate the annotations in several ways: (a) by testing their accuracy with respect to a gold standard, (b) by evaluating the performance of POS models trained on
1One of the reviewers alerted us to an unpublished masters thesis, which uses pre-annotation to reduce tagging to fewer multiple-choice questions.",1 Introduction,[0],[0]
"See Related Work section for details.
",1 Introduction,[0],[0]
"377
the annotations across several existing data sets, as well as (c) by applying our models in downstream tasks.",1 Introduction,[0],[0]
"We show that with minimal context and annotation effort, we can produce structured annotations of near-expert quality.",1 Introduction,[0],[0]
"We also show that these annotations lead to better POS tagging models than previous models learned from crowdsourced lexicons (Li et al., 2012).",1 Introduction,[0],[0]
"Finally, we show that models learned from these annotations are competitive with models learned from expert annotations on various downstream tasks.",1 Introduction,[0],[0]
We crowdsource the training section of the data from Gimpel et al. (2011)2 with POS tags.,2 Our Approach,[0],[0]
"We use Crowdflower,3 to collect five annotations for each word, and then find the most likely label for each word among the possible annotations.",2 Our Approach,[0],[0]
See Figure 1 for an example.,2 Our Approach,[0],[0]
"If the correct label is not among the annotations, we are unable to recover the correct answer.",2 Our Approach,[0],[0]
This was the case for 1497 instances in our data (cf.,2 Our Approach,[0],[0]
the token “:” in the example).,2 Our Approach,[0],[0]
"We thus report on oracle score, i.e., the best label sequence that could possibly be found, which is correct except for the missing tokens.",2 Our Approach,[0],[0]
"Note that while we report agreement between the crowdsourced annotations and the crowdsourced annotations, our main evaluations are based on models learned from expert vs. crowdsourced annotations and downstream applications thereof (chunking and NER).",2 Our Approach,[0],[0]
We take care in evaluating our models across different data sets to avoid biasing our evaluations to particular annotations.,2 Our Approach,[0],[0]
"All the data sets used in our experiments are publicly available at http://lowlands.ku.dk/results/.
2http://www.ark.cs.cmu.edu/TweetNLP/ 3http://crowdflower.com",2 Our Approach,[0],[0]
"In order to use the annotations to train models that can be applied across various data sets, i.e., making out-of-sample evaluation possible (see Section 5), we follow Hovy et al. (2014) in using the universal tag set (Petrov et al., 2012) with 12 labels.
",3 Crowdsourcing Sequential Annotation,[0],[0]
Annotators were given a bold-faced word with two words on either side and asked to select the most appropriate tag from a drop down menu.,3 Crowdsourcing Sequential Annotation,[0],[0]
"For each tag, we spell out the name of the syntactic category, and provide a few example words.",3 Crowdsourcing Sequential Annotation,[0],[0]
See Figure 2 for a screenshot of the interface.,3 Crowdsourcing Sequential Annotation,[0],[0]
"Annotators were also told that words can belong to several classes, depending on the context.",3 Crowdsourcing Sequential Annotation,[0],[0]
"No additional guidelines were given.
",3 Crowdsourcing Sequential Annotation,[0],[0]
Only trusted annotators (in Crowdflower: Bronze skills) that had answered correctly on 4 gold tokens (randomly chosen from a set of 20 gold tokens provided by the authors) were allowed to submit annotations.,3 Crowdsourcing Sequential Annotation,[0],[0]
"In total, 177 individual annotators supplied answers.",3 Crowdsourcing Sequential Annotation,[0],[0]
We paid annotators a reward of $0.05 for 10 tokens.,3 Crowdsourcing Sequential Annotation,[0],[0]
"The full data set contains 14,619 tokens.",3 Crowdsourcing Sequential Annotation,[0],[0]
Completion of the task took slightly less than 10 days.,3 Crowdsourcing Sequential Annotation,[0],[0]
Contributors were very satisfied with the task (4.5 on a scale from 1 to 5).,3 Crowdsourcing Sequential Annotation,[0],[0]
"In particular, they felt instructions were clear (4.4/5), and that the pay was reasonable (4.1/5).",3 Crowdsourcing Sequential Annotation,[0],[0]
"After collecting the annotations, we need to aggregate the annotations to derive a single answer for each token.",4 Label Aggregation,[0],[0]
"In the simplest scheme, we choose the majority label, i.e., the label picked by most annotators.",4 Label Aggregation,[0],[0]
"In case of ties, we select the final label at random.",4 Label Aggregation,[0],[0]
"Since this is a stochastic process, we average results over 100 runs.",4 Label Aggregation,[0],[0]
We refer to this as MAJORITY VOTING (MV).,4 Label Aggregation,[0],[0]
Note that in MV we trust all annotators to the same degree.,4 Label Aggregation,[0],[0]
"However, crowdsourcing attracts people with different mo-
tives, and not all of them are equally reliable— even the ones with Bronze level.",4 Label Aggregation,[0],[0]
"Ideally, we would like to factor this into our decision process.
",4 Label Aggregation,[0],[0]
"We use MACE4 (Hovy et al., 2013) as our second scheme to learn both the most likely answer and a competence estimate for each of the annotators.",4 Label Aggregation,[0],[0]
"MACE treats annotator competence and the correct answer as hidden variables and estimates their parameters via EM (Dempster et al., 1977).",4 Label Aggregation,[0],[0]
"We use MACE with default parameter settings to give us the weighted average for each annotated example.
",4 Label Aggregation,[0],[0]
"Finally, we also tried applying the joint learning scheme in Rodrigues et al. (2013), but their scheme requires that entire sequences are annotated by the same annotators, which we don’t have, and it expects BIO sequences, rather than POS tags.
",4 Label Aggregation,[0],[0]
"Dictionaries Decoding tasks profit from the use of dictionaries (Merialdo, 1994; Johnson, 2007; Ravi and Knight, 2009) by restricting the number of tags that need to be considered for each word, also known as type constraints (Täckström et al., 2013).",4 Label Aggregation,[0],[0]
"We follow Li et al. (2012) in including Wiktionary information as type constraints into our decoding: if a word is found in Wiktionary, we disregard all annotations that are not licensed by the dictionary entry.",4 Label Aggregation,[0],[0]
"If the word is not found in Wiktionary, or if none of its annotations is licensed by Wiktionary, we keep the original annotations.",4 Label Aggregation,[0],[0]
"Since we aggregate annotations independently (unlike Viterbi decoding), we basically use Wiktionary as a pre-filtering step, such that MV and MACE only operate on the reduced annotations.",4 Label Aggregation,[0],[0]
Each of the two aggregation schemes above produces a final label sequence ŷ for our training corpus.,5 Experiments,[0],[0]
"We evaluate the resulting annotated data in three ways.
1.",5 Experiments,[0],[0]
We compare ŷ to the available expert annotation on the training data.,5 Experiments,[0],[0]
"This tells us how similar lay annotation is to professional annotation.
2.",5 Experiments,[0],[0]
"Ultimately, we want to use structured annotations for supervised training, where annotation quality influences model performance on held-out test data.",5 Experiments,[0],[0]
"To test this, we train a CRF model (Lafferty et al., 2001) with simple orthographic features and word clusters (Owoputi et al., 2013)
4http://www.isi.edu/publications/ licensed-sw/mace/
on the annotated Twitter data described in Gimpel et al. (2011).",5 Experiments,[0],[0]
"Leaving out the dedicated test set to avoid in-sample bias, we evaluate our models across three data sets: RITTER (the 10% test split of the data in Ritter et al. (2011) used in Derczynski et al. (2013)), the test set from Foster et al. (2011), and the data set described in Hovy et al. (2014).
",5 Experiments,[0],[0]
We will make the preprocessed data sets available to the public to facilitate comparison.,5 Experiments,[0],[0]
"In addition to a supervised model trained on expert annotations, we compare our tagging accuracy with that of a weakly supervised system (Li et al., 2012) re-trained on 400,000 unlabeled tweets to adapt to Twitter, but using a crowdsourced lexicon, namely Wiktionary, to constrain inference.",5 Experiments,[0],[0]
"We use parameter settings from Li et al. (2012), as well as their Wikipedia dump, available from their project website.5
3.",5 Experiments,[0],[0]
"POS tagging is often the first step for further analysis, such as chunking, parsing, etc.",5 Experiments,[0],[0]
We test the downstream performance of the POS models from the previous step on chunking and NER.,5 Experiments,[0],[0]
"We use the models to annotate the training data portion of each task with POS tags, and use them as features in a chunking and NER model.",5 Experiments,[0],[0]
"For both tasks, we train a CRF model on the respective (POS-augmented) training set, and evaluate it on several held-out test sets.",5 Experiments,[0],[0]
"For chunking, we use the test sets from Foster et al. (2011) and Ritter et al. (2011) (with the splits from Derczynski et al. (2013)).",5 Experiments,[0],[0]
"For NER, we use data from Finin et al. (2010) and again Ritter et al. (2011).",5 Experiments,[0],[0]
"For chunking, we follow Sha and Pereira (2003) for the set of features, including token and POS information.",5 Experiments,[0],[0]
"For NER, we use standard features, including POS tags (from the previous experiments), indicators for hyphens, digits, single quotes, upper/lowercase, 3-character prefix and suffix information, and Brown word cluster features6 with 2,4,8,16 bitstring prefixes estimated from a large Twitter corpus (Owoputi et al., 2013).",5 Experiments,[0],[0]
We report macro-averages over all these data sets.,5 Experiments,[0],[0]
Agreement with expert annotators Table 1 shows the accuracy of each aggregation compared to the gold labels.,6 Results,[0],[0]
"The crowdsourced annotations
5https://code.google.com/p/ wikily-supervised-pos-tagger/
6http://www.ark.cs.cmu.edu/TweetNLP/
aggregated using MV agree with the expert annotations in 79.54% of the cases.",6 Results,[0],[0]
"If we pre-filter the data using Wiktionary, the agreement becomes 80.58%.",6 Results,[0],[0]
MACE leads to higher agreement with expert annotations under both conditions (79.89 and 80.75).,6 Results,[0],[0]
"The small difference indicates that annotators are consistent and largely reliable, thus confirming the Bronze-level qualification we required.",6 Results,[0],[0]
"Both schemes cannot recover the correct answer for the 1497 cases where none of the crowdsourced labels matched the gold label, i.e. y /∈ Zi.",6 Results,[0],[0]
"The best possible result either of them could achieve (the oracle) would be matching all but the missing labels, an agreement of 89.63%.
",6 Results,[0],[0]
Most of the cases where the correct label was not among the annotations belong to a small set of confusions.,6 Results,[0],[0]
"The most frequent was mislabeling “:” and “. . .”, both mapped to X. Annotators mostly decided to label these tokens as punctuation (.).",6 Results,[0],[0]
"They also predominantly labeled your, my and this as PRON (for the former two), and a variety of labels for the latter, when the gold label is DET.
",6 Results,[0],[0]
"Effect on POS Tagging Accuracy Usually, we don’t want to match a gold standard, but we rather want to create new annotated training data.",6 Results,[0],[0]
"Crowdsourcing matches our gold standard to about 80%, but the question remains how useful this data is when training models on it.",6 Results,[0],[0]
"After all, inter-annotator agreement among professional an-
notators on this task is only around 90% (Gimpel et al., 2011; Hovy et al., 2014).",6 Results,[0],[0]
"In order to evaluate how much each aggregation scheme influences tagging performance of the resulting model, we train separate models on each scheme’s annotations and test on the same four data sets.",6 Results,[0],[0]
Table 2 shows the results.,6 Results,[0],[0]
Note that the differences between the four schemes are insignificant.,6 Results,[0],[0]
"More importantly, however, POS tagging accuracy using crowdsourced annotations are on average only 2.6% worse than gold using professional annotations.",6 Results,[0],[0]
"On the other hand, performance is much better than the weakly supervised approach by Li et al. (2012), which only relies on a crowdsourced POS lexicon.
",6 Results,[0],[0]
Downstream Performance Table 3 shows the accuracy when using the POS models trained in the previous evaluation step.,6 Results,[0],[0]
Note that we present the average over the two data sets used for each task.,6 Results,[0],[0]
Note also how the Wiktionary constraints lead to improvements in downstream performance.,6 Results,[0],[0]
"In chunking, we see that using the crowdsourced annotations leads to worse performance than using the professional annotations.",6 Results,[0],[0]
"For NER, however, we find that some of the POS taggers trained on aggregated data produce better NER performance than POS taggers trained on expert-annotated gold data.",6 Results,[0],[0]
"Since the only difference between models are the respective POS features, the results suggest that at least for some tasks, POS taggers learned from crowdsourced annotations may be as good as those learned from expert annotations.",6 Results,[0],[0]
"There is considerable work in the literature on modeling answer correctness and annotator competence as latent variables (Dawid and Skene,
1979; Smyth et al., 1995; Carpenter, 2008; Whitehill et al., 2009; Welinder et al., 2010; Yan et al., 2010; Raykar and Yu, 2012).",7 Related Work,[0],[0]
Rodrigues et al. (2013) recently presented a sequential model for this.,7 Related Work,[0],[0]
They estimate annotator competence as latent variables in a CRF model using EM.,7 Related Work,[0],[0]
"They evaluate their approach on synthetic and NER data annotated on Mechanical Turk, showing improvements over the MV baselines and the multi-label model by Dredze et al. (2009).",7 Related Work,[0],[0]
"The latter do not model annotator reliability but rather model label priors by integrating them into the CRF objective, and re-estimating them during learning.",7 Related Work,[0],[0]
"Both require annotators to supply a full sentence, while we use minimal context, which requires less annotator commitment and makes the task more flexible.",7 Related Work,[0],[0]
"Unfortunately, we could not run those models on our data due to label incompatibility and the fact that we typically do not have complete sequences annotated by the same annotators.
",7 Related Work,[0],[0]
Mainzer (2011) actually presents an earlier paper on crowdsourcing POS tagging.,7 Related Work,[0],[0]
"However, it differs from our approach in several ways.",7 Related Work,[0],[0]
It uses the Penn Treebank tag set to annotate Wikipedia data (which is much more canonical than Twitter) via a Java applet.,7 Related Work,[0],[0]
"The applet automatically labels certain categories, and only presents the users with a series of multiple choice questions for the remainder.",7 Related Work,[0],[0]
"This is highly effective, as it eliminates some sources of possible disagreement.",7 Related Work,[0],[0]
"In contrast, we do not pre-label any tokens, but always present the annotators with all labels.",7 Related Work,[0],[0]
We use crowdsourcing to collect POS annotations with minimal context (five-word windows).,8 Conclusion,[0],[0]
"While the performance of POS models learned from this data is still slightly below that of models trained on expert annotations, models learned from aggregations approach oracle performance for POS tagging.",8 Conclusion,[0],[0]
"In general, we find that the use of a dictionary tends to make aggregations more useful, irrespective of aggregation method.",8 Conclusion,[0],[0]
"For some downstream tasks, models using the aggregated POS tags perform even better than models using expert-annotated tags.",8 Conclusion,[0],[0]
We would like to thank the anonymous reviewers for valuable comments and feedback.,Acknowledgments,[0],[0]
"This research is funded by the ERC Starting Grant LOW-
LANDS",Acknowledgments,[0],[0]
No. 313695.,Acknowledgments,[0],[0]
Crowdsourcing lets us collect multiple annotations for an item from several annotators.,abstractText,[0],[0]
"Typically, these are annotations for non-sequential classification tasks.",abstractText,[0],[0]
"While there has been some work on crowdsourcing named entity annotations, researchers have largely assumed that syntactic tasks such as part-of-speech (POS) tagging cannot be crowdsourced.",abstractText,[0],[0]
This paper shows that workers can actually annotate sequential data almost as well as experts.,abstractText,[0],[0]
"Further, we show that the models learned from crowdsourced annotations fare as well as the models learned from expert annotations in downstream tasks.",abstractText,[0],[0]
Experiments with crowdsourced re-annotation of a POS tagging data set,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3275–3284 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
3275",text,[0],[0]
Character-level features are an essential part of many Natural Language Processing (NLP) tasks.,1 Introduction,[0],[0]
"These features are for instance used for language modeling (Kim et al., 2016), part-of-speech tagging (Plank et al., 2016) and machine translation (Luong and Manning, 2016).",1 Introduction,[0],[0]
"They are especially useful in the context of part-of-speech and morphological tagging, where for example the suffix -s can easily differentiate plural words from singular words in English or Spanish.
",1 Introduction,[0],[0]
The use of character-level features is not new.,1 Introduction,[0],[0]
"Rule-based taggers were amongst the earliest systems that used character-level features/rules for grammatical tagging (Klein and Simmons, 1963).",1 Introduction,[0],[0]
"Other approaches rely on fixed lists of affixes (Ratnaparkhi, 1996; Toutanova et al., 2003).",1 Introduction,[0],[0]
"Next, these features are used by a tagging model, such
as a rule-based model or statistical model.",1 Introduction,[0],[0]
"Rulebased taggers are transparent models that allow us to easily trace back why the tagger made a certain decision (e.g., Brill (1994)).",1 Introduction,[0],[0]
"Similarly, statistical models are merely a weighted sum of features.
",1 Introduction,[0],[0]
"For example, Brill (1994)’s transformationbased error-driven tagger uses a set of templates to derive rules by fixing errors.",1 Introduction,[0],[0]
"The following rule template:
""Change the most-likely tag X to Y if the last (1,2,3,4) characters of the word are x"",
resulted in the rule:
""Change the tag common noun to plural common noun if the word has suffix -s"".
",1 Introduction,[0],[0]
"Subsequently, whenever the tagger makes a tagging mistake, it is easy to trace back why this happened.",1 Introduction,[0],[0]
"Following the above rule, the word mistress will mistakingly be tagged as a plural common noun while it actually is a common noun1.
",1 Introduction,[0],[0]
"This is in stark contrast with the most recent generation of part-of-speech and morphological taggers which mainly rely on neural networks.
1In Brill (1994), an additional rule encodes an exception to this rule to correctly tag the word mistress.
",1 Introduction,[0],[0]
"Words are split into individual characters and are in general either aggregated using a Bidirectional Long Short-Term Memory network (BiLSTM) (Plank et al., 2016) or Convolutional Neural Network (CNN) (dos Santos and Zadrozny, 2014).",1 Introduction,[0],[0]
"However, it is currently unknown which characterlevel patterns these neural network models learn and whether these patterns coincide with our linguistic knowledge.",1 Introduction,[0],[0]
"Moreover, different neural network architectures are currently only compared quantitatively and lack a qualitative analysis.
",1 Introduction,[0],[0]
"In this paper, we investigate which character patterns neural networks learn and to what extent those patterns comprise any known linguistic rules.",1 Introduction,[0],[0]
"We do this for three morphologically different languages: Finnish, Spanish and Swedish.",1 Introduction,[0],[0]
A Spanish example is shown in Figure 1.,1 Introduction,[0],[0]
"By visualizing the contributions of each character, we observe that the model indeed uses the suffix -s to correctly predict that the word is plural.
",1 Introduction,[0],[0]
"Our main contributions are as follows:
• We show how word-level tagging decisions can be traced back to specific sets of characters and interactions between them.
",1 Introduction,[0],[0]
"• We extend the contextual decomposition method (Murdoch et al., 2018) to CNNs.
",1 Introduction,[0],[0]
"• We quantitatively compare CNN and BiLSTM models in the context of morphological tagging by performing an evaluation on three manually segmented and morphologically annotated corpora.
",1 Introduction,[0],[0]
"• We found out that the studied neural models are able to implicitly discover character patterns that coincide with the same rules linguists use to indicate the morphological function of subword segments.
",1 Introduction,[0],[0]
Our implementation is available online2.,1 Introduction,[0],[0]
"Neural network-based taggers currently outperform statistical taggers in morphological tagging (Heigold et al., 2017) and part-of-speech tagging (Plank et al., 2016) for a wide variety of languages.",2 Related Work,[0],[0]
Character-level features form a crucial part of many of these systems.,2 Related Work,[0],[0]
"Generally, two neural network architectures are considered for aggregating the individual characters: a BiLSTM (Ling
2https://github.com/FredericGodin/ ContextualDecomposition-NLP
et al., 2015; Plank et al., 2016) or a CNN (dos Santos and Zadrozny, 2014; Bjerva et al., 2016; Heigold et al., 2017).",2 Related Work,[0],[0]
"These architectures outperform similar models that use manually defined features (Ling et al., 2015; dos Santos and Zadrozny, 2014).",2 Related Work,[0],[0]
"However, it is still unclear which useful character-level features they have learned.",2 Related Work,[0],[0]
Architectures are compared quantitatively but lack insight into learned patterns.,2 Related Work,[0],[0]
"Moreover, Vania and Lopez (2017) showed in the context of language modeling that training a BiLSTM on ground truth morphological features still yields better results than eight other character-based neural network architectures.",2 Related Work,[0],[0]
"Hence, this raises the question which patterns neural networks learn and whether these patterns coincide with manually-defined linguistic rules.
",2 Related Work,[0],[0]
"While a number of interpretation techniques have been proposed for images (Springenberg et al., 2014; Selvaraju et al., 2017; Shrikumar et al., 2017), these are generally not applicable in the context of NLP where LSTMs are mainly used.",2 Related Work,[0],[0]
"Moreover, gradient-based techniques are not trustworthy when strongly saturating activation functions such as tanh and sigmoid are used (e.g., Li et al. (2016a)).",2 Related Work,[0],[0]
"Hence, current interpretations in NLP are limited to visualizing the magnitude of the LSTM hidden states of each word (Linzen et al., 2016; Radford et al., 2017; Strobelt et al., 2018), removing words (Li et al., 2016b; Kádár et al., 2017) or changing words (Linzen et al., 2016) and measuring the impact, or training surrogate tasks (Adi et al., 2017; Chrupała et al., 2017; Belinkov et al., 2017).",2 Related Work,[0],[0]
These techniques only provide limited local interpretations and do not model fine-grained interactions of groups of inputs or intermediate representations.,2 Related Work,[0],[0]
"In contrast, Murdoch et al. (2018) recently introduced an LSTM interpretation technique called Contextual Decomposition (CD), providing a solution to the aforementioned issues.",2 Related Work,[0],[0]
"We will build upon this interpretation technique and introduce an extension for CNNs, making it possible to compare different neural network architectures within a single interpretation framework.",2 Related Work,[0],[0]
"For visualizing the contributions of character sets, we use the recently introduced Contextual Decomposition (CD) framework, as originally developed for LSTMs (Murdoch et al., 2018), and extend it to
CNNs.",3 Method,[0],[0]
"First, we introduce the concept of CD, followed by the extension for CNNs.",3 Method,[0],[0]
"For details on CD for LSTMs, we refer the reader to the aforementioned paper.",3 Method,[0],[0]
"Finally, we explain how the CD of the final classification layer is done.",3 Method,[0],[0]
"The idea behind CD is that, in the context of character-level decomposition, we can decompose the output value of the network for a certain class into two distinct groups of contributions: (1) contributions originating from a specific character or set of characters within a word and (2) contributions originating from all the other characters within the same word.
",3.1 Contextual decomposition,[0],[0]
"More generally, we can decompose every output value z of every neural network component into a relevant contribution β and an irrelevant contribution γ:
z = β + γ (1)",3.1 Contextual decomposition,[0],[0]
"A CNN typically consist of three components: the convolution itself, an activation function and an optional max-pooling operation.",3.2 Decomposing CNN layers,[0],[0]
"We will discuss each component in the next paragraphs.
",3.2 Decomposing CNN layers,[0],[0]
"Decomposing the convolution Given a sequence of character embeddings x1, ...,xT ∈ Rd1 of length T , we can calculate the convolution of size n of a single filter over the sequence x1:T by applying the following equation to each n-length subsequence {xt+i, i = 0, .., n",3.2 Decomposing CNN layers,[0],[0]
"− 1}, denoted as xt:t+n−1:
zt = n−1∑",3.2 Decomposing CNN layers,[0],[0]
"i=0 Wi · xt+i + b, (2)
with zt ∈ R and where W ∈ Rd1×n and b ∈ R are the weight matrix and bias of the convolutional filter.",3.2 Decomposing CNN layers,[0],[0]
"Wi denotes the i-th column of the weight matrix W .
",3.2 Decomposing CNN layers,[0],[0]
"When we want to calculate the contribution of a subset of characters, where S is the set of corresponding character position indexes and S ⊆ {1, ..., T}, we should decompose the output of the filter zt into three parts:
zt = βt + γt + b. (3)
",3.2 Decomposing CNN layers,[0],[0]
"That is, the relevant contribution βt originating from the selected subset of characters with indexes S, the irrelevant contribution γt originating
from the remaining characters in the sequence, and a bias which is deemed neutral (Murdoch et al., 2018).
",3.2 Decomposing CNN layers,[0],[0]
"This can be achieved by decomposing the convolution itself as follows:
βt = n−1∑ i=0",3.2 Decomposing CNN layers,[0],[0]
"Wi · xt+i (t+ i) ∈ S, (4)
",3.2 Decomposing CNN layers,[0],[0]
γt = n−1∑ i=0 Wi · xt+i (t+ i) /∈,3.2 Decomposing CNN layers,[0],[0]
"S, (5)
Linearizing the activation function After applying a linear transformation to the input, a nonlinearity is typically applied.",3.2 Decomposing CNN layers,[0],[0]
"In CNNs, the ReLU activation function is often used.
",3.2 Decomposing CNN layers,[0],[0]
"In Murdoch et al. (2018), a linearization method for the non-linear activation function f is proposed, based on the differences of partial sums of all N components yi involved in the preactivation sum zt.",3.2 Decomposing CNN layers,[0],[0]
"In other words, we want to split fReLU (zt) =",3.2 Decomposing CNN layers,[0],[0]
"fReLU ( ∑N i=1 yi) into a sum of individual linearized contributions LfReLU (yi), namely fReLU ( ∑N i=1 yi)",3.2 Decomposing CNN layers,[0],[0]
= ∑N i=1 LfReLU (yi).,3.2 Decomposing CNN layers,[0],[0]
"To that end, we compute LfReLU (yk), the linearized contribution of yk as the average difference of partial sums over all possible permutations π1, ..., πMN of all N components yi involved:
Lf (yk) =
1
MN MN∑ i=1",3.2 Decomposing CNN layers,[0],[0]
[f( π−1i (k)∑ l=1 yπi(l))− f( π−1i (k)−1∑ l=1 yπi(l)),3.2 Decomposing CNN layers,[0],[0]
"]
(6)
Consequently, we can decompose the output ct after the activation function as follows:
ct =",3.2 Decomposing CNN layers,[0],[0]
"fReLU (zt) (7)
=",3.2 Decomposing CNN layers,[0],[0]
"fReLU (βz,t + γz,t + b) (8)
=LReLU (βz,t)
+",3.2 Decomposing CNN layers,[0],[0]
"[LReLU (γz,t) + LReLU (b)] (9)
=βc,t + γc,t (10)
",3.2 Decomposing CNN layers,[0],[0]
"Following Murdoch et al. (2018), βc,t contains the contributions that can be directly attributed to the specific set of input indexes S. Hence, the bias b is part of γc,t. Note that, while the decomposition in Eq.",3.2 Decomposing CNN layers,[0],[0]
"(10) is exact in terms of the total sum, the individual attribution to relevant (βc,t) and irrelevant (γc,t) is an approximation, due to the linearization.
",3.2 Decomposing CNN layers,[0],[0]
"Max-pooling over time When applying a fixedsize convolution over a variable-length sequence, the output is again of variable size.",3.2 Decomposing CNN layers,[0],[0]
"Hence, a maxpooling operation is executed over the time dimension, resulting in a fixed-size representation that is independent of the sequence length:
c = max t (ct).",3.2 Decomposing CNN layers,[0],[0]
"(11)
Instead of applying a max operation over the βc,t and γc,t contributions separately, we first determine the position t of the highest ct value and propagate the corresponding βc,t and γc,t values.",3.2 Decomposing CNN layers,[0],[0]
"The final layer is a classification layer, which is the same for a CNN- or LSTM-based architecture.",3.3 Calculating the final contribution scores,[0],[0]
"The probability pj of predicting class j is defined as follows:
pj = eWj ·x+bj∑C i=1",3.3 Calculating the final contribution scores,[0],[0]
"e Wi·x+bi , (12)
in which W ∈ Rd2×C is a weight matrix and Wi the i-th column, x ∈ Rd2 the input, b ∈ Rd2 the bias vector and bi the i-th element, d2 the input vector size and C the total number of classes.
",3.3 Calculating the final contribution scores,[0],[0]
The input x is either the output c of a CNN or h of a LSTM.,3.3 Calculating the final contribution scores,[0],[0]
"Consequently, we can decompose x into β and γ contributions.",3.3 Calculating the final contribution scores,[0],[0]
"In practice, we only consider the preactivation and decompose it as follows:
Wj · x+ bj =Wj · β +Wj · γ + bj .",3.3 Calculating the final contribution scores,[0],[0]
"(13)
Finally, the contribution of a set of characters with indexes S to the final score of class j is equal to Wj · β.",3.3 Calculating the final contribution scores,[0],[0]
The latter score is used throughout the paper for visualizing contributions of sets of characters.,3.3 Calculating the final contribution scores,[0],[0]
"We execute experiments on morphological tagging in three different languages: Finnish, Spanish and Swedish.",4 Experimental Setup,[0],[0]
"We describe the dataset in Section 4.1, whereas model and training details can be found in Section 4.2.",4 Experimental Setup,[0],[0]
"For our experiments, we use the Universal Dependencies 1.4 (UD) dataset (Nivre et al., 2016), which contains morphological features for a large number of sentences.",4.1 Dataset,[0],[0]
"Additionally, we acquired
manually-annotated character-level morphological segmentations and labels for a subset of the test set for three morphological different languages: Finnish, Spanish and Swedish.",4.1 Dataset,[0],[0]
"3
For each language, Silfverberg and Hulden (2017) selected the first non-unique 300 words from the UD test set and manually segmented each word according to the associated lemma and morphological features in the dataset.",4.1 Dataset,[0],[0]
"Whenever possible, they assigned each feature to a specific subset of characters.",4.1 Dataset,[0],[0]
"For example, the Spanish word ""económicas"" is segmented as follows:
• económic : lemma=económico
• a : gender=feminine
• s : number=plural
For our experiments, we are only interested in word/feature pairs for which a feature can be assigned to a specific subset of characters.",4.1 Dataset,[0],[0]
"Hence, we filter the test set on those specific word/feature pairs.",4.1 Dataset,[0],[0]
"In the above example, we have two word/feature pairs.",4.1 Dataset,[0],[0]
"This resulted in 278, 340 and 137 word/feature pairs for Finnish, Spanish and Swedish, respectively.",4.1 Dataset,[0],[0]
"Using the same procedure, we selected relevant feature classes, resulting in 12, 6 and 9 feature classes for Finnish, Spanish and Swedish, respectively.4 For each class, when a feature was not available, we introduced an additional Not Applicable (NA) label.
",4.1 Dataset,[0],[0]
We always train and validate on the full UD dataset for which we have filtered out all duplicate words.,4.1 Dataset,[0],[0]
"After that, we perform our analysis on either the UD test set or the annotated subset of manually segmented and annotated words.",4.1 Dataset,[0],[0]
"An overview can be found in Table 1.
",4.1 Dataset,[0],[0]
"3Available online: http://github.com/mpsilfve/ud-segmen ter/commit/5959214d494cbc13e53e1b26650813ff950d2ee3
4Full list available as supplementary material",4.1 Dataset,[0],[0]
"We experiment with both a CNN and BiLSTM architecture for character-level modeling of words.
",4.2 Model,[0],[0]
"At the input, we split every word into characters and add a start-of-word (ˆ) and an end-of-word ($) character.",4.2 Model,[0],[0]
"With every character, we associate a character embedding of size 50.
",4.2 Model,[0],[0]
"Our CNN architecture is inspired by Kim et al. (2016) and consists of a set of filters of varying width, followed by a ReLU activation function and a max-over-time pooling operation.",4.2 Model,[0],[0]
"We adopt their small-CNN parameter choices and have 25, 50, 75, 100, 125 and 150 convolutional filters of size 1, 2, 3, 4, 5 and 6, respectively.",4.2 Model,[0],[0]
"We do not add an additional highway layer.
",4.2 Model,[0],[0]
"For the character-level BiLSTM architecture, we follow the variant used in Plank et al. (2016).",4.2 Model,[0],[0]
"That is, we simply run a BiLSTM over all the characters and concatenate the final forward and backward hidden state.",4.2 Model,[0],[0]
"To obtain a similar number of parameters as the CNN model, we set the hidden state size to 100 units for each LSTM.
",4.2 Model,[0],[0]
"Finally, the word-level representation generated by either the CNN or BiLSTM architecture is classified by a multinomial logistic regression layer.",4.2 Model,[0],[0]
Each morphological class type has a different layer.,4.2 Model,[0],[0]
"We do not take into account context to rule out any influence originating from somewhere other than the characters of the word itself.
",4.2 Model,[0],[0]
"Training details For morphological tagging, we train a single model for all classes at once.",4.2 Model,[0],[0]
We minimize the joint loss by summing the cross-entropy losses of each class.,4.2 Model,[0],[0]
"We orthogonally initialize all weight matrices, except for the embeddings, which are uniformly initialized ([- 0.01;0.01]).",4.2 Model,[0],[0]
"All models are trained using Adam (Kingma and Ba, 2015) with minibatches of size 20 and learning rate 0.001.",4.2 Model,[0],[0]
No specific regularization is used.,4.2 Model,[0],[0]
We select our final model based on early stopping on the validation set.,4.2 Model,[0],[0]
"First, we verify that the CD algorithm works correctly by executing a controlled experiment with a synthetic token.",5 Experiments,[0],[0]
"Next, we quantitatively and qualitatively evaluate on the full test set.",5 Experiments,[0],[0]
"To verify that the contextual decomposition of CNNs works correctly, we devise an experiment
in which we add a synthetic token to a word of a certain class, testing whether this token gets a high attribution score with respect to that specific class.
",5.1 Validation of contextual decomposition for convolutional neural networks,[0],[0]
"Given a word w and a corresponding binary label t, we add a synthetic character c to the beginning of word w with probability psyn if that word belongs to the class t = 1 and with probability 1 − psyn if that word belongs to the class t = 0.",5.1 Validation of contextual decomposition for convolutional neural networks,[0],[0]
"Consequently, if psyn = 1, the model should predict the label with a 100% accuracy, thus attributing this to the synthetic character c. When psyn = 0.5, the synthetic character does not provide any additional information about the label t, and c should thus have a small contribution.
",5.1 Validation of contextual decomposition for convolutional neural networks,[0],[0]
Experimental setup We train a CNN model on the Spanish dataset and only use words having the morphological label number.,5.1 Validation of contextual decomposition for convolutional neural networks,[0],[0]
"This label has two classes plur and sing, and assign those classes to the binary labels zero and one, respectively.",5.1 Validation of contextual decomposition for convolutional neural networks,[0],[0]
"Furthermore, we add a synthetic character to each word with probability psyn, varying psyn from 1 to 0.5 with steps of 0.1.",5.1 Validation of contextual decomposition for convolutional neural networks,[0],[0]
We selected 112 unique word/feature pairs from our test set with label sing or plur.,5.1 Validation of contextual decomposition for convolutional neural networks,[0],[0]
"While plurality is marked by the suffix s, a variety of suffixes are used for the singular form.",5.1 Validation of contextual decomposition for convolutional neural networks,[0],[0]
"Therefore, we focus on the latter class (t = 1).",5.1 Validation of contextual decomposition for convolutional neural networks,[0],[0]
"The corresponding suffix is called the Ground Truth (GT) character.
",5.1 Validation of contextual decomposition for convolutional neural networks,[0],[0]
"To measure the impact of psyn, we add a synthetic character to each word of the class t = 1 and
calculate the contribution of each character by using the CD algorithm.",5.1 Validation of contextual decomposition for convolutional neural networks,[0],[0]
We run the experiment five times with a different random seed and report the average correct attribution.,5.1 Validation of contextual decomposition for convolutional neural networks,[0],[0]
"The attribution is correct if the contribution of the synthetic/GT character is the highest contribution of all character contributions.
",5.1 Validation of contextual decomposition for convolutional neural networks,[0],[0]
Results The results of our evaluation are depicted in Figure 2.,5.1 Validation of contextual decomposition for convolutional neural networks,[0],[0]
"When psyn = 1, all words of the class t = 1 contain the synthetic character, and consequently, the accuracy for predicting t = 1 is indeed 100%.",5.1 Validation of contextual decomposition for convolutional neural networks,[0],[0]
"Moreover, the correct prediction is effectively attributed to the synthetic character (‘syn.",5.1 Validation of contextual decomposition for convolutional neural networks,[0],[0]
char attr.’,5.1 Validation of contextual decomposition for convolutional neural networks,[0],[0]
"in Figure 2 at 100%), with the GT character being deemed irrelevant.",5.1 Validation of contextual decomposition for convolutional neural networks,[0],[0]
"When the synthetic character probability psyn is lowered, the synthetic character is less trustworthy and the GT character becomes more important (increasing ‘GT char attr.’",5.1 Validation of contextual decomposition for convolutional neural networks,[0],[0]
in Figure 2).,5.1 Validation of contextual decomposition for convolutional neural networks,[0],[0]
"Finally, when psyn = 0.5, the synthetic character is equally plausible in both classes.",5.1 Validation of contextual decomposition for convolutional neural networks,[0],[0]
"Hence, the contribution of the synthetic character becomes irrelevant and the model attributes the prediction to other characters.
",5.1 Validation of contextual decomposition for convolutional neural networks,[0],[0]
"Consequently, we can conclude that whenever there is a clear character-level pattern, the model learns the pattern and the CD algorithm is able to accurately attribute it to the correct character.",5.1 Validation of contextual decomposition for convolutional neural networks,[0],[0]
"In this section, we measure and analyze (1) which characters contribute most to the final prediction of a certain label and (2) whether those contributions coincide with our linguistic knowledge about a language.",5.2 Evaluation of character-level attribution,[0],[0]
"To that end, we train a model to predict morphological features, given a particular word.",5.2 Evaluation of character-level attribution,[0],[0]
"The model does not have prior word seg-
mentation information and thus needs to discover useful character patterns by itself.",5.2 Evaluation of character-level attribution,[0],[0]
"After training, we calculate the attribution scores of each character pattern within a word with respect to the correct feature class using CD, and evaluate whether this coincides with the ground truth attribution.
",5.2 Evaluation of character-level attribution,[0],[0]
"Model We train CNN and BiLSTM models on Finnish, Spanish and Swedish.",5.2 Evaluation of character-level attribution,[0],[0]
"The average accuracies on the full test set are reported in Table 2.5 As a reference for the trained models’ ability to predict morphological feature classes, we provide a naive baseline, constructed from the majority vote for each feature type.
",5.2 Evaluation of character-level attribution,[0],[0]
"Overall, our neural models yield substantially higher average accuracies than the baseline and perform very similar.",5.2 Evaluation of character-level attribution,[0],[0]
"Consequently, both the CNN and LSTM models learned useful character patterns for predicting the correct morphological feature classes.",5.2 Evaluation of character-level attribution,[0],[0]
"Hence, this raises the question whether these patterns coincide with our linguistic knowledge.
",5.2 Evaluation of character-level attribution,[0],[0]
"Evaluation For each annotated word/feature pair, we measure if the ground truth character se-
5The results of the individual classes are provided as supplementary material.
",5.2 Evaluation of character-level attribution,[0],[0]
ˆ,5.2 Evaluation of character-level attribution,[0],[0]
o,5.2 Evaluation of character-level attribution,[0],[0]
l,5.2 Evaluation of character-level attribution,[0],[0]
"i v a t $ BiLSTM
CNN
-3.2 0",5.2 Evaluation of character-level attribution,[0],[0]
"3.2
(a) Example of Finnish.",5.2 Evaluation of character-level attribution,[0],[0]
"Word (verb): olivat (were), target class: Tense=Past
ˆ g r a t u",5.2 Evaluation of character-level attribution,[0],[0]
"i t a $ BiLSTM
CNN
-2.6 0",5.2 Evaluation of character-level attribution,[0],[0]
"2.6
(b) Example of Spanish.",5.2 Evaluation of character-level attribution,[0],[0]
"Word (adjective): gratuita (free), target: Gender=Fem.
quence corresponds to the set or sequence of characters with the same length within the considered word that has the highest contribution for predicting the correct label for that word.
",5.2 Evaluation of character-level attribution,[0],[0]
"In the first setup, we only compare with character sequences having a consecutive set of characters (denoted cons).",5.2 Evaluation of character-level attribution,[0],[0]
"In the second setup, we compare with any set of characters (denoted all).",5.2 Evaluation of character-level attribution,[0],[0]
"We rank the contributions of each character set and report top one, two, and three scores.",5.2 Evaluation of character-level attribution,[0],[0]
"Because startof-word and end-of-word characters are not annotated in the dataset, we do not consider them part of the candidate character sets.
",5.2 Evaluation of character-level attribution,[0],[0]
"Results The aggregated results for all classes and character sequence lengths are shown in Fig-
ure 3.",5.2 Evaluation of character-level attribution,[0],[0]
"In general, we observe that for almost all models and setups, the contextual decomposition attribution coincides with the manually-defined segmentations for at least half of the word/feature pairs.",5.2 Evaluation of character-level attribution,[0],[0]
"When we only consider the top two consecutive sequences (marked as cons), accuracies range from 76% up to 93% for all three languages.",5.2 Evaluation of character-level attribution,[0],[0]
"For Spanish and Swedish, the top two accuracies for character sets (marked as all) are still above 67%, despite the large space of possible character sets, whereas all ground truth patterns are consecutive sequences.",5.2 Evaluation of character-level attribution,[0],[0]
"While the accuracy for Finnish is lower, the top two accuracy is still above 50%.
",5.2 Evaluation of character-level attribution,[0],[0]
"Examples for Finnish, Spanish and Swedish are shown in Figure 4.",5.2 Evaluation of character-level attribution,[0],[0]
"For Finnish, the character with the highest contribution i coincides with the ground truth character for the CNN model.",5.2 Evaluation of character-level attribution,[0],[0]
"This is not the case for the BiLSTM model which focuses on the character v, even though the correct label is predicted.",5.2 Evaluation of character-level attribution,[0],[0]
"For Spanish, both models strongly focus on the ground truth character a for predicting the feminine gender.",5.2 Evaluation of character-level attribution,[0],[0]
"For Swedish, the ground truth character sequence is the suffix or which denotes plurality.",5.2 Evaluation of character-level attribution,[0],[0]
"Given that or consists of two characters, all contributions of character sets of two characters are visualized.",5.2 Evaluation of character-level attribution,[0],[0]
"As can be seen, the most important set of two characters is {o,r} for the CNN and {k,r} for the BiLSTM model.",5.2 Evaluation of character-level attribution,[0],[0]
"However, {o,r} is the second most important character set for the BiLSTM model.",5.2 Evaluation of character-level attribution,[0],[0]
"Consequently, the BiLSTM model deemed the interaction between a root and suffix character more important than between two suffix characters.",5.2 Evaluation of character-level attribution,[0],[0]
"In the previous section, we showed that there is a strong relationship between the manually-defined morphological segmentation and the patterns a neural network learns.",5.3 Analysis of learned patterns,[0],[0]
"However, there is still an accuracy gap between the results obtained using consecutive sequences only and results obtained using all possible character sets.",5.3 Analysis of learned patterns,[0],[0]
"Hence, this leads to the question which patterns the neural network focuses on, other than the manually defined patterns we evaluated before.",5.3 Analysis of learned patterns,[0],[0]
"To that end, for each of the three languages, we selected a morphological class of interest and evaluated for all words in the full UD test set that were assigned to that class what the most important character set of length one, two and three was.",5.3 Analysis of learned patterns,[0],[0]
"In other words, we evaluated for each word for which the class was cor-
rectly predicted, which character set had the highest positive contribution towards predicting that class.",5.3 Analysis of learned patterns,[0],[0]
"The results can be found in Table 3.
",5.3 Analysis of learned patterns,[0],[0]
"Finnish In Finnish, adding the suffix i to a verb, transforms it in the past tense.",5.3 Analysis of learned patterns,[0],[0]
"Sometimes the character s is added, resulting in the suffix si.",5.3 Analysis of learned patterns,[0],[0]
The latter is a frequently used bigram pattern by the CNN but less by the BiLSTM.,5.3 Analysis of learned patterns,[0],[0]
"The BiLSTM combines the suffix i with another suffix vat which denotes third person plural in the character pattern iv_t.
Spanish While there is no single clear-cut rule for the Spanish gender, in general the suffix a denotes the feminine gender in adjectives.",5.3 Analysis of learned patterns,[0],[0]
"However, there exist many nouns that are feminine but do not have the suffix a. Teschner and Russell (1984) identify d, and ión as typical endings of feminine nouns, which our models identified too as for example ad$ or ió/sió.
",5.3 Analysis of learned patterns,[0],[0]
"Swedish In Swedish, there exist four suffixes for creating a plural form: or, ar, (e)r and n. Both models identified the suffix or.",5.3 Analysis of learned patterns,[0],[0]
"However, similar to Finnish, multiple suffixes are merged.",5.3 Analysis of learned patterns,[0],[0]
"In Swedish, the suffix na only occurs together with one of the first three plural suffixes.",5.3 Analysis of learned patterns,[0],[0]
"Hence, both models correctly identified this pattern as an important pattern for predicting the class number=plural, rather than the linguistically-defined pattern.",5.3 Analysis of learned patterns,[0],[0]
"In the previous section, the pattern a$ showed to be the most important pattern in 34% of the correctly-predicted feminine Spanish words in our dataset.",5.4 Interactions of learned patterns,[0],[0]
"However, there exist many words that end with the character a that are not feminine.",5.4 Interactions of learned patterns,[0],[0]
For example the third person singular form of the verb gustar is gusta.,5.4 Interactions of learned patterns,[0],[0]
"Hence, this raises the question if the model will classify gusta wrongly as feminine or correctly as NA.",5.4 Interactions of learned patterns,[0],[0]
"As an illustration of the applicability of CD for morphological analysis, we will study this case in more detail.
",5.4 Interactions of learned patterns,[0],[0]
"From the full UD test set, we selected all words that end with the character a and that do not belong to the class gender=feminine.",5.4 Interactions of learned patterns,[0],[0]
"Using the Spanish CNN model, we predicted the gender class for each word and divided the words into two groups: predicted as feminine and predicted as not-feminine (_NA_ or masculine).",5.4 Interactions of learned patterns,[0],[0]
The resulted in 44 and 199 words.,5.4 Interactions of learned patterns,[0],[0]
"Next, for each word in both groups we calculated the most positively and negatively contributing character set out of all possible character sets of any length within the considered word, using the CD algorithm.",5.4 Interactions of learned patterns,[0],[0]
"We compared the contribution scores in both groups using a Kruskal-Wallis significance test.6 While no significant (p < 0.05) difference could be found between the positive contributions of both groups (p=1.000), a borderline significant difference could be found between the negative
6The full statistical analysis is provided as supplementary material.
contributions of words predicted as feminine and words predicted as not-feminine (p=0.070).
",5.4 Interactions of learned patterns,[0],[0]
"Consequently, the CNN model’s classification decision is based on finding enough negative evidence to counteract the positive evidence found in the pattern a$, which CD was able to uncover.
",5.4 Interactions of learned patterns,[0],[0]
A visualization of this interaction is shown in Figure 5 for the word gusta.,5.4 Interactions of learned patterns,[0],[0]
"While the positive evidence is the strongest for the class feminine, the model identifies the verb stem gust as negative evidence which ultimately leads to the correct final prediction NA.",5.4 Interactions of learned patterns,[0],[0]
"While neural network-based models are part of many NLP systems, little is understood on how they handle the input data.",6 Conclusion,[0],[0]
"We investigated how specific character sequences at the input of a neural network model contribute to word-level tagging decisions at the output, and if those contributions follow linguistically interpretable rules.
",6 Conclusion,[0],[0]
"First, we presented an analysis and visualization technique to decompose the output of CNN models into separate input contributions, based on the principles outlined by Murdoch et al. (2018) for LSTMs.",6 Conclusion,[0],[0]
This allowed us then to quantitatively and qualitatively compare the character-level patterns the CNNs and BiLSTMs learned for the task of morphological tagging.,6 Conclusion,[0],[0]
"We showed that these patterns generally coincide with the morphological segments as defined by linguists for three morphologically different languages, but that sometimes other linguistically plausible patterns are learned.",6 Conclusion,[0],[0]
"Finally, we showed that our CD algorithm for CNNs is able to explain why the model made a wrong or correct prediction.
",6 Conclusion,[0],[0]
"By visualizing the contributions of each input unit or combinations thereof, we believe that much can be learned on how a neural network handles
the input data, why it makes certain decisions, or even for debugging neural network models.",6 Conclusion,[0],[0]
The authors would like to thank the anonymous reviewers and members of IDLab for their valuable feedback.,Acknowledgments,[0],[0]
"FG would like to thank Kim Bettens for helping out with the statistical analysis.
",Acknowledgments,[0],[0]
"The research activities as described in this paper were funded by Ghent University, imec, Flanders Innovation & Entrepreneurship (VLAIO), the Fund for Scientific Research-Flanders (FWOFlanders), and the European Union.",Acknowledgments,[0],[0]
Character-level features are currently used in different neural network-based natural language processing algorithms.,abstractText,[0],[0]
"However, little is known about the character-level patterns those models learn.",abstractText,[0],[0]
"Moreover, models are often compared only quantitatively while a qualitative analysis is missing.",abstractText,[0],[0]
"In this paper, we investigate which character-level patterns neural networks learn and if those patterns coincide with manually-defined word segmentations and annotations.",abstractText,[0],[0]
"To that end, we extend the contextual decomposition (Murdoch et al., 2018) technique to convolutional neural networks which allows us to compare convolutional neural networks and bidirectional long short-term memory networks.",abstractText,[0],[0]
We evaluate and compare these models for the task of morphological tagging on three morphologically different languages and show that these models implicitly discover understandable linguistic rules.,abstractText,[0],[0]
Explaining Character-Aware Neural Networks for Word-Level Prediction: Do They Discover Linguistic Rules?,title,[0],[0]
"It is now well known that modern convolutional neural networks (e.g. Krizhevsky et al. 2012, Simonyan & Zisserman 2015, He et al. 2016, Szegedy et al. 2016) can achieve remarkable performance on large-scale image databases, e.g. ImageNet (Deng et al. 2009) and Places 365 (Zhou et al. 2017), but it is really dissatisfying to see the vast amounts of data, computing time and power consumption that are necessary to train deep networks.",1. Introduction,[0],[0]
"Fortunately, such convolutional networks, once trained on a large database, can be refined to solve related but different visual tasks by means of transfer learning, using fine-tuning (Yosinski et al. 2014, Simonyan & Zisserman 2015).
",1. Introduction,[0],[0]
"Some form of knowledge is believed to be extracted by
1Sorbonne universités, Université de technologie de Compiègne, CNRS, Heudiasyc, UMR 7253, Compiègne, France.",1. Introduction,[0],[0]
"Correspondence to: Xuhong LI <xuhong.li@hds.utc.fr>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
learning from the large-scale database of the source task and this knowledge is then transferred to the target task by initializing the network with the pre-trained parameters.,1. Introduction,[0],[0]
"However, we will show in the experimental section that some parameters may be driven far away from their initial values during fine-tuning.",1. Introduction,[0],[0]
"This leads to important losses of the initial knowledge that is assumed to be relevant for the targeted problem.
",1. Introduction,[0],[0]
"In order to help preserve the knowledge embedded in the initial network, we consider a series of other parameter regularization methods during fine-tuning.",1. Introduction,[0],[0]
"We argue that the standard L2 regularization, which drives the parameters towards the origin, is not adequate in the framework of transfer learning, where the initial values provide a more sensible reference point than the origin.",1. Introduction,[0],[0]
"This simple modification keeps the original control of overfitting, by constraining the effective search space around the initial solution, while encouraging committing to the acquired knowledge.",1. Introduction,[0],[0]
"We show that it has noticeable effects in inductive transfer learning scenarios.
",1. Introduction,[0],[0]
"This paper copes with the inconsistency that still prevails in transfer learning scenarios, where the model is initialized with some parameters, while the abuse of L2 regularization encourages departing from these initial values.",1. Introduction,[0],[0]
"We thus advocate for a coherent parameter regularization approach, where the pre-trained model is both used as the starting point of the optimization process and as the reference in the penalty that encodes an explicit inductive bias.",1. Introduction,[0],[0]
This type of penalty will be designated with SP to recall that they encourage similarity with the starting point of the fine-tuning process.,1. Introduction,[0],[0]
"We evaluate regularizers based on the L2, Lasso and Group-Lasso penalties, which can freeze some individual parameters, or groups of parameters, to the pre-trained parameters.",1. Introduction,[0],[0]
Fisher information is also taken into account when we test L2-SP and Group-Lasso-SP approaches.,1. Introduction,[0],[0]
Our experiments indicate that all tested parameter regularization methods using the pre-trained parameters as a reference get an edge over the standard L2 weight decay approach.,1. Introduction,[0],[0]
We eventually recommend using L2-SP as the standard baseline for solving transfer learning tasks and benchmarking new algorithms.,1. Introduction,[0],[0]
"In this section, we recall the approaches to inductive transfer learning in convolutional networks.",2. Related Work,[0],[0]
We focus on approaches that also encourage similarity (of features or parameters) on different models.,2. Related Work,[0],[0]
Our proposal departs either by the goal pursued or by the type of model used.,2. Related Work,[0],[0]
Regularization has been a means to build shrinkage estimators for decades.,2.1. Shrinking Toward Chosen Parameters,[0],[0]
"Shrinking towards zero is the most common form of shrinkage, but shrinking towards adaptively chosen targets has been around for some time, starting with Stein shrinkage (see e.g. Lehmann & Casella 1998, chapter 5), where it can be related to empirical Bayes arguments.",2.1. Shrinking Toward Chosen Parameters,[0],[0]
"In transfer learning, it has been used in maximum entropy models (Chelba & Acero 2006) or SVM (Yang et al. 2007, Aytar & Zisserman 2011, Tommasi et al. 2014).",2.1. Shrinking Toward Chosen Parameters,[0],[0]
"These approaches were shown to outperform standard L2 regularization with limited labeled data in the target task (Aytar & Zisserman 2011, Tommasi et al. 2014).
",2.1. Shrinking Toward Chosen Parameters,[0],[0]
"These relatives differ from the application to deep networks in several respects, the more important one being that they consider a fixed representation, where transfer learning aims at producing similar classification parameters in that space, that is, similar classification rules.",2.1. Shrinking Toward Chosen Parameters,[0],[0]
"For deep networks, transfer usually aims at learning similar representations upon which classification parameters will be learned from scratch.",2.1. Shrinking Toward Chosen Parameters,[0],[0]
"Hence, even though the techniques we discuss here are very similar regarding the analytical form of the regularizers, they operate on parameters having a very different role.",2.1. Shrinking Toward Chosen Parameters,[0],[0]
"Regarding transfer learning, we follow here the nomenclature of Pan & Yang (2010), who categorized several types of transfer learning according to domain and task settings during the transfer.",2.2. Transfer Learning for Deep Networks,[0],[0]
"A domain corresponds to the feature space and its distribution, whereas a task corresponds to the label space and its conditional distribution with respect to features.",2.2. Transfer Learning for Deep Networks,[0],[0]
"The initial learning problem is defined on the source domain and source task, whereas the new learning problem is defined on the target domain and the target task.
",2.2. Transfer Learning for Deep Networks,[0],[0]
"In the typology of Pan & Yang, we consider the inductive transfer learning setting, where the target domain is identical to the source domain, and the target task is different from the source task.",2.2. Transfer Learning for Deep Networks,[0],[0]
"We furthermore focus on the case where a vast amount of data was available for training on the source problem, and some limited amount of labeled data is available for solving the target problem.",2.2. Transfer Learning for Deep Networks,[0],[0]
"Under this setting, we aim at improving the performance on the target problem through parameter regularization methods that explicitly encourage the similarity of the solutions to the target and source prob-
lems.",2.2. Transfer Learning for Deep Networks,[0],[0]
"Note that, though we refer here to problems that were formalized or popularized after (Pan & Yang 2010), such as lifelong learning, Pan & Yang’s typology remains valid.",2.2. Transfer Learning for Deep Networks,[0],[0]
Donahue et al. (2014) repurposed features extracted from different layers of the pre-trained AlexNet of Krizhevsky et al. (2012) and plugged them into an SVM or a logistic regression classifier.,2.2.1. REPRESENTATION TRANSFER,[0],[0]
This approach outperformed the state of the art of that time on the Caltech-101 database (Fei-Fei et al. 2006).,2.2.1. REPRESENTATION TRANSFER,[0],[0]
"Later, Yosinski et al. (2014) showed that finetuning the whole AlexNet resulted in better performance than using the network as a static feature extractor.",2.2.1. REPRESENTATION TRANSFER,[0],[0]
"Finetuning pre-trained VGG (Simonyan & Zisserman 2015) on the image classification task of VOC-2012 (Everingham et al. 2010) and Caltech 256 (Griffin et al. 2007) achieved the best results of that time.
",2.2.1. REPRESENTATION TRANSFER,[0],[0]
Ge & Yu (2017) proposed a scheme for selecting a subset of images from the source problem that have similar local features to those in the target problem and then jointly finetuned a pre-trained convolutional network.,2.2.1. REPRESENTATION TRANSFER,[0],[0]
"Besides image classification, many procedures for object detection (Girshick et al. 2014, Redmon et al. 2016, Ren et al. 2015) and image segmentation (Long et al. 2015a, Chen et al. 2017, Zhao et al. 2017) have been proposed relying on fine-tuning to improve over training from scratch.",2.2.1. REPRESENTATION TRANSFER,[0],[0]
"These approaches showed promising results in a challenging transfer learning setup, as going from classification to object detection or image segmentation requires rather heavy modifications of the architecture of the network.
",2.2.1. REPRESENTATION TRANSFER,[0],[0]
The success of transfer learning with convolutional networks relies on the generality of the learned representations that have been constructed from a large database like ImageNet.,2.2.1. REPRESENTATION TRANSFER,[0],[0]
"Yosinski et al. (2014) also quantified the transferability of these pieces of information in different layers, e.g. the first layers learn general features, the middle layers learn highlevel semantic features and the last layers learn the features that are very specific to a particular task.",2.2.1. REPRESENTATION TRANSFER,[0],[0]
That can be also noticed by the visualization of features (Zeiler & Fergus 2014).,2.2.1. REPRESENTATION TRANSFER,[0],[0]
"Overall, the learned representations can be conveyed to related but different domains and the parameters in the network are reusable for different tasks.",2.2.1. REPRESENTATION TRANSFER,[0],[0]
"In lifelong learning (Thrun & Mitchell 1995, Pentina & Lampert 2015), where a series of tasks is learned sequentially by a single model, the knowledge extracted from the previous tasks may be lost as new tasks are learned, resulting in what is known as catastrophic forgetting.",2.2.2. REGULARIZERS IN RELATED LEARNING SETUPS,[0],[0]
"In order to achieve a good performance on all tasks, Li & Hoiem (2017) proposed to use the outputs of the target examples, computed by the original network on the source task, to de-
fine a learning scheme preserving the memory of the source tasks when training on the target task.",2.2.2. REGULARIZERS IN RELATED LEARNING SETUPS,[0],[0]
"They also tried to preserve the pre-trained parameters instead of the outputs of examples but they did not obtain interesting results.
",2.2.2. REGULARIZERS IN RELATED LEARNING SETUPS,[0],[0]
Kirkpatrick et al. (2017) developed a similar approach with success.,2.2.2. REGULARIZERS IN RELATED LEARNING SETUPS,[0],[0]
They get sensible improvements by measuring the sensitivity of the parameters of the network learned on the source data thanks to the Fisher information.,2.2.2. REGULARIZERS IN RELATED LEARNING SETUPS,[0],[0]
"The Fisher information matrix defines a metric in parameter space that is used in their regularizer to preserve the representation learned on the source data, thereby retaining the knowledge acquired on the previous tasks.",2.2.2. REGULARIZERS IN RELATED LEARNING SETUPS,[0],[0]
"This scheme, named elastic weight consolidation, was shown to avoid forgetting, but fine-tuning with plain stochastic gradient descent was more effective than elastic weight consolidation for learning new tasks.",2.2.2. REGULARIZERS IN RELATED LEARNING SETUPS,[0],[0]
"Hence, elastic weight consolidation may be thought as being inadequate for transfer learning, where performance is only measured on the target task.",2.2.2. REGULARIZERS IN RELATED LEARNING SETUPS,[0],[0]
"We will show that this conclusion is not appropriate in typical transfer learning scenarios with few target examples.
",2.2.2. REGULARIZERS IN RELATED LEARNING SETUPS,[0],[0]
"In domain adaptation (Long et al. 2015b), where the target domain differs from the source domain whereas the target task is identical to the source task and no (or few) target examples are labeled, most approaches are searching for a common representation space for source and target domains to reduce domain shift.",2.2.2. REGULARIZERS IN RELATED LEARNING SETUPS,[0],[0]
Rozantsev et al. (2016) proposed a parameter regularization scheme for encouraging the similarity of the representations of the source and the target domains.,2.2.2. REGULARIZERS IN RELATED LEARNING SETUPS,[0],[0]
"Their regularizer encourages similar source and target parameters, up to a linear transformation.",2.2.2. REGULARIZERS IN RELATED LEARNING SETUPS,[0],[0]
"Still in domain adaptation, besides vision, encouraging similar parameters in deep networks has been proposed in speaker adaptation problems (Liao 2013, Ochiai et al. 2014) and neural machine translation (Barone et al. 2017), where it proved to be helpful.
",2.2.2. REGULARIZERS IN RELATED LEARNING SETUPS,[0],[0]
"The L2-SP regularizer was used independently by Grachten & Chacón (2017) for transfer in vision application, but where they used a random reinitialization of parameters.",2.2.2. REGULARIZERS IN RELATED LEARNING SETUPS,[0],[0]
"For convex optimization problems, this is equivalent to finetuning with L2-SP, but we are obviously not in that situation.",2.2.2. REGULARIZERS IN RELATED LEARNING SETUPS,[0],[0]
Grachten & Chacón (2017) conclude that their strategy behaves similarly to learning from scratch.,2.2.2. REGULARIZERS IN RELATED LEARNING SETUPS,[0],[0]
We will show that using the starting point as an initialization of the fine-tuning process and as the reference in the regularizer improves results consistently upon the standard fine-tuning process.,2.2.2. REGULARIZERS IN RELATED LEARNING SETUPS,[0],[0]
"In this section, we detail the penalties we consider for finetuning.",3. Regularizers for Fine-Tuning,[0],[0]
Parameter regularization is critical when learning from small databases.,3. Regularizers for Fine-Tuning,[0],[0]
"When learning from scratch, regularization is aimed at facilitating optimization and avoiding
overfitting, by implicitly restricting the capacity of the network, that is, the effective size of the search space.",3. Regularizers for Fine-Tuning,[0],[0]
"In transfer learning, the role of regularization is similar, but the starting point of the fine-tuning process conveys information that pertains to the source problem (domain and task).",3. Regularizers for Fine-Tuning,[0],[0]
"Hence, the network capacity has not to be restricted blindly: the pre-trained model sets a reference that can be used to define the functional space effectively explored during finetuning.
",3. Regularizers for Fine-Tuning,[0],[0]
"Since we are using early stopping, fine-tuning a pre-trained model is an implicit form of inductive bias towards the initial solution.",3. Regularizers for Fine-Tuning,[0],[0]
"We explore here how a coherent explicit inductive bias, encoded by a regularization term, affects the training process.",3. Regularizers for Fine-Tuning,[0],[0]
"Section 4 shows that all such schemes get an edge over the standard approaches that either use weight decay or freeze part of the network for preserving the low-level representations that are built in the first layers of the network.
",3. Regularizers for Fine-Tuning,[0],[0]
Let w ∈,3. Regularizers for Fine-Tuning,[0],[0]
Rn be the parameter vector containing all the network parameters that are to be adapted to the target task.,3. Regularizers for Fine-Tuning,[0],[0]
The regularized objective function J̃ that is to be optimized is the sum of the standard objective function J and the regularizer Ω(w).,3. Regularizers for Fine-Tuning,[0],[0]
"In our experiments, J is the negative log-likelihood, so that the criterion J̃ could be interpreted in terms of maximum a posteriori estimation, where the regularizer Ω(w) would act as the log prior of w. More generally, the minimizer of J̃ is a trade-off between the data-fitting term and the regularization term.
L2 penalty The current baseline penalty for transfer learning is the usual L2 penalty, also known as weight decay, since it drives the weights of the network to zero:
Ω(w) = α
2 ‖w‖22 , (1)
where α is the regularization parameter setting the strength of the penalty and ‖·‖p is the p-norm of a vector.
",3. Regularizers for Fine-Tuning,[0],[0]
"L2-SP Let w0 be the parameter vector of the model pretrained on the source problem, acting as the starting point (-SP) in fine-tuning.",3. Regularizers for Fine-Tuning,[0],[0]
"Using this initial vector as the reference in the L2 penalty, we get:
Ω(w) = α
2
∥∥w −w0∥∥2 2 .",3. Regularizers for Fine-Tuning,[0],[0]
"(2)
Typically, the transfer to a target task requires some modifications of the network architecture used for the source task, such as on the last layer used for predicting the outputs.",3. Regularizers for Fine-Tuning,[0],[0]
"Then, there is no one-to-one mapping between w and w0, and we use two penalties: one for the part of the target network that shares the architecture of the source network, denoted wS , the other one for the novel part, denoted wS̄ .
",3. Regularizers for Fine-Tuning,[0],[0]
"The compound penalty then becomes:
Ω(w) = α
2 ∥∥wS −w0S∥∥22 + β2 ‖wS̄‖22 .",3. Regularizers for Fine-Tuning,[0],[0]
"(3) L2-SP-Fisher Elastic weight consolidation (Kirkpatrick et al. 2017) was proposed to avoid catastrophic forgetting in the setup of lifelong learning, where several tasks should be learned sequentially.",3. Regularizers for Fine-Tuning,[0],[0]
"In addition to preserving the initial parameter vector w0, it consists in using the estimated Fisher information to define the distance between wS and w0S .",3. Regularizers for Fine-Tuning,[0],[0]
"More precisely, it relies on the diagonal of the Fisher information matrix, resulting in the following penalty:
Ω(w) = α
2 ∑ j∈S F̂jj ( wj − w0j )2 + β 2 ‖wS̄‖ 2 2 , (4)
where F̂jj is the estimate of the jth diagonal element of the Fisher information matrix.",3. Regularizers for Fine-Tuning,[0],[0]
"It is computed as the average of the squared Fisher’s score on the source problem, using the inputs of the source data:
F̂jj = 1
m m∑ i=1 K∑ k=1 fk(x (i);w0) ( ∂ ∂wj log fk(x (i);w0) )2 ,
where the outer average estimates the expectation with respect to inputs x and the inner weighted sum is the estimate of the conditional expectation of outputs given input x(i), with outputs drawn from a categorical distribution of parameters (f1(x(i);w), . . .",3. Regularizers for Fine-Tuning,[0],[0]
", fk(x(i);w), . . .",3. Regularizers for Fine-Tuning,[0],[0]
", fK(x(i);w)).
",3. Regularizers for Fine-Tuning,[0],[0]
"L1-SP We also experiment the L1 variant of L2-SP:
Ω(w) = α ∥∥wS",3. Regularizers for Fine-Tuning,[0],[0]
−w0S∥∥1 + β2 ‖wS̄‖22 .,3. Regularizers for Fine-Tuning,[0],[0]
"(5)
The usual L1 penalty encourages sparsity; here, by using w0S as a reference in the penalty, L
1-SP encourages some components of the parameter vector to be frozen, equal to the pre-trained initial values.",3. Regularizers for Fine-Tuning,[0],[0]
The penalty can thus be thought as intermediate between L2-SP (3) and the strategies consisting in freezing a part of the initial network.,3. Regularizers for Fine-Tuning,[0],[0]
"We explore below other ways of doing so.
",3. Regularizers for Fine-Tuning,[0],[0]
"Group-Lasso-SP (GL-SP) Instead of freezing some individual parameters, we may encourage freezing some groups of parameters corresponding to channels of convolution kernels.",3. Regularizers for Fine-Tuning,[0],[0]
"Formally, we endow the set of parameters with a group structure, defined by a fixed partition of the index set I = {1, . . .",3. Regularizers for Fine-Tuning,[0],[0]
", p}, that is, I = ⋃G g=0 Gg, with Gg ∩ Gh = ∅ for g 6= h.",3. Regularizers for Fine-Tuning,[0],[0]
"In our setup, G0 = S̄, and for g > 0, Gg is the set of fan-in parameters of channel g.",3. Regularizers for Fine-Tuning,[0],[0]
"Let pg denote the cardinality of group g, and wGg ∈ Rpg be the vector (wj)j∈Gg .",3. Regularizers for Fine-Tuning,[0],[0]
"Then, the GL-SP penalty is:
Ω(w) = α G∑",3. Regularizers for Fine-Tuning,[0],[0]
g=1 sg ∥∥∥wGg −w0Gg∥∥∥,3. Regularizers for Fine-Tuning,[0],[0]
"2 + β 2 ‖wS̄‖ 2 2 , (6)
where w0G0 = w 0 S̄ 4 = 0, and,",3. Regularizers for Fine-Tuning,[0],[0]
"for g > 0, sg is a predefined constant that may be used to balance the different cardinalities of groups.",3. Regularizers for Fine-Tuning,[0],[0]
"In our experiments, we used sg = p 1/2 g .
",3. Regularizers for Fine-Tuning,[0],[0]
"Our implementation of Group-Lasso-SP can freeze feature extractors at any depth of the convolutional network, to preserve the pre-trained feature extractors as a whole instead of isolated pre-trained parameters.",3. Regularizers for Fine-Tuning,[0],[0]
"The group Gg of size pg = hg × wg × dg gathers all the parameters of a convolution kernel of height hg, width wg, and depth dg.",3. Regularizers for Fine-Tuning,[0],[0]
"This grouping is done at each layer of the network, for each output channel, so that the group index g corresponds to two indexes in the network architecture: the layer index l and the output channel index at layer l.",3. Regularizers for Fine-Tuning,[0],[0]
"If we have cl such channels at layer l, we have a total of G = ∑ l cl groups.
",3. Regularizers for Fine-Tuning,[0],[0]
Group-Lasso-SP-Fisher (GL-SP-Fisher),3. Regularizers for Fine-Tuning,[0],[0]
"Following the idea of L2-SP-Fisher, the Fisher version of GL-SP is:
Ω(w) = α G∑ g=1 sg ( ∑ j∈Gg F̂jj ( wj − w0j )2 )1/2 + β 2 ‖wG0‖ 2 2 .",3. Regularizers for Fine-Tuning,[0],[0]
We evaluate the aforementioned parameter regularizers on several pairs of source and target tasks.,4. Experiments,[0],[0]
"We use ResNet (He et al. 2016) as our base network, since it has proven its wide applicability on transfer learning tasks.",4. Experiments,[0],[0]
"Conventionally, if the target task is also a classification task, the training process starts by replacing the last layer with a new one, randomly generated, whose size depends on the number of classes in the target task.",4. Experiments,[0],[0]
"For comparing the effect of similarity between the source problem and the target problem on transfer learning, we chose two source databases: ImageNet (Deng et al. 2009) for generic object recognition and Places 365 (Zhou et al. 2017) for scene classification.",4.1. Source and Target Databases,[0],[0]
"Likewise, we have three different databases related to three target problems: Caltech 256 (Griffin et al. 2007) contains different objects for generic object recognition; MIT Indoors 67 (Quattoni & Torralba 2009) consists of 67 indoor scene categories; Stanford Dogs 120 (Khosla et al. 2011) contains images of 120 breeds of dogs; Each target database is split into training and testing sets following the suggestion of their creators (see Table 1 for details).",4.1. Source and Target Databases,[0],[0]
"In addition, we consider two configurations for Caltech 256: 30 or 60 examples randomly drawn from each category for training, and 20 remaining examples for test.",4.1. Source and Target Databases,[0],[0]
Most images in those databases are color images.,4.2. Training Details,[0],[0]
"If not, we create a three-channel image by duplicating the grayscale data.",4.2. Training Details,[0],[0]
"All images are pre-processed: we resize images to 256×256 and subtract the mean activity computed over the training set from each channel, then we adopt random blur, random mirror and random crop to 224×224 for data augmentation.",4.2. Training Details,[0],[0]
The network parameters are regularized as described in Section 3.,4.2. Training Details,[0],[0]
"Cross validation is used for searching the best regularization hyperparameters α and β: α differs across experiments, and β = 0.01 is consistently picked by cross-validation for regularizing the last layer.",4.2. Training Details,[0],[0]
"Figure 1 illustrates that the test accuracy varies smoothly according to the regularization strength, and that there is a sensible benefit in penalizing the last layer (that is, β ≥ 0) for the best α values.",4.2. Training Details,[0],[0]
"When applicable, the Fisher information matrix is estimated on the source database.",4.2. Training Details,[0],[0]
The two source databases (ImageNet or Places 365) yield different estimates.,4.2. Training Details,[0],[0]
"Regarding testing, we use central crops as inputs to compute the classification accuracy.
",4.2. Training Details,[0],[0]
Stochastic gradient descent with momentum 0.9 is used for optimization.,4.2. Training Details,[0],[0]
We run 9000 iterations and divide the learning rate by 10 after 6000 iterations.,4.2. Training Details,[0],[0]
"The initial learning rates are 0.005, 0.01 or 0.02, depending on the tasks.",4.2. Training Details,[0],[0]
Batch size is 64.,4.2. Training Details,[0],[0]
"Then, under the best configuration, we repeat five times the learning process to obtain an average classification accuracy and standard deviation.",4.2. Training Details,[0],[0]
All the experiments are performed with Tensorflow (Abadi et al. 2015).,4.2. Training Details,[0],[0]
"Table 2 displays the results of fine-tuning with L2-SP and L2-SP-Fisher, which are compared to the current baseline of fine-tuning with L2.",4.3.1. FINE-TUNING FROM A SIMILAR SOURCE,[0],[0]
We report the average accuracies and their standard deviations on 5 different runs.,4.3.1. FINE-TUNING FROM A SIMILAR SOURCE,[0],[0]
"Since we use the same data and the same starting point, runs differ only due to the randomness of stochastic gradient descent and to the weight initialization of the last layer.",4.3.1. FINE-TUNING FROM A SIMILAR SOURCE,[0],[0]
"We can observe that L2-SP and L2-SP-Fisher always improve over L2, and that when less training data are available for the target problem, the improvement of L2-SP and L2-SPFisher compared to L2 are more important.",4.3.1. FINE-TUNING FROM A SIMILAR SOURCE,[0],[0]
"Meanwhile, no large difference is observed between L2-SP and L2-SPFisher.
",4.3.1. FINE-TUNING FROM A SIMILAR SOURCE,[0],[0]
"We can boost the performance and outperform the state of the art (Ge & Yu 2017) in some cases by exploiting more training techniques and post-processing methods, which are described in the supplementary material.",4.3.1. FINE-TUNING FROM A SIMILAR SOURCE,[0],[0]
A comprehensive view of our experimental results is given in Figure 2.,"4.3.2. BEHAVIOR ACROSS PENALTIES, SOURCE AND TARGET DATABASES",[0],[0]
Each plot corresponds to one of the four target databases listed in Table 1.,"4.3.2. BEHAVIOR ACROSS PENALTIES, SOURCE AND TARGET DATABASES",[0],[0]
"The light red points mark the accuracies of transfer learning when using Places 365 as the source database, whereas the dark blue points correspond to the results obtained with ImageNet.","4.3.2. BEHAVIOR ACROSS PENALTIES, SOURCE AND TARGET DATABASES",[0],[0]
"As expected, the results
Table 2.","4.3.2. BEHAVIOR ACROSS PENALTIES, SOURCE AND TARGET DATABASES",[0],[0]
"Average classification accuracies (in %) of L2, L2-SP and L2-SP-Fisher on 5 different runs.","4.3.2. BEHAVIOR ACROSS PENALTIES, SOURCE AND TARGET DATABASES",[0],[0]
"The source database is Places 365 for MIT Indoors 67 and ImageNet for Stanford Dogs 120 and Caltech 256.
","4.3.2. BEHAVIOR ACROSS PENALTIES, SOURCE AND TARGET DATABASES",[0],[0]
"MIT Indoors 67 Stanford Dogs 120 Caltech 256 – 30 Caltech 256 – 60 L2 79.6±0.5 81.4±0.2 81.5±0.2 85.3±0.2
L2-SP 84.2±0.3 85.1±0.2 83.5±0.1 86.4±0.2 L2-SP-Fisher 84.0±0.4 85.1±0.2 83.3±0.1 86.0±0.1
of transfer learning are much better when source and target are alike: the scene classification target task MIT Indoor 67 (top left) is better transferred from the scene classification source task Places 365, whereas the object recognition target tasks benefit more from the object recognition source task ImageNet.","4.3.2. BEHAVIOR ACROSS PENALTIES, SOURCE AND TARGET DATABASES",[0],[0]
It is however interesting to note that the trends are similar for the two source databases: all the fine-tuning strategies based on penalties using the starting point -SP as a reference perform consistently better than standard finetuning (L2).,"4.3.2. BEHAVIOR ACROSS PENALTIES, SOURCE AND TARGET DATABASES",[0],[0]
"There is thus a benefit in having an explicit bias towards the starting point, even when the target task is not too similar to the source task.
","4.3.2. BEHAVIOR ACROSS PENALTIES, SOURCE AND TARGET DATABASES",[0],[0]
This benefit is comparable for L2-SP and L2-SP-Fisher penalties; the strategies based on L1 and Group-Lasso penalties behave rather poorly in comparison.,"4.3.2. BEHAVIOR ACROSS PENALTIES, SOURCE AND TARGET DATABASES",[0],[0]
They are even less accurate than the plain L2 strategy on Caltech 256 – 60 when the source problem is Places 365.,"4.3.2. BEHAVIOR ACROSS PENALTIES, SOURCE AND TARGET DATABASES",[0],[0]
"Stochastic gradient
descent does not handle well these penalties whose gradient is discontinuous at the starting point where the optimization starts.","4.3.2. BEHAVIOR ACROSS PENALTIES, SOURCE AND TARGET DATABASES",[0],[0]
"The stochastic forward-backward splitting algorithm of Duchi & Singer (2009), which is related to proximal methods, leads to substandard results, presumably due to the absence of a momentum term.","4.3.2. BEHAVIOR ACROSS PENALTIES, SOURCE AND TARGET DATABASES",[0],[0]
"In the end, we used plain stochastic gradient descent on a smoothed version of the penalties eliminating the discontinuities of their gradients, but some instability remains.
","4.3.2. BEHAVIOR ACROSS PENALTIES, SOURCE AND TARGET DATABASES",[0],[0]
"Finally, the variants using the Fisher information matrix behave like the simpler variants using a Euclidean metric on parameters.","4.3.2. BEHAVIOR ACROSS PENALTIES, SOURCE AND TARGET DATABASES",[0],[0]
"We believe that this is due to the fact that, contrary to lifelong learning, our objective does not favor solutions that retain accuracy on the source task.","4.3.2. BEHAVIOR ACROSS PENALTIES, SOURCE AND TARGET DATABASES",[0],[0]
The metric defined by the Fisher information matrix may thus be less relevant for our actual objective that only relates to the target task.,"4.3.2. BEHAVIOR ACROSS PENALTIES, SOURCE AND TARGET DATABASES",[0],[0]
"Table 3 confirms that L2-SP-Fisher is indeed a better approach in the situation of lifelong learning, where accuracies on the source tasks matter.","4.3.2. BEHAVIOR ACROSS PENALTIES, SOURCE AND TARGET DATABASES",[0],[0]
"It reports the drop in performance when the fine-tuned models are applied on the source task, without any retraining, simply using the original classification layer instead of the classification layer learned for the target task.","4.3.2. BEHAVIOR ACROSS PENALTIES, SOURCE AND TARGET DATABASES",[0],[0]
The performance drop is smaller for L2SP-Fisher than for L2-SP.,"4.3.2. BEHAVIOR ACROSS PENALTIES, SOURCE AND TARGET DATABASES",[0],[0]
"In comparison, L2 fine-tuning results in catastrophic forgetting: the performance on the source task is considerably affected by fine-tuning.
4.3.3.","4.3.2. BEHAVIOR ACROSS PENALTIES, SOURCE AND TARGET DATABASES",[0],[0]
"FINE-TUNING vs. FREEZING THE NETWORK
Freezing the first layers of a network during transfer learning is another way to ensure a very strong inductive bias,
letting less degrees of freedom to transfer learning.","4.3.2. BEHAVIOR ACROSS PENALTIES, SOURCE AND TARGET DATABASES",[0],[0]
"Figure 3 shows that this strategy, which is costly to implement if one looks for the optimal number of layers to be frozen, can improve L2 fine-tuning considerably, but that it is a rather inefficient strategy for L2-SP fine-tuning.","4.3.2. BEHAVIOR ACROSS PENALTIES, SOURCE AND TARGET DATABASES",[0],[0]
"Among all possible choices, L2 fine-tuning with partial freezing is dominated by the plain L2-SP fine-tuning.","4.3.2. BEHAVIOR ACROSS PENALTIES, SOURCE AND TARGET DATABASES",[0],[0]
Note that L2-SP-Fisher (not displayed) behaves similarly to L2-SP.,"4.3.2. BEHAVIOR ACROSS PENALTIES, SOURCE AND TARGET DATABASES",[0],[0]
"Among all -SP methods, L2-SP and L2-SP-Fisher always reach a better accuracy on the target task.",4.4. Analysis and Discussion,[0],[0]
"We expected L2SP-Fisher to outperform L2-SP, since Fisher information helps in lifelong learning, but there is no significant difference between the two options.",4.4. Analysis and Discussion,[0],[0]
"Since L2-SP is simpler than L2-SP-Fisher, we recommend the former, and we focus on the analysis of L2-SP, although most of the analysis and the discussion would also apply to L2-SP-Fisher.",4.4. Analysis and Discussion,[0],[0]
"The -SP penalties introduce no extra parameters, and they only increase slightly the computational burden.",4.4.1. COMPUTATIONAL EFFICIENCY,[0],[0]
L2-SP increases the number of floating point operations required for a learning step of ResNet-101 by less than 1%.,4.4.1. COMPUTATIONAL EFFICIENCY,[0],[0]
"Hence, at a negligible computational cost, we can obtain significant improvements in classification accuracy, and no additional cost is experienced at test time.",4.4.1. COMPUTATIONAL EFFICIENCY,[0],[0]
Analytical results are very difficult to obtain in the deep learning framework.,4.4.2. THEORETICAL INSIGHTS,[0],[0]
"Under some (highly) simplifying assumptions, we show in supplementary material that the optimum of the regularized objective function with L2-SP is a compromise between the optimum of the unregularized objective function and the pre-trained parameter vector, precisely an affine combination along the directions of eigenvectors of the Hessian matrix of the unregularized objective function.",4.4.2. THEORETICAL INSIGHTS,[0],[0]
This contrasts with L2 that leads to a compromise between the optimum of the unregularized objective function and the origin.,4.4.2. THEORETICAL INSIGHTS,[0],[0]
"Clearly, searching for a solution in the vicinity of the pre-trained parameters is intuitively much more appealing, since it is the actual motivation for using the pre-trained parameters as the starting point of the fine-tuning process.
",4.4.2. THEORETICAL INSIGHTS,[0],[0]
"Using L2-SP instead of L2 can also be motivated by an analogy with shrinkage estimation (see e.g. Lehmann & Casella 1998, chapter 5).",4.4.2. THEORETICAL INSIGHTS,[0],[0]
"Although it is known that shrinking toward any reference is better than raw fitting, it is also known that shrinking towards a value that is close to the “true parameters” is more effective.",4.4.2. THEORETICAL INSIGHTS,[0],[0]
"The notion of “true parameters” is not readily applicable to deep networks, but the connection with Stein shrinking effect may be inspiring by surveying the literature considering shrinkage towards other references, such as linear subspaces.",4.4.2. THEORETICAL INSIGHTS,[0],[0]
"In particular, it is likely that manifolds of parameters defined from the pre-trained network would provide a more relevant reference than the single parameter value provided by the pre-trained network.",4.4.2. THEORETICAL INSIGHTS,[0],[0]
"We complement our experimental results by an analysis relying on the activations of the hidden units of the network, to provide another view on the differences between L2 and L2SP fine-tuning.",4.4.3. LAYER-WISE ANALYSIS,[0],[0]
"Activation similarities are easier to interpret than parameter similarities, and they provide a view of the network that is closer to the functional perspective we are actually pursuing.",4.4.3. LAYER-WISE ANALYSIS,[0],[0]
"Matching individual activations makes sense, provided that the networks slightly differ before and after tuning so that few roles are switched between units or feature maps.
",4.4.3. LAYER-WISE ANALYSIS,[0],[0]
"The dependency between the pre-trained and the fine-tuned
activations throughout the network is displayed in Figure 4, with boxplots of the R2 coefficients, gathered layer-wise, of the fine-tuned activations with respect to the original activations.",4.4.3. LAYER-WISE ANALYSIS,[0],[0]
"This figure shows that, indeed, the roles of units or feature maps have not changed much after L2-SP and L2SP-Fisher fine-tuning.",4.4.3. LAYER-WISE ANALYSIS,[0],[0]
"The R2 coefficients are very close to 1 on the first layers, and smoothly decrease throughout the network, staying quite high, around 0.6, for L2-SP and L2-SP-Fisher at the greatest depth.",4.4.3. LAYER-WISE ANALYSIS,[0],[0]
"In contrast, for L2 regularization, some important changes are already visible in the first layers, and the R2 coefficients eventually reach quite low values at the greatest depth.",4.4.3. LAYER-WISE ANALYSIS,[0],[0]
"This illustrates in details how the roles of the network units is remarkably retained with L2-SP and L2-SP-Fisher fine-tuning, not only for the first layers of the networks, but also for the last high-level representations before classification.",4.4.3. LAYER-WISE ANALYSIS,[0],[0]
We described and tested simple regularization techniques for inductive transfer learning.,5. Conclusion,[0],[0]
"They all encode an explicit bias towards the solution learned on the source task, resulting in a compromise with the pre-trained parameter that is coherent with the original motivation for fine-tuning.",5. Conclusion,[0],[0]
"All the regularizers evaluated here have been already used for other purposes or in other contexts, but we demonstrated their relevance for inductive transfer learning with deep convolutional networks.
",5. Conclusion,[0],[0]
"We show that a simple L2 penalty using the starting point as a reference, L2-SP, is useful, even if early stopping is used.",5. Conclusion,[0],[0]
This penalty is much more effective than the standard L2 penalty that is commonly used in fine-tuning.,5. Conclusion,[0],[0]
It is also more effective and simpler to implement than the strategy consisting in freezing the first layers of a network.,5. Conclusion,[0],[0]
"We provide
theoretical hints and strong experimental evidence showing that L2-SP retains the memory of the features learned on the source database.",5. Conclusion,[0],[0]
"We thus believe that this simple L2-SP scheme should be considered as the standard baseline in inductive transfer learning, and that future improvements of transfer learning should rely on this baseline.
",5. Conclusion,[0],[0]
"Besides, we tested the effect of more elaborate penalties, based on L1 or Group-L1 norms, or based on Fisher information.",5. Conclusion,[0],[0]
"None of the L1 or Group-L1 options seem to be valuable in the context of inductive transfer learning that we considered here, and using the Fisher information with L2SP does not improve accuracy on the target task.",5. Conclusion,[0],[0]
"Different approaches, which implement an implicit bias at the functional level, alike Li & Hoiem (2017), remain to be tested: being based on a different principle, their value should be assessed in the framework of inductive transfer learning.",5. Conclusion,[0],[0]
"This work was carried out with the supports of the China Scholarship Council and of a PEPS grant through the DESSTOPT project jointly managed by the National Institute of Mathematical Sciences and their Interactions (INSMI) and the Institute of Information Science and their Interactions (INS2I) of the CNRS, France.",Acknowledgments,[0],[0]
We acknowledge the support of NVIDIA Corporation with the donation of GPUs used for this research.,Acknowledgments,[0],[0]
"In inductive transfer learning, fine-tuning pretrained convolutional networks substantially outperforms training from scratch.",abstractText,[0],[0]
"When using finetuning, the underlying assumption is that the pretrained model extracts generic features, which are at least partially relevant for solving the target task, but would be difficult to extract from the limited amount of data available on the target task.",abstractText,[0],[0]
"However, besides the initialization with the pre-trained model and the early stopping, there is no mechanism in fine-tuning for retaining the features learned on the source task.",abstractText,[0],[0]
"In this paper, we investigate several regularization schemes that explicitly promote the similarity of the final solution with the initial model.",abstractText,[0],[0]
"We show the benefit of having an explicit inductive bias towards the initial model, and we eventually recommend a simple L penalty with the pre-trained model being a reference as the baseline of penalty for transfer learning tasks.",abstractText,[0],[0]
Explicit Inductive Bias for Transfer Learning with Convolutional Networks,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2826–2831 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"Neural machine translation (NMT) has been rapidly developed in recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Tu et al., 2016).",1 Introduction,[0],[0]
"The encoderdecoder architecture is widely employed, in which the encoder summarizes the source sentence into a vector representation, and the decoder generates the target sentence word by word from the vector representation.",1 Introduction,[0],[0]
"Using the encoder-decoder framework as well as gating and attention techniques, it has been shown that the performance of NMT has surpassed the performance of traditional statistical machine translation (SMT) on various language pairs (Luong et al., 2015).
",1 Introduction,[0],[0]
"The continuous vector representation of a symbol encodes multiple dimensions of similarity, equivalent to encoding more than one meaning of
∗Corresponding Author: Zhaopeng Tu
a word.",1 Introduction,[0],[0]
"Consequently, NMT needs to spend a substantial amount of its capacity in disambiguating source and target words based on the context defined by a source sentence (Choi et al., 2016).",1 Introduction,[0],[0]
"Consistency is another critical issue in documentlevel translation, where a repeated term should keep the same translation throughout the whole document (Xiao et al., 2011; Carpuat and Simard, 2012).",1 Introduction,[0],[0]
"Nevertheless, current NMT models still process a documents by translating each sentence alone, suffering from inconsistency and ambiguity arising from a single source sentence.",1 Introduction,[0],[0]
"These problems are difficult to alleviate using only limited intra-sentence context.
",1 Introduction,[0],[0]
"The cross-sentence context, or global context, has proven helpful to better capture the meaning or intention in sequential tasks such as query suggestion (Sordoni et al., 2015) and dialogue modeling (Vinyals and Le, 2015; Serban et al., 2016).",1 Introduction,[0],[0]
"The leverage of global context for NMT, however, has received relatively little attention from the research community.1",1 Introduction,[0],[0]
"In this paper, we propose a cross-sentence context-aware NMT model, which considers the influence of previous source sentences in the same document.2
Specifically, we employ a hierarchy of Recurrent Neural Networks (RNNs) to summarize the cross-sentence context from source-side previous sentences, which deploys an additional documentlevel RNN on top of the sentence-level RNN encoder (Sordoni et al., 2015).",1 Introduction,[0],[0]
"After obtaining the global context, we design several strategies to integrate it into NMT to translate the current sentence:
• Initialization, that uses the history represen1To the best of our knowledge, our work and Jean et al. (2017) are two independently early attempts to model crosssentence context for NMT.
",1 Introduction,[0],[0]
"2In our preliminary experiments, considering target-side history inversely harms translation performance, since it suffers from serious error propagation problems.
2826
tation as the initial state of the encoder, decoder, or both;
• Auxiliary Context, that uses the history representation as static cross-sentence context, which works together with the dynamic intrasentence context produced by an attention model, to good effect.
",1 Introduction,[0],[0]
"• Gating Auxiliary Context, that adds a gate to Auxiliary Context, which decides the amount of global context used in generating the next target word at each step of decoding.
",1 Introduction,[0],[0]
"Experimental results show that the proposed initialization and auxiliary context (w/ or w/o gating) mechanisms significantly improve translation performance individually, and combining them achieves further improvement.",1 Introduction,[0],[0]
"Given a source sentence xm to be translated, we consider its K previous sentences in the same document as cross-sentence context C = {xm−K , ...,xm−1}.",2 Approach,[0],[0]
"In this section, we first model C, which is then integrated into NMT.",2 Approach,[0],[0]
"As shown in Figure 1, we summarize the representation of C in a hierarchical way:
Sentence RNN For a sentence xk in C, the sentence RNN reads the corresponding words {x1,k, ..., xn,k, . . .",2.1 Summarizing Global Context,[0],[0]
", xN,k}",2.1 Summarizing Global Context,[0],[0]
"sequentially and updates its hidden state:
hn,k = f(hn−1,k, xn,k) (1)
where f(·) is an activation function, and hn,k is the hidden state at time n. The last state hN,k stores order-sensitive information about all the words in xk, which is used to represent the summary of the whole sentence, i.e. Sk ≡",2.1 Summarizing Global Context,[0],[0]
"hN,k.",2.1 Summarizing Global Context,[0],[0]
"After processing
each sentence in C, we can obtain all sentencelevel representations, which will be fed into document RNN.
",2.1 Summarizing Global Context,[0],[0]
"Document RNN It takes as input the sequence of the above sentence-level representations {S1, ..., Sk, ..., SK} and computes the hidden state as:
hk = f(hk−1, Sk) (2)
where hk is the recurrent state at time k, which summarizes the previous sentences that have been processed to the position",2.1 Summarizing Global Context,[0],[0]
"k. Similarly, we use the last hidden state to represent the summary of the global context, i.e. D ≡ hK .",2.1 Summarizing Global Context,[0],[0]
"We propose three strategies to integrate the history representation D into NMT:
Initialization We useD to initialize either NMT encoder, NMT decoder or both.",2.2 Integrating Global Context into NMT,[0],[0]
"For encoder, we useD as the initialization state rather than all-zero states as in the standard NMT (Bahdanau et al., 2015).",2.2 Integrating Global Context into NMT,[0],[0]
"For decoder, we rewrite the calculation of the initial hidden state s0 = tanh(WshN ) as s0 = tanh(WshN +WDD) where hN is the last hidden state in encoder and {Ws,WD} are the corresponding weight metrices.
",2.2 Integrating Global Context into NMT,[0],[0]
"Auxiliary Context In standard NMT, as shown in Figure 2 (a), the decoder hidden state for time i is computed by
si = f(si−1, yi−1, ci) (3)
where yi−1 is the most recently generated target word, and ci is the intra-sentence context summarized by NMT encoder for time i.",2.2 Integrating Global Context into NMT,[0],[0]
"As shown in Figure 2 (b), Auxiliary Context method adds the representation of cross-sentence context D to jointly update the decoding state si:
si = f(si−1, yi−1, ci, D) (4)
",2.2 Integrating Global Context into NMT,[0],[0]
"In this strategy, D serves as an auxiliary information source to better capture the meaning of the source sentence.",2.2 Integrating Global Context into NMT,[0],[0]
Now the gated NMT decoder has four inputs rather than the original three ones.,2.2 Integrating Global Context into NMT,[0],[0]
"The concatenation [ci, D], which embeds both intraand cross-sentence contexts, can be fed to the decoder as a single representation.",2.2 Integrating Global Context into NMT,[0],[0]
"We only need to modify the size of the corresponding parameter matrix for least modification effort.
",2.2 Integrating Global Context into NMT,[0],[0]
Gating Auxiliary Context The starting point for this strategy is an observation: the need for information from the global context differs from step to step during generation of the target words.,2.2 Integrating Global Context into NMT,[0],[0]
"For example, global context is more in demand when generating target words for ambiguous source words, while less by others.",2.2 Integrating Global Context into NMT,[0],[0]
"To this end, we extend auxiliary context strategy by introducing a context gate (Tu et al., 2017a) to dynamically control the amount of information flowing from the auxiliary global context at each decoding step, as shown in Figure 2 (c).
",2.2 Integrating Global Context into NMT,[0],[0]
"Intuitively, at each decoding step i, the context gate looks at decoding environment (i.e., si, yi−1, and ci), and outputs a number between 0 and 1 for each element in D, where 1 denotes “completely transferring this” while 0 denotes “completely ignoring this”.",2.2 Integrating Global Context into NMT,[0],[0]
"The global context vector D is then processed with an element-wise multiplication before being fed to the decoder activation layer.
",2.2 Integrating Global Context into NMT,[0],[0]
"Formally, the context gate consists of a sigmoid neural network layer and an element-wise multiplication operation.",2.2 Integrating Global Context into NMT,[0],[0]
"It assigns an element-wise weight to D, computed by
zi = σ(Uzsi−1 +Wzyi−1 + Czci) (5)
",2.2 Integrating Global Context into NMT,[0],[0]
"Here σ(·) is a logistic sigmoid function, and {Wz, Uz, Cz} are the weight matrices, which are trained to learn when to exploit global context to maximize the overall translation performance.",2.2 Integrating Global Context into NMT,[0],[0]
"Note that zi has the same dimensionality asD, and thus each element in the global context vector has its own weight.",2.2 Integrating Global Context into NMT,[0],[0]
"Accordingly, the decoder hidden state is updated by
si = f(si−1, yi−1, ci, zi ⊗D) (6)",2.2 Integrating Global Context into NMT,[0],[0]
We carried out experiments on Chinese–English translation task.,3.1 Setup,[0],[0]
"As the document information is necessary when selecting the previous sentences, we collect all LDC corpora that contain document boundary.",3.1 Setup,[0],[0]
The training corpus consists of 1M sentence pairs extracted from LDC corpora3 with 25.4M Chinese words and 32.4M English words.,3.1 Setup,[0],[0]
"We chose the NIST05 (MT05) as our development set, and NIST06 (MT06) and NIST08",3.1 Setup,[0],[0]
(MT08) as test sets.,3.1 Setup,[0],[0]
"We used case-insensitive BLEU score (Papineni et al., 2002) as our evaluation metric, and sign-test (Collins et al., 2005) for calculating statistical significance.
",3.1 Setup,[0],[0]
"We implemented our approach on top of an open source attention-based NMT model, Nematus4 (Sennrich and Haddow, 2016; Sennrich et al., 2017).",3.1 Setup,[0],[0]
"We limited the source and target vocabularies to the most frequent 35K words in Chinese and English, covering approximately 97.1% and 99.4% of the data in the two languages respectively.",3.1 Setup,[0],[0]
We trained each model on sentences of length up to 80 words in the training data with early stopping.,3.1 Setup,[0],[0]
"The word embedding dimension was 600, the hidden layer size was 1000, and the batch size was 80.",3.1 Setup,[0],[0]
"All our models considered the previous three sentences (i.e., K = 3) as crosssentence context.
",3.1 Setup,[0],[0]
"3The LDC corpora indexes are: 2003E07, 2003E14, 2004T07, 2005E83, 2005T06, 2006E24, 2006E34, 2006E85, 2006E92, 2007E87, 2007E101, 2007T09, 2008E40, 2008E56, 2009E16, 2009E95.
4Available at https://github.com/EdinburghNLP/nematus.",3.1 Setup,[0],[0]
Table 1 shows the translation performance in terms of BLEU score.,3.2 Results,[0],[0]
"Clearly, the proposed approaches significantly outperforms baseline in all cases.
",3.2 Results,[0],[0]
"Baseline (Rows 1-2) NEMATUS significantly outperforms Moses – a commonly used phrasebased SMT system (Koehn et al., 2007), by 2.3 BLEU points on average, indicating that it is a strong NMT baseline system.",3.2 Results,[0],[0]
"It is consistent with the results in (Tu et al., 2017b) (i.e., 26.93 vs. 29.41) on training corpora of similar scale.
",3.2 Results,[0],[0]
"Initialization Strategy (Rows 3-5) Initenc and Initdec improve translation performance by around +1.0 and +1.3 BLEU points individually, proving the effectiveness of warm-start with crosssentence context.",3.2 Results,[0],[0]
"Combining them achieves a further improvement.
",3.2 Results,[0],[0]
Auxiliary Context Strategies (Rows 6-7) The gating auxiliary context strategy achieves a significant improvement of around +1.0 BLEU point over its non-gating counterpart.,3.2 Results,[0],[0]
"This shows that, by acting as a critic, the introduced context gate learns to distinguish the different needs of the global context for generating target words.
",3.2 Results,[0],[0]
Combining (Row 8),3.2 Results,[0],[0]
"Finally, we combine the best variants from the initialization and auxiliary context strategies, and achieve the best performance, improving upon NEMATUS by +2.1 BLEU points.",3.2 Results,[0],[0]
This indicates the two types of strategies are complementary to each other.,3.2 Results,[0],[0]
"We first investigate to what extent the mistranslated errors are fixed by the proposed system.
",3.3 Analysis,[0],[0]
We randomly select 15 documents (about 60 sentences) from the test sets.,3.3 Analysis,[0],[0]
"As shown in Table 2, we count how many related errors: i) are made by NMT (Total), and ii) fixed by our method (Fixed); as well as iii) newly generated (New).",3.3 Analysis,[0],[0]
"About Ambiguity, while we found that 38 words/phrases were translated into incorrect equivalents, 76% of them are corrected by our model.",3.3 Analysis,[0],[0]
"Similarly, we solved 75% of the Inconsistency errors including lexical, tense and definiteness (definite or indefinite articles) cases.",3.3 Analysis,[0],[0]
"However, we also observe that our system brings relative 21% new errors.
",3.3 Analysis,[0],[0]
Case Study Table 3 shows an example.,3.3 Analysis,[0],[0]
The word “腐官” (corrupt officials) is mis-translated as “enemy” by the baseline system.,3.3 Analysis,[0],[0]
"With the help
of the similar word “贪官” in the previous sentence, our approach successfully correct this mistake.",3.3 Analysis,[0],[0]
This demonstrates that cross-sentence context indeed helps resolve certain ambiguities.,3.3 Analysis,[0],[0]
"While our approach is built on top of hierarchical recurrent encoder-decoder (HRED) (Sordoni et al., 2015), there are several key differences which reflect how we have generalized from the original model.",4 Related Work,[0],[0]
"Sordoni et al. (2015) use HRED to summarize a single representation from both the current and previous sentences, which limits itself to (1) it is only applicable to encoder-decoder framework without attention model, (2) the representation can only be used to initialize decoder.",4 Related Work,[0],[0]
"In contrast, we use HRED to summarize the previous sentences alone, which provides additional cross-sentence context for NMT.",4 Related Work,[0],[0]
"Our approach is more flexible at (1) it is applicable to any encoderdecoder frameworks (e.g., with attention), (2) the cross-sentence context can be used to initialize either encoder, decoder or both.
",4 Related Work,[0],[0]
"While both our approach and Serban et al. (2016) use Auxiliary Context mechanism for incorporating cross-sentence context, there are two main differences: 1) we have separate parameters to better control the effects of the cross- and intrasentence contexts, while they only have one parameter matrix to manage the single representation that encodes both contexts; 2) based on the intuition that not every target word generation requires equivalent cross-sentence context, we introduce a context gate (Tu et al., 2017a) to control the amount of information from it, while they don’t.
",4 Related Work,[0],[0]
"At the same time, some researchers propose to use an additional set of an encoder and attention to model more information.",4 Related Work,[0],[0]
"For example, Jean et al. (2017) use it to encode and select part of the previous source sentence for generating each target word.",4 Related Work,[0],[0]
Calixto et al. (2017) utilize global image features extracted using a pre-trained convolutional neural network and incorporate them in NMT.,4 Related Work,[0],[0]
"As additional attention leads to more computational cost, they can only incorporate limited information such as single preceding sentence in Jean et al. (2017).",4 Related Work,[0],[0]
"However, our architecture is free to this limitation, thus we use multiple preceding sentences (e.g. K = 3) in our experiments.
",4 Related Work,[0],[0]
"Our work is also related to multi-source (Zoph and Knight, 2016) and multi-target NMT (Dong
et al., 2015), which incorporate additional source or target languages.",4 Related Work,[0],[0]
"They investigate one-tomany or many-to-one languages translation tasks by integrating additional encoders or decoders into encoder-decoder framework, and their experiments show promising results.",4 Related Work,[0],[0]
"We proposed two complementary approaches to integrating cross-sentence context: 1) a warmstart of encoder and decoder with global context representation, and 2) cross-sentence context serves as an auxiliary information source for updating decoder states, in which an introduced context gate plays an important role.",5 Conclusion and Future Work,[0],[0]
We quantitatively and qualitatively demonstrated that the presented model significantly outperforms a strong attention-based NMT baseline system.,5 Conclusion and Future Work,[0],[0]
"We release the code for these experiments at https:// www.github.com/tuzhaopeng/LC-NMT.
",5 Conclusion and Future Work,[0],[0]
"Our models benefit from larger contexts, and would be possibly further enhanced by other document level information, such as discourse relations.",5 Conclusion and Future Work,[0],[0]
We propose to study such models for full length documents with more linguistic features in future work.,5 Conclusion and Future Work,[0],[0]
This work is supported by the Science Foundation of Ireland (SFI) ADAPT project (Grant No.:13/RC/2106).,Acknowledgments,[0],[0]
The authors also wish to thank the anonymous reviewers for many helpful comments with special thanks to Henry Elder for his generous help on proofreading of this manuscript.,Acknowledgments,[0],[0]
"In translation, considering the document as a whole can help to resolve ambiguities and inconsistencies.",abstractText,[0],[0]
"In this paper, we propose a cross-sentence context-aware approach and investigate the influence of historical contextual information on the performance of neural machine translation (NMT).",abstractText,[0],[0]
"First, this history is summarized in a hierarchical way.",abstractText,[0],[0]
"We then integrate the historical representation into NMT in two strategies: 1) a warm-start of encoder and decoder states, and 2) an auxiliary context source for updating decoder states.",abstractText,[0],[0]
Experimental results on a large Chinese-English translation task show that our approach significantly improves upon a strong attention-based NMT system by up to +2.1 BLEU points.,abstractText,[0],[0]
Exploiting Cross-Sentence Context for Neural Machine Translation,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4253–4262 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
4253",text,[0],[0]
"Neural machine translation (NMT) models have advanced the machine translation community in recent years (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014).",1 Introduction,[0],[0]
"NMT models generally consist of two components: an encoder network to summarize the input sentence into sequential representations, based on which a decoder network generates target sentence word by word with an attention model (Bahdanau et al., 2015; Luong et al., 2015).
",1 Introduction,[0],[0]
"Nowadays, advanced NMT models generally implement encoder and decoder as multiple layers, regardless of the specific model architectures such as RNN (Zhou et al., 2016; Wu et al., 2016), CNN (Gehring et al., 2017), or Self-Attention Network (Vaswani et al., 2017; Chen et al., 2018).
∗ Zhaopeng Tu is the corresponding author of the paper.",1 Introduction,[0],[0]
"This work was conducted when Zi-Yi Dou was interning at Tencent AI Lab.
",1 Introduction,[0],[0]
"Several researchers have revealed that different layers are able to capture different types of syntax and semantic information (Shi et al., 2016; Peters et al., 2018; Anastasopoulos and Chiang, 2018).",1 Introduction,[0],[0]
"For example, Shi et al. (2016) find that both local and global source syntax are learned by the NMT encoder and different types of syntax are captured at different layers.
",1 Introduction,[0],[0]
"However, current NMT models only leverage the top layers of encoder and decoder in the subsequent process, which misses the opportunity to exploit useful information embedded in other layers.",1 Introduction,[0],[0]
"Recently, aggregating layers to better fuse semantic and spatial information has proven to be of profound value in computer vision tasks (Huang et al., 2017; Yu et al., 2018).",1 Introduction,[0],[0]
"In natural language processing community, Peters et al. (2018) have proven that simultaneously exposing all layer representations outperforms methods that utilize just the top layer for transfer learning tasks.
",1 Introduction,[0],[0]
"Inspired by these findings, we propose to exploit deep representations for NMT models.",1 Introduction,[0],[0]
"Specifically, we investigate two types of strategies to better fuse information across layers, ranging from layer aggregation to multi-layer attention.",1 Introduction,[0],[0]
"While layer aggregation strategies combine hidden states at the same position across different layers, multi-layer attention allows the model to combine information in different positions.",1 Introduction,[0],[0]
"In addition, we introduce an auxiliary objective to encourage different layers to capture diverse information, which we believe would make the deep representations more meaningful.
",1 Introduction,[0],[0]
We evaluated our approach on two widelyused WMT14 English⇒German and WMT17 Chinese⇒English translation tasks.,1 Introduction,[0],[0]
"We employed TRANSFORMER (Vaswani et al., 2017) as the baseline system since it has proven to outperform other architectures on the two tasks (Vaswani et al., 2017; Hassan et al., 2018).",1 Introduction,[0],[0]
"Experimen-
tal results show that exploiting deep representations consistently improves translation performance over the vanilla TRANSFORMER model across language pairs.",1 Introduction,[0],[0]
It is worth mentioning that TRANSFORMER-BASE with deep representations exploitation outperforms the vanilla TRANSFORMER-BIG model with only less than half of the parameters.,1 Introduction,[0],[0]
"Deep representations have proven to be of profound value in machine translation (Meng et al., 2016; Zhou et al., 2016).",2 Background: Deep NMT,[0],[0]
Multiple-layer encoder and decoder are employed to perform the translation task through a series of nonlinear transformations from the representation of input sequences to final output sequences.,2 Background: Deep NMT,[0],[0]
"The layer can be implemented as RNN (Wu et al., 2016), CNN (Gehring et al., 2017), or Self-Attention Network (Vaswani et al., 2017).",2 Background: Deep NMT,[0],[0]
"In this work, we take the advanced Transformer as an example, which will be used in experiments later.",2 Background: Deep NMT,[0],[0]
"However, we note that the proposed approach is generally applicable to any other type of NMT architectures.
",2 Background: Deep NMT,[0],[0]
"Specifically, the encoder is composed of a stack of L identical layers, each of which has two sublayers.",2 Background: Deep NMT,[0],[0]
"The first sub-layer is a self-attention network, and the second one is a position-wise fully connected feed-forward network.",2 Background: Deep NMT,[0],[0]
"A residual connection (He et al., 2016) is employed around each of the two sub-layers, followed by layer normalization (Ba et al., 2016).",2 Background: Deep NMT,[0],[0]
"Formally, the output of the first sub-layer Cle and the second sub-layer H l e are calculated as
Cle = LN ( ATT(Qle,K l−1 e ,V l−1 e )",2 Background: Deep NMT,[0],[0]
"+H l−1 e ) ,
Hle = LN ( FFN(Cle) +C l e ) , (1)
where ATT(·), LN(·), and FFN(·) are selfattention mechanism, layer normalization, and feed-forward networks with ReLU activation in between, respectively.",2 Background: Deep NMT,[0],[0]
"{Qle,Kl−1e ,Vl−1e } are query, key and value vectors that are transformed from the (l-1)-th encoder layer Hl−1e .
",2 Background: Deep NMT,[0],[0]
The decoder is also composed of a stack of L identical layers.,2 Background: Deep NMT,[0],[0]
"In addition to two sub-layers in each decoder layer, the decoder inserts a third sublayer Dld to perform attention over the output of
the encoder stack HLe :
Cld = LN ( ATT(Qld,K l−1 d ,V l−1 d )",2 Background: Deep NMT,[0],[0]
"+H l−1 d ) ,
Dld = LN ( ATT(Cld,K L e ,V L e )",2 Background: Deep NMT,[0],[0]
"+C l d ) ,
Hld = LN ( FFN(Dld) +D l d ) , (2)
where {Qld,K l−1 d ,V l−1 d } are transformed from the (l-1)-th decoder layer Hl−1d , and {K L e ,V L e } are transformed from the top layer of the encoder.",2 Background: Deep NMT,[0],[0]
"The top layer of the decoder HLd is used to generate the final output sequence.
",2 Background: Deep NMT,[0],[0]
"Multi-layer network can be considered as a strong feature extractor with extended receptive fields capable of linking salient features from the entire sequence (Chen et al., 2018).",2 Background: Deep NMT,[0],[0]
"However, one potential problem about the vanilla Transformer, as shown in Figure 1a, is that both the encoder and decoder stack layers in sequence and only utilize the information in the top layer.",2 Background: Deep NMT,[0],[0]
"While studies have shown deeper layers extract more semantic and more global features (Zeiler and Fergus, 2014; Peters et al., 2018), these do not prove that the last layer is the ultimate representation for any task.",2 Background: Deep NMT,[0],[0]
"Although residual connections have been incorporated to combine layers, these connections have been “shallow” themselves, and only fuse by simple, one-step operations (Yu et al., 2018).",2 Background: Deep NMT,[0],[0]
"We investigate here how to better fuse information across layers for NMT models.
",2 Background: Deep NMT,[0],[0]
"In the following sections, we simplify the equations to Hl = LAYER(Hl−1) for brevity.",2 Background: Deep NMT,[0],[0]
"In this section, we first introduce how to exploit deep representations by simultaneously exposing all of the signals from all layers (Sec 3.1).",3 Proposed Approaches,[0],[0]
"Then, to explicitly encourage different layers to incorporate various information, we propose one way to measure the diversity between layers and add a regularization term to our objective function to maximize the diversity across layers (Sec 3.2).",3 Proposed Approaches,[0],[0]
"To exploit deep representations, we investigate two types of strategies to fuse information across layers, from layer aggregation to multi-layer attention.",3.1 Deep Representations,[0],[0]
"While layer aggregation strategies combine hidden states at the same position across different layers, multi-layer attention allows the model to combine information in different positions.",3.1 Deep Representations,[0],[0]
"While the aggregation strategies are inspired by previous work, there are several differences since we have simplified and generalized from the original model, as described below.",3.1.1 Layer Aggregation,[0],[0]
Dense Connection.,3.1.1 Layer Aggregation,[0],[0]
"The first strategy is to allow all layers to directly access previous layers:
Hl = f(H1, . . .",3.1.1 Layer Aggregation,[0],[0]
",Hl−1).",3.1.1 Layer Aggregation,[0],[0]
"(3)
In this work, we mainly investigate whether densely connected networks work for NMT, which have proven successful in computer vision tasks (Huang et al., 2017).",3.1.1 Layer Aggregation,[0],[0]
"The basic strategy of densely connected networks is to connect each layer to every previous layer with a residual connection:
Hl = Layer(Hl−1) + l−1∑ i=1",3.1.1 Layer Aggregation,[0],[0]
"Hi. (4)
",3.1.1 Layer Aggregation,[0],[0]
Figure 1b illustrates the idea of this approach.,3.1.1 Layer Aggregation,[0],[0]
"Our implementation differs from (Huang et al., 2017) in that we use an addition instead of a concatenation operation in order to keep the state size constant.",3.1.1 Layer Aggregation,[0],[0]
"Another reason is that concatenation operation is computationally expensive, while residual connections are more efficient.
",3.1.1 Layer Aggregation,[0],[0]
"While dense connection directly feeds previous layers to the subsequent layers, the following mechanisms maintain additional layers to aggregate standard layers, from shallow linear combination, to deep non-linear aggregation.
",3.1.1 Layer Aggregation,[0],[0]
Linear Combination.,3.1.1 Layer Aggregation,[0],[0]
"As shown in Figure 1c, an intuitive strategy is to linearly combine the outputs of all layers:
Ĥ = L∑ l=1",3.1.1 Layer Aggregation,[0],[0]
"WlH l, (5)
",3.1.1 Layer Aggregation,[0],[0]
"where {W1, . . .",3.1.1 Layer Aggregation,[0],[0]
",WL} are trainable matrices.",3.1.1 Layer Aggregation,[0],[0]
"While the strategy is similar in spirit to (Peters et al., 2018), there are two main differences: (1) they use normalized weights while we directly use parameters that could be either positive or negative numbers, which may benefit from more modeling flexibility.",3.1.1 Layer Aggregation,[0],[0]
"(2) they use a scalar that is shared by all elements in the layer states, while we use learnable matrices.",3.1.1 Layer Aggregation,[0],[0]
"The latter offers a more precise control of the combination by allowing the model to be more expressive than scalars (Tu et al., 2017).
",3.1.1 Layer Aggregation,[0],[0]
"We also investigate strategies that iteratively and hierarchically merge layers by incorporating more depth and sharing, which have proven effective for computer vision tasks (Yu et al., 2018).
",3.1.1 Layer Aggregation,[0],[0]
Iterative Aggregation.,3.1.1 Layer Aggregation,[0],[0]
"As illustrated in Figure 1d, iterative aggregation follows the iterated stacking of the backbone architecture.",3.1.1 Layer Aggregation,[0],[0]
"Aggregation begins at the shallowest, smallest scale and then iteratively merges deeper, larger scales.",3.1.1 Layer Aggregation,[0],[0]
"The iterative deep aggregation function I for a series of layers Hl1 = {H1, · · · ,Hl} with increasingly deeper and semantic information is formulated as
Ĥl = I(Hl1) = AGG(H",3.1.1 Layer Aggregation,[0],[0]
"l, Ĥl−1), (6)
where we set Ĥ1 = H1 and AGG(·, ·) is the aggregation function:
AGG(x, y) = LN(FF([x; y])",3.1.1 Layer Aggregation,[0],[0]
+ x+ y).,3.1.1 Layer Aggregation,[0],[0]
"(7)
As seen, in this work, we first concatenate x and y into z =",3.1.1 Layer Aggregation,[0],[0]
"[x; y], which is subsequently fed to a feed-forward network with a sigmoid activation in between.",3.1.1 Layer Aggregation,[0],[0]
Residual connection and layer normalization are also employed.,3.1.1 Layer Aggregation,[0],[0]
"Specifically, both x and y have residual connections to the output.",3.1.1 Layer Aggregation,[0],[0]
"The choice of the aggregation function will be further studied in the experiment section.
",3.1.1 Layer Aggregation,[0],[0]
Hierarchical Aggregation.,3.1.1 Layer Aggregation,[0],[0]
"While iterative aggregation deeply combines states, it may still be insufficient to fuse the layers for its sequential architecture.",3.1.1 Layer Aggregation,[0],[0]
"Hierarchical aggregation, on the other hand, merges layers through a tree structure to preserve and combine feature channels, as shown in Figure 2.",3.1.1 Layer Aggregation,[0],[0]
"The original model proposed by Yu et al. (2018) requires the number of layers to be the power of two, which limits the applicability of these methods to a broader range of NMT architectures (e.g. six layers in (Vaswani et al., 2017)).",3.1.1 Layer Aggregation,[0],[0]
"To solve this problem, we introduce a CNN-like tree with the filter size being two, as shown in Figure 2a.",3.1.1 Layer Aggregation,[0],[0]
"Following (Yu et al., 2018), we first merge aggregation nodes of the same depth for efficiency so that there would be at most one aggregation node for each depth.",3.1.1 Layer Aggregation,[0],[0]
"Then, we further feed the output of an aggregation node back into the backbone as the input to the next sub-tree, instead of only routing intermediate aggregations further up the tree, as shown in Figure 2b.",3.1.1 Layer Aggregation,[0],[0]
"The interaction between aggregation and backbone nodes allows the model to better preserve features.
",3.1.1 Layer Aggregation,[0],[0]
"Formally, each aggregation node Ĥi is calculated as
Ĥi = { AGG(H2i−1,H2i), i = 1 AGG(H2i−1,H2i, Ĥi−1), i = 2, 3
where AGG(H2i−1,H2i) is computed via Eqn. 7, and AGG(H2i−1,H2i, Ĥi−1) is computed as
AGG(x, y, z) = LN(FF([x; y; z])",3.1.1 Layer Aggregation,[0],[0]
"+ x+ y + z).
",3.1.1 Layer Aggregation,[0],[0]
The aggregation node at the top layer ĤL/2 serves as the final output of the network.,3.1.1 Layer Aggregation,[0],[0]
"Partially inspired by Meng et al. (2016), we also propose to introduce a multi-layer attention mechanism into deep NMT models, for more power of
transforming information across layers.",3.1.2 Multi-Layer Attention,[0],[0]
"In other words, for constructing each hidden state in any layer-l, we allow the self-attention model to attend any layers lower than l, instead of just layer l-1:
Cl−1 = ATT(Q l,Kl−1,Vl−1), Cl−2 = ATT(Q l,Kl−2,Vl−2),
. .",3.1.2 Multi-Layer Attention,[0],[0]
".
",3.1.2 Multi-Layer Attention,[0],[0]
Cl−k = ATT(Q,3.1.2 Multi-Layer Attention,[0],[0]
"l,Kl−k,Vl−k),
Cl = AGG(Cl−1, . . .",3.1.2 Multi-Layer Attention,[0],[0]
",C l −k), (8)
where Cl−i is sequential vectors queried from layer",3.1.2 Multi-Layer Attention,[0],[0]
"l-i using a separate attention model, and AGG(·) is similar to the pre-defined aggregation function to transform k vectors {Cl−1, . . .",3.1.2 Multi-Layer Attention,[0],[0]
",Cl−k} to a d-dimension vector, which is subsequently used to construct the encoder and decoder layers via Eqn. 1 and 2 respectively.",3.1.2 Multi-Layer Attention,[0],[0]
"Note that multilayer attention only modifies the self-attention blocks in both encoder and decoder, while does not revises the encoder-decoder attention blocks.",3.1.2 Multi-Layer Attention,[0],[0]
"Intuitively, combining layers would be more meaningful if different layers are able to capture diverse information.",3.2 Layer Diversity,[0],[0]
"Therefore, we explicitly add a regularization term to encourage the diversities between layers:
L = Llikelihood + λLdiversity, (9)
where λ is a hyper-parameter and is set to 1.0 in this paper.",3.2 Layer Diversity,[0],[0]
"Specifically, the regularization term measures the average of the distance between ev-
ery two adjacent layers:
Ldiversity = 1
L− 1 L−1∑ l=1 D(Hl,Hl+1).",3.2 Layer Diversity,[0],[0]
"(10)
HereD(Hl,Hl+1) is the averaged cosine-squared distance between the states in layers",3.2 Layer Diversity,[0],[0]
"Hl = {hl1, . . .",3.2 Layer Diversity,[0],[0]
",hlN} and Hl+1",3.2 Layer Diversity,[0],[0]
"= {h l+1 1 , . . .",3.2 Layer Diversity,[0],[0]
",h l+1 N }:
D(Hl,Hl+1)",3.2 Layer Diversity,[0],[0]
"= 1
N N∑ n=1",3.2 Layer Diversity,[0],[0]
"(1− cos2(hln,hl+1n )).
",3.2 Layer Diversity,[0],[0]
"The cosine-squared distance between two vectors is maximized when two vectors are linearly independent and minimized when two vectors are linearly dependent, which satisfies our goal.1",3.2 Layer Diversity,[0],[0]
Dataset.,4.1 Setup,[0],[0]
"To compare with the results reported by previous work (Gehring et al., 2017; Vaswani et al., 2017; Hassan et al., 2018), we conducted experiments on both Chinese⇒English (Zh⇒En) and English⇒German (En⇒De) translation tasks.",4.1 Setup,[0],[0]
"For the Zh⇒En task, we used all of the available parallel data with maximum length limited to 50, consisting of about 20.62 million sentence pairs.",4.1 Setup,[0],[0]
We used newsdev2017 as the development set and newstest2017 as the test set.,4.1 Setup,[0],[0]
"For the En⇒De task, we trained on the widely-used WMT14 dataset consisting of about 4.56 million sentence pairs.",4.1 Setup,[0],[0]
We used newstest2013 as the development set and newstest2014 as the test set.,4.1 Setup,[0],[0]
"Byte-pair encoding (BPE) was employed to alleviate the Out-of-Vocabulary problem (Sennrich et al., 2016) with 32K merge operations for both language pairs.",4.1 Setup,[0],[0]
"We used 4-gram NIST BLEU score (Papineni et al., 2002) as the evaluation metric, and sign-test (Collins et al., 2005) to test for statistical significance.
Models.",4.1 Setup,[0],[0]
"We evaluated the proposed approaches on advanced Transformer model (Vaswani et al., 2017), and implemented on top of an open-source toolkit – THUMT (Zhang et al., 2017).",4.1 Setup,[0],[0]
"We followed Vaswani et al. (2017) to set the configurations and train the models, and have reproduced
1We use cosine-squared distance instead of cosine distance, since the latter is maximized when two vectors are in opposite directions.",4.1 Setup,[0],[0]
"In such case, the two vectors are in fact linearly dependent, while we aim at encouraging the vectors independent from each other.
",4.1 Setup,[0],[0]
their reported results on the En⇒De task.,4.1 Setup,[0],[0]
The parameters of the proposed models were initialized by the pre-trained model.,4.1 Setup,[0],[0]
"We tried k = 2 and k = 3 for the multi-layer attention model, which allows to attend to the lower two or three layers.
",4.1 Setup,[0],[0]
"We have tested both Base and Big models, which differ at hidden size (512 vs. 1024), filter size (2048 vs. 4096) and the number of attention heads (8 vs. 16).2 All the models were trained on eight NVIDIA P40 GPUs where each was allocated with a batch size of 4096 tokens.",4.1 Setup,[0],[0]
"In consideration of computation cost, we studied model variations with Base model on En⇒De task, and evaluated overall performance with Big model on both Zh⇒En and En⇒De tasks.",4.1 Setup,[0],[0]
Table 1 shows the results on WMT14 En⇒De translation task.,4.2 Results,[0],[0]
"As seen, the proposed approaches improve the translation quality in all cases, although there are still considerable differences among different variations.
",4.2 Results,[0],[0]
"Model Complexity Except for dense connection, all other deep representation strategies introduce new parameters, ranging from 14.7M to 33.6M. Accordingly, the training speed decreases due to more efforts to train the new parameters.",4.2 Results,[0],[0]
"Layer aggregation mechanisms only marginally decrease decoding speed, while multi-layer attention decreases decoding speed by 21% due to an additional attention process for each layer.
",4.2 Results,[0],[0]
Layer Aggregation (Rows 2-5):,4.2 Results,[0],[0]
"Although dense connection and linear combination only marginally improve translation performance, iterative and hierarchical aggregation strategies achieve more significant improvements, which are up to +0.99 BLEU points better than the baseline model.",4.2 Results,[0],[0]
"This indicates that deep aggregations outperform their shallow counterparts by incorporating more depth and sharing, which is consistent with the results in computer vision tasks (Yu et al., 2018).
",4.2 Results,[0],[0]
"Multi-Layer Attention (Rows 6-7): Benefiting from the power of attention models, multi-layer attention model can also significantly outperform baseline, although it only attends to one or two additional layers.",4.2 Results,[0],[0]
"However, increasing the number of lower layers to be attended from k = 2 to
2Here “filter size” refers to the hidden size of the feedforward network in the Transformer model.
",4.2 Results,[0],[0]
"k = 3 only gains marginal improvement, at the cost of slower training and decoding speeds.",4.2 Results,[0],[0]
"In the following experiments, we set set k = 2 for the multi-layer attention model.
",4.2 Results,[0],[0]
Layer Diversity (Rows 8-10): The introduced diversity regularization consistently improves performance in all cases by encouraging different layers to capture diverse information.,4.2 Results,[0],[0]
Our best model outperforms the vanilla Transformer by +1.14 BLEU points.,4.2 Results,[0],[0]
"In the following experiments, we used hierarchical aggregation with diversity regularization (Row 8) as the default strategy.
",4.2 Results,[0],[0]
Main Results Table 2 lists the results on both WMT17 Zh⇒En and WMT14 En⇒De translation tasks.,4.2 Results,[0],[0]
"As seen, exploiting deep represen-
tations consistently improves translation performance across model variations and language pairs, demonstrating the effectiveness and universality of the proposed approach.",4.2 Results,[0],[0]
"It is worth mentioning that TRANSFORMER-BASE with deep representations exploitation outperforms the vanilla TRANSFORMER-BIG model, with only less than half of the parameters.",4.2 Results,[0],[0]
We conducted extensive analysis from different perspectives to better understand our model.,4.3 Analysis,[0],[0]
"All results are reported on the En⇒De task with TRANSFORMER-BASE.
",4.3 Analysis,[0],[0]
"Sentence Length
B LE
U
22
23
24
25
26
27
28
29
30
31
Length of Source Sentence
(0, 10
]
(10 , 2
0]
(20 , 3
0]
(30 , 4
0]
(40 , 5
0] (50 , )
",4.3 Analysis,[0],[0]
"Ours Base
B LE
U
25
26
27
28
29
30
Length of Source Sentence
(0, 15
]
(15 , 3
0]
(30 , 4
5] > 4 5
Ours Base
B LE
U
25
26
27
28
29
30
Length of Source Sentence
(0, 15
]
(15 , 3
0]
(30 , 4
5]",4.3 Analysis,[0],[0]
"> 4 5
Hier.+Div. Hier.",4.3 Analysis,[0],[0]
"Base
Figure 4: BLEU scores on the En⇒De test set with respect to various input sentence lengths.",4.3 Analysis,[0],[0]
“Hier.” denotes hierarchical aggregation and “Div.” denotes diversity regularization.,4.3 Analysis,[0],[0]
"Following Bahdanau et al. (2015) and Tu et al. (2016), we grouped sentences of similar lengths together and computed the BLEU score for each group, as shown in Figure 4.",4.3.1 Length Analysis,[0],[0]
"Generally, the performance of TRANSFORMER-BASE goes up with the increase of input sentence lengths, which is superior to the performance of RNN-based NMT models on long sentences reported by (Bentivogli et al., 2016).",4.3.1 Length Analysis,[0],[0]
"We attribute this to the strength of self-attention mechanism to model global dependencies without regard to their distance.
",4.3.1 Length Analysis,[0],[0]
"Clearly, the proposed approaches outperform the baseline model in all length segments, while there are still considerable differences between the two variations.",4.3.1 Length Analysis,[0],[0]
"Hierarchical aggregation consistently outperforms the baseline model, and the improvement goes up on long sentences.",4.3.1 Length Analysis,[0],[0]
One possible reason is that long sentences indeed require deep aggregation mechanisms.,4.3.1 Length Analysis,[0],[0]
"Introducing diversity regularization further improves performance on most sentences (e.g. ≤ 45), while the improvement degrades on long sentences (e.g. > 45).",4.3.1 Length Analysis,[0],[0]
"We conjecture that complex long sentences may need to store duplicate information across layers, which conflicts with the diversity objective.",4.3.1 Length Analysis,[0],[0]
"Both encoder and decoder are composed of a stack of L layers, which may benefit from the proposed approach.",4.3.2 Effect on Encoder and Decoder,[0],[0]
"In this experiment, we investigated how our models affect the two components, as shown
Model Applied to BLEU Encoder Decoder
BASE N/A N/A 26.13
OURS X × 26.32 × X 26.41 X X 26.69
Table 3:",4.3.2 Effect on Encoder and Decoder,[0],[0]
"Experimental results of applying hierarchical aggregation to different components on En⇒De validation set.
in Table 3.",4.3.2 Effect on Encoder and Decoder,[0],[0]
"Exploiting deep representations of encoder or decoder individually consistently outperforms the vanilla baseline model, and exploiting both components further improves the performance.",4.3.2 Effect on Encoder and Decoder,[0],[0]
These results provide support for the claim that exploiting deep representations is useful for both understanding input sequence and generating output sequence.,4.3.2 Effect on Encoder and Decoder,[0],[0]
"As described in Section 3.1.1, the function of hierarchical layer aggregation is defined as
AGG(x, y, z) = LN(FF([x; y; z])",4.3.3 Impact of Aggregation Choices,[0],[0]
"+ x+ y + z),
where FF(·) is a feed-forward network with a sigmoid activation in between.",4.3.3 Impact of Aggregation Choices,[0],[0]
"In addition, all the input layers {x, y, z} have residual connections to the output.",4.3.3 Impact of Aggregation Choices,[0],[0]
"In this experiment, we evaluated the impact of residual connection options, as well as different choices for the aggregation function, as shown in Table 4.
",4.3.3 Impact of Aggregation Choices,[0],[0]
"Concerning residual connections, if none of the input layers are connected to the output layer (“None”), the performance would decrease.",4.3.3 Impact of Aggregation Choices,[0],[0]
"The translation performance is improved when the output is connected to only the top level of the input layers (“Top”), while connecting to all input layers (“All”) achieves the best performance.",4.3.3 Impact of Aggregation Choices,[0],[0]
"This indi-
cates that cross-layer connections are necessary to avoid the gradient vanishing problem.
",4.3.3 Impact of Aggregation Choices,[0],[0]
"Besides the feed-forward network with sigmoid activation, we also tried two other aggregation functions for FF(·): (1) A feed-forward network with a RELU activation in between; and (2) multihead self-attention layer that constitutes the encoder and decoder layers in the TRANSFORMER model.",4.3.3 Impact of Aggregation Choices,[0],[0]
"As seen, all the three functions consistently improve the translation performance, proving the robustness of the proposed approaches.
",4.3.3 Impact of Aggregation Choices,[0],[0]
"4.4 Visualization of Aggregation
To investigate the impact of diversity regularization, we visualized the exploitation of the input representations for hierarchical aggregation in encoder side, as shown in Figure 5.",4.3.3 Impact of Aggregation Choices,[0],[0]
"Let Hi = {H2i, H2i−1, Ĥ i−1} be the input representations, we calculated the exploitation of the j-th input as
sj =
∑ w∈Wj |w|∑
Hj′∈H { ∑ w′∈Wj′ |w′|} , (11)
where Wj is the parameter matrix associated with the input Hj .",4.3.3 Impact of Aggregation Choices,[0],[0]
The score sj is a rough estimation of the contribution of Hj to the aggregation,4.3.3 Impact of Aggregation Choices,[0],[0]
"Ĥ i.
We have two observations.",4.3.3 Impact of Aggregation Choices,[0],[0]
"First, the model tends to utilize the bottom layer more than the top one, indicating the necessity of fusing information across layers.",4.3.3 Impact of Aggregation Choices,[0],[0]
"Second, using the diversity regularization in Figure 5(b) can encourage each layer to contribute more equally to the aggregation.",4.3.3 Impact of Aggregation Choices,[0],[0]
We hypothesize this is because of the diversity regularization term encouraging the different layers to contain diverse and equally important information.,4.3.3 Impact of Aggregation Choices,[0],[0]
Representation learning is at the core of deep learning.,5 Related Work,[0],[0]
"Our work is inspired by technological advances in representation learning, specifically in the field of deep representation learning and representation interpretation.
",5 Related Work,[0],[0]
"Deep Representation Learning Deep neural networks have advanced the state of the art in various communities, such as computer vision and natural language processing.",5 Related Work,[0],[0]
"One key challenge of training deep networks lies in how to transform information across layers, especially when the network consists of hundreds of layers.
",5 Related Work,[0],[0]
"In response to this problem, ResNet (He et al., 2016) uses skip connections to combine layers by simple, one-step operations.",5 Related Work,[0],[0]
"Densely connected network (Huang et al., 2017) is designed to better propagate features and losses through skip connections that concatenate all the layers in stages.",5 Related Work,[0],[0]
"Yu et al. (2018) design structures iteratively and hierarchically merge the feature hierarchy to better fuse information in a deep fusion.
",5 Related Work,[0],[0]
"Concerning machine translation, Meng et al. (2016) and Zhou et al. (2016) have shown that deep networks with advanced connecting strategies outperform their shallow counterparts.",5 Related Work,[0],[0]
"Due to its simplicity and effectiveness, skip connection becomes a standard component of state-of-the-art NMT models (Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017).",5 Related Work,[0],[0]
"In this work, we prove that deep representation exploitation can further improve performance over simply using skip connections.
",5 Related Work,[0],[0]
"Representation Interpretation Several researchers have tried to visualize the representation of each layer to help better understand what information each layer captures (Zeiler and Fergus, 2014; Li et al., 2016; Ding et al., 2017).",5 Related Work,[0],[0]
"Concerning natural language processing tasks, Shi et al. (2016) find that both local and global source syntax are learned by the NMT encoder and different types of syntax are captured at different layers.",5 Related Work,[0],[0]
Anastasopoulos and Chiang (2018) show that higher level layers are more representative than lower level layers.,5 Related Work,[0],[0]
Peters et al. (2018) demonstrate that higher-level layers capture context-dependent aspects of word meaning while lower-level layers model aspects of syntax.,5 Related Work,[0],[0]
"Inspired by these observations, we propose to expose all of these representations to better
fuse information across layers.",5 Related Work,[0],[0]
"In addition, we introduce a regularization to encourage different layers to capture diverse information.",5 Related Work,[0],[0]
"In this work, we propose to better exploit deep representations that are learned by multiple layers for neural machine translation.",6 Conclusion,[0],[0]
"Specifically, the hierarchical aggregation with diversity regularization achieves the best performance by incorporating more depth and sharing across layers and by encouraging layers to capture different information.",6 Conclusion,[0],[0]
"Experimental results on WMT14 English⇒German and WMT17 Chinese⇒English show that the proposed approach consistently outperforms the state-of-theart TRANSFORMER baseline by +0.54 and +0.63 BLEU points, respectively.",6 Conclusion,[0],[0]
"By visualizing the aggregation process, we find that our model indeed utilizes lower layers to effectively fuse the information across layers.
",6 Conclusion,[0],[0]
"Future directions include validating our approach on other architectures such as RNN (Bahdanau et al., 2015) or CNN (Gehring et al., 2017) based NMT models, as well as combining with other advanced techniques (Shaw et al., 2018; Shen et al., 2018; Yang et al., 2018; Li et al., 2018) to further improve the performance of TRANSFORMER.",6 Conclusion,[0],[0]
We thank the anonymous reviewers for their insightful comments.,Acknowledgments,[0],[0]
"Advanced neural machine translation (NMT) models generally implement encoder and decoder as multiple layers, which allows systems to model complex functions and capture complicated linguistic structures.",abstractText,[0],[0]
"However, only the top layers of encoder and decoder are leveraged in the subsequent process, which misses the opportunity to exploit the useful information embedded in other layers.",abstractText,[0],[0]
"In this work, we propose to simultaneously expose all of these signals with layer aggregation and multi-layer attention mechanisms.",abstractText,[0],[0]
"In addition, we introduce an auxiliary regularization term to encourage different layers to capture diverse information.",abstractText,[0],[0]
Experimental results on widely-used WMT14 English⇒German and WMT17 Chinese⇒English translation data demonstrate the effectiveness and universality of the proposed approach.,abstractText,[0],[0]
Exploiting Deep Representations for Neural Machine Translation,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 155–160 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-2024",text,[0],[0]
"Neural models are powerful in part due to their ability to learn good representations of raw textual inputs, mitigating the need for extensive task-specific feature engineering (Collobert et al., 2011).",1 Introduction,[0],[0]
"However, a downside of learning from scratch is failing to capitalize on prior linguistic or semantic knowledge, often encoded in existing resources such as ontologies.",1 Introduction,[0],[0]
Such prior knowledge can be particularly valuable when estimating highly flexible models.,1 Introduction,[0],[0]
"In this work, we address how to exploit known relationships between words when training neural models for NLP tasks.
",1 Introduction,[0],[0]
"We propose exploiting the feature-hashing trick, originally proposed as a means of neural network compression (Chen et al., 2015).",1 Introduction,[0],[0]
"Here we instead view the partial parameter sharing induced by feature hashing as a flexible mechanism for tying together network node weights that we believe
to be similar a priori.",1 Introduction,[0],[0]
"In effect, this acts as a regularizer that constrains the model to learn weights that agree with the domain knowledge codified in external resources like ontologies.
",1 Introduction,[0],[0]
"More specifically, as external resources we use Brown clusters (Brown et al., 1992), WordNet (Miller, 1995) and the Unified Medical Language System (UMLS) (Bodenreider, 2004).",1 Introduction,[0],[0]
From these we derive groups of words with similar meaning.,1 Introduction,[0],[0]
We then use feature hashing to share a subset of weights between the embeddings of words that belong to the same semantic group(s).,1 Introduction,[0],[0]
"This forces the model to respect prior domain knowledge, insofar as words similar under a given ontology are compelled to have similar embeddings.
",1 Introduction,[0],[0]
"Our contribution is a novel, simple and flexible method for injecting domain knowledge into neural models via stochastic weight sharing.",1 Introduction,[0],[0]
"Results on seven diverse classification tasks (three sentiment and four biomedical) show that our method consistently improves performance over (1) baselines that fail to capitalize on domain knowledge, and (2) an approach that uses retrofitting (Faruqui et al., 2014) as a preprocessing step to encode domain knowledge prior to training.
155",1 Introduction,[0],[0]
"We incorporate similarity relations codified in existing resources (here derived from Brown clusters, SentiWordNet and the UMLS) as prior knowledge in a Convolutional Neural Network (CNN).1 To achieve this we construct a shared embedding matrix such that words known a priori to be similar are constrained to share some fraction of embedding weights.
",2 Grouped Weight Sharing,[0],[0]
"Concretely, suppose we have N groups of words derived from an external resource.",2 Grouped Weight Sharing,[0],[0]
"Note that one could derive such groups in several ways; e.g., using the synsets in SentiWordNet.",2 Grouped Weight Sharing,[0],[0]
"We denote groups by {g1, g2, ..., gN}.",2 Grouped Weight Sharing,[0],[0]
"Each group is associated with an embedding ggi , which we initialize by averaging the pre-trained embeddings of each word in the group.
",2 Grouped Weight Sharing,[0],[0]
"To exploit both grouped and independent word weights, we adopt a two-channel CNN model (Zhang et al., 2016b).",2 Grouped Weight Sharing,[0],[0]
The embedding matrix of the first channel is initialized with pre-trained word vectors.,2 Grouped Weight Sharing,[0],[0]
We denote this input by Ep ∈ RV×d,2 Grouped Weight Sharing,[0],[0]
(V is the vocabulary size and d the dimension of the word embeddings).,2 Grouped Weight Sharing,[0],[0]
The second channel input matrix is initialized with our proposed weightsharing embedding Es ∈ RV×d.,2 Grouped Weight Sharing,[0],[0]
"Es is initialized by drawing from both Ep and the external resource following the process we describe below.
",2 Grouped Weight Sharing,[0],[0]
"Given an input text sequence of length l, we construct sequence embedding representations Wp ∈",2 Grouped Weight Sharing,[0],[0]
Rl×d,2 Grouped Weight Sharing,[0],[0]
and,2 Grouped Weight Sharing,[0],[0]
Ws ∈,2 Grouped Weight Sharing,[0],[0]
Rl×d using the corresponding embedding matrices.,2 Grouped Weight Sharing,[0],[0]
We then apply independent sets of linear convolution filters on these two matrices.,2 Grouped Weight Sharing,[0],[0]
Each filter will generate a feature map vector v ∈,2 Grouped Weight Sharing,[0],[0]
Rl−h+1 (h is the filter height).,2 Grouped Weight Sharing,[0],[0]
"We perform 1-max pooling over each v, extracting one scalar per feature map.",2 Grouped Weight Sharing,[0],[0]
"Finally, we concatenate scalars from all of the feature maps (from both channels) into a feature vector which is fed to a softmax function to predict the label (Figure 2).
",2 Grouped Weight Sharing,[0],[0]
We initialize Es as follows.,2 Grouped Weight Sharing,[0],[0]
Each row ei ∈ Rd of Es is the embedding of word i. Words may belong to one or more groups.,2 Grouped Weight Sharing,[0],[0]
"A mapping function G(i) retrieves the groups that word i belongs to, i.e., G(i) returns a subset of {g1, g2, ..., gN}, which we denote by {g(i)1 , g (i) 2 ...",2 Grouped Weight Sharing,[0],[0]
"g (i) K }, where K is the number of groups that contain word i. To initialize Es, for each dimension j of each word embedding ei, we use a hash function hi to map
1The idea of sharing weights to reflect known similarity is general and could be applied with other neural architectures.
",2 Grouped Weight Sharing,[0],[0]
(hash) the index j to one of the K group IDs:,2 Grouped Weight Sharing,[0],[0]
"hi : N → {g(i)1 , g (i) 2 ...",2 Grouped Weight Sharing,[0],[0]
g (i) K }.,2 Grouped Weight Sharing,[0],[0]
"Following (Weinberger et al., 2009; Shi et al., 2009), we use a second hash function b to remove bias induced by hashing.",2 Grouped Weight Sharing,[0],[0]
"This is a signing function, i.e., it maps (i, j) tuples to {+1,−1} 2.",2 Grouped Weight Sharing,[0],[0]
"We then set ei,j to the product of ghi(j),j and b(i, j).",2 Grouped Weight Sharing,[0],[0]
h and b are both approximately uniform hash functions.,2 Grouped Weight Sharing,[0],[0]
"Algorithm 1 provides the full initialization procedure.
",2 Grouped Weight Sharing,[0],[0]
"Algorithm 1 Initialization of Es
1: for i in {1, . . .",2 Grouped Weight Sharing,[0],[0]
", V } do 2: {g(i)1 , g (i) 2 , . . .",2 Grouped Weight Sharing,[0],[0]
", g (i) K } := G(i).",2 Grouped Weight Sharing,[0],[0]
"3: for j ∈ {1, . . .",2 Grouped Weight Sharing,[0],[0]
", d} do 4: ei,j := ghi(j),j · b(i, j) 5: end for 6: end for
For illustration, consider Figure 1.",2 Grouped Weight Sharing,[0],[0]
"Here g1 contains three words: good, nice and amazing, while g2 has two words: good and interesting.",2 Grouped Weight Sharing,[0],[0]
"The group embeddings gg1 , gg2 are initialized as averages over the pre-trained embeddings of the words they comprise.",2 Grouped Weight Sharing,[0],[0]
"Here, embedding parameters e1,1 and e2,1 are both mapped to gg1,1, and thus share this value.",2 Grouped Weight Sharing,[0],[0]
"Similarly, e1,3 and e2,3 will share value at gg1,3.",2 Grouped Weight Sharing,[0],[0]
"We have elided the second hash function b from this figure for simplicity.
",2 Grouped Weight Sharing,[0],[0]
"2Empirically, we found that using this signing function does not affect performance.
",2 Grouped Weight Sharing,[0],[0]
"During training, we update Ep as usual using back-propagation (Rumelhart et al., 1986).",2 Grouped Weight Sharing,[0],[0]
We update Es and group embeddings g in a manner similar to Chen et al. (2015).,2 Grouped Weight Sharing,[0],[0]
"In the forward propagation before each training step (mini-batch), we derive the value of ei,j from g:
ei,j := ghi(j),j ∗",2 Grouped Weight Sharing,[0],[0]
"b(i, j) (1)
We use this newly updated ei,j to perform forward propagation in the CNN.
",2 Grouped Weight Sharing,[0],[0]
"During backward propagation, we first compute the gradient of Es, and then we use this to derive the gradient w.r.t gs.",2 Grouped Weight Sharing,[0],[0]
"To do this, for each dimension j in ggk , we aggregate the gradients w.r.t E s whose elements are mapped to this dimension:
∇ggk,j := ∑
(i,j)
∇Esi,j · δhi(j)=gk · b(i, j) (2)
where δhi(j)=gk = 1 when h i(j) = gk, and 0 otherwise.",2 Grouped Weight Sharing,[0],[0]
Each training step involves executing Equations 1 and 2.,2 Grouped Weight Sharing,[0],[0]
"Once the shared gradient is calculated, gradient descent proceeds as usual.",2 Grouped Weight Sharing,[0],[0]
"We update all parameters aside from the shared weights in the standard way.
",2 Grouped Weight Sharing,[0],[0]
The number of parameters in our approach scales linearly with the number of channels.,2 Grouped Weight Sharing,[0],[0]
"But the gradients can actually be back-propagated in a distributed way for each channel, since the convolutional and embedding layers are independent across these.",2 Grouped Weight Sharing,[0],[0]
Thus training time scales approximately linearly with the number of parameters in one channel (if the gradient is back-propagated in a distributed way).,2 Grouped Weight Sharing,[0],[0]
"We use three sentiment datasets: a movie review (MR) dataset (Pang and Lee, 2005)3; a customer review (CR) dataset (Hu and Liu, 2004)4; and an opinion dataset (MPQA) (Wiebe et al., 2005)5.
",3.1 Datasets,[0],[0]
"We also use four biomedical datasets, which concern systematic reviews.",3.1 Datasets,[0],[0]
The task here is to classify published articles describing clinical trials as relevant or not to a well-specified clinical question.,3.1 Datasets,[0],[0]
"Articles deemed relevant are included in
3www.cs.cornell.edu/people/pabo/ movie-review-data/
4www.cs.uic.edu/˜liub/FBS/ sentiment-analysis.html
5mpqa.cs.pitt.edu/corpora/mpqa_corpus/
the corresponding review, which is a synthesis of all pertinent evidence (Wallace et al., 2010).",3.1 Datasets,[0],[0]
"We use data from reviews that concerned: clopidogrel (CL) for cardiovascular conditions (Dahabreh et al., 2013); biomarkers for assessing iron deficiency in anemia (AN) experienced by patients with kidney disease (Chung et al., 2012); statins (ST) (Cohen et al., 2006); and proton beam (PB) therapy (Terasawa et al., 2009).",3.1 Datasets,[0],[0]
"We use SentiWordNet (Baccianella et al., 2010)6 for the sentiment tasks.",3.2 Implementation Details and Baselines,[0],[0]
"SentiWordNet assigns to each synset of wordnet three sentiment scores: positivity, negativity and objectivity, constrained to sum to 1.",3.2 Implementation Details and Baselines,[0],[0]
"We keep only the synsets with positivity or negativity scores greater than 0, i.e., we remove synsets deemed objective.",3.2 Implementation Details and Baselines,[0],[0]
The synsets in SentiWordNet constitute our groups.,3.2 Implementation Details and Baselines,[0],[0]
We also use the Brown clustering algorithm7 on the three sentiment datasets.,3.2 Implementation Details and Baselines,[0],[0]
"We generate 1000 clusters and treat each as a group.
",3.2 Implementation Details and Baselines,[0],[0]
"For the biomedical datasets, we use the Medical Subject Headings (MeSH) terms8 attached to each abstract to classify them.",3.2 Implementation Details and Baselines,[0],[0]
Each MeSH term has a tree number indicating the path from the root in the UMLS.,3.2 Implementation Details and Baselines,[0],[0]
"For example, ‘Alagille Syndrome’ has tree number ‘C06.552.150.125’; periods denote tree splits, numbers are nodes.",3.2 Implementation Details and Baselines,[0],[0]
"We induce groups comprising MeSH terms that share the same first three parent nodes, e.g., all terms with ‘C06.552.150’ as their tree number prefix constitute one group.
",3.2 Implementation Details and Baselines,[0],[0]
We compare our approach to several baselines.,3.2 Implementation Details and Baselines,[0],[0]
"All use pre-trained embeddings to initialize Ep, but we explore several approaches to exploiting Es: (1) randomly initialize Es; (2) initialize Es to reflect the group embedding g, but do not share weights during the training process, i.e., do not constrain their weights to be equal when we perform back-propagation; (3) use linguistic resources to retro-fit (Faruqui et al., 2014) the pretrained embeddings, and use these to initialize Es.",3.2 Implementation Details and Baselines,[0],[0]
"For retro-fitting, we first construct a graph derived from SentiWordNet.",3.2 Implementation Details and Baselines,[0],[0]
Then we run beliefpropagation on the graph to encourage linked words to have similar vectors.,3.2 Implementation Details and Baselines,[0],[0]
"This is a preprocessing step only; we do not impose weight sharing constraints during training.
",3.2 Implementation Details and Baselines,[0],[0]
"6sentiwordnet.isti.cnr.it 7github.com/percyliang/brown-cluster 8www.nlm.nih.gov/bsd/disted/
meshtutorial/
For the sentiment datasets we use three filter heights (3,4,5) for each of the two CNN channels.",3.2 Implementation Details and Baselines,[0],[0]
"For the biomedical datasets, we use only one filter height (1), because the inputs are unstructured MeSH terms.9",3.2 Implementation Details and Baselines,[0],[0]
In both cases we use 100 filters of each unique height.,3.2 Implementation Details and Baselines,[0],[0]
"For the sentiment datasets, we use Google word2vec (Mikolov et al., 2013)10 to initialize Ep.",3.2 Implementation Details and Baselines,[0],[0]
"For the biomedical datasets, we use word2vec trained on biomedical texts (Moen and Ananiadou, 2013)11 to initialize Ep.",3.2 Implementation Details and Baselines,[0],[0]
"For parameter estimation, we use Adadelta (Zeiler, 2012).",3.2 Implementation Details and Baselines,[0],[0]
"Because the biomedical datasets are imbalanced, we use downsampling (Zhang et al., 2016a; Zhang and Wallace, 2015) to effectively train on balanced subsets of the data.
",3.2 Implementation Details and Baselines,[0],[0]
"We developed our approach using the MR sentiment dataset, tuning our approach to constructing groups from the available resources – experiments on other sentiment datasets were run after we finalized the model and hyperparameters.",3.2 Implementation Details and Baselines,[0],[0]
"Similarly, we used the anemia (AN) review as a development set for the biomedical tasks, especially w.r.t.",3.2 Implementation Details and Baselines,[0],[0]
constructing groups from MeSH terms using UMLS.,3.2 Implementation Details and Baselines,[0],[0]
"We replicate each experiment five times (each is a 10-fold cross validation), and report the mean (min, max) across these replications.",4 Results,[0],[0]
"Results on the sentiment and biomedical corpora in are presented in Tables 2 and 3, respectively.12 These exploit different external resources to induce the word groupings that in turn inform weight sharing.",4 Results,[0],[0]
"We report AUC for the biomedical datasets because these are highly imbalanced (see Table 1).
",4 Results,[0],[0]
"Our method improves performance compared to all relevant baselines (including an approach that
9For this work we are ignoring title and abstract texts.",4 Results,[0],[0]
"10code.google.com/archive/p/word2vec/ 11bio.nlplab.org/ 12Sentiment task results are not directly comparable to
prior work due to different preprocessing steps.
also exploits external knowledge via retrofitting) in six of seven cases.",4 Results,[0],[0]
"Informing weight initialization using external resources improves performance independently, but additional gains are realized by also enforcing sharing during training.
",4 Results,[0],[0]
"We note that our aim here is not necessarily to achieve state-of-art results on any given dataset, but rather to evaluate the proposed method for incorporating external linguistic resources into neural models via weight sharing.",4 Results,[0],[0]
We have therefore compared to baselines that enable us to assess this.,4 Results,[0],[0]
Neural Models for NLP.,5 Related Work,[0],[0]
"Recently there has been enormous interest in neural models for NLP generally (Collobert et al., 2011; Goldberg, 2016).",5 Related Work,[0],[0]
"Most relevant to this work, simple CNN based models (which we have built on here) have proven extremely effective for text categorization (Kim, 2014; Zhang and Wallace, 2015).",5 Related Work,[0],[0]
Exploiting Linguistic Resources.,5 Related Work,[0],[0]
A potential drawback to learning from scratch in end-to-end neural models is a failure to capitalize on existing knowledge sources.,5 Related Work,[0],[0]
"There have been efforts to exploit such resources specifically to induce better word vectors (Yu and Dredze, 2014; Faruqui et al., 2014; Yu et al., 2016; Xu et al., 2014).",5 Related Work,[0],[0]
"But these models do not attempt to exploit external resources jointly during training for a particular downstream task (which uses word embeddings as inputs), as we do here.
",5 Related Work,[0],[0]
Past work on sparse linear models has shown the potential of exploiting linguistic knowledge in statistical NLP models.,5 Related Work,[0],[0]
"For example, Yogatama and Smith (2014) used external resources to inform structured, grouped regularization of loglinear text classification models, yielding improvements over standard regularization approaches.",5 Related Work,[0],[0]
"Elsewhere, Doshi-Velez et al. (2015) proposed a variant of LDA that exploits a priori known tree-
structured relations between tokens (e.g., derived from the UMLS) in topic modeling.",5 Related Work,[0],[0]
Weight-sharing in NNs.,5 Related Work,[0],[0]
Recent work has considered stochastically sharing weights in neural models.,5 Related Work,[0],[0]
"Notably, Chen et al. (2015).",5 Related Work,[0],[0]
proposed randomly sharing weights in neural networks.,5 Related Work,[0],[0]
"Elsewhere, Han et al. (2015) proposed quantized weight sharing as an intermediate step in their deep compression model.",5 Related Work,[0],[0]
"In these works, the primary motivation was model compression, whereas here we view the hashing trick as a mechanism to encode domain knowledge.",5 Related Work,[0],[0]
We have proposed a novel method for incorporating prior semantic knowledge into neural models via stochastic weight sharing.,6 Conclusion,[0],[0]
We have showed it generally improves text classification performance vs. model variants which do not exploit external resources and vs. an approach based on retrofitting prior to training.,6 Conclusion,[0],[0]
"In future work, we will investigate generalizing our approach beyond classification, and to inform weight sharing using other varieties and sources of linguistic knowledge.
Acknowledgements.",6 Conclusion,[0],[0]
"This work was made possible by NPRP grant NPRP 7-1313-1-245 from the Qatar National Research
Fund (a member of Qatar Foundation).",6 Conclusion,[0],[0]
"The statements made
herein are solely the responsibility of the authors.",6 Conclusion,[0],[0]
A fundamental advantage of neural models for NLP is their ability to learn representations from scratch.,abstractText,[0],[0]
"However, in practice this often means ignoring existing external linguistic resources, e.g., WordNet or domain specific ontologies such as the Unified Medical Language System (UMLS).",abstractText,[0],[0]
"We propose a general, novel method for exploiting such resources via weight sharing.",abstractText,[0],[0]
Prior work on weight sharing in neural networks has considered it largely as a means of model compression.,abstractText,[0],[0]
"In contrast, we treat weight sharing as a flexible mechanism for incorporating prior knowledge into neural models.",abstractText,[0],[0]
We show that this approach consistently yields improved performance on classification tasks compared to baseline strategies that do not exploit weight sharing.,abstractText,[0],[0]
Exploiting Domain Knowledge via Grouped Weight Sharing with Application to Text Categorization,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 918–924 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
918",text,[0],[0]
"The task of semantic parsing is to translate text to its formal meaning representations, such as logical forms or structured queries.",1 Introduction,[0],[0]
"Recent neural semantic parsers approach this problem by learning soft alignments between natural language and logical forms from (text, logic) pairs (Jia and Liang, 2016; Dong and Lapata, 2016; Krishnamurthy et al., 2017).",1 Introduction,[0],[0]
All these parsers follow the conventional encoder-decoder architecture that first encodes the text into a distributional representation and then decodes it to a logical form.,1 Introduction,[0],[0]
"These parsers may differ in the choice of the decoders, such as sequence or tree decoders, but they utilize the same encoder which is essentially a sequential Long Short-Term Memory network (SeqLSTM).",1 Introduction,[0],[0]
"This encoder only extracts word order features while neglecting useful syntactic information, such as dependency parse and constituency parse.
",1 Introduction,[0],[0]
"However, the syntactic features capture important structural information of the natural lan-
∗ Work done when the author was at IBM Research.
guage input, which complements the simple word sequence.",1 Introduction,[0],[0]
"For example, a dependency graph presents grammatical relations that hold among the words; and a constituent tree provides a phrase structure representation.",1 Introduction,[0],[0]
"Intuitively, by incorporating such additional information, the encoder could produce a more meaningful and robust sentence representation.",1 Introduction,[0],[0]
"The combination of these features (i.e., sequence + trees) forms a general graph structure (see Figure 1).",1 Introduction,[0],[0]
This inspires us to apply a graph encoder to produce a representation of a graph-structured input.,1 Introduction,[0],[0]
"The graph encoder also has the advantages that it could simultaneously encode all types of syntactic contexts, and incorporate multiple types of syntactic structures in a unified way.
",1 Introduction,[0],[0]
"In this paper, we first introduce a structure, namely syntactic graph, to represent three types of syntactic information, i.e., word order, dependency and constituency features (see §2).",1 Introduction,[0],[0]
"We then employ a novel graph-to-sequence (Graph2Seq) model (Xu et al., 2018), which consists of a graph encoder and a sequence decoder, to learn the representation of the syntactic graph (see §3).",1 Introduction,[0],[0]
"Specifically, the graph encoder learns the representation of each node by aggregating information from its K-hop neighbors.",1 Introduction,[0],[0]
"Given the learned node embeddings, the graph encoder uses a pooling-based method to generate the graph embedding.",1 Introduction,[0],[0]
"On the decoder side, a Recurrent Neural Network (RNN) decoder takes the graph embedding as its initial hidden state to generate the logical form while employing an attention mechanism over the node embeddings.",1 Introduction,[0],[0]
"Experimental results show that our model achieves the competitive performance on Jobs640, ATIS, and Geo880 datasets.
",1 Introduction,[0],[0]
"Different from existing works, we also investigate the robustness of our model by evaluating the model on two types of adversarial examples (Belinkov and Bisk, 2017; Cheng et al., 2018).
",1 Introduction,[0],[0]
"Experimental results show that the model coupling all syntactic features has the best robustness, achieving the best performance.",1 Introduction,[0],[0]
Our code and data is available at https://github.com/IBM/ Text-to-LogicForm.,1 Introduction,[0],[0]
"We represent three types of syntactic features, i.e., word order, dependency parse and constituency parse, as the syntactic graph (see Figure 1).",2 Syntactic Graph,[0],[0]
•Word Order Features.,2 Syntactic Graph,[0],[0]
Previous neural semantic parsers mainly use these features by building a SeqLSTM that works on the word sequence.,2 Syntactic Graph,[0],[0]
Our syntactic graph also incorporates this information by generating a node for each word and connecting them in the chain form.,2 Syntactic Graph,[0],[0]
"In order to capture the forward and backward contextual information, we link these nodes in two directions, that is, from left to right and from right to left.",2 Syntactic Graph,[0],[0]
• Dependency Features.,2 Syntactic Graph,[0],[0]
A dependency parse describes the grammatical relations that hold among words.,2 Syntactic Graph,[0],[0]
"Reddy et al. (2016, 2017) have demonstrated that the dependency parse tree could be directly transformed to a logical form, which indicates that the dependency information (i.e., tree structure and dependency labels) is critical to the semantic parsing task.",2 Syntactic Graph,[0],[0]
We incorporate this information into the syntactic graph by adding directed edges between the word nodes and assign them with dependency labels.,2 Syntactic Graph,[0],[0]
• Constituency Features.,2 Syntactic Graph,[0],[0]
"Similar to the dependency parse, the constituency parse represents the phrase structure, which is also important to the semantic parsing task.",2 Syntactic Graph,[0],[0]
"Take Figure 1 as an example: given the constituent tree that explicitly annotates “not related with AI” (node #1) is a proposition phrase, the model could learn a meaningful embedding for this phrase by encoding this structure into the model.",2 Syntactic Graph,[0],[0]
"Motivated by this observation, we
add the non-terminal nodes of the constituent tree and the edges describing their parent-child relationships into the syntactic graph.",2 Syntactic Graph,[0],[0]
"After building the syntactic graph for the input text, we employ a novel graph-to-sequence model (Xu et al., 2018), which includes a graph encoder and a sequence decoder with attention mechanism, to map the syntactic graph to the logical form.",3 Graph-to-sequence Model for Semantic Parsing,[0],[0]
"Conceptually, the graph encoder generates node embeddings for each node by accumulating information from its K-hop neighbors, and then produces a graph embedding for the entire graph by abstracting all these node embeddings.",3 Graph-to-sequence Model for Semantic Parsing,[0],[0]
"Next, the sequence decoder takes the graph embedding as the initial hidden state, and calculates the attention over all node embeddings on the encoder side to generate logical forms.",3 Graph-to-sequence Model for Semantic Parsing,[0],[0]
"Note that this graph encoder does not explicitly encode the edge label information, therefore, for each labeled edge, we add a node whose text attribute is the edge’s label.
",3 Graph-to-sequence Model for Semantic Parsing,[0],[0]
Node Embedding.,3 Graph-to-sequence Model for Semantic Parsing,[0],[0]
"Given the syntactic graph G = (V, E), we take the embedding generation process for node v ∈ V as an example to explain the node embedding generation algorithm1: (1) We first transform node v’s text attribute to a feature vector, av, by looking up the embedding matrix We; (2) The neighbors of v are categorized into forward neighbors N`(v) and backward neighbors Na(v) according to the edge direction.",3 Graph-to-sequence Model for Semantic Parsing,[0],[0]
"In particular, N`(v) returns the nodes that v directs to and Na(v) returns the nodes that direct to v; (3) We aggregate the forward representations of v’s forward neighbors {hk−1u` , ∀u ∈ N`(v)} into
1Interested readers may refer to (Xu et al., 2018) for more implementation details.
",3 Graph-to-sequence Model for Semantic Parsing,[0],[0]
"a single vector, hkN`(v), where k∈{1, ...,K} is the iteration index.",3 Graph-to-sequence Model for Semantic Parsing,[0],[0]
"Specifically, this aggregator feeds each neighbor’s vector to a fully-connected neural network and applies an element-wise max-pooling operation to capture different aspects of the neighbor set.",3 Graph-to-sequence Model for Semantic Parsing,[0],[0]
"Notice that, at iteration k, this aggregator only uses the representations generated at k − 1.",3 Graph-to-sequence Model for Semantic Parsing,[0],[0]
The initial forward representation of each node is its feature vector calculated in step (1); (4) We concatenate v’s current forward representation hk−1v` with the newly generated neighborhood vector hkN`(v).,3 Graph-to-sequence Model for Semantic Parsing,[0],[0]
"The resulted vector is fed into a fully connected layer with nonlinear activation function σ, which updates the forward representation of v, hkv`, to be used at the next iteration; (5) We update the backward representation of v, hkva using the similar procedure as introduced in step (3) and (4) except that operating on the backward representations; (6) We repeat steps (3)∼(5) K times, and the concatenation of the final forward and backward representations is used as the final representation of v. Since the neighbor information from different hops may have different impacts on the node embedding, we learn a distinct aggregator at each iteration.",3 Graph-to-sequence Model for Semantic Parsing,[0],[0]
Graph Embedding.,3 Graph-to-sequence Model for Semantic Parsing,[0],[0]
"We feed the obtained node embeddings into a fully-connected neural network, and apply the element-wise max-pooling operation on all node embeddings.",3 Graph-to-sequence Model for Semantic Parsing,[0],[0]
We did not find substantial performance improvement using mean-pooling.,3 Graph-to-sequence Model for Semantic Parsing,[0],[0]
Sequence Decoding.,3 Graph-to-sequence Model for Semantic Parsing,[0],[0]
"The decoder is an RNN which predicts the next token yi given all the previous words y<i = y1, ..., yi−1, the RNN hidden state si for time-step i and the context vector ci that captures the attention of the encoder side.",3 Graph-to-sequence Model for Semantic Parsing,[0],[0]
"In particular, the context vector ci depends on a set of node representations (h1,...,hV ) to which the encoder maps the input graph.",3 Graph-to-sequence Model for Semantic Parsing,[0],[0]
The context vector ci is dynamically computed using an attention mechanism over the node representations.,3 Graph-to-sequence Model for Semantic Parsing,[0],[0]
The whole model is jointly trained to maximize the conditional log-probability of the correct description given a source graph.,3 Graph-to-sequence Model for Semantic Parsing,[0],[0]
"In the inference phase, we use the beam search algorithm to generate a description with beam size = 3.",3 Graph-to-sequence Model for Semantic Parsing,[0],[0]
"We evaluate our model on three datasets: Jobs640, a set of 640 queries to a database of job listings;
Geo880, a set of 880 queries to a database of U.S. geography; and ATIS, a set of 5,410 queries to a flight booking system.",4 Experiments,[0],[0]
"We use the standard train/development/test split as previous works, and the logical form accuracy as our evaluation metric.
",4 Experiments,[0],[0]
"The model is trained using the Adam optimizer (Kingma and Ba, 2014), with mini-batch size 30.",4 Experiments,[0],[0]
"Our hyper-parameters are cross-validated on the training set for Jobs640 and Geo880, and tuned on the development set for ATIS.",4 Experiments,[0],[0]
The learning rate is set to 0.001.,4 Experiments,[0],[0]
"The decoder has 1 layer, and its hidden state size is 300.",4 Experiments,[0],[0]
"The dropout strategy (Srivastava et al., 2014) with the ratio of 0.5 is applied at the decoder layer to avoid overfitting.",4 Experiments,[0],[0]
We is initialized using GloVe word vectors from Pennington et al. (2014) and the dimension of word embedding is 300.,4 Experiments,[0],[0]
"For the graph encoder, the hop size K is set to 10, the non-linearity function σ is implemented as ReLU (Glorot et al., 2011), the parameters of the aggregators are randomly initialized.",4 Experiments,[0],[0]
"We use the Stanford CoreNLP tool (Manning et al., 2014) to generate the dependency and constituent trees.
Results and Discussion.",4 Experiments,[0],[0]
Table 1 summarizes the results of our model and existing semantic parsers on three datasets.,4 Experiments,[0],[0]
"Our model achieves competitive performance on Jobs640, ATIS and Geo880.",4 Experiments,[0],[0]
"Our work is the first to use both multiple trees and the word sequence for semantic parsing, and it outperforms the Seq2Seq model reported in Dong and Lapata (2016), which only uses limited syntactic information.",4 Experiments,[0],[0]
Comparison with Baseline.,4 Experiments,[0],[0]
"To better demonstrate that our work is an effective way to utilize both multiple trees and the word sequence for semantic parsing, we compare with an addi-
tional straightforward baseline method (referred as BASELINE in Table 1).",4 Experiments,[0],[0]
"To deal with the graph input, the BASELINE decomposes the graph embedding to two steps and applies different types of encoders sequentially: (1) a SeqLSTM to extract word order features, which results in word embeddings, Wseq; (",4 Experiments,[0],[0]
"2) two TreeLSTMs (Tai et al., 2015) to extract the dependency tree and constituency features while taking Wseq as initial word embeddings.",4 Experiments,[0],[0]
"The resulted word embeddings and nonterminal node embeddings (from TreeLSTMs) are then fed into a sequence decoder.
",4 Experiments,[0],[0]
We can see that our model significantly outperforms the BASELINE.,4 Experiments,[0],[0]
One possible reason is that our graph encoder jointly extracts these features in a unified model by propagating the dependency and constituency information to all nodes in the syntactic graph.,4 Experiments,[0],[0]
"However, BASELINE separately models these features using two distinct TreeLSTMs.",4 Experiments,[0],[0]
"As a result, the non-terminal tree nodes only retain only one type of syntactic information propagated from their descendants in the tree.
",4 Experiments,[0],[0]
Ablation Study.,4 Experiments,[0],[0]
"In Table 1, we also report the results of three ablation variants of our model, i.e., without word order features/dependency features/constituency features.",4 Experiments,[0],[0]
"We find that Graph2Seq is superior to Seq2Seq (Dong and Lapata, 2016) which is expected since Graph2Seq exploits more syntactic information.",4 Experiments,[0],[0]
"Among these features, the word order feature have more impact on the performance than other two syntactic features.",4 Experiments,[0],[0]
"By incorporating either the dependency or the constituency features, the model could gain further performance improvement, which underlines the importance of utilizing more aspects of syntactic information.",4 Experiments,[0],[0]
"Finally, removing both syntactic features (w/ ONLY word order) performs slightly worse compared to the Seq2Seq baseline.",4 Experiments,[0],[0]
"This shows that using K=10 hops is good enough for memorizing the sentences in our benchmarks, although still weaker compared to a bidirectional LSTM encoder.
",4 Experiments,[0],[0]
A natural question here is on which type of queries our model could benefit from incorporating these parse features.,4 Experiments,[0],[0]
"By analyzing the queries and our predicted logical forms, we find that the parse features mainly improve the prediction accuracy for the queries with complex logical forms.",4 Experiments,[0],[0]
Table 2 gives some running examples of complicated queries in three datasets.,4 Experiments,[0],[0]
"We find that the model that exploits three syntactic information
could correctly predict these logical forms while the model that only uses word order features may fail.
",4 Experiments,[0],[0]
Robustness Study.,4 Experiments,[0],[0]
"Different from previous works, we evaluate the robustness of our model by creating adversarial examples with the hope to investigate the impact of introducing more syntactic information on robustness.",4 Experiments,[0],[0]
"Specifically, we create two types of adversarial examples and conduct experiments on the ATIS dataset.",4 Experiments,[0],[0]
"Following Belinkov and Bisk (2017), we first experiment with the synthetic noise, SWAP, which swaps two letters (e.g. noise→nosie).",4 Experiments,[0],[0]
It is common to see such noisy information when typing quickly.,4 Experiments,[0],[0]
"Given a text, we randomly perform swap on m ∈ {1, 2, 3, 4, 5} words that not correspond to the operators or arguments in logical forms, ensuring the meaning of the text is not changed.",4 Experiments,[0],[0]
"We train Graph2Seq on the training data and first evaluate it on the original development data, Devori.",4 Experiments,[0],[0]
"Then we use the same model but evaluate it on a variant of Devori, whose queries contain m swapped words.
",4 Experiments,[0],[0]
"Figure 2 summarizes the results of our model on the first type of adversarial examples, i.e., the ATIS development set with the SWAP noise.",4 Experiments,[0],[0]
"From Figure 2, we can see that (1) the performance of our model on all combinations of features degrade significantly when increasing the number of swapped words; (2) the model that uses three syntactic features (our default model) always achieves the best performance, and the performance gap
compared to others increases when rising the number of swapped words; (3) word order features are the most sensitive to the word sequence while the dependency and constituency features seem more robust to such noisy information; (4) thanks to the robustness of the dependency and constituency features, the default model performs significantly better than the one that only uses word order features on the noisy sentences.",4 Experiments,[0],[0]
"These findings demonstrate that incorporating more aspects of syntactic information could enhance the robustness of the model.
",4 Experiments,[0],[0]
We also experiment with the paraphrase of the input text as the second type of adversarial examples.,4 Experiments,[0],[0]
"More specifically, we collect the paraphrase of a text by first translating it to the other language such as Chinese and then translating it back to English, using the Google Translate service.",4 Experiments,[0],[0]
We use this method to collect a new variant of Devori whose queries are the paraphrases of the original ones.,4 Experiments,[0],[0]
"By manually reading these queries, we find 94% queries convey the same meaning as original ones.",4 Experiments,[0],[0]
"Similar to the first experiment, we still train the model on Devori and evaluate it on the newly created dataset.
",4 Experiments,[0],[0]
"Table 3 shows the results of our model on the second type of adversarial examples, i.e., the paraphrased ATIS development set.",4 Experiments,[0],[0]
"We also report the
result of our model on the original ATIS development set.",4 Experiments,[0],[0]
"We can see that (1) no matter which feature our model uses, the performance degrades at least 2.5% on the paraphrased dataset; (2) the model that only uses word order features achieves the worst robustness to the paraphrased queries while the dependency feature seems more robust than other two features.",4 Experiments,[0],[0]
(3) simultaneously utilizing three syntactic features could greatly enhance the robustness of our model.,4 Experiments,[0],[0]
These results again demonstrate that our model could benefit from incorporating more aspects of syntactic information.,4 Experiments,[0],[0]
Existing works of generating text representation has evolved into two main streams.,5 Related Work,[0],[0]
"The first one is based on the word order, that is, either generating general purpose and domain independent embeddings of word sequences (Wu et al., 2018a; Arora et al., 2017), or building Bi-directional LSTMs over the text (Zhang et al., 2018).",5 Related Work,[0],[0]
"These methods neglect other syntactic information, which, however, has been proved to be useful in shallow semantic parsing, e.g., semantic role labeling (Punyakanok et al., 2008).",5 Related Work,[0],[0]
"To address this, recent works attempt to incorporate these syntactic information into the text representation.",5 Related Work,[0],[0]
"For example, Xu et al. (2016) builds separated neural networks for different types of syntactic annotation.",5 Related Work,[0],[0]
Gormley et al. (2015); Wu et al. (2018b) decompose a graph to simpler sub-graphs and embed these subgraphs independently.,5 Related Work,[0],[0]
"Our approach, compared to the above methods, provided a unified solution to arbitrary combinations of syntactic graphs.",5 Related Work,[0],[0]
"In parallel to syntactic features, other works leverage additional information such as dialogue and paraphrasing for semantic parsing (Su and Yan, 2017; Gur et al., 2018).",5 Related Work,[0],[0]
Existing neural semantic parsers mainly leverage word order features while neglecting other valuable syntactic information.,6 Conclusions,[0],[0]
"To address this, we propose to build a syntactic graph which represents three types of syntactic information, and further apply a novel graph-to-sequence model to map the syntactic graph to a logical form.",6 Conclusions,[0],[0]
"Experimental results show that the robustness of our model is improved due to the incorporating more aspects of syntactic information, and our model outperforms previous semantic parsing systems.",6 Conclusions,[0],[0]
"Existing neural semantic parsers mainly utilize a sequence encoder, i.e., a sequential LSTM, to extract word order features while neglecting other valuable syntactic information such as dependency or constituent trees.",abstractText,[0],[0]
"In this paper, we first propose to use the syntactic graph to represent three types of syntactic information, i.e., word order, dependency and constituency features; then employ a graph-tosequence model to encode the syntactic graph and decode a logical form.",abstractText,[0],[0]
"Experimental results on benchmark datasets show that our model is comparable to the state-of-the-art on Jobs640, ATIS, and Geo880.",abstractText,[0],[0]
Experimental results on adversarial examples demonstrate the robustness of the model is also improved by encoding more syntactic information.,abstractText,[0],[0]
Exploiting Rich Syntactic Information for Semantic Parsing with Graph-to-Sequence Model,title,[0],[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2193–2203, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics",text,[0],[0]
The problem of discovering semantic relationships between two sentences has given birth to several NLP tasks over the years.,1 Introduction,[0],[0]
"Textual entailment (Dagan et al., 2013, inter alia) asks about the truth of a hypothesis sentence given another sentence (or more generally a paragraph).",1 Introduction,[0],[0]
"Paraphrase identification (Dolan et al., 2004, inter alia) asks whether two sentences have the same meaning.",1 Introduction,[0],[0]
"Foregoing the binary entailment and paraphrase decisions, the semantic textual similarity (STS) task (Agirre et al., 2012) asks for a numeric measure of semantic equivalence between two sentences.",1 Introduction,[0],[0]
"All three tasks have attracted much interest in the form of shared tasks.
",1 Introduction,[0],[0]
"While various approaches have been proposed to predict these sentence relationships, a commonly employed strategy (Das and Smith, 2009; Chang et
al., 2010a) is to postulate an alignment between constituents of the sentences and use this alignment to make the final prediction (a binary decision or a numeric similarity score).",1 Introduction,[0],[0]
"The implicit assumption in such approaches is that better constituent alignments can lead to better identification of semantic relationships between sentences.
",1 Introduction,[0],[0]
Constituent alignments serve two purposes.,1 Introduction,[0],[0]
"First, they act as an intermediate representation for predicting the final output.",1 Introduction,[0],[0]
"Second, the alignments help interpret (and debug) decisions made by the overall system.",1 Introduction,[0],[0]
"For example, the alignment between the sentences in Figure 1 can not only be useful to determine the equivalence of the two sentences, but also help reason about the predictions.
",1 Introduction,[0],[0]
"The importance of this intermediate representation led to the creation of the interpretable semantic textual similarity task (Agirre et al., 2015a) that focuses on predicting chunk-level alignments and similarities.",1 Introduction,[0],[0]
"However, while extensive resources exist for sentence-level relationships, human annotated chunk-aligned data is comparatively smaller.
",1 Introduction,[0],[0]
"In this paper, we address the following question: can we use sentence-level resources to better pre-
2193
dict constituent alignments and similarities?",1 Introduction,[0],[0]
"To answer this question, we focus on the semantic textual similarity (STS) task and its interpretable variant.",1 Introduction,[0],[0]
We propose a joint model that aligns constituents and integrates the information across the aligned edges to predict both constituent and sentence level similarity.,1 Introduction,[0],[0]
"The key advantage of modeling these two problems jointly is that, during training, the sentence-level information can provide feedback to the constituent-level predictions.
",1 Introduction,[0],[0]
We evaluate our model on the SemEval-2016 task of interpretable STS.,1 Introduction,[0],[0]
"We show that even without the sentence information, our joint model that uses constituent alignments and similarities forms a strong baseline.",1 Introduction,[0],[0]
"Further, our easily extensible joint model can incorporate sentence-level similarity judgments to produce alignments and chunk similarities that are comparable to the best results in the shared task.
",1 Introduction,[0],[0]
"In summary, the contributions of this paper are:
1.",1 Introduction,[0],[0]
We present the first joint model for predicting constituent alignments and similarities.,1 Introduction,[0],[0]
"Our model can naturally take advantage of the much larger sentence-level annotations.
2.",1 Introduction,[0],[0]
We evaluate our model on the SemEval-2016 task of interpretable semantic similarity and show state-of-the-art results.,1 Introduction,[0],[0]
"In this section, we will introduce the notation used in the paper using the sentences in Figure 1 as a running example.",2 Problem Definition,[0],[0]
"The input to the problem is a pair of sentences, denoted by",2 Problem Definition,[0],[0]
"x. We will assume that the sentences are chunked (Tjong Kim Sang and Buchholz, 2000) into constituents.",2 Problem Definition,[0],[0]
We denote the chunks using subscripts.,2 Problem Definition,[0],[0]
"Thus, the input x consists of two sequences of chunks s = (s1, s2, · · · ) and t = (t1, t2, · · · ) respectively.",2 Problem Definition,[0],[0]
"In our running example, we have s = (Gunmen, abduct, seven foreign workers) and t =",2 Problem Definition,[0],[0]
"(Seven foreign workers, kidnapped).
",2 Problem Definition,[0],[0]
"The output consists of three components:
1.",2 Problem Definition,[0],[0]
Alignment:,2 Problem Definition,[0],[0]
"The alignment between a pair of chunks is a labeled, undirected edge that explains the relation that exists between them.",2 Problem Definition,[0],[0]
"The labels can be one of EQUI (semantically
equivalent), OPPO (opposite meaning in context), SPE1, SPE2 (the chunk from s is more specific than the one from t and vice versa), SIMI (similar meaning, but none of the previous ones) or REL (related, but none of the above)1.",2 Problem Definition,[1.0000000317868856],"['The labels can be one of EQUI (semantically equivalent), OPPO (opposite meaning in context), SPE1, SPE2 (the chunk from s is more specific than the one from t and vice versa), SIMI (similar meaning, but none of the previous ones) or REL (related, but none of the above)1.']"
"In Figure 1, we see two EQUI edges.",2 Problem Definition,[0],[0]
"A chunk from either sentence can be unaligned, as in the case of the chunk Gunmen.
",2 Problem Definition,[0],[0]
We will use y to denote the alignment for an input x.,2 Problem Definition,[0],[0]
"The alignment y consists of a sequence of triples of the form (si, tj , l).",2 Problem Definition,[0],[0]
"Here, si and tj denote a pair of chunks that are aligned with a label l. For brevity, we will include unaligned chunks into this format using a special null chunk and label to indicate that a chunk is unaligned.",2 Problem Definition,[1.0],"['Here, si and tj denote a pair of chunks that are aligned with a label l. For brevity, we will include unaligned chunks into this format using a special null chunk and label to indicate that a chunk is unaligned.']"
"Thus, the alignment for our running example contain the triple (Gunmen, ∅, ∅).
2.",2 Problem Definition,[0],[0]
"Chunk similarity: Every aligned chunk is associated with a relatedness score between zero and five, denoting the range from unrelated to equivalent.",2 Problem Definition,[0],[0]
Note that even chunks labeled OPPO can be assigned a high score because the polarity is captured by the label rather than the score.,2 Problem Definition,[0],[0]
"We will denote the chunk similarities using z, comprising of numeric zi,j,l for elements of the corresponding alignment y. For an unaligned chunk, the corresponding similarity z is fixed to zero.
3.",2 Problem Definition,[0],[0]
"Sentence similarity: The pair of sentences is associated with a scalar score from zero to five, to be interpreted as above.",2 Problem Definition,[1.0],"['Sentence similarity: The pair of sentences is associated with a scalar score from zero to five, to be interpreted as above.']"
"We will use r to denote the sentence similarity for an input x.
Thus, the prediction problem is the following: Given a pair of chunked sentences x = (s, t), predict the alignment y, the alignment similarities z and the sentence similarity r. Note that this problem definition integrates the canonical semantic textual similarity task (only predicting r) and its interpretable variant (predicting both y and z) into a single task.
",2 Problem Definition,[0],[0]
"1We refer the reader to the guidelines of the task (Agirre et al., 2015a) for further details on these labels.",2 Problem Definition,[0],[0]
"Also, for simplicity, in this paper, we ignore the factuality and polarity tags from the interpretable task.",2 Problem Definition,[0],[0]
"This section describes our model for predicting alignments, alignment scores, and the sentence similarity scores for a given pair of sentences.",3 Predicting Alignments and Similarities,[1.0],"['This section describes our model for predicting alignments, alignment scores, and the sentence similarity scores for a given pair of sentences.']"
"We will assume that learning is complete and we have all the scoring functions we need and defer discussing the parameterization and learning to Section 4.
",3 Predicting Alignments and Similarities,[0],[0]
We frame the problem of inference as an instance of an integer linear program (ILP).,3 Predicting Alignments and Similarities,[0],[0]
We will first see the scoring functions and the ILP formulation in Section 3.1.,3 Predicting Alignments and Similarities,[0],[0]
"Then, in Section 3.2, we will see how we can directly read off the similarity scores at both chunk and sentence level from the alignment.",3 Predicting Alignments and Similarities,[1.0],"['Then, in Section 3.2, we will see how we can directly read off the similarity scores at both chunk and sentence level from the alignment.']"
"We have two kinds of 0-1 inference variables to represent labeled aligned chunks and unaligned chunks.
",3.1 Alignment via Integer Linear Programs,[0],[0]
"We will use the inference variables 1i,j,l to denote the decision that chunks si and tj are aligned with a label l. To allow chunks to be unaligned, the variables 1i,0 and 10,j denote the decisions that si and tj are unaligned respectively.
",3.1 Alignment via Integer Linear Programs,[0],[0]
Every inference decision is scored by the trained model.,3.1 Alignment via Integer Linear Programs,[0],[0]
"Thus, we have score(i, j, l), score(i, 0) and score(0, j) for the three kinds of inference variables respectively.",3.1 Alignment via Integer Linear Programs,[0],[0]
"All scores are of the form A ( wTΦ (·, s, t) ) , where w is a weight vector that is learned, Φ (·, s, t) is a feature function whose arguments include the constituents and labels in question, and A is a sigmoidal activation function that flattens the scores to the range",3.1 Alignment via Integer Linear Programs,[0],[0]
"[0, 5].",3.1 Alignment via Integer Linear Programs,[0],[0]
"In all our experiments, we used the function A(x) = 5
1+e−x .",3.1 Alignment via Integer Linear Programs,[0],[0]
The goal of inference is to find the assignment to the inference variables that maximizes total score.,3.1 Alignment via Integer Linear Programs,[0],[0]
"That is, we seek to solve
arg max 1∈C
∑
i,j,l
score(i, j, l)1i,j,l
+ ∑
i
score(i, 0)1i,0
+ ∑
j
score(0, j)10,j (1)
",3.1 Alignment via Integer Linear Programs,[0],[0]
"Here 1 represents all the inference variables together and C denotes the set of all valid assignments to the variables, defined by the following set of constraints:
1.",3.1 Alignment via Integer Linear Programs,[0],[0]
"A pair of chunks can have at most one label.
2.",3.1 Alignment via Integer Linear Programs,[0],[0]
"Either a chunk can be unaligned or it should participate in a labeled alignment with exactly one chunk of the other sentence.
",3.1 Alignment via Integer Linear Programs,[0],[0]
"We can convert these constraints into linear inequalities over the inference variables using standard techniques for ILP inference (Roth and Yih, 2004)2.",3.1 Alignment via Integer Linear Programs,[0],[0]
"Note that, by construction, there is a oneto-one mapping from an assignment to the inference variables 1 and the alignment y.",3.1 Alignment via Integer Linear Programs,[0],[0]
"In the rest of the paper, we use these two symbols interchangeably, using 1 referring details of inference and y referring to the alignment as a sequence of labeled edges.",3.1 Alignment via Integer Linear Programs,[0],[0]
"To complete the prediction, we need to compute the numeric chunk and sentence similarities given the alignment y.",3.2 From Alignments to Similarities,[0],[0]
"In each case, we make modeling assumptions about how the alignments and similarities are related, as described below.
",3.2 From Alignments to Similarities,[0],[0]
"Chunk similarities To predict the chunk similarities, we assume that the label-specific chunk similarities of aligned chunks are the best edge-weights for the corresponding inference variables.",3.2 From Alignments to Similarities,[0],[0]
"That is, for a pair of chunks (si, tj) that are aligned with a label l, the chunk pair similarity zi,j,l is the coefficient associated with the corresponding inference variable.",3.2 From Alignments to Similarities,[1.0],"['That is, for a pair of chunks (si, tj) that are aligned with a label l, the chunk pair similarity zi,j,l is the coefficient associated with the corresponding inference variable.']"
"If the alignment edge indicates an unaligned chunk, then the corresponding score is zero.",3.2 From Alignments to Similarities,[0],[0]
"That is,
zi,j,l =
{ A ( wTΦ (si, tj , l, s, t) )",3.2 From Alignments to Similarities,[0],[0]
"if l 6= ∅
0 if l = ∅. (2)
But can chunk similarities directly be used to find good alignments?",3.2 From Alignments to Similarities,[0],[0]
"To validate this assumption, we performed a pilot experiment on the chunk aligned part of our training dataset.",3.2 From Alignments to Similarities,[0],[0]
"We used the gold standard chunk similarities as scores of the inference variables in the integer program in Eq. 1, with the variables associated with unaligned chunks being scored zero.",3.2 From Alignments to Similarities,[0],[0]
"We found that this experiment gives a near-perfect typed alignment F-score of 0.9875.
2While it may be possible to find the score maximizing alignment in the presence of these constraints using dynamic programming (say, a variant of the Kuhn-Munkres algorithm), we model inference as an ILP to allow us the flexibility to explore more sophisticated output interactions in the future.
",3.2 From Alignments to Similarities,[0],[0]
"The slight disparity is because the inference only allows 1-to-1 matches between chunks (constraint 2), which does not hold in a small number of examples.
",3.2 From Alignments to Similarities,[0],[0]
"Sentence similarities Given the aligned chunks y, the similarity between the sentences s and t (i.e., in our notation, r) is the weighted average of the chunk similarities (i.e., zi,j,l).",3.2 From Alignments to Similarities,[0],[0]
"Formally,
r = 1 |y| ∑
(si,tj ,l)∈y αlzi,j,l. (3)
Note that the weights αl depend only on the labels associated with the alignment edge and are designed to capture the polarity and strength of the label.",3.2 From Alignments to Similarities,[0],[0]
Eq. 3 bridges sentence similarities and chunk similarities.,3.2 From Alignments to Similarities,[0],[0]
"During learning, this provides the feedback from sentence similarities to chunk similarities.",3.2 From Alignments to Similarities,[0],[0]
The values of theα’s can be learned or fixed before learning commences.,3.2 From Alignments to Similarities,[0],[0]
"To simplify our model, we choose the latter approach .",3.2 From Alignments to Similarities,[0],[0]
"Section 5 gives more details.
",3.2 From Alignments to Similarities,[0],[0]
"Features To complete the description of the model, we now describe the features that define the scoring functions.",3.2 From Alignments to Similarities,[0],[0]
"We use standard features from the STS literature (Karumuri et al., 2015; Agirre et al., 2015b; Banjade et al., 2015).
",3.2 From Alignments to Similarities,[0],[0]
"For a pair of chunks, we extract the following similarity features: (1) Absolute cosine similarities of GloVe embeddings (Pennington et al., 2014) of head words, (2) WordNet based Resnik (Resnik, 1995), Leacock (Leacock and Chodorow, 1998) and Lin (Lin, 1998) similarities of head words, (3) Jaccard similarity of content words and lemmas.",3.2 From Alignments to Similarities,[0],[0]
"In addition, we also add indicators for: (1) the part of speech tags of the pair of head words, (2) the pair of head words being present in the lexical large section of the Paraphrase Database (Ganitkevitch et al., 2013), (3) a chunk being longer than the other while both are not named entity chunks, (4) a chunk having more content words than the other, (5) contents of one chunk being a part of the other, (6) having the same named entity type or numeric words, (7) sharing synonyms or antonyms, (8) sharing conjunctions or prepositions, (9) the existence of unigram/bigram/trigram overlap, (10) if only one chunk has a negation, and (11) a chunk having extra content words that are also present in the other sentence.
",3.2 From Alignments to Similarities,[0],[0]
"For a chunk being unaligned, we conjoin an indicator that the chunk is unaligned with the part of speech tag of its head word.",3.2 From Alignments to Similarities,[0],[0]
"In the model proposed above, by predicting the alignment, we will be able to deterministically calculate both chunk and sentence level similarities.",3.3 Discussion,[1.0],"['In the model proposed above, by predicting the alignment, we will be able to deterministically calculate both chunk and sentence level similarities.']"
"This is in contrast to other approaches for the STS task, which first align constituents and then extract features from alignments to predict similarities in a pipelined fashion.",3.3 Discussion,[1.0],"['This is in contrast to other approaches for the STS task, which first align constituents and then extract features from alignments to predict similarities in a pipelined fashion.']"
"The joint prediction of alignment and similarities allows us to address the primary motivation of the paper, namely using the abundant sentence level data to train the aligner and scorer.
",3.3 Discussion,[0.9999999569292597],"['The joint prediction of alignment and similarities allows us to address the primary motivation of the paper, namely using the abundant sentence level data to train the aligner and scorer.']"
The crucial assumption that drives the joint model is that the same set of parameters that can discover a good alignment can also predict similarities.,3.3 Discussion,[0],[0]
"This assumption – similar to the one made by Chang et al. (2010b) – and the associated model described above, imply that the goal of learning is to find parameters that drive the inference towards good alignments and similarities.",3.3 Discussion,[1.0],"['This assumption – similar to the one made by Chang et al. (2010b) – and the associated model described above, imply that the goal of learning is to find parameters that drive the inference towards good alignments and similarities.']"
"Under the proposed model, the alignment directly predicts the chunk and sentence similarities as well.",4 Learning the Alignment Model,[0],[0]
"We utilize two datasets to learn the model:
1.",4 Learning the Alignment Model,[0],[0]
"The alignment dataset DA consists of fully annotated aligned chunks and respective chunk similarity scores.
",4 Learning the Alignment Model,[0],[0]
2.,4 Learning the Alignment Model,[0],[0]
"The sentence dataset DS that consists of pairs of sentences where each pair is labeled with a numeric similarity score between zero and five.
",4 Learning the Alignment Model,[0.9999999725419063],['The sentence dataset DS that consists of pairs of sentences where each pair is labeled with a numeric similarity score between zero and five.']
The goal of learning is to use these two datasets to train the model parameters.,4 Learning the Alignment Model,[0],[0]
"Note that unlike standard multi-task learning problems, the two tasks in our case are tightly coupled both in terms of their definition and via the model described in Section 3.
",4 Learning the Alignment Model,[0],[0]
"We define three types of loss functions corresponding to the three components of the final output (i.e., alignment, chunk similarity and sentence similarity).",4 Learning the Alignment Model,[1.0],"['We define three types of loss functions corresponding to the three components of the final output (i.e., alignment, chunk similarity and sentence similarity).']"
"Naturally, for each kind of loss, we assume that we have the corresponding ground truth.",4 Learning the Alignment Model,[0],[0]
We will denote ground truth similarity scores and alignments using asterisks.,4 Learning the Alignment Model,[1.0],['We will denote ground truth similarity scores and alignments using asterisks.']
"Also, the loss functions
defined below depend on the weight vector w, but this is not shown to simplify notation.
1.",4 Learning the Alignment Model,[0],[0]
The alignment loss La is a structured loss function that penalizes alignments that are far away from the ground truth.,4 Learning the Alignment Model,[1.0],['The alignment loss La is a structured loss function that penalizes alignments that are far away from the ground truth.']
"We used the structured hinge loss (Taskar et al., 2004; Tsochantaridis et al., 2005) for this purpose.
La(s, t,y ∗) = max y wTΦ",4 Learning the Alignment Model,[0],[0]
"(s, t,y)
+∆",4 Learning the Alignment Model,[0],[0]
"(y,y∗)−wTΦ (s, t,y∗) .
",4 Learning the Alignment Model,[0],[0]
"Here, ∆ refers to the Hamming distance between the alignments.
2.",4 Learning the Alignment Model,[0],[0]
The chunk score loss Lc is designed to penalize errors in predicted chunk level similarities.,4 Learning the Alignment Model,[1.0],['The chunk score loss Lc is designed to penalize errors in predicted chunk level similarities.']
"To account for cases where chunk boundaries may be incorrect, we define this loss as the sum of squared errors of token similarities.",4 Learning the Alignment Model,[0],[0]
"However, neither our output nor the gold standard similarities are at the granularity of tokens.",4 Learning the Alignment Model,[0],[0]
"Thus, to compute the loss, we project the chunk scores zi,j,l for an aligned chunk pair (si, tj , l) to the tokens that constitute the chunks by equally partitioning the scores among all possible internal alignments.",4 Learning the Alignment Model,[0],[0]
"In other words, for a token wi in the chunk si and token wj in chunk sj , we define token similarity scores as
z(wi, wj , l) = zi,j,l
N(si,tj)
Here, the normalizing function N is the product of the number of tokens in the chunks3.",4 Learning the Alignment Model,[0],[0]
Note that this definition of the token similarity scores applies to both predicted and gold standard similarities.,4 Learning the Alignment Model,[0],[0]
"Unaligned tokens are associated with a zero score.
",4 Learning the Alignment Model,[0],[0]
"We can now define the loss for a token pair (wi, wj) ∈ (s, t) and a label l as the squared error of their token similarity scores:
l(wi, wj , l) =",4 Learning the Alignment Model,[0],[0]
"(z(wi, wj , l)− z∗(wi, wj ,",4 Learning the Alignment Model,[0],[0]
"l))2
3Following the official evaluation of the interpretable STS task, we also experimented with the max(|si|, |tj |) for the normalizer, but we found via cross validation that the product performs better.
",4 Learning the Alignment Model,[0],[0]
"The chunk loss score Lc for a sentence pair is the sum of all the losses over all pairs of tokens and labels.
",4 Learning the Alignment Model,[0],[0]
"Lc(s, t,y,y ∗, z, z∗) =
∑
wi,wj ,l
l(wi, wj , l)
3.",4 Learning the Alignment Model,[0],[0]
The sentence similarity loss Ls provides feedback to the aligner by penalizing alignments that are far away from the ground truth in their similarity assessments.,4 Learning the Alignment Model,[1.0],['The sentence similarity loss Ls provides feedback to the aligner by penalizing alignments that are far away from the ground truth in their similarity assessments.']
"For a pair of sentences (s, t), given the ground truth sentence similarity r∗ and the predicted sentence similarity r (using Equation (3)), the sentence similarity loss is the squared error:
Ls(s, t, r ∗)",4 Learning the Alignment Model,[0],[0]
"= (r − r∗)2 .
",4 Learning the Alignment Model,[0],[0]
Our learning objective is the weighted combination of the above three components and a `2 regularizer on the weight vector.,4 Learning the Alignment Model,[0],[0]
"The importance of each type of loss is controlled by a corresponding hyperparameter: λa, λc and λs respectively.
",4 Learning the Alignment Model,[0],[0]
Learning algorithm,4 Learning the Alignment Model,[0],[0]
"We have two scenarios to consider: with only alignment dataset DA, and with both DA and sentence dataset DS .",4 Learning the Alignment Model,[0.9962398722049732],"['Learning algorithm We have two scenarios to consider: with only alignment dataset DA, and with both DA and sentence dataset DS .']"
"Note that even if we train only on the alignment dataset DA, our learning objective is not convex because the activation function is sigmoidal (in Section 3.1).
",4 Learning the Alignment Model,[0],[0]
"In both cases, we use stochastic gradient descent with minibatch updates as the optimizer.",4 Learning the Alignment Model,[0],[0]
"In the first scenario, we simply perform the optimization using the alignment and the chunk score losses.",4 Learning the Alignment Model,[1.0],"['In the first scenario, we simply perform the optimization using the alignment and the chunk score losses.']"
"We found by preliminary experiments on training data that initializing the weights to one performed best.
",4 Learning the Alignment Model,[0],[0]
"Algorithm 1 Learning alignments and similarities, given alignment dataset DA and sentence similarity dataset DS .",4 Learning the Alignment Model,[0],[0]
"See the text for more details.
1: Initialize all weights to one.",4 Learning the Alignment Model,[0],[0]
2: w0 ← SGD(DA): Train an initial model 3: Use w0 to predict alignments on examples in DS .,4 Learning the Alignment Model,[0],[0]
Call this D̂S .,4 Learning the Alignment Model,[0],[0]
4: w ← SGD(DA ∪ D̂S): Train on both sets of examples.,4 Learning the Alignment Model,[0],[0]
"5: return w
When we have both DA and DS (Algorithm 1), we first initialize the model on the alignment data
only.",4 Learning the Alignment Model,[0],[0]
"Using this initial model, we hypothesize alignments on all examples in DS to get fully labeled examples.",4 Learning the Alignment Model,[0],[0]
"Then, we optimize the full objective (all three loss terms) on the combined dataset.",4 Learning the Alignment Model,[0],[0]
"Because our goal is to study the impact on the chunk level predictions, in the full model, the sentence loss does not play a part on examples from DA.",4 Learning the Alignment Model,[0],[0]
"The primary research question we seek to answer via experiments is: Can we better predict chunk alignments and similarities by taking advantage of sentence level similarity data?
",5 Experiments and Results,[0],[0]
"Datasets We used the training and test data from the 2016 SemEval shared tasks of predicting semantic textual similarity (Agirre et al., 2016a) and interpretable STS (Agirre et al., 2016b), that is, tasks 1 and 2 respectively.",5 Experiments and Results,[0],[0]
"For our experiments, we used the headlines and images sections of the data.",5 Experiments and Results,[0],[0]
"The data for the interpretable STS task, consisting of manually aligned and scored chunks, provides the alignment datasets for training (DA).",5 Experiments and Results,[0],[0]
"The headlines section of the training data consists for 756 sentence pairs, while the images section consists for 750 sentence pairs.",5 Experiments and Results,[0],[0]
The data for the STS task acts as our sentence level training dataset (DS).,5 Experiments and Results,[0],[0]
"For the headlines section, we used the 2013 headlines test set consisting of 750 sentence pairs with gold sentence similarity scores.",5 Experiments and Results,[0],[0]
"For the images section, we used the 2014 images test set consisting of 750 examples.",5 Experiments and Results,[0],[0]
"We evaluated our models on the official Task 2 test set, consisting of 375 sentence pairs for both the headlines and images sections.",5 Experiments and Results,[0],[0]
"In all experiments, we used gold standard chunk boundaries if they are available (i.e., for DA).
",5 Experiments and Results,[0],[0]
"Pre-processing We pre-processed the sentences with parts of speech using the Stanford CoreNLP toolkit (Manning et al., 2014).",5 Experiments and Results,[0],[0]
"Since our setting assumes that we have the chunks as input, we used the Illinois shallow parser (Clarke et al., 2012) to extract chunks from DS .",5 Experiments and Results,[0],[0]
We post-processed the predicted chunks to correct for errors using the following steps: 1.,5 Experiments and Results,[0],[0]
Split on punctuation; 2.,5 Experiments and Results,[0],[0]
Split on verbs in NP; 3.,5 Experiments and Results,[0],[0]
Split on nouns in VP; 4.,5 Experiments and Results,[0],[0]
Merge PP+NP into PP; 5.,5 Experiments and Results,[0],[0]
"Merge VP+PRT into VP if the PRT chunk is not a preposition or a subordinating
conjunction; 6.",5 Experiments and Results,[0],[0]
Merge SBAR+NP into SBAR; and 7.,5 Experiments and Results,[0],[0]
Create new contiguous chunks using tokens that are marked as being outside a chunk by the shallow parser.,5 Experiments and Results,[0],[0]
"We found that using the above postprocessing rules, improved the F1 of chunk accuracy from 0.7865 to 0.8130.",5 Experiments and Results,[0],[0]
We also found via crossvalidation that this post-processing improved overall alignment accuracy.,5 Experiments and Results,[0],[0]
"The reader may refer to other STS resources (Karumuri et al., 2015) for further improvements along this direction.
",5 Experiments and Results,[0],[0]
"Experimental setup We performed stochastic gradient descent for 200 epochs in our experiments, with a mini-batch size of 20.",5 Experiments and Results,[0],[0]
"We determined the three λ’s using cross-validation, with different hyperparameters for examples fromDA andDS .",5 Experiments and Results,[0],[0]
Table 1 lists the best hyperparameter values.,5 Experiments and Results,[0],[0]
"For performing inference, we used the Gurobi optimizer4.
",5 Experiments and Results,[0],[0]
"As noted in Section 3.1, the parameter αl combines chunk scores into sentence scores.",5 Experiments and Results,[1.0],"['As noted in Section 3.1, the parameter αl combines chunk scores into sentence scores.']"
"To find these hyper-parameters, we used a set of 426 sentences from the from the headlines training data that had both sentence and chunk annotation.",5 Experiments and Results,[1.0],"['To find these hyper-parameters, we used a set of 426 sentences from the from the headlines training data that had both sentence and chunk annotation.']"
We simplified the search by assuming that αEqui is always 1.0 and all labels other than OPPO have the same α.,5 Experiments and Results,[1.0],['We simplified the search by assuming that αEqui is always 1.0 and all labels other than OPPO have the same α.']
"Using grid search over [−1, 1] in increments of 0.1, we selected α’s that gave us the highest Pearson correlation for sentence level similarities.",5 Experiments and Results,[0],[0]
"The best α’s (with a Pearson correlation of 0.7635) were:
αl =    1, l = EQUI, −1, l = OPPO, 0.7, otherwise
Results Following the official evaluation for the SemEval task, we evaluate both alignments and their
4http://www.gurobi.com/
corresponding similarity scores.",5 Experiments and Results,[0],[0]
"The typed alignment evaluation (denoted by typed ali in the results table) measures F1 over the alignment edges where the types need to match, but scores are ignored.",5 Experiments and Results,[0],[0]
"The typed similarity evaluation (denoted by typed score) is the more stringent evaluation that measures F1 of the alignment edge labels, but penalizes them if the similarity scores do not match.",5 Experiments and Results,[0],[0]
The untyped versions of alignment and scored alignment evaluations ignore alignment labels.,5 Experiments and Results,[0],[0]
"These metrics, based on Melamed (1997), are tailored for the interpretable STS task5.",5 Experiments and Results,[0],[0]
We refer the reader to the guidelines of the task for further details.,5 Experiments and Results,[0],[0]
We report both scores in Table 2.,5 Experiments and Results,[0],[0]
"We also list the performance of the baseline system (Sultan et al., 2014a) and the top ranked systems from the 2016 shared task for each dataset6.
",5 Experiments and Results,[1.0000000197836107],"['We also list the performance of the baseline system (Sultan et al., 2014a) and the top ranked systems from the 2016 shared task for each dataset6.']"
"By comparing the rows labeledDA andDA +DS in Table 2 (a) and Table 2 (b), we see that in both the headlines and the images datasets, adding sentence level information improves the untyped score, lifting the stricter typed score F1.",5 Experiments and Results,[0],[0]
"On the headlines dataset, incorporating sentence-level information degrades both the untyped and typed alignment quality because we cross-validated on the typed score metric.
",5 Experiments and Results,[0],[0]
"The typed score metric is the combination of untyped alignment, untyped score and typed alignment.",5 Experiments and Results,[1.0],"['The typed score metric is the combination of untyped alignment, untyped score and typed alignment.']"
"From the row DA +DS in Table 2(a), we observe that the typed score F1 is slightly behind that of rank 1 system while all other three metrics are significantly better, indicating that we need to improve our modeling of the intersection of the three aspects.",5 Experiments and Results,[0],[0]
"However, this does not apply to images
5In the SemEval 2016 shared task, the typed score is the metric used for system ranking.
",5 Experiments and Results,[0],[0]
"6http://alt.qcri.org/semeval2016/task2/
dataset where the improvement on the typed score F1 comes from the typed alignment.
",5 Experiments and Results,[0],[0]
"Further, we see that even our base model that only depends on the alignment data offers strong alignment F1 scores.",5 Experiments and Results,[1.0],"['Further, we see that even our base model that only depends on the alignment data offers strong alignment F1 scores.']"
This validates the utility of jointly modeling alignments and chunk similarities.,5 Experiments and Results,[1.0],['This validates the utility of jointly modeling alignments and chunk similarities.']
Adding sentence data to this already strong system leads to performance that is comparable to or better than the state-of-the-art systems.,5 Experiments and Results,[0],[0]
"Indeed, our final results would have been ranked first on the images task and a close second on the headlines task in the official standings.
",5 Experiments and Results,[0],[0]
The most significant feedback coming from sentence-level information is with respect to the chunk similarity scores.,5 Experiments and Results,[0],[0]
"While we observed slight change in the unscored alignment performance, for both the headlines and the images datasets, we saw improvements in both scored precision and recall when sentence level data was used.",5 Experiments and Results,[1.0],"['While we observed slight change in the unscored alignment performance, for both the headlines and the images datasets, we saw improvements in both scored precision and recall when sentence level data was used.']"
"In this section, first, we report the results of manual error analysis.",6 Analysis and Discussion,[0],[0]
"Then, we study the ability of our model to handle data from different domains.",6 Analysis and Discussion,[0],[0]
"To perform a manual error analysis, we selected 40 examples from the development set of the headlines section.",6.1 Error Analysis,[0],[0]
We classified the errors made by the full model trained on the alignment and sentence datasets.,6.1 Error Analysis,[0],[0]
"Below, we report the four most significant types of errors:
1.",6.1 Error Analysis,[0],[0]
"Contextual implication: Chunks that are meant to be aligned are not synonyms by them-
selves but are implied by the context.",6.1 Error Analysis,[0],[0]
"For instance, Israeli forces and security forces might be equivalent in certain contexts.",6.1 Error Analysis,[0],[0]
"Out of the 16 instances of EQUI being misclassified as SPE, eight were caused by the features’ inability to ascertain contextual implications.",6.1 Error Analysis,[0],[0]
"This also accounted for four out of the 15 failures to identify alignments.
2.",6.1 Error Analysis,[0],[0]
"Semantic phrase understanding: These are the cases where our lexical resources failed, e. g., ablaze and left burning.",6.1 Error Analysis,[0],[0]
This accounted for ten of the 15 chunk alignment failures and nine of the 21 labeling errors.,6.1 Error Analysis,[0],[0]
"Among these, some errors (four alignment failures and four labeling errors) were much simpler than others that could be handled with relatively simple features (e.g. family reunions↔ family unions).
",6.1 Error Analysis,[0],[0]
3.,6.1 Error Analysis,[0],[0]
Preposition semantics:,6.1 Error Analysis,[0],[0]
The inability to account for preposition semantics accounts for three of the 16 cases where EQUI is mistaken as a SPE.,6.1 Error Analysis,[0],[0]
"Some examples include at 91 ↔ aged 91 and catch fire↔ after fire.
4.",6.1 Error Analysis,[0],[0]
"Underestimated EQUI score: Ten out of 14 cases of score underestimation happened on EQUI label.
",6.1 Error Analysis,[0],[0]
Our analysis suggests that we need better contextual features and phrasal features to make further gains in aligning constituents.,6.1 Error Analysis,[0],[0]
"In all the experiments in Section 5, we used sentence datasets belonging to the same domain as the alignment dataset (either headlines or images).",6.2 Does the text domain matter?,[0],[0]
"Given that our model can take advantage of two separate datasets, a natural question to ask is how the domain of the sentence dataset influences overall alignment performance.",6.2 Does the text domain matter?,[0],[0]
"Additionally, we can also ask how well the trained classifiers perform on out-ofdomain data.",6.2 Does the text domain matter?,[0],[0]
We performed a series of experiments to explore these two questions.,6.2 Does the text domain matter?,[0],[0]
"Table 3 summarizes the results of these experiments.
",6.2 Does the text domain matter?,[0],[0]
The columns labeled Train and Test of the table show the training and test sets used.,6.2 Does the text domain matter?,[0],[0]
"Each dataset can be either the headlines section (denoted by hdln), or the images section (img) or not used
(∅).",6.2 Does the text domain matter?,[0],[0]
The last two columns report performance on the test set.,6.2 Does the text domain matter?,[0],[0]
"The rows 1 and 5 in the table correspond to the in-domain settings and match the results of typed alignment and score in Table 2.
",6.2 Does the text domain matter?,[0],[0]
"When the headlines data is tested on the images section, we see that there is the usual domain adaptation problem (row 3 vs row 1) and using target images sentence data does not help (row 4 vs row 3).",6.2 Does the text domain matter?,[0],[0]
"In contrast, even though there is a domain adaptation problem when we compare the rows 5 and 7, we see that once again, using headlines sentence data improves the predicted scores (row 7 vs row 8).",6.2 Does the text domain matter?,[0],[0]
"This observation can be explained by the fact that the images sentences are relatively simpler and headlines dataset can provide richer features in comparison, thus allowing for stronger feedback from sentences to constituents.
",6.2 Does the text domain matter?,[0],[0]
The next question concerns how the domain of the sentence dataset DS influences alignment and similarity performance.,6.2 Does the text domain matter?,[0],[0]
"To answer this, we can compare the results in every pair of rows (i.e., 1 vs 2, 3 vs 4, etc.)",6.2 Does the text domain matter?,[0],[0]
"We see that when the sentence data from the image data is used in conjunction to the headlines chunk data, it invariably makes the classifiers worse.",6.2 Does the text domain matter?,[0],[0]
"In contrast, the opposite trend is observed when the headlines sentence data augments the images chunk data.",6.2 Does the text domain matter?,[0],[0]
"This can once again be explained by relatively simpler sentence constructions in the images set, suggesting that we can leverage linguistically complex corpora to improve alignment on simpler ones.",6.2 Does the text domain matter?,[0],[0]
"Indeed, surprisingly, we obtain marginally better performance on the images set when we use images chunk level data in conjunction
with the headlines sentence data (row 6 vs the row labeled DA +DS in the Table 2(b)).",6.2 Does the text domain matter?,[0],[0]
Aligning words and phrases between pairs of sentences is widely studied in NLP.,7 Related Work,[0],[0]
"Machine translation has a rich research history of using alignments (for e.g., (Koehn et al., 2003; Och and Ney, 2003)), going back to the IBM models (Brown et al., 1993).",7 Related Work,[0],[0]
"From the learning perspective, the alignments are often treated as latent variables during learning, as in this work where we treated alignments in the sentence level training examples as latent variables.",7 Related Work,[0],[0]
"Our work is also conceptually related to (Ganchev et al., 2008), which asked whether improved alignment error implied better translation.
",7 Related Work,[0],[0]
"Outside of machine translation, alignments are employed either explicitly or implicitly for recognizing textual entailment (Brockett, 2007; Chang et al., 2010a) and paraphrase recognition (Das and Smith, 2009; Chang et al., 2010a).",7 Related Work,[0],[0]
"Additionally, alignments are explored in multiple ways (tokens, phrases, parse trees and dependency graphs) as a foundation for natural logic inference (Chambers et al., 2007; MacCartney and Manning, 2007; MacCartney et al., 2008).",7 Related Work,[0],[0]
"Our proposed aligner can be used to aid such applications.
",7 Related Work,[0],[0]
"For predicting sentence similarities, in both variants of the task, word or chunk alignments have extensively been used (Sultan et al., 2015; Sultan et al., 2014a; Sultan et al., 2014b; Hänig et al., 2015; Karumuri et al., 2015; Agirre et al., 2015b; Banjade et al., 2015, and others).",7 Related Work,[0],[0]
"In contrast to these systems, we proposed a model that is trained jointly to predict alignments, chunk similarities and sentence similarities.",7 Related Work,[0],[0]
"To our knowledge, this is the first approach that combines sentence-level similarity data with fine grained alignments to train a chunk aligner.",7 Related Work,[0],[0]
"In this paper, we presented the first joint framework for aligning sentence constituents and predicting constituent and sentence similarities.",8 Conclusion,[0],[0]
We showed that our predictive model can be trained using both aligned constituent data and sentence similarity data.,8 Conclusion,[0],[0]
"Our jointly trained model achieves stateof-the-art performance on the task of predicting in-
terpretable sentence similarities.",8 Conclusion,[0],[0]
The authors wish to thank the anonymous reviewers and the members of the Utah NLP group for their valuable comments and pointers to references.,Acknowledgments,[0],[0]
We study the problem of jointly aligning sentence constituents and predicting their similarities.,abstractText,[0],[0]
"While extensive sentence similarity data exists, manually generating reference alignments and labeling the similarities of the aligned chunks is comparatively onerous.",abstractText,[0],[0]
This prompts the natural question of whether we can exploit easy-to-create sentence level data to train better aligners.,abstractText,[0],[0]
"In this paper, we present a model that learns to jointly align constituents of two sentences and also predict their similarities.",abstractText,[0],[0]
"By taking advantage of both sentence and constituent level data, we show that our model achieves state-of-the-art performance at predicting alignments and constituent similarities.",abstractText,[0],[0]
Exploiting Sentence Similarities for Better Alignments,title,[0],[0]
