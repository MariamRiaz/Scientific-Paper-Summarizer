8	5	However, these methods lack a well justified underlying probabilistic model, are sensitive to the choice of underlying metric, and do not give a principled answer to the fundamental question of required number of clusters for the data at hand.
10	52	First, our probabilistic method goes beyond the widely-adopted class conditional independence assumption of feature variables, e.g., as in the latent class model (McCutcheon, 1987).
14	8	Fourth, the proposed method can be easily parallelized to achieve a competitive running time with respect to many lightweight geometric clustering methods.
23	9	Having modeled the data generation process via a tree graphical model, we are left with finding a robust approach for assigning each data point to its cluster.
60	11	A probabilistic graphical model (Koller & Friedman, 2009) consists of a set of inter-dependent random variables X = (X1, .
63	10	The edges represents the dependence relations between pairs of variables.
66	44	In this paper, we use discrete and categorical interchangeably and focus on non-ordinal discrete variables, although ordinal discrete variables are of interest in practice as well.
80	7	We choose tree-models as they strike a elegant balance between computational efficiency and flexibility of the model.
90	30	Given a probability density function, p(X), a mode is a local maximum in both the continuous neighborhood and discrete neighborhood, formally: Definition 1 (Modes).
121	9	At each step of the algorithm, we first update all discrete variables until no better elements exist within the discrete neighborhood N dδ (x) with δ = 1.
122	14	Next, we update all continuous variables using gradient descent, until the gradient of f at continuous dimensions∇cf becomes zero.
126	8	The best neighbor within Hamming distance one, argminz∈Nd1 (x) f(z), can be computed using dynamic programming.
128	7	It remains to compute the gradient of f in the contin- uous domain, ∇cf .
131	5	Treating them differently, the partial derivative: ∂f(x) ∂xi = −(1− di) ∑N n=1 Kh1i(y n i − xi) yni −xi h21i∑N n=1 Kh1i(y n i − xi) − ∑ j∈Ic:(i,j)∈E ∑N n=1 Kh2i(y n i − xi)Kh2j (ynj − xj) yni −xi h22i∑N n=1 Kh2i(y n i − xi)Kh2j (ynj − xj) − ∑ k∈Id:(i,j)∈E ∑N n=1 Kh2i(y n i − xi)Jynk = xkK yni −xi h22i∑N n=1 Kh2i(y n i − xi) (3.3) Algorithm 2 Merging Data Using Topological Persistence 1: Input: Ĝ = (V̂, Ê), density function p : V̂ → R+, persistence threshold τ 2: Output: Clusters C 3: C ← ∅ 4: Sort elements in V̂ according to the density function values, so that p(vi) ≥ p(vi+1), ∀vi, vi+1 ∈ V̂ .
132	11	5: for i = 1 to |V̂| do 6: nbd← {vj | (vi, vj) ∈ Ê ∧ j < i} 7: // neighbors of vi with smaller indices (bigger p) 8: if nbd = ∅ then 9: create a new cluster c = {vi} 10: birth(c)← p(vi) 11: C ← C ∪ {c} 12: else 13: Cnbd ← all clusters containing nodes in nbd 14: cmax ← argmaxc∈Cnbd birth(c) 15: for all c ∈ Cnbd and c 6= cmax do 16: persistence(c)← birth(c)− p(vi) 17: if persistence(c) < τ then 18: // merge c into cmax 19: cmax ← cmax ∪ c 20: C ← C\{c} 21: end if 22: end for 23: // assign vi to cmax 24: cmax ← cmax ∪ {vi} 25: end if 26: end for
135	11	In such cases, the method tends to produce a large number of modes, and thus over-segments the data into small clusters.
140	13	We focus on zerodimensional topological structures in this paper, although the theory is much more general.
141	11	We estimate the saliency of a peak (mode) using its “relative height”, namely, the difference between its height and the level at which its basin of attraction meets the one of another higher mode.
143	13	As t decreases, we monitor the topological changes of the progressively growing superlevel set, X t = {x ∈ X | p(x) ≥ t}, that is, the domain whose probability density value is no smaller than t. Each mode attributes to the birth of a new connected component in the superlevel set and the component is killed when it meets another component created by a higher mode.
147	10	This gives us a principled way to merge modes.
151	10	In practice, however, a uniform sampling will have exponential size to the dimension.
156	24	In this paper, we propose to compute persistence based on all points we encountered during the mode-seeking procedure.
160	8	This provides us a well-suited underlying graph describing the density landscape.
161	9	Finally, to ensure the graph is fully connected, and the space between modes are well described, we add edges (green edges) connecting points from neighboring attractive basins, as well as the lowest point along these edges (green markers).
162	31	Note that this is the only time when the distance metric plays a role in our model.
163	58	We use a sum of the Hamming distance and Euclidean distance.
165	67	Sort all nodes in decreasing order of their density function values.
166	13	Add them into the superlevel set one-by-one.
167	25	To add a node vi, we check whether it is adjacent to any nodes that have been included.
169	50	If vi is connected to multiple existing connected components, we keep the one with the earliest birth time, cmax, and merge some others into cmax.
