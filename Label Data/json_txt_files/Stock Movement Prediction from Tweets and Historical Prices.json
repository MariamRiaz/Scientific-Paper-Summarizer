{"sections": [{"text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1970\u20131979 Melbourne, Australia, July 15 - 20, 2018. c\u00a92018 Association for Computational Linguistics\n1970"}, {"heading": "1 Introduction", "text": "Stock movement prediction has long attracted both investors and researchers (Frankel, 1995; Edwards et al., 2007; Bollen et al., 2011; Hu et al., 2018). We present a model to predict stock price movement from tweets and historical stock prices.\nIn natural language processing (NLP), public news and social media are two primary content resources for stock market prediction, and the models that use these sources are often discriminative. Among them, classic research relies heavily on feature engineering (Schumaker and Chen, 2009; Oliveira et al., 2013). With the prevalence of deep neural networks (Le and Mikolov, 2014), eventdriven approaches were studied with structured event representations (Ding et al., 2014, 2015).\n1https://github.com/yumoxu/ stocknet-dataset\nMore recently, Hu et al. (2018) propose to mine news sequence directly from text with hierarchical attention mechanisms for stock trend prediction.\nHowever, stock movement prediction is widely considered difficult due to the high stochasticity of the market: stock prices are largely driven by new information, resulting in a random-walk pattern (Malkiel, 1999). Instead of using only deterministic features, generative topic models were extended to jointly learn topics and sentiments for the task (Si et al., 2013; Nguyen and Shirai, 2015). Compared to discriminative models, generative models have the natural advantage in depicting the generative process from market information to stock signals and introducing randomness. However, these models underrepresent chaotic social texts with bag-of-words and employ simple discrete latent variables.\nIn essence, stock movement prediction is a time series problem. The significance of the temporal dependency between movement predictions is not addressed in existing NLP research. For instance, when a company suffers from a major scandal on a trading day d1, generally, its stock price will have a downtrend in the coming trading days until day d2, i.e. [d1, d2].2 If a stock predictor can recognize this decline pattern, it is likely to benefit all the predictions of the movements during [d1, d2]. Otherwise, the accuracy in this interval might be harmed. This predictive dependency is a result of the fact that public information, e.g. a company scandal, needs time to be absorbed into movements over time (Luss and d\u2019Aspremont, 2015), and thus is largely shared across temporally-close predictions.\nAiming to tackle the above-mentioned outstanding research gaps in terms of modeling high market stochasticity, chaotic market information and temporally-dependent prediction, we propose\n2We use the notation [a, b] to denote the interval of integer numbers between a and b.\nStockNet, a deep generative model for stock movement prediction.\nTo better incorporate stochastic factors, we generate stock movements from latent driven factors modeled with recurrent, continuous latent variables. Motivated by Variational Auto-Encoders (VAEs; Kingma and Welling, 2013; Rezende et al., 2014), we propose a novel decoder with a variational architecture and derive a recurrent variational lower bound for end-to-end training (Section 5.2). To the best of our knowledge, StockNet is the first deep generative model for stock movement prediction.\nTo fully exploit market information, StockNet directly learns from data without pre-extracting structured events. We build market sources by referring to both fundamental information, e.g. tweets, and technical features, e.g. historical stock prices (Section 5.1).3 To accurately depict predictive dependencies, we assume that the movement prediction for a stock can benefit from learning to predict its historical movements in a lag window. We propose trading-day alignment as the framework basis (Section 4), and further provide a novel multi-task learning objective (Section 5.3).\nWe evaluate StockNet on a stock movement prediction task with a new dataset that we collected. Compared with strong baselines, our experiments show that StockNet achieves state-of-the-art performance by incorporating both data from Twitter and historical stock price listings."}, {"heading": "2 Problem Formulation", "text": "We aim at predicting the movement of a target stock s in a pre-selected stock collection S on a target trading day d. Formally, we use the market information comprising of relevant social media corporaM, i.e. tweets, and historical prices, in the lag [d \u2212\u2206d, d \u2212 1] where \u2206d is a fixed lag size. We estimate the binary movement where 1 denotes rise and 0 denotes fall,\ny = 1 ( pcd > p c d\u22121 )\n(1)\nwhere pcd denotes the adjusted closing price adjusted for corporate actions affecting stock prices, e.g. dividends and splits.4 The adjusted closing\n3To a fundamentalist, stocks have their intrinsic values that can be derived from the behavior and performance of their company. On the contrary, technical analysis considers only the trends and patterns of the stock price.\n4 Technically, d \u2212 1 may not be an eligible trading day and thus has no available price information. In the rest of this\nprice is widely used for predicting stock price movement (Xie et al., 2013) or financial volatility (Rekabsaz et al., 2017)."}, {"heading": "3 Data Collection", "text": "In finance, stocks are categorized into 9 industries: Basic Materials, Consumer Goods, Healthcare, Services, Utilities, Conglomerates, Financial, Industrial Goods and Technology.5 Since high-tradevolume-stocks tend to be discussed more on Twitter, we select the two-year price movements from 01/01/2014 to 01/01/2016 of 88 stocks to target, coming from all the 8 stocks in Conglomerates and the top 10 stocks in capital size in each of the other 8 industries (see supplementary material).\nWe observe that there are a number of targets with exceptionally minor movement ratios. In a three-way stock trend prediction task, a common practice is to categorize these movements to another \u201cpreserve\u201d class by setting upper and lower thresholds on the stock price change (Hu et al., 2018). Since we aim at the binary classification of stock changes identifiable from social media, we set two particular thresholds, - 0.5% and 0.55% and simply remove 38.72% of the selected targets with the movement percents between the two thresholds. Samples with the movement percents \u2264-0.5% and >0.55% are labeled with 0 and 1, respectively. The two thresholds are selected to balance the two classes, resulting in 26,614 prediction targets in the whole dataset with 49.78% and 50.22% of them in the two classes. We split them temporally and 20,339 movements between 01/01/2014 and 01/08/2015 are for training, 2,555 movements from 01/08/2015 to 01/10/2015 are for development, and 3,720 movements from 01/10/2015 to 01/01/2016 are for test.\nThere are two main components in our dataset:6 a Twitter dataset and a historical price dataset. We access Twitter data under the official license of Twitter, then retrieve stock-specific tweets by querying regexes made up of NASDAQ ticker symbols, e.g. \u201c\\$GOOG\\b\u201d for Google Inc.. We preprocess tweet texts using the NLTK package (Bird et al., 2009) with the particular Twitter\npaper, the problem is solved by keeping the notational consistency with our recurrent model and using its time step t to index trading days. Details will be provided in Section 4. We use d here to make the formulation easier to follow.\n5https://finance.yahoo.com/industries 6Our dataset is available at https://github.com/\nyumoxu/stocknet-dataset.\nmode, including for tokenization and treatment of hyperlinks, hashtags and the \u201c@\u201d identifier. To alleviate sparsity, we further filter samples by ensuring there is at least one tweet for each corpus in the lag. We extract historical prices for the 88 selected stocks to build the historical price dataset from Yahoo Finance.7"}, {"heading": "4 Model Overview", "text": "We provide an overview of data alignment, model factorization and model components.\nAs explained in Section 1, we assume that predicting the movement on trading day d can benefit from predicting the movements on its former trading days. However, due to the general principle of sample independence, building connections directly across samples with temporally-close target dates is problematic for model training.\nAs an alternative, we notice that within a sample with a target trading day d there are likely to be other trading days than d in its lag that can simulate the prediction targets close to d. Motivated by this observation and multi-task learning (Caruana, 1998), we make movement predictions not only for d, but also other trading days existing in the lag. For instance, as shown in Figure 2, for a sample targeting 07/08/2012 and a 5-day lag, 03/08/2012 and 06/08/2012 are eligible trading days in the lag and we also make predictions for them using the market information in this sample. The relations between these predictions can thus be captured within the scope of a sample.\nAs shown in the instance above, not every single date in a lag is an eligible trading day, e.g. weekends and holidays. To better organize and use the input, we regard the trading day, instead of the\n7http://finance.yahoo.com\ncalendar day used in existing research, as the basic unit for building samples. To this end, we first find all the T eligible trading days referred in a sample, in other words, existing in the time interval [d \u2212 \u2206d + 1, d]. For clarity, in the scope of one sample, we index these trading days with t \u2208 [1, T ],8 and each of them maps to an actual (absolute) trading day dt. We then propose trading-day alignment: we reorganize our inputs, including the tweet corpora and historical prices, by aligning them to these T trading days. Specifically, on the tth trading day, we recognize market signals from the corpus Mt in [dt\u22121, dt) and the historical prices pt on dt\u22121, for predicting the movement yt on dt. We provide an aligned sample for illustration in Figure 2. As a result, every single unit in a sample is a trading day, and we can predict a sequence of movements y = [y1, . . . , yT ]. The main target is yT while the remainder y\u2217 = [y1, . . . , yT\u22121] serves as the temporal auxiliary target. We use these in addition to the main target to improve prediction accuracy (Section 5.3).\nWe model the generative process shown in Figure 1. We encode observed market information as a random variable X = [x1; . . . ;xT ], from which we generate the latent driven factor Z = [z1; . . . ; zT ] for our prediction task. For the aforementioned multi-task learning purpose, we aim at modeling the conditional probability distribution p\u03b8 (y|X) = \u222b Z p\u03b8 (y, Z|X) instead of p\u03b8(yT |X). We write the following factorization for generation,\np\u03b8 (y, Z|X) = p\u03b8 (yT |X,Z) p\u03b8(zT |z<T , X) (2) T\u22121\u220f\nt=1\np\u03b8 (yt|x\u2264t, zt) p\u03b8 (zt|z<t, x\u2264t, yt)\nwhere for a given indexed matrix of T vectors [v1; . . . ; vT ], we denote by v<t and v\u2264t the submatrix [v1; . . . ; vt\u22121] and the submatrix [v1; . . . ; vt], respectively. Since y\u2217 is known in generation, we use the posterior p\u03b8 (zt|z<t, x\u2264t, yt) , t < T to incorporate market signals more accurately and only use the prior p\u03b8(zT |z<T , X) when generating zT . Besides, when t < T , yt is independent of z<t while our main prediction target, yT is made dependent on z<T through a temporal attention mechanism (Section 5.3).\nWe show StockNet modeling the above generative process in Figure 2. In a nutshell, StockNet\n8It holds that T \u2265 1 since d is undoubtedly a trading day.\ncomprises three primary components following a bottom-up fashion,\n1. Market Information Encoder (MIE) that encodes tweets and prices to X;\n2. Variational Movement Decoder (VMD) that infers Z with X, y and decodes stock movements y from X,Z;\n3. Attentive Temporal Auxiliary (ATA) that integrates temporal loss through an attention mechanism for model training."}, {"heading": "5 Model Components", "text": "We detail next the components of our model (MIE, VMD, ATA) and the way we estimate our model parameters."}, {"heading": "5.1 Market Information Encoder", "text": "MIE encodes information from social media and stock prices to enhance market information quality, and outputs the market information input X for VMD. Each temporal input is defined as\nxt = [ct, pt] (3)\nwhere ct and pt are the corpus embedding and the historical price vector, respectively.\nThe basic strategy of acquiring ct is to first feed messages into the Message Embedding Layer for their low-dimensional representations, then selectively gather them according to their quality. To handle the circumstance that multiple stocks are discussed in one single message, in addition to text information, we incorporate the position information of stock symbols mentioned in messages as well. Specifically, the layer consists of a forward GRU and a backward GRU for the preceding and following contexts of a stock symbol, s, respectively. Formally, in the message corpus of the tth trading day, we denote the word sequence of the kth message, k \u2208 [1,K], as W where W`? = s, `? \u2208 [1, L], and its word embedding matrix as E = [e1; e2; . . . ; eL]. We run the two GRUs as follows,\n\u2212\u2192 h f = \u2212\u2212\u2212\u2192 GRU(ef , \u2212\u2192 h f\u22121) (4) \u2190\u2212 h b = \u2190\u2212\u2212\u2212 GRU(eb, \u2190\u2212 h b+1) (5)\nm = ( \u2212\u2192 h `? + \u2190\u2212 h `?)/2 (6)\nwhere f \u2208 [1, . . . , `?], b \u2208 [`?, . . . , L]. The stock symbol is regarded as the last unit in both the preceding and the following contexts where the hidden values, \u2212\u2192 h l? , \u2190\u2212 h l? , are averaged to acquire the message embedding m. Gathering all message embeddings for the tth trading day, we have a mes-\nsage embedding matrix Mt \u2208 Rdm\u00d7K . In practice, the layer takes as inputs a five-rank tensor for a mini-batch, and yields all Mt in the batch with shared parameters.\nTweet quality varies drastically. Inspired by the news-level attention (Hu et al., 2018), we weight messages with their respective salience in collective intelligence measurement. Specifically, we first project Mt non-linearly to ut, the normalized attention weight over the corpus,\nut = \u03b6(w \u1d40 u tanh(Wm,uMt)) (7)\nwhere \u03b6(\u00b7) is the softmax function and Wm,u \u2208 Rdm\u00d7dm , wu \u2208 Rdm\u00d71 are model parameters. Then we compose messages accordingly to acquire the corpus embedding,\nct = Mtu \u1d40 t . (8)\nSince it is the price change that determines the stock movement rather than the absolute price value, instead of directly feeding the raw price vector p\u0303t = [ p\u0303ct , p\u0303 h t , p\u0303 l t ] comprising of the adjusted closing, highest and lowest price on a trading day t, into the networks, we normalize it with its last adjusted closing price, pt = p\u0303t/p\u0303ct\u22121 \u2212 1. We then concatenate ct with pt to form the final market information input xt for the decoder."}, {"heading": "5.2 Variational Movement Decoder", "text": "The purpose of VMD is to recurrently infer and decode the latent driven factor Z and the movement y from the encoded market information X .\nInference While latent driven factors help to depict the market status leading to stock movements, the posterior inference in the generative model shown in Eq. (2) is intractable. Following the spirit of the VAE, we use deep neural networks to fit latent distributions, i.e. the prior p\u03b8 (zt|z<t, x\u2264t) and the posterior p\u03b8 (zt|z<t, x\u2264t, yt), and sidestep the intractability through neural approximation and reparameterization (Kingma and Welling, 2013; Rezende et al., 2014). We first employ a variational approximator q\u03c6 (zt|z<t, x\u2264t, yt) for the intractable posterior. We observe the following factorization,\nq\u03c6 (Z|X, y) = T\u220f\nt=1\nq\u03c6 (zt|z<t, x\u2264t, yt) . (9)\nNeural approximation aims at minimizing the Kullback-Leibler divergence between the q\u03c6 (Z|X, y) and p\u03b8 (Z|X, y). Instead of optimizing it directly, we observe that the following equation naturally holds,\nlog p\u03b8 (y|X) (10) =DKL [q\u03c6 (Z|X, y) \u2016 p\u03b8 (Z|X, y)] +Eq\u03c6(Z|X,y) [log p\u03b8 (y|X,Z)] \u2212DKL [q\u03c6 (Z|X, y) \u2016 p\u03b8 (Z|X)]\nwhere DKL [q \u2016 p] is the Kullback-Leibler divergence between the distributions q and p. Therefore, we equivalently maximize the following variational recurrent lower bound by plugging Eq. (2, 9) into Eq. (10),\nL (\u03b8, \u03c6;X, y) (11)\n=\nT\u2211\nt=1\nEq\u03c6(zt|z<t,x\u2264t,yt) { log p\u03b8 (yt|x\u2264t, z\u2264t)\u2212\nDKL [q\u03c6 (zt|z<t, x\u2264t, yt) \u2016 p\u03b8 (zt|z<t, x\u2264t)] }\n\u2264 log p\u03b8 (y|X)\nwhere the likelihood term\np\u03b8 (yt|x\u2264t, z\u2264t) = { p\u03b8 (yt|x\u2264t, zt) , if t < T p\u03b8 (yT |X,Z) , if t = T.\n(12) Li et al. (2017) also provide a lower bound for inferring directly-connected recurrent latent variables in text summarization. In their work, priors are modeled with p\u03b8 (zt) \u223c N (0, I), which, in fact, turns the KL term into a static regularization term encouraging sparsity. In Eq. (11), we provide a more theoretically rigorous lower bound where the KL term with p\u03b8 (zt|z<t, x\u2264t) plays a dynamic role in inferring dependent latent variables for every different model input and latent history.\nDecoding As per time series, VMD adopts an RNN with a GRU cell to extract features and decode stock signals recurrently,\nhst = GRU(xt, h s t\u22121). (13)\nWe let the approximator q\u03c6 (zt|z<t, x\u2264t, yt) subject to a standard multivariate Gaussian distribution N (\u00b5, \u03b42I). We calculate \u00b5 and \u03b4 as\n\u00b5t = W \u03c6 z,\u00b5h z t + b \u03c6 \u00b5 (14)\nlog \u03b42t = W \u03c6 z,\u03b4h z t + b \u03c6 \u03b4 (15)\nand the shared hidden representation hzt as\nhzt = tanh(W \u03c6 z [zt\u22121, xt, h s t , yt] + b \u03c6 z ) (16)\nwhere W \u03c6z,\u00b5,W \u03c6 z,\u03b4,W \u03c6 z are weight matrices and b\u03c6\u00b5, b \u03c6 \u03b4 , b \u03c6 z are biases.\nSince Gaussian distribution belongs to the \u201clocation-scale\u201d distribution family, we can further reparameterize zt as\nzt = \u00b5t + \u03b4t (17)\nwhere denotes an element-wise product. The noise term \u223c N (0, I) naturally involves stochastic signals in our model.\nSimilarly, We let the prior p\u03b8 (zt|z<t, x\u2264t) \u223c N (\u00b5\u2032, \u03b4\u20322I). Its calculation is the same as that of the posterior except the absence of yt and independent model parameters,\n\u00b5\u2032t = W \u03b8 o,\u00b5h z t \u2032 + b\u03b8\u00b5 (18)\nlog \u03b4\u2032 2 t = W \u03b8 o,\u03b4h z t \u2032 + b\u03b8\u03b4 (19)\nwhere\nhzt \u2032 = tanh(W \u03b8z [zt\u22121, xt, h s t ] + b \u03b8 z). (20)\nFollowing Zhang et al. (2016), differently from the posterior, we set the prior zt = \u00b5\u2032t during decoding. Finally, we integrate deterministic features and the final prediction hypothesis is given as\ngt = tanh(Wg[xt, h s t , zt] + bg) (21)\ny\u0303t = \u03b6(Wygt + by), t < T (22)\nwhere Wg,Wy are weight matrices and bg, by are biases. The softmax function \u03b6(\u00b7) outputs the confidence distribution over up and down. As introduced in Section 4, the decoding of the main target yT depends on z<T and thus lies at the interface between VMD and ATA. We will elaborate on it in the next section."}, {"heading": "5.3 Attentive Temporal Auxiliary", "text": "With the acquisition of a sequence of auxiliary predictions Y\u0303 \u2217 = [y\u03031; . . . ; y\u0303T\u22121], we incorporate two-folded auxiliary effects into the main prediction and the training objective flexibly by first introducing a shared temporal attention mechanism.\nSince each hypothesis of a temporal auxiliary contributes unequally to the main prediction and model training, as shown in Figure 3, temporal attention calculates their weights in these two contributions by employing two scoring components: an\ninformation score and a dependency score. Specifically,\nv\u2032i = w \u1d40 i tanh(Wg,iG \u2217) (23) v\u2032d = g \u1d40 T tanh(Wg,dG \u2217) (24)\nv\u2217 = \u03b6(v\u2032i v\u2032d) (25)\nwhere Wg,i,Wg,d \u2208 Rdg\u00d7dg , wi \u2208 Rdg\u00d71 are model parameters. The integrated representations G\u2217 = [g1; . . . ; gT\u22121] and gT are reused as the final representations of temporal market information. The information score v\u2032i evaluates historical trading days as per their own information quality, while the dependency score v\u2032d captures their dependencies with our main target. We integrate the two and acquire the final normalized attention weight v\u2217 \u2208 R1\u00d7(T\u22121) by feeding their elementwise product into the softmax function.\nAs a result, the main prediction can benefit from temporally-close hypotheses have been made and we decode our main hypothesis y\u0303T as\ny\u0303T = \u03b6(WT [Y\u0303 \u2217v\u2217\u1d40, gT ] + bT ) (26)\nwhere WT is a weight matrix and bT is a bias. As to the model objective, we use the Monte Carlo method to approximate the expectation term in Eq. (11) and typically only one sample is used for gradient computation. To incorporate varied temporal importance at the objective level, we first break down the approximated L into a series of temporal objectives f \u2208 RT\u00d71 where ft comprises a likelihood term and a KL term for a trading day t,\nft = log p\u03b8 (yt|x\u2264t, z\u2264t) (27) \u2212 \u03bbDKL [q\u03c6 (zt|z<t, x\u2264t, yt) \u2016 p\u03b8 (zt|z<t, x\u2264t)]\nwhere we adopt the KL term annealing trick (Bowman et al., 2016; Semeniuta et al., 2017) and add a linearly-increasing KL term weight \u03bb \u2208 (0, 1] to gradually release the KL regularization effect in the training procedure. Then we reuse v\u2217 to build the final temporal weight vector v \u2208 R1\u00d7T ,\nv = [\u03b1v\u2217, 1] (28)\nwhere 1 is for the main prediction and we adopt the auxiliary weight \u03b1 \u2208 [0, 1] to control the overall auxiliary effects on the model training. \u03b1 is tuned on the development set and its effects will be discussed at length in Section 6.5. Finally, we write the training objective F by recomposition,\nF (\u03b8, \u03c6;X, y) = 1 N\nN\u2211\nn\nv(n)f (n) (29)\nwhere our model can learn to generalize with the selective attendance of temporal auxiliary. We take the derivative of F with respect to all the model parameters {\u03b8, \u03c6} through backpropagation for the update."}, {"heading": "6 Experiments", "text": "In this section, we detail our experimental setup and results."}, {"heading": "6.1 Training Setup", "text": "We use a 5-day lag window for sample construction and 32 shuffled samples in a batch.9 The maximal token number contained in a message and the maximal message number on a trading day are empirically set to 30 and 40, respectively, with the excess clipped. Since all tweets in the batched samples are simultaneously fed into the model, we set the word embedding size to 50 instead of larger sizes to control memory costs and make model training feasible on one single GPU (11GB memory). We set the hidden size of Message Embedding Layer to 100 and that of VMD to 150. All weight matrices in the model are initialized with the fan-in trick and biases are initialized with zero. We train the model with an Adam optimizer (Kingma and Ba, 2014) with the initial learning rate of 0.001. Following Bowman et al. (2016), we\n9Typically the lag size is set between 3 and 10. As introduced in Section 4, trading days are treated as basic units in StockNet and 3 calendar days are thus too short to guarantee the existence of more than one trading day in a lag, e.g. the prediction for the movement of Monday. We also experiment with 7 and 10 but they do not yield better results than 5.\nuse the input dropout rate of 0.3 to regularize latent variables. Tensorflow (Abadi et al., 2016) is used to construct the computational graph of StockNet and hyper-parameters are tweaked on the development set."}, {"heading": "6.2 Evaluation Metrics", "text": "Following previous work for stock prediction (Xie et al., 2013; Ding et al., 2015), we adopt the standard measure of accuracy and Matthews Correlation Coefficient (MCC) as evaluation metrics. MCC avoids bias due to data skew. Given the confusion matrix ( tp fn fp tn ) containing the number of samples classified as true positive, false positive, true negative and false negative, MCC is calculated as\nMCC = tp\u00d7 tn\u2212 fp\u00d7 fn\u221a\n(tp + fp)(tp + fn)(tn + fp)(tn + fn) .\n(30)"}, {"heading": "6.3 Baselines and Proposed Models", "text": "We construct the following five baselines in different genres,10\n\u2022 RAND: a naive predictor making random guess in up or down. \u2022 ARIMA: Autoregressive Integrated Moving\nAverage, an advanced technical analysis method using only price signals (Brown, 2004) . \u2022 RANDFOREST: a discriminative Random For-\nest classifier using Word2vec text representations (Pagolu et al., 2016). \u2022 TSLDA: a generative topic model jointly\nlearning topics and sentiments (Nguyen and Shirai, 2015). \u2022 HAN: a state-of-the-art discriminative deep\nneural network with hierarchical attention (Hu et al., 2018).\nTo make a detailed analysis of all the primary components in StockNet, in addition to HEDGEFUNDANALYST, the fully-equipped StockNet, we also construct the following four variations, \u2022 TECHNICALANALYST: the generative StockNet\nusing only historical prices. \u2022 FUNDAMENTALANALYST: the generative Stock-\nNet using only tweet information. \u2022 INDEPENDENTANALYST: the generative Stock-\nNet without temporal auxiliary targets.\n10We do not treat event-driven models as comparable methods since our model uses no event pre-extraction tool.\n\u2022 DISCRIMINATIVEANALYST: the discriminative StockNet directly optimizing the likelihood objective. Following Zhang et al. (2016), we set zt = \u00b5\u2032t to take out the effects of the KL term."}, {"heading": "6.4 Results", "text": "Since stock prediction is a challenging task and a minor improvement usually leads to large potential profits, the accuracy of 56% is generally reported as a satisfying result for binary stock movement prediction (Nguyen and Shirai, 2015). We show the performance of the baselines and our proposed models in Table 1. TLSDA is the best baseline in MCC while HAN is the best baseline in accuracy. Our model, HEDGEFUNDANALYST achieves the best performance of 58.23 in accuracy and 0.080796 in MCC, outperforming TLSDA and HAN with 4.16, 0.59 in accuracy, and 0.015414, 0.028996 in MCC, respectively.\nThough slightly better than random guess, classic technical analysis, e.g. ARIMA, does not yield satisfying results. Similar in using only historical prices, TECHNICALANALYST shows an obvious advantage in this task compared ARIMA. We believe there are two major reasons: (1) TECHNICALANALYST learns from training data and incorporates more flexible non-linearity; (2) our test set contains a large number of stocks while ARIMA is more sensitive to peculiar sequence stationarity. It is worth noting that FUNDAMENTALANA-\nLYST gains exceptionally competitive results with only 0.009092 less in MCC than HEDGEFUNDANALYST. The performance of FUNDAMENTALANALYST and TECHNICALANALYST confirm the positive effects from tweets and historical prices in stock movement prediction, respectively. As an effective ensemble of the two market information, HEDGEFUNDANALYST gains even better performance.\nCompared with DISCRIMINATIVEANALYST, the performance improvements of HEDGEFUNDANALYST are not from enlarging the networks, demonstrating that modeling underlying market status explicitly with latent driven factors indeed benefits stock movement prediction. The comparison with INDEPENDENTANALYST also shows the effectiveness of capturing temporal dependencies between predictions with the temporal auxiliary. However, the effects of the temporal auxiliary are more complex and will be analyzed further in the next section."}, {"heading": "6.5 Effects of Temporal Auxiliary", "text": "We provide a detailed discuss of how the temporal auxiliary affects model performance. As introduced in Eq. (28), the temporal auxiliary weight \u03b1 controls the overall effects of the objective-level temporal auxiliary to our model. Figure 4 presents how the performance of HEDGEFUNDANALYST and DISCRIMINATIVEANALYST fluctuates with \u03b1.\nAs shown in Figure 4, enhanced by the temporal auxiliary, HEDGEFUNDANALYST approaches the best performance at 0.5, and DISCRIMINATIVEANALYST\nachieves its maximum at 0.7. In fact, objectivelevel auxiliary can be regarded as a denoising regularizer: for a sample with a specific movement as the main target, the market source in the lag can be heterogeneous, e.g. affected by bad news, tweets on earlier days are negative but turn to positive due to timely crises management. Without temporal auxiliary tasks, the model tries to identify positive signals on earlier days only for the main target of rise movement, which is likely to result in pure noise. In such cases, temporal auxiliary tasks help to filter market sources in the lag as per their respective aligned auxiliary movements. Besides, from the perspective of training variational models, the temporal auxiliary helps HEDGEFUNDANALYST to encode more useful information into the latent driven factor Z, which is consistent with recent research in VAEs (Semeniuta et al., 2017). Compared with HEDGEFUNDANALYST that contains a KL term performing dynamic regularization, DISCRIMINATIVEANALYST requires stronger regularization effects coming with a bigger \u03b1 to achieve its best performance.\nSince y\u2217 also involves in generating yT through the temporal attention, tweaking \u03b1 acts as a tradeoff between focusing on the main target and generalizing by denoising. Therefore, as shown in Figure 4, our models do not linearly benefit from incorporating temporal auxiliary. In fact, the two models follow a similar pattern in terms of performance change: the curves first drop down with the increase of \u03b1, except the MCC curve for DISCRIMINATIVEANALYST rising up temporarily at 0.3. After that, the curves ascend abruptly to their maximums, then keep descending till \u03b1 = 1. Though the start phase of increasing \u03b1 even leads to worse performance, when auxiliary effects are properly introduced, the two models finally gain better results than those with no involvement of auxiliary effects, e.g. INDEPENDENTANALYST."}, {"heading": "7 Conclusion", "text": "We demonstrated the effectiveness of deep generative approaches for stock movement prediction from social media data by introducing StockNet, a neural network architecture for this task. We tested our model on a new comprehensive dataset and showed it performs better than strong baselines, including implementation of previous work. Our comprehensive dataset is publicly available at https://github.com/\nyumoxu/stocknet-dataset."}, {"heading": "Acknowledgments", "text": "The authors would like to thank the three anonymous reviewers and Miles Osborne for their helpful comments. This research was supported by a grant from Bloomberg and by the H2020 project SUMMA, under grant agreement 688139."}], "year": 2018, "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "authors": ["Mart\u0131\u0301n Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin"], "year": 2016}, {"title": "Natural language processing with Python: analyzing text with the natural language toolkit", "authors": ["Steven Bird", "Ewan Klein", "Edward Loper."], "venue": "O\u2019Reilly Media, Inc.", "year": 2009}, {"title": "Twitter mood predicts the stock market", "authors": ["Johan Bollen", "Huina Mao", "Xiaojun Zeng."], "venue": "Journal of computational science 2(1):1\u20138.", "year": 2011}, {"title": "Generating sentences from a continuous space", "authors": ["Samuel R Bowman", "Luke Vilnis", "Oriol Vinyals", "Andrew Dai", "Rafal Jozefowicz", "Samy Bengio."], "venue": "Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning.", "year": 2016}, {"title": "Smoothing, forecasting and prediction of discrete time series", "authors": ["Robert Goodell Brown."], "venue": "Courier Corporation.", "year": 2004}, {"title": "Multitask learning", "authors": ["Rich Caruana."], "venue": "Learning to learn, Springer, pages 95\u2013133.", "year": 1998}, {"title": "Using structured events to predict stock price movement: An empirical investigation", "authors": ["Xiao Ding", "Yue Zhang", "Ting Liu", "Junwen Duan."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. Doha, Qatar, pages", "year": 2014}, {"title": "Deep learning for event-driven stock prediction", "authors": ["Xiao Ding", "Yue Zhang", "Ting Liu", "Junwen Duan."], "venue": "Proceedings of the 24th International Conference on Artificial Intelligence. Buenos Aires, Argentina, pages 2327\u20132333.", "year": 2015}, {"title": "Technical analysis of stock trends", "authors": ["Robert D Edwards", "WHC Bassetti", "John Magee."], "venue": "CRC press.", "year": 2007}, {"title": "Financial markets and monetary policy", "authors": ["Jeffrey A Frankel."], "venue": "MIT Press.", "year": 1995}, {"title": "Listening to chaotic whispers", "authors": ["Tie-Yan Liu"], "year": 2018}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "authors": ["Danilo Jimenez Rezende", "Shakir Mohamed", "Daan Wierstra."], "venue": "Proceedings of the 31th International Conference on Machine Learning. Beijing, China, pages 1278\u2013", "year": 2014}, {"title": "Textual analysis of stock market prediction using breaking financial news: The azfin text system", "authors": ["Robert P Schumaker", "Hsinchun Chen."], "venue": "ACM Transactions on Information Systems 27(2):12.", "year": 2009}, {"title": "A hybrid convolutional variational autoencoder for text generation", "authors": ["Stanislau Semeniuta", "Aliaksei Severyn", "Erhardt Barth."], "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Copenhagen, Denmark,", "year": 2017}, {"title": "Exploiting topic based twitter sentiment for stock prediction", "authors": ["Jianfeng Si", "Arjun Mukherjee", "Bing Liu", "Qing Li", "Huayi Li", "Xiaotie Deng."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short", "year": 2013}, {"title": "Semantic frames to predict stock price movement", "authors": ["Boyi Xie", "Rebecca J Passonneau", "Leon Wu", "Germ\u00e1n G Creamer."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics. Sofia, Bulgaria, volume 1,", "year": 2013}, {"title": "Variational neural machine translation", "authors": ["Biao Zhang", "Deyi Xiong", "Hong Duan", "Min Zhang"], "venue": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Austin, Texas,", "year": 2016}], "id": "SP:936b21b76a888f3c3ee9f7ddd4f2cef2bbbd29bd", "authors": [{"name": "Yumo Xu", "affiliations": []}, {"name": "Shay B. Cohen", "affiliations": []}], "abstractText": "Stock movement prediction is a challenging problem: the market is highly stochastic, and we make temporally-dependent predictions from chaotic data. We treat these three complexities and present a novel deep generative model jointly exploiting text and price signals for this task. Unlike the case with discriminative or topic modeling, our model introduces recurrent, continuous latent variables for a better treatment of stochasticity, and uses neural variational inference to address the intractable posterior inference. We also provide a hybrid objective with temporal auxiliary to flexibly capture predictive dependencies. We demonstrate the stateof-the-art performance of our proposed model on a new stock movement prediction dataset which we collected.1", "title": "Stock Movement Prediction from Tweets and Historical Prices"}