33	28	Section 7 concludes the article.
48	17	An n × n matrix M is called a structured matrix when it has a low displacement rank γ (Pan, 2001).
49	30	More precisely, with the proper choice of operator matrices A and B, if the Sylvester displacement ∇A,B(M) := AM−MB (1) and the Stein displacement ∆A,B(M) := M− AMB (2) of matrix M have a rank γ bounded by a value that is independent of the size of M, then matrix M is referred to as a matrix with a low displacement rank (Pan, 2001).
51	7	Even a full-rank matrix may have small displacement rank with appropriate choice of displacement operators (A,B).
52	21	Figure 1 illustrates a series of commonly used structured matrices, including a circulant matrix, a Cauchy matrix, a Toeplitz matrix, a Hankel matrix, and a Vandermonde matrix, and Table 1 summarizes their displacement ranks and corresponding displacement operators.
53	14	The general procedure of handling LDR matrices generally takes three steps: Compression, Computation with Displacements, Decompression.
55	70	In particular, if one of the displacement operator has the property that its power equals the identity matrix, then one can use the following method to decompress directly: Lemma 3.1.
56	43	If A is an a-potent matrix (i.e., Aq = aI for some positive integer q ≤ n), then M = [ q−1∑ k=0 Ak∆A,B(M)Bk ] (I− aBq)−1.
57	24	See Corollary 4.3.7 in (Pan, 2001).
58	21	One of the most important characteristics of structured matrices is their low number of independent variables.
59	50	The number of independent parameters is O(n) for an n-byn structured matrix instead of the order of n2, which indicates that the storage complexity can be potentially reduced to O(n).
60	46	Besides, the computational complexity for many matrix operations, such as matrix-vector multiplication, matrix inversion, etc., can be significantly reduced when operating on the structured ones.
63	11	For certain lemmas and theorems such as Lemma 3.1, only the form on n × n square matrices is needed for the derivation procedure in this paper.
66	26	Without loss of generality, we focus on a feed-forward neural network with one fully-connected (hidden) layer, which is similar network setup as (Cybenko, 1989).
72	9	In this paper, we aim at providing theoretical support on the accuracy of function approximation using LDR neural networks, which represents the “effectiveness” of LDR neural networks compared with the original neural networks.
74	23	(5) • Fix a positive integer n, find an upper bound so that for any continuous function f(x) there exists a bias vector θ and an LDR matrix with at most n rows satisfying equation (5).
75	16	• Find a multi-layer LDR neural network that achieves error bound (5) but with fewer parameters.
76	96	The first task is handled in Section 4, which is the universal approximation property of LDR neural networks.
77	42	It states that the LDR neural networks could approximate an arbitrary continuous function arbitrarily well and is the underpinning of the widespread applications.
78	19	The error bounds for shallow and deep neural networks are derived in Section 5.
79	5	In addition, we derived explicit back-propagation expressions for LDR neural networks in Section 6.
80	2	We call a family of matrices S to have representation property if for any vector v ∈ Rn, there exists a matrix M ∈ SA,B such that v is a column of M .
82	3	1 have this representation property because of their explicit pattern.
83	8	In this section we will prove that this property also holds for many other LDR families.
84	34	Based on this result, we are able to prove the universal approximation property of neural networks utilizing only LDR matrices.
