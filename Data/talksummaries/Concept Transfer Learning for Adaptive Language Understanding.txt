0	16	The language understanding (LU) module is a key component of dialogue system (DS), parsing user’s utterances into corresponding semantic concepts (or semantic slots 1).
1	59	For example, the utterance “Show me flights from Boston to New York” can be parsed into (from city=Boston, to city=New York) (Pieraccini et al., 1992).
3	29	the rest of this paper to some extent.
4	56	With sufficient in-domain data and deep learning models (e.g. recurrent neural networks, bidirectional long-short term memory network), statistical methods have achieved satisfactory performance in the slot filling task recently (Kurata et al., 2016; Vu, 2016; Liu and Lane, 2016).
9	26	Each semantic slot is composed of different atomic concepts, e.g. slot “from city” can be defined as a tuple of atoms [“from location”,“city name”], and “date of birth” can be defined as [“date”,“birth”].
13	29	For comparison, we also introduce a competitive method which automatically learns slot representation from the word sequence of each slot name.
41	27	To solve this problem, we propose to use atomic concepts to represent the semantic slots.
45	16	We propose a criteria to construct atomic concept manually.
53	26	They should be located in the first dimension.
54	73	“from location” depends on the context like a pattern of “a flight leaves [city name]”, which should be in the second dimension.
74	46	The slot is indicated as an atomic concept tuple based on hierarchical concept structure.
82	23	(b) Atomic concept dependent Atomic concepts can also be regarded dependently (i.e. ACD) so that atomic concept prediction depends on the former predicted results.
92	20	We didn’t perform any post-processing but considered the unseen slot as a wrong prediction.
94	20	The literal description of slot used in this paper is the word sequence of each slot name, which can be obtained automatically.
96	16	Therefore, it is very meaningful to compare this method with the atomic concept tuples involving human knowledge.
101	16	In other words, the slot tag si is broken down into IOBi and slot name SNi, e.g. the slot tag “B-from city” is split into “B” and “from city”.
106	31	We define p(SNi|hi) = softmax(W · hi)T δSNi where W ∈ RA×B is a matrix, hi ∈ RB is a vector, A is the number of all different slot names.
133	20	ATIS X test is synthesized from the standard ATIS test set by randomly replacing the value of each slot with an unseen one.
134	18	The unseen value sets are collected from the training set according to bottom-level concepts (e.g. “city name”, “airport name”).
135	17	For example, if the value set of “from city” is {“New York”, “Boston”} and the value set of “to city” is {“Boston”}, then the unseen value for “to city” is “New York”.
136	23	The test sentence “Flights to [xx:to city]” can be replaced to “Flights to [New York:to city]”.
158	45	Moreover, ACD predicts the atomic concepts dependently, gains 0.50% (significant level 95%) over the AC.
159	58	Worth to mention that ACD achieves a new state-of-the-art performance of the standard slot-tagging task on the ATIS dataset, with only the lexicon features 3.
162	72	From Table 2, we can see that: 1) The plain slot filling models (PS, Encoder-decoder) are not on par with other models.
163	19	2) The atomic-concepts based slot filling gets a slight improvement over the PS with dict-feats, considering the concepts independently (AC).
165	25	4) The method based on slot name embedding (described in Section 5) achieves a slight improvement than AC, which implies that it is possible to reveal the relationship between slots automatically.
167	37	The word of “late” is never covered by the slot “period of day” in the training set.
169	24	Luckily, the “late” is covered by the family of the slot “period of day” in the training set, e.g. “arrive time.period of day”.
183	27	We use transcripts as input, and make slot-value alignment by string matching simply.
184	24	The experimental settings are similar to the ATIS’s, whereas the seed data in DSTC 3 is also used for validation.
