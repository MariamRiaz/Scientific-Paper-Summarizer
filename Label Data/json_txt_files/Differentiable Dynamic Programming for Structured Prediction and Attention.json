{"sections": [{"text": "structured combinatorial problems by iteratively breaking them down into smaller subproblems. In spite of their versatility, many DP algorithms are non-differentiable, which hampers their use as a layer in neural networks trained by backpropagation. To address this issue, we propose to smooth the max operator in the dynamic programming recursion, using a strongly convex regularizer. This allows to relax both the optimal value and solution of the original combinatorial problem, and turns a broad class of DP algorithms into differentiable operators. Theoretically, we provide a new probabilistic perspective on backpropagating through these DP operators, and relate them to inference in graphical models. We derive two particular instantiations of our framework, a smoothed Viterbi algorithm for sequence prediction and a smoothed DTW algorithm for time-series alignment. We showcase these instantiations on structured prediction (audio-to-score alignment, NER) and on structured and sparse attention for translation.\nModern neural networks are composed of multiple layers of nested functions. Although layers usually consist of elementary linear algebraic operations and simple nonlinearities, there is a growing need for layers that output the value or the solution of an optimization problem. This can be used to design loss functions that capture relevant regularities in the input (Lample et al., 2016; Cuturi & Blondel, 2017) or to create layers that impose prior structure on the output (Kim et al., 2017; Amos & Kolter, 2017; Niculae & Blondel, 2017; Djolonga & Krause, 2017).\nAmong these works, several involve a convex optimization\n1Inria, CEA, Universite\u0301 Paris-Saclay, Gif-sur-Yvette, France. Work performed at 2NTT Communication Science Laboratories, Kyoto, Japan. Correspondence to: AM <arthur.mensch@m4x.org>, MB <mathieu@mblondel.org>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nproblem (Amos & Kolter, 2017; Niculae & Blondel, 2017; Djolonga & Krause, 2017); others solve certain combinatorial optimization problems by dynamic programming (Kim et al., 2017; Cuturi & Blondel, 2017; Nowak et al., 2018). However, because dynamic programs (Bellman, 1952) are usually non-differentiable, virtually all these works resort to the formalism of conditional random fields (CRFs) (Lafferty et al., 2001), which can be seen as changing the semiring used by the dynamic program \u2014 replacing all values by their exponentials and all (max,+) operations with (+,\u00d7) operations (Verdu & Poor, 1987). While this modification smoothes the dynamic program, it looses the sparsity of solutions, since hard assignments become soft ones. Moreover, a general understanding of how to relax and differentiate dynamic programs is lacking. In this work, we propose to do so by leveraging smoothing (Moreau, 1965; Nesterov, 2005) and backpropagation (Linnainmaa, 1970). We make the following contributions.\n1) We present a unified framework for turning a broad class of dynamic programs (DP) into differentiable operators. Unlike existing works, we propose to change the semiring to use (max\u2126,+) operations, where max\u2126 is a max operator smoothed with a strongly convex regularizer \u2126 (\u00a71).\n2) We show that the resulting DP operators, that we call DP\u2126, are smoothed relaxations of the original DP algorithm and satisfy several key properties, chief among them convexity. In addition, we show that their gradient,\u2207DP\u2126, is equal to the expected trajectory of a certain random walk and can be used as a sound relaxation to the original dynamic program\u2019s solution. Using negative entropy for \u2126 recovers existing CRF-based works from a different perspective \u2014 we provide new arguments as to why this \u2126 is a good choice. On the other hand, using squared \u21132 norm for \u2126 leads to new algorithms whose expected solution is sparse. We derive a clean and efficient method to backpropagate gradients, both through DP\u2126 and \u2207DP\u2126. This allows us to define differentiable DP layers that can be incorporated in neural networks trained end-to-end (\u00a72).\n3) We illustrate how to to derive two particular instantiations of our framework, a smoothed Viterbi algorithm for sequence prediction and a smoothed DTW algorithm for supervised time-series alignment (\u00a73). The latter is illustrated in Figure 1. Finally, we showcase these two instanti-\nations on structured prediction tasks (\u00a74) and on structured attention for neural machine translation (\u00a75).\nNotation. We denote scalars, vectors and matrices using lower-case, bold lower-case and bold upper-case letters, e.g., y, y and Y . We denote the elements of Y by yi,j and its rows by yi. We denote the Frobenius inner product between A and B by \u3008A,B\u3009 , \u2211\ni,j ai,jbi,j . We denote the\n(D\u2212 1)-probability simplex by\u25b3D , {\u03bb \u2208 RD+ : \u2016\u03bb\u20161 = 1}. We write conv(Y) , { \u2211\nY \u2208Y \u03bbY Y : \u03bb \u2208 \u25b3 |Y|} the\nconvex hull of Y , [N ] the set {1, . . . , N} and supp(x) , {j \u2208 [D] : xj 6= 0} the support of x \u2208 R D. We denote the Shannon entropy by H(q) , \u2211\ni qi log qi.\nWe have released an optimized and modular PyTorch implementation for reproduction and reuse."}, {"heading": "1. Smoothed max operators", "text": "In this section, we introduce smoothed max operators (Nesterov, 2005; Beck & Teboulle, 2012; Niculae & Blondel, 2017), that will serve as a powerful and generic abstraction to define differentiable dynamic programs in \u00a72. Let \u2126 : RD \u2192 R be a strongly convex function on\u25b3D and let x \u2208 RD. We define the max operator smoothed by \u2126 as:\nmax\u2126(x) , max q\u2208\u25b3D \u3008q,x\u3009 \u2212 \u2126(q). (1)\nIn other words, max\u2126 is the convex conjugate of \u2126, restricted to the simplex. From the duality between strong convexity and smoothness, max\u2126 is smooth: differentiable everywhere and with Lipschitz continuous gradient. Since the argument that achieves the maximum in (1) is unique,\nfrom Danskin\u2019s theorem (1966), it is equal to the gradient:\n\u2207max\u2126(x) = argmax q\u2208\u25b3D \u3008q,x\u3009 \u2212 \u2126(q).\nThe gradient is differentiable almost everywhere for any strongly-convex \u2126 (everywhere for negentropy). Next, we state properties that will be useful throughout this paper.\nLemma 1. Properties of max\u2126 operators\nLet x = (x1, . . . , xD) \u22a4 \u2208 RD.\n1. Boundedness: If \u2126 is lower-bounded by L\u2126,D and upper-bounded by U\u2126,D on the simplex\u25b3 D, then\nmax(x)\u2212 U\u2126,D \u2264 max\u2126(x) \u2264 max(x)\u2212 L\u2126,D.\n2. Distributivity of + over max\u2126: max\u2126(x+ c1) = max\u2126(x) + c \u2200c \u2208 R.\n3. Commutativity: If \u2126(Pq) = \u2126(q), where P is a permutation matrix, then max\u2126(Px) = max\u2126(x).\n4. Non-decreasingness in each coordinate:\nmax\u2126(x) \u2264 max\u2126(y) \u2200x \u2264 y\n5. Insensitivity to \u2212\u221e: xj = \u2212\u221e\u21d2 \u2207max\u2126(x)j = 0.\nProofs are given in \u00a7A.1. In particular, property 3 holds whenever \u2126(q) =\n\u2211D i=1 \u03c9(qi), for some function \u03c9. We\nfocus in this paper on two specific regularizers \u2126: the negentropy \u2212H and the squared \u21132 norm. For these choices, all properties above are satisfied and we can derive closedform expressions for max\u2126, its gradient and its Hessian \u2014 see \u00a7B.1. When using negentropy, max\u2126 becomes the log-sum-exp and\u2207max\u2126 the softmax. The former satisfies associativity, which as we shall see, makes it natural to use in dynamic programming. With the squared \u21132 regularization, as observed by Martins & Astudillo (2016); Niculae & Blondel (2017), the gradient\u2207max\u2126 is sparse. This will prove useful to enforce sparsity in the models we study."}, {"heading": "2. Differentiable DP layers", "text": "Dynamic programming (DP) is a generic way of solving combinatorial optimization problems by recursively solving problems on smaller sets. We first introduce this category of algorithms in a broad setting, then use smoothed max operators to define differentiable DP layers."}, {"heading": "2.1. Dynamic programming on a DAG", "text": "Every problem solved by dynamic programming reduces to finding the highest-scoring path between a start node and an end node, on a weighted directed acyclic graph (DAG). We therefore introduce our formalism on this generic problem, and give concrete examples in \u00a73.\nFormally, let G = (V, E) be a DAG, with nodes V and edges E . We write N = |V| \u2265 2 the number of nodes.\nWithout loss of generality, we number the nodes in topological order, from 1 (start) to N (end), and thus V = [N ]. Node 1 is the only node without parents, and node N the only node without children. Every directed edge (i, j) from a parent node j to a child node i has a weight \u03b8i,j \u2208 R. We gather the edge weights in a matrix \u03b8 \u2208 \u0398 \u2286 RN\u00d7N , setting \u03b8i,j = \u2212\u221e if (i, j) /\u2208 E and \u03b81,1 = 1. We consider the set Y of all paths in G from node 1 to node N . Any path Y \u2208 Y can be represented as a N \u00d7 N binary matrix, with yi,j = 1 if the path goes through the edge (i, j) and yi,j = 0 otherwise. In the sequel, paths will have a one-to-one correspondence with discrete structures such as sequences or alignments. Using this representation, \u3008Y ,\u03b8\u3009 corresponds to the cumulated sum of edge weights, along the path Y . The computation of the highest score among all paths amounts to solving the combinatorial problem\nLP(\u03b8) , max Y \u2208Y \u3008Y ,\u03b8\u3009 \u2208 R. (2)\nAlthough the size of Y is in general exponential in N , LP(\u03b8) can be computed in one topologically-ordered pass over G using dynamic programming. We let Pi be the set of parent nodes of node i in graph G and define recursively\nv1(\u03b8) , 0\n\u2200 i \u2208 [2, . . . , N ] : vi(\u03b8) , max j\u2208Pi \u03b8i,j + vj(\u03b8). (3)\nThis algorithm outputs DP(\u03b8) , vN (\u03b8). We now show that this is precisely the highest score among all paths.\nProposition 1. Optimality of dynamic programming\n\u2200\u03b8 \u2208 \u0398 : DP(\u03b8) = LP(\u03b8)\nThe optimality of recursion (3) is well-known (Bellman, 1952). We prove it again with our formalism in \u00a7A.2, since it exhibits the two key properties that the max operator must satisfy to guarantee optimality: distributivity of + over it and associativity. The cost of computing DP(\u03b8) is O(|E|), which is exponentially better than O(|Y|).\nIn many applications, we will often rather be interested in the argument that achieves the maximum, i.e., one of the highest-scoring paths\nY \u22c6(\u03b8) \u2208 argmax Y \u2208Y \u3008Y ,\u03b8\u3009. (4)\nThis argument can be computed by backtracking, that we now relate to computing subgradients of LP(\u03b8).\nLinear program, lack of differentiality. Unfortunately, LP(\u03b8) is not differentiable everywhere. To see why this is the case, notice that (2) can be rewritten as a linear program over the convex polytope conv(Y):\nLP(\u03b8) = max Y \u2208conv(Y) \u3008Y ,\u03b8\u3009.\nFrom the generalized Danskin theorem (Bertsekas, 1971),\nY \u22c6(\u03b8) \u2208 \u2202LP(\u03b8) = argmax Y \u2208conv(Y) \u3008Y ,\u03b8\u3009,\nwhere \u2202 denotes the subdifferential of LP(\u03b8), i.e., the set of subgradients. When Y \u22c6(\u03b8) is unique, \u2202LP(\u03b8) is a singleton and Y \u22c6 is equal to the gradient of LP(\u03b8), that we write\u2207LP(\u03b8). Unfortunately, Y \u22c6(\u03b8) is not always unique, meaning that LP(\u03b8) is not differentiable everywhere. As we will show in \u00a74.2, this hinders optimization as we can only train models involving LP(\u03b8) with subgradient methods. Worse, Y \u22c6(\u03b8), a function from \u0398 to Y , is discontinuous and has null or undefined derivatives. It is thus impossible to use it in a model trained by gradient descent."}, {"heading": "2.2. Smoothed max layers", "text": "To address the lack of differentiability of dynamic programming, we introduce the operator max\u2126, presented in \u00a71, and consider two approaches.\nSmoothing the linear program. Let us define the \u2126smoothed maximum of a function f : Y \u2192 R over a finite set Y using the following shorthand notation:\nmax\u2126 Y \u2208Y f(Y ) , max\u2126((f(Y ))Y \u2208Y).\nA natural way to circumvent the lack of differentiability of LP(\u03b8) is then to replace the global max operator by max\u2126:\nLP\u2126(\u03b8) , max\u2126 Y \u2208Y \u3008Y ,\u03b8\u3009 \u2208 R. (5)\nFrom \u00a71, LP\u2126(\u03b8) is convex and, as long as \u2126 is strongly convex, differentiable everywhere. In addition, \u2207LP\u2126(\u03b8) is Lipschitz continuous and thus differentiable almost everywhere. Unfortunately, solving (5) for general \u2126 is likely intractable when Y has an exponential size.\nSmoothing the dynamic program. As a tractable alternative, we propose an algorithmic smoothing. Namely, we replace max by max\u2126 locally within the DP recursion. Omitting the dependence on \u2126, this defines a smoothed recursion over the new sequence (vi(\u03b8)) N i=1:\nv1(\u03b8) , 0\n\u2200i \u2208 [2, . . . , N ] : vi(\u03b8) , max\u2126 j\u2208Pi \u03b8i,j + vj(\u03b8). (6)\nThe new algorithm outputs DP\u2126(\u03b8), vN (\u03b8), the smoothed highest score. Smoothing the max operator locally brings the same benefit as before \u2014 DP\u2126(\u03b8) is smooth and \u2207DP\u2126(\u03b8) is differentiable almost everywhere. However, computing DP\u2126(\u03b8) is now always tractable, since it simply requires to evaluate (vi(\u03b8)) N i=1 in topological order, as in\nthe original recursion (3). Although LP\u2126(\u03b8) and DP\u2126(\u03b8) are generally different (in fact, LP\u2126(\u03b8) \u2265 DP\u2126(\u03b8) for all \u03b8 \u2208 \u0398), we now show that DP\u2126(\u03b8) is a sensible approximation of LP(\u03b8) in several respects.\nProposition 2. Properties of DP\u2126\n1. DP\u2126(\u03b8) is convex\n2. LP(\u03b8)\u2212DP\u2126(\u03b8) is bounded above and below: it lies in (N \u2212 1)[L\u2126,N , U\u2126,N ], with Lemma 1 notations.\n3. When \u2126 is separable, DP\u2126(\u03b8) = LP\u2126(\u03b8) if and only if \u2126 = \u2212\u03b3H , where \u03b3 \u2265 0.\nProofs are given in \u00a7A.3. The first claim can be surprising due to the recursive definition of DP\u2126(\u03b8). The second claim implies that DP\u03b3\u2126(\u03b8) converges to LP(\u03b8) when the regularization vanishes: DP\u03b3\u2126(\u03b8)\u2192\u03b3\u21920 LP(\u03b8); LP\u03b3\u2126(\u03b8) also satisfies this property. The \u201cif\u201d direction of the third claim follows by showing that max\u2212\u03b3H satisfies associativity. This recovers known results in the framework of message passing algorithms for probabilistic graphical models (e.g., Wainwright & Jordan, 2008, Section 4.1.3), with a more algebraic point of view. The key role that the distributive and associative properties play into breaking down large problems into smaller ones has long been noted (Verdu & Poor, 1987; Aji & McEliece, 2000). However, the \u201cand only if\u201d part of the claim is new to our knowledge. Its proof shows that max\u2212\u03b3H is the only max\u2126 satisfying associativity, exhibiting a functional equation from information theory (Horibe, 1988). While this provides an argument in favor of entropic regularization, \u211322 regularization has different benefits in terms of sparsity of the solutions."}, {"heading": "2.3. Relaxed argmax layers", "text": "It is easy to check that \u2207LP\u2126(\u03b8) belongs to conv(Y) and can be interpreted as an expected path under some distribution induced by \u2207max\u2126, over all possible Y \u2208 Y \u2014 see \u00a7A.4 for details. This makes \u2207LP\u2126(\u03b8) interpretable as a continuous relaxation of the highest-scoring path Y \u22c6(\u03b8) defined in (4). However, like LP\u2126(\u03b8), computing \u2207LP\u2126(\u03b8) is likely intractable in the general case. Fortunately, \u2207DP\u2126(\u03b8) is always easily computable by backpropagation and enjoys similar properties, as we now show.\nComputing \u2207DP\u2126(\u03b8). Computing \u2207DP\u2126(\u03b8) can be broken down into two steps. First, we compute and record the local gradients alongside the recursive step (6):\n\u2200 i \u2208 [N ] : qi(\u03b8) , \u2207max\u2126(\u03b8i + v(\u03b8)) \u2208 \u25b3 N ,\nwhere v(\u03b8) , (v1(\u03b8), . . . , vN (\u03b8)). Since we assume that \u03b8i,j = \u2212\u221e if (i, j) 6\u2208 E , we have supp(qi(\u03b8)) = Pi. This ensures that, similarly to vi(\u03b8), qi(\u03b8) exclusively depends on (vj(\u03b8))j\u2208Pi . Let Cj be the children of node j \u2208 [N ]. A\nstraighforward application of backpropagation (cf. \u00a7A.5) yields a recursion run in reverse-topological order, starting from node j = N \u2212 1 down to j = 1:\n\u2200 i \u2208 Cj : ei,j \u2190 e\u0304iqi,j then e\u0304j \u2190 \u2211\ni\u2208Cj\nei,j ,\nwhere e\u0304N \u2190 1 and ei,j \u2190 0 for (i, j) /\u2208 E . The final output is \u2207DP\u2126(\u03b8) = E. Assuming max\u2126 can be computed in linear time, the total cost is O(|E|), the same as DP(\u03b8). Pseudo-code is summarized in \u00a7A.5.\nAssociated path distribution. The backpropagation we derived has a probabilistic interpretation. Indeed, Q(\u03b8) \u2208 R N\u00d7N can be interpreted as a transition matrix: it defines a random walk on the graph G, i.e., a finite Markov chain with states V and transition probabilities supported by E . The random walk starts from node N and, when at node i, hops to node j \u2208 Pi with probability qi,j . It always ends at node 1, which is absorbing. The walk follows the path Y \u2208 Y with a probability p\u03b8,\u2126(Y ), which is simply the product of the qi,j of visited edges. Thus, Q(\u03b8) defines a path distribution p\u03b8,\u2126. Our next proposition shows that \u2207DP\u2126(Y ) \u2208 conv(Y) and is equal to the expected path E\u03b8,\u2126[Y ] under that distribution.\nProposition 3. \u2207DP\u2126(\u03b8) as an expected path\n\u2200\u03b8 \u2208 \u0398 : \u2207DP\u2126(\u03b8) = E\u03b8,\u2126[Y ] = E \u2208 conv(Y).\nProof is provided in \u00a7A.5. Moreover, \u2207DP\u2126(\u03b8) is a principled relaxation of the highest-scoring path Y \u22c6(\u03b8), in the sense that it converges to a subgradient of LP(\u03b8) as the regularization vanishes: \u2207DP\u03b3\u2126(\u03b8) \u2212\u2212\u2212\u2192\n\u03b3\u21920 Y \u22c6(\u03b8) \u2208 \u2202LP(\u03b8).\nWhen \u2126 = \u2212\u03b3H , the distributions underpinning LP\u2126(\u03b8) and DP\u2126(\u03b8) coincide and reduce to the Gibbs distribution p\u03b8,\u2126(Y ) \u221d exp(\u3008\u03b8,Y \u3009/\u03b3). The value LP\u2126(\u03b8) = DP\u2126(\u03b8) is then equal to the log partition. When \u2126 = \u03b3\u2016 \u00b7 \u20162, some transitions between nodes have zero probability and hence some paths have zero probability under the distribution p\u03b8,\u2126. Thus, \u2207DP\u2126(\u03b8) is typically sparse \u2014 this will prove interesting to introspect the various models we consider (typically, the smaller \u03b3, the sparser \u2207DP\u2126(\u03b8))."}, {"heading": "2.4. Multiplication with the Hessian\u22072DP\u2126(\u03b8)Z", "text": "Using \u2207DP\u2126(\u03b8) as a layer involves backpropagating through \u2207DP\u2126(\u03b8). This requires to apply the Jacobian of \u2207DP\u2126 operator (a linear map from R N\u00d7N to RN\u00d7N ), or in other words to apply the Hessian of DP\u2126, to an input sensibility vector Z, computing\n\u22072DP\u2126(\u03b8)Z = \u2207\u3008\u2207DP\u2126(\u03b8),Z\u3009 \u2208 R N\u00d7N ,\nwhere derivatives are w.r.t. \u03b8. The above vector may be computed in two ways, that differ in the order in which\nderivatives are computed. Using automatic differentiation frameworks such as PyTorch (Paszke et al., 2017), we may backpropagate over the computational graph a first time to compute the gradient \u2207DP\u2126(\u03b8), while recording operations. We may then compute \u3008\u2207DP\u2126(\u03b8),Z\u3009, and backpropagate once again. However, due to the structure of the problem, it proves more efficient, adapting Pearlmutter\u2019s approach (1994), to directly compute \u3008\u2207DP\u2126(\u03b8),Z\u3009 \u2208 R, namely, the directional derivative at \u03b8 along Z. This is done by applying the chain rule in one topologicallyordered pass over G. Similarly to the gradient computation, we record products with the local Hessians Hi(\u03b8) , \u22072max\u2126(\u03b8i + v(\u03b8)) along the way. We then compute the gradient of the directional derivative using backpropagation. This yields a recursion for computing \u22072DP\u2126(\u03b8)Z in reverse topological-order over G. The complete derivation and the pseudo-code are given in \u00a7A.7. They allow to implement DP\u2126 as as a custom twice-differentiable module in existing software. For both approaches, the computational cost is O(|E|), the same as for gradient computation. In our experiments in \u00a74.2, our custom Hessian-vector product computation brings a 3\u00d7/12\u00d7 speed-up during the backward pass on GPU/CPU vs. automatic differentiation.\nRelated works. Smoothing LP formulations was also used for MAP inference (Meshi et al., 2015) or optimal transport (Blondel et al., 2018) but these works do not address how to differentiate through the smoothed formulation. An alternative approach to create structured prediction layers, fundamentally different both in the forward and backward passes, is SparseMAP (Niculae et al., 2018).\nSummary. We have proposed DP\u2126(\u03b8), a smooth, convex and tractable relaxation to the value of LP(\u03b8). We have also shown that \u2207DP\u2126(\u03b8) belongs to conv(Y) and is therefore a sound relaxation to solutions of LP(\u03b8). To conclude this section, we formally define our proposed two layers.\nDefinition 1. Differentiable dynamic programming layers\nValue layer: DP\u2126(\u03b8) \u2208 R\nGradient layer: \u2207DP\u2126(\u03b8) \u2208 conv(Y)"}, {"heading": "3. Examples of computational graphs", "text": "We now illustrate two instantiations of our framework for specific computational graphs."}, {"heading": "3.1. Sequence prediction", "text": "We demonstrate in this section how to instantiate DP\u2126 to the computational graph of the Viterbi algorithm (Viterbi, 1967; Rabiner, 1990), one of the most famous instances of DP algorithm. We call the resulting operator Vit\u2126. We wish to tag a sequence X = (x1, . . . ,xT ) of vectors in R D\n(e.g., word representations) with the most probable output sequence (e.g., entity tags) y = (y1, . . . , yT ) \u2208 [S] T . This problem can be cast as finding the highest-scoring path on a treillis G. While y can always be represented as a sparse N\u00d7N binary matrix, it is convenient to represent it instead as a T\u00d7S\u00d7S binary tensor Y , such that yt,i,j = 1 if y transitions from node j to node i on time t, and 0 otherwise \u2014 we set y0 = 1. The potentials can similarly be organized as a T\u00d7S\u00d7S real tensor, such that \u03b8t,i,j = \u03c6t(xt, i, j). Traditionally, the potential functions \u03c6t were human-engineered (Sutton et al., 2012, \u00a72.5). In recent works and in this paper, they are learned end-to-end (Bottou et al., 1997; Collobert et al., 2011; Lample et al., 2016).\nUsing the above binary tensor representation, the inner product \u3008Y ,\u03b8\u3009 is equal to \u2211T\nt=1 \u03c6t(xt, yt, yt\u22121), y\u2019s cumulated score. This is illustrated in Figure 2 on the task of part-of-speech tagging. The bold arrows indicate one possible output sequence y, i.e., one possible path in G.\nWhen \u2126 = \u2212H , we recover linear-chain conditional random fields (CRFs) (Lafferty et al., 2001) and the probability of y (Y in tensor representation) given X is\np\u03b8,\u2212H(y|X)\u221d exp(\u3008Y ,\u03b8\u3009)= exp (\nT\u2211\nt=1\n\u03c6t(xt, yt, yt\u22121) ) .\nFrom Prop. 3, the gradient \u2207Vit\u2212H(\u03b8) = E \u2208 R T\u00d7S\u00d7S is such that et,i,j = p\u03b8,\u2212H(yt = i, yt\u22121 = j|X). The marginal probability of state i at time t is simply p\u03b8,\u2212H(yt = i|X) = \u2211S j=1 et,i,j . Using a different \u2126 simply changes the distribution over state transitions. When \u2126 = \u2016 \u00b7 \u20162, the marginal probabilities are typically sparse. Pseudo-code for Vit\u2126(\u03b8), as well as gradient and Hessian-product computations, is provided in \u00a7B.2. The case \u2126 = \u2016 \u00b7 \u20162 is new to our knowledge.\nWhen \u2126 = \u2212H , the marginal probabilities are traditionally computed using the forward-backward algorithm (Baum & Petrie, 1966). In contrast, we compute \u2207Vit\u2212H(\u03b8) using backpropagation while efficiently maintaining the marginalization. An advantage of our approach is that all operations are numerically stable. The relation between forward-backward and backpropagation has been noted before (e.g., Eisner (2016)). However, the analysis is led using (+,\u00d7) operations, instead of (max\u2126,+) as we do. Our\nViterbi instantiation can be generalized to graphical models with a tree structure, and to approximate inference in general graphical models, since unrolled loopy belief propagation (Pearl, 1988) yields a dynamic program. We note that continuous beam search (Goyal et al., 2017) can also be cleanly rewritten and extended using Vit\u2126 operators."}, {"heading": "3.2. Time-series alignment", "text": "We now demonstrate how to instantiate DP\u2126 to the computational graph of dynamic time warping (DTW) (Sakoe & Chiba, 1978), whose goal is to seek the minimal cost alignment between two time-series. We call the resulting operator DTW\u2126. Formally, let NA and NB be the lengths of two time-series, A and B. Let ai and bj be the i th and jth observations of A and B, respectively. Since edge weights only depend on child nodes, it is convenient to rearrange Y and \u03b8 as NA \u00d7 NB matrices. Namely, we represent an alignment Y as a NA \u00d7 NB binary matrix, such that yi,j = 1 if ai is aligned with bj , and 0 otherwise. Likewise, we represent \u03b8 as a NA \u00d7 NB matrix. A classical example is \u03b8i,j = d(ai, bj), for some differentiable discrepancy measure d. We write Y the set of all monotonic alignment matrices, such that the path that connects the upper-left (1, 1) matrix entry to the lower-right (NA, NB) one uses only \u2193,\u2192,\u0581moves. The DAG associated with Y is illustrated in Figure 3 with NA = 4 and NB = 3 below.\nAgain, the bold arrows indicate one possible path Y \u2208 Y from start to end in the DAG, and correspond to one possible alignment. Using this representation, the cost of an alignment (cumulated cost along the path) is conveniently computed by \u3008Y ,\u03b8\u3009. The value DTW\u2126(\u03b8) can be used to define a loss between alignments or between time-series. Following Proposition 3, \u2207DTW\u2126(\u03b8) = E \u2208 R NA\u00d7NB can be understood as a soft alignment matrix. This matrix is sparse when \u2126 = \u2016 \u00b7 \u20162, as illustrated in Figure 1 (right).\nPseudo-code to compute DTW\u2126(\u03b8) as well as its gradient and its Hessian products are provided in \u00a7B.3. When \u2126 = \u2212H , DTW\u2126(\u03b8) is a conditional random field known as soft-DTW, and the probability p\u03b8,\u2126(Y |A,B) is a Gibbs distribution similar to \u00a73.1 (Cuturi & Blondel, 2017).\nHowever, the case \u2126 = \u2016 \u00b7 \u20162 and the computation of \u22072DTW\u2126(\u03b8)Z are new and allow new applications."}, {"heading": "4. Differentiable structured prediction", "text": "We now apply the proposed layers, DP\u2126(\u03b8) and\u2207DP\u2126(\u03b8), to structured prediction (Bak\u0131r et al., 2007), whose goal is to predict a structured output Y \u2208 Y associated with a structured input X \u2208 X . We define old and new structured losses, and demonstrate them on two structured prediction tasks: named entity recognition and time-series alignment."}, {"heading": "4.1. Structured loss functions", "text": "Throughout this section, we assume that the potentials \u03b8 \u2208 \u0398 have already been computed using a function from X to \u0398 and let C : Y\u00d7Y \u2192 R+ be a cost function between the ground-truth output Ytrue and the predicted output Y .\nConvex losses. Because C is typically non-convex, the cost-augmented structured hinge loss (Tsochantaridis et al., 2005) is often used instead for linear models\n\u2113C(Ytrue;\u03b8) , max Y \u2208Y C(Ytrue,Y )+\u3008Y ,\u03b8\u3009\u2212\u3008Ytrue,\u03b8\u3009. (7)\nThis is a convex upper-bound on C(Ytrue,Y \u22c6(\u03b8)), where Y \u22c6(\u03b8) is defined in (4). To make the cost-augmented decoding tractable, it is usually assumed that C(Ytrue,Y ) is linear in Y , i. e., it can be written as \u3008CYtrue ,Y \u3009 for some matrix CYtrue . We can then rewrite (7) using our notation as\n\u2113C(Ytrue;\u03b8) = LP(\u03b8 +CYtrue)\u2212 \u3008Ytrue,\u03b8\u3009.\nHowever, this loss function is non-differentiable. We therefore propose to relax LP by substituting it with DP\u2126:\n\u2113C,\u2126(Ytrue;\u03b8) , DP\u2126(\u03b8 +CYtrue)\u2212 \u3008Ytrue,\u03b8\u3009.\nLosses in this class are convex, smooth, tractable for any \u2126, and by Proposition 2 property 2 a sensible approximation of \u2113C . In addition, they only require to backpropagate through DP\u2126(\u03b8) at training time. It is easy to check that we recover the structured perceptron loss with \u21130,0 (Collins, 2002), the structured hinge loss with \u2113C,0 (Tsochantaridis et al., 2005) and the CRF loss with \u21130,\u2212H (Lafferty et al., 2001). The last one has been used on top of LSTMs in several recent works (Lample et al., 2016; Ma & Hovy, 2016). Minimizing \u21130,\u2212H(\u03b8) is equivalent to maximizing the likelihood p\u03b8,\u2212H(Ytrue). However, minimizing \u21130,\u2016\u00b7\u20162 is not equivalent to maximizing p\u03b8,\u2016\u00b7\u20162(Ytrue). In fact, the former is convex while the latter is not.\nNon-convex losses. A direct approach that uses the output distribution p\u03b8,\u2126 consists in minimizing the risk\u2211 y\u2208Y p\u03b8,\u2212H(Y )C(Ytrue,Y ). As shown by Stoyanov &\nEisner (2012), this can be achieved by backpropagating through the minimum risk decoder. However, the risk is usually non-differentiable, piecewise constant (Smith & Eisner, 2006) and several smoothing heuristics are necessary to make the method work (Stoyanov & Eisner, 2012).\nAnother principled approach is to consider a differentiable approximation \u2206: Y \u00d7 conv(Y) \u2192 R+ of the cost C. We can then relax C(Ytrue,Y\n\u22c6(\u03b8)) by \u2206(Ytrue,\u2207DP\u2126(\u03b8)). Unlike minimum risk training, this approach is differentiable everywhere when \u2126 = \u2212H . Both approaches require to backpropagate through \u2207DP\u2126(\u03b8), which is twice as costly as backpropagating through DP\u2126(\u03b8) (see \u00a72.4)."}, {"heading": "4.2. Named entity recognition", "text": "Let X = (x1, \u00b7 \u00b7 \u00b7 ,xT ) be an input sentence, where each word xt is represented by a vector in R D, computed using a neural recurrent architecture trained end-to-end. We wish to tag each word with named entities, i.e., identify blocks of words that correspond to names, locations, dates, etc. We use the specialized operator Vit\u2126 described in \u00a73.1. We construct the potential tensor \u03b8(X) \u2208 RT\u00d7S\u00d7S as\n\u2200 t > 1, \u03b8(X)t,i,j , w \u22a4 i xt + bi + ti,j ,\nand \u03b8(X)1,i,j , w \u22a4 i xt + bi, where (wi, bi) \u2208 R D \u00d7 R is the linear classifier associated with tag i and T \u2208 RS\u00d7S is a transition matrix. We learn W , b and T along with the network producing X , and compare two losses:\nSurrogate convex loss: \u21130,\u2126(Ytrue;\u03b8),\nRelaxed loss: \u2206(Ytrue,\u2207DP\u2126(\u03b8)),\nwhere \u2206(Ytrue,Y ) is the squared \u21132 distance when \u2126 = \u2016 \u00b7 \u201622 and the Kullback-Leibler divergence when \u2126 = \u2212H , applied row-wise to the marginalization of Ytrue and Y .\nExperiments. We measure the performance of the different losses and regularizations on the four languages of the CoNLL 2003 dataset. Following Lample et al. (2016), who use the \u21130,\u2212H loss, we use a character LSTM and FastText (Joulin et al., 2016) pretrained embeddings computed using on Wikipedia. Those are fed to a word bidirectional LSTM to obtain X . Architecture details are provided in \u00a7C.1. Results are reported in Table 1, along with reference results with different pretrained embeddings. We first note that the non-regularized structured perceptron loss \u21130,0, that involves working with subgradients of DP(\u03b8), perform significantly worse than regularized losses. With proper parameter selections, all regularized losses perform within 1% F1-score of each other, although entropyregularized losses perform slightly better on 3/4 languages. However, the \u211322-regularized losses yield sparse predictions, whereas entropy regularization always yields dense probability vectors. Qualitatively, this allows to identify ambiguous predictions more easily, as illustrated in \u00a7C.1. Sparse\npredictions also allows to enumerate all non-zero probability entities, and to trade precision for recall at test time."}, {"heading": "4.3. Supervised audio-to-score transcription", "text": "We use our framework to perform supervised audio-toscore alignment on the Bach 10 dataset (Duan & Pardo, 2011). The dataset consists of 10 music pieces with audio tracks, MIDI transcriptions, and annotated alignments between them. We transform the audio tracks into a sequence of audio frames using a feature extractor (see \u00a7C.2) to obtain a sequence A \u2208 RNA\u00d7D, while the associated score sequence is represented by B \u2208 RNB\u00d7K (each row bj is a one-hot vector corresponding to one key bj). Each pair (A,B) is associated to an alignment Ytrue \u2208 R NA\u00d7NB . As described in \u00a73.2, we define a discrepancy matrix \u03b8 \u2208 R NA\u00d7NB between the elements of the two sequences. We set the cost between an audio frame and a key to be the loglikelihood of this key given a multinomial linear classifier:\n\u2200 i \u2208 [NA], li , \u2212 log(softmax(W \u22a4ai + c)) \u2208 R K\nand \u2200 j \u2208 [NB ], \u03b8i,j , li,bj ,\nwhere (W , c) \u2208 RD\u00d7K\u00d7RK are learned classifier parameters. We predict a soft alignment by Y = \u2207DTW\u2212H(\u03b8). Following (Garreau et al., 2014), we define the relaxed loss\n\u2206(Ytrue,Y ) , \u2016L(Y \u2212 Ytrue) \u22a4\u20162F ,\nwhere L a the lower triangular matrix filled with 1. When Y \u2208 Y is a true alignement matrix, \u2206(Ytrue,Y ) is the area between the path of Ytrue and Y , which corresponds to the mean absolute deviation in the audio literature. When Y \u2208 conv(Y), it is a convex relaxation of the area. At test time, once \u03b8 is learned, we use the non-regularized DTW algorithm to output a hard alignment Y \u22c6(\u03b8) \u2208 Y .\nResults. We perform a leave-one-out cross-validation of our model performance, learning the multinomial classifier on 9 pieces and assessing the quality of the alignment on the remaining piece. We report the mean absolute deviation on both train and test sets. A solid baseline consists in learning the multinomial classifier (W , c) beforehand, i.e., without end-to-end training. We then use this model to compute \u03b8 as in (4.3) and obtain Y \u22c6(\u03b8). As shown in Table 2, our end-to-end technique outperforms this baseline by a large margin. We also demonstrate in \u00a7C.2 that the alignments obtained by end-to-end training are visibly closer to the ground truth. End-to-end training thus allows to fine-tune the distance matrix \u03b8 for the task at hand."}, {"heading": "5. Structured and sparse attention", "text": "We show in this section how to apply our framework to neural sequence-to-sequence models augmented with an attention mechanism (Bahdanau et al., 2015). An encoder first produces a list of vectors X = (x1, . . . ,xT ) representing the input sequence. A decoder is then used to greedily produce the corresponding output sequence. To simplify the notation, we focus on one time step of the decoding procedure. Given the decoder\u2019s current hidden state z and X as inputs, the role of the attention mechanism is to produce a distribution w \u2208 \u25b3T over X , for the current time step. This distribution is then typically used to produce a context vector c , X\u22a4w, that is in turn invoved in the computation of the output sequence\u2019s next element.\nStructured attention layers. Kim et al. (2017) proposed a segmentation attention layer, which is capable of taking into account the transitions between elements of X . They use a linear-chain CRF to model the probability p\u03b8,\u2212H(y|X) of a sequence y = (y1, . . . , yT ), where each yt is either 1 (\u201cpay attention\u201d) or 0. They then propose to use normalized marginal probabilities as attention weights: wt \u221d p\u03b8,\u2212H(yt = 1|X). They show how to backpropagate gradients through the forward-backward algorithm, which they use to compute the marginal probabilities.\nGeneralizing structured attention. Using the notation from \u00a73.1, any y can be represented as a tensor Y \u2208 {0, 1}T\u00d72\u00d72 and the potentials as a tensor \u03b8 \u2208 RT\u00d72\u00d72. Similarly to Kim et al. (2017), we define\n\u03b8t,1,j , xtMz + t1,j and \u03b8t,0,j , t0,j ,\nwhere xMz is a learned bilinear form and T \u2208 R2\u00d72 is a learned transition matrix. Following \u00a73.1, the gradient \u2207Vit\u2126(\u03b8) is equal to the expected matrix E \u2208 R T\u00d72\u00d72 and the marginals are obtained by marginalizing that matrix. Hence, we can set wt \u221d p\u03b8,\u2126(yt = 1|X) = et,1,0 + et,1,1. Backpropagating through \u2207Vit\u2126(\u03b8) can be carried out using our approach outlined in \u00a72.4. This approach is not only more general, but also simpler and more robust to under-\nflow problems than backpropagating through the forwardbackward algorithm as done by Kim et al. (2017).\nExperiments. We demonstrate structured attention layers with an LSTM encoder and decoder to perform French to English translation using data from a 1 million sentence subset of the WMT14 FR-EN challenge. We illustrate an example of attenion map obtained with negentropy and \u211322 regularizations in Figure 4. Non-zero elements are underlined with borders: \u211322-regularized attention maps are sparse and more interpretable \u2014 this provides a structured alternative to sparsemax attention (Martins & Astudillo, 2016). Results were all within 0.8 point of BLEU score on the newstest2014 dataset. For French to English, standard softmax attention obtained 27.96, while entropy and \u211322 regularized structured attention obtained 27.96 and 27.19 \u2014 introducing structure and sparsity therefore provides enhanced interpretability with comparable peformance. We provide model details, full results and further visualizations in \u00a7C.3."}, {"heading": "6. Conclusion", "text": "We proposed a theoretical framework for turning a broad class of dynamic programs into convex, differentiable and tractable operators, using the novel point of view of smoothed max operators. Our work sheds a new light on how to transform dynamic programs that predict hard assignments (e.g., the maximum a-posteriori estimator in a probabilistic graphical model or an alignment matrix between two time-series) into continuous and probabilistic ones. We provided a new argument in favor of negentropy regularization by showing that it is the only one to preserve associativity of the smoothed max operator. We showed that different regularizations induce different distributions over outputs and that \u211322 regularization has other benefits, in terms of sparsity of the expected outputs. Generally speaking, performing inference in a graphical model and backpropagating through it reduces to computing the first and second-order derivatives of a relaxed maximum-likelihood estimation \u2014 leveraging this observation yields elegant and efficient algorithms that are readily usable in deep learning frameworks, with various promising applications."}, {"heading": "Acknowledgements", "text": "MB thanks Vlad Niculae and Marco Cuturi for many fruitful discussions. AM thanks Julien Mairal, Inria Thoth and Inria Parietal for lending him the computational resources necessary to run the experiments. He thanks University Paris-Saclay for allowing him to do an internship at NTT, and Olivier Grisel for his insightful comments on natural language processing models."}], "year": 2018, "references": [{"title": "The generalized distributive law", "authors": ["S.M. Aji", "R.J. McEliece"], "venue": "IEEE Transactions on Information Theory,", "year": 2000}, {"title": "OptNet: Differentiable optimization as a layer in neural networks", "authors": ["B. Amos", "J.Z. Kolter"], "venue": "In Proc. of ICML,", "year": 2017}, {"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "authors": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "In Proc. of ICLR,", "year": 2015}, {"title": "Predicting Structured Data", "authors": ["G. Bak\u0131r", "T. Hofmann", "B. Sch\u00f6lkopf", "A.J. Smola", "B. Taskar", "S.V.N. Vishwanathan"], "year": 2007}, {"title": "Why Delannoy numbers", "authors": ["C. Banderier", "S. Schwer"], "venue": "Journal of Statistical Planning and Inference,", "year": 2005}, {"title": "Statistical inference for probabilistic functions of finite state Markov chains", "authors": ["L.E. Baum", "T. Petrie"], "venue": "The Annals of Mathematical Statistics,", "year": 1966}, {"title": "Smoothing and first order methods: A unified framework", "authors": ["A. Beck", "M. Teboulle"], "venue": "SIAM Journal on Optimization,", "year": 2012}, {"title": "On the theory of dynamic programming", "authors": ["R. Bellman"], "venue": "Proc. of the National Academy of Sciences,", "year": 1952}, {"title": "Control of uncertain systems with a setmembership description of the uncertainty", "authors": ["D.P. Bertsekas"], "venue": "PhD thesis, Massachusetts Institute of Technology,", "year": 1971}, {"title": "Smooth and sparse optimal transport", "authors": ["M. Blondel", "V. Seguy", "A. Rolet"], "venue": "In Proc. of AISTATS,", "year": 2018}, {"title": "Global training of document processing systems using graph transformer networks", "authors": ["L. Bottou", "Y. Bengio", "Y.L. Cun"], "venue": "In Proc. of CVPR,", "year": 1997}, {"title": "Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms", "authors": ["M. Collins"], "venue": "In Proc. of ACL, pp", "year": 2002}, {"title": "Natural language processing (almost) from scratch", "authors": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "The Journal of Machine Learning Research,", "year": 2011}, {"title": "Soft-DTW: a differentiable loss function for time-series", "authors": ["M. Cuturi", "M. Blondel"], "venue": "In Proc. of ICML,", "year": 2017}, {"title": "The theory of max-min, with applications", "authors": ["J.M. Danskin"], "venue": "SIAM Journal on Applied Mathematics,", "year": 1966}, {"title": "Differentiable learning of submodular functions", "authors": ["J. Djolonga", "A. Krause"], "venue": "In Proc. of NIPS,", "year": 2017}, {"title": "Soundprism: An online system for score-informed source separation of music audio", "authors": ["Z. Duan", "B. Pardo"], "venue": "IEEE Journal of Selected Topics in Signal Processing,", "year": 2011}, {"title": "Structured Prediction for NLP", "authors": ["D. Garreau", "R. Lajugie", "S. Arlot", "F. Bach"], "year": 2016}, {"title": "Structured attention networks", "authors": ["Y. Kim", "C. Denton", "L. Hoang", "A.M. Rush"], "venue": "In Proc. of ICLR,", "year": 2017}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "authors": ["J. Lafferty", "A. McCallum", "F.C. Pereira"], "venue": "In Proc. of ICML, pp", "year": 2001}, {"title": "Neural architectures for named entity recognition", "authors": ["G. Lample", "M. Ballesteros", "S. Subramanian", "K. Kawakami", "C. Dyer"], "venue": "In Proc. of NAACL,", "year": 2016}, {"title": "The representation of the cumulative rounding error of an algorithm as a Taylor expansion of the local rounding errors", "authors": ["S. Linnainmaa"], "venue": "PhD thesis, Univ. Helsinki,", "year": 1970}, {"title": "Effective approaches to attention-based neural machine translation", "authors": ["T. Luong", "H. Pham", "C.D. Manning"], "venue": "In Proc. of EMNLP,", "year": 2015}, {"title": "End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF", "authors": ["X. Ma", "E. Hovy"], "venue": "In Proc. of ACL,", "year": 2016}, {"title": "From softmax to sparsemax: A sparse model of attention and multi-label classification", "authors": ["A.F. Martins", "R.F. Astudillo"], "venue": "In Proc. of ICML,", "year": 2016}, {"title": "Smooth and strong: MAP inference with linear convergence", "authors": ["O. Meshi", "M. Mahdavi", "A.G. Schwing"], "venue": "In Proc. of NIPS,", "year": 2015}, {"title": "A finite algorithm for finding the projection of a point onto the canonical simplex of R", "authors": ["C. Michelot"], "venue": "Journal of Optimization Theory and Applications,", "year": 1986}, {"title": "Proximit\u00e9 et dualit\u00e9 dans un espace hilbertien", "authors": ["Moreau", "J.-J"], "venue": "Bullet de la Socie\u0301te\u0301 Mathe\u0301mathique de France,", "year": 1965}, {"title": "Smooth minimization of non-smooth functions", "authors": ["Y. Nesterov"], "venue": "Mathematical Programming,", "year": 2005}, {"title": "A regularized framework for sparse and structured neural attention", "authors": ["V. Niculae", "M. Blondel"], "venue": "In Proc. of NIPS,", "year": 2017}, {"title": "Sparsemap: Differentiable sparse structured inference", "authors": ["V. Niculae", "A.F. Martins", "M. Blondel", "C. Cardie"], "venue": "In Proc. of ICML,", "year": 2018}, {"title": "Pytorch: Tensors and dynamic neural networks in Python with strong GPU acceleration, 2017", "authors": ["A. Paszke", "S. Gross", "S. Chintala", "G. Chanan"], "year": 2017}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference", "authors": ["J. Pearl"], "year": 1988}, {"title": "Fast exact multiplication by the Hessian", "authors": ["B.A. Pearlmutter"], "venue": "Neural computation,", "year": 1994}, {"title": "A tutorial on hidden Markov models and selected applications in speech recognition", "authors": ["L.R. Rabiner"], "venue": "In Readings in Speech Recognition,", "year": 1990}, {"title": "Dynamic programming algorithm optimization for spoken word recognition", "authors": ["H. Sakoe", "S. Chiba"], "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing,", "year": 1978}, {"title": "Minimum risk annealing for training log-linear models", "authors": ["D.A. Smith", "J. Eisner"], "venue": "In Proc. of COLING/ACL,", "year": 2006}, {"title": "Minimum-risk training of approximate CRF-based NLP systems", "authors": ["V. Stoyanov", "J. Eisner"], "venue": "In Proc. of NAACL, pp", "year": 2012}, {"title": "Objects counted by the central Delannoy numbers", "authors": ["R.A. Sulanke"], "venue": "Journal of Integer Sequences,", "year": 2003}, {"title": "An introduction to conditional random fields", "authors": ["C. Sutton", "A McCallum"], "venue": "Foundations and Trends in Machine Learning,", "year": 2012}, {"title": "Large margin methods for structured and interdependent output variables", "authors": ["I. Tsochantaridis", "T. Joachims", "T. Hofmann", "Y. Altun"], "venue": "Journal of Machine Learning Research,", "year": 2005}, {"title": "Abstract dynamic programming models under commutativity conditions", "authors": ["S. Verdu", "H.V. Poor"], "venue": "SIAM Journal on Control and Optimization,", "year": 1987}, {"title": "Error bounds for convolutional codes and an asymptotically optimum decoding algorithm", "authors": ["A. Viterbi"], "venue": "IEEE Transactions on Information Theory,", "year": 1967}, {"title": "Graphical models, exponential families, and variational inference", "authors": ["M.J. Wainwright", "M.I. Jordan"], "venue": "Foundations and Trends in Machine Learning,", "year": 2008}], "id": "SP:fd3ecdc679fa8b5d5a4fa0f9adadda5191a54cdb", "authors": [{"name": "Arthur Mensch", "affiliations": []}, {"name": "Mathieu Blondel", "affiliations": []}], "abstractText": "Dynamic programming (DP) solves a variety of structured combinatorial problems by iteratively breaking them down into smaller subproblems. In spite of their versatility, many DP algorithms are non-differentiable, which hampers their use as a layer in neural networks trained by backpropagation. To address this issue, we propose to smooth the max operator in the dynamic programming recursion, using a strongly convex regularizer. This allows to relax both the optimal value and solution of the original combinatorial problem, and turns a broad class of DP algorithms into differentiable operators. Theoretically, we provide a new probabilistic perspective on backpropagating through these DP operators, and relate them to inference in graphical models. We derive two particular instantiations of our framework, a smoothed Viterbi algorithm for sequence prediction and a smoothed DTW algorithm for time-series alignment. We showcase these instantiations on structured prediction (audio-to-score alignment, NER) and on structured and sparse attention for translation. Modern neural networks are composed of multiple layers of nested functions. Although layers usually consist of elementary linear algebraic operations and simple nonlinearities, there is a growing need for layers that output the value or the solution of an optimization problem. This can be used to design loss functions that capture relevant regularities in the input (Lample et al., 2016; Cuturi & Blondel, 2017) or to create layers that impose prior structure on the output (Kim et al., 2017; Amos & Kolter, 2017; Niculae & Blondel, 2017; Djolonga & Krause, 2017). Among these works, several involve a convex optimization Inria, CEA, Universit\u00e9 Paris-Saclay, Gif-sur-Yvette, France. Work performed at NTT Communication Science Laboratories, Kyoto, Japan. Correspondence to: AM <arthur.mensch@m4x.org>, MB <mathieu@mblondel.org>. Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s). problem (Amos & Kolter, 2017; Niculae & Blondel, 2017; Djolonga & Krause, 2017); others solve certain combinatorial optimization problems by dynamic programming (Kim et al., 2017; Cuturi & Blondel, 2017; Nowak et al., 2018). However, because dynamic programs (Bellman, 1952) are usually non-differentiable, virtually all these works resort to the formalism of conditional random fields (CRFs) (Lafferty et al., 2001), which can be seen as changing the semiring used by the dynamic program \u2014 replacing all values by their exponentials and all (max,+) operations with (+,\u00d7) operations (Verdu & Poor, 1987). While this modification smoothes the dynamic program, it looses the sparsity of solutions, since hard assignments become soft ones. Moreover, a general understanding of how to relax and differentiate dynamic programs is lacking. In this work, we propose to do so by leveraging smoothing (Moreau, 1965; Nesterov, 2005) and backpropagation (Linnainmaa, 1970). We make the following contributions. 1) We present a unified framework for turning a broad class of dynamic programs (DP) into differentiable operators. Unlike existing works, we propose to change the semiring to use (max\u03a9,+) operations, where max\u03a9 is a max operator smoothed with a strongly convex regularizer \u03a9 (\u00a71). 2) We show that the resulting DP operators, that we call DP\u03a9, are smoothed relaxations of the original DP algorithm and satisfy several key properties, chief among them convexity. In addition, we show that their gradient,\u2207DP\u03a9, is equal to the expected trajectory of a certain random walk and can be used as a sound relaxation to the original dynamic program\u2019s solution. Using negative entropy for \u03a9 recovers existing CRF-based works from a different perspective \u2014 we provide new arguments as to why this \u03a9 is a good choice. On the other hand, using squared l2 norm for \u03a9 leads to new algorithms whose expected solution is sparse. We derive a clean and efficient method to backpropagate gradients, both through DP\u03a9 and \u2207DP\u03a9. This allows us to define differentiable DP layers that can be incorporated in neural networks trained end-to-end (\u00a72). 3) We illustrate how to to derive two particular instantiations of our framework, a smoothed Viterbi algorithm for sequence prediction and a smoothed DTW algorithm for supervised time-series alignment (\u00a73). The latter is illustrated in Figure 1. Finally, we showcase these two instantiDifferentiable Dynamic Programming for Structured Prediction and Attention DTW H(\u03b8) = 7.49 DTWk\u00b7k2(\u03b8) = 9.61 Figure 1. DTW\u03a9(\u03b8) is an instantiation of the proposed smoothed dynamic programming operator, DP\u03a9(\u03b8), to the dynamic time warping (DTW) computational graph. In this picture, \u03b8 is the squared Euclidean distance matrix between the observations of two time-series. The gradient \u2207DTW\u03a9(\u03b8) is equal to the expected alignment under a certain random walk characterized in \u00a72.3 and is a sound continuous relaxation to the hard DTW alignment between the two time-series (here depicted with a yellow path). Unlike negentropy regularization (left), l22 regularization leads to exactly sparse alignments (right). Our framework allows to backpropagate through both DTW\u03a9(\u03b8) and \u2207DTW\u03a9(\u03b8), which makes it possible to learn the distance matrix \u03b8 end-to-end. ations on structured prediction tasks (\u00a74) and on structured attention for neural machine translation (\u00a75). Notation. We denote scalars, vectors and matrices using lower-case, bold lower-case and bold upper-case letters, e.g., y, y and Y . We denote the elements of Y by yi,j and its rows by yi. We denote the Frobenius inner product between A and B by \u3008A,B\u3009 , \u2211 i,j ai,jbi,j . We denote the (D\u2212 1)-probability simplex by\u25b3 , {\u03bb \u2208 R+ : \u2016\u03bb\u20161 = 1}. We write conv(Y) , { \u2211 Y \u2208Y \u03bbY Y : \u03bb \u2208 \u25b3 } the convex hull of Y , [N ] the set {1, . . . , N} and supp(x) , {j \u2208 [D] : xj 6= 0} the support of x \u2208 R . We denote the Shannon entropy by H(q) , \u2211 i qi log qi. We have released an optimized and modular PyTorch implementation for reproduction and reuse. 1. Smoothed max operators In this section, we introduce smoothed max operators (Nesterov, 2005; Beck & Teboulle, 2012; Niculae & Blondel, 2017), that will serve as a powerful and generic abstraction to define differentiable dynamic programs in \u00a72. Let \u03a9 : R \u2192 R be a strongly convex function on\u25b3 and let x \u2208 R. We define the max operator smoothed by \u03a9 as: max\u03a9(x) , max q\u2208\u25b3D \u3008q,x\u3009 \u2212 \u03a9(q). (1) In other words, max\u03a9 is the convex conjugate of \u03a9, restricted to the simplex. From the duality between strong convexity and smoothness, max\u03a9 is smooth: differentiable everywhere and with Lipschitz continuous gradient. Since the argument that achieves the maximum in (1) is unique, from Danskin\u2019s theorem (1966), it is equal to the gradient: \u2207max\u03a9(x) = argmax q\u2208\u25b3D \u3008q,x\u3009 \u2212 \u03a9(q). The gradient is differentiable almost everywhere for any strongly-convex \u03a9 (everywhere for negentropy). Next, we state properties that will be useful throughout this paper. Lemma 1. Properties of max\u03a9 operators Let x = (x1, . . . , xD) \u22a4 \u2208 R. 1. Boundedness: If \u03a9 is lower-bounded by L\u03a9,D and upper-bounded by U\u03a9,D on the simplex\u25b3 , then max(x)\u2212 U\u03a9,D \u2264 max\u03a9(x) \u2264 max(x)\u2212 L\u03a9,D. 2. Distributivity of + over max\u03a9: max\u03a9(x+ c1) = max\u03a9(x) + c \u2200c \u2208 R. 3. Commutativity: If \u03a9(Pq) = \u03a9(q), where P is a permutation matrix, then max\u03a9(Px) = max\u03a9(x). 4. Non-decreasingness in each coordinate: max\u03a9(x) \u2264 max\u03a9(y) \u2200x \u2264 y 5. Insensitivity to \u2212\u221e: xj = \u2212\u221e\u21d2 \u2207max\u03a9(x)j = 0. Proofs are given in \u00a7A.1. In particular, property 3 holds whenever \u03a9(q) = \u2211D i=1 \u03c9(qi), for some function \u03c9. We focus in this paper on two specific regularizers \u03a9: the negentropy \u2212H and the squared l2 norm. For these choices, all properties above are satisfied and we can derive closedform expressions for max\u03a9, its gradient and its Hessian \u2014 see \u00a7B.1. When using negentropy, max\u03a9 becomes the log-sum-exp and\u2207max\u03a9 the softmax. The former satisfies associativity, which as we shall see, makes it natural to use in dynamic programming. With the squared l2 regularization, as observed by Martins & Astudillo (2016); Niculae & Blondel (2017), the gradient\u2207max\u03a9 is sparse. This will prove useful to enforce sparsity in the models we study. 2. Differentiable DP layers Dynamic programming (DP) is a generic way of solving combinatorial optimization problems by recursively solving problems on smaller sets. We first introduce this category of algorithms in a broad setting, then use smoothed max operators to define differentiable DP layers. 2.1. Dynamic programming on a DAG Every problem solved by dynamic programming reduces to finding the highest-scoring path between a start node and an end node, on a weighted directed acyclic graph (DAG). We therefore introduce our formalism on this generic problem, and give concrete examples in \u00a73. Formally, let G = (V, E) be a DAG, with nodes V and edges E . We write N = |V| \u2265 2 the number of nodes. Differentiable Dynamic Programming for Structured Prediction and Attention Without loss of generality, we number the nodes in topological order, from 1 (start) to N (end), and thus V = [N ]. Node 1 is the only node without parents, and node N the only node without children. Every directed edge (i, j) from a parent node j to a child node i has a weight \u03b8i,j \u2208 R. We gather the edge weights in a matrix \u03b8 \u2208 \u0398 \u2286 R , setting \u03b8i,j = \u2212\u221e if (i, j) / \u2208 E and \u03b81,1 = 1. We consider the set Y of all paths in G from node 1 to node N . Any path Y \u2208 Y can be represented as a N \u00d7 N binary matrix, with yi,j = 1 if the path goes through the edge (i, j) and yi,j = 0 otherwise. In the sequel, paths will have a one-to-one correspondence with discrete structures such as sequences or alignments. Using this representation, \u3008Y ,\u03b8\u3009 corresponds to the cumulated sum of edge weights, along the path Y . The computation of the highest score among all paths amounts to solving the combinatorial problem LP(\u03b8) , max Y \u2208Y \u3008Y ,\u03b8\u3009 \u2208 R. (2) Although the size of Y is in general exponential in N , LP(\u03b8) can be computed in one topologically-ordered pass over G using dynamic programming. We let Pi be the set of parent nodes of node i in graph G and define recursively v1(\u03b8) , 0 \u2200 i \u2208 [2, . . . , N ] : vi(\u03b8) , max j\u2208Pi \u03b8i,j + vj(\u03b8). (3) This algorithm outputs DP(\u03b8) , vN (\u03b8). We now show that this is precisely the highest score among all paths. Proposition 1. Optimality of dynamic programming \u2200\u03b8 \u2208 \u0398 : DP(\u03b8) = LP(\u03b8) The optimality of recursion (3) is well-known (Bellman, 1952). We prove it again with our formalism in \u00a7A.2, since it exhibits the two key properties that the max operator must satisfy to guarantee optimality: distributivity of + over it and associativity. The cost of computing DP(\u03b8) is O(|E|), which is exponentially better than O(|Y|). In many applications, we will often rather be interested in the argument that achieves the maximum, i.e., one of the highest-scoring paths Y (\u03b8) \u2208 argmax Y \u2208Y \u3008Y ,\u03b8\u3009. (4) This argument can be computed by backtracking, that we now relate to computing subgradients of LP(\u03b8). Linear program, lack of differentiality. Unfortunately, LP(\u03b8) is not differentiable everywhere. To see why this is the case, notice that (2) can be rewritten as a linear program over the convex polytope conv(Y): LP(\u03b8) = max Y \u2208conv(Y) \u3008Y ,\u03b8\u3009. From the generalized Danskin theorem (Bertsekas, 1971), Y (\u03b8) \u2208 \u2202LP(\u03b8) = argmax Y \u2208conv(Y) \u3008Y ,\u03b8\u3009, where \u2202 denotes the subdifferential of LP(\u03b8), i.e., the set of subgradients. When Y (\u03b8) is unique, \u2202LP(\u03b8) is a singleton and Y \u22c6 is equal to the gradient of LP(\u03b8), that we write\u2207LP(\u03b8). Unfortunately, Y (\u03b8) is not always unique, meaning that LP(\u03b8) is not differentiable everywhere. As we will show in \u00a74.2, this hinders optimization as we can only train models involving LP(\u03b8) with subgradient methods. Worse, Y (\u03b8), a function from \u0398 to Y , is discontinuous and has null or undefined derivatives. It is thus impossible to use it in a model trained by gradient descent. 2.2. Smoothed max layers To address the lack of differentiability of dynamic programming, we introduce the operator max\u03a9, presented in \u00a71, and consider two approaches. Smoothing the linear program. Let us define the \u03a9smoothed maximum of a function f : Y \u2192 R over a finite set Y using the following shorthand notation: max\u03a9 Y \u2208Y f(Y ) , max\u03a9((f(Y ))Y \u2208Y). A natural way to circumvent the lack of differentiability of LP(\u03b8) is then to replace the global max operator by max\u03a9: LP\u03a9(\u03b8) , max\u03a9 Y \u2208Y \u3008Y ,\u03b8\u3009 \u2208 R. (5) From \u00a71, LP\u03a9(\u03b8) is convex and, as long as \u03a9 is strongly convex, differentiable everywhere. In addition, \u2207LP\u03a9(\u03b8) is Lipschitz continuous and thus differentiable almost everywhere. Unfortunately, solving (5) for general \u03a9 is likely intractable when Y has an exponential size. Smoothing the dynamic program. As a tractable alternative, we propose an algorithmic smoothing. Namely, we replace max by max\u03a9 locally within the DP recursion. Omitting the dependence on \u03a9, this defines a smoothed recursion over the new sequence (vi(\u03b8)) N i=1: v1(\u03b8) , 0 \u2200i \u2208 [2, . . . , N ] : vi(\u03b8) , max\u03a9 j\u2208Pi \u03b8i,j + vj(\u03b8). (6) The new algorithm outputs DP\u03a9(\u03b8), vN (\u03b8), the smoothed highest score. Smoothing the max operator locally brings the same benefit as before \u2014 DP\u03a9(\u03b8) is smooth and \u2207DP\u03a9(\u03b8) is differentiable almost everywhere. However, computing DP\u03a9(\u03b8) is now always tractable, since it simply requires to evaluate (vi(\u03b8)) N i=1 in topological order, as in Differentiable Dynamic Programming for Structured Prediction and Attention the original recursion (3). Although LP\u03a9(\u03b8) and DP\u03a9(\u03b8) are generally different (in fact, LP\u03a9(\u03b8) \u2265 DP\u03a9(\u03b8) for all \u03b8 \u2208 \u0398), we now show that DP\u03a9(\u03b8) is a sensible approximation of LP(\u03b8) in several respects. Proposition 2. Properties of DP\u03a9", "title": "Differentiable Dynamic Programming for Structured Prediction and Attention"}