4	228	In healthcare facilities, medical records are assigned a set of standardized codes for billing purposes (NCHS, 1978).
5	18	Automatically annotating tweets with hashtags, while the labels are not fixed, can also be represented as a large-scale multi-label classification problem (Weston et al., 2014).
7	40	First, the documents may be long, sometimes containing more than a thousand words (Mullenbach et al., 2018).
16	56	A few labels occur more than 10,000 times, around 5,000 labels occur between 1 and 10 times, and of the 17,000 diagnosis and procedure labels, more than 50% never occur.
27	18	Also, frequent labels are generally easier to predict using machine-learning based methods.
29	13	Thus, we believe methods that handle infrequent and unseen labels in the multi-label setting are important.
106	18	The CNN will return a document feature matrixD ∈ R(n−s+1)×u where each column ofD is a feature map, u is the total number of convolution filters, n is the number of words in the document, and s is the width of convolution filters.
108	30	We use the label descriptors to generate a feature vector for each label.
114	124	Because we work with textual data, we simply share the word embeddings between the convolutional layer and the label vector creation step to form vi.
115	15	Similar to the work byMullenbach et al. (2018), we employ label-wise attention to avoid the needle in the haystack situation encountered with long documents.
117	14	For example, with a single attention, we would only look at one spot in the document and assume that spot contains the relevant information needed to predict all labels.
118	13	In the multi-class setting, this assumption is plausible.
125	12	GivenD2, we generate the label-wise attention vector ai = softmax(D 2 vi), i = 1, .
127	24	Finally, we use D, and generate L label-specific document vector representations ci = a T i D, i = 1, .
141	31	Now, to compare the final label vector v3i with its document vector ci, we transform the document vector into ei = ReLU(Woci + bo), i = 1, .
146	20	In this paper, we use two medical datasets for evaluation purposes: MIMIC II (Jouhet et al., 2012) and MIMIC III (Johnson et al., 2016).
149	110	Following a generalized zeroshot learning evaluation methodology (Xian et al., 2017), we split the ICD-9 labels into three groups based on frequencies in the training dataset: The frequent group S that contains all labels that occur > 5 times, the few-shot group F that contains labels that occur between 1 and 5 times, and the zero-shot group Z of labels that never occur in the training dataset, but occur in the test/dev sets.
173	31	The alphanumeric structure defines a simple hierarchy over all ICD-9 codes.
174	25	For example, “systolic heart failure” (428.2) and “diastolic heart failure” (428.3) are both children of the “heart failure” code 428.
187	50	R@k is preferred for few- and zero-shot labels, because P@k quickly goes to zero as k increases and gets bigger than the number of group specific labels assigned to each instance.
188	38	Furthermore, for medical coding, these models are typically used as a recommendation engine to help coders.
189	74	Unless a label appears at the top of the ranking, the annotator will not see it.
190	116	Thus, ranking metrics better measure the usefulness of our systems.
191	90	For the frequent and fewshot labels we compare to state-of-the-art methods on the MIMIC II and MIMIC III datasets including ACNN (Mullenbach et al., 2018) and a CNN method introduced in Baumel et al. (2018).
192	26	We also compare with the L1 regularized logistic regression model used in Vani et al. (2017).
195	45	To use ESZSL, we must specify feature vectors for each label.
196	31	For zero-shot methods, the label vectors used are crucial regardless of the learning method used.
198	25	We average 200 dimensional ICD-9 descriptor word embeddings generated by Pyysalo et al. (2013) which are pretrained on PubMed, Wikipedia, and PubMed Central (ESZSL + W2V).
203	12	We factorize Y into two matrices U ∈ RN×300 and V ∈ R300×L using graph regularized alternating least squares (GRALS) (Rao et al., 2015).
206	35	Table 2 shows the results for MIMIC II.
