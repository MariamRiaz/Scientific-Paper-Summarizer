0	28	Graphs are a very effective data structure to represent relationships between entities (e.g., social and collaboration networks, influence graphs).
1	101	Over the years, many machine learning problems have been defined and solved exploiting the graph representation, such as graph-regularized least squares (LAPRLS, Belkin et al. 2005), Laplacian smoothing (LAPSMO, Sadhanala et al. 2016) graph semi-supervised learning (SSL, Chapelle et al. 2010; Zhu et al. 2003), laplacian embedding (LE, Belkin & Niyogi 2001, and spectral clustering (SC, Von Luxburg 2007).
2	24	The intuition behind graph-based learning is that the information expressed by the graph helps to capture the underlying structure of the problem (e.g., a manifold), thus improving the learning.
3	25	For instance, LAPSMO and SSL rely on the assumption that nodes that are close in the graph are more likely to have similar labels.
4	74	Similarly, LE and SC try to find a lowdimensional representation of the nodes using the eigenvectors of the Laplacian of the graph.
5	87	In general, given a graph G of n nodes and m edges, most of graph-based learning tasks require computing the minimum of a cost function based on the associated n× n Laplacian matrix LG , which contains m non-zero entries.
7	25	A complete review of the literature on large-scale graph learning is beyond the scope of this paper and we only consider methods that reduce learning space and time complexity starting from a given graph received as input.1 We identify mainly three possible approaches.
8	86	We can (1) reduce runtime replacing the pseudo-inverse operator L+G with an iterative solver, (2) reduce time and space complexity replacing the large graph G with a sparser approximationH, or (3) reduce runtime and increase memory capacity by distributing the computation across multiple machines.
9	20	Iterative methods can solve a number of learning problems without explicitly constructing L+G (e.g., gradient descent, GD, for LAPSMO, iterative averaging for SSL, and the power method for SC).
10	24	In this case we only need O(m) time per iteration.
11	15	Unfortunately, all simple iterative methods (e.g., GD) converge in a number of iterations proportional to the condition number of the Laplacian, κ = λmax(LG)/λmin(LG), which may grow linearly with the number of nodes n, thus removing the advantage of the iterative method, whose complexity tends to O(n3) in the worst case.
12	19	Advanced iterative methods, such as the preconditioned conjugate gradient, use preconditioning to find an accurate solution in a number of iterations independent of κ. Koutis et al. (2011) gives a nearly-linear solver for Laplacians or strongly diagonally dominant (SDD) matrices, that using a chain of preconditioners, converges in only O(m log(n)) time.
13	34	As space and time costs scale with the number of edges, a natural desire is to reduce m by sparsifying and distributing the graph.
15	32	A simple graph-sparsification technique is to sample nq (with q > 1) edges from G with probabilities proportional to the edge weights with replacement.
16	36	While computationally very efficient, uniform sampling requires sampling a number of edges proportional to O(nµ(G)) (i.e., q ∝ µ(G)), where µ(G) is the coherence of the Laplacian matrix, and it can grow as large as n when the graph is highly structured (e.g., if there is a single edge e connecting two components of the graph we need to sample all of the edges of the graph— potentially O(n2)—to guarantee that we do not exclude e and generate an inappropriate H).
17	12	A more refined approach is the k-neighbors (kN) sparsifier (Sadhanala et al., 2016), which performs local sparsifications node-by-node by keeping all edges at nodes with degree smaller than q, and samples them proportionally to their weights whenever the degree is bigger than q.
18	38	While in certain structured graphs, this method may perform much better than uniform (Von Luxburg et al., 2014), in the general case q, still needs to scale with the coherence µ(G).
19	98	A more effective method is to sample edges proportionally to their effective resistance, which intuitively measures the importance of an edge in preserving the minimum distance between two nodes.
21	22	Nonetheless, computing effective resistances also requires the pseudo-inverse L+G , thus being as expensive as solving any graph-Laplacian learning problem.
22	102	When the number of edges m is too large to fit the whole graph in a single machine, we are forced to distribute the edges across multiple machines.
27	18	In this paper, we propose a new approach that aims at integrating the benefits of the three different methods above.
28	206	Using the large memory and computational capacity of distributed computing and leveraging the sequen- tial sparsification methods of Kelner & Levin (2013) and Calandriello et al. (2017), we show how to compute an accurate sparsifierH of graph G inO(n log3(n)) time,O(n log2(n)) work and O(n log(n)) memory, using only log(n) rounds of communication.
29	111	Afterwards, learning tasks can be solved directly on LH on a single machine using near-linear time solvers, resulting in an overallO(n log3(n)) runtime.
30	104	Moreover, we show that the regularization used in some graphbased learning algorithms allows using even sparser graphs.
34	27	We denote with G = (V, E), an undirected weighted graph with n nodes V and m edges E .
36	54	Given graphs G and G′ over the same set of nodes V , G + G′ denotes the graph obtained by summing the weights of their edges.
37	14	For graph G, we introduce the weighted adjacency matrix AG with entries [AG ]i,j = aei,j , the total weights A = ∑ e ae , and the diagonal degree matrix DG with entries [DG ]i,i , ∑ j aei,j .
38	29	The Laplacian of G is the PSD matrix LG , DG − AG .
39	71	Furthermore, we assume that G is connected and thus LG has only one eigenvalue equal to 0 and Ker(LG) = 1.
40	133	Let L+G be the pseudoinverse of LG and L −1/2 G = (L + G ) 1/2.
