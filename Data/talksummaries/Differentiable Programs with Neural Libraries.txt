3	46	In this work we flip the roles of the neural network and differentiable computer architecture.
11	15	Using an illustrative set of PPBE tasks we aim to emphasize two specific benefits of our hybrid models: First, the source code representation in the controller allows modularity: the neural components are small functions that specialize to different tasks within the larger program structure.
14	10	In our experiments, we consider a lifelong learning setting in which we train the system on a sequence of PPBE tasks that share perceptual subtasks.
23	40	The TERPRET language (Gaunt et al., 2016) provides a system for constructing differentiable program interpreters that can induce source code operating on basic data types (e.g. integers) from input-output examples.
27	62	TERPRET TERPRET programs specify a differentiable interpreter by defining the relationship between Inputs and Outputs via a set of inferrable Params (that define an executable program) and Vars (that store intermediate results).
30	87	1 illustrates an example application of the language.
31	33	TERPRET can be translated into a TensorFlow (Abadi et al., 2015) computation graph which can then be trained using standard methods.
33	26	The statement z.set to(foo(x, y)) is translated into µzi = ∑ jk Iijkµ x jµ y k where µ a represents the marginal distribution for the variable a and I is an indicator tensor 1[i = foo(j, k)].
37	43	NEURAL TERPRET To handle perceptual data, we relax the restriction that all variables need to be finite integers.
38	18	We introduce a new floating point Tensor type whose dimensions are fixed at declaration, and which is suitable for storing perceptual data.
39	44	Additionally, we introduce learnable functions that can process integer or tensor variables.
40	45	A learnable function is declared using @Learn([d1, .
45	24	The inputs of the function are simply concatenated.
47	82	Learnable parameters for the generated network are shared across every use of the function in the NTPT program, and as they naturally fit into the computation graph for the remaining TERPRET program, the whole system is trained end-to-end.
48	65	We illustrate an example NTPT program for learning navigation tasks in a maze of street signs (Stallkamp et al., 2011) in Fig.
53	24	The final task in the learning lifetime is more complex and designed to test generalization properties: the system must learn to compute the results of variable-length mathematical expressions expressed using handwritten symbols.
59	43	We set 4 tasks based on this grid: compute the sum of the digits in the (1) top row, (2) left column, (3) bottom row, (4) right column.
80	33	This restriction is imposed because if a differentiable program tries to make a call to one of N untrained networks based on an unknown parameter net choice = Param(N), then the system effectively sees the N nets together with the net choice parameter as one large untrained network, which cannot usefully be split apart into the N components after training.
88	18	• ADD(·, ·): accepts two register addresses and returns the sum of their contents.
101	22	To evaluate the merits of including the source code structure in NTPT models, we build baselines that replace the differentiable program interpreter with neural networks, thereby creating purely neural solutions to the lifelong PPBE tasks.
133	12	We train on batches of data drawn from a time-evolving probability distribution over all 8 tasks in the 2×2 scenarios (see the top of Fig.
137	28	Note that even when we have stopped presenting examples, the performance on this task continues to increase as we train on later tasks - an example of reverse transfer.
147	12	Note that although PNNs are effective at avoiding catastrophic forgetting, there is no clear overall winner between the MTNN and PNN baselines.
151	17	For the NTPT model we train on arithmetic expressions containing only 2 digits.
155	17	The learned source code provably generalizes perfectly to expressions containing any number of digits, and the only limitation on the performance on long expressions comes from the repeated application of the imperfect net 0.
159	33	8(b) shows generalization of the NTPT and LSTM models on expressions of up to 16 digits (31 symbols) after training to convergence.
183	92	We speculate that this could be a reason for the models being resistant to catastrophic forgetting, as the model either chooses to use a classifier, or ignores it (which leaves the component unchanged).
184	43	The second benefit is that learning programs imposes a bias that favors learning models that exhibit strong generalization.
185	15	Additionally, the source code representation has the advantage of being interpretable by humans, allowing verification and incorporation of domain knowledge describing the shape of the problem through the source code structure.
186	11	The primary limitation of this design is that it is known that differentiable interpreters are difficult to train on problems significantly more complex than those presented here (Kurach et al., 2015; Neelakantan et al., 2016; Gaunt et al., 2016).
