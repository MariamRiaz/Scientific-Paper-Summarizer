0	47	Humor is one of the most interesting and puzzling research areas in the field of natural language understanding.
2	72	When a computer converses with a human being, if it can figure out the humor in human’s language, it can better understand the true meaning of human language, and thereby make better decisions that improve the user experience.
4	83	The task of Humor Recognition refers to determining whether a sentence in a given context expresses a certain degree of humor.
48	19	The dataset we use to conduct our humor recognition experiments includes two parts: Pun of the Day 1 and the 16000 One-Liner dataset (Mihalcea and Strapparava, 2005).
49	18	The two data sets only contain humorous text.
50	28	In order to acquire negative samples for the humor classification task, we sample negative samples from four resources, including AP News2, New York Times, Yahoo!
53	20	For example, the humor sentences in our positive datasets often relate to daily lives, such as “My wife tells me I’m a skeptic, but I don’t believe a word she says.”.
56	29	To deal with this issue, we extract our negative instances in a way that tries to minimize such domain differences by (1) selecting negative instances whose words are all contained in our positive instance word dictionary and (2) forcing the text length of non-humorous instances to follow the similar length restriction as humorous examples, i.e. one sentence with an average length of 10-30 words.
70	35	Humor and ambiguity often come together when a listener expects one meaning, but is forced to use another meaning.
74	22	To capture the ambiguity contained in a sentence, we utilize the lexical resource WordNet (Fellbaum, 1998) and capture the ambiguity as follows: • Sense Combination: the sense combination in a sentence computed as follows: we first use a POS tagger (Toutanova et al., 2003) to identify Noun, Verb, Adj, Adv.
104	21	Therefore, formally defined, a humor anchor is a meaningful, complete, minimal set of word spans in a sentence that potentially enable one of the latent structures of Section 4 to occur.
105	22	(1) Meaningful means humor anchors are meaningful word spans, not meaningless stop words in a sentence; (2) Completeness shows that all possible humor anchors should be covered by this anchor set and no individual span in this anchor set is capable enough to enable humor; (3) Minimal emphasizes that it is the combination of these anchors together that prompts comic effect; discarding any anchors from this candidate set destroys the humorous effect.
106	34	Based on the humor anchor requirements listed above, we scoped humor anchor candidates to words or phrases that belong to the syntactic categories of Noun, Verb, Noun Phrase, Verb Phrase, ADVP or ADJP.
112	22	Its basic idea is summarized as follows: Each complete sentence has a predicted humor score, which is computed via a humor recognition classifier trained on all data points.
122	19	The size of Ki should be smaller than a threshold t, |Ki| ≤ t. f(Xi/Ki) is the recomputed humor score for sentence i after removing Ki.
124	25	The subset Ki that gives the maximal decrement is returned as our extracted humor anchors for sentence i.
126	17	In this section, we validate the performance of different semantic structures we extracted on humor recognition and how the combination of the structures contributes to classification.
139	41	It is evident that Incongruity performs the best among all latent semantic structures in the context of Pun of the Day and both Ambiguity and Phonetic substantially contribute to recognition performance on the 16000 One Liners dataset.
141	20	Most puns are well structured and play with contrasting or incongruous meaning.
145	35	Here, we denote the combination of four latent structures and KNN features as Human Centric Features (HCF).
155	43	The classifier that is used to predict the humor score is trained on all data instances.
159	42	As we can see, extracted humor anchors are quite reasonable in explaining the humor causes or focuses.
168	22	Ai and Bi are the humor anchor sets of sentence i provided by annotator A and B respectively.
169	71	The AARs on Pun of the Day and 16000 One Liners datasets are 0.618 and 0.433 respectively, computed by averaging the AAR scores between any two different annotators, which indicate relatively reasonable agreement.
171	164	The Random Extraction baseline selects humor anchors by sampling words in a sentence randomly.
172	17	Similarly, POS Extraction baseline generates anchors by narrowing down all the words in a sentence to a set of certain POS, e.g. Noun, Verb, Noun Phrase, Verb Phrase, ADVP and ADJP and then sampling words from this set.
174	27	To identify whether two anchors are the same, we introduce two measurements: Exact (EX) Matching and At-Least-One (ALO) Matching.
176	50	For ALO, two anchors are considered the same if they have at least one word in common.
178	83	We then average the three annotators’ individual scores to get the final extraction performance.
184	36	The above two subsections described the performance of both humor recognition and humor anchor extraction tasks.
