0	13	Neural machine translation (NMT) emerged in the last few years as a very successful paradigm (Sutskever et al., 2014; Bahdanau et al., 2014; Gehring et al., 2017; Vaswani et al., 2017).
1	34	While NMT is generally more fluent than previous statistical systems, adequacy is still a major concern (Koehn and Knowles, 2017): common mistakes include dropping source words and repeating words in the generated translation.
2	36	Previous work has attempted to mitigate this problem in various ways.
3	15	Wu et al. (2016) incorporate coverage and length penalties during beam search—a simple yet limited solution, since it only affects the scores of translation hypotheses that are already in the beam.
4	28	Other approaches involve architectural changes: providing coverage vectors to track the attention history (Mi et al., 2016; Tu et al., 2016), using gating architectures and adaptive attention to control the amount of source context provided (Tu et al., 2017a; Li and Zhu, 2017), or adding a reconstruction loss (Tu et al., 2017b).
5	14	Feng et al. (2016) also use the notion of fertility implicitly in their proposed model.
8	19	Namely, we replace the traditional softmax by other recently proposed transformations that either promote attention sparsity (Martins and Astudillo, 2016) or upper bound the amount of attention a word can receive (Martins and Kreutzer, 2017).
14	12	This transformation has two levels of sparsity: over time steps, and over the attended words at each step.
16	26	max (Martins and Astudillo, 2016), constrained softmax (Martins and Kreutzer, 2017), and our newly proposed constrained sparsemax.
19	29	Let x := x1:J and y := y1:T denote the source and target sentences, respectively.
23	47	This vector is computed as ct := Hαt, where αt is a probability distribution that represents the attention over the source words, commonly obtained as αt = softmax(zt), (2) where zt ∈ RJ is a vector of scores.
32	16	In words, it is the Euclidean projection of the scores z onto the probability simplex.
33	43	These projections tend to hit the boundary of the simplex, yielding a sparse probability distribution.
36	21	The constrained softmax transformation was recently proposed by Martins and Kreutzer (2017) in the context of easy-first sequence tagging, being defined as follows: csoftmax(z;u) := argmin α∈∆J KL(α‖ softmax(z)) s.t.
38	16	is the Kullback-Leibler divergence.
39	32	In other words, it returns the distribution closest to softmax(z) whose attention probabilities are bounded by u. Martins and Kreutzer (2017) have shown that this transformation can be evaluated in O(J log J) time and its gradients backpropagated in O(J) time.
41	33	Namely, let βt−1 :=∑t−1 τ=1ατ denote the cumulative attention that each source word has received up to time step t, and let f := (fj)Jj=1 be a vector containing fertility upper bounds for each source word.
42	15	The attention at step t is computed as αt = csoftmax(zt,f − βt−1).
60	14	The reason we add this token is the following: without the sink token, the length of the generated target sentence can never exceed ∑ j fj words if we use constrained softmax/sparsemax.
63	17	To avoid missing source words, we implemented a simple strategy to encourage more attention to words with larger credit: we redefine the pre-attention word scores as z′t = zt + cut, where c is a constant (c = 0.2 in our experiments).
66	28	We focused on small datasets, as they are the most affected by coverage mistakes.
88	12	Then, the DROP-score computes the percentage of source words that aligned with some word from the reference translation, but not with any word from the predicted translation.
90	47	Generally, they also obtain better REP and DROP scores than csoftmax and softmax, which suggests that sparse attention alleviates the problem of coverage to some extent.
92	99	We see that the PREDICTED strategy outperforms the others both in terms of BLEU and METEOR, albeit slightly.
94	49	We see that in the case of softmax repetitions, the decoder attends repeatedly to the same portion of the source sentence (the expression “letzten hundert” in the first sentence and “regierung” in the second sentence).
95	25	Not only did csparsemax avoid repetitions, but it also yielded a sparse set of alignments, as expected.
96	18	Appendix B provides more examples of translations from all models in discussion.
97	18	We proposed a new approach to address the coverage problem in NMT, by replacing the softmax attentional transformation by sparse and constrained alternatives: sparsemax, constrained softmax, and the newly proposed constrained sparsemax.
98	16	For the latter, we derived efficient forward and backward propagation algorithms.
99	36	By incorporating a model for fertility prediction, our attention transformations led to sparse alignments, avoiding repeated words in the translation.
