{"sections": [{"heading": "1. Introduction", "text": "Low-rank matrix recovery problem has been extensively studied during the past decades, due to its wide range of applications, such as collaborative filtering (Srebro et al., 2004; Rennie & Srebro, 2005) and multi-label learning (Cabral et al., 2011; Xu et al., 2013). The objective of low-rank matrix recovery is to estimate the unknown lowrank matrix X\u21e4 2 Rd1\u21e5d2 from partial observations, such as a set of linear measurements in matrix sensing or a subset of its entries in matrix completion. Significant efforts have been made to estimate low-rank matrices, among which one of the most prevalent approaches is nuclear norm re-\n*Equal contribution 1Department of Computer Science, University of Virginia, Charlottesville, Virginia, USA. Correspondence to: Quanquan Gu <qg5w@virginia.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nlaxation based optimization (Srebro et al., 2004; Cande\u0300s & Tao, 2010; Rohde et al., 2011; Recht et al., 2010; Negahban & Wainwright, 2011; 2012; Gui & Gu, 2015). While such convex relaxation based methods enjoy a rigorous theoretical guarantee to recover the unknown low-rank matrix, due to the nuclear norm regularization/minimization, these algorithms involve a singular value decomposition at each iteration, whose time complexity is O(d3) to recover a d\u21e5d matrix. Hence, they are computationally very expensive.\nIn order to address the aforementioned computational issue, recent studies (Keshavan et al., 2009; 2010; Jain et al., 2013a; Jain & Netrapalli, 2014; Hardt, 2014; Hardt & Wootters, 2014; Hardt et al., 2014; Zhao et al., 2015; Chen & Wainwright, 2015; Sun & Luo, 2015; Zheng & Lafferty, 2015; 2016; Tu et al., 2015; Bhojanapalli et al., 2015; Park et al., 2016b; Wang et al., 2016) have been carried out to perform factorization on the matrix space, which naturally ensures the low-rankness of the produced estimator. Although this matrix factorization technique converts the previous optimization problem into a nonconvex one, which is more difficult to analyze, it significantly improves the computational efficiency.\nHowever, for large-scale matrix recovery, such nonconvex optimization approaches are still computationally expensive, because they are based on gradient descent or alternating minimization, which involve the time-consuming calculation of full gradient at each iteration. De Sa et al. (2014) developed a stochastic gradient descent approach for Gaussian ensembles, but the sample complexity (i.e., number of measurements or observations required for exact recovery) of their algorithm is not optimal. Recently, Jin et al. (2016) and Zhang et al. (2017b) proposed stochastic gradient descent algorithms for noiseless matrix completion and matrix sensing, respectively. Although these algorithms achieve linear rate of convergence and improved computational complexity over aforementioned deterministic optimization based approaches, they are limited to specific low-rank matrix recovery problems, and unable to be extended to more general problems and settings.\nIn this paper, inspired by the idea of variance reduction for stochastic gradient (Schmidt et al., 2013; Konec\u030cny\u0300 & Richta\u0301rik, 2013; Johnson & Zhang, 2013; Defazio et al.,\n2014a;b; Mairal, 2014; Xiao & Zhang, 2014; Konec\u030cny\u0300 et al., 2014; Reddi et al., 2016; Allen-Zhu & Hazan, 2016; Chen & Gu, 2016; Zhang & Gu, 2016), we propose a unified stochastic gradient descent framework with variance reduction for low-rank matrix recovery, which integrates both optimization-theoretic and statistical analyses. To the best of our knowledge, this is the first unified accelerated stochastic gradient descent framework for low-rank matrix recovery with strong convergence guarantees. With a desired initial estimator given by a general initialization algorithm, we show that our algorithm achieves linear convergence rate and better computational complexity against the state-of-the-art algorithms. The contributions of our work are further highlighted as follows:\n1. We develop a generic stochastic variance-reduced gradient descent algorithm for low-rank matrix recovery, which can be applied to various low rank-matrix estimation problems, including matrix sensing, noisy matrix completion and one-bit matrix completion. In particular, for noisy matrix sensing, it is guaranteed to linearly converge to the unknown low-rank matrix up to the minimax statistical precision (Negahban & Wainwright, 2011; Wang et al., 2016); while for noiseless matrix sensing, our algorithm achieves the optimal sample complexity (Recht et al., 2010; Tu et al., 2015; Wang et al., 2016), and attains a linear rate of convergence. Besides, for noisy matrix completion, it achieves the best-known sample complexity required by nonconvex matrix factorization (Zheng & Lafferty, 2016).\n2. At the core of our algorithm, we construct a novel semi-stochastic gradient term, which is substantially different from the one if following the original stochastic variance-reduced gradient using chain rule (Johnson & Zhang, 2013). This uniquely constructed semi-stochastic gradient has not appeared in the literature, and is essential for deriving the minimax optimal statistical rate.\n3. Our unified framework is built upon the mild restricted strong convexity and smoothness conditions (Negahban et al., 2009; Negahban & Wainwright, 2011) regarding the objective function. Based on the above mentioned conditions, we derive an innovative projected notion of the restricted Lipschitz continuous gradient property, which we believe is of independent interest for other nonconvex problems to prove sharp statistical rates. We further establish the linear convergence rate of our generic algorithm. Besides, for each specific examples, we verify that the conditions required in the generic setting are satisfied with high probability, which demonstrates the applicability of our framework.\n4. Our algorithm has a lower computational complexity compared with existing approaches (Jain et al., 2013a; Zhao et al., 2015; Chen & Wainwright, 2015; Zheng & Lafferty, 2015; 2016; Tu et al., 2015; Bhojanapalli et al., 2015;\nPark et al., 2016b; Wang et al., 2016). More specifically, to achieve \u270f precision, the gradient complexity1 of our algorithm is O (N + 2b) log(1/\u270f)\n. Here N denotes the total number of observations, d denotes the dimensionality of the unknown low-rank matrix X\u21e4, b denotes the batch size, and  denotes the condition number of X\u21e4 (see Section 2 for a detailed definition). In particular, if the condition number satisfies   N/b, our algorithm is computationally more efficient than the state-of-the-art generic algorithm in Wang et al. (2016).\nNotation. We use [d] and I d to denote {1, 2, . . . , d} and d\u21e5d identity matrix respectively. We write A>A = I\nd2 , if A 2 Rd1\u21e5d2 is orthonormal. For any matrix A 2 Rd1\u21e5d2 , we use A\ni,\u21e4 and A\u21e4,j to denote the i-th row and j-th column of A, respectively. In addition, we use A\nij to denote the (i, j)-th element of A. Denote the row space and column space of A by row(A) and col(A) respectively. Let d = max{d\n1 , d 2 }, and ` (A) be the `-th largest singular value of A. For vector x 2 Rd, we use kxk\nq\n=\n(\u2303\nd\ni=1\n|x i |q)1/q to denote its ` q vector norm for 0 < q < 1. Denote the spectral and Frobenius norm of A by kAk\n2 and kAk\nF respectively. We use kAk1,1 = maxi,j |Aij | to denote the element-wise infinity norm of A, and we use kAk\n2,1 to represent the largest `2-norm of its rows. Given two sequences {a\nn } and {b n }, we write a n = O(b n ) if there exists a constant 0 < C\n1 < 1 such that a n  C 1 b n . Note that other notations are defined throughout the paper."}, {"heading": "2. Methodology", "text": "In this section, we present our generic stochastic gradient descent algorithm with variance reduction as well as several illustrative examples."}, {"heading": "2.1. Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery", "text": "First, we briefly introduce the general problem setup for low-rank matrix recovery. Suppose X\u21e4 2 Rd1\u21e5d2 is an unknown rank-r matrix. Let the singular value decomposition (SVD) of X\u21e4 be X\u21e4 = U \u21e4 \u2303\u21e4V \u21e4> , where U\n\u21e4 2 Rd1\u21e5r, V\n\u21e4 2 Rd2\u21e5r are orthonormal matrices, and \u2303\u21e4 2 Rr\u21e5r is a diagonal matrix. Let\n1\n2\n\u00b7 \u00b7 \u00b7 r 0 be the sorted nonzero singular values of X\u21e4, and denote the condition number of X\u21e4 by , i.e.,  =\n1 / r . Besides, let U\u21e4 = U \u21e4 (\u2303\u21e4)1/2 and V\u21e4 = V \u21e4 (\u2303\u21e4)1/2. Recall that we aim to recover X\u21e4 through a collection of N observations or measurements. Let L\nN : Rd1\u21e5d2 ! R be the sample loss function, which evaluates the fitness of any matrix X associated with the total N observations. Then the low-rank\n1Gradient complexity is defined as the number of gradients calculated in total.\nmatrix recovery problem can be formulated as follows:\nminX2Rd1\u21e5d2 LN (X) := 1 N\nP\nN\ni=1\n` i (X),\nsubject to X 2 C, rank(X)  r, (2.1)\nwhere ` i (X) measures the fitness of X associated with the i-th observation. Here C \u2713 Rd1\u21e5d2 is a feasible set, such that X\u21e4 2 C. In order to more efficiently estimate the unknown low-rank matrix, following Jain et al. (2013a); Tu et al. (2015); Zheng & Lafferty (2016); Park et al. (2016a); Wang et al. (2016), we decompose X as UV> and consider the following nonconvex optimization problem via matrix factorization:\nminU2C1,V2C2 LN (UV>) := 1 N P N i=1 ` i (UV>), (2.2)\nwhere C 1 \u2713 Rd1\u21e5r, C 2 \u2713 Rd2\u21e5r are the rotation-invariant sets induced by C. Recall X\u21e4 can be factorized as X\u21e4 = U\u21e4V\u21e4>, then we need to make sure that U\u21e4 2 C\n1 and V\u21e4 2 C\n2 . Besides, it can be seen from (2.2) that the optimal solution is not unique in terms of rotation. In order to deal with such identifiability issue, following Tu et al. (2015); Zheng & Lafferty (2016); Park et al. (2016b), we consider the following regularized optimization problem:\nminU2C1,V2C2 FN (U,V) := LN (UV>) +R(U,V),\nwhere the regularization term is defined as R(U,V) = kU>U V>Vk2\nF /8. We further decompose the objective function F\nN (U,V) into n components to apply stochastic variance-reduced gradient descent:\nF N (U,V) := 1 n\nP\nn\ni=1\nF i (U,V), (2.3)\nwhere we assume N = nb, and b denotes batch size, i.e., the number of observations associated with each F\ni . More specifically, we have\nF i (U,V) = L i (UV>) +R(U,V),\nL i (UV>) = 1 b P b j=1 ` ij (UV > ). (2.4)\nTherefore, based on (2.3) and (2.4), we are able to apply the stochastic variance-reduced gradient, which is displayed as Algorithm 1. As will be seen in later theoretical analysis, the variance of the proposed stochastic gradient indeed decreases as the iteration number increases, which leads to a faster convergence rate. Let PCi be the projection operator onto the feasible set C\ni\nin Algorithm 1, where i 2 {1, 2}.\nNote that our proposed Algorithm 1 is different from the standard stochastic variance-reduced gradient algorithm (Johnson & Zhang, 2013) in several aspects. First, instead of conducting gradient descent directly on X, our algorithm performs alternating stochastic gradient descent on the factorized matrices U and V, which leads to a better computational complexity but a more challenging analysis. Second,\nwe construct a novel semi-stochastic gradient term for U (resp. V) as rUFit(U,V) rLit(eX)V +rLN (eX)V, which is different from rUFit(U,V) rUFit(eU, eV) + rUFN (eU, eV) if following the original stochastic variance reduced gradient descent (Johnson & Zhang, 2013). This uniquely devised semi-stochastic gradient is essential for deriving the minimax optimal statistical rate. Last but not least, we introduce a projection step to ensure that the estimator produced at each iteration belongs to a feasible set, which is necessary for various low-rank matrix recovery problems. We also note that Reddi et al. (2016); Allen-Zhu & Hazan (2016) recently developed SVRG algorithms for general nonconvex finite-sum optimization problem. However, their algorithms only guarantee a sublinear rate of convergence to a stationary point, and cannot exploit the special structure of low-rank matrix factorization. In stark contrast, our algorithm is able to leverage the structure of the problem and guaranteed to linearly converge to the unknown low-rank matrix instead of a stationary point. Algorithm 1 Low-Rank Stochastic Variance-Reduced Gradient Descent (LRSVRG) Input: loss function L\nN ; step size \u2318; number of iterations S,m; initial solution (eU0, eV0). for: s = 1, 2, . . . , S do\neU = eUs 1, eV = eVs 1, eX = eUeV> U0 = eU, V0 = eV for: t = 0, 1, 2, . . . ,m 1 do\nRandomly pick i t 2 {1, 2, . . . , n} Ut+1 = PC1\nUt \u2318(rUFit(Ut,Vt) rL\nit( eX)Vt +rL N ( eX)Vt)\nVt+1 = PC2 Vt \u2318(rVFit(Ut,Vt) rL\nit( eX)>Ut +rL N ( eX)>Ut)\nend for (\neUs, eVs) = (Ut,Vt), random t 2 {0, . . . ,m 1} end for\nOutput: (eUS , eVS).\nAlgorithm 2 Initialization Input: loss function L\nN ; step size \u2327 ; iteration number T . initialize: X\n0 = 0 for: t = 1, 2, 3, . . . , T do X\nt = P r X t 1 \u2327rLN (Xt 1)\nend for [U 0 ,\u23030,V 0 ] = SVD r (X T )\neU0 = U 0 (\u23030)1/2, eV0 = V 0 (\u23030)1/2\nOutput: (eU0, eV0)\nAs will be seen in later analysis, Algorithm 1 requires a good initial solution to guarantee the linear convergence rate. To obtain such an initial solution, we employ the initialization algorithm in Algorithm 2, which is originally proposed in Wang et al. (2016). For any rank-r matrix X 2 Rd1\u21e5d2 , we use SVD\nr\n(X) to denote its singular\nvalue decomposition. If SVD r (X) = [U,\u2303,V], we use P r\n(X) = U\u2303V> to denote the best rank-r approximation of X, or in other words, P\nr denotes the projection operator such that P\nr (X) = argmin rank(Y)r kX YkF ."}, {"heading": "2.2. Applications to Specific Models", "text": "In this subsection, we introduce two examples, which include matrix sensing and matrix completion, to illustrate the applicability of our proposed algorithm (Algorithm 1). The application of our algorithm to one-bit matrix completion can be found in Appendix A. To apply the proposed method, we only need to specify the form of F\nN (U,V) for each specific model, as defined in (2.3)."}, {"heading": "2.2.1. MATRIX SENSING", "text": "In matrix sensing (Recht et al., 2010; Negahban & Wainwright, 2011), we intend to recover the unknown matrix X\u21e4 2 Rd1\u21e5d2 with rank-r from a set of noisy linear measurements such that y = A(X\u21e4) + \u270f, where the linear measurement operator A : Rd1\u21e5d2 ! RN is defined as A(X) = (hA\n1 ,Xi, hA 2 ,Xi, . . . , hA N ,Xi)>, for any X 2 Rd1\u21e5d2 . Here N denotes the number of observations, and \u270f represents a sub-Gaussian noise vector with i.i.d. elements and parameter \u232b. In addition, for each sensing matrix A\ni 2 Rd1\u21e5d2 , it has i.i.d. standard Gaussian entries. Therefore, we formulate F\nN (U,V) for matrix sensing as follows F\nN\n(U,V) = n 1 P n\ni=1 FSi(U,V), where for each component function, we have FSi(U,V) = kySi ASi(UV>)k22/(2b) + R(U,V). Note that R(U,V) denotes the regularizer, which is defined in Section 2.1. In addition, {S\ni }n i=1 denote the mutually disjoint subsets such that [n\ni=1\nS i = [N ], and ASi is defined as a linear measurement operator ASi : Rd1\u21e5d2 ! Rb, satisfying ASi(X) = (hA\ni1 ,Xi, hAi2 ,Xi, . . . , hAib ,Xi)>, with corresponding observations ySi = (yi1 , yi2 , . . . , yib)>."}, {"heading": "2.2.2. MATRIX COMPLETION", "text": "For matrix completion with noisy observations (Rohde et al., 2011; Koltchinskii et al., 2011; Negahban & Wainwright, 2012), our primary goal is to recover the unknown low-rank matrix X\u21e4 2 Rd1\u21e5d2 from a set of randomly observed noisy elements. For example, one commonly-used model is the uniform observation model, which is defined as follows:\nY jk :=\n\u21e2 X\u21e4 jk + Z jk , with probability p, \u21e4, otherwise,\nwhere Z 2 Rd1\u21e5d2 is a noise matrix such that each element Z\njk follows i.i.d. Gaussian distribution with variance \u232b2/(d\n1 d 2 ), and we call Y 2 Rd1\u21e5d2 the observation matrix. In particular, we observe each elements independently with probability p 2 (0, 1). We\ndenote \u2326 \u2713 [d 1 ] \u21e5 [d 2 ] by the index set of the observed entries, then F\n\u2326 (U,V) for matrix completion is formulated as F\n\u2326\n(U,V) = n 1 P n\ni=1 F \u2326Si (U,V), where each component function is defined as F\n\u2326Si (U,V) =\nP\n(j,k)2\u2326Si (U\nj\u21e4V> k\u21e4 Yjk)2/(2b) +R(U,V). Note that\n{\u2326Si}n i=1 denote the mutually disjoint subsets such that [n i=1\n\u2326Si = \u2326. In addition, we have |\u2326Si | = b for i = 1, . . . , n such that |\u2326| = nb."}, {"heading": "3. Main Theory", "text": "In this section, we present our main theoretical results for Algorithms 1 and 2. We first introduce several definitions for simplicity. Recall that the singular value decomposition of X\u21e4 is X\u21e4 = U \u21e4 \u2303\u21e4V \u21e4> , then following Tu et al. (2015); Zheng & Lafferty (2016), we define Y\u21e4 2 R(d1+d2)\u21e5(d1+d2) as the corresponding lifted positive semidefinite matrix of X\u21e4 2 Rd1\u21e5d2 in higher dimension\nY\u21e4 =\n U\u21e4U\u21e4> U\u21e4V\u21e4>\nV\u21e4U\u21e4> V\u21e4V\u21e4>\n= Z\u21e4Z\u21e4>,\nwhere U\u21e4 = U \u21e4 (\u2303\u21e4)1/2, V\u21e4 = V \u21e4 (\u2303\u21e4)1/2, and Z\u21e4 is defined as Z\u21e4 = [U\u21e4;V\u21e4] 2 R(d1+d2)\u21e5r. Besides, we define the solution set in terms of the true parameter Z\u21e4 as follows:\nZ = n Z 2 R(d1+d2)\u21e5r Z = Z\u21e4R for some R 2 Q r o ,\nwhere Q r denotes the set of r \u21e5 r orthonormal matrices. According to this definition, for any Z 2 Z , we can obtain X\u21e4 = Z\nU Z> V , where Z U and Z V denote the top d 1 \u21e5 r and bottom d\n2 \u21e5 r matrices of Z 2 R(d1+d2)\u21e5r respectively. Definition 3.1. Define the distance between Z and Z\u21e4 in terms of the optimal rotation as d(Z,Z\u21e4) such that\nd(Z,Z\u21e4) = min eZ2Z kZ eZk F = min R2Qr kZ Z\u21e4Rk F .\nNote that if d(Z,Z\u21e4)  p 1 , we have kX X\u21e4k F\n c p 1\nd(Z,Z\u21e4), where c is a constant (Yi et al., 2016). Definition 3.2. Define the neighbourhood of Z\u21e4 with radius R as\nB(R) = n Z 2 R(d1+d2)\u21e5r d(Z,Z\u21e4)  R o .\nNext, we lay out several conditions, which are essential for proving our main theory. We impose restricted strong convexity (RSC) and smoothness (RSS) conditions (Negahban et al., 2009; Loh & Wainwright, 2013) on the sample loss function L\nN . Condition 3.3 (Restricted Strong Convexity). Assume L\nN\nis restricted strongly convex with parameter \u00b5, such that for all matrices X,Y 2 Rd1\u21e5d2 with rank at most 3r\nL N (Y) L N (X) + hrL N (X),Y Xi+ \u00b5 2 kY Xk2 F .\nCondition 3.4 (Restricted Strong Smoothness). Assume L N\nis restricted strongly smooth with parameter L, such that for all matrices X,Y 2 Rd1\u21e5d2 with rank at most 3r\nL N (Y)  L N (X) + hrL N (X),Y Xi+ L 2 kY Xk2 F .\nBased on Conditions 3.3 and 3.4, we prove that the sample loss function L\nN satisfies a projected notion of the restricted Lipschitz continuous gradient property as displayed in the following lemma. Lemma 3.5. Suppose the sample loss function L\nN satisfies Conditions 3.3 and 3.4. For any rank-r matrices X,Y 2 Rd1\u21e5d2 , let the singular value decomposition of X be U\n1 \u2303 1 V > 1 , then we have\nL N (X) L N (Y) + hrL N (Y),X Yi\n+\n1 4L keU>(rL N (X) rL N (Y))k2 F\n+\n1 4L k(rL N (X) rL N (Y))eVk2 F ,\nwhere eU 2 Rd1\u21e5r1 is an orthonormal matrix with r 1  3r which satisfies col(U\n1 ) \u2713 col(eU), and eV 2 Rd2\u21e5r2 is an orthonormal matrix with r\n2  3r that satisfies col(V 1 ) \u2713 col(eV), and L is the RSS parameter.\nLemma 3.5 is essential to analyze the nonconvex optimization for low-rank matrix recovery and derive a linear convergence rate. Since the RSC and RSS conditions can only be verified over the subspace of low-rank matrices, the standard Lipschitz continuous gradient property could not be derived. That is why we need such a restricted version of Lipschitz continuous gradient property. To the best of our knowledge, this new notion of Lipschitz continuous gradient has never been proposed in the literature before. We believe it can be of broader interests for other nonconvex optimization problems to prove tight bounds.\nMoreover, we assume that the gradient of the sample loss function rL\nN at X\u21e4 is upper bounded. Condition 3.6. Recall the unknown rank-r matrix X\u21e4 2 Rd1\u21e5d2 . Given a fixed sample size N and tolerance parameter 2 (0, 1), we let \u270f(N, ) be the smallest scalar such that with probability at least 1 , we have\nkrL N (X\u21e4)k 2  \u270f(N, ),\nwhere \u270f(N, ) depends on sample size N and .\nFinally, we assume that each component loss function L i in (2.4) satisfies the restricted strong smoothness condition. Condition 3.7 (Restricted Strong Smoothness for each Component). Given a fixed batch size b, assume L\ni is restricted strongly smooth with parameter L0, such that for\nall matrices X,Y 2 Rd1\u21e5d2 with rank at most 3r\nL i (Y)  L i (X) + hrL i (X),Y Xi+ L 0\n2\nkY Xk2 F .\nIn latter analysis for generic setting, we assume that Conditions 3.3-3.7 hold, while for each specific model, we will verify these conditions respectively in the appendix."}, {"heading": "3.1. Results for the Generic Setting", "text": "The following theorem shows that, in general, Algorithm 1 converges linearly to the unknown low-rank matrix X\u21e4 up to a statistical precision. Theorem 3.8 (LRSVRG). Suppose that Conditions 3.3, 3.4, 3.6, and 3.7 are satisfied. There exist constants c 1 , c 2 , c 3 and c 4\nsuch that for any eZ0 = [eU0; eV0] 2 B(c\n2 p r ) with c 2  min{1/4, p\n2\u00b50/(5(3L+ 1))}, if the sample size N is large enough such that \u270f2(N, )  c2 2 (1 \u21e2)\u00b50 2 r /(c 3\nr), where \u00b50 = min{\u00b5, 1}, and the contraction parameter \u21e2 is defined as follows:\n\u21e2 = 10\n\u00b50\n\u2713\n1\n\u2318m 1\n+ c 4 \u2318 1\nL02 \u25c6 ,\nthen with the step size \u2318 = c 1 / 1 and the number of iterations m properly chosen, the estimator eZS = [eUS ; eVS ] outputed from Algorithm 1 satisfies\nE \u21e5 d2(eZS ,Z\u21e4) \u21e4  \u21e2Sd2(eZ0,Z\u21e4) + c3r\u270f 2 (N, )\n(1 \u21e2)\u00b50 r\n, (3.1)\nwith probability at least 1 . Remark 3.9. Theorem 3.8 implies that to achieve linear rate of convergence, it is necessary to set the step size \u2318 to be small enough and the inner loop iterations m to be large enough such that \u21e2 < 1. Here we present a specific example to demonstrate such \u21e2 is attainable. As stated in Theorem 3.8, if we set the step size \u2318 = c0\n1\n/ 1 , where c0 1 = \u00b50/ 15c 4 L02\n, then the contraction parameter \u21e2 is calculated as follows:\n\u21e2 = 10\nm\u00b50\u2318 1 +\n2\n3\n.\nTherefore, under the condition that m c 5 2, we obtain \u21e2  5/6 < 1, which leads to the linear convergence rate of Algorithm 1. Besides, our algorithm also achieves the linear convergence in terms of reconstruction error, since the reconstruction error keXs X\u21e4k2\nF can be upper bounded by C\n1 \u00b7 d2(eZs,Z\u21e4), where C is a constant. Remark 3.10. The right hand side of (3.1) consists of two parts, where the first one represents the optimization error and the second one denotes the statistical error. Note that in the noiseless case, since \u270f(N, ) = 0, the statistical error becomes zero. As stated in Remark 3.9, with\nappropriate \u2318 and m, we are able to achieve the linear rate of convergence. Therefore, in order to make sure the optimization error satisfies \u21e2Sd2(eZ0,Z\u21e4)  \u270f, it suffices to perform S = O log(1/\u270f)\nouter loop iterations. Recall that from Remark 3.9 we have m = O(2). Since for each outer loop iteration, it is required to calculate m mixed stochastic variance-reduced gradients and one full gradient, the overall gradient complexity for our algorithm to achieve \u270f precision is\nO \u21e3 (N + 2b) log \u21e3 1\n\u270f\n\u2318\u2318\n.\nHowever, the gradient complexity of the state-of-the-art gradient descent based algorithm (Wang et al., 2016) to achieve \u270f precision is O N log(1/\u270f)\n. Therefore, provided that   n, our method is computationally more efficient than the state-of-the-art gradient descent approach. The detailed comparison of the overall computational complexity among different methods for each specific model can be found in next subsection.\nTo satisfy the initial condition eZ0 2 B(c 2 p r\n) in Theorem 3.8, according to Lemma 5.14 in Tu et al. (2015), it suffices to guarantee that eX0 is close enough to the unknown rankr matrix X\u21e4 such that keX0 X\u21e4k\nF  c r , where c  min{1/2, 2c\n2 }. The following theorem shows the output of Algorithm 2 can satisfy this condition. Theorem 3.11. (Wang et al., 2016) Suppose the sample loss function L\nN satisfies Conditions 3.3, 3.4 and 3.6. Let eX0 = eU0 eV0>, where (eU0, eV0) is the produced initial solution in Algorithm 2. If L/\u00b5 2 (1, 4/3), then with step size \u2327 = 1/L, we have with probability at least 1 that\nkeX0 X\u21e4k F  \u21e2T kX\u21e4k F + 2\np 3r\u270f(N, )\nL(1 \u21e2) ,\nwhere \u21e2 = 2 p 1 \u00b5/L is the contraction parameter.\nTheorem 3.11 suggests that, in order to guarantee keX0 X\u21e4k\nF  c r , we need to perform at least T = log(c0\nr /kX\u21e4k F\n)/ log(\u21e2) number of iterations to ensure the optimization error is small enough, and it is also necessary to make sure the sample size N is large enough such that \u270f(N, )  c0L(1 \u21e2)\nr\n/ 2 p 3r\n, which corresponds to a sufficiently small statistical error."}, {"heading": "3.2. Implications for Specific Models", "text": "In this subsection, we demonstrate the implications of our generic theory to specific models. For each specific model, we only need to verify Conditions 3.3-3.7. We denote d = max{d\n1 , d 2\n} in the following discussions."}, {"heading": "3.2.1. MATRIX SENSING", "text": "We provide the theoretical guarantee of our algorithm for matrix sensing.\nCorollary 3.12. Consider matrix sensing with standard normal linear operator A and noise vector \u270f, whose entries follow i.i.d. sub-Gaussian distribution with parameter \u232b. There exist constants {c\ni }8 i=1 such that if the number of observations satisfies N c\n1 rd and we choose the parameters \u2318 = c\n2 / 1 , where c 2 = \u00b50/ c 3  , m c 4\n2, then for any initial solution satisfies eZ0 2 B(c\n5 p r\n), with probability at least 1 c\n6\nexp\nc 7 d , the output of Algorithm 1 satisfies\nE \u21e5 d2(eZS ,Z\u21e4) \u21e4  \u21e2Sd2(eZ0,Z\u21e4) + c 8 \u232b2 rd\nN , (3.2)\nwhere the contraction parameter \u21e2 < 1. Remark 3.13. According to (3.2), in the noisy setting, the output of our algorithm achieves O p rd/N\nstatistical error after O log(N/(rd))\nnumber of outer loop iterations. This statistical error matches the minimax lower bound for matrix sensing (Negahban & Wainwright, 2011). In the noiseless case, to ensure the restricted strong convexity and smoothness conditions of our objective function, we require sample size N = O(rd), which attains the optimal sample complexity for matrix sensing (Recht et al., 2010; Tu et al., 2015; Wang et al., 2016). Most importantly, from Remark 3.10 we know that for the output eZS of our algorithm, the overall computational complexity of our algorithm to achieve \u270f precision for matrix sensing is O (Nd2 + 2bd2) log(1/\u270f)\n. Nevertheless, the overall computational complexity for the state-of-the-art gradient descent algorithms in both noiseless (Tu et al., 2015) and noisy (Wang et al., 2016) cases to obtain \u270f precision is O Nd2 log(1/\u270f)\n. Therefore, our algorithm is more efficient provided that   n, which is consistent with the result obtained by (Zhang et al., 2017b). In their work, they proposed an accelerated stochastic gradient descent method for matrix sensing based on the restricted isometry property. However, since the restricted isometry property is more restrictive than the restricted strong convex and smoothness conditions, their results cannot be applied to more general low-rank matrix recovery problems."}, {"heading": "3.2.2. MATRIX COMPLETION", "text": "We provide the theoretical guarantee of our algorithm for matrix completion. In particular, we consider a partial observation model, which means only the elements over a subset X \u2713 [d\n1 ] \u21e5 [d 2 ] are observed. In addition, we assume a uniform sampling model for X , which is defined as 8(j, k) 2 X , j \u21e0 uniform([d\n1 ]), k \u21e0 uniform([d 2 ]). To avoid overly sparse matrices (Gross, 2011; Negahban & Wainwright, 2012), we impose the following incoherence condition (Cande\u0300s & Recht, 2009). More specifically, suppose the singular value decomposition of X\u21e4 is X\u21e4 = U \u21e4 \u2303\u21e4V \u21e4> , we assume the following conditions hold kU\u21e4k 2,1  p r/d 1 and kV\u21e4k 2,1 \nA Universal Variance Reduction-Based Catalyst for Nonconvex Low-Rank Matrix Recovery p\nr/d 2 , where r denotes the rank of X\u21e4, and denotes the incoherence parameter for X\u21e4.\nIn order to make sure our produced estimator satisfies incoherence constraint, we need a projection step, which is displayed in Algorithm 1. Therefore, we construct two feasible sets C\ni\n= {A 2 Rdi\u21e5r kAk 2,1  \u21b5i}, where\n\u21b5 i =\np\nr 1 /d i , and i 2 {1, 2}. Thus for any U 2 C 1\nand V 2 C 2 , we have X = UV> 2 C = {A 2 Rd1\u21e5d2 kAk1,1  \u21b5}, where \u21b5 = r 1/ p d 1 d 2 .\nWe have the following convergence result of our algorithm for matrix completion. Corollary 3.14. Consider noisy matrix completion under uniform sampling model. Suppose X\u21e4 satisfies incoherence condition. There exist constants {c\ni }7 i=1 such that if we choose parameters \u2318 = c\n1 / 1 , where c 1 = \u00b50/ c 2 \n, m c\n3 2, and the number of observations satisfies N c\n4 r2d log d, then for any initial solution satisfies eZ0 2 B(c\n5 p r ), then with probability at least 1 c 6\n/d, the output of Algorithm 1 satisfies\nE[d2(eZS ,Z\u21e4) \u21e4  \u21e2Sd2(eZ0,Z\u21e4) + c 7\nrd log d\nN , (3.3)\nwhere = max{\u232b2, r 2 2 1 }, the contraction parameter \u21e2 < 1. Remark 3.15. Corollary 3.14 implies that after O log(N/(r2d log d))\nnumber of outer loops, our algorithm achieves O r p d log d/N\nstatistical error, which is near optimal compared with the minimax lower bound O( p\nrd log d/N) for matrix completion proved in Negahban & Wainwright (2012); Koltchinskii et al. (2011). And its sample complexity is O(r2d log d), which matches the best-known sample complexity of matrix completion using nonconvex matrix factorization (Zheng & Lafferty, 2016). Recall that from Remark 3.10, the overall computational complexity of our algorithm to reach \u270f accuracy for matrix completion is O (N + 2b)r3d log(1/\u270f)\n. However, for the state-of-the-art gradient descent based algorithms, the computational complexity for both noiseless (Zheng & Lafferty, 2016) and noisy (Wang et al., 2016) cases to obtain \u270f accuracy is O Nr3d log(1/\u270f)\n. Thus the computational complexity of our algorithm is lower than the state-of-the-art gradient descent methods if we have   n. In addition, for the online stochastic gradient descent algorithm (Jin et al., 2016), the overall computational complexity is O(r44d log(1/\u270f)). Since their results has a fourth power dependency on both r and , our method can yield a significant improvement over the online method when r, is large."}, {"heading": "4. Experiments", "text": "In this section, we present the experimental performance of our proposed algorithm for different models based on\nnumerical simulations and real data experiments."}, {"heading": "4.1. Numerical Simulations", "text": "We first investigate the effectiveness of our proposed algorithm compared with the state-of-the-art gradient descent algorithm (Wang et al., 2016; Zheng & Lafferty, 2016). Then, we evaluate the sample complexity required by both methods to achieve exact recovery in the noiseless case. Finally, we illustrate the statistical error of our method in the noisy case. Note that both algorithms use the same initialization method (Algorithm 2) with optimal parameters selected by cross validation. Furthermore, all results are averaged over 30 trials. Note that due to the space limit, we only lay out simulation results for matrix completion, results for other models can be found in Appendix A.\nFor matrix completion, we consider the unknown low-rank matrix X\u21e4 in the following settings: (i) d\n1 = 100, d 2 =\n80, r = 2; (ii) d 1 = 120, d 2 = 100, r = 3; (iii) d 1 = 140, d 2\n= 120, r = 4. First, we generate the unknown lowrank matrix X\u21e4 as X\u21e4 = U\u21e4V\u21e4>, where U\u21e4 2 Rd1\u21e5r and V\u21e4 2 Rd2\u21e5r are randomly generated. Next, we use uniform observation model to obtain data matrix Y. Finally, we consider two settings: (1) noisy case: the noise follows i.i.d. normal distribution with variance 2 = 0.25 and (2) noiseless case.\nFor the results of convergence rate, we show the mean squared error kbX X\u21e4k2\nF\n/(d 1 d 2 ) in log scale versus number of effective data passes. Figures 1(a) and 1(c) illustrate the linear rate of convergence of our algorithm (LRSVRG) in the setting (i). The results imply that after the same number of effective data passes, our algorithm is more efficient than the state-of-the-art gradient descent algorithm in estimation error. For the results of sample complexity, we illustrate the empirical probability of exact recovery under rescaled sample size N/(rd log d). For the estimator bX given by different algorithms, it is considered to achieve exact recovery, if the relative error kbX X\u21e4k\nF /kX\u21e4k F\nis less than 10 3. Figure 1(b) shows the empirical recovery probability of different methods in the setting (i). It implies a phase transition around N = 3rd log d. Although our theoretical results requires O(r2d log d) sample complexity, the simulation results suggest that our method achieves the optimal sample complexity N = O(rd log d). Note that we leave out results in other settings to avoid redundancy since we get similar patterns for these results. The results of statistical error are displayed in Figure 1(d), which is consistent with our main result in Corollary 3.14."}, {"heading": "4.2. Real Data Experiments", "text": "We apply our proposed stochastic variance-reduced gradient algorithm for matrix completion to collaborative filtering in recommendation system, and compare it with sev-\neral state-of-the-art matrix completion algorithms, including singular value projection (SVP) (Jain et al., 2010), trace norm constraint (TNC) (Jaggi et al., 2010), alternating minimization (AltMin) (Jain et al., 2013b), spectral regularization (SoftImpute) (Mazumder et al., 2010), rank-one matrix pursuit (Wang et al., 2014), nuclear norm penalty (Negahban & Wainwright, 2011), nonconvex SCAD penalty (Gui & Gu, 2015) and gradient descent (Zheng & Lafferty, 2016). In particular, we use three large recommendation datasets called Jester1, Jester2 and Jester3 (Goldberg et al., 2001), which contain anonymous ratings on 100 jokes from different users. The jester datasets consist of {24983, 23500, 24938} rows and 100 columns respectively, with {106, 106, 6 \u21e5 105} ratings correspondingly. Besides, the rating scales take value from [ 10, 10]. Our goal is to recover the whole rating matrix based on partial observations. Therefore, we randomly choose half of the ratings as our observed data, and predict the other half based on different matrix completion algorithms. We perform 10 different observed/unobserved entry splittings, and record the averaged root mean square error (RMSE) as well as CPU time for different algorithms. We summarize the comparisons in Table 1, which suggests that our proposed LRSVRG algorithm outperforms all the other baseline algorithms in terms of RMSE and CPU time, which aligns well with our theory."}, {"heading": "5. Conclusions and Future Work", "text": "We proposed a unified stochastic variance-reduced gradient descent framework for low-rank matrix recovery that integrates both optimization-theoretic and statistical analyses. Based on the mild restricted strong convexity and smoothness conditions, we derived a projected notion of the restricted Lipschitz continuous gradient property, and established the linear convergence rate of our proposed algorithm. With an appropriate initialization procedure, we proved that our algorithm enjoys improved computational complexity compared with existing approaches. There are still many interesting problems along this line of research. For example, we will study accelerating the low-rank plus sparse matrix/tensor recovery (Gu et al., 2014; 2016; Yi et al., 2016; Zhang et al., 2017a) through variance reduction technique in the future."}, {"heading": "Acknowledgment", "text": "We would like to thank the anonymous reviewers for their helpful comments. This research was sponsored in part by the National Science Foundation IIS-1618948 and IIS1652539. The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agencies."}], "year": 2017, "references": [{"title": "Fast global convergence rates of gradient methods for highdimensional statistical recovery", "authors": ["Agarwal", "Alekh", "Negahban", "Sahand", "Wainwright", "Martin J"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2010}, {"title": "Variance reduction for faster non-convex optimization", "authors": ["Allen-Zhu", "Zeyuan", "Hazan", "Elad"], "venue": "arXiv preprint arXiv:1603.05643,", "year": 2016}, {"title": "1-bit matrix completion under exact low-rank constraint", "authors": ["Bhaskar", "Sonia A", "Javanmard", "Adel"], "venue": "In Information Sciences and Systems (CISS),", "year": 2015}, {"title": "Dropping convexity for faster semi-definite optimization", "authors": ["Bhojanapalli", "Srinadh", "Kyrillidis", "Anastasios", "Sanghavi", "Sujay"], "venue": "arXiv preprint,", "year": 2015}, {"title": "Matrix completion for multi-label image classification", "authors": ["Cabral", "Ricardo Silveira", "De la Torre", "Fernando", "Costeira", "Jo\u00e3o Paulo", "Bernardino", "Alexandre"], "venue": "In NIPS,", "year": 2011}, {"title": "A max-norm constrained minimization approach to 1-bit matrix completion", "authors": ["Cai", "Tony", "Zhou", "Wen-Xin"], "venue": "Journal of Machine Learning Research,", "year": 2013}, {"title": "Exact matrix completion via convex optimization", "authors": ["Cand\u00e8s", "Emmanuel J", "Recht", "Benjamin"], "venue": "Foundations of Computational mathematics,", "year": 2009}, {"title": "The power of convex relaxation: Near-optimal matrix completion", "authors": ["Cand\u00e8s", "Emmanuel J", "Tao", "Terence"], "venue": "Information Theory, IEEE Transactions on,", "year": 2010}, {"title": "Accelerated stochastic block coordinate gradient descent for sparsity constrained nonconvex optimization", "authors": ["Chen", "Jinghui", "Gu", "Quanquan"], "venue": "In Conference on Uncertainty in Artificial Intelligence,", "year": 2016}, {"title": "Fast low-rank estimation by projected gradient descent: General statistical and algorithmic guarantees", "authors": ["Chen", "Yudong", "Wainwright", "Martin J"], "venue": "arXiv preprint arXiv:1509.03025,", "year": 2015}, {"title": "1-bit matrix completion", "authors": ["Davenport", "Mark A", "Plan", "Yaniv", "van den Berg", "Ewout", "Wootters", "Mary"], "venue": "Information and Inference,", "year": 2014}, {"title": "Global convergence of stochastic gradient descent for some non-convex matrix problems", "authors": ["De Sa", "Christopher", "Olukotun", "Kunle", "R\u00e9"], "venue": "arXiv preprint arXiv:1411.1134,", "year": 2014}, {"title": "Saga: A fast incremental gradient method with support for nonstrongly convex composite objectives", "authors": ["Defazio", "Aaron", "Bach", "Francis", "Lacoste-Julien", "Simon"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2014}, {"title": "Finito: A faster, permutable incremental gradient method for big data problems", "authors": ["Defazio", "Aaron J", "Caetano", "Tib\u00e9rio S", "Domke", "Justin"], "venue": "In Proceedings of the International Conference on Machine Learning,", "year": 2014}, {"title": "Eigentaste: A constant time collaborative filtering algorithm", "authors": ["Goldberg", "Ken", "Roeder", "Theresa", "Gupta", "Dhruv", "Perkins", "Chris"], "venue": "Information Retrieval,", "year": 2001}, {"title": "Recovering low-rank matrices from few coefficients in any basis", "authors": ["Gross", "David"], "venue": "IEEE Transactions on Information Theory,", "year": 2011}, {"title": "Robust tensor decomposition with gross corruption", "authors": ["Gu", "Quanquan", "Gui", "Huan", "Han", "Jiawei"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2014}, {"title": "Low-rank and sparse structure pursuit via alternating minimization", "authors": ["Gu", "Quanquan", "Wang", "Zhaoran Wang", "Liu", "Han"], "venue": "In Artificial Intelligence and Statistics,", "year": 2016}, {"title": "Towards faster rates and oracle property for low-rank matrix estimation", "authors": ["Gui", "Huan", "Gu", "Quanquan"], "venue": "arXiv preprint arXiv:1505.04780,", "year": 2015}, {"title": "Understanding alternating minimization for matrix completion", "authors": ["Hardt", "Moritz"], "venue": "In FOCS, pp. 651\u2013660", "year": 2014}, {"title": "Fast matrix completion without the condition number", "authors": ["Hardt", "Moritz", "Wootters", "Mary"], "venue": "In COLT, pp", "year": 2014}, {"title": "Computational limits for matrix completion", "authors": ["Hardt", "Moritz", "Meka", "Raghu", "Raghavendra", "Prasad", "Weitz", "Benjamin"], "venue": "In COLT, pp", "year": 2014}, {"title": "A simple algorithm for nuclear norm regularized problems", "authors": ["Jaggi", "Martin", "Sulovsk", "Marek"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "year": 2010}, {"title": "Fast exact matrix completion with finite samples", "authors": ["Jain", "Prateek", "Netrapalli", "Praneeth"], "venue": "arXiv preprint,", "year": 2014}, {"title": "Guaranteed rank minimization via singular value projection", "authors": ["Jain", "Prateek", "Meka", "Raghu", "Dhillon", "Inderjit S"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2010}, {"title": "Lowrank matrix completion using alternating minimization", "authors": ["Jain", "Prateek", "Netrapalli", "Praneeth", "Sanghavi", "Sujay"], "venue": "In STOC, pp", "year": 2013}, {"title": "Lowrank matrix completion using alternating minimization", "authors": ["Jain", "Prateek", "Netrapalli", "Praneeth", "Sanghavi", "Sujay"], "venue": "In Proceedings of the forty-fifth annual ACM symposium on Theory of computing,", "year": 2013}, {"title": "Provable efficient online matrix completion via non-convex stochastic gradient descent", "authors": ["Jin", "Chi", "Kakade", "Sham M", "Netrapalli", "Praneeth"], "venue": "arXiv preprint arXiv:1605.08370,", "year": 2016}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "authors": ["Johnson", "Rie", "Zhang", "Tong"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2013}, {"title": "Matrix completion from a few entries", "authors": ["Keshavan", "Raghunandan H", "Oh", "Sewoong", "Montanari", "Andrea"], "venue": "In 2009 IEEE International Symposium on Information Theory,", "year": 2009}, {"title": "Matrix completion from noisy entries", "authors": ["Keshavan", "Raghunandan H", "Montanari", "Andrea", "Oh", "Sewoong"], "venue": "Journal of Machine Learning Research,", "year": 2010}, {"title": "Nuclear-norm penalization and optimal rates for noisy low-rank matrix completion", "authors": ["Koltchinskii", "Vladimir", "Lounici", "Karim", "Tsybakov", "Alexandre B"], "venue": "The Annals of Statistics,", "year": 2011}, {"title": "Semi-stochastic gradient descent methods", "authors": ["Kone\u010dn\u1ef3", "Jakub", "Richt\u00e1rik", "Peter"], "year": 2013}, {"title": "ms2gd: Mini-batch semi-stochastic gradient descent in the proximal setting", "authors": ["Kone\u010dn\u1ef3", "Jakub", "Liu", "Jie", "Richt\u00e1rik", "Peter", "Tak\u00e1\u010d", "Martin"], "year": 2014}, {"title": "Regularized mestimators with nonconvexity: Statistical and algorithmic theory for local optima", "authors": ["Loh", "Po-Ling", "Wainwright", "Martin J"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2013}, {"title": "Incremental majorization-minimization optimization with application to large-scale machine learning", "authors": ["Mairal", "Julien"], "year": 2014}, {"title": "Spectral regularization algorithms for learning large incomplete matrices", "authors": ["Mazumder", "Rahul", "Hastie", "Trevor", "Tibshirani", "Robert"], "venue": "Journal of machine learning research,", "year": 2010}, {"title": "Estimation of (near) low-rank matrices with noise and high-dimensional scaling", "authors": ["Negahban", "Sahand", "Wainwright", "Martin J"], "venue": "The Annals of Statistics,", "year": 2011}, {"title": "Restricted strong convexity and weighted matrix completion: Optimal bounds with noise", "authors": ["Negahban", "Sahand", "Wainwright", "Martin J"], "venue": "Journal of Machine Learning Research,", "year": 2012}, {"title": "A unified framework for high-dimensional analysis of m-estimators with decomposable regularizers", "authors": ["Negahban", "Sahand", "Yu", "Bin", "Wainwright", "Martin J", "Ravikumar", "Pradeep K"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2009}, {"title": "Optimal statistical and computational rates for one bit matrix completion", "authors": ["Ni", "Renkun", "Gu", "Quanquan"], "venue": "In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics,", "year": 2016}, {"title": "Provable burermonteiro factorization for a class of norm-constrained matrix problems", "authors": ["Park", "Dohyung", "Kyrillidis", "Anastasios", "Bhojanapalli", "Srinadh", "Caramanis", "Constantine", "Sanghavi", "Sujay"], "year": 2016}, {"title": "Finding low-rank solutions to matrix problems, efficiently and provably", "authors": ["Park", "Dohyung", "Kyrillidis", "Anastasios", "Caramanis", "Constantine", "Sanghavi", "Sujay"], "venue": "arXiv preprint arXiv:1606.03168,", "year": 2016}, {"title": "Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization", "authors": ["Recht", "Benjamin", "Fazel", "Maryam", "Parrilo", "Pablo A"], "venue": "SIAM review,", "year": 2010}, {"title": "Stochastic variance reduction for nonconvex optimization", "authors": ["Reddi", "Sashank J", "Hefny", "Ahmed", "Sra", "Suvrit", "P\u00f3cz\u00f3s", "Barnab\u00e1s", "Smola", "Alex"], "venue": "arXiv preprint arXiv:1603.06160,", "year": 2016}, {"title": "Fast maximum margin matrix factorization for collaborative prediction", "authors": ["Rennie", "Jasson DM", "Srebro", "Nathan"], "venue": "In Proceedings of the 22nd international conference on Machine learning,", "year": 2005}, {"title": "Estimation of high-dimensional low-rank matrices", "authors": ["Rohde", "Angelika", "Tsybakov", "Alexandre B"], "venue": "The Annals of Statistics,", "year": 2011}, {"title": "Minimizing finite sums with the stochastic average gradient", "authors": ["Schmidt", "Mark", "Roux", "Nicolas Le", "Bach", "Francis"], "year": 2013}, {"title": "Maximum-margin matrix factorization", "authors": ["Srebro", "Nathan", "Rennie", "Jason", "Jaakkola", "Tommi S"], "venue": "In Advances in neural information processing systems,", "year": 2004}, {"title": "Guaranteed matrix completion via nonconvex factorization", "authors": ["Sun", "Ruoyu", "Luo", "Zhi-Quan"], "venue": "In Foundations of Computer Science (FOCS),", "year": 2015}, {"title": "Low-rank solutions of linear matrix equations via procrustes flow", "authors": ["Tu", "Stephen", "Boczar", "Ross", "Soltanolkotabi", "Mahdi", "Recht", "Benjamin"], "venue": "arXiv preprint arXiv:1507.03566,", "year": 2015}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "authors": ["R. Vershynin"], "venue": "arXiv preprint arXiv:1011.3027,", "year": 2010}, {"title": "A unified computational and statistical framework for nonconvex low-rank matrix estimation", "authors": ["Wang", "Lingxiao", "Zhang", "Xiao", "Gu", "Quanquan"], "venue": "arXiv preprint arXiv:1610.05275,", "year": 2016}, {"title": "Rank-one matrix pursuit for matrix completion", "authors": ["Wang", "Zheng", "Lai", "Ming-Jun", "Lu", "Zhaosong", "Fan", "Wei", "Davulcu", "Hasan", "Ye", "Jieping"], "venue": "In ICML, pp", "year": 2014}, {"title": "A proximal stochastic gradient method with progressive variance reduction", "authors": ["Xiao", "Lin", "Zhang", "Tong"], "venue": "SIAM Journal on Optimization,", "year": 2014}, {"title": "Speedup matrix completion with side information: Application to multi-label learning", "authors": ["Xu", "Miao", "Jin", "Rong", "Zhou", "Zhi-Hua"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2013}, {"title": "Fast algorithms for robust pca via gradient descent", "authors": ["Yi", "Xinyang", "Park", "Dohyung", "Chen", "Yudong", "Caramanis", "Constantine"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2016}, {"title": "Accelerated stochastic block coordinate descent with optimal sampling", "authors": ["Zhang", "Aston", "Gu", "Quanquan"], "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "year": 2016}, {"title": "A nonconvex free lunch for low-rank plus sparse matrix recovery", "authors": ["Zhang", "Xiao", "Wang", "Lingxiao", "Gu", "Quanquan"], "venue": "arXiv preprint arXiv:1702.06525,", "year": 2017}, {"title": "Stochastic variance-reduced gradient descent for low-rank matrix recovery from linear measurements", "authors": ["Zhang", "Xiao", "Wang", "Lingxiao", "Gu", "Quanquan"], "venue": "arXiv preprint arXiv:1701.00481,", "year": 2017}, {"title": "A nonconvex optimization framework for low rank matrix estimation", "authors": ["Zhao", "Tuo", "Wang", "Zhaoran", "Liu", "Han"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2015}, {"title": "A convergent gradient descent algorithm for rank minimization and semidefinite programming from random linear measurements", "authors": ["Zheng", "Qinqing", "Lafferty", "John"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2015}, {"title": "Convergence analysis for rectangular matrix completion using burer-monteiro factorization and gradient descent", "authors": ["Zheng", "Qinqing", "Lafferty", "John"], "venue": "arXiv preprint arXiv:1605.07051,", "year": 2016}], "id": "SP:860f67e34c2a6daa1138ea5793f8c1d0d7dbdda5", "authors": [{"name": "Lingxiao Wang", "affiliations": []}, {"name": "Xiao Zhang", "affiliations": []}, {"name": "Quanquan Gu", "affiliations": []}], "abstractText": "We propose a generic framework based on a new stochastic variance-reduced gradient descent algorithm for accelerating nonconvex low-rank matrix recovery. Starting from an appropriate initial estimator, our proposed algorithm performs projected gradient descent based on a novel semi-stochastic gradient specifically designed for low-rank matrix recovery. Based upon the mild restricted strong convexity and smoothness conditions, we derive a projected notion of the restricted Lipschitz continuous gradient property, and prove that our algorithm enjoys linear convergence rate to the unknown low-rank matrix with an improved computational complexity. Moreover, our algorithm can be employed to both noiseless and noisy observations, where the (near) optimal sample complexity and statistical rate can be attained respectively. We further illustrate the superiority of our generic framework through several specific examples, both theoretically and experimentally.", "title": "A Unified Variance Reduction-Based Framework for Nonconvex Low-Rank Matrix Recovery"}