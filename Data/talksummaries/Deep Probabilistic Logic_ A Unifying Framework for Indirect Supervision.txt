24	93	Biomedicine offers a particularly attractive application domain for exploring indirect supervision.
25	46	Biomedical literature grows by over one million each year1, making it imperative to develop machine reading methods for automating knowledge curation (Figure 2).
26	52	While crowd sourcing is hardly applicable, there are rich domain knowledge and structured resources to exploit for indirect supervision.
27	107	Using cross-sentence relation extraction and entity linking as case studies, we show that distant supervision, data programming, and joint inference can be seamlessly combined in DPL to substantially improve machine reading accuracy, without requiring any manually labeled examples.2
70	26	We then formulate the learning objective and show how it can be optimized using variational EM.
85	27	A hard constraint is the special case when wv =∞ (in practice, it suffices to set it to a large number, e.g., 10).
87	40	However, this may be suboptimal.
88	31	Therefore, we consider a general Bayesian learning setting where each wv is drawn from a prespecified prior distribution wv ∼ P (wv|αv).
89	16	Fixed wv amounts to the special case when the prior is concentrated on the preset value.
92	16	For example, for relation extraction, distant supervision from a knowledge base of known relations will set fKB(Xi, Yi) = I[In-KB(Xi, r)∧Yi = r], where In-KB(Xi, r) is true iff the entity tuple in Xi is known to have relation r in the KB.
97	31	Joint inference Constraints on instances or model expectations can be imposed by introducing the corresponding virtual evidence (Ganchev et al., 2010) (Proposition 2.1).
110	75	In this paper, however, we opted for a modular approach using variational EM.
115	24	Note that this inference problem is considerably simpler than endto-end inference with probabilistic logic, since the bulk of the computation is encapsulated by Ψ.
118	57	In the M-step, we treat the variational approximation qi(Yi) as probabilistic labels, and use them to optimize Φ and Ψ via standard supervised learning, which is equivalent to minimizing the KL divergence between the probabilistic labels and the conditional likelihood of Y given X under the supervision module (Φ) and prediction module (Ψ), respectively.
120	17	Likewise, for the supervision module, this optimization reduces to standard parameter learning for log-linear models (i.e., learning all wv’s that are not fixed).
125	34	The first expectation, on the other hand, requires probabilistic inference in the graphical model.
131	25	Here, distant supervision prefers classifying mention pairs of known relations, whereas the data programming formula opposes classifying instances resembling citations, and the joint inference formula ensures that at least one mention pair of a known relation is classified as positive.
134	28	A common strategy is to subsample negative examples to attain a balanced dataset.
137	19	In DPL, an additional challenge is that the labels are probabilistic and change over iterations.
143	31	While promising, their results still leave much room to improve.
145	108	In this section, we use cross-sentence relation extraction as a case study for combining indirect supervision using deep probabilistic logic (DPL).
147	16	Next, we apply DPL to entity linking itself and attain similar improvement.
150	15	Evaluation Comparing indirect supervision methods is challenging as there is often no annotated test set for evaluating precision and recall.
151	22	In such cases, we resort to the standard strategy used in prior work by reporting sample precision (estimated proportion of correct system extractions) and absolute recall (estimated number of correct system extractions).
152	67	Absolute recall is proportional to recall and can be used to compare different systems (modulo estimation errors).
162	36	Entity linking In this subsection, we used the entity linker from Literome (Poon et al., 2014) to identify drug, gene, and mutation mentions, as in Peng et al. (2017).
169	35	See Peng et al. (2017) for details.
170	39	Supervision module We used DPL to combine three indirect supervision strategies for crosssentence relation extraction (Table 1).
171	38	For distant supervision, we used GDKD and CIVIC as in Peng et al. (2017).
174	36	For development, we sampled 250 positive extractions from DPL using only distant supervision (Peng et al., 2017) and excluded them from future training and evaluation.
