2	59	While syntactic grammars (Marcus et al., 1993; Nivre, 2005) induce a rooted tree structure over the sentence by connecting verbal predicates to their arguments, these semantic representations often take the form of the more general labeled graph structure, and aim to capture a wider notion of propositions (e.g, nominalizations, adjectivals, or appositives).
3	31	In particular, we will focus on the three graph-based semantic representations collected in the Broad-Coverage Semantic Dependency Parsing SemEval shared task (SDP) (Oepen et al., 2015): (1) DELPH-IN Bi- Lexical Dependencies (DM) (Flickinger, 2000),1 (2) Enju Predicate-Argument Structures (PAS) (Miyao et al., 2014), and (3) Prague Semantic Dependencies (PSD) (Hajic et al., 2012).
5	28	In this work we take a novel approach to graph parsing, casting sentence-level semantic parsing as a multilingual machine-translation task (MT).
11	15	However, to the best of our knowledge, all such current methods actually sidestep the challenge of graph linearization – they reduce the input graph to a tree using lossy heuristics, which are specifi- 1 DM is automatically derived from Minimal Recursion Semantics (MRS) (Copestake et al., 1999).
29	24	SDP has enabled the application of machine learning models for the task.
41	64	For example, in neural machine translation, Aharoni and Goldberg (2017) showed that predicting targetside linearized syntactic trees can improve the quality and grammatically of the predicted translations.
51	25	We define the task of semantic translation, as converting to, and between, different sentencelevel semantic representations.
54	16	Preceding the input semantic representation are identifiers for source and target representation schemes (e.g., “PAS”, “DM” or “PSD”).
55	58	The semantic translation task is then to produceMtarget.
56	31	I.e., the sentence’s representation under the target formalism.
57	19	This definition is broad enough to encapsulate many sentence-level representations, and in this work we will use the three SDP representations, as well as an empty “RAW” representation (where E(G) = ∅ for all sentences) to allow for translations from raw input sentences.
62	74	While previous work have had certain structural constraints on their input (e.g., imposing tree or noncyclic constructions), in this work, we construct a lossless function which allows us to feed the sequence-to-sequence network with a linearized general graph representation and expect a linearized graph in its output.
64	29	We do this by converting an SDP graph such that all nodes are reachable from node v1.
72	17	For example, the words “can” and “greatest” in Figure 1a reside in different components, and therefore no single path (which traverses along the graph’s edge direction) will discover both of them.
73	41	To overcome this limitation, we make sure that all nodes are reachable from node v1, corresponding to the first word in the sentence, from which we start our traversal.
76	18	For example, revisit the previously mentioned “can” and “greatest” nodes in Figure 1a, which are connected using “SHIFT” edges.
80	21	Formally, our linearization of a given DFS traversal is composed of 3 types of elements (see Figure 2 for example): First, a Node reference identifies a node in the graph, which in turn corresponds to word in the SDP formalism.
81	62	We identify nodes using two tokens: (1) Their position in the sentence, relative to the previous node in the path (while the first position in the linearization is written in absolute terms, as “0”), and (2) Explicitly writing the word corresponding to the node.
95	16	In Section 6 we show empirically that our model benefits from explicitly generating these redundancies during decoding.
133	23	English word embeddings were fixed with 300-dimensional GloVe embeddings (Pennington et al., 2014), while the graph elements, which consist of a lexicon of roughly 400 tokens across three representations, were randomly initialized.
150	14	To simulate such scenario, we retrained the models on a randomly selected set of 33% of the train sentences for each representation (11, 886 sentences), such that the three representations overlap on only 10% (3, 565 sentences).
151	19	The results in Table 5 show that our approach is more resilient to the decrease in annotation overlap, outperforming the state-of-the-art model on the DM and PSD task, as well as on the average score.
154	117	This is done by presenting it with a linearized graph of one representation and asking it to translate it to another.
155	25	To the best of our knowledge, this is the first work which tries to accomplish this.
157	24	These evaluations provide several interesting comparisons between the representations: (1) For all representations, translating from any of the other two is easier than parsing from raw text, (2) The PAS and DM representations can be converted between them with high accuracy (95.7% and 96.1%, respectively).
158	55	This can be due to their structural resemblance, noted in previous work (Peng et al., 2017a; Oepen et al., 2015), and (3) While PSD serves as a viable input for conversion to DM and PAS (92.1% F1 on average), it is relatively harder to convert either of them to PSD (88.6%).
159	152	This might indicate that PSD subsumes some of the information in DM and PAS.
160	49	We presented a novel sequence-to-sequence approach to the task of semantic dependency parsing, by casting the problem as multi-lingual machine translation.
162	65	Following, we showed that our model, inspired by neural MT, benefits from the inter-task training signal, reaching performance almost on-par with current state of the art in several scenarios.
163	35	Future work can employ this linearization function within downstream applications, as was done with syntactic linearization, or extend this framework with other graph-based representations, such as universal dependencies (Nivre et al., 2016) or AMR (Banarescu et al., 2013).
