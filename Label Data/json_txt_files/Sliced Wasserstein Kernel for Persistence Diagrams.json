{"sections": [{"heading": "1. Introduction", "text": "Topological Data Analysis (TDA) is an emerging trend in data science, grounded on topological methods to design descriptors for complex data\u2014see e.g. (Carlsson, 2009) for an introduction to the subject. The descriptors of TDA can be used in various contexts, in particular statistical learning and geometric inference, where they provide useful insight into the structure of data. Applications of TDA can be found in a number of scientific areas, including computer vision (Li et al., 2014), materials science (Hiraoka et al., 2016), and brain science (Singh et al., 2008), to name\n1INRIA Saclay 2CREST, ENSAE, Universite\u0301 Paris Saclay. Correspondence to: Mathieu Carrie\u0300re <mathieu.carriere@inria.fr>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\na few. The tools developed in TDA are built upon persistent homology theory (Edelsbrunner & Harer, 2010; Oudot, 2015), and their main output is a descriptor called persistence diagram (PD), which encodes the topology of a space at all scales in the form of a point cloud with multiplicities in the plane R2\u2014see Section 2.1 for more details.\nPDs as features. The main strength of PDs is their stability with respect to perturbations of the data (Chazal et al., 2009b; 2013). On the downside, their use in learning tasks is not straightforward. Indeed, a large class of learning methods, such as SVM or PCA, requires a Hilbert structure on the descriptors space, which is not the case for the space of PDs. Actually, many simple operators of Rn, such as addition, average or scalar product, have no analogues in that space. Mapping PDs to vectors in Rn or in some infinite-dimensional Hilbert space is one possible approach to facilitate their use in discriminative settings.\nRelated work. A series of recent contributions have proposed kernels for PDs, falling into two classes. The first class of methods builds explicit feature maps: One can, for instance, compute and sample functions extracted from PDs (Bubenik, 2015; Adams et al., 2017; Robins & Turner, 2016); sort the entries of the distance matrices of the PDs (Carrie\u0300re et al., 2015); treat the PD points as roots of a complex polynomial, whose coefficients are concatenated (Fabio & Ferri, 2015). The second class of methods, which is more relevant to our work, defines implicitly feature maps by focusing instead on building kernels for PDs. For instance, Reininghaus et al. (2015) use solutions of the heat differential equation in the plane and compare them with the usual L2(R2) dot product. Kusano et al. (2016) handle a PD as a discrete measure on the plane, and follow by using kernel mean embeddings with Gaussian kernels\u2014see Section 4 for precise definitions. Both kernels are provably stable, in the sense that the metric they induce in their respective reproducing kernel Hilbert space (RKHS) is bounded above by the distance between PDs. Although these kernels are injective, there is no evidence that their induced RKHS distances are discriminative and therefore follow the geometry of the bottleneck distances, which are more widely accepted distances to compare PDs.\nContributions. In this article, we use the sliced Wasserstein (SW) distance (Rabin et al., 2011) to define a new ker-\nnel for PDs, which we prove to be both stable and discriminative. Specifically, we provide distortion bounds on the SW distance that quantify its ability to mimic bottleneck distances between PDs. This is in contrast to other kernels for PDs, which only focus on stability. We also propose a simple approximation algorithm to speed up the computation of that kernel, confirm experimentally its discriminative power and show that it outperforms experimentally both proposals of (Kusano et al., 2016) and (Reininghaus et al., 2015) in several supervised classification problems."}, {"heading": "2. Background on TDA and Kernels", "text": "We briefly review in this section relevant material on TDA, notably persistence diagrams, and technical properties of positive and negative definite kernel functions."}, {"heading": "2.1. Persistent Homology", "text": "Persistent homology (Zomorodian & Carlsson, 2005; Edelsbrunner & Harer, 2008; Oudot, 2015) is a technique inherited from algebraic topology for computing stable signatures on real-valued functions. Given f : X \u2192 R as input, persistent homology outputs a planar point set with multiplicities, called the persistence diagram of f and denoted by Dg f . See Figure 1 for an example. To understand the meaning of each point in this diagram, it suffices to know that, to compute Dg f , persistent homology considers the family of sublevel sets of f , i.e. the sets of the form f\u22121((\u2212\u221e, t]) for t \u2208 R, and it records the topological events (e.g. creation or merge of a connected component, creation or filling of a loop, void, etc.) that occur in f\u22121((\u2212\u221e, t]) as t ranges from \u2212\u221e to +\u221e. Then, each point p \u2208 Dg f represents the lifespan of a particular topological feature (connected component, loop, void, etc.), with its creation and destruction times as coordinates. See again Figure 1 for an illustration.\nFor the interested reader, we point out that the mathematical tool used by persistent homology to track the topological events in the family of sublevel sets is homological algebra, which turns the parametrized family of sublevel sets into a parametrized family of vector spaces and linear maps. Computing persistent homology then boils down to computing a family of bases for the vector spaces, which are compatible with the linear maps.\nDistance between PDs. We now define the pth diagram distance between PDs. Let p \u2208 N and Dg1,Dg2 be two PDs. Let \u0393 : Dg1 \u2287 A \u2192 B \u2286 Dg2 be a partial bijection between Dg1 and Dg2. Then, for any point x \u2208 A, the cost of x is defined as c(x) := \u2016x \u2212 \u0393(x)\u2016p\u221e, and for any point y \u2208 (Dg1 t Dg2) \\ (A t B), the cost of y is defined as c\u2032(y) := \u2016y \u2212 \u03c0\u2206(y)\u2016p\u221e, where \u03c0\u2206 is the projection onto the diagonal \u2206 = {(x, x) | x \u2208 R}. The cost c(\u0393)\nis defined as: c(\u0393) := ( \u2211 x c(x) + \u2211 y c \u2032(y))1/p. We then define the pth diagram distance dp as the cost of the best partial bijection between the PDs:\ndp(Dg1,Dg2) = inf \u0393 c(\u0393).\nIn the particular case p = +\u221e, the cost of \u0393 is defined as c(\u0393) := max{maxx \u03b4(x) + maxy \u03b4\u2032(y)}. The corresponding distance d\u221e is often called the bottleneck distance. One can show that dp \u2192 d\u221e when p \u2192 +\u221e. A fundamental property of PDs is their stability with respect to (small) perturbations of their originating functions. Indeed, the stability theorem (Bauer & Lesnick, 2015; Chazal et al., 2009a; 2016; Cohen-Steiner et al., 2007) asserts that for any f, g : X \u2192 R, we have\nd\u221e(Dg f, Dg g) \u2264 \u2016f \u2212 g\u2016\u221e, (1)\nSee again Figure 1 for an illustration.\nIn practice, PDs can be used as descriptors for data via the choice of appropriate filtering functions f , e.g. distance to the data in the ambient space, eccentricity, curvature, etc. The main strengths of the obtained descriptors are: (a) to be provably stable as mentioned previously; (b) to be invariant under reparametrization of the data; and (c) to encode information about the topology of the data, which is complementary and of an essentially different nature compared to geometric or statistical quantities. These properties have made persistence diagrams useful in a variety of contexts, including the ones mentioned in the introduction of the paper. For further details on persistent homology and on applications of PDs, the interested reader can refer e.g. to (Oudot, 2015) and the references therein."}, {"heading": "2.2. Kernel Methods", "text": "Positive Definite Kernels. Given a set X , a function k : X \u00d7 X \u2192 R is called a positive definite kernel if for all integers n, for all families x1, ..., xn of points in X , the matrix [k(xi, xj)]i,j is itself positive semi-definite. For brevity we will refer to positive definite kernels as kernels in the rest of the paper. It is known that kernels generalize scalar products, in the sense that, given a kernel k, there exists a Reproducing Kernel Hilbert Space (RKHS) Hk and a feature map \u03c6 : X \u2192 Hk such that k(x1, x2) = \u3008\u03c6(x1), \u03c6(x2)\u3009Hk . A kernel k also induces a distance dk on X that can be computed as the Hilbert norm of the difference between two embeddings:\nd2k(x1, x2) def. = k(x1, x1) + k(x2, x2)\u2212 2 k(x1, x2).\nWe will be particularly interested in this distance, since one of the goals we will aim for will be that of designing a kernel k for persistence diagrams such that dk has low distortion with respect to d1.\nNegative Definite and RBF Kernels. A standard way to construct a kernel is to exponentiate the negative of a Euclidean distance. Indeed, the Gaussian kernel for vectors with parameter \u03c3 > 0 does follow that template approach: k\u03c3(x, y) = exp ( \u2212\u2016x\u2212y\u2016 2\n2\u03c32\n) . An important theo-\nrem of Berg et al. (1984) (Theorem 3.2.2, p.74) states that such an approach to build kernels, namely setting\nk\u03c3(x, y) def. = exp ( \u2212f(x, y)\n2\u03c32\n) ,\nfor an arbitrary function f can only yield a valid positive definite kernel for all \u03c3 > 0 if and only if f is a negative semi-definite function, namely that, for all integers n, \u2200x1, ..., xn \u2208 X , \u2200a1, ..., an \u2208 Rn such that \u2211 i ai = 0,\u2211\ni,j aiajf(xi, xj) \u2264 0. Unfortunately, as observed in Appendix A of Reininghaus et al. (2014), d1 is not negative semi-definite (it only suffices to sample a family of point clouds to observe experimentally that more often than not the inequality above will be violated for a particular weight vector a). In this article, we use an approximation of d1 with the Sliced Wasserstein distance, which is provably negative semi-definite, and we use it to define a RBF kernel that can be easily tuned thanks to its bandwidth parameter \u03c3.\n2.3. Wasserstein distance for unnormalized measures on R\nThe Wasserstein distance (Villani, 2009, \u00a76) is a distance between probability measures. For reasons that will become clear in the next section, we will focus on a variant of that distance: the 1-Wasserstein distance for nonnegative, not necessarily normalized, measures on the real line (Santambrogio, 2015, \u00a72). Let \u00b5 and \u03bd be two nonnegative mea-\nsures on the real line such that |\u00b5| = \u00b5(R) and |\u03bd| = \u03bd(R) are equal to the same number r. We define the three following objects:\nW(\u00b5, \u03bd) = inf P\u2208\u03a0(\u00b5,\u03bd) \u222b\u222b R\u00d7R |x\u2212 y|P (dx,dy) (2)\nQr(\u00b5, \u03bd) = r \u222b R |M\u22121(x)\u2212N\u22121(x)|dx (3)\nL(\u00b5, \u03bd) = inf f\u22081\u2212Lipschitz \u222b R f(x)[\u00b5(dx)\u2212 \u03bd(dx)] (4)\nwhere \u03a0(\u00b5, \u03bd) is the set of measures on R2 with marginals \u00b5 and \u03bd, and M\u22121 and N\u22121 the generalized quantile functions of the probability measures \u00b5/r and \u03bd/r respectively.\nProposition 2.1. We haveW = Qr = L. Additionally (i) Qr is negative definite on the space of measures of mass r; (ii) for any three positive measures \u00b5, \u03bd, \u03b3 such that |\u00b5| = |\u03bd|, we have L(\u00b5+ \u03b3, \u03bd + \u03b3) = L(\u00b5, \u03bd).\nEquation (2) is the generic Kantorovich formulation of optimal transport, which is easily generalized to other cost functions and spaces, the variant being that we consider an unnormalized mass by reflecting it directly in the set \u03a0. The equality between (2) and (3) is only valid for probability measures on the real line. Because the cost function | \u00b7 | is homogeneous, we see that the scaling factor r can be removed when considering the quantile function and multiplied back. The equality between (2) and (4) is due to the well known Kantorovich duality for a distance cost (Villani, 2009, Particular case 5.4) which can also be trivially generalized to unnormalized measures, proving therefore the main statement of the proposition. The definition of Qr shows that the Wasserstein distance is the l1 norm of\nrM\u22121 \u2212 rN\u22121, and is therefore a negative definite kernel (as the l1 distance between two direct representations of \u00b5 and \u03bd as functions rM\u22121 and rN\u22121), proving point (i). The second statement is immediate.\nWe conclude with an important practical remark: for two unnormalized uniform empirical measures \u00b5 = \u2211n i=1 \u03b4xi\nand \u03bd = \u2211n i=1 \u03b4yi of the same size, with ordered x1 \u2264\n\u00b7 \u00b7 \u00b7 \u2264 xn and y1 \u2264 \u00b7 \u00b7 \u00b7 \u2264 yn, one has: L(\u00b5, \u03bd) =\u2211n i=1 |xi\u2212yi| = \u2016X\u2212Y \u20161, whereX = (x1, ..., xn) \u2208 Rn and Y = (y1, ..., yn) \u2208 Rn."}, {"heading": "3. The Sliced Wasserstein Kernel", "text": "In this section we define a new kernel between PDs, called the Sliced Wasserstein (SW) kernel, based on the Sliced Wasserstein metric of Rabin et al. (2011). The idea underlying this metric is to slice the plane with lines passing through the origin, to project the measures onto these lines whereW is computed, and to integrate those distances over all possible lines. Formally: Definition 3.1. Given \u03b8 \u2208 R2 with \u2016\u03b8\u20162 = 1, let L(\u03b8) denote the line {\u03bb \u03b8 | \u03bb \u2208 R}, and let \u03c0\u03b8 : R2 \u2192 L(\u03b8) be the orthogonal projection onto L(\u03b8). Let Dg1,Dg2 be two PDs, and let \u00b5\u03b81 := \u2211 p\u2208Dg1 \u03b4\u03c0\u03b8(p) and \u00b5 \u03b8 1\u2206 :=\u2211\np\u2208Dg1 \u03b4\u03c0\u03b8\u25e6\u03c0\u2206(p), and similarly for \u00b5 \u03b8 2, where \u03c0\u2206 is the orthogonal projection onto the diagonal. Then, the Sliced Wasserstein distance is defined as:\nSW(Dg1,Dg2) def. =\n1\n2\u03c0 \u222b S1 W(\u00b5\u03b81 + \u00b5\u03b82\u2206, \u00b5\u03b82 + \u00b5\u03b81\u2206)d\u03b8.\nNote that, by symmetry, one can restrict on the half-circle [\u2212\u03c02 , \u03c02 ] and normalize by \u03c0 instead of 2\u03c0. SinceQr is negative semi-definite, we can deduce that SW itself is negative semi-definite: Lemma 3.2. Let X be the set of bounded and finite PDs. Then, SW is negative semi-definite on X . Proof. Let n \u2208 N\u2217, a1, ..., an \u2208 R such that \u2211 i ai = 0 and\nDg1, ...,Dgn \u2208 X . Given 1 \u2264 i \u2264 n, we let \u00b5\u0303\u03b8i := \u00b5\u03b8i +\u2211 q\u2208Dgk,k 6=i \u03b4\u03c0\u03b8\u25e6\u03c0\u2206(q), \u00b5\u0303 \u03b8 ij\u2206 := \u2211 p\u2208Dgk,k 6=i,j\n\u03b4\u03c0\u03b8\u25e6\u03c0\u2206(p) and d = \u2211 i |Dgi|. Then:\u2211\ni,j\naiajW(\u00b5\u03b8i + \u00b5\u03b8j\u2206, \u00b5\u03b8j + \u00b5\u03b8i\u2206)\n= \u2211 i,j aiajL(\u00b5\u03b8i + \u00b5\u03b8j\u2206, \u00b5\u03b8j + \u00b5\u03b8i\u2206)\n= \u2211 i,j aiajL(\u00b5\u03b8i + \u00b5\u03b8j\u2206 + \u00b5\u03b8ij\u2206, \u00b5\u03b8j + \u00b5\u03b8i\u2206 + \u00b5\u03b8ij\u2206)\n= \u2211 i,j aiajL(\u00b5\u0303\u03b8i , \u00b5\u0303\u03b8j ) = \u2211 i,j aiajQd(\u00b5\u0303\u03b8i , \u00b5\u0303\u03b8j ) \u2264 0\nThe result follows by linearity of integration.\nHence, the theorem of Berg et al. (1984) allows us to define a valid kernel with:\nkSW(Dg1,Dg2) def. = exp\n( \u2212SW(Dg1,Dg2)\n2\u03c32\n) . (5)\nMetric equivalence. We now give the main theoretical result of this article, which states that SW is equivalent to d1. This has to be compared with (Reininghaus et al., 2015) and (Kusano et al., 2016), which only prove stability and injectivity. Our equivalence result states that the kSW, in addition to be stable and injective, also preserves the metric between PDs, which should intuitively lead to an improvement of the classification power. This intuition is illustrated in Section 4 and Figure 4, where we show an improvement of classification accuracies on several benchmark applications.\nTheorem 3.3. Let X be the set of bounded PDs with cardinalities bounded by N \u2208 N\u2217. Let Dg1,Dg2 \u2208 X . Then, one has:\nd1(Dg1,Dg2)\n2M \u2264 SW(Dg1,Dg2) \u2264 2\n\u221a 2d1(Dg1,Dg2),\nwhere M = 1 + 2N(2N \u2212 1).\nProof. Let s\u03b8 : Dg1 \u222a\u03c0\u2206(Dg2)\u2192 Dg2 \u222a\u03c0\u2206(Dg1) be the one-to-one bijection between Dg1 \u222a \u03c0\u2206(Dg2) and Dg2 \u222a \u03c0\u2206(Dg1) induced by W(\u00b5\u03b81 + \u00b5\u03b82\u2206, \u00b5\u03b82 + \u00b5\u03b81\u2206), and let s be the one-to-one bijection between Dg1 \u222a \u03c0\u2206(Dg2) and Dg2 \u222a \u03c0\u2206(Dg1) induced by the partial bijection achieving d1(Dg1,Dg2).\nUpper bound. Recall that \u2016\u03b8\u20162 = 1. We have: W(\u00b5\u03b81 + \u00b5\u03b82\u2206, \u00b5\u03b82 + \u00b5\u03b81\u2206) = \u2211 |\u3008p\u2212 s\u03b8(p), \u03b8\u3009|\n\u2264 \u2211 |\u3008p\u2212 s(p), \u03b8\u3009| \u2264 \u221a 2 \u2211 \u2016p\u2212 s(p)\u2016\u221e \u2264 2 \u221a\n2d1(Dg1,Dg2),\nwhere the sum is taken over all p \u2208 Dg1 \u222a \u03c0\u2206(Dg2). The upper bound follows by linearity.\nLower bound. The idea is to use the fact that s\u03b8 is a piecewise-constant function of \u03b8, and that it has at most 2+2N(2N \u22121) critical values \u03980, ...,\u0398M in [\u2212\u03c02 , \u03c02 ]. Indeed, it suffices to look at all \u03b8 such that \u3008p1\u2212p2, \u03b8\u3009 = 0 for some p1, p2 in Dg1 \u222a \u03c0\u2206(Dg2) or Dg2 \u222a \u03c0\u2206(Dg1). Then:\u222b \u0398i+1\n\u0398i\n\u2211 |\u3008p\u2212 s\u03b8(p), \u03b8\u3009|d\u03b8\n= \u2211 \u2016p\u2212 s\u0398i(p)\u20162 \u222b \u0398i+1 \u0398i |cos(\u2220(p\u2212 s\u0398i(p), \u03b8))|d\u03b8\n\u2265 \u2211 \u2016p\u2212 s\u0398i(p)\u20162(\u0398i+1 \u2212\u0398i)2/2\u03c0\n\u2265 (\u0398i+1 \u2212\u0398i)2d1(Dg1,Dg2)/2\u03c0,\nwhere the sum is again taken over all p \u2208 Dg1 \u222a \u03c0\u2206(Dg2), and where the inequality used to lower bound the integral of the cosine is obtained by concavity. The lower bound follows then from the Cauchy-Schwarz inequality.\nNote that the lower bound depends on the cardinalities of the PDs, and it becomes close to 0 if the PDs have a large number of points. On the other hand, the upper bound is oblivious to the cardinality. A corollary of Theorem 3.3 is that dkSW , the distance induced by kSW in its RKHS, is also equivalent to d1 in a broader sense: there exist continuous, positive and monotone functions g, h such that g(0) = h(0) = 0 and g \u25e6 d1 \u2264 dkSW \u2264 h \u25e6 d1. When the condition on the cardinalities of PDs is relaxed, e.g. when we only assume the PDs to be finite and bounded, with no uniform bound, the feature map \u03c6SW associated to kSW remains continuous and injective w.r.t. d1. This means that kSW can be turned into a universal kernel by considering exp(kSW) (cf Theorem 1 in (Kwitt et al., 2015)). This can be useful in a variety of tasks, including tests on distributions of PDs.\nComputation. In practice, we propose to approximate kSW in O(N log(N)) time using Algorithm 1. This algorithm first samples M directions in the half-circle S+1 ; it then computes, for each sample \u03b8i and for each PD Dg, the scalar products between the points of Dg and \u03b8i, to sort them next in a vector V\u03b8i(Dg). Finally, the `1-norm between the vectors is averaged over the sampled directions: SWM (Dg1,Dg2) = 1 M \u2211M i=1 \u2016V\u03b8i(Dg1) \u2212 V\u03b8i(Dg2)\u20161. Note that one can easily adapt the proof of Lemma 3.2 to show that SWM is negative semi-definite by using the linearity of the sum. Hence, this approximation remains a kernel. If the two PDs have cardinalities bounded by N , then the running time of this procedure is O(MN log(N)). This approximation of kSW is useful since, as shown in Section 4, we have observed empirically that just a few directions are sufficient to get good classification accuracies. Note that the exact computation of kSW is also possible in O(N2log(N)) time using the algorithm described in (Carrie\u0300re et al., 2017)."}, {"heading": "4. Experiments", "text": "In this section, we compare kSW to kPSS and kPWG on several benchmark applications for which PDs have been proven useful. We compare these kernels in terms of classification accuracies and computational cost. We review first our experimental setting, and then all our tasks.\nExperimental setting All kernels are handled with the LIBSVM (Chang & Lin, 2011) implementation of C-SVM, and results are averaged over 10 runs on a 2.4GHz Intel Xeon E5530 Quad Core. The\nAlgorithm 1 Computation of SWM Input: Dg1 = {p11 ... p1N1}, Dg2 = {p21 ... p2N2},M . Add \u03c0\u2206(Dg1) to Dg2 and vice-versa. Let SWM = 0; \u03b8 = \u2212\u03c0/2; s = \u03c0/M ; for i = 1 ... M do\nStore the products \u3008p1k, \u03b8\u3009 in an array V1; Store the products \u3008p2k, \u03b8\u3009 in an array V2; Sort V1 and V2 in ascending order; SWM = SWM + s\u2016V1 \u2212 V2\u20161; \u03b8 = \u03b8 + s;\nend for Output: (1/\u03c0)SWM ;\ncost factor C is cross-validated in the following grid: {0.001, 0.01, 0.1, 1, 10, 100, 1000}. Table 1 summarizes the number of labels, and the number of training and test instances for each task. Figure 2 illustrate how we use PDs to represent complex data. We first describe the two baselines we considered, along with their parameterization, followed by our proposal.\nPSS. The Persistence Scale Space kernel kPSS (Reininghaus et al., 2015) is defined as the scalar product of the two solutions of the heat diffusion equation with initial Dirac sources located at the PD points. It has the following closed form expression: kPSS(Dg1,Dg2) =\n1 8\u03c0t \u2211 p\u2208Dg1 \u2211 q\u2208Dg2 exp ( \u2212\u2016p\u2212q\u2016 2 8t ) \u2212 exp ( \u2212\u2016p\u2212q\u0304\u2016 2 8t ) , where q\u0304 = (y, x) is the symmetric of q = (x, y) along the diagonal. Since there is no clear heuristic on how to tune t, this parameter is chosen in the applications by ten-fold cross-validation with random 50%-50% training-test splits and with the following set of NPSS = 13 values: 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100, 500 and 1000.\nPWG. Let K, p, \u03c1 be positive parameters. Let k\u03c1 be the Gaussian kernel with parameter \u03c1 and associated RKHS H\u03c1. Let Dg1,Dg2 be two PDs, and let \u00b51 := \u2211 x\u2208Dg1 arctan(Kd\u221e(x,\u2206) p)k\u03c1(\u00b7, x) \u2208 H\u03c1 be the kernel mean embedding of Dg1 weigthed by the diagonal distances. Let \u00b52 be defined similarly.\nLet \u03c4 > 0. The Persistence Weighted Gaussian kernel kPWG (Kusano et al., 2016; 2017) is defined as kPWG(Dg1,Dg2) = exp ( \u2212\u2016\u00b51\u2212\u00b52\u2016H\u03c12\u03c42 ) , i.e. the Gaussian kernel with parameter \u03c4 on H\u03c1. The authors in (Kusano et al., 2016) provide heuristics to compute K, \u03c1 and \u03c4 and give a rule of thumb to tune p. Hence, in the applications we select p according to the rule of thumb, and we use ten-fold cross-validation with random 50%-50% training-test splits to chose K, \u03c1 and \u03c4 . The ranges of possible values is obtained by multiplying the values computed with the heuristics with the following range of 5 factors: 0.01, 0.1, 1, 10 and 100, leading to NPWG = 5\u00d7 5\u00d7 5 = 125 different sets of parameters.\nParameters for kSW. The kernel we propose has only one parameter, the bandwidth \u03c3 in Eq. (5), which we choose using ten-fold cross-validation with random 50%- 50% training-test splits. The range of possible values is obtained by computing the squareroot of the median, the first and the last deciles of all SW(Dgi,Dgj) in the training set, then by multiplying these values by the following range of 5 factors: 0.01, 0.1, 1, 10 and 100, leading to NSW = 5\u00d7 3 = 15 possible values.\nParameter Tuning. The bandwidth of kSW is, in practice, easier to tune than the parameters of its two competitors when using grid search. Indeed, as is the case for all infinitely divisible kernels, the Gram matrix does not need to be recomputed for each choice of \u03c3, since it only suffices to compute all the Sliced Wasserstein distances between PDs in the training set once. On the contrary, neither kPSS nor kPWG share this property, and require recomputations for each hyperparameter choice. Note however that this improvement may no longer hold if one uses other methods to tune parameters. For instance, using kPWG without cross-validation is possible with the heuristics given by the authors in (Kusano et al., 2016), and leads to smaller training times, but also to worse accuracies."}, {"heading": "4.1. 3D shape segmentation", "text": "Our first task, whose goal is to produce point classifiers for 3D shapes, follows that presented in (Carrie\u0300re et al., 2015).\nData. We use some categories of the mesh segmentation benchmark of Chen et al. (Chen et al., 2009), which contains 3D shapes classified in several categories (\u201cairplane\u201d, \u201chuman\u201d, \u201cant\u201d...). For each category, our goal is to design a classifier that can assign, to each point in the shape, a\nlabel that describes the relative location of that point in the shape. For instance, possible labels are, for the human category, \u201chead\u201d, \u201ctorso\u201d, \u201carm\u201d... To train classifiers, we compute a PD per point using the geodesic distance function to this point\u2014see (Carrie\u0300re et al., 2015) for details. We use 1-dimensional persistent homology (0-dimensional would not be informative since the shapes are connected, leading to solely one point with coordinates (0,+\u221e) per PD). For each category, the training set contains one hundredth of the points of the first five 3D shapes, and the test set contains one hundredth of the points of the remaining shapes in that category. Points in training and test sets are evenly sampled. See Figure 2. Here, we focus on comparison between PDs, and not on achieving state-of-the-art results. It has been proven that PDs bring complementary information to classical descriptors in this task\u2014see (Carrie\u0300re et al., 2015), hence reinforcing their discriminative power with appropriate kernels is of great interest. Finally, since data points are in R3, we set the p parameter of kPWG to 5.\nResults. Classification accuracies are given in Table 2. For most categories, kSW outperforms competing kernels by a significant margin. The variance of the results over the run is also less than that of its competitors. However, training times are not better in general. Hence, we also provide the results for an approximation of kSW with 10 directions. As one can see from Table 2 and from Figure 3, this approximation leaves the accuracies almost unchanged, while the training times become comparable with the ones of the\nother competitors. Moreover, according to Figure 3, using even less directions would slightly decrease the accuracies, but still outperform the competitors performances, while decreasing even more the training times."}, {"heading": "4.2. Orbit recognition", "text": "In our second experiment, we use synthetized data. The goal is to retrieve parameters of dynamical system orbits, following an experiment proposed in (Adams et al., 2017).\nData. We study the linked twist map, a discrete dynamical system modeling fluid flow. It was used in (Hertzsch et al., 2007) to model flows in DNA microarrays. Its orbits can be computed given a parameter r > 0 and initial positions (x0, y0) \u2208 [0, 1]\u00d7 [0, 1] as follows:\n{ xn+1 = xn + ryn(1\u2212 yn) mod 1 yn+1 = yn + rxn+1(1\u2212 xn+1) mod 1\nDepending on the values of r, the orbits may exhibit very different behaviors. For instance, as one can see in Figure 2, when r is 2, there seems to be no interesting topological features in the orbit, while voids form when r is 1. Following (Adams et al., 2017), we use 5 different parameters r = 2.5, 3.5, 4, 4.1, 4.3, that act as labels. For each parameter, we generate 100 orbits with 1000 points and random initial positions. We then compute the PDs of the distance functions to the point clouds with the GUDHI\nlibrary (The GUDHI Project, 2015) and we use them (in all homological dimensions) to produce an orbit classifier that predicts the parameter values, by training over a 70%-30% training-test split of the data. Since data points are in R2, we set the p parameter of kPWG to 4.\nResults. Since the PDs contain thousands of points, we use kernel approximations to speed up the computation of the Gram matrices. In order for the approximation error to be bounded by 10\u22123, we use an approximation of kSW with 6 directions (as one can see from Figure 3, this has a small impact on the accuracy), we approximate kPWG with 1000 random Fourier features (Rahimi & Recht, 2008), and we approximate kPSS using Fast Gauss Transform (Morariu et al., 2009) with a normalized error of 10\u221210. One can see from Table 2 that the accuracy is increased a lot with kSW. Concerning training times, there is also a large improvement since we tune the parameters with grid search. Indeed, each Gram matrix needs not be recomputed for each parameter when using kSW."}, {"heading": "4.3. Texture classification", "text": "Our last experiment is inspired from (Reininghaus et al., 2015) and (Li et al., 2014). We use the OUTEX00000 data base (Ojala et al., 2002) for texture classification.\nData. PDs are obtained for each texture image by computing first the sign component of CLBP descriptors (Guo et al., 2010) with radius R = 1 and P = 8 neighbors for each image, and then compute the persistent homology of this descriptor using the GUDHI library (The GUDHI Project, 2015). See Figure 2. Note that, contrary to the experiment of (Reininghaus et al., 2015), we do not downsample the images to 32\u00d7 32 images, but keep the original 128 \u00d7 128 images. Following (Reininghaus et al., 2015), we restrict the focus to 0-dimensional persistent homology. We also use the first 50%-50% training-test split given in the database to produce classifiers. Since data points are in R2, we set the p parameter of kPWG to 4.\nResults We use the same approximation procedure as in Section 4.2. According to Figure 3, even though the approximation of SW is rough, this has again a small impact on the accuracy, while reducing the training time by a significant margin. As one can see from Table 2, using kPSS leads to almost state-of-the-art results (Ojala et al., 2002; Guo et al., 2010), closely followed by the accuracies of kSW and kPWG. The best timing is given by kSW, again because we use grid search. Hence, kSW almost achieves the best result, and its training time is better than the ones of its competitors, due to the grid search parameter tuning.\nMetric Distortion. To illustrate the equivalence theorem, we also show in Figure 4 a scatter plot where each point\nrepresents the comparison of two PDs taken from the Airplane segmentation data set. Similar plots can be obtained with the other datasets considered here. For all points, the x-axis quantifies the first diagram distance d1 for that pair, while the y-axis is the logarithm of the RKHS distance induced by either kSW, kPSS, kPWG or a Gaussian kernel directly applied to d1, to obtain comparable quantities. We use the parameters given by the cross-validation procedure described above. One can see that the distances induced by kSW are less spread than the others, suggesting that the metric induced by kSW is more discriminative. Moreover the distances given by kSW and the Gaussian kernel on d1 exhibit the same behavior, suggesting that kSW is the best natural equivalent of a Gaussian kernel for PDs."}, {"heading": "5. Conclusion", "text": "In this article, we introduce the Sliced Wasserstein kernel, a new kernel for PDs that is provably equivalent to the first diagram distance between PDs. We provide fast algorithms to approximate it, and show on several datasets substantial improvements in accuracy and training times (when tuning parameters is done with grid search) over competing kernels. A particularly appealing property of that kernel is that it is infinitely divisible, substantially facilitating the tuning of parameters through cross validation.\nAcknowledgements. We thank the anonymous referees for their insightful comments. SO was supported by ERC grant Gudhi and by ANR project TopData. MC was supported by a chaire de l\u2019IDEX Paris Saclay."}], "year": 2017, "references": [{"title": "Persistence Images: A Stable Vector Representation of Persistent Homology", "authors": ["H. Adams", "T. Emerson", "M. Kirby", "R. Neville", "C. Peterson", "P. Shipman", "S. Chepushtanova", "E. Hanson", "F. Motta", "L. Ziegelmeier"], "venue": "Journal Machine Learning Research,", "year": 2017}, {"title": "Induced matchings and the algebraic stability of persistence barcodes", "authors": ["U. Bauer", "M. Lesnick"], "venue": "Journal of Computational Geometry,", "year": 2015}, {"title": "Statistical Topological Data Analysis using Persistence Landscapes", "authors": ["P. Bubenik"], "venue": "Journal Machine Learning Research,", "year": 2015}, {"title": "Topology and data", "authors": ["G. Carlsson"], "venue": "Bulletin American Mathematical Society,", "year": 2009}, {"title": "Stable Topological Signatures for Points on 3D Shapes", "authors": ["M. Carri\u00e8re", "S. Oudot", "M. Ovsjanikov"], "venue": "In Proceedings 13th Symposium Geometry Processing,", "year": 2015}, {"title": "LIBSVM: A library for support vector machines", "authors": ["C. Chang", "C. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "year": 2011}, {"title": "Proximity of persistence modules and their diagrams", "authors": ["F. Chazal", "D. Cohen-Steiner", "M. Glisse", "L. Guibas", "S. Oudot"], "venue": "In Proceedings 25th Symposium Computational Geometry,", "year": 2009}, {"title": "Gromov-Hausdorff Stable Signatures for Shapes using Persistence", "authors": ["F. Chazal", "D. Cohen-Steiner", "L. Guibas", "F. M\u00e9moli", "S. Oudot"], "venue": "Computer Graphics Forum,", "year": 2009}, {"title": "Persistence stability for geometric complexes", "authors": ["F. Chazal", "V. de Silva", "S. Oudot"], "venue": "Geometriae Dedicata,", "year": 2013}, {"title": "The structure and stability of persistence", "authors": ["F. Chazal", "V. de Silva", "M. Glisse", "S. Oudot"], "year": 2016}, {"title": "A Benchmark for 3D Mesh Segmentation", "authors": ["X. Chen", "A. Golovinskiy", "T. Funkhouser"], "venue": "ACM Trans. Graph.,", "year": 2009}, {"title": "Stability of persistence diagrams", "authors": ["D. Cohen-Steiner", "H. Edelsbrunner", "J. Harer"], "venue": "Discrete Computational Geometry,", "year": 2007}, {"title": "Computational Topology: an introduction", "authors": ["H. Edelsbrunner", "J. Harer"], "venue": "AMS Bookstore,", "year": 2010}, {"title": "Persistent homology-a survey", "authors": ["Edelsbrunner", "Herbert", "Harer", "John"], "venue": "Contemporary mathematics,", "year": 2008}, {"title": "Comparing persistence diagrams through complex vectors", "authors": ["Fabio", "B. Di", "M. Ferri"], "venue": "CoRR, abs/1505.01335,", "year": 2015}, {"title": "A completed modeling of local binary pattern operator for texture classification", "authors": ["Z. Guo", "L. Zhang", "D. Zhang"], "venue": "IEEE Trans. Image Processing,", "year": 2010}, {"title": "DNA microarrays: design principles for maximizing ergodic, chaotic mixing", "authors": ["Hertzsch", "J.-M", "R. Sturman", "S. Wiggins"], "venue": "In Small,", "year": 2007}, {"title": "Hierarchical structures of amorphous solids characterized by persistent homology", "authors": ["Y. Hiraoka", "T. Nakamura", "A. Hirata", "E. Escolar", "K. Matsue", "Y. Nishiura"], "venue": "In Proceedings National Academy of Science,", "year": 2016}, {"title": "Persistence Weighted Gaussian Kernel for Topological Data Analysis", "authors": ["G. Kusano", "K. Fukumizu", "Y. Hiraoka"], "venue": "In Proceedings 33rd International Conference on Machine Learning,", "year": 2016}, {"title": "Kernel method for persistence diagrams via kernel embedding and weight factor", "authors": ["G. Kusano", "K. Fukumizu", "Y. Hiraoka"], "year": 2017}, {"title": "Statistical Topological Data Analysis - A Kernel Perspective", "authors": ["Kwitt", "Roland", "Huber", "Stefan", "Niethammer", "Marc", "Lin", "Weili", "Bauer", "Ulrich"], "venue": "In Advances in Neural Information Processing Systems", "year": 2015}, {"title": "PersistenceBased Structural Recognition", "authors": ["C. Li", "M. Ovsjanikov", "F. Chazal"], "venue": "In Proceedings Conference Computer Vision Pattern Recognition,", "year": 2010}, {"title": "Automatic online tuning for fast Gaussian summation", "authors": ["V. Morariu", "B. Srinivasan", "V. Raykar", "R. Duraiswami", "L. Davis"], "venue": "In Advances Neural Information Processing Systems", "year": 2009}, {"title": "Outex - new framework for empirical evaluation of texture analysis algorithms", "authors": ["T. Ojala", "T. M\u00e4enp\u00e4\u00e4", "M. Pietik\u00e4inen", "J. Viertola", "J. Kyll\u00f6nen", "S. Huovinen"], "venue": "In Proceedings 16th International Conference Pattern Recognition,", "year": 2002}, {"title": "Persistence Theory: From Quiver Representations to Data Analysis", "authors": ["S. Oudot"], "venue": "American Mathematical Society,", "year": 2015}, {"title": "Wasserstein barycenter and its application to texture mixing", "authors": ["J. Rabin", "G. Peyr\u00e9", "J. Delon", "M. Bernot"], "venue": "In International Conference Scale Space Variational Methods Computer Vision, pp", "year": 2011}, {"title": "Random Features for Large-Scale Kernel Machines", "authors": ["A. Rahimi", "B. Recht"], "venue": "In Advances Neural Information Processing Systems", "year": 2008}, {"title": "A Stable Multi-Scale Kernel for Topological Machine Learning", "authors": ["J. Reininghaus", "S. Huber", "U. Bauer", "R. Kwitt"], "venue": "CoRR, abs/1412.6821,", "year": 2014}, {"title": "A Stable Multi-Scale Kernel for Topological Machine Learning", "authors": ["J. Reininghaus", "S. Huber", "U. Bauer", "R. Kwitt"], "venue": "In Proceedings Conference Computer Vision Pattern Recognition,", "year": 2015}, {"title": "Principal Component Analysis of Persistent Homology Rank Functions with case studies of Spatial Point Patterns, Sphere Packing and Colloids", "authors": ["V. Robins", "K. Turner"], "venue": "Physica D: Nonlinear Phenomena,", "year": 2016}, {"title": "Optimal transport for applied mathematicians", "authors": ["Santambrogio", "Filippo"], "venue": "Birka\u0308user,", "year": 2015}, {"title": "Topological analysis of population activity in visual cortex", "authors": ["G. Singh", "F. Memoli", "T. Ishkhanov", "G. Sapiro", "G. Carlsson", "D. Ringach"], "venue": "Journal of Vision,", "year": 2008}, {"title": "Optimal transport : old and new", "authors": ["C. Villani"], "year": 2009}, {"title": "Computing persistent homology", "authors": ["Zomorodian", "Afra", "Carlsson", "Gunnar"], "venue": "Discrete & Computational Geometry,", "year": 2005}], "id": "SP:0bd7a6d732db6541cdd02771cf471ae2e9e32ed9", "authors": [{"name": "Mathieu Carri\u00e8re", "affiliations": []}, {"name": "Marco Cuturi", "affiliations": []}, {"name": "Steve Oudot", "affiliations": []}], "abstractText": "Persistence diagrams (PDs) play a key role in topological data analysis (TDA), in which they are routinely used to describe topological properties of complicated shapes. PDs enjoy strong stability properties and have proven their utility in various learning contexts. They do not, however, live in a space naturally endowed with a Hilbert structure and are usually compared with non-Hilbertian distances, such as the bottleneck distance. To incorporate PDs in a convex learning pipeline, several kernels have been proposed with a strong emphasis on the stability of the resulting RKHS distance w.r.t. perturbations of the PDs. In this article, we use the Sliced Wasserstein approximation of the Wasserstein distance to define a new kernel for PDs, which is not only provably stable but also discriminative (with a bound depending on the number of points in the PDs) w.r.t. the first diagram distance between PDs. We also demonstrate its practicality, by developing an approximation technique to reduce kernel computation time, and show that our proposal compares favorably to existing kernels for PDs on several benchmarks.", "title": "Sliced Wasserstein Kernel for Persistence Diagrams"}