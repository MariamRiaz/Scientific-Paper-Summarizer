0	35	Neural machine translation (NMT) is an end-to-end approach to machine translation (Sutskever et al., 2014).
3	52	There have been several attempts to use convolutional encoder models for neural machine translation in the past but they were either only applied to rescoring n-best lists of classical systems (Kalchbrenner and Blunsom, 2013) or were not competitive to recurrent alternatives (Cho et al., 2014a).
5	38	For example, convolutional networks operate over a fixed-size window of the input sequence which enables the simultaneous computation of all features for a source sentence.
8	41	Because processing is bottom-up, all words undergo the same number of transformations, whereas for RNNs the first word is over-processed and the last word is transformed only once.
18	35	We use LSTMs (Hochreiter and Schmidhuber, 1997) for all decoder networks whose state si comprises of a cell vector and a hidden vector hi which is output by the LSTM at each time step.
23	19	Conditional input ci is a weighted sum of attention scores ai ∈ Rm and encoder outputs z.
34	27	As a remedy, we add position embeddings to encode the absolute position of each source word within a sentence.
37	19	Similar to the recurrent encoder (§2), the attention scores aij are computed from the pooled representations zj , however, the conditional input ci is a weighted sum of the embeddings ej , not zj , i.e., ej = wj + lj , zj = 1 k bk/2c∑ t=−bk/2c ej+t, ci = m∑ j=1 aijej The input sequence is padded prior to pooling such that the encoder output matches the input length |z| = |x|.
42	26	For instance, stacking 5 convolutions with kernel width k = 3 results in an input field of 11 words, i.e., each output depends on 11 input words, and the non-linearities allow the encoder to exploit the full input field, or to concentrate on fewer words as needed.
47	38	The final encoder consists of two stacked convolutional networks (Figure 1): CNN-a produces the encoder output zj to compute the attention scores ai, while the conditional input ci to the decoder is computed by summing the outputs of CNN-c, zj = CNN-a(e)j , ci = m∑ j=1 aij CNN-c(e)j .
127	24	Previous work also used multi-layer setups, e.g., (Chung et al., 2016) has two layers both in the encoder and the decoder with 1024 hidden units, and (Yang et al., 2016) use 1000 hidden units per LSTM.
132	100	For a singlelayer decoder, a deep convolutional encoder outperforms the BiLSTM accuracy by 0.3 BLEU and for a two-layer decoder, our very deep convolutional encoder with up to 20 layers outperforms the BiLSTM by 0.4 BLEU.
139	20	Figure 2 shows accuracy for a different number of layers of both CNNs with and without residual connections.
143	18	In general, choosing two to three times as many layers in CNN-a as in CNN-c is a good rule of thumb.
144	19	Without residual connections, the model fails to utilize the increase in modeling power from additional layers, and performance drops significantly for deeper encoders (Figure 2b).
145	36	Our convolutional architecture relies on two sets of networks, CNN-a for attention score computation ai and CNN-c for the conditional input ci to be fed to the decoder.
153	27	In Appendix B we investigate whether deep convolutional encoders are required for translating long sentences and observe that even relatively shallow encoders perform well on long sentences.
154	44	For training, we use the fast CuDNN LSTM implementation for layers without attention and experiment on IWSLT’14 with batch size 32.
155	20	The single-layer BiLSTM model trains at 4300 target words/second, while the 6/3 deep convolutional encoder compares at 6400 words/second on an NVidia Tesla M40 GPU.
158	36	We use vocabulary selection which can speed up generation by up to a factor of ten at no cost in accuracy via making the time to compute the final output layer negligible (Mi et al., 2016; L’Hostis et al., 2016).
164	26	Our best model on this dataset generates 203 words/second but at slightly lower accuracy compared to the full vocabulary setting in Table 2.
166	35	The smaller embedding size is not the only reason for the speed-up.
169	29	We believe that this is likely due to better cache locality for convolutional layers on CPUs: an LSTM with fused gates7 requires two big matrix multiplications with different weights as well as additions, multiplications and non-linearities for each source word, while the output of each convolutional layer can be computed as whole with a single matrix multiply.
173	75	We introduced a simple encoder model for neural machine translation based on convolutional networks.
175	79	We find it essential to use source position embeddings as well as different CNNs for attention score computation and conditional input aggregation.
176	41	Our experiments show that convolutional encoders perform on par or better than baselines based on bi-directional LSTM encoders.
179	81	Future work includes better training to enable faster convergence with the convolutional encoder to better leverage the higher processing speed.
182	34	In Figure 4 and Figure 5, we plot attention scores for a sample WMT’15 English-German and WMT’14 English-French translation with BiLSTM and deep convolutional encoders.
185	27	For CNN encoders the scores are less focused but still indicate an approximate source location, e.g., in Figure 4b, when moving the clause ”over 1,000 people were taken hostage” to the back of the translation.
186	38	For some models, attention maxima are consistently shifted by one token as both in Figure 4b and Figure 5b.
