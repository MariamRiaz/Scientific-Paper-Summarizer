{"sections": [{"text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 662\u2013666 Melbourne, Australia, July 15 - 20, 2018. c\u00a92018 Association for Computational Linguistics\n662"}, {"heading": "1 Introduction", "text": "Simultaneous Interpretation (SI) is an inherently difficult task that carries significant cognitive and attentional burdens. The role of the simultaneous interpreter is to accurately render the source speech in a given target language in a timely and precise manner. Interpreters employ a range of strategies, including generalization and summarization, to convey the source message as efficiently and reliably as possible (He et al., 2016). Unfortunately, the interpreter is pitched against the limits of human memory and stamina, and after only minutes of interpreting, the number of errors made by an interpreter begins to increase exponentially (Moser-Mercer et al., 1998).\n1https://github.com/craigastewart/qe sim interp\nWe examine the task of estimating simultaneous interpreter performance: automatically predicting when interpreters are interpreting smoothly and when they are struggling. This has several immediate potential applications, one of which being in Computer-Assisted Interpretation (CAI). CAI is quickly gaining traction in the interpreting community, with software products such as InterpretBank (Fantinouli, 2016) deployed in interpreting booths to provide live and interactive terminology support. Figure 1(b) shows how this might work; both the interpreter and the CAI system receive the source message and the system displays assistive information in the form of terminology and informational support.\nWhile this might improve the quality of interpreter output, there is a danger that these systems will provide too much information and increase the cognitive load imposed upon the interpreter (Fantinouli, 2018). Intuitively, the ideal level of support depends on current interpreter performance. The system can minimize distraction by providing assistance only when an interpreter is struggling. This level of support could be moderated appropriately if interpreter performance can be accurately predicted. Figure 1(c) demonstrates\nhow our proposed quality estimation (QE) system receives and evaluates interpreter output, allowing the CAI system to appropriately lower the amount of information passed to the interpreter, maximizing the quality of interpreter output.\nAs a concrete method for estimating interpreter performance, we turn to existing work on QE for machine translation (MT) systems (Specia et al., 2010, 2015), which takes in the source sentence and MT-generated outputs and estimates a measure of quality. In doing so, we arrive at two natural research questions:\n1. Do existing methods for performing QE on MT output also allow for accurate estimation of interpreter performance, despite the inherent differences between MT and SI?\n2. What unique aspects of the problem of interpreter performance estimation, such as the availability of prosody and other linguistic cues, can be exploited to further improve the accuracy of our predictions?\nThe remainder of the paper describes methods and experiments on English-Japanese (ENJA), English-French (EN-FR), and English-Italian (EN-IT) interpretation data attempting to answer these questions."}, {"heading": "2 Quality Estimation", "text": "Blatz et al. (2004) first proposed the problem of measuring the quality of MT output as a prediction task, given that existing metrics such as BLEU (Papineni et al., 2002) rely on the availability of reference translations to evaluate MT output quality, which aren\u2019t always available. As such, QE has since received widespread attention in the MT community and since 2012 has been included as a task in the Workshop on Statistical Machine Translation (Callison-Burch et al., 2012), using approaches ranging from linear classifiers (Ueffing and Ney, 2007; Luong et al., 2014) to neural models (Martins et al., 2016, 2017).\nQuEst++ (Specia et al., 2015) is a well-known QE pipeline that supports word-level, sentencelevel, and document-level QE. Its effectiveness and flexibility make it an attractive candidate for our proposed task. There are two main modules to QuEst++: a feature extractor and a learning module. The feature extractor produces an intermediate representation of the source and translation in\na continuous feature vector. The goal of the learning module, given a source and translation pair, is to predict the quality of the translation, either as a label or as a continuous value. This module is trained on example translations that have an assigned score (such as BLEU) and then predicts the score of a new example. QuEst++ offers a range of learning algorithms but defaults to Support Vector Regression for sentence-level QE."}, {"heading": "3 Quality Estimation for Interpretation", "text": "The default, out-of-the-box, sentence-level feature set for QuEst++ includes seventeen features such as number of tokens in source/target utterances, average token length, n-gram frequency, etc. (Specia et al., 2015). While this feature set is effective for evaluation of MT output, SI output is inherently different\u2014full of pauses, hesitations, paraphrases, re-orderings and repetitions. In the following sections, we describe our methods to adapt QE to handle these phenomena."}, {"heading": "3.1 Interpretation-specific Features", "text": "To adapt QE to interpreter output, we augment the baseline feature set with four additional types of features that may indicate a struggling interpreter.\nRatio of pauses/hesitations/incomplete words: Sridhar et al. (2013) propose that interpreters regularly use pauses to gain more time to think and as a cognitive strategy to manage memory constraints. An increased number of hesitations or incomplete words in interpreter output might indicate that an interpreter is struggling to produce accurate output. In our particular case, both corpora we use in experiments are annotated for pauses and partial renditions of words.\nRatio of non-specific words: Interpreters often compress output by replacing or omitting common nouns to avoid specific terminology (Sridhar et al., 2013), either to prevent redundancy or to ease cognitive load. For example: \u201cThe chairman explained the proposal to the delegates\u201d might be rendered in a target language as \u201che explained it to them.\u201d To capture this, we include a feature that checks for words from a pre-determined seed list of pronouns and demonstrative adjectives.\nRatio of \u2018quasi-\u2019cognates: In related language pairs, often words of a similar root are orthographically similar, for example \u201cartificial\u201d(EN), \u201cartificiel\u201d(FR) and \u201cartificiale\u201d(IT). Likewise in\nJapanese, words adapted from English are transcribed in katakana script to indicate their foreign origin. Transliterated words in interpreted speech could represent facilitated translation by language proximity, or an attempt to produce an approximation of a word that the interpreter did not know. We include a feature that counts the number of words that share at least 50% identical orthography (for EN, FR, IT) or are rendered in the interpreter transcript in katakana (JA).\nRatio of number of words: We further include three features from the bank of features provided with QuEst++ that compare source and target length and the amount of transcribed punctuation. Information about utterance length makes sense in an interpreting scenario, given the aforementioned strategies of omission and compression of information. A list, for example, may be compressed to avoid redundancy or may be an erroneous omission (Barik, 1994)."}, {"heading": "3.2 Evaluation Metric", "text": "Novice interpreters are assessed for accuracy on the number of omissions, additions and the inaccurate renditions of lexical items and longer phrases (Altman, 1994), but recovery of content and correct terminology are highly valued. While no large corpus exists that has been manually annotated with these measures, they align with the phenomena that MT evaluation tries to solve. One important design decision is which evaluation metric to target in our QE system. There is an abundance of evaluation metrics available for MT including WER (Su et al.), BLEU (Papineni et al., 2002), NIST (Doddington, 2002) and METEOR (Denkowski and Lavie, 2014), all of which compare the similarity between reference translations and translations. Interpreter output is fundamentally different from any reference that we may use in evaluation because interpreters employ a range of economizing strategies such as segmentation, omission, generalization, and reformulation (Riccardi, 2005). As such, measuring interpretation quality by some metrics employed in MT such as BLEU can result in artificially low scores (Shimizu et al., 2013). To mitigate this, we use METEOR, a more sophisticated MT evaluation metric that considers paraphrases and contentfunction word distinctions, and thus should be better equipped to deal with the disparity between MT and SI. Better handling of these divergences\nfor evaluation of interpreter output, or fine-grained evaluation based on measures from interpretation studies is an interesting direction for future work."}, {"heading": "4 Data: Interpretation Corpora", "text": "For our EN-JA language data we train the pipeline on combined data from seven TED Talks taken from the NAIST TED SI corpus (Shimizu et al., 2013). This corpus provides human transcribed SI output from three interpreters of low, intermediate and high levels of proficiency denoted B-rank, Arank and S-rank respectively, with 559 utterances from each interpreter. The corpus also provides written translations of the source speech, which we use as reference translations when evaluating interpreter output using METEOR.\nOur EN-FR and EN-IT data are drawn from the EPTIC corpus (Bernardini et al., 2016), which provides source and interpreter transcripts for speeches from the European Parliament (manually transcribed to include vocal expressions), as well as translations of transcripts of the source speech. The EN-FR and EN-IT datasets contain 739 and 731 utterances respectively. While the EPTIC translations are accurate, they were created from an official transcript that differs significantly in register from the source speech. As a proxy for our experiments, we generated translations of the original speech using Google Translate, which resulted in much more qualitatively reliable METEOR scores than the EPTIC translations."}, {"heading": "5 Interpreter Quality Experiments", "text": "To evaluate the quality of our QE system, we use the Pearson\u2019s r correlation between the predicted and true METEOR for each language pair (Graham, 2015). As a baseline, we train QuEst++ on the out-of-the-box feature set (Section 2).\nWe use k-fold cross-validation individually on EN-JA, EN-FR, and EN-IT source-interpreter language pairs with a held-out development set and test set for each fold. For each experiment setting, we run the experiment for each fold (ten iterations for each set) and evaluate average Pearson\u2019s r correlation on the development set.\nIn our baseline setting, we extract features based on the default QuEst++ sentence-level feature set (baseline). We ablate baseline features through cross-validation and remove features relating to bigram and trigram frequency and punctuation frequency in the source utterance, creating\na more effective trimmed model (trimmed). Subsequently, we add our interpreter features (Section 3.1) and arrive at our proposed model. We then repeat each experiment using the test set data from each fold and compare the resulting average Pearson\u2019s r scores."}, {"heading": "5.1 Results", "text": "Table 1 shows our primary results comparing the baseline, trimmed, and proposed feature sets. Our first observation is that, even with the baseline feature set, QE obtains respectable correlation scores, proving feasible as a method to predict interpreter performance. Our trimmed feature set performs moderately better than the baseline for Japanese, and slightly under-performs for French and Italian. However, our proposed, interpreter-focused model out-performs in all language settings with notable gains in particular for EN-JA(A-Rank) (+0.104), achieving its highest accuracy on the EN-FR dataset. Over all datasets, the gain of the proposed model is statistically significant at p < 0.05 by the pairwise bootstrap (Koehn, 2004)."}, {"heading": "5.2 Analysis", "text": "We further present two analyses: ablation on the full feature set and a qualitative comparison. Table 2 iteratively reduces the feature set by first removing the \u2018quasi-\u2019cognate feature (w/o cog); specific words (w/o spec); pauses, hesitations, and incomplete words (w/o fill); and finally sentence length and punctuation differences (w/o length).\nRelative difference in utterance length appears to aid Japanese and French above other languages. Cognates are particularly useful in EN-FR and EN-IT; this may be indicative of the corpus domain (European Parliament proceedings being rich in Latinate legalese) or of cognate frequency in those languages. In Japanese, cognates were\nmore indicative of quality for the more skilled interpreter (S-rank). While pauses and hesitations seem to aid the model in EN-FR and EN-IT, they appear to hinder EN-JA.\nBelow is a qualitative EN-IT example with a METEOR score of 0.079 (being substantially lower than the average METEOR score across all datasets; 0.262). The baseline model prediction of its score was 0.127, and our proposed model, 0.066:\nSOURCE: \u201cWill the Parliament grant President Dilma Rousseff, on the very first occasion after her groundbaking groundbreaking election and for no sound formal reason, the kind of debate that we usually reserve for people like Mugabe? So, I ask you to remove Brazil from the agenda of the urgencies.\u201d\nINTERP: \u201cEhm il Parlamento... dopo le elezioni... daremdar spazio a un dibattito sul ehm sul caso per esempio del presidente Mugabe invece di mettere il Brasile all\u2019ordine del giorno?\u201d\nGLOSS: \u201cEhm the Parliament... after the elections... we\u2019ll gi- will give way to a debate on the ehm on the case for example of President Mugabe instead of putting Brazil on the agenda?\u201d\nOur model can better capture the issues in this example because it has many interpretation specific qualities (pauses, compression, and omission). This is an example in which a CAI system might offer assistance to an interpreter struggling to produce an accurate rendition."}, {"heading": "6 Conclusion", "text": "We introduce a novel and effective application of QE to evaluate interpreter output, which could be immediately applied to allow CAI systems to selectively offer assistance to struggling interpreters. This work uses METEOR to evaluate interpreter output, but creation of fine-grained mea-\nsures to evaluate various aspects of interpreter performance is an interesting avenue for future work."}, {"heading": "Acknowledgements", "text": "This material is based upon work supported by the National Science Foundation under Grant No. 1748663 and Graduate Research Fellowship No. DGE1745016."}], "year": 2018, "references": [{"title": "Error Analysis in the Teaching of Simultaneous Interpreting: A Pilot Study", "authors": ["Janet Altman."], "venue": "John Benjamins Publishing Company, Edinburgh, Scotland.", "year": 1994}, {"title": "A Description of Various Types of Omissions, Additions and Errors of Translation Encountered in Simultaneous Interpretation", "authors": ["Henry Barik."], "venue": "John Benjamins Publishing Company, North Carolina, USA.", "year": 1994}, {"title": "From EPIC to EPTIC exploring simplification in interpreting and translation from an intermodal perspective", "authors": ["Silvia Bernardini", "Adriano Ferraresi", "Maja Milievi."], "venue": "Target. International Journal of Translation Studies, 28(1):61\u201386.", "year": 2016}, {"title": "Confidence estimation for machine translation", "authors": ["John Blatz", "Erin Fitzgerald", "George Foster", "Simona Gandrabur", "Cyril Goutte", "Alex Kulesza", "Alberto Sanchis", "Nicola Ueffing."], "venue": "Proceedings of the 20th International Conference on Computational", "year": 2004}, {"title": "Findings of the 2012 workshop on statistical machine translation", "authors": ["Chris Callison-Burch", "Philipp Koehn", "Christof Monz", "Matt Post", "Radu Soricut", "Lucia Specia."], "venue": "Proc. WMT.", "year": 2012}, {"title": "Meteor universal: Language specific translation evaluation for any target language", "authors": ["Michael Denkowski", "Alon Lavie."], "venue": "Proc. WMT.", "year": 2014}, {"title": "Automatic evaluation of machine translation quality using n-gram cooccurrence statistics", "authors": ["George Doddington."], "venue": "Proc. HLT, pages 138\u2013145.", "year": 2002}, {"title": "Interpretbank", "authors": ["Claudio Fantinouli."], "venue": "redefining computer-assisted interpreting tools. In Proceedings of the 38th Conference Translating and the Computer, pages 42\u201355.", "year": 2016}, {"title": "Trends in e-tools and resources for translators and interpreters", "authors": ["Claudio Fantinouli."], "venue": "Isabel Durn and Gloria Corpas (eds.), pages 153\u2013174.", "year": 2018}, {"title": "Improving evaluation of machine translation quality estimation", "authors": ["Yvette Graham."], "venue": "Proc. ACL, pages 1804\u20131813.", "year": 2015}, {"title": "Interpretese vs", "authors": ["He He", "Jordan Boyd-Graber", "Hal Daum\u00e9 III."], "venue": "translationese: The uniqueness of human strategies in simultaneous interpretation. In Proc. NAACL, pages 971\u2013976.", "year": 2016}, {"title": "Statistical significance tests for machine translation evaluation", "authors": ["Philipp Koehn."], "venue": "Proc. EMNLP, pages 388\u2013395.", "year": 2004}, {"title": "LIG system for word level QE task at WMT14", "authors": ["Ngoc-Quang Luong", "Laurent Besacier", "Benjamin Lecouteux."], "venue": "pages 335\u2013341.", "year": 2014}, {"title": "Unbabel\u2019s participation in the WMT16 word-level translation quality estimation shared task", "authors": ["Andr\u00e9 Martins", "Ramon Astudillo", "Chris Hokamp", "Fabio Kepler."], "venue": "pages 806\u2013811.", "year": 2016}, {"title": "Pushing the limits of translation quality estimation", "authors": ["Andr\u00e9 F.T. Martins", "Marcin Junczys-Dowmunt", "Fabio N. Kepler", "Ramn Astudillo", "Chris Hokamp", "Roman Grundkiewicz."], "venue": "Transactions of the Association for Computational Linguistics,", "year": 2017}, {"title": "Prolonged turns in interpreting: Effects on quality, physiological and psychological stress", "authors": ["B Moser-Mercer", "A Knzli", "M Korac."], "venue": "INTERPRETING, pages 47\u201365.", "year": 1998}, {"title": "BLEU: a method for automatic evaluation of machine translation", "authors": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proc. ACL, pages 311\u2013318.", "year": 2002}, {"title": "On the evolution of interpreting strategies in simultaneous interpreting", "authors": ["Alessandra Riccardi."], "venue": "Meta, 50(2):753\u2013767.", "year": 2005}, {"title": "Constructing a speech translation system using simultaneous interpretation data", "authors": ["Hiroaki Shimizu", "Graham Neubig", "Sakriani Sakti", "Tomoki Toda", "Satoshi Nakamura."], "venue": "Proc. IWSLT, pages 212\u2013218.", "year": 2013}, {"title": "Multi-level translation quality prediction with quest++", "authors": ["Lucia Specia", "Gustavo Paetzold", "Carolina Scarton."], "venue": "Proc. ACL, pages 115\u2013120.", "year": 2015}, {"title": "Machine translation evaluation versus quality estimation", "authors": ["Lucia Specia", "Dhwaj Raj", "Marco Turchi."], "venue": "Machine Translation, 24(1):39\u201350.", "year": 2010}, {"title": "Segmentation strategies for streaming speech translation", "authors": ["Vivek Kumar Rangarajan Sridhar", "John Chen", "Srinivas Bangalore", "Andrej Ljolje", "Rathinavelu Chengalvarayan."], "venue": "Proc. NAACL, pages 230\u2013 238.", "year": 2013}, {"title": "Wordlevel confidence estimation for machine translation", "authors": ["Nicola Ueffing", "Hermann Ney."], "venue": "Computational Linguistics, 33(1):9\u201340.", "year": 2007}], "id": "SP:2aaf6cc01666cc733318b05e050988d75323cccd", "authors": [{"name": "Craig Stewart", "affiliations": []}, {"name": "Nikolai Vogler", "affiliations": []}, {"name": "Junjie Hu", "affiliations": []}, {"name": "Jordan Boyd-Graber", "affiliations": []}, {"name": "Graham Neubig", "affiliations": []}], "abstractText": "Simultaneous interpretation, translation of the spoken word in real-time, is both highly challenging and physically demanding. Methods to predict interpreter confidence and the adequacy of the interpreted message have a number of potential applications, such as in computerassisted interpretation interfaces or pedagogical tools. We propose the task of predicting simultaneous interpreter performance by building on existing methodology for quality estimation (QE) of machine translation output. In experiments over five settings in three language pairs, we extend a QE pipeline to estimate interpreter performance (as approximated by the METEOR evaluation metric) and propose novel features reflecting interpretation strategy and evaluation measures that further improve prediction accuracy.1", "title": "Automatic Estimation of Simultaneous Interpreter Performance"}